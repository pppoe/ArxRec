<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260113.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization", "author": "Wei-Tse Cheng and Yen-Jen Chiou and Yuan-Fu Yang", "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Project page:https://breeze1124.github.io/rgs-slam-project-page/", "link": "http://arxiv.org/abs/2601.00705v2", "date": "2026-01-13", "relevancy": 3.4948, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8041}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6543}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization&body=Title%3A%20RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization%0AAuthor%3A%20Wei-Tse%20Cheng%20and%20Yen-Jen%20Chiou%20and%20Yuan-Fu%20Yang%0AAbstract%3A%20We%20introduce%20RGS-SLAM%2C%20a%20robust%20Gaussian-splatting%20SLAM%20framework%20that%20replaces%20the%20residual-driven%20densification%20stage%20of%20GS-SLAM%20with%20a%20training-free%20correspondence-to-Gaussian%20initialization.%20Instead%20of%20progressively%20adding%20Gaussians%20as%20residuals%20reveal%20missing%20geometry%2C%20RGS-SLAM%20performs%20a%20one-shot%20triangulation%20of%20dense%20multi-view%20correspondences%20derived%20from%20DINOv3%20descriptors%20refined%20through%20a%20confidence-aware%20inlier%20classifier%2C%20generating%20a%20well-distributed%20and%20structure-aware%20Gaussian%20seed%20prior%20to%20optimization.%20This%20initialization%20stabilizes%20early%20mapping%20and%20accelerates%20convergence%20by%20roughly%2020%5C%25%2C%20yielding%20higher%20rendering%20fidelity%20in%20texture-rich%20and%20cluttered%20scenes%20while%20remaining%20fully%20compatible%20with%20existing%20GS-SLAM%20pipelines.%20Evaluated%20on%20the%20TUM%20RGB-D%20and%20Replica%20datasets%2C%20RGS-SLAM%20achieves%20competitive%20or%20superior%20localization%20and%20reconstruction%20accuracy%20compared%20with%20state-of-the-art%20Gaussian%20and%20point-based%20SLAM%20systems%2C%20sustaining%20real-time%20mapping%20performance%20at%20up%20to%20925%20FPS.%20Project%20page%3Ahttps%3A//breeze1124.github.io/rgs-slam-project-page/%0ALink%3A%20http%3A//arxiv.org/abs/2601.00705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGS-SLAM%253A%2520Robust%2520Gaussian%2520Splatting%2520SLAM%2520with%2520One-Shot%2520Dense%2520Initialization%26entry.906535625%3DWei-Tse%2520Cheng%2520and%2520Yen-Jen%2520Chiou%2520and%2520Yuan-Fu%2520Yang%26entry.1292438233%3DWe%2520introduce%2520RGS-SLAM%252C%2520a%2520robust%2520Gaussian-splatting%2520SLAM%2520framework%2520that%2520replaces%2520the%2520residual-driven%2520densification%2520stage%2520of%2520GS-SLAM%2520with%2520a%2520training-free%2520correspondence-to-Gaussian%2520initialization.%2520Instead%2520of%2520progressively%2520adding%2520Gaussians%2520as%2520residuals%2520reveal%2520missing%2520geometry%252C%2520RGS-SLAM%2520performs%2520a%2520one-shot%2520triangulation%2520of%2520dense%2520multi-view%2520correspondences%2520derived%2520from%2520DINOv3%2520descriptors%2520refined%2520through%2520a%2520confidence-aware%2520inlier%2520classifier%252C%2520generating%2520a%2520well-distributed%2520and%2520structure-aware%2520Gaussian%2520seed%2520prior%2520to%2520optimization.%2520This%2520initialization%2520stabilizes%2520early%2520mapping%2520and%2520accelerates%2520convergence%2520by%2520roughly%252020%255C%2525%252C%2520yielding%2520higher%2520rendering%2520fidelity%2520in%2520texture-rich%2520and%2520cluttered%2520scenes%2520while%2520remaining%2520fully%2520compatible%2520with%2520existing%2520GS-SLAM%2520pipelines.%2520Evaluated%2520on%2520the%2520TUM%2520RGB-D%2520and%2520Replica%2520datasets%252C%2520RGS-SLAM%2520achieves%2520competitive%2520or%2520superior%2520localization%2520and%2520reconstruction%2520accuracy%2520compared%2520with%2520state-of-the-art%2520Gaussian%2520and%2520point-based%2520SLAM%2520systems%252C%2520sustaining%2520real-time%2520mapping%2520performance%2520at%2520up%2520to%2520925%2520FPS.%2520Project%2520page%253Ahttps%253A//breeze1124.github.io/rgs-slam-project-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization&entry.906535625=Wei-Tse%20Cheng%20and%20Yen-Jen%20Chiou%20and%20Yuan-Fu%20Yang&entry.1292438233=We%20introduce%20RGS-SLAM%2C%20a%20robust%20Gaussian-splatting%20SLAM%20framework%20that%20replaces%20the%20residual-driven%20densification%20stage%20of%20GS-SLAM%20with%20a%20training-free%20correspondence-to-Gaussian%20initialization.%20Instead%20of%20progressively%20adding%20Gaussians%20as%20residuals%20reveal%20missing%20geometry%2C%20RGS-SLAM%20performs%20a%20one-shot%20triangulation%20of%20dense%20multi-view%20correspondences%20derived%20from%20DINOv3%20descriptors%20refined%20through%20a%20confidence-aware%20inlier%20classifier%2C%20generating%20a%20well-distributed%20and%20structure-aware%20Gaussian%20seed%20prior%20to%20optimization.%20This%20initialization%20stabilizes%20early%20mapping%20and%20accelerates%20convergence%20by%20roughly%2020%5C%25%2C%20yielding%20higher%20rendering%20fidelity%20in%20texture-rich%20and%20cluttered%20scenes%20while%20remaining%20fully%20compatible%20with%20existing%20GS-SLAM%20pipelines.%20Evaluated%20on%20the%20TUM%20RGB-D%20and%20Replica%20datasets%2C%20RGS-SLAM%20achieves%20competitive%20or%20superior%20localization%20and%20reconstruction%20accuracy%20compared%20with%20state-of-the-art%20Gaussian%20and%20point-based%20SLAM%20systems%2C%20sustaining%20real-time%20mapping%20performance%20at%20up%20to%20925%20FPS.%20Project%20page%3Ahttps%3A//breeze1124.github.io/rgs-slam-project-page/&entry.1838667208=http%3A//arxiv.org/abs/2601.00705v2&entry.124074799=Read"},
{"title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting", "author": "Nan Wang and Yuantao Chen and Lixing Xiao and Weiqing Xiao and Bohan Li and Zhaoxi Chen and Chongjie Ye and Shaocong Xu and Saining Zhang and Ziyang Yan and Pierre Merriaux and Lei Lei and Tianfan Xue and Hao Zhao", "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.", "link": "http://arxiv.org/abs/2506.05280v4", "date": "2026-01-13", "relevancy": 3.2229, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6496}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Appearance%20Codes%20and%20Bilateral%20Grids%20for%20Driving%20Scene%20Gaussian%20Splatting&body=Title%3A%20Unifying%20Appearance%20Codes%20and%20Bilateral%20Grids%20for%20Driving%20Scene%20Gaussian%20Splatting%0AAuthor%3A%20Nan%20Wang%20and%20Yuantao%20Chen%20and%20Lixing%20Xiao%20and%20Weiqing%20Xiao%20and%20Bohan%20Li%20and%20Zhaoxi%20Chen%20and%20Chongjie%20Ye%20and%20Shaocong%20Xu%20and%20Saining%20Zhang%20and%20Ziyang%20Yan%20and%20Pierre%20Merriaux%20and%20Lei%20Lei%20and%20Tianfan%20Xue%20and%20Hao%20Zhao%0AAbstract%3A%20Neural%20rendering%20techniques%2C%20including%20NeRF%20and%20Gaussian%20Splatting%20%28GS%29%2C%20rely%20on%20photometric%20consistency%20to%20produce%20high-quality%20reconstructions.%20However%2C%20in%20real-world%20scenarios%2C%20it%20is%20challenging%20to%20guarantee%20perfect%20photometric%20consistency%20in%20acquired%20images.%20Appearance%20codes%20have%20been%20widely%20used%20to%20address%20this%20issue%2C%20but%20their%20modeling%20capability%20is%20limited%2C%20as%20a%20single%20code%20is%20applied%20to%20the%20entire%20image.%20Recently%2C%20the%20bilateral%20grid%20was%20introduced%20to%20perform%20pixel-wise%20color%20mapping%2C%20but%20it%20is%20difficult%20to%20optimize%20and%20constrain%20effectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20multi-scale%20bilateral%20grid%20that%20unifies%20appearance%20codes%20and%20bilateral%20grids.%20We%20demonstrate%20that%20this%20approach%20significantly%20improves%20geometric%20accuracy%20in%20dynamic%2C%20decoupled%20autonomous%20driving%20scene%20reconstruction%2C%20outperforming%20both%20appearance%20codes%20and%20bilateral%20grids.%20This%20is%20crucial%20for%20autonomous%20driving%2C%20where%20accurate%20geometry%20is%20important%20for%20obstacle%20avoidance%20and%20control.%20Our%20method%20shows%20strong%20results%20across%20four%20datasets%3A%20Waymo%2C%20NuScenes%2C%20Argoverse%2C%20and%20PandaSet.%20We%20further%20demonstrate%20that%20the%20improvement%20in%20geometry%20is%20driven%20by%20the%20multi-scale%20bilateral%20grid%2C%20which%20effectively%20reduces%20floaters%20caused%20by%20photometric%20inconsistency.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05280v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Appearance%2520Codes%2520and%2520Bilateral%2520Grids%2520for%2520Driving%2520Scene%2520Gaussian%2520Splatting%26entry.906535625%3DNan%2520Wang%2520and%2520Yuantao%2520Chen%2520and%2520Lixing%2520Xiao%2520and%2520Weiqing%2520Xiao%2520and%2520Bohan%2520Li%2520and%2520Zhaoxi%2520Chen%2520and%2520Chongjie%2520Ye%2520and%2520Shaocong%2520Xu%2520and%2520Saining%2520Zhang%2520and%2520Ziyang%2520Yan%2520and%2520Pierre%2520Merriaux%2520and%2520Lei%2520Lei%2520and%2520Tianfan%2520Xue%2520and%2520Hao%2520Zhao%26entry.1292438233%3DNeural%2520rendering%2520techniques%252C%2520including%2520NeRF%2520and%2520Gaussian%2520Splatting%2520%2528GS%2529%252C%2520rely%2520on%2520photometric%2520consistency%2520to%2520produce%2520high-quality%2520reconstructions.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520it%2520is%2520challenging%2520to%2520guarantee%2520perfect%2520photometric%2520consistency%2520in%2520acquired%2520images.%2520Appearance%2520codes%2520have%2520been%2520widely%2520used%2520to%2520address%2520this%2520issue%252C%2520but%2520their%2520modeling%2520capability%2520is%2520limited%252C%2520as%2520a%2520single%2520code%2520is%2520applied%2520to%2520the%2520entire%2520image.%2520Recently%252C%2520the%2520bilateral%2520grid%2520was%2520introduced%2520to%2520perform%2520pixel-wise%2520color%2520mapping%252C%2520but%2520it%2520is%2520difficult%2520to%2520optimize%2520and%2520constrain%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520multi-scale%2520bilateral%2520grid%2520that%2520unifies%2520appearance%2520codes%2520and%2520bilateral%2520grids.%2520We%2520demonstrate%2520that%2520this%2520approach%2520significantly%2520improves%2520geometric%2520accuracy%2520in%2520dynamic%252C%2520decoupled%2520autonomous%2520driving%2520scene%2520reconstruction%252C%2520outperforming%2520both%2520appearance%2520codes%2520and%2520bilateral%2520grids.%2520This%2520is%2520crucial%2520for%2520autonomous%2520driving%252C%2520where%2520accurate%2520geometry%2520is%2520important%2520for%2520obstacle%2520avoidance%2520and%2520control.%2520Our%2520method%2520shows%2520strong%2520results%2520across%2520four%2520datasets%253A%2520Waymo%252C%2520NuScenes%252C%2520Argoverse%252C%2520and%2520PandaSet.%2520We%2520further%2520demonstrate%2520that%2520the%2520improvement%2520in%2520geometry%2520is%2520driven%2520by%2520the%2520multi-scale%2520bilateral%2520grid%252C%2520which%2520effectively%2520reduces%2520floaters%2520caused%2520by%2520photometric%2520inconsistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05280v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Appearance%20Codes%20and%20Bilateral%20Grids%20for%20Driving%20Scene%20Gaussian%20Splatting&entry.906535625=Nan%20Wang%20and%20Yuantao%20Chen%20and%20Lixing%20Xiao%20and%20Weiqing%20Xiao%20and%20Bohan%20Li%20and%20Zhaoxi%20Chen%20and%20Chongjie%20Ye%20and%20Shaocong%20Xu%20and%20Saining%20Zhang%20and%20Ziyang%20Yan%20and%20Pierre%20Merriaux%20and%20Lei%20Lei%20and%20Tianfan%20Xue%20and%20Hao%20Zhao&entry.1292438233=Neural%20rendering%20techniques%2C%20including%20NeRF%20and%20Gaussian%20Splatting%20%28GS%29%2C%20rely%20on%20photometric%20consistency%20to%20produce%20high-quality%20reconstructions.%20However%2C%20in%20real-world%20scenarios%2C%20it%20is%20challenging%20to%20guarantee%20perfect%20photometric%20consistency%20in%20acquired%20images.%20Appearance%20codes%20have%20been%20widely%20used%20to%20address%20this%20issue%2C%20but%20their%20modeling%20capability%20is%20limited%2C%20as%20a%20single%20code%20is%20applied%20to%20the%20entire%20image.%20Recently%2C%20the%20bilateral%20grid%20was%20introduced%20to%20perform%20pixel-wise%20color%20mapping%2C%20but%20it%20is%20difficult%20to%20optimize%20and%20constrain%20effectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20multi-scale%20bilateral%20grid%20that%20unifies%20appearance%20codes%20and%20bilateral%20grids.%20We%20demonstrate%20that%20this%20approach%20significantly%20improves%20geometric%20accuracy%20in%20dynamic%2C%20decoupled%20autonomous%20driving%20scene%20reconstruction%2C%20outperforming%20both%20appearance%20codes%20and%20bilateral%20grids.%20This%20is%20crucial%20for%20autonomous%20driving%2C%20where%20accurate%20geometry%20is%20important%20for%20obstacle%20avoidance%20and%20control.%20Our%20method%20shows%20strong%20results%20across%20four%20datasets%3A%20Waymo%2C%20NuScenes%2C%20Argoverse%2C%20and%20PandaSet.%20We%20further%20demonstrate%20that%20the%20improvement%20in%20geometry%20is%20driven%20by%20the%20multi-scale%20bilateral%20grid%2C%20which%20effectively%20reduces%20floaters%20caused%20by%20photometric%20inconsistency.&entry.1838667208=http%3A//arxiv.org/abs/2506.05280v4&entry.124074799=Read"},
{"title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation", "author": "Changli Wu and Haodong Wang and Jiayi Ji and Yutian Yao and Chunsai Du and Jihua Kang and Yanwei Fu and Liujuan Cao", "abstract": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.", "link": "http://arxiv.org/abs/2601.06874v2", "date": "2026-01-13", "relevancy": 3.1015, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.64}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVGGT%3A%20Multimodal%20Visual%20Geometry%20Grounded%20Transformer%20for%20Multiview%203D%20Referring%20Expression%20Segmentation&body=Title%3A%20MVGGT%3A%20Multimodal%20Visual%20Geometry%20Grounded%20Transformer%20for%20Multiview%203D%20Referring%20Expression%20Segmentation%0AAuthor%3A%20Changli%20Wu%20and%20Haodong%20Wang%20and%20Jiayi%20Ji%20and%20Yutian%20Yao%20and%20Chunsai%20Du%20and%20Jihua%20Kang%20and%20Yanwei%20Fu%20and%20Liujuan%20Cao%0AAbstract%3A%20Most%20existing%203D%20referring%20expression%20segmentation%20%283DRES%29%20methods%20rely%20on%20dense%2C%20high-quality%20point%20clouds%2C%20while%20real-world%20agents%20such%20as%20robots%20and%20mobile%20phones%20operate%20with%20only%20a%20few%20sparse%20RGB%20views%20and%20strict%20latency%20constraints.%20We%20introduce%20Multi-view%203D%20Referring%20Expression%20Segmentation%20%28MV-3DRES%29%2C%20where%20the%20model%20must%20recover%20scene%20structure%20and%20segment%20the%20referred%20object%20directly%20from%20sparse%20multi-view%20images.%20Traditional%20two-stage%20pipelines%2C%20which%20first%20reconstruct%20a%20point%20cloud%20and%20then%20perform%20segmentation%2C%20often%20yield%20low-quality%20geometry%2C%20produce%20coarse%20or%20degraded%20target%20regions%2C%20and%20run%20slowly.%20We%20propose%20the%20Multimodal%20Visual%20Geometry%20Grounded%20Transformer%20%28MVGGT%29%2C%20an%20efficient%20end-to-end%20framework%20that%20integrates%20language%20information%20into%20sparse-view%20geometric%20reasoning%20through%20a%20dual-branch%20design.%20Training%20in%20this%20setting%20exposes%20a%20critical%20optimization%20barrier%2C%20termed%20Foreground%20Gradient%20Dilution%20%28FGD%29%2C%20where%20sparse%203D%20signals%20lead%20to%20weak%20supervision.%20To%20resolve%20this%2C%20we%20introduce%20Per-view%20No-target%20Suppression%20Optimization%20%28PVSO%29%2C%20which%20provides%20stronger%20and%20more%20balanced%20gradients%20across%20views%2C%20enabling%20stable%20and%20efficient%20learning.%20To%20support%20consistent%20evaluation%2C%20we%20build%20MVRefer%2C%20a%20benchmark%20that%20defines%20standardized%20settings%20and%20metrics%20for%20MV-3DRES.%20Experiments%20show%20that%20MVGGT%20establishes%20the%20first%20strong%20baseline%20and%20achieves%20both%20high%20accuracy%20and%20fast%20inference%2C%20outperforming%20existing%20alternatives.%20Code%20and%20models%20are%20publicly%20available%20at%20https%3A//mvggt.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVGGT%253A%2520Multimodal%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520for%2520Multiview%25203D%2520Referring%2520Expression%2520Segmentation%26entry.906535625%3DChangli%2520Wu%2520and%2520Haodong%2520Wang%2520and%2520Jiayi%2520Ji%2520and%2520Yutian%2520Yao%2520and%2520Chunsai%2520Du%2520and%2520Jihua%2520Kang%2520and%2520Yanwei%2520Fu%2520and%2520Liujuan%2520Cao%26entry.1292438233%3DMost%2520existing%25203D%2520referring%2520expression%2520segmentation%2520%25283DRES%2529%2520methods%2520rely%2520on%2520dense%252C%2520high-quality%2520point%2520clouds%252C%2520while%2520real-world%2520agents%2520such%2520as%2520robots%2520and%2520mobile%2520phones%2520operate%2520with%2520only%2520a%2520few%2520sparse%2520RGB%2520views%2520and%2520strict%2520latency%2520constraints.%2520We%2520introduce%2520Multi-view%25203D%2520Referring%2520Expression%2520Segmentation%2520%2528MV-3DRES%2529%252C%2520where%2520the%2520model%2520must%2520recover%2520scene%2520structure%2520and%2520segment%2520the%2520referred%2520object%2520directly%2520from%2520sparse%2520multi-view%2520images.%2520Traditional%2520two-stage%2520pipelines%252C%2520which%2520first%2520reconstruct%2520a%2520point%2520cloud%2520and%2520then%2520perform%2520segmentation%252C%2520often%2520yield%2520low-quality%2520geometry%252C%2520produce%2520coarse%2520or%2520degraded%2520target%2520regions%252C%2520and%2520run%2520slowly.%2520We%2520propose%2520the%2520Multimodal%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528MVGGT%2529%252C%2520an%2520efficient%2520end-to-end%2520framework%2520that%2520integrates%2520language%2520information%2520into%2520sparse-view%2520geometric%2520reasoning%2520through%2520a%2520dual-branch%2520design.%2520Training%2520in%2520this%2520setting%2520exposes%2520a%2520critical%2520optimization%2520barrier%252C%2520termed%2520Foreground%2520Gradient%2520Dilution%2520%2528FGD%2529%252C%2520where%2520sparse%25203D%2520signals%2520lead%2520to%2520weak%2520supervision.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520Per-view%2520No-target%2520Suppression%2520Optimization%2520%2528PVSO%2529%252C%2520which%2520provides%2520stronger%2520and%2520more%2520balanced%2520gradients%2520across%2520views%252C%2520enabling%2520stable%2520and%2520efficient%2520learning.%2520To%2520support%2520consistent%2520evaluation%252C%2520we%2520build%2520MVRefer%252C%2520a%2520benchmark%2520that%2520defines%2520standardized%2520settings%2520and%2520metrics%2520for%2520MV-3DRES.%2520Experiments%2520show%2520that%2520MVGGT%2520establishes%2520the%2520first%2520strong%2520baseline%2520and%2520achieves%2520both%2520high%2520accuracy%2520and%2520fast%2520inference%252C%2520outperforming%2520existing%2520alternatives.%2520Code%2520and%2520models%2520are%2520publicly%2520available%2520at%2520https%253A//mvggt.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVGGT%3A%20Multimodal%20Visual%20Geometry%20Grounded%20Transformer%20for%20Multiview%203D%20Referring%20Expression%20Segmentation&entry.906535625=Changli%20Wu%20and%20Haodong%20Wang%20and%20Jiayi%20Ji%20and%20Yutian%20Yao%20and%20Chunsai%20Du%20and%20Jihua%20Kang%20and%20Yanwei%20Fu%20and%20Liujuan%20Cao&entry.1292438233=Most%20existing%203D%20referring%20expression%20segmentation%20%283DRES%29%20methods%20rely%20on%20dense%2C%20high-quality%20point%20clouds%2C%20while%20real-world%20agents%20such%20as%20robots%20and%20mobile%20phones%20operate%20with%20only%20a%20few%20sparse%20RGB%20views%20and%20strict%20latency%20constraints.%20We%20introduce%20Multi-view%203D%20Referring%20Expression%20Segmentation%20%28MV-3DRES%29%2C%20where%20the%20model%20must%20recover%20scene%20structure%20and%20segment%20the%20referred%20object%20directly%20from%20sparse%20multi-view%20images.%20Traditional%20two-stage%20pipelines%2C%20which%20first%20reconstruct%20a%20point%20cloud%20and%20then%20perform%20segmentation%2C%20often%20yield%20low-quality%20geometry%2C%20produce%20coarse%20or%20degraded%20target%20regions%2C%20and%20run%20slowly.%20We%20propose%20the%20Multimodal%20Visual%20Geometry%20Grounded%20Transformer%20%28MVGGT%29%2C%20an%20efficient%20end-to-end%20framework%20that%20integrates%20language%20information%20into%20sparse-view%20geometric%20reasoning%20through%20a%20dual-branch%20design.%20Training%20in%20this%20setting%20exposes%20a%20critical%20optimization%20barrier%2C%20termed%20Foreground%20Gradient%20Dilution%20%28FGD%29%2C%20where%20sparse%203D%20signals%20lead%20to%20weak%20supervision.%20To%20resolve%20this%2C%20we%20introduce%20Per-view%20No-target%20Suppression%20Optimization%20%28PVSO%29%2C%20which%20provides%20stronger%20and%20more%20balanced%20gradients%20across%20views%2C%20enabling%20stable%20and%20efficient%20learning.%20To%20support%20consistent%20evaluation%2C%20we%20build%20MVRefer%2C%20a%20benchmark%20that%20defines%20standardized%20settings%20and%20metrics%20for%20MV-3DRES.%20Experiments%20show%20that%20MVGGT%20establishes%20the%20first%20strong%20baseline%20and%20achieves%20both%20high%20accuracy%20and%20fast%20inference%2C%20outperforming%20existing%20alternatives.%20Code%20and%20models%20are%20publicly%20available%20at%20https%3A//mvggt.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2601.06874v2&entry.124074799=Read"},
{"title": "End-to-End Video Character Replacement without Structural Guidance", "author": "Zhengbo Xu and Jie Ma and Ziheng Wang and Zhan Peng and Jun Liang and Jing Li", "abstract": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha", "link": "http://arxiv.org/abs/2601.08587v1", "date": "2026-01-13", "relevancy": 3.096, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.626}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Video%20Character%20Replacement%20without%20Structural%20Guidance&body=Title%3A%20End-to-End%20Video%20Character%20Replacement%20without%20Structural%20Guidance%0AAuthor%3A%20Zhengbo%20Xu%20and%20Jie%20Ma%20and%20Ziheng%20Wang%20and%20Zhan%20Peng%20and%20Jun%20Liang%20and%20Jing%20Li%0AAbstract%3A%20Controllable%20video%20character%20replacement%20with%20a%20user-provided%20identity%20remains%20a%20challenging%20problem%20due%20to%20the%20lack%20of%20paired%20video%20data.%20Prior%20works%20have%20predominantly%20relied%20on%20a%20reconstruction-based%20paradigm%20that%20requires%20per-frame%20segmentation%20masks%20and%20explicit%20structural%20guidance%20%28e.g.%2C%20skeleton%2C%20depth%29.%20This%20reliance%2C%20however%2C%20severely%20limits%20their%20generalizability%20in%20complex%20scenarios%20involving%20occlusions%2C%20character-object%20interactions%2C%20unusual%20poses%2C%20or%20challenging%20illumination%2C%20often%20leading%20to%20visual%20artifacts%20and%20temporal%20inconsistencies.%20In%20this%20paper%2C%20we%20propose%20MoCha%2C%20a%20pioneering%20framework%20that%20bypasses%20these%20limitations%20by%20requiring%20only%20a%20single%20arbitrary%20frame%20mask.%20To%20effectively%20adapt%20the%20multi-modal%20input%20condition%20and%20enhance%20facial%20identity%2C%20we%20introduce%20a%20condition-aware%20RoPE%20and%20employ%20an%20RL-based%20post-training%20stage.%20Furthermore%2C%20to%20overcome%20the%20scarcity%20of%20qualified%20paired-training%20data%2C%20we%20propose%20a%20comprehensive%20data%20construction%20pipeline.%20Specifically%2C%20we%20design%20three%20specialized%20datasets%3A%20a%20high-fidelity%20rendered%20dataset%20built%20with%20Unreal%20Engine%205%20%28UE5%29%2C%20an%20expression-driven%20dataset%20synthesized%20by%20current%20portrait%20animation%20techniques%2C%20and%20an%20augmented%20dataset%20derived%20from%20existing%20video-mask%20pairs.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20substantially%20outperforms%20existing%20state-of-the-art%20approaches.%20We%20will%20release%20the%20code%20to%20facilitate%20further%20research.%20Please%20refer%20to%20our%20project%20page%20for%20more%20details%3A%20orange-3dv-team.github.io/MoCha%0ALink%3A%20http%3A//arxiv.org/abs/2601.08587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Video%2520Character%2520Replacement%2520without%2520Structural%2520Guidance%26entry.906535625%3DZhengbo%2520Xu%2520and%2520Jie%2520Ma%2520and%2520Ziheng%2520Wang%2520and%2520Zhan%2520Peng%2520and%2520Jun%2520Liang%2520and%2520Jing%2520Li%26entry.1292438233%3DControllable%2520video%2520character%2520replacement%2520with%2520a%2520user-provided%2520identity%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520lack%2520of%2520paired%2520video%2520data.%2520Prior%2520works%2520have%2520predominantly%2520relied%2520on%2520a%2520reconstruction-based%2520paradigm%2520that%2520requires%2520per-frame%2520segmentation%2520masks%2520and%2520explicit%2520structural%2520guidance%2520%2528e.g.%252C%2520skeleton%252C%2520depth%2529.%2520This%2520reliance%252C%2520however%252C%2520severely%2520limits%2520their%2520generalizability%2520in%2520complex%2520scenarios%2520involving%2520occlusions%252C%2520character-object%2520interactions%252C%2520unusual%2520poses%252C%2520or%2520challenging%2520illumination%252C%2520often%2520leading%2520to%2520visual%2520artifacts%2520and%2520temporal%2520inconsistencies.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MoCha%252C%2520a%2520pioneering%2520framework%2520that%2520bypasses%2520these%2520limitations%2520by%2520requiring%2520only%2520a%2520single%2520arbitrary%2520frame%2520mask.%2520To%2520effectively%2520adapt%2520the%2520multi-modal%2520input%2520condition%2520and%2520enhance%2520facial%2520identity%252C%2520we%2520introduce%2520a%2520condition-aware%2520RoPE%2520and%2520employ%2520an%2520RL-based%2520post-training%2520stage.%2520Furthermore%252C%2520to%2520overcome%2520the%2520scarcity%2520of%2520qualified%2520paired-training%2520data%252C%2520we%2520propose%2520a%2520comprehensive%2520data%2520construction%2520pipeline.%2520Specifically%252C%2520we%2520design%2520three%2520specialized%2520datasets%253A%2520a%2520high-fidelity%2520rendered%2520dataset%2520built%2520with%2520Unreal%2520Engine%25205%2520%2528UE5%2529%252C%2520an%2520expression-driven%2520dataset%2520synthesized%2520by%2520current%2520portrait%2520animation%2520techniques%252C%2520and%2520an%2520augmented%2520dataset%2520derived%2520from%2520existing%2520video-mask%2520pairs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520substantially%2520outperforms%2520existing%2520state-of-the-art%2520approaches.%2520We%2520will%2520release%2520the%2520code%2520to%2520facilitate%2520further%2520research.%2520Please%2520refer%2520to%2520our%2520project%2520page%2520for%2520more%2520details%253A%2520orange-3dv-team.github.io/MoCha%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Video%20Character%20Replacement%20without%20Structural%20Guidance&entry.906535625=Zhengbo%20Xu%20and%20Jie%20Ma%20and%20Ziheng%20Wang%20and%20Zhan%20Peng%20and%20Jun%20Liang%20and%20Jing%20Li&entry.1292438233=Controllable%20video%20character%20replacement%20with%20a%20user-provided%20identity%20remains%20a%20challenging%20problem%20due%20to%20the%20lack%20of%20paired%20video%20data.%20Prior%20works%20have%20predominantly%20relied%20on%20a%20reconstruction-based%20paradigm%20that%20requires%20per-frame%20segmentation%20masks%20and%20explicit%20structural%20guidance%20%28e.g.%2C%20skeleton%2C%20depth%29.%20This%20reliance%2C%20however%2C%20severely%20limits%20their%20generalizability%20in%20complex%20scenarios%20involving%20occlusions%2C%20character-object%20interactions%2C%20unusual%20poses%2C%20or%20challenging%20illumination%2C%20often%20leading%20to%20visual%20artifacts%20and%20temporal%20inconsistencies.%20In%20this%20paper%2C%20we%20propose%20MoCha%2C%20a%20pioneering%20framework%20that%20bypasses%20these%20limitations%20by%20requiring%20only%20a%20single%20arbitrary%20frame%20mask.%20To%20effectively%20adapt%20the%20multi-modal%20input%20condition%20and%20enhance%20facial%20identity%2C%20we%20introduce%20a%20condition-aware%20RoPE%20and%20employ%20an%20RL-based%20post-training%20stage.%20Furthermore%2C%20to%20overcome%20the%20scarcity%20of%20qualified%20paired-training%20data%2C%20we%20propose%20a%20comprehensive%20data%20construction%20pipeline.%20Specifically%2C%20we%20design%20three%20specialized%20datasets%3A%20a%20high-fidelity%20rendered%20dataset%20built%20with%20Unreal%20Engine%205%20%28UE5%29%2C%20an%20expression-driven%20dataset%20synthesized%20by%20current%20portrait%20animation%20techniques%2C%20and%20an%20augmented%20dataset%20derived%20from%20existing%20video-mask%20pairs.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20substantially%20outperforms%20existing%20state-of-the-art%20approaches.%20We%20will%20release%20the%20code%20to%20facilitate%20further%20research.%20Please%20refer%20to%20our%20project%20page%20for%20more%20details%3A%20orange-3dv-team.github.io/MoCha&entry.1838667208=http%3A//arxiv.org/abs/2601.08587v1&entry.124074799=Read"},
{"title": "3AM: Segment Anything with Geometric Consistency in Videos", "author": "Yang-Che Sun and Cheng Sun and Chin-Yang Lin and Fu-En Yang and Min-Hung Chen and Yen-Yu Lin and Yu-Lun Liu", "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/", "link": "http://arxiv.org/abs/2601.08831v1", "date": "2026-01-13", "relevancy": 3.0756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6096}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203AM%3A%20Segment%20Anything%20with%20Geometric%20Consistency%20in%20Videos&body=Title%3A%203AM%3A%20Segment%20Anything%20with%20Geometric%20Consistency%20in%20Videos%0AAuthor%3A%20Yang-Che%20Sun%20and%20Cheng%20Sun%20and%20Chin-Yang%20Lin%20and%20Fu-En%20Yang%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Video%20object%20segmentation%20methods%20like%20SAM2%20achieve%20strong%20performance%20through%20memory-based%20architectures%20but%20struggle%20under%20large%20viewpoint%20changes%20due%20to%20reliance%20on%20appearance%20features.%20Traditional%203D%20instance%20segmentation%20methods%20address%20viewpoint%20consistency%20but%20require%20camera%20poses%2C%20depth%20maps%2C%20and%20expensive%20preprocessing.%20We%20introduce%203AM%2C%20a%20training-time%20enhancement%20that%20integrates%203D-aware%20features%20from%20MUSt3R%20into%20SAM2.%20Our%20lightweight%20Feature%20Merger%20fuses%20multi-level%20MUSt3R%20features%20that%20encode%20implicit%20geometric%20correspondence.%20Combined%20with%20SAM2%27s%20appearance%20features%2C%20the%20model%20achieves%20geometry-consistent%20recognition%20grounded%20in%20both%20spatial%20position%20and%20visual%20similarity.%20We%20propose%20a%20field-of-view%20aware%20sampling%20strategy%20ensuring%20frames%20observe%20spatially%20consistent%20object%20regions%20for%20reliable%203D%20correspondence%20learning.%20Critically%2C%20our%20method%20requires%20only%20RGB%20input%20at%20inference%2C%20with%20no%20camera%20poses%20or%20preprocessing.%20On%20challenging%20datasets%20with%20wide-baseline%20motion%20%28ScanNet%2B%2B%2C%20Replica%29%2C%203AM%20substantially%20outperforms%20SAM2%20and%20extensions%2C%20achieving%2090.6%25%20IoU%20and%2071.7%25%20Positive%20IoU%20on%20ScanNet%2B%2B%27s%20Selected%20Subset%2C%20improving%20over%20state-of-the-art%20VOS%20methods%20by%20%2B15.9%20and%20%2B30.4%20points.%20Project%20page%3A%20https%3A//jayisaking.github.io/3AM-Page/%0ALink%3A%20http%3A//arxiv.org/abs/2601.08831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3AM%253A%2520Segment%2520Anything%2520with%2520Geometric%2520Consistency%2520in%2520Videos%26entry.906535625%3DYang-Che%2520Sun%2520and%2520Cheng%2520Sun%2520and%2520Chin-Yang%2520Lin%2520and%2520Fu-En%2520Yang%2520and%2520Min-Hung%2520Chen%2520and%2520Yen-Yu%2520Lin%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DVideo%2520object%2520segmentation%2520methods%2520like%2520SAM2%2520achieve%2520strong%2520performance%2520through%2520memory-based%2520architectures%2520but%2520struggle%2520under%2520large%2520viewpoint%2520changes%2520due%2520to%2520reliance%2520on%2520appearance%2520features.%2520Traditional%25203D%2520instance%2520segmentation%2520methods%2520address%2520viewpoint%2520consistency%2520but%2520require%2520camera%2520poses%252C%2520depth%2520maps%252C%2520and%2520expensive%2520preprocessing.%2520We%2520introduce%25203AM%252C%2520a%2520training-time%2520enhancement%2520that%2520integrates%25203D-aware%2520features%2520from%2520MUSt3R%2520into%2520SAM2.%2520Our%2520lightweight%2520Feature%2520Merger%2520fuses%2520multi-level%2520MUSt3R%2520features%2520that%2520encode%2520implicit%2520geometric%2520correspondence.%2520Combined%2520with%2520SAM2%2527s%2520appearance%2520features%252C%2520the%2520model%2520achieves%2520geometry-consistent%2520recognition%2520grounded%2520in%2520both%2520spatial%2520position%2520and%2520visual%2520similarity.%2520We%2520propose%2520a%2520field-of-view%2520aware%2520sampling%2520strategy%2520ensuring%2520frames%2520observe%2520spatially%2520consistent%2520object%2520regions%2520for%2520reliable%25203D%2520correspondence%2520learning.%2520Critically%252C%2520our%2520method%2520requires%2520only%2520RGB%2520input%2520at%2520inference%252C%2520with%2520no%2520camera%2520poses%2520or%2520preprocessing.%2520On%2520challenging%2520datasets%2520with%2520wide-baseline%2520motion%2520%2528ScanNet%252B%252B%252C%2520Replica%2529%252C%25203AM%2520substantially%2520outperforms%2520SAM2%2520and%2520extensions%252C%2520achieving%252090.6%2525%2520IoU%2520and%252071.7%2525%2520Positive%2520IoU%2520on%2520ScanNet%252B%252B%2527s%2520Selected%2520Subset%252C%2520improving%2520over%2520state-of-the-art%2520VOS%2520methods%2520by%2520%252B15.9%2520and%2520%252B30.4%2520points.%2520Project%2520page%253A%2520https%253A//jayisaking.github.io/3AM-Page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3AM%3A%20Segment%20Anything%20with%20Geometric%20Consistency%20in%20Videos&entry.906535625=Yang-Che%20Sun%20and%20Cheng%20Sun%20and%20Chin-Yang%20Lin%20and%20Fu-En%20Yang%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu&entry.1292438233=Video%20object%20segmentation%20methods%20like%20SAM2%20achieve%20strong%20performance%20through%20memory-based%20architectures%20but%20struggle%20under%20large%20viewpoint%20changes%20due%20to%20reliance%20on%20appearance%20features.%20Traditional%203D%20instance%20segmentation%20methods%20address%20viewpoint%20consistency%20but%20require%20camera%20poses%2C%20depth%20maps%2C%20and%20expensive%20preprocessing.%20We%20introduce%203AM%2C%20a%20training-time%20enhancement%20that%20integrates%203D-aware%20features%20from%20MUSt3R%20into%20SAM2.%20Our%20lightweight%20Feature%20Merger%20fuses%20multi-level%20MUSt3R%20features%20that%20encode%20implicit%20geometric%20correspondence.%20Combined%20with%20SAM2%27s%20appearance%20features%2C%20the%20model%20achieves%20geometry-consistent%20recognition%20grounded%20in%20both%20spatial%20position%20and%20visual%20similarity.%20We%20propose%20a%20field-of-view%20aware%20sampling%20strategy%20ensuring%20frames%20observe%20spatially%20consistent%20object%20regions%20for%20reliable%203D%20correspondence%20learning.%20Critically%2C%20our%20method%20requires%20only%20RGB%20input%20at%20inference%2C%20with%20no%20camera%20poses%20or%20preprocessing.%20On%20challenging%20datasets%20with%20wide-baseline%20motion%20%28ScanNet%2B%2B%2C%20Replica%29%2C%203AM%20substantially%20outperforms%20SAM2%20and%20extensions%2C%20achieving%2090.6%25%20IoU%20and%2071.7%25%20Positive%20IoU%20on%20ScanNet%2B%2B%27s%20Selected%20Subset%2C%20improving%20over%20state-of-the-art%20VOS%20methods%20by%20%2B15.9%20and%20%2B30.4%20points.%20Project%20page%3A%20https%3A//jayisaking.github.io/3AM-Page/&entry.1838667208=http%3A//arxiv.org/abs/2601.08831v1&entry.124074799=Read"},
{"title": "SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration", "author": "Chentian Sun", "abstract": "Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.", "link": "http://arxiv.org/abs/2601.08414v1", "date": "2026-01-13", "relevancy": 2.9811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6361}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARK%3A%20Scalable%20Real-Time%20Point%20Cloud%20Aggregation%20with%20Multi-View%20Self-Calibration&body=Title%3A%20SPARK%3A%20Scalable%20Real-Time%20Point%20Cloud%20Aggregation%20with%20Multi-View%20Self-Calibration%0AAuthor%3A%20Chentian%20Sun%0AAbstract%3A%20Real-time%20multi-camera%203D%20reconstruction%20is%20crucial%20for%203D%20perception%2C%20immersive%20interaction%2C%20and%20robotics.%20Existing%20methods%20struggle%20with%20multi-view%20fusion%2C%20camera%20extrinsic%20uncertainty%2C%20and%20scalability%20for%20large%20camera%20setups.%20We%20propose%20SPARK%2C%20a%20self-calibrating%20real-time%20multi-camera%20point%20cloud%20reconstruction%20framework%20that%20jointly%20handles%20point%20cloud%20fusion%20and%20extrinsic%20uncertainty.%20SPARK%20consists%20of%3A%20%281%29%20a%20geometry-aware%20online%20extrinsic%20estimation%20module%20leveraging%20multi-view%20priors%20and%20enforcing%20cross-view%20and%20temporal%20consistency%20for%20stable%20self-calibration%2C%20and%20%282%29%20a%20confidence-driven%20point%20cloud%20fusion%20strategy%20modeling%20depth%20reliability%20and%20visibility%20at%20pixel%20and%20point%20levels%20to%20suppress%20noise%20and%20view-dependent%20inconsistencies.%20By%20performing%20frame-wise%20fusion%20without%20accumulation%2C%20SPARK%20produces%20stable%20point%20clouds%20in%20dynamic%20scenes%20while%20scaling%20linearly%20with%20the%20number%20of%20cameras.%20Extensive%20experiments%20on%20real-world%20multi-camera%20systems%20show%20that%20SPARK%20outperforms%20existing%20approaches%20in%20extrinsic%20accuracy%2C%20geometric%20consistency%2C%20temporal%20stability%2C%20and%20real-time%20performance%2C%20demonstrating%20its%20effectiveness%20and%20scalability%20for%20large-scale%20multi-camera%203D%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARK%253A%2520Scalable%2520Real-Time%2520Point%2520Cloud%2520Aggregation%2520with%2520Multi-View%2520Self-Calibration%26entry.906535625%3DChentian%2520Sun%26entry.1292438233%3DReal-time%2520multi-camera%25203D%2520reconstruction%2520is%2520crucial%2520for%25203D%2520perception%252C%2520immersive%2520interaction%252C%2520and%2520robotics.%2520Existing%2520methods%2520struggle%2520with%2520multi-view%2520fusion%252C%2520camera%2520extrinsic%2520uncertainty%252C%2520and%2520scalability%2520for%2520large%2520camera%2520setups.%2520We%2520propose%2520SPARK%252C%2520a%2520self-calibrating%2520real-time%2520multi-camera%2520point%2520cloud%2520reconstruction%2520framework%2520that%2520jointly%2520handles%2520point%2520cloud%2520fusion%2520and%2520extrinsic%2520uncertainty.%2520SPARK%2520consists%2520of%253A%2520%25281%2529%2520a%2520geometry-aware%2520online%2520extrinsic%2520estimation%2520module%2520leveraging%2520multi-view%2520priors%2520and%2520enforcing%2520cross-view%2520and%2520temporal%2520consistency%2520for%2520stable%2520self-calibration%252C%2520and%2520%25282%2529%2520a%2520confidence-driven%2520point%2520cloud%2520fusion%2520strategy%2520modeling%2520depth%2520reliability%2520and%2520visibility%2520at%2520pixel%2520and%2520point%2520levels%2520to%2520suppress%2520noise%2520and%2520view-dependent%2520inconsistencies.%2520By%2520performing%2520frame-wise%2520fusion%2520without%2520accumulation%252C%2520SPARK%2520produces%2520stable%2520point%2520clouds%2520in%2520dynamic%2520scenes%2520while%2520scaling%2520linearly%2520with%2520the%2520number%2520of%2520cameras.%2520Extensive%2520experiments%2520on%2520real-world%2520multi-camera%2520systems%2520show%2520that%2520SPARK%2520outperforms%2520existing%2520approaches%2520in%2520extrinsic%2520accuracy%252C%2520geometric%2520consistency%252C%2520temporal%2520stability%252C%2520and%2520real-time%2520performance%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520scalability%2520for%2520large-scale%2520multi-camera%25203D%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARK%3A%20Scalable%20Real-Time%20Point%20Cloud%20Aggregation%20with%20Multi-View%20Self-Calibration&entry.906535625=Chentian%20Sun&entry.1292438233=Real-time%20multi-camera%203D%20reconstruction%20is%20crucial%20for%203D%20perception%2C%20immersive%20interaction%2C%20and%20robotics.%20Existing%20methods%20struggle%20with%20multi-view%20fusion%2C%20camera%20extrinsic%20uncertainty%2C%20and%20scalability%20for%20large%20camera%20setups.%20We%20propose%20SPARK%2C%20a%20self-calibrating%20real-time%20multi-camera%20point%20cloud%20reconstruction%20framework%20that%20jointly%20handles%20point%20cloud%20fusion%20and%20extrinsic%20uncertainty.%20SPARK%20consists%20of%3A%20%281%29%20a%20geometry-aware%20online%20extrinsic%20estimation%20module%20leveraging%20multi-view%20priors%20and%20enforcing%20cross-view%20and%20temporal%20consistency%20for%20stable%20self-calibration%2C%20and%20%282%29%20a%20confidence-driven%20point%20cloud%20fusion%20strategy%20modeling%20depth%20reliability%20and%20visibility%20at%20pixel%20and%20point%20levels%20to%20suppress%20noise%20and%20view-dependent%20inconsistencies.%20By%20performing%20frame-wise%20fusion%20without%20accumulation%2C%20SPARK%20produces%20stable%20point%20clouds%20in%20dynamic%20scenes%20while%20scaling%20linearly%20with%20the%20number%20of%20cameras.%20Extensive%20experiments%20on%20real-world%20multi-camera%20systems%20show%20that%20SPARK%20outperforms%20existing%20approaches%20in%20extrinsic%20accuracy%2C%20geometric%20consistency%2C%20temporal%20stability%2C%20and%20real-time%20performance%2C%20demonstrating%20its%20effectiveness%20and%20scalability%20for%20large-scale%20multi-camera%203D%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2601.08414v1&entry.124074799=Read"},
{"title": "Learning-based Multi-View Stereo: A Survey", "author": "Fangjinhua Wang and Qingtian Zhu and Di Chang and Quankai Gao and Junlin Han and Tong Zhang and Richard Hartley and Marc Pollefeys", "abstract": "3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented/Virtual Reality (AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area.", "link": "http://arxiv.org/abs/2408.15235v3", "date": "2026-01-13", "relevancy": 2.9042, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5809}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Multi-View%20Stereo%3A%20A%20Survey&body=Title%3A%20Learning-based%20Multi-View%20Stereo%3A%20A%20Survey%0AAuthor%3A%20Fangjinhua%20Wang%20and%20Qingtian%20Zhu%20and%20Di%20Chang%20and%20Quankai%20Gao%20and%20Junlin%20Han%20and%20Tong%20Zhang%20and%20Richard%20Hartley%20and%20Marc%20Pollefeys%0AAbstract%3A%203D%20reconstruction%20aims%20to%20recover%20the%20dense%203D%20structure%20of%20a%20scene.%20It%20plays%20an%20essential%20role%20in%20various%20applications%20such%20as%20Augmented/Virtual%20Reality%20%28AR/VR%29%2C%20autonomous%20driving%20and%20robotics.%20Leveraging%20multiple%20views%20of%20a%20scene%20captured%20from%20different%20viewpoints%2C%20Multi-View%20Stereo%20%28MVS%29%20algorithms%20synthesize%20a%20comprehensive%203D%20representation%2C%20enabling%20precise%20reconstruction%20in%20complex%20environments.%20Due%20to%20its%20efficiency%20and%20effectiveness%2C%20MVS%20has%20become%20a%20pivotal%20method%20for%20image-based%203D%20reconstruction.%20Recently%2C%20with%20the%20success%20of%20deep%20learning%2C%20many%20learning-based%20MVS%20methods%20have%20been%20proposed%2C%20achieving%20impressive%20performance%20against%20traditional%20methods.%20We%20categorize%20these%20learning-based%20methods%20as%3A%20depth%20map-based%2C%20voxel-based%2C%20NeRF-based%2C%203D%20Gaussian%20Splatting-based%2C%20and%20large%20feed-forward%20methods.%20Among%20these%2C%20we%20focus%20significantly%20on%20depth%20map-based%20methods%2C%20which%20are%20the%20main%20family%20of%20MVS%20due%20to%20their%20conciseness%2C%20flexibility%20and%20scalability.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20the%20literature%20at%20the%20time%20of%20this%20writing.%20We%20investigate%20these%20learning-based%20methods%2C%20summarize%20their%20performances%20on%20popular%20benchmarks%2C%20and%20discuss%20promising%20future%20research%20directions%20in%20this%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2408.15235v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520Multi-View%2520Stereo%253A%2520A%2520Survey%26entry.906535625%3DFangjinhua%2520Wang%2520and%2520Qingtian%2520Zhu%2520and%2520Di%2520Chang%2520and%2520Quankai%2520Gao%2520and%2520Junlin%2520Han%2520and%2520Tong%2520Zhang%2520and%2520Richard%2520Hartley%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D3D%2520reconstruction%2520aims%2520to%2520recover%2520the%2520dense%25203D%2520structure%2520of%2520a%2520scene.%2520It%2520plays%2520an%2520essential%2520role%2520in%2520various%2520applications%2520such%2520as%2520Augmented/Virtual%2520Reality%2520%2528AR/VR%2529%252C%2520autonomous%2520driving%2520and%2520robotics.%2520Leveraging%2520multiple%2520views%2520of%2520a%2520scene%2520captured%2520from%2520different%2520viewpoints%252C%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520algorithms%2520synthesize%2520a%2520comprehensive%25203D%2520representation%252C%2520enabling%2520precise%2520reconstruction%2520in%2520complex%2520environments.%2520Due%2520to%2520its%2520efficiency%2520and%2520effectiveness%252C%2520MVS%2520has%2520become%2520a%2520pivotal%2520method%2520for%2520image-based%25203D%2520reconstruction.%2520Recently%252C%2520with%2520the%2520success%2520of%2520deep%2520learning%252C%2520many%2520learning-based%2520MVS%2520methods%2520have%2520been%2520proposed%252C%2520achieving%2520impressive%2520performance%2520against%2520traditional%2520methods.%2520We%2520categorize%2520these%2520learning-based%2520methods%2520as%253A%2520depth%2520map-based%252C%2520voxel-based%252C%2520NeRF-based%252C%25203D%2520Gaussian%2520Splatting-based%252C%2520and%2520large%2520feed-forward%2520methods.%2520Among%2520these%252C%2520we%2520focus%2520significantly%2520on%2520depth%2520map-based%2520methods%252C%2520which%2520are%2520the%2520main%2520family%2520of%2520MVS%2520due%2520to%2520their%2520conciseness%252C%2520flexibility%2520and%2520scalability.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%2520the%2520literature%2520at%2520the%2520time%2520of%2520this%2520writing.%2520We%2520investigate%2520these%2520learning-based%2520methods%252C%2520summarize%2520their%2520performances%2520on%2520popular%2520benchmarks%252C%2520and%2520discuss%2520promising%2520future%2520research%2520directions%2520in%2520this%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15235v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Multi-View%20Stereo%3A%20A%20Survey&entry.906535625=Fangjinhua%20Wang%20and%20Qingtian%20Zhu%20and%20Di%20Chang%20and%20Quankai%20Gao%20and%20Junlin%20Han%20and%20Tong%20Zhang%20and%20Richard%20Hartley%20and%20Marc%20Pollefeys&entry.1292438233=3D%20reconstruction%20aims%20to%20recover%20the%20dense%203D%20structure%20of%20a%20scene.%20It%20plays%20an%20essential%20role%20in%20various%20applications%20such%20as%20Augmented/Virtual%20Reality%20%28AR/VR%29%2C%20autonomous%20driving%20and%20robotics.%20Leveraging%20multiple%20views%20of%20a%20scene%20captured%20from%20different%20viewpoints%2C%20Multi-View%20Stereo%20%28MVS%29%20algorithms%20synthesize%20a%20comprehensive%203D%20representation%2C%20enabling%20precise%20reconstruction%20in%20complex%20environments.%20Due%20to%20its%20efficiency%20and%20effectiveness%2C%20MVS%20has%20become%20a%20pivotal%20method%20for%20image-based%203D%20reconstruction.%20Recently%2C%20with%20the%20success%20of%20deep%20learning%2C%20many%20learning-based%20MVS%20methods%20have%20been%20proposed%2C%20achieving%20impressive%20performance%20against%20traditional%20methods.%20We%20categorize%20these%20learning-based%20methods%20as%3A%20depth%20map-based%2C%20voxel-based%2C%20NeRF-based%2C%203D%20Gaussian%20Splatting-based%2C%20and%20large%20feed-forward%20methods.%20Among%20these%2C%20we%20focus%20significantly%20on%20depth%20map-based%20methods%2C%20which%20are%20the%20main%20family%20of%20MVS%20due%20to%20their%20conciseness%2C%20flexibility%20and%20scalability.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20the%20literature%20at%20the%20time%20of%20this%20writing.%20We%20investigate%20these%20learning-based%20methods%2C%20summarize%20their%20performances%20on%20popular%20benchmarks%2C%20and%20discuss%20promising%20future%20research%20directions%20in%20this%20area.&entry.1838667208=http%3A//arxiv.org/abs/2408.15235v3&entry.124074799=Read"},
{"title": "Deep Learning Based Facial Retargeting Using Local Patches", "author": "Yeonsoo Choi and Inyup Lee and Sihun Cha and Seonghyeon Kim and Sunjin Jung and Junyong Noh", "abstract": "In the era of digital animation, the quest to produce lifelike facial animations for virtual characters has led to the development of various retargeting methods. While the retargeting facial motion between models of similar shapes has been very successful, challenges arise when the retargeting is performed on stylized or exaggerated 3D characters that deviate significantly from human facial structures. In this scenario, it is important to consider the target character's facial structure and possible range of motion to preserve the semantics assumed by the original facial motions after the retargeting. To achieve this, we propose a local patch-based retargeting method that transfers facial animations captured in a source performance video to a target stylized 3D character. Our method consists of three modules. The Automatic Patch Extraction Module extracts local patches from the source video frame. These patches are processed through the Reenactment Module to generate correspondingly re-enacted target local patches. The Weight Estimation Module calculates the animation parameters for the target character at every frame for the creation of a complete facial animation sequence. Extensive experiments demonstrate that our method can successfully transfer the semantic meaning of source facial expressions to stylized characters with considerable variations in facial feature proportion.", "link": "http://arxiv.org/abs/2601.08429v1", "date": "2026-01-13", "relevancy": 2.8906, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.614}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.561}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Based%20Facial%20Retargeting%20Using%20Local%20Patches&body=Title%3A%20Deep%20Learning%20Based%20Facial%20Retargeting%20Using%20Local%20Patches%0AAuthor%3A%20Yeonsoo%20Choi%20and%20Inyup%20Lee%20and%20Sihun%20Cha%20and%20Seonghyeon%20Kim%20and%20Sunjin%20Jung%20and%20Junyong%20Noh%0AAbstract%3A%20In%20the%20era%20of%20digital%20animation%2C%20the%20quest%20to%20produce%20lifelike%20facial%20animations%20for%20virtual%20characters%20has%20led%20to%20the%20development%20of%20various%20retargeting%20methods.%20While%20the%20retargeting%20facial%20motion%20between%20models%20of%20similar%20shapes%20has%20been%20very%20successful%2C%20challenges%20arise%20when%20the%20retargeting%20is%20performed%20on%20stylized%20or%20exaggerated%203D%20characters%20that%20deviate%20significantly%20from%20human%20facial%20structures.%20In%20this%20scenario%2C%20it%20is%20important%20to%20consider%20the%20target%20character%27s%20facial%20structure%20and%20possible%20range%20of%20motion%20to%20preserve%20the%20semantics%20assumed%20by%20the%20original%20facial%20motions%20after%20the%20retargeting.%20To%20achieve%20this%2C%20we%20propose%20a%20local%20patch-based%20retargeting%20method%20that%20transfers%20facial%20animations%20captured%20in%20a%20source%20performance%20video%20to%20a%20target%20stylized%203D%20character.%20Our%20method%20consists%20of%20three%20modules.%20The%20Automatic%20Patch%20Extraction%20Module%20extracts%20local%20patches%20from%20the%20source%20video%20frame.%20These%20patches%20are%20processed%20through%20the%20Reenactment%20Module%20to%20generate%20correspondingly%20re-enacted%20target%20local%20patches.%20The%20Weight%20Estimation%20Module%20calculates%20the%20animation%20parameters%20for%20the%20target%20character%20at%20every%20frame%20for%20the%20creation%20of%20a%20complete%20facial%20animation%20sequence.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20can%20successfully%20transfer%20the%20semantic%20meaning%20of%20source%20facial%20expressions%20to%20stylized%20characters%20with%20considerable%20variations%20in%20facial%20feature%20proportion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Based%2520Facial%2520Retargeting%2520Using%2520Local%2520Patches%26entry.906535625%3DYeonsoo%2520Choi%2520and%2520Inyup%2520Lee%2520and%2520Sihun%2520Cha%2520and%2520Seonghyeon%2520Kim%2520and%2520Sunjin%2520Jung%2520and%2520Junyong%2520Noh%26entry.1292438233%3DIn%2520the%2520era%2520of%2520digital%2520animation%252C%2520the%2520quest%2520to%2520produce%2520lifelike%2520facial%2520animations%2520for%2520virtual%2520characters%2520has%2520led%2520to%2520the%2520development%2520of%2520various%2520retargeting%2520methods.%2520While%2520the%2520retargeting%2520facial%2520motion%2520between%2520models%2520of%2520similar%2520shapes%2520has%2520been%2520very%2520successful%252C%2520challenges%2520arise%2520when%2520the%2520retargeting%2520is%2520performed%2520on%2520stylized%2520or%2520exaggerated%25203D%2520characters%2520that%2520deviate%2520significantly%2520from%2520human%2520facial%2520structures.%2520In%2520this%2520scenario%252C%2520it%2520is%2520important%2520to%2520consider%2520the%2520target%2520character%2527s%2520facial%2520structure%2520and%2520possible%2520range%2520of%2520motion%2520to%2520preserve%2520the%2520semantics%2520assumed%2520by%2520the%2520original%2520facial%2520motions%2520after%2520the%2520retargeting.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520local%2520patch-based%2520retargeting%2520method%2520that%2520transfers%2520facial%2520animations%2520captured%2520in%2520a%2520source%2520performance%2520video%2520to%2520a%2520target%2520stylized%25203D%2520character.%2520Our%2520method%2520consists%2520of%2520three%2520modules.%2520The%2520Automatic%2520Patch%2520Extraction%2520Module%2520extracts%2520local%2520patches%2520from%2520the%2520source%2520video%2520frame.%2520These%2520patches%2520are%2520processed%2520through%2520the%2520Reenactment%2520Module%2520to%2520generate%2520correspondingly%2520re-enacted%2520target%2520local%2520patches.%2520The%2520Weight%2520Estimation%2520Module%2520calculates%2520the%2520animation%2520parameters%2520for%2520the%2520target%2520character%2520at%2520every%2520frame%2520for%2520the%2520creation%2520of%2520a%2520complete%2520facial%2520animation%2520sequence.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520can%2520successfully%2520transfer%2520the%2520semantic%2520meaning%2520of%2520source%2520facial%2520expressions%2520to%2520stylized%2520characters%2520with%2520considerable%2520variations%2520in%2520facial%2520feature%2520proportion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Facial%20Retargeting%20Using%20Local%20Patches&entry.906535625=Yeonsoo%20Choi%20and%20Inyup%20Lee%20and%20Sihun%20Cha%20and%20Seonghyeon%20Kim%20and%20Sunjin%20Jung%20and%20Junyong%20Noh&entry.1292438233=In%20the%20era%20of%20digital%20animation%2C%20the%20quest%20to%20produce%20lifelike%20facial%20animations%20for%20virtual%20characters%20has%20led%20to%20the%20development%20of%20various%20retargeting%20methods.%20While%20the%20retargeting%20facial%20motion%20between%20models%20of%20similar%20shapes%20has%20been%20very%20successful%2C%20challenges%20arise%20when%20the%20retargeting%20is%20performed%20on%20stylized%20or%20exaggerated%203D%20characters%20that%20deviate%20significantly%20from%20human%20facial%20structures.%20In%20this%20scenario%2C%20it%20is%20important%20to%20consider%20the%20target%20character%27s%20facial%20structure%20and%20possible%20range%20of%20motion%20to%20preserve%20the%20semantics%20assumed%20by%20the%20original%20facial%20motions%20after%20the%20retargeting.%20To%20achieve%20this%2C%20we%20propose%20a%20local%20patch-based%20retargeting%20method%20that%20transfers%20facial%20animations%20captured%20in%20a%20source%20performance%20video%20to%20a%20target%20stylized%203D%20character.%20Our%20method%20consists%20of%20three%20modules.%20The%20Automatic%20Patch%20Extraction%20Module%20extracts%20local%20patches%20from%20the%20source%20video%20frame.%20These%20patches%20are%20processed%20through%20the%20Reenactment%20Module%20to%20generate%20correspondingly%20re-enacted%20target%20local%20patches.%20The%20Weight%20Estimation%20Module%20calculates%20the%20animation%20parameters%20for%20the%20target%20character%20at%20every%20frame%20for%20the%20creation%20of%20a%20complete%20facial%20animation%20sequence.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20can%20successfully%20transfer%20the%20semantic%20meaning%20of%20source%20facial%20expressions%20to%20stylized%20characters%20with%20considerable%20variations%20in%20facial%20feature%20proportion.&entry.1838667208=http%3A//arxiv.org/abs/2601.08429v1&entry.124074799=Read"},
{"title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps", "author": "Krzysztof Zielinski and Dominik Belter", "abstract": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.", "link": "http://arxiv.org/abs/2601.08520v1", "date": "2026-01-13", "relevancy": 2.8608, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6084}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5595}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keyframe-based%20Dense%20Mapping%20with%20the%20Graph%20of%20View-Dependent%20Local%20Maps&body=Title%3A%20Keyframe-based%20Dense%20Mapping%20with%20the%20Graph%20of%20View-Dependent%20Local%20Maps%0AAuthor%3A%20Krzysztof%20Zielinski%20and%20Dominik%20Belter%0AAbstract%3A%20In%20this%20article%2C%20we%20propose%20a%20new%20keyframe-based%20mapping%20system.%20The%20proposed%20method%20updates%20local%20Normal%20Distribution%20Transform%20maps%20%28NDT%29%20using%20data%20from%20an%20RGB-D%20sensor.%20The%20cells%20of%20the%20NDT%20are%20stored%20in%202D%20view-dependent%20structures%20to%20better%20utilize%20the%20properties%20and%20uncertainty%20model%20of%20RGB-D%20cameras.%20This%20method%20naturally%20represents%20an%20object%20closer%20to%20the%20camera%20origin%20with%20higher%20precision.%20The%20local%20maps%20are%20stored%20in%20the%20pose%20graph%20which%20allows%20correcting%20global%20map%20after%20loop%20closure%20detection.%20We%20also%20propose%20a%20procedure%20that%20allows%20merging%20and%20filtering%20local%20maps%20to%20obtain%20a%20global%20map%20of%20the%20environment.%20Finally%2C%20we%20compare%20our%20method%20with%20Octomap%20and%20NDT-OM%20and%20provide%20example%20applications%20of%20the%20proposed%20mapping%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeyframe-based%2520Dense%2520Mapping%2520with%2520the%2520Graph%2520of%2520View-Dependent%2520Local%2520Maps%26entry.906535625%3DKrzysztof%2520Zielinski%2520and%2520Dominik%2520Belter%26entry.1292438233%3DIn%2520this%2520article%252C%2520we%2520propose%2520a%2520new%2520keyframe-based%2520mapping%2520system.%2520The%2520proposed%2520method%2520updates%2520local%2520Normal%2520Distribution%2520Transform%2520maps%2520%2528NDT%2529%2520using%2520data%2520from%2520an%2520RGB-D%2520sensor.%2520The%2520cells%2520of%2520the%2520NDT%2520are%2520stored%2520in%25202D%2520view-dependent%2520structures%2520to%2520better%2520utilize%2520the%2520properties%2520and%2520uncertainty%2520model%2520of%2520RGB-D%2520cameras.%2520This%2520method%2520naturally%2520represents%2520an%2520object%2520closer%2520to%2520the%2520camera%2520origin%2520with%2520higher%2520precision.%2520The%2520local%2520maps%2520are%2520stored%2520in%2520the%2520pose%2520graph%2520which%2520allows%2520correcting%2520global%2520map%2520after%2520loop%2520closure%2520detection.%2520We%2520also%2520propose%2520a%2520procedure%2520that%2520allows%2520merging%2520and%2520filtering%2520local%2520maps%2520to%2520obtain%2520a%2520global%2520map%2520of%2520the%2520environment.%2520Finally%252C%2520we%2520compare%2520our%2520method%2520with%2520Octomap%2520and%2520NDT-OM%2520and%2520provide%2520example%2520applications%2520of%2520the%2520proposed%2520mapping%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keyframe-based%20Dense%20Mapping%20with%20the%20Graph%20of%20View-Dependent%20Local%20Maps&entry.906535625=Krzysztof%20Zielinski%20and%20Dominik%20Belter&entry.1292438233=In%20this%20article%2C%20we%20propose%20a%20new%20keyframe-based%20mapping%20system.%20The%20proposed%20method%20updates%20local%20Normal%20Distribution%20Transform%20maps%20%28NDT%29%20using%20data%20from%20an%20RGB-D%20sensor.%20The%20cells%20of%20the%20NDT%20are%20stored%20in%202D%20view-dependent%20structures%20to%20better%20utilize%20the%20properties%20and%20uncertainty%20model%20of%20RGB-D%20cameras.%20This%20method%20naturally%20represents%20an%20object%20closer%20to%20the%20camera%20origin%20with%20higher%20precision.%20The%20local%20maps%20are%20stored%20in%20the%20pose%20graph%20which%20allows%20correcting%20global%20map%20after%20loop%20closure%20detection.%20We%20also%20propose%20a%20procedure%20that%20allows%20merging%20and%20filtering%20local%20maps%20to%20obtain%20a%20global%20map%20of%20the%20environment.%20Finally%2C%20we%20compare%20our%20method%20with%20Octomap%20and%20NDT-OM%20and%20provide%20example%20applications%20of%20the%20proposed%20mapping%20method.&entry.1838667208=http%3A//arxiv.org/abs/2601.08520v1&entry.124074799=Read"},
{"title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios", "author": "Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud", "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.", "link": "http://arxiv.org/abs/2601.08620v1", "date": "2026-01-13", "relevancy": 2.839, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViDoRe%20V3%3A%20A%20Comprehensive%20Evaluation%20of%20Retrieval%20Augmented%20Generation%20in%20Complex%20Real-World%20Scenarios&body=Title%3A%20ViDoRe%20V3%3A%20A%20Comprehensive%20Evaluation%20of%20Retrieval%20Augmented%20Generation%20in%20Complex%20Real-World%20Scenarios%0AAuthor%3A%20Ant%C3%B3nio%20Loison%20and%20Quentin%20Mac%C3%A9%20and%20Antoine%20Edy%20and%20Victor%20Xing%20and%20Tom%20Balough%20and%20Gabriel%20Moreira%20and%20Bo%20Liu%20and%20Manuel%20Faysse%20and%20C%C3%A9line%20Hudelot%20and%20Gautier%20Viaud%0AAbstract%3A%20Retrieval-Augmented%20Generation%20%28RAG%29%20pipelines%20must%20address%20challenges%20beyond%20simple%20single-document%20retrieval%2C%20such%20as%20interpreting%20visual%20elements%20%28tables%2C%20charts%2C%20images%29%2C%20synthesizing%20information%20across%20documents%2C%20and%20providing%20accurate%20source%20grounding.%20Existing%20benchmarks%20fail%20to%20capture%20this%20complexity%2C%20often%20focusing%20on%20textual%20data%2C%20single-document%20comprehension%2C%20or%20evaluating%20retrieval%20and%20generation%20in%20isolation.%20We%20introduce%20ViDoRe%20v3%2C%20a%20comprehensive%20multimodal%20RAG%20benchmark%20featuring%20multi-type%20queries%20over%20visually%20rich%20document%20corpora.%20It%20covers%2010%20datasets%20across%20diverse%20professional%20domains%2C%20comprising%20~26%2C000%20document%20pages%20paired%20with%203%2C099%20human-verified%20queries%2C%20each%20available%20in%206%20languages.%20Through%2012%2C000%20hours%20of%20human%20annotation%20effort%2C%20we%20provide%20high-quality%20annotations%20for%20retrieval%20relevance%2C%20bounding%20box%20localization%2C%20and%20verified%20reference%20answers.%20Our%20evaluation%20of%20state-of-the-art%20RAG%20pipelines%20reveals%20that%20visual%20retrievers%20outperform%20textual%20ones%2C%20late-interaction%20models%20and%20textual%20reranking%20substantially%20improve%20performance%2C%20and%20hybrid%20or%20purely%20visual%20contexts%20enhance%20answer%20generation%20quality.%20However%2C%20current%20models%20still%20struggle%20with%20non-textual%20elements%2C%20open-ended%20queries%2C%20and%20fine-grained%20visual%20grounding.%20To%20encourage%20progress%20in%20addressing%20these%20challenges%2C%20the%20benchmark%20is%20released%20under%20a%20commercially%20permissive%20license%20at%20https%3A//hf.co/vidore.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViDoRe%2520V3%253A%2520A%2520Comprehensive%2520Evaluation%2520of%2520Retrieval%2520Augmented%2520Generation%2520in%2520Complex%2520Real-World%2520Scenarios%26entry.906535625%3DAnt%25C3%25B3nio%2520Loison%2520and%2520Quentin%2520Mac%25C3%25A9%2520and%2520Antoine%2520Edy%2520and%2520Victor%2520Xing%2520and%2520Tom%2520Balough%2520and%2520Gabriel%2520Moreira%2520and%2520Bo%2520Liu%2520and%2520Manuel%2520Faysse%2520and%2520C%25C3%25A9line%2520Hudelot%2520and%2520Gautier%2520Viaud%26entry.1292438233%3DRetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520pipelines%2520must%2520address%2520challenges%2520beyond%2520simple%2520single-document%2520retrieval%252C%2520such%2520as%2520interpreting%2520visual%2520elements%2520%2528tables%252C%2520charts%252C%2520images%2529%252C%2520synthesizing%2520information%2520across%2520documents%252C%2520and%2520providing%2520accurate%2520source%2520grounding.%2520Existing%2520benchmarks%2520fail%2520to%2520capture%2520this%2520complexity%252C%2520often%2520focusing%2520on%2520textual%2520data%252C%2520single-document%2520comprehension%252C%2520or%2520evaluating%2520retrieval%2520and%2520generation%2520in%2520isolation.%2520We%2520introduce%2520ViDoRe%2520v3%252C%2520a%2520comprehensive%2520multimodal%2520RAG%2520benchmark%2520featuring%2520multi-type%2520queries%2520over%2520visually%2520rich%2520document%2520corpora.%2520It%2520covers%252010%2520datasets%2520across%2520diverse%2520professional%2520domains%252C%2520comprising%2520~26%252C000%2520document%2520pages%2520paired%2520with%25203%252C099%2520human-verified%2520queries%252C%2520each%2520available%2520in%25206%2520languages.%2520Through%252012%252C000%2520hours%2520of%2520human%2520annotation%2520effort%252C%2520we%2520provide%2520high-quality%2520annotations%2520for%2520retrieval%2520relevance%252C%2520bounding%2520box%2520localization%252C%2520and%2520verified%2520reference%2520answers.%2520Our%2520evaluation%2520of%2520state-of-the-art%2520RAG%2520pipelines%2520reveals%2520that%2520visual%2520retrievers%2520outperform%2520textual%2520ones%252C%2520late-interaction%2520models%2520and%2520textual%2520reranking%2520substantially%2520improve%2520performance%252C%2520and%2520hybrid%2520or%2520purely%2520visual%2520contexts%2520enhance%2520answer%2520generation%2520quality.%2520However%252C%2520current%2520models%2520still%2520struggle%2520with%2520non-textual%2520elements%252C%2520open-ended%2520queries%252C%2520and%2520fine-grained%2520visual%2520grounding.%2520To%2520encourage%2520progress%2520in%2520addressing%2520these%2520challenges%252C%2520the%2520benchmark%2520is%2520released%2520under%2520a%2520commercially%2520permissive%2520license%2520at%2520https%253A//hf.co/vidore.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViDoRe%20V3%3A%20A%20Comprehensive%20Evaluation%20of%20Retrieval%20Augmented%20Generation%20in%20Complex%20Real-World%20Scenarios&entry.906535625=Ant%C3%B3nio%20Loison%20and%20Quentin%20Mac%C3%A9%20and%20Antoine%20Edy%20and%20Victor%20Xing%20and%20Tom%20Balough%20and%20Gabriel%20Moreira%20and%20Bo%20Liu%20and%20Manuel%20Faysse%20and%20C%C3%A9line%20Hudelot%20and%20Gautier%20Viaud&entry.1292438233=Retrieval-Augmented%20Generation%20%28RAG%29%20pipelines%20must%20address%20challenges%20beyond%20simple%20single-document%20retrieval%2C%20such%20as%20interpreting%20visual%20elements%20%28tables%2C%20charts%2C%20images%29%2C%20synthesizing%20information%20across%20documents%2C%20and%20providing%20accurate%20source%20grounding.%20Existing%20benchmarks%20fail%20to%20capture%20this%20complexity%2C%20often%20focusing%20on%20textual%20data%2C%20single-document%20comprehension%2C%20or%20evaluating%20retrieval%20and%20generation%20in%20isolation.%20We%20introduce%20ViDoRe%20v3%2C%20a%20comprehensive%20multimodal%20RAG%20benchmark%20featuring%20multi-type%20queries%20over%20visually%20rich%20document%20corpora.%20It%20covers%2010%20datasets%20across%20diverse%20professional%20domains%2C%20comprising%20~26%2C000%20document%20pages%20paired%20with%203%2C099%20human-verified%20queries%2C%20each%20available%20in%206%20languages.%20Through%2012%2C000%20hours%20of%20human%20annotation%20effort%2C%20we%20provide%20high-quality%20annotations%20for%20retrieval%20relevance%2C%20bounding%20box%20localization%2C%20and%20verified%20reference%20answers.%20Our%20evaluation%20of%20state-of-the-art%20RAG%20pipelines%20reveals%20that%20visual%20retrievers%20outperform%20textual%20ones%2C%20late-interaction%20models%20and%20textual%20reranking%20substantially%20improve%20performance%2C%20and%20hybrid%20or%20purely%20visual%20contexts%20enhance%20answer%20generation%20quality.%20However%2C%20current%20models%20still%20struggle%20with%20non-textual%20elements%2C%20open-ended%20queries%2C%20and%20fine-grained%20visual%20grounding.%20To%20encourage%20progress%20in%20addressing%20these%20challenges%2C%20the%20benchmark%20is%20released%20under%20a%20commercially%20permissive%20license%20at%20https%3A//hf.co/vidore.&entry.1838667208=http%3A//arxiv.org/abs/2601.08620v1&entry.124074799=Read"},
{"title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification", "author": "Yingying Feng and Jie Li and Jie Hu and Yukang Zhang and Lei Tan and Jiayi Ji", "abstract": "Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively. The code is available at: \\textcolor{magenta}{https://github.com/stone96123/MDReID}.", "link": "http://arxiv.org/abs/2510.23301v2", "date": "2026-01-13", "relevancy": 2.8174, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5677}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDReID%3A%20Modality-Decoupled%20Learning%20for%20Any-to-Any%20Multi-Modal%20Object%20Re-Identification&body=Title%3A%20MDReID%3A%20Modality-Decoupled%20Learning%20for%20Any-to-Any%20Multi-Modal%20Object%20Re-Identification%0AAuthor%3A%20Yingying%20Feng%20and%20Jie%20Li%20and%20Jie%20Hu%20and%20Yukang%20Zhang%20and%20Lei%20Tan%20and%20Jiayi%20Ji%0AAbstract%3A%20Real-world%20object%20re-identification%20%28ReID%29%20systems%20often%20face%20modality%20inconsistencies%2C%20where%20query%20and%20gallery%20images%20come%20from%20different%20sensors%20%28e.g.%2C%20RGB%2C%20NIR%2C%20TIR%29.%20However%2C%20most%20existing%20methods%20assume%20modality-matched%20conditions%2C%20which%20limits%20their%20robustness%20and%20scalability%20in%20practical%20applications.%20To%20address%20this%20challenge%2C%20we%20propose%20MDReID%2C%20a%20flexible%20any-to-any%20image-level%20ReID%20framework%20designed%20to%20operate%20under%20both%20modality-matched%20and%20modality-mismatched%20scenarios.%20MDReID%20builds%20on%20the%20insight%20that%20modality%20information%20can%20be%20decomposed%20into%20two%20components%3A%20modality-shared%20features%20that%20are%20predictable%20and%20transferable%2C%20and%20modality-specific%20features%20that%20capture%20unique%2C%20modality-dependent%20characteristics.%20To%20effectively%20leverage%20this%2C%20MDReID%20introduces%20two%20key%20components%3A%20the%20Modality%20Decoupling%20Learning%20%28MDL%29%20and%20Modality-aware%20Metric%20Learning%20%28MML%29.%20Specifically%2C%20MDL%20explicitly%20decomposes%20modality%20features%20into%20modality-shared%20and%20modality-specific%20representations%2C%20enabling%20effective%20retrieval%20in%20both%20modality-aligned%20and%20mismatched%20scenarios.%20MML%2C%20a%20tailored%20metric%20learning%20strategy%2C%20further%20enforces%20orthogonality%20and%20complementarity%20between%20the%20two%20components%20to%20enhance%20discriminative%20power%20across%20modalities.%20Extensive%20experiments%20conducted%20on%20three%20challenging%20multi-modality%20ReID%20benchmarks%20%28RGBNT201%2C%20RGBNT100%2C%20MSVR310%29%20consistently%20demonstrate%20the%20superiority%20of%20MDReID.%20Notably%2C%20MDReID%20achieves%20significant%20mAP%20improvements%20of%209.8%5C%25%2C%203.0%5C%25%2C%20and%2011.5%5C%25%20in%20general%20modality-matched%20scenarios%2C%20and%20average%20gains%20of%203.4%5C%25%2C%2011.8%5C%25%2C%20and%2010.9%5C%25%20in%20modality-mismatched%20scenarios%2C%20respectively.%20The%20code%20is%20available%20at%3A%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/stone96123/MDReID%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDReID%253A%2520Modality-Decoupled%2520Learning%2520for%2520Any-to-Any%2520Multi-Modal%2520Object%2520Re-Identification%26entry.906535625%3DYingying%2520Feng%2520and%2520Jie%2520Li%2520and%2520Jie%2520Hu%2520and%2520Yukang%2520Zhang%2520and%2520Lei%2520Tan%2520and%2520Jiayi%2520Ji%26entry.1292438233%3DReal-world%2520object%2520re-identification%2520%2528ReID%2529%2520systems%2520often%2520face%2520modality%2520inconsistencies%252C%2520where%2520query%2520and%2520gallery%2520images%2520come%2520from%2520different%2520sensors%2520%2528e.g.%252C%2520RGB%252C%2520NIR%252C%2520TIR%2529.%2520However%252C%2520most%2520existing%2520methods%2520assume%2520modality-matched%2520conditions%252C%2520which%2520limits%2520their%2520robustness%2520and%2520scalability%2520in%2520practical%2520applications.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520MDReID%252C%2520a%2520flexible%2520any-to-any%2520image-level%2520ReID%2520framework%2520designed%2520to%2520operate%2520under%2520both%2520modality-matched%2520and%2520modality-mismatched%2520scenarios.%2520MDReID%2520builds%2520on%2520the%2520insight%2520that%2520modality%2520information%2520can%2520be%2520decomposed%2520into%2520two%2520components%253A%2520modality-shared%2520features%2520that%2520are%2520predictable%2520and%2520transferable%252C%2520and%2520modality-specific%2520features%2520that%2520capture%2520unique%252C%2520modality-dependent%2520characteristics.%2520To%2520effectively%2520leverage%2520this%252C%2520MDReID%2520introduces%2520two%2520key%2520components%253A%2520the%2520Modality%2520Decoupling%2520Learning%2520%2528MDL%2529%2520and%2520Modality-aware%2520Metric%2520Learning%2520%2528MML%2529.%2520Specifically%252C%2520MDL%2520explicitly%2520decomposes%2520modality%2520features%2520into%2520modality-shared%2520and%2520modality-specific%2520representations%252C%2520enabling%2520effective%2520retrieval%2520in%2520both%2520modality-aligned%2520and%2520mismatched%2520scenarios.%2520MML%252C%2520a%2520tailored%2520metric%2520learning%2520strategy%252C%2520further%2520enforces%2520orthogonality%2520and%2520complementarity%2520between%2520the%2520two%2520components%2520to%2520enhance%2520discriminative%2520power%2520across%2520modalities.%2520Extensive%2520experiments%2520conducted%2520on%2520three%2520challenging%2520multi-modality%2520ReID%2520benchmarks%2520%2528RGBNT201%252C%2520RGBNT100%252C%2520MSVR310%2529%2520consistently%2520demonstrate%2520the%2520superiority%2520of%2520MDReID.%2520Notably%252C%2520MDReID%2520achieves%2520significant%2520mAP%2520improvements%2520of%25209.8%255C%2525%252C%25203.0%255C%2525%252C%2520and%252011.5%255C%2525%2520in%2520general%2520modality-matched%2520scenarios%252C%2520and%2520average%2520gains%2520of%25203.4%255C%2525%252C%252011.8%255C%2525%252C%2520and%252010.9%255C%2525%2520in%2520modality-mismatched%2520scenarios%252C%2520respectively.%2520The%2520code%2520is%2520available%2520at%253A%2520%255Ctextcolor%257Bmagenta%257D%257Bhttps%253A//github.com/stone96123/MDReID%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDReID%3A%20Modality-Decoupled%20Learning%20for%20Any-to-Any%20Multi-Modal%20Object%20Re-Identification&entry.906535625=Yingying%20Feng%20and%20Jie%20Li%20and%20Jie%20Hu%20and%20Yukang%20Zhang%20and%20Lei%20Tan%20and%20Jiayi%20Ji&entry.1292438233=Real-world%20object%20re-identification%20%28ReID%29%20systems%20often%20face%20modality%20inconsistencies%2C%20where%20query%20and%20gallery%20images%20come%20from%20different%20sensors%20%28e.g.%2C%20RGB%2C%20NIR%2C%20TIR%29.%20However%2C%20most%20existing%20methods%20assume%20modality-matched%20conditions%2C%20which%20limits%20their%20robustness%20and%20scalability%20in%20practical%20applications.%20To%20address%20this%20challenge%2C%20we%20propose%20MDReID%2C%20a%20flexible%20any-to-any%20image-level%20ReID%20framework%20designed%20to%20operate%20under%20both%20modality-matched%20and%20modality-mismatched%20scenarios.%20MDReID%20builds%20on%20the%20insight%20that%20modality%20information%20can%20be%20decomposed%20into%20two%20components%3A%20modality-shared%20features%20that%20are%20predictable%20and%20transferable%2C%20and%20modality-specific%20features%20that%20capture%20unique%2C%20modality-dependent%20characteristics.%20To%20effectively%20leverage%20this%2C%20MDReID%20introduces%20two%20key%20components%3A%20the%20Modality%20Decoupling%20Learning%20%28MDL%29%20and%20Modality-aware%20Metric%20Learning%20%28MML%29.%20Specifically%2C%20MDL%20explicitly%20decomposes%20modality%20features%20into%20modality-shared%20and%20modality-specific%20representations%2C%20enabling%20effective%20retrieval%20in%20both%20modality-aligned%20and%20mismatched%20scenarios.%20MML%2C%20a%20tailored%20metric%20learning%20strategy%2C%20further%20enforces%20orthogonality%20and%20complementarity%20between%20the%20two%20components%20to%20enhance%20discriminative%20power%20across%20modalities.%20Extensive%20experiments%20conducted%20on%20three%20challenging%20multi-modality%20ReID%20benchmarks%20%28RGBNT201%2C%20RGBNT100%2C%20MSVR310%29%20consistently%20demonstrate%20the%20superiority%20of%20MDReID.%20Notably%2C%20MDReID%20achieves%20significant%20mAP%20improvements%20of%209.8%5C%25%2C%203.0%5C%25%2C%20and%2011.5%5C%25%20in%20general%20modality-matched%20scenarios%2C%20and%20average%20gains%20of%203.4%5C%25%2C%2011.8%5C%25%2C%20and%2010.9%5C%25%20in%20modality-mismatched%20scenarios%2C%20respectively.%20The%20code%20is%20available%20at%3A%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/stone96123/MDReID%7D.&entry.1838667208=http%3A//arxiv.org/abs/2510.23301v2&entry.124074799=Read"},
{"title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning", "author": "Shashanka Venkataramanan and Valentinos Pariza and Mohammadreza Salehi and Lukas Knobel and Spyros Gidaris and Elias Ramzi and Andrei Bursuc and Yuki M. Asano", "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.", "link": "http://arxiv.org/abs/2507.14137v3", "date": "2026-01-13", "relevancy": 2.7941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%20Learning&body=Title%3A%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%20Learning%0AAuthor%3A%20Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%20%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%20surpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%20CLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%20pipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%20a%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%20SSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%20large%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%20account%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%20introduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%20nested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%20into%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%20enabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%20novel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%20biases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%20content.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%20demonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%20establish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%20open%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%20the%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%20https%3A//github.com/valeoai/Franca.%0ALink%3A%20http%3A//arxiv.org/abs/2507.14137v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFranca%253A%2520Nested%2520Matryoshka%2520Clustering%2520for%2520Scalable%2520Visual%2520Representation%2520Learning%26entry.906535625%3DShashanka%2520Venkataramanan%2520and%2520Valentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Lukas%2520Knobel%2520and%2520Spyros%2520Gidaris%2520and%2520Elias%2520Ramzi%2520and%2520Andrei%2520Bursuc%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3DWe%2520present%2520Franca%2520%2528pronounced%2520Fran-ka%2529%253A%2520free%2520one%253B%2520the%2520first%2520fully%2520open-source%2520%2528data%252C%2520code%252C%2520weights%2529%2520vision%2520foundation%2520model%2520that%2520matches%2520and%2520in%2520many%2520cases%2520surpasses%2520the%2520performance%2520of%2520state-of-the-art%2520proprietary%2520models%252C%2520e.g.%252C%2520DINOv2%252C%2520CLIP%252C%2520SigLIPv2%252C%2520etc.%2520Our%2520approach%2520is%2520grounded%2520in%2520a%2520transparent%2520training%2520pipeline%2520inspired%2520by%2520Web-SSL%2520and%2520uses%2520publicly%2520available%2520data%253A%2520ImageNet-21K%2520and%2520a%2520subset%2520of%2520ReLAION-2B.%2520Beyond%2520model%2520release%252C%2520we%2520tackle%2520critical%2520limitations%2520in%2520SSL%2520clustering%2520methods.%2520While%2520modern%2520models%2520rely%2520on%2520assigning%2520image%2520features%2520to%2520large%2520codebooks%2520via%2520clustering%2520algorithms%2520like%2520Sinkhorn-Knopp%252C%2520they%2520fail%2520to%2520account%2520for%2520the%2520inherent%2520ambiguity%2520in%2520clustering%2520semantics.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520parameter-efficient%252C%2520multi-head%2520clustering%2520projector%2520based%2520on%2520nested%2520Matryoshka%2520representations.%2520This%2520design%2520progressively%2520refines%2520features%2520into%2520increasingly%2520fine-grained%2520clusters%2520without%2520increasing%2520the%2520model%2520size%252C%2520enabling%2520both%2520performance%2520and%2520memory%2520efficiency.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520positional%2520disentanglement%2520strategy%2520that%2520explicitly%2520removes%2520positional%2520biases%2520from%2520dense%2520representations%252C%2520thereby%2520improving%2520the%2520encoding%2520of%2520semantic%2520content.%2520This%2520leads%2520to%2520consistent%2520gains%2520on%2520several%2520downstream%2520benchmarks%252C%2520demonstrating%2520the%2520utility%2520of%2520cleaner%2520feature%2520spaces.%2520Our%2520contributions%2520establish%2520a%2520new%2520standard%2520for%2520transparent%252C%2520high-performance%2520vision%2520models%2520and%2520open%2520a%2520path%2520toward%2520more%2520reproducible%2520and%2520generalizable%2520foundation%2520models%2520for%2520the%2520broader%2520AI%2520community.%2520The%2520code%2520and%2520model%2520checkpoints%2520are%2520available%2520at%2520https%253A//github.com/valeoai/Franca.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14137v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%20Learning&entry.906535625=Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano&entry.1292438233=We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%20%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%20surpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%20CLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%20pipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%20a%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%20SSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%20large%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%20account%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%20introduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%20nested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%20into%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%20enabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%20novel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%20biases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%20content.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%20demonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%20establish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%20open%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%20the%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%20https%3A//github.com/valeoai/Franca.&entry.1838667208=http%3A//arxiv.org/abs/2507.14137v3&entry.124074799=Read"},
{"title": "GSAlign: Geometric and Semantic Alignment Network for Aerial-Ground Person Re-Identification", "author": "Qiao Li and Jie Li and Yukang Zhang and Lei Tan and Jing Chen and Jiayi Ji", "abstract": "Aerial-Ground person re-identification (AG-ReID) is an emerging yet challenging task that aims to match pedestrian images captured from drastically different viewpoints, typically from unmanned aerial vehicles (UAVs) and ground-based surveillance cameras. The task poses significant challenges due to extreme viewpoint discrepancies, occlusions, and domain gaps between aerial and ground imagery. While prior works have made progress by learning cross-view representations, they remain limited in handling severe pose variations and spatial misalignment. To address these issues, we propose a Geometric and Semantic Alignment Network (GSAlign) tailored for AG-ReID. GSAlign introduces two key components to jointly tackle geometric distortion and semantic misalignment in aerial-ground matching: a Learnable Thin Plate Spline (LTPS) Module and a Dynamic Alignment Module (DAM). The LTPS module adaptively warps pedestrian features based on a set of learned keypoints, effectively compensating for geometric variations caused by extreme viewpoint changes. In parallel, the DAM estimates visibility-aware representation masks that highlight visible body regions at the semantic level, thereby alleviating the negative impact of occlusions and partial observations in cross-view correspondence. A comprehensive evaluation on CARGO with four matching protocols demonstrates the effectiveness of GSAlign, achieving significant improvements of +18.8\\% in mAP and +16.8\\% in Rank-1 accuracy over previous state-of-the-art methods on the aerial-ground setting.", "link": "http://arxiv.org/abs/2510.22268v2", "date": "2026-01-13", "relevancy": 2.7573, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSAlign%3A%20Geometric%20and%20Semantic%20Alignment%20Network%20for%20Aerial-Ground%20Person%20Re-Identification&body=Title%3A%20GSAlign%3A%20Geometric%20and%20Semantic%20Alignment%20Network%20for%20Aerial-Ground%20Person%20Re-Identification%0AAuthor%3A%20Qiao%20Li%20and%20Jie%20Li%20and%20Yukang%20Zhang%20and%20Lei%20Tan%20and%20Jing%20Chen%20and%20Jiayi%20Ji%0AAbstract%3A%20Aerial-Ground%20person%20re-identification%20%28AG-ReID%29%20is%20an%20emerging%20yet%20challenging%20task%20that%20aims%20to%20match%20pedestrian%20images%20captured%20from%20drastically%20different%20viewpoints%2C%20typically%20from%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%20ground-based%20surveillance%20cameras.%20The%20task%20poses%20significant%20challenges%20due%20to%20extreme%20viewpoint%20discrepancies%2C%20occlusions%2C%20and%20domain%20gaps%20between%20aerial%20and%20ground%20imagery.%20While%20prior%20works%20have%20made%20progress%20by%20learning%20cross-view%20representations%2C%20they%20remain%20limited%20in%20handling%20severe%20pose%20variations%20and%20spatial%20misalignment.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Geometric%20and%20Semantic%20Alignment%20Network%20%28GSAlign%29%20tailored%20for%20AG-ReID.%20GSAlign%20introduces%20two%20key%20components%20to%20jointly%20tackle%20geometric%20distortion%20and%20semantic%20misalignment%20in%20aerial-ground%20matching%3A%20a%20Learnable%20Thin%20Plate%20Spline%20%28LTPS%29%20Module%20and%20a%20Dynamic%20Alignment%20Module%20%28DAM%29.%20The%20LTPS%20module%20adaptively%20warps%20pedestrian%20features%20based%20on%20a%20set%20of%20learned%20keypoints%2C%20effectively%20compensating%20for%20geometric%20variations%20caused%20by%20extreme%20viewpoint%20changes.%20In%20parallel%2C%20the%20DAM%20estimates%20visibility-aware%20representation%20masks%20that%20highlight%20visible%20body%20regions%20at%20the%20semantic%20level%2C%20thereby%20alleviating%20the%20negative%20impact%20of%20occlusions%20and%20partial%20observations%20in%20cross-view%20correspondence.%20A%20comprehensive%20evaluation%20on%20CARGO%20with%20four%20matching%20protocols%20demonstrates%20the%20effectiveness%20of%20GSAlign%2C%20achieving%20significant%20improvements%20of%20%2B18.8%5C%25%20in%20mAP%20and%20%2B16.8%5C%25%20in%20Rank-1%20accuracy%20over%20previous%20state-of-the-art%20methods%20on%20the%20aerial-ground%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSAlign%253A%2520Geometric%2520and%2520Semantic%2520Alignment%2520Network%2520for%2520Aerial-Ground%2520Person%2520Re-Identification%26entry.906535625%3DQiao%2520Li%2520and%2520Jie%2520Li%2520and%2520Yukang%2520Zhang%2520and%2520Lei%2520Tan%2520and%2520Jing%2520Chen%2520and%2520Jiayi%2520Ji%26entry.1292438233%3DAerial-Ground%2520person%2520re-identification%2520%2528AG-ReID%2529%2520is%2520an%2520emerging%2520yet%2520challenging%2520task%2520that%2520aims%2520to%2520match%2520pedestrian%2520images%2520captured%2520from%2520drastically%2520different%2520viewpoints%252C%2520typically%2520from%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520and%2520ground-based%2520surveillance%2520cameras.%2520The%2520task%2520poses%2520significant%2520challenges%2520due%2520to%2520extreme%2520viewpoint%2520discrepancies%252C%2520occlusions%252C%2520and%2520domain%2520gaps%2520between%2520aerial%2520and%2520ground%2520imagery.%2520While%2520prior%2520works%2520have%2520made%2520progress%2520by%2520learning%2520cross-view%2520representations%252C%2520they%2520remain%2520limited%2520in%2520handling%2520severe%2520pose%2520variations%2520and%2520spatial%2520misalignment.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520Geometric%2520and%2520Semantic%2520Alignment%2520Network%2520%2528GSAlign%2529%2520tailored%2520for%2520AG-ReID.%2520GSAlign%2520introduces%2520two%2520key%2520components%2520to%2520jointly%2520tackle%2520geometric%2520distortion%2520and%2520semantic%2520misalignment%2520in%2520aerial-ground%2520matching%253A%2520a%2520Learnable%2520Thin%2520Plate%2520Spline%2520%2528LTPS%2529%2520Module%2520and%2520a%2520Dynamic%2520Alignment%2520Module%2520%2528DAM%2529.%2520The%2520LTPS%2520module%2520adaptively%2520warps%2520pedestrian%2520features%2520based%2520on%2520a%2520set%2520of%2520learned%2520keypoints%252C%2520effectively%2520compensating%2520for%2520geometric%2520variations%2520caused%2520by%2520extreme%2520viewpoint%2520changes.%2520In%2520parallel%252C%2520the%2520DAM%2520estimates%2520visibility-aware%2520representation%2520masks%2520that%2520highlight%2520visible%2520body%2520regions%2520at%2520the%2520semantic%2520level%252C%2520thereby%2520alleviating%2520the%2520negative%2520impact%2520of%2520occlusions%2520and%2520partial%2520observations%2520in%2520cross-view%2520correspondence.%2520A%2520comprehensive%2520evaluation%2520on%2520CARGO%2520with%2520four%2520matching%2520protocols%2520demonstrates%2520the%2520effectiveness%2520of%2520GSAlign%252C%2520achieving%2520significant%2520improvements%2520of%2520%252B18.8%255C%2525%2520in%2520mAP%2520and%2520%252B16.8%255C%2525%2520in%2520Rank-1%2520accuracy%2520over%2520previous%2520state-of-the-art%2520methods%2520on%2520the%2520aerial-ground%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSAlign%3A%20Geometric%20and%20Semantic%20Alignment%20Network%20for%20Aerial-Ground%20Person%20Re-Identification&entry.906535625=Qiao%20Li%20and%20Jie%20Li%20and%20Yukang%20Zhang%20and%20Lei%20Tan%20and%20Jing%20Chen%20and%20Jiayi%20Ji&entry.1292438233=Aerial-Ground%20person%20re-identification%20%28AG-ReID%29%20is%20an%20emerging%20yet%20challenging%20task%20that%20aims%20to%20match%20pedestrian%20images%20captured%20from%20drastically%20different%20viewpoints%2C%20typically%20from%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%20ground-based%20surveillance%20cameras.%20The%20task%20poses%20significant%20challenges%20due%20to%20extreme%20viewpoint%20discrepancies%2C%20occlusions%2C%20and%20domain%20gaps%20between%20aerial%20and%20ground%20imagery.%20While%20prior%20works%20have%20made%20progress%20by%20learning%20cross-view%20representations%2C%20they%20remain%20limited%20in%20handling%20severe%20pose%20variations%20and%20spatial%20misalignment.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Geometric%20and%20Semantic%20Alignment%20Network%20%28GSAlign%29%20tailored%20for%20AG-ReID.%20GSAlign%20introduces%20two%20key%20components%20to%20jointly%20tackle%20geometric%20distortion%20and%20semantic%20misalignment%20in%20aerial-ground%20matching%3A%20a%20Learnable%20Thin%20Plate%20Spline%20%28LTPS%29%20Module%20and%20a%20Dynamic%20Alignment%20Module%20%28DAM%29.%20The%20LTPS%20module%20adaptively%20warps%20pedestrian%20features%20based%20on%20a%20set%20of%20learned%20keypoints%2C%20effectively%20compensating%20for%20geometric%20variations%20caused%20by%20extreme%20viewpoint%20changes.%20In%20parallel%2C%20the%20DAM%20estimates%20visibility-aware%20representation%20masks%20that%20highlight%20visible%20body%20regions%20at%20the%20semantic%20level%2C%20thereby%20alleviating%20the%20negative%20impact%20of%20occlusions%20and%20partial%20observations%20in%20cross-view%20correspondence.%20A%20comprehensive%20evaluation%20on%20CARGO%20with%20four%20matching%20protocols%20demonstrates%20the%20effectiveness%20of%20GSAlign%2C%20achieving%20significant%20improvements%20of%20%2B18.8%5C%25%20in%20mAP%20and%20%2B16.8%5C%25%20in%20Rank-1%20accuracy%20over%20previous%20state-of-the-art%20methods%20on%20the%20aerial-ground%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2510.22268v2&entry.124074799=Read"},
{"title": "Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models", "author": "Tolgay Atinc Uzun and Dmitry Ignatov and Radu Timofte", "abstract": "Channel configuration search the optimization of layer specifications such as layer widths in deep neural networks presents a complex combinatorial challenge constrained by tensor shape compatibility and computational budgets. We posit that Large Language Models (LLMs) offer a transformative approach to Neural Architecture Search (NAS), capable of reasoning about architectural code structure in ways that traditional heuristics cannot. In this paper, we investigate the application of an LLM-driven NAS framework to the problem of channel configuration. We formulate the search as a sequence of conditional code generation tasks, where an LLM refines architectural specifications based on performance telemetry. Crucially, we address the data scarcity problem by generating a vast corpus of valid, shape-consistent architectures via Abstract Syntax Tree (AST) mutations. While these mutated networks are not necessarily high-performing, they provide the critical volume of structural data required for the LLM to learn the latent relationship between channel configurations and model performance. This allows the LLM to internalize complex design patterns and apply them to optimize feature extraction strategies. Experimental results on CIFAR-100 validate the efficacy of this approach, demonstrating that the model yields statistically significant improvements in accuracy. Our analysis confirms that the LLM successfully acquires domain-specific architectural priors, distinguishing this method from random search and highlighting the immense potential of language-driven design in deep learning.", "link": "http://arxiv.org/abs/2601.08517v1", "date": "2026-01-13", "relevancy": 2.7483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Loop%20LLM%20Discovery%20of%20Non-Standard%20Channel%20Priors%20in%20Vision%20Models&body=Title%3A%20Closed-Loop%20LLM%20Discovery%20of%20Non-Standard%20Channel%20Priors%20in%20Vision%20Models%0AAuthor%3A%20Tolgay%20Atinc%20Uzun%20and%20Dmitry%20Ignatov%20and%20Radu%20Timofte%0AAbstract%3A%20Channel%20configuration%20search%20the%20optimization%20of%20layer%20specifications%20such%20as%20layer%20widths%20in%20deep%20neural%20networks%20presents%20a%20complex%20combinatorial%20challenge%20constrained%20by%20tensor%20shape%20compatibility%20and%20computational%20budgets.%20We%20posit%20that%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20transformative%20approach%20to%20Neural%20Architecture%20Search%20%28NAS%29%2C%20capable%20of%20reasoning%20about%20architectural%20code%20structure%20in%20ways%20that%20traditional%20heuristics%20cannot.%20In%20this%20paper%2C%20we%20investigate%20the%20application%20of%20an%20LLM-driven%20NAS%20framework%20to%20the%20problem%20of%20channel%20configuration.%20We%20formulate%20the%20search%20as%20a%20sequence%20of%20conditional%20code%20generation%20tasks%2C%20where%20an%20LLM%20refines%20architectural%20specifications%20based%20on%20performance%20telemetry.%20Crucially%2C%20we%20address%20the%20data%20scarcity%20problem%20by%20generating%20a%20vast%20corpus%20of%20valid%2C%20shape-consistent%20architectures%20via%20Abstract%20Syntax%20Tree%20%28AST%29%20mutations.%20While%20these%20mutated%20networks%20are%20not%20necessarily%20high-performing%2C%20they%20provide%20the%20critical%20volume%20of%20structural%20data%20required%20for%20the%20LLM%20to%20learn%20the%20latent%20relationship%20between%20channel%20configurations%20and%20model%20performance.%20This%20allows%20the%20LLM%20to%20internalize%20complex%20design%20patterns%20and%20apply%20them%20to%20optimize%20feature%20extraction%20strategies.%20Experimental%20results%20on%20CIFAR-100%20validate%20the%20efficacy%20of%20this%20approach%2C%20demonstrating%20that%20the%20model%20yields%20statistically%20significant%20improvements%20in%20accuracy.%20Our%20analysis%20confirms%20that%20the%20LLM%20successfully%20acquires%20domain-specific%20architectural%20priors%2C%20distinguishing%20this%20method%20from%20random%20search%20and%20highlighting%20the%20immense%20potential%20of%20language-driven%20design%20in%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Loop%2520LLM%2520Discovery%2520of%2520Non-Standard%2520Channel%2520Priors%2520in%2520Vision%2520Models%26entry.906535625%3DTolgay%2520Atinc%2520Uzun%2520and%2520Dmitry%2520Ignatov%2520and%2520Radu%2520Timofte%26entry.1292438233%3DChannel%2520configuration%2520search%2520the%2520optimization%2520of%2520layer%2520specifications%2520such%2520as%2520layer%2520widths%2520in%2520deep%2520neural%2520networks%2520presents%2520a%2520complex%2520combinatorial%2520challenge%2520constrained%2520by%2520tensor%2520shape%2520compatibility%2520and%2520computational%2520budgets.%2520We%2520posit%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520transformative%2520approach%2520to%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%252C%2520capable%2520of%2520reasoning%2520about%2520architectural%2520code%2520structure%2520in%2520ways%2520that%2520traditional%2520heuristics%2520cannot.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520application%2520of%2520an%2520LLM-driven%2520NAS%2520framework%2520to%2520the%2520problem%2520of%2520channel%2520configuration.%2520We%2520formulate%2520the%2520search%2520as%2520a%2520sequence%2520of%2520conditional%2520code%2520generation%2520tasks%252C%2520where%2520an%2520LLM%2520refines%2520architectural%2520specifications%2520based%2520on%2520performance%2520telemetry.%2520Crucially%252C%2520we%2520address%2520the%2520data%2520scarcity%2520problem%2520by%2520generating%2520a%2520vast%2520corpus%2520of%2520valid%252C%2520shape-consistent%2520architectures%2520via%2520Abstract%2520Syntax%2520Tree%2520%2528AST%2529%2520mutations.%2520While%2520these%2520mutated%2520networks%2520are%2520not%2520necessarily%2520high-performing%252C%2520they%2520provide%2520the%2520critical%2520volume%2520of%2520structural%2520data%2520required%2520for%2520the%2520LLM%2520to%2520learn%2520the%2520latent%2520relationship%2520between%2520channel%2520configurations%2520and%2520model%2520performance.%2520This%2520allows%2520the%2520LLM%2520to%2520internalize%2520complex%2520design%2520patterns%2520and%2520apply%2520them%2520to%2520optimize%2520feature%2520extraction%2520strategies.%2520Experimental%2520results%2520on%2520CIFAR-100%2520validate%2520the%2520efficacy%2520of%2520this%2520approach%252C%2520demonstrating%2520that%2520the%2520model%2520yields%2520statistically%2520significant%2520improvements%2520in%2520accuracy.%2520Our%2520analysis%2520confirms%2520that%2520the%2520LLM%2520successfully%2520acquires%2520domain-specific%2520architectural%2520priors%252C%2520distinguishing%2520this%2520method%2520from%2520random%2520search%2520and%2520highlighting%2520the%2520immense%2520potential%2520of%2520language-driven%2520design%2520in%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Loop%20LLM%20Discovery%20of%20Non-Standard%20Channel%20Priors%20in%20Vision%20Models&entry.906535625=Tolgay%20Atinc%20Uzun%20and%20Dmitry%20Ignatov%20and%20Radu%20Timofte&entry.1292438233=Channel%20configuration%20search%20the%20optimization%20of%20layer%20specifications%20such%20as%20layer%20widths%20in%20deep%20neural%20networks%20presents%20a%20complex%20combinatorial%20challenge%20constrained%20by%20tensor%20shape%20compatibility%20and%20computational%20budgets.%20We%20posit%20that%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20transformative%20approach%20to%20Neural%20Architecture%20Search%20%28NAS%29%2C%20capable%20of%20reasoning%20about%20architectural%20code%20structure%20in%20ways%20that%20traditional%20heuristics%20cannot.%20In%20this%20paper%2C%20we%20investigate%20the%20application%20of%20an%20LLM-driven%20NAS%20framework%20to%20the%20problem%20of%20channel%20configuration.%20We%20formulate%20the%20search%20as%20a%20sequence%20of%20conditional%20code%20generation%20tasks%2C%20where%20an%20LLM%20refines%20architectural%20specifications%20based%20on%20performance%20telemetry.%20Crucially%2C%20we%20address%20the%20data%20scarcity%20problem%20by%20generating%20a%20vast%20corpus%20of%20valid%2C%20shape-consistent%20architectures%20via%20Abstract%20Syntax%20Tree%20%28AST%29%20mutations.%20While%20these%20mutated%20networks%20are%20not%20necessarily%20high-performing%2C%20they%20provide%20the%20critical%20volume%20of%20structural%20data%20required%20for%20the%20LLM%20to%20learn%20the%20latent%20relationship%20between%20channel%20configurations%20and%20model%20performance.%20This%20allows%20the%20LLM%20to%20internalize%20complex%20design%20patterns%20and%20apply%20them%20to%20optimize%20feature%20extraction%20strategies.%20Experimental%20results%20on%20CIFAR-100%20validate%20the%20efficacy%20of%20this%20approach%2C%20demonstrating%20that%20the%20model%20yields%20statistically%20significant%20improvements%20in%20accuracy.%20Our%20analysis%20confirms%20that%20the%20LLM%20successfully%20acquires%20domain-specific%20architectural%20priors%2C%20distinguishing%20this%20method%20from%20random%20search%20and%20highlighting%20the%20immense%20potential%20of%20language-driven%20design%20in%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.08517v1&entry.124074799=Read"},
{"title": "ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving", "author": "Farhad G. Zanjani and Hong Cai and Amirhossein Habibian", "abstract": "Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making. To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable. While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.\n  We introduce ViewMorpher3D, a multi-view image enhancement framework based on image diffusion models, designed to elevate photorealism and multi-view coherence in driving scenes. Unlike single-view approaches, ViewMorpher3D jointly processes a set of rendered views conditioned on camera poses, 3D geometric priors, and temporally adjacent or spatially overlapping reference views. This enables the model to infer missing details, suppress rendering artifacts, and enforce cross-view consistency.\n  Our framework accommodates variable numbers of cameras and flexible reference/target view configurations, making it adaptable to diverse sensor setups. Experiments on real-world driving datasets demonstrate substantial improvements in image quality metrics, effectively reducing artifacts while preserving geometric fidelity.", "link": "http://arxiv.org/abs/2601.07540v2", "date": "2026-01-13", "relevancy": 2.7343, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6935}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6935}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving&body=Title%3A%20ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving%0AAuthor%3A%20Farhad%20G.%20Zanjani%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian%0AAbstract%3A%20Autonomous%20driving%20systems%20rely%20heavily%20on%20multi-view%20images%20to%20ensure%20accurate%20perception%20and%20robust%20decision-making.%20To%20effectively%20develop%20and%20evaluate%20perception%20stacks%20and%20planning%20algorithms%2C%20realistic%20closed-loop%20simulators%20are%20indispensable.%20While%203D%20reconstruction%20techniques%20such%20as%20Gaussian%20Splatting%20offer%20promising%20avenues%20for%20simulator%20construction%2C%20the%20rendered%20novel%20views%20often%20exhibit%20artifacts%2C%20particularly%20in%20extrapolated%20perspectives%20or%20when%20available%20observations%20are%20sparse.%0A%20%20We%20introduce%20ViewMorpher3D%2C%20a%20multi-view%20image%20enhancement%20framework%20based%20on%20image%20diffusion%20models%2C%20designed%20to%20elevate%20photorealism%20and%20multi-view%20coherence%20in%20driving%20scenes.%20Unlike%20single-view%20approaches%2C%20ViewMorpher3D%20jointly%20processes%20a%20set%20of%20rendered%20views%20conditioned%20on%20camera%20poses%2C%203D%20geometric%20priors%2C%20and%20temporally%20adjacent%20or%20spatially%20overlapping%20reference%20views.%20This%20enables%20the%20model%20to%20infer%20missing%20details%2C%20suppress%20rendering%20artifacts%2C%20and%20enforce%20cross-view%20consistency.%0A%20%20Our%20framework%20accommodates%20variable%20numbers%20of%20cameras%20and%20flexible%20reference/target%20view%20configurations%2C%20making%20it%20adaptable%20to%20diverse%20sensor%20setups.%20Experiments%20on%20real-world%20driving%20datasets%20demonstrate%20substantial%20improvements%20in%20image%20quality%20metrics%2C%20effectively%20reducing%20artifacts%20while%20preserving%20geometric%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewMorpher3D%253A%2520A%25203D-aware%2520Diffusion%2520Framework%2520for%2520Multi-Camera%2520Novel%2520View%2520Synthesis%2520in%2520Autonomous%2520Driving%26entry.906535625%3DFarhad%2520G.%2520Zanjani%2520and%2520Hong%2520Cai%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3DAutonomous%2520driving%2520systems%2520rely%2520heavily%2520on%2520multi-view%2520images%2520to%2520ensure%2520accurate%2520perception%2520and%2520robust%2520decision-making.%2520To%2520effectively%2520develop%2520and%2520evaluate%2520perception%2520stacks%2520and%2520planning%2520algorithms%252C%2520realistic%2520closed-loop%2520simulators%2520are%2520indispensable.%2520While%25203D%2520reconstruction%2520techniques%2520such%2520as%2520Gaussian%2520Splatting%2520offer%2520promising%2520avenues%2520for%2520simulator%2520construction%252C%2520the%2520rendered%2520novel%2520views%2520often%2520exhibit%2520artifacts%252C%2520particularly%2520in%2520extrapolated%2520perspectives%2520or%2520when%2520available%2520observations%2520are%2520sparse.%250A%2520%2520We%2520introduce%2520ViewMorpher3D%252C%2520a%2520multi-view%2520image%2520enhancement%2520framework%2520based%2520on%2520image%2520diffusion%2520models%252C%2520designed%2520to%2520elevate%2520photorealism%2520and%2520multi-view%2520coherence%2520in%2520driving%2520scenes.%2520Unlike%2520single-view%2520approaches%252C%2520ViewMorpher3D%2520jointly%2520processes%2520a%2520set%2520of%2520rendered%2520views%2520conditioned%2520on%2520camera%2520poses%252C%25203D%2520geometric%2520priors%252C%2520and%2520temporally%2520adjacent%2520or%2520spatially%2520overlapping%2520reference%2520views.%2520This%2520enables%2520the%2520model%2520to%2520infer%2520missing%2520details%252C%2520suppress%2520rendering%2520artifacts%252C%2520and%2520enforce%2520cross-view%2520consistency.%250A%2520%2520Our%2520framework%2520accommodates%2520variable%2520numbers%2520of%2520cameras%2520and%2520flexible%2520reference/target%2520view%2520configurations%252C%2520making%2520it%2520adaptable%2520to%2520diverse%2520sensor%2520setups.%2520Experiments%2520on%2520real-world%2520driving%2520datasets%2520demonstrate%2520substantial%2520improvements%2520in%2520image%2520quality%2520metrics%252C%2520effectively%2520reducing%2520artifacts%2520while%2520preserving%2520geometric%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving&entry.906535625=Farhad%20G.%20Zanjani%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian&entry.1292438233=Autonomous%20driving%20systems%20rely%20heavily%20on%20multi-view%20images%20to%20ensure%20accurate%20perception%20and%20robust%20decision-making.%20To%20effectively%20develop%20and%20evaluate%20perception%20stacks%20and%20planning%20algorithms%2C%20realistic%20closed-loop%20simulators%20are%20indispensable.%20While%203D%20reconstruction%20techniques%20such%20as%20Gaussian%20Splatting%20offer%20promising%20avenues%20for%20simulator%20construction%2C%20the%20rendered%20novel%20views%20often%20exhibit%20artifacts%2C%20particularly%20in%20extrapolated%20perspectives%20or%20when%20available%20observations%20are%20sparse.%0A%20%20We%20introduce%20ViewMorpher3D%2C%20a%20multi-view%20image%20enhancement%20framework%20based%20on%20image%20diffusion%20models%2C%20designed%20to%20elevate%20photorealism%20and%20multi-view%20coherence%20in%20driving%20scenes.%20Unlike%20single-view%20approaches%2C%20ViewMorpher3D%20jointly%20processes%20a%20set%20of%20rendered%20views%20conditioned%20on%20camera%20poses%2C%203D%20geometric%20priors%2C%20and%20temporally%20adjacent%20or%20spatially%20overlapping%20reference%20views.%20This%20enables%20the%20model%20to%20infer%20missing%20details%2C%20suppress%20rendering%20artifacts%2C%20and%20enforce%20cross-view%20consistency.%0A%20%20Our%20framework%20accommodates%20variable%20numbers%20of%20cameras%20and%20flexible%20reference/target%20view%20configurations%2C%20making%20it%20adaptable%20to%20diverse%20sensor%20setups.%20Experiments%20on%20real-world%20driving%20datasets%20demonstrate%20substantial%20improvements%20in%20image%20quality%20metrics%2C%20effectively%20reducing%20artifacts%20while%20preserving%20geometric%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2601.07540v2&entry.124074799=Read"},
{"title": "Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation", "author": "Runfeng Qu and Ole Hall and Pia K Bideau and Julie Ouerfelli-Ethier and Martin Rolfs and Klaus Obermayer and Olaf Hellwich", "abstract": "Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision", "link": "http://arxiv.org/abs/2601.08728v1", "date": "2026-01-13", "relevancy": 2.6839, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5422}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Salience-SGG%3A%20Enhancing%20Unbiased%20Scene%20Graph%20Generation%20with%20Iterative%20Salience%20Estimation&body=Title%3A%20Salience-SGG%3A%20Enhancing%20Unbiased%20Scene%20Graph%20Generation%20with%20Iterative%20Salience%20Estimation%0AAuthor%3A%20Runfeng%20Qu%20and%20Ole%20Hall%20and%20Pia%20K%20Bideau%20and%20Julie%20Ouerfelli-Ethier%20and%20Martin%20Rolfs%20and%20Klaus%20Obermayer%20and%20Olaf%20Hellwich%0AAbstract%3A%20Scene%20Graph%20Generation%20%28SGG%29%20suffers%20from%20a%20long-tailed%20distribution%2C%20where%20a%20few%20predicate%20classes%20dominate%20while%20many%20others%20are%20underrepresented%2C%20leading%20to%20biased%20models%20that%20underperform%20on%20rare%20relations.%20Unbiased-SGG%20methods%20address%20this%20issue%20by%20implementing%20debiasing%20strategies%2C%20but%20often%20at%20the%20cost%20of%20spatial%20understanding%2C%20resulting%20in%20an%20over-reliance%20on%20semantic%20priors.%20We%20introduce%20Salience-SGG%2C%20a%20novel%20framework%20featuring%20an%20Iterative%20Salience%20Decoder%20%28ISD%29%20that%20emphasizes%20triplets%20with%20salient%20spatial%20structures.%20To%20support%20this%2C%20we%20propose%20semantic-agnostic%20salience%20labels%20guiding%20ISD.%20Evaluations%20on%20Visual%20Genome%2C%20Open%20Images%20V6%2C%20and%20GQA-200%20show%20that%20Salience-SGG%20achieves%20state-of-the-art%20performance%20and%20improves%20existing%20Unbiased-SGG%20methods%20in%20their%20spatial%20understanding%20as%20demonstrated%20by%20the%20Pairwise%20Localization%20Average%20Precision%0ALink%3A%20http%3A//arxiv.org/abs/2601.08728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSalience-SGG%253A%2520Enhancing%2520Unbiased%2520Scene%2520Graph%2520Generation%2520with%2520Iterative%2520Salience%2520Estimation%26entry.906535625%3DRunfeng%2520Qu%2520and%2520Ole%2520Hall%2520and%2520Pia%2520K%2520Bideau%2520and%2520Julie%2520Ouerfelli-Ethier%2520and%2520Martin%2520Rolfs%2520and%2520Klaus%2520Obermayer%2520and%2520Olaf%2520Hellwich%26entry.1292438233%3DScene%2520Graph%2520Generation%2520%2528SGG%2529%2520suffers%2520from%2520a%2520long-tailed%2520distribution%252C%2520where%2520a%2520few%2520predicate%2520classes%2520dominate%2520while%2520many%2520others%2520are%2520underrepresented%252C%2520leading%2520to%2520biased%2520models%2520that%2520underperform%2520on%2520rare%2520relations.%2520Unbiased-SGG%2520methods%2520address%2520this%2520issue%2520by%2520implementing%2520debiasing%2520strategies%252C%2520but%2520often%2520at%2520the%2520cost%2520of%2520spatial%2520understanding%252C%2520resulting%2520in%2520an%2520over-reliance%2520on%2520semantic%2520priors.%2520We%2520introduce%2520Salience-SGG%252C%2520a%2520novel%2520framework%2520featuring%2520an%2520Iterative%2520Salience%2520Decoder%2520%2528ISD%2529%2520that%2520emphasizes%2520triplets%2520with%2520salient%2520spatial%2520structures.%2520To%2520support%2520this%252C%2520we%2520propose%2520semantic-agnostic%2520salience%2520labels%2520guiding%2520ISD.%2520Evaluations%2520on%2520Visual%2520Genome%252C%2520Open%2520Images%2520V6%252C%2520and%2520GQA-200%2520show%2520that%2520Salience-SGG%2520achieves%2520state-of-the-art%2520performance%2520and%2520improves%2520existing%2520Unbiased-SGG%2520methods%2520in%2520their%2520spatial%2520understanding%2520as%2520demonstrated%2520by%2520the%2520Pairwise%2520Localization%2520Average%2520Precision%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Salience-SGG%3A%20Enhancing%20Unbiased%20Scene%20Graph%20Generation%20with%20Iterative%20Salience%20Estimation&entry.906535625=Runfeng%20Qu%20and%20Ole%20Hall%20and%20Pia%20K%20Bideau%20and%20Julie%20Ouerfelli-Ethier%20and%20Martin%20Rolfs%20and%20Klaus%20Obermayer%20and%20Olaf%20Hellwich&entry.1292438233=Scene%20Graph%20Generation%20%28SGG%29%20suffers%20from%20a%20long-tailed%20distribution%2C%20where%20a%20few%20predicate%20classes%20dominate%20while%20many%20others%20are%20underrepresented%2C%20leading%20to%20biased%20models%20that%20underperform%20on%20rare%20relations.%20Unbiased-SGG%20methods%20address%20this%20issue%20by%20implementing%20debiasing%20strategies%2C%20but%20often%20at%20the%20cost%20of%20spatial%20understanding%2C%20resulting%20in%20an%20over-reliance%20on%20semantic%20priors.%20We%20introduce%20Salience-SGG%2C%20a%20novel%20framework%20featuring%20an%20Iterative%20Salience%20Decoder%20%28ISD%29%20that%20emphasizes%20triplets%20with%20salient%20spatial%20structures.%20To%20support%20this%2C%20we%20propose%20semantic-agnostic%20salience%20labels%20guiding%20ISD.%20Evaluations%20on%20Visual%20Genome%2C%20Open%20Images%20V6%2C%20and%20GQA-200%20show%20that%20Salience-SGG%20achieves%20state-of-the-art%20performance%20and%20improves%20existing%20Unbiased-SGG%20methods%20in%20their%20spatial%20understanding%20as%20demonstrated%20by%20the%20Pairwise%20Localization%20Average%20Precision&entry.1838667208=http%3A//arxiv.org/abs/2601.08728v1&entry.124074799=Read"},
{"title": "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models", "author": "Renyang Liu and Kangjie Chen and Han Qiu and Jie Zhang and Kwok-Yan Lam and Tianwei Zhang and See-Kiong Ng", "abstract": "Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.", "link": "http://arxiv.org/abs/2601.08623v1", "date": "2026-01-13", "relevancy": 2.6824, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5367}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeRedir%3A%20Prompt%20Embedding%20Redirection%20for%20Robust%20Unlearning%20in%20Image%20Generation%20Models&body=Title%3A%20SafeRedir%3A%20Prompt%20Embedding%20Redirection%20for%20Robust%20Unlearning%20in%20Image%20Generation%20Models%0AAuthor%3A%20Renyang%20Liu%20and%20Kangjie%20Chen%20and%20Han%20Qiu%20and%20Jie%20Zhang%20and%20Kwok-Yan%20Lam%20and%20Tianwei%20Zhang%20and%20See-Kiong%20Ng%0AAbstract%3A%20Image%20generation%20models%20%28IGMs%29%2C%20while%20capable%20of%20producing%20impressive%20and%20creative%20content%2C%20often%20memorize%20a%20wide%20range%20of%20undesirable%20concepts%20from%20their%20training%20data%2C%20leading%20to%20the%20reproduction%20of%20unsafe%20content%20such%20as%20NSFW%20imagery%20and%20copyrighted%20artistic%20styles.%20Such%20behaviors%20pose%20persistent%20safety%20and%20compliance%20risks%20in%20real-world%20deployments%20and%20cannot%20be%20reliably%20mitigated%20by%20post-hoc%20filtering%2C%20owing%20to%20the%20limited%20robustness%20of%20such%20mechanisms%20and%20a%20lack%20of%20fine-grained%20semantic%20control.%20Recent%20unlearning%20methods%20seek%20to%20erase%20harmful%20concepts%20at%20the%20model%20level%2C%20which%20exhibit%20the%20limitations%20of%20requiring%20costly%20retraining%2C%20degrading%20the%20quality%20of%20benign%20generations%2C%20or%20failing%20to%20withstand%20prompt%20paraphrasing%20and%20adversarial%20attacks.%20To%20address%20these%20challenges%2C%20we%20introduce%20SafeRedir%2C%20a%20lightweight%20inference-time%20framework%20for%20robust%20unlearning%20via%20prompt%20embedding%20redirection.%20Without%20modifying%20the%20underlying%20IGMs%2C%20SafeRedir%20adaptively%20routes%20unsafe%20prompts%20toward%20safe%20semantic%20regions%20through%20token-level%20interventions%20in%20the%20embedding%20space.%20The%20framework%20comprises%20two%20core%20components%3A%20a%20latent-aware%20multi-modal%20safety%20classifier%20for%20identifying%20unsafe%20generation%20trajectories%2C%20and%20a%20token-level%20delta%20generator%20for%20precise%20semantic%20redirection%2C%20equipped%20with%20auxiliary%20predictors%20for%20token%20masking%20and%20adaptive%20scaling%20to%20localize%20and%20regulate%20the%20intervention.%20Empirical%20results%20across%20multiple%20representative%20unlearning%20tasks%20demonstrate%20that%20SafeRedir%20achieves%20effective%20unlearning%20capability%2C%20high%20semantic%20and%20perceptual%20preservation%2C%20robust%20image%20quality%2C%20and%20enhanced%20resistance%20to%20adversarial%20attacks.%20Furthermore%2C%20SafeRedir%20generalizes%20effectively%20across%20a%20variety%20of%20diffusion%20backbones%20and%20existing%20unlearned%20models%2C%20validating%20its%20plug-and-play%20compatibility%20and%20broad%20applicability.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/ryliu68/SafeRedir.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeRedir%253A%2520Prompt%2520Embedding%2520Redirection%2520for%2520Robust%2520Unlearning%2520in%2520Image%2520Generation%2520Models%26entry.906535625%3DRenyang%2520Liu%2520and%2520Kangjie%2520Chen%2520and%2520Han%2520Qiu%2520and%2520Jie%2520Zhang%2520and%2520Kwok-Yan%2520Lam%2520and%2520Tianwei%2520Zhang%2520and%2520See-Kiong%2520Ng%26entry.1292438233%3DImage%2520generation%2520models%2520%2528IGMs%2529%252C%2520while%2520capable%2520of%2520producing%2520impressive%2520and%2520creative%2520content%252C%2520often%2520memorize%2520a%2520wide%2520range%2520of%2520undesirable%2520concepts%2520from%2520their%2520training%2520data%252C%2520leading%2520to%2520the%2520reproduction%2520of%2520unsafe%2520content%2520such%2520as%2520NSFW%2520imagery%2520and%2520copyrighted%2520artistic%2520styles.%2520Such%2520behaviors%2520pose%2520persistent%2520safety%2520and%2520compliance%2520risks%2520in%2520real-world%2520deployments%2520and%2520cannot%2520be%2520reliably%2520mitigated%2520by%2520post-hoc%2520filtering%252C%2520owing%2520to%2520the%2520limited%2520robustness%2520of%2520such%2520mechanisms%2520and%2520a%2520lack%2520of%2520fine-grained%2520semantic%2520control.%2520Recent%2520unlearning%2520methods%2520seek%2520to%2520erase%2520harmful%2520concepts%2520at%2520the%2520model%2520level%252C%2520which%2520exhibit%2520the%2520limitations%2520of%2520requiring%2520costly%2520retraining%252C%2520degrading%2520the%2520quality%2520of%2520benign%2520generations%252C%2520or%2520failing%2520to%2520withstand%2520prompt%2520paraphrasing%2520and%2520adversarial%2520attacks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520SafeRedir%252C%2520a%2520lightweight%2520inference-time%2520framework%2520for%2520robust%2520unlearning%2520via%2520prompt%2520embedding%2520redirection.%2520Without%2520modifying%2520the%2520underlying%2520IGMs%252C%2520SafeRedir%2520adaptively%2520routes%2520unsafe%2520prompts%2520toward%2520safe%2520semantic%2520regions%2520through%2520token-level%2520interventions%2520in%2520the%2520embedding%2520space.%2520The%2520framework%2520comprises%2520two%2520core%2520components%253A%2520a%2520latent-aware%2520multi-modal%2520safety%2520classifier%2520for%2520identifying%2520unsafe%2520generation%2520trajectories%252C%2520and%2520a%2520token-level%2520delta%2520generator%2520for%2520precise%2520semantic%2520redirection%252C%2520equipped%2520with%2520auxiliary%2520predictors%2520for%2520token%2520masking%2520and%2520adaptive%2520scaling%2520to%2520localize%2520and%2520regulate%2520the%2520intervention.%2520Empirical%2520results%2520across%2520multiple%2520representative%2520unlearning%2520tasks%2520demonstrate%2520that%2520SafeRedir%2520achieves%2520effective%2520unlearning%2520capability%252C%2520high%2520semantic%2520and%2520perceptual%2520preservation%252C%2520robust%2520image%2520quality%252C%2520and%2520enhanced%2520resistance%2520to%2520adversarial%2520attacks.%2520Furthermore%252C%2520SafeRedir%2520generalizes%2520effectively%2520across%2520a%2520variety%2520of%2520diffusion%2520backbones%2520and%2520existing%2520unlearned%2520models%252C%2520validating%2520its%2520plug-and-play%2520compatibility%2520and%2520broad%2520applicability.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/ryliu68/SafeRedir.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeRedir%3A%20Prompt%20Embedding%20Redirection%20for%20Robust%20Unlearning%20in%20Image%20Generation%20Models&entry.906535625=Renyang%20Liu%20and%20Kangjie%20Chen%20and%20Han%20Qiu%20and%20Jie%20Zhang%20and%20Kwok-Yan%20Lam%20and%20Tianwei%20Zhang%20and%20See-Kiong%20Ng&entry.1292438233=Image%20generation%20models%20%28IGMs%29%2C%20while%20capable%20of%20producing%20impressive%20and%20creative%20content%2C%20often%20memorize%20a%20wide%20range%20of%20undesirable%20concepts%20from%20their%20training%20data%2C%20leading%20to%20the%20reproduction%20of%20unsafe%20content%20such%20as%20NSFW%20imagery%20and%20copyrighted%20artistic%20styles.%20Such%20behaviors%20pose%20persistent%20safety%20and%20compliance%20risks%20in%20real-world%20deployments%20and%20cannot%20be%20reliably%20mitigated%20by%20post-hoc%20filtering%2C%20owing%20to%20the%20limited%20robustness%20of%20such%20mechanisms%20and%20a%20lack%20of%20fine-grained%20semantic%20control.%20Recent%20unlearning%20methods%20seek%20to%20erase%20harmful%20concepts%20at%20the%20model%20level%2C%20which%20exhibit%20the%20limitations%20of%20requiring%20costly%20retraining%2C%20degrading%20the%20quality%20of%20benign%20generations%2C%20or%20failing%20to%20withstand%20prompt%20paraphrasing%20and%20adversarial%20attacks.%20To%20address%20these%20challenges%2C%20we%20introduce%20SafeRedir%2C%20a%20lightweight%20inference-time%20framework%20for%20robust%20unlearning%20via%20prompt%20embedding%20redirection.%20Without%20modifying%20the%20underlying%20IGMs%2C%20SafeRedir%20adaptively%20routes%20unsafe%20prompts%20toward%20safe%20semantic%20regions%20through%20token-level%20interventions%20in%20the%20embedding%20space.%20The%20framework%20comprises%20two%20core%20components%3A%20a%20latent-aware%20multi-modal%20safety%20classifier%20for%20identifying%20unsafe%20generation%20trajectories%2C%20and%20a%20token-level%20delta%20generator%20for%20precise%20semantic%20redirection%2C%20equipped%20with%20auxiliary%20predictors%20for%20token%20masking%20and%20adaptive%20scaling%20to%20localize%20and%20regulate%20the%20intervention.%20Empirical%20results%20across%20multiple%20representative%20unlearning%20tasks%20demonstrate%20that%20SafeRedir%20achieves%20effective%20unlearning%20capability%2C%20high%20semantic%20and%20perceptual%20preservation%2C%20robust%20image%20quality%2C%20and%20enhanced%20resistance%20to%20adversarial%20attacks.%20Furthermore%2C%20SafeRedir%20generalizes%20effectively%20across%20a%20variety%20of%20diffusion%20backbones%20and%20existing%20unlearned%20models%2C%20validating%20its%20plug-and-play%20compatibility%20and%20broad%20applicability.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/ryliu68/SafeRedir.&entry.1838667208=http%3A//arxiv.org/abs/2601.08623v1&entry.124074799=Read"},
{"title": "Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling", "author": "Takamichi Miyata and Sumiko Miyata and Andrew Morris", "abstract": "Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.", "link": "http://arxiv.org/abs/2601.08467v1", "date": "2026-01-13", "relevancy": 2.6799, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Distracted%20Driver%20Detection%20via%20Vision%20Language%20Models%20with%20Double%20Decoupling&body=Title%3A%20Zero-Shot%20Distracted%20Driver%20Detection%20via%20Vision%20Language%20Models%20with%20Double%20Decoupling%0AAuthor%3A%20Takamichi%20Miyata%20and%20Sumiko%20Miyata%20and%20Andrew%20Morris%0AAbstract%3A%20Distracted%20driving%20is%20a%20major%20cause%20of%20traffic%20collisions%2C%20calling%20for%20robust%20and%20scalable%20detection%20methods.%20Vision-language%20models%20%28VLMs%29%20enable%20strong%20zero-shot%20image%20classification%2C%20but%20existing%20VLM-based%20distracted%20driver%20detectors%20often%20underperform%20in%20real-world%20conditions.%20We%20identify%20subject-specific%20appearance%20variations%20%28e.g.%2C%20clothing%2C%20age%2C%20and%20gender%29%20as%20a%20key%20bottleneck%3A%20VLMs%20entangle%20these%20factors%20with%20behavior%20cues%2C%20leading%20to%20decisions%20driven%20by%20who%20the%20driver%20is%20rather%20than%20what%20the%20driver%20is%20doing.%20To%20address%20this%2C%20we%20propose%20a%20subject%20decoupling%20framework%20that%20extracts%20a%20driver%20appearance%20embedding%20and%20removes%20its%20influence%20from%20the%20image%20embedding%20prior%20to%20zero-shot%20classification%2C%20thereby%20emphasizing%20distraction-relevant%20evidence.%20We%20further%20orthogonalize%20text%20embeddings%20via%20metric%20projection%20onto%20Stiefel%20manifold%20to%20improve%20separability%20while%20staying%20close%20to%20the%20original%20semantics.%20Experiments%20demonstrate%20consistent%20gains%20over%20prior%20baselines%2C%20indicating%20the%20promise%20of%20our%20approach%20for%20practical%20road-safety%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Distracted%2520Driver%2520Detection%2520via%2520Vision%2520Language%2520Models%2520with%2520Double%2520Decoupling%26entry.906535625%3DTakamichi%2520Miyata%2520and%2520Sumiko%2520Miyata%2520and%2520Andrew%2520Morris%26entry.1292438233%3DDistracted%2520driving%2520is%2520a%2520major%2520cause%2520of%2520traffic%2520collisions%252C%2520calling%2520for%2520robust%2520and%2520scalable%2520detection%2520methods.%2520Vision-language%2520models%2520%2528VLMs%2529%2520enable%2520strong%2520zero-shot%2520image%2520classification%252C%2520but%2520existing%2520VLM-based%2520distracted%2520driver%2520detectors%2520often%2520underperform%2520in%2520real-world%2520conditions.%2520We%2520identify%2520subject-specific%2520appearance%2520variations%2520%2528e.g.%252C%2520clothing%252C%2520age%252C%2520and%2520gender%2529%2520as%2520a%2520key%2520bottleneck%253A%2520VLMs%2520entangle%2520these%2520factors%2520with%2520behavior%2520cues%252C%2520leading%2520to%2520decisions%2520driven%2520by%2520who%2520the%2520driver%2520is%2520rather%2520than%2520what%2520the%2520driver%2520is%2520doing.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520subject%2520decoupling%2520framework%2520that%2520extracts%2520a%2520driver%2520appearance%2520embedding%2520and%2520removes%2520its%2520influence%2520from%2520the%2520image%2520embedding%2520prior%2520to%2520zero-shot%2520classification%252C%2520thereby%2520emphasizing%2520distraction-relevant%2520evidence.%2520We%2520further%2520orthogonalize%2520text%2520embeddings%2520via%2520metric%2520projection%2520onto%2520Stiefel%2520manifold%2520to%2520improve%2520separability%2520while%2520staying%2520close%2520to%2520the%2520original%2520semantics.%2520Experiments%2520demonstrate%2520consistent%2520gains%2520over%2520prior%2520baselines%252C%2520indicating%2520the%2520promise%2520of%2520our%2520approach%2520for%2520practical%2520road-safety%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Distracted%20Driver%20Detection%20via%20Vision%20Language%20Models%20with%20Double%20Decoupling&entry.906535625=Takamichi%20Miyata%20and%20Sumiko%20Miyata%20and%20Andrew%20Morris&entry.1292438233=Distracted%20driving%20is%20a%20major%20cause%20of%20traffic%20collisions%2C%20calling%20for%20robust%20and%20scalable%20detection%20methods.%20Vision-language%20models%20%28VLMs%29%20enable%20strong%20zero-shot%20image%20classification%2C%20but%20existing%20VLM-based%20distracted%20driver%20detectors%20often%20underperform%20in%20real-world%20conditions.%20We%20identify%20subject-specific%20appearance%20variations%20%28e.g.%2C%20clothing%2C%20age%2C%20and%20gender%29%20as%20a%20key%20bottleneck%3A%20VLMs%20entangle%20these%20factors%20with%20behavior%20cues%2C%20leading%20to%20decisions%20driven%20by%20who%20the%20driver%20is%20rather%20than%20what%20the%20driver%20is%20doing.%20To%20address%20this%2C%20we%20propose%20a%20subject%20decoupling%20framework%20that%20extracts%20a%20driver%20appearance%20embedding%20and%20removes%20its%20influence%20from%20the%20image%20embedding%20prior%20to%20zero-shot%20classification%2C%20thereby%20emphasizing%20distraction-relevant%20evidence.%20We%20further%20orthogonalize%20text%20embeddings%20via%20metric%20projection%20onto%20Stiefel%20manifold%20to%20improve%20separability%20while%20staying%20close%20to%20the%20original%20semantics.%20Experiments%20demonstrate%20consistent%20gains%20over%20prior%20baselines%2C%20indicating%20the%20promise%20of%20our%20approach%20for%20practical%20road-safety%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.08467v1&entry.124074799=Read"},
{"title": "Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation", "author": "Yizhan Feng and Hichem Snoussi and Yuhang Wang and Jing Teng and Abel Cherouat and Tian Wang", "abstract": "With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs", "link": "http://arxiv.org/abs/2601.08412v1", "date": "2026-01-13", "relevancy": 2.6659, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5565}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Distillation%20with%20CoT%20Guidance%20for%20Edge-Drone%20Control%20Code%20Generation&body=Title%3A%20Hybrid%20Distillation%20with%20CoT%20Guidance%20for%20Edge-Drone%20Control%20Code%20Generation%0AAuthor%3A%20Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Yuhang%20Wang%20and%20Jing%20Teng%20and%20Abel%20Cherouat%20and%20Tian%20Wang%0AAbstract%3A%20With%20large%20language%20models%20demonstrating%20significant%20potential%20in%20code%20generation%20tasks%2C%20their%20application%20to%20onboard%20control%20of%20resource-constrained%20Unmanned%20Aerial%20Vehicles%20has%20emerged%20as%20an%20important%20research%20direction.%20However%2C%20a%20notable%20contradiction%20exists%20between%20the%20high%20resource%20consumption%20of%20large%20models%20and%20the%20real-time%2C%20lightweight%20requirements%20of%20UAV%20platforms.%20This%20paper%20proposes%20an%20integrated%20approach%20that%20combines%20knowledge%20distillation%2C%20chain-of-thought%20guidance%2C%20and%20supervised%20fine-tuning%20for%20UAV%20multi-SDK%20control%20tasks%2C%20aiming%20to%20efficiently%20transfer%20complex%20reasoning%20and%20code%20generation%20capabilities%20to%20smaller%20models.%20Firstly%2C%20a%20high-quality%20dataset%20covering%20various%20mainstream%20UAV%20SDKs%20is%20constructed%2C%20featuring%20instruction-code-reasoning%20chains%2C%20and%20incorporates%20counterfactual%20negative%20samples%20for%20data%20augmentation%2C%20guiding%20the%20model%20to%20learn%20the%20end-to-end%20logic%20from%20instruction%20parsing%20to%20code%20generation.%20Secondly%2C%20leveraging%20DeepSeek-Coder-V2-Lite%20quantized%20via%20QLoRA%20as%20the%20teacher%20model%2C%20and%20based%20on%20a%20hybrid%20black-box%20and%20white-box%20distillation%20strategy%2C%20high-quality%20chain-of-thought%20soft%20labels%20are%20generated.%20These%20are%20combined%20with%20a%20weighted%20cross-entropy%20loss%20using%20hard%20labels%20to%20transfer%20complex%20reasoning%20capabilities%20to%20the%20smaller%20student%20model.%20Finally%2C%20through%20prompt%20tuning%20engineering%20optimized%20for%20the%20UAV%20control%20scenario%2C%20the%20model%20performance%20on%20core%20tasks%20such%20as%20SDK%20type%20recognition%20and%20function%20call%20matching%20is%20enhanced.%20Experimental%20results%20indicate%20that%20the%20distilled%20lightweight%20model%20maintains%20high%20code%20generation%20accuracy%20while%20achieving%20significant%20improvements%20in%20deployment%20and%20inference%20efficiency%2C%20effectively%20demonstrating%20the%20feasibility%20and%20superiority%20of%20our%20approach%20in%20achieving%20precise%20and%20lightweight%20intelligent%20control%20for%20UAVs%0ALink%3A%20http%3A//arxiv.org/abs/2601.08412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Distillation%2520with%2520CoT%2520Guidance%2520for%2520Edge-Drone%2520Control%2520Code%2520Generation%26entry.906535625%3DYizhan%2520Feng%2520and%2520Hichem%2520Snoussi%2520and%2520Yuhang%2520Wang%2520and%2520Jing%2520Teng%2520and%2520Abel%2520Cherouat%2520and%2520Tian%2520Wang%26entry.1292438233%3DWith%2520large%2520language%2520models%2520demonstrating%2520significant%2520potential%2520in%2520code%2520generation%2520tasks%252C%2520their%2520application%2520to%2520onboard%2520control%2520of%2520resource-constrained%2520Unmanned%2520Aerial%2520Vehicles%2520has%2520emerged%2520as%2520an%2520important%2520research%2520direction.%2520However%252C%2520a%2520notable%2520contradiction%2520exists%2520between%2520the%2520high%2520resource%2520consumption%2520of%2520large%2520models%2520and%2520the%2520real-time%252C%2520lightweight%2520requirements%2520of%2520UAV%2520platforms.%2520This%2520paper%2520proposes%2520an%2520integrated%2520approach%2520that%2520combines%2520knowledge%2520distillation%252C%2520chain-of-thought%2520guidance%252C%2520and%2520supervised%2520fine-tuning%2520for%2520UAV%2520multi-SDK%2520control%2520tasks%252C%2520aiming%2520to%2520efficiently%2520transfer%2520complex%2520reasoning%2520and%2520code%2520generation%2520capabilities%2520to%2520smaller%2520models.%2520Firstly%252C%2520a%2520high-quality%2520dataset%2520covering%2520various%2520mainstream%2520UAV%2520SDKs%2520is%2520constructed%252C%2520featuring%2520instruction-code-reasoning%2520chains%252C%2520and%2520incorporates%2520counterfactual%2520negative%2520samples%2520for%2520data%2520augmentation%252C%2520guiding%2520the%2520model%2520to%2520learn%2520the%2520end-to-end%2520logic%2520from%2520instruction%2520parsing%2520to%2520code%2520generation.%2520Secondly%252C%2520leveraging%2520DeepSeek-Coder-V2-Lite%2520quantized%2520via%2520QLoRA%2520as%2520the%2520teacher%2520model%252C%2520and%2520based%2520on%2520a%2520hybrid%2520black-box%2520and%2520white-box%2520distillation%2520strategy%252C%2520high-quality%2520chain-of-thought%2520soft%2520labels%2520are%2520generated.%2520These%2520are%2520combined%2520with%2520a%2520weighted%2520cross-entropy%2520loss%2520using%2520hard%2520labels%2520to%2520transfer%2520complex%2520reasoning%2520capabilities%2520to%2520the%2520smaller%2520student%2520model.%2520Finally%252C%2520through%2520prompt%2520tuning%2520engineering%2520optimized%2520for%2520the%2520UAV%2520control%2520scenario%252C%2520the%2520model%2520performance%2520on%2520core%2520tasks%2520such%2520as%2520SDK%2520type%2520recognition%2520and%2520function%2520call%2520matching%2520is%2520enhanced.%2520Experimental%2520results%2520indicate%2520that%2520the%2520distilled%2520lightweight%2520model%2520maintains%2520high%2520code%2520generation%2520accuracy%2520while%2520achieving%2520significant%2520improvements%2520in%2520deployment%2520and%2520inference%2520efficiency%252C%2520effectively%2520demonstrating%2520the%2520feasibility%2520and%2520superiority%2520of%2520our%2520approach%2520in%2520achieving%2520precise%2520and%2520lightweight%2520intelligent%2520control%2520for%2520UAVs%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Distillation%20with%20CoT%20Guidance%20for%20Edge-Drone%20Control%20Code%20Generation&entry.906535625=Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Yuhang%20Wang%20and%20Jing%20Teng%20and%20Abel%20Cherouat%20and%20Tian%20Wang&entry.1292438233=With%20large%20language%20models%20demonstrating%20significant%20potential%20in%20code%20generation%20tasks%2C%20their%20application%20to%20onboard%20control%20of%20resource-constrained%20Unmanned%20Aerial%20Vehicles%20has%20emerged%20as%20an%20important%20research%20direction.%20However%2C%20a%20notable%20contradiction%20exists%20between%20the%20high%20resource%20consumption%20of%20large%20models%20and%20the%20real-time%2C%20lightweight%20requirements%20of%20UAV%20platforms.%20This%20paper%20proposes%20an%20integrated%20approach%20that%20combines%20knowledge%20distillation%2C%20chain-of-thought%20guidance%2C%20and%20supervised%20fine-tuning%20for%20UAV%20multi-SDK%20control%20tasks%2C%20aiming%20to%20efficiently%20transfer%20complex%20reasoning%20and%20code%20generation%20capabilities%20to%20smaller%20models.%20Firstly%2C%20a%20high-quality%20dataset%20covering%20various%20mainstream%20UAV%20SDKs%20is%20constructed%2C%20featuring%20instruction-code-reasoning%20chains%2C%20and%20incorporates%20counterfactual%20negative%20samples%20for%20data%20augmentation%2C%20guiding%20the%20model%20to%20learn%20the%20end-to-end%20logic%20from%20instruction%20parsing%20to%20code%20generation.%20Secondly%2C%20leveraging%20DeepSeek-Coder-V2-Lite%20quantized%20via%20QLoRA%20as%20the%20teacher%20model%2C%20and%20based%20on%20a%20hybrid%20black-box%20and%20white-box%20distillation%20strategy%2C%20high-quality%20chain-of-thought%20soft%20labels%20are%20generated.%20These%20are%20combined%20with%20a%20weighted%20cross-entropy%20loss%20using%20hard%20labels%20to%20transfer%20complex%20reasoning%20capabilities%20to%20the%20smaller%20student%20model.%20Finally%2C%20through%20prompt%20tuning%20engineering%20optimized%20for%20the%20UAV%20control%20scenario%2C%20the%20model%20performance%20on%20core%20tasks%20such%20as%20SDK%20type%20recognition%20and%20function%20call%20matching%20is%20enhanced.%20Experimental%20results%20indicate%20that%20the%20distilled%20lightweight%20model%20maintains%20high%20code%20generation%20accuracy%20while%20achieving%20significant%20improvements%20in%20deployment%20and%20inference%20efficiency%2C%20effectively%20demonstrating%20the%20feasibility%20and%20superiority%20of%20our%20approach%20in%20achieving%20precise%20and%20lightweight%20intelligent%20control%20for%20UAVs&entry.1838667208=http%3A//arxiv.org/abs/2601.08412v1&entry.124074799=Read"},
{"title": "The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection", "author": "Md Shafiqul Islam and Shakti Prasad Padhy and Douglas Allaire and Raymundo Arr\u00f3yave", "abstract": "Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.", "link": "http://arxiv.org/abs/2601.05371v2", "date": "2026-01-13", "relevancy": 2.6455, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5448}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5251}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Kernel%20Manifold%3A%20A%20Geometric%20Approach%20to%20Gaussian%20Process%20Model%20Selection&body=Title%3A%20The%20Kernel%20Manifold%3A%20A%20Geometric%20Approach%20to%20Gaussian%20Process%20Model%20Selection%0AAuthor%3A%20Md%20Shafiqul%20Islam%20and%20Shakti%20Prasad%20Padhy%20and%20Douglas%20Allaire%20and%20Raymundo%20Arr%C3%B3yave%0AAbstract%3A%20Gaussian%20Process%20%28GP%29%20regression%20is%20a%20powerful%20nonparametric%20Bayesian%20framework%2C%20but%20its%20performance%20depends%20critically%20on%20the%20choice%20of%20covariance%20kernel.%20Selecting%20an%20appropriate%20kernel%20is%20therefore%20central%20to%20model%20quality%2C%20yet%20remains%20one%20of%20the%20most%20challenging%20and%20computationally%20expensive%20steps%20in%20probabilistic%20modeling.%20We%20present%20a%20Bayesian%20optimization%20framework%20built%20on%20kernel-of-kernels%20geometry%2C%20using%20expected%20divergence-based%20distances%20between%20GP%20priors%20to%20explore%20kernel%20space%20efficiently.%20A%20multidimensional%20scaling%20%28MDS%29%20embedding%20of%20this%20distance%20matrix%20maps%20a%20discrete%20kernel%20library%20into%20a%20continuous%20Euclidean%20manifold%2C%20enabling%20smooth%20BO.%20In%20this%20formulation%2C%20the%20input%20space%20comprises%20kernel%20compositions%2C%20the%20objective%20is%20the%20log%20marginal%20likelihood%2C%20and%20featurization%20is%20given%20by%20the%20MDS%20coordinates.%20When%20the%20divergence%20yields%20a%20valid%20metric%2C%20the%20embedding%20preserves%20geometry%20and%20produces%20a%20stable%20BO%20landscape.%20We%20demonstrate%20the%20approach%20on%20synthetic%20benchmarks%2C%20real-world%20time-series%20datasets%2C%20and%20an%20additive%20manufacturing%20case%20study%20predicting%20melt-pool%20geometry%2C%20achieving%20superior%20predictive%20accuracy%20and%20uncertainty%20calibration%20relative%20to%20baselines%20including%20Large%20Language%20Model%20%28LLM%29-guided%20search.%20This%20framework%20establishes%20a%20reusable%20probabilistic%20geometry%20for%20kernel%20search%2C%20with%20direct%20relevance%20to%20GP%20modeling%20and%20deep%20kernel%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Kernel%2520Manifold%253A%2520A%2520Geometric%2520Approach%2520to%2520Gaussian%2520Process%2520Model%2520Selection%26entry.906535625%3DMd%2520Shafiqul%2520Islam%2520and%2520Shakti%2520Prasad%2520Padhy%2520and%2520Douglas%2520Allaire%2520and%2520Raymundo%2520Arr%25C3%25B3yave%26entry.1292438233%3DGaussian%2520Process%2520%2528GP%2529%2520regression%2520is%2520a%2520powerful%2520nonparametric%2520Bayesian%2520framework%252C%2520but%2520its%2520performance%2520depends%2520critically%2520on%2520the%2520choice%2520of%2520covariance%2520kernel.%2520Selecting%2520an%2520appropriate%2520kernel%2520is%2520therefore%2520central%2520to%2520model%2520quality%252C%2520yet%2520remains%2520one%2520of%2520the%2520most%2520challenging%2520and%2520computationally%2520expensive%2520steps%2520in%2520probabilistic%2520modeling.%2520We%2520present%2520a%2520Bayesian%2520optimization%2520framework%2520built%2520on%2520kernel-of-kernels%2520geometry%252C%2520using%2520expected%2520divergence-based%2520distances%2520between%2520GP%2520priors%2520to%2520explore%2520kernel%2520space%2520efficiently.%2520A%2520multidimensional%2520scaling%2520%2528MDS%2529%2520embedding%2520of%2520this%2520distance%2520matrix%2520maps%2520a%2520discrete%2520kernel%2520library%2520into%2520a%2520continuous%2520Euclidean%2520manifold%252C%2520enabling%2520smooth%2520BO.%2520In%2520this%2520formulation%252C%2520the%2520input%2520space%2520comprises%2520kernel%2520compositions%252C%2520the%2520objective%2520is%2520the%2520log%2520marginal%2520likelihood%252C%2520and%2520featurization%2520is%2520given%2520by%2520the%2520MDS%2520coordinates.%2520When%2520the%2520divergence%2520yields%2520a%2520valid%2520metric%252C%2520the%2520embedding%2520preserves%2520geometry%2520and%2520produces%2520a%2520stable%2520BO%2520landscape.%2520We%2520demonstrate%2520the%2520approach%2520on%2520synthetic%2520benchmarks%252C%2520real-world%2520time-series%2520datasets%252C%2520and%2520an%2520additive%2520manufacturing%2520case%2520study%2520predicting%2520melt-pool%2520geometry%252C%2520achieving%2520superior%2520predictive%2520accuracy%2520and%2520uncertainty%2520calibration%2520relative%2520to%2520baselines%2520including%2520Large%2520Language%2520Model%2520%2528LLM%2529-guided%2520search.%2520This%2520framework%2520establishes%2520a%2520reusable%2520probabilistic%2520geometry%2520for%2520kernel%2520search%252C%2520with%2520direct%2520relevance%2520to%2520GP%2520modeling%2520and%2520deep%2520kernel%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Kernel%20Manifold%3A%20A%20Geometric%20Approach%20to%20Gaussian%20Process%20Model%20Selection&entry.906535625=Md%20Shafiqul%20Islam%20and%20Shakti%20Prasad%20Padhy%20and%20Douglas%20Allaire%20and%20Raymundo%20Arr%C3%B3yave&entry.1292438233=Gaussian%20Process%20%28GP%29%20regression%20is%20a%20powerful%20nonparametric%20Bayesian%20framework%2C%20but%20its%20performance%20depends%20critically%20on%20the%20choice%20of%20covariance%20kernel.%20Selecting%20an%20appropriate%20kernel%20is%20therefore%20central%20to%20model%20quality%2C%20yet%20remains%20one%20of%20the%20most%20challenging%20and%20computationally%20expensive%20steps%20in%20probabilistic%20modeling.%20We%20present%20a%20Bayesian%20optimization%20framework%20built%20on%20kernel-of-kernels%20geometry%2C%20using%20expected%20divergence-based%20distances%20between%20GP%20priors%20to%20explore%20kernel%20space%20efficiently.%20A%20multidimensional%20scaling%20%28MDS%29%20embedding%20of%20this%20distance%20matrix%20maps%20a%20discrete%20kernel%20library%20into%20a%20continuous%20Euclidean%20manifold%2C%20enabling%20smooth%20BO.%20In%20this%20formulation%2C%20the%20input%20space%20comprises%20kernel%20compositions%2C%20the%20objective%20is%20the%20log%20marginal%20likelihood%2C%20and%20featurization%20is%20given%20by%20the%20MDS%20coordinates.%20When%20the%20divergence%20yields%20a%20valid%20metric%2C%20the%20embedding%20preserves%20geometry%20and%20produces%20a%20stable%20BO%20landscape.%20We%20demonstrate%20the%20approach%20on%20synthetic%20benchmarks%2C%20real-world%20time-series%20datasets%2C%20and%20an%20additive%20manufacturing%20case%20study%20predicting%20melt-pool%20geometry%2C%20achieving%20superior%20predictive%20accuracy%20and%20uncertainty%20calibration%20relative%20to%20baselines%20including%20Large%20Language%20Model%20%28LLM%29-guided%20search.%20This%20framework%20establishes%20a%20reusable%20probabilistic%20geometry%20for%20kernel%20search%2C%20with%20direct%20relevance%20to%20GP%20modeling%20and%20deep%20kernel%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.05371v2&entry.124074799=Read"},
{"title": "Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures", "author": "Sucheta Ghosh and Zahra Monfared and Felix Dietrich", "abstract": "We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.", "link": "http://arxiv.org/abs/2601.08549v1", "date": "2026-01-13", "relevancy": 2.6229, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures&body=Title%3A%20Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures%0AAuthor%3A%20Sucheta%20Ghosh%20and%20Zahra%20Monfared%20and%20Felix%20Dietrich%0AAbstract%3A%20We%20introduce%20a%20two-stage%20multitask%20learning%20framework%20for%20analyzing%20Electroencephalography%20%28EEG%29%20signals%20that%20integrates%20denoising%2C%20dynamical%20modeling%2C%20and%20representation%20learning.%20In%20the%20first%20stage%2C%20a%20denoising%20autoencoder%20is%20trained%20to%20suppress%20artifacts%20and%20stabilize%20temporal%20dynamics%2C%20providing%20robust%20signal%20representations.%20In%20the%20second%20stage%2C%20a%20multitask%20architecture%20processes%20these%20denoised%20signals%20to%20achieve%20three%20objectives%3A%20motor%20imagery%20classification%2C%20chaotic%20versus%20non-chaotic%20regime%20discrimination%20using%20Lyapunov%20exponent-based%20labels%2C%20and%20self-supervised%20contrastive%20representation%20learning%20with%20NT-Xent%20loss.%20A%20convolutional%20backbone%20combined%20with%20a%20Transformer%20encoder%20captures%20spatial-temporal%20structure%2C%20while%20the%20dynamical%20task%20encourages%20sensitivity%20to%20nonlinear%20brain%20dynamics.%20This%20staged%20design%20mitigates%20interference%20between%20reconstruction%20and%20discriminative%20goals%2C%20improves%20stability%20across%20datasets%2C%20and%20supports%20reproducible%20training%20by%20clearly%20separating%20noise%20reduction%20from%20higher-level%20feature%20learning.%20Empirical%20studies%20show%20that%20our%20framework%20not%20only%20enhances%20robustness%20and%20generalization%20but%20also%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20methods%20in%20EEG%20decoding%2C%20highlighting%20the%20effectiveness%20of%20combining%20denoising%2C%20dynamical%20features%2C%20and%20self-supervised%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520and%2520Multi-Task%2520Learning%2520on%2520Noisy%2520Brain%2520Signals%2520with%2520Nonlinear%2520Dynamical%2520Signatures%26entry.906535625%3DSucheta%2520Ghosh%2520and%2520Zahra%2520Monfared%2520and%2520Felix%2520Dietrich%26entry.1292438233%3DWe%2520introduce%2520a%2520two-stage%2520multitask%2520learning%2520framework%2520for%2520analyzing%2520Electroencephalography%2520%2528EEG%2529%2520signals%2520that%2520integrates%2520denoising%252C%2520dynamical%2520modeling%252C%2520and%2520representation%2520learning.%2520In%2520the%2520first%2520stage%252C%2520a%2520denoising%2520autoencoder%2520is%2520trained%2520to%2520suppress%2520artifacts%2520and%2520stabilize%2520temporal%2520dynamics%252C%2520providing%2520robust%2520signal%2520representations.%2520In%2520the%2520second%2520stage%252C%2520a%2520multitask%2520architecture%2520processes%2520these%2520denoised%2520signals%2520to%2520achieve%2520three%2520objectives%253A%2520motor%2520imagery%2520classification%252C%2520chaotic%2520versus%2520non-chaotic%2520regime%2520discrimination%2520using%2520Lyapunov%2520exponent-based%2520labels%252C%2520and%2520self-supervised%2520contrastive%2520representation%2520learning%2520with%2520NT-Xent%2520loss.%2520A%2520convolutional%2520backbone%2520combined%2520with%2520a%2520Transformer%2520encoder%2520captures%2520spatial-temporal%2520structure%252C%2520while%2520the%2520dynamical%2520task%2520encourages%2520sensitivity%2520to%2520nonlinear%2520brain%2520dynamics.%2520This%2520staged%2520design%2520mitigates%2520interference%2520between%2520reconstruction%2520and%2520discriminative%2520goals%252C%2520improves%2520stability%2520across%2520datasets%252C%2520and%2520supports%2520reproducible%2520training%2520by%2520clearly%2520separating%2520noise%2520reduction%2520from%2520higher-level%2520feature%2520learning.%2520Empirical%2520studies%2520show%2520that%2520our%2520framework%2520not%2520only%2520enhances%2520robustness%2520and%2520generalization%2520but%2520also%2520surpasses%2520strong%2520baselines%2520and%2520recent%2520state-of-the-art%2520methods%2520in%2520EEG%2520decoding%252C%2520highlighting%2520the%2520effectiveness%2520of%2520combining%2520denoising%252C%2520dynamical%2520features%252C%2520and%2520self-supervised%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures&entry.906535625=Sucheta%20Ghosh%20and%20Zahra%20Monfared%20and%20Felix%20Dietrich&entry.1292438233=We%20introduce%20a%20two-stage%20multitask%20learning%20framework%20for%20analyzing%20Electroencephalography%20%28EEG%29%20signals%20that%20integrates%20denoising%2C%20dynamical%20modeling%2C%20and%20representation%20learning.%20In%20the%20first%20stage%2C%20a%20denoising%20autoencoder%20is%20trained%20to%20suppress%20artifacts%20and%20stabilize%20temporal%20dynamics%2C%20providing%20robust%20signal%20representations.%20In%20the%20second%20stage%2C%20a%20multitask%20architecture%20processes%20these%20denoised%20signals%20to%20achieve%20three%20objectives%3A%20motor%20imagery%20classification%2C%20chaotic%20versus%20non-chaotic%20regime%20discrimination%20using%20Lyapunov%20exponent-based%20labels%2C%20and%20self-supervised%20contrastive%20representation%20learning%20with%20NT-Xent%20loss.%20A%20convolutional%20backbone%20combined%20with%20a%20Transformer%20encoder%20captures%20spatial-temporal%20structure%2C%20while%20the%20dynamical%20task%20encourages%20sensitivity%20to%20nonlinear%20brain%20dynamics.%20This%20staged%20design%20mitigates%20interference%20between%20reconstruction%20and%20discriminative%20goals%2C%20improves%20stability%20across%20datasets%2C%20and%20supports%20reproducible%20training%20by%20clearly%20separating%20noise%20reduction%20from%20higher-level%20feature%20learning.%20Empirical%20studies%20show%20that%20our%20framework%20not%20only%20enhances%20robustness%20and%20generalization%20but%20also%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20methods%20in%20EEG%20decoding%2C%20highlighting%20the%20effectiveness%20of%20combining%20denoising%2C%20dynamical%20features%2C%20and%20self-supervised%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.08549v1&entry.124074799=Read"},
{"title": "DentalX: Context-Aware Dental Disease Detection with Radiographs", "author": "Zhi Qin Tan and Xiatian Zhu and Owen Addison and Yunpeng Li", "abstract": "Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.", "link": "http://arxiv.org/abs/2601.08797v1", "date": "2026-01-13", "relevancy": 2.6227, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DentalX%3A%20Context-Aware%20Dental%20Disease%20Detection%20with%20Radiographs&body=Title%3A%20DentalX%3A%20Context-Aware%20Dental%20Disease%20Detection%20with%20Radiographs%0AAuthor%3A%20Zhi%20Qin%20Tan%20and%20Xiatian%20Zhu%20and%20Owen%20Addison%20and%20Yunpeng%20Li%0AAbstract%3A%20Diagnosing%20dental%20diseases%20from%20radiographs%20is%20time-consuming%20and%20challenging%20due%20to%20the%20subtle%20nature%20of%20diagnostic%20evidence.%20Existing%20methods%2C%20which%20rely%20on%20object%20detection%20models%20designed%20for%20natural%20images%20with%20more%20distinct%20target%20patterns%2C%20struggle%20to%20detect%20dental%20diseases%20that%20present%20with%20far%20less%20visual%20support.%20To%20address%20this%20challenge%2C%20we%20propose%20%7B%5Cbf%20DentalX%7D%2C%20a%20novel%20context-aware%20dental%20disease%20detection%20approach%20that%20leverages%20oral%20structure%20information%20to%20mitigate%20the%20visual%20ambiguity%20inherent%20in%20radiographs.%20Specifically%2C%20we%20introduce%20a%20structural%20context%20extraction%20module%20that%20learns%20an%20auxiliary%20task%3A%20semantic%20segmentation%20of%20dental%20anatomy.%20The%20module%20extracts%20meaningful%20structural%20context%20and%20integrates%20it%20into%20the%20primary%20disease%20detection%20task%20to%20enhance%20the%20detection%20of%20subtle%20dental%20diseases.%20Extensive%20experiments%20on%20a%20dedicated%20benchmark%20demonstrate%20that%20DentalX%20significantly%20outperforms%20prior%20methods%20in%20both%20tasks.%20This%20mutual%20benefit%20arises%20naturally%20during%20model%20optimization%2C%20as%20the%20correlation%20between%20the%20two%20tasks%20is%20effectively%20captured.%20Our%20code%20is%20available%20at%20https%3A//github.com/zhiqin1998/DentYOLOX.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDentalX%253A%2520Context-Aware%2520Dental%2520Disease%2520Detection%2520with%2520Radiographs%26entry.906535625%3DZhi%2520Qin%2520Tan%2520and%2520Xiatian%2520Zhu%2520and%2520Owen%2520Addison%2520and%2520Yunpeng%2520Li%26entry.1292438233%3DDiagnosing%2520dental%2520diseases%2520from%2520radiographs%2520is%2520time-consuming%2520and%2520challenging%2520due%2520to%2520the%2520subtle%2520nature%2520of%2520diagnostic%2520evidence.%2520Existing%2520methods%252C%2520which%2520rely%2520on%2520object%2520detection%2520models%2520designed%2520for%2520natural%2520images%2520with%2520more%2520distinct%2520target%2520patterns%252C%2520struggle%2520to%2520detect%2520dental%2520diseases%2520that%2520present%2520with%2520far%2520less%2520visual%2520support.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%257B%255Cbf%2520DentalX%257D%252C%2520a%2520novel%2520context-aware%2520dental%2520disease%2520detection%2520approach%2520that%2520leverages%2520oral%2520structure%2520information%2520to%2520mitigate%2520the%2520visual%2520ambiguity%2520inherent%2520in%2520radiographs.%2520Specifically%252C%2520we%2520introduce%2520a%2520structural%2520context%2520extraction%2520module%2520that%2520learns%2520an%2520auxiliary%2520task%253A%2520semantic%2520segmentation%2520of%2520dental%2520anatomy.%2520The%2520module%2520extracts%2520meaningful%2520structural%2520context%2520and%2520integrates%2520it%2520into%2520the%2520primary%2520disease%2520detection%2520task%2520to%2520enhance%2520the%2520detection%2520of%2520subtle%2520dental%2520diseases.%2520Extensive%2520experiments%2520on%2520a%2520dedicated%2520benchmark%2520demonstrate%2520that%2520DentalX%2520significantly%2520outperforms%2520prior%2520methods%2520in%2520both%2520tasks.%2520This%2520mutual%2520benefit%2520arises%2520naturally%2520during%2520model%2520optimization%252C%2520as%2520the%2520correlation%2520between%2520the%2520two%2520tasks%2520is%2520effectively%2520captured.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhiqin1998/DentYOLOX.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DentalX%3A%20Context-Aware%20Dental%20Disease%20Detection%20with%20Radiographs&entry.906535625=Zhi%20Qin%20Tan%20and%20Xiatian%20Zhu%20and%20Owen%20Addison%20and%20Yunpeng%20Li&entry.1292438233=Diagnosing%20dental%20diseases%20from%20radiographs%20is%20time-consuming%20and%20challenging%20due%20to%20the%20subtle%20nature%20of%20diagnostic%20evidence.%20Existing%20methods%2C%20which%20rely%20on%20object%20detection%20models%20designed%20for%20natural%20images%20with%20more%20distinct%20target%20patterns%2C%20struggle%20to%20detect%20dental%20diseases%20that%20present%20with%20far%20less%20visual%20support.%20To%20address%20this%20challenge%2C%20we%20propose%20%7B%5Cbf%20DentalX%7D%2C%20a%20novel%20context-aware%20dental%20disease%20detection%20approach%20that%20leverages%20oral%20structure%20information%20to%20mitigate%20the%20visual%20ambiguity%20inherent%20in%20radiographs.%20Specifically%2C%20we%20introduce%20a%20structural%20context%20extraction%20module%20that%20learns%20an%20auxiliary%20task%3A%20semantic%20segmentation%20of%20dental%20anatomy.%20The%20module%20extracts%20meaningful%20structural%20context%20and%20integrates%20it%20into%20the%20primary%20disease%20detection%20task%20to%20enhance%20the%20detection%20of%20subtle%20dental%20diseases.%20Extensive%20experiments%20on%20a%20dedicated%20benchmark%20demonstrate%20that%20DentalX%20significantly%20outperforms%20prior%20methods%20in%20both%20tasks.%20This%20mutual%20benefit%20arises%20naturally%20during%20model%20optimization%2C%20as%20the%20correlation%20between%20the%20two%20tasks%20is%20effectively%20captured.%20Our%20code%20is%20available%20at%20https%3A//github.com/zhiqin1998/DentYOLOX.&entry.1838667208=http%3A//arxiv.org/abs/2601.08797v1&entry.124074799=Read"},
{"title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding", "author": "Zenghua Liao and Jinzhi Liao and Xiang Zhao", "abstract": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.", "link": "http://arxiv.org/abs/2601.08653v1", "date": "2026-01-13", "relevancy": 2.6178, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding&body=Title%3A%20Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding%0AAuthor%3A%20Zenghua%20Liao%20and%20Jinzhi%20Liao%20and%20Xiang%20Zhao%0AAbstract%3A%20Large%20Language%20Models%20are%20rapidly%20emerging%20as%20web-native%20interfaces%20to%20social%20platforms.%20On%20the%20social%20web%2C%20users%20frequently%20have%20ambiguous%20and%20dynamic%20goals%2C%20making%20complex%20intent%20understanding-rather%20than%20single-turn%20execution-the%20cornerstone%20of%20effective%20human-LLM%20collaboration.%20Existing%20approaches%20attempt%20to%20clarify%20user%20intents%20through%20sequential%20or%20parallel%20questioning%2C%20yet%20they%20fall%20short%20of%20addressing%20the%20core%20challenge%3A%20modeling%20the%20logical%20dependencies%20among%20clarification%20questions.%20Inspired%20by%20the%20Cognitive%20Load%20Theory%2C%20we%20propose%20Prism%2C%20a%20novel%20framework%20for%20complex%20intent%20understanding%20that%20enables%20logically%20coherent%20and%20efficient%20intent%20clarification.%20Prism%20comprises%20four%20tailored%20modules%3A%20a%20complex%20intent%20decomposition%20module%2C%20which%20decomposes%20user%20intents%20into%20smaller%2C%20well-structured%20elements%20and%20identifies%20logical%20dependencies%20among%20them%3B%20a%20logical%20clarification%20generation%20module%2C%20which%20organizes%20clarification%20questions%20based%20on%20these%20dependencies%20to%20ensure%20coherent%2C%20low-friction%20interactions%3B%20an%20intent-aware%20reward%20module%2C%20which%20evaluates%20the%20quality%20of%20clarification%20trajectories%20via%20an%20intent-aware%20reward%20function%20and%20leverages%20Monte%20Carlo%20Sample%20to%20simulate%20user-LLM%20interactions%20for%20large-scale%2Chigh-quality%20training%20data%20generation%3B%20and%20a%20self-evolved%20intent%20tuning%20module%2C%20which%20iteratively%20refines%20the%20LLM%27s%20logical%20clarification%20capability%20through%20data-driven%20feedback%20and%20optimization.%20Prism%20consistently%20outperforms%20existing%20approaches%20across%20clarification%20interactions%2C%20intent%20execution%2C%20and%20cognitive%20load%20benchmarks.%20It%20achieves%20stateof-the-art%20logical%20consistency%2C%20reduces%20logical%20conflicts%20to%2011.5%25%2C%20increases%20user%20satisfaction%20by%2014.4%25%2C%20and%20decreases%20task%20completion%20time%20by%2034.8%25.%20All%20data%20and%20code%20are%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrism%253A%2520Towards%2520Lowering%2520User%2520Cognitive%2520Load%2520in%2520LLMs%2520via%2520Complex%2520Intent%2520Understanding%26entry.906535625%3DZenghua%2520Liao%2520and%2520Jinzhi%2520Liao%2520and%2520Xiang%2520Zhao%26entry.1292438233%3DLarge%2520Language%2520Models%2520are%2520rapidly%2520emerging%2520as%2520web-native%2520interfaces%2520to%2520social%2520platforms.%2520On%2520the%2520social%2520web%252C%2520users%2520frequently%2520have%2520ambiguous%2520and%2520dynamic%2520goals%252C%2520making%2520complex%2520intent%2520understanding-rather%2520than%2520single-turn%2520execution-the%2520cornerstone%2520of%2520effective%2520human-LLM%2520collaboration.%2520Existing%2520approaches%2520attempt%2520to%2520clarify%2520user%2520intents%2520through%2520sequential%2520or%2520parallel%2520questioning%252C%2520yet%2520they%2520fall%2520short%2520of%2520addressing%2520the%2520core%2520challenge%253A%2520modeling%2520the%2520logical%2520dependencies%2520among%2520clarification%2520questions.%2520Inspired%2520by%2520the%2520Cognitive%2520Load%2520Theory%252C%2520we%2520propose%2520Prism%252C%2520a%2520novel%2520framework%2520for%2520complex%2520intent%2520understanding%2520that%2520enables%2520logically%2520coherent%2520and%2520efficient%2520intent%2520clarification.%2520Prism%2520comprises%2520four%2520tailored%2520modules%253A%2520a%2520complex%2520intent%2520decomposition%2520module%252C%2520which%2520decomposes%2520user%2520intents%2520into%2520smaller%252C%2520well-structured%2520elements%2520and%2520identifies%2520logical%2520dependencies%2520among%2520them%253B%2520a%2520logical%2520clarification%2520generation%2520module%252C%2520which%2520organizes%2520clarification%2520questions%2520based%2520on%2520these%2520dependencies%2520to%2520ensure%2520coherent%252C%2520low-friction%2520interactions%253B%2520an%2520intent-aware%2520reward%2520module%252C%2520which%2520evaluates%2520the%2520quality%2520of%2520clarification%2520trajectories%2520via%2520an%2520intent-aware%2520reward%2520function%2520and%2520leverages%2520Monte%2520Carlo%2520Sample%2520to%2520simulate%2520user-LLM%2520interactions%2520for%2520large-scale%252Chigh-quality%2520training%2520data%2520generation%253B%2520and%2520a%2520self-evolved%2520intent%2520tuning%2520module%252C%2520which%2520iteratively%2520refines%2520the%2520LLM%2527s%2520logical%2520clarification%2520capability%2520through%2520data-driven%2520feedback%2520and%2520optimization.%2520Prism%2520consistently%2520outperforms%2520existing%2520approaches%2520across%2520clarification%2520interactions%252C%2520intent%2520execution%252C%2520and%2520cognitive%2520load%2520benchmarks.%2520It%2520achieves%2520stateof-the-art%2520logical%2520consistency%252C%2520reduces%2520logical%2520conflicts%2520to%252011.5%2525%252C%2520increases%2520user%2520satisfaction%2520by%252014.4%2525%252C%2520and%2520decreases%2520task%2520completion%2520time%2520by%252034.8%2525.%2520All%2520data%2520and%2520code%2520are%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding&entry.906535625=Zenghua%20Liao%20and%20Jinzhi%20Liao%20and%20Xiang%20Zhao&entry.1292438233=Large%20Language%20Models%20are%20rapidly%20emerging%20as%20web-native%20interfaces%20to%20social%20platforms.%20On%20the%20social%20web%2C%20users%20frequently%20have%20ambiguous%20and%20dynamic%20goals%2C%20making%20complex%20intent%20understanding-rather%20than%20single-turn%20execution-the%20cornerstone%20of%20effective%20human-LLM%20collaboration.%20Existing%20approaches%20attempt%20to%20clarify%20user%20intents%20through%20sequential%20or%20parallel%20questioning%2C%20yet%20they%20fall%20short%20of%20addressing%20the%20core%20challenge%3A%20modeling%20the%20logical%20dependencies%20among%20clarification%20questions.%20Inspired%20by%20the%20Cognitive%20Load%20Theory%2C%20we%20propose%20Prism%2C%20a%20novel%20framework%20for%20complex%20intent%20understanding%20that%20enables%20logically%20coherent%20and%20efficient%20intent%20clarification.%20Prism%20comprises%20four%20tailored%20modules%3A%20a%20complex%20intent%20decomposition%20module%2C%20which%20decomposes%20user%20intents%20into%20smaller%2C%20well-structured%20elements%20and%20identifies%20logical%20dependencies%20among%20them%3B%20a%20logical%20clarification%20generation%20module%2C%20which%20organizes%20clarification%20questions%20based%20on%20these%20dependencies%20to%20ensure%20coherent%2C%20low-friction%20interactions%3B%20an%20intent-aware%20reward%20module%2C%20which%20evaluates%20the%20quality%20of%20clarification%20trajectories%20via%20an%20intent-aware%20reward%20function%20and%20leverages%20Monte%20Carlo%20Sample%20to%20simulate%20user-LLM%20interactions%20for%20large-scale%2Chigh-quality%20training%20data%20generation%3B%20and%20a%20self-evolved%20intent%20tuning%20module%2C%20which%20iteratively%20refines%20the%20LLM%27s%20logical%20clarification%20capability%20through%20data-driven%20feedback%20and%20optimization.%20Prism%20consistently%20outperforms%20existing%20approaches%20across%20clarification%20interactions%2C%20intent%20execution%2C%20and%20cognitive%20load%20benchmarks.%20It%20achieves%20stateof-the-art%20logical%20consistency%2C%20reduces%20logical%20conflicts%20to%2011.5%25%2C%20increases%20user%20satisfaction%20by%2014.4%25%2C%20and%20decreases%20task%20completion%20time%20by%2034.8%25.%20All%20data%20and%20code%20are%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.08653v1&entry.124074799=Read"},
{"title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding", "author": "Juntao Jiang and Jiangning Zhang and Yali Bi and Jinsheng Bai and Weixuan Liu and Weiwei Jin and Zhucun Xue and Yong Liu and Xiaobin Hu and Shuicheng Yan", "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. To address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features 1) a diverse, multi-level difficulty dataset covering 24 examination types, 2) 13 varying-difficulty tasks, 3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning, and 4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare. Project page at https://juntaojianggavin.github.io/projects/M3CoTBench/.", "link": "http://arxiv.org/abs/2601.08758v1", "date": "2026-01-13", "relevancy": 2.6138, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3CoTBench%3A%20Benchmark%20Chain-of-Thought%20of%20MLLMs%20in%20Medical%20Image%20Understanding&body=Title%3A%20M3CoTBench%3A%20Benchmark%20Chain-of-Thought%20of%20MLLMs%20in%20Medical%20Image%20Understanding%0AAuthor%3A%20Juntao%20Jiang%20and%20Jiangning%20Zhang%20and%20Yali%20Bi%20and%20Jinsheng%20Bai%20and%20Weixuan%20Liu%20and%20Weiwei%20Jin%20and%20Zhucun%20Xue%20and%20Yong%20Liu%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan%0AAbstract%3A%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20proven%20effective%20in%20enhancing%20large%20language%20models%20by%20encouraging%20step-by-step%20intermediate%20reasoning%2C%20and%20recent%20advances%20have%20extended%20this%20paradigm%20to%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20In%20the%20medical%20domain%2C%20where%20diagnostic%20decisions%20depend%20on%20nuanced%20visual%20cues%20and%20sequential%20reasoning%2C%20CoT%20aligns%20naturally%20with%20clinical%20thinking%20processes.%20However%2C%20Current%20benchmarks%20for%20medical%20image%20understanding%20generally%20focus%20on%20the%20final%20answer%20while%20ignoring%20the%20reasoning%20path.%20An%20opaque%20process%20lacks%20reliable%20bases%20for%20judgment%2C%20making%20it%20difficult%20to%20assist%20doctors%20in%20diagnosis.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20new%20M3CoTBench%20benchmark%20specifically%20designed%20to%20evaluate%20the%20correctness%2C%20efficiency%2C%20impact%2C%20and%20consistency%20of%20CoT%20reasoning%20in%20medical%20image%20understanding.%20M3CoTBench%20features%201%29%20a%20diverse%2C%20multi-level%20difficulty%20dataset%20covering%2024%20examination%20types%2C%202%29%2013%20varying-difficulty%20tasks%2C%203%29%20a%20suite%20of%20CoT-specific%20evaluation%20metrics%20%28correctness%2C%20efficiency%2C%20impact%2C%20and%20consistency%29%20tailored%20to%20clinical%20reasoning%2C%20and%204%29%20a%20performance%20analysis%20of%20multiple%20MLLMs.%20M3CoTBench%20systematically%20evaluates%20CoT%20reasoning%20across%20diverse%20medical%20imaging%20tasks%2C%20revealing%20current%20limitations%20of%20MLLMs%20in%20generating%20reliable%20and%20clinically%20interpretable%20reasoning%2C%20and%20aims%20to%20foster%20the%20development%20of%20transparent%2C%20trustworthy%2C%20and%20diagnostically%20accurate%20AI%20systems%20for%20healthcare.%20Project%20page%20at%20https%3A//juntaojianggavin.github.io/projects/M3CoTBench/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3CoTBench%253A%2520Benchmark%2520Chain-of-Thought%2520of%2520MLLMs%2520in%2520Medical%2520Image%2520Understanding%26entry.906535625%3DJuntao%2520Jiang%2520and%2520Jiangning%2520Zhang%2520and%2520Yali%2520Bi%2520and%2520Jinsheng%2520Bai%2520and%2520Weixuan%2520Liu%2520and%2520Weiwei%2520Jin%2520and%2520Zhucun%2520Xue%2520and%2520Yong%2520Liu%2520and%2520Xiaobin%2520Hu%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3DChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520has%2520proven%2520effective%2520in%2520enhancing%2520large%2520language%2520models%2520by%2520encouraging%2520step-by-step%2520intermediate%2520reasoning%252C%2520and%2520recent%2520advances%2520have%2520extended%2520this%2520paradigm%2520to%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520In%2520the%2520medical%2520domain%252C%2520where%2520diagnostic%2520decisions%2520depend%2520on%2520nuanced%2520visual%2520cues%2520and%2520sequential%2520reasoning%252C%2520CoT%2520aligns%2520naturally%2520with%2520clinical%2520thinking%2520processes.%2520However%252C%2520Current%2520benchmarks%2520for%2520medical%2520image%2520understanding%2520generally%2520focus%2520on%2520the%2520final%2520answer%2520while%2520ignoring%2520the%2520reasoning%2520path.%2520An%2520opaque%2520process%2520lacks%2520reliable%2520bases%2520for%2520judgment%252C%2520making%2520it%2520difficult%2520to%2520assist%2520doctors%2520in%2520diagnosis.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520new%2520M3CoTBench%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520the%2520correctness%252C%2520efficiency%252C%2520impact%252C%2520and%2520consistency%2520of%2520CoT%2520reasoning%2520in%2520medical%2520image%2520understanding.%2520M3CoTBench%2520features%25201%2529%2520a%2520diverse%252C%2520multi-level%2520difficulty%2520dataset%2520covering%252024%2520examination%2520types%252C%25202%2529%252013%2520varying-difficulty%2520tasks%252C%25203%2529%2520a%2520suite%2520of%2520CoT-specific%2520evaluation%2520metrics%2520%2528correctness%252C%2520efficiency%252C%2520impact%252C%2520and%2520consistency%2529%2520tailored%2520to%2520clinical%2520reasoning%252C%2520and%25204%2529%2520a%2520performance%2520analysis%2520of%2520multiple%2520MLLMs.%2520M3CoTBench%2520systematically%2520evaluates%2520CoT%2520reasoning%2520across%2520diverse%2520medical%2520imaging%2520tasks%252C%2520revealing%2520current%2520limitations%2520of%2520MLLMs%2520in%2520generating%2520reliable%2520and%2520clinically%2520interpretable%2520reasoning%252C%2520and%2520aims%2520to%2520foster%2520the%2520development%2520of%2520transparent%252C%2520trustworthy%252C%2520and%2520diagnostically%2520accurate%2520AI%2520systems%2520for%2520healthcare.%2520Project%2520page%2520at%2520https%253A//juntaojianggavin.github.io/projects/M3CoTBench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3CoTBench%3A%20Benchmark%20Chain-of-Thought%20of%20MLLMs%20in%20Medical%20Image%20Understanding&entry.906535625=Juntao%20Jiang%20and%20Jiangning%20Zhang%20and%20Yali%20Bi%20and%20Jinsheng%20Bai%20and%20Weixuan%20Liu%20and%20Weiwei%20Jin%20and%20Zhucun%20Xue%20and%20Yong%20Liu%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan&entry.1292438233=Chain-of-Thought%20%28CoT%29%20reasoning%20has%20proven%20effective%20in%20enhancing%20large%20language%20models%20by%20encouraging%20step-by-step%20intermediate%20reasoning%2C%20and%20recent%20advances%20have%20extended%20this%20paradigm%20to%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20In%20the%20medical%20domain%2C%20where%20diagnostic%20decisions%20depend%20on%20nuanced%20visual%20cues%20and%20sequential%20reasoning%2C%20CoT%20aligns%20naturally%20with%20clinical%20thinking%20processes.%20However%2C%20Current%20benchmarks%20for%20medical%20image%20understanding%20generally%20focus%20on%20the%20final%20answer%20while%20ignoring%20the%20reasoning%20path.%20An%20opaque%20process%20lacks%20reliable%20bases%20for%20judgment%2C%20making%20it%20difficult%20to%20assist%20doctors%20in%20diagnosis.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20new%20M3CoTBench%20benchmark%20specifically%20designed%20to%20evaluate%20the%20correctness%2C%20efficiency%2C%20impact%2C%20and%20consistency%20of%20CoT%20reasoning%20in%20medical%20image%20understanding.%20M3CoTBench%20features%201%29%20a%20diverse%2C%20multi-level%20difficulty%20dataset%20covering%2024%20examination%20types%2C%202%29%2013%20varying-difficulty%20tasks%2C%203%29%20a%20suite%20of%20CoT-specific%20evaluation%20metrics%20%28correctness%2C%20efficiency%2C%20impact%2C%20and%20consistency%29%20tailored%20to%20clinical%20reasoning%2C%20and%204%29%20a%20performance%20analysis%20of%20multiple%20MLLMs.%20M3CoTBench%20systematically%20evaluates%20CoT%20reasoning%20across%20diverse%20medical%20imaging%20tasks%2C%20revealing%20current%20limitations%20of%20MLLMs%20in%20generating%20reliable%20and%20clinically%20interpretable%20reasoning%2C%20and%20aims%20to%20foster%20the%20development%20of%20transparent%2C%20trustworthy%2C%20and%20diagnostically%20accurate%20AI%20systems%20for%20healthcare.%20Project%20page%20at%20https%3A//juntaojianggavin.github.io/projects/M3CoTBench/.&entry.1838667208=http%3A//arxiv.org/abs/2601.08758v1&entry.124074799=Read"},
{"title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning", "author": "Jae Hyoung Jeon and Cheolsu Lim and Myungjoo Kang", "abstract": "Recent success in contrastive learning has sparked growing interest in more effectively leveraging multiple augmented views of data. While prior methods incorporate multiple views at the loss or feature level, they primarily capture pairwise relationships and fail to model the joint structure across all views. In this work, we propose a divergence-based similarity function (DSF) that explicitly captures the joint structure by representing each set of augmented views as a distribution and measuring similarity as the divergence between distributions. Extensive experiments demonstrate that DSF consistently improves performance across diverse tasks, including kNN classification, linear evaluation, transfer learning, and distribution shift, while also achieving greater efficiency than other multi-view methods. Furthermore, we establish a connection between DSF and cosine similarity, and demonstrate that, unlike cosine similarity, DSF operates effectively without the need for tuning a temperature hyperparameter.", "link": "http://arxiv.org/abs/2507.06560v3", "date": "2026-01-13", "relevancy": 2.5874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.518}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divergence-Based%20Similarity%20Function%20for%20Multi-View%20Contrastive%20Learning&body=Title%3A%20Divergence-Based%20Similarity%20Function%20for%20Multi-View%20Contrastive%20Learning%0AAuthor%3A%20Jae%20Hyoung%20Jeon%20and%20Cheolsu%20Lim%20and%20Myungjoo%20Kang%0AAbstract%3A%20Recent%20success%20in%20contrastive%20learning%20has%20sparked%20growing%20interest%20in%20more%20effectively%20leveraging%20multiple%20augmented%20views%20of%20data.%20While%20prior%20methods%20incorporate%20multiple%20views%20at%20the%20loss%20or%20feature%20level%2C%20they%20primarily%20capture%20pairwise%20relationships%20and%20fail%20to%20model%20the%20joint%20structure%20across%20all%20views.%20In%20this%20work%2C%20we%20propose%20a%20divergence-based%20similarity%20function%20%28DSF%29%20that%20explicitly%20captures%20the%20joint%20structure%20by%20representing%20each%20set%20of%20augmented%20views%20as%20a%20distribution%20and%20measuring%20similarity%20as%20the%20divergence%20between%20distributions.%20Extensive%20experiments%20demonstrate%20that%20DSF%20consistently%20improves%20performance%20across%20diverse%20tasks%2C%20including%20kNN%20classification%2C%20linear%20evaluation%2C%20transfer%20learning%2C%20and%20distribution%20shift%2C%20while%20also%20achieving%20greater%20efficiency%20than%20other%20multi-view%20methods.%20Furthermore%2C%20we%20establish%20a%20connection%20between%20DSF%20and%20cosine%20similarity%2C%20and%20demonstrate%20that%2C%20unlike%20cosine%20similarity%2C%20DSF%20operates%20effectively%20without%20the%20need%20for%20tuning%20a%20temperature%20hyperparameter.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06560v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivergence-Based%2520Similarity%2520Function%2520for%2520Multi-View%2520Contrastive%2520Learning%26entry.906535625%3DJae%2520Hyoung%2520Jeon%2520and%2520Cheolsu%2520Lim%2520and%2520Myungjoo%2520Kang%26entry.1292438233%3DRecent%2520success%2520in%2520contrastive%2520learning%2520has%2520sparked%2520growing%2520interest%2520in%2520more%2520effectively%2520leveraging%2520multiple%2520augmented%2520views%2520of%2520data.%2520While%2520prior%2520methods%2520incorporate%2520multiple%2520views%2520at%2520the%2520loss%2520or%2520feature%2520level%252C%2520they%2520primarily%2520capture%2520pairwise%2520relationships%2520and%2520fail%2520to%2520model%2520the%2520joint%2520structure%2520across%2520all%2520views.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520divergence-based%2520similarity%2520function%2520%2528DSF%2529%2520that%2520explicitly%2520captures%2520the%2520joint%2520structure%2520by%2520representing%2520each%2520set%2520of%2520augmented%2520views%2520as%2520a%2520distribution%2520and%2520measuring%2520similarity%2520as%2520the%2520divergence%2520between%2520distributions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DSF%2520consistently%2520improves%2520performance%2520across%2520diverse%2520tasks%252C%2520including%2520kNN%2520classification%252C%2520linear%2520evaluation%252C%2520transfer%2520learning%252C%2520and%2520distribution%2520shift%252C%2520while%2520also%2520achieving%2520greater%2520efficiency%2520than%2520other%2520multi-view%2520methods.%2520Furthermore%252C%2520we%2520establish%2520a%2520connection%2520between%2520DSF%2520and%2520cosine%2520similarity%252C%2520and%2520demonstrate%2520that%252C%2520unlike%2520cosine%2520similarity%252C%2520DSF%2520operates%2520effectively%2520without%2520the%2520need%2520for%2520tuning%2520a%2520temperature%2520hyperparameter.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06560v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divergence-Based%20Similarity%20Function%20for%20Multi-View%20Contrastive%20Learning&entry.906535625=Jae%20Hyoung%20Jeon%20and%20Cheolsu%20Lim%20and%20Myungjoo%20Kang&entry.1292438233=Recent%20success%20in%20contrastive%20learning%20has%20sparked%20growing%20interest%20in%20more%20effectively%20leveraging%20multiple%20augmented%20views%20of%20data.%20While%20prior%20methods%20incorporate%20multiple%20views%20at%20the%20loss%20or%20feature%20level%2C%20they%20primarily%20capture%20pairwise%20relationships%20and%20fail%20to%20model%20the%20joint%20structure%20across%20all%20views.%20In%20this%20work%2C%20we%20propose%20a%20divergence-based%20similarity%20function%20%28DSF%29%20that%20explicitly%20captures%20the%20joint%20structure%20by%20representing%20each%20set%20of%20augmented%20views%20as%20a%20distribution%20and%20measuring%20similarity%20as%20the%20divergence%20between%20distributions.%20Extensive%20experiments%20demonstrate%20that%20DSF%20consistently%20improves%20performance%20across%20diverse%20tasks%2C%20including%20kNN%20classification%2C%20linear%20evaluation%2C%20transfer%20learning%2C%20and%20distribution%20shift%2C%20while%20also%20achieving%20greater%20efficiency%20than%20other%20multi-view%20methods.%20Furthermore%2C%20we%20establish%20a%20connection%20between%20DSF%20and%20cosine%20similarity%2C%20and%20demonstrate%20that%2C%20unlike%20cosine%20similarity%2C%20DSF%20operates%20effectively%20without%20the%20need%20for%20tuning%20a%20temperature%20hyperparameter.&entry.1838667208=http%3A//arxiv.org/abs/2507.06560v3&entry.124074799=Read"},
{"title": "ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis", "author": "Jian Chen and Peilin Zhou and Yining Hua and Dading Chong and Meng Cao and Yaowei Li and Wei Chen and Bing Zhu and Junwei Liang and Zixuan Yuan", "abstract": "Meteorological heatmaps play a vital role in deciphering extreme weather phenomena, yet their inherent complexities marked by irregular contours, unstructured patterns, and complex color variations present unique analytical hurdles for state-of-the-art Vision-Language Models (VLMs). Current state-of-the-art models like GPT-4o, Qwen-VL, and LLaVA 1.6 struggle with tasks such as precise color identification and spatial localization, resulting in inaccurate or incomplete interpretations. To address these challenges, we introduce Sparse Position and Outline Tracking (SPOT), a novel algorithm specifically designed to process irregularly shaped colored regions in visual data. SPOT identifies and localizes these regions by extracting their spatial coordinates, enabling structured representations of irregular shapes. Building on SPOT, we construct ClimateIQA, a novel meteorological visual question answering (VQA) dataset, comprising 26,280 high-resolution heatmaps and 762,120 instruction samples for wind gust, total precipitation, wind chill index and heat index analysis. ClimateIQA enhances VLM training by incorporating spatial cues, geographic metadata, and reanalysis data, improving model accuracy in interpreting and describing extreme weather features. Furthermore, we develop Climate-Zoo, a suite of fine-tuned VLMs based on SPOT-empowered ClimateIQA, which significantly outperforms existing models in meteorological heatmap tasks.", "link": "http://arxiv.org/abs/2406.09838v4", "date": "2026-01-13", "relevancy": 2.5746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClimateIQA%3A%20A%20New%20Dataset%20and%20Benchmark%20to%20Advance%20Vision-Language%20Models%20in%20Meteorology%20Anomalies%20Analysis&body=Title%3A%20ClimateIQA%3A%20A%20New%20Dataset%20and%20Benchmark%20to%20Advance%20Vision-Language%20Models%20in%20Meteorology%20Anomalies%20Analysis%0AAuthor%3A%20Jian%20Chen%20and%20Peilin%20Zhou%20and%20Yining%20Hua%20and%20Dading%20Chong%20and%20Meng%20Cao%20and%20Yaowei%20Li%20and%20Wei%20Chen%20and%20Bing%20Zhu%20and%20Junwei%20Liang%20and%20Zixuan%20Yuan%0AAbstract%3A%20Meteorological%20heatmaps%20play%20a%20vital%20role%20in%20deciphering%20extreme%20weather%20phenomena%2C%20yet%20their%20inherent%20complexities%20marked%20by%20irregular%20contours%2C%20unstructured%20patterns%2C%20and%20complex%20color%20variations%20present%20unique%20analytical%20hurdles%20for%20state-of-the-art%20Vision-Language%20Models%20%28VLMs%29.%20Current%20state-of-the-art%20models%20like%20GPT-4o%2C%20Qwen-VL%2C%20and%20LLaVA%201.6%20struggle%20with%20tasks%20such%20as%20precise%20color%20identification%20and%20spatial%20localization%2C%20resulting%20in%20inaccurate%20or%20incomplete%20interpretations.%20To%20address%20these%20challenges%2C%20we%20introduce%20Sparse%20Position%20and%20Outline%20Tracking%20%28SPOT%29%2C%20a%20novel%20algorithm%20specifically%20designed%20to%20process%20irregularly%20shaped%20colored%20regions%20in%20visual%20data.%20SPOT%20identifies%20and%20localizes%20these%20regions%20by%20extracting%20their%20spatial%20coordinates%2C%20enabling%20structured%20representations%20of%20irregular%20shapes.%20Building%20on%20SPOT%2C%20we%20construct%20ClimateIQA%2C%20a%20novel%20meteorological%20visual%20question%20answering%20%28VQA%29%20dataset%2C%20comprising%2026%2C280%20high-resolution%20heatmaps%20and%20762%2C120%20instruction%20samples%20for%20wind%20gust%2C%20total%20precipitation%2C%20wind%20chill%20index%20and%20heat%20index%20analysis.%20ClimateIQA%20enhances%20VLM%20training%20by%20incorporating%20spatial%20cues%2C%20geographic%20metadata%2C%20and%20reanalysis%20data%2C%20improving%20model%20accuracy%20in%20interpreting%20and%20describing%20extreme%20weather%20features.%20Furthermore%2C%20we%20develop%20Climate-Zoo%2C%20a%20suite%20of%20fine-tuned%20VLMs%20based%20on%20SPOT-empowered%20ClimateIQA%2C%20which%20significantly%20outperforms%20existing%20models%20in%20meteorological%20heatmap%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2406.09838v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClimateIQA%253A%2520A%2520New%2520Dataset%2520and%2520Benchmark%2520to%2520Advance%2520Vision-Language%2520Models%2520in%2520Meteorology%2520Anomalies%2520Analysis%26entry.906535625%3DJian%2520Chen%2520and%2520Peilin%2520Zhou%2520and%2520Yining%2520Hua%2520and%2520Dading%2520Chong%2520and%2520Meng%2520Cao%2520and%2520Yaowei%2520Li%2520and%2520Wei%2520Chen%2520and%2520Bing%2520Zhu%2520and%2520Junwei%2520Liang%2520and%2520Zixuan%2520Yuan%26entry.1292438233%3DMeteorological%2520heatmaps%2520play%2520a%2520vital%2520role%2520in%2520deciphering%2520extreme%2520weather%2520phenomena%252C%2520yet%2520their%2520inherent%2520complexities%2520marked%2520by%2520irregular%2520contours%252C%2520unstructured%2520patterns%252C%2520and%2520complex%2520color%2520variations%2520present%2520unique%2520analytical%2520hurdles%2520for%2520state-of-the-art%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520Current%2520state-of-the-art%2520models%2520like%2520GPT-4o%252C%2520Qwen-VL%252C%2520and%2520LLaVA%25201.6%2520struggle%2520with%2520tasks%2520such%2520as%2520precise%2520color%2520identification%2520and%2520spatial%2520localization%252C%2520resulting%2520in%2520inaccurate%2520or%2520incomplete%2520interpretations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Sparse%2520Position%2520and%2520Outline%2520Tracking%2520%2528SPOT%2529%252C%2520a%2520novel%2520algorithm%2520specifically%2520designed%2520to%2520process%2520irregularly%2520shaped%2520colored%2520regions%2520in%2520visual%2520data.%2520SPOT%2520identifies%2520and%2520localizes%2520these%2520regions%2520by%2520extracting%2520their%2520spatial%2520coordinates%252C%2520enabling%2520structured%2520representations%2520of%2520irregular%2520shapes.%2520Building%2520on%2520SPOT%252C%2520we%2520construct%2520ClimateIQA%252C%2520a%2520novel%2520meteorological%2520visual%2520question%2520answering%2520%2528VQA%2529%2520dataset%252C%2520comprising%252026%252C280%2520high-resolution%2520heatmaps%2520and%2520762%252C120%2520instruction%2520samples%2520for%2520wind%2520gust%252C%2520total%2520precipitation%252C%2520wind%2520chill%2520index%2520and%2520heat%2520index%2520analysis.%2520ClimateIQA%2520enhances%2520VLM%2520training%2520by%2520incorporating%2520spatial%2520cues%252C%2520geographic%2520metadata%252C%2520and%2520reanalysis%2520data%252C%2520improving%2520model%2520accuracy%2520in%2520interpreting%2520and%2520describing%2520extreme%2520weather%2520features.%2520Furthermore%252C%2520we%2520develop%2520Climate-Zoo%252C%2520a%2520suite%2520of%2520fine-tuned%2520VLMs%2520based%2520on%2520SPOT-empowered%2520ClimateIQA%252C%2520which%2520significantly%2520outperforms%2520existing%2520models%2520in%2520meteorological%2520heatmap%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09838v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClimateIQA%3A%20A%20New%20Dataset%20and%20Benchmark%20to%20Advance%20Vision-Language%20Models%20in%20Meteorology%20Anomalies%20Analysis&entry.906535625=Jian%20Chen%20and%20Peilin%20Zhou%20and%20Yining%20Hua%20and%20Dading%20Chong%20and%20Meng%20Cao%20and%20Yaowei%20Li%20and%20Wei%20Chen%20and%20Bing%20Zhu%20and%20Junwei%20Liang%20and%20Zixuan%20Yuan&entry.1292438233=Meteorological%20heatmaps%20play%20a%20vital%20role%20in%20deciphering%20extreme%20weather%20phenomena%2C%20yet%20their%20inherent%20complexities%20marked%20by%20irregular%20contours%2C%20unstructured%20patterns%2C%20and%20complex%20color%20variations%20present%20unique%20analytical%20hurdles%20for%20state-of-the-art%20Vision-Language%20Models%20%28VLMs%29.%20Current%20state-of-the-art%20models%20like%20GPT-4o%2C%20Qwen-VL%2C%20and%20LLaVA%201.6%20struggle%20with%20tasks%20such%20as%20precise%20color%20identification%20and%20spatial%20localization%2C%20resulting%20in%20inaccurate%20or%20incomplete%20interpretations.%20To%20address%20these%20challenges%2C%20we%20introduce%20Sparse%20Position%20and%20Outline%20Tracking%20%28SPOT%29%2C%20a%20novel%20algorithm%20specifically%20designed%20to%20process%20irregularly%20shaped%20colored%20regions%20in%20visual%20data.%20SPOT%20identifies%20and%20localizes%20these%20regions%20by%20extracting%20their%20spatial%20coordinates%2C%20enabling%20structured%20representations%20of%20irregular%20shapes.%20Building%20on%20SPOT%2C%20we%20construct%20ClimateIQA%2C%20a%20novel%20meteorological%20visual%20question%20answering%20%28VQA%29%20dataset%2C%20comprising%2026%2C280%20high-resolution%20heatmaps%20and%20762%2C120%20instruction%20samples%20for%20wind%20gust%2C%20total%20precipitation%2C%20wind%20chill%20index%20and%20heat%20index%20analysis.%20ClimateIQA%20enhances%20VLM%20training%20by%20incorporating%20spatial%20cues%2C%20geographic%20metadata%2C%20and%20reanalysis%20data%2C%20improving%20model%20accuracy%20in%20interpreting%20and%20describing%20extreme%20weather%20features.%20Furthermore%2C%20we%20develop%20Climate-Zoo%2C%20a%20suite%20of%20fine-tuned%20VLMs%20based%20on%20SPOT-empowered%20ClimateIQA%2C%20which%20significantly%20outperforms%20existing%20models%20in%20meteorological%20heatmap%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2406.09838v4&entry.124074799=Read"},
{"title": "Modality-Decoupled RGB-Thermal Object Detector via Query Fusion", "author": "Chao Tian and Zikun Zhou and Chao Yang and Guoqing Zhu and Fu'an Zhong and Zhenyu He", "abstract": "The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.", "link": "http://arxiv.org/abs/2601.08458v1", "date": "2026-01-13", "relevancy": 2.5683, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Decoupled%20RGB-Thermal%20Object%20Detector%20via%20Query%20Fusion&body=Title%3A%20Modality-Decoupled%20RGB-Thermal%20Object%20Detector%20via%20Query%20Fusion%0AAuthor%3A%20Chao%20Tian%20and%20Zikun%20Zhou%20and%20Chao%20Yang%20and%20Guoqing%20Zhu%20and%20Fu%27an%20Zhong%20and%20Zhenyu%20He%0AAbstract%3A%20The%20advantage%20of%20RGB-Thermal%20%28RGB-T%29%20detection%20lies%20in%20its%20ability%20to%20perform%20modality%20fusion%20and%20integrate%20cross-modality%20complementary%20information%2C%20enabling%20robust%20detection%20under%20diverse%20illumination%20and%20weather%20conditions.%20However%2C%20under%20extreme%20conditions%20where%20one%20modality%20exhibits%20poor%20quality%20and%20disturbs%20detection%2C%20modality%20separation%20is%20necessary%20to%20mitigate%20the%20impact%20of%20noise.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Modality-Decoupled%20RGB-T%20detection%20framework%20with%20Query%20Fusion%20%28MDQF%29%20to%20balance%20modality%20complementation%20and%20separation.%20In%20this%20framework%2C%20DETR-like%20detectors%20are%20employed%20as%20separate%20branches%20for%20the%20RGB%20and%20TIR%20images%2C%20with%20query%20fusion%20interspersed%20between%20the%20two%20branches%20in%20each%20refinement%20stage.%20Herein%2C%20query%20fusion%20is%20performed%20by%20feeding%20the%20high-quality%20queries%20from%20one%20branch%20to%20the%20other%20one%20after%20query%20selection%20and%20adaptation.%20This%20design%20effectively%20excludes%20the%20degraded%20modality%20and%20corrects%20the%20predictions%20using%20high-quality%20queries.%20Moreover%2C%20the%20decoupled%20framework%20allows%20us%20to%20optimize%20each%20individual%20branch%20with%20unpaired%20RGB%20or%20TIR%20images%2C%20eliminating%20the%20need%20for%20paired%20RGB-T%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20delivers%20superior%20performance%20to%20existing%20RGB-T%20detectors%20and%20achieves%20better%20modality%20independence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Decoupled%2520RGB-Thermal%2520Object%2520Detector%2520via%2520Query%2520Fusion%26entry.906535625%3DChao%2520Tian%2520and%2520Zikun%2520Zhou%2520and%2520Chao%2520Yang%2520and%2520Guoqing%2520Zhu%2520and%2520Fu%2527an%2520Zhong%2520and%2520Zhenyu%2520He%26entry.1292438233%3DThe%2520advantage%2520of%2520RGB-Thermal%2520%2528RGB-T%2529%2520detection%2520lies%2520in%2520its%2520ability%2520to%2520perform%2520modality%2520fusion%2520and%2520integrate%2520cross-modality%2520complementary%2520information%252C%2520enabling%2520robust%2520detection%2520under%2520diverse%2520illumination%2520and%2520weather%2520conditions.%2520However%252C%2520under%2520extreme%2520conditions%2520where%2520one%2520modality%2520exhibits%2520poor%2520quality%2520and%2520disturbs%2520detection%252C%2520modality%2520separation%2520is%2520necessary%2520to%2520mitigate%2520the%2520impact%2520of%2520noise.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520Modality-Decoupled%2520RGB-T%2520detection%2520framework%2520with%2520Query%2520Fusion%2520%2528MDQF%2529%2520to%2520balance%2520modality%2520complementation%2520and%2520separation.%2520In%2520this%2520framework%252C%2520DETR-like%2520detectors%2520are%2520employed%2520as%2520separate%2520branches%2520for%2520the%2520RGB%2520and%2520TIR%2520images%252C%2520with%2520query%2520fusion%2520interspersed%2520between%2520the%2520two%2520branches%2520in%2520each%2520refinement%2520stage.%2520Herein%252C%2520query%2520fusion%2520is%2520performed%2520by%2520feeding%2520the%2520high-quality%2520queries%2520from%2520one%2520branch%2520to%2520the%2520other%2520one%2520after%2520query%2520selection%2520and%2520adaptation.%2520This%2520design%2520effectively%2520excludes%2520the%2520degraded%2520modality%2520and%2520corrects%2520the%2520predictions%2520using%2520high-quality%2520queries.%2520Moreover%252C%2520the%2520decoupled%2520framework%2520allows%2520us%2520to%2520optimize%2520each%2520individual%2520branch%2520with%2520unpaired%2520RGB%2520or%2520TIR%2520images%252C%2520eliminating%2520the%2520need%2520for%2520paired%2520RGB-T%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520delivers%2520superior%2520performance%2520to%2520existing%2520RGB-T%2520detectors%2520and%2520achieves%2520better%2520modality%2520independence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Decoupled%20RGB-Thermal%20Object%20Detector%20via%20Query%20Fusion&entry.906535625=Chao%20Tian%20and%20Zikun%20Zhou%20and%20Chao%20Yang%20and%20Guoqing%20Zhu%20and%20Fu%27an%20Zhong%20and%20Zhenyu%20He&entry.1292438233=The%20advantage%20of%20RGB-Thermal%20%28RGB-T%29%20detection%20lies%20in%20its%20ability%20to%20perform%20modality%20fusion%20and%20integrate%20cross-modality%20complementary%20information%2C%20enabling%20robust%20detection%20under%20diverse%20illumination%20and%20weather%20conditions.%20However%2C%20under%20extreme%20conditions%20where%20one%20modality%20exhibits%20poor%20quality%20and%20disturbs%20detection%2C%20modality%20separation%20is%20necessary%20to%20mitigate%20the%20impact%20of%20noise.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Modality-Decoupled%20RGB-T%20detection%20framework%20with%20Query%20Fusion%20%28MDQF%29%20to%20balance%20modality%20complementation%20and%20separation.%20In%20this%20framework%2C%20DETR-like%20detectors%20are%20employed%20as%20separate%20branches%20for%20the%20RGB%20and%20TIR%20images%2C%20with%20query%20fusion%20interspersed%20between%20the%20two%20branches%20in%20each%20refinement%20stage.%20Herein%2C%20query%20fusion%20is%20performed%20by%20feeding%20the%20high-quality%20queries%20from%20one%20branch%20to%20the%20other%20one%20after%20query%20selection%20and%20adaptation.%20This%20design%20effectively%20excludes%20the%20degraded%20modality%20and%20corrects%20the%20predictions%20using%20high-quality%20queries.%20Moreover%2C%20the%20decoupled%20framework%20allows%20us%20to%20optimize%20each%20individual%20branch%20with%20unpaired%20RGB%20or%20TIR%20images%2C%20eliminating%20the%20need%20for%20paired%20RGB-T%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20delivers%20superior%20performance%20to%20existing%20RGB-T%20detectors%20and%20achieves%20better%20modality%20independence.&entry.1838667208=http%3A//arxiv.org/abs/2601.08458v1&entry.124074799=Read"},
{"title": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "author": "Jingtong Yue and Ziqi Huang and Zhaoxi Chen and Xintao Wang and Pengfei Wan and Ziwei Liu", "abstract": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.", "link": "http://arxiv.org/abs/2511.08585v2", "date": "2026-01-13", "relevancy": 2.5585, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6817}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6437}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20the%20Visual%20World%20with%20Artificial%20Intelligence%3A%20A%20Roadmap&body=Title%3A%20Simulating%20the%20Visual%20World%20with%20Artificial%20Intelligence%3A%20A%20Roadmap%0AAuthor%3A%20Jingtong%20Yue%20and%20Ziqi%20Huang%20and%20Zhaoxi%20Chen%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Ziwei%20Liu%0AAbstract%3A%20The%20landscape%20of%20video%20generation%20is%20shifting%2C%20from%20a%20focus%20on%20generating%20visually%20appealing%20clips%20to%20building%20virtual%20environments%20that%20support%20interaction%20and%20maintain%20physical%20plausibility.%20These%20developments%20point%20toward%20the%20emergence%20of%20video%20foundation%20models%20that%20function%20not%20only%20as%20visual%20generators%20but%20also%20as%20implicit%20world%20models%2C%20models%20that%20simulate%20the%20physical%20dynamics%2C%20agent-environment%20interactions%2C%20and%20task%20planning%20that%20govern%20real%20or%20imagined%20worlds.%20This%20survey%20provides%20a%20systematic%20overview%20of%20this%20evolution%2C%20conceptualizing%20modern%20video%20foundation%20models%20as%20the%20combination%20of%20two%20core%20components%3A%20an%20implicit%20world%20model%20and%20a%20video%20renderer.%20The%20world%20model%20encodes%20structured%20knowledge%20about%20the%20world%2C%20including%20physical%20laws%2C%20interaction%20dynamics%2C%20and%20agent%20behavior.%20It%20serves%20as%20a%20latent%20simulation%20engine%20that%20enables%20coherent%20visual%20reasoning%2C%20long-term%20temporal%20consistency%2C%20and%20goal-driven%20planning.%20The%20video%20renderer%20transforms%20this%20latent%20simulation%20into%20realistic%20visual%20observations%2C%20effectively%20producing%20videos%20as%20a%20%22window%22%20into%20the%20simulated%20world.%20We%20trace%20the%20progression%20of%20video%20generation%20through%20four%20generations%2C%20in%20which%20the%20core%20capabilities%20advance%20step%20by%20step%2C%20ultimately%20culminating%20in%20a%20world%20model%2C%20built%20upon%20a%20video%20generation%20model%2C%20that%20embodies%20intrinsic%20physical%20plausibility%2C%20real-time%20multimodal%20interaction%2C%20and%20planning%20capabilities%20spanning%20multiple%20spatiotemporal%20scales.%20For%20each%20generation%2C%20we%20define%20its%20core%20characteristics%2C%20highlight%20representative%20works%2C%20and%20examine%20their%20application%20domains%20such%20as%20robotics%2C%20autonomous%20driving%2C%20and%20interactive%20gaming.%20Finally%2C%20we%20discuss%20open%20challenges%20and%20design%20principles%20for%20next-generation%20world%20models%2C%20including%20the%20role%20of%20agent%20intelligence%20in%20shaping%20and%20evaluating%20these%20systems.%20An%20up-to-date%20list%20of%20related%20works%20is%20maintained%20at%20this%20link.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520the%2520Visual%2520World%2520with%2520Artificial%2520Intelligence%253A%2520A%2520Roadmap%26entry.906535625%3DJingtong%2520Yue%2520and%2520Ziqi%2520Huang%2520and%2520Zhaoxi%2520Chen%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DThe%2520landscape%2520of%2520video%2520generation%2520is%2520shifting%252C%2520from%2520a%2520focus%2520on%2520generating%2520visually%2520appealing%2520clips%2520to%2520building%2520virtual%2520environments%2520that%2520support%2520interaction%2520and%2520maintain%2520physical%2520plausibility.%2520These%2520developments%2520point%2520toward%2520the%2520emergence%2520of%2520video%2520foundation%2520models%2520that%2520function%2520not%2520only%2520as%2520visual%2520generators%2520but%2520also%2520as%2520implicit%2520world%2520models%252C%2520models%2520that%2520simulate%2520the%2520physical%2520dynamics%252C%2520agent-environment%2520interactions%252C%2520and%2520task%2520planning%2520that%2520govern%2520real%2520or%2520imagined%2520worlds.%2520This%2520survey%2520provides%2520a%2520systematic%2520overview%2520of%2520this%2520evolution%252C%2520conceptualizing%2520modern%2520video%2520foundation%2520models%2520as%2520the%2520combination%2520of%2520two%2520core%2520components%253A%2520an%2520implicit%2520world%2520model%2520and%2520a%2520video%2520renderer.%2520The%2520world%2520model%2520encodes%2520structured%2520knowledge%2520about%2520the%2520world%252C%2520including%2520physical%2520laws%252C%2520interaction%2520dynamics%252C%2520and%2520agent%2520behavior.%2520It%2520serves%2520as%2520a%2520latent%2520simulation%2520engine%2520that%2520enables%2520coherent%2520visual%2520reasoning%252C%2520long-term%2520temporal%2520consistency%252C%2520and%2520goal-driven%2520planning.%2520The%2520video%2520renderer%2520transforms%2520this%2520latent%2520simulation%2520into%2520realistic%2520visual%2520observations%252C%2520effectively%2520producing%2520videos%2520as%2520a%2520%2522window%2522%2520into%2520the%2520simulated%2520world.%2520We%2520trace%2520the%2520progression%2520of%2520video%2520generation%2520through%2520four%2520generations%252C%2520in%2520which%2520the%2520core%2520capabilities%2520advance%2520step%2520by%2520step%252C%2520ultimately%2520culminating%2520in%2520a%2520world%2520model%252C%2520built%2520upon%2520a%2520video%2520generation%2520model%252C%2520that%2520embodies%2520intrinsic%2520physical%2520plausibility%252C%2520real-time%2520multimodal%2520interaction%252C%2520and%2520planning%2520capabilities%2520spanning%2520multiple%2520spatiotemporal%2520scales.%2520For%2520each%2520generation%252C%2520we%2520define%2520its%2520core%2520characteristics%252C%2520highlight%2520representative%2520works%252C%2520and%2520examine%2520their%2520application%2520domains%2520such%2520as%2520robotics%252C%2520autonomous%2520driving%252C%2520and%2520interactive%2520gaming.%2520Finally%252C%2520we%2520discuss%2520open%2520challenges%2520and%2520design%2520principles%2520for%2520next-generation%2520world%2520models%252C%2520including%2520the%2520role%2520of%2520agent%2520intelligence%2520in%2520shaping%2520and%2520evaluating%2520these%2520systems.%2520An%2520up-to-date%2520list%2520of%2520related%2520works%2520is%2520maintained%2520at%2520this%2520link.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20the%20Visual%20World%20with%20Artificial%20Intelligence%3A%20A%20Roadmap&entry.906535625=Jingtong%20Yue%20and%20Ziqi%20Huang%20and%20Zhaoxi%20Chen%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Ziwei%20Liu&entry.1292438233=The%20landscape%20of%20video%20generation%20is%20shifting%2C%20from%20a%20focus%20on%20generating%20visually%20appealing%20clips%20to%20building%20virtual%20environments%20that%20support%20interaction%20and%20maintain%20physical%20plausibility.%20These%20developments%20point%20toward%20the%20emergence%20of%20video%20foundation%20models%20that%20function%20not%20only%20as%20visual%20generators%20but%20also%20as%20implicit%20world%20models%2C%20models%20that%20simulate%20the%20physical%20dynamics%2C%20agent-environment%20interactions%2C%20and%20task%20planning%20that%20govern%20real%20or%20imagined%20worlds.%20This%20survey%20provides%20a%20systematic%20overview%20of%20this%20evolution%2C%20conceptualizing%20modern%20video%20foundation%20models%20as%20the%20combination%20of%20two%20core%20components%3A%20an%20implicit%20world%20model%20and%20a%20video%20renderer.%20The%20world%20model%20encodes%20structured%20knowledge%20about%20the%20world%2C%20including%20physical%20laws%2C%20interaction%20dynamics%2C%20and%20agent%20behavior.%20It%20serves%20as%20a%20latent%20simulation%20engine%20that%20enables%20coherent%20visual%20reasoning%2C%20long-term%20temporal%20consistency%2C%20and%20goal-driven%20planning.%20The%20video%20renderer%20transforms%20this%20latent%20simulation%20into%20realistic%20visual%20observations%2C%20effectively%20producing%20videos%20as%20a%20%22window%22%20into%20the%20simulated%20world.%20We%20trace%20the%20progression%20of%20video%20generation%20through%20four%20generations%2C%20in%20which%20the%20core%20capabilities%20advance%20step%20by%20step%2C%20ultimately%20culminating%20in%20a%20world%20model%2C%20built%20upon%20a%20video%20generation%20model%2C%20that%20embodies%20intrinsic%20physical%20plausibility%2C%20real-time%20multimodal%20interaction%2C%20and%20planning%20capabilities%20spanning%20multiple%20spatiotemporal%20scales.%20For%20each%20generation%2C%20we%20define%20its%20core%20characteristics%2C%20highlight%20representative%20works%2C%20and%20examine%20their%20application%20domains%20such%20as%20robotics%2C%20autonomous%20driving%2C%20and%20interactive%20gaming.%20Finally%2C%20we%20discuss%20open%20challenges%20and%20design%20principles%20for%20next-generation%20world%20models%2C%20including%20the%20role%20of%20agent%20intelligence%20in%20shaping%20and%20evaluating%20these%20systems.%20An%20up-to-date%20list%20of%20related%20works%20is%20maintained%20at%20this%20link.&entry.1838667208=http%3A//arxiv.org/abs/2511.08585v2&entry.124074799=Read"},
{"title": "Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering", "author": "Nonghai Zhang and Weitao Ma and Zhanyu Ma and Jun Xu and Jiuchong Gao and Jinghua Hao and Renqing He and Jingwen Xu", "abstract": "Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.", "link": "http://arxiv.org/abs/2601.08427v1", "date": "2026-01-13", "relevancy": 2.5349, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Silence%20the%20Judge%3A%20Reinforcement%20Learning%20with%20Self-Verifier%20via%20Latent%20Geometric%20Clustering&body=Title%3A%20Silence%20the%20Judge%3A%20Reinforcement%20Learning%20with%20Self-Verifier%20via%20Latent%20Geometric%20Clustering%0AAuthor%3A%20Nonghai%20Zhang%20and%20Weitao%20Ma%20and%20Zhanyu%20Ma%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He%20and%20Jingwen%20Xu%0AAbstract%3A%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20significantly%20enhances%20the%20reasoning%20performance%20of%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20this%20success%20heavily%20relies%20on%20expensive%20external%20verifiers%20or%20human%20rules.%20Such%20dependency%20not%20only%20leads%20to%20significant%20computational%20costs%20and%20training%20latency%2C%20but%20also%20yields%20sparse%20rewards%20that%20hinder%20optimization%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20Latent-GRPO%2C%20a%20framework%20that%20derives%20intrinsic%20rewards%20directly%20from%20latent%20space%20geometry.%20Crucially%2C%20our%20empirical%20analysis%20reveals%20a%20compelling%20geometric%20property%3A%20terminal%20token%20representations%20of%20correct%20reasoning%20trajectories%20form%20dense%20clusters%20with%20high%20intra-class%20similarity%2C%20whereas%20incorrect%20trajectories%20remain%20scattered%20as%20outliers.%20In%20light%20of%20this%20discovery%2C%20we%20introduce%20the%20Iterative%20Robust%20Centroid%20Estimation%20%28IRCE%29%20algorithm%2C%20which%20generates%20dense%2C%20continuous%20rewards%20by%20mitigating%20magnitude%20fluctuations%20via%20spherical%20projection%20and%20estimating%20a%20robust%20%60%60truth%20centroid%27%27%20through%20iterative%20aggregation.%20Experimental%20results%20on%20multiple%20datasets%20show%20that%20our%20method%20maintains%20model%20performance%20while%20achieving%20a%20training%20speedup%20of%20over%202x%20compared%20to%20baselines.%20Furthermore%2C%20extensive%20results%20demonstrate%20strong%20generalization%20ability%20and%20robustness.%20The%20code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSilence%2520the%2520Judge%253A%2520Reinforcement%2520Learning%2520with%2520Self-Verifier%2520via%2520Latent%2520Geometric%2520Clustering%26entry.906535625%3DNonghai%2520Zhang%2520and%2520Weitao%2520Ma%2520and%2520Zhanyu%2520Ma%2520and%2520Jun%2520Xu%2520and%2520Jiuchong%2520Gao%2520and%2520Jinghua%2520Hao%2520and%2520Renqing%2520He%2520and%2520Jingwen%2520Xu%26entry.1292438233%3DGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520significantly%2520enhances%2520the%2520reasoning%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520this%2520success%2520heavily%2520relies%2520on%2520expensive%2520external%2520verifiers%2520or%2520human%2520rules.%2520Such%2520dependency%2520not%2520only%2520leads%2520to%2520significant%2520computational%2520costs%2520and%2520training%2520latency%252C%2520but%2520also%2520yields%2520sparse%2520rewards%2520that%2520hinder%2520optimization%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Latent-GRPO%252C%2520a%2520framework%2520that%2520derives%2520intrinsic%2520rewards%2520directly%2520from%2520latent%2520space%2520geometry.%2520Crucially%252C%2520our%2520empirical%2520analysis%2520reveals%2520a%2520compelling%2520geometric%2520property%253A%2520terminal%2520token%2520representations%2520of%2520correct%2520reasoning%2520trajectories%2520form%2520dense%2520clusters%2520with%2520high%2520intra-class%2520similarity%252C%2520whereas%2520incorrect%2520trajectories%2520remain%2520scattered%2520as%2520outliers.%2520In%2520light%2520of%2520this%2520discovery%252C%2520we%2520introduce%2520the%2520Iterative%2520Robust%2520Centroid%2520Estimation%2520%2528IRCE%2529%2520algorithm%252C%2520which%2520generates%2520dense%252C%2520continuous%2520rewards%2520by%2520mitigating%2520magnitude%2520fluctuations%2520via%2520spherical%2520projection%2520and%2520estimating%2520a%2520robust%2520%2560%2560truth%2520centroid%2527%2527%2520through%2520iterative%2520aggregation.%2520Experimental%2520results%2520on%2520multiple%2520datasets%2520show%2520that%2520our%2520method%2520maintains%2520model%2520performance%2520while%2520achieving%2520a%2520training%2520speedup%2520of%2520over%25202x%2520compared%2520to%2520baselines.%2520Furthermore%252C%2520extensive%2520results%2520demonstrate%2520strong%2520generalization%2520ability%2520and%2520robustness.%2520The%2520code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Silence%20the%20Judge%3A%20Reinforcement%20Learning%20with%20Self-Verifier%20via%20Latent%20Geometric%20Clustering&entry.906535625=Nonghai%20Zhang%20and%20Weitao%20Ma%20and%20Zhanyu%20Ma%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He%20and%20Jingwen%20Xu&entry.1292438233=Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20significantly%20enhances%20the%20reasoning%20performance%20of%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20this%20success%20heavily%20relies%20on%20expensive%20external%20verifiers%20or%20human%20rules.%20Such%20dependency%20not%20only%20leads%20to%20significant%20computational%20costs%20and%20training%20latency%2C%20but%20also%20yields%20sparse%20rewards%20that%20hinder%20optimization%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20Latent-GRPO%2C%20a%20framework%20that%20derives%20intrinsic%20rewards%20directly%20from%20latent%20space%20geometry.%20Crucially%2C%20our%20empirical%20analysis%20reveals%20a%20compelling%20geometric%20property%3A%20terminal%20token%20representations%20of%20correct%20reasoning%20trajectories%20form%20dense%20clusters%20with%20high%20intra-class%20similarity%2C%20whereas%20incorrect%20trajectories%20remain%20scattered%20as%20outliers.%20In%20light%20of%20this%20discovery%2C%20we%20introduce%20the%20Iterative%20Robust%20Centroid%20Estimation%20%28IRCE%29%20algorithm%2C%20which%20generates%20dense%2C%20continuous%20rewards%20by%20mitigating%20magnitude%20fluctuations%20via%20spherical%20projection%20and%20estimating%20a%20robust%20%60%60truth%20centroid%27%27%20through%20iterative%20aggregation.%20Experimental%20results%20on%20multiple%20datasets%20show%20that%20our%20method%20maintains%20model%20performance%20while%20achieving%20a%20training%20speedup%20of%20over%202x%20compared%20to%20baselines.%20Furthermore%2C%20extensive%20results%20demonstrate%20strong%20generalization%20ability%20and%20robustness.%20The%20code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2601.08427v1&entry.124074799=Read"},
{"title": "Reasoning Matters for 3D Visual Grounding", "author": "Hsiang-Wei Huang and Kuang-Ming Chen and Wenhao Chai and Cheng-Yen Yang and Jen-Hao Cheng and Jenq-Neng Hwang", "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.", "link": "http://arxiv.org/abs/2601.08811v1", "date": "2026-01-13", "relevancy": 2.5051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6417}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Matters%20for%203D%20Visual%20Grounding&body=Title%3A%20Reasoning%20Matters%20for%203D%20Visual%20Grounding%0AAuthor%3A%20Hsiang-Wei%20Huang%20and%20Kuang-Ming%20Chen%20and%20Wenhao%20Chai%20and%20Cheng-Yen%20Yang%20and%20Jen-Hao%20Cheng%20and%20Jenq-Neng%20Hwang%0AAbstract%3A%20The%20recent%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20strong%20reasoning%20ability%20has%20driven%20research%20in%20various%20domains%20such%20as%20mathematics%2C%20coding%2C%20and%20scientific%20discovery.%20Meanwhile%2C%203D%20visual%20grounding%2C%20as%20a%20fundamental%20task%20in%203D%20understanding%2C%20still%20remains%20challenging%20due%20to%20the%20limited%20reasoning%20ability%20of%20recent%203D%20visual%20grounding%20models.%20Most%20of%20the%20current%20methods%20incorporate%20a%20text%20encoder%20and%20visual%20feature%20encoder%20to%20generate%20cross-modal%20fuse%20features%20and%20predict%20the%20referring%20object.%20These%20models%20often%20require%20supervised%20training%20on%20extensive%203D%20annotation%20data.%20On%20the%20other%20hand%2C%20recent%20research%20also%20focus%20on%20scaling%20synthetic%20data%20to%20train%20stronger%203D%20visual%20grounding%20LLM%2C%20however%2C%20the%20performance%20gain%20remains%20limited%20and%20non-proportional%20to%20the%20data%20collection%20cost.%20In%20this%20work%2C%20we%20propose%20a%203D%20visual%20grounding%20data%20pipeline%2C%20which%20is%20capable%20of%20automatically%20synthesizing%203D%20visual%20grounding%20data%20along%20with%20corresponding%20reasoning%20process.%20Additionally%2C%20we%20leverage%20the%20generated%20data%20for%20LLM%20fine-tuning%20and%20introduce%20Reason3DVG-8B%2C%20a%20strong%203D%20visual%20grounding%20LLM%20that%20outperforms%20previous%20LLM-based%20method%203D-GRAND%20using%20only%201.6%25%20of%20their%20training%20data%2C%20demonstrating%20the%20effectiveness%20of%20our%20data%20and%20the%20importance%20of%20reasoning%20in%203D%20visual%20grounding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Matters%2520for%25203D%2520Visual%2520Grounding%26entry.906535625%3DHsiang-Wei%2520Huang%2520and%2520Kuang-Ming%2520Chen%2520and%2520Wenhao%2520Chai%2520and%2520Cheng-Yen%2520Yang%2520and%2520Jen-Hao%2520Cheng%2520and%2520Jenq-Neng%2520Hwang%26entry.1292438233%3DThe%2520recent%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520strong%2520reasoning%2520ability%2520has%2520driven%2520research%2520in%2520various%2520domains%2520such%2520as%2520mathematics%252C%2520coding%252C%2520and%2520scientific%2520discovery.%2520Meanwhile%252C%25203D%2520visual%2520grounding%252C%2520as%2520a%2520fundamental%2520task%2520in%25203D%2520understanding%252C%2520still%2520remains%2520challenging%2520due%2520to%2520the%2520limited%2520reasoning%2520ability%2520of%2520recent%25203D%2520visual%2520grounding%2520models.%2520Most%2520of%2520the%2520current%2520methods%2520incorporate%2520a%2520text%2520encoder%2520and%2520visual%2520feature%2520encoder%2520to%2520generate%2520cross-modal%2520fuse%2520features%2520and%2520predict%2520the%2520referring%2520object.%2520These%2520models%2520often%2520require%2520supervised%2520training%2520on%2520extensive%25203D%2520annotation%2520data.%2520On%2520the%2520other%2520hand%252C%2520recent%2520research%2520also%2520focus%2520on%2520scaling%2520synthetic%2520data%2520to%2520train%2520stronger%25203D%2520visual%2520grounding%2520LLM%252C%2520however%252C%2520the%2520performance%2520gain%2520remains%2520limited%2520and%2520non-proportional%2520to%2520the%2520data%2520collection%2520cost.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%25203D%2520visual%2520grounding%2520data%2520pipeline%252C%2520which%2520is%2520capable%2520of%2520automatically%2520synthesizing%25203D%2520visual%2520grounding%2520data%2520along%2520with%2520corresponding%2520reasoning%2520process.%2520Additionally%252C%2520we%2520leverage%2520the%2520generated%2520data%2520for%2520LLM%2520fine-tuning%2520and%2520introduce%2520Reason3DVG-8B%252C%2520a%2520strong%25203D%2520visual%2520grounding%2520LLM%2520that%2520outperforms%2520previous%2520LLM-based%2520method%25203D-GRAND%2520using%2520only%25201.6%2525%2520of%2520their%2520training%2520data%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520data%2520and%2520the%2520importance%2520of%2520reasoning%2520in%25203D%2520visual%2520grounding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Matters%20for%203D%20Visual%20Grounding&entry.906535625=Hsiang-Wei%20Huang%20and%20Kuang-Ming%20Chen%20and%20Wenhao%20Chai%20and%20Cheng-Yen%20Yang%20and%20Jen-Hao%20Cheng%20and%20Jenq-Neng%20Hwang&entry.1292438233=The%20recent%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20strong%20reasoning%20ability%20has%20driven%20research%20in%20various%20domains%20such%20as%20mathematics%2C%20coding%2C%20and%20scientific%20discovery.%20Meanwhile%2C%203D%20visual%20grounding%2C%20as%20a%20fundamental%20task%20in%203D%20understanding%2C%20still%20remains%20challenging%20due%20to%20the%20limited%20reasoning%20ability%20of%20recent%203D%20visual%20grounding%20models.%20Most%20of%20the%20current%20methods%20incorporate%20a%20text%20encoder%20and%20visual%20feature%20encoder%20to%20generate%20cross-modal%20fuse%20features%20and%20predict%20the%20referring%20object.%20These%20models%20often%20require%20supervised%20training%20on%20extensive%203D%20annotation%20data.%20On%20the%20other%20hand%2C%20recent%20research%20also%20focus%20on%20scaling%20synthetic%20data%20to%20train%20stronger%203D%20visual%20grounding%20LLM%2C%20however%2C%20the%20performance%20gain%20remains%20limited%20and%20non-proportional%20to%20the%20data%20collection%20cost.%20In%20this%20work%2C%20we%20propose%20a%203D%20visual%20grounding%20data%20pipeline%2C%20which%20is%20capable%20of%20automatically%20synthesizing%203D%20visual%20grounding%20data%20along%20with%20corresponding%20reasoning%20process.%20Additionally%2C%20we%20leverage%20the%20generated%20data%20for%20LLM%20fine-tuning%20and%20introduce%20Reason3DVG-8B%2C%20a%20strong%203D%20visual%20grounding%20LLM%20that%20outperforms%20previous%20LLM-based%20method%203D-GRAND%20using%20only%201.6%25%20of%20their%20training%20data%2C%20demonstrating%20the%20effectiveness%20of%20our%20data%20and%20the%20importance%20of%20reasoning%20in%203D%20visual%20grounding.&entry.1838667208=http%3A//arxiv.org/abs/2601.08811v1&entry.124074799=Read"},
{"title": "Multi-Personality Generation of LLMs at Decoding-time", "author": "Rongxin Chen and Yunfan Li and Yige Yuan and Bingbing Xu and Huawei Shen", "abstract": "Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a \"free lunch\" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .", "link": "http://arxiv.org/abs/2511.01891v3", "date": "2026-01-13", "relevancy": 2.4735, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5005}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4954}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Personality%20Generation%20of%20LLMs%20at%20Decoding-time&body=Title%3A%20Multi-Personality%20Generation%20of%20LLMs%20at%20Decoding-time%0AAuthor%3A%20Rongxin%20Chen%20and%20Yunfan%20Li%20and%20Yige%20Yuan%20and%20Bingbing%20Xu%20and%20Huawei%20Shen%0AAbstract%3A%20Multi-personality%20generation%20for%20LLMs%2C%20enabling%20simultaneous%20embodiment%20of%20multiple%20personalization%20attributes%2C%20is%20a%20fundamental%20challenge.%20Existing%20retraining-based%20approaches%20are%20costly%20and%20poorly%20scalable%2C%20while%20decoding-time%20methods%20often%20rely%20on%20external%20models%20or%20heuristics%2C%20limiting%20flexibility%20and%20robustness.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Multi-Personality%20Generation%20%28MPG%29%20framework%20under%20the%20decoding-time%20combination%20paradigm.%20It%20flexibly%20controls%20multi-personality%20without%20relying%20on%20scarce%20multi-dimensional%20models%20or%20extra%20training%2C%20leveraging%20implicit%20density%20ratios%20in%20single-dimensional%20models%20as%20a%20%22free%20lunch%22%20to%20reformulate%20the%20task%20as%20sampling%20from%20a%20target%20strategy%20aggregating%20these%20ratios.%20To%20implement%20MPG%20efficiently%2C%20we%20design%20Speculative%20Chunk-level%20based%20Rejection%20sampling%20%28SCR%29%2C%20which%20generates%20responses%20in%20chunks%20and%20parallelly%20validates%20them%20via%20estimated%20thresholds%20within%20a%20sliding%20window.%20This%20significantly%20reduces%20computational%20overhead%20while%20maintaining%20high-quality%20generation.%20Experiments%20on%20MBTI%20personality%20and%20Role-Playing%20demonstrate%20the%20effectiveness%20of%20MPG%2C%20showing%20improvements%20up%20to%2016%25-18%25.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/Libra117/MPG%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01891v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Personality%2520Generation%2520of%2520LLMs%2520at%2520Decoding-time%26entry.906535625%3DRongxin%2520Chen%2520and%2520Yunfan%2520Li%2520and%2520Yige%2520Yuan%2520and%2520Bingbing%2520Xu%2520and%2520Huawei%2520Shen%26entry.1292438233%3DMulti-personality%2520generation%2520for%2520LLMs%252C%2520enabling%2520simultaneous%2520embodiment%2520of%2520multiple%2520personalization%2520attributes%252C%2520is%2520a%2520fundamental%2520challenge.%2520Existing%2520retraining-based%2520approaches%2520are%2520costly%2520and%2520poorly%2520scalable%252C%2520while%2520decoding-time%2520methods%2520often%2520rely%2520on%2520external%2520models%2520or%2520heuristics%252C%2520limiting%2520flexibility%2520and%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Multi-Personality%2520Generation%2520%2528MPG%2529%2520framework%2520under%2520the%2520decoding-time%2520combination%2520paradigm.%2520It%2520flexibly%2520controls%2520multi-personality%2520without%2520relying%2520on%2520scarce%2520multi-dimensional%2520models%2520or%2520extra%2520training%252C%2520leveraging%2520implicit%2520density%2520ratios%2520in%2520single-dimensional%2520models%2520as%2520a%2520%2522free%2520lunch%2522%2520to%2520reformulate%2520the%2520task%2520as%2520sampling%2520from%2520a%2520target%2520strategy%2520aggregating%2520these%2520ratios.%2520To%2520implement%2520MPG%2520efficiently%252C%2520we%2520design%2520Speculative%2520Chunk-level%2520based%2520Rejection%2520sampling%2520%2528SCR%2529%252C%2520which%2520generates%2520responses%2520in%2520chunks%2520and%2520parallelly%2520validates%2520them%2520via%2520estimated%2520thresholds%2520within%2520a%2520sliding%2520window.%2520This%2520significantly%2520reduces%2520computational%2520overhead%2520while%2520maintaining%2520high-quality%2520generation.%2520Experiments%2520on%2520MBTI%2520personality%2520and%2520Role-Playing%2520demonstrate%2520the%2520effectiveness%2520of%2520MPG%252C%2520showing%2520improvements%2520up%2520to%252016%2525-18%2525.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/Libra117/MPG%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01891v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Personality%20Generation%20of%20LLMs%20at%20Decoding-time&entry.906535625=Rongxin%20Chen%20and%20Yunfan%20Li%20and%20Yige%20Yuan%20and%20Bingbing%20Xu%20and%20Huawei%20Shen&entry.1292438233=Multi-personality%20generation%20for%20LLMs%2C%20enabling%20simultaneous%20embodiment%20of%20multiple%20personalization%20attributes%2C%20is%20a%20fundamental%20challenge.%20Existing%20retraining-based%20approaches%20are%20costly%20and%20poorly%20scalable%2C%20while%20decoding-time%20methods%20often%20rely%20on%20external%20models%20or%20heuristics%2C%20limiting%20flexibility%20and%20robustness.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Multi-Personality%20Generation%20%28MPG%29%20framework%20under%20the%20decoding-time%20combination%20paradigm.%20It%20flexibly%20controls%20multi-personality%20without%20relying%20on%20scarce%20multi-dimensional%20models%20or%20extra%20training%2C%20leveraging%20implicit%20density%20ratios%20in%20single-dimensional%20models%20as%20a%20%22free%20lunch%22%20to%20reformulate%20the%20task%20as%20sampling%20from%20a%20target%20strategy%20aggregating%20these%20ratios.%20To%20implement%20MPG%20efficiently%2C%20we%20design%20Speculative%20Chunk-level%20based%20Rejection%20sampling%20%28SCR%29%2C%20which%20generates%20responses%20in%20chunks%20and%20parallelly%20validates%20them%20via%20estimated%20thresholds%20within%20a%20sliding%20window.%20This%20significantly%20reduces%20computational%20overhead%20while%20maintaining%20high-quality%20generation.%20Experiments%20on%20MBTI%20personality%20and%20Role-Playing%20demonstrate%20the%20effectiveness%20of%20MPG%2C%20showing%20improvements%20up%20to%2016%25-18%25.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/Libra117/MPG%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.01891v3&entry.124074799=Read"},
{"title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation", "author": "Giulio Corallo and Paolo Papotti", "abstract": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.", "link": "http://arxiv.org/abs/2601.08670v1", "date": "2026-01-13", "relevancy": 2.4411, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4957}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Context-of-Experts%20Decoding%20for%20Retrieval%20Augmented%20Generation&body=Title%3A%20Parallel%20Context-of-Experts%20Decoding%20for%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Giulio%20Corallo%20and%20Paolo%20Papotti%0AAbstract%3A%20Retrieval%20Augmented%20Generation%20faces%20a%20trade-off%3A%20concatenating%20documents%20in%20a%20long%20prompt%20enables%20multi-document%20reasoning%20but%20creates%20prefill%20bottlenecks%2C%20while%20encoding%20document%20KV%20caches%20separately%20offers%20speed%20but%20breaks%20cross-document%20interaction.%20We%20propose%20Parallel%20Context-of-Experts%20Decoding%20%28Pced%29%2C%20a%20training-free%20framework%20that%20shifts%20evidence%20aggregation%20from%20the%20attention%20mechanism%20to%20the%20decoding.%20Pced%20treats%20retrieved%20documents%20as%20isolated%20%22experts%22%2C%20synchronizing%20their%20predictions%20via%20a%20novel%20retrieval-aware%20contrastive%20decoding%20rule%20that%20weighs%20expert%20logits%20against%20the%20model%20prior.%20This%20approach%20recovers%20cross-document%20reasoning%20capabilities%20without%20constructing%20a%20shared%20attention%20across%20documents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Context-of-Experts%2520Decoding%2520for%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DGiulio%2520Corallo%2520and%2520Paolo%2520Papotti%26entry.1292438233%3DRetrieval%2520Augmented%2520Generation%2520faces%2520a%2520trade-off%253A%2520concatenating%2520documents%2520in%2520a%2520long%2520prompt%2520enables%2520multi-document%2520reasoning%2520but%2520creates%2520prefill%2520bottlenecks%252C%2520while%2520encoding%2520document%2520KV%2520caches%2520separately%2520offers%2520speed%2520but%2520breaks%2520cross-document%2520interaction.%2520We%2520propose%2520Parallel%2520Context-of-Experts%2520Decoding%2520%2528Pced%2529%252C%2520a%2520training-free%2520framework%2520that%2520shifts%2520evidence%2520aggregation%2520from%2520the%2520attention%2520mechanism%2520to%2520the%2520decoding.%2520Pced%2520treats%2520retrieved%2520documents%2520as%2520isolated%2520%2522experts%2522%252C%2520synchronizing%2520their%2520predictions%2520via%2520a%2520novel%2520retrieval-aware%2520contrastive%2520decoding%2520rule%2520that%2520weighs%2520expert%2520logits%2520against%2520the%2520model%2520prior.%2520This%2520approach%2520recovers%2520cross-document%2520reasoning%2520capabilities%2520without%2520constructing%2520a%2520shared%2520attention%2520across%2520documents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Context-of-Experts%20Decoding%20for%20Retrieval%20Augmented%20Generation&entry.906535625=Giulio%20Corallo%20and%20Paolo%20Papotti&entry.1292438233=Retrieval%20Augmented%20Generation%20faces%20a%20trade-off%3A%20concatenating%20documents%20in%20a%20long%20prompt%20enables%20multi-document%20reasoning%20but%20creates%20prefill%20bottlenecks%2C%20while%20encoding%20document%20KV%20caches%20separately%20offers%20speed%20but%20breaks%20cross-document%20interaction.%20We%20propose%20Parallel%20Context-of-Experts%20Decoding%20%28Pced%29%2C%20a%20training-free%20framework%20that%20shifts%20evidence%20aggregation%20from%20the%20attention%20mechanism%20to%20the%20decoding.%20Pced%20treats%20retrieved%20documents%20as%20isolated%20%22experts%22%2C%20synchronizing%20their%20predictions%20via%20a%20novel%20retrieval-aware%20contrastive%20decoding%20rule%20that%20weighs%20expert%20logits%20against%20the%20model%20prior.%20This%20approach%20recovers%20cross-document%20reasoning%20capabilities%20without%20constructing%20a%20shared%20attention%20across%20documents.&entry.1838667208=http%3A//arxiv.org/abs/2601.08670v1&entry.124074799=Read"},
{"title": "Explaning with trees: interpreting CNNs using hierarchies", "author": "Caroline Mazini Rodrigues and Nicolas Boutry and Laurent Najman", "abstract": "Challenges persist in providing interpretable explanations for neural network reasoning in explainable AI (xAI). Existing methods like Integrated Gradients produce noisy maps, and LIME, while intuitive, may deviate from the model's reasoning. We introduce a framework that uses hierarchical segmentation techniques for faithful and interpretable explanations of Convolutional Neural Networks (CNNs). Our method constructs model-based hierarchical segmentations that maintain the model's reasoning fidelity and allows both human-centric and model-centric segmentation. This approach offers multiscale explanations, aiding bias identification and enhancing understanding of neural network decision-making. Experiments show that our framework, xAiTrees, delivers highly interpretable and faithful model explanations, not only surpassing traditional xAI methods but shedding new light on a novel approach to enhancing xAI interpretability.", "link": "http://arxiv.org/abs/2406.13257v2", "date": "2026-01-13", "relevancy": 2.4205, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaning%20with%20trees%3A%20interpreting%20CNNs%20using%20hierarchies&body=Title%3A%20Explaning%20with%20trees%3A%20interpreting%20CNNs%20using%20hierarchies%0AAuthor%3A%20Caroline%20Mazini%20Rodrigues%20and%20Nicolas%20Boutry%20and%20Laurent%20Najman%0AAbstract%3A%20Challenges%20persist%20in%20providing%20interpretable%20explanations%20for%20neural%20network%20reasoning%20in%20explainable%20AI%20%28xAI%29.%20Existing%20methods%20like%20Integrated%20Gradients%20produce%20noisy%20maps%2C%20and%20LIME%2C%20while%20intuitive%2C%20may%20deviate%20from%20the%20model%27s%20reasoning.%20We%20introduce%20a%20framework%20that%20uses%20hierarchical%20segmentation%20techniques%20for%20faithful%20and%20interpretable%20explanations%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20Our%20method%20constructs%20model-based%20hierarchical%20segmentations%20that%20maintain%20the%20model%27s%20reasoning%20fidelity%20and%20allows%20both%20human-centric%20and%20model-centric%20segmentation.%20This%20approach%20offers%20multiscale%20explanations%2C%20aiding%20bias%20identification%20and%20enhancing%20understanding%20of%20neural%20network%20decision-making.%20Experiments%20show%20that%20our%20framework%2C%20xAiTrees%2C%20delivers%20highly%20interpretable%20and%20faithful%20model%20explanations%2C%20not%20only%20surpassing%20traditional%20xAI%20methods%20but%20shedding%20new%20light%20on%20a%20novel%20approach%20to%20enhancing%20xAI%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2406.13257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaning%2520with%2520trees%253A%2520interpreting%2520CNNs%2520using%2520hierarchies%26entry.906535625%3DCaroline%2520Mazini%2520Rodrigues%2520and%2520Nicolas%2520Boutry%2520and%2520Laurent%2520Najman%26entry.1292438233%3DChallenges%2520persist%2520in%2520providing%2520interpretable%2520explanations%2520for%2520neural%2520network%2520reasoning%2520in%2520explainable%2520AI%2520%2528xAI%2529.%2520Existing%2520methods%2520like%2520Integrated%2520Gradients%2520produce%2520noisy%2520maps%252C%2520and%2520LIME%252C%2520while%2520intuitive%252C%2520may%2520deviate%2520from%2520the%2520model%2527s%2520reasoning.%2520We%2520introduce%2520a%2520framework%2520that%2520uses%2520hierarchical%2520segmentation%2520techniques%2520for%2520faithful%2520and%2520interpretable%2520explanations%2520of%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529.%2520Our%2520method%2520constructs%2520model-based%2520hierarchical%2520segmentations%2520that%2520maintain%2520the%2520model%2527s%2520reasoning%2520fidelity%2520and%2520allows%2520both%2520human-centric%2520and%2520model-centric%2520segmentation.%2520This%2520approach%2520offers%2520multiscale%2520explanations%252C%2520aiding%2520bias%2520identification%2520and%2520enhancing%2520understanding%2520of%2520neural%2520network%2520decision-making.%2520Experiments%2520show%2520that%2520our%2520framework%252C%2520xAiTrees%252C%2520delivers%2520highly%2520interpretable%2520and%2520faithful%2520model%2520explanations%252C%2520not%2520only%2520surpassing%2520traditional%2520xAI%2520methods%2520but%2520shedding%2520new%2520light%2520on%2520a%2520novel%2520approach%2520to%2520enhancing%2520xAI%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaning%20with%20trees%3A%20interpreting%20CNNs%20using%20hierarchies&entry.906535625=Caroline%20Mazini%20Rodrigues%20and%20Nicolas%20Boutry%20and%20Laurent%20Najman&entry.1292438233=Challenges%20persist%20in%20providing%20interpretable%20explanations%20for%20neural%20network%20reasoning%20in%20explainable%20AI%20%28xAI%29.%20Existing%20methods%20like%20Integrated%20Gradients%20produce%20noisy%20maps%2C%20and%20LIME%2C%20while%20intuitive%2C%20may%20deviate%20from%20the%20model%27s%20reasoning.%20We%20introduce%20a%20framework%20that%20uses%20hierarchical%20segmentation%20techniques%20for%20faithful%20and%20interpretable%20explanations%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20Our%20method%20constructs%20model-based%20hierarchical%20segmentations%20that%20maintain%20the%20model%27s%20reasoning%20fidelity%20and%20allows%20both%20human-centric%20and%20model-centric%20segmentation.%20This%20approach%20offers%20multiscale%20explanations%2C%20aiding%20bias%20identification%20and%20enhancing%20understanding%20of%20neural%20network%20decision-making.%20Experiments%20show%20that%20our%20framework%2C%20xAiTrees%2C%20delivers%20highly%20interpretable%20and%20faithful%20model%20explanations%2C%20not%20only%20surpassing%20traditional%20xAI%20methods%20but%20shedding%20new%20light%20on%20a%20novel%20approach%20to%20enhancing%20xAI%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2406.13257v2&entry.124074799=Read"},
{"title": "HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction", "author": "Hongli Chen and Pengcheng Fang and Yuxia Chen and Yingxuan Ren and Jing Hao and Fangfang Tang and Xiaohao Cai and Shanshan Shan and Feng Liu", "abstract": "Reconstructing high-fidelity MR images from undersampled k-space data remains a challenging problem in MRI. While Mamba variants for vision tasks offer promising long-range modeling capabilities with linear-time complexity, their direct application to MRI reconstruction inherits two key limitations: (1) insensitivity to high-frequency anatomical details; and (2) reliance on redundant multi-directional scanning. To address these limitations, we introduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based architecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks. Specifically, the WL block performs fidelity-preserving spectral decoupling, producing complementary low- and high-frequency streams. This separation enables the HiFi-Mamba block to focus on low-frequency structures, enhancing global feature modeling. Concurrently, the HiFi-Mamba block selectively integrates high-frequency features through adaptive state-space modulation, preserving comprehensive spectral details. To eliminate the scanning redundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal strategy that preserves long-range modeling capability with improved computational efficiency. Extensive experiments on standard MRI reconstruction benchmarks demonstrate that HiFi-Mamba consistently outperforms state-of-the-art CNN-based, Transformer-based, and other Mamba-based models in reconstruction accuracy while maintaining a compact and efficient model design.", "link": "http://arxiv.org/abs/2508.09179v2", "date": "2026-01-13", "relevancy": 2.4014, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4908}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4802}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFi-Mamba%3A%20Dual-Stream%20W-Laplacian%20Enhanced%20Mamba%20for%20High-Fidelity%20MRI%20Reconstruction&body=Title%3A%20HiFi-Mamba%3A%20Dual-Stream%20W-Laplacian%20Enhanced%20Mamba%20for%20High-Fidelity%20MRI%20Reconstruction%0AAuthor%3A%20Hongli%20Chen%20and%20Pengcheng%20Fang%20and%20Yuxia%20Chen%20and%20Yingxuan%20Ren%20and%20Jing%20Hao%20and%20Fangfang%20Tang%20and%20Xiaohao%20Cai%20and%20Shanshan%20Shan%20and%20Feng%20Liu%0AAbstract%3A%20Reconstructing%20high-fidelity%20MR%20images%20from%20undersampled%20k-space%20data%20remains%20a%20challenging%20problem%20in%20MRI.%20While%20Mamba%20variants%20for%20vision%20tasks%20offer%20promising%20long-range%20modeling%20capabilities%20with%20linear-time%20complexity%2C%20their%20direct%20application%20to%20MRI%20reconstruction%20inherits%20two%20key%20limitations%3A%20%281%29%20insensitivity%20to%20high-frequency%20anatomical%20details%3B%20and%20%282%29%20reliance%20on%20redundant%20multi-directional%20scanning.%20To%20address%20these%20limitations%2C%20we%20introduce%20High-Fidelity%20Mamba%20%28HiFi-Mamba%29%2C%20a%20novel%20dual-stream%20Mamba-based%20architecture%20comprising%20stacked%20W-Laplacian%20%28WL%29%20and%20HiFi-Mamba%20blocks.%20Specifically%2C%20the%20WL%20block%20performs%20fidelity-preserving%20spectral%20decoupling%2C%20producing%20complementary%20low-%20and%20high-frequency%20streams.%20This%20separation%20enables%20the%20HiFi-Mamba%20block%20to%20focus%20on%20low-frequency%20structures%2C%20enhancing%20global%20feature%20modeling.%20Concurrently%2C%20the%20HiFi-Mamba%20block%20selectively%20integrates%20high-frequency%20features%20through%20adaptive%20state-space%20modulation%2C%20preserving%20comprehensive%20spectral%20details.%20To%20eliminate%20the%20scanning%20redundancy%2C%20the%20HiFi-Mamba%20block%20adopts%20a%20streamlined%20unidirectional%20traversal%20strategy%20that%20preserves%20long-range%20modeling%20capability%20with%20improved%20computational%20efficiency.%20Extensive%20experiments%20on%20standard%20MRI%20reconstruction%20benchmarks%20demonstrate%20that%20HiFi-Mamba%20consistently%20outperforms%20state-of-the-art%20CNN-based%2C%20Transformer-based%2C%20and%20other%20Mamba-based%20models%20in%20reconstruction%20accuracy%20while%20maintaining%20a%20compact%20and%20efficient%20model%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFi-Mamba%253A%2520Dual-Stream%2520W-Laplacian%2520Enhanced%2520Mamba%2520for%2520High-Fidelity%2520MRI%2520Reconstruction%26entry.906535625%3DHongli%2520Chen%2520and%2520Pengcheng%2520Fang%2520and%2520Yuxia%2520Chen%2520and%2520Yingxuan%2520Ren%2520and%2520Jing%2520Hao%2520and%2520Fangfang%2520Tang%2520and%2520Xiaohao%2520Cai%2520and%2520Shanshan%2520Shan%2520and%2520Feng%2520Liu%26entry.1292438233%3DReconstructing%2520high-fidelity%2520MR%2520images%2520from%2520undersampled%2520k-space%2520data%2520remains%2520a%2520challenging%2520problem%2520in%2520MRI.%2520While%2520Mamba%2520variants%2520for%2520vision%2520tasks%2520offer%2520promising%2520long-range%2520modeling%2520capabilities%2520with%2520linear-time%2520complexity%252C%2520their%2520direct%2520application%2520to%2520MRI%2520reconstruction%2520inherits%2520two%2520key%2520limitations%253A%2520%25281%2529%2520insensitivity%2520to%2520high-frequency%2520anatomical%2520details%253B%2520and%2520%25282%2529%2520reliance%2520on%2520redundant%2520multi-directional%2520scanning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520High-Fidelity%2520Mamba%2520%2528HiFi-Mamba%2529%252C%2520a%2520novel%2520dual-stream%2520Mamba-based%2520architecture%2520comprising%2520stacked%2520W-Laplacian%2520%2528WL%2529%2520and%2520HiFi-Mamba%2520blocks.%2520Specifically%252C%2520the%2520WL%2520block%2520performs%2520fidelity-preserving%2520spectral%2520decoupling%252C%2520producing%2520complementary%2520low-%2520and%2520high-frequency%2520streams.%2520This%2520separation%2520enables%2520the%2520HiFi-Mamba%2520block%2520to%2520focus%2520on%2520low-frequency%2520structures%252C%2520enhancing%2520global%2520feature%2520modeling.%2520Concurrently%252C%2520the%2520HiFi-Mamba%2520block%2520selectively%2520integrates%2520high-frequency%2520features%2520through%2520adaptive%2520state-space%2520modulation%252C%2520preserving%2520comprehensive%2520spectral%2520details.%2520To%2520eliminate%2520the%2520scanning%2520redundancy%252C%2520the%2520HiFi-Mamba%2520block%2520adopts%2520a%2520streamlined%2520unidirectional%2520traversal%2520strategy%2520that%2520preserves%2520long-range%2520modeling%2520capability%2520with%2520improved%2520computational%2520efficiency.%2520Extensive%2520experiments%2520on%2520standard%2520MRI%2520reconstruction%2520benchmarks%2520demonstrate%2520that%2520HiFi-Mamba%2520consistently%2520outperforms%2520state-of-the-art%2520CNN-based%252C%2520Transformer-based%252C%2520and%2520other%2520Mamba-based%2520models%2520in%2520reconstruction%2520accuracy%2520while%2520maintaining%2520a%2520compact%2520and%2520efficient%2520model%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-Mamba%3A%20Dual-Stream%20W-Laplacian%20Enhanced%20Mamba%20for%20High-Fidelity%20MRI%20Reconstruction&entry.906535625=Hongli%20Chen%20and%20Pengcheng%20Fang%20and%20Yuxia%20Chen%20and%20Yingxuan%20Ren%20and%20Jing%20Hao%20and%20Fangfang%20Tang%20and%20Xiaohao%20Cai%20and%20Shanshan%20Shan%20and%20Feng%20Liu&entry.1292438233=Reconstructing%20high-fidelity%20MR%20images%20from%20undersampled%20k-space%20data%20remains%20a%20challenging%20problem%20in%20MRI.%20While%20Mamba%20variants%20for%20vision%20tasks%20offer%20promising%20long-range%20modeling%20capabilities%20with%20linear-time%20complexity%2C%20their%20direct%20application%20to%20MRI%20reconstruction%20inherits%20two%20key%20limitations%3A%20%281%29%20insensitivity%20to%20high-frequency%20anatomical%20details%3B%20and%20%282%29%20reliance%20on%20redundant%20multi-directional%20scanning.%20To%20address%20these%20limitations%2C%20we%20introduce%20High-Fidelity%20Mamba%20%28HiFi-Mamba%29%2C%20a%20novel%20dual-stream%20Mamba-based%20architecture%20comprising%20stacked%20W-Laplacian%20%28WL%29%20and%20HiFi-Mamba%20blocks.%20Specifically%2C%20the%20WL%20block%20performs%20fidelity-preserving%20spectral%20decoupling%2C%20producing%20complementary%20low-%20and%20high-frequency%20streams.%20This%20separation%20enables%20the%20HiFi-Mamba%20block%20to%20focus%20on%20low-frequency%20structures%2C%20enhancing%20global%20feature%20modeling.%20Concurrently%2C%20the%20HiFi-Mamba%20block%20selectively%20integrates%20high-frequency%20features%20through%20adaptive%20state-space%20modulation%2C%20preserving%20comprehensive%20spectral%20details.%20To%20eliminate%20the%20scanning%20redundancy%2C%20the%20HiFi-Mamba%20block%20adopts%20a%20streamlined%20unidirectional%20traversal%20strategy%20that%20preserves%20long-range%20modeling%20capability%20with%20improved%20computational%20efficiency.%20Extensive%20experiments%20on%20standard%20MRI%20reconstruction%20benchmarks%20demonstrate%20that%20HiFi-Mamba%20consistently%20outperforms%20state-of-the-art%20CNN-based%2C%20Transformer-based%2C%20and%20other%20Mamba-based%20models%20in%20reconstruction%20accuracy%20while%20maintaining%20a%20compact%20and%20efficient%20model%20design.&entry.1838667208=http%3A//arxiv.org/abs/2508.09179v2&entry.124074799=Read"},
{"title": "CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading", "author": "Mishan Aliev and Dmitry Baranchuk and Kirill Struminsky", "abstract": "This work investigates text-to-texture synthesis using diffusion models to generate physically-based texture maps. We aim to achieve realistic model appearances under varying lighting conditions. A prominent solution for the task is score distillation sampling. It allows recovering a complex texture using gradient guidance given a differentiable rasterization and shading pipeline. However, in practice, the aforementioned solution in conjunction with the widespread latent diffusion models produces severe visual artifacts and requires additional regularization such as implicit texture parameterization. As a more direct alternative, we propose an approach using cascaded diffusion models for texture synthesis (CasTex). In our setup, score distillation sampling yields high-quality textures out-of-the box. In particular, we were able to omit implicit texture parameterization in favor of an explicit parameterization to improve the procedure. In the experiments, we show that our approach significantly outperforms state-of-the-art optimization-based solutions on public texture synthesis benchmarks.", "link": "http://arxiv.org/abs/2504.06856v2", "date": "2026-01-13", "relevancy": 2.3995, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6295}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6131}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CasTex%3A%20Cascaded%20Text-to-Texture%20Synthesis%20via%20Explicit%20Texture%20Maps%20and%20Physically-Based%20Shading&body=Title%3A%20CasTex%3A%20Cascaded%20Text-to-Texture%20Synthesis%20via%20Explicit%20Texture%20Maps%20and%20Physically-Based%20Shading%0AAuthor%3A%20Mishan%20Aliev%20and%20Dmitry%20Baranchuk%20and%20Kirill%20Struminsky%0AAbstract%3A%20This%20work%20investigates%20text-to-texture%20synthesis%20using%20diffusion%20models%20to%20generate%20physically-based%20texture%20maps.%20We%20aim%20to%20achieve%20realistic%20model%20appearances%20under%20varying%20lighting%20conditions.%20A%20prominent%20solution%20for%20the%20task%20is%20score%20distillation%20sampling.%20It%20allows%20recovering%20a%20complex%20texture%20using%20gradient%20guidance%20given%20a%20differentiable%20rasterization%20and%20shading%20pipeline.%20However%2C%20in%20practice%2C%20the%20aforementioned%20solution%20in%20conjunction%20with%20the%20widespread%20latent%20diffusion%20models%20produces%20severe%20visual%20artifacts%20and%20requires%20additional%20regularization%20such%20as%20implicit%20texture%20parameterization.%20As%20a%20more%20direct%20alternative%2C%20we%20propose%20an%20approach%20using%20cascaded%20diffusion%20models%20for%20texture%20synthesis%20%28CasTex%29.%20In%20our%20setup%2C%20score%20distillation%20sampling%20yields%20high-quality%20textures%20out-of-the%20box.%20In%20particular%2C%20we%20were%20able%20to%20omit%20implicit%20texture%20parameterization%20in%20favor%20of%20an%20explicit%20parameterization%20to%20improve%20the%20procedure.%20In%20the%20experiments%2C%20we%20show%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20optimization-based%20solutions%20on%20public%20texture%20synthesis%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2504.06856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCasTex%253A%2520Cascaded%2520Text-to-Texture%2520Synthesis%2520via%2520Explicit%2520Texture%2520Maps%2520and%2520Physically-Based%2520Shading%26entry.906535625%3DMishan%2520Aliev%2520and%2520Dmitry%2520Baranchuk%2520and%2520Kirill%2520Struminsky%26entry.1292438233%3DThis%2520work%2520investigates%2520text-to-texture%2520synthesis%2520using%2520diffusion%2520models%2520to%2520generate%2520physically-based%2520texture%2520maps.%2520We%2520aim%2520to%2520achieve%2520realistic%2520model%2520appearances%2520under%2520varying%2520lighting%2520conditions.%2520A%2520prominent%2520solution%2520for%2520the%2520task%2520is%2520score%2520distillation%2520sampling.%2520It%2520allows%2520recovering%2520a%2520complex%2520texture%2520using%2520gradient%2520guidance%2520given%2520a%2520differentiable%2520rasterization%2520and%2520shading%2520pipeline.%2520However%252C%2520in%2520practice%252C%2520the%2520aforementioned%2520solution%2520in%2520conjunction%2520with%2520the%2520widespread%2520latent%2520diffusion%2520models%2520produces%2520severe%2520visual%2520artifacts%2520and%2520requires%2520additional%2520regularization%2520such%2520as%2520implicit%2520texture%2520parameterization.%2520As%2520a%2520more%2520direct%2520alternative%252C%2520we%2520propose%2520an%2520approach%2520using%2520cascaded%2520diffusion%2520models%2520for%2520texture%2520synthesis%2520%2528CasTex%2529.%2520In%2520our%2520setup%252C%2520score%2520distillation%2520sampling%2520yields%2520high-quality%2520textures%2520out-of-the%2520box.%2520In%2520particular%252C%2520we%2520were%2520able%2520to%2520omit%2520implicit%2520texture%2520parameterization%2520in%2520favor%2520of%2520an%2520explicit%2520parameterization%2520to%2520improve%2520the%2520procedure.%2520In%2520the%2520experiments%252C%2520we%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520optimization-based%2520solutions%2520on%2520public%2520texture%2520synthesis%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CasTex%3A%20Cascaded%20Text-to-Texture%20Synthesis%20via%20Explicit%20Texture%20Maps%20and%20Physically-Based%20Shading&entry.906535625=Mishan%20Aliev%20and%20Dmitry%20Baranchuk%20and%20Kirill%20Struminsky&entry.1292438233=This%20work%20investigates%20text-to-texture%20synthesis%20using%20diffusion%20models%20to%20generate%20physically-based%20texture%20maps.%20We%20aim%20to%20achieve%20realistic%20model%20appearances%20under%20varying%20lighting%20conditions.%20A%20prominent%20solution%20for%20the%20task%20is%20score%20distillation%20sampling.%20It%20allows%20recovering%20a%20complex%20texture%20using%20gradient%20guidance%20given%20a%20differentiable%20rasterization%20and%20shading%20pipeline.%20However%2C%20in%20practice%2C%20the%20aforementioned%20solution%20in%20conjunction%20with%20the%20widespread%20latent%20diffusion%20models%20produces%20severe%20visual%20artifacts%20and%20requires%20additional%20regularization%20such%20as%20implicit%20texture%20parameterization.%20As%20a%20more%20direct%20alternative%2C%20we%20propose%20an%20approach%20using%20cascaded%20diffusion%20models%20for%20texture%20synthesis%20%28CasTex%29.%20In%20our%20setup%2C%20score%20distillation%20sampling%20yields%20high-quality%20textures%20out-of-the%20box.%20In%20particular%2C%20we%20were%20able%20to%20omit%20implicit%20texture%20parameterization%20in%20favor%20of%20an%20explicit%20parameterization%20to%20improve%20the%20procedure.%20In%20the%20experiments%2C%20we%20show%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20optimization-based%20solutions%20on%20public%20texture%20synthesis%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2504.06856v2&entry.124074799=Read"},
{"title": "Motion Attribution for Video Generation", "author": "Xindi Wu and Despoina Paschalidou and Jun Gao and Antonio Torralba and Laura Leal-Taix\u00e9 and Olga Russakovsky and Sanja Fidler and Jonathan Lorraine", "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.", "link": "http://arxiv.org/abs/2601.08828v1", "date": "2026-01-13", "relevancy": 2.3883, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6455}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5887}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Attribution%20for%20Video%20Generation&body=Title%3A%20Motion%20Attribution%20for%20Video%20Generation%0AAuthor%3A%20Xindi%20Wu%20and%20Despoina%20Paschalidou%20and%20Jun%20Gao%20and%20Antonio%20Torralba%20and%20Laura%20Leal-Taix%C3%A9%20and%20Olga%20Russakovsky%20and%20Sanja%20Fidler%20and%20Jonathan%20Lorraine%0AAbstract%3A%20Despite%20the%20rapid%20progress%20of%20video%20generation%20models%2C%20the%20role%20of%20data%20in%20influencing%20motion%20is%20poorly%20understood.%20We%20present%20Motive%20%28MOTIon%20attribution%20for%20Video%20gEneration%29%2C%20a%20motion-centric%2C%20gradient-based%20data%20attribution%20framework%20that%20scales%20to%20modern%2C%20large%2C%20high-quality%20video%20datasets%20and%20models.%20We%20use%20this%20to%20study%20which%20fine-tuning%20clips%20improve%20or%20degrade%20temporal%20dynamics.%20Motive%20isolates%20temporal%20dynamics%20from%20static%20appearance%20via%20motion-weighted%20loss%20masks%2C%20yielding%20efficient%20and%20scalable%20motion-specific%20influence%20computation.%20On%20text-to-video%20models%2C%20Motive%20identifies%20clips%20that%20strongly%20affect%20motion%20and%20guides%20data%20curation%20that%20improves%20temporal%20consistency%20and%20physical%20plausibility.%20With%20Motive-selected%20high-influence%20data%2C%20our%20method%20improves%20both%20motion%20smoothness%20and%20dynamic%20degree%20on%20VBench%2C%20achieving%20a%2074.1%25%20human%20preference%20win%20rate%20compared%20with%20the%20pretrained%20base%20model.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20to%20attribute%20motion%20rather%20than%20visual%20appearance%20in%20video%20generative%20models%20and%20to%20use%20it%20to%20curate%20fine-tuning%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Attribution%2520for%2520Video%2520Generation%26entry.906535625%3DXindi%2520Wu%2520and%2520Despoina%2520Paschalidou%2520and%2520Jun%2520Gao%2520and%2520Antonio%2520Torralba%2520and%2520Laura%2520Leal-Taix%25C3%25A9%2520and%2520Olga%2520Russakovsky%2520and%2520Sanja%2520Fidler%2520and%2520Jonathan%2520Lorraine%26entry.1292438233%3DDespite%2520the%2520rapid%2520progress%2520of%2520video%2520generation%2520models%252C%2520the%2520role%2520of%2520data%2520in%2520influencing%2520motion%2520is%2520poorly%2520understood.%2520We%2520present%2520Motive%2520%2528MOTIon%2520attribution%2520for%2520Video%2520gEneration%2529%252C%2520a%2520motion-centric%252C%2520gradient-based%2520data%2520attribution%2520framework%2520that%2520scales%2520to%2520modern%252C%2520large%252C%2520high-quality%2520video%2520datasets%2520and%2520models.%2520We%2520use%2520this%2520to%2520study%2520which%2520fine-tuning%2520clips%2520improve%2520or%2520degrade%2520temporal%2520dynamics.%2520Motive%2520isolates%2520temporal%2520dynamics%2520from%2520static%2520appearance%2520via%2520motion-weighted%2520loss%2520masks%252C%2520yielding%2520efficient%2520and%2520scalable%2520motion-specific%2520influence%2520computation.%2520On%2520text-to-video%2520models%252C%2520Motive%2520identifies%2520clips%2520that%2520strongly%2520affect%2520motion%2520and%2520guides%2520data%2520curation%2520that%2520improves%2520temporal%2520consistency%2520and%2520physical%2520plausibility.%2520With%2520Motive-selected%2520high-influence%2520data%252C%2520our%2520method%2520improves%2520both%2520motion%2520smoothness%2520and%2520dynamic%2520degree%2520on%2520VBench%252C%2520achieving%2520a%252074.1%2525%2520human%2520preference%2520win%2520rate%2520compared%2520with%2520the%2520pretrained%2520base%2520model.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520framework%2520to%2520attribute%2520motion%2520rather%2520than%2520visual%2520appearance%2520in%2520video%2520generative%2520models%2520and%2520to%2520use%2520it%2520to%2520curate%2520fine-tuning%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Attribution%20for%20Video%20Generation&entry.906535625=Xindi%20Wu%20and%20Despoina%20Paschalidou%20and%20Jun%20Gao%20and%20Antonio%20Torralba%20and%20Laura%20Leal-Taix%C3%A9%20and%20Olga%20Russakovsky%20and%20Sanja%20Fidler%20and%20Jonathan%20Lorraine&entry.1292438233=Despite%20the%20rapid%20progress%20of%20video%20generation%20models%2C%20the%20role%20of%20data%20in%20influencing%20motion%20is%20poorly%20understood.%20We%20present%20Motive%20%28MOTIon%20attribution%20for%20Video%20gEneration%29%2C%20a%20motion-centric%2C%20gradient-based%20data%20attribution%20framework%20that%20scales%20to%20modern%2C%20large%2C%20high-quality%20video%20datasets%20and%20models.%20We%20use%20this%20to%20study%20which%20fine-tuning%20clips%20improve%20or%20degrade%20temporal%20dynamics.%20Motive%20isolates%20temporal%20dynamics%20from%20static%20appearance%20via%20motion-weighted%20loss%20masks%2C%20yielding%20efficient%20and%20scalable%20motion-specific%20influence%20computation.%20On%20text-to-video%20models%2C%20Motive%20identifies%20clips%20that%20strongly%20affect%20motion%20and%20guides%20data%20curation%20that%20improves%20temporal%20consistency%20and%20physical%20plausibility.%20With%20Motive-selected%20high-influence%20data%2C%20our%20method%20improves%20both%20motion%20smoothness%20and%20dynamic%20degree%20on%20VBench%2C%20achieving%20a%2074.1%25%20human%20preference%20win%20rate%20compared%20with%20the%20pretrained%20base%20model.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20to%20attribute%20motion%20rather%20than%20visual%20appearance%20in%20video%20generative%20models%20and%20to%20use%20it%20to%20curate%20fine-tuning%20data.&entry.1838667208=http%3A//arxiv.org/abs/2601.08828v1&entry.124074799=Read"},
{"title": "MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP", "author": "Aditya Chaudhary and Sneha Barman and Mainak Singha and Ankit Jha and Girish Mishra and Biplab Banerjee", "abstract": "In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.", "link": "http://arxiv.org/abs/2601.08420v1", "date": "2026-01-13", "relevancy": 2.3872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMLGNet%3A%20Cross-Modal%20Alignment%20of%20Remote%20Sensing%20Data%20using%20CLIP&body=Title%3A%20MMLGNet%3A%20Cross-Modal%20Alignment%20of%20Remote%20Sensing%20Data%20using%20CLIP%0AAuthor%3A%20Aditya%20Chaudhary%20and%20Sneha%20Barman%20and%20Mainak%20Singha%20and%20Ankit%20Jha%20and%20Girish%20Mishra%20and%20Biplab%20Banerjee%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20novel%20multimodal%20framework%2C%20Multimodal%20Language-Guided%20Network%20%28MMLGNet%29%2C%20to%20align%20heterogeneous%20remote%20sensing%20modalities%20like%20Hyperspectral%20Imaging%20%28HSI%29%20and%20LiDAR%20with%20natural%20language%20semantics%20using%20vision-language%20models%20such%20as%20CLIP.%20With%20the%20increasing%20availability%20of%20multimodal%20Earth%20observation%20data%2C%20there%20is%20a%20growing%20need%20for%20methods%20that%20effectively%20fuse%20spectral%2C%20spatial%2C%20and%20geometric%20information%20while%20enabling%20semantic-level%20understanding.%20MMLGNet%20employs%20modality-specific%20encoders%20and%20aligns%20visual%20features%20with%20handcrafted%20textual%20embeddings%20in%20a%20shared%20latent%20space%20via%20bi-directional%20contrastive%20learning.%20Inspired%20by%20CLIP%27s%20training%20paradigm%2C%20our%20approach%20bridges%20the%20gap%20between%20high-dimensional%20remote%20sensing%20data%20and%20language-guided%20interpretation.%20Notably%2C%20MMLGNet%20achieves%20strong%20performance%20with%20simple%20CNN-based%20encoders%2C%20outperforming%20several%20established%20multimodal%20visual-only%20methods%20on%20two%20benchmark%20datasets%2C%20demonstrating%20the%20significant%20benefit%20of%20language%20supervision.%20Codes%20are%20available%20at%20https%3A//github.com/AdityaChaudhary2913/CLIP_HSI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMLGNet%253A%2520Cross-Modal%2520Alignment%2520of%2520Remote%2520Sensing%2520Data%2520using%2520CLIP%26entry.906535625%3DAditya%2520Chaudhary%2520and%2520Sneha%2520Barman%2520and%2520Mainak%2520Singha%2520and%2520Ankit%2520Jha%2520and%2520Girish%2520Mishra%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520framework%252C%2520Multimodal%2520Language-Guided%2520Network%2520%2528MMLGNet%2529%252C%2520to%2520align%2520heterogeneous%2520remote%2520sensing%2520modalities%2520like%2520Hyperspectral%2520Imaging%2520%2528HSI%2529%2520and%2520LiDAR%2520with%2520natural%2520language%2520semantics%2520using%2520vision-language%2520models%2520such%2520as%2520CLIP.%2520With%2520the%2520increasing%2520availability%2520of%2520multimodal%2520Earth%2520observation%2520data%252C%2520there%2520is%2520a%2520growing%2520need%2520for%2520methods%2520that%2520effectively%2520fuse%2520spectral%252C%2520spatial%252C%2520and%2520geometric%2520information%2520while%2520enabling%2520semantic-level%2520understanding.%2520MMLGNet%2520employs%2520modality-specific%2520encoders%2520and%2520aligns%2520visual%2520features%2520with%2520handcrafted%2520textual%2520embeddings%2520in%2520a%2520shared%2520latent%2520space%2520via%2520bi-directional%2520contrastive%2520learning.%2520Inspired%2520by%2520CLIP%2527s%2520training%2520paradigm%252C%2520our%2520approach%2520bridges%2520the%2520gap%2520between%2520high-dimensional%2520remote%2520sensing%2520data%2520and%2520language-guided%2520interpretation.%2520Notably%252C%2520MMLGNet%2520achieves%2520strong%2520performance%2520with%2520simple%2520CNN-based%2520encoders%252C%2520outperforming%2520several%2520established%2520multimodal%2520visual-only%2520methods%2520on%2520two%2520benchmark%2520datasets%252C%2520demonstrating%2520the%2520significant%2520benefit%2520of%2520language%2520supervision.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/AdityaChaudhary2913/CLIP_HSI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMLGNet%3A%20Cross-Modal%20Alignment%20of%20Remote%20Sensing%20Data%20using%20CLIP&entry.906535625=Aditya%20Chaudhary%20and%20Sneha%20Barman%20and%20Mainak%20Singha%20and%20Ankit%20Jha%20and%20Girish%20Mishra%20and%20Biplab%20Banerjee&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20novel%20multimodal%20framework%2C%20Multimodal%20Language-Guided%20Network%20%28MMLGNet%29%2C%20to%20align%20heterogeneous%20remote%20sensing%20modalities%20like%20Hyperspectral%20Imaging%20%28HSI%29%20and%20LiDAR%20with%20natural%20language%20semantics%20using%20vision-language%20models%20such%20as%20CLIP.%20With%20the%20increasing%20availability%20of%20multimodal%20Earth%20observation%20data%2C%20there%20is%20a%20growing%20need%20for%20methods%20that%20effectively%20fuse%20spectral%2C%20spatial%2C%20and%20geometric%20information%20while%20enabling%20semantic-level%20understanding.%20MMLGNet%20employs%20modality-specific%20encoders%20and%20aligns%20visual%20features%20with%20handcrafted%20textual%20embeddings%20in%20a%20shared%20latent%20space%20via%20bi-directional%20contrastive%20learning.%20Inspired%20by%20CLIP%27s%20training%20paradigm%2C%20our%20approach%20bridges%20the%20gap%20between%20high-dimensional%20remote%20sensing%20data%20and%20language-guided%20interpretation.%20Notably%2C%20MMLGNet%20achieves%20strong%20performance%20with%20simple%20CNN-based%20encoders%2C%20outperforming%20several%20established%20multimodal%20visual-only%20methods%20on%20two%20benchmark%20datasets%2C%20demonstrating%20the%20significant%20benefit%20of%20language%20supervision.%20Codes%20are%20available%20at%20https%3A//github.com/AdityaChaudhary2913/CLIP_HSI.&entry.1838667208=http%3A//arxiv.org/abs/2601.08420v1&entry.124074799=Read"},
{"title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation", "author": "Dongjie Fu and Tengjiao Sun and Pengcheng Fang and Xiaohao Cai and Hansung Kim", "abstract": "Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.", "link": "http://arxiv.org/abs/2506.05952v3", "date": "2026-01-13", "relevancy": 2.3838, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6341}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5697}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation&body=Title%3A%20MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation%0AAuthor%3A%20Dongjie%20Fu%20and%20Tengjiao%20Sun%20and%20Pengcheng%20Fang%20and%20Xiaohao%20Cai%20and%20Hansung%20Kim%0AAbstract%3A%20Recent%20advances%20in%20transformer-based%20text-to-motion%20generation%20have%20led%20to%20impressive%20progress%20in%20synthesizing%20high-quality%20human%20motion.%20Nevertheless%2C%20jointly%20achieving%20high%20fidelity%2C%20streaming%20capability%2C%20real-time%20responsiveness%2C%20and%20scalability%20remains%20a%20fundamental%20challenge.%20In%20this%20paper%2C%20we%20propose%20MOGO%20%28Motion%20Generation%20with%20One-pass%29%2C%20a%20novel%20autoregressive%20framework%20tailored%20for%20efficient%20and%20real-time%203D%20motion%20generation.%20MOGO%20comprises%20two%20key%20components%3A%20%281%29%20MoSA-VQ%2C%20a%20motion%20scale-adaptive%20residual%20vector%20quantization%20module%20that%20hierarchically%20discretizes%20motion%20sequences%20with%20learnable%20scaling%20to%20produce%20compact%20yet%20expressive%20representations%3B%20and%20%282%29%20RQHC-Transformer%2C%20a%20residual%20quantized%20hierarchical%20causal%20transformer%20that%20generates%20multi-layer%20motion%20tokens%20in%20a%20single%20forward%20pass%2C%20significantly%20reducing%20inference%20latency.%20To%20enhance%20semantic%20fidelity%2C%20we%20further%20introduce%20a%20text%20condition%20alignment%20mechanism%20that%20improves%20motion%20decoding%20under%20textual%20control.%20Extensive%20experiments%20on%20benchmark%20datasets%20including%20HumanML3D%2C%20KIT-ML%2C%20and%20CMP%20demonstrate%20that%20MOGO%20achieves%20competitive%20or%20superior%20generation%20quality%20compared%20to%20state-of-the-art%20transformer-based%20methods%2C%20while%20offering%20substantial%20improvements%20in%20real-time%20performance%2C%20streaming%20generation%2C%20and%20generalization%20under%20zero-shot%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05952v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOGO%253A%2520Residual%2520Quantized%2520Hierarchical%2520Causal%2520Transformer%2520for%2520High-Quality%2520and%2520Real-Time%25203D%2520Human%2520Motion%2520Generation%26entry.906535625%3DDongjie%2520Fu%2520and%2520Tengjiao%2520Sun%2520and%2520Pengcheng%2520Fang%2520and%2520Xiaohao%2520Cai%2520and%2520Hansung%2520Kim%26entry.1292438233%3DRecent%2520advances%2520in%2520transformer-based%2520text-to-motion%2520generation%2520have%2520led%2520to%2520impressive%2520progress%2520in%2520synthesizing%2520high-quality%2520human%2520motion.%2520Nevertheless%252C%2520jointly%2520achieving%2520high%2520fidelity%252C%2520streaming%2520capability%252C%2520real-time%2520responsiveness%252C%2520and%2520scalability%2520remains%2520a%2520fundamental%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MOGO%2520%2528Motion%2520Generation%2520with%2520One-pass%2529%252C%2520a%2520novel%2520autoregressive%2520framework%2520tailored%2520for%2520efficient%2520and%2520real-time%25203D%2520motion%2520generation.%2520MOGO%2520comprises%2520two%2520key%2520components%253A%2520%25281%2529%2520MoSA-VQ%252C%2520a%2520motion%2520scale-adaptive%2520residual%2520vector%2520quantization%2520module%2520that%2520hierarchically%2520discretizes%2520motion%2520sequences%2520with%2520learnable%2520scaling%2520to%2520produce%2520compact%2520yet%2520expressive%2520representations%253B%2520and%2520%25282%2529%2520RQHC-Transformer%252C%2520a%2520residual%2520quantized%2520hierarchical%2520causal%2520transformer%2520that%2520generates%2520multi-layer%2520motion%2520tokens%2520in%2520a%2520single%2520forward%2520pass%252C%2520significantly%2520reducing%2520inference%2520latency.%2520To%2520enhance%2520semantic%2520fidelity%252C%2520we%2520further%2520introduce%2520a%2520text%2520condition%2520alignment%2520mechanism%2520that%2520improves%2520motion%2520decoding%2520under%2520textual%2520control.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520including%2520HumanML3D%252C%2520KIT-ML%252C%2520and%2520CMP%2520demonstrate%2520that%2520MOGO%2520achieves%2520competitive%2520or%2520superior%2520generation%2520quality%2520compared%2520to%2520state-of-the-art%2520transformer-based%2520methods%252C%2520while%2520offering%2520substantial%2520improvements%2520in%2520real-time%2520performance%252C%2520streaming%2520generation%252C%2520and%2520generalization%2520under%2520zero-shot%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05952v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation&entry.906535625=Dongjie%20Fu%20and%20Tengjiao%20Sun%20and%20Pengcheng%20Fang%20and%20Xiaohao%20Cai%20and%20Hansung%20Kim&entry.1292438233=Recent%20advances%20in%20transformer-based%20text-to-motion%20generation%20have%20led%20to%20impressive%20progress%20in%20synthesizing%20high-quality%20human%20motion.%20Nevertheless%2C%20jointly%20achieving%20high%20fidelity%2C%20streaming%20capability%2C%20real-time%20responsiveness%2C%20and%20scalability%20remains%20a%20fundamental%20challenge.%20In%20this%20paper%2C%20we%20propose%20MOGO%20%28Motion%20Generation%20with%20One-pass%29%2C%20a%20novel%20autoregressive%20framework%20tailored%20for%20efficient%20and%20real-time%203D%20motion%20generation.%20MOGO%20comprises%20two%20key%20components%3A%20%281%29%20MoSA-VQ%2C%20a%20motion%20scale-adaptive%20residual%20vector%20quantization%20module%20that%20hierarchically%20discretizes%20motion%20sequences%20with%20learnable%20scaling%20to%20produce%20compact%20yet%20expressive%20representations%3B%20and%20%282%29%20RQHC-Transformer%2C%20a%20residual%20quantized%20hierarchical%20causal%20transformer%20that%20generates%20multi-layer%20motion%20tokens%20in%20a%20single%20forward%20pass%2C%20significantly%20reducing%20inference%20latency.%20To%20enhance%20semantic%20fidelity%2C%20we%20further%20introduce%20a%20text%20condition%20alignment%20mechanism%20that%20improves%20motion%20decoding%20under%20textual%20control.%20Extensive%20experiments%20on%20benchmark%20datasets%20including%20HumanML3D%2C%20KIT-ML%2C%20and%20CMP%20demonstrate%20that%20MOGO%20achieves%20competitive%20or%20superior%20generation%20quality%20compared%20to%20state-of-the-art%20transformer-based%20methods%2C%20while%20offering%20substantial%20improvements%20in%20real-time%20performance%2C%20streaming%20generation%2C%20and%20generalization%20under%20zero-shot%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2506.05952v3&entry.124074799=Read"},
{"title": "Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set", "author": "Kaivalya Rawal and Eoin Delaney and Zihao Fu and Sandra Wachter and Chris Russell", "abstract": "Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper \"Evaluating Model Explanations without Ground Truth\", we proposed three principles of explanation evaluation and a new method \"AXE\" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.", "link": "http://arxiv.org/abs/2601.08703v1", "date": "2026-01-13", "relevancy": 2.3758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Ability%20of%20Explanations%20to%20Disambiguate%20Models%20in%20a%20Rashomon%20Set&body=Title%3A%20Evaluating%20the%20Ability%20of%20Explanations%20to%20Disambiguate%20Models%20in%20a%20Rashomon%20Set%0AAuthor%3A%20Kaivalya%20Rawal%20and%20Eoin%20Delaney%20and%20Zihao%20Fu%20and%20Sandra%20Wachter%20and%20Chris%20Russell%0AAbstract%3A%20Explainable%20artificial%20intelligence%20%28XAI%29%20is%20concerned%20with%20producing%20explanations%20indicating%20the%20inner%20workings%20of%20models.%20For%20a%20Rashomon%20set%20of%20similarly%20performing%20models%2C%20explanations%20provide%20a%20way%20of%20disambiguating%20the%20behavior%20of%20individual%20models%2C%20helping%20select%20models%20for%20deployment.%20However%20explanations%20themselves%20can%20vary%20depending%20on%20the%20explainer%20used%2C%20and%20need%20to%20be%20evaluated.%20In%20the%20paper%20%22Evaluating%20Model%20Explanations%20without%20Ground%20Truth%22%2C%20we%20proposed%20three%20principles%20of%20explanation%20evaluation%20and%20a%20new%20method%20%22AXE%22%20to%20evaluate%20the%20quality%20of%20feature-importance%20explanations.%20We%20go%20on%20to%20illustrate%20how%20evaluation%20metrics%20that%20rely%20on%20comparing%20model%20explanations%20against%20ideal%20ground%20truth%20explanations%20obscure%20behavioral%20differences%20within%20a%20Rashomon%20set.%20Explanation%20evaluation%20aligned%20with%20our%20proposed%20principles%20would%20highlight%20these%20differences%20instead%2C%20helping%20select%20models%20from%20the%20Rashomon%20set.%20The%20selection%20of%20alternate%20models%20from%20the%20Rashomon%20set%20can%20maintain%20identical%20predictions%20but%20mislead%20explainers%20into%20generating%20false%20explanations%2C%20and%20mislead%20evaluation%20methods%20into%20considering%20the%20false%20explanations%20to%20be%20of%20high%20quality.%20AXE%2C%20our%20proposed%20explanation%20evaluation%20method%2C%20can%20detect%20this%20adversarial%20fairwashing%20of%20explanations%20with%20a%20100%25%20success%20rate.%20Unlike%20prior%20explanation%20evaluation%20strategies%20such%20as%20those%20based%20on%20model%20sensitivity%20or%20ground%20truth%20comparison%2C%20AXE%20can%20determine%20when%20protected%20attributes%20are%20used%20to%20make%20predictions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Ability%2520of%2520Explanations%2520to%2520Disambiguate%2520Models%2520in%2520a%2520Rashomon%2520Set%26entry.906535625%3DKaivalya%2520Rawal%2520and%2520Eoin%2520Delaney%2520and%2520Zihao%2520Fu%2520and%2520Sandra%2520Wachter%2520and%2520Chris%2520Russell%26entry.1292438233%3DExplainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520is%2520concerned%2520with%2520producing%2520explanations%2520indicating%2520the%2520inner%2520workings%2520of%2520models.%2520For%2520a%2520Rashomon%2520set%2520of%2520similarly%2520performing%2520models%252C%2520explanations%2520provide%2520a%2520way%2520of%2520disambiguating%2520the%2520behavior%2520of%2520individual%2520models%252C%2520helping%2520select%2520models%2520for%2520deployment.%2520However%2520explanations%2520themselves%2520can%2520vary%2520depending%2520on%2520the%2520explainer%2520used%252C%2520and%2520need%2520to%2520be%2520evaluated.%2520In%2520the%2520paper%2520%2522Evaluating%2520Model%2520Explanations%2520without%2520Ground%2520Truth%2522%252C%2520we%2520proposed%2520three%2520principles%2520of%2520explanation%2520evaluation%2520and%2520a%2520new%2520method%2520%2522AXE%2522%2520to%2520evaluate%2520the%2520quality%2520of%2520feature-importance%2520explanations.%2520We%2520go%2520on%2520to%2520illustrate%2520how%2520evaluation%2520metrics%2520that%2520rely%2520on%2520comparing%2520model%2520explanations%2520against%2520ideal%2520ground%2520truth%2520explanations%2520obscure%2520behavioral%2520differences%2520within%2520a%2520Rashomon%2520set.%2520Explanation%2520evaluation%2520aligned%2520with%2520our%2520proposed%2520principles%2520would%2520highlight%2520these%2520differences%2520instead%252C%2520helping%2520select%2520models%2520from%2520the%2520Rashomon%2520set.%2520The%2520selection%2520of%2520alternate%2520models%2520from%2520the%2520Rashomon%2520set%2520can%2520maintain%2520identical%2520predictions%2520but%2520mislead%2520explainers%2520into%2520generating%2520false%2520explanations%252C%2520and%2520mislead%2520evaluation%2520methods%2520into%2520considering%2520the%2520false%2520explanations%2520to%2520be%2520of%2520high%2520quality.%2520AXE%252C%2520our%2520proposed%2520explanation%2520evaluation%2520method%252C%2520can%2520detect%2520this%2520adversarial%2520fairwashing%2520of%2520explanations%2520with%2520a%2520100%2525%2520success%2520rate.%2520Unlike%2520prior%2520explanation%2520evaluation%2520strategies%2520such%2520as%2520those%2520based%2520on%2520model%2520sensitivity%2520or%2520ground%2520truth%2520comparison%252C%2520AXE%2520can%2520determine%2520when%2520protected%2520attributes%2520are%2520used%2520to%2520make%2520predictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Ability%20of%20Explanations%20to%20Disambiguate%20Models%20in%20a%20Rashomon%20Set&entry.906535625=Kaivalya%20Rawal%20and%20Eoin%20Delaney%20and%20Zihao%20Fu%20and%20Sandra%20Wachter%20and%20Chris%20Russell&entry.1292438233=Explainable%20artificial%20intelligence%20%28XAI%29%20is%20concerned%20with%20producing%20explanations%20indicating%20the%20inner%20workings%20of%20models.%20For%20a%20Rashomon%20set%20of%20similarly%20performing%20models%2C%20explanations%20provide%20a%20way%20of%20disambiguating%20the%20behavior%20of%20individual%20models%2C%20helping%20select%20models%20for%20deployment.%20However%20explanations%20themselves%20can%20vary%20depending%20on%20the%20explainer%20used%2C%20and%20need%20to%20be%20evaluated.%20In%20the%20paper%20%22Evaluating%20Model%20Explanations%20without%20Ground%20Truth%22%2C%20we%20proposed%20three%20principles%20of%20explanation%20evaluation%20and%20a%20new%20method%20%22AXE%22%20to%20evaluate%20the%20quality%20of%20feature-importance%20explanations.%20We%20go%20on%20to%20illustrate%20how%20evaluation%20metrics%20that%20rely%20on%20comparing%20model%20explanations%20against%20ideal%20ground%20truth%20explanations%20obscure%20behavioral%20differences%20within%20a%20Rashomon%20set.%20Explanation%20evaluation%20aligned%20with%20our%20proposed%20principles%20would%20highlight%20these%20differences%20instead%2C%20helping%20select%20models%20from%20the%20Rashomon%20set.%20The%20selection%20of%20alternate%20models%20from%20the%20Rashomon%20set%20can%20maintain%20identical%20predictions%20but%20mislead%20explainers%20into%20generating%20false%20explanations%2C%20and%20mislead%20evaluation%20methods%20into%20considering%20the%20false%20explanations%20to%20be%20of%20high%20quality.%20AXE%2C%20our%20proposed%20explanation%20evaluation%20method%2C%20can%20detect%20this%20adversarial%20fairwashing%20of%20explanations%20with%20a%20100%25%20success%20rate.%20Unlike%20prior%20explanation%20evaluation%20strategies%20such%20as%20those%20based%20on%20model%20sensitivity%20or%20ground%20truth%20comparison%2C%20AXE%20can%20determine%20when%20protected%20attributes%20are%20used%20to%20make%20predictions.&entry.1838667208=http%3A//arxiv.org/abs/2601.08703v1&entry.124074799=Read"},
{"title": "Rewriting Video: Text-Driven Reauthoring of Video Footage", "author": "Sitong Wang and Anh Truong and Lydia B. Chilton and Dingzeyu Li", "abstract": "Video is a powerful medium for communication and storytelling, yet reauthoring existing footage remains challenging. Even simple edits often demand expertise, time, and careful planning, constraining how creators envision and shape their narratives. Recent advances in generative AI suggest a new paradigm: what if editing a video were as straightforward as rewriting text? To investigate this, we present a tech probe and a study on text-driven video reauthoring. Our approach involves two technical contributions: (1) a generative reconstruction algorithm that reverse-engineers video into an editable text prompt, and (2) an interactive probe, Rewrite Kit, that allows creators to manipulate these prompts. A technical evaluation of the algorithm reveals a critical human-AI perceptual gap. A probe study with 12 creators surfaced novel use cases such as virtual reshooting, synthetic continuity, and aesthetic restyling. It also highlighted key tensions around coherence, control, and creative alignment in this new paradigm. Our work contributes empirical insights into the opportunities and challenges of text-driven video reauthoring, offering design implications for future co-creative video tools.", "link": "http://arxiv.org/abs/2601.08565v1", "date": "2026-01-13", "relevancy": 2.3755, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6217}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5936}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rewriting%20Video%3A%20Text-Driven%20Reauthoring%20of%20Video%20Footage&body=Title%3A%20Rewriting%20Video%3A%20Text-Driven%20Reauthoring%20of%20Video%20Footage%0AAuthor%3A%20Sitong%20Wang%20and%20Anh%20Truong%20and%20Lydia%20B.%20Chilton%20and%20Dingzeyu%20Li%0AAbstract%3A%20Video%20is%20a%20powerful%20medium%20for%20communication%20and%20storytelling%2C%20yet%20reauthoring%20existing%20footage%20remains%20challenging.%20Even%20simple%20edits%20often%20demand%20expertise%2C%20time%2C%20and%20careful%20planning%2C%20constraining%20how%20creators%20envision%20and%20shape%20their%20narratives.%20Recent%20advances%20in%20generative%20AI%20suggest%20a%20new%20paradigm%3A%20what%20if%20editing%20a%20video%20were%20as%20straightforward%20as%20rewriting%20text%3F%20To%20investigate%20this%2C%20we%20present%20a%20tech%20probe%20and%20a%20study%20on%20text-driven%20video%20reauthoring.%20Our%20approach%20involves%20two%20technical%20contributions%3A%20%281%29%20a%20generative%20reconstruction%20algorithm%20that%20reverse-engineers%20video%20into%20an%20editable%20text%20prompt%2C%20and%20%282%29%20an%20interactive%20probe%2C%20Rewrite%20Kit%2C%20that%20allows%20creators%20to%20manipulate%20these%20prompts.%20A%20technical%20evaluation%20of%20the%20algorithm%20reveals%20a%20critical%20human-AI%20perceptual%20gap.%20A%20probe%20study%20with%2012%20creators%20surfaced%20novel%20use%20cases%20such%20as%20virtual%20reshooting%2C%20synthetic%20continuity%2C%20and%20aesthetic%20restyling.%20It%20also%20highlighted%20key%20tensions%20around%20coherence%2C%20control%2C%20and%20creative%20alignment%20in%20this%20new%20paradigm.%20Our%20work%20contributes%20empirical%20insights%20into%20the%20opportunities%20and%20challenges%20of%20text-driven%20video%20reauthoring%2C%20offering%20design%20implications%20for%20future%20co-creative%20video%20tools.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewriting%2520Video%253A%2520Text-Driven%2520Reauthoring%2520of%2520Video%2520Footage%26entry.906535625%3DSitong%2520Wang%2520and%2520Anh%2520Truong%2520and%2520Lydia%2520B.%2520Chilton%2520and%2520Dingzeyu%2520Li%26entry.1292438233%3DVideo%2520is%2520a%2520powerful%2520medium%2520for%2520communication%2520and%2520storytelling%252C%2520yet%2520reauthoring%2520existing%2520footage%2520remains%2520challenging.%2520Even%2520simple%2520edits%2520often%2520demand%2520expertise%252C%2520time%252C%2520and%2520careful%2520planning%252C%2520constraining%2520how%2520creators%2520envision%2520and%2520shape%2520their%2520narratives.%2520Recent%2520advances%2520in%2520generative%2520AI%2520suggest%2520a%2520new%2520paradigm%253A%2520what%2520if%2520editing%2520a%2520video%2520were%2520as%2520straightforward%2520as%2520rewriting%2520text%253F%2520To%2520investigate%2520this%252C%2520we%2520present%2520a%2520tech%2520probe%2520and%2520a%2520study%2520on%2520text-driven%2520video%2520reauthoring.%2520Our%2520approach%2520involves%2520two%2520technical%2520contributions%253A%2520%25281%2529%2520a%2520generative%2520reconstruction%2520algorithm%2520that%2520reverse-engineers%2520video%2520into%2520an%2520editable%2520text%2520prompt%252C%2520and%2520%25282%2529%2520an%2520interactive%2520probe%252C%2520Rewrite%2520Kit%252C%2520that%2520allows%2520creators%2520to%2520manipulate%2520these%2520prompts.%2520A%2520technical%2520evaluation%2520of%2520the%2520algorithm%2520reveals%2520a%2520critical%2520human-AI%2520perceptual%2520gap.%2520A%2520probe%2520study%2520with%252012%2520creators%2520surfaced%2520novel%2520use%2520cases%2520such%2520as%2520virtual%2520reshooting%252C%2520synthetic%2520continuity%252C%2520and%2520aesthetic%2520restyling.%2520It%2520also%2520highlighted%2520key%2520tensions%2520around%2520coherence%252C%2520control%252C%2520and%2520creative%2520alignment%2520in%2520this%2520new%2520paradigm.%2520Our%2520work%2520contributes%2520empirical%2520insights%2520into%2520the%2520opportunities%2520and%2520challenges%2520of%2520text-driven%2520video%2520reauthoring%252C%2520offering%2520design%2520implications%2520for%2520future%2520co-creative%2520video%2520tools.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rewriting%20Video%3A%20Text-Driven%20Reauthoring%20of%20Video%20Footage&entry.906535625=Sitong%20Wang%20and%20Anh%20Truong%20and%20Lydia%20B.%20Chilton%20and%20Dingzeyu%20Li&entry.1292438233=Video%20is%20a%20powerful%20medium%20for%20communication%20and%20storytelling%2C%20yet%20reauthoring%20existing%20footage%20remains%20challenging.%20Even%20simple%20edits%20often%20demand%20expertise%2C%20time%2C%20and%20careful%20planning%2C%20constraining%20how%20creators%20envision%20and%20shape%20their%20narratives.%20Recent%20advances%20in%20generative%20AI%20suggest%20a%20new%20paradigm%3A%20what%20if%20editing%20a%20video%20were%20as%20straightforward%20as%20rewriting%20text%3F%20To%20investigate%20this%2C%20we%20present%20a%20tech%20probe%20and%20a%20study%20on%20text-driven%20video%20reauthoring.%20Our%20approach%20involves%20two%20technical%20contributions%3A%20%281%29%20a%20generative%20reconstruction%20algorithm%20that%20reverse-engineers%20video%20into%20an%20editable%20text%20prompt%2C%20and%20%282%29%20an%20interactive%20probe%2C%20Rewrite%20Kit%2C%20that%20allows%20creators%20to%20manipulate%20these%20prompts.%20A%20technical%20evaluation%20of%20the%20algorithm%20reveals%20a%20critical%20human-AI%20perceptual%20gap.%20A%20probe%20study%20with%2012%20creators%20surfaced%20novel%20use%20cases%20such%20as%20virtual%20reshooting%2C%20synthetic%20continuity%2C%20and%20aesthetic%20restyling.%20It%20also%20highlighted%20key%20tensions%20around%20coherence%2C%20control%2C%20and%20creative%20alignment%20in%20this%20new%20paradigm.%20Our%20work%20contributes%20empirical%20insights%20into%20the%20opportunities%20and%20challenges%20of%20text-driven%20video%20reauthoring%2C%20offering%20design%20implications%20for%20future%20co-creative%20video%20tools.&entry.1838667208=http%3A//arxiv.org/abs/2601.08565v1&entry.124074799=Read"},
{"title": "AI as Entertainment", "author": "Cody Kommers and Ari Holtzman", "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.", "link": "http://arxiv.org/abs/2601.08768v1", "date": "2026-01-13", "relevancy": 2.3507, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5159}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20as%20Entertainment&body=Title%3A%20AI%20as%20Entertainment%0AAuthor%3A%20Cody%20Kommers%20and%20Ari%20Holtzman%0AAbstract%3A%20Generative%20AI%20systems%20are%20predominantly%20designed%2C%20evaluated%2C%20and%20marketed%20as%20intelligent%20systems%20which%20will%20benefit%20society%20by%20augmenting%20or%20automating%20human%20cognitive%20labor%2C%20promising%20to%20increase%20personal%2C%20corporate%2C%20and%20macroeconomic%20productivity.%20But%20this%20mainstream%20narrative%20about%20what%20AI%20is%20and%20what%20it%20can%20do%20is%20in%20tension%20with%20another%20emerging%20use%20case%3A%20entertainment.%20We%20argue%20that%20the%20field%20of%20AI%20is%20unprepared%20to%20measure%20or%20respond%20to%20how%20the%20proliferation%20of%20entertaining%20AI-generated%20content%20will%20impact%20society.%20Emerging%20data%20suggest%20AI%20is%20already%20widely%20adopted%20for%20entertainment%20purposes%20--%20especially%20by%20young%20people%20--%20and%20represents%20a%20large%20potential%20source%20of%20revenue.%20We%20contend%20that%20entertainment%20will%20become%20a%20primary%20business%20model%20for%20major%20AI%20corporations%20seeking%20returns%20on%20massive%20infrastructure%20investments%3B%20this%20will%20exert%20a%20powerful%20influence%20on%20the%20technology%20these%20companies%20produce%20in%20the%20coming%20years.%20Examining%20current%20evaluation%20practices%2C%20we%20identify%20a%20critical%20asymmetry%3A%20while%20AI%20assessments%20rigorously%20measure%20both%20benefits%20and%20harms%20of%20intelligence%2C%20they%20focus%20almost%20exclusively%20on%20cultural%20harms.%20We%20lack%20frameworks%20for%20articulating%20how%20cultural%20outputs%20might%20be%20actively%20beneficial.%20Drawing%20on%20insights%20from%20the%20humanities%2C%20we%20propose%20%22thick%20entertainment%22%20as%20a%20framework%20for%20evaluating%20AI-generated%20cultural%20content%20--%20one%20that%20considers%20entertainment%27s%20role%20in%20meaning-making%2C%20identity%20formation%2C%20and%20social%20connection%20rather%20than%20simply%20minimizing%20harm.%20While%20AI%20is%20often%20touted%20for%20its%20potential%20to%20revolutionize%20productivity%2C%20in%20the%20long%20run%20we%20may%20find%20that%20AI%20turns%20out%20to%20be%20as%20much%20about%20%22intelligence%22%20as%20social%20media%20is%20about%20social%20connection.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520as%2520Entertainment%26entry.906535625%3DCody%2520Kommers%2520and%2520Ari%2520Holtzman%26entry.1292438233%3DGenerative%2520AI%2520systems%2520are%2520predominantly%2520designed%252C%2520evaluated%252C%2520and%2520marketed%2520as%2520intelligent%2520systems%2520which%2520will%2520benefit%2520society%2520by%2520augmenting%2520or%2520automating%2520human%2520cognitive%2520labor%252C%2520promising%2520to%2520increase%2520personal%252C%2520corporate%252C%2520and%2520macroeconomic%2520productivity.%2520But%2520this%2520mainstream%2520narrative%2520about%2520what%2520AI%2520is%2520and%2520what%2520it%2520can%2520do%2520is%2520in%2520tension%2520with%2520another%2520emerging%2520use%2520case%253A%2520entertainment.%2520We%2520argue%2520that%2520the%2520field%2520of%2520AI%2520is%2520unprepared%2520to%2520measure%2520or%2520respond%2520to%2520how%2520the%2520proliferation%2520of%2520entertaining%2520AI-generated%2520content%2520will%2520impact%2520society.%2520Emerging%2520data%2520suggest%2520AI%2520is%2520already%2520widely%2520adopted%2520for%2520entertainment%2520purposes%2520--%2520especially%2520by%2520young%2520people%2520--%2520and%2520represents%2520a%2520large%2520potential%2520source%2520of%2520revenue.%2520We%2520contend%2520that%2520entertainment%2520will%2520become%2520a%2520primary%2520business%2520model%2520for%2520major%2520AI%2520corporations%2520seeking%2520returns%2520on%2520massive%2520infrastructure%2520investments%253B%2520this%2520will%2520exert%2520a%2520powerful%2520influence%2520on%2520the%2520technology%2520these%2520companies%2520produce%2520in%2520the%2520coming%2520years.%2520Examining%2520current%2520evaluation%2520practices%252C%2520we%2520identify%2520a%2520critical%2520asymmetry%253A%2520while%2520AI%2520assessments%2520rigorously%2520measure%2520both%2520benefits%2520and%2520harms%2520of%2520intelligence%252C%2520they%2520focus%2520almost%2520exclusively%2520on%2520cultural%2520harms.%2520We%2520lack%2520frameworks%2520for%2520articulating%2520how%2520cultural%2520outputs%2520might%2520be%2520actively%2520beneficial.%2520Drawing%2520on%2520insights%2520from%2520the%2520humanities%252C%2520we%2520propose%2520%2522thick%2520entertainment%2522%2520as%2520a%2520framework%2520for%2520evaluating%2520AI-generated%2520cultural%2520content%2520--%2520one%2520that%2520considers%2520entertainment%2527s%2520role%2520in%2520meaning-making%252C%2520identity%2520formation%252C%2520and%2520social%2520connection%2520rather%2520than%2520simply%2520minimizing%2520harm.%2520While%2520AI%2520is%2520often%2520touted%2520for%2520its%2520potential%2520to%2520revolutionize%2520productivity%252C%2520in%2520the%2520long%2520run%2520we%2520may%2520find%2520that%2520AI%2520turns%2520out%2520to%2520be%2520as%2520much%2520about%2520%2522intelligence%2522%2520as%2520social%2520media%2520is%2520about%2520social%2520connection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20as%20Entertainment&entry.906535625=Cody%20Kommers%20and%20Ari%20Holtzman&entry.1292438233=Generative%20AI%20systems%20are%20predominantly%20designed%2C%20evaluated%2C%20and%20marketed%20as%20intelligent%20systems%20which%20will%20benefit%20society%20by%20augmenting%20or%20automating%20human%20cognitive%20labor%2C%20promising%20to%20increase%20personal%2C%20corporate%2C%20and%20macroeconomic%20productivity.%20But%20this%20mainstream%20narrative%20about%20what%20AI%20is%20and%20what%20it%20can%20do%20is%20in%20tension%20with%20another%20emerging%20use%20case%3A%20entertainment.%20We%20argue%20that%20the%20field%20of%20AI%20is%20unprepared%20to%20measure%20or%20respond%20to%20how%20the%20proliferation%20of%20entertaining%20AI-generated%20content%20will%20impact%20society.%20Emerging%20data%20suggest%20AI%20is%20already%20widely%20adopted%20for%20entertainment%20purposes%20--%20especially%20by%20young%20people%20--%20and%20represents%20a%20large%20potential%20source%20of%20revenue.%20We%20contend%20that%20entertainment%20will%20become%20a%20primary%20business%20model%20for%20major%20AI%20corporations%20seeking%20returns%20on%20massive%20infrastructure%20investments%3B%20this%20will%20exert%20a%20powerful%20influence%20on%20the%20technology%20these%20companies%20produce%20in%20the%20coming%20years.%20Examining%20current%20evaluation%20practices%2C%20we%20identify%20a%20critical%20asymmetry%3A%20while%20AI%20assessments%20rigorously%20measure%20both%20benefits%20and%20harms%20of%20intelligence%2C%20they%20focus%20almost%20exclusively%20on%20cultural%20harms.%20We%20lack%20frameworks%20for%20articulating%20how%20cultural%20outputs%20might%20be%20actively%20beneficial.%20Drawing%20on%20insights%20from%20the%20humanities%2C%20we%20propose%20%22thick%20entertainment%22%20as%20a%20framework%20for%20evaluating%20AI-generated%20cultural%20content%20--%20one%20that%20considers%20entertainment%27s%20role%20in%20meaning-making%2C%20identity%20formation%2C%20and%20social%20connection%20rather%20than%20simply%20minimizing%20harm.%20While%20AI%20is%20often%20touted%20for%20its%20potential%20to%20revolutionize%20productivity%2C%20in%20the%20long%20run%20we%20may%20find%20that%20AI%20turns%20out%20to%20be%20as%20much%20about%20%22intelligence%22%20as%20social%20media%20is%20about%20social%20connection.&entry.1838667208=http%3A//arxiv.org/abs/2601.08768v1&entry.124074799=Read"},
{"title": "sui-1: Grounded and Verifiable Long-Form Summarization", "author": "Benedikt Droste and Jan Philipp Harries and Maximilian Idahl and Bj\u00f6rn Pl\u00fcster", "abstract": "Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.", "link": "http://arxiv.org/abs/2601.08472v1", "date": "2026-01-13", "relevancy": 2.3469, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20sui-1%3A%20Grounded%20and%20Verifiable%20Long-Form%20Summarization&body=Title%3A%20sui-1%3A%20Grounded%20and%20Verifiable%20Long-Form%20Summarization%0AAuthor%3A%20Benedikt%20Droste%20and%20Jan%20Philipp%20Harries%20and%20Maximilian%20Idahl%20and%20Bj%C3%B6rn%20Pl%C3%BCster%0AAbstract%3A%20Large%20language%20models%20frequently%20generate%20plausible%20but%20unfaithful%20summaries%20that%20users%20cannot%20verify%20against%20source%20text%2C%20a%20critical%20limitation%20in%20compliance-sensitive%20domains%20such%20as%20government%20and%20legal%20analysis.%20We%20present%20sui-1%2C%20a%2024B%20parameter%20model%20that%20produces%20abstractive%20summaries%20with%20inline%20citations%2C%20enabling%20users%20to%20trace%20each%20claim%20to%20its%20source%20sentence.%20Our%20synthetic%20data%20pipeline%20combines%20chain-of-thought%20prompting%20with%20multi-stage%20verification%2C%20generating%20over%2022%2C000%20high-quality%20training%20examples%20across%20five%20languages%20from%20diverse%20sources%20including%20parliamentary%20documents%2C%20web%20text%2C%20and%20Wikipedia.%20Evaluation%20shows%20sui-1%20significantly%20outperforms%20all%20tested%20open-weight%20baselines%2C%20including%20models%20with%203x%20more%20parameters.%20These%20results%20demonstrate%20that%20task-specific%20training%20substantially%20outperforms%20scale%20alone%20for%20citation-grounded%20summarization.%20Model%20weights%20and%20an%20interactive%20demo%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsui-1%253A%2520Grounded%2520and%2520Verifiable%2520Long-Form%2520Summarization%26entry.906535625%3DBenedikt%2520Droste%2520and%2520Jan%2520Philipp%2520Harries%2520and%2520Maximilian%2520Idahl%2520and%2520Bj%25C3%25B6rn%2520Pl%25C3%25BCster%26entry.1292438233%3DLarge%2520language%2520models%2520frequently%2520generate%2520plausible%2520but%2520unfaithful%2520summaries%2520that%2520users%2520cannot%2520verify%2520against%2520source%2520text%252C%2520a%2520critical%2520limitation%2520in%2520compliance-sensitive%2520domains%2520such%2520as%2520government%2520and%2520legal%2520analysis.%2520We%2520present%2520sui-1%252C%2520a%252024B%2520parameter%2520model%2520that%2520produces%2520abstractive%2520summaries%2520with%2520inline%2520citations%252C%2520enabling%2520users%2520to%2520trace%2520each%2520claim%2520to%2520its%2520source%2520sentence.%2520Our%2520synthetic%2520data%2520pipeline%2520combines%2520chain-of-thought%2520prompting%2520with%2520multi-stage%2520verification%252C%2520generating%2520over%252022%252C000%2520high-quality%2520training%2520examples%2520across%2520five%2520languages%2520from%2520diverse%2520sources%2520including%2520parliamentary%2520documents%252C%2520web%2520text%252C%2520and%2520Wikipedia.%2520Evaluation%2520shows%2520sui-1%2520significantly%2520outperforms%2520all%2520tested%2520open-weight%2520baselines%252C%2520including%2520models%2520with%25203x%2520more%2520parameters.%2520These%2520results%2520demonstrate%2520that%2520task-specific%2520training%2520substantially%2520outperforms%2520scale%2520alone%2520for%2520citation-grounded%2520summarization.%2520Model%2520weights%2520and%2520an%2520interactive%2520demo%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sui-1%3A%20Grounded%20and%20Verifiable%20Long-Form%20Summarization&entry.906535625=Benedikt%20Droste%20and%20Jan%20Philipp%20Harries%20and%20Maximilian%20Idahl%20and%20Bj%C3%B6rn%20Pl%C3%BCster&entry.1292438233=Large%20language%20models%20frequently%20generate%20plausible%20but%20unfaithful%20summaries%20that%20users%20cannot%20verify%20against%20source%20text%2C%20a%20critical%20limitation%20in%20compliance-sensitive%20domains%20such%20as%20government%20and%20legal%20analysis.%20We%20present%20sui-1%2C%20a%2024B%20parameter%20model%20that%20produces%20abstractive%20summaries%20with%20inline%20citations%2C%20enabling%20users%20to%20trace%20each%20claim%20to%20its%20source%20sentence.%20Our%20synthetic%20data%20pipeline%20combines%20chain-of-thought%20prompting%20with%20multi-stage%20verification%2C%20generating%20over%2022%2C000%20high-quality%20training%20examples%20across%20five%20languages%20from%20diverse%20sources%20including%20parliamentary%20documents%2C%20web%20text%2C%20and%20Wikipedia.%20Evaluation%20shows%20sui-1%20significantly%20outperforms%20all%20tested%20open-weight%20baselines%2C%20including%20models%20with%203x%20more%20parameters.%20These%20results%20demonstrate%20that%20task-specific%20training%20substantially%20outperforms%20scale%20alone%20for%20citation-grounded%20summarization.%20Model%20weights%20and%20an%20interactive%20demo%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.08472v1&entry.124074799=Read"},
{"title": "PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning", "author": "Kexin Baoa and Fanzhao Lin and Zichen Wang and Yong Li and Dan Zeng and Shiming Ge", "abstract": "Few-shot class-incremental learning (FSCIL) aims to continually adapt a model on a limited number of new-class examples, facing two well-known challenges: catastrophic forgetting and overfitting to new classes. Existing methods tend to freeze more parts of network components and finetune others with an extra memory during incremental sessions. These methods emphasize preserving prior knowledge to ensure proficiency in recognizing old classes, thereby mitigating catastrophic forgetting. Meanwhile, constraining fewer parameters can help in overcoming overfitting with the assistance of prior knowledge. Following previous methods, we retain more prior knowledge and propose a prior knowledge-infused neural network (PKI) to facilitate FSCIL. PKI consists of a backbone, an ensemble of projectors, a classifier, and an extra memory. In each incremental session, we build a new projector and add it to the ensemble. Subsequently, we finetune the new projector and the classifier jointly with other frozen network components, ensuring the rich prior knowledge is utilized effectively. By cascading projectors, PKI integrates prior knowledge accumulated from previous sessions and learns new knowledge flexibly, which helps to recognize old classes and efficiently learn new classes. Further, to reduce the resource consumption associated with keeping many projectors, we design two variants of the prior knowledge-infused neural network (PKIV-1 and PKIV-2) to trade off a balance between resource consumption and performance by reducing the number of projectors. Extensive experiments on three popular benchmarks demonstrate that our approach outperforms state-of-the-art methods.", "link": "http://arxiv.org/abs/2601.08493v1", "date": "2026-01-13", "relevancy": 2.3451, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4716}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PKI%3A%20Prior%20Knowledge-Infused%20Neural%20Network%20for%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20PKI%3A%20Prior%20Knowledge-Infused%20Neural%20Network%20for%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Kexin%20Baoa%20and%20Fanzhao%20Lin%20and%20Zichen%20Wang%20and%20Yong%20Li%20and%20Dan%20Zeng%20and%20Shiming%20Ge%0AAbstract%3A%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20aims%20to%20continually%20adapt%20a%20model%20on%20a%20limited%20number%20of%20new-class%20examples%2C%20facing%20two%20well-known%20challenges%3A%20catastrophic%20forgetting%20and%20overfitting%20to%20new%20classes.%20Existing%20methods%20tend%20to%20freeze%20more%20parts%20of%20network%20components%20and%20finetune%20others%20with%20an%20extra%20memory%20during%20incremental%20sessions.%20These%20methods%20emphasize%20preserving%20prior%20knowledge%20to%20ensure%20proficiency%20in%20recognizing%20old%20classes%2C%20thereby%20mitigating%20catastrophic%20forgetting.%20Meanwhile%2C%20constraining%20fewer%20parameters%20can%20help%20in%20overcoming%20overfitting%20with%20the%20assistance%20of%20prior%20knowledge.%20Following%20previous%20methods%2C%20we%20retain%20more%20prior%20knowledge%20and%20propose%20a%20prior%20knowledge-infused%20neural%20network%20%28PKI%29%20to%20facilitate%20FSCIL.%20PKI%20consists%20of%20a%20backbone%2C%20an%20ensemble%20of%20projectors%2C%20a%20classifier%2C%20and%20an%20extra%20memory.%20In%20each%20incremental%20session%2C%20we%20build%20a%20new%20projector%20and%20add%20it%20to%20the%20ensemble.%20Subsequently%2C%20we%20finetune%20the%20new%20projector%20and%20the%20classifier%20jointly%20with%20other%20frozen%20network%20components%2C%20ensuring%20the%20rich%20prior%20knowledge%20is%20utilized%20effectively.%20By%20cascading%20projectors%2C%20PKI%20integrates%20prior%20knowledge%20accumulated%20from%20previous%20sessions%20and%20learns%20new%20knowledge%20flexibly%2C%20which%20helps%20to%20recognize%20old%20classes%20and%20efficiently%20learn%20new%20classes.%20Further%2C%20to%20reduce%20the%20resource%20consumption%20associated%20with%20keeping%20many%20projectors%2C%20we%20design%20two%20variants%20of%20the%20prior%20knowledge-infused%20neural%20network%20%28PKIV-1%20and%20PKIV-2%29%20to%20trade%20off%20a%20balance%20between%20resource%20consumption%20and%20performance%20by%20reducing%20the%20number%20of%20projectors.%20Extensive%20experiments%20on%20three%20popular%20benchmarks%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPKI%253A%2520Prior%2520Knowledge-Infused%2520Neural%2520Network%2520for%2520Few-Shot%2520Class-Incremental%2520Learning%26entry.906535625%3DKexin%2520Baoa%2520and%2520Fanzhao%2520Lin%2520and%2520Zichen%2520Wang%2520and%2520Yong%2520Li%2520and%2520Dan%2520Zeng%2520and%2520Shiming%2520Ge%26entry.1292438233%3DFew-shot%2520class-incremental%2520learning%2520%2528FSCIL%2529%2520aims%2520to%2520continually%2520adapt%2520a%2520model%2520on%2520a%2520limited%2520number%2520of%2520new-class%2520examples%252C%2520facing%2520two%2520well-known%2520challenges%253A%2520catastrophic%2520forgetting%2520and%2520overfitting%2520to%2520new%2520classes.%2520Existing%2520methods%2520tend%2520to%2520freeze%2520more%2520parts%2520of%2520network%2520components%2520and%2520finetune%2520others%2520with%2520an%2520extra%2520memory%2520during%2520incremental%2520sessions.%2520These%2520methods%2520emphasize%2520preserving%2520prior%2520knowledge%2520to%2520ensure%2520proficiency%2520in%2520recognizing%2520old%2520classes%252C%2520thereby%2520mitigating%2520catastrophic%2520forgetting.%2520Meanwhile%252C%2520constraining%2520fewer%2520parameters%2520can%2520help%2520in%2520overcoming%2520overfitting%2520with%2520the%2520assistance%2520of%2520prior%2520knowledge.%2520Following%2520previous%2520methods%252C%2520we%2520retain%2520more%2520prior%2520knowledge%2520and%2520propose%2520a%2520prior%2520knowledge-infused%2520neural%2520network%2520%2528PKI%2529%2520to%2520facilitate%2520FSCIL.%2520PKI%2520consists%2520of%2520a%2520backbone%252C%2520an%2520ensemble%2520of%2520projectors%252C%2520a%2520classifier%252C%2520and%2520an%2520extra%2520memory.%2520In%2520each%2520incremental%2520session%252C%2520we%2520build%2520a%2520new%2520projector%2520and%2520add%2520it%2520to%2520the%2520ensemble.%2520Subsequently%252C%2520we%2520finetune%2520the%2520new%2520projector%2520and%2520the%2520classifier%2520jointly%2520with%2520other%2520frozen%2520network%2520components%252C%2520ensuring%2520the%2520rich%2520prior%2520knowledge%2520is%2520utilized%2520effectively.%2520By%2520cascading%2520projectors%252C%2520PKI%2520integrates%2520prior%2520knowledge%2520accumulated%2520from%2520previous%2520sessions%2520and%2520learns%2520new%2520knowledge%2520flexibly%252C%2520which%2520helps%2520to%2520recognize%2520old%2520classes%2520and%2520efficiently%2520learn%2520new%2520classes.%2520Further%252C%2520to%2520reduce%2520the%2520resource%2520consumption%2520associated%2520with%2520keeping%2520many%2520projectors%252C%2520we%2520design%2520two%2520variants%2520of%2520the%2520prior%2520knowledge-infused%2520neural%2520network%2520%2528PKIV-1%2520and%2520PKIV-2%2529%2520to%2520trade%2520off%2520a%2520balance%2520between%2520resource%2520consumption%2520and%2520performance%2520by%2520reducing%2520the%2520number%2520of%2520projectors.%2520Extensive%2520experiments%2520on%2520three%2520popular%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PKI%3A%20Prior%20Knowledge-Infused%20Neural%20Network%20for%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Kexin%20Baoa%20and%20Fanzhao%20Lin%20and%20Zichen%20Wang%20and%20Yong%20Li%20and%20Dan%20Zeng%20and%20Shiming%20Ge&entry.1292438233=Few-shot%20class-incremental%20learning%20%28FSCIL%29%20aims%20to%20continually%20adapt%20a%20model%20on%20a%20limited%20number%20of%20new-class%20examples%2C%20facing%20two%20well-known%20challenges%3A%20catastrophic%20forgetting%20and%20overfitting%20to%20new%20classes.%20Existing%20methods%20tend%20to%20freeze%20more%20parts%20of%20network%20components%20and%20finetune%20others%20with%20an%20extra%20memory%20during%20incremental%20sessions.%20These%20methods%20emphasize%20preserving%20prior%20knowledge%20to%20ensure%20proficiency%20in%20recognizing%20old%20classes%2C%20thereby%20mitigating%20catastrophic%20forgetting.%20Meanwhile%2C%20constraining%20fewer%20parameters%20can%20help%20in%20overcoming%20overfitting%20with%20the%20assistance%20of%20prior%20knowledge.%20Following%20previous%20methods%2C%20we%20retain%20more%20prior%20knowledge%20and%20propose%20a%20prior%20knowledge-infused%20neural%20network%20%28PKI%29%20to%20facilitate%20FSCIL.%20PKI%20consists%20of%20a%20backbone%2C%20an%20ensemble%20of%20projectors%2C%20a%20classifier%2C%20and%20an%20extra%20memory.%20In%20each%20incremental%20session%2C%20we%20build%20a%20new%20projector%20and%20add%20it%20to%20the%20ensemble.%20Subsequently%2C%20we%20finetune%20the%20new%20projector%20and%20the%20classifier%20jointly%20with%20other%20frozen%20network%20components%2C%20ensuring%20the%20rich%20prior%20knowledge%20is%20utilized%20effectively.%20By%20cascading%20projectors%2C%20PKI%20integrates%20prior%20knowledge%20accumulated%20from%20previous%20sessions%20and%20learns%20new%20knowledge%20flexibly%2C%20which%20helps%20to%20recognize%20old%20classes%20and%20efficiently%20learn%20new%20classes.%20Further%2C%20to%20reduce%20the%20resource%20consumption%20associated%20with%20keeping%20many%20projectors%2C%20we%20design%20two%20variants%20of%20the%20prior%20knowledge-infused%20neural%20network%20%28PKIV-1%20and%20PKIV-2%29%20to%20trade%20off%20a%20balance%20between%20resource%20consumption%20and%20performance%20by%20reducing%20the%20number%20of%20projectors.%20Extensive%20experiments%20on%20three%20popular%20benchmarks%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.08493v1&entry.124074799=Read"},
{"title": "TROJail: Trajectory-Level Optimization for Multi-Turn Large Language Model Jailbreaks with Process Rewards", "author": "Xiqiao Xiong and Ouxiang Li and Zhuo Liu and Moxin Li and Wentao Shi and Fengbin Zhu and Qifan Wang and Fuli Feng", "abstract": "Large language models have seen widespread adoption, yet they remain vulnerable to multi-turn jailbreak attacks, threatening their safe deployment. This has led to the task of training automated multi-turn attackers to probe model safety vulnerabilities. However, existing approaches typically rely on turn-level optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate this task as a multi-turn reinforcement learning problem, directly optimizing the harmfulness of the final-turn response as the outcome reward. To address the sparse supervision of the outcome reward, we introduce TROJail, which employs two process rewards to evaluate the utility of intermediate prompts and integrate them into advantage estimation. These rewards (1) penalize overly harmful prompts that trigger the model's refusal mechanism, and (2) encourage steering the semantic relevance of responses toward the targeted harmful content. Experimental results show improved attack success rates across multiple models and benchmarks, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/TROJail. Warning: This paper contains examples of harmful content.", "link": "http://arxiv.org/abs/2512.07761v2", "date": "2026-01-13", "relevancy": 2.3399, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TROJail%3A%20Trajectory-Level%20Optimization%20for%20Multi-Turn%20Large%20Language%20Model%20Jailbreaks%20with%20Process%20Rewards&body=Title%3A%20TROJail%3A%20Trajectory-Level%20Optimization%20for%20Multi-Turn%20Large%20Language%20Model%20Jailbreaks%20with%20Process%20Rewards%0AAuthor%3A%20Xiqiao%20Xiong%20and%20Ouxiang%20Li%20and%20Zhuo%20Liu%20and%20Moxin%20Li%20and%20Wentao%20Shi%20and%20Fengbin%20Zhu%20and%20Qifan%20Wang%20and%20Fuli%20Feng%0AAbstract%3A%20Large%20language%20models%20have%20seen%20widespread%20adoption%2C%20yet%20they%20remain%20vulnerable%20to%20multi-turn%20jailbreak%20attacks%2C%20threatening%20their%20safe%20deployment.%20This%20has%20led%20to%20the%20task%20of%20training%20automated%20multi-turn%20attackers%20to%20probe%20model%20safety%20vulnerabilities.%20However%2C%20existing%20approaches%20typically%20rely%20on%20turn-level%20optimization%2C%20which%20is%20insufficient%20for%20learning%20long-term%20attack%20strategies.%20To%20bridge%20this%20gap%2C%20we%20formulate%20this%20task%20as%20a%20multi-turn%20reinforcement%20learning%20problem%2C%20directly%20optimizing%20the%20harmfulness%20of%20the%20final-turn%20response%20as%20the%20outcome%20reward.%20To%20address%20the%20sparse%20supervision%20of%20the%20outcome%20reward%2C%20we%20introduce%20TROJail%2C%20which%20employs%20two%20process%20rewards%20to%20evaluate%20the%20utility%20of%20intermediate%20prompts%20and%20integrate%20them%20into%20advantage%20estimation.%20These%20rewards%20%281%29%20penalize%20overly%20harmful%20prompts%20that%20trigger%20the%20model%27s%20refusal%20mechanism%2C%20and%20%282%29%20encourage%20steering%20the%20semantic%20relevance%20of%20responses%20toward%20the%20targeted%20harmful%20content.%20Experimental%20results%20show%20improved%20attack%20success%20rates%20across%20multiple%20models%20and%20benchmarks%2C%20highlighting%20the%20effectiveness%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/xxiqiao/TROJail.%20Warning%3A%20This%20paper%20contains%20examples%20of%20harmful%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTROJail%253A%2520Trajectory-Level%2520Optimization%2520for%2520Multi-Turn%2520Large%2520Language%2520Model%2520Jailbreaks%2520with%2520Process%2520Rewards%26entry.906535625%3DXiqiao%2520Xiong%2520and%2520Ouxiang%2520Li%2520and%2520Zhuo%2520Liu%2520and%2520Moxin%2520Li%2520and%2520Wentao%2520Shi%2520and%2520Fengbin%2520Zhu%2520and%2520Qifan%2520Wang%2520and%2520Fuli%2520Feng%26entry.1292438233%3DLarge%2520language%2520models%2520have%2520seen%2520widespread%2520adoption%252C%2520yet%2520they%2520remain%2520vulnerable%2520to%2520multi-turn%2520jailbreak%2520attacks%252C%2520threatening%2520their%2520safe%2520deployment.%2520This%2520has%2520led%2520to%2520the%2520task%2520of%2520training%2520automated%2520multi-turn%2520attackers%2520to%2520probe%2520model%2520safety%2520vulnerabilities.%2520However%252C%2520existing%2520approaches%2520typically%2520rely%2520on%2520turn-level%2520optimization%252C%2520which%2520is%2520insufficient%2520for%2520learning%2520long-term%2520attack%2520strategies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520formulate%2520this%2520task%2520as%2520a%2520multi-turn%2520reinforcement%2520learning%2520problem%252C%2520directly%2520optimizing%2520the%2520harmfulness%2520of%2520the%2520final-turn%2520response%2520as%2520the%2520outcome%2520reward.%2520To%2520address%2520the%2520sparse%2520supervision%2520of%2520the%2520outcome%2520reward%252C%2520we%2520introduce%2520TROJail%252C%2520which%2520employs%2520two%2520process%2520rewards%2520to%2520evaluate%2520the%2520utility%2520of%2520intermediate%2520prompts%2520and%2520integrate%2520them%2520into%2520advantage%2520estimation.%2520These%2520rewards%2520%25281%2529%2520penalize%2520overly%2520harmful%2520prompts%2520that%2520trigger%2520the%2520model%2527s%2520refusal%2520mechanism%252C%2520and%2520%25282%2529%2520encourage%2520steering%2520the%2520semantic%2520relevance%2520of%2520responses%2520toward%2520the%2520targeted%2520harmful%2520content.%2520Experimental%2520results%2520show%2520improved%2520attack%2520success%2520rates%2520across%2520multiple%2520models%2520and%2520benchmarks%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520approach.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/xxiqiao/TROJail.%2520Warning%253A%2520This%2520paper%2520contains%2520examples%2520of%2520harmful%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TROJail%3A%20Trajectory-Level%20Optimization%20for%20Multi-Turn%20Large%20Language%20Model%20Jailbreaks%20with%20Process%20Rewards&entry.906535625=Xiqiao%20Xiong%20and%20Ouxiang%20Li%20and%20Zhuo%20Liu%20and%20Moxin%20Li%20and%20Wentao%20Shi%20and%20Fengbin%20Zhu%20and%20Qifan%20Wang%20and%20Fuli%20Feng&entry.1292438233=Large%20language%20models%20have%20seen%20widespread%20adoption%2C%20yet%20they%20remain%20vulnerable%20to%20multi-turn%20jailbreak%20attacks%2C%20threatening%20their%20safe%20deployment.%20This%20has%20led%20to%20the%20task%20of%20training%20automated%20multi-turn%20attackers%20to%20probe%20model%20safety%20vulnerabilities.%20However%2C%20existing%20approaches%20typically%20rely%20on%20turn-level%20optimization%2C%20which%20is%20insufficient%20for%20learning%20long-term%20attack%20strategies.%20To%20bridge%20this%20gap%2C%20we%20formulate%20this%20task%20as%20a%20multi-turn%20reinforcement%20learning%20problem%2C%20directly%20optimizing%20the%20harmfulness%20of%20the%20final-turn%20response%20as%20the%20outcome%20reward.%20To%20address%20the%20sparse%20supervision%20of%20the%20outcome%20reward%2C%20we%20introduce%20TROJail%2C%20which%20employs%20two%20process%20rewards%20to%20evaluate%20the%20utility%20of%20intermediate%20prompts%20and%20integrate%20them%20into%20advantage%20estimation.%20These%20rewards%20%281%29%20penalize%20overly%20harmful%20prompts%20that%20trigger%20the%20model%27s%20refusal%20mechanism%2C%20and%20%282%29%20encourage%20steering%20the%20semantic%20relevance%20of%20responses%20toward%20the%20targeted%20harmful%20content.%20Experimental%20results%20show%20improved%20attack%20success%20rates%20across%20multiple%20models%20and%20benchmarks%2C%20highlighting%20the%20effectiveness%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/xxiqiao/TROJail.%20Warning%3A%20This%20paper%20contains%20examples%20of%20harmful%20content.&entry.1838667208=http%3A//arxiv.org/abs/2512.07761v2&entry.124074799=Read"},
{"title": "CausAdv: A Causal-based Framework for Detecting Adversarial Examples", "author": "Hichem Debbi", "abstract": "Deep learning has led to tremendous success in computer vision, largely due to Convolutional Neural Networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations. This vulnerability of adversarial examples has has motivated research into improving model robustness through adversarial detection and defense methods. In this paper, we address the adversarial robustness of CNNs through causal reasoning. We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns both causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. We then perform a statistical analysis of the filters' CI across clean and adversarial samples, to demonstrate that adversarial examples exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarial detection without the need to train a separate detector. Moreover, we illustrate the efficiency of causal explanations as a helpful detection tool by visualizing the extracted causal features.", "link": "http://arxiv.org/abs/2411.00839v2", "date": "2026-01-13", "relevancy": 2.3281, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4689}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4678}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausAdv%3A%20A%20Causal-based%20Framework%20for%20Detecting%20Adversarial%20Examples&body=Title%3A%20CausAdv%3A%20A%20Causal-based%20Framework%20for%20Detecting%20Adversarial%20Examples%0AAuthor%3A%20Hichem%20Debbi%0AAbstract%3A%20Deep%20learning%20has%20led%20to%20tremendous%20success%20in%20computer%20vision%2C%20largely%20due%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20However%2C%20CNNs%20have%20been%20shown%20to%20be%20vulnerable%20to%20crafted%20adversarial%20perturbations.%20This%20vulnerability%20of%20adversarial%20examples%20has%20has%20motivated%20research%20into%20improving%20model%20robustness%20through%20adversarial%20detection%20and%20defense%20methods.%20In%20this%20paper%2C%20we%20address%20the%20adversarial%20robustness%20of%20CNNs%20through%20causal%20reasoning.%20We%20propose%20CausAdv%3A%20a%20causal%20framework%20for%20detecting%20adversarial%20examples%20based%20on%20counterfactual%20reasoning.%20CausAdv%20learns%20both%20causal%20and%20non-causal%20features%20of%20every%20input%2C%20and%20quantifies%20the%20counterfactual%20information%20%28CI%29%20of%20every%20filter%20of%20the%20last%20convolutional%20layer.%20We%20then%20perform%20a%20statistical%20analysis%20of%20the%20filters%27%20CI%20across%20clean%20and%20adversarial%20samples%2C%20to%20demonstrate%20that%20adversarial%20examples%20exhibit%20different%20CI%20distributions%20compared%20to%20clean%20samples.%20Our%20results%20show%20that%20causal%20reasoning%20enhances%20the%20process%20of%20adversarial%20detection%20without%20the%20need%20to%20train%20a%20separate%20detector.%20Moreover%2C%20we%20illustrate%20the%20efficiency%20of%20causal%20explanations%20as%20a%20helpful%20detection%20tool%20by%20visualizing%20the%20extracted%20causal%20features.%0ALink%3A%20http%3A//arxiv.org/abs/2411.00839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausAdv%253A%2520A%2520Causal-based%2520Framework%2520for%2520Detecting%2520Adversarial%2520Examples%26entry.906535625%3DHichem%2520Debbi%26entry.1292438233%3DDeep%2520learning%2520has%2520led%2520to%2520tremendous%2520success%2520in%2520computer%2520vision%252C%2520largely%2520due%2520to%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529.%2520However%252C%2520CNNs%2520have%2520been%2520shown%2520to%2520be%2520vulnerable%2520to%2520crafted%2520adversarial%2520perturbations.%2520This%2520vulnerability%2520of%2520adversarial%2520examples%2520has%2520has%2520motivated%2520research%2520into%2520improving%2520model%2520robustness%2520through%2520adversarial%2520detection%2520and%2520defense%2520methods.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520adversarial%2520robustness%2520of%2520CNNs%2520through%2520causal%2520reasoning.%2520We%2520propose%2520CausAdv%253A%2520a%2520causal%2520framework%2520for%2520detecting%2520adversarial%2520examples%2520based%2520on%2520counterfactual%2520reasoning.%2520CausAdv%2520learns%2520both%2520causal%2520and%2520non-causal%2520features%2520of%2520every%2520input%252C%2520and%2520quantifies%2520the%2520counterfactual%2520information%2520%2528CI%2529%2520of%2520every%2520filter%2520of%2520the%2520last%2520convolutional%2520layer.%2520We%2520then%2520perform%2520a%2520statistical%2520analysis%2520of%2520the%2520filters%2527%2520CI%2520across%2520clean%2520and%2520adversarial%2520samples%252C%2520to%2520demonstrate%2520that%2520adversarial%2520examples%2520exhibit%2520different%2520CI%2520distributions%2520compared%2520to%2520clean%2520samples.%2520Our%2520results%2520show%2520that%2520causal%2520reasoning%2520enhances%2520the%2520process%2520of%2520adversarial%2520detection%2520without%2520the%2520need%2520to%2520train%2520a%2520separate%2520detector.%2520Moreover%252C%2520we%2520illustrate%2520the%2520efficiency%2520of%2520causal%2520explanations%2520as%2520a%2520helpful%2520detection%2520tool%2520by%2520visualizing%2520the%2520extracted%2520causal%2520features.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausAdv%3A%20A%20Causal-based%20Framework%20for%20Detecting%20Adversarial%20Examples&entry.906535625=Hichem%20Debbi&entry.1292438233=Deep%20learning%20has%20led%20to%20tremendous%20success%20in%20computer%20vision%2C%20largely%20due%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20However%2C%20CNNs%20have%20been%20shown%20to%20be%20vulnerable%20to%20crafted%20adversarial%20perturbations.%20This%20vulnerability%20of%20adversarial%20examples%20has%20has%20motivated%20research%20into%20improving%20model%20robustness%20through%20adversarial%20detection%20and%20defense%20methods.%20In%20this%20paper%2C%20we%20address%20the%20adversarial%20robustness%20of%20CNNs%20through%20causal%20reasoning.%20We%20propose%20CausAdv%3A%20a%20causal%20framework%20for%20detecting%20adversarial%20examples%20based%20on%20counterfactual%20reasoning.%20CausAdv%20learns%20both%20causal%20and%20non-causal%20features%20of%20every%20input%2C%20and%20quantifies%20the%20counterfactual%20information%20%28CI%29%20of%20every%20filter%20of%20the%20last%20convolutional%20layer.%20We%20then%20perform%20a%20statistical%20analysis%20of%20the%20filters%27%20CI%20across%20clean%20and%20adversarial%20samples%2C%20to%20demonstrate%20that%20adversarial%20examples%20exhibit%20different%20CI%20distributions%20compared%20to%20clean%20samples.%20Our%20results%20show%20that%20causal%20reasoning%20enhances%20the%20process%20of%20adversarial%20detection%20without%20the%20need%20to%20train%20a%20separate%20detector.%20Moreover%2C%20we%20illustrate%20the%20efficiency%20of%20causal%20explanations%20as%20a%20helpful%20detection%20tool%20by%20visualizing%20the%20extracted%20causal%20features.&entry.1838667208=http%3A//arxiv.org/abs/2411.00839v2&entry.124074799=Read"},
{"title": "Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching", "author": "Maayan Yesharim and R. G. Bina Perl and Uri Roll and Sarig Gafny and Eli Geffen and Yoav Ram", "abstract": "Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.", "link": "http://arxiv.org/abs/2601.08798v1", "date": "2026-01-13", "relevancy": 2.3254, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4885}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.458}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-perfect%20photo-ID%20of%20the%20Hula%20painted%20frog%20with%20zero-shot%20deep%20local-feature%20matching&body=Title%3A%20Near-perfect%20photo-ID%20of%20the%20Hula%20painted%20frog%20with%20zero-shot%20deep%20local-feature%20matching%0AAuthor%3A%20Maayan%20Yesharim%20and%20R.%20G.%20Bina%20Perl%20and%20Uri%20Roll%20and%20Sarig%20Gafny%20and%20Eli%20Geffen%20and%20Yoav%20Ram%0AAbstract%3A%20Accurate%20individual%20identification%20is%20essential%20for%20monitoring%20rare%20amphibians%2C%20yet%20invasive%20marking%20is%20often%20unsuitable%20for%20critically%20endangered%20species.%20We%20evaluate%20state-of-the-art%20computer-vision%20methods%20for%20photographic%20re-identification%20of%20the%20Hula%20painted%20frog%20%28Latonia%20nigriventer%29%20using%201%2C233%20ventral%20images%20from%20191%20individuals%20collected%20during%202013-2020%20capture-recapture%20surveys.%20We%20compare%20deep%20local-feature%20matching%20in%20a%20zero-shot%20setting%20with%20deep%20global-feature%20embedding%20models.%20The%20local-feature%20pipeline%20achieves%2098%25%20top-1%20closed-set%20identification%20accuracy%2C%20outperforming%20all%20global-feature%20models%3B%20fine-tuning%20improves%20the%20best%20global-feature%20model%20to%2060%25%20top-1%20%2891%25%20top-10%29%20but%20remains%20below%20local%20matching.%20To%20combine%20scalability%20with%20accuracy%2C%20we%20implement%20a%20two-stage%20workflow%20in%20which%20a%20fine-tuned%20global-feature%20model%20retrieves%20a%20short%20candidate%20list%20that%20is%20re-ranked%20by%20local-feature%20matching%2C%20reducing%20end-to-end%20runtime%20from%206.5-7.8%20hours%20to%20~38%20minutes%20while%20maintaining%20~96%25%20top-1%20closed-set%20accuracy%20on%20the%20labeled%20dataset.%20Separation%20of%20match%20scores%20between%20same-%20and%20different-individual%20pairs%20supports%20thresholding%20for%20open-set%20identification%2C%20enabling%20practical%20handling%20of%20novel%20individuals.%20We%20deploy%20this%20pipeline%20as%20a%20web%20application%20for%20routine%20field%20use%2C%20providing%20rapid%2C%20standardized%2C%20non-invasive%20identification%20to%20support%20conservation%20monitoring%20and%20capture-recapture%20analyses.%20Overall%2C%20in%20this%20species%2C%20zero-shot%20deep%20local-feature%20matching%20outperformed%20global-feature%20embedding%20and%20provides%20a%20strong%20default%20for%20photo-identification.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-perfect%2520photo-ID%2520of%2520the%2520Hula%2520painted%2520frog%2520with%2520zero-shot%2520deep%2520local-feature%2520matching%26entry.906535625%3DMaayan%2520Yesharim%2520and%2520R.%2520G.%2520Bina%2520Perl%2520and%2520Uri%2520Roll%2520and%2520Sarig%2520Gafny%2520and%2520Eli%2520Geffen%2520and%2520Yoav%2520Ram%26entry.1292438233%3DAccurate%2520individual%2520identification%2520is%2520essential%2520for%2520monitoring%2520rare%2520amphibians%252C%2520yet%2520invasive%2520marking%2520is%2520often%2520unsuitable%2520for%2520critically%2520endangered%2520species.%2520We%2520evaluate%2520state-of-the-art%2520computer-vision%2520methods%2520for%2520photographic%2520re-identification%2520of%2520the%2520Hula%2520painted%2520frog%2520%2528Latonia%2520nigriventer%2529%2520using%25201%252C233%2520ventral%2520images%2520from%2520191%2520individuals%2520collected%2520during%25202013-2020%2520capture-recapture%2520surveys.%2520We%2520compare%2520deep%2520local-feature%2520matching%2520in%2520a%2520zero-shot%2520setting%2520with%2520deep%2520global-feature%2520embedding%2520models.%2520The%2520local-feature%2520pipeline%2520achieves%252098%2525%2520top-1%2520closed-set%2520identification%2520accuracy%252C%2520outperforming%2520all%2520global-feature%2520models%253B%2520fine-tuning%2520improves%2520the%2520best%2520global-feature%2520model%2520to%252060%2525%2520top-1%2520%252891%2525%2520top-10%2529%2520but%2520remains%2520below%2520local%2520matching.%2520To%2520combine%2520scalability%2520with%2520accuracy%252C%2520we%2520implement%2520a%2520two-stage%2520workflow%2520in%2520which%2520a%2520fine-tuned%2520global-feature%2520model%2520retrieves%2520a%2520short%2520candidate%2520list%2520that%2520is%2520re-ranked%2520by%2520local-feature%2520matching%252C%2520reducing%2520end-to-end%2520runtime%2520from%25206.5-7.8%2520hours%2520to%2520~38%2520minutes%2520while%2520maintaining%2520~96%2525%2520top-1%2520closed-set%2520accuracy%2520on%2520the%2520labeled%2520dataset.%2520Separation%2520of%2520match%2520scores%2520between%2520same-%2520and%2520different-individual%2520pairs%2520supports%2520thresholding%2520for%2520open-set%2520identification%252C%2520enabling%2520practical%2520handling%2520of%2520novel%2520individuals.%2520We%2520deploy%2520this%2520pipeline%2520as%2520a%2520web%2520application%2520for%2520routine%2520field%2520use%252C%2520providing%2520rapid%252C%2520standardized%252C%2520non-invasive%2520identification%2520to%2520support%2520conservation%2520monitoring%2520and%2520capture-recapture%2520analyses.%2520Overall%252C%2520in%2520this%2520species%252C%2520zero-shot%2520deep%2520local-feature%2520matching%2520outperformed%2520global-feature%2520embedding%2520and%2520provides%2520a%2520strong%2520default%2520for%2520photo-identification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-perfect%20photo-ID%20of%20the%20Hula%20painted%20frog%20with%20zero-shot%20deep%20local-feature%20matching&entry.906535625=Maayan%20Yesharim%20and%20R.%20G.%20Bina%20Perl%20and%20Uri%20Roll%20and%20Sarig%20Gafny%20and%20Eli%20Geffen%20and%20Yoav%20Ram&entry.1292438233=Accurate%20individual%20identification%20is%20essential%20for%20monitoring%20rare%20amphibians%2C%20yet%20invasive%20marking%20is%20often%20unsuitable%20for%20critically%20endangered%20species.%20We%20evaluate%20state-of-the-art%20computer-vision%20methods%20for%20photographic%20re-identification%20of%20the%20Hula%20painted%20frog%20%28Latonia%20nigriventer%29%20using%201%2C233%20ventral%20images%20from%20191%20individuals%20collected%20during%202013-2020%20capture-recapture%20surveys.%20We%20compare%20deep%20local-feature%20matching%20in%20a%20zero-shot%20setting%20with%20deep%20global-feature%20embedding%20models.%20The%20local-feature%20pipeline%20achieves%2098%25%20top-1%20closed-set%20identification%20accuracy%2C%20outperforming%20all%20global-feature%20models%3B%20fine-tuning%20improves%20the%20best%20global-feature%20model%20to%2060%25%20top-1%20%2891%25%20top-10%29%20but%20remains%20below%20local%20matching.%20To%20combine%20scalability%20with%20accuracy%2C%20we%20implement%20a%20two-stage%20workflow%20in%20which%20a%20fine-tuned%20global-feature%20model%20retrieves%20a%20short%20candidate%20list%20that%20is%20re-ranked%20by%20local-feature%20matching%2C%20reducing%20end-to-end%20runtime%20from%206.5-7.8%20hours%20to%20~38%20minutes%20while%20maintaining%20~96%25%20top-1%20closed-set%20accuracy%20on%20the%20labeled%20dataset.%20Separation%20of%20match%20scores%20between%20same-%20and%20different-individual%20pairs%20supports%20thresholding%20for%20open-set%20identification%2C%20enabling%20practical%20handling%20of%20novel%20individuals.%20We%20deploy%20this%20pipeline%20as%20a%20web%20application%20for%20routine%20field%20use%2C%20providing%20rapid%2C%20standardized%2C%20non-invasive%20identification%20to%20support%20conservation%20monitoring%20and%20capture-recapture%20analyses.%20Overall%2C%20in%20this%20species%2C%20zero-shot%20deep%20local-feature%20matching%20outperformed%20global-feature%20embedding%20and%20provides%20a%20strong%20default%20for%20photo-identification.&entry.1838667208=http%3A//arxiv.org/abs/2601.08798v1&entry.124074799=Read"},
{"title": "REVNET: Rotation-Equivariant Point Cloud Completion via Vector Neuron Anchor Transformer", "author": "Zhifan Ni and Eckehard Steinbach", "abstract": "Incomplete point clouds captured by 3D sensors often result in the loss of both geometric and semantic information. Most existing point cloud completion methods are built on rotation-variant frameworks trained with data in canonical poses, limiting their applicability in real-world scenarios. While data augmentation with random rotations can partially mitigate this issue, it significantly increases the learning burden and still fails to guarantee robust performance under arbitrary poses. To address this challenge, we propose the Rotation-Equivariant Anchor Transformer (REVNET), a novel framework built upon the Vector Neuron (VN) network for robust point cloud completion under arbitrary rotations. To preserve local details, we represent partial point clouds as sets of equivariant anchors and design a VN Missing Anchor Transformer to predict the positions and features of missing anchors. Furthermore, we extend VN networks with a rotation-equivariant bias formulation and a ZCA-based layer normalization to improve feature expressiveness. Leveraging the flexible conversion between equivariant and invariant VN features, our model can generate point coordinates with greater stability. Experimental results show that our method outperforms state-of-the-art approaches on the synthetic MVP dataset in the equivariant setting. On the real-world KITTI dataset, REVNET delivers competitive results compared to non-equivariant networks, without requiring input pose alignment. The source code will be released on GitHub under URL: https://github.com/nizhf/REVNET.", "link": "http://arxiv.org/abs/2601.08558v1", "date": "2026-01-13", "relevancy": 2.2886, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.595}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5725}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVNET%3A%20Rotation-Equivariant%20Point%20Cloud%20Completion%20via%20Vector%20Neuron%20Anchor%20Transformer&body=Title%3A%20REVNET%3A%20Rotation-Equivariant%20Point%20Cloud%20Completion%20via%20Vector%20Neuron%20Anchor%20Transformer%0AAuthor%3A%20Zhifan%20Ni%20and%20Eckehard%20Steinbach%0AAbstract%3A%20Incomplete%20point%20clouds%20captured%20by%203D%20sensors%20often%20result%20in%20the%20loss%20of%20both%20geometric%20and%20semantic%20information.%20Most%20existing%20point%20cloud%20completion%20methods%20are%20built%20on%20rotation-variant%20frameworks%20trained%20with%20data%20in%20canonical%20poses%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%20While%20data%20augmentation%20with%20random%20rotations%20can%20partially%20mitigate%20this%20issue%2C%20it%20significantly%20increases%20the%20learning%20burden%20and%20still%20fails%20to%20guarantee%20robust%20performance%20under%20arbitrary%20poses.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Rotation-Equivariant%20Anchor%20Transformer%20%28REVNET%29%2C%20a%20novel%20framework%20built%20upon%20the%20Vector%20Neuron%20%28VN%29%20network%20for%20robust%20point%20cloud%20completion%20under%20arbitrary%20rotations.%20To%20preserve%20local%20details%2C%20we%20represent%20partial%20point%20clouds%20as%20sets%20of%20equivariant%20anchors%20and%20design%20a%20VN%20Missing%20Anchor%20Transformer%20to%20predict%20the%20positions%20and%20features%20of%20missing%20anchors.%20Furthermore%2C%20we%20extend%20VN%20networks%20with%20a%20rotation-equivariant%20bias%20formulation%20and%20a%20ZCA-based%20layer%20normalization%20to%20improve%20feature%20expressiveness.%20Leveraging%20the%20flexible%20conversion%20between%20equivariant%20and%20invariant%20VN%20features%2C%20our%20model%20can%20generate%20point%20coordinates%20with%20greater%20stability.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%20the%20synthetic%20MVP%20dataset%20in%20the%20equivariant%20setting.%20On%20the%20real-world%20KITTI%20dataset%2C%20REVNET%20delivers%20competitive%20results%20compared%20to%20non-equivariant%20networks%2C%20without%20requiring%20input%20pose%20alignment.%20The%20source%20code%20will%20be%20released%20on%20GitHub%20under%20URL%3A%20https%3A//github.com/nizhf/REVNET.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVNET%253A%2520Rotation-Equivariant%2520Point%2520Cloud%2520Completion%2520via%2520Vector%2520Neuron%2520Anchor%2520Transformer%26entry.906535625%3DZhifan%2520Ni%2520and%2520Eckehard%2520Steinbach%26entry.1292438233%3DIncomplete%2520point%2520clouds%2520captured%2520by%25203D%2520sensors%2520often%2520result%2520in%2520the%2520loss%2520of%2520both%2520geometric%2520and%2520semantic%2520information.%2520Most%2520existing%2520point%2520cloud%2520completion%2520methods%2520are%2520built%2520on%2520rotation-variant%2520frameworks%2520trained%2520with%2520data%2520in%2520canonical%2520poses%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520scenarios.%2520While%2520data%2520augmentation%2520with%2520random%2520rotations%2520can%2520partially%2520mitigate%2520this%2520issue%252C%2520it%2520significantly%2520increases%2520the%2520learning%2520burden%2520and%2520still%2520fails%2520to%2520guarantee%2520robust%2520performance%2520under%2520arbitrary%2520poses.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Rotation-Equivariant%2520Anchor%2520Transformer%2520%2528REVNET%2529%252C%2520a%2520novel%2520framework%2520built%2520upon%2520the%2520Vector%2520Neuron%2520%2528VN%2529%2520network%2520for%2520robust%2520point%2520cloud%2520completion%2520under%2520arbitrary%2520rotations.%2520To%2520preserve%2520local%2520details%252C%2520we%2520represent%2520partial%2520point%2520clouds%2520as%2520sets%2520of%2520equivariant%2520anchors%2520and%2520design%2520a%2520VN%2520Missing%2520Anchor%2520Transformer%2520to%2520predict%2520the%2520positions%2520and%2520features%2520of%2520missing%2520anchors.%2520Furthermore%252C%2520we%2520extend%2520VN%2520networks%2520with%2520a%2520rotation-equivariant%2520bias%2520formulation%2520and%2520a%2520ZCA-based%2520layer%2520normalization%2520to%2520improve%2520feature%2520expressiveness.%2520Leveraging%2520the%2520flexible%2520conversion%2520between%2520equivariant%2520and%2520invariant%2520VN%2520features%252C%2520our%2520model%2520can%2520generate%2520point%2520coordinates%2520with%2520greater%2520stability.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520approaches%2520on%2520the%2520synthetic%2520MVP%2520dataset%2520in%2520the%2520equivariant%2520setting.%2520On%2520the%2520real-world%2520KITTI%2520dataset%252C%2520REVNET%2520delivers%2520competitive%2520results%2520compared%2520to%2520non-equivariant%2520networks%252C%2520without%2520requiring%2520input%2520pose%2520alignment.%2520The%2520source%2520code%2520will%2520be%2520released%2520on%2520GitHub%2520under%2520URL%253A%2520https%253A//github.com/nizhf/REVNET.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVNET%3A%20Rotation-Equivariant%20Point%20Cloud%20Completion%20via%20Vector%20Neuron%20Anchor%20Transformer&entry.906535625=Zhifan%20Ni%20and%20Eckehard%20Steinbach&entry.1292438233=Incomplete%20point%20clouds%20captured%20by%203D%20sensors%20often%20result%20in%20the%20loss%20of%20both%20geometric%20and%20semantic%20information.%20Most%20existing%20point%20cloud%20completion%20methods%20are%20built%20on%20rotation-variant%20frameworks%20trained%20with%20data%20in%20canonical%20poses%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%20While%20data%20augmentation%20with%20random%20rotations%20can%20partially%20mitigate%20this%20issue%2C%20it%20significantly%20increases%20the%20learning%20burden%20and%20still%20fails%20to%20guarantee%20robust%20performance%20under%20arbitrary%20poses.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Rotation-Equivariant%20Anchor%20Transformer%20%28REVNET%29%2C%20a%20novel%20framework%20built%20upon%20the%20Vector%20Neuron%20%28VN%29%20network%20for%20robust%20point%20cloud%20completion%20under%20arbitrary%20rotations.%20To%20preserve%20local%20details%2C%20we%20represent%20partial%20point%20clouds%20as%20sets%20of%20equivariant%20anchors%20and%20design%20a%20VN%20Missing%20Anchor%20Transformer%20to%20predict%20the%20positions%20and%20features%20of%20missing%20anchors.%20Furthermore%2C%20we%20extend%20VN%20networks%20with%20a%20rotation-equivariant%20bias%20formulation%20and%20a%20ZCA-based%20layer%20normalization%20to%20improve%20feature%20expressiveness.%20Leveraging%20the%20flexible%20conversion%20between%20equivariant%20and%20invariant%20VN%20features%2C%20our%20model%20can%20generate%20point%20coordinates%20with%20greater%20stability.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%20the%20synthetic%20MVP%20dataset%20in%20the%20equivariant%20setting.%20On%20the%20real-world%20KITTI%20dataset%2C%20REVNET%20delivers%20competitive%20results%20compared%20to%20non-equivariant%20networks%2C%20without%20requiring%20input%20pose%20alignment.%20The%20source%20code%20will%20be%20released%20on%20GitHub%20under%20URL%3A%20https%3A//github.com/nizhf/REVNET.&entry.1838667208=http%3A//arxiv.org/abs/2601.08558v1&entry.124074799=Read"},
{"title": "Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification", "author": "Tayyab Rehman and Giovanni De Gasperis and Aly Shmahell", "abstract": "Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.", "link": "http://arxiv.org/abs/2601.06204v2", "date": "2026-01-13", "relevancy": 2.2819, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascading%20multi-agent%20anomaly%20detection%20in%20surveillance%20systems%20via%20vision-language%20models%20and%20embedding-based%20classification&body=Title%3A%20Cascading%20multi-agent%20anomaly%20detection%20in%20surveillance%20systems%20via%20vision-language%20models%20and%20embedding-based%20classification%0AAuthor%3A%20Tayyab%20Rehman%20and%20Giovanni%20De%20Gasperis%20and%20Aly%20Shmahell%0AAbstract%3A%20Intelligent%20anomaly%20detection%20in%20dynamic%20visual%20environments%20requires%20reconciling%20real-time%20performance%20with%20semantic%20interpretability.%20Conventional%20approaches%20address%20only%20fragments%20of%20this%20challenge.%20Reconstruction-based%20models%20capture%20low-level%20deviations%20without%20contextual%20reasoning%2C%20object%20detectors%20provide%20speed%20but%20limited%20semantics%2C%20and%20large%20vision-language%20systems%20deliver%20interpretability%20at%20prohibitive%20computational%20cost.%20This%20work%20introduces%20a%20cascading%20multi-agent%20framework%20that%20unifies%20these%20complementary%20paradigms%20into%20a%20coherent%20and%20interpretable%20architecture.%20Early%20modules%20perform%20reconstruction-gated%20filtering%20and%20object-level%20assessment%2C%20while%20higher-level%20reasoning%20agents%20are%20selectively%20invoked%20to%20interpret%20semantically%20ambiguous%20events.%20The%20system%20employs%20adaptive%20escalation%20thresholds%20and%20a%20publish-subscribe%20communication%20backbone%2C%20enabling%20asynchronous%20coordination%20and%20scalable%20deployment%20across%20heterogeneous%20hardware.%20Extensive%20evaluation%20on%20large-scale%20monitoring%20data%20demonstrates%20that%20the%20proposed%20cascade%20achieves%20a%20threefold%20reduction%20in%20latency%20compared%20to%20direct%20vision-language%20inference%2C%20while%20maintaining%20high%20perceptual%20fidelity%20%28PSNR%20%3D%2038.3%20dB%2C%20SSIM%20%3D%200.965%29%20and%20consistent%20semantic%20labeling.%20The%20framework%20advances%20beyond%20conventional%20detection%20pipelines%20by%20combining%20early-exit%20efficiency%2C%20adaptive%20multi-agent%20reasoning%2C%20and%20explainable%20anomaly%20attribution%2C%20establishing%20a%20reproducible%20and%20energy-efficient%20foundation%20for%20scalable%20intelligent%20visual%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascading%2520multi-agent%2520anomaly%2520detection%2520in%2520surveillance%2520systems%2520via%2520vision-language%2520models%2520and%2520embedding-based%2520classification%26entry.906535625%3DTayyab%2520Rehman%2520and%2520Giovanni%2520De%2520Gasperis%2520and%2520Aly%2520Shmahell%26entry.1292438233%3DIntelligent%2520anomaly%2520detection%2520in%2520dynamic%2520visual%2520environments%2520requires%2520reconciling%2520real-time%2520performance%2520with%2520semantic%2520interpretability.%2520Conventional%2520approaches%2520address%2520only%2520fragments%2520of%2520this%2520challenge.%2520Reconstruction-based%2520models%2520capture%2520low-level%2520deviations%2520without%2520contextual%2520reasoning%252C%2520object%2520detectors%2520provide%2520speed%2520but%2520limited%2520semantics%252C%2520and%2520large%2520vision-language%2520systems%2520deliver%2520interpretability%2520at%2520prohibitive%2520computational%2520cost.%2520This%2520work%2520introduces%2520a%2520cascading%2520multi-agent%2520framework%2520that%2520unifies%2520these%2520complementary%2520paradigms%2520into%2520a%2520coherent%2520and%2520interpretable%2520architecture.%2520Early%2520modules%2520perform%2520reconstruction-gated%2520filtering%2520and%2520object-level%2520assessment%252C%2520while%2520higher-level%2520reasoning%2520agents%2520are%2520selectively%2520invoked%2520to%2520interpret%2520semantically%2520ambiguous%2520events.%2520The%2520system%2520employs%2520adaptive%2520escalation%2520thresholds%2520and%2520a%2520publish-subscribe%2520communication%2520backbone%252C%2520enabling%2520asynchronous%2520coordination%2520and%2520scalable%2520deployment%2520across%2520heterogeneous%2520hardware.%2520Extensive%2520evaluation%2520on%2520large-scale%2520monitoring%2520data%2520demonstrates%2520that%2520the%2520proposed%2520cascade%2520achieves%2520a%2520threefold%2520reduction%2520in%2520latency%2520compared%2520to%2520direct%2520vision-language%2520inference%252C%2520while%2520maintaining%2520high%2520perceptual%2520fidelity%2520%2528PSNR%2520%253D%252038.3%2520dB%252C%2520SSIM%2520%253D%25200.965%2529%2520and%2520consistent%2520semantic%2520labeling.%2520The%2520framework%2520advances%2520beyond%2520conventional%2520detection%2520pipelines%2520by%2520combining%2520early-exit%2520efficiency%252C%2520adaptive%2520multi-agent%2520reasoning%252C%2520and%2520explainable%2520anomaly%2520attribution%252C%2520establishing%2520a%2520reproducible%2520and%2520energy-efficient%2520foundation%2520for%2520scalable%2520intelligent%2520visual%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascading%20multi-agent%20anomaly%20detection%20in%20surveillance%20systems%20via%20vision-language%20models%20and%20embedding-based%20classification&entry.906535625=Tayyab%20Rehman%20and%20Giovanni%20De%20Gasperis%20and%20Aly%20Shmahell&entry.1292438233=Intelligent%20anomaly%20detection%20in%20dynamic%20visual%20environments%20requires%20reconciling%20real-time%20performance%20with%20semantic%20interpretability.%20Conventional%20approaches%20address%20only%20fragments%20of%20this%20challenge.%20Reconstruction-based%20models%20capture%20low-level%20deviations%20without%20contextual%20reasoning%2C%20object%20detectors%20provide%20speed%20but%20limited%20semantics%2C%20and%20large%20vision-language%20systems%20deliver%20interpretability%20at%20prohibitive%20computational%20cost.%20This%20work%20introduces%20a%20cascading%20multi-agent%20framework%20that%20unifies%20these%20complementary%20paradigms%20into%20a%20coherent%20and%20interpretable%20architecture.%20Early%20modules%20perform%20reconstruction-gated%20filtering%20and%20object-level%20assessment%2C%20while%20higher-level%20reasoning%20agents%20are%20selectively%20invoked%20to%20interpret%20semantically%20ambiguous%20events.%20The%20system%20employs%20adaptive%20escalation%20thresholds%20and%20a%20publish-subscribe%20communication%20backbone%2C%20enabling%20asynchronous%20coordination%20and%20scalable%20deployment%20across%20heterogeneous%20hardware.%20Extensive%20evaluation%20on%20large-scale%20monitoring%20data%20demonstrates%20that%20the%20proposed%20cascade%20achieves%20a%20threefold%20reduction%20in%20latency%20compared%20to%20direct%20vision-language%20inference%2C%20while%20maintaining%20high%20perceptual%20fidelity%20%28PSNR%20%3D%2038.3%20dB%2C%20SSIM%20%3D%200.965%29%20and%20consistent%20semantic%20labeling.%20The%20framework%20advances%20beyond%20conventional%20detection%20pipelines%20by%20combining%20early-exit%20efficiency%2C%20adaptive%20multi-agent%20reasoning%2C%20and%20explainable%20anomaly%20attribution%2C%20establishing%20a%20reproducible%20and%20energy-efficient%20foundation%20for%20scalable%20intelligent%20visual%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2601.06204v2&entry.124074799=Read"},
{"title": "Apollo: Unified Multi-Task Audio-Video Joint Generation", "author": "Jun Wang and Chunyu Qiang and Yuxin Guo and Yiran Wang and Xijuan Zeng and Feng Deng", "abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Apollo and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Apollo scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.", "link": "http://arxiv.org/abs/2601.04151v2", "date": "2026-01-13", "relevancy": 2.2788, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apollo%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation&body=Title%3A%20Apollo%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation%0AAuthor%3A%20Jun%20Wang%20and%20Chunyu%20Qiang%20and%20Yuxin%20Guo%20and%20Yiran%20Wang%20and%20Xijuan%20Zeng%20and%20Feng%20Deng%0AAbstract%3A%20Audio-video%20joint%20generation%20has%20progressed%20rapidly%2C%20yet%20substantial%20challenges%20still%20remain.%20Non-commercial%20approaches%20still%20suffer%20audio-visual%20asynchrony%2C%20poor%20lip-speech%20alignment%2C%20and%20unimodal%20degradation%2C%20which%20can%20be%20stemmed%20from%20weak%20audio-visual%20correspondence%20modeling%2C%20limited%20generalization%2C%20and%20scarce%20high-quality%20dense-caption%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Apollo%20and%20delve%20into%20three%20axes--model%20architecture%2C%20training%20strategy%2C%20and%20data%20curation.%20Architecturally%2C%20we%20adopt%20a%20single-tower%20design%20with%20unified%20DiT%20blocks%20and%20an%20Omni-Full%20Attention%20mechanism%2C%20achieving%20tight%20audio-visual%20alignment%20and%20strong%20scalability.%20Training-wise%2C%20we%20adopt%20a%20progressive%20multitask%20regime--random%20modality%20masking%20to%20joint%20optimization%20across%20tasks%2C%20and%20a%20multistage%20curriculum%2C%20yielding%20robust%20representations%2C%20strengthening%20A-V%20aligned%20world%20knowledge%2C%20and%20preventing%20unimodal%20collapse.%20For%20datasets%2C%20we%20present%20the%20first%20large-scale%20audio-video%20dataset%20with%20dense%20captions%2C%20and%20introduce%20a%20novel%20automated%20data-construction%20pipeline%20which%20annotates%20and%20filters%20millions%20of%20diverse%2C%20high-quality%2C%20strictly%20aligned%20audio-video-caption%20triplets.%20Building%20on%20this%2C%20Apollo%20scales%20to%20large%20datasets%2C%20delivering%20high-fidelity%2C%20semantically%20and%20temporally%20aligned%2C%20instruction-following%20generation%20in%20both%20joint%20and%20unimodal%20settings%20while%20generalizing%20robustly%20to%20out-of-distribution%20scenarios.%20Across%20tasks%2C%20it%20substantially%20outperforms%20prior%20methods%20by%20a%20large%20margin%20and%20achieves%20performance%20comparable%20to%20Veo%203%2C%20offering%20a%20unified%2C%20scalable%20path%20toward%20next-generation%20audio-video%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApollo%253A%2520Unified%2520Multi-Task%2520Audio-Video%2520Joint%2520Generation%26entry.906535625%3DJun%2520Wang%2520and%2520Chunyu%2520Qiang%2520and%2520Yuxin%2520Guo%2520and%2520Yiran%2520Wang%2520and%2520Xijuan%2520Zeng%2520and%2520Feng%2520Deng%26entry.1292438233%3DAudio-video%2520joint%2520generation%2520has%2520progressed%2520rapidly%252C%2520yet%2520substantial%2520challenges%2520still%2520remain.%2520Non-commercial%2520approaches%2520still%2520suffer%2520audio-visual%2520asynchrony%252C%2520poor%2520lip-speech%2520alignment%252C%2520and%2520unimodal%2520degradation%252C%2520which%2520can%2520be%2520stemmed%2520from%2520weak%2520audio-visual%2520correspondence%2520modeling%252C%2520limited%2520generalization%252C%2520and%2520scarce%2520high-quality%2520dense-caption%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Apollo%2520and%2520delve%2520into%2520three%2520axes--model%2520architecture%252C%2520training%2520strategy%252C%2520and%2520data%2520curation.%2520Architecturally%252C%2520we%2520adopt%2520a%2520single-tower%2520design%2520with%2520unified%2520DiT%2520blocks%2520and%2520an%2520Omni-Full%2520Attention%2520mechanism%252C%2520achieving%2520tight%2520audio-visual%2520alignment%2520and%2520strong%2520scalability.%2520Training-wise%252C%2520we%2520adopt%2520a%2520progressive%2520multitask%2520regime--random%2520modality%2520masking%2520to%2520joint%2520optimization%2520across%2520tasks%252C%2520and%2520a%2520multistage%2520curriculum%252C%2520yielding%2520robust%2520representations%252C%2520strengthening%2520A-V%2520aligned%2520world%2520knowledge%252C%2520and%2520preventing%2520unimodal%2520collapse.%2520For%2520datasets%252C%2520we%2520present%2520the%2520first%2520large-scale%2520audio-video%2520dataset%2520with%2520dense%2520captions%252C%2520and%2520introduce%2520a%2520novel%2520automated%2520data-construction%2520pipeline%2520which%2520annotates%2520and%2520filters%2520millions%2520of%2520diverse%252C%2520high-quality%252C%2520strictly%2520aligned%2520audio-video-caption%2520triplets.%2520Building%2520on%2520this%252C%2520Apollo%2520scales%2520to%2520large%2520datasets%252C%2520delivering%2520high-fidelity%252C%2520semantically%2520and%2520temporally%2520aligned%252C%2520instruction-following%2520generation%2520in%2520both%2520joint%2520and%2520unimodal%2520settings%2520while%2520generalizing%2520robustly%2520to%2520out-of-distribution%2520scenarios.%2520Across%2520tasks%252C%2520it%2520substantially%2520outperforms%2520prior%2520methods%2520by%2520a%2520large%2520margin%2520and%2520achieves%2520performance%2520comparable%2520to%2520Veo%25203%252C%2520offering%2520a%2520unified%252C%2520scalable%2520path%2520toward%2520next-generation%2520audio-video%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apollo%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation&entry.906535625=Jun%20Wang%20and%20Chunyu%20Qiang%20and%20Yuxin%20Guo%20and%20Yiran%20Wang%20and%20Xijuan%20Zeng%20and%20Feng%20Deng&entry.1292438233=Audio-video%20joint%20generation%20has%20progressed%20rapidly%2C%20yet%20substantial%20challenges%20still%20remain.%20Non-commercial%20approaches%20still%20suffer%20audio-visual%20asynchrony%2C%20poor%20lip-speech%20alignment%2C%20and%20unimodal%20degradation%2C%20which%20can%20be%20stemmed%20from%20weak%20audio-visual%20correspondence%20modeling%2C%20limited%20generalization%2C%20and%20scarce%20high-quality%20dense-caption%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Apollo%20and%20delve%20into%20three%20axes--model%20architecture%2C%20training%20strategy%2C%20and%20data%20curation.%20Architecturally%2C%20we%20adopt%20a%20single-tower%20design%20with%20unified%20DiT%20blocks%20and%20an%20Omni-Full%20Attention%20mechanism%2C%20achieving%20tight%20audio-visual%20alignment%20and%20strong%20scalability.%20Training-wise%2C%20we%20adopt%20a%20progressive%20multitask%20regime--random%20modality%20masking%20to%20joint%20optimization%20across%20tasks%2C%20and%20a%20multistage%20curriculum%2C%20yielding%20robust%20representations%2C%20strengthening%20A-V%20aligned%20world%20knowledge%2C%20and%20preventing%20unimodal%20collapse.%20For%20datasets%2C%20we%20present%20the%20first%20large-scale%20audio-video%20dataset%20with%20dense%20captions%2C%20and%20introduce%20a%20novel%20automated%20data-construction%20pipeline%20which%20annotates%20and%20filters%20millions%20of%20diverse%2C%20high-quality%2C%20strictly%20aligned%20audio-video-caption%20triplets.%20Building%20on%20this%2C%20Apollo%20scales%20to%20large%20datasets%2C%20delivering%20high-fidelity%2C%20semantically%20and%20temporally%20aligned%2C%20instruction-following%20generation%20in%20both%20joint%20and%20unimodal%20settings%20while%20generalizing%20robustly%20to%20out-of-distribution%20scenarios.%20Across%20tasks%2C%20it%20substantially%20outperforms%20prior%20methods%20by%20a%20large%20margin%20and%20achieves%20performance%20comparable%20to%20Veo%203%2C%20offering%20a%20unified%2C%20scalable%20path%20toward%20next-generation%20audio-video%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2601.04151v2&entry.124074799=Read"},
{"title": "Reasoning Models Will Blatantly Lie About Their Reasoning", "author": "William Walden", "abstract": "It has been shown that Large Reasoning Models (LRMs) may not *say what they think*: they do not always volunteer information about how certain parts of the input influence their reasoning. But it is one thing for a model to *omit* such information and another, worse thing to *lie* about it. Here, we extend the work of Chen et al. (2025) to show that LRMs will do just this: they will flatly deny relying on hints provided in the prompt in answering multiple choice questions -- even when directly asked to reflect on unusual (i.e. hinted) prompt content, even when allowed to use hints, and even though experiments *show* them to be using the hints. Our results thus have discouraging implications for CoT monitoring and interpretability.", "link": "http://arxiv.org/abs/2601.07663v2", "date": "2026-01-13", "relevancy": 2.2727, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning&body=Title%3A%20Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning%0AAuthor%3A%20William%20Walden%0AAbstract%3A%20It%20has%20been%20shown%20that%20Large%20Reasoning%20Models%20%28LRMs%29%20may%20not%20%2Asay%20what%20they%20think%2A%3A%20they%20do%20not%20always%20volunteer%20information%20about%20how%20certain%20parts%20of%20the%20input%20influence%20their%20reasoning.%20But%20it%20is%20one%20thing%20for%20a%20model%20to%20%2Aomit%2A%20such%20information%20and%20another%2C%20worse%20thing%20to%20%2Alie%2A%20about%20it.%20Here%2C%20we%20extend%20the%20work%20of%20Chen%20et%20al.%20%282025%29%20to%20show%20that%20LRMs%20will%20do%20just%20this%3A%20they%20will%20flatly%20deny%20relying%20on%20hints%20provided%20in%20the%20prompt%20in%20answering%20multiple%20choice%20questions%20--%20even%20when%20directly%20asked%20to%20reflect%20on%20unusual%20%28i.e.%20hinted%29%20prompt%20content%2C%20even%20when%20allowed%20to%20use%20hints%2C%20and%20even%20though%20experiments%20%2Ashow%2A%20them%20to%20be%20using%20the%20hints.%20Our%20results%20thus%20have%20discouraging%20implications%20for%20CoT%20monitoring%20and%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Models%2520Will%2520Blatantly%2520Lie%2520About%2520Their%2520Reasoning%26entry.906535625%3DWilliam%2520Walden%26entry.1292438233%3DIt%2520has%2520been%2520shown%2520that%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520may%2520not%2520%252Asay%2520what%2520they%2520think%252A%253A%2520they%2520do%2520not%2520always%2520volunteer%2520information%2520about%2520how%2520certain%2520parts%2520of%2520the%2520input%2520influence%2520their%2520reasoning.%2520But%2520it%2520is%2520one%2520thing%2520for%2520a%2520model%2520to%2520%252Aomit%252A%2520such%2520information%2520and%2520another%252C%2520worse%2520thing%2520to%2520%252Alie%252A%2520about%2520it.%2520Here%252C%2520we%2520extend%2520the%2520work%2520of%2520Chen%2520et%2520al.%2520%25282025%2529%2520to%2520show%2520that%2520LRMs%2520will%2520do%2520just%2520this%253A%2520they%2520will%2520flatly%2520deny%2520relying%2520on%2520hints%2520provided%2520in%2520the%2520prompt%2520in%2520answering%2520multiple%2520choice%2520questions%2520--%2520even%2520when%2520directly%2520asked%2520to%2520reflect%2520on%2520unusual%2520%2528i.e.%2520hinted%2529%2520prompt%2520content%252C%2520even%2520when%2520allowed%2520to%2520use%2520hints%252C%2520and%2520even%2520though%2520experiments%2520%252Ashow%252A%2520them%2520to%2520be%2520using%2520the%2520hints.%2520Our%2520results%2520thus%2520have%2520discouraging%2520implications%2520for%2520CoT%2520monitoring%2520and%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning&entry.906535625=William%20Walden&entry.1292438233=It%20has%20been%20shown%20that%20Large%20Reasoning%20Models%20%28LRMs%29%20may%20not%20%2Asay%20what%20they%20think%2A%3A%20they%20do%20not%20always%20volunteer%20information%20about%20how%20certain%20parts%20of%20the%20input%20influence%20their%20reasoning.%20But%20it%20is%20one%20thing%20for%20a%20model%20to%20%2Aomit%2A%20such%20information%20and%20another%2C%20worse%20thing%20to%20%2Alie%2A%20about%20it.%20Here%2C%20we%20extend%20the%20work%20of%20Chen%20et%20al.%20%282025%29%20to%20show%20that%20LRMs%20will%20do%20just%20this%3A%20they%20will%20flatly%20deny%20relying%20on%20hints%20provided%20in%20the%20prompt%20in%20answering%20multiple%20choice%20questions%20--%20even%20when%20directly%20asked%20to%20reflect%20on%20unusual%20%28i.e.%20hinted%29%20prompt%20content%2C%20even%20when%20allowed%20to%20use%20hints%2C%20and%20even%20though%20experiments%20%2Ashow%2A%20them%20to%20be%20using%20the%20hints.%20Our%20results%20thus%20have%20discouraging%20implications%20for%20CoT%20monitoring%20and%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2601.07663v2&entry.124074799=Read"},
{"title": "Safe Language Generation in the Limit", "author": "Antonios Anastasopoulos and Giuseppe Ateniese and Evgenios M. Kornaropoulos", "abstract": "Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.\n  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.", "link": "http://arxiv.org/abs/2601.08648v1", "date": "2026-01-13", "relevancy": 2.2712, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4877}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4544}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Language%20Generation%20in%20the%20Limit&body=Title%3A%20Safe%20Language%20Generation%20in%20the%20Limit%0AAuthor%3A%20Antonios%20Anastasopoulos%20and%20Giuseppe%20Ateniese%20and%20Evgenios%20M.%20Kornaropoulos%0AAbstract%3A%20Recent%20results%20in%20learning%20a%20language%20in%20the%20limit%20have%20shown%20that%2C%20although%20language%20identification%20is%20impossible%2C%20language%20generation%20is%20tractable.%20As%20this%20foundational%20area%20expands%2C%20we%20need%20to%20consider%20the%20implications%20of%20language%20generation%20in%20real-world%20settings.%0A%20%20This%20work%20offers%20the%20first%20theoretical%20treatment%20of%20safe%20language%20generation.%20Building%20on%20the%20computational%20paradigm%20of%20learning%20in%20the%20limit%2C%20we%20formalize%20the%20tasks%20of%20safe%20language%20identification%20and%20generation.%20We%20prove%20that%20under%20this%20model%2C%20safe%20language%20identification%20is%20impossible%2C%20and%20that%20safe%20language%20generation%20is%20at%20least%20as%20hard%20as%20%28vanilla%29%20language%20identification%2C%20which%20is%20also%20impossible.%20Last%2C%20we%20discuss%20several%20intractable%20and%20tractable%20cases.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Language%2520Generation%2520in%2520the%2520Limit%26entry.906535625%3DAntonios%2520Anastasopoulos%2520and%2520Giuseppe%2520Ateniese%2520and%2520Evgenios%2520M.%2520Kornaropoulos%26entry.1292438233%3DRecent%2520results%2520in%2520learning%2520a%2520language%2520in%2520the%2520limit%2520have%2520shown%2520that%252C%2520although%2520language%2520identification%2520is%2520impossible%252C%2520language%2520generation%2520is%2520tractable.%2520As%2520this%2520foundational%2520area%2520expands%252C%2520we%2520need%2520to%2520consider%2520the%2520implications%2520of%2520language%2520generation%2520in%2520real-world%2520settings.%250A%2520%2520This%2520work%2520offers%2520the%2520first%2520theoretical%2520treatment%2520of%2520safe%2520language%2520generation.%2520Building%2520on%2520the%2520computational%2520paradigm%2520of%2520learning%2520in%2520the%2520limit%252C%2520we%2520formalize%2520the%2520tasks%2520of%2520safe%2520language%2520identification%2520and%2520generation.%2520We%2520prove%2520that%2520under%2520this%2520model%252C%2520safe%2520language%2520identification%2520is%2520impossible%252C%2520and%2520that%2520safe%2520language%2520generation%2520is%2520at%2520least%2520as%2520hard%2520as%2520%2528vanilla%2529%2520language%2520identification%252C%2520which%2520is%2520also%2520impossible.%2520Last%252C%2520we%2520discuss%2520several%2520intractable%2520and%2520tractable%2520cases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Language%20Generation%20in%20the%20Limit&entry.906535625=Antonios%20Anastasopoulos%20and%20Giuseppe%20Ateniese%20and%20Evgenios%20M.%20Kornaropoulos&entry.1292438233=Recent%20results%20in%20learning%20a%20language%20in%20the%20limit%20have%20shown%20that%2C%20although%20language%20identification%20is%20impossible%2C%20language%20generation%20is%20tractable.%20As%20this%20foundational%20area%20expands%2C%20we%20need%20to%20consider%20the%20implications%20of%20language%20generation%20in%20real-world%20settings.%0A%20%20This%20work%20offers%20the%20first%20theoretical%20treatment%20of%20safe%20language%20generation.%20Building%20on%20the%20computational%20paradigm%20of%20learning%20in%20the%20limit%2C%20we%20formalize%20the%20tasks%20of%20safe%20language%20identification%20and%20generation.%20We%20prove%20that%20under%20this%20model%2C%20safe%20language%20identification%20is%20impossible%2C%20and%20that%20safe%20language%20generation%20is%20at%20least%20as%20hard%20as%20%28vanilla%29%20language%20identification%2C%20which%20is%20also%20impossible.%20Last%2C%20we%20discuss%20several%20intractable%20and%20tractable%20cases.&entry.1838667208=http%3A//arxiv.org/abs/2601.08648v1&entry.124074799=Read"},
{"title": "Enabling Population-Level Parallelism in Tree-Based Genetic Programming for GPU Acceleration", "author": "Zhihong Wu and Lishuang Wang and Kebin Sun and Zhuozhao Li and Ran Cheng", "abstract": "Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm for tasks such as symbolic regression, classification, and robotic control. Due to the intensive computational demands of running TGP, GPU acceleration is crucial for achieving scalable performance. However, efficient GPU-based execution of TGP remains challenging, primarily due to three core issues: (1) the structural heterogeneity of program individuals, (2) the complexity of integrating multiple levels of parallelism, and (3) the incompatibility between high-performance CUDA execution and flexible Python-based environments. To address these issues, we propose EvoGP, a high-performance framework tailored for GPU acceleration of TGP via population-level parallel execution. First, EvoGP introduces a tensorized representation that encodes variable-sized trees into fixed-shape, memory-aligned arrays, enabling uniform memory access and parallel computation across diverse individuals. Second, EvoGP adopts an adaptive parallelism strategy that dynamically combines intra- and inter-individual parallelism based on dataset size, ensuring high GPU utilization across a broad spectrum of tasks. Third, EvoGP embeds custom CUDA kernels into the PyTorch runtime, achieving seamless integration with Python-based environments such as Gym, MuJoCo, Brax, and Genesis. Experimental results demonstrate that EvoGP achieves a peak throughput exceeding $10^{11}$ GPops/s. Specifcially, this performance represents a speedup of up to $304\\times$ over existing GPU-based TGP implementations and $18\\times$ over state-of-the-art CPU-based libraries. Furthermore, EvoGP maintains comparable accuracy and exhibits improved scalability across large population sizes. EvoGP is open source and accessible at: https://github.com/EMI-Group/evogp.", "link": "http://arxiv.org/abs/2501.17168v6", "date": "2026-01-13", "relevancy": 2.2637, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4584}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%20for%20GPU%20Acceleration&body=Title%3A%20Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%20for%20GPU%20Acceleration%0AAuthor%3A%20Zhihong%20Wu%20and%20Lishuang%20Wang%20and%20Kebin%20Sun%20and%20Zhuozhao%20Li%20and%20Ran%20Cheng%0AAbstract%3A%20Tree-based%20Genetic%20Programming%20%28TGP%29%20is%20a%20widely%20used%20evolutionary%20algorithm%20for%20tasks%20such%20as%20symbolic%20regression%2C%20classification%2C%20and%20robotic%20control.%20Due%20to%20the%20intensive%20computational%20demands%20of%20running%20TGP%2C%20GPU%20acceleration%20is%20crucial%20for%20achieving%20scalable%20performance.%20However%2C%20efficient%20GPU-based%20execution%20of%20TGP%20remains%20challenging%2C%20primarily%20due%20to%20three%20core%20issues%3A%20%281%29%20the%20structural%20heterogeneity%20of%20program%20individuals%2C%20%282%29%20the%20complexity%20of%20integrating%20multiple%20levels%20of%20parallelism%2C%20and%20%283%29%20the%20incompatibility%20between%20high-performance%20CUDA%20execution%20and%20flexible%20Python-based%20environments.%20To%20address%20these%20issues%2C%20we%20propose%20EvoGP%2C%20a%20high-performance%20framework%20tailored%20for%20GPU%20acceleration%20of%20TGP%20via%20population-level%20parallel%20execution.%20First%2C%20EvoGP%20introduces%20a%20tensorized%20representation%20that%20encodes%20variable-sized%20trees%20into%20fixed-shape%2C%20memory-aligned%20arrays%2C%20enabling%20uniform%20memory%20access%20and%20parallel%20computation%20across%20diverse%20individuals.%20Second%2C%20EvoGP%20adopts%20an%20adaptive%20parallelism%20strategy%20that%20dynamically%20combines%20intra-%20and%20inter-individual%20parallelism%20based%20on%20dataset%20size%2C%20ensuring%20high%20GPU%20utilization%20across%20a%20broad%20spectrum%20of%20tasks.%20Third%2C%20EvoGP%20embeds%20custom%20CUDA%20kernels%20into%20the%20PyTorch%20runtime%2C%20achieving%20seamless%20integration%20with%20Python-based%20environments%20such%20as%20Gym%2C%20MuJoCo%2C%20Brax%2C%20and%20Genesis.%20Experimental%20results%20demonstrate%20that%20EvoGP%20achieves%20a%20peak%20throughput%20exceeding%20%2410%5E%7B11%7D%24%20GPops/s.%20Specifcially%2C%20this%20performance%20represents%20a%20speedup%20of%20up%20to%20%24304%5Ctimes%24%20over%20existing%20GPU-based%20TGP%20implementations%20and%20%2418%5Ctimes%24%20over%20state-of-the-art%20CPU-based%20libraries.%20Furthermore%2C%20EvoGP%20maintains%20comparable%20accuracy%20and%20exhibits%20improved%20scalability%20across%20large%20population%20sizes.%20EvoGP%20is%20open%20source%20and%20accessible%20at%3A%20https%3A//github.com/EMI-Group/evogp.%0ALink%3A%20http%3A//arxiv.org/abs/2501.17168v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Population-Level%2520Parallelism%2520in%2520Tree-Based%2520Genetic%2520Programming%2520for%2520GPU%2520Acceleration%26entry.906535625%3DZhihong%2520Wu%2520and%2520Lishuang%2520Wang%2520and%2520Kebin%2520Sun%2520and%2520Zhuozhao%2520Li%2520and%2520Ran%2520Cheng%26entry.1292438233%3DTree-based%2520Genetic%2520Programming%2520%2528TGP%2529%2520is%2520a%2520widely%2520used%2520evolutionary%2520algorithm%2520for%2520tasks%2520such%2520as%2520symbolic%2520regression%252C%2520classification%252C%2520and%2520robotic%2520control.%2520Due%2520to%2520the%2520intensive%2520computational%2520demands%2520of%2520running%2520TGP%252C%2520GPU%2520acceleration%2520is%2520crucial%2520for%2520achieving%2520scalable%2520performance.%2520However%252C%2520efficient%2520GPU-based%2520execution%2520of%2520TGP%2520remains%2520challenging%252C%2520primarily%2520due%2520to%2520three%2520core%2520issues%253A%2520%25281%2529%2520the%2520structural%2520heterogeneity%2520of%2520program%2520individuals%252C%2520%25282%2529%2520the%2520complexity%2520of%2520integrating%2520multiple%2520levels%2520of%2520parallelism%252C%2520and%2520%25283%2529%2520the%2520incompatibility%2520between%2520high-performance%2520CUDA%2520execution%2520and%2520flexible%2520Python-based%2520environments.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520EvoGP%252C%2520a%2520high-performance%2520framework%2520tailored%2520for%2520GPU%2520acceleration%2520of%2520TGP%2520via%2520population-level%2520parallel%2520execution.%2520First%252C%2520EvoGP%2520introduces%2520a%2520tensorized%2520representation%2520that%2520encodes%2520variable-sized%2520trees%2520into%2520fixed-shape%252C%2520memory-aligned%2520arrays%252C%2520enabling%2520uniform%2520memory%2520access%2520and%2520parallel%2520computation%2520across%2520diverse%2520individuals.%2520Second%252C%2520EvoGP%2520adopts%2520an%2520adaptive%2520parallelism%2520strategy%2520that%2520dynamically%2520combines%2520intra-%2520and%2520inter-individual%2520parallelism%2520based%2520on%2520dataset%2520size%252C%2520ensuring%2520high%2520GPU%2520utilization%2520across%2520a%2520broad%2520spectrum%2520of%2520tasks.%2520Third%252C%2520EvoGP%2520embeds%2520custom%2520CUDA%2520kernels%2520into%2520the%2520PyTorch%2520runtime%252C%2520achieving%2520seamless%2520integration%2520with%2520Python-based%2520environments%2520such%2520as%2520Gym%252C%2520MuJoCo%252C%2520Brax%252C%2520and%2520Genesis.%2520Experimental%2520results%2520demonstrate%2520that%2520EvoGP%2520achieves%2520a%2520peak%2520throughput%2520exceeding%2520%252410%255E%257B11%257D%2524%2520GPops/s.%2520Specifcially%252C%2520this%2520performance%2520represents%2520a%2520speedup%2520of%2520up%2520to%2520%2524304%255Ctimes%2524%2520over%2520existing%2520GPU-based%2520TGP%2520implementations%2520and%2520%252418%255Ctimes%2524%2520over%2520state-of-the-art%2520CPU-based%2520libraries.%2520Furthermore%252C%2520EvoGP%2520maintains%2520comparable%2520accuracy%2520and%2520exhibits%2520improved%2520scalability%2520across%2520large%2520population%2520sizes.%2520EvoGP%2520is%2520open%2520source%2520and%2520accessible%2520at%253A%2520https%253A//github.com/EMI-Group/evogp.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17168v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%20for%20GPU%20Acceleration&entry.906535625=Zhihong%20Wu%20and%20Lishuang%20Wang%20and%20Kebin%20Sun%20and%20Zhuozhao%20Li%20and%20Ran%20Cheng&entry.1292438233=Tree-based%20Genetic%20Programming%20%28TGP%29%20is%20a%20widely%20used%20evolutionary%20algorithm%20for%20tasks%20such%20as%20symbolic%20regression%2C%20classification%2C%20and%20robotic%20control.%20Due%20to%20the%20intensive%20computational%20demands%20of%20running%20TGP%2C%20GPU%20acceleration%20is%20crucial%20for%20achieving%20scalable%20performance.%20However%2C%20efficient%20GPU-based%20execution%20of%20TGP%20remains%20challenging%2C%20primarily%20due%20to%20three%20core%20issues%3A%20%281%29%20the%20structural%20heterogeneity%20of%20program%20individuals%2C%20%282%29%20the%20complexity%20of%20integrating%20multiple%20levels%20of%20parallelism%2C%20and%20%283%29%20the%20incompatibility%20between%20high-performance%20CUDA%20execution%20and%20flexible%20Python-based%20environments.%20To%20address%20these%20issues%2C%20we%20propose%20EvoGP%2C%20a%20high-performance%20framework%20tailored%20for%20GPU%20acceleration%20of%20TGP%20via%20population-level%20parallel%20execution.%20First%2C%20EvoGP%20introduces%20a%20tensorized%20representation%20that%20encodes%20variable-sized%20trees%20into%20fixed-shape%2C%20memory-aligned%20arrays%2C%20enabling%20uniform%20memory%20access%20and%20parallel%20computation%20across%20diverse%20individuals.%20Second%2C%20EvoGP%20adopts%20an%20adaptive%20parallelism%20strategy%20that%20dynamically%20combines%20intra-%20and%20inter-individual%20parallelism%20based%20on%20dataset%20size%2C%20ensuring%20high%20GPU%20utilization%20across%20a%20broad%20spectrum%20of%20tasks.%20Third%2C%20EvoGP%20embeds%20custom%20CUDA%20kernels%20into%20the%20PyTorch%20runtime%2C%20achieving%20seamless%20integration%20with%20Python-based%20environments%20such%20as%20Gym%2C%20MuJoCo%2C%20Brax%2C%20and%20Genesis.%20Experimental%20results%20demonstrate%20that%20EvoGP%20achieves%20a%20peak%20throughput%20exceeding%20%2410%5E%7B11%7D%24%20GPops/s.%20Specifcially%2C%20this%20performance%20represents%20a%20speedup%20of%20up%20to%20%24304%5Ctimes%24%20over%20existing%20GPU-based%20TGP%20implementations%20and%20%2418%5Ctimes%24%20over%20state-of-the-art%20CPU-based%20libraries.%20Furthermore%2C%20EvoGP%20maintains%20comparable%20accuracy%20and%20exhibits%20improved%20scalability%20across%20large%20population%20sizes.%20EvoGP%20is%20open%20source%20and%20accessible%20at%3A%20https%3A//github.com/EMI-Group/evogp.&entry.1838667208=http%3A//arxiv.org/abs/2501.17168v6&entry.124074799=Read"},
{"title": "Real-Time Localization Framework for Autonomous Basketball Robots", "author": "Naren Medarametla and Sreejon Mondal", "abstract": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.", "link": "http://arxiv.org/abs/2601.08713v1", "date": "2026-01-13", "relevancy": 2.2573, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5708}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Localization%20Framework%20for%20Autonomous%20Basketball%20Robots&body=Title%3A%20Real-Time%20Localization%20Framework%20for%20Autonomous%20Basketball%20Robots%0AAuthor%3A%20Naren%20Medarametla%20and%20Sreejon%20Mondal%0AAbstract%3A%20Localization%20is%20a%20fundamental%20capability%20for%20autonomous%20robots%2C%20enabling%20them%20to%20operate%20effectively%20in%20dynamic%20environments.%20In%20Robocon%202025%2C%20accurate%20and%20reliable%20localization%20is%20crucial%20for%20improving%20shooting%20precision%2C%20avoiding%20collisions%20with%20other%20robots%2C%20and%20navigating%20the%20competition%20field%20efficiently.%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20localization%20algorithm%20that%20integrates%20classical%20techniques%20with%20learning%20based%20methods%20that%20rely%20solely%20on%20visual%20data%20from%20the%20court%27s%20floor%20to%20achieve%20self-localization%20on%20the%20basketball%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Localization%2520Framework%2520for%2520Autonomous%2520Basketball%2520Robots%26entry.906535625%3DNaren%2520Medarametla%2520and%2520Sreejon%2520Mondal%26entry.1292438233%3DLocalization%2520is%2520a%2520fundamental%2520capability%2520for%2520autonomous%2520robots%252C%2520enabling%2520them%2520to%2520operate%2520effectively%2520in%2520dynamic%2520environments.%2520In%2520Robocon%25202025%252C%2520accurate%2520and%2520reliable%2520localization%2520is%2520crucial%2520for%2520improving%2520shooting%2520precision%252C%2520avoiding%2520collisions%2520with%2520other%2520robots%252C%2520and%2520navigating%2520the%2520competition%2520field%2520efficiently.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hybrid%2520localization%2520algorithm%2520that%2520integrates%2520classical%2520techniques%2520with%2520learning%2520based%2520methods%2520that%2520rely%2520solely%2520on%2520visual%2520data%2520from%2520the%2520court%2527s%2520floor%2520to%2520achieve%2520self-localization%2520on%2520the%2520basketball%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Localization%20Framework%20for%20Autonomous%20Basketball%20Robots&entry.906535625=Naren%20Medarametla%20and%20Sreejon%20Mondal&entry.1292438233=Localization%20is%20a%20fundamental%20capability%20for%20autonomous%20robots%2C%20enabling%20them%20to%20operate%20effectively%20in%20dynamic%20environments.%20In%20Robocon%202025%2C%20accurate%20and%20reliable%20localization%20is%20crucial%20for%20improving%20shooting%20precision%2C%20avoiding%20collisions%20with%20other%20robots%2C%20and%20navigating%20the%20competition%20field%20efficiently.%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20localization%20algorithm%20that%20integrates%20classical%20techniques%20with%20learning%20based%20methods%20that%20rely%20solely%20on%20visual%20data%20from%20the%20court%27s%20floor%20to%20achieve%20self-localization%20on%20the%20basketball%20field.&entry.1838667208=http%3A//arxiv.org/abs/2601.08713v1&entry.124074799=Read"},
{"title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback", "author": "Prithwish Jana and Sam Davidson and Bhavana Bhasker and Andrey Kan and Anoop Deoras and Laurent Callot", "abstract": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.", "link": "http://arxiv.org/abs/2601.08734v1", "date": "2026-01-13", "relevancy": 2.2536, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TerraFormer%3A%20Automated%20Infrastructure-as-Code%20with%20LLMs%20Fine-Tuned%20via%20Policy-Guided%20Verifier%20Feedback&body=Title%3A%20TerraFormer%3A%20Automated%20Infrastructure-as-Code%20with%20LLMs%20Fine-Tuned%20via%20Policy-Guided%20Verifier%20Feedback%0AAuthor%3A%20Prithwish%20Jana%20and%20Sam%20Davidson%20and%20Bhavana%20Bhasker%20and%20Andrey%20Kan%20and%20Anoop%20Deoras%20and%20Laurent%20Callot%0AAbstract%3A%20Automating%20Infrastructure-as-Code%20%28IaC%29%20is%20challenging%2C%20and%20large%20language%20models%20%28LLMs%29%20often%20produce%20incorrect%20configurations%20from%20natural%20language%20%28NL%29.%20We%20present%20TerraFormer%2C%20a%20neuro-symbolic%20framework%20for%20IaC%20generation%20and%20mutation%20that%20combines%20supervised%20fine-tuning%20with%20verifier-guided%20reinforcement%20learning%2C%20using%20formal%20verification%20tools%20to%20provide%20feedback%20on%20syntax%2C%20deployability%2C%20and%20policy%20compliance.%20We%20curate%20two%20large%2C%20high-quality%20NL-to-IaC%20datasets%2C%20TF-Gen%20%28152k%20instances%29%20and%20TF-Mutn%20%2852k%20instances%29%2C%20via%20multi-stage%20verification%20and%20iterative%20LLM%20self-correction.%20Evaluations%20against%2017%20state-of-the-art%20LLMs%2C%20including%20~50x%20larger%20models%20like%20Sonnet%203.7%2C%20DeepSeek-R1%2C%20and%20GPT-4.1%2C%20show%20that%20TerraFormer%20improves%20correctness%20over%20its%20base%20LLM%20by%2015.94%25%20on%20IaC-Eval%2C%2011.65%25%20on%20TF-Gen%20%28Test%29%2C%20and%2019.60%25%20on%20TF-Mutn%20%28Test%29.%20It%20outperforms%20larger%20models%20on%20both%20TF-Gen%20%28Test%29%20and%20TF-Mutn%20%28Test%29%2C%20ranks%20third%20on%20IaC-Eval%2C%20and%20achieves%20top%20best-practices%20and%20security%20compliance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerraFormer%253A%2520Automated%2520Infrastructure-as-Code%2520with%2520LLMs%2520Fine-Tuned%2520via%2520Policy-Guided%2520Verifier%2520Feedback%26entry.906535625%3DPrithwish%2520Jana%2520and%2520Sam%2520Davidson%2520and%2520Bhavana%2520Bhasker%2520and%2520Andrey%2520Kan%2520and%2520Anoop%2520Deoras%2520and%2520Laurent%2520Callot%26entry.1292438233%3DAutomating%2520Infrastructure-as-Code%2520%2528IaC%2529%2520is%2520challenging%252C%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520produce%2520incorrect%2520configurations%2520from%2520natural%2520language%2520%2528NL%2529.%2520We%2520present%2520TerraFormer%252C%2520a%2520neuro-symbolic%2520framework%2520for%2520IaC%2520generation%2520and%2520mutation%2520that%2520combines%2520supervised%2520fine-tuning%2520with%2520verifier-guided%2520reinforcement%2520learning%252C%2520using%2520formal%2520verification%2520tools%2520to%2520provide%2520feedback%2520on%2520syntax%252C%2520deployability%252C%2520and%2520policy%2520compliance.%2520We%2520curate%2520two%2520large%252C%2520high-quality%2520NL-to-IaC%2520datasets%252C%2520TF-Gen%2520%2528152k%2520instances%2529%2520and%2520TF-Mutn%2520%252852k%2520instances%2529%252C%2520via%2520multi-stage%2520verification%2520and%2520iterative%2520LLM%2520self-correction.%2520Evaluations%2520against%252017%2520state-of-the-art%2520LLMs%252C%2520including%2520~50x%2520larger%2520models%2520like%2520Sonnet%25203.7%252C%2520DeepSeek-R1%252C%2520and%2520GPT-4.1%252C%2520show%2520that%2520TerraFormer%2520improves%2520correctness%2520over%2520its%2520base%2520LLM%2520by%252015.94%2525%2520on%2520IaC-Eval%252C%252011.65%2525%2520on%2520TF-Gen%2520%2528Test%2529%252C%2520and%252019.60%2525%2520on%2520TF-Mutn%2520%2528Test%2529.%2520It%2520outperforms%2520larger%2520models%2520on%2520both%2520TF-Gen%2520%2528Test%2529%2520and%2520TF-Mutn%2520%2528Test%2529%252C%2520ranks%2520third%2520on%2520IaC-Eval%252C%2520and%2520achieves%2520top%2520best-practices%2520and%2520security%2520compliance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TerraFormer%3A%20Automated%20Infrastructure-as-Code%20with%20LLMs%20Fine-Tuned%20via%20Policy-Guided%20Verifier%20Feedback&entry.906535625=Prithwish%20Jana%20and%20Sam%20Davidson%20and%20Bhavana%20Bhasker%20and%20Andrey%20Kan%20and%20Anoop%20Deoras%20and%20Laurent%20Callot&entry.1292438233=Automating%20Infrastructure-as-Code%20%28IaC%29%20is%20challenging%2C%20and%20large%20language%20models%20%28LLMs%29%20often%20produce%20incorrect%20configurations%20from%20natural%20language%20%28NL%29.%20We%20present%20TerraFormer%2C%20a%20neuro-symbolic%20framework%20for%20IaC%20generation%20and%20mutation%20that%20combines%20supervised%20fine-tuning%20with%20verifier-guided%20reinforcement%20learning%2C%20using%20formal%20verification%20tools%20to%20provide%20feedback%20on%20syntax%2C%20deployability%2C%20and%20policy%20compliance.%20We%20curate%20two%20large%2C%20high-quality%20NL-to-IaC%20datasets%2C%20TF-Gen%20%28152k%20instances%29%20and%20TF-Mutn%20%2852k%20instances%29%2C%20via%20multi-stage%20verification%20and%20iterative%20LLM%20self-correction.%20Evaluations%20against%2017%20state-of-the-art%20LLMs%2C%20including%20~50x%20larger%20models%20like%20Sonnet%203.7%2C%20DeepSeek-R1%2C%20and%20GPT-4.1%2C%20show%20that%20TerraFormer%20improves%20correctness%20over%20its%20base%20LLM%20by%2015.94%25%20on%20IaC-Eval%2C%2011.65%25%20on%20TF-Gen%20%28Test%29%2C%20and%2019.60%25%20on%20TF-Mutn%20%28Test%29.%20It%20outperforms%20larger%20models%20on%20both%20TF-Gen%20%28Test%29%20and%20TF-Mutn%20%28Test%29%2C%20ranks%20third%20on%20IaC-Eval%2C%20and%20achieves%20top%20best-practices%20and%20security%20compliance.&entry.1838667208=http%3A//arxiv.org/abs/2601.08734v1&entry.124074799=Read"},
{"title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement", "author": "Zhenlong Dai and Zhuoluo Zhao and Hengning Wang and Xiu Tang and Sai Wu and Chang Yao and Zhipeng Gao and Jingyuan Chen", "abstract": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.", "link": "http://arxiv.org/abs/2601.08545v1", "date": "2026-01-13", "relevancy": 2.2506, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4562}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learner-Tailored%20Program%20Repair%3A%20A%20Solution%20Generator%20with%20Iterative%20Edit-Driven%20Retrieval%20Enhancement&body=Title%3A%20Learner-Tailored%20Program%20Repair%3A%20A%20Solution%20Generator%20with%20Iterative%20Edit-Driven%20Retrieval%20Enhancement%0AAuthor%3A%20Zhenlong%20Dai%20and%20Zhuoluo%20Zhao%20and%20Hengning%20Wang%20and%20Xiu%20Tang%20and%20Sai%20Wu%20and%20Chang%20Yao%20and%20Zhipeng%20Gao%20and%20Jingyuan%20Chen%0AAbstract%3A%20With%20the%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20field%20of%20programming%2C%20intelligent%20programming%20coaching%20systems%20have%20gained%20widespread%20attention.%20However%2C%20most%20research%20focuses%20on%20repairing%20the%20buggy%20code%20of%20programming%20learners%20without%20providing%20the%20underlying%20causes%20of%20the%20bugs.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20novel%20task%2C%20namely%20%5Ctextbf%7BLPR%7D%20%28%5Ctextbf%7BL%7Dearner-Tailored%20%5Ctextbf%7BP%7Drogram%20%5Ctextbf%7BR%7Depair%29.%20We%20then%20propose%20a%20novel%20and%20effective%20framework%2C%20%5Ctextbf%7B%5Ctextsc%7B%5CMethodName%7B%7D%7D%7D%20%28%5Ctextbf%7BL%7Dearner-Tailored%20%5Ctextbf%7BS%7Dolution%20%5Ctextbf%7BG%7Denerator%29%2C%20to%20enhance%20program%20repair%20while%20offering%20the%20bug%20descriptions%20for%20the%20buggy%20code.%20In%20the%20first%20stage%2C%20we%20utilize%20a%20repair%20solution%20retrieval%20framework%20to%20construct%20a%20solution%20retrieval%20database%20and%20then%20employ%20an%20edit-driven%20code%20retrieval%20approach%20to%20retrieve%20valuable%20solutions%2C%20guiding%20LLMs%20in%20identifying%20and%20fixing%20the%20bugs%20in%20buggy%20code.%20In%20the%20second%20stage%2C%20we%20propose%20a%20solution-guided%20program%20repair%20method%2C%20which%20fixes%20the%20code%20and%20provides%20explanations%20under%20the%20guidance%20of%20retrieval%20solutions.%20Moreover%2C%20we%20propose%20an%20Iterative%20Retrieval%20Enhancement%20method%20that%20utilizes%20evaluation%20results%20of%20the%20generated%20code%20to%20iteratively%20optimize%20the%20retrieval%20direction%20and%20explore%20more%20suitable%20repair%20strategies%2C%20improving%20performance%20in%20practical%20programming%20coaching%20scenarios.%20The%20experimental%20results%20show%20that%20our%20approach%20outperforms%20a%20set%20of%20baselines%20by%20a%20large%20margin%2C%20validating%20the%20effectiveness%20of%20our%20framework%20for%20the%20newly%20proposed%20LPR%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearner-Tailored%2520Program%2520Repair%253A%2520A%2520Solution%2520Generator%2520with%2520Iterative%2520Edit-Driven%2520Retrieval%2520Enhancement%26entry.906535625%3DZhenlong%2520Dai%2520and%2520Zhuoluo%2520Zhao%2520and%2520Hengning%2520Wang%2520and%2520Xiu%2520Tang%2520and%2520Sai%2520Wu%2520and%2520Chang%2520Yao%2520and%2520Zhipeng%2520Gao%2520and%2520Jingyuan%2520Chen%26entry.1292438233%3DWith%2520the%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520the%2520field%2520of%2520programming%252C%2520intelligent%2520programming%2520coaching%2520systems%2520have%2520gained%2520widespread%2520attention.%2520However%252C%2520most%2520research%2520focuses%2520on%2520repairing%2520the%2520buggy%2520code%2520of%2520programming%2520learners%2520without%2520providing%2520the%2520underlying%2520causes%2520of%2520the%2520bugs.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520task%252C%2520namely%2520%255Ctextbf%257BLPR%257D%2520%2528%255Ctextbf%257BL%257Dearner-Tailored%2520%255Ctextbf%257BP%257Drogram%2520%255Ctextbf%257BR%257Depair%2529.%2520We%2520then%2520propose%2520a%2520novel%2520and%2520effective%2520framework%252C%2520%255Ctextbf%257B%255Ctextsc%257B%255CMethodName%257B%257D%257D%257D%2520%2528%255Ctextbf%257BL%257Dearner-Tailored%2520%255Ctextbf%257BS%257Dolution%2520%255Ctextbf%257BG%257Denerator%2529%252C%2520to%2520enhance%2520program%2520repair%2520while%2520offering%2520the%2520bug%2520descriptions%2520for%2520the%2520buggy%2520code.%2520In%2520the%2520first%2520stage%252C%2520we%2520utilize%2520a%2520repair%2520solution%2520retrieval%2520framework%2520to%2520construct%2520a%2520solution%2520retrieval%2520database%2520and%2520then%2520employ%2520an%2520edit-driven%2520code%2520retrieval%2520approach%2520to%2520retrieve%2520valuable%2520solutions%252C%2520guiding%2520LLMs%2520in%2520identifying%2520and%2520fixing%2520the%2520bugs%2520in%2520buggy%2520code.%2520In%2520the%2520second%2520stage%252C%2520we%2520propose%2520a%2520solution-guided%2520program%2520repair%2520method%252C%2520which%2520fixes%2520the%2520code%2520and%2520provides%2520explanations%2520under%2520the%2520guidance%2520of%2520retrieval%2520solutions.%2520Moreover%252C%2520we%2520propose%2520an%2520Iterative%2520Retrieval%2520Enhancement%2520method%2520that%2520utilizes%2520evaluation%2520results%2520of%2520the%2520generated%2520code%2520to%2520iteratively%2520optimize%2520the%2520retrieval%2520direction%2520and%2520explore%2520more%2520suitable%2520repair%2520strategies%252C%2520improving%2520performance%2520in%2520practical%2520programming%2520coaching%2520scenarios.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520approach%2520outperforms%2520a%2520set%2520of%2520baselines%2520by%2520a%2520large%2520margin%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520framework%2520for%2520the%2520newly%2520proposed%2520LPR%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learner-Tailored%20Program%20Repair%3A%20A%20Solution%20Generator%20with%20Iterative%20Edit-Driven%20Retrieval%20Enhancement&entry.906535625=Zhenlong%20Dai%20and%20Zhuoluo%20Zhao%20and%20Hengning%20Wang%20and%20Xiu%20Tang%20and%20Sai%20Wu%20and%20Chang%20Yao%20and%20Zhipeng%20Gao%20and%20Jingyuan%20Chen&entry.1292438233=With%20the%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20field%20of%20programming%2C%20intelligent%20programming%20coaching%20systems%20have%20gained%20widespread%20attention.%20However%2C%20most%20research%20focuses%20on%20repairing%20the%20buggy%20code%20of%20programming%20learners%20without%20providing%20the%20underlying%20causes%20of%20the%20bugs.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20novel%20task%2C%20namely%20%5Ctextbf%7BLPR%7D%20%28%5Ctextbf%7BL%7Dearner-Tailored%20%5Ctextbf%7BP%7Drogram%20%5Ctextbf%7BR%7Depair%29.%20We%20then%20propose%20a%20novel%20and%20effective%20framework%2C%20%5Ctextbf%7B%5Ctextsc%7B%5CMethodName%7B%7D%7D%7D%20%28%5Ctextbf%7BL%7Dearner-Tailored%20%5Ctextbf%7BS%7Dolution%20%5Ctextbf%7BG%7Denerator%29%2C%20to%20enhance%20program%20repair%20while%20offering%20the%20bug%20descriptions%20for%20the%20buggy%20code.%20In%20the%20first%20stage%2C%20we%20utilize%20a%20repair%20solution%20retrieval%20framework%20to%20construct%20a%20solution%20retrieval%20database%20and%20then%20employ%20an%20edit-driven%20code%20retrieval%20approach%20to%20retrieve%20valuable%20solutions%2C%20guiding%20LLMs%20in%20identifying%20and%20fixing%20the%20bugs%20in%20buggy%20code.%20In%20the%20second%20stage%2C%20we%20propose%20a%20solution-guided%20program%20repair%20method%2C%20which%20fixes%20the%20code%20and%20provides%20explanations%20under%20the%20guidance%20of%20retrieval%20solutions.%20Moreover%2C%20we%20propose%20an%20Iterative%20Retrieval%20Enhancement%20method%20that%20utilizes%20evaluation%20results%20of%20the%20generated%20code%20to%20iteratively%20optimize%20the%20retrieval%20direction%20and%20explore%20more%20suitable%20repair%20strategies%2C%20improving%20performance%20in%20practical%20programming%20coaching%20scenarios.%20The%20experimental%20results%20show%20that%20our%20approach%20outperforms%20a%20set%20of%20baselines%20by%20a%20large%20margin%2C%20validating%20the%20effectiveness%20of%20our%20framework%20for%20the%20newly%20proposed%20LPR%20task.&entry.1838667208=http%3A//arxiv.org/abs/2601.08545v1&entry.124074799=Read"},
{"title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2", "author": "Yizhan Feng and Hichem Snoussi and Jing Teng and Jian Liu and Yuyang Wang and Abel Cherouat and Tian Wang", "abstract": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.", "link": "http://arxiv.org/abs/2601.08408v1", "date": "2026-01-13", "relevancy": 2.2393, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Optimized%20Multimodal%20Learning%20for%20UAV%20Video%20Understanding%20via%20BLIP-2&body=Title%3A%20Edge-Optimized%20Multimodal%20Learning%20for%20UAV%20Video%20Understanding%20via%20BLIP-2%0AAuthor%3A%20Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Jing%20Teng%20and%20Jian%20Liu%20and%20Yuyang%20Wang%20and%20Abel%20Cherouat%20and%20Tian%20Wang%0AAbstract%3A%20The%20demand%20for%20real-time%20visual%20understanding%20and%20interaction%20in%20complex%20scenarios%20is%20increasingly%20critical%20for%20unmanned%20aerial%20vehicles.%20However%2C%20a%20significant%20challenge%20arises%20from%20the%20contradiction%20between%20the%20high%20computational%20cost%20of%20large%20Vision%20language%20models%20and%20the%20limited%20computing%20resources%20available%20on%20UAV%20edge%20devices.%20To%20address%20this%20challenge%2C%20this%20paper%20proposes%20a%20lightweight%20multimodal%20task%20platform%20based%20on%20BLIP-2%2C%20integrated%20with%20YOLO-World%20and%20YOLOv8-Seg%20models.%20This%20integration%20extends%20the%20multi-task%20capabilities%20of%20BLIP-2%20for%20UAV%20applications%20with%20minimal%20adaptation%20and%20without%20requiring%20task-specific%20fine-tuning%20on%20drone%20data.%20Firstly%2C%20the%20deep%20integration%20of%20BLIP-2%20with%20YOLO%20models%20enables%20it%20to%20leverage%20the%20precise%20perceptual%20results%20of%20YOLO%20for%20fundamental%20tasks%20like%20object%20detection%20and%20instance%20segmentation%2C%20thereby%20facilitating%20deeper%20visual-attention%20understanding%20and%20reasoning.%20Secondly%2C%20a%20content-aware%20key%20frame%20sampling%20mechanism%20based%20on%20K-Means%20clustering%20is%20designed%2C%20which%20incorporates%20intelligent%20frame%20selection%20and%20temporal%20feature%20concatenation.%20This%20equips%20the%20lightweight%20BLIP-2%20architecture%20with%20the%20capability%20to%20handle%20video-level%20interactive%20tasks%20effectively.%20Thirdly%2C%20a%20unified%20prompt%20optimization%20scheme%20for%20multi-task%20adaptation%20is%20implemented.%20This%20scheme%20strategically%20injects%20structured%20event%20logs%20from%20the%20YOLO%20models%20as%20contextual%20information%20into%20BLIP-2%27s%20input.%20Combined%20with%20output%20constraints%20designed%20to%20filter%20out%20technical%20details%2C%20this%20approach%20effectively%20guides%20the%20model%20to%20generate%20accurate%20and%20contextually%20relevant%20outputs%20for%20various%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Optimized%2520Multimodal%2520Learning%2520for%2520UAV%2520Video%2520Understanding%2520via%2520BLIP-2%26entry.906535625%3DYizhan%2520Feng%2520and%2520Hichem%2520Snoussi%2520and%2520Jing%2520Teng%2520and%2520Jian%2520Liu%2520and%2520Yuyang%2520Wang%2520and%2520Abel%2520Cherouat%2520and%2520Tian%2520Wang%26entry.1292438233%3DThe%2520demand%2520for%2520real-time%2520visual%2520understanding%2520and%2520interaction%2520in%2520complex%2520scenarios%2520is%2520increasingly%2520critical%2520for%2520unmanned%2520aerial%2520vehicles.%2520However%252C%2520a%2520significant%2520challenge%2520arises%2520from%2520the%2520contradiction%2520between%2520the%2520high%2520computational%2520cost%2520of%2520large%2520Vision%2520language%2520models%2520and%2520the%2520limited%2520computing%2520resources%2520available%2520on%2520UAV%2520edge%2520devices.%2520To%2520address%2520this%2520challenge%252C%2520this%2520paper%2520proposes%2520a%2520lightweight%2520multimodal%2520task%2520platform%2520based%2520on%2520BLIP-2%252C%2520integrated%2520with%2520YOLO-World%2520and%2520YOLOv8-Seg%2520models.%2520This%2520integration%2520extends%2520the%2520multi-task%2520capabilities%2520of%2520BLIP-2%2520for%2520UAV%2520applications%2520with%2520minimal%2520adaptation%2520and%2520without%2520requiring%2520task-specific%2520fine-tuning%2520on%2520drone%2520data.%2520Firstly%252C%2520the%2520deep%2520integration%2520of%2520BLIP-2%2520with%2520YOLO%2520models%2520enables%2520it%2520to%2520leverage%2520the%2520precise%2520perceptual%2520results%2520of%2520YOLO%2520for%2520fundamental%2520tasks%2520like%2520object%2520detection%2520and%2520instance%2520segmentation%252C%2520thereby%2520facilitating%2520deeper%2520visual-attention%2520understanding%2520and%2520reasoning.%2520Secondly%252C%2520a%2520content-aware%2520key%2520frame%2520sampling%2520mechanism%2520based%2520on%2520K-Means%2520clustering%2520is%2520designed%252C%2520which%2520incorporates%2520intelligent%2520frame%2520selection%2520and%2520temporal%2520feature%2520concatenation.%2520This%2520equips%2520the%2520lightweight%2520BLIP-2%2520architecture%2520with%2520the%2520capability%2520to%2520handle%2520video-level%2520interactive%2520tasks%2520effectively.%2520Thirdly%252C%2520a%2520unified%2520prompt%2520optimization%2520scheme%2520for%2520multi-task%2520adaptation%2520is%2520implemented.%2520This%2520scheme%2520strategically%2520injects%2520structured%2520event%2520logs%2520from%2520the%2520YOLO%2520models%2520as%2520contextual%2520information%2520into%2520BLIP-2%2527s%2520input.%2520Combined%2520with%2520output%2520constraints%2520designed%2520to%2520filter%2520out%2520technical%2520details%252C%2520this%2520approach%2520effectively%2520guides%2520the%2520model%2520to%2520generate%2520accurate%2520and%2520contextually%2520relevant%2520outputs%2520for%2520various%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Optimized%20Multimodal%20Learning%20for%20UAV%20Video%20Understanding%20via%20BLIP-2&entry.906535625=Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Jing%20Teng%20and%20Jian%20Liu%20and%20Yuyang%20Wang%20and%20Abel%20Cherouat%20and%20Tian%20Wang&entry.1292438233=The%20demand%20for%20real-time%20visual%20understanding%20and%20interaction%20in%20complex%20scenarios%20is%20increasingly%20critical%20for%20unmanned%20aerial%20vehicles.%20However%2C%20a%20significant%20challenge%20arises%20from%20the%20contradiction%20between%20the%20high%20computational%20cost%20of%20large%20Vision%20language%20models%20and%20the%20limited%20computing%20resources%20available%20on%20UAV%20edge%20devices.%20To%20address%20this%20challenge%2C%20this%20paper%20proposes%20a%20lightweight%20multimodal%20task%20platform%20based%20on%20BLIP-2%2C%20integrated%20with%20YOLO-World%20and%20YOLOv8-Seg%20models.%20This%20integration%20extends%20the%20multi-task%20capabilities%20of%20BLIP-2%20for%20UAV%20applications%20with%20minimal%20adaptation%20and%20without%20requiring%20task-specific%20fine-tuning%20on%20drone%20data.%20Firstly%2C%20the%20deep%20integration%20of%20BLIP-2%20with%20YOLO%20models%20enables%20it%20to%20leverage%20the%20precise%20perceptual%20results%20of%20YOLO%20for%20fundamental%20tasks%20like%20object%20detection%20and%20instance%20segmentation%2C%20thereby%20facilitating%20deeper%20visual-attention%20understanding%20and%20reasoning.%20Secondly%2C%20a%20content-aware%20key%20frame%20sampling%20mechanism%20based%20on%20K-Means%20clustering%20is%20designed%2C%20which%20incorporates%20intelligent%20frame%20selection%20and%20temporal%20feature%20concatenation.%20This%20equips%20the%20lightweight%20BLIP-2%20architecture%20with%20the%20capability%20to%20handle%20video-level%20interactive%20tasks%20effectively.%20Thirdly%2C%20a%20unified%20prompt%20optimization%20scheme%20for%20multi-task%20adaptation%20is%20implemented.%20This%20scheme%20strategically%20injects%20structured%20event%20logs%20from%20the%20YOLO%20models%20as%20contextual%20information%20into%20BLIP-2%27s%20input.%20Combined%20with%20output%20constraints%20designed%20to%20filter%20out%20technical%20details%2C%20this%20approach%20effectively%20guides%20the%20model%20to%20generate%20accurate%20and%20contextually%20relevant%20outputs%20for%20various%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.08408v1&entry.124074799=Read"},
{"title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving", "author": "Muxi Diao and Lele Yang and Hongbo Yin and Zhexu Wang and Yejie Wang and Daxin Tian and Kongming Liang and Zhanyu Ma", "abstract": "Effective autonomous driving hinges on robust reasoning across perception, prediction, planning, and behavior. However, conventional end-to-end models fail to generalize in complex scenarios due to the lack of structured reasoning. While recent vision-language models (VLMs) have been applied to driving tasks, they typically rely on isolated modules and static supervision, limiting their ability to support multi-stage decision-making. We present AutoDriveRL, a unified training framework that formulates autonomous driving as a structured reasoning process over four core tasks. Each task is independently modeled as a vision-language QA problem and optimized using task-specific reward models, enabling fine-grained reinforcement signals at different reasoning stages. Within this framework, we train DriveRX, a cross-task reasoning VLM designed for multi-stage decision-making. DriveRX achieves strong performance on the public benchmark, outperforming GPT-4o in behavior reasoning and demonstrating robustness under complex or corrupted driving conditions. DriveRX serves as a high-level semantic reasoning backbone, producing structured stage-wise reasoning chains that enhance decision consistency. These outputs also provide high-quality supervisory signals for annotation and downstream planning/control models. We release the AutoDriveRL framework and DriveRX to support future research.", "link": "http://arxiv.org/abs/2505.20665v2", "date": "2026-01-13", "relevancy": 2.222, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveRX%3A%20A%20Vision-Language%20Reasoning%20Model%20for%20Cross-Task%20Autonomous%20Driving&body=Title%3A%20DriveRX%3A%20A%20Vision-Language%20Reasoning%20Model%20for%20Cross-Task%20Autonomous%20Driving%0AAuthor%3A%20Muxi%20Diao%20and%20Lele%20Yang%20and%20Hongbo%20Yin%20and%20Zhexu%20Wang%20and%20Yejie%20Wang%20and%20Daxin%20Tian%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma%0AAbstract%3A%20Effective%20autonomous%20driving%20hinges%20on%20robust%20reasoning%20across%20perception%2C%20prediction%2C%20planning%2C%20and%20behavior.%20However%2C%20conventional%20end-to-end%20models%20fail%20to%20generalize%20in%20complex%20scenarios%20due%20to%20the%20lack%20of%20structured%20reasoning.%20While%20recent%20vision-language%20models%20%28VLMs%29%20have%20been%20applied%20to%20driving%20tasks%2C%20they%20typically%20rely%20on%20isolated%20modules%20and%20static%20supervision%2C%20limiting%20their%20ability%20to%20support%20multi-stage%20decision-making.%20We%20present%20AutoDriveRL%2C%20a%20unified%20training%20framework%20that%20formulates%20autonomous%20driving%20as%20a%20structured%20reasoning%20process%20over%20four%20core%20tasks.%20Each%20task%20is%20independently%20modeled%20as%20a%20vision-language%20QA%20problem%20and%20optimized%20using%20task-specific%20reward%20models%2C%20enabling%20fine-grained%20reinforcement%20signals%20at%20different%20reasoning%20stages.%20Within%20this%20framework%2C%20we%20train%20DriveRX%2C%20a%20cross-task%20reasoning%20VLM%20designed%20for%20multi-stage%20decision-making.%20DriveRX%20achieves%20strong%20performance%20on%20the%20public%20benchmark%2C%20outperforming%20GPT-4o%20in%20behavior%20reasoning%20and%20demonstrating%20robustness%20under%20complex%20or%20corrupted%20driving%20conditions.%20DriveRX%20serves%20as%20a%20high-level%20semantic%20reasoning%20backbone%2C%20producing%20structured%20stage-wise%20reasoning%20chains%20that%20enhance%20decision%20consistency.%20These%20outputs%20also%20provide%20high-quality%20supervisory%20signals%20for%20annotation%20and%20downstream%20planning/control%20models.%20We%20release%20the%20AutoDriveRL%20framework%20and%20DriveRX%20to%20support%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveRX%253A%2520A%2520Vision-Language%2520Reasoning%2520Model%2520for%2520Cross-Task%2520Autonomous%2520Driving%26entry.906535625%3DMuxi%2520Diao%2520and%2520Lele%2520Yang%2520and%2520Hongbo%2520Yin%2520and%2520Zhexu%2520Wang%2520and%2520Yejie%2520Wang%2520and%2520Daxin%2520Tian%2520and%2520Kongming%2520Liang%2520and%2520Zhanyu%2520Ma%26entry.1292438233%3DEffective%2520autonomous%2520driving%2520hinges%2520on%2520robust%2520reasoning%2520across%2520perception%252C%2520prediction%252C%2520planning%252C%2520and%2520behavior.%2520However%252C%2520conventional%2520end-to-end%2520models%2520fail%2520to%2520generalize%2520in%2520complex%2520scenarios%2520due%2520to%2520the%2520lack%2520of%2520structured%2520reasoning.%2520While%2520recent%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520been%2520applied%2520to%2520driving%2520tasks%252C%2520they%2520typically%2520rely%2520on%2520isolated%2520modules%2520and%2520static%2520supervision%252C%2520limiting%2520their%2520ability%2520to%2520support%2520multi-stage%2520decision-making.%2520We%2520present%2520AutoDriveRL%252C%2520a%2520unified%2520training%2520framework%2520that%2520formulates%2520autonomous%2520driving%2520as%2520a%2520structured%2520reasoning%2520process%2520over%2520four%2520core%2520tasks.%2520Each%2520task%2520is%2520independently%2520modeled%2520as%2520a%2520vision-language%2520QA%2520problem%2520and%2520optimized%2520using%2520task-specific%2520reward%2520models%252C%2520enabling%2520fine-grained%2520reinforcement%2520signals%2520at%2520different%2520reasoning%2520stages.%2520Within%2520this%2520framework%252C%2520we%2520train%2520DriveRX%252C%2520a%2520cross-task%2520reasoning%2520VLM%2520designed%2520for%2520multi-stage%2520decision-making.%2520DriveRX%2520achieves%2520strong%2520performance%2520on%2520the%2520public%2520benchmark%252C%2520outperforming%2520GPT-4o%2520in%2520behavior%2520reasoning%2520and%2520demonstrating%2520robustness%2520under%2520complex%2520or%2520corrupted%2520driving%2520conditions.%2520DriveRX%2520serves%2520as%2520a%2520high-level%2520semantic%2520reasoning%2520backbone%252C%2520producing%2520structured%2520stage-wise%2520reasoning%2520chains%2520that%2520enhance%2520decision%2520consistency.%2520These%2520outputs%2520also%2520provide%2520high-quality%2520supervisory%2520signals%2520for%2520annotation%2520and%2520downstream%2520planning/control%2520models.%2520We%2520release%2520the%2520AutoDriveRL%2520framework%2520and%2520DriveRX%2520to%2520support%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveRX%3A%20A%20Vision-Language%20Reasoning%20Model%20for%20Cross-Task%20Autonomous%20Driving&entry.906535625=Muxi%20Diao%20and%20Lele%20Yang%20and%20Hongbo%20Yin%20and%20Zhexu%20Wang%20and%20Yejie%20Wang%20and%20Daxin%20Tian%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma&entry.1292438233=Effective%20autonomous%20driving%20hinges%20on%20robust%20reasoning%20across%20perception%2C%20prediction%2C%20planning%2C%20and%20behavior.%20However%2C%20conventional%20end-to-end%20models%20fail%20to%20generalize%20in%20complex%20scenarios%20due%20to%20the%20lack%20of%20structured%20reasoning.%20While%20recent%20vision-language%20models%20%28VLMs%29%20have%20been%20applied%20to%20driving%20tasks%2C%20they%20typically%20rely%20on%20isolated%20modules%20and%20static%20supervision%2C%20limiting%20their%20ability%20to%20support%20multi-stage%20decision-making.%20We%20present%20AutoDriveRL%2C%20a%20unified%20training%20framework%20that%20formulates%20autonomous%20driving%20as%20a%20structured%20reasoning%20process%20over%20four%20core%20tasks.%20Each%20task%20is%20independently%20modeled%20as%20a%20vision-language%20QA%20problem%20and%20optimized%20using%20task-specific%20reward%20models%2C%20enabling%20fine-grained%20reinforcement%20signals%20at%20different%20reasoning%20stages.%20Within%20this%20framework%2C%20we%20train%20DriveRX%2C%20a%20cross-task%20reasoning%20VLM%20designed%20for%20multi-stage%20decision-making.%20DriveRX%20achieves%20strong%20performance%20on%20the%20public%20benchmark%2C%20outperforming%20GPT-4o%20in%20behavior%20reasoning%20and%20demonstrating%20robustness%20under%20complex%20or%20corrupted%20driving%20conditions.%20DriveRX%20serves%20as%20a%20high-level%20semantic%20reasoning%20backbone%2C%20producing%20structured%20stage-wise%20reasoning%20chains%20that%20enhance%20decision%20consistency.%20These%20outputs%20also%20provide%20high-quality%20supervisory%20signals%20for%20annotation%20and%20downstream%20planning/control%20models.%20We%20release%20the%20AutoDriveRL%20framework%20and%20DriveRX%20to%20support%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2505.20665v2&entry.124074799=Read"},
{"title": "Creativity in AI as Emergence from Domain-Limited Generative Models", "author": "Corina Chutaux", "abstract": "Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.", "link": "http://arxiv.org/abs/2601.08388v1", "date": "2026-01-13", "relevancy": 2.2218, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5825}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5579}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creativity%20in%20AI%20as%20Emergence%20from%20Domain-Limited%20Generative%20Models&body=Title%3A%20Creativity%20in%20AI%20as%20Emergence%20from%20Domain-Limited%20Generative%20Models%0AAuthor%3A%20Corina%20Chutaux%0AAbstract%3A%20Creativity%20in%20artificial%20intelligence%20is%20most%20often%20addressed%20through%20evaluative%20frameworks%20that%20aim%20to%20measure%20novelty%2C%20diversity%2C%20or%20usefulness%20in%20generated%20outputs.%20While%20such%20approaches%20have%20provided%20valuable%20insights%20into%20the%20behavior%20of%20modern%20generative%20models%2C%20they%20largely%20treat%20creativity%20as%20a%20property%20to%20be%20assessed%20rather%20than%20as%20a%20phenomenon%20to%20be%20explicitly%20modeled.%20In%20parallel%2C%20recent%20advances%20in%20large-scale%20generative%20systems%2C%20particularly%20multimodal%20architectures%2C%20have%20demonstrated%20increasingly%20sophisticated%20forms%20of%20pattern%20recombination%2C%20raising%20questions%20about%20the%20nature%20and%20limits%20of%20machine%20creativity.%20This%20paper%20proposes%20a%20generative%20perspective%20on%20creativity%20in%20AI%2C%20framing%20it%20as%20an%20emergent%20property%20of%20domain-limited%20generative%20models%20embedded%20within%20bounded%20informational%20environments.%20Rather%20than%20introducing%20new%20evaluative%20criteria%2C%20we%20focus%20on%20the%20structural%20and%20contextual%20conditions%20under%20which%20creative%20behaviors%20arise.%20We%20introduce%20a%20conceptual%20decomposition%20of%20creativity%20into%20four%20interacting%20components-pattern-based%20generation%2C%20induced%20world%20models%2C%20contextual%20grounding%2C%20and%20arbitrarity%2C%20and%20examine%20how%20these%20components%20manifest%20in%20multimodal%20generative%20systems.%20By%20grounding%20creativity%20in%20the%20interaction%20between%20generative%20dynamics%20and%20domain-specific%20representations%2C%20this%20work%20aims%20to%20provide%20a%20technical%20framework%20for%20studying%20creativity%20as%20an%20emergent%20phenomenon%20in%20AI%20systems%2C%20rather%20than%20as%20a%20post%20hoc%20evaluative%20label.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreativity%2520in%2520AI%2520as%2520Emergence%2520from%2520Domain-Limited%2520Generative%2520Models%26entry.906535625%3DCorina%2520Chutaux%26entry.1292438233%3DCreativity%2520in%2520artificial%2520intelligence%2520is%2520most%2520often%2520addressed%2520through%2520evaluative%2520frameworks%2520that%2520aim%2520to%2520measure%2520novelty%252C%2520diversity%252C%2520or%2520usefulness%2520in%2520generated%2520outputs.%2520While%2520such%2520approaches%2520have%2520provided%2520valuable%2520insights%2520into%2520the%2520behavior%2520of%2520modern%2520generative%2520models%252C%2520they%2520largely%2520treat%2520creativity%2520as%2520a%2520property%2520to%2520be%2520assessed%2520rather%2520than%2520as%2520a%2520phenomenon%2520to%2520be%2520explicitly%2520modeled.%2520In%2520parallel%252C%2520recent%2520advances%2520in%2520large-scale%2520generative%2520systems%252C%2520particularly%2520multimodal%2520architectures%252C%2520have%2520demonstrated%2520increasingly%2520sophisticated%2520forms%2520of%2520pattern%2520recombination%252C%2520raising%2520questions%2520about%2520the%2520nature%2520and%2520limits%2520of%2520machine%2520creativity.%2520This%2520paper%2520proposes%2520a%2520generative%2520perspective%2520on%2520creativity%2520in%2520AI%252C%2520framing%2520it%2520as%2520an%2520emergent%2520property%2520of%2520domain-limited%2520generative%2520models%2520embedded%2520within%2520bounded%2520informational%2520environments.%2520Rather%2520than%2520introducing%2520new%2520evaluative%2520criteria%252C%2520we%2520focus%2520on%2520the%2520structural%2520and%2520contextual%2520conditions%2520under%2520which%2520creative%2520behaviors%2520arise.%2520We%2520introduce%2520a%2520conceptual%2520decomposition%2520of%2520creativity%2520into%2520four%2520interacting%2520components-pattern-based%2520generation%252C%2520induced%2520world%2520models%252C%2520contextual%2520grounding%252C%2520and%2520arbitrarity%252C%2520and%2520examine%2520how%2520these%2520components%2520manifest%2520in%2520multimodal%2520generative%2520systems.%2520By%2520grounding%2520creativity%2520in%2520the%2520interaction%2520between%2520generative%2520dynamics%2520and%2520domain-specific%2520representations%252C%2520this%2520work%2520aims%2520to%2520provide%2520a%2520technical%2520framework%2520for%2520studying%2520creativity%2520as%2520an%2520emergent%2520phenomenon%2520in%2520AI%2520systems%252C%2520rather%2520than%2520as%2520a%2520post%2520hoc%2520evaluative%2520label.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creativity%20in%20AI%20as%20Emergence%20from%20Domain-Limited%20Generative%20Models&entry.906535625=Corina%20Chutaux&entry.1292438233=Creativity%20in%20artificial%20intelligence%20is%20most%20often%20addressed%20through%20evaluative%20frameworks%20that%20aim%20to%20measure%20novelty%2C%20diversity%2C%20or%20usefulness%20in%20generated%20outputs.%20While%20such%20approaches%20have%20provided%20valuable%20insights%20into%20the%20behavior%20of%20modern%20generative%20models%2C%20they%20largely%20treat%20creativity%20as%20a%20property%20to%20be%20assessed%20rather%20than%20as%20a%20phenomenon%20to%20be%20explicitly%20modeled.%20In%20parallel%2C%20recent%20advances%20in%20large-scale%20generative%20systems%2C%20particularly%20multimodal%20architectures%2C%20have%20demonstrated%20increasingly%20sophisticated%20forms%20of%20pattern%20recombination%2C%20raising%20questions%20about%20the%20nature%20and%20limits%20of%20machine%20creativity.%20This%20paper%20proposes%20a%20generative%20perspective%20on%20creativity%20in%20AI%2C%20framing%20it%20as%20an%20emergent%20property%20of%20domain-limited%20generative%20models%20embedded%20within%20bounded%20informational%20environments.%20Rather%20than%20introducing%20new%20evaluative%20criteria%2C%20we%20focus%20on%20the%20structural%20and%20contextual%20conditions%20under%20which%20creative%20behaviors%20arise.%20We%20introduce%20a%20conceptual%20decomposition%20of%20creativity%20into%20four%20interacting%20components-pattern-based%20generation%2C%20induced%20world%20models%2C%20contextual%20grounding%2C%20and%20arbitrarity%2C%20and%20examine%20how%20these%20components%20manifest%20in%20multimodal%20generative%20systems.%20By%20grounding%20creativity%20in%20the%20interaction%20between%20generative%20dynamics%20and%20domain-specific%20representations%2C%20this%20work%20aims%20to%20provide%20a%20technical%20framework%20for%20studying%20creativity%20as%20an%20emergent%20phenomenon%20in%20AI%20systems%2C%20rather%20than%20as%20a%20post%20hoc%20evaluative%20label.&entry.1838667208=http%3A//arxiv.org/abs/2601.08388v1&entry.124074799=Read"},
{"title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "author": "Hsiang-Wei Huang and Junbin Lu and Kuang-Ming Chen and Jenq-Neng Hwang", "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "link": "http://arxiv.org/abs/2601.08829v1", "date": "2026-01-13", "relevancy": 2.1995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20LLM%20Agent%20Reviewer%20Dynamics%20in%20Elo-Ranked%20Review%20System&body=Title%3A%20Modeling%20LLM%20Agent%20Reviewer%20Dynamics%20in%20Elo-Ranked%20Review%20System%0AAuthor%3A%20Hsiang-Wei%20Huang%20and%20Junbin%20Lu%20and%20Kuang-Ming%20Chen%20and%20Jenq-Neng%20Hwang%0AAbstract%3A%20In%20this%20work%2C%20we%20explore%20the%20Large%20Language%20Model%20%28LLM%29%20agent%20reviewer%20dynamics%20in%20an%20Elo-ranked%20review%20system%20using%20real-world%20conference%20paper%20submissions.%20Multiple%20LLM%20agent%20reviewers%20with%20different%20personas%20are%20engage%20in%20multi%20round%20review%20interactions%20moderated%20by%20an%20Area%20Chair.%20We%20compare%20a%20baseline%20setting%20with%20conditions%20that%20incorporate%20Elo%20ratings%20and%20reviewer%20memory.%20Our%20simulation%20results%20showcase%20several%20interesting%20findings%2C%20including%20how%20incorporating%20Elo%20improves%20Area%20Chair%20decision%20accuracy%2C%20as%20well%20as%20reviewers%27%20adaptive%20review%20strategy%20that%20exploits%20our%20Elo%20system%20without%20improving%20review%20effort.%20Our%20code%20is%20available%20at%20https%3A//github.com/hsiangwei0903/EloReview.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520LLM%2520Agent%2520Reviewer%2520Dynamics%2520in%2520Elo-Ranked%2520Review%2520System%26entry.906535625%3DHsiang-Wei%2520Huang%2520and%2520Junbin%2520Lu%2520and%2520Kuang-Ming%2520Chen%2520and%2520Jenq-Neng%2520Hwang%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520explore%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agent%2520reviewer%2520dynamics%2520in%2520an%2520Elo-ranked%2520review%2520system%2520using%2520real-world%2520conference%2520paper%2520submissions.%2520Multiple%2520LLM%2520agent%2520reviewers%2520with%2520different%2520personas%2520are%2520engage%2520in%2520multi%2520round%2520review%2520interactions%2520moderated%2520by%2520an%2520Area%2520Chair.%2520We%2520compare%2520a%2520baseline%2520setting%2520with%2520conditions%2520that%2520incorporate%2520Elo%2520ratings%2520and%2520reviewer%2520memory.%2520Our%2520simulation%2520results%2520showcase%2520several%2520interesting%2520findings%252C%2520including%2520how%2520incorporating%2520Elo%2520improves%2520Area%2520Chair%2520decision%2520accuracy%252C%2520as%2520well%2520as%2520reviewers%2527%2520adaptive%2520review%2520strategy%2520that%2520exploits%2520our%2520Elo%2520system%2520without%2520improving%2520review%2520effort.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hsiangwei0903/EloReview.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20LLM%20Agent%20Reviewer%20Dynamics%20in%20Elo-Ranked%20Review%20System&entry.906535625=Hsiang-Wei%20Huang%20and%20Junbin%20Lu%20and%20Kuang-Ming%20Chen%20and%20Jenq-Neng%20Hwang&entry.1292438233=In%20this%20work%2C%20we%20explore%20the%20Large%20Language%20Model%20%28LLM%29%20agent%20reviewer%20dynamics%20in%20an%20Elo-ranked%20review%20system%20using%20real-world%20conference%20paper%20submissions.%20Multiple%20LLM%20agent%20reviewers%20with%20different%20personas%20are%20engage%20in%20multi%20round%20review%20interactions%20moderated%20by%20an%20Area%20Chair.%20We%20compare%20a%20baseline%20setting%20with%20conditions%20that%20incorporate%20Elo%20ratings%20and%20reviewer%20memory.%20Our%20simulation%20results%20showcase%20several%20interesting%20findings%2C%20including%20how%20incorporating%20Elo%20improves%20Area%20Chair%20decision%20accuracy%2C%20as%20well%20as%20reviewers%27%20adaptive%20review%20strategy%20that%20exploits%20our%20Elo%20system%20without%20improving%20review%20effort.%20Our%20code%20is%20available%20at%20https%3A//github.com/hsiangwei0903/EloReview.&entry.1838667208=http%3A//arxiv.org/abs/2601.08829v1&entry.124074799=Read"},
{"title": "PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges With a Geometry-Aware 3DETR", "author": "Fabio F. Oberweger and Michael Schwingshackl and Vanessa Staderini", "abstract": "We present PI3DETR, an end-to-end framework that directly predicts 3D parametric curve instances from raw point clouds, avoiding the intermediate representations and multi-stage processing common in prior work. Extending 3DETR, our model introduces a geometry-aware matching strategy and specialized loss functions that enable unified detection of differently parameterized curve types, including cubic B\u00e9zier curves, line segments, circles, and arcs, in a single forward pass. Optional post-processing steps further refine predictions without adding complexity. This streamlined design improves robustness to noise and varying sampling densities, addressing critical challenges in real world LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC dataset and generalizes effectively to real sensor data, offering a simple yet powerful solution for 3D edge and curve estimation.", "link": "http://arxiv.org/abs/2509.03262v2", "date": "2026-01-13", "relevancy": 2.1972, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5823}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20With%20a%20Geometry-Aware%203DETR&body=Title%3A%20PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20With%20a%20Geometry-Aware%203DETR%0AAuthor%3A%20Fabio%20F.%20Oberweger%20and%20Michael%20Schwingshackl%20and%20Vanessa%20Staderini%0AAbstract%3A%20We%20present%20PI3DETR%2C%20an%20end-to-end%20framework%20that%20directly%20predicts%203D%20parametric%20curve%20instances%20from%20raw%20point%20clouds%2C%20avoiding%20the%20intermediate%20representations%20and%20multi-stage%20processing%20common%20in%20prior%20work.%20Extending%203DETR%2C%20our%20model%20introduces%20a%20geometry-aware%20matching%20strategy%20and%20specialized%20loss%20functions%20that%20enable%20unified%20detection%20of%20differently%20parameterized%20curve%20types%2C%20including%20cubic%20B%C3%A9zier%20curves%2C%20line%20segments%2C%20circles%2C%20and%20arcs%2C%20in%20a%20single%20forward%20pass.%20Optional%20post-processing%20steps%20further%20refine%20predictions%20without%20adding%20complexity.%20This%20streamlined%20design%20improves%20robustness%20to%20noise%20and%20varying%20sampling%20densities%2C%20addressing%20critical%20challenges%20in%20real%20world%20LiDAR%20and%203D%20sensing%20scenarios.%20PI3DETR%20sets%20a%20new%20state-of-the-art%20on%20the%20ABC%20dataset%20and%20generalizes%20effectively%20to%20real%20sensor%20data%2C%20offering%20a%20simple%20yet%20powerful%20solution%20for%203D%20edge%20and%20curve%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.03262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPI3DETR%253A%2520Parametric%2520Instance%2520Detection%2520of%25203D%2520Point%2520Cloud%2520Edges%2520With%2520a%2520Geometry-Aware%25203DETR%26entry.906535625%3DFabio%2520F.%2520Oberweger%2520and%2520Michael%2520Schwingshackl%2520and%2520Vanessa%2520Staderini%26entry.1292438233%3DWe%2520present%2520PI3DETR%252C%2520an%2520end-to-end%2520framework%2520that%2520directly%2520predicts%25203D%2520parametric%2520curve%2520instances%2520from%2520raw%2520point%2520clouds%252C%2520avoiding%2520the%2520intermediate%2520representations%2520and%2520multi-stage%2520processing%2520common%2520in%2520prior%2520work.%2520Extending%25203DETR%252C%2520our%2520model%2520introduces%2520a%2520geometry-aware%2520matching%2520strategy%2520and%2520specialized%2520loss%2520functions%2520that%2520enable%2520unified%2520detection%2520of%2520differently%2520parameterized%2520curve%2520types%252C%2520including%2520cubic%2520B%25C3%25A9zier%2520curves%252C%2520line%2520segments%252C%2520circles%252C%2520and%2520arcs%252C%2520in%2520a%2520single%2520forward%2520pass.%2520Optional%2520post-processing%2520steps%2520further%2520refine%2520predictions%2520without%2520adding%2520complexity.%2520This%2520streamlined%2520design%2520improves%2520robustness%2520to%2520noise%2520and%2520varying%2520sampling%2520densities%252C%2520addressing%2520critical%2520challenges%2520in%2520real%2520world%2520LiDAR%2520and%25203D%2520sensing%2520scenarios.%2520PI3DETR%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520the%2520ABC%2520dataset%2520and%2520generalizes%2520effectively%2520to%2520real%2520sensor%2520data%252C%2520offering%2520a%2520simple%2520yet%2520powerful%2520solution%2520for%25203D%2520edge%2520and%2520curve%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20With%20a%20Geometry-Aware%203DETR&entry.906535625=Fabio%20F.%20Oberweger%20and%20Michael%20Schwingshackl%20and%20Vanessa%20Staderini&entry.1292438233=We%20present%20PI3DETR%2C%20an%20end-to-end%20framework%20that%20directly%20predicts%203D%20parametric%20curve%20instances%20from%20raw%20point%20clouds%2C%20avoiding%20the%20intermediate%20representations%20and%20multi-stage%20processing%20common%20in%20prior%20work.%20Extending%203DETR%2C%20our%20model%20introduces%20a%20geometry-aware%20matching%20strategy%20and%20specialized%20loss%20functions%20that%20enable%20unified%20detection%20of%20differently%20parameterized%20curve%20types%2C%20including%20cubic%20B%C3%A9zier%20curves%2C%20line%20segments%2C%20circles%2C%20and%20arcs%2C%20in%20a%20single%20forward%20pass.%20Optional%20post-processing%20steps%20further%20refine%20predictions%20without%20adding%20complexity.%20This%20streamlined%20design%20improves%20robustness%20to%20noise%20and%20varying%20sampling%20densities%2C%20addressing%20critical%20challenges%20in%20real%20world%20LiDAR%20and%203D%20sensing%20scenarios.%20PI3DETR%20sets%20a%20new%20state-of-the-art%20on%20the%20ABC%20dataset%20and%20generalizes%20effectively%20to%20real%20sensor%20data%2C%20offering%20a%20simple%20yet%20powerful%20solution%20for%203D%20edge%20and%20curve%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2509.03262v2&entry.124074799=Read"},
{"title": "EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models", "author": "S\u00f6ren Schleibaum and Anton Frederik Thielmann and Julian Teusch and Benjamin S\u00e4fken and J\u00f6rg P. M\u00fcller", "abstract": "Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.", "link": "http://arxiv.org/abs/2601.08556v1", "date": "2026-01-13", "relevancy": 2.1911, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5665}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EviNAM%3A%20Intelligibility%20and%20Uncertainty%20via%20Evidential%20Neural%20Additive%20Models&body=Title%3A%20EviNAM%3A%20Intelligibility%20and%20Uncertainty%20via%20Evidential%20Neural%20Additive%20Models%0AAuthor%3A%20S%C3%B6ren%20Schleibaum%20and%20Anton%20Frederik%20Thielmann%20and%20Julian%20Teusch%20and%20Benjamin%20S%C3%A4fken%20and%20J%C3%B6rg%20P.%20M%C3%BCller%0AAbstract%3A%20Intelligibility%20and%20accurate%20uncertainty%20estimation%20are%20crucial%20for%20reliable%20decision-making.%20In%20this%20paper%2C%20we%20propose%20EviNAM%2C%20an%20extension%20of%20evidential%20learning%20that%20integrates%20the%20interpretability%20of%20Neural%20Additive%20Models%20%28NAMs%29%20with%20principled%20uncertainty%20estimation.%20Unlike%20standard%20Bayesian%20neural%20networks%20and%20previous%20evidential%20methods%2C%20EviNAM%20enables%2C%20in%20a%20single%20pass%2C%20both%20the%20estimation%20of%20the%20aleatoric%20and%20epistemic%20uncertainty%20as%20well%20as%20explicit%20feature%20contributions.%20Experiments%20on%20synthetic%20and%20real%20data%20demonstrate%20that%20EviNAM%20matches%20state-of-the-art%20predictive%20performance.%20While%20we%20focus%20on%20regression%2C%20our%20method%20extends%20naturally%20to%20classification%20and%20generalized%20additive%20models%2C%20offering%20a%20path%20toward%20more%20intelligible%20and%20trustworthy%20predictions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEviNAM%253A%2520Intelligibility%2520and%2520Uncertainty%2520via%2520Evidential%2520Neural%2520Additive%2520Models%26entry.906535625%3DS%25C3%25B6ren%2520Schleibaum%2520and%2520Anton%2520Frederik%2520Thielmann%2520and%2520Julian%2520Teusch%2520and%2520Benjamin%2520S%25C3%25A4fken%2520and%2520J%25C3%25B6rg%2520P.%2520M%25C3%25BCller%26entry.1292438233%3DIntelligibility%2520and%2520accurate%2520uncertainty%2520estimation%2520are%2520crucial%2520for%2520reliable%2520decision-making.%2520In%2520this%2520paper%252C%2520we%2520propose%2520EviNAM%252C%2520an%2520extension%2520of%2520evidential%2520learning%2520that%2520integrates%2520the%2520interpretability%2520of%2520Neural%2520Additive%2520Models%2520%2528NAMs%2529%2520with%2520principled%2520uncertainty%2520estimation.%2520Unlike%2520standard%2520Bayesian%2520neural%2520networks%2520and%2520previous%2520evidential%2520methods%252C%2520EviNAM%2520enables%252C%2520in%2520a%2520single%2520pass%252C%2520both%2520the%2520estimation%2520of%2520the%2520aleatoric%2520and%2520epistemic%2520uncertainty%2520as%2520well%2520as%2520explicit%2520feature%2520contributions.%2520Experiments%2520on%2520synthetic%2520and%2520real%2520data%2520demonstrate%2520that%2520EviNAM%2520matches%2520state-of-the-art%2520predictive%2520performance.%2520While%2520we%2520focus%2520on%2520regression%252C%2520our%2520method%2520extends%2520naturally%2520to%2520classification%2520and%2520generalized%2520additive%2520models%252C%2520offering%2520a%2520path%2520toward%2520more%2520intelligible%2520and%2520trustworthy%2520predictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EviNAM%3A%20Intelligibility%20and%20Uncertainty%20via%20Evidential%20Neural%20Additive%20Models&entry.906535625=S%C3%B6ren%20Schleibaum%20and%20Anton%20Frederik%20Thielmann%20and%20Julian%20Teusch%20and%20Benjamin%20S%C3%A4fken%20and%20J%C3%B6rg%20P.%20M%C3%BCller&entry.1292438233=Intelligibility%20and%20accurate%20uncertainty%20estimation%20are%20crucial%20for%20reliable%20decision-making.%20In%20this%20paper%2C%20we%20propose%20EviNAM%2C%20an%20extension%20of%20evidential%20learning%20that%20integrates%20the%20interpretability%20of%20Neural%20Additive%20Models%20%28NAMs%29%20with%20principled%20uncertainty%20estimation.%20Unlike%20standard%20Bayesian%20neural%20networks%20and%20previous%20evidential%20methods%2C%20EviNAM%20enables%2C%20in%20a%20single%20pass%2C%20both%20the%20estimation%20of%20the%20aleatoric%20and%20epistemic%20uncertainty%20as%20well%20as%20explicit%20feature%20contributions.%20Experiments%20on%20synthetic%20and%20real%20data%20demonstrate%20that%20EviNAM%20matches%20state-of-the-art%20predictive%20performance.%20While%20we%20focus%20on%20regression%2C%20our%20method%20extends%20naturally%20to%20classification%20and%20generalized%20additive%20models%2C%20offering%20a%20path%20toward%20more%20intelligible%20and%20trustworthy%20predictions.&entry.1838667208=http%3A//arxiv.org/abs/2601.08556v1&entry.124074799=Read"},
{"title": "A Mesh-Adaptive Hypergraph Neural Network for Unsteady Flow Around Oscillating and Rotating Structures", "author": "Rui Gao and Zhi Cheng and Rajeev K. Jaiman", "abstract": "Graph neural networks, recently introduced into the field of fluid flow surrogate modeling, have been successfully applied to model the temporal evolution of various fluid flow systems. Existing applications, however, are mostly restricted to cases where the domain is time-invariant. The present work extends the application of graph neural network-based modeling to fluid flow around structures rotating with respect to a certain axis. Specifically, we propose to apply a graph neural network-based surrogate model with part of the mesh/graph co-rotating with the structure and part of the mesh/graph static. A single layer of interface cells are constructed at the interface between the two parts and are allowed to distort and adapt, which helps in circumventing the difficulty of interpolating information encoded by the neural network at every graph neural network layer. Dedicated reconstruction and re-projection schemes are designed to counter the error caused by the distortion and connectivity change of the interface cells. The effectiveness of our proposed framework is examined on two test cases: (i) fluid flow around a 2D oscillating airfoil, and (ii) fluid flow past a 3D rotating cube. Our results show that the model achieves stable rollout predictions over hundreds or even a thousand time steps. We further demonstrate that one could enforce accurate, error-bounded prediction results by incorporating the measurements from sparse pressure sensors. In addition to the accurate flow field predictions, the lift and drag force predictions closely match with the computational fluid dynamics calculations, highlighting the potential of the framework for modeling fluid flow around rotating structures, and paving the path towards a graph neural network-based surrogate model for more complex scenarios like flow around marine propellers.", "link": "http://arxiv.org/abs/2503.22252v2", "date": "2026-01-13", "relevancy": 2.1887, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5872}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5729}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mesh-Adaptive%20Hypergraph%20Neural%20Network%20for%20Unsteady%20Flow%20Around%20Oscillating%20and%20Rotating%20Structures&body=Title%3A%20A%20Mesh-Adaptive%20Hypergraph%20Neural%20Network%20for%20Unsteady%20Flow%20Around%20Oscillating%20and%20Rotating%20Structures%0AAuthor%3A%20Rui%20Gao%20and%20Zhi%20Cheng%20and%20Rajeev%20K.%20Jaiman%0AAbstract%3A%20Graph%20neural%20networks%2C%20recently%20introduced%20into%20the%20field%20of%20fluid%20flow%20surrogate%20modeling%2C%20have%20been%20successfully%20applied%20to%20model%20the%20temporal%20evolution%20of%20various%20fluid%20flow%20systems.%20Existing%20applications%2C%20however%2C%20are%20mostly%20restricted%20to%20cases%20where%20the%20domain%20is%20time-invariant.%20The%20present%20work%20extends%20the%20application%20of%20graph%20neural%20network-based%20modeling%20to%20fluid%20flow%20around%20structures%20rotating%20with%20respect%20to%20a%20certain%20axis.%20Specifically%2C%20we%20propose%20to%20apply%20a%20graph%20neural%20network-based%20surrogate%20model%20with%20part%20of%20the%20mesh/graph%20co-rotating%20with%20the%20structure%20and%20part%20of%20the%20mesh/graph%20static.%20A%20single%20layer%20of%20interface%20cells%20are%20constructed%20at%20the%20interface%20between%20the%20two%20parts%20and%20are%20allowed%20to%20distort%20and%20adapt%2C%20which%20helps%20in%20circumventing%20the%20difficulty%20of%20interpolating%20information%20encoded%20by%20the%20neural%20network%20at%20every%20graph%20neural%20network%20layer.%20Dedicated%20reconstruction%20and%20re-projection%20schemes%20are%20designed%20to%20counter%20the%20error%20caused%20by%20the%20distortion%20and%20connectivity%20change%20of%20the%20interface%20cells.%20The%20effectiveness%20of%20our%20proposed%20framework%20is%20examined%20on%20two%20test%20cases%3A%20%28i%29%20fluid%20flow%20around%20a%202D%20oscillating%20airfoil%2C%20and%20%28ii%29%20fluid%20flow%20past%20a%203D%20rotating%20cube.%20Our%20results%20show%20that%20the%20model%20achieves%20stable%20rollout%20predictions%20over%20hundreds%20or%20even%20a%20thousand%20time%20steps.%20We%20further%20demonstrate%20that%20one%20could%20enforce%20accurate%2C%20error-bounded%20prediction%20results%20by%20incorporating%20the%20measurements%20from%20sparse%20pressure%20sensors.%20In%20addition%20to%20the%20accurate%20flow%20field%20predictions%2C%20the%20lift%20and%20drag%20force%20predictions%20closely%20match%20with%20the%20computational%20fluid%20dynamics%20calculations%2C%20highlighting%20the%20potential%20of%20the%20framework%20for%20modeling%20fluid%20flow%20around%20rotating%20structures%2C%20and%20paving%20the%20path%20towards%20a%20graph%20neural%20network-based%20surrogate%20model%20for%20more%20complex%20scenarios%20like%20flow%20around%20marine%20propellers.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mesh-Adaptive%2520Hypergraph%2520Neural%2520Network%2520for%2520Unsteady%2520Flow%2520Around%2520Oscillating%2520and%2520Rotating%2520Structures%26entry.906535625%3DRui%2520Gao%2520and%2520Zhi%2520Cheng%2520and%2520Rajeev%2520K.%2520Jaiman%26entry.1292438233%3DGraph%2520neural%2520networks%252C%2520recently%2520introduced%2520into%2520the%2520field%2520of%2520fluid%2520flow%2520surrogate%2520modeling%252C%2520have%2520been%2520successfully%2520applied%2520to%2520model%2520the%2520temporal%2520evolution%2520of%2520various%2520fluid%2520flow%2520systems.%2520Existing%2520applications%252C%2520however%252C%2520are%2520mostly%2520restricted%2520to%2520cases%2520where%2520the%2520domain%2520is%2520time-invariant.%2520The%2520present%2520work%2520extends%2520the%2520application%2520of%2520graph%2520neural%2520network-based%2520modeling%2520to%2520fluid%2520flow%2520around%2520structures%2520rotating%2520with%2520respect%2520to%2520a%2520certain%2520axis.%2520Specifically%252C%2520we%2520propose%2520to%2520apply%2520a%2520graph%2520neural%2520network-based%2520surrogate%2520model%2520with%2520part%2520of%2520the%2520mesh/graph%2520co-rotating%2520with%2520the%2520structure%2520and%2520part%2520of%2520the%2520mesh/graph%2520static.%2520A%2520single%2520layer%2520of%2520interface%2520cells%2520are%2520constructed%2520at%2520the%2520interface%2520between%2520the%2520two%2520parts%2520and%2520are%2520allowed%2520to%2520distort%2520and%2520adapt%252C%2520which%2520helps%2520in%2520circumventing%2520the%2520difficulty%2520of%2520interpolating%2520information%2520encoded%2520by%2520the%2520neural%2520network%2520at%2520every%2520graph%2520neural%2520network%2520layer.%2520Dedicated%2520reconstruction%2520and%2520re-projection%2520schemes%2520are%2520designed%2520to%2520counter%2520the%2520error%2520caused%2520by%2520the%2520distortion%2520and%2520connectivity%2520change%2520of%2520the%2520interface%2520cells.%2520The%2520effectiveness%2520of%2520our%2520proposed%2520framework%2520is%2520examined%2520on%2520two%2520test%2520cases%253A%2520%2528i%2529%2520fluid%2520flow%2520around%2520a%25202D%2520oscillating%2520airfoil%252C%2520and%2520%2528ii%2529%2520fluid%2520flow%2520past%2520a%25203D%2520rotating%2520cube.%2520Our%2520results%2520show%2520that%2520the%2520model%2520achieves%2520stable%2520rollout%2520predictions%2520over%2520hundreds%2520or%2520even%2520a%2520thousand%2520time%2520steps.%2520We%2520further%2520demonstrate%2520that%2520one%2520could%2520enforce%2520accurate%252C%2520error-bounded%2520prediction%2520results%2520by%2520incorporating%2520the%2520measurements%2520from%2520sparse%2520pressure%2520sensors.%2520In%2520addition%2520to%2520the%2520accurate%2520flow%2520field%2520predictions%252C%2520the%2520lift%2520and%2520drag%2520force%2520predictions%2520closely%2520match%2520with%2520the%2520computational%2520fluid%2520dynamics%2520calculations%252C%2520highlighting%2520the%2520potential%2520of%2520the%2520framework%2520for%2520modeling%2520fluid%2520flow%2520around%2520rotating%2520structures%252C%2520and%2520paving%2520the%2520path%2520towards%2520a%2520graph%2520neural%2520network-based%2520surrogate%2520model%2520for%2520more%2520complex%2520scenarios%2520like%2520flow%2520around%2520marine%2520propellers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mesh-Adaptive%20Hypergraph%20Neural%20Network%20for%20Unsteady%20Flow%20Around%20Oscillating%20and%20Rotating%20Structures&entry.906535625=Rui%20Gao%20and%20Zhi%20Cheng%20and%20Rajeev%20K.%20Jaiman&entry.1292438233=Graph%20neural%20networks%2C%20recently%20introduced%20into%20the%20field%20of%20fluid%20flow%20surrogate%20modeling%2C%20have%20been%20successfully%20applied%20to%20model%20the%20temporal%20evolution%20of%20various%20fluid%20flow%20systems.%20Existing%20applications%2C%20however%2C%20are%20mostly%20restricted%20to%20cases%20where%20the%20domain%20is%20time-invariant.%20The%20present%20work%20extends%20the%20application%20of%20graph%20neural%20network-based%20modeling%20to%20fluid%20flow%20around%20structures%20rotating%20with%20respect%20to%20a%20certain%20axis.%20Specifically%2C%20we%20propose%20to%20apply%20a%20graph%20neural%20network-based%20surrogate%20model%20with%20part%20of%20the%20mesh/graph%20co-rotating%20with%20the%20structure%20and%20part%20of%20the%20mesh/graph%20static.%20A%20single%20layer%20of%20interface%20cells%20are%20constructed%20at%20the%20interface%20between%20the%20two%20parts%20and%20are%20allowed%20to%20distort%20and%20adapt%2C%20which%20helps%20in%20circumventing%20the%20difficulty%20of%20interpolating%20information%20encoded%20by%20the%20neural%20network%20at%20every%20graph%20neural%20network%20layer.%20Dedicated%20reconstruction%20and%20re-projection%20schemes%20are%20designed%20to%20counter%20the%20error%20caused%20by%20the%20distortion%20and%20connectivity%20change%20of%20the%20interface%20cells.%20The%20effectiveness%20of%20our%20proposed%20framework%20is%20examined%20on%20two%20test%20cases%3A%20%28i%29%20fluid%20flow%20around%20a%202D%20oscillating%20airfoil%2C%20and%20%28ii%29%20fluid%20flow%20past%20a%203D%20rotating%20cube.%20Our%20results%20show%20that%20the%20model%20achieves%20stable%20rollout%20predictions%20over%20hundreds%20or%20even%20a%20thousand%20time%20steps.%20We%20further%20demonstrate%20that%20one%20could%20enforce%20accurate%2C%20error-bounded%20prediction%20results%20by%20incorporating%20the%20measurements%20from%20sparse%20pressure%20sensors.%20In%20addition%20to%20the%20accurate%20flow%20field%20predictions%2C%20the%20lift%20and%20drag%20force%20predictions%20closely%20match%20with%20the%20computational%20fluid%20dynamics%20calculations%2C%20highlighting%20the%20potential%20of%20the%20framework%20for%20modeling%20fluid%20flow%20around%20rotating%20structures%2C%20and%20paving%20the%20path%20towards%20a%20graph%20neural%20network-based%20surrogate%20model%20for%20more%20complex%20scenarios%20like%20flow%20around%20marine%20propellers.&entry.1838667208=http%3A//arxiv.org/abs/2503.22252v2&entry.124074799=Read"},
{"title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking", "author": "Haonan Tang and Yanjun Chen and Lezhi Jiang and Qianfei Li and Xinyu Guo", "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.", "link": "http://arxiv.org/abs/2512.02789v4", "date": "2026-01-13", "relevancy": 2.1836, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5383}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&body=Title%3A%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking%0AAuthor%3A%20Haonan%20Tang%20and%20Yanjun%20Chen%20and%20Lezhi%20Jiang%20and%20Qianfei%20Li%20and%20Xinyu%20Guo%0AAbstract%3A%20The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02789v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackNetV5%253A%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520and%2520Motion%2520Direction%2520Decoupling%2520for%2520Fast%2520Object%2520Tracking%26entry.906535625%3DHaonan%2520Tang%2520and%2520Yanjun%2520Chen%2520and%2520Lezhi%2520Jiang%2520and%2520Qianfei%2520Li%2520and%2520Xinyu%2520Guo%26entry.1292438233%3DThe%2520TrackNet%2520series%2520has%2520established%2520a%2520strong%2520baseline%2520for%2520fast-moving%2520small%2520object%2520tracking%2520in%2520sports.%2520However%252C%2520existing%2520iterations%2520face%2520significant%2520limitations%253A%2520V1-V3%2520struggle%2520with%2520occlusions%2520due%2520to%2520a%2520reliance%2520on%2520purely%2520visual%2520cues%252C%2520while%2520TrackNetV4%252C%2520despite%2520introducing%2520motion%2520inputs%252C%2520suffers%2520from%2520directional%2520ambiguity%2520as%2520its%2520absolute%2520difference%2520method%2520discards%2520motion%2520polarity.%2520To%2520overcome%2520these%2520bottlenecks%252C%2520we%2520propose%2520TrackNetV5%252C%2520a%2520robust%2520architecture%2520integrating%2520two%2520novel%2520mechanisms.%2520First%252C%2520to%2520recover%2520lost%2520directional%2520priors%252C%2520we%2520introduce%2520the%2520Motion%2520Direction%2520Decoupling%2520%2528MDD%2529%2520module.%2520Unlike%2520V4%252C%2520MDD%2520decomposes%2520temporal%2520dynamics%2520into%2520signed%2520polarity%2520fields%252C%2520explicitly%2520encoding%2520both%2520movement%2520occurrence%2520and%2520trajectory%2520direction.%2520Second%252C%2520we%2520propose%2520the%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520%2528R-STR%2529%2520head.%2520Operating%2520on%2520a%2520coarse-to-fine%2520paradigm%252C%2520this%2520Transformer-based%2520module%2520leverages%2520factorized%2520spatio-temporal%2520contexts%2520to%2520estimate%2520a%2520corrective%2520residual%252C%2520effectively%2520recovering%2520occluded%2520targets.%2520Extensive%2520experiments%2520on%2520the%2520TrackNetV2%2520dataset%2520demonstrate%2520that%2520TrackNetV5%2520achieves%2520a%2520new%2520state-of-the-art%2520F1-score%2520of%25200.9859%2520and%2520an%2520accuracy%2520of%25200.9733%252C%2520significantly%2520outperforming%2520previous%2520versions.%2520Notably%252C%2520this%2520performance%2520leap%2520is%2520achieved%2520with%2520a%2520marginal%25203.7%2525%2520increase%2520in%2520FLOPs%2520compared%2520to%2520V4%252C%2520maintaining%2520real-time%2520inference%2520capabilities%2520while%2520delivering%2520superior%2520tracking%2520precision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02789v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&entry.906535625=Haonan%20Tang%20and%20Yanjun%20Chen%20and%20Lezhi%20Jiang%20and%20Qianfei%20Li%20and%20Xinyu%20Guo&entry.1292438233=The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.&entry.1838667208=http%3A//arxiv.org/abs/2512.02789v4&entry.124074799=Read"},
{"title": "CoMa: Contextual Massing Generation with Vision-Language Models", "author": "Evgenii Maslov and Valentin Khrulkov and Anastasia Volkova and Anton Gusarov and Andrey Kuznetsov and Ivan Oseledets", "abstract": "The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.", "link": "http://arxiv.org/abs/2601.08464v1", "date": "2026-01-13", "relevancy": 2.1793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMa%3A%20Contextual%20Massing%20Generation%20with%20Vision-Language%20Models&body=Title%3A%20CoMa%3A%20Contextual%20Massing%20Generation%20with%20Vision-Language%20Models%0AAuthor%3A%20Evgenii%20Maslov%20and%20Valentin%20Khrulkov%20and%20Anastasia%20Volkova%20and%20Anton%20Gusarov%20and%20Andrey%20Kuznetsov%20and%20Ivan%20Oseledets%0AAbstract%3A%20The%20conceptual%20design%20phase%20in%20architecture%20and%20urban%20planning%2C%20particularly%20building%20massing%2C%20is%20complex%20and%20heavily%20reliant%20on%20designer%20intuition%20and%20manual%20effort.%20To%20address%20this%2C%20we%20propose%20an%20automated%20framework%20for%20generating%20building%20massing%20based%20on%20functional%20requirements%20and%20site%20context.%20A%20primary%20obstacle%20to%20such%20data-driven%20methods%20has%20been%20the%20lack%20of%20suitable%20datasets.%20Consequently%2C%20we%20introduce%20the%20CoMa-20K%20dataset%2C%20a%20comprehensive%20collection%20that%20includes%20detailed%20massing%20geometries%2C%20associated%20economical%20and%20programmatic%20data%2C%20and%20visual%20representations%20of%20the%20development%20site%20within%20its%20existing%20urban%20context.%20We%20benchmark%20this%20dataset%20by%20formulating%20massing%20generation%20as%20a%20conditional%20task%20for%20Vision-Language%20Models%20%28VLMs%29%2C%20evaluating%20both%20fine-tuned%20and%20large%20zero-shot%20models.%20Our%20experiments%20reveal%20the%20inherent%20complexity%20of%20the%20task%20while%20demonstrating%20the%20potential%20of%20VLMs%20to%20produce%20context-sensitive%20massing%20options.%20The%20dataset%20and%20analysis%20establish%20a%20foundational%20benchmark%20and%20highlight%20significant%20opportunities%20for%20future%20research%20in%20data-driven%20architectural%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMa%253A%2520Contextual%2520Massing%2520Generation%2520with%2520Vision-Language%2520Models%26entry.906535625%3DEvgenii%2520Maslov%2520and%2520Valentin%2520Khrulkov%2520and%2520Anastasia%2520Volkova%2520and%2520Anton%2520Gusarov%2520and%2520Andrey%2520Kuznetsov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3DThe%2520conceptual%2520design%2520phase%2520in%2520architecture%2520and%2520urban%2520planning%252C%2520particularly%2520building%2520massing%252C%2520is%2520complex%2520and%2520heavily%2520reliant%2520on%2520designer%2520intuition%2520and%2520manual%2520effort.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520automated%2520framework%2520for%2520generating%2520building%2520massing%2520based%2520on%2520functional%2520requirements%2520and%2520site%2520context.%2520A%2520primary%2520obstacle%2520to%2520such%2520data-driven%2520methods%2520has%2520been%2520the%2520lack%2520of%2520suitable%2520datasets.%2520Consequently%252C%2520we%2520introduce%2520the%2520CoMa-20K%2520dataset%252C%2520a%2520comprehensive%2520collection%2520that%2520includes%2520detailed%2520massing%2520geometries%252C%2520associated%2520economical%2520and%2520programmatic%2520data%252C%2520and%2520visual%2520representations%2520of%2520the%2520development%2520site%2520within%2520its%2520existing%2520urban%2520context.%2520We%2520benchmark%2520this%2520dataset%2520by%2520formulating%2520massing%2520generation%2520as%2520a%2520conditional%2520task%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520evaluating%2520both%2520fine-tuned%2520and%2520large%2520zero-shot%2520models.%2520Our%2520experiments%2520reveal%2520the%2520inherent%2520complexity%2520of%2520the%2520task%2520while%2520demonstrating%2520the%2520potential%2520of%2520VLMs%2520to%2520produce%2520context-sensitive%2520massing%2520options.%2520The%2520dataset%2520and%2520analysis%2520establish%2520a%2520foundational%2520benchmark%2520and%2520highlight%2520significant%2520opportunities%2520for%2520future%2520research%2520in%2520data-driven%2520architectural%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMa%3A%20Contextual%20Massing%20Generation%20with%20Vision-Language%20Models&entry.906535625=Evgenii%20Maslov%20and%20Valentin%20Khrulkov%20and%20Anastasia%20Volkova%20and%20Anton%20Gusarov%20and%20Andrey%20Kuznetsov%20and%20Ivan%20Oseledets&entry.1292438233=The%20conceptual%20design%20phase%20in%20architecture%20and%20urban%20planning%2C%20particularly%20building%20massing%2C%20is%20complex%20and%20heavily%20reliant%20on%20designer%20intuition%20and%20manual%20effort.%20To%20address%20this%2C%20we%20propose%20an%20automated%20framework%20for%20generating%20building%20massing%20based%20on%20functional%20requirements%20and%20site%20context.%20A%20primary%20obstacle%20to%20such%20data-driven%20methods%20has%20been%20the%20lack%20of%20suitable%20datasets.%20Consequently%2C%20we%20introduce%20the%20CoMa-20K%20dataset%2C%20a%20comprehensive%20collection%20that%20includes%20detailed%20massing%20geometries%2C%20associated%20economical%20and%20programmatic%20data%2C%20and%20visual%20representations%20of%20the%20development%20site%20within%20its%20existing%20urban%20context.%20We%20benchmark%20this%20dataset%20by%20formulating%20massing%20generation%20as%20a%20conditional%20task%20for%20Vision-Language%20Models%20%28VLMs%29%2C%20evaluating%20both%20fine-tuned%20and%20large%20zero-shot%20models.%20Our%20experiments%20reveal%20the%20inherent%20complexity%20of%20the%20task%20while%20demonstrating%20the%20potential%20of%20VLMs%20to%20produce%20context-sensitive%20massing%20options.%20The%20dataset%20and%20analysis%20establish%20a%20foundational%20benchmark%20and%20highlight%20significant%20opportunities%20for%20future%20research%20in%20data-driven%20architectural%20design.&entry.1838667208=http%3A//arxiv.org/abs/2601.08464v1&entry.124074799=Read"},
{"title": "Hallucination, reliability, and the role of generative AI in science", "author": "Charles Rathkopf", "abstract": "Generative AI increasingly supports scientific inference, from protein structure prediction to weather forecasting. Yet its distinctive failure mode, hallucination, raises epistemic alarm bells. I argue that this failure mode can be addressed by shifting from data-centric to phenomenon-centric assessment. Through case studies of AlphaFold and GenCast, I show how scientific workflows discipline generative models through theory-guided training and confidence-based error screening. These strategies convert hallucination from an unmanageable epistemic threat into bounded risk. When embedded in such workflows, generative models support reliable inference despite opacity, provided they operate in theoretically mature domains.", "link": "http://arxiv.org/abs/2504.08526v2", "date": "2026-01-13", "relevancy": 2.1614, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5626}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.528}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucination%2C%20reliability%2C%20and%20the%20role%20of%20generative%20AI%20in%20science&body=Title%3A%20Hallucination%2C%20reliability%2C%20and%20the%20role%20of%20generative%20AI%20in%20science%0AAuthor%3A%20Charles%20Rathkopf%0AAbstract%3A%20Generative%20AI%20increasingly%20supports%20scientific%20inference%2C%20from%20protein%20structure%20prediction%20to%20weather%20forecasting.%20Yet%20its%20distinctive%20failure%20mode%2C%20hallucination%2C%20raises%20epistemic%20alarm%20bells.%20I%20argue%20that%20this%20failure%20mode%20can%20be%20addressed%20by%20shifting%20from%20data-centric%20to%20phenomenon-centric%20assessment.%20Through%20case%20studies%20of%20AlphaFold%20and%20GenCast%2C%20I%20show%20how%20scientific%20workflows%20discipline%20generative%20models%20through%20theory-guided%20training%20and%20confidence-based%20error%20screening.%20These%20strategies%20convert%20hallucination%20from%20an%20unmanageable%20epistemic%20threat%20into%20bounded%20risk.%20When%20embedded%20in%20such%20workflows%2C%20generative%20models%20support%20reliable%20inference%20despite%20opacity%2C%20provided%20they%20operate%20in%20theoretically%20mature%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2504.08526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucination%252C%2520reliability%252C%2520and%2520the%2520role%2520of%2520generative%2520AI%2520in%2520science%26entry.906535625%3DCharles%2520Rathkopf%26entry.1292438233%3DGenerative%2520AI%2520increasingly%2520supports%2520scientific%2520inference%252C%2520from%2520protein%2520structure%2520prediction%2520to%2520weather%2520forecasting.%2520Yet%2520its%2520distinctive%2520failure%2520mode%252C%2520hallucination%252C%2520raises%2520epistemic%2520alarm%2520bells.%2520I%2520argue%2520that%2520this%2520failure%2520mode%2520can%2520be%2520addressed%2520by%2520shifting%2520from%2520data-centric%2520to%2520phenomenon-centric%2520assessment.%2520Through%2520case%2520studies%2520of%2520AlphaFold%2520and%2520GenCast%252C%2520I%2520show%2520how%2520scientific%2520workflows%2520discipline%2520generative%2520models%2520through%2520theory-guided%2520training%2520and%2520confidence-based%2520error%2520screening.%2520These%2520strategies%2520convert%2520hallucination%2520from%2520an%2520unmanageable%2520epistemic%2520threat%2520into%2520bounded%2520risk.%2520When%2520embedded%2520in%2520such%2520workflows%252C%2520generative%2520models%2520support%2520reliable%2520inference%2520despite%2520opacity%252C%2520provided%2520they%2520operate%2520in%2520theoretically%2520mature%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucination%2C%20reliability%2C%20and%20the%20role%20of%20generative%20AI%20in%20science&entry.906535625=Charles%20Rathkopf&entry.1292438233=Generative%20AI%20increasingly%20supports%20scientific%20inference%2C%20from%20protein%20structure%20prediction%20to%20weather%20forecasting.%20Yet%20its%20distinctive%20failure%20mode%2C%20hallucination%2C%20raises%20epistemic%20alarm%20bells.%20I%20argue%20that%20this%20failure%20mode%20can%20be%20addressed%20by%20shifting%20from%20data-centric%20to%20phenomenon-centric%20assessment.%20Through%20case%20studies%20of%20AlphaFold%20and%20GenCast%2C%20I%20show%20how%20scientific%20workflows%20discipline%20generative%20models%20through%20theory-guided%20training%20and%20confidence-based%20error%20screening.%20These%20strategies%20convert%20hallucination%20from%20an%20unmanageable%20epistemic%20threat%20into%20bounded%20risk.%20When%20embedded%20in%20such%20workflows%2C%20generative%20models%20support%20reliable%20inference%20despite%20opacity%2C%20provided%20they%20operate%20in%20theoretically%20mature%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2504.08526v2&entry.124074799=Read"},
{"title": "DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion", "author": "Chenxu Han and Sean Bin Yang and Jilin Hu", "abstract": "Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.", "link": "http://arxiv.org/abs/2601.08482v1", "date": "2026-01-13", "relevancy": 2.1564, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5539}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5466}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffMM%3A%20Efficient%20Method%20for%20Accurate%20Noisy%20and%20Sparse%20Trajectory%20Map%20Matching%20via%20One%20Step%20Diffusion&body=Title%3A%20DiffMM%3A%20Efficient%20Method%20for%20Accurate%20Noisy%20and%20Sparse%20Trajectory%20Map%20Matching%20via%20One%20Step%20Diffusion%0AAuthor%3A%20Chenxu%20Han%20and%20Sean%20Bin%20Yang%20and%20Jilin%20Hu%0AAbstract%3A%20Map%20matching%20for%20sparse%20trajectories%20is%20a%20fundamental%20problem%20for%20many%20trajectory-based%20applications%2C%20e.g.%2C%20traffic%20scheduling%20and%20traffic%20flow%20analysis.%20Existing%20methods%20for%20map%20matching%20are%20generally%20based%20on%20Hidden%20Markov%20Model%20%28HMM%29%20or%20encoder-decoder%20framework.%20However%2C%20these%20methods%20continue%20to%20face%20significant%20challenges%20when%20handling%20noisy%20or%20sparsely%20sampled%20GPS%20trajectories.%20To%20address%20these%20limitations%2C%20we%20propose%20DiffMM%2C%20an%20encoder-diffusion-based%20map%20matching%20framework%20that%20produces%20effective%20yet%20efficient%20matching%20results%20through%20a%20one-step%20diffusion%20process.%20We%20first%20introduce%20a%20road%20segment-aware%20trajectory%20encoder%20that%20jointly%20embeds%20the%20input%20trajectory%20and%20its%20surrounding%20candidate%20road%20segments%20into%20a%20shared%20latent%20space%20through%20an%20attention%20mechanism.%20Next%2C%20we%20propose%20a%20one%20step%20diffusion%20method%20to%20realize%20map%20matching%20through%20a%20shortcut%20model%20by%20leveraging%20the%20joint%20embedding%20of%20the%20trajectory%20and%20candidate%20road%20segments%20as%20conditioning%20context.%20We%20conduct%20extensive%20experiments%20on%20large-scale%20trajectory%20datasets%2C%20demonstrating%20that%20our%20approach%20consistently%20outperforms%20state-of-the-art%20map%20matching%20methods%20in%20terms%20of%20both%20accuracy%20and%20efficiency%2C%20particularly%20for%20sparse%20trajectories%20and%20complex%20road%20network%20topologies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffMM%253A%2520Efficient%2520Method%2520for%2520Accurate%2520Noisy%2520and%2520Sparse%2520Trajectory%2520Map%2520Matching%2520via%2520One%2520Step%2520Diffusion%26entry.906535625%3DChenxu%2520Han%2520and%2520Sean%2520Bin%2520Yang%2520and%2520Jilin%2520Hu%26entry.1292438233%3DMap%2520matching%2520for%2520sparse%2520trajectories%2520is%2520a%2520fundamental%2520problem%2520for%2520many%2520trajectory-based%2520applications%252C%2520e.g.%252C%2520traffic%2520scheduling%2520and%2520traffic%2520flow%2520analysis.%2520Existing%2520methods%2520for%2520map%2520matching%2520are%2520generally%2520based%2520on%2520Hidden%2520Markov%2520Model%2520%2528HMM%2529%2520or%2520encoder-decoder%2520framework.%2520However%252C%2520these%2520methods%2520continue%2520to%2520face%2520significant%2520challenges%2520when%2520handling%2520noisy%2520or%2520sparsely%2520sampled%2520GPS%2520trajectories.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520DiffMM%252C%2520an%2520encoder-diffusion-based%2520map%2520matching%2520framework%2520that%2520produces%2520effective%2520yet%2520efficient%2520matching%2520results%2520through%2520a%2520one-step%2520diffusion%2520process.%2520We%2520first%2520introduce%2520a%2520road%2520segment-aware%2520trajectory%2520encoder%2520that%2520jointly%2520embeds%2520the%2520input%2520trajectory%2520and%2520its%2520surrounding%2520candidate%2520road%2520segments%2520into%2520a%2520shared%2520latent%2520space%2520through%2520an%2520attention%2520mechanism.%2520Next%252C%2520we%2520propose%2520a%2520one%2520step%2520diffusion%2520method%2520to%2520realize%2520map%2520matching%2520through%2520a%2520shortcut%2520model%2520by%2520leveraging%2520the%2520joint%2520embedding%2520of%2520the%2520trajectory%2520and%2520candidate%2520road%2520segments%2520as%2520conditioning%2520context.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520large-scale%2520trajectory%2520datasets%252C%2520demonstrating%2520that%2520our%2520approach%2520consistently%2520outperforms%2520state-of-the-art%2520map%2520matching%2520methods%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520efficiency%252C%2520particularly%2520for%2520sparse%2520trajectories%2520and%2520complex%2520road%2520network%2520topologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffMM%3A%20Efficient%20Method%20for%20Accurate%20Noisy%20and%20Sparse%20Trajectory%20Map%20Matching%20via%20One%20Step%20Diffusion&entry.906535625=Chenxu%20Han%20and%20Sean%20Bin%20Yang%20and%20Jilin%20Hu&entry.1292438233=Map%20matching%20for%20sparse%20trajectories%20is%20a%20fundamental%20problem%20for%20many%20trajectory-based%20applications%2C%20e.g.%2C%20traffic%20scheduling%20and%20traffic%20flow%20analysis.%20Existing%20methods%20for%20map%20matching%20are%20generally%20based%20on%20Hidden%20Markov%20Model%20%28HMM%29%20or%20encoder-decoder%20framework.%20However%2C%20these%20methods%20continue%20to%20face%20significant%20challenges%20when%20handling%20noisy%20or%20sparsely%20sampled%20GPS%20trajectories.%20To%20address%20these%20limitations%2C%20we%20propose%20DiffMM%2C%20an%20encoder-diffusion-based%20map%20matching%20framework%20that%20produces%20effective%20yet%20efficient%20matching%20results%20through%20a%20one-step%20diffusion%20process.%20We%20first%20introduce%20a%20road%20segment-aware%20trajectory%20encoder%20that%20jointly%20embeds%20the%20input%20trajectory%20and%20its%20surrounding%20candidate%20road%20segments%20into%20a%20shared%20latent%20space%20through%20an%20attention%20mechanism.%20Next%2C%20we%20propose%20a%20one%20step%20diffusion%20method%20to%20realize%20map%20matching%20through%20a%20shortcut%20model%20by%20leveraging%20the%20joint%20embedding%20of%20the%20trajectory%20and%20candidate%20road%20segments%20as%20conditioning%20context.%20We%20conduct%20extensive%20experiments%20on%20large-scale%20trajectory%20datasets%2C%20demonstrating%20that%20our%20approach%20consistently%20outperforms%20state-of-the-art%20map%20matching%20methods%20in%20terms%20of%20both%20accuracy%20and%20efficiency%2C%20particularly%20for%20sparse%20trajectories%20and%20complex%20road%20network%20topologies.&entry.1838667208=http%3A//arxiv.org/abs/2601.08482v1&entry.124074799=Read"},
{"title": "A Vision for Multisensory Intelligence: Sensing, Science, and Synergy", "author": "Paul Pu Liang", "abstract": "Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.", "link": "http://arxiv.org/abs/2601.04563v3", "date": "2026-01-13", "relevancy": 2.1423, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Vision%20for%20Multisensory%20Intelligence%3A%20Sensing%2C%20Science%2C%20and%20Synergy&body=Title%3A%20A%20Vision%20for%20Multisensory%20Intelligence%3A%20Sensing%2C%20Science%2C%20and%20Synergy%0AAuthor%3A%20Paul%20Pu%20Liang%0AAbstract%3A%20Our%20experience%20of%20the%20world%20is%20multisensory%2C%20spanning%20a%20synthesis%20of%20language%2C%20sight%2C%20sound%2C%20touch%2C%20taste%2C%20and%20smell.%20Yet%2C%20artificial%20intelligence%20has%20primarily%20advanced%20in%20digital%20modalities%20like%20text%2C%20vision%2C%20and%20audio.%20This%20paper%20outlines%20a%20research%20vision%20for%20multisensory%20artificial%20intelligence%20over%20the%20next%20decade.%20This%20new%20set%20of%20technologies%20can%20change%20how%20humans%20and%20AI%20experience%20and%20interact%20with%20one%20another%2C%20by%20connecting%20AI%20to%20the%20human%20senses%20and%20a%20rich%20spectrum%20of%20signals%20from%20physiological%20and%20tactile%20cues%20on%20the%20body%2C%20to%20physical%20and%20social%20signals%20in%20homes%2C%20cities%2C%20and%20the%20environment.%20We%20outline%20how%20this%20field%20must%20advance%20through%20three%20interrelated%20themes%20of%20sensing%2C%20science%2C%20and%20synergy.%20Firstly%2C%20research%20in%20sensing%20should%20extend%20how%20AI%20captures%20the%20world%20in%20richer%20ways%20beyond%20the%20digital%20medium.%20Secondly%2C%20developing%20a%20principled%20science%20for%20quantifying%20multimodal%20heterogeneity%20and%20interactions%2C%20developing%20unified%20modeling%20architectures%20and%20representations%2C%20and%20understanding%20cross-modal%20transfer.%20Finally%2C%20we%20present%20new%20technical%20challenges%20to%20learn%20synergy%20between%20modalities%20and%20between%20humans%20and%20AI%2C%20covering%20multisensory%20integration%2C%20alignment%2C%20reasoning%2C%20generation%2C%20generalization%2C%20and%20experience.%20Accompanying%20this%20vision%20paper%20are%20a%20series%20of%20projects%2C%20resources%2C%20and%20demos%20of%20latest%20advances%20from%20the%20Multisensory%20Intelligence%20group%20at%20the%20MIT%20Media%20Lab%2C%20see%20https%3A//mit-mi.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Vision%2520for%2520Multisensory%2520Intelligence%253A%2520Sensing%252C%2520Science%252C%2520and%2520Synergy%26entry.906535625%3DPaul%2520Pu%2520Liang%26entry.1292438233%3DOur%2520experience%2520of%2520the%2520world%2520is%2520multisensory%252C%2520spanning%2520a%2520synthesis%2520of%2520language%252C%2520sight%252C%2520sound%252C%2520touch%252C%2520taste%252C%2520and%2520smell.%2520Yet%252C%2520artificial%2520intelligence%2520has%2520primarily%2520advanced%2520in%2520digital%2520modalities%2520like%2520text%252C%2520vision%252C%2520and%2520audio.%2520This%2520paper%2520outlines%2520a%2520research%2520vision%2520for%2520multisensory%2520artificial%2520intelligence%2520over%2520the%2520next%2520decade.%2520This%2520new%2520set%2520of%2520technologies%2520can%2520change%2520how%2520humans%2520and%2520AI%2520experience%2520and%2520interact%2520with%2520one%2520another%252C%2520by%2520connecting%2520AI%2520to%2520the%2520human%2520senses%2520and%2520a%2520rich%2520spectrum%2520of%2520signals%2520from%2520physiological%2520and%2520tactile%2520cues%2520on%2520the%2520body%252C%2520to%2520physical%2520and%2520social%2520signals%2520in%2520homes%252C%2520cities%252C%2520and%2520the%2520environment.%2520We%2520outline%2520how%2520this%2520field%2520must%2520advance%2520through%2520three%2520interrelated%2520themes%2520of%2520sensing%252C%2520science%252C%2520and%2520synergy.%2520Firstly%252C%2520research%2520in%2520sensing%2520should%2520extend%2520how%2520AI%2520captures%2520the%2520world%2520in%2520richer%2520ways%2520beyond%2520the%2520digital%2520medium.%2520Secondly%252C%2520developing%2520a%2520principled%2520science%2520for%2520quantifying%2520multimodal%2520heterogeneity%2520and%2520interactions%252C%2520developing%2520unified%2520modeling%2520architectures%2520and%2520representations%252C%2520and%2520understanding%2520cross-modal%2520transfer.%2520Finally%252C%2520we%2520present%2520new%2520technical%2520challenges%2520to%2520learn%2520synergy%2520between%2520modalities%2520and%2520between%2520humans%2520and%2520AI%252C%2520covering%2520multisensory%2520integration%252C%2520alignment%252C%2520reasoning%252C%2520generation%252C%2520generalization%252C%2520and%2520experience.%2520Accompanying%2520this%2520vision%2520paper%2520are%2520a%2520series%2520of%2520projects%252C%2520resources%252C%2520and%2520demos%2520of%2520latest%2520advances%2520from%2520the%2520Multisensory%2520Intelligence%2520group%2520at%2520the%2520MIT%2520Media%2520Lab%252C%2520see%2520https%253A//mit-mi.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Vision%20for%20Multisensory%20Intelligence%3A%20Sensing%2C%20Science%2C%20and%20Synergy&entry.906535625=Paul%20Pu%20Liang&entry.1292438233=Our%20experience%20of%20the%20world%20is%20multisensory%2C%20spanning%20a%20synthesis%20of%20language%2C%20sight%2C%20sound%2C%20touch%2C%20taste%2C%20and%20smell.%20Yet%2C%20artificial%20intelligence%20has%20primarily%20advanced%20in%20digital%20modalities%20like%20text%2C%20vision%2C%20and%20audio.%20This%20paper%20outlines%20a%20research%20vision%20for%20multisensory%20artificial%20intelligence%20over%20the%20next%20decade.%20This%20new%20set%20of%20technologies%20can%20change%20how%20humans%20and%20AI%20experience%20and%20interact%20with%20one%20another%2C%20by%20connecting%20AI%20to%20the%20human%20senses%20and%20a%20rich%20spectrum%20of%20signals%20from%20physiological%20and%20tactile%20cues%20on%20the%20body%2C%20to%20physical%20and%20social%20signals%20in%20homes%2C%20cities%2C%20and%20the%20environment.%20We%20outline%20how%20this%20field%20must%20advance%20through%20three%20interrelated%20themes%20of%20sensing%2C%20science%2C%20and%20synergy.%20Firstly%2C%20research%20in%20sensing%20should%20extend%20how%20AI%20captures%20the%20world%20in%20richer%20ways%20beyond%20the%20digital%20medium.%20Secondly%2C%20developing%20a%20principled%20science%20for%20quantifying%20multimodal%20heterogeneity%20and%20interactions%2C%20developing%20unified%20modeling%20architectures%20and%20representations%2C%20and%20understanding%20cross-modal%20transfer.%20Finally%2C%20we%20present%20new%20technical%20challenges%20to%20learn%20synergy%20between%20modalities%20and%20between%20humans%20and%20AI%2C%20covering%20multisensory%20integration%2C%20alignment%2C%20reasoning%2C%20generation%2C%20generalization%2C%20and%20experience.%20Accompanying%20this%20vision%20paper%20are%20a%20series%20of%20projects%2C%20resources%2C%20and%20demos%20of%20latest%20advances%20from%20the%20Multisensory%20Intelligence%20group%20at%20the%20MIT%20Media%20Lab%2C%20see%20https%3A//mit-mi.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2601.04563v3&entry.124074799=Read"},
{"title": "Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning", "author": "Ankita Raj and Chetan Arora", "abstract": "Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting. Code: https://github.com/rajankita/TrAP", "link": "http://arxiv.org/abs/2511.12735v2", "date": "2026-01-13", "relevancy": 2.1348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5358}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Attacks%20on%20Open%20Vocabulary%20Object%20Detectors%20via%20Multi-Modal%20Prompt%20Tuning&body=Title%3A%20Backdoor%20Attacks%20on%20Open%20Vocabulary%20Object%20Detectors%20via%20Multi-Modal%20Prompt%20Tuning%0AAuthor%3A%20Ankita%20Raj%20and%20Chetan%20Arora%0AAbstract%3A%20Open-vocabulary%20object%20detectors%20%28OVODs%29%20unify%20vision%20and%20language%20to%20detect%20arbitrary%20object%20categories%20based%20on%20text%20prompts%2C%20enabling%20strong%20zero-shot%20generalization%20to%20novel%20concepts.%20As%20these%20models%20gain%20traction%20in%20high-stakes%20applications%20such%20as%20robotics%2C%20autonomous%20driving%2C%20and%20surveillance%2C%20understanding%20their%20security%20risks%20becomes%20crucial.%20In%20this%20work%2C%20we%20conduct%20the%20first%20study%20of%20backdoor%20attacks%20on%20OVODs%20and%20reveal%20a%20new%20attack%20surface%20introduced%20by%20prompt%20tuning.%20We%20propose%20TrAP%20%28Trigger-Aware%20Prompt%20tuning%29%2C%20a%20multi-modal%20backdoor%20injection%20strategy%20that%20jointly%20optimizes%20prompt%20parameters%20in%20both%20image%20and%20text%20modalities%20along%20with%20visual%20triggers.%20TrAP%20enables%20the%20attacker%20to%20implant%20malicious%20behavior%20using%20lightweight%2C%20learnable%20prompt%20tokens%20without%20retraining%20the%20base%20model%20weights%2C%20thus%20preserving%20generalization%20while%20embedding%20a%20hidden%20backdoor.%20We%20adopt%20a%20curriculum-based%20training%20strategy%20that%20progressively%20shrinks%20the%20trigger%20size%2C%20enabling%20effective%20backdoor%20activation%20using%20small%20trigger%20patches%20at%20inference.%20Experiments%20across%20multiple%20datasets%20show%20that%20TrAP%20achieves%20high%20attack%20success%20rates%20for%20both%20object%20misclassification%20and%20object%20disappearance%20attacks%2C%20while%20also%20improving%20clean%20image%20performance%20on%20downstream%20datasets%20compared%20to%20the%20zero-shot%20setting.%20Code%3A%20https%3A//github.com/rajankita/TrAP%0ALink%3A%20http%3A//arxiv.org/abs/2511.12735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Attacks%2520on%2520Open%2520Vocabulary%2520Object%2520Detectors%2520via%2520Multi-Modal%2520Prompt%2520Tuning%26entry.906535625%3DAnkita%2520Raj%2520and%2520Chetan%2520Arora%26entry.1292438233%3DOpen-vocabulary%2520object%2520detectors%2520%2528OVODs%2529%2520unify%2520vision%2520and%2520language%2520to%2520detect%2520arbitrary%2520object%2520categories%2520based%2520on%2520text%2520prompts%252C%2520enabling%2520strong%2520zero-shot%2520generalization%2520to%2520novel%2520concepts.%2520As%2520these%2520models%2520gain%2520traction%2520in%2520high-stakes%2520applications%2520such%2520as%2520robotics%252C%2520autonomous%2520driving%252C%2520and%2520surveillance%252C%2520understanding%2520their%2520security%2520risks%2520becomes%2520crucial.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%2520study%2520of%2520backdoor%2520attacks%2520on%2520OVODs%2520and%2520reveal%2520a%2520new%2520attack%2520surface%2520introduced%2520by%2520prompt%2520tuning.%2520We%2520propose%2520TrAP%2520%2528Trigger-Aware%2520Prompt%2520tuning%2529%252C%2520a%2520multi-modal%2520backdoor%2520injection%2520strategy%2520that%2520jointly%2520optimizes%2520prompt%2520parameters%2520in%2520both%2520image%2520and%2520text%2520modalities%2520along%2520with%2520visual%2520triggers.%2520TrAP%2520enables%2520the%2520attacker%2520to%2520implant%2520malicious%2520behavior%2520using%2520lightweight%252C%2520learnable%2520prompt%2520tokens%2520without%2520retraining%2520the%2520base%2520model%2520weights%252C%2520thus%2520preserving%2520generalization%2520while%2520embedding%2520a%2520hidden%2520backdoor.%2520We%2520adopt%2520a%2520curriculum-based%2520training%2520strategy%2520that%2520progressively%2520shrinks%2520the%2520trigger%2520size%252C%2520enabling%2520effective%2520backdoor%2520activation%2520using%2520small%2520trigger%2520patches%2520at%2520inference.%2520Experiments%2520across%2520multiple%2520datasets%2520show%2520that%2520TrAP%2520achieves%2520high%2520attack%2520success%2520rates%2520for%2520both%2520object%2520misclassification%2520and%2520object%2520disappearance%2520attacks%252C%2520while%2520also%2520improving%2520clean%2520image%2520performance%2520on%2520downstream%2520datasets%2520compared%2520to%2520the%2520zero-shot%2520setting.%2520Code%253A%2520https%253A//github.com/rajankita/TrAP%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Attacks%20on%20Open%20Vocabulary%20Object%20Detectors%20via%20Multi-Modal%20Prompt%20Tuning&entry.906535625=Ankita%20Raj%20and%20Chetan%20Arora&entry.1292438233=Open-vocabulary%20object%20detectors%20%28OVODs%29%20unify%20vision%20and%20language%20to%20detect%20arbitrary%20object%20categories%20based%20on%20text%20prompts%2C%20enabling%20strong%20zero-shot%20generalization%20to%20novel%20concepts.%20As%20these%20models%20gain%20traction%20in%20high-stakes%20applications%20such%20as%20robotics%2C%20autonomous%20driving%2C%20and%20surveillance%2C%20understanding%20their%20security%20risks%20becomes%20crucial.%20In%20this%20work%2C%20we%20conduct%20the%20first%20study%20of%20backdoor%20attacks%20on%20OVODs%20and%20reveal%20a%20new%20attack%20surface%20introduced%20by%20prompt%20tuning.%20We%20propose%20TrAP%20%28Trigger-Aware%20Prompt%20tuning%29%2C%20a%20multi-modal%20backdoor%20injection%20strategy%20that%20jointly%20optimizes%20prompt%20parameters%20in%20both%20image%20and%20text%20modalities%20along%20with%20visual%20triggers.%20TrAP%20enables%20the%20attacker%20to%20implant%20malicious%20behavior%20using%20lightweight%2C%20learnable%20prompt%20tokens%20without%20retraining%20the%20base%20model%20weights%2C%20thus%20preserving%20generalization%20while%20embedding%20a%20hidden%20backdoor.%20We%20adopt%20a%20curriculum-based%20training%20strategy%20that%20progressively%20shrinks%20the%20trigger%20size%2C%20enabling%20effective%20backdoor%20activation%20using%20small%20trigger%20patches%20at%20inference.%20Experiments%20across%20multiple%20datasets%20show%20that%20TrAP%20achieves%20high%20attack%20success%20rates%20for%20both%20object%20misclassification%20and%20object%20disappearance%20attacks%2C%20while%20also%20improving%20clean%20image%20performance%20on%20downstream%20datasets%20compared%20to%20the%20zero-shot%20setting.%20Code%3A%20https%3A//github.com/rajankita/TrAP&entry.1838667208=http%3A//arxiv.org/abs/2511.12735v2&entry.124074799=Read"},
{"title": "Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models", "author": "Hao Tang and Yu Liu and Shuanglin Yan and Fei Shen and Shengfeng He and Jing Qin", "abstract": "Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.", "link": "http://arxiv.org/abs/2601.08476v1", "date": "2026-01-13", "relevancy": 2.1333, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Proxy%20Evolving%20for%20OOD%20Detection%20with%20Vision-Language%20Models&body=Title%3A%20Cross-modal%20Proxy%20Evolving%20for%20OOD%20Detection%20with%20Vision-Language%20Models%0AAuthor%3A%20Hao%20Tang%20and%20Yu%20Liu%20and%20Shuanglin%20Yan%20and%20Fei%20Shen%20and%20Shengfeng%20He%20and%20Jing%20Qin%0AAbstract%3A%20Reliable%20zero-shot%20detection%20of%20out-of-distribution%20%28OOD%29%20inputs%20is%20critical%20for%20deploying%20vision-language%20models%20in%20open-world%20settings.%20However%2C%20the%20lack%20of%20labeled%20negatives%20in%20zero-shot%20OOD%20detection%20necessitates%20proxy%20signals%20that%20remain%20effective%20under%20distribution%20shift.%20Existing%20negative-label%20methods%20rely%20on%20a%20fixed%20set%20of%20textual%20proxies%2C%20which%20%28i%29%20sparsely%20sample%20the%20semantic%20space%20beyond%20in-distribution%20%28ID%29%20classes%20and%20%28ii%29%20remain%20static%20while%20only%20visual%20features%20drift%2C%20leading%20to%20cross-modal%20misalignment%20and%20unstable%20predictions.%20In%20this%20paper%2C%20we%20propose%20CoEvo%2C%20a%20training-%20and%20annotation-free%20test-time%20framework%20that%20performs%20bidirectional%2C%20sample-conditioned%20adaptation%20of%20both%20textual%20and%20visual%20proxies.%20Specifically%2C%20CoEvo%20introduces%20a%20proxy-aligned%20co-evolution%20mechanism%20to%20maintain%20two%20evolving%20proxy%20caches%2C%20which%20dynamically%20mines%20contextual%20textual%20negatives%20guided%20by%20test%20images%20and%20iteratively%20refines%20visual%20proxies%2C%20progressively%20realigning%20cross-modal%20similarities%20and%20enlarging%20local%20OOD%20margins.%20Finally%2C%20we%20dynamically%20re-weight%20the%20contributions%20of%20dual-modal%20proxies%20to%20obtain%20a%20calibrated%20OOD%20score%20that%20is%20robust%20to%20distribution%20shift.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20CoEvo%20achieves%20state-of-the-art%20performance%2C%20improving%20AUROC%20by%201.33%25%20and%20reducing%20FPR95%20by%2045.98%25%20on%20ImageNet-1K%20compared%20to%20strong%20negative-label%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520Proxy%2520Evolving%2520for%2520OOD%2520Detection%2520with%2520Vision-Language%2520Models%26entry.906535625%3DHao%2520Tang%2520and%2520Yu%2520Liu%2520and%2520Shuanglin%2520Yan%2520and%2520Fei%2520Shen%2520and%2520Shengfeng%2520He%2520and%2520Jing%2520Qin%26entry.1292438233%3DReliable%2520zero-shot%2520detection%2520of%2520out-of-distribution%2520%2528OOD%2529%2520inputs%2520is%2520critical%2520for%2520deploying%2520vision-language%2520models%2520in%2520open-world%2520settings.%2520However%252C%2520the%2520lack%2520of%2520labeled%2520negatives%2520in%2520zero-shot%2520OOD%2520detection%2520necessitates%2520proxy%2520signals%2520that%2520remain%2520effective%2520under%2520distribution%2520shift.%2520Existing%2520negative-label%2520methods%2520rely%2520on%2520a%2520fixed%2520set%2520of%2520textual%2520proxies%252C%2520which%2520%2528i%2529%2520sparsely%2520sample%2520the%2520semantic%2520space%2520beyond%2520in-distribution%2520%2528ID%2529%2520classes%2520and%2520%2528ii%2529%2520remain%2520static%2520while%2520only%2520visual%2520features%2520drift%252C%2520leading%2520to%2520cross-modal%2520misalignment%2520and%2520unstable%2520predictions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CoEvo%252C%2520a%2520training-%2520and%2520annotation-free%2520test-time%2520framework%2520that%2520performs%2520bidirectional%252C%2520sample-conditioned%2520adaptation%2520of%2520both%2520textual%2520and%2520visual%2520proxies.%2520Specifically%252C%2520CoEvo%2520introduces%2520a%2520proxy-aligned%2520co-evolution%2520mechanism%2520to%2520maintain%2520two%2520evolving%2520proxy%2520caches%252C%2520which%2520dynamically%2520mines%2520contextual%2520textual%2520negatives%2520guided%2520by%2520test%2520images%2520and%2520iteratively%2520refines%2520visual%2520proxies%252C%2520progressively%2520realigning%2520cross-modal%2520similarities%2520and%2520enlarging%2520local%2520OOD%2520margins.%2520Finally%252C%2520we%2520dynamically%2520re-weight%2520the%2520contributions%2520of%2520dual-modal%2520proxies%2520to%2520obtain%2520a%2520calibrated%2520OOD%2520score%2520that%2520is%2520robust%2520to%2520distribution%2520shift.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520CoEvo%2520achieves%2520state-of-the-art%2520performance%252C%2520improving%2520AUROC%2520by%25201.33%2525%2520and%2520reducing%2520FPR95%2520by%252045.98%2525%2520on%2520ImageNet-1K%2520compared%2520to%2520strong%2520negative-label%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Proxy%20Evolving%20for%20OOD%20Detection%20with%20Vision-Language%20Models&entry.906535625=Hao%20Tang%20and%20Yu%20Liu%20and%20Shuanglin%20Yan%20and%20Fei%20Shen%20and%20Shengfeng%20He%20and%20Jing%20Qin&entry.1292438233=Reliable%20zero-shot%20detection%20of%20out-of-distribution%20%28OOD%29%20inputs%20is%20critical%20for%20deploying%20vision-language%20models%20in%20open-world%20settings.%20However%2C%20the%20lack%20of%20labeled%20negatives%20in%20zero-shot%20OOD%20detection%20necessitates%20proxy%20signals%20that%20remain%20effective%20under%20distribution%20shift.%20Existing%20negative-label%20methods%20rely%20on%20a%20fixed%20set%20of%20textual%20proxies%2C%20which%20%28i%29%20sparsely%20sample%20the%20semantic%20space%20beyond%20in-distribution%20%28ID%29%20classes%20and%20%28ii%29%20remain%20static%20while%20only%20visual%20features%20drift%2C%20leading%20to%20cross-modal%20misalignment%20and%20unstable%20predictions.%20In%20this%20paper%2C%20we%20propose%20CoEvo%2C%20a%20training-%20and%20annotation-free%20test-time%20framework%20that%20performs%20bidirectional%2C%20sample-conditioned%20adaptation%20of%20both%20textual%20and%20visual%20proxies.%20Specifically%2C%20CoEvo%20introduces%20a%20proxy-aligned%20co-evolution%20mechanism%20to%20maintain%20two%20evolving%20proxy%20caches%2C%20which%20dynamically%20mines%20contextual%20textual%20negatives%20guided%20by%20test%20images%20and%20iteratively%20refines%20visual%20proxies%2C%20progressively%20realigning%20cross-modal%20similarities%20and%20enlarging%20local%20OOD%20margins.%20Finally%2C%20we%20dynamically%20re-weight%20the%20contributions%20of%20dual-modal%20proxies%20to%20obtain%20a%20calibrated%20OOD%20score%20that%20is%20robust%20to%20distribution%20shift.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20CoEvo%20achieves%20state-of-the-art%20performance%2C%20improving%20AUROC%20by%201.33%25%20and%20reducing%20FPR95%20by%2045.98%25%20on%20ImageNet-1K%20compared%20to%20strong%20negative-label%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.08476v1&entry.124074799=Read"},
{"title": "Decentralized Autoregressive Generation", "author": "Stepan Maschan and Haoxuan Qu and Jun Liu", "abstract": "We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.", "link": "http://arxiv.org/abs/2601.03184v2", "date": "2026-01-13", "relevancy": 2.1308, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5544}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5371}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Autoregressive%20Generation&body=Title%3A%20Decentralized%20Autoregressive%20Generation%0AAuthor%3A%20Stepan%20Maschan%20and%20Haoxuan%20Qu%20and%20Jun%20Liu%0AAbstract%3A%20We%20present%20a%20theoretical%20analysis%20of%20decentralization%20of%20autoregressive%20generation.%20We%20define%20the%20Decentralized%20Discrete%20Flow%20Matching%20objective%2C%20by%20expressing%20probability%20generating%20velocity%20as%20a%20linear%20combination%20of%20expert%20flows.%20We%20also%20conduct%20experiments%20demonstrating%20the%20equivalence%20between%20decentralized%20and%20centralized%20training%20settings%20for%20multimodal%20language%20models%20across%20diverse%20set%20of%20benchmarks.%20Specifically%2C%20we%20compare%20two%20distinct%20paradigms%3A%20LLaVA%20and%20InternVL%202.5-1B%2C%20which%20uses%20a%20fixed%20CLIP%20vision%20encoder%20and%20performs%20full-parameter%20fine-tuning%20%28ViT%2BMLP%2BLLM%29%20during%20the%20instruction%20tuning%20stage.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Autoregressive%2520Generation%26entry.906535625%3DStepan%2520Maschan%2520and%2520Haoxuan%2520Qu%2520and%2520Jun%2520Liu%26entry.1292438233%3DWe%2520present%2520a%2520theoretical%2520analysis%2520of%2520decentralization%2520of%2520autoregressive%2520generation.%2520We%2520define%2520the%2520Decentralized%2520Discrete%2520Flow%2520Matching%2520objective%252C%2520by%2520expressing%2520probability%2520generating%2520velocity%2520as%2520a%2520linear%2520combination%2520of%2520expert%2520flows.%2520We%2520also%2520conduct%2520experiments%2520demonstrating%2520the%2520equivalence%2520between%2520decentralized%2520and%2520centralized%2520training%2520settings%2520for%2520multimodal%2520language%2520models%2520across%2520diverse%2520set%2520of%2520benchmarks.%2520Specifically%252C%2520we%2520compare%2520two%2520distinct%2520paradigms%253A%2520LLaVA%2520and%2520InternVL%25202.5-1B%252C%2520which%2520uses%2520a%2520fixed%2520CLIP%2520vision%2520encoder%2520and%2520performs%2520full-parameter%2520fine-tuning%2520%2528ViT%252BMLP%252BLLM%2529%2520during%2520the%2520instruction%2520tuning%2520stage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Autoregressive%20Generation&entry.906535625=Stepan%20Maschan%20and%20Haoxuan%20Qu%20and%20Jun%20Liu&entry.1292438233=We%20present%20a%20theoretical%20analysis%20of%20decentralization%20of%20autoregressive%20generation.%20We%20define%20the%20Decentralized%20Discrete%20Flow%20Matching%20objective%2C%20by%20expressing%20probability%20generating%20velocity%20as%20a%20linear%20combination%20of%20expert%20flows.%20We%20also%20conduct%20experiments%20demonstrating%20the%20equivalence%20between%20decentralized%20and%20centralized%20training%20settings%20for%20multimodal%20language%20models%20across%20diverse%20set%20of%20benchmarks.%20Specifically%2C%20we%20compare%20two%20distinct%20paradigms%3A%20LLaVA%20and%20InternVL%202.5-1B%2C%20which%20uses%20a%20fixed%20CLIP%20vision%20encoder%20and%20performs%20full-parameter%20fine-tuning%20%28ViT%2BMLP%2BLLM%29%20during%20the%20instruction%20tuning%20stage.&entry.1838667208=http%3A//arxiv.org/abs/2601.03184v2&entry.124074799=Read"},
{"title": "S3-CLIP: Video Super Resolution for Person-ReID", "author": "Tamas Endrei and Gyorgy Cserey", "abstract": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.", "link": "http://arxiv.org/abs/2601.08807v1", "date": "2026-01-13", "relevancy": 2.1272, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5363}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5296}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S3-CLIP%3A%20Video%20Super%20Resolution%20for%20Person-ReID&body=Title%3A%20S3-CLIP%3A%20Video%20Super%20Resolution%20for%20Person-ReID%0AAuthor%3A%20Tamas%20Endrei%20and%20Gyorgy%20Cserey%0AAbstract%3A%20Tracklet%20quality%20is%20often%20treated%20as%20an%20afterthought%20in%20most%20person%20re-identification%20%28ReID%29%20methods%2C%20with%20the%20majority%20of%20research%20presenting%20architectural%20modifications%20to%20foundational%20models.%20Such%20approaches%20neglect%20an%20important%20limitation%2C%20posing%20challenges%20when%20deploying%20ReID%20systems%20in%20real-world%2C%20difficult%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20S3-CLIP%2C%20a%20video%20super-resolution-based%20CLIP-ReID%20framework%20developed%20for%20the%20VReID-XFD%20challenge%20at%20WACV%202026.%20The%20proposed%20method%20integrates%20recent%20advances%20in%20super-resolution%20networks%20with%20task-driven%20super-resolution%20pipelines%2C%20adapting%20them%20to%20the%20video-based%20person%20re-identification%20setting.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20represents%20the%20first%20systematic%20investigation%20of%20video%20super-resolution%20as%20a%20means%20of%20enhancing%20tracklet%20quality%20for%20person%20ReID%2C%20particularly%20under%20challenging%20cross-view%20conditions.%20Experimental%20results%20demonstrate%20performance%20competitive%20with%20the%20baseline%2C%20achieving%2037.52%25%20mAP%20in%20aerial-to-ground%20and%2029.16%25%20mAP%20in%20ground-to-aerial%20scenarios.%20In%20the%20ground-to-aerial%20setting%2C%20S3-CLIP%20achieves%20substantial%20gains%20in%20ranking%20accuracy%2C%20improving%20Rank-1%2C%20Rank-5%2C%20and%20Rank-10%20performance%20by%2011.24%25%2C%2013.48%25%2C%20and%2017.98%25%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS3-CLIP%253A%2520Video%2520Super%2520Resolution%2520for%2520Person-ReID%26entry.906535625%3DTamas%2520Endrei%2520and%2520Gyorgy%2520Cserey%26entry.1292438233%3DTracklet%2520quality%2520is%2520often%2520treated%2520as%2520an%2520afterthought%2520in%2520most%2520person%2520re-identification%2520%2528ReID%2529%2520methods%252C%2520with%2520the%2520majority%2520of%2520research%2520presenting%2520architectural%2520modifications%2520to%2520foundational%2520models.%2520Such%2520approaches%2520neglect%2520an%2520important%2520limitation%252C%2520posing%2520challenges%2520when%2520deploying%2520ReID%2520systems%2520in%2520real-world%252C%2520difficult%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520S3-CLIP%252C%2520a%2520video%2520super-resolution-based%2520CLIP-ReID%2520framework%2520developed%2520for%2520the%2520VReID-XFD%2520challenge%2520at%2520WACV%25202026.%2520The%2520proposed%2520method%2520integrates%2520recent%2520advances%2520in%2520super-resolution%2520networks%2520with%2520task-driven%2520super-resolution%2520pipelines%252C%2520adapting%2520them%2520to%2520the%2520video-based%2520person%2520re-identification%2520setting.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520represents%2520the%2520first%2520systematic%2520investigation%2520of%2520video%2520super-resolution%2520as%2520a%2520means%2520of%2520enhancing%2520tracklet%2520quality%2520for%2520person%2520ReID%252C%2520particularly%2520under%2520challenging%2520cross-view%2520conditions.%2520Experimental%2520results%2520demonstrate%2520performance%2520competitive%2520with%2520the%2520baseline%252C%2520achieving%252037.52%2525%2520mAP%2520in%2520aerial-to-ground%2520and%252029.16%2525%2520mAP%2520in%2520ground-to-aerial%2520scenarios.%2520In%2520the%2520ground-to-aerial%2520setting%252C%2520S3-CLIP%2520achieves%2520substantial%2520gains%2520in%2520ranking%2520accuracy%252C%2520improving%2520Rank-1%252C%2520Rank-5%252C%2520and%2520Rank-10%2520performance%2520by%252011.24%2525%252C%252013.48%2525%252C%2520and%252017.98%2525%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S3-CLIP%3A%20Video%20Super%20Resolution%20for%20Person-ReID&entry.906535625=Tamas%20Endrei%20and%20Gyorgy%20Cserey&entry.1292438233=Tracklet%20quality%20is%20often%20treated%20as%20an%20afterthought%20in%20most%20person%20re-identification%20%28ReID%29%20methods%2C%20with%20the%20majority%20of%20research%20presenting%20architectural%20modifications%20to%20foundational%20models.%20Such%20approaches%20neglect%20an%20important%20limitation%2C%20posing%20challenges%20when%20deploying%20ReID%20systems%20in%20real-world%2C%20difficult%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20S3-CLIP%2C%20a%20video%20super-resolution-based%20CLIP-ReID%20framework%20developed%20for%20the%20VReID-XFD%20challenge%20at%20WACV%202026.%20The%20proposed%20method%20integrates%20recent%20advances%20in%20super-resolution%20networks%20with%20task-driven%20super-resolution%20pipelines%2C%20adapting%20them%20to%20the%20video-based%20person%20re-identification%20setting.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20represents%20the%20first%20systematic%20investigation%20of%20video%20super-resolution%20as%20a%20means%20of%20enhancing%20tracklet%20quality%20for%20person%20ReID%2C%20particularly%20under%20challenging%20cross-view%20conditions.%20Experimental%20results%20demonstrate%20performance%20competitive%20with%20the%20baseline%2C%20achieving%2037.52%25%20mAP%20in%20aerial-to-ground%20and%2029.16%25%20mAP%20in%20ground-to-aerial%20scenarios.%20In%20the%20ground-to-aerial%20setting%2C%20S3-CLIP%20achieves%20substantial%20gains%20in%20ranking%20accuracy%2C%20improving%20Rank-1%2C%20Rank-5%2C%20and%20Rank-10%20performance%20by%2011.24%25%2C%2013.48%25%2C%20and%2017.98%25%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.08807v1&entry.124074799=Read"},
{"title": "EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers", "author": "Wenwen Liao and Hang Ruan", "abstract": "Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.", "link": "http://arxiv.org/abs/2601.08499v1", "date": "2026-01-13", "relevancy": 2.1271, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5385}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5321}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientFSL%3A%20Enhancing%20Few-Shot%20Classification%20via%20Query-Only%20Tuning%20in%20Vision%20Transformers&body=Title%3A%20EfficientFSL%3A%20Enhancing%20Few-Shot%20Classification%20via%20Query-Only%20Tuning%20in%20Vision%20Transformers%0AAuthor%3A%20Wenwen%20Liao%20and%20Hang%20Ruan%0AAbstract%3A%20Large%20models%20such%20as%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20superiority%20over%20smaller%20architectures%20like%20ResNet%20in%20few-shot%20classification%2C%20owing%20to%20their%20powerful%20representational%20capacity.%20However%2C%20fine-tuning%20such%20large%20models%20demands%20extensive%20GPU%20memory%20and%20prolonged%20training%20time%2C%20making%20them%20impractical%20for%20many%20real-world%20low-resource%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20EfficientFSL%2C%20a%20query-only%20fine-tuning%20framework%20tailored%20specifically%20for%20few-shot%20classification%20with%20ViT%2C%20which%20achieves%20competitive%20performance%20while%20significantly%20reducing%20computational%20overhead.%20EfficientFSL%20fully%20leverages%20the%20knowledge%20embedded%20in%20the%20pre-trained%20model%20and%20its%20strong%20comprehension%20ability%2C%20achieving%20high%20classification%20accuracy%20with%20an%20extremely%20small%20number%20of%20tunable%20parameters.%20Specifically%2C%20we%20introduce%20a%20lightweight%20trainable%20Forward%20Block%20to%20synthesize%20task-specific%20queries%20that%20extract%20informative%20features%20from%20the%20intermediate%20representations%20of%20the%20pre-trained%20model%20in%20a%20query-only%20manner.%20We%20further%20propose%20a%20Combine%20Block%20to%20fuse%20multi-layer%20outputs%2C%20enhancing%20the%20depth%20and%20robustness%20of%20feature%20representations.%20Finally%2C%20a%20Support-Query%20Attention%20Block%20mitigates%20distribution%20shift%20by%20adjusting%20prototypes%20to%20align%20with%20the%20query%20set%20distribution.%20With%20minimal%20trainable%20parameters%2C%20EfficientFSL%20achieves%20state-of-the-art%20performance%20on%20four%20in-domain%20few-shot%20datasets%20and%20six%20cross-domain%20datasets%2C%20demonstrating%20its%20effectiveness%20in%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientFSL%253A%2520Enhancing%2520Few-Shot%2520Classification%2520via%2520Query-Only%2520Tuning%2520in%2520Vision%2520Transformers%26entry.906535625%3DWenwen%2520Liao%2520and%2520Hang%2520Ruan%26entry.1292438233%3DLarge%2520models%2520such%2520as%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520remarkable%2520superiority%2520over%2520smaller%2520architectures%2520like%2520ResNet%2520in%2520few-shot%2520classification%252C%2520owing%2520to%2520their%2520powerful%2520representational%2520capacity.%2520However%252C%2520fine-tuning%2520such%2520large%2520models%2520demands%2520extensive%2520GPU%2520memory%2520and%2520prolonged%2520training%2520time%252C%2520making%2520them%2520impractical%2520for%2520many%2520real-world%2520low-resource%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520EfficientFSL%252C%2520a%2520query-only%2520fine-tuning%2520framework%2520tailored%2520specifically%2520for%2520few-shot%2520classification%2520with%2520ViT%252C%2520which%2520achieves%2520competitive%2520performance%2520while%2520significantly%2520reducing%2520computational%2520overhead.%2520EfficientFSL%2520fully%2520leverages%2520the%2520knowledge%2520embedded%2520in%2520the%2520pre-trained%2520model%2520and%2520its%2520strong%2520comprehension%2520ability%252C%2520achieving%2520high%2520classification%2520accuracy%2520with%2520an%2520extremely%2520small%2520number%2520of%2520tunable%2520parameters.%2520Specifically%252C%2520we%2520introduce%2520a%2520lightweight%2520trainable%2520Forward%2520Block%2520to%2520synthesize%2520task-specific%2520queries%2520that%2520extract%2520informative%2520features%2520from%2520the%2520intermediate%2520representations%2520of%2520the%2520pre-trained%2520model%2520in%2520a%2520query-only%2520manner.%2520We%2520further%2520propose%2520a%2520Combine%2520Block%2520to%2520fuse%2520multi-layer%2520outputs%252C%2520enhancing%2520the%2520depth%2520and%2520robustness%2520of%2520feature%2520representations.%2520Finally%252C%2520a%2520Support-Query%2520Attention%2520Block%2520mitigates%2520distribution%2520shift%2520by%2520adjusting%2520prototypes%2520to%2520align%2520with%2520the%2520query%2520set%2520distribution.%2520With%2520minimal%2520trainable%2520parameters%252C%2520EfficientFSL%2520achieves%2520state-of-the-art%2520performance%2520on%2520four%2520in-domain%2520few-shot%2520datasets%2520and%2520six%2520cross-domain%2520datasets%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientFSL%3A%20Enhancing%20Few-Shot%20Classification%20via%20Query-Only%20Tuning%20in%20Vision%20Transformers&entry.906535625=Wenwen%20Liao%20and%20Hang%20Ruan&entry.1292438233=Large%20models%20such%20as%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20superiority%20over%20smaller%20architectures%20like%20ResNet%20in%20few-shot%20classification%2C%20owing%20to%20their%20powerful%20representational%20capacity.%20However%2C%20fine-tuning%20such%20large%20models%20demands%20extensive%20GPU%20memory%20and%20prolonged%20training%20time%2C%20making%20them%20impractical%20for%20many%20real-world%20low-resource%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20EfficientFSL%2C%20a%20query-only%20fine-tuning%20framework%20tailored%20specifically%20for%20few-shot%20classification%20with%20ViT%2C%20which%20achieves%20competitive%20performance%20while%20significantly%20reducing%20computational%20overhead.%20EfficientFSL%20fully%20leverages%20the%20knowledge%20embedded%20in%20the%20pre-trained%20model%20and%20its%20strong%20comprehension%20ability%2C%20achieving%20high%20classification%20accuracy%20with%20an%20extremely%20small%20number%20of%20tunable%20parameters.%20Specifically%2C%20we%20introduce%20a%20lightweight%20trainable%20Forward%20Block%20to%20synthesize%20task-specific%20queries%20that%20extract%20informative%20features%20from%20the%20intermediate%20representations%20of%20the%20pre-trained%20model%20in%20a%20query-only%20manner.%20We%20further%20propose%20a%20Combine%20Block%20to%20fuse%20multi-layer%20outputs%2C%20enhancing%20the%20depth%20and%20robustness%20of%20feature%20representations.%20Finally%2C%20a%20Support-Query%20Attention%20Block%20mitigates%20distribution%20shift%20by%20adjusting%20prototypes%20to%20align%20with%20the%20query%20set%20distribution.%20With%20minimal%20trainable%20parameters%2C%20EfficientFSL%20achieves%20state-of-the-art%20performance%20on%20four%20in-domain%20few-shot%20datasets%20and%20six%20cross-domain%20datasets%2C%20demonstrating%20its%20effectiveness%20in%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.08499v1&entry.124074799=Read"},
{"title": "Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse", "author": "Warissara Booranamaitree and Xusheng Du and Yushu Cai and Zhengyang Wang and Ye Zhang and Haoran Xie", "abstract": "Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.", "link": "http://arxiv.org/abs/2601.08531v1", "date": "2026-01-13", "relevancy": 2.1268, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5336}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-Based%20Facade%20Renovation%20With%20Generative%20AI%3A%20A%20Streamlined%20Framework%20for%20Bypassing%20As-Built%20Modelling%20in%20Industrial%20Adaptive%20Reuse&body=Title%3A%20Sketch-Based%20Facade%20Renovation%20With%20Generative%20AI%3A%20A%20Streamlined%20Framework%20for%20Bypassing%20As-Built%20Modelling%20in%20Industrial%20Adaptive%20Reuse%0AAuthor%3A%20Warissara%20Booranamaitree%20and%20Xusheng%20Du%20and%20Yushu%20Cai%20and%20Zhengyang%20Wang%20and%20Ye%20Zhang%20and%20Haoran%20Xie%0AAbstract%3A%20Facade%20renovation%20offers%20a%20more%20sustainable%20alternative%20to%20full%20demolition%2C%20yet%20producing%20design%20proposals%20that%20preserve%20existing%20structures%20while%20expressing%20new%20intent%20remains%20challenging.%20Current%20workflows%20typically%20require%20detailed%20as-built%20modelling%20before%20design%2C%20which%20is%20time-consuming%2C%20labour-intensive%2C%20and%20often%20involves%20repeated%20revisions.%20To%20solve%20this%20issue%2C%20we%20propose%20a%20three-stage%20framework%20combining%20generative%20artificial%20intelligence%20%28AI%29%20and%20vision-language%20models%20%28VLM%29%20that%20directly%20processes%20rough%20structural%20sketch%20and%20textual%20descriptions%20to%20produce%20consistent%20renovation%20proposals.%20First%2C%20the%20input%20sketch%20is%20used%20by%20a%20fine-tuned%20VLM%20model%20to%20predict%20bounding%20boxes%20specifying%20where%20modifications%20are%20needed%20and%20which%20components%20should%20be%20added.%20Next%2C%20a%20stable%20diffusion%20model%20generates%20detailed%20sketches%20of%20new%20elements%2C%20which%20are%20merged%20with%20the%20original%20outline%20through%20a%20generative%20inpainting%20pipeline.%20Finally%2C%20ControlNet%20is%20employed%20to%20refine%20the%20result%20into%20a%20photorealistic%20image.%20Experiments%20on%20datasets%20and%20real%20industrial%20buildings%20indicate%20that%20the%20proposed%20framework%20can%20generate%20renovation%20proposals%20that%20preserve%20the%20original%20structure%20while%20improving%20facade%20detail%20quality.%20This%20approach%20effectively%20bypasses%20the%20need%20for%20detailed%20as-built%20modelling%2C%20enabling%20architects%20to%20rapidly%20explore%20design%20alternatives%2C%20iterate%20on%20early-stage%20concepts%2C%20and%20communicate%20renovation%20intentions%20with%20greater%20clarity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-Based%2520Facade%2520Renovation%2520With%2520Generative%2520AI%253A%2520A%2520Streamlined%2520Framework%2520for%2520Bypassing%2520As-Built%2520Modelling%2520in%2520Industrial%2520Adaptive%2520Reuse%26entry.906535625%3DWarissara%2520Booranamaitree%2520and%2520Xusheng%2520Du%2520and%2520Yushu%2520Cai%2520and%2520Zhengyang%2520Wang%2520and%2520Ye%2520Zhang%2520and%2520Haoran%2520Xie%26entry.1292438233%3DFacade%2520renovation%2520offers%2520a%2520more%2520sustainable%2520alternative%2520to%2520full%2520demolition%252C%2520yet%2520producing%2520design%2520proposals%2520that%2520preserve%2520existing%2520structures%2520while%2520expressing%2520new%2520intent%2520remains%2520challenging.%2520Current%2520workflows%2520typically%2520require%2520detailed%2520as-built%2520modelling%2520before%2520design%252C%2520which%2520is%2520time-consuming%252C%2520labour-intensive%252C%2520and%2520often%2520involves%2520repeated%2520revisions.%2520To%2520solve%2520this%2520issue%252C%2520we%2520propose%2520a%2520three-stage%2520framework%2520combining%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520vision-language%2520models%2520%2528VLM%2529%2520that%2520directly%2520processes%2520rough%2520structural%2520sketch%2520and%2520textual%2520descriptions%2520to%2520produce%2520consistent%2520renovation%2520proposals.%2520First%252C%2520the%2520input%2520sketch%2520is%2520used%2520by%2520a%2520fine-tuned%2520VLM%2520model%2520to%2520predict%2520bounding%2520boxes%2520specifying%2520where%2520modifications%2520are%2520needed%2520and%2520which%2520components%2520should%2520be%2520added.%2520Next%252C%2520a%2520stable%2520diffusion%2520model%2520generates%2520detailed%2520sketches%2520of%2520new%2520elements%252C%2520which%2520are%2520merged%2520with%2520the%2520original%2520outline%2520through%2520a%2520generative%2520inpainting%2520pipeline.%2520Finally%252C%2520ControlNet%2520is%2520employed%2520to%2520refine%2520the%2520result%2520into%2520a%2520photorealistic%2520image.%2520Experiments%2520on%2520datasets%2520and%2520real%2520industrial%2520buildings%2520indicate%2520that%2520the%2520proposed%2520framework%2520can%2520generate%2520renovation%2520proposals%2520that%2520preserve%2520the%2520original%2520structure%2520while%2520improving%2520facade%2520detail%2520quality.%2520This%2520approach%2520effectively%2520bypasses%2520the%2520need%2520for%2520detailed%2520as-built%2520modelling%252C%2520enabling%2520architects%2520to%2520rapidly%2520explore%2520design%2520alternatives%252C%2520iterate%2520on%2520early-stage%2520concepts%252C%2520and%2520communicate%2520renovation%2520intentions%2520with%2520greater%2520clarity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-Based%20Facade%20Renovation%20With%20Generative%20AI%3A%20A%20Streamlined%20Framework%20for%20Bypassing%20As-Built%20Modelling%20in%20Industrial%20Adaptive%20Reuse&entry.906535625=Warissara%20Booranamaitree%20and%20Xusheng%20Du%20and%20Yushu%20Cai%20and%20Zhengyang%20Wang%20and%20Ye%20Zhang%20and%20Haoran%20Xie&entry.1292438233=Facade%20renovation%20offers%20a%20more%20sustainable%20alternative%20to%20full%20demolition%2C%20yet%20producing%20design%20proposals%20that%20preserve%20existing%20structures%20while%20expressing%20new%20intent%20remains%20challenging.%20Current%20workflows%20typically%20require%20detailed%20as-built%20modelling%20before%20design%2C%20which%20is%20time-consuming%2C%20labour-intensive%2C%20and%20often%20involves%20repeated%20revisions.%20To%20solve%20this%20issue%2C%20we%20propose%20a%20three-stage%20framework%20combining%20generative%20artificial%20intelligence%20%28AI%29%20and%20vision-language%20models%20%28VLM%29%20that%20directly%20processes%20rough%20structural%20sketch%20and%20textual%20descriptions%20to%20produce%20consistent%20renovation%20proposals.%20First%2C%20the%20input%20sketch%20is%20used%20by%20a%20fine-tuned%20VLM%20model%20to%20predict%20bounding%20boxes%20specifying%20where%20modifications%20are%20needed%20and%20which%20components%20should%20be%20added.%20Next%2C%20a%20stable%20diffusion%20model%20generates%20detailed%20sketches%20of%20new%20elements%2C%20which%20are%20merged%20with%20the%20original%20outline%20through%20a%20generative%20inpainting%20pipeline.%20Finally%2C%20ControlNet%20is%20employed%20to%20refine%20the%20result%20into%20a%20photorealistic%20image.%20Experiments%20on%20datasets%20and%20real%20industrial%20buildings%20indicate%20that%20the%20proposed%20framework%20can%20generate%20renovation%20proposals%20that%20preserve%20the%20original%20structure%20while%20improving%20facade%20detail%20quality.%20This%20approach%20effectively%20bypasses%20the%20need%20for%20detailed%20as-built%20modelling%2C%20enabling%20architects%20to%20rapidly%20explore%20design%20alternatives%2C%20iterate%20on%20early-stage%20concepts%2C%20and%20communicate%20renovation%20intentions%20with%20greater%20clarity.&entry.1838667208=http%3A//arxiv.org/abs/2601.08531v1&entry.124074799=Read"},
{"title": "SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling", "author": "Xi Chen and Hongxun Yao and Sicheng Zhao and Jiankun Zhu and Jing Jiang and Kui Jiang", "abstract": "Source-free domain adaptation (SFDA) tackles the critical challenge of adapting source-pretrained models to unlabeled target domains without access to source data, overcoming data privacy and storage limitations in real-world applications. However, existing SFDA approaches struggle with the trade-off between perception field and computational efficiency in domain-invariant feature learning. Recently, Mamba has offered a promising solution through its selective scan mechanism, which enables long-range dependency modeling with linear complexity. However, the Visual Mamba (i.e., VMamba) remains limited in capturing channel-wise frequency characteristics critical for domain alignment and maintaining spatial robustness under significant domain shifts. To address these, we propose a framework called SfMamba to fully explore the stable dependency in source-free model transfer. SfMamba introduces Channel-wise Visual State-Space block that enables channel-sequence scanning for domain-invariant feature extraction. In addition, SfMamba involves a Semantic-Consistent Shuffle strategy that disrupts background patch sequences in 2D selective scan while preserving prediction consistency to mitigate error accumulation. Comprehensive evaluations across multiple benchmarks show that SfMamba achieves consistently stronger performance than existing methods while maintaining favorable parameter efficiency, offering a practical solution for SFDA. Our code is available at https://github.com/chenxi52/SfMamba.", "link": "http://arxiv.org/abs/2601.08608v1", "date": "2026-01-13", "relevancy": 2.1217, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5275}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SfMamba%3A%20Efficient%20Source-Free%20Domain%20Adaptation%20via%20Selective%20Scan%20Modeling&body=Title%3A%20SfMamba%3A%20Efficient%20Source-Free%20Domain%20Adaptation%20via%20Selective%20Scan%20Modeling%0AAuthor%3A%20Xi%20Chen%20and%20Hongxun%20Yao%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Jing%20Jiang%20and%20Kui%20Jiang%0AAbstract%3A%20Source-free%20domain%20adaptation%20%28SFDA%29%20tackles%20the%20critical%20challenge%20of%20adapting%20source-pretrained%20models%20to%20unlabeled%20target%20domains%20without%20access%20to%20source%20data%2C%20overcoming%20data%20privacy%20and%20storage%20limitations%20in%20real-world%20applications.%20However%2C%20existing%20SFDA%20approaches%20struggle%20with%20the%20trade-off%20between%20perception%20field%20and%20computational%20efficiency%20in%20domain-invariant%20feature%20learning.%20Recently%2C%20Mamba%20has%20offered%20a%20promising%20solution%20through%20its%20selective%20scan%20mechanism%2C%20which%20enables%20long-range%20dependency%20modeling%20with%20linear%20complexity.%20However%2C%20the%20Visual%20Mamba%20%28i.e.%2C%20VMamba%29%20remains%20limited%20in%20capturing%20channel-wise%20frequency%20characteristics%20critical%20for%20domain%20alignment%20and%20maintaining%20spatial%20robustness%20under%20significant%20domain%20shifts.%20To%20address%20these%2C%20we%20propose%20a%20framework%20called%20SfMamba%20to%20fully%20explore%20the%20stable%20dependency%20in%20source-free%20model%20transfer.%20SfMamba%20introduces%20Channel-wise%20Visual%20State-Space%20block%20that%20enables%20channel-sequence%20scanning%20for%20domain-invariant%20feature%20extraction.%20In%20addition%2C%20SfMamba%20involves%20a%20Semantic-Consistent%20Shuffle%20strategy%20that%20disrupts%20background%20patch%20sequences%20in%202D%20selective%20scan%20while%20preserving%20prediction%20consistency%20to%20mitigate%20error%20accumulation.%20Comprehensive%20evaluations%20across%20multiple%20benchmarks%20show%20that%20SfMamba%20achieves%20consistently%20stronger%20performance%20than%20existing%20methods%20while%20maintaining%20favorable%20parameter%20efficiency%2C%20offering%20a%20practical%20solution%20for%20SFDA.%20Our%20code%20is%20available%20at%20https%3A//github.com/chenxi52/SfMamba.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSfMamba%253A%2520Efficient%2520Source-Free%2520Domain%2520Adaptation%2520via%2520Selective%2520Scan%2520Modeling%26entry.906535625%3DXi%2520Chen%2520and%2520Hongxun%2520Yao%2520and%2520Sicheng%2520Zhao%2520and%2520Jiankun%2520Zhu%2520and%2520Jing%2520Jiang%2520and%2520Kui%2520Jiang%26entry.1292438233%3DSource-free%2520domain%2520adaptation%2520%2528SFDA%2529%2520tackles%2520the%2520critical%2520challenge%2520of%2520adapting%2520source-pretrained%2520models%2520to%2520unlabeled%2520target%2520domains%2520without%2520access%2520to%2520source%2520data%252C%2520overcoming%2520data%2520privacy%2520and%2520storage%2520limitations%2520in%2520real-world%2520applications.%2520However%252C%2520existing%2520SFDA%2520approaches%2520struggle%2520with%2520the%2520trade-off%2520between%2520perception%2520field%2520and%2520computational%2520efficiency%2520in%2520domain-invariant%2520feature%2520learning.%2520Recently%252C%2520Mamba%2520has%2520offered%2520a%2520promising%2520solution%2520through%2520its%2520selective%2520scan%2520mechanism%252C%2520which%2520enables%2520long-range%2520dependency%2520modeling%2520with%2520linear%2520complexity.%2520However%252C%2520the%2520Visual%2520Mamba%2520%2528i.e.%252C%2520VMamba%2529%2520remains%2520limited%2520in%2520capturing%2520channel-wise%2520frequency%2520characteristics%2520critical%2520for%2520domain%2520alignment%2520and%2520maintaining%2520spatial%2520robustness%2520under%2520significant%2520domain%2520shifts.%2520To%2520address%2520these%252C%2520we%2520propose%2520a%2520framework%2520called%2520SfMamba%2520to%2520fully%2520explore%2520the%2520stable%2520dependency%2520in%2520source-free%2520model%2520transfer.%2520SfMamba%2520introduces%2520Channel-wise%2520Visual%2520State-Space%2520block%2520that%2520enables%2520channel-sequence%2520scanning%2520for%2520domain-invariant%2520feature%2520extraction.%2520In%2520addition%252C%2520SfMamba%2520involves%2520a%2520Semantic-Consistent%2520Shuffle%2520strategy%2520that%2520disrupts%2520background%2520patch%2520sequences%2520in%25202D%2520selective%2520scan%2520while%2520preserving%2520prediction%2520consistency%2520to%2520mitigate%2520error%2520accumulation.%2520Comprehensive%2520evaluations%2520across%2520multiple%2520benchmarks%2520show%2520that%2520SfMamba%2520achieves%2520consistently%2520stronger%2520performance%2520than%2520existing%2520methods%2520while%2520maintaining%2520favorable%2520parameter%2520efficiency%252C%2520offering%2520a%2520practical%2520solution%2520for%2520SFDA.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/chenxi52/SfMamba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SfMamba%3A%20Efficient%20Source-Free%20Domain%20Adaptation%20via%20Selective%20Scan%20Modeling&entry.906535625=Xi%20Chen%20and%20Hongxun%20Yao%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Jing%20Jiang%20and%20Kui%20Jiang&entry.1292438233=Source-free%20domain%20adaptation%20%28SFDA%29%20tackles%20the%20critical%20challenge%20of%20adapting%20source-pretrained%20models%20to%20unlabeled%20target%20domains%20without%20access%20to%20source%20data%2C%20overcoming%20data%20privacy%20and%20storage%20limitations%20in%20real-world%20applications.%20However%2C%20existing%20SFDA%20approaches%20struggle%20with%20the%20trade-off%20between%20perception%20field%20and%20computational%20efficiency%20in%20domain-invariant%20feature%20learning.%20Recently%2C%20Mamba%20has%20offered%20a%20promising%20solution%20through%20its%20selective%20scan%20mechanism%2C%20which%20enables%20long-range%20dependency%20modeling%20with%20linear%20complexity.%20However%2C%20the%20Visual%20Mamba%20%28i.e.%2C%20VMamba%29%20remains%20limited%20in%20capturing%20channel-wise%20frequency%20characteristics%20critical%20for%20domain%20alignment%20and%20maintaining%20spatial%20robustness%20under%20significant%20domain%20shifts.%20To%20address%20these%2C%20we%20propose%20a%20framework%20called%20SfMamba%20to%20fully%20explore%20the%20stable%20dependency%20in%20source-free%20model%20transfer.%20SfMamba%20introduces%20Channel-wise%20Visual%20State-Space%20block%20that%20enables%20channel-sequence%20scanning%20for%20domain-invariant%20feature%20extraction.%20In%20addition%2C%20SfMamba%20involves%20a%20Semantic-Consistent%20Shuffle%20strategy%20that%20disrupts%20background%20patch%20sequences%20in%202D%20selective%20scan%20while%20preserving%20prediction%20consistency%20to%20mitigate%20error%20accumulation.%20Comprehensive%20evaluations%20across%20multiple%20benchmarks%20show%20that%20SfMamba%20achieves%20consistently%20stronger%20performance%20than%20existing%20methods%20while%20maintaining%20favorable%20parameter%20efficiency%2C%20offering%20a%20practical%20solution%20for%20SFDA.%20Our%20code%20is%20available%20at%20https%3A//github.com/chenxi52/SfMamba.&entry.1838667208=http%3A//arxiv.org/abs/2601.08608v1&entry.124074799=Read"},
{"title": "Aggregating Diverse Cue Experts for AI-Generated Image Detection", "author": "Lei Tan and Shuwei Li and Mohan Kankanhalli and Robby T. Tan", "abstract": "The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.", "link": "http://arxiv.org/abs/2601.08790v1", "date": "2026-01-13", "relevancy": 2.1192, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5385}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5248}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aggregating%20Diverse%20Cue%20Experts%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20Aggregating%20Diverse%20Cue%20Experts%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Lei%20Tan%20and%20Shuwei%20Li%20and%20Mohan%20Kankanhalli%20and%20Robby%20T.%20Tan%0AAbstract%3A%20The%20rapid%20emergence%20of%20image%20synthesis%20models%20poses%20challenges%20to%20the%20generalization%20of%20AI-generated%20image%20detectors.%20However%2C%20existing%20methods%20often%20rely%20on%20model-specific%20features%2C%20leading%20to%20overfitting%20and%20poor%20generalization.%20In%20this%20paper%2C%20we%20introduce%20the%20Multi-Cue%20Aggregation%20Network%20%28MCAN%29%2C%20a%20novel%20framework%20that%20integrates%20different%20yet%20complementary%20cues%20in%20a%20unified%20network.%20MCAN%20employs%20a%20mixture-of-encoders%20adapter%20to%20dynamically%20process%20these%20cues%2C%20enabling%20more%20adaptive%20and%20robust%20feature%20representation.%20Our%20cues%20include%20the%20input%20image%20itself%2C%20which%20represents%20the%20overall%20content%2C%20and%20high-frequency%20components%20that%20emphasize%20edge%20details.%20Additionally%2C%20we%20introduce%20a%20Chromatic%20Inconsistency%20%28CI%29%20cue%2C%20which%20normalizes%20intensity%20values%20and%20captures%20noise%20information%20introduced%20during%20the%20image%20acquisition%20process%20in%20real%20images%2C%20making%20these%20noise%20patterns%20more%20distinguishable%20from%20those%20in%20AI-generated%20content.%20Unlike%20prior%20methods%2C%20MCAN%27s%20novelty%20lies%20in%20its%20unified%20multi-cue%20aggregation%20framework%2C%20which%20integrates%20spatial%2C%20frequency-domain%2C%20and%20chromaticity-based%20information%20for%20enhanced%20representation%20learning.%20These%20cues%20are%20intrinsically%20more%20indicative%20of%20real%20images%2C%20enhancing%20cross-model%20generalization.%20Extensive%20experiments%20on%20the%20GenImage%2C%20Chameleon%2C%20and%20UniversalFakeDetect%20benchmark%20validate%20the%20state-of-the-art%20performance%20of%20MCAN.%20In%20the%20GenImage%20dataset%2C%20MCAN%20outperforms%20the%20best%20state-of-the-art%20method%20by%20up%20to%207.4%25%20in%20average%20ACC%20across%20eight%20different%20image%20generators.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggregating%2520Diverse%2520Cue%2520Experts%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DLei%2520Tan%2520and%2520Shuwei%2520Li%2520and%2520Mohan%2520Kankanhalli%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3DThe%2520rapid%2520emergence%2520of%2520image%2520synthesis%2520models%2520poses%2520challenges%2520to%2520the%2520generalization%2520of%2520AI-generated%2520image%2520detectors.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%2520model-specific%2520features%252C%2520leading%2520to%2520overfitting%2520and%2520poor%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Multi-Cue%2520Aggregation%2520Network%2520%2528MCAN%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520different%2520yet%2520complementary%2520cues%2520in%2520a%2520unified%2520network.%2520MCAN%2520employs%2520a%2520mixture-of-encoders%2520adapter%2520to%2520dynamically%2520process%2520these%2520cues%252C%2520enabling%2520more%2520adaptive%2520and%2520robust%2520feature%2520representation.%2520Our%2520cues%2520include%2520the%2520input%2520image%2520itself%252C%2520which%2520represents%2520the%2520overall%2520content%252C%2520and%2520high-frequency%2520components%2520that%2520emphasize%2520edge%2520details.%2520Additionally%252C%2520we%2520introduce%2520a%2520Chromatic%2520Inconsistency%2520%2528CI%2529%2520cue%252C%2520which%2520normalizes%2520intensity%2520values%2520and%2520captures%2520noise%2520information%2520introduced%2520during%2520the%2520image%2520acquisition%2520process%2520in%2520real%2520images%252C%2520making%2520these%2520noise%2520patterns%2520more%2520distinguishable%2520from%2520those%2520in%2520AI-generated%2520content.%2520Unlike%2520prior%2520methods%252C%2520MCAN%2527s%2520novelty%2520lies%2520in%2520its%2520unified%2520multi-cue%2520aggregation%2520framework%252C%2520which%2520integrates%2520spatial%252C%2520frequency-domain%252C%2520and%2520chromaticity-based%2520information%2520for%2520enhanced%2520representation%2520learning.%2520These%2520cues%2520are%2520intrinsically%2520more%2520indicative%2520of%2520real%2520images%252C%2520enhancing%2520cross-model%2520generalization.%2520Extensive%2520experiments%2520on%2520the%2520GenImage%252C%2520Chameleon%252C%2520and%2520UniversalFakeDetect%2520benchmark%2520validate%2520the%2520state-of-the-art%2520performance%2520of%2520MCAN.%2520In%2520the%2520GenImage%2520dataset%252C%2520MCAN%2520outperforms%2520the%2520best%2520state-of-the-art%2520method%2520by%2520up%2520to%25207.4%2525%2520in%2520average%2520ACC%2520across%2520eight%2520different%2520image%2520generators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggregating%20Diverse%20Cue%20Experts%20for%20AI-Generated%20Image%20Detection&entry.906535625=Lei%20Tan%20and%20Shuwei%20Li%20and%20Mohan%20Kankanhalli%20and%20Robby%20T.%20Tan&entry.1292438233=The%20rapid%20emergence%20of%20image%20synthesis%20models%20poses%20challenges%20to%20the%20generalization%20of%20AI-generated%20image%20detectors.%20However%2C%20existing%20methods%20often%20rely%20on%20model-specific%20features%2C%20leading%20to%20overfitting%20and%20poor%20generalization.%20In%20this%20paper%2C%20we%20introduce%20the%20Multi-Cue%20Aggregation%20Network%20%28MCAN%29%2C%20a%20novel%20framework%20that%20integrates%20different%20yet%20complementary%20cues%20in%20a%20unified%20network.%20MCAN%20employs%20a%20mixture-of-encoders%20adapter%20to%20dynamically%20process%20these%20cues%2C%20enabling%20more%20adaptive%20and%20robust%20feature%20representation.%20Our%20cues%20include%20the%20input%20image%20itself%2C%20which%20represents%20the%20overall%20content%2C%20and%20high-frequency%20components%20that%20emphasize%20edge%20details.%20Additionally%2C%20we%20introduce%20a%20Chromatic%20Inconsistency%20%28CI%29%20cue%2C%20which%20normalizes%20intensity%20values%20and%20captures%20noise%20information%20introduced%20during%20the%20image%20acquisition%20process%20in%20real%20images%2C%20making%20these%20noise%20patterns%20more%20distinguishable%20from%20those%20in%20AI-generated%20content.%20Unlike%20prior%20methods%2C%20MCAN%27s%20novelty%20lies%20in%20its%20unified%20multi-cue%20aggregation%20framework%2C%20which%20integrates%20spatial%2C%20frequency-domain%2C%20and%20chromaticity-based%20information%20for%20enhanced%20representation%20learning.%20These%20cues%20are%20intrinsically%20more%20indicative%20of%20real%20images%2C%20enhancing%20cross-model%20generalization.%20Extensive%20experiments%20on%20the%20GenImage%2C%20Chameleon%2C%20and%20UniversalFakeDetect%20benchmark%20validate%20the%20state-of-the-art%20performance%20of%20MCAN.%20In%20the%20GenImage%20dataset%2C%20MCAN%20outperforms%20the%20best%20state-of-the-art%20method%20by%20up%20to%207.4%25%20in%20average%20ACC%20across%20eight%20different%20image%20generators.&entry.1838667208=http%3A//arxiv.org/abs/2601.08790v1&entry.124074799=Read"},
{"title": "Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments", "author": "Yizhan Feng and Hichem Snoussi and Jing Teng and Abel Cherouat and Tian Wang", "abstract": "Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.", "link": "http://arxiv.org/abs/2601.08405v1", "date": "2026-01-13", "relevancy": 2.1181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20to%20Enhance%20Multi-task%20Drone%20Operations%20in%20Simulated%20Environments&body=Title%3A%20Large%20Language%20Models%20to%20Enhance%20Multi-task%20Drone%20Operations%20in%20Simulated%20Environments%0AAuthor%3A%20Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Jing%20Teng%20and%20Abel%20Cherouat%20and%20Tian%20Wang%0AAbstract%3A%20Benefiting%20from%20the%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%2C%20human-drone%20interaction%20has%20reached%20unprecedented%20opportunities.%20In%20this%20paper%2C%20we%20propose%20a%20method%20that%20integrates%20a%20fine-tuned%20CodeT5%20model%20with%20the%20Unreal%20Engine-based%20AirSim%20drone%20simulator%20to%20efficiently%20execute%20multi-task%20operations%20using%20natural%20language%20commands.%20This%20approach%20enables%20users%20to%20interact%20with%20simulated%20drones%20through%20prompts%20or%20command%20descriptions%2C%20allowing%20them%20to%20easily%20access%20and%20control%20the%20drone%27s%20status%2C%20significantly%20lowering%20the%20operational%20threshold.%20In%20the%20AirSim%20simulator%2C%20we%20can%20flexibly%20construct%20visually%20realistic%20dynamic%20environments%20to%20simulate%20drone%20applications%20in%20complex%20scenarios.%20By%20combining%20a%20large%20dataset%20of%20%28natural%20language%2C%20program%20code%29%20command-execution%20pairs%20generated%20by%20ChatGPT%20with%20developer-written%20drone%20code%20as%20training%20data%2C%20we%20fine-tune%20the%20CodeT5%20to%20achieve%20automated%20translation%20from%20natural%20language%20to%20executable%20code%20for%20drone%20tasks.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20exhibits%20superior%20task%20execution%20efficiency%20and%20command%20understanding%20capabilities%20in%20simulated%20environments.%20In%20the%20future%2C%20we%20plan%20to%20extend%20the%20model%20functionality%20in%20a%20modular%20manner%2C%20enhancing%20its%20adaptability%20to%20complex%20scenarios%20and%20driving%20the%20application%20of%20drone%20technologies%20in%20real-world%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520to%2520Enhance%2520Multi-task%2520Drone%2520Operations%2520in%2520Simulated%2520Environments%26entry.906535625%3DYizhan%2520Feng%2520and%2520Hichem%2520Snoussi%2520and%2520Jing%2520Teng%2520and%2520Abel%2520Cherouat%2520and%2520Tian%2520Wang%26entry.1292438233%3DBenefiting%2520from%2520the%2520rapid%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520human-drone%2520interaction%2520has%2520reached%2520unprecedented%2520opportunities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520that%2520integrates%2520a%2520fine-tuned%2520CodeT5%2520model%2520with%2520the%2520Unreal%2520Engine-based%2520AirSim%2520drone%2520simulator%2520to%2520efficiently%2520execute%2520multi-task%2520operations%2520using%2520natural%2520language%2520commands.%2520This%2520approach%2520enables%2520users%2520to%2520interact%2520with%2520simulated%2520drones%2520through%2520prompts%2520or%2520command%2520descriptions%252C%2520allowing%2520them%2520to%2520easily%2520access%2520and%2520control%2520the%2520drone%2527s%2520status%252C%2520significantly%2520lowering%2520the%2520operational%2520threshold.%2520In%2520the%2520AirSim%2520simulator%252C%2520we%2520can%2520flexibly%2520construct%2520visually%2520realistic%2520dynamic%2520environments%2520to%2520simulate%2520drone%2520applications%2520in%2520complex%2520scenarios.%2520By%2520combining%2520a%2520large%2520dataset%2520of%2520%2528natural%2520language%252C%2520program%2520code%2529%2520command-execution%2520pairs%2520generated%2520by%2520ChatGPT%2520with%2520developer-written%2520drone%2520code%2520as%2520training%2520data%252C%2520we%2520fine-tune%2520the%2520CodeT5%2520to%2520achieve%2520automated%2520translation%2520from%2520natural%2520language%2520to%2520executable%2520code%2520for%2520drone%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520exhibits%2520superior%2520task%2520execution%2520efficiency%2520and%2520command%2520understanding%2520capabilities%2520in%2520simulated%2520environments.%2520In%2520the%2520future%252C%2520we%2520plan%2520to%2520extend%2520the%2520model%2520functionality%2520in%2520a%2520modular%2520manner%252C%2520enhancing%2520its%2520adaptability%2520to%2520complex%2520scenarios%2520and%2520driving%2520the%2520application%2520of%2520drone%2520technologies%2520in%2520real-world%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20to%20Enhance%20Multi-task%20Drone%20Operations%20in%20Simulated%20Environments&entry.906535625=Yizhan%20Feng%20and%20Hichem%20Snoussi%20and%20Jing%20Teng%20and%20Abel%20Cherouat%20and%20Tian%20Wang&entry.1292438233=Benefiting%20from%20the%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%2C%20human-drone%20interaction%20has%20reached%20unprecedented%20opportunities.%20In%20this%20paper%2C%20we%20propose%20a%20method%20that%20integrates%20a%20fine-tuned%20CodeT5%20model%20with%20the%20Unreal%20Engine-based%20AirSim%20drone%20simulator%20to%20efficiently%20execute%20multi-task%20operations%20using%20natural%20language%20commands.%20This%20approach%20enables%20users%20to%20interact%20with%20simulated%20drones%20through%20prompts%20or%20command%20descriptions%2C%20allowing%20them%20to%20easily%20access%20and%20control%20the%20drone%27s%20status%2C%20significantly%20lowering%20the%20operational%20threshold.%20In%20the%20AirSim%20simulator%2C%20we%20can%20flexibly%20construct%20visually%20realistic%20dynamic%20environments%20to%20simulate%20drone%20applications%20in%20complex%20scenarios.%20By%20combining%20a%20large%20dataset%20of%20%28natural%20language%2C%20program%20code%29%20command-execution%20pairs%20generated%20by%20ChatGPT%20with%20developer-written%20drone%20code%20as%20training%20data%2C%20we%20fine-tune%20the%20CodeT5%20to%20achieve%20automated%20translation%20from%20natural%20language%20to%20executable%20code%20for%20drone%20tasks.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20exhibits%20superior%20task%20execution%20efficiency%20and%20command%20understanding%20capabilities%20in%20simulated%20environments.%20In%20the%20future%2C%20we%20plan%20to%20extend%20the%20model%20functionality%20in%20a%20modular%20manner%2C%20enhancing%20its%20adaptability%20to%20complex%20scenarios%20and%20driving%20the%20application%20of%20drone%20technologies%20in%20real-world%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.08405v1&entry.124074799=Read"},
{"title": "Learning Force Distribution Estimation for the GelSight Mini Optical Tactile Sensor Based on Finite Element Analysis", "author": "Erik Helmut and Luca Dziarski and Niklas Funk and Boris Belousov and Jan Peters", "abstract": "Contact-rich manipulation remains a major challenge in robotics. Optical tactile sensors like GelSight Mini offer a low-cost solution for contact sensing by capturing soft-body deformations of the silicone gel. However, accurately inferring shear and normal force distributions from these gel deformations has yet to be fully addressed. In this work, we propose a machine learning approach using a U-net architecture to predict force distributions directly from the sensor's raw images. Our model, trained on force distributions inferred from \\ac{fea}, demonstrates promising accuracy in predicting normal and shear force distributions for the commercially available GelSight Mini sensor. It also shows potential for generalization across indenters, sensors of the same type, and for enabling real-time application. The codebase, dataset and models are open-sourced and available at https://feats-ai.github.io .", "link": "http://arxiv.org/abs/2411.03315v3", "date": "2026-01-13", "relevancy": 2.1162, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5731}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5267}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Force%20Distribution%20Estimation%20for%20the%20GelSight%20Mini%20Optical%20Tactile%20Sensor%20Based%20on%20Finite%20Element%20Analysis&body=Title%3A%20Learning%20Force%20Distribution%20Estimation%20for%20the%20GelSight%20Mini%20Optical%20Tactile%20Sensor%20Based%20on%20Finite%20Element%20Analysis%0AAuthor%3A%20Erik%20Helmut%20and%20Luca%20Dziarski%20and%20Niklas%20Funk%20and%20Boris%20Belousov%20and%20Jan%20Peters%0AAbstract%3A%20Contact-rich%20manipulation%20remains%20a%20major%20challenge%20in%20robotics.%20Optical%20tactile%20sensors%20like%20GelSight%20Mini%20offer%20a%20low-cost%20solution%20for%20contact%20sensing%20by%20capturing%20soft-body%20deformations%20of%20the%20silicone%20gel.%20However%2C%20accurately%20inferring%20shear%20and%20normal%20force%20distributions%20from%20these%20gel%20deformations%20has%20yet%20to%20be%20fully%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20machine%20learning%20approach%20using%20a%20U-net%20architecture%20to%20predict%20force%20distributions%20directly%20from%20the%20sensor%27s%20raw%20images.%20Our%20model%2C%20trained%20on%20force%20distributions%20inferred%20from%20%5Cac%7Bfea%7D%2C%20demonstrates%20promising%20accuracy%20in%20predicting%20normal%20and%20shear%20force%20distributions%20for%20the%20commercially%20available%20GelSight%20Mini%20sensor.%20It%20also%20shows%20potential%20for%20generalization%20across%20indenters%2C%20sensors%20of%20the%20same%20type%2C%20and%20for%20enabling%20real-time%20application.%20The%20codebase%2C%20dataset%20and%20models%20are%20open-sourced%20and%20available%20at%20https%3A//feats-ai.github.io%20.%0ALink%3A%20http%3A//arxiv.org/abs/2411.03315v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Force%2520Distribution%2520Estimation%2520for%2520the%2520GelSight%2520Mini%2520Optical%2520Tactile%2520Sensor%2520Based%2520on%2520Finite%2520Element%2520Analysis%26entry.906535625%3DErik%2520Helmut%2520and%2520Luca%2520Dziarski%2520and%2520Niklas%2520Funk%2520and%2520Boris%2520Belousov%2520and%2520Jan%2520Peters%26entry.1292438233%3DContact-rich%2520manipulation%2520remains%2520a%2520major%2520challenge%2520in%2520robotics.%2520Optical%2520tactile%2520sensors%2520like%2520GelSight%2520Mini%2520offer%2520a%2520low-cost%2520solution%2520for%2520contact%2520sensing%2520by%2520capturing%2520soft-body%2520deformations%2520of%2520the%2520silicone%2520gel.%2520However%252C%2520accurately%2520inferring%2520shear%2520and%2520normal%2520force%2520distributions%2520from%2520these%2520gel%2520deformations%2520has%2520yet%2520to%2520be%2520fully%2520addressed.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520machine%2520learning%2520approach%2520using%2520a%2520U-net%2520architecture%2520to%2520predict%2520force%2520distributions%2520directly%2520from%2520the%2520sensor%2527s%2520raw%2520images.%2520Our%2520model%252C%2520trained%2520on%2520force%2520distributions%2520inferred%2520from%2520%255Cac%257Bfea%257D%252C%2520demonstrates%2520promising%2520accuracy%2520in%2520predicting%2520normal%2520and%2520shear%2520force%2520distributions%2520for%2520the%2520commercially%2520available%2520GelSight%2520Mini%2520sensor.%2520It%2520also%2520shows%2520potential%2520for%2520generalization%2520across%2520indenters%252C%2520sensors%2520of%2520the%2520same%2520type%252C%2520and%2520for%2520enabling%2520real-time%2520application.%2520The%2520codebase%252C%2520dataset%2520and%2520models%2520are%2520open-sourced%2520and%2520available%2520at%2520https%253A//feats-ai.github.io%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03315v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Force%20Distribution%20Estimation%20for%20the%20GelSight%20Mini%20Optical%20Tactile%20Sensor%20Based%20on%20Finite%20Element%20Analysis&entry.906535625=Erik%20Helmut%20and%20Luca%20Dziarski%20and%20Niklas%20Funk%20and%20Boris%20Belousov%20and%20Jan%20Peters&entry.1292438233=Contact-rich%20manipulation%20remains%20a%20major%20challenge%20in%20robotics.%20Optical%20tactile%20sensors%20like%20GelSight%20Mini%20offer%20a%20low-cost%20solution%20for%20contact%20sensing%20by%20capturing%20soft-body%20deformations%20of%20the%20silicone%20gel.%20However%2C%20accurately%20inferring%20shear%20and%20normal%20force%20distributions%20from%20these%20gel%20deformations%20has%20yet%20to%20be%20fully%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20machine%20learning%20approach%20using%20a%20U-net%20architecture%20to%20predict%20force%20distributions%20directly%20from%20the%20sensor%27s%20raw%20images.%20Our%20model%2C%20trained%20on%20force%20distributions%20inferred%20from%20%5Cac%7Bfea%7D%2C%20demonstrates%20promising%20accuracy%20in%20predicting%20normal%20and%20shear%20force%20distributions%20for%20the%20commercially%20available%20GelSight%20Mini%20sensor.%20It%20also%20shows%20potential%20for%20generalization%20across%20indenters%2C%20sensors%20of%20the%20same%20type%2C%20and%20for%20enabling%20real-time%20application.%20The%20codebase%2C%20dataset%20and%20models%20are%20open-sourced%20and%20available%20at%20https%3A//feats-ai.github.io%20.&entry.1838667208=http%3A//arxiv.org/abs/2411.03315v3&entry.124074799=Read"},
{"title": "A Hybrid Model-based and Data-based Approach Developed for a Prosthetic Hand Wrist", "author": "Shifa Sulaiman and Francesco Schetter and Mehul Menon and Fanny Ficuciello", "abstract": "The incorporation of advanced control algorithms into prosthetic hands significantly enhances their ability to replicate the intricate motions of a human hand. This work introduces a model-based controller that combines an Artificial Neural Network (ANN) approach with a Sliding Mode Controller (SMC) designed for a tendon-driven soft continuum wrist integrated into a prosthetic hand known as \"PRISMA HAND II\". Our research focuses on developing a controller that provides a fast dynamic response with reduced computational effort during wrist motions. The proposed controller consists of an ANN for computing bending angles together with an SMC to regulate tendon forces. Kinematic and dynamic models of the wrist are formulated using the Piece-wise Constant Curvature (PCC) hypothesis. The performance of the proposed controller is compared with other control strategies developed for the same wrist. Simulation studies and experimental validations of the fabricated wrist using the controller are included in the paper.", "link": "http://arxiv.org/abs/2601.08711v1", "date": "2026-01-13", "relevancy": 2.1134, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Model-based%20and%20Data-based%20Approach%20Developed%20for%20a%20Prosthetic%20Hand%20Wrist&body=Title%3A%20A%20Hybrid%20Model-based%20and%20Data-based%20Approach%20Developed%20for%20a%20Prosthetic%20Hand%20Wrist%0AAuthor%3A%20Shifa%20Sulaiman%20and%20Francesco%20Schetter%20and%20Mehul%20Menon%20and%20Fanny%20Ficuciello%0AAbstract%3A%20The%20incorporation%20of%20advanced%20control%20algorithms%20into%20prosthetic%20hands%20significantly%20enhances%20their%20ability%20to%20replicate%20the%20intricate%20motions%20of%20a%20human%20hand.%20This%20work%20introduces%20a%20model-based%20controller%20that%20combines%20an%20Artificial%20Neural%20Network%20%28ANN%29%20approach%20with%20a%20Sliding%20Mode%20Controller%20%28SMC%29%20designed%20for%20a%20tendon-driven%20soft%20continuum%20wrist%20integrated%20into%20a%20prosthetic%20hand%20known%20as%20%22PRISMA%20HAND%20II%22.%20Our%20research%20focuses%20on%20developing%20a%20controller%20that%20provides%20a%20fast%20dynamic%20response%20with%20reduced%20computational%20effort%20during%20wrist%20motions.%20The%20proposed%20controller%20consists%20of%20an%20ANN%20for%20computing%20bending%20angles%20together%20with%20an%20SMC%20to%20regulate%20tendon%20forces.%20Kinematic%20and%20dynamic%20models%20of%20the%20wrist%20are%20formulated%20using%20the%20Piece-wise%20Constant%20Curvature%20%28PCC%29%20hypothesis.%20The%20performance%20of%20the%20proposed%20controller%20is%20compared%20with%20other%20control%20strategies%20developed%20for%20the%20same%20wrist.%20Simulation%20studies%20and%20experimental%20validations%20of%20the%20fabricated%20wrist%20using%20the%20controller%20are%20included%20in%20the%20paper.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Model-based%2520and%2520Data-based%2520Approach%2520Developed%2520for%2520a%2520Prosthetic%2520Hand%2520Wrist%26entry.906535625%3DShifa%2520Sulaiman%2520and%2520Francesco%2520Schetter%2520and%2520Mehul%2520Menon%2520and%2520Fanny%2520Ficuciello%26entry.1292438233%3DThe%2520incorporation%2520of%2520advanced%2520control%2520algorithms%2520into%2520prosthetic%2520hands%2520significantly%2520enhances%2520their%2520ability%2520to%2520replicate%2520the%2520intricate%2520motions%2520of%2520a%2520human%2520hand.%2520This%2520work%2520introduces%2520a%2520model-based%2520controller%2520that%2520combines%2520an%2520Artificial%2520Neural%2520Network%2520%2528ANN%2529%2520approach%2520with%2520a%2520Sliding%2520Mode%2520Controller%2520%2528SMC%2529%2520designed%2520for%2520a%2520tendon-driven%2520soft%2520continuum%2520wrist%2520integrated%2520into%2520a%2520prosthetic%2520hand%2520known%2520as%2520%2522PRISMA%2520HAND%2520II%2522.%2520Our%2520research%2520focuses%2520on%2520developing%2520a%2520controller%2520that%2520provides%2520a%2520fast%2520dynamic%2520response%2520with%2520reduced%2520computational%2520effort%2520during%2520wrist%2520motions.%2520The%2520proposed%2520controller%2520consists%2520of%2520an%2520ANN%2520for%2520computing%2520bending%2520angles%2520together%2520with%2520an%2520SMC%2520to%2520regulate%2520tendon%2520forces.%2520Kinematic%2520and%2520dynamic%2520models%2520of%2520the%2520wrist%2520are%2520formulated%2520using%2520the%2520Piece-wise%2520Constant%2520Curvature%2520%2528PCC%2529%2520hypothesis.%2520The%2520performance%2520of%2520the%2520proposed%2520controller%2520is%2520compared%2520with%2520other%2520control%2520strategies%2520developed%2520for%2520the%2520same%2520wrist.%2520Simulation%2520studies%2520and%2520experimental%2520validations%2520of%2520the%2520fabricated%2520wrist%2520using%2520the%2520controller%2520are%2520included%2520in%2520the%2520paper.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Model-based%20and%20Data-based%20Approach%20Developed%20for%20a%20Prosthetic%20Hand%20Wrist&entry.906535625=Shifa%20Sulaiman%20and%20Francesco%20Schetter%20and%20Mehul%20Menon%20and%20Fanny%20Ficuciello&entry.1292438233=The%20incorporation%20of%20advanced%20control%20algorithms%20into%20prosthetic%20hands%20significantly%20enhances%20their%20ability%20to%20replicate%20the%20intricate%20motions%20of%20a%20human%20hand.%20This%20work%20introduces%20a%20model-based%20controller%20that%20combines%20an%20Artificial%20Neural%20Network%20%28ANN%29%20approach%20with%20a%20Sliding%20Mode%20Controller%20%28SMC%29%20designed%20for%20a%20tendon-driven%20soft%20continuum%20wrist%20integrated%20into%20a%20prosthetic%20hand%20known%20as%20%22PRISMA%20HAND%20II%22.%20Our%20research%20focuses%20on%20developing%20a%20controller%20that%20provides%20a%20fast%20dynamic%20response%20with%20reduced%20computational%20effort%20during%20wrist%20motions.%20The%20proposed%20controller%20consists%20of%20an%20ANN%20for%20computing%20bending%20angles%20together%20with%20an%20SMC%20to%20regulate%20tendon%20forces.%20Kinematic%20and%20dynamic%20models%20of%20the%20wrist%20are%20formulated%20using%20the%20Piece-wise%20Constant%20Curvature%20%28PCC%29%20hypothesis.%20The%20performance%20of%20the%20proposed%20controller%20is%20compared%20with%20other%20control%20strategies%20developed%20for%20the%20same%20wrist.%20Simulation%20studies%20and%20experimental%20validations%20of%20the%20fabricated%20wrist%20using%20the%20controller%20are%20included%20in%20the%20paper.&entry.1838667208=http%3A//arxiv.org/abs/2601.08711v1&entry.124074799=Read"},
{"title": "STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays", "author": "Qiuyu Tian and Yiding Li and Fengyi Chen and Zequn Liu and Youyong Kong and Fan Guo and Yuyao Li and Jinjing Shen and Zhijing Xie and Yiyun Luo and Xin Zhang", "abstract": "Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.", "link": "http://arxiv.org/abs/2601.08510v1", "date": "2026-01-13", "relevancy": 2.1052, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAGE%3A%20A%20Benchmark%20for%20Knowledge%20Graph%20Construction%2C%20Question%20Answering%2C%20and%20In-Script%20Role-Playing%20over%20Movie%20Screenplays&body=Title%3A%20STAGE%3A%20A%20Benchmark%20for%20Knowledge%20Graph%20Construction%2C%20Question%20Answering%2C%20and%20In-Script%20Role-Playing%20over%20Movie%20Screenplays%0AAuthor%3A%20Qiuyu%20Tian%20and%20Yiding%20Li%20and%20Fengyi%20Chen%20and%20Zequn%20Liu%20and%20Youyong%20Kong%20and%20Fan%20Guo%20and%20Yuyao%20Li%20and%20Jinjing%20Shen%20and%20Zhijing%20Xie%20and%20Yiyun%20Luo%20and%20Xin%20Zhang%0AAbstract%3A%20Movie%20screenplays%20are%20rich%20long-form%20narratives%20that%20interleave%20complex%20character%20relationships%2C%20temporally%20ordered%20events%2C%20and%20dialogue-driven%20interactions.%20While%20prior%20benchmarks%20target%20individual%20subtasks%20such%20as%20question%20answering%20or%20dialogue%20generation%2C%20they%20rarely%20evaluate%20whether%20models%20can%20construct%20a%20coherent%20story%20world%20and%20use%20it%20consistently%20across%20multiple%20forms%20of%20reasoning%20and%20generation.%20We%20introduce%20STAGE%20%28Screenplay%20Text%2C%20Agents%2C%20Graphs%20and%20Evaluation%29%2C%20a%20unified%20benchmark%20for%20narrative%20understanding%20over%20full-length%20movie%20screenplays.%20STAGE%20defines%20four%20tasks%3A%20knowledge%20graph%20construction%2C%20scene-level%20event%20summarization%2C%20long-context%20screenplay%20question%20answering%2C%20and%20in-script%20character%20role-playing%2C%20all%20grounded%20in%20a%20shared%20narrative%20world%20representation.%20The%20benchmark%20provides%20cleaned%20scripts%2C%20curated%20knowledge%20graphs%2C%20and%20event-%20and%20character-centric%20annotations%20for%20150%20films%20across%20English%20and%20Chinese%2C%20enabling%20holistic%20evaluation%20of%20models%27%20abilities%20to%20build%20world%20representations%2C%20abstract%20and%20verify%20narrative%20events%2C%20reason%20over%20long%20narratives%2C%20and%20generate%20character-consistent%20responses.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAGE%253A%2520A%2520Benchmark%2520for%2520Knowledge%2520Graph%2520Construction%252C%2520Question%2520Answering%252C%2520and%2520In-Script%2520Role-Playing%2520over%2520Movie%2520Screenplays%26entry.906535625%3DQiuyu%2520Tian%2520and%2520Yiding%2520Li%2520and%2520Fengyi%2520Chen%2520and%2520Zequn%2520Liu%2520and%2520Youyong%2520Kong%2520and%2520Fan%2520Guo%2520and%2520Yuyao%2520Li%2520and%2520Jinjing%2520Shen%2520and%2520Zhijing%2520Xie%2520and%2520Yiyun%2520Luo%2520and%2520Xin%2520Zhang%26entry.1292438233%3DMovie%2520screenplays%2520are%2520rich%2520long-form%2520narratives%2520that%2520interleave%2520complex%2520character%2520relationships%252C%2520temporally%2520ordered%2520events%252C%2520and%2520dialogue-driven%2520interactions.%2520While%2520prior%2520benchmarks%2520target%2520individual%2520subtasks%2520such%2520as%2520question%2520answering%2520or%2520dialogue%2520generation%252C%2520they%2520rarely%2520evaluate%2520whether%2520models%2520can%2520construct%2520a%2520coherent%2520story%2520world%2520and%2520use%2520it%2520consistently%2520across%2520multiple%2520forms%2520of%2520reasoning%2520and%2520generation.%2520We%2520introduce%2520STAGE%2520%2528Screenplay%2520Text%252C%2520Agents%252C%2520Graphs%2520and%2520Evaluation%2529%252C%2520a%2520unified%2520benchmark%2520for%2520narrative%2520understanding%2520over%2520full-length%2520movie%2520screenplays.%2520STAGE%2520defines%2520four%2520tasks%253A%2520knowledge%2520graph%2520construction%252C%2520scene-level%2520event%2520summarization%252C%2520long-context%2520screenplay%2520question%2520answering%252C%2520and%2520in-script%2520character%2520role-playing%252C%2520all%2520grounded%2520in%2520a%2520shared%2520narrative%2520world%2520representation.%2520The%2520benchmark%2520provides%2520cleaned%2520scripts%252C%2520curated%2520knowledge%2520graphs%252C%2520and%2520event-%2520and%2520character-centric%2520annotations%2520for%2520150%2520films%2520across%2520English%2520and%2520Chinese%252C%2520enabling%2520holistic%2520evaluation%2520of%2520models%2527%2520abilities%2520to%2520build%2520world%2520representations%252C%2520abstract%2520and%2520verify%2520narrative%2520events%252C%2520reason%2520over%2520long%2520narratives%252C%2520and%2520generate%2520character-consistent%2520responses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAGE%3A%20A%20Benchmark%20for%20Knowledge%20Graph%20Construction%2C%20Question%20Answering%2C%20and%20In-Script%20Role-Playing%20over%20Movie%20Screenplays&entry.906535625=Qiuyu%20Tian%20and%20Yiding%20Li%20and%20Fengyi%20Chen%20and%20Zequn%20Liu%20and%20Youyong%20Kong%20and%20Fan%20Guo%20and%20Yuyao%20Li%20and%20Jinjing%20Shen%20and%20Zhijing%20Xie%20and%20Yiyun%20Luo%20and%20Xin%20Zhang&entry.1292438233=Movie%20screenplays%20are%20rich%20long-form%20narratives%20that%20interleave%20complex%20character%20relationships%2C%20temporally%20ordered%20events%2C%20and%20dialogue-driven%20interactions.%20While%20prior%20benchmarks%20target%20individual%20subtasks%20such%20as%20question%20answering%20or%20dialogue%20generation%2C%20they%20rarely%20evaluate%20whether%20models%20can%20construct%20a%20coherent%20story%20world%20and%20use%20it%20consistently%20across%20multiple%20forms%20of%20reasoning%20and%20generation.%20We%20introduce%20STAGE%20%28Screenplay%20Text%2C%20Agents%2C%20Graphs%20and%20Evaluation%29%2C%20a%20unified%20benchmark%20for%20narrative%20understanding%20over%20full-length%20movie%20screenplays.%20STAGE%20defines%20four%20tasks%3A%20knowledge%20graph%20construction%2C%20scene-level%20event%20summarization%2C%20long-context%20screenplay%20question%20answering%2C%20and%20in-script%20character%20role-playing%2C%20all%20grounded%20in%20a%20shared%20narrative%20world%20representation.%20The%20benchmark%20provides%20cleaned%20scripts%2C%20curated%20knowledge%20graphs%2C%20and%20event-%20and%20character-centric%20annotations%20for%20150%20films%20across%20English%20and%20Chinese%2C%20enabling%20holistic%20evaluation%20of%20models%27%20abilities%20to%20build%20world%20representations%2C%20abstract%20and%20verify%20narrative%20events%2C%20reason%20over%20long%20narratives%2C%20and%20generate%20character-consistent%20responses.&entry.1838667208=http%3A//arxiv.org/abs/2601.08510v1&entry.124074799=Read"},
{"title": "Attacks on fairness in Federated Learning", "author": "Joseph Rance and Filip Svoboda", "abstract": "Federated Learning is an important emerging distributed training paradigm that keeps data private on clients. It is now well understood that by controlling only a small subset of FL clients, it is possible to introduce a backdoor to a federated learning model, in the presence of certain attributes. In this paper, we present a new type of attack that compromises the fairness of the trained model. Fairness is understood to be the attribute-level performance distribution of a trained model. It is particularly salient in domains where, for example, skewed accuracy discrimination between subpopulations could have disastrous consequences. We find that by employing a threat model similar to that of a backdoor attack, an attacker is able to influence the aggregated model to have an unfair performance distribution between any given set of attributes. Furthermore, we find that this attack is possible by controlling only a single client. While combating naturally induced unfairness in FL has previously been discussed in depth, its artificially induced kind has been neglected. We show that defending against attacks on fairness should be a critical consideration in any situation where unfairness in a trained model could benefit a user who participated in its training.", "link": "http://arxiv.org/abs/2311.12715v3", "date": "2026-01-13", "relevancy": 2.1052, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4336}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4239}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attacks%20on%20fairness%20in%20Federated%20Learning&body=Title%3A%20Attacks%20on%20fairness%20in%20Federated%20Learning%0AAuthor%3A%20Joseph%20Rance%20and%20Filip%20Svoboda%0AAbstract%3A%20Federated%20Learning%20is%20an%20important%20emerging%20distributed%20training%20paradigm%20that%20keeps%20data%20private%20on%20clients.%20It%20is%20now%20well%20understood%20that%20by%20controlling%20only%20a%20small%20subset%20of%20FL%20clients%2C%20it%20is%20possible%20to%20introduce%20a%20backdoor%20to%20a%20federated%20learning%20model%2C%20in%20the%20presence%20of%20certain%20attributes.%20In%20this%20paper%2C%20we%20present%20a%20new%20type%20of%20attack%20that%20compromises%20the%20fairness%20of%20the%20trained%20model.%20Fairness%20is%20understood%20to%20be%20the%20attribute-level%20performance%20distribution%20of%20a%20trained%20model.%20It%20is%20particularly%20salient%20in%20domains%20where%2C%20for%20example%2C%20skewed%20accuracy%20discrimination%20between%20subpopulations%20could%20have%20disastrous%20consequences.%20We%20find%20that%20by%20employing%20a%20threat%20model%20similar%20to%20that%20of%20a%20backdoor%20attack%2C%20an%20attacker%20is%20able%20to%20influence%20the%20aggregated%20model%20to%20have%20an%20unfair%20performance%20distribution%20between%20any%20given%20set%20of%20attributes.%20Furthermore%2C%20we%20find%20that%20this%20attack%20is%20possible%20by%20controlling%20only%20a%20single%20client.%20While%20combating%20naturally%20induced%20unfairness%20in%20FL%20has%20previously%20been%20discussed%20in%20depth%2C%20its%20artificially%20induced%20kind%20has%20been%20neglected.%20We%20show%20that%20defending%20against%20attacks%20on%20fairness%20should%20be%20a%20critical%20consideration%20in%20any%20situation%20where%20unfairness%20in%20a%20trained%20model%20could%20benefit%20a%20user%20who%20participated%20in%20its%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2311.12715v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttacks%2520on%2520fairness%2520in%2520Federated%2520Learning%26entry.906535625%3DJoseph%2520Rance%2520and%2520Filip%2520Svoboda%26entry.1292438233%3DFederated%2520Learning%2520is%2520an%2520important%2520emerging%2520distributed%2520training%2520paradigm%2520that%2520keeps%2520data%2520private%2520on%2520clients.%2520It%2520is%2520now%2520well%2520understood%2520that%2520by%2520controlling%2520only%2520a%2520small%2520subset%2520of%2520FL%2520clients%252C%2520it%2520is%2520possible%2520to%2520introduce%2520a%2520backdoor%2520to%2520a%2520federated%2520learning%2520model%252C%2520in%2520the%2520presence%2520of%2520certain%2520attributes.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520type%2520of%2520attack%2520that%2520compromises%2520the%2520fairness%2520of%2520the%2520trained%2520model.%2520Fairness%2520is%2520understood%2520to%2520be%2520the%2520attribute-level%2520performance%2520distribution%2520of%2520a%2520trained%2520model.%2520It%2520is%2520particularly%2520salient%2520in%2520domains%2520where%252C%2520for%2520example%252C%2520skewed%2520accuracy%2520discrimination%2520between%2520subpopulations%2520could%2520have%2520disastrous%2520consequences.%2520We%2520find%2520that%2520by%2520employing%2520a%2520threat%2520model%2520similar%2520to%2520that%2520of%2520a%2520backdoor%2520attack%252C%2520an%2520attacker%2520is%2520able%2520to%2520influence%2520the%2520aggregated%2520model%2520to%2520have%2520an%2520unfair%2520performance%2520distribution%2520between%2520any%2520given%2520set%2520of%2520attributes.%2520Furthermore%252C%2520we%2520find%2520that%2520this%2520attack%2520is%2520possible%2520by%2520controlling%2520only%2520a%2520single%2520client.%2520While%2520combating%2520naturally%2520induced%2520unfairness%2520in%2520FL%2520has%2520previously%2520been%2520discussed%2520in%2520depth%252C%2520its%2520artificially%2520induced%2520kind%2520has%2520been%2520neglected.%2520We%2520show%2520that%2520defending%2520against%2520attacks%2520on%2520fairness%2520should%2520be%2520a%2520critical%2520consideration%2520in%2520any%2520situation%2520where%2520unfairness%2520in%2520a%2520trained%2520model%2520could%2520benefit%2520a%2520user%2520who%2520participated%2520in%2520its%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12715v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacks%20on%20fairness%20in%20Federated%20Learning&entry.906535625=Joseph%20Rance%20and%20Filip%20Svoboda&entry.1292438233=Federated%20Learning%20is%20an%20important%20emerging%20distributed%20training%20paradigm%20that%20keeps%20data%20private%20on%20clients.%20It%20is%20now%20well%20understood%20that%20by%20controlling%20only%20a%20small%20subset%20of%20FL%20clients%2C%20it%20is%20possible%20to%20introduce%20a%20backdoor%20to%20a%20federated%20learning%20model%2C%20in%20the%20presence%20of%20certain%20attributes.%20In%20this%20paper%2C%20we%20present%20a%20new%20type%20of%20attack%20that%20compromises%20the%20fairness%20of%20the%20trained%20model.%20Fairness%20is%20understood%20to%20be%20the%20attribute-level%20performance%20distribution%20of%20a%20trained%20model.%20It%20is%20particularly%20salient%20in%20domains%20where%2C%20for%20example%2C%20skewed%20accuracy%20discrimination%20between%20subpopulations%20could%20have%20disastrous%20consequences.%20We%20find%20that%20by%20employing%20a%20threat%20model%20similar%20to%20that%20of%20a%20backdoor%20attack%2C%20an%20attacker%20is%20able%20to%20influence%20the%20aggregated%20model%20to%20have%20an%20unfair%20performance%20distribution%20between%20any%20given%20set%20of%20attributes.%20Furthermore%2C%20we%20find%20that%20this%20attack%20is%20possible%20by%20controlling%20only%20a%20single%20client.%20While%20combating%20naturally%20induced%20unfairness%20in%20FL%20has%20previously%20been%20discussed%20in%20depth%2C%20its%20artificially%20induced%20kind%20has%20been%20neglected.%20We%20show%20that%20defending%20against%20attacks%20on%20fairness%20should%20be%20a%20critical%20consideration%20in%20any%20situation%20where%20unfairness%20in%20a%20trained%20model%20could%20benefit%20a%20user%20who%20participated%20in%20its%20training.&entry.1838667208=http%3A//arxiv.org/abs/2311.12715v3&entry.124074799=Read"},
{"title": "Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains", "author": "Shizheng Wen and Arsh Kumbhat and Levi Lingsch and Sepehr Mousavi and Yizhou Zhao and Praveen Chandrashekar and Siddhartha Mishra", "abstract": "The very challenging task of learning solution operators of PDEs on arbitrary domains accurately and efficiently is of vital importance to engineering and industrial simulations. Despite the existence of many operator learning algorithms to approximate such PDEs, we find that accurate models are not necessarily computationally efficient and vice versa. We address this issue by proposing a geometry aware operator transformer (GAOT) for learning PDEs on arbitrary domains. GAOT combines novel multiscale attentional graph neural operator encoders and decoders, together with geometry embeddings and (vision) transformer processors to accurately map information about the domain and the inputs into a robust approximation of the PDE solution. Multiple innovations in the implementation of GAOT also ensure computational efficiency and scalability. We demonstrate this significant gain in both accuracy and efficiency of GAOT over several baselines on a large number of learning tasks from a diverse set of PDEs, including achieving state of the art performance on three large scale three-dimensional industrial CFD datasets.", "link": "http://arxiv.org/abs/2505.18781v4", "date": "2026-01-13", "relevancy": 2.0971, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5577}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5273}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Aware%20Operator%20Transformer%20as%20an%20Efficient%20and%20Accurate%20Neural%20Surrogate%20for%20PDEs%20on%20Arbitrary%20Domains&body=Title%3A%20Geometry%20Aware%20Operator%20Transformer%20as%20an%20Efficient%20and%20Accurate%20Neural%20Surrogate%20for%20PDEs%20on%20Arbitrary%20Domains%0AAuthor%3A%20Shizheng%20Wen%20and%20Arsh%20Kumbhat%20and%20Levi%20Lingsch%20and%20Sepehr%20Mousavi%20and%20Yizhou%20Zhao%20and%20Praveen%20Chandrashekar%20and%20Siddhartha%20Mishra%0AAbstract%3A%20The%20very%20challenging%20task%20of%20learning%20solution%20operators%20of%20PDEs%20on%20arbitrary%20domains%20accurately%20and%20efficiently%20is%20of%20vital%20importance%20to%20engineering%20and%20industrial%20simulations.%20Despite%20the%20existence%20of%20many%20operator%20learning%20algorithms%20to%20approximate%20such%20PDEs%2C%20we%20find%20that%20accurate%20models%20are%20not%20necessarily%20computationally%20efficient%20and%20vice%20versa.%20We%20address%20this%20issue%20by%20proposing%20a%20geometry%20aware%20operator%20transformer%20%28GAOT%29%20for%20learning%20PDEs%20on%20arbitrary%20domains.%20GAOT%20combines%20novel%20multiscale%20attentional%20graph%20neural%20operator%20encoders%20and%20decoders%2C%20together%20with%20geometry%20embeddings%20and%20%28vision%29%20transformer%20processors%20to%20accurately%20map%20information%20about%20the%20domain%20and%20the%20inputs%20into%20a%20robust%20approximation%20of%20the%20PDE%20solution.%20Multiple%20innovations%20in%20the%20implementation%20of%20GAOT%20also%20ensure%20computational%20efficiency%20and%20scalability.%20We%20demonstrate%20this%20significant%20gain%20in%20both%20accuracy%20and%20efficiency%20of%20GAOT%20over%20several%20baselines%20on%20a%20large%20number%20of%20learning%20tasks%20from%20a%20diverse%20set%20of%20PDEs%2C%20including%20achieving%20state%20of%20the%20art%20performance%20on%20three%20large%20scale%20three-dimensional%20industrial%20CFD%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18781v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Aware%2520Operator%2520Transformer%2520as%2520an%2520Efficient%2520and%2520Accurate%2520Neural%2520Surrogate%2520for%2520PDEs%2520on%2520Arbitrary%2520Domains%26entry.906535625%3DShizheng%2520Wen%2520and%2520Arsh%2520Kumbhat%2520and%2520Levi%2520Lingsch%2520and%2520Sepehr%2520Mousavi%2520and%2520Yizhou%2520Zhao%2520and%2520Praveen%2520Chandrashekar%2520and%2520Siddhartha%2520Mishra%26entry.1292438233%3DThe%2520very%2520challenging%2520task%2520of%2520learning%2520solution%2520operators%2520of%2520PDEs%2520on%2520arbitrary%2520domains%2520accurately%2520and%2520efficiently%2520is%2520of%2520vital%2520importance%2520to%2520engineering%2520and%2520industrial%2520simulations.%2520Despite%2520the%2520existence%2520of%2520many%2520operator%2520learning%2520algorithms%2520to%2520approximate%2520such%2520PDEs%252C%2520we%2520find%2520that%2520accurate%2520models%2520are%2520not%2520necessarily%2520computationally%2520efficient%2520and%2520vice%2520versa.%2520We%2520address%2520this%2520issue%2520by%2520proposing%2520a%2520geometry%2520aware%2520operator%2520transformer%2520%2528GAOT%2529%2520for%2520learning%2520PDEs%2520on%2520arbitrary%2520domains.%2520GAOT%2520combines%2520novel%2520multiscale%2520attentional%2520graph%2520neural%2520operator%2520encoders%2520and%2520decoders%252C%2520together%2520with%2520geometry%2520embeddings%2520and%2520%2528vision%2529%2520transformer%2520processors%2520to%2520accurately%2520map%2520information%2520about%2520the%2520domain%2520and%2520the%2520inputs%2520into%2520a%2520robust%2520approximation%2520of%2520the%2520PDE%2520solution.%2520Multiple%2520innovations%2520in%2520the%2520implementation%2520of%2520GAOT%2520also%2520ensure%2520computational%2520efficiency%2520and%2520scalability.%2520We%2520demonstrate%2520this%2520significant%2520gain%2520in%2520both%2520accuracy%2520and%2520efficiency%2520of%2520GAOT%2520over%2520several%2520baselines%2520on%2520a%2520large%2520number%2520of%2520learning%2520tasks%2520from%2520a%2520diverse%2520set%2520of%2520PDEs%252C%2520including%2520achieving%2520state%2520of%2520the%2520art%2520performance%2520on%2520three%2520large%2520scale%2520three-dimensional%2520industrial%2520CFD%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18781v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Aware%20Operator%20Transformer%20as%20an%20Efficient%20and%20Accurate%20Neural%20Surrogate%20for%20PDEs%20on%20Arbitrary%20Domains&entry.906535625=Shizheng%20Wen%20and%20Arsh%20Kumbhat%20and%20Levi%20Lingsch%20and%20Sepehr%20Mousavi%20and%20Yizhou%20Zhao%20and%20Praveen%20Chandrashekar%20and%20Siddhartha%20Mishra&entry.1292438233=The%20very%20challenging%20task%20of%20learning%20solution%20operators%20of%20PDEs%20on%20arbitrary%20domains%20accurately%20and%20efficiently%20is%20of%20vital%20importance%20to%20engineering%20and%20industrial%20simulations.%20Despite%20the%20existence%20of%20many%20operator%20learning%20algorithms%20to%20approximate%20such%20PDEs%2C%20we%20find%20that%20accurate%20models%20are%20not%20necessarily%20computationally%20efficient%20and%20vice%20versa.%20We%20address%20this%20issue%20by%20proposing%20a%20geometry%20aware%20operator%20transformer%20%28GAOT%29%20for%20learning%20PDEs%20on%20arbitrary%20domains.%20GAOT%20combines%20novel%20multiscale%20attentional%20graph%20neural%20operator%20encoders%20and%20decoders%2C%20together%20with%20geometry%20embeddings%20and%20%28vision%29%20transformer%20processors%20to%20accurately%20map%20information%20about%20the%20domain%20and%20the%20inputs%20into%20a%20robust%20approximation%20of%20the%20PDE%20solution.%20Multiple%20innovations%20in%20the%20implementation%20of%20GAOT%20also%20ensure%20computational%20efficiency%20and%20scalability.%20We%20demonstrate%20this%20significant%20gain%20in%20both%20accuracy%20and%20efficiency%20of%20GAOT%20over%20several%20baselines%20on%20a%20large%20number%20of%20learning%20tasks%20from%20a%20diverse%20set%20of%20PDEs%2C%20including%20achieving%20state%20of%20the%20art%20performance%20on%20three%20large%20scale%20three-dimensional%20industrial%20CFD%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2505.18781v4&entry.124074799=Read"},
{"title": "Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime", "author": "Binh Duong Nguyen and Stefan Sandfeld", "abstract": "Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.", "link": "http://arxiv.org/abs/2601.08404v1", "date": "2026-01-13", "relevancy": 2.0888, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5283}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-distribution%20generalization%20of%20deep-learning%20surrogates%20for%202D%20PDE-generated%20dynamics%20in%20the%20small-data%20regime&body=Title%3A%20Out-of-distribution%20generalization%20of%20deep-learning%20surrogates%20for%202D%20PDE-generated%20dynamics%20in%20the%20small-data%20regime%0AAuthor%3A%20Binh%20Duong%20Nguyen%20and%20Stefan%20Sandfeld%0AAbstract%3A%20Partial%20differential%20equations%20%28PDEs%29%20are%20a%20central%20tool%20for%20modeling%20the%20dynamics%20of%20physical%2C%20engineering%2C%20and%20materials%20systems%2C%20but%20high-fidelity%20simulations%20are%20often%20computationally%20expensive.%20At%20the%20same%20time%2C%20many%20scientific%20applications%20can%20be%20viewed%20as%20the%20evolution%20of%20spatially%20distributed%20fields%2C%20making%20data-driven%20forecasting%20of%20such%20fields%20a%20core%20task%20in%20scientific%20machine%20learning.%20In%20this%20work%20we%20study%20autoregressive%20deep-learning%20surrogates%20for%20two-dimensional%20PDE%20dynamics%20on%20periodic%20domains%2C%20focusing%20on%20generalization%20to%20out-of-distribution%20initial%20conditions%20within%20a%20fixed%20PDE%20and%20parameter%20regime%20and%20on%20strict%20small-data%20settings%20with%20at%20most%20%24%5Cmathcal%7BO%7D%2810%5E2%29%24%20simulated%20trajectories%20per%20system.%20We%20introduce%20a%20multi-channel%20U-Net%20%5B...%5D%2C%20evaluate%20it%20on%20five%20qualitatively%20different%20PDE%20families%20and%20compare%20it%20to%20ViT%2C%20AFNO%2C%20PDE-Transformer%2C%20and%20KAN-UNet%20under%20a%20common%20training%20setup.%20Across%20all%20datasets%2C%20me-UNet%20matches%20or%20outperforms%20these%20more%20complex%20architectures%20in%20terms%20of%20field-space%20error%2C%20spectral%20similarity%2C%20and%20physics-based%20metrics%20for%20in-distribution%20rollouts%2C%20while%20requiring%20substantially%20less%20training%20time.%20It%20also%20generalizes%20qualitatively%20to%20unseen%20initial%20conditions%20with%20as%20few%20as%20%24%5Capprox%2020%24%20training%20simulations.%20A%20data-efficiency%20study%20and%20Grad-CAM%20analysis%20further%20suggest%20that%2C%20in%20small-data%20periodic%202D%20PDE%20settings%2C%20convolutional%20architectures%20with%20inductive%20biases%20aligned%20to%20locality%20and%20periodic%20boundary%20conditions%20remain%20strong%20contenders%20for%20accurate%20and%20moderately%20out-of-distribution-robust%20surrogate%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-distribution%2520generalization%2520of%2520deep-learning%2520surrogates%2520for%25202D%2520PDE-generated%2520dynamics%2520in%2520the%2520small-data%2520regime%26entry.906535625%3DBinh%2520Duong%2520Nguyen%2520and%2520Stefan%2520Sandfeld%26entry.1292438233%3DPartial%2520differential%2520equations%2520%2528PDEs%2529%2520are%2520a%2520central%2520tool%2520for%2520modeling%2520the%2520dynamics%2520of%2520physical%252C%2520engineering%252C%2520and%2520materials%2520systems%252C%2520but%2520high-fidelity%2520simulations%2520are%2520often%2520computationally%2520expensive.%2520At%2520the%2520same%2520time%252C%2520many%2520scientific%2520applications%2520can%2520be%2520viewed%2520as%2520the%2520evolution%2520of%2520spatially%2520distributed%2520fields%252C%2520making%2520data-driven%2520forecasting%2520of%2520such%2520fields%2520a%2520core%2520task%2520in%2520scientific%2520machine%2520learning.%2520In%2520this%2520work%2520we%2520study%2520autoregressive%2520deep-learning%2520surrogates%2520for%2520two-dimensional%2520PDE%2520dynamics%2520on%2520periodic%2520domains%252C%2520focusing%2520on%2520generalization%2520to%2520out-of-distribution%2520initial%2520conditions%2520within%2520a%2520fixed%2520PDE%2520and%2520parameter%2520regime%2520and%2520on%2520strict%2520small-data%2520settings%2520with%2520at%2520most%2520%2524%255Cmathcal%257BO%257D%252810%255E2%2529%2524%2520simulated%2520trajectories%2520per%2520system.%2520We%2520introduce%2520a%2520multi-channel%2520U-Net%2520%255B...%255D%252C%2520evaluate%2520it%2520on%2520five%2520qualitatively%2520different%2520PDE%2520families%2520and%2520compare%2520it%2520to%2520ViT%252C%2520AFNO%252C%2520PDE-Transformer%252C%2520and%2520KAN-UNet%2520under%2520a%2520common%2520training%2520setup.%2520Across%2520all%2520datasets%252C%2520me-UNet%2520matches%2520or%2520outperforms%2520these%2520more%2520complex%2520architectures%2520in%2520terms%2520of%2520field-space%2520error%252C%2520spectral%2520similarity%252C%2520and%2520physics-based%2520metrics%2520for%2520in-distribution%2520rollouts%252C%2520while%2520requiring%2520substantially%2520less%2520training%2520time.%2520It%2520also%2520generalizes%2520qualitatively%2520to%2520unseen%2520initial%2520conditions%2520with%2520as%2520few%2520as%2520%2524%255Capprox%252020%2524%2520training%2520simulations.%2520A%2520data-efficiency%2520study%2520and%2520Grad-CAM%2520analysis%2520further%2520suggest%2520that%252C%2520in%2520small-data%2520periodic%25202D%2520PDE%2520settings%252C%2520convolutional%2520architectures%2520with%2520inductive%2520biases%2520aligned%2520to%2520locality%2520and%2520periodic%2520boundary%2520conditions%2520remain%2520strong%2520contenders%2520for%2520accurate%2520and%2520moderately%2520out-of-distribution-robust%2520surrogate%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-distribution%20generalization%20of%20deep-learning%20surrogates%20for%202D%20PDE-generated%20dynamics%20in%20the%20small-data%20regime&entry.906535625=Binh%20Duong%20Nguyen%20and%20Stefan%20Sandfeld&entry.1292438233=Partial%20differential%20equations%20%28PDEs%29%20are%20a%20central%20tool%20for%20modeling%20the%20dynamics%20of%20physical%2C%20engineering%2C%20and%20materials%20systems%2C%20but%20high-fidelity%20simulations%20are%20often%20computationally%20expensive.%20At%20the%20same%20time%2C%20many%20scientific%20applications%20can%20be%20viewed%20as%20the%20evolution%20of%20spatially%20distributed%20fields%2C%20making%20data-driven%20forecasting%20of%20such%20fields%20a%20core%20task%20in%20scientific%20machine%20learning.%20In%20this%20work%20we%20study%20autoregressive%20deep-learning%20surrogates%20for%20two-dimensional%20PDE%20dynamics%20on%20periodic%20domains%2C%20focusing%20on%20generalization%20to%20out-of-distribution%20initial%20conditions%20within%20a%20fixed%20PDE%20and%20parameter%20regime%20and%20on%20strict%20small-data%20settings%20with%20at%20most%20%24%5Cmathcal%7BO%7D%2810%5E2%29%24%20simulated%20trajectories%20per%20system.%20We%20introduce%20a%20multi-channel%20U-Net%20%5B...%5D%2C%20evaluate%20it%20on%20five%20qualitatively%20different%20PDE%20families%20and%20compare%20it%20to%20ViT%2C%20AFNO%2C%20PDE-Transformer%2C%20and%20KAN-UNet%20under%20a%20common%20training%20setup.%20Across%20all%20datasets%2C%20me-UNet%20matches%20or%20outperforms%20these%20more%20complex%20architectures%20in%20terms%20of%20field-space%20error%2C%20spectral%20similarity%2C%20and%20physics-based%20metrics%20for%20in-distribution%20rollouts%2C%20while%20requiring%20substantially%20less%20training%20time.%20It%20also%20generalizes%20qualitatively%20to%20unseen%20initial%20conditions%20with%20as%20few%20as%20%24%5Capprox%2020%24%20training%20simulations.%20A%20data-efficiency%20study%20and%20Grad-CAM%20analysis%20further%20suggest%20that%2C%20in%20small-data%20periodic%202D%20PDE%20settings%2C%20convolutional%20architectures%20with%20inductive%20biases%20aligned%20to%20locality%20and%20periodic%20boundary%20conditions%20remain%20strong%20contenders%20for%20accurate%20and%20moderately%20out-of-distribution-robust%20surrogate%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2601.08404v1&entry.124074799=Read"},
{"title": "Accelerated Methods with Complexity Separation Under Data Similarity for Federated Learning Problems", "author": "Dmitry Bylinkin and Sergey Skorik and Dmitriy Bystrov and Leonid Berezin and Aram Avetisyan and Aleksandr Beznosikov", "abstract": "Heterogeneity within data distribution poses a challenge in many modern federated learning tasks. We formalize it as an optimization problem involving a computationally heavy composite under data similarity. By employing different sets of assumptions, we present several approaches to develop communication-efficient methods. An optimal algorithm is proposed for the convex case. The constructed theory is validated through a series of experiments across various problems.", "link": "http://arxiv.org/abs/2601.08614v1", "date": "2026-01-13", "relevancy": 2.0875, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4263}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4187}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Methods%20with%20Complexity%20Separation%20Under%20Data%20Similarity%20for%20Federated%20Learning%20Problems&body=Title%3A%20Accelerated%20Methods%20with%20Complexity%20Separation%20Under%20Data%20Similarity%20for%20Federated%20Learning%20Problems%0AAuthor%3A%20Dmitry%20Bylinkin%20and%20Sergey%20Skorik%20and%20Dmitriy%20Bystrov%20and%20Leonid%20Berezin%20and%20Aram%20Avetisyan%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20Heterogeneity%20within%20data%20distribution%20poses%20a%20challenge%20in%20many%20modern%20federated%20learning%20tasks.%20We%20formalize%20it%20as%20an%20optimization%20problem%20involving%20a%20computationally%20heavy%20composite%20under%20data%20similarity.%20By%20employing%20different%20sets%20of%20assumptions%2C%20we%20present%20several%20approaches%20to%20develop%20communication-efficient%20methods.%20An%20optimal%20algorithm%20is%20proposed%20for%20the%20convex%20case.%20The%20constructed%20theory%20is%20validated%20through%20a%20series%20of%20experiments%20across%20various%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Methods%2520with%2520Complexity%2520Separation%2520Under%2520Data%2520Similarity%2520for%2520Federated%2520Learning%2520Problems%26entry.906535625%3DDmitry%2520Bylinkin%2520and%2520Sergey%2520Skorik%2520and%2520Dmitriy%2520Bystrov%2520and%2520Leonid%2520Berezin%2520and%2520Aram%2520Avetisyan%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3DHeterogeneity%2520within%2520data%2520distribution%2520poses%2520a%2520challenge%2520in%2520many%2520modern%2520federated%2520learning%2520tasks.%2520We%2520formalize%2520it%2520as%2520an%2520optimization%2520problem%2520involving%2520a%2520computationally%2520heavy%2520composite%2520under%2520data%2520similarity.%2520By%2520employing%2520different%2520sets%2520of%2520assumptions%252C%2520we%2520present%2520several%2520approaches%2520to%2520develop%2520communication-efficient%2520methods.%2520An%2520optimal%2520algorithm%2520is%2520proposed%2520for%2520the%2520convex%2520case.%2520The%2520constructed%2520theory%2520is%2520validated%2520through%2520a%2520series%2520of%2520experiments%2520across%2520various%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Methods%20with%20Complexity%20Separation%20Under%20Data%20Similarity%20for%20Federated%20Learning%20Problems&entry.906535625=Dmitry%20Bylinkin%20and%20Sergey%20Skorik%20and%20Dmitriy%20Bystrov%20and%20Leonid%20Berezin%20and%20Aram%20Avetisyan%20and%20Aleksandr%20Beznosikov&entry.1292438233=Heterogeneity%20within%20data%20distribution%20poses%20a%20challenge%20in%20many%20modern%20federated%20learning%20tasks.%20We%20formalize%20it%20as%20an%20optimization%20problem%20involving%20a%20computationally%20heavy%20composite%20under%20data%20similarity.%20By%20employing%20different%20sets%20of%20assumptions%2C%20we%20present%20several%20approaches%20to%20develop%20communication-efficient%20methods.%20An%20optimal%20algorithm%20is%20proposed%20for%20the%20convex%20case.%20The%20constructed%20theory%20is%20validated%20through%20a%20series%20of%20experiments%20across%20various%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2601.08614v1&entry.124074799=Read"},
{"title": "Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts", "author": "Sebastian Rodriguez Beltran and Marlon Tobaben and Joonas J\u00e4lk\u00f6 and Niki Loppi and Antti Honkela", "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.", "link": "http://arxiv.org/abs/2406.17298v3", "date": "2026-01-13", "relevancy": 2.0775, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5202}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Scalable%20Implementation%20of%20Differentially%20Private%20Deep%20Learning%20without%20Shortcuts&body=Title%3A%20Efficient%20and%20Scalable%20Implementation%20of%20Differentially%20Private%20Deep%20Learning%20without%20Shortcuts%0AAuthor%3A%20Sebastian%20Rodriguez%20Beltran%20and%20Marlon%20Tobaben%20and%20Joonas%20J%C3%A4lk%C3%B6%20and%20Niki%20Loppi%20and%20Antti%20Honkela%0AAbstract%3A%20Differentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20is%20the%20standard%20algorithm%20for%20training%20machine%20learning%20models%20under%20differential%20privacy%20%28DP%29.%20The%20most%20common%20DP-SGD%20privacy%20accountants%20rely%20on%20Poisson%20subsampling%20to%20ensure%20the%20theoretical%20DP%20guarantees.%20Implementing%20computationally%20efficient%20DP-SGD%20with%20Poisson%20subsampling%20is%20not%20trivial%2C%20which%20leads%20many%20implementations%20to%20taking%20a%20shortcut%20by%20using%20computationally%20faster%20subsampling.%20We%20quantify%20the%20computational%20cost%20of%20training%20deep%20learning%20models%20under%20DP%20by%20implementing%20and%20benchmarking%20efficient%20methods%20with%20the%20correct%20Poisson%20subsampling.%20We%20find%20that%20using%20the%20naive%20implementation%20of%20DP-SGD%20with%20Opacus%20in%20PyTorch%20has%20a%20throughput%20between%202.6%20and%208%20times%20lower%20than%20that%20of%20SGD.%20However%2C%20efficient%20gradient%20clipping%20implementations%20like%20Ghost%20Clipping%20can%20roughly%20halve%20this%20cost.%20We%20propose%20an%20alternative%20computationally%20efficient%20implementation%20of%20DP-SGD%20with%20JAX%20that%20uses%20Poisson%20subsampling%20and%20performs%20comparably%20with%20efficient%20clipping%20optimizations%20based%20on%20PyTorch.%20We%20study%20the%20scaling%20behavior%20using%20up%20to%2080%20GPUs%20and%20find%20that%20DP-SGD%20scales%20better%20than%20SGD.%20We%20share%20our%20library%20at%20https%3A//github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.%0ALink%3A%20http%3A//arxiv.org/abs/2406.17298v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Scalable%2520Implementation%2520of%2520Differentially%2520Private%2520Deep%2520Learning%2520without%2520Shortcuts%26entry.906535625%3DSebastian%2520Rodriguez%2520Beltran%2520and%2520Marlon%2520Tobaben%2520and%2520Joonas%2520J%25C3%25A4lk%25C3%25B6%2520and%2520Niki%2520Loppi%2520and%2520Antti%2520Honkela%26entry.1292438233%3DDifferentially%2520private%2520stochastic%2520gradient%2520descent%2520%2528DP-SGD%2529%2520is%2520the%2520standard%2520algorithm%2520for%2520training%2520machine%2520learning%2520models%2520under%2520differential%2520privacy%2520%2528DP%2529.%2520The%2520most%2520common%2520DP-SGD%2520privacy%2520accountants%2520rely%2520on%2520Poisson%2520subsampling%2520to%2520ensure%2520the%2520theoretical%2520DP%2520guarantees.%2520Implementing%2520computationally%2520efficient%2520DP-SGD%2520with%2520Poisson%2520subsampling%2520is%2520not%2520trivial%252C%2520which%2520leads%2520many%2520implementations%2520to%2520taking%2520a%2520shortcut%2520by%2520using%2520computationally%2520faster%2520subsampling.%2520We%2520quantify%2520the%2520computational%2520cost%2520of%2520training%2520deep%2520learning%2520models%2520under%2520DP%2520by%2520implementing%2520and%2520benchmarking%2520efficient%2520methods%2520with%2520the%2520correct%2520Poisson%2520subsampling.%2520We%2520find%2520that%2520using%2520the%2520naive%2520implementation%2520of%2520DP-SGD%2520with%2520Opacus%2520in%2520PyTorch%2520has%2520a%2520throughput%2520between%25202.6%2520and%25208%2520times%2520lower%2520than%2520that%2520of%2520SGD.%2520However%252C%2520efficient%2520gradient%2520clipping%2520implementations%2520like%2520Ghost%2520Clipping%2520can%2520roughly%2520halve%2520this%2520cost.%2520We%2520propose%2520an%2520alternative%2520computationally%2520efficient%2520implementation%2520of%2520DP-SGD%2520with%2520JAX%2520that%2520uses%2520Poisson%2520subsampling%2520and%2520performs%2520comparably%2520with%2520efficient%2520clipping%2520optimizations%2520based%2520on%2520PyTorch.%2520We%2520study%2520the%2520scaling%2520behavior%2520using%2520up%2520to%252080%2520GPUs%2520and%2520find%2520that%2520DP-SGD%2520scales%2520better%2520than%2520SGD.%2520We%2520share%2520our%2520library%2520at%2520https%253A//github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17298v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Scalable%20Implementation%20of%20Differentially%20Private%20Deep%20Learning%20without%20Shortcuts&entry.906535625=Sebastian%20Rodriguez%20Beltran%20and%20Marlon%20Tobaben%20and%20Joonas%20J%C3%A4lk%C3%B6%20and%20Niki%20Loppi%20and%20Antti%20Honkela&entry.1292438233=Differentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20is%20the%20standard%20algorithm%20for%20training%20machine%20learning%20models%20under%20differential%20privacy%20%28DP%29.%20The%20most%20common%20DP-SGD%20privacy%20accountants%20rely%20on%20Poisson%20subsampling%20to%20ensure%20the%20theoretical%20DP%20guarantees.%20Implementing%20computationally%20efficient%20DP-SGD%20with%20Poisson%20subsampling%20is%20not%20trivial%2C%20which%20leads%20many%20implementations%20to%20taking%20a%20shortcut%20by%20using%20computationally%20faster%20subsampling.%20We%20quantify%20the%20computational%20cost%20of%20training%20deep%20learning%20models%20under%20DP%20by%20implementing%20and%20benchmarking%20efficient%20methods%20with%20the%20correct%20Poisson%20subsampling.%20We%20find%20that%20using%20the%20naive%20implementation%20of%20DP-SGD%20with%20Opacus%20in%20PyTorch%20has%20a%20throughput%20between%202.6%20and%208%20times%20lower%20than%20that%20of%20SGD.%20However%2C%20efficient%20gradient%20clipping%20implementations%20like%20Ghost%20Clipping%20can%20roughly%20halve%20this%20cost.%20We%20propose%20an%20alternative%20computationally%20efficient%20implementation%20of%20DP-SGD%20with%20JAX%20that%20uses%20Poisson%20subsampling%20and%20performs%20comparably%20with%20efficient%20clipping%20optimizations%20based%20on%20PyTorch.%20We%20study%20the%20scaling%20behavior%20using%20up%20to%2080%20GPUs%20and%20find%20that%20DP-SGD%20scales%20better%20than%20SGD.%20We%20share%20our%20library%20at%20https%3A//github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.&entry.1838667208=http%3A//arxiv.org/abs/2406.17298v3&entry.124074799=Read"},
{"title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "author": "Qiguang Chen and Yantao Du and Ziniu Li and Jinhao Liu and Songyao Duan and Jiarui Guo and Minghao Liu and Jiaheng Liu and Tong Yang and Ge Zhang and Libo Qin and Wanxiang Che and Wenhao Huang", "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "link": "http://arxiv.org/abs/2601.06002v2", "date": "2026-01-13", "relevancy": 2.0721, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning&body=Title%3A%20The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Qiguang%20Chen%20and%20Yantao%20Du%20and%20Ziniu%20Li%20and%20Jinhao%20Liu%20and%20Songyao%20Duan%20and%20Jiarui%20Guo%20and%20Minghao%20Liu%20and%20Jiaheng%20Liu%20and%20Tong%20Yang%20and%20Ge%20Zhang%20and%20Libo%20Qin%20and%20Wanxiang%20Che%20and%20Wenhao%20Huang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20learn%20effective%20long%20chain-of-thought%20%28Long%20CoT%29%20reasoning%20from%20human%20or%20non-Long-CoT%20LLMs%20imitation.%20To%20understand%20this%2C%20we%20propose%20that%20effective%20and%20learnable%20Long%20CoT%20trajectories%20feature%20stable%20molecular-like%20structures%20in%20unified%20view%2C%20which%20are%20formed%20by%20three%20interaction%20types%3A%20Deep-Reasoning%20%28covalent-like%29%2C%20Self-Reflection%20%28hydrogen-bond-like%29%2C%20and%20Self-Exploration%20%28van%20der%20Waals-like%29.%20Analysis%20of%20distilled%20trajectories%20reveals%20these%20structures%20emerge%20from%20Long%20CoT%20fine-tuning%2C%20not%20keyword%20imitation.%20We%20introduce%20Effective%20Semantic%20Isomers%20and%20show%20that%20only%20bonds%20promoting%20fast%20entropy%20convergence%20support%20stable%20Long%20CoT%20learning%2C%20while%20structural%20competition%20impairs%20training.%20Drawing%20on%20these%20findings%2C%20we%20present%20Mole-Syn%2C%20a%20distribution-transfer-graph%20method%20that%20guides%20synthesis%20of%20effective%20Long%20CoT%20structures%2C%20boosting%20performance%20and%20RL%20stability%20across%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Molecular%2520Structure%2520of%2520Thought%253A%2520Mapping%2520the%2520Topology%2520of%2520Long%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DQiguang%2520Chen%2520and%2520Yantao%2520Du%2520and%2520Ziniu%2520Li%2520and%2520Jinhao%2520Liu%2520and%2520Songyao%2520Duan%2520and%2520Jiarui%2520Guo%2520and%2520Minghao%2520Liu%2520and%2520Jiaheng%2520Liu%2520and%2520Tong%2520Yang%2520and%2520Ge%2520Zhang%2520and%2520Libo%2520Qin%2520and%2520Wanxiang%2520Che%2520and%2520Wenhao%2520Huang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520often%2520fail%2520to%2520learn%2520effective%2520long%2520chain-of-thought%2520%2528Long%2520CoT%2529%2520reasoning%2520from%2520human%2520or%2520non-Long-CoT%2520LLMs%2520imitation.%2520To%2520understand%2520this%252C%2520we%2520propose%2520that%2520effective%2520and%2520learnable%2520Long%2520CoT%2520trajectories%2520feature%2520stable%2520molecular-like%2520structures%2520in%2520unified%2520view%252C%2520which%2520are%2520formed%2520by%2520three%2520interaction%2520types%253A%2520Deep-Reasoning%2520%2528covalent-like%2529%252C%2520Self-Reflection%2520%2528hydrogen-bond-like%2529%252C%2520and%2520Self-Exploration%2520%2528van%2520der%2520Waals-like%2529.%2520Analysis%2520of%2520distilled%2520trajectories%2520reveals%2520these%2520structures%2520emerge%2520from%2520Long%2520CoT%2520fine-tuning%252C%2520not%2520keyword%2520imitation.%2520We%2520introduce%2520Effective%2520Semantic%2520Isomers%2520and%2520show%2520that%2520only%2520bonds%2520promoting%2520fast%2520entropy%2520convergence%2520support%2520stable%2520Long%2520CoT%2520learning%252C%2520while%2520structural%2520competition%2520impairs%2520training.%2520Drawing%2520on%2520these%2520findings%252C%2520we%2520present%2520Mole-Syn%252C%2520a%2520distribution-transfer-graph%2520method%2520that%2520guides%2520synthesis%2520of%2520effective%2520Long%2520CoT%2520structures%252C%2520boosting%2520performance%2520and%2520RL%2520stability%2520across%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning&entry.906535625=Qiguang%20Chen%20and%20Yantao%20Du%20and%20Ziniu%20Li%20and%20Jinhao%20Liu%20and%20Songyao%20Duan%20and%20Jiarui%20Guo%20and%20Minghao%20Liu%20and%20Jiaheng%20Liu%20and%20Tong%20Yang%20and%20Ge%20Zhang%20and%20Libo%20Qin%20and%20Wanxiang%20Che%20and%20Wenhao%20Huang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20learn%20effective%20long%20chain-of-thought%20%28Long%20CoT%29%20reasoning%20from%20human%20or%20non-Long-CoT%20LLMs%20imitation.%20To%20understand%20this%2C%20we%20propose%20that%20effective%20and%20learnable%20Long%20CoT%20trajectories%20feature%20stable%20molecular-like%20structures%20in%20unified%20view%2C%20which%20are%20formed%20by%20three%20interaction%20types%3A%20Deep-Reasoning%20%28covalent-like%29%2C%20Self-Reflection%20%28hydrogen-bond-like%29%2C%20and%20Self-Exploration%20%28van%20der%20Waals-like%29.%20Analysis%20of%20distilled%20trajectories%20reveals%20these%20structures%20emerge%20from%20Long%20CoT%20fine-tuning%2C%20not%20keyword%20imitation.%20We%20introduce%20Effective%20Semantic%20Isomers%20and%20show%20that%20only%20bonds%20promoting%20fast%20entropy%20convergence%20support%20stable%20Long%20CoT%20learning%2C%20while%20structural%20competition%20impairs%20training.%20Drawing%20on%20these%20findings%2C%20we%20present%20Mole-Syn%2C%20a%20distribution-transfer-graph%20method%20that%20guides%20synthesis%20of%20effective%20Long%20CoT%20structures%2C%20boosting%20performance%20and%20RL%20stability%20across%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.06002v2&entry.124074799=Read"},
{"title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions", "author": "Yanxu Zhu and Shitong Duan and Xiangxu Zhang and Jitao Sang and Peng Zhang and Tun Lu and Xiao Zhou and Jing Yao and Xiaoyuan Yi and Xing Xie", "abstract": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.", "link": "http://arxiv.org/abs/2507.21503v4", "date": "2026-01-13", "relevancy": 2.0586, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5255}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoHoBench%3A%20Assessing%20Honesty%20of%20Multimodal%20Large%20Language%20Models%20via%20Unanswerable%20Visual%20Questions&body=Title%3A%20MoHoBench%3A%20Assessing%20Honesty%20of%20Multimodal%20Large%20Language%20Models%20via%20Unanswerable%20Visual%20Questions%0AAuthor%3A%20Yanxu%20Zhu%20and%20Shitong%20Duan%20and%20Xiangxu%20Zhang%20and%20Jitao%20Sang%20and%20Peng%20Zhang%20and%20Tun%20Lu%20and%20Xiao%20Zhou%20and%20Jing%20Yao%20and%20Xiaoyuan%20Yi%20and%20Xing%20Xie%0AAbstract%3A%20Recently%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20advancements%20in%20vision-language%20tasks%2C%20yet%20produce%20potentially%20harmful%20or%20untrustworthy%20content.%20Despite%20substantial%20work%20investigating%20the%20trustworthiness%20of%20language%20models%2C%20MMLMs%27%20capability%20to%20act%20honestly%2C%20especially%20when%20faced%20with%20visually%20unanswerable%20questions%2C%20remains%20largely%20underexplored.%20This%20work%20presents%20the%20first%20systematic%20assessment%20of%20honesty%20behaviors%20across%20various%20MLLMs.%20We%20ground%20honesty%20in%20models%27%20response%20behaviors%20to%20unanswerable%20visual%20questions%2C%20define%20four%20representative%20types%20of%20such%20questions%2C%20and%20construct%20MoHoBench%2C%20a%20large-scale%20MMLM%20honest%20benchmark%2C%20consisting%20of%2012k%2B%20visual%20question%20samples%2C%20whose%20quality%20is%20guaranteed%20by%20multi-stage%20filtering%20and%20human%20verification.%20Using%20MoHoBench%2C%20we%20benchmarked%20the%20honesty%20of%2028%20popular%20MMLMs%20and%20conducted%20a%20comprehensive%20analysis.%20Our%20findings%20show%20that%3A%20%281%29%20most%20models%20fail%20to%20appropriately%20refuse%20to%20answer%20when%20necessary%2C%20and%20%282%29%20MMLMs%27%20honesty%20is%20not%20solely%20a%20language%20modeling%20issue%2C%20but%20is%20deeply%20influenced%20by%20visual%20information%2C%20necessitating%20the%20development%20of%20dedicated%20methods%20for%20multimodal%20honesty%20alignment.%20Therefore%2C%20we%20implemented%20initial%20alignment%20methods%20using%20supervised%20and%20preference%20learning%20to%20improve%20honesty%20behavior%2C%20providing%20a%20foundation%20for%20future%20work%20on%20trustworthy%20MLLMs.%20Our%20data%20and%20code%20can%20be%20found%20at%20https%3A//github.com/yanxuzhu/MoHoBench.%0ALink%3A%20http%3A//arxiv.org/abs/2507.21503v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoHoBench%253A%2520Assessing%2520Honesty%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Unanswerable%2520Visual%2520Questions%26entry.906535625%3DYanxu%2520Zhu%2520and%2520Shitong%2520Duan%2520and%2520Xiangxu%2520Zhang%2520and%2520Jitao%2520Sang%2520and%2520Peng%2520Zhang%2520and%2520Tun%2520Lu%2520and%2520Xiao%2520Zhou%2520and%2520Jing%2520Yao%2520and%2520Xiaoyuan%2520Yi%2520and%2520Xing%2520Xie%26entry.1292438233%3DRecently%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520considerable%2520advancements%2520in%2520vision-language%2520tasks%252C%2520yet%2520produce%2520potentially%2520harmful%2520or%2520untrustworthy%2520content.%2520Despite%2520substantial%2520work%2520investigating%2520the%2520trustworthiness%2520of%2520language%2520models%252C%2520MMLMs%2527%2520capability%2520to%2520act%2520honestly%252C%2520especially%2520when%2520faced%2520with%2520visually%2520unanswerable%2520questions%252C%2520remains%2520largely%2520underexplored.%2520This%2520work%2520presents%2520the%2520first%2520systematic%2520assessment%2520of%2520honesty%2520behaviors%2520across%2520various%2520MLLMs.%2520We%2520ground%2520honesty%2520in%2520models%2527%2520response%2520behaviors%2520to%2520unanswerable%2520visual%2520questions%252C%2520define%2520four%2520representative%2520types%2520of%2520such%2520questions%252C%2520and%2520construct%2520MoHoBench%252C%2520a%2520large-scale%2520MMLM%2520honest%2520benchmark%252C%2520consisting%2520of%252012k%252B%2520visual%2520question%2520samples%252C%2520whose%2520quality%2520is%2520guaranteed%2520by%2520multi-stage%2520filtering%2520and%2520human%2520verification.%2520Using%2520MoHoBench%252C%2520we%2520benchmarked%2520the%2520honesty%2520of%252028%2520popular%2520MMLMs%2520and%2520conducted%2520a%2520comprehensive%2520analysis.%2520Our%2520findings%2520show%2520that%253A%2520%25281%2529%2520most%2520models%2520fail%2520to%2520appropriately%2520refuse%2520to%2520answer%2520when%2520necessary%252C%2520and%2520%25282%2529%2520MMLMs%2527%2520honesty%2520is%2520not%2520solely%2520a%2520language%2520modeling%2520issue%252C%2520but%2520is%2520deeply%2520influenced%2520by%2520visual%2520information%252C%2520necessitating%2520the%2520development%2520of%2520dedicated%2520methods%2520for%2520multimodal%2520honesty%2520alignment.%2520Therefore%252C%2520we%2520implemented%2520initial%2520alignment%2520methods%2520using%2520supervised%2520and%2520preference%2520learning%2520to%2520improve%2520honesty%2520behavior%252C%2520providing%2520a%2520foundation%2520for%2520future%2520work%2520on%2520trustworthy%2520MLLMs.%2520Our%2520data%2520and%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/yanxuzhu/MoHoBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21503v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoHoBench%3A%20Assessing%20Honesty%20of%20Multimodal%20Large%20Language%20Models%20via%20Unanswerable%20Visual%20Questions&entry.906535625=Yanxu%20Zhu%20and%20Shitong%20Duan%20and%20Xiangxu%20Zhang%20and%20Jitao%20Sang%20and%20Peng%20Zhang%20and%20Tun%20Lu%20and%20Xiao%20Zhou%20and%20Jing%20Yao%20and%20Xiaoyuan%20Yi%20and%20Xing%20Xie&entry.1292438233=Recently%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20advancements%20in%20vision-language%20tasks%2C%20yet%20produce%20potentially%20harmful%20or%20untrustworthy%20content.%20Despite%20substantial%20work%20investigating%20the%20trustworthiness%20of%20language%20models%2C%20MMLMs%27%20capability%20to%20act%20honestly%2C%20especially%20when%20faced%20with%20visually%20unanswerable%20questions%2C%20remains%20largely%20underexplored.%20This%20work%20presents%20the%20first%20systematic%20assessment%20of%20honesty%20behaviors%20across%20various%20MLLMs.%20We%20ground%20honesty%20in%20models%27%20response%20behaviors%20to%20unanswerable%20visual%20questions%2C%20define%20four%20representative%20types%20of%20such%20questions%2C%20and%20construct%20MoHoBench%2C%20a%20large-scale%20MMLM%20honest%20benchmark%2C%20consisting%20of%2012k%2B%20visual%20question%20samples%2C%20whose%20quality%20is%20guaranteed%20by%20multi-stage%20filtering%20and%20human%20verification.%20Using%20MoHoBench%2C%20we%20benchmarked%20the%20honesty%20of%2028%20popular%20MMLMs%20and%20conducted%20a%20comprehensive%20analysis.%20Our%20findings%20show%20that%3A%20%281%29%20most%20models%20fail%20to%20appropriately%20refuse%20to%20answer%20when%20necessary%2C%20and%20%282%29%20MMLMs%27%20honesty%20is%20not%20solely%20a%20language%20modeling%20issue%2C%20but%20is%20deeply%20influenced%20by%20visual%20information%2C%20necessitating%20the%20development%20of%20dedicated%20methods%20for%20multimodal%20honesty%20alignment.%20Therefore%2C%20we%20implemented%20initial%20alignment%20methods%20using%20supervised%20and%20preference%20learning%20to%20improve%20honesty%20behavior%2C%20providing%20a%20foundation%20for%20future%20work%20on%20trustworthy%20MLLMs.%20Our%20data%20and%20code%20can%20be%20found%20at%20https%3A//github.com/yanxuzhu/MoHoBench.&entry.1838667208=http%3A//arxiv.org/abs/2507.21503v4&entry.124074799=Read"},
{"title": "To Retrieve or To Think? An Agentic Approach for Context Evolution", "author": "Rubing Chen and Jian Wang and Wenjie Li and Xiao-Yong Wei and Qing Li", "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.", "link": "http://arxiv.org/abs/2601.08747v1", "date": "2026-01-13", "relevancy": 2.0571, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5577}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Retrieve%20or%20To%20Think%3F%20An%20Agentic%20Approach%20for%20Context%20Evolution&body=Title%3A%20To%20Retrieve%20or%20To%20Think%3F%20An%20Agentic%20Approach%20for%20Context%20Evolution%0AAuthor%3A%20Rubing%20Chen%20and%20Jian%20Wang%20and%20Wenjie%20Li%20and%20Xiao-Yong%20Wei%20and%20Qing%20Li%0AAbstract%3A%20Current%20context%20augmentation%20methods%2C%20such%20as%20retrieval-augmented%20generation%2C%20are%20essential%20for%20solving%20knowledge-intensive%20reasoning%20tasks.However%2C%20they%20typically%20adhere%20to%20a%20rigid%2C%20brute-force%20strategy%20that%20executes%20retrieval%20at%20every%20step.%20This%20indiscriminate%20approach%20not%20only%20incurs%20unnecessary%20computational%20costs%20but%20also%20degrades%20performance%20by%20saturating%20the%20context%20with%20irrelevant%20noise.%20To%20address%20these%20limitations%2C%20we%20introduce%20Agentic%20Context%20Evolution%20%28ACE%29%2C%20a%20framework%20inspired%20by%20human%20metacognition%20that%20dynamically%20determines%20whether%20to%20seek%20new%20evidence%20or%20reason%20with%20existing%20knowledge.%20ACE%20employs%20a%20central%20orchestrator%20agent%20to%20make%20decisions%20strategically%20via%20majority%20voting.It%20aims%20to%20alternate%20between%20activating%20a%20retriever%20agent%20for%20external%20retrieval%20and%20a%20reasoner%20agent%20for%20internal%20analysis%20and%20refinement.%20By%20eliminating%20redundant%20retrieval%20steps%2C%20ACE%20maintains%20a%20concise%20and%20evolved%20context.%20Extensive%20experiments%20on%20challenging%20multi-hop%20QA%20benchmarks%20demonstrate%20that%20ACE%20significantly%20outperforms%20competitive%20baselines%20in%20accuracy%20while%20achieving%20efficient%20token%20consumption.Our%20work%20provides%20valuable%20insights%20into%20advancing%20context-evolved%20generation%20for%20complex%2C%20knowledge-intensive%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Retrieve%2520or%2520To%2520Think%253F%2520An%2520Agentic%2520Approach%2520for%2520Context%2520Evolution%26entry.906535625%3DRubing%2520Chen%2520and%2520Jian%2520Wang%2520and%2520Wenjie%2520Li%2520and%2520Xiao-Yong%2520Wei%2520and%2520Qing%2520Li%26entry.1292438233%3DCurrent%2520context%2520augmentation%2520methods%252C%2520such%2520as%2520retrieval-augmented%2520generation%252C%2520are%2520essential%2520for%2520solving%2520knowledge-intensive%2520reasoning%2520tasks.However%252C%2520they%2520typically%2520adhere%2520to%2520a%2520rigid%252C%2520brute-force%2520strategy%2520that%2520executes%2520retrieval%2520at%2520every%2520step.%2520This%2520indiscriminate%2520approach%2520not%2520only%2520incurs%2520unnecessary%2520computational%2520costs%2520but%2520also%2520degrades%2520performance%2520by%2520saturating%2520the%2520context%2520with%2520irrelevant%2520noise.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Agentic%2520Context%2520Evolution%2520%2528ACE%2529%252C%2520a%2520framework%2520inspired%2520by%2520human%2520metacognition%2520that%2520dynamically%2520determines%2520whether%2520to%2520seek%2520new%2520evidence%2520or%2520reason%2520with%2520existing%2520knowledge.%2520ACE%2520employs%2520a%2520central%2520orchestrator%2520agent%2520to%2520make%2520decisions%2520strategically%2520via%2520majority%2520voting.It%2520aims%2520to%2520alternate%2520between%2520activating%2520a%2520retriever%2520agent%2520for%2520external%2520retrieval%2520and%2520a%2520reasoner%2520agent%2520for%2520internal%2520analysis%2520and%2520refinement.%2520By%2520eliminating%2520redundant%2520retrieval%2520steps%252C%2520ACE%2520maintains%2520a%2520concise%2520and%2520evolved%2520context.%2520Extensive%2520experiments%2520on%2520challenging%2520multi-hop%2520QA%2520benchmarks%2520demonstrate%2520that%2520ACE%2520significantly%2520outperforms%2520competitive%2520baselines%2520in%2520accuracy%2520while%2520achieving%2520efficient%2520token%2520consumption.Our%2520work%2520provides%2520valuable%2520insights%2520into%2520advancing%2520context-evolved%2520generation%2520for%2520complex%252C%2520knowledge-intensive%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Retrieve%20or%20To%20Think%3F%20An%20Agentic%20Approach%20for%20Context%20Evolution&entry.906535625=Rubing%20Chen%20and%20Jian%20Wang%20and%20Wenjie%20Li%20and%20Xiao-Yong%20Wei%20and%20Qing%20Li&entry.1292438233=Current%20context%20augmentation%20methods%2C%20such%20as%20retrieval-augmented%20generation%2C%20are%20essential%20for%20solving%20knowledge-intensive%20reasoning%20tasks.However%2C%20they%20typically%20adhere%20to%20a%20rigid%2C%20brute-force%20strategy%20that%20executes%20retrieval%20at%20every%20step.%20This%20indiscriminate%20approach%20not%20only%20incurs%20unnecessary%20computational%20costs%20but%20also%20degrades%20performance%20by%20saturating%20the%20context%20with%20irrelevant%20noise.%20To%20address%20these%20limitations%2C%20we%20introduce%20Agentic%20Context%20Evolution%20%28ACE%29%2C%20a%20framework%20inspired%20by%20human%20metacognition%20that%20dynamically%20determines%20whether%20to%20seek%20new%20evidence%20or%20reason%20with%20existing%20knowledge.%20ACE%20employs%20a%20central%20orchestrator%20agent%20to%20make%20decisions%20strategically%20via%20majority%20voting.It%20aims%20to%20alternate%20between%20activating%20a%20retriever%20agent%20for%20external%20retrieval%20and%20a%20reasoner%20agent%20for%20internal%20analysis%20and%20refinement.%20By%20eliminating%20redundant%20retrieval%20steps%2C%20ACE%20maintains%20a%20concise%20and%20evolved%20context.%20Extensive%20experiments%20on%20challenging%20multi-hop%20QA%20benchmarks%20demonstrate%20that%20ACE%20significantly%20outperforms%20competitive%20baselines%20in%20accuracy%20while%20achieving%20efficient%20token%20consumption.Our%20work%20provides%20valuable%20insights%20into%20advancing%20context-evolved%20generation%20for%20complex%2C%20knowledge-intensive%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.08747v1&entry.124074799=Read"},
{"title": "RAVEN: Erasing Invisible Watermarks via Novel View Synthesis", "author": "Fahad Shamshad and Nils Lukas and Karthik Nandakumar", "abstract": "Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.", "link": "http://arxiv.org/abs/2601.08832v1", "date": "2026-01-13", "relevancy": 2.055, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5188}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5103}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVEN%3A%20Erasing%20Invisible%20Watermarks%20via%20Novel%20View%20Synthesis&body=Title%3A%20RAVEN%3A%20Erasing%20Invisible%20Watermarks%20via%20Novel%20View%20Synthesis%0AAuthor%3A%20Fahad%20Shamshad%20and%20Nils%20Lukas%20and%20Karthik%20Nandakumar%0AAbstract%3A%20Invisible%20watermarking%20has%20become%20a%20critical%20mechanism%20for%20authenticating%20AI-generated%20image%20content%2C%20with%20major%20platforms%20deploying%20watermarking%20schemes%20at%20scale.%20However%2C%20evaluating%20the%20vulnerability%20of%20these%20schemes%20against%20sophisticated%20removal%20attacks%20remains%20essential%20to%20assess%20their%20reliability%20and%20guide%20robust%20design.%20In%20this%20work%2C%20we%20expose%20a%20fundamental%20vulnerability%20in%20invisible%20watermarks%20by%20reformulating%20watermark%20removal%20as%20a%20view%20synthesis%20problem.%20Our%20key%20insight%20is%20that%20generating%20a%20perceptually%20consistent%20alternative%20view%20of%20the%20same%20semantic%20content%2C%20akin%20to%20re-observing%20a%20scene%20from%20a%20shifted%20perspective%2C%20naturally%20removes%20the%20embedded%20watermark%20while%20preserving%20visual%20fidelity.%20This%20reveals%20a%20critical%20gap%3A%20watermarks%20robust%20to%20pixel-space%20and%20frequency-domain%20attacks%20remain%20vulnerable%20to%20semantic-preserving%20viewpoint%20transformations.%20We%20introduce%20a%20zero-shot%20diffusion-based%20framework%20that%20applies%20controlled%20geometric%20transformations%20in%20latent%20space%2C%20augmented%20with%20view-guided%20correspondence%20attention%20to%20maintain%20structural%20consistency%20during%20reconstruction.%20Operating%20on%20frozen%20pre-trained%20models%20without%20detector%20access%20or%20watermark%20knowledge%2C%20our%20method%20achieves%20state-of-the-art%20watermark%20suppression%20across%2015%20watermarking%20methods--outperforming%2014%20baseline%20attacks%20while%20maintaining%20superior%20perceptual%20quality%20across%20multiple%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVEN%253A%2520Erasing%2520Invisible%2520Watermarks%2520via%2520Novel%2520View%2520Synthesis%26entry.906535625%3DFahad%2520Shamshad%2520and%2520Nils%2520Lukas%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3DInvisible%2520watermarking%2520has%2520become%2520a%2520critical%2520mechanism%2520for%2520authenticating%2520AI-generated%2520image%2520content%252C%2520with%2520major%2520platforms%2520deploying%2520watermarking%2520schemes%2520at%2520scale.%2520However%252C%2520evaluating%2520the%2520vulnerability%2520of%2520these%2520schemes%2520against%2520sophisticated%2520removal%2520attacks%2520remains%2520essential%2520to%2520assess%2520their%2520reliability%2520and%2520guide%2520robust%2520design.%2520In%2520this%2520work%252C%2520we%2520expose%2520a%2520fundamental%2520vulnerability%2520in%2520invisible%2520watermarks%2520by%2520reformulating%2520watermark%2520removal%2520as%2520a%2520view%2520synthesis%2520problem.%2520Our%2520key%2520insight%2520is%2520that%2520generating%2520a%2520perceptually%2520consistent%2520alternative%2520view%2520of%2520the%2520same%2520semantic%2520content%252C%2520akin%2520to%2520re-observing%2520a%2520scene%2520from%2520a%2520shifted%2520perspective%252C%2520naturally%2520removes%2520the%2520embedded%2520watermark%2520while%2520preserving%2520visual%2520fidelity.%2520This%2520reveals%2520a%2520critical%2520gap%253A%2520watermarks%2520robust%2520to%2520pixel-space%2520and%2520frequency-domain%2520attacks%2520remain%2520vulnerable%2520to%2520semantic-preserving%2520viewpoint%2520transformations.%2520We%2520introduce%2520a%2520zero-shot%2520diffusion-based%2520framework%2520that%2520applies%2520controlled%2520geometric%2520transformations%2520in%2520latent%2520space%252C%2520augmented%2520with%2520view-guided%2520correspondence%2520attention%2520to%2520maintain%2520structural%2520consistency%2520during%2520reconstruction.%2520Operating%2520on%2520frozen%2520pre-trained%2520models%2520without%2520detector%2520access%2520or%2520watermark%2520knowledge%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520watermark%2520suppression%2520across%252015%2520watermarking%2520methods--outperforming%252014%2520baseline%2520attacks%2520while%2520maintaining%2520superior%2520perceptual%2520quality%2520across%2520multiple%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVEN%3A%20Erasing%20Invisible%20Watermarks%20via%20Novel%20View%20Synthesis&entry.906535625=Fahad%20Shamshad%20and%20Nils%20Lukas%20and%20Karthik%20Nandakumar&entry.1292438233=Invisible%20watermarking%20has%20become%20a%20critical%20mechanism%20for%20authenticating%20AI-generated%20image%20content%2C%20with%20major%20platforms%20deploying%20watermarking%20schemes%20at%20scale.%20However%2C%20evaluating%20the%20vulnerability%20of%20these%20schemes%20against%20sophisticated%20removal%20attacks%20remains%20essential%20to%20assess%20their%20reliability%20and%20guide%20robust%20design.%20In%20this%20work%2C%20we%20expose%20a%20fundamental%20vulnerability%20in%20invisible%20watermarks%20by%20reformulating%20watermark%20removal%20as%20a%20view%20synthesis%20problem.%20Our%20key%20insight%20is%20that%20generating%20a%20perceptually%20consistent%20alternative%20view%20of%20the%20same%20semantic%20content%2C%20akin%20to%20re-observing%20a%20scene%20from%20a%20shifted%20perspective%2C%20naturally%20removes%20the%20embedded%20watermark%20while%20preserving%20visual%20fidelity.%20This%20reveals%20a%20critical%20gap%3A%20watermarks%20robust%20to%20pixel-space%20and%20frequency-domain%20attacks%20remain%20vulnerable%20to%20semantic-preserving%20viewpoint%20transformations.%20We%20introduce%20a%20zero-shot%20diffusion-based%20framework%20that%20applies%20controlled%20geometric%20transformations%20in%20latent%20space%2C%20augmented%20with%20view-guided%20correspondence%20attention%20to%20maintain%20structural%20consistency%20during%20reconstruction.%20Operating%20on%20frozen%20pre-trained%20models%20without%20detector%20access%20or%20watermark%20knowledge%2C%20our%20method%20achieves%20state-of-the-art%20watermark%20suppression%20across%2015%20watermarking%20methods--outperforming%2014%20baseline%20attacks%20while%20maintaining%20superior%20perceptual%20quality%20across%20multiple%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.08832v1&entry.124074799=Read"},
{"title": "Feed-Forward Optimization With Delayed Feedback for Neural Network Training", "author": "Katharina Fl\u00fcgel and Daniel Coquelin and Marie Weiel and Charlotte Debus and Achim Streit and Markus G\u00f6tz", "abstract": "Backpropagation has long been criticized for being biologically implausible due to its reliance on concepts that are not viable in natural learning processes. Two core issues are the weight transport and update locking problems caused by the forward-backward dependencies, which limit biological plausibility, computational efficiency, and parallelization. Although several alternatives have been proposed to increase biological plausibility, they often come at the cost of reduced predictive performance. This paper proposes an alternative approach to training feed-forward neural networks addressing these issues by using approximate gradient information. We introduce Feed-Forward with delayed Feedback (F$^3$), which approximates gradients using fixed random feedback paths and delayed error information from the previous epoch to balance biological plausibility with predictive performance. We evaluate F$^3$ across multiple tasks and architectures, including both fully-connected and Transformer networks. Our results demonstrate that, compared to similarly plausible approaches, F$^3$ significantly improves predictive performance, narrowing the gap to backpropagation by up to 56% for classification and 96% for regression. This work is a step towards more biologically plausible learning algorithms while opening up new avenues for energy-efficient and parallelizable neural network training.", "link": "http://arxiv.org/abs/2304.13372v2", "date": "2026-01-13", "relevancy": 2.0378, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5488}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5132}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feed-Forward%20Optimization%20With%20Delayed%20Feedback%20for%20Neural%20Network%20Training&body=Title%3A%20Feed-Forward%20Optimization%20With%20Delayed%20Feedback%20for%20Neural%20Network%20Training%0AAuthor%3A%20Katharina%20Fl%C3%BCgel%20and%20Daniel%20Coquelin%20and%20Marie%20Weiel%20and%20Charlotte%20Debus%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz%0AAbstract%3A%20Backpropagation%20has%20long%20been%20criticized%20for%20being%20biologically%20implausible%20due%20to%20its%20reliance%20on%20concepts%20that%20are%20not%20viable%20in%20natural%20learning%20processes.%20Two%20core%20issues%20are%20the%20weight%20transport%20and%20update%20locking%20problems%20caused%20by%20the%20forward-backward%20dependencies%2C%20which%20limit%20biological%20plausibility%2C%20computational%20efficiency%2C%20and%20parallelization.%20Although%20several%20alternatives%20have%20been%20proposed%20to%20increase%20biological%20plausibility%2C%20they%20often%20come%20at%20the%20cost%20of%20reduced%20predictive%20performance.%20This%20paper%20proposes%20an%20alternative%20approach%20to%20training%20feed-forward%20neural%20networks%20addressing%20these%20issues%20by%20using%20approximate%20gradient%20information.%20We%20introduce%20Feed-Forward%20with%20delayed%20Feedback%20%28F%24%5E3%24%29%2C%20which%20approximates%20gradients%20using%20fixed%20random%20feedback%20paths%20and%20delayed%20error%20information%20from%20the%20previous%20epoch%20to%20balance%20biological%20plausibility%20with%20predictive%20performance.%20We%20evaluate%20F%24%5E3%24%20across%20multiple%20tasks%20and%20architectures%2C%20including%20both%20fully-connected%20and%20Transformer%20networks.%20Our%20results%20demonstrate%20that%2C%20compared%20to%20similarly%20plausible%20approaches%2C%20F%24%5E3%24%20significantly%20improves%20predictive%20performance%2C%20narrowing%20the%20gap%20to%20backpropagation%20by%20up%20to%2056%25%20for%20classification%20and%2096%25%20for%20regression.%20This%20work%20is%20a%20step%20towards%20more%20biologically%20plausible%20learning%20algorithms%20while%20opening%20up%20new%20avenues%20for%20energy-efficient%20and%20parallelizable%20neural%20network%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2304.13372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeed-Forward%2520Optimization%2520With%2520Delayed%2520Feedback%2520for%2520Neural%2520Network%2520Training%26entry.906535625%3DKatharina%2520Fl%25C3%25BCgel%2520and%2520Daniel%2520Coquelin%2520and%2520Marie%2520Weiel%2520and%2520Charlotte%2520Debus%2520and%2520Achim%2520Streit%2520and%2520Markus%2520G%25C3%25B6tz%26entry.1292438233%3DBackpropagation%2520has%2520long%2520been%2520criticized%2520for%2520being%2520biologically%2520implausible%2520due%2520to%2520its%2520reliance%2520on%2520concepts%2520that%2520are%2520not%2520viable%2520in%2520natural%2520learning%2520processes.%2520Two%2520core%2520issues%2520are%2520the%2520weight%2520transport%2520and%2520update%2520locking%2520problems%2520caused%2520by%2520the%2520forward-backward%2520dependencies%252C%2520which%2520limit%2520biological%2520plausibility%252C%2520computational%2520efficiency%252C%2520and%2520parallelization.%2520Although%2520several%2520alternatives%2520have%2520been%2520proposed%2520to%2520increase%2520biological%2520plausibility%252C%2520they%2520often%2520come%2520at%2520the%2520cost%2520of%2520reduced%2520predictive%2520performance.%2520This%2520paper%2520proposes%2520an%2520alternative%2520approach%2520to%2520training%2520feed-forward%2520neural%2520networks%2520addressing%2520these%2520issues%2520by%2520using%2520approximate%2520gradient%2520information.%2520We%2520introduce%2520Feed-Forward%2520with%2520delayed%2520Feedback%2520%2528F%2524%255E3%2524%2529%252C%2520which%2520approximates%2520gradients%2520using%2520fixed%2520random%2520feedback%2520paths%2520and%2520delayed%2520error%2520information%2520from%2520the%2520previous%2520epoch%2520to%2520balance%2520biological%2520plausibility%2520with%2520predictive%2520performance.%2520We%2520evaluate%2520F%2524%255E3%2524%2520across%2520multiple%2520tasks%2520and%2520architectures%252C%2520including%2520both%2520fully-connected%2520and%2520Transformer%2520networks.%2520Our%2520results%2520demonstrate%2520that%252C%2520compared%2520to%2520similarly%2520plausible%2520approaches%252C%2520F%2524%255E3%2524%2520significantly%2520improves%2520predictive%2520performance%252C%2520narrowing%2520the%2520gap%2520to%2520backpropagation%2520by%2520up%2520to%252056%2525%2520for%2520classification%2520and%252096%2525%2520for%2520regression.%2520This%2520work%2520is%2520a%2520step%2520towards%2520more%2520biologically%2520plausible%2520learning%2520algorithms%2520while%2520opening%2520up%2520new%2520avenues%2520for%2520energy-efficient%2520and%2520parallelizable%2520neural%2520network%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.13372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feed-Forward%20Optimization%20With%20Delayed%20Feedback%20for%20Neural%20Network%20Training&entry.906535625=Katharina%20Fl%C3%BCgel%20and%20Daniel%20Coquelin%20and%20Marie%20Weiel%20and%20Charlotte%20Debus%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz&entry.1292438233=Backpropagation%20has%20long%20been%20criticized%20for%20being%20biologically%20implausible%20due%20to%20its%20reliance%20on%20concepts%20that%20are%20not%20viable%20in%20natural%20learning%20processes.%20Two%20core%20issues%20are%20the%20weight%20transport%20and%20update%20locking%20problems%20caused%20by%20the%20forward-backward%20dependencies%2C%20which%20limit%20biological%20plausibility%2C%20computational%20efficiency%2C%20and%20parallelization.%20Although%20several%20alternatives%20have%20been%20proposed%20to%20increase%20biological%20plausibility%2C%20they%20often%20come%20at%20the%20cost%20of%20reduced%20predictive%20performance.%20This%20paper%20proposes%20an%20alternative%20approach%20to%20training%20feed-forward%20neural%20networks%20addressing%20these%20issues%20by%20using%20approximate%20gradient%20information.%20We%20introduce%20Feed-Forward%20with%20delayed%20Feedback%20%28F%24%5E3%24%29%2C%20which%20approximates%20gradients%20using%20fixed%20random%20feedback%20paths%20and%20delayed%20error%20information%20from%20the%20previous%20epoch%20to%20balance%20biological%20plausibility%20with%20predictive%20performance.%20We%20evaluate%20F%24%5E3%24%20across%20multiple%20tasks%20and%20architectures%2C%20including%20both%20fully-connected%20and%20Transformer%20networks.%20Our%20results%20demonstrate%20that%2C%20compared%20to%20similarly%20plausible%20approaches%2C%20F%24%5E3%24%20significantly%20improves%20predictive%20performance%2C%20narrowing%20the%20gap%20to%20backpropagation%20by%20up%20to%2056%25%20for%20classification%20and%2096%25%20for%20regression.%20This%20work%20is%20a%20step%20towards%20more%20biologically%20plausible%20learning%20algorithms%20while%20opening%20up%20new%20avenues%20for%20energy-efficient%20and%20parallelizable%20neural%20network%20training.&entry.1838667208=http%3A//arxiv.org/abs/2304.13372v2&entry.124074799=Read"},
{"title": "Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis", "author": "Yi Qin and Lehan Wang and Chenxu Zhao and Alex P. W. Lee and Xiaomeng Li", "abstract": "Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.", "link": "http://arxiv.org/abs/2601.08440v1", "date": "2026-01-13", "relevancy": 2.0347, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incentivizing%20Cardiologist-Like%20Reasoning%20in%20MLLMs%20for%20Interpretable%20Echocardiographic%20Diagnosis&body=Title%3A%20Incentivizing%20Cardiologist-Like%20Reasoning%20in%20MLLMs%20for%20Interpretable%20Echocardiographic%20Diagnosis%0AAuthor%3A%20Yi%20Qin%20and%20Lehan%20Wang%20and%20Chenxu%20Zhao%20and%20Alex%20P.%20W.%20Lee%20and%20Xiaomeng%20Li%0AAbstract%3A%20Echocardiographic%20diagnosis%20is%20vital%20for%20cardiac%20screening%20yet%20remains%20challenging.%20Existing%20echocardiography%20foundation%20models%20do%20not%20effectively%20capture%20the%20relationships%20between%20quantitative%20measurements%20and%20clinical%20manifestations%2C%20whereas%20medical%20reasoning%20multimodal%20large%20language%20models%20%28MLLMs%29%20require%20costly%20construction%20of%20detailed%20reasoning%20paths%20and%20remain%20ineffective%20at%20directly%20incorporating%20such%20echocardiographic%20priors%20into%20their%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20comprising%20Cardiac%20Reasoning%20Template%20%28CRT%29%20and%20CardiacMind%20to%20enhance%20MLLM%27s%20echocardiographic%20reasoning%20by%20introducing%20cardiologist-like%20mindset.%20Specifically%2C%20CRT%20provides%20stepwise%20canonical%20diagnostic%20procedures%20for%20complex%20cardiac%20diseases%20to%20streamline%20reasoning%20path%20construction%20without%20the%20need%20for%20costly%20case-by-case%20verification.%20To%20incentivize%20reasoning%20MLLM%20under%20CRT%2C%20we%20develop%20CardiacMind%2C%20a%20new%20reinforcement%20learning%20scheme%20with%20three%20novel%20rewards%3A%20Procedural%20Quantity%20Reward%20%28PQtR%29%2C%20Procedural%20Quality%20Reward%20%28PQlR%29%2C%20and%20Echocardiographic%20Semantic%20Reward%20%28ESR%29.%20PQtR%20promotes%20detailed%20reasoning%3B%20PQlR%20promotes%20integration%20of%20evidence%20across%20views%20and%20modalities%2C%20while%20ESR%20grounds%20stepwise%20descriptions%20in%20visual%20content.%20Our%20methods%20show%20a%2048%25%20improvement%20in%20multiview%20echocardiographic%20diagnosis%20for%2015%20complex%20cardiac%20diseases%20and%20a%205%25%20improvement%20on%20CardiacNet-PAH%20over%20prior%20methods.%20The%20user%20study%20on%20our%20method%27s%20reasoning%20outputs%20shows%2093.33%25%20clinician%20agreement%20with%20cardiologist-like%20reasoning%20logic.%20Our%20code%20will%20be%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncentivizing%2520Cardiologist-Like%2520Reasoning%2520in%2520MLLMs%2520for%2520Interpretable%2520Echocardiographic%2520Diagnosis%26entry.906535625%3DYi%2520Qin%2520and%2520Lehan%2520Wang%2520and%2520Chenxu%2520Zhao%2520and%2520Alex%2520P.%2520W.%2520Lee%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3DEchocardiographic%2520diagnosis%2520is%2520vital%2520for%2520cardiac%2520screening%2520yet%2520remains%2520challenging.%2520Existing%2520echocardiography%2520foundation%2520models%2520do%2520not%2520effectively%2520capture%2520the%2520relationships%2520between%2520quantitative%2520measurements%2520and%2520clinical%2520manifestations%252C%2520whereas%2520medical%2520reasoning%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520require%2520costly%2520construction%2520of%2520detailed%2520reasoning%2520paths%2520and%2520remain%2520ineffective%2520at%2520directly%2520incorporating%2520such%2520echocardiographic%2520priors%2520into%2520their%2520reasoning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520approach%2520comprising%2520Cardiac%2520Reasoning%2520Template%2520%2528CRT%2529%2520and%2520CardiacMind%2520to%2520enhance%2520MLLM%2527s%2520echocardiographic%2520reasoning%2520by%2520introducing%2520cardiologist-like%2520mindset.%2520Specifically%252C%2520CRT%2520provides%2520stepwise%2520canonical%2520diagnostic%2520procedures%2520for%2520complex%2520cardiac%2520diseases%2520to%2520streamline%2520reasoning%2520path%2520construction%2520without%2520the%2520need%2520for%2520costly%2520case-by-case%2520verification.%2520To%2520incentivize%2520reasoning%2520MLLM%2520under%2520CRT%252C%2520we%2520develop%2520CardiacMind%252C%2520a%2520new%2520reinforcement%2520learning%2520scheme%2520with%2520three%2520novel%2520rewards%253A%2520Procedural%2520Quantity%2520Reward%2520%2528PQtR%2529%252C%2520Procedural%2520Quality%2520Reward%2520%2528PQlR%2529%252C%2520and%2520Echocardiographic%2520Semantic%2520Reward%2520%2528ESR%2529.%2520PQtR%2520promotes%2520detailed%2520reasoning%253B%2520PQlR%2520promotes%2520integration%2520of%2520evidence%2520across%2520views%2520and%2520modalities%252C%2520while%2520ESR%2520grounds%2520stepwise%2520descriptions%2520in%2520visual%2520content.%2520Our%2520methods%2520show%2520a%252048%2525%2520improvement%2520in%2520multiview%2520echocardiographic%2520diagnosis%2520for%252015%2520complex%2520cardiac%2520diseases%2520and%2520a%25205%2525%2520improvement%2520on%2520CardiacNet-PAH%2520over%2520prior%2520methods.%2520The%2520user%2520study%2520on%2520our%2520method%2527s%2520reasoning%2520outputs%2520shows%252093.33%2525%2520clinician%2520agreement%2520with%2520cardiologist-like%2520reasoning%2520logic.%2520Our%2520code%2520will%2520be%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incentivizing%20Cardiologist-Like%20Reasoning%20in%20MLLMs%20for%20Interpretable%20Echocardiographic%20Diagnosis&entry.906535625=Yi%20Qin%20and%20Lehan%20Wang%20and%20Chenxu%20Zhao%20and%20Alex%20P.%20W.%20Lee%20and%20Xiaomeng%20Li&entry.1292438233=Echocardiographic%20diagnosis%20is%20vital%20for%20cardiac%20screening%20yet%20remains%20challenging.%20Existing%20echocardiography%20foundation%20models%20do%20not%20effectively%20capture%20the%20relationships%20between%20quantitative%20measurements%20and%20clinical%20manifestations%2C%20whereas%20medical%20reasoning%20multimodal%20large%20language%20models%20%28MLLMs%29%20require%20costly%20construction%20of%20detailed%20reasoning%20paths%20and%20remain%20ineffective%20at%20directly%20incorporating%20such%20echocardiographic%20priors%20into%20their%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20comprising%20Cardiac%20Reasoning%20Template%20%28CRT%29%20and%20CardiacMind%20to%20enhance%20MLLM%27s%20echocardiographic%20reasoning%20by%20introducing%20cardiologist-like%20mindset.%20Specifically%2C%20CRT%20provides%20stepwise%20canonical%20diagnostic%20procedures%20for%20complex%20cardiac%20diseases%20to%20streamline%20reasoning%20path%20construction%20without%20the%20need%20for%20costly%20case-by-case%20verification.%20To%20incentivize%20reasoning%20MLLM%20under%20CRT%2C%20we%20develop%20CardiacMind%2C%20a%20new%20reinforcement%20learning%20scheme%20with%20three%20novel%20rewards%3A%20Procedural%20Quantity%20Reward%20%28PQtR%29%2C%20Procedural%20Quality%20Reward%20%28PQlR%29%2C%20and%20Echocardiographic%20Semantic%20Reward%20%28ESR%29.%20PQtR%20promotes%20detailed%20reasoning%3B%20PQlR%20promotes%20integration%20of%20evidence%20across%20views%20and%20modalities%2C%20while%20ESR%20grounds%20stepwise%20descriptions%20in%20visual%20content.%20Our%20methods%20show%20a%2048%25%20improvement%20in%20multiview%20echocardiographic%20diagnosis%20for%2015%20complex%20cardiac%20diseases%20and%20a%205%25%20improvement%20on%20CardiacNet-PAH%20over%20prior%20methods.%20The%20user%20study%20on%20our%20method%27s%20reasoning%20outputs%20shows%2093.33%25%20clinician%20agreement%20with%20cardiologist-like%20reasoning%20logic.%20Our%20code%20will%20be%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.08440v1&entry.124074799=Read"},
{"title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning", "author": "Jiangshan Duo and Hanyu Li and Hailin Zhang and Yudong Wang and Sujian Li and Liang Zhao", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.", "link": "http://arxiv.org/abs/2601.08468v1", "date": "2026-01-13", "relevancy": 1.9175, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4707}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JudgeRLVR%3A%20Judge%20First%2C%20Generate%20Second%20for%20Efficient%20Reasoning&body=Title%3A%20JudgeRLVR%3A%20Judge%20First%2C%20Generate%20Second%20for%20Efficient%20Reasoning%0AAuthor%3A%20Jiangshan%20Duo%20and%20Hanyu%20Li%20and%20Hailin%20Zhang%20and%20Yudong%20Wang%20and%20Sujian%20Li%20and%20Liang%20Zhao%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20become%20a%20standard%20paradigm%20for%20reasoning%20in%20Large%20Language%20Models.%20However%2C%20optimizing%20solely%20for%20final-answer%20correctness%20often%20drives%20models%20into%20aimless%2C%20verbose%20exploration%2C%20where%20they%20rely%20on%20exhaustive%20trial-and-error%20tactics%20rather%20than%20structured%20planning%20to%20reach%20solutions.%20While%20heuristic%20constraints%20like%20length%20penalties%20can%20reduce%20verbosity%2C%20they%20often%20truncate%20essential%20reasoning%20steps%2C%20creating%20a%20difficult%20trade-off%20between%20efficiency%20and%20verification.%20In%20this%20paper%2C%20we%20argue%20that%20discriminative%20capability%20is%20a%20prerequisite%20for%20efficient%20generation%3A%20by%20learning%20to%20distinguish%20valid%20solutions%2C%20a%20model%20can%20internalize%20a%20guidance%20signal%20that%20prunes%20the%20search%20space.%20We%20propose%20JudgeRLVR%2C%20a%20two-stage%20judge-then-generate%20paradigm.%20In%20the%20first%20stage%2C%20we%20train%20the%20model%20to%20judge%20solution%20responses%20with%20verifiable%20answers.%20In%20the%20second%20stage%2C%20we%20fine-tune%20the%20same%20model%20with%20vanilla%20generating%20RLVR%20initialized%20from%20the%20judge.%20Compared%20to%20Vanilla%20RLVR%20using%20the%20same%20math-domain%20training%20data%2C%20JudgeRLVR%20achieves%20a%20better%20quality--efficiency%20trade-off%20for%20Qwen3-30B-A3B%3A%20on%20in-domain%20math%2C%20it%20delivers%20about%20%2B3.7%20points%20average%20accuracy%20gain%20with%20-42%5C%25%20average%20generation%20length%3B%20on%20out-of-domain%20benchmarks%2C%20it%20delivers%20about%20%2B4.5%20points%20average%20accuracy%20improvement%2C%20demonstrating%20enhanced%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudgeRLVR%253A%2520Judge%2520First%252C%2520Generate%2520Second%2520for%2520Efficient%2520Reasoning%26entry.906535625%3DJiangshan%2520Duo%2520and%2520Hanyu%2520Li%2520and%2520Hailin%2520Zhang%2520and%2520Yudong%2520Wang%2520and%2520Sujian%2520Li%2520and%2520Liang%2520Zhao%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520become%2520a%2520standard%2520paradigm%2520for%2520reasoning%2520in%2520Large%2520Language%2520Models.%2520However%252C%2520optimizing%2520solely%2520for%2520final-answer%2520correctness%2520often%2520drives%2520models%2520into%2520aimless%252C%2520verbose%2520exploration%252C%2520where%2520they%2520rely%2520on%2520exhaustive%2520trial-and-error%2520tactics%2520rather%2520than%2520structured%2520planning%2520to%2520reach%2520solutions.%2520While%2520heuristic%2520constraints%2520like%2520length%2520penalties%2520can%2520reduce%2520verbosity%252C%2520they%2520often%2520truncate%2520essential%2520reasoning%2520steps%252C%2520creating%2520a%2520difficult%2520trade-off%2520between%2520efficiency%2520and%2520verification.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520discriminative%2520capability%2520is%2520a%2520prerequisite%2520for%2520efficient%2520generation%253A%2520by%2520learning%2520to%2520distinguish%2520valid%2520solutions%252C%2520a%2520model%2520can%2520internalize%2520a%2520guidance%2520signal%2520that%2520prunes%2520the%2520search%2520space.%2520We%2520propose%2520JudgeRLVR%252C%2520a%2520two-stage%2520judge-then-generate%2520paradigm.%2520In%2520the%2520first%2520stage%252C%2520we%2520train%2520the%2520model%2520to%2520judge%2520solution%2520responses%2520with%2520verifiable%2520answers.%2520In%2520the%2520second%2520stage%252C%2520we%2520fine-tune%2520the%2520same%2520model%2520with%2520vanilla%2520generating%2520RLVR%2520initialized%2520from%2520the%2520judge.%2520Compared%2520to%2520Vanilla%2520RLVR%2520using%2520the%2520same%2520math-domain%2520training%2520data%252C%2520JudgeRLVR%2520achieves%2520a%2520better%2520quality--efficiency%2520trade-off%2520for%2520Qwen3-30B-A3B%253A%2520on%2520in-domain%2520math%252C%2520it%2520delivers%2520about%2520%252B3.7%2520points%2520average%2520accuracy%2520gain%2520with%2520-42%255C%2525%2520average%2520generation%2520length%253B%2520on%2520out-of-domain%2520benchmarks%252C%2520it%2520delivers%2520about%2520%252B4.5%2520points%2520average%2520accuracy%2520improvement%252C%2520demonstrating%2520enhanced%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JudgeRLVR%3A%20Judge%20First%2C%20Generate%20Second%20for%20Efficient%20Reasoning&entry.906535625=Jiangshan%20Duo%20and%20Hanyu%20Li%20and%20Hailin%20Zhang%20and%20Yudong%20Wang%20and%20Sujian%20Li%20and%20Liang%20Zhao&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20become%20a%20standard%20paradigm%20for%20reasoning%20in%20Large%20Language%20Models.%20However%2C%20optimizing%20solely%20for%20final-answer%20correctness%20often%20drives%20models%20into%20aimless%2C%20verbose%20exploration%2C%20where%20they%20rely%20on%20exhaustive%20trial-and-error%20tactics%20rather%20than%20structured%20planning%20to%20reach%20solutions.%20While%20heuristic%20constraints%20like%20length%20penalties%20can%20reduce%20verbosity%2C%20they%20often%20truncate%20essential%20reasoning%20steps%2C%20creating%20a%20difficult%20trade-off%20between%20efficiency%20and%20verification.%20In%20this%20paper%2C%20we%20argue%20that%20discriminative%20capability%20is%20a%20prerequisite%20for%20efficient%20generation%3A%20by%20learning%20to%20distinguish%20valid%20solutions%2C%20a%20model%20can%20internalize%20a%20guidance%20signal%20that%20prunes%20the%20search%20space.%20We%20propose%20JudgeRLVR%2C%20a%20two-stage%20judge-then-generate%20paradigm.%20In%20the%20first%20stage%2C%20we%20train%20the%20model%20to%20judge%20solution%20responses%20with%20verifiable%20answers.%20In%20the%20second%20stage%2C%20we%20fine-tune%20the%20same%20model%20with%20vanilla%20generating%20RLVR%20initialized%20from%20the%20judge.%20Compared%20to%20Vanilla%20RLVR%20using%20the%20same%20math-domain%20training%20data%2C%20JudgeRLVR%20achieves%20a%20better%20quality--efficiency%20trade-off%20for%20Qwen3-30B-A3B%3A%20on%20in-domain%20math%2C%20it%20delivers%20about%20%2B3.7%20points%20average%20accuracy%20gain%20with%20-42%5C%25%20average%20generation%20length%3B%20on%20out-of-domain%20benchmarks%2C%20it%20delivers%20about%20%2B4.5%20points%20average%20accuracy%20improvement%2C%20demonstrating%20enhanced%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2601.08468v1&entry.124074799=Read"},
{"title": "SafePro: Evaluating the Safety of Professional-Level AI Agents", "author": "Kaiwen Zhou and Shreedhar Jangam and Ashwin Nagarajan and Tejas Polu and Suhas Oruganti and Chengzhi Liu and Ching-Chen Kuo and Yuting Zheng and Sravana Narayanaraju and Xin Eric Wang", "abstract": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.", "link": "http://arxiv.org/abs/2601.06663v2", "date": "2026-01-13", "relevancy": 1.6604, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafePro%3A%20Evaluating%20the%20Safety%20of%20Professional-Level%20AI%20Agents&body=Title%3A%20SafePro%3A%20Evaluating%20the%20Safety%20of%20Professional-Level%20AI%20Agents%0AAuthor%3A%20Kaiwen%20Zhou%20and%20Shreedhar%20Jangam%20and%20Ashwin%20Nagarajan%20and%20Tejas%20Polu%20and%20Suhas%20Oruganti%20and%20Chengzhi%20Liu%20and%20Ching-Chen%20Kuo%20and%20Yuting%20Zheng%20and%20Sravana%20Narayanaraju%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20Large%20language%20model-based%20agents%20are%20rapidly%20evolving%20from%20simple%20conversational%20assistants%20into%20autonomous%20systems%20capable%20of%20performing%20complex%2C%20professional-level%20tasks%20in%20various%20domains.%20While%20these%20advancements%20promise%20significant%20productivity%20gains%2C%20they%20also%20introduce%20critical%20safety%20risks%20that%20remain%20under-explored.%20Existing%20safety%20evaluations%20primarily%20focus%20on%20simple%2C%20daily%20assistance%20tasks%2C%20failing%20to%20capture%20the%20intricate%20decision-making%20processes%20and%20potential%20consequences%20of%20misaligned%20behaviors%20in%20professional%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BSafePro%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20safety%20alignment%20of%20AI%20agents%20performing%20professional%20activities.%20SafePro%20features%20a%20dataset%20of%20high-complexity%20tasks%20across%20diverse%20professional%20domains%20with%20safety%20risks%2C%20developed%20through%20a%20rigorous%20iterative%20creation%20and%20review%20process.%20Our%20evaluation%20of%20state-of-the-art%20AI%20models%20reveals%20significant%20safety%20vulnerabilities%20and%20uncovers%20new%20unsafe%20behaviors%20in%20professional%20contexts.%20We%20further%20show%20that%20these%20models%20exhibit%20both%20insufficient%20safety%20judgment%20and%20weak%20safety%20alignment%20when%20executing%20complex%20professional%20tasks.%20In%20addition%2C%20we%20investigate%20safety%20mitigation%20strategies%20for%20improving%20agent%20safety%20in%20these%20scenarios%20and%20observe%20encouraging%20improvements.%20Together%2C%20our%20findings%20highlight%20the%20urgent%20need%20for%20robust%20safety%20mechanisms%20tailored%20to%20the%20next%20generation%20of%20professional%20AI%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafePro%253A%2520Evaluating%2520the%2520Safety%2520of%2520Professional-Level%2520AI%2520Agents%26entry.906535625%3DKaiwen%2520Zhou%2520and%2520Shreedhar%2520Jangam%2520and%2520Ashwin%2520Nagarajan%2520and%2520Tejas%2520Polu%2520and%2520Suhas%2520Oruganti%2520and%2520Chengzhi%2520Liu%2520and%2520Ching-Chen%2520Kuo%2520and%2520Yuting%2520Zheng%2520and%2520Sravana%2520Narayanaraju%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DLarge%2520language%2520model-based%2520agents%2520are%2520rapidly%2520evolving%2520from%2520simple%2520conversational%2520assistants%2520into%2520autonomous%2520systems%2520capable%2520of%2520performing%2520complex%252C%2520professional-level%2520tasks%2520in%2520various%2520domains.%2520While%2520these%2520advancements%2520promise%2520significant%2520productivity%2520gains%252C%2520they%2520also%2520introduce%2520critical%2520safety%2520risks%2520that%2520remain%2520under-explored.%2520Existing%2520safety%2520evaluations%2520primarily%2520focus%2520on%2520simple%252C%2520daily%2520assistance%2520tasks%252C%2520failing%2520to%2520capture%2520the%2520intricate%2520decision-making%2520processes%2520and%2520potential%2520consequences%2520of%2520misaligned%2520behaviors%2520in%2520professional%2520settings.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520%255Ctextbf%257BSafePro%257D%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520safety%2520alignment%2520of%2520AI%2520agents%2520performing%2520professional%2520activities.%2520SafePro%2520features%2520a%2520dataset%2520of%2520high-complexity%2520tasks%2520across%2520diverse%2520professional%2520domains%2520with%2520safety%2520risks%252C%2520developed%2520through%2520a%2520rigorous%2520iterative%2520creation%2520and%2520review%2520process.%2520Our%2520evaluation%2520of%2520state-of-the-art%2520AI%2520models%2520reveals%2520significant%2520safety%2520vulnerabilities%2520and%2520uncovers%2520new%2520unsafe%2520behaviors%2520in%2520professional%2520contexts.%2520We%2520further%2520show%2520that%2520these%2520models%2520exhibit%2520both%2520insufficient%2520safety%2520judgment%2520and%2520weak%2520safety%2520alignment%2520when%2520executing%2520complex%2520professional%2520tasks.%2520In%2520addition%252C%2520we%2520investigate%2520safety%2520mitigation%2520strategies%2520for%2520improving%2520agent%2520safety%2520in%2520these%2520scenarios%2520and%2520observe%2520encouraging%2520improvements.%2520Together%252C%2520our%2520findings%2520highlight%2520the%2520urgent%2520need%2520for%2520robust%2520safety%2520mechanisms%2520tailored%2520to%2520the%2520next%2520generation%2520of%2520professional%2520AI%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafePro%3A%20Evaluating%20the%20Safety%20of%20Professional-Level%20AI%20Agents&entry.906535625=Kaiwen%20Zhou%20and%20Shreedhar%20Jangam%20and%20Ashwin%20Nagarajan%20and%20Tejas%20Polu%20and%20Suhas%20Oruganti%20and%20Chengzhi%20Liu%20and%20Ching-Chen%20Kuo%20and%20Yuting%20Zheng%20and%20Sravana%20Narayanaraju%20and%20Xin%20Eric%20Wang&entry.1292438233=Large%20language%20model-based%20agents%20are%20rapidly%20evolving%20from%20simple%20conversational%20assistants%20into%20autonomous%20systems%20capable%20of%20performing%20complex%2C%20professional-level%20tasks%20in%20various%20domains.%20While%20these%20advancements%20promise%20significant%20productivity%20gains%2C%20they%20also%20introduce%20critical%20safety%20risks%20that%20remain%20under-explored.%20Existing%20safety%20evaluations%20primarily%20focus%20on%20simple%2C%20daily%20assistance%20tasks%2C%20failing%20to%20capture%20the%20intricate%20decision-making%20processes%20and%20potential%20consequences%20of%20misaligned%20behaviors%20in%20professional%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BSafePro%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20safety%20alignment%20of%20AI%20agents%20performing%20professional%20activities.%20SafePro%20features%20a%20dataset%20of%20high-complexity%20tasks%20across%20diverse%20professional%20domains%20with%20safety%20risks%2C%20developed%20through%20a%20rigorous%20iterative%20creation%20and%20review%20process.%20Our%20evaluation%20of%20state-of-the-art%20AI%20models%20reveals%20significant%20safety%20vulnerabilities%20and%20uncovers%20new%20unsafe%20behaviors%20in%20professional%20contexts.%20We%20further%20show%20that%20these%20models%20exhibit%20both%20insufficient%20safety%20judgment%20and%20weak%20safety%20alignment%20when%20executing%20complex%20professional%20tasks.%20In%20addition%2C%20we%20investigate%20safety%20mitigation%20strategies%20for%20improving%20agent%20safety%20in%20these%20scenarios%20and%20observe%20encouraging%20improvements.%20Together%2C%20our%20findings%20highlight%20the%20urgent%20need%20for%20robust%20safety%20mechanisms%20tailored%20to%20the%20next%20generation%20of%20professional%20AI%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.06663v2&entry.124074799=Read"},
{"title": "Generative Adversarial Networks for Image Super-Resolution: A Survey", "author": "Ziang Wu and Xuanyu Zhang and Yinbo Yu and Qi Zhu and Jerry Chun-Wei Lin and Chunwei Tian", "abstract": "Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We begin by surveying the development of GANs and popular GAN variants for image-related applications, and then analyze motivations, implementations and differences of GANs based optimization methods and discriminative learning for image super-resolution in terms of supervised, semi-supervised and unsupervised manners, where these GANs are analyzed via integrating different network architectures, prior knowledge, loss functions and multiple tasks. Secondly, we compare the performances of these popular GANs on public datasets via quantitative and qualitative analysis in SISR. Finally, we highlight challenges of GANs and potential research points for SISR.", "link": "http://arxiv.org/abs/2204.13620v5", "date": "2026-01-13", "relevancy": 1.9448, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4885}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20for%20Image%20Super-Resolution%3A%20A%20Survey&body=Title%3A%20Generative%20Adversarial%20Networks%20for%20Image%20Super-Resolution%3A%20A%20Survey%0AAuthor%3A%20Ziang%20Wu%20and%20Xuanyu%20Zhang%20and%20Yinbo%20Yu%20and%20Qi%20Zhu%20and%20Jerry%20Chun-Wei%20Lin%20and%20Chunwei%20Tian%0AAbstract%3A%20Single%20image%20super-resolution%20%28SISR%29%20has%20played%20an%20important%20role%20in%20the%20field%20of%20image%20processing.%20Recent%20generative%20adversarial%20networks%20%28GANs%29%20can%20achieve%20excellent%20results%20on%20low-resolution%20images.%20However%2C%20there%20are%20little%20literatures%20summarizing%20different%20GANs%20in%20SISR.%20In%20this%20paper%2C%20we%20conduct%20a%20comparative%20study%20of%20GANs%20from%20different%20perspectives.%20We%20begin%20by%20surveying%20the%20development%20of%20GANs%20and%20popular%20GAN%20variants%20for%20image-related%20applications%2C%20and%20then%20analyze%20motivations%2C%20implementations%20and%20differences%20of%20GANs%20based%20optimization%20methods%20and%20discriminative%20learning%20for%20image%20super-resolution%20in%20terms%20of%20supervised%2C%20semi-supervised%20and%20unsupervised%20manners%2C%20where%20these%20GANs%20are%20analyzed%20via%20integrating%20different%20network%20architectures%2C%20prior%20knowledge%2C%20loss%20functions%20and%20multiple%20tasks.%20Secondly%2C%20we%20compare%20the%20performances%20of%20these%20popular%20GANs%20on%20public%20datasets%20via%20quantitative%20and%20qualitative%20analysis%20in%20SISR.%20Finally%2C%20we%20highlight%20challenges%20of%20GANs%20and%20potential%20research%20points%20for%20SISR.%0ALink%3A%20http%3A//arxiv.org/abs/2204.13620v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520for%2520Image%2520Super-Resolution%253A%2520A%2520Survey%26entry.906535625%3DZiang%2520Wu%2520and%2520Xuanyu%2520Zhang%2520and%2520Yinbo%2520Yu%2520and%2520Qi%2520Zhu%2520and%2520Jerry%2520Chun-Wei%2520Lin%2520and%2520Chunwei%2520Tian%26entry.1292438233%3DSingle%2520image%2520super-resolution%2520%2528SISR%2529%2520has%2520played%2520an%2520important%2520role%2520in%2520the%2520field%2520of%2520image%2520processing.%2520Recent%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%2520can%2520achieve%2520excellent%2520results%2520on%2520low-resolution%2520images.%2520However%252C%2520there%2520are%2520little%2520literatures%2520summarizing%2520different%2520GANs%2520in%2520SISR.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520comparative%2520study%2520of%2520GANs%2520from%2520different%2520perspectives.%2520We%2520begin%2520by%2520surveying%2520the%2520development%2520of%2520GANs%2520and%2520popular%2520GAN%2520variants%2520for%2520image-related%2520applications%252C%2520and%2520then%2520analyze%2520motivations%252C%2520implementations%2520and%2520differences%2520of%2520GANs%2520based%2520optimization%2520methods%2520and%2520discriminative%2520learning%2520for%2520image%2520super-resolution%2520in%2520terms%2520of%2520supervised%252C%2520semi-supervised%2520and%2520unsupervised%2520manners%252C%2520where%2520these%2520GANs%2520are%2520analyzed%2520via%2520integrating%2520different%2520network%2520architectures%252C%2520prior%2520knowledge%252C%2520loss%2520functions%2520and%2520multiple%2520tasks.%2520Secondly%252C%2520we%2520compare%2520the%2520performances%2520of%2520these%2520popular%2520GANs%2520on%2520public%2520datasets%2520via%2520quantitative%2520and%2520qualitative%2520analysis%2520in%2520SISR.%2520Finally%252C%2520we%2520highlight%2520challenges%2520of%2520GANs%2520and%2520potential%2520research%2520points%2520for%2520SISR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.13620v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20for%20Image%20Super-Resolution%3A%20A%20Survey&entry.906535625=Ziang%20Wu%20and%20Xuanyu%20Zhang%20and%20Yinbo%20Yu%20and%20Qi%20Zhu%20and%20Jerry%20Chun-Wei%20Lin%20and%20Chunwei%20Tian&entry.1292438233=Single%20image%20super-resolution%20%28SISR%29%20has%20played%20an%20important%20role%20in%20the%20field%20of%20image%20processing.%20Recent%20generative%20adversarial%20networks%20%28GANs%29%20can%20achieve%20excellent%20results%20on%20low-resolution%20images.%20However%2C%20there%20are%20little%20literatures%20summarizing%20different%20GANs%20in%20SISR.%20In%20this%20paper%2C%20we%20conduct%20a%20comparative%20study%20of%20GANs%20from%20different%20perspectives.%20We%20begin%20by%20surveying%20the%20development%20of%20GANs%20and%20popular%20GAN%20variants%20for%20image-related%20applications%2C%20and%20then%20analyze%20motivations%2C%20implementations%20and%20differences%20of%20GANs%20based%20optimization%20methods%20and%20discriminative%20learning%20for%20image%20super-resolution%20in%20terms%20of%20supervised%2C%20semi-supervised%20and%20unsupervised%20manners%2C%20where%20these%20GANs%20are%20analyzed%20via%20integrating%20different%20network%20architectures%2C%20prior%20knowledge%2C%20loss%20functions%20and%20multiple%20tasks.%20Secondly%2C%20we%20compare%20the%20performances%20of%20these%20popular%20GANs%20on%20public%20datasets%20via%20quantitative%20and%20qualitative%20analysis%20in%20SISR.%20Finally%2C%20we%20highlight%20challenges%20of%20GANs%20and%20potential%20research%20points%20for%20SISR.&entry.1838667208=http%3A//arxiv.org/abs/2204.13620v5&entry.124074799=Read"},
{"title": "QP-Based Control of an Underactuated Aerial Manipulator under Constraints", "author": "Nesserine Laribi and Mohammed Rida Mokhtari and Abdelaziz Benallegue and Abdelhafid El-Hadri and Mehdi Benallegue", "abstract": "This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.", "link": "http://arxiv.org/abs/2601.08523v1", "date": "2026-01-13", "relevancy": 1.9908, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5102}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QP-Based%20Control%20of%20an%20Underactuated%20Aerial%20Manipulator%20under%20Constraints&body=Title%3A%20QP-Based%20Control%20of%20an%20Underactuated%20Aerial%20Manipulator%20under%20Constraints%0AAuthor%3A%20Nesserine%20Laribi%20and%20Mohammed%20Rida%20Mokhtari%20and%20Abdelaziz%20Benallegue%20and%20Abdelhafid%20El-Hadri%20and%20Mehdi%20Benallegue%0AAbstract%3A%20This%20paper%20presents%20a%20constraint-aware%20control%20framework%20for%20underactuated%20aerial%20manipulators%2C%20enabling%20accurate%20end-effector%20trajectory%20tracking%20while%20explicitly%20accounting%20for%20safety%20and%20feasibility%20constraints.%20The%20control%20problem%20is%20formulated%20as%20a%20quadratic%20program%20that%20computes%20dynamically%20consistent%20generalized%20accelerations%20subject%20to%20underactuation%2C%20actuator%20bounds%2C%20and%20system%20constraints.%20To%20enhance%20robustness%20against%20disturbances%2C%20modeling%20uncertainties%2C%20and%20steady-state%20errors%2C%20a%20passivity-based%20integral%20action%20is%20incorporated%20at%20the%20torque%20level%20without%20compromising%20feasibility.%20The%20effectiveness%20of%20the%20proposed%20approach%20is%20demonstrated%20through%20high-fidelity%20physics-based%20simulations%2C%20which%20include%20parameter%20perturbations%2C%20viscous%20joint%20friction%2C%20and%20realistic%20sensing%20and%20state-estimation%20effects.%20This%20demonstrates%20accurate%20tracking%2C%20smooth%20control%20inputs%2C%20and%20reliable%20constraint%20satisfaction%20under%20realistic%20operating%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQP-Based%2520Control%2520of%2520an%2520Underactuated%2520Aerial%2520Manipulator%2520under%2520Constraints%26entry.906535625%3DNesserine%2520Laribi%2520and%2520Mohammed%2520Rida%2520Mokhtari%2520and%2520Abdelaziz%2520Benallegue%2520and%2520Abdelhafid%2520El-Hadri%2520and%2520Mehdi%2520Benallegue%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520constraint-aware%2520control%2520framework%2520for%2520underactuated%2520aerial%2520manipulators%252C%2520enabling%2520accurate%2520end-effector%2520trajectory%2520tracking%2520while%2520explicitly%2520accounting%2520for%2520safety%2520and%2520feasibility%2520constraints.%2520The%2520control%2520problem%2520is%2520formulated%2520as%2520a%2520quadratic%2520program%2520that%2520computes%2520dynamically%2520consistent%2520generalized%2520accelerations%2520subject%2520to%2520underactuation%252C%2520actuator%2520bounds%252C%2520and%2520system%2520constraints.%2520To%2520enhance%2520robustness%2520against%2520disturbances%252C%2520modeling%2520uncertainties%252C%2520and%2520steady-state%2520errors%252C%2520a%2520passivity-based%2520integral%2520action%2520is%2520incorporated%2520at%2520the%2520torque%2520level%2520without%2520compromising%2520feasibility.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520is%2520demonstrated%2520through%2520high-fidelity%2520physics-based%2520simulations%252C%2520which%2520include%2520parameter%2520perturbations%252C%2520viscous%2520joint%2520friction%252C%2520and%2520realistic%2520sensing%2520and%2520state-estimation%2520effects.%2520This%2520demonstrates%2520accurate%2520tracking%252C%2520smooth%2520control%2520inputs%252C%2520and%2520reliable%2520constraint%2520satisfaction%2520under%2520realistic%2520operating%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QP-Based%20Control%20of%20an%20Underactuated%20Aerial%20Manipulator%20under%20Constraints&entry.906535625=Nesserine%20Laribi%20and%20Mohammed%20Rida%20Mokhtari%20and%20Abdelaziz%20Benallegue%20and%20Abdelhafid%20El-Hadri%20and%20Mehdi%20Benallegue&entry.1292438233=This%20paper%20presents%20a%20constraint-aware%20control%20framework%20for%20underactuated%20aerial%20manipulators%2C%20enabling%20accurate%20end-effector%20trajectory%20tracking%20while%20explicitly%20accounting%20for%20safety%20and%20feasibility%20constraints.%20The%20control%20problem%20is%20formulated%20as%20a%20quadratic%20program%20that%20computes%20dynamically%20consistent%20generalized%20accelerations%20subject%20to%20underactuation%2C%20actuator%20bounds%2C%20and%20system%20constraints.%20To%20enhance%20robustness%20against%20disturbances%2C%20modeling%20uncertainties%2C%20and%20steady-state%20errors%2C%20a%20passivity-based%20integral%20action%20is%20incorporated%20at%20the%20torque%20level%20without%20compromising%20feasibility.%20The%20effectiveness%20of%20the%20proposed%20approach%20is%20demonstrated%20through%20high-fidelity%20physics-based%20simulations%2C%20which%20include%20parameter%20perturbations%2C%20viscous%20joint%20friction%2C%20and%20realistic%20sensing%20and%20state-estimation%20effects.%20This%20demonstrates%20accurate%20tracking%2C%20smooth%20control%20inputs%2C%20and%20reliable%20constraint%20satisfaction%20under%20realistic%20operating%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2601.08523v1&entry.124074799=Read"},
{"title": "Automated Lesion Segmentation of Stroke MRI Using nnU-Net: A Comprehensive External Validation Across Acute and Chronic Lesions", "author": "Tammar Truzman and Matthew A. Lambon Ralph and Ajay D. Halai", "abstract": "Accurate and generalisable segmentation of stroke lesions from magnetic resonance imaging (MRI) is essential for advancing clinical research, prognostic modelling, and personalised interventions. Although deep learning has improved automated lesion delineation, many existing models are optimised for narrow imaging contexts and generalise poorly to independent datasets, modalities, and stroke stages. Here, we systematically evaluated stroke lesion segmentation using the nnU-Net framework across multiple heterogeneous, publicly available MRI datasets spanning acute and chronic stroke. Models were trained and tested on diffusion-weighted imaging (DWI), fluid-attenuated inversion recovery (FLAIR), and T1-weighted MRI, and evaluated on independent datasets. Across stroke stages, models showed robust generalisation, with segmentation accuracy approaching reported inter-rater reliability. Performance varied with imaging modality and training data characteristics. In acute stroke, DWI-trained models consistently outperformed FLAIR-based models, with only modest gains from multimodal combinations. In chronic stroke, increasing training set size improved performance, with diminishing returns beyond several hundred cases. Lesion volume was a key determinant of accuracy: smaller lesions were harder to segment, and models trained on restricted volume ranges generalised poorly. MRI image quality further constrained generalisability: models trained on lower-quality scans transferred poorly, whereas those trained on higher-quality data generalised well to noisier images. Discrepancies between predictions and reference masks were often attributable to limitations in manual annotations. Together, these findings show that automated lesion segmentation can approach human-level performance while identifying key factors governing generalisability and informing the development of lesion segmentation tools.", "link": "http://arxiv.org/abs/2601.08701v1", "date": "2026-01-13", "relevancy": 1.9576, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5024}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.481}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Lesion%20Segmentation%20of%20Stroke%20MRI%20Using%20nnU-Net%3A%20A%20Comprehensive%20External%20Validation%20Across%20Acute%20and%20Chronic%20Lesions&body=Title%3A%20Automated%20Lesion%20Segmentation%20of%20Stroke%20MRI%20Using%20nnU-Net%3A%20A%20Comprehensive%20External%20Validation%20Across%20Acute%20and%20Chronic%20Lesions%0AAuthor%3A%20Tammar%20Truzman%20and%20Matthew%20A.%20Lambon%20Ralph%20and%20Ajay%20D.%20Halai%0AAbstract%3A%20Accurate%20and%20generalisable%20segmentation%20of%20stroke%20lesions%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20advancing%20clinical%20research%2C%20prognostic%20modelling%2C%20and%20personalised%20interventions.%20Although%20deep%20learning%20has%20improved%20automated%20lesion%20delineation%2C%20many%20existing%20models%20are%20optimised%20for%20narrow%20imaging%20contexts%20and%20generalise%20poorly%20to%20independent%20datasets%2C%20modalities%2C%20and%20stroke%20stages.%20Here%2C%20we%20systematically%20evaluated%20stroke%20lesion%20segmentation%20using%20the%20nnU-Net%20framework%20across%20multiple%20heterogeneous%2C%20publicly%20available%20MRI%20datasets%20spanning%20acute%20and%20chronic%20stroke.%20Models%20were%20trained%20and%20tested%20on%20diffusion-weighted%20imaging%20%28DWI%29%2C%20fluid-attenuated%20inversion%20recovery%20%28FLAIR%29%2C%20and%20T1-weighted%20MRI%2C%20and%20evaluated%20on%20independent%20datasets.%20Across%20stroke%20stages%2C%20models%20showed%20robust%20generalisation%2C%20with%20segmentation%20accuracy%20approaching%20reported%20inter-rater%20reliability.%20Performance%20varied%20with%20imaging%20modality%20and%20training%20data%20characteristics.%20In%20acute%20stroke%2C%20DWI-trained%20models%20consistently%20outperformed%20FLAIR-based%20models%2C%20with%20only%20modest%20gains%20from%20multimodal%20combinations.%20In%20chronic%20stroke%2C%20increasing%20training%20set%20size%20improved%20performance%2C%20with%20diminishing%20returns%20beyond%20several%20hundred%20cases.%20Lesion%20volume%20was%20a%20key%20determinant%20of%20accuracy%3A%20smaller%20lesions%20were%20harder%20to%20segment%2C%20and%20models%20trained%20on%20restricted%20volume%20ranges%20generalised%20poorly.%20MRI%20image%20quality%20further%20constrained%20generalisability%3A%20models%20trained%20on%20lower-quality%20scans%20transferred%20poorly%2C%20whereas%20those%20trained%20on%20higher-quality%20data%20generalised%20well%20to%20noisier%20images.%20Discrepancies%20between%20predictions%20and%20reference%20masks%20were%20often%20attributable%20to%20limitations%20in%20manual%20annotations.%20Together%2C%20these%20findings%20show%20that%20automated%20lesion%20segmentation%20can%20approach%20human-level%20performance%20while%20identifying%20key%20factors%20governing%20generalisability%20and%20informing%20the%20development%20of%20lesion%20segmentation%20tools.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Lesion%2520Segmentation%2520of%2520Stroke%2520MRI%2520Using%2520nnU-Net%253A%2520A%2520Comprehensive%2520External%2520Validation%2520Across%2520Acute%2520and%2520Chronic%2520Lesions%26entry.906535625%3DTammar%2520Truzman%2520and%2520Matthew%2520A.%2520Lambon%2520Ralph%2520and%2520Ajay%2520D.%2520Halai%26entry.1292438233%3DAccurate%2520and%2520generalisable%2520segmentation%2520of%2520stroke%2520lesions%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520essential%2520for%2520advancing%2520clinical%2520research%252C%2520prognostic%2520modelling%252C%2520and%2520personalised%2520interventions.%2520Although%2520deep%2520learning%2520has%2520improved%2520automated%2520lesion%2520delineation%252C%2520many%2520existing%2520models%2520are%2520optimised%2520for%2520narrow%2520imaging%2520contexts%2520and%2520generalise%2520poorly%2520to%2520independent%2520datasets%252C%2520modalities%252C%2520and%2520stroke%2520stages.%2520Here%252C%2520we%2520systematically%2520evaluated%2520stroke%2520lesion%2520segmentation%2520using%2520the%2520nnU-Net%2520framework%2520across%2520multiple%2520heterogeneous%252C%2520publicly%2520available%2520MRI%2520datasets%2520spanning%2520acute%2520and%2520chronic%2520stroke.%2520Models%2520were%2520trained%2520and%2520tested%2520on%2520diffusion-weighted%2520imaging%2520%2528DWI%2529%252C%2520fluid-attenuated%2520inversion%2520recovery%2520%2528FLAIR%2529%252C%2520and%2520T1-weighted%2520MRI%252C%2520and%2520evaluated%2520on%2520independent%2520datasets.%2520Across%2520stroke%2520stages%252C%2520models%2520showed%2520robust%2520generalisation%252C%2520with%2520segmentation%2520accuracy%2520approaching%2520reported%2520inter-rater%2520reliability.%2520Performance%2520varied%2520with%2520imaging%2520modality%2520and%2520training%2520data%2520characteristics.%2520In%2520acute%2520stroke%252C%2520DWI-trained%2520models%2520consistently%2520outperformed%2520FLAIR-based%2520models%252C%2520with%2520only%2520modest%2520gains%2520from%2520multimodal%2520combinations.%2520In%2520chronic%2520stroke%252C%2520increasing%2520training%2520set%2520size%2520improved%2520performance%252C%2520with%2520diminishing%2520returns%2520beyond%2520several%2520hundred%2520cases.%2520Lesion%2520volume%2520was%2520a%2520key%2520determinant%2520of%2520accuracy%253A%2520smaller%2520lesions%2520were%2520harder%2520to%2520segment%252C%2520and%2520models%2520trained%2520on%2520restricted%2520volume%2520ranges%2520generalised%2520poorly.%2520MRI%2520image%2520quality%2520further%2520constrained%2520generalisability%253A%2520models%2520trained%2520on%2520lower-quality%2520scans%2520transferred%2520poorly%252C%2520whereas%2520those%2520trained%2520on%2520higher-quality%2520data%2520generalised%2520well%2520to%2520noisier%2520images.%2520Discrepancies%2520between%2520predictions%2520and%2520reference%2520masks%2520were%2520often%2520attributable%2520to%2520limitations%2520in%2520manual%2520annotations.%2520Together%252C%2520these%2520findings%2520show%2520that%2520automated%2520lesion%2520segmentation%2520can%2520approach%2520human-level%2520performance%2520while%2520identifying%2520key%2520factors%2520governing%2520generalisability%2520and%2520informing%2520the%2520development%2520of%2520lesion%2520segmentation%2520tools.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Lesion%20Segmentation%20of%20Stroke%20MRI%20Using%20nnU-Net%3A%20A%20Comprehensive%20External%20Validation%20Across%20Acute%20and%20Chronic%20Lesions&entry.906535625=Tammar%20Truzman%20and%20Matthew%20A.%20Lambon%20Ralph%20and%20Ajay%20D.%20Halai&entry.1292438233=Accurate%20and%20generalisable%20segmentation%20of%20stroke%20lesions%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20advancing%20clinical%20research%2C%20prognostic%20modelling%2C%20and%20personalised%20interventions.%20Although%20deep%20learning%20has%20improved%20automated%20lesion%20delineation%2C%20many%20existing%20models%20are%20optimised%20for%20narrow%20imaging%20contexts%20and%20generalise%20poorly%20to%20independent%20datasets%2C%20modalities%2C%20and%20stroke%20stages.%20Here%2C%20we%20systematically%20evaluated%20stroke%20lesion%20segmentation%20using%20the%20nnU-Net%20framework%20across%20multiple%20heterogeneous%2C%20publicly%20available%20MRI%20datasets%20spanning%20acute%20and%20chronic%20stroke.%20Models%20were%20trained%20and%20tested%20on%20diffusion-weighted%20imaging%20%28DWI%29%2C%20fluid-attenuated%20inversion%20recovery%20%28FLAIR%29%2C%20and%20T1-weighted%20MRI%2C%20and%20evaluated%20on%20independent%20datasets.%20Across%20stroke%20stages%2C%20models%20showed%20robust%20generalisation%2C%20with%20segmentation%20accuracy%20approaching%20reported%20inter-rater%20reliability.%20Performance%20varied%20with%20imaging%20modality%20and%20training%20data%20characteristics.%20In%20acute%20stroke%2C%20DWI-trained%20models%20consistently%20outperformed%20FLAIR-based%20models%2C%20with%20only%20modest%20gains%20from%20multimodal%20combinations.%20In%20chronic%20stroke%2C%20increasing%20training%20set%20size%20improved%20performance%2C%20with%20diminishing%20returns%20beyond%20several%20hundred%20cases.%20Lesion%20volume%20was%20a%20key%20determinant%20of%20accuracy%3A%20smaller%20lesions%20were%20harder%20to%20segment%2C%20and%20models%20trained%20on%20restricted%20volume%20ranges%20generalised%20poorly.%20MRI%20image%20quality%20further%20constrained%20generalisability%3A%20models%20trained%20on%20lower-quality%20scans%20transferred%20poorly%2C%20whereas%20those%20trained%20on%20higher-quality%20data%20generalised%20well%20to%20noisier%20images.%20Discrepancies%20between%20predictions%20and%20reference%20masks%20were%20often%20attributable%20to%20limitations%20in%20manual%20annotations.%20Together%2C%20these%20findings%20show%20that%20automated%20lesion%20segmentation%20can%20approach%20human-level%20performance%20while%20identifying%20key%20factors%20governing%20generalisability%20and%20informing%20the%20development%20of%20lesion%20segmentation%20tools.&entry.1838667208=http%3A//arxiv.org/abs/2601.08701v1&entry.124074799=Read"},
{"title": "TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations", "author": "Hamid Gadirov and Martijn Westra and Steffen Frey", "abstract": "Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized K\u00e1rm\u00e1n vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.", "link": "http://arxiv.org/abs/2601.08659v1", "date": "2026-01-13", "relevancy": 2.0002, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5095}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4964}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20Reconstruction-Based%20Anomaly%20Detection%20in%20Ensemble%20and%20Time-Dependent%20Simulations&body=Title%3A%20TRACE%3A%20Reconstruction-Based%20Anomaly%20Detection%20in%20Ensemble%20and%20Time-Dependent%20Simulations%0AAuthor%3A%20Hamid%20Gadirov%20and%20Martijn%20Westra%20and%20Steffen%20Frey%0AAbstract%3A%20Detecting%20anomalies%20in%20high-dimensional%2C%20time-dependent%20simulation%20data%20is%20challenging%20due%20to%20complex%20spatial%20and%20temporal%20dynamics.%20We%20study%20reconstruction-based%20anomaly%20detection%20for%20ensemble%20data%20from%20parameterized%20K%C3%A1rm%C3%A1n%20vortex%20street%20simulations%20using%20convolutional%20autoencoders.%20We%20compare%20a%202D%20autoencoder%20operating%20on%20individual%20frames%20with%20a%203D%20autoencoder%20that%20processes%20short%20temporal%20stacks.%20The%202D%20model%20identifies%20localized%20spatial%20irregularities%20in%20single%20time%20steps%2C%20while%20the%203D%20model%20exploits%20spatio-temporal%20context%20to%20detect%20anomalous%20motion%20patterns%20and%20reduces%20redundant%20detections%20across%20time.%20We%20further%20evaluate%20volumetric%20time-dependent%20data%20and%20find%20that%20reconstruction%20errors%20are%20strongly%20influenced%20by%20the%20spatial%20distribution%20of%20mass%2C%20with%20highly%20concentrated%20regions%20yielding%20larger%20errors%20than%20dispersed%20configurations.%20Our%20results%20highlight%20the%20importance%20of%20temporal%20context%20for%20robust%20anomaly%20detection%20in%20dynamic%20simulations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520Reconstruction-Based%2520Anomaly%2520Detection%2520in%2520Ensemble%2520and%2520Time-Dependent%2520Simulations%26entry.906535625%3DHamid%2520Gadirov%2520and%2520Martijn%2520Westra%2520and%2520Steffen%2520Frey%26entry.1292438233%3DDetecting%2520anomalies%2520in%2520high-dimensional%252C%2520time-dependent%2520simulation%2520data%2520is%2520challenging%2520due%2520to%2520complex%2520spatial%2520and%2520temporal%2520dynamics.%2520We%2520study%2520reconstruction-based%2520anomaly%2520detection%2520for%2520ensemble%2520data%2520from%2520parameterized%2520K%25C3%25A1rm%25C3%25A1n%2520vortex%2520street%2520simulations%2520using%2520convolutional%2520autoencoders.%2520We%2520compare%2520a%25202D%2520autoencoder%2520operating%2520on%2520individual%2520frames%2520with%2520a%25203D%2520autoencoder%2520that%2520processes%2520short%2520temporal%2520stacks.%2520The%25202D%2520model%2520identifies%2520localized%2520spatial%2520irregularities%2520in%2520single%2520time%2520steps%252C%2520while%2520the%25203D%2520model%2520exploits%2520spatio-temporal%2520context%2520to%2520detect%2520anomalous%2520motion%2520patterns%2520and%2520reduces%2520redundant%2520detections%2520across%2520time.%2520We%2520further%2520evaluate%2520volumetric%2520time-dependent%2520data%2520and%2520find%2520that%2520reconstruction%2520errors%2520are%2520strongly%2520influenced%2520by%2520the%2520spatial%2520distribution%2520of%2520mass%252C%2520with%2520highly%2520concentrated%2520regions%2520yielding%2520larger%2520errors%2520than%2520dispersed%2520configurations.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520temporal%2520context%2520for%2520robust%2520anomaly%2520detection%2520in%2520dynamic%2520simulations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20Reconstruction-Based%20Anomaly%20Detection%20in%20Ensemble%20and%20Time-Dependent%20Simulations&entry.906535625=Hamid%20Gadirov%20and%20Martijn%20Westra%20and%20Steffen%20Frey&entry.1292438233=Detecting%20anomalies%20in%20high-dimensional%2C%20time-dependent%20simulation%20data%20is%20challenging%20due%20to%20complex%20spatial%20and%20temporal%20dynamics.%20We%20study%20reconstruction-based%20anomaly%20detection%20for%20ensemble%20data%20from%20parameterized%20K%C3%A1rm%C3%A1n%20vortex%20street%20simulations%20using%20convolutional%20autoencoders.%20We%20compare%20a%202D%20autoencoder%20operating%20on%20individual%20frames%20with%20a%203D%20autoencoder%20that%20processes%20short%20temporal%20stacks.%20The%202D%20model%20identifies%20localized%20spatial%20irregularities%20in%20single%20time%20steps%2C%20while%20the%203D%20model%20exploits%20spatio-temporal%20context%20to%20detect%20anomalous%20motion%20patterns%20and%20reduces%20redundant%20detections%20across%20time.%20We%20further%20evaluate%20volumetric%20time-dependent%20data%20and%20find%20that%20reconstruction%20errors%20are%20strongly%20influenced%20by%20the%20spatial%20distribution%20of%20mass%2C%20with%20highly%20concentrated%20regions%20yielding%20larger%20errors%20than%20dispersed%20configurations.%20Our%20results%20highlight%20the%20importance%20of%20temporal%20context%20for%20robust%20anomaly%20detection%20in%20dynamic%20simulations.&entry.1838667208=http%3A//arxiv.org/abs/2601.08659v1&entry.124074799=Read"},
{"title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN", "author": "Yanhua Zhao", "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.", "link": "http://arxiv.org/abs/2601.08776v1", "date": "2026-01-13", "relevancy": 1.9495, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4876}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4873}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Translating%20Light-Sheet%20Microscopy%20Images%20to%20Virtual%20H%26E%20Using%20CycleGAN&body=Title%3A%20Translating%20Light-Sheet%20Microscopy%20Images%20to%20Virtual%20H%26E%20Using%20CycleGAN%0AAuthor%3A%20Yanhua%20Zhao%0AAbstract%3A%20Histopathology%20analysis%20relies%20on%20Hematoxylin%20and%20Eosin%20%28H%26E%29%20staining%2C%20but%20fluorescence%20microscopy%20offers%20complementary%20information.%20Converting%20fluorescence%20images%20to%20H%26E-like%20appearance%20can%20aid%20interpretation%20and%20integration%20with%20standard%20workflows.%20We%20present%20a%20Cycle-Consistent%20Adversarial%20Network%20%28CycleGAN%29%20approach%20for%20unpaired%20image-to-image%20translation%20from%20multi-channel%20fluorescence%20microscopy%20to%20pseudo%20H%26E%20stained%20histopathology%20images.%20The%20method%20combines%20C01%20and%20C02%20fluorescence%20channels%20into%20RGB%20and%20learns%20a%20bidirectional%20mapping%20between%20fluorescence%20and%20H%26E%20domains%20without%20paired%20training%20data.%20The%20architecture%20uses%20ResNet-based%20generators%20with%20residual%20blocks%20and%20PatchGAN%20discriminators%2C%20trained%20with%20adversarial%2C%20cycle-consistency%2C%20and%20identity%20losses.%20Experiments%20on%20fluorescence%20microscopy%20datasets%20show%20the%20model%20generates%20realistic%20pseudo%20H%26E%20images%20that%20preserve%20morphological%20structures%20while%20adopting%20H%26E-like%20color%20characteristics.%20This%20enables%20visualization%20of%20fluorescence%20data%20in%20a%20format%20familiar%20to%20pathologists%20and%20supports%20integration%20with%20existing%20H%26E-based%20analysis%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranslating%2520Light-Sheet%2520Microscopy%2520Images%2520to%2520Virtual%2520H%2526E%2520Using%2520CycleGAN%26entry.906535625%3DYanhua%2520Zhao%26entry.1292438233%3DHistopathology%2520analysis%2520relies%2520on%2520Hematoxylin%2520and%2520Eosin%2520%2528H%2526E%2529%2520staining%252C%2520but%2520fluorescence%2520microscopy%2520offers%2520complementary%2520information.%2520Converting%2520fluorescence%2520images%2520to%2520H%2526E-like%2520appearance%2520can%2520aid%2520interpretation%2520and%2520integration%2520with%2520standard%2520workflows.%2520We%2520present%2520a%2520Cycle-Consistent%2520Adversarial%2520Network%2520%2528CycleGAN%2529%2520approach%2520for%2520unpaired%2520image-to-image%2520translation%2520from%2520multi-channel%2520fluorescence%2520microscopy%2520to%2520pseudo%2520H%2526E%2520stained%2520histopathology%2520images.%2520The%2520method%2520combines%2520C01%2520and%2520C02%2520fluorescence%2520channels%2520into%2520RGB%2520and%2520learns%2520a%2520bidirectional%2520mapping%2520between%2520fluorescence%2520and%2520H%2526E%2520domains%2520without%2520paired%2520training%2520data.%2520The%2520architecture%2520uses%2520ResNet-based%2520generators%2520with%2520residual%2520blocks%2520and%2520PatchGAN%2520discriminators%252C%2520trained%2520with%2520adversarial%252C%2520cycle-consistency%252C%2520and%2520identity%2520losses.%2520Experiments%2520on%2520fluorescence%2520microscopy%2520datasets%2520show%2520the%2520model%2520generates%2520realistic%2520pseudo%2520H%2526E%2520images%2520that%2520preserve%2520morphological%2520structures%2520while%2520adopting%2520H%2526E-like%2520color%2520characteristics.%2520This%2520enables%2520visualization%2520of%2520fluorescence%2520data%2520in%2520a%2520format%2520familiar%2520to%2520pathologists%2520and%2520supports%2520integration%2520with%2520existing%2520H%2526E-based%2520analysis%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Translating%20Light-Sheet%20Microscopy%20Images%20to%20Virtual%20H%26E%20Using%20CycleGAN&entry.906535625=Yanhua%20Zhao&entry.1292438233=Histopathology%20analysis%20relies%20on%20Hematoxylin%20and%20Eosin%20%28H%26E%29%20staining%2C%20but%20fluorescence%20microscopy%20offers%20complementary%20information.%20Converting%20fluorescence%20images%20to%20H%26E-like%20appearance%20can%20aid%20interpretation%20and%20integration%20with%20standard%20workflows.%20We%20present%20a%20Cycle-Consistent%20Adversarial%20Network%20%28CycleGAN%29%20approach%20for%20unpaired%20image-to-image%20translation%20from%20multi-channel%20fluorescence%20microscopy%20to%20pseudo%20H%26E%20stained%20histopathology%20images.%20The%20method%20combines%20C01%20and%20C02%20fluorescence%20channels%20into%20RGB%20and%20learns%20a%20bidirectional%20mapping%20between%20fluorescence%20and%20H%26E%20domains%20without%20paired%20training%20data.%20The%20architecture%20uses%20ResNet-based%20generators%20with%20residual%20blocks%20and%20PatchGAN%20discriminators%2C%20trained%20with%20adversarial%2C%20cycle-consistency%2C%20and%20identity%20losses.%20Experiments%20on%20fluorescence%20microscopy%20datasets%20show%20the%20model%20generates%20realistic%20pseudo%20H%26E%20images%20that%20preserve%20morphological%20structures%20while%20adopting%20H%26E-like%20color%20characteristics.%20This%20enables%20visualization%20of%20fluorescence%20data%20in%20a%20format%20familiar%20to%20pathologists%20and%20supports%20integration%20with%20existing%20H%26E-based%20analysis%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2601.08776v1&entry.124074799=Read"},
{"title": "What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting", "author": "Jinkwan Jang and Hyunbin Jin and Hyungjin Park and Kyubyung Chae and Taesup Kim", "abstract": "Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.", "link": "http://arxiv.org/abs/2601.08509v1", "date": "2026-01-13", "relevancy": 1.4288, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5045}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20If%20TSF%3A%20A%20Benchmark%20for%20Reframing%20Forecasting%20as%20Scenario-Guided%20Multimodal%20Forecasting&body=Title%3A%20What%20If%20TSF%3A%20A%20Benchmark%20for%20Reframing%20Forecasting%20as%20Scenario-Guided%20Multimodal%20Forecasting%0AAuthor%3A%20Jinkwan%20Jang%20and%20Hyunbin%20Jin%20and%20Hyungjin%20Park%20and%20Kyubyung%20Chae%20and%20Taesup%20Kim%0AAbstract%3A%20Time%20series%20forecasting%20is%20critical%20to%20real-world%20decision%20making%2C%20yet%20most%20existing%20approaches%20remain%20unimodal%20and%20rely%20on%20extrapolating%20historical%20patterns.%20While%20recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20highlights%20the%20potential%20for%20multimodal%20forecasting%2C%20existing%20benchmarks%20largely%20provide%20retrospective%20or%20misaligned%20raw%20context%2C%20making%20it%20unclear%20whether%20such%20models%20meaningfully%20leverage%20textual%20inputs.%20In%20practice%2C%20human%20experts%20incorporate%20what-if%20scenarios%20with%20historical%20evidence%2C%20often%20producing%20distinct%20forecasts%20from%20the%20same%20observations%20under%20different%20scenarios.%20Inspired%20by%20this%2C%20we%20introduce%20What%20If%20TSF%20%28WIT%29%2C%20a%20multimodal%20forecasting%20benchmark%20designed%20to%20evaluate%20whether%20models%20can%20condition%20their%20forecasts%20on%20contextual%20text%2C%20especially%20future%20scenarios.%20By%20providing%20expert-crafted%20plausible%20or%20counterfactual%20scenarios%2C%20WIT%20offers%20a%20rigorous%20testbed%20for%20scenario-guided%20multimodal%20forecasting.%20The%20benchmark%20is%20available%20at%20https%3A//github.com/jinkwan1115/WhatIfTSF.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520If%2520TSF%253A%2520A%2520Benchmark%2520for%2520Reframing%2520Forecasting%2520as%2520Scenario-Guided%2520Multimodal%2520Forecasting%26entry.906535625%3DJinkwan%2520Jang%2520and%2520Hyunbin%2520Jin%2520and%2520Hyungjin%2520Park%2520and%2520Kyubyung%2520Chae%2520and%2520Taesup%2520Kim%26entry.1292438233%3DTime%2520series%2520forecasting%2520is%2520critical%2520to%2520real-world%2520decision%2520making%252C%2520yet%2520most%2520existing%2520approaches%2520remain%2520unimodal%2520and%2520rely%2520on%2520extrapolating%2520historical%2520patterns.%2520While%2520recent%2520progress%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520highlights%2520the%2520potential%2520for%2520multimodal%2520forecasting%252C%2520existing%2520benchmarks%2520largely%2520provide%2520retrospective%2520or%2520misaligned%2520raw%2520context%252C%2520making%2520it%2520unclear%2520whether%2520such%2520models%2520meaningfully%2520leverage%2520textual%2520inputs.%2520In%2520practice%252C%2520human%2520experts%2520incorporate%2520what-if%2520scenarios%2520with%2520historical%2520evidence%252C%2520often%2520producing%2520distinct%2520forecasts%2520from%2520the%2520same%2520observations%2520under%2520different%2520scenarios.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%2520What%2520If%2520TSF%2520%2528WIT%2529%252C%2520a%2520multimodal%2520forecasting%2520benchmark%2520designed%2520to%2520evaluate%2520whether%2520models%2520can%2520condition%2520their%2520forecasts%2520on%2520contextual%2520text%252C%2520especially%2520future%2520scenarios.%2520By%2520providing%2520expert-crafted%2520plausible%2520or%2520counterfactual%2520scenarios%252C%2520WIT%2520offers%2520a%2520rigorous%2520testbed%2520for%2520scenario-guided%2520multimodal%2520forecasting.%2520The%2520benchmark%2520is%2520available%2520at%2520https%253A//github.com/jinkwan1115/WhatIfTSF.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20If%20TSF%3A%20A%20Benchmark%20for%20Reframing%20Forecasting%20as%20Scenario-Guided%20Multimodal%20Forecasting&entry.906535625=Jinkwan%20Jang%20and%20Hyunbin%20Jin%20and%20Hyungjin%20Park%20and%20Kyubyung%20Chae%20and%20Taesup%20Kim&entry.1292438233=Time%20series%20forecasting%20is%20critical%20to%20real-world%20decision%20making%2C%20yet%20most%20existing%20approaches%20remain%20unimodal%20and%20rely%20on%20extrapolating%20historical%20patterns.%20While%20recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20highlights%20the%20potential%20for%20multimodal%20forecasting%2C%20existing%20benchmarks%20largely%20provide%20retrospective%20or%20misaligned%20raw%20context%2C%20making%20it%20unclear%20whether%20such%20models%20meaningfully%20leverage%20textual%20inputs.%20In%20practice%2C%20human%20experts%20incorporate%20what-if%20scenarios%20with%20historical%20evidence%2C%20often%20producing%20distinct%20forecasts%20from%20the%20same%20observations%20under%20different%20scenarios.%20Inspired%20by%20this%2C%20we%20introduce%20What%20If%20TSF%20%28WIT%29%2C%20a%20multimodal%20forecasting%20benchmark%20designed%20to%20evaluate%20whether%20models%20can%20condition%20their%20forecasts%20on%20contextual%20text%2C%20especially%20future%20scenarios.%20By%20providing%20expert-crafted%20plausible%20or%20counterfactual%20scenarios%2C%20WIT%20offers%20a%20rigorous%20testbed%20for%20scenario-guided%20multimodal%20forecasting.%20The%20benchmark%20is%20available%20at%20https%3A//github.com/jinkwan1115/WhatIfTSF.&entry.1838667208=http%3A//arxiv.org/abs/2601.08509v1&entry.124074799=Read"},
{"title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory", "author": "Shaoan Wang and Yuanfei Luo and Xingyu Chen and Aocheng Luo and Dongyue Li and Chang Liu and Sheng Chen and Yangang Zhang and Junzhi Yu", "abstract": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.", "link": "http://arxiv.org/abs/2601.08665v1", "date": "2026-01-13", "relevancy": 1.6275, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLingNav%3A%20Embodied%20Navigation%20with%20Adaptive%20Reasoning%20and%20Visual-Assisted%20Linguistic%20Memory&body=Title%3A%20VLingNav%3A%20Embodied%20Navigation%20with%20Adaptive%20Reasoning%20and%20Visual-Assisted%20Linguistic%20Memory%0AAuthor%3A%20Shaoan%20Wang%20and%20Yuanfei%20Luo%20and%20Xingyu%20Chen%20and%20Aocheng%20Luo%20and%20Dongyue%20Li%20and%20Chang%20Liu%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Junzhi%20Yu%0AAbstract%3A%20VLA%20models%20have%20shown%20promising%20potential%20in%20embodied%20navigation%20by%20unifying%20perception%20and%20planning%20while%20inheriting%20the%20strong%20generalization%20abilities%20of%20large%20VLMs.%20However%2C%20most%20existing%20VLA%20models%20rely%20on%20reactive%20mappings%20directly%20from%20observations%20to%20actions%2C%20lacking%20the%20explicit%20reasoning%20capabilities%20and%20persistent%20memory%20required%20for%20complex%2C%20long-horizon%20navigation%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20VLingNav%2C%20a%20VLA%20model%20for%20embodied%20navigation%20grounded%20in%20linguistic-driven%20cognition.%20First%2C%20inspired%20by%20the%20dual-process%20theory%20of%20human%20cognition%2C%20we%20introduce%20an%20adaptive%20chain-of-thought%20mechanism%2C%20which%20dynamically%20triggers%20explicit%20reasoning%20only%20when%20necessary%2C%20enabling%20the%20agent%20to%20fluidly%20switch%20between%20fast%2C%20intuitive%20execution%20and%20slow%2C%20deliberate%20planning.%20Second%2C%20to%20handle%20long-horizon%20spatial%20dependencies%2C%20we%20develop%20a%20visual-assisted%20linguistic%20memory%20module%20that%20constructs%20a%20persistent%2C%20cross-modal%20semantic%20memory%2C%20enabling%20the%20agent%20to%20recall%20past%20observations%20to%20prevent%20repetitive%20exploration%20and%20infer%20movement%20trends%20for%20dynamic%20environments.%20For%20the%20training%20recipe%2C%20we%20construct%20Nav-AdaCoT-2.9M%2C%20the%20largest%20embodied%20navigation%20dataset%20with%20reasoning%20annotations%20to%20date%2C%20enriched%20with%20adaptive%20CoT%20annotations%20that%20induce%20a%20reasoning%20paradigm%20capable%20of%20adjusting%20both%20when%20to%20think%20and%20what%20to%20think%20about.%20Moreover%2C%20we%20incorporate%20an%20online%20expert-guided%20reinforcement%20learning%20stage%2C%20enabling%20the%20model%20to%20surpass%20pure%20imitation%20learning%20and%20to%20acquire%20more%20robust%2C%20self-explored%20navigation%20behaviors.%20Extensive%20experiments%20demonstrate%20that%20VLingNav%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%20embodied%20navigation%20benchmarks.%20Notably%2C%20VLingNav%20transfers%20to%20real-world%20robotic%20platforms%20in%20a%20zero-shot%20manner%2C%20executing%20various%20navigation%20tasks%20and%20demonstrating%20strong%20cross-domain%20and%20cross-task%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLingNav%253A%2520Embodied%2520Navigation%2520with%2520Adaptive%2520Reasoning%2520and%2520Visual-Assisted%2520Linguistic%2520Memory%26entry.906535625%3DShaoan%2520Wang%2520and%2520Yuanfei%2520Luo%2520and%2520Xingyu%2520Chen%2520and%2520Aocheng%2520Luo%2520and%2520Dongyue%2520Li%2520and%2520Chang%2520Liu%2520and%2520Sheng%2520Chen%2520and%2520Yangang%2520Zhang%2520and%2520Junzhi%2520Yu%26entry.1292438233%3DVLA%2520models%2520have%2520shown%2520promising%2520potential%2520in%2520embodied%2520navigation%2520by%2520unifying%2520perception%2520and%2520planning%2520while%2520inheriting%2520the%2520strong%2520generalization%2520abilities%2520of%2520large%2520VLMs.%2520However%252C%2520most%2520existing%2520VLA%2520models%2520rely%2520on%2520reactive%2520mappings%2520directly%2520from%2520observations%2520to%2520actions%252C%2520lacking%2520the%2520explicit%2520reasoning%2520capabilities%2520and%2520persistent%2520memory%2520required%2520for%2520complex%252C%2520long-horizon%2520navigation%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520VLingNav%252C%2520a%2520VLA%2520model%2520for%2520embodied%2520navigation%2520grounded%2520in%2520linguistic-driven%2520cognition.%2520First%252C%2520inspired%2520by%2520the%2520dual-process%2520theory%2520of%2520human%2520cognition%252C%2520we%2520introduce%2520an%2520adaptive%2520chain-of-thought%2520mechanism%252C%2520which%2520dynamically%2520triggers%2520explicit%2520reasoning%2520only%2520when%2520necessary%252C%2520enabling%2520the%2520agent%2520to%2520fluidly%2520switch%2520between%2520fast%252C%2520intuitive%2520execution%2520and%2520slow%252C%2520deliberate%2520planning.%2520Second%252C%2520to%2520handle%2520long-horizon%2520spatial%2520dependencies%252C%2520we%2520develop%2520a%2520visual-assisted%2520linguistic%2520memory%2520module%2520that%2520constructs%2520a%2520persistent%252C%2520cross-modal%2520semantic%2520memory%252C%2520enabling%2520the%2520agent%2520to%2520recall%2520past%2520observations%2520to%2520prevent%2520repetitive%2520exploration%2520and%2520infer%2520movement%2520trends%2520for%2520dynamic%2520environments.%2520For%2520the%2520training%2520recipe%252C%2520we%2520construct%2520Nav-AdaCoT-2.9M%252C%2520the%2520largest%2520embodied%2520navigation%2520dataset%2520with%2520reasoning%2520annotations%2520to%2520date%252C%2520enriched%2520with%2520adaptive%2520CoT%2520annotations%2520that%2520induce%2520a%2520reasoning%2520paradigm%2520capable%2520of%2520adjusting%2520both%2520when%2520to%2520think%2520and%2520what%2520to%2520think%2520about.%2520Moreover%252C%2520we%2520incorporate%2520an%2520online%2520expert-guided%2520reinforcement%2520learning%2520stage%252C%2520enabling%2520the%2520model%2520to%2520surpass%2520pure%2520imitation%2520learning%2520and%2520to%2520acquire%2520more%2520robust%252C%2520self-explored%2520navigation%2520behaviors.%2520Extensive%2520experiments%2520demonstrate%2520that%2520VLingNav%2520achieves%2520state-of-the-art%2520performance%2520across%2520a%2520wide%2520range%2520of%2520embodied%2520navigation%2520benchmarks.%2520Notably%252C%2520VLingNav%2520transfers%2520to%2520real-world%2520robotic%2520platforms%2520in%2520a%2520zero-shot%2520manner%252C%2520executing%2520various%2520navigation%2520tasks%2520and%2520demonstrating%2520strong%2520cross-domain%2520and%2520cross-task%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLingNav%3A%20Embodied%20Navigation%20with%20Adaptive%20Reasoning%20and%20Visual-Assisted%20Linguistic%20Memory&entry.906535625=Shaoan%20Wang%20and%20Yuanfei%20Luo%20and%20Xingyu%20Chen%20and%20Aocheng%20Luo%20and%20Dongyue%20Li%20and%20Chang%20Liu%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Junzhi%20Yu&entry.1292438233=VLA%20models%20have%20shown%20promising%20potential%20in%20embodied%20navigation%20by%20unifying%20perception%20and%20planning%20while%20inheriting%20the%20strong%20generalization%20abilities%20of%20large%20VLMs.%20However%2C%20most%20existing%20VLA%20models%20rely%20on%20reactive%20mappings%20directly%20from%20observations%20to%20actions%2C%20lacking%20the%20explicit%20reasoning%20capabilities%20and%20persistent%20memory%20required%20for%20complex%2C%20long-horizon%20navigation%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20VLingNav%2C%20a%20VLA%20model%20for%20embodied%20navigation%20grounded%20in%20linguistic-driven%20cognition.%20First%2C%20inspired%20by%20the%20dual-process%20theory%20of%20human%20cognition%2C%20we%20introduce%20an%20adaptive%20chain-of-thought%20mechanism%2C%20which%20dynamically%20triggers%20explicit%20reasoning%20only%20when%20necessary%2C%20enabling%20the%20agent%20to%20fluidly%20switch%20between%20fast%2C%20intuitive%20execution%20and%20slow%2C%20deliberate%20planning.%20Second%2C%20to%20handle%20long-horizon%20spatial%20dependencies%2C%20we%20develop%20a%20visual-assisted%20linguistic%20memory%20module%20that%20constructs%20a%20persistent%2C%20cross-modal%20semantic%20memory%2C%20enabling%20the%20agent%20to%20recall%20past%20observations%20to%20prevent%20repetitive%20exploration%20and%20infer%20movement%20trends%20for%20dynamic%20environments.%20For%20the%20training%20recipe%2C%20we%20construct%20Nav-AdaCoT-2.9M%2C%20the%20largest%20embodied%20navigation%20dataset%20with%20reasoning%20annotations%20to%20date%2C%20enriched%20with%20adaptive%20CoT%20annotations%20that%20induce%20a%20reasoning%20paradigm%20capable%20of%20adjusting%20both%20when%20to%20think%20and%20what%20to%20think%20about.%20Moreover%2C%20we%20incorporate%20an%20online%20expert-guided%20reinforcement%20learning%20stage%2C%20enabling%20the%20model%20to%20surpass%20pure%20imitation%20learning%20and%20to%20acquire%20more%20robust%2C%20self-explored%20navigation%20behaviors.%20Extensive%20experiments%20demonstrate%20that%20VLingNav%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%20embodied%20navigation%20benchmarks.%20Notably%2C%20VLingNav%20transfers%20to%20real-world%20robotic%20platforms%20in%20a%20zero-shot%20manner%2C%20executing%20various%20navigation%20tasks%20and%20demonstrating%20strong%20cross-domain%20and%20cross-task%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2601.08665v1&entry.124074799=Read"},
{"title": "PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning", "author": "Xiaoyou Liu and Xinyi Mou and Shengbin Yue and Liang Wang and Yuqing Wang and Qiexiang Wang and Tianrui Qin and Wangchunshu Zhou and Zhongyu Wei", "abstract": "As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.", "link": "http://arxiv.org/abs/2601.08679v1", "date": "2026-01-13", "relevancy": 1.5215, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersonaDual%3A%20Balancing%20Personalization%20and%20Objectivity%20via%20Adaptive%20Reasoning&body=Title%3A%20PersonaDual%3A%20Balancing%20Personalization%20and%20Objectivity%20via%20Adaptive%20Reasoning%0AAuthor%3A%20Xiaoyou%20Liu%20and%20Xinyi%20Mou%20and%20Shengbin%20Yue%20and%20Liang%20Wang%20and%20Yuqing%20Wang%20and%20Qiexiang%20Wang%20and%20Tianrui%20Qin%20and%20Wangchunshu%20Zhou%20and%20Zhongyu%20Wei%0AAbstract%3A%20As%20users%20increasingly%20expect%20LLMs%20to%20align%20with%20their%20preferences%2C%20personalized%20information%20becomes%20valuable.%20However%2C%20personalized%20information%20can%20be%20a%20double-edged%20sword%3A%20it%20can%20improve%20interaction%20but%20may%20compromise%20objectivity%20and%20factual%20correctness%2C%20especially%20when%20it%20is%20misaligned%20with%20the%20question.%20To%20alleviate%20this%20problem%2C%20we%20propose%20PersonaDual%2C%20a%20framework%20that%20supports%20both%20general-purpose%20objective%20reasoning%20and%20personalized%20reasoning%20in%20a%20single%20model%2C%20and%20adaptively%20switches%20modes%20based%20on%20context.%20PersonaDual%20is%20first%20trained%20with%20SFT%20to%20learn%20two%20reasoning%20patterns%2C%20and%20then%20further%20optimized%20via%20reinforcement%20learning%20with%20our%20proposed%20DualGRPO%20to%20improve%20mode%20selection.%20Experiments%20on%20objective%20and%20personalized%20benchmarks%20show%20that%20PersonaDual%20preserves%20the%20benefits%20of%20personalization%20while%20reducing%20interference%2C%20achieving%20near%20interference-free%20performance%20and%20better%20leveraging%20helpful%20personalized%20signals%20to%20improve%20objective%20problem-solving.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonaDual%253A%2520Balancing%2520Personalization%2520and%2520Objectivity%2520via%2520Adaptive%2520Reasoning%26entry.906535625%3DXiaoyou%2520Liu%2520and%2520Xinyi%2520Mou%2520and%2520Shengbin%2520Yue%2520and%2520Liang%2520Wang%2520and%2520Yuqing%2520Wang%2520and%2520Qiexiang%2520Wang%2520and%2520Tianrui%2520Qin%2520and%2520Wangchunshu%2520Zhou%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3DAs%2520users%2520increasingly%2520expect%2520LLMs%2520to%2520align%2520with%2520their%2520preferences%252C%2520personalized%2520information%2520becomes%2520valuable.%2520However%252C%2520personalized%2520information%2520can%2520be%2520a%2520double-edged%2520sword%253A%2520it%2520can%2520improve%2520interaction%2520but%2520may%2520compromise%2520objectivity%2520and%2520factual%2520correctness%252C%2520especially%2520when%2520it%2520is%2520misaligned%2520with%2520the%2520question.%2520To%2520alleviate%2520this%2520problem%252C%2520we%2520propose%2520PersonaDual%252C%2520a%2520framework%2520that%2520supports%2520both%2520general-purpose%2520objective%2520reasoning%2520and%2520personalized%2520reasoning%2520in%2520a%2520single%2520model%252C%2520and%2520adaptively%2520switches%2520modes%2520based%2520on%2520context.%2520PersonaDual%2520is%2520first%2520trained%2520with%2520SFT%2520to%2520learn%2520two%2520reasoning%2520patterns%252C%2520and%2520then%2520further%2520optimized%2520via%2520reinforcement%2520learning%2520with%2520our%2520proposed%2520DualGRPO%2520to%2520improve%2520mode%2520selection.%2520Experiments%2520on%2520objective%2520and%2520personalized%2520benchmarks%2520show%2520that%2520PersonaDual%2520preserves%2520the%2520benefits%2520of%2520personalization%2520while%2520reducing%2520interference%252C%2520achieving%2520near%2520interference-free%2520performance%2520and%2520better%2520leveraging%2520helpful%2520personalized%2520signals%2520to%2520improve%2520objective%2520problem-solving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersonaDual%3A%20Balancing%20Personalization%20and%20Objectivity%20via%20Adaptive%20Reasoning&entry.906535625=Xiaoyou%20Liu%20and%20Xinyi%20Mou%20and%20Shengbin%20Yue%20and%20Liang%20Wang%20and%20Yuqing%20Wang%20and%20Qiexiang%20Wang%20and%20Tianrui%20Qin%20and%20Wangchunshu%20Zhou%20and%20Zhongyu%20Wei&entry.1292438233=As%20users%20increasingly%20expect%20LLMs%20to%20align%20with%20their%20preferences%2C%20personalized%20information%20becomes%20valuable.%20However%2C%20personalized%20information%20can%20be%20a%20double-edged%20sword%3A%20it%20can%20improve%20interaction%20but%20may%20compromise%20objectivity%20and%20factual%20correctness%2C%20especially%20when%20it%20is%20misaligned%20with%20the%20question.%20To%20alleviate%20this%20problem%2C%20we%20propose%20PersonaDual%2C%20a%20framework%20that%20supports%20both%20general-purpose%20objective%20reasoning%20and%20personalized%20reasoning%20in%20a%20single%20model%2C%20and%20adaptively%20switches%20modes%20based%20on%20context.%20PersonaDual%20is%20first%20trained%20with%20SFT%20to%20learn%20two%20reasoning%20patterns%2C%20and%20then%20further%20optimized%20via%20reinforcement%20learning%20with%20our%20proposed%20DualGRPO%20to%20improve%20mode%20selection.%20Experiments%20on%20objective%20and%20personalized%20benchmarks%20show%20that%20PersonaDual%20preserves%20the%20benefits%20of%20personalization%20while%20reducing%20interference%2C%20achieving%20near%20interference-free%20performance%20and%20better%20leveraging%20helpful%20personalized%20signals%20to%20improve%20objective%20problem-solving.&entry.1838667208=http%3A//arxiv.org/abs/2601.08679v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


