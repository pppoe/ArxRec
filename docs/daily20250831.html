<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250828.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale\n  Feature for Generalizable Gaussian Splatting", "author": "Yuxi Hu and Jun Zhang and Kuangyi Chen and Zhe Zhang and Friedrich Fraundorfer", "abstract": "  Generalizable Gaussian Splatting aims to synthesize novel views for unseen\nscenes without per-scene optimization. In particular, recent advancements\nutilize feed-forward networks to predict per-pixel Gaussian parameters,\nenabling high-quality synthesis from sparse input views. However, existing\napproaches fall short in encoding discriminative, multi-view consistent\nfeatures for Gaussian predictions, which struggle to construct accurate\ngeometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a\nframework that enhances feature learning by incorporating context-aware,\ncross-dimension, and cross-scale constraints. Our architecture integrates three\nlightweight modules into a unified rendering pipeline, improving feature fusion\nand enabling photorealistic synthesis without requiring additional supervision.\nExtensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS\nachieves state-of-the-art rendering quality and generalization ability. Code is\navailable at: https://github.com/YuhsiHu/C3-GS.\n", "link": "http://arxiv.org/abs/2508.20754v1", "date": "2025-08-28", "relevancy": 3.2826, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6798}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%7BC%7D%5E%7B3%7D%24-GS%3A%20Learning%20Context-aware%2C%20Cross-dimension%2C%20Cross-scale%0A%20%20Feature%20for%20Generalizable%20Gaussian%20Splatting&body=Title%3A%20%24%7BC%7D%5E%7B3%7D%24-GS%3A%20Learning%20Context-aware%2C%20Cross-dimension%2C%20Cross-scale%0A%20%20Feature%20for%20Generalizable%20Gaussian%20Splatting%0AAuthor%3A%20Yuxi%20Hu%20and%20Jun%20Zhang%20and%20Kuangyi%20Chen%20and%20Zhe%20Zhang%20and%20Friedrich%20Fraundorfer%0AAbstract%3A%20%20%20Generalizable%20Gaussian%20Splatting%20aims%20to%20synthesize%20novel%20views%20for%20unseen%0Ascenes%20without%20per-scene%20optimization.%20In%20particular%2C%20recent%20advancements%0Autilize%20feed-forward%20networks%20to%20predict%20per-pixel%20Gaussian%20parameters%2C%0Aenabling%20high-quality%20synthesis%20from%20sparse%20input%20views.%20However%2C%20existing%0Aapproaches%20fall%20short%20in%20encoding%20discriminative%2C%20multi-view%20consistent%0Afeatures%20for%20Gaussian%20predictions%2C%20which%20struggle%20to%20construct%20accurate%0Ageometry%20with%20sparse%20views.%20To%20address%20this%2C%20we%20propose%20%24%5Cmathbf%7BC%7D%5E%7B3%7D%24-GS%2C%20a%0Aframework%20that%20enhances%20feature%20learning%20by%20incorporating%20context-aware%2C%0Across-dimension%2C%20and%20cross-scale%20constraints.%20Our%20architecture%20integrates%20three%0Alightweight%20modules%20into%20a%20unified%20rendering%20pipeline%2C%20improving%20feature%20fusion%0Aand%20enabling%20photorealistic%20synthesis%20without%20requiring%20additional%20supervision.%0AExtensive%20experiments%20on%20benchmark%20datasets%20validate%20that%20%24%5Cmathbf%7BC%7D%5E%7B3%7D%24-GS%0Aachieves%20state-of-the-art%20rendering%20quality%20and%20generalization%20ability.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YuhsiHu/C3-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%257BC%257D%255E%257B3%257D%2524-GS%253A%2520Learning%2520Context-aware%252C%2520Cross-dimension%252C%2520Cross-scale%250A%2520%2520Feature%2520for%2520Generalizable%2520Gaussian%2520Splatting%26entry.906535625%3DYuxi%2520Hu%2520and%2520Jun%2520Zhang%2520and%2520Kuangyi%2520Chen%2520and%2520Zhe%2520Zhang%2520and%2520Friedrich%2520Fraundorfer%26entry.1292438233%3D%2520%2520Generalizable%2520Gaussian%2520Splatting%2520aims%2520to%2520synthesize%2520novel%2520views%2520for%2520unseen%250Ascenes%2520without%2520per-scene%2520optimization.%2520In%2520particular%252C%2520recent%2520advancements%250Autilize%2520feed-forward%2520networks%2520to%2520predict%2520per-pixel%2520Gaussian%2520parameters%252C%250Aenabling%2520high-quality%2520synthesis%2520from%2520sparse%2520input%2520views.%2520However%252C%2520existing%250Aapproaches%2520fall%2520short%2520in%2520encoding%2520discriminative%252C%2520multi-view%2520consistent%250Afeatures%2520for%2520Gaussian%2520predictions%252C%2520which%2520struggle%2520to%2520construct%2520accurate%250Ageometry%2520with%2520sparse%2520views.%2520To%2520address%2520this%252C%2520we%2520propose%2520%2524%255Cmathbf%257BC%257D%255E%257B3%257D%2524-GS%252C%2520a%250Aframework%2520that%2520enhances%2520feature%2520learning%2520by%2520incorporating%2520context-aware%252C%250Across-dimension%252C%2520and%2520cross-scale%2520constraints.%2520Our%2520architecture%2520integrates%2520three%250Alightweight%2520modules%2520into%2520a%2520unified%2520rendering%2520pipeline%252C%2520improving%2520feature%2520fusion%250Aand%2520enabling%2520photorealistic%2520synthesis%2520without%2520requiring%2520additional%2520supervision.%250AExtensive%2520experiments%2520on%2520benchmark%2520datasets%2520validate%2520that%2520%2524%255Cmathbf%257BC%257D%255E%257B3%257D%2524-GS%250Aachieves%2520state-of-the-art%2520rendering%2520quality%2520and%2520generalization%2520ability.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/YuhsiHu/C3-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%7BC%7D%5E%7B3%7D%24-GS%3A%20Learning%20Context-aware%2C%20Cross-dimension%2C%20Cross-scale%0A%20%20Feature%20for%20Generalizable%20Gaussian%20Splatting&entry.906535625=Yuxi%20Hu%20and%20Jun%20Zhang%20and%20Kuangyi%20Chen%20and%20Zhe%20Zhang%20and%20Friedrich%20Fraundorfer&entry.1292438233=%20%20Generalizable%20Gaussian%20Splatting%20aims%20to%20synthesize%20novel%20views%20for%20unseen%0Ascenes%20without%20per-scene%20optimization.%20In%20particular%2C%20recent%20advancements%0Autilize%20feed-forward%20networks%20to%20predict%20per-pixel%20Gaussian%20parameters%2C%0Aenabling%20high-quality%20synthesis%20from%20sparse%20input%20views.%20However%2C%20existing%0Aapproaches%20fall%20short%20in%20encoding%20discriminative%2C%20multi-view%20consistent%0Afeatures%20for%20Gaussian%20predictions%2C%20which%20struggle%20to%20construct%20accurate%0Ageometry%20with%20sparse%20views.%20To%20address%20this%2C%20we%20propose%20%24%5Cmathbf%7BC%7D%5E%7B3%7D%24-GS%2C%20a%0Aframework%20that%20enhances%20feature%20learning%20by%20incorporating%20context-aware%2C%0Across-dimension%2C%20and%20cross-scale%20constraints.%20Our%20architecture%20integrates%20three%0Alightweight%20modules%20into%20a%20unified%20rendering%20pipeline%2C%20improving%20feature%20fusion%0Aand%20enabling%20photorealistic%20synthesis%20without%20requiring%20additional%20supervision.%0AExtensive%20experiments%20on%20benchmark%20datasets%20validate%20that%20%24%5Cmathbf%7BC%7D%5E%7B3%7D%24-GS%0Aachieves%20state-of-the-art%20rendering%20quality%20and%20generalization%20ability.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YuhsiHu/C3-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20754v1&entry.124074799=Read"},
{"title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes", "author": "Yajiao Xiong and Xiaoyu Zhou and Yongtao Wan and Deqing Sun and Ming-Hsuan Yang", "abstract": "  We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io\n", "link": "http://arxiv.org/abs/2508.20965v1", "date": "2025-08-28", "relevancy": 3.2159, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.671}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6423}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrivingGaussian%2B%2B%3A%20Towards%20Realistic%20Reconstruction%20and%20Editable%0A%20%20Simulation%20for%20Surrounding%20Dynamic%20Driving%20Scenes&body=Title%3A%20DrivingGaussian%2B%2B%3A%20Towards%20Realistic%20Reconstruction%20and%20Editable%0A%20%20Simulation%20for%20Surrounding%20Dynamic%20Driving%20Scenes%0AAuthor%3A%20Yajiao%20Xiong%20and%20Xiaoyu%20Zhou%20and%20Yongtao%20Wan%20and%20Deqing%20Sun%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20We%20present%20DrivingGaussian%2B%2B%2C%20an%20efficient%20and%20effective%20framework%20for%0Arealistic%20reconstructing%20and%20controllable%20editing%20of%20surrounding%20dynamic%0Aautonomous%20driving%20scenes.%20DrivingGaussian%2B%2B%20models%20the%20static%20background%20using%0Aincremental%203D%20Gaussians%20and%20reconstructs%20moving%20objects%20with%20a%20composite%0Adynamic%20Gaussian%20graph%2C%20ensuring%20accurate%20positions%20and%20occlusions.%20By%0Aintegrating%20a%20LiDAR%20prior%2C%20it%20achieves%20detailed%20and%20consistent%20scene%0Areconstruction%2C%20outperforming%20existing%20methods%20in%20dynamic%20scene%20reconstruction%0Aand%20photorealistic%20surround-view%20synthesis.%20DrivingGaussian%2B%2B%20supports%0Atraining-free%20controllable%20editing%20for%20dynamic%20driving%20scenes%2C%20including%0Atexture%20modification%2C%20weather%20simulation%2C%20and%20object%20manipulation%2C%20leveraging%0Amulti-view%20images%20and%20depth%20priors.%20By%20integrating%20large%20language%20models%20%28LLMs%29%0Aand%20controllable%20editing%2C%20our%20method%20can%20automatically%20generate%20dynamic%20object%0Amotion%20trajectories%20and%20enhance%20their%20realism%20during%20the%20optimization%20process.%0ADrivingGaussian%2B%2B%20demonstrates%20consistent%20and%20realistic%20editing%20results%20and%0Agenerates%20dynamic%20multi-view%20driving%20scenarios%2C%20while%20significantly%20enhancing%0Ascene%20diversity.%20More%20results%20and%20code%20can%20be%20found%20at%20the%20project%20site%3A%0Ahttps%3A//xiong-creator.github.io/DrivingGaussian_plus.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrivingGaussian%252B%252B%253A%2520Towards%2520Realistic%2520Reconstruction%2520and%2520Editable%250A%2520%2520Simulation%2520for%2520Surrounding%2520Dynamic%2520Driving%2520Scenes%26entry.906535625%3DYajiao%2520Xiong%2520and%2520Xiaoyu%2520Zhou%2520and%2520Yongtao%2520Wan%2520and%2520Deqing%2520Sun%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520DrivingGaussian%252B%252B%252C%2520an%2520efficient%2520and%2520effective%2520framework%2520for%250Arealistic%2520reconstructing%2520and%2520controllable%2520editing%2520of%2520surrounding%2520dynamic%250Aautonomous%2520driving%2520scenes.%2520DrivingGaussian%252B%252B%2520models%2520the%2520static%2520background%2520using%250Aincremental%25203D%2520Gaussians%2520and%2520reconstructs%2520moving%2520objects%2520with%2520a%2520composite%250Adynamic%2520Gaussian%2520graph%252C%2520ensuring%2520accurate%2520positions%2520and%2520occlusions.%2520By%250Aintegrating%2520a%2520LiDAR%2520prior%252C%2520it%2520achieves%2520detailed%2520and%2520consistent%2520scene%250Areconstruction%252C%2520outperforming%2520existing%2520methods%2520in%2520dynamic%2520scene%2520reconstruction%250Aand%2520photorealistic%2520surround-view%2520synthesis.%2520DrivingGaussian%252B%252B%2520supports%250Atraining-free%2520controllable%2520editing%2520for%2520dynamic%2520driving%2520scenes%252C%2520including%250Atexture%2520modification%252C%2520weather%2520simulation%252C%2520and%2520object%2520manipulation%252C%2520leveraging%250Amulti-view%2520images%2520and%2520depth%2520priors.%2520By%2520integrating%2520large%2520language%2520models%2520%2528LLMs%2529%250Aand%2520controllable%2520editing%252C%2520our%2520method%2520can%2520automatically%2520generate%2520dynamic%2520object%250Amotion%2520trajectories%2520and%2520enhance%2520their%2520realism%2520during%2520the%2520optimization%2520process.%250ADrivingGaussian%252B%252B%2520demonstrates%2520consistent%2520and%2520realistic%2520editing%2520results%2520and%250Agenerates%2520dynamic%2520multi-view%2520driving%2520scenarios%252C%2520while%2520significantly%2520enhancing%250Ascene%2520diversity.%2520More%2520results%2520and%2520code%2520can%2520be%2520found%2520at%2520the%2520project%2520site%253A%250Ahttps%253A//xiong-creator.github.io/DrivingGaussian_plus.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrivingGaussian%2B%2B%3A%20Towards%20Realistic%20Reconstruction%20and%20Editable%0A%20%20Simulation%20for%20Surrounding%20Dynamic%20Driving%20Scenes&entry.906535625=Yajiao%20Xiong%20and%20Xiaoyu%20Zhou%20and%20Yongtao%20Wan%20and%20Deqing%20Sun%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20We%20present%20DrivingGaussian%2B%2B%2C%20an%20efficient%20and%20effective%20framework%20for%0Arealistic%20reconstructing%20and%20controllable%20editing%20of%20surrounding%20dynamic%0Aautonomous%20driving%20scenes.%20DrivingGaussian%2B%2B%20models%20the%20static%20background%20using%0Aincremental%203D%20Gaussians%20and%20reconstructs%20moving%20objects%20with%20a%20composite%0Adynamic%20Gaussian%20graph%2C%20ensuring%20accurate%20positions%20and%20occlusions.%20By%0Aintegrating%20a%20LiDAR%20prior%2C%20it%20achieves%20detailed%20and%20consistent%20scene%0Areconstruction%2C%20outperforming%20existing%20methods%20in%20dynamic%20scene%20reconstruction%0Aand%20photorealistic%20surround-view%20synthesis.%20DrivingGaussian%2B%2B%20supports%0Atraining-free%20controllable%20editing%20for%20dynamic%20driving%20scenes%2C%20including%0Atexture%20modification%2C%20weather%20simulation%2C%20and%20object%20manipulation%2C%20leveraging%0Amulti-view%20images%20and%20depth%20priors.%20By%20integrating%20large%20language%20models%20%28LLMs%29%0Aand%20controllable%20editing%2C%20our%20method%20can%20automatically%20generate%20dynamic%20object%0Amotion%20trajectories%20and%20enhance%20their%20realism%20during%20the%20optimization%20process.%0ADrivingGaussian%2B%2B%20demonstrates%20consistent%20and%20realistic%20editing%20results%20and%0Agenerates%20dynamic%20multi-view%20driving%20scenarios%2C%20while%20significantly%20enhancing%0Ascene%20diversity.%20More%20results%20and%20code%20can%20be%20found%20at%20the%20project%20site%3A%0Ahttps%3A//xiong-creator.github.io/DrivingGaussian_plus.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20965v1&entry.124074799=Read"},
{"title": "Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation\n  Models for Medical Image Segmentation", "author": "Yifan Gao and Haoyue Li and Feng Yuan and Xiaosong Wang and Xin Gao", "abstract": "  Foundation models pre-trained on large-scale natural image datasets offer a\npowerful paradigm for medical image segmentation. However, effectively\ntransferring their learned representations for precise clinical applications\nremains a challenge. In this work, we propose Dino U-Net, a novel\nencoder-decoder architecture designed to exploit the high-fidelity dense\nfeatures of the DINOv3 vision foundation model. Our architecture introduces an\nencoder built upon a frozen DINOv3 backbone, which employs a specialized\nadapter to fuse the model's rich semantic features with low-level spatial\ndetails. To preserve the quality of these representations during dimensionality\nreduction, we design a new fidelity-aware projection module (FAPM) that\neffectively refines and projects the features for the decoder. We conducted\nextensive experiments on seven diverse public medical image segmentation\ndatasets. Our results show that Dino U-Net achieves state-of-the-art\nperformance, consistently outperforming previous methods across various imaging\nmodalities. Our framework proves to be highly scalable, with segmentation\naccuracy consistently improving as the backbone model size increases up to the\n7-billion-parameter variant. The findings demonstrate that leveraging the\nsuperior, dense-pretrained features from a general-purpose foundation model\nprovides a highly effective and parameter-efficient approach to advance the\naccuracy of medical image segmentation. The code is available at\nhttps://github.com/yifangao112/DinoUNet.\n", "link": "http://arxiv.org/abs/2508.20909v1", "date": "2025-08-28", "relevancy": 3.0004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6179}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dino%20U-Net%3A%20Exploiting%20High-Fidelity%20Dense%20Features%20from%20Foundation%0A%20%20Models%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Dino%20U-Net%3A%20Exploiting%20High-Fidelity%20Dense%20Features%20from%20Foundation%0A%20%20Models%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yifan%20Gao%20and%20Haoyue%20Li%20and%20Feng%20Yuan%20and%20Xiaosong%20Wang%20and%20Xin%20Gao%0AAbstract%3A%20%20%20Foundation%20models%20pre-trained%20on%20large-scale%20natural%20image%20datasets%20offer%20a%0Apowerful%20paradigm%20for%20medical%20image%20segmentation.%20However%2C%20effectively%0Atransferring%20their%20learned%20representations%20for%20precise%20clinical%20applications%0Aremains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20Dino%20U-Net%2C%20a%20novel%0Aencoder-decoder%20architecture%20designed%20to%20exploit%20the%20high-fidelity%20dense%0Afeatures%20of%20the%20DINOv3%20vision%20foundation%20model.%20Our%20architecture%20introduces%20an%0Aencoder%20built%20upon%20a%20frozen%20DINOv3%20backbone%2C%20which%20employs%20a%20specialized%0Aadapter%20to%20fuse%20the%20model%27s%20rich%20semantic%20features%20with%20low-level%20spatial%0Adetails.%20To%20preserve%20the%20quality%20of%20these%20representations%20during%20dimensionality%0Areduction%2C%20we%20design%20a%20new%20fidelity-aware%20projection%20module%20%28FAPM%29%20that%0Aeffectively%20refines%20and%20projects%20the%20features%20for%20the%20decoder.%20We%20conducted%0Aextensive%20experiments%20on%20seven%20diverse%20public%20medical%20image%20segmentation%0Adatasets.%20Our%20results%20show%20that%20Dino%20U-Net%20achieves%20state-of-the-art%0Aperformance%2C%20consistently%20outperforming%20previous%20methods%20across%20various%20imaging%0Amodalities.%20Our%20framework%20proves%20to%20be%20highly%20scalable%2C%20with%20segmentation%0Aaccuracy%20consistently%20improving%20as%20the%20backbone%20model%20size%20increases%20up%20to%20the%0A7-billion-parameter%20variant.%20The%20findings%20demonstrate%20that%20leveraging%20the%0Asuperior%2C%20dense-pretrained%20features%20from%20a%20general-purpose%20foundation%20model%0Aprovides%20a%20highly%20effective%20and%20parameter-efficient%20approach%20to%20advance%20the%0Aaccuracy%20of%20medical%20image%20segmentation.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yifangao112/DinoUNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDino%2520U-Net%253A%2520Exploiting%2520High-Fidelity%2520Dense%2520Features%2520from%2520Foundation%250A%2520%2520Models%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYifan%2520Gao%2520and%2520Haoyue%2520Li%2520and%2520Feng%2520Yuan%2520and%2520Xiaosong%2520Wang%2520and%2520Xin%2520Gao%26entry.1292438233%3D%2520%2520Foundation%2520models%2520pre-trained%2520on%2520large-scale%2520natural%2520image%2520datasets%2520offer%2520a%250Apowerful%2520paradigm%2520for%2520medical%2520image%2520segmentation.%2520However%252C%2520effectively%250Atransferring%2520their%2520learned%2520representations%2520for%2520precise%2520clinical%2520applications%250Aremains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%2520Dino%2520U-Net%252C%2520a%2520novel%250Aencoder-decoder%2520architecture%2520designed%2520to%2520exploit%2520the%2520high-fidelity%2520dense%250Afeatures%2520of%2520the%2520DINOv3%2520vision%2520foundation%2520model.%2520Our%2520architecture%2520introduces%2520an%250Aencoder%2520built%2520upon%2520a%2520frozen%2520DINOv3%2520backbone%252C%2520which%2520employs%2520a%2520specialized%250Aadapter%2520to%2520fuse%2520the%2520model%2527s%2520rich%2520semantic%2520features%2520with%2520low-level%2520spatial%250Adetails.%2520To%2520preserve%2520the%2520quality%2520of%2520these%2520representations%2520during%2520dimensionality%250Areduction%252C%2520we%2520design%2520a%2520new%2520fidelity-aware%2520projection%2520module%2520%2528FAPM%2529%2520that%250Aeffectively%2520refines%2520and%2520projects%2520the%2520features%2520for%2520the%2520decoder.%2520We%2520conducted%250Aextensive%2520experiments%2520on%2520seven%2520diverse%2520public%2520medical%2520image%2520segmentation%250Adatasets.%2520Our%2520results%2520show%2520that%2520Dino%2520U-Net%2520achieves%2520state-of-the-art%250Aperformance%252C%2520consistently%2520outperforming%2520previous%2520methods%2520across%2520various%2520imaging%250Amodalities.%2520Our%2520framework%2520proves%2520to%2520be%2520highly%2520scalable%252C%2520with%2520segmentation%250Aaccuracy%2520consistently%2520improving%2520as%2520the%2520backbone%2520model%2520size%2520increases%2520up%2520to%2520the%250A7-billion-parameter%2520variant.%2520The%2520findings%2520demonstrate%2520that%2520leveraging%2520the%250Asuperior%252C%2520dense-pretrained%2520features%2520from%2520a%2520general-purpose%2520foundation%2520model%250Aprovides%2520a%2520highly%2520effective%2520and%2520parameter-efficient%2520approach%2520to%2520advance%2520the%250Aaccuracy%2520of%2520medical%2520image%2520segmentation.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yifangao112/DinoUNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dino%20U-Net%3A%20Exploiting%20High-Fidelity%20Dense%20Features%20from%20Foundation%0A%20%20Models%20for%20Medical%20Image%20Segmentation&entry.906535625=Yifan%20Gao%20and%20Haoyue%20Li%20and%20Feng%20Yuan%20and%20Xiaosong%20Wang%20and%20Xin%20Gao&entry.1292438233=%20%20Foundation%20models%20pre-trained%20on%20large-scale%20natural%20image%20datasets%20offer%20a%0Apowerful%20paradigm%20for%20medical%20image%20segmentation.%20However%2C%20effectively%0Atransferring%20their%20learned%20representations%20for%20precise%20clinical%20applications%0Aremains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20Dino%20U-Net%2C%20a%20novel%0Aencoder-decoder%20architecture%20designed%20to%20exploit%20the%20high-fidelity%20dense%0Afeatures%20of%20the%20DINOv3%20vision%20foundation%20model.%20Our%20architecture%20introduces%20an%0Aencoder%20built%20upon%20a%20frozen%20DINOv3%20backbone%2C%20which%20employs%20a%20specialized%0Aadapter%20to%20fuse%20the%20model%27s%20rich%20semantic%20features%20with%20low-level%20spatial%0Adetails.%20To%20preserve%20the%20quality%20of%20these%20representations%20during%20dimensionality%0Areduction%2C%20we%20design%20a%20new%20fidelity-aware%20projection%20module%20%28FAPM%29%20that%0Aeffectively%20refines%20and%20projects%20the%20features%20for%20the%20decoder.%20We%20conducted%0Aextensive%20experiments%20on%20seven%20diverse%20public%20medical%20image%20segmentation%0Adatasets.%20Our%20results%20show%20that%20Dino%20U-Net%20achieves%20state-of-the-art%0Aperformance%2C%20consistently%20outperforming%20previous%20methods%20across%20various%20imaging%0Amodalities.%20Our%20framework%20proves%20to%20be%20highly%20scalable%2C%20with%20segmentation%0Aaccuracy%20consistently%20improving%20as%20the%20backbone%20model%20size%20increases%20up%20to%20the%0A7-billion-parameter%20variant.%20The%20findings%20demonstrate%20that%20leveraging%20the%0Asuperior%2C%20dense-pretrained%20features%20from%20a%20general-purpose%20foundation%20model%0Aprovides%20a%20highly%20effective%20and%20parameter-efficient%20approach%20to%20advance%20the%0Aaccuracy%20of%20medical%20image%20segmentation.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yifangao112/DinoUNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20909v1&entry.124074799=Read"},
{"title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection", "author": "Jiajie Li and Boyang Sun and Luca Di Giammarino and Hermann Blum and Marc Pollefeys", "abstract": "  Reliable localization is critical for robot navigation, yet most existing\nsystems implicitly assume that all viewing directions at a location are equally\ninformative. In practice, localization becomes unreliable when the robot\nobserves unmapped, ambiguous, or uninformative regions. To address this, we\npresent ActLoc, an active viewpoint-aware planning framework for enhancing\nlocalization accuracy for general robot navigation tasks. At its core, ActLoc\nemploys a largescale trained attention-based model for viewpoint selection. The\nmodel encodes a metric map and the camera poses used during map construction,\nand predicts localization accuracy across yaw and pitch directions at arbitrary\n3D locations. These per-point accuracy distributions are incorporated into a\npath planner, enabling the robot to actively select camera orientations that\nmaximize localization robustness while respecting task and motion constraints.\nActLoc achieves stateof-the-art results on single-viewpoint selection and\ngeneralizes effectively to fulltrajectory planning. Its modular design makes it\nreadily applicable to diverse robot navigation and inspection tasks.\n", "link": "http://arxiv.org/abs/2508.20981v1", "date": "2025-08-28", "relevancy": 2.9835, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6465}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5802}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActLoc%3A%20Learning%20to%20Localize%20on%20the%20Move%20via%20Active%20Viewpoint%20Selection&body=Title%3A%20ActLoc%3A%20Learning%20to%20Localize%20on%20the%20Move%20via%20Active%20Viewpoint%20Selection%0AAuthor%3A%20Jiajie%20Li%20and%20Boyang%20Sun%20and%20Luca%20Di%20Giammarino%20and%20Hermann%20Blum%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Reliable%20localization%20is%20critical%20for%20robot%20navigation%2C%20yet%20most%20existing%0Asystems%20implicitly%20assume%20that%20all%20viewing%20directions%20at%20a%20location%20are%20equally%0Ainformative.%20In%20practice%2C%20localization%20becomes%20unreliable%20when%20the%20robot%0Aobserves%20unmapped%2C%20ambiguous%2C%20or%20uninformative%20regions.%20To%20address%20this%2C%20we%0Apresent%20ActLoc%2C%20an%20active%20viewpoint-aware%20planning%20framework%20for%20enhancing%0Alocalization%20accuracy%20for%20general%20robot%20navigation%20tasks.%20At%20its%20core%2C%20ActLoc%0Aemploys%20a%20largescale%20trained%20attention-based%20model%20for%20viewpoint%20selection.%20The%0Amodel%20encodes%20a%20metric%20map%20and%20the%20camera%20poses%20used%20during%20map%20construction%2C%0Aand%20predicts%20localization%20accuracy%20across%20yaw%20and%20pitch%20directions%20at%20arbitrary%0A3D%20locations.%20These%20per-point%20accuracy%20distributions%20are%20incorporated%20into%20a%0Apath%20planner%2C%20enabling%20the%20robot%20to%20actively%20select%20camera%20orientations%20that%0Amaximize%20localization%20robustness%20while%20respecting%20task%20and%20motion%20constraints.%0AActLoc%20achieves%20stateof-the-art%20results%20on%20single-viewpoint%20selection%20and%0Ageneralizes%20effectively%20to%20fulltrajectory%20planning.%20Its%20modular%20design%20makes%20it%0Areadily%20applicable%20to%20diverse%20robot%20navigation%20and%20inspection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActLoc%253A%2520Learning%2520to%2520Localize%2520on%2520the%2520Move%2520via%2520Active%2520Viewpoint%2520Selection%26entry.906535625%3DJiajie%2520Li%2520and%2520Boyang%2520Sun%2520and%2520Luca%2520Di%2520Giammarino%2520and%2520Hermann%2520Blum%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Reliable%2520localization%2520is%2520critical%2520for%2520robot%2520navigation%252C%2520yet%2520most%2520existing%250Asystems%2520implicitly%2520assume%2520that%2520all%2520viewing%2520directions%2520at%2520a%2520location%2520are%2520equally%250Ainformative.%2520In%2520practice%252C%2520localization%2520becomes%2520unreliable%2520when%2520the%2520robot%250Aobserves%2520unmapped%252C%2520ambiguous%252C%2520or%2520uninformative%2520regions.%2520To%2520address%2520this%252C%2520we%250Apresent%2520ActLoc%252C%2520an%2520active%2520viewpoint-aware%2520planning%2520framework%2520for%2520enhancing%250Alocalization%2520accuracy%2520for%2520general%2520robot%2520navigation%2520tasks.%2520At%2520its%2520core%252C%2520ActLoc%250Aemploys%2520a%2520largescale%2520trained%2520attention-based%2520model%2520for%2520viewpoint%2520selection.%2520The%250Amodel%2520encodes%2520a%2520metric%2520map%2520and%2520the%2520camera%2520poses%2520used%2520during%2520map%2520construction%252C%250Aand%2520predicts%2520localization%2520accuracy%2520across%2520yaw%2520and%2520pitch%2520directions%2520at%2520arbitrary%250A3D%2520locations.%2520These%2520per-point%2520accuracy%2520distributions%2520are%2520incorporated%2520into%2520a%250Apath%2520planner%252C%2520enabling%2520the%2520robot%2520to%2520actively%2520select%2520camera%2520orientations%2520that%250Amaximize%2520localization%2520robustness%2520while%2520respecting%2520task%2520and%2520motion%2520constraints.%250AActLoc%2520achieves%2520stateof-the-art%2520results%2520on%2520single-viewpoint%2520selection%2520and%250Ageneralizes%2520effectively%2520to%2520fulltrajectory%2520planning.%2520Its%2520modular%2520design%2520makes%2520it%250Areadily%2520applicable%2520to%2520diverse%2520robot%2520navigation%2520and%2520inspection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActLoc%3A%20Learning%20to%20Localize%20on%20the%20Move%20via%20Active%20Viewpoint%20Selection&entry.906535625=Jiajie%20Li%20and%20Boyang%20Sun%20and%20Luca%20Di%20Giammarino%20and%20Hermann%20Blum%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Reliable%20localization%20is%20critical%20for%20robot%20navigation%2C%20yet%20most%20existing%0Asystems%20implicitly%20assume%20that%20all%20viewing%20directions%20at%20a%20location%20are%20equally%0Ainformative.%20In%20practice%2C%20localization%20becomes%20unreliable%20when%20the%20robot%0Aobserves%20unmapped%2C%20ambiguous%2C%20or%20uninformative%20regions.%20To%20address%20this%2C%20we%0Apresent%20ActLoc%2C%20an%20active%20viewpoint-aware%20planning%20framework%20for%20enhancing%0Alocalization%20accuracy%20for%20general%20robot%20navigation%20tasks.%20At%20its%20core%2C%20ActLoc%0Aemploys%20a%20largescale%20trained%20attention-based%20model%20for%20viewpoint%20selection.%20The%0Amodel%20encodes%20a%20metric%20map%20and%20the%20camera%20poses%20used%20during%20map%20construction%2C%0Aand%20predicts%20localization%20accuracy%20across%20yaw%20and%20pitch%20directions%20at%20arbitrary%0A3D%20locations.%20These%20per-point%20accuracy%20distributions%20are%20incorporated%20into%20a%0Apath%20planner%2C%20enabling%20the%20robot%20to%20actively%20select%20camera%20orientations%20that%0Amaximize%20localization%20robustness%20while%20respecting%20task%20and%20motion%20constraints.%0AActLoc%20achieves%20stateof-the-art%20results%20on%20single-viewpoint%20selection%20and%0Ageneralizes%20effectively%20to%20fulltrajectory%20planning.%20Its%20modular%20design%20makes%20it%0Areadily%20applicable%20to%20diverse%20robot%20navigation%20and%20inspection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20981v1&entry.124074799=Read"},
{"title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification", "author": "Wei Li and Renshan Zhang and Rui Shao and Jie He and Liqiang Nie", "abstract": "  Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.\n", "link": "http://arxiv.org/abs/2508.21046v1", "date": "2025-08-28", "relevancy": 2.83, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification&body=Title%3A%20CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification%0AAuthor%3A%20Wei%20Li%20and%20Renshan%20Zhang%20and%20Rui%20Shao%20and%20Jie%20He%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Recent%20Vision-Language-Action%20%28VLA%29%20models%20built%20on%20pre-trained%0AVision-Language%20Models%20%28VLMs%29%20require%20extensive%20post-training%2C%20resulting%20in%0Ahigh%20computational%20overhead%20that%20limits%20scalability%20and%20deployment.We%20propose%0ACogVLA%2C%20a%20Cognition-Aligned%20Vision-Language-Action%20framework%20that%20leverages%0Ainstruction-driven%20routing%20and%20sparsification%20to%20improve%20both%20efficiency%20and%0Aperformance.%20CogVLA%20draws%20inspiration%20from%20human%20multimodal%20coordination%20and%0Aintroduces%20a%203-stage%20progressive%20architecture.%201%29%20Encoder-FiLM%20based%0AAggregation%20Routing%20%28EFA-Routing%29%20injects%20instruction%20information%20into%20the%0Avision%20encoder%20to%20selectively%20aggregate%20and%20compress%20dual-stream%20visual%20tokens%2C%0Aforming%20a%20instruction-aware%20latent%20representation.%202%29%20Building%20upon%20this%0Acompact%20visual%20encoding%2C%20LLM-FiLM%20based%20Pruning%20Routing%20%28LFP-Routing%29%0Aintroduces%20action%20intent%20into%20the%20language%20model%20by%20pruning%0Ainstruction-irrelevant%20visually%20grounded%20tokens%2C%20thereby%20achieving%20token-level%0Asparsity.%203%29%20To%20ensure%20that%20compressed%20perception%20inputs%20can%20still%20support%0Aaccurate%20and%20coherent%20action%20generation%2C%20we%20introduce%20V-L-A%20Coupled%20Attention%0A%28CAtten%29%2C%20which%20combines%20causal%20vision-language%20attention%20with%20bidirectional%0Aaction%20parallel%20decoding.%20Extensive%20experiments%20on%20the%20LIBERO%20benchmark%20and%0Areal-world%20robotic%20tasks%20demonstrate%20that%20CogVLA%20achieves%20state-of-the-art%0Aperformance%20with%20success%20rates%20of%2097.4%25%20and%2070.0%25%2C%20respectively%2C%20while%20reducing%0Atraining%20costs%20by%202.5-fold%20and%20decreasing%20inference%20latency%20by%202.8-fold%0Acompared%20to%20OpenVLA.%20CogVLA%20is%20open-sourced%20and%20publicly%20available%20at%0Ahttps%3A//github.com/JiuTian-VL/CogVLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogVLA%253A%2520Cognition-Aligned%2520Vision-Language-Action%2520Model%2520via%250A%2520%2520Instruction-Driven%2520Routing%2520%2526%2520Sparsification%26entry.906535625%3DWei%2520Li%2520and%2520Renshan%2520Zhang%2520and%2520Rui%2520Shao%2520and%2520Jie%2520He%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Recent%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520built%2520on%2520pre-trained%250AVision-Language%2520Models%2520%2528VLMs%2529%2520require%2520extensive%2520post-training%252C%2520resulting%2520in%250Ahigh%2520computational%2520overhead%2520that%2520limits%2520scalability%2520and%2520deployment.We%2520propose%250ACogVLA%252C%2520a%2520Cognition-Aligned%2520Vision-Language-Action%2520framework%2520that%2520leverages%250Ainstruction-driven%2520routing%2520and%2520sparsification%2520to%2520improve%2520both%2520efficiency%2520and%250Aperformance.%2520CogVLA%2520draws%2520inspiration%2520from%2520human%2520multimodal%2520coordination%2520and%250Aintroduces%2520a%25203-stage%2520progressive%2520architecture.%25201%2529%2520Encoder-FiLM%2520based%250AAggregation%2520Routing%2520%2528EFA-Routing%2529%2520injects%2520instruction%2520information%2520into%2520the%250Avision%2520encoder%2520to%2520selectively%2520aggregate%2520and%2520compress%2520dual-stream%2520visual%2520tokens%252C%250Aforming%2520a%2520instruction-aware%2520latent%2520representation.%25202%2529%2520Building%2520upon%2520this%250Acompact%2520visual%2520encoding%252C%2520LLM-FiLM%2520based%2520Pruning%2520Routing%2520%2528LFP-Routing%2529%250Aintroduces%2520action%2520intent%2520into%2520the%2520language%2520model%2520by%2520pruning%250Ainstruction-irrelevant%2520visually%2520grounded%2520tokens%252C%2520thereby%2520achieving%2520token-level%250Asparsity.%25203%2529%2520To%2520ensure%2520that%2520compressed%2520perception%2520inputs%2520can%2520still%2520support%250Aaccurate%2520and%2520coherent%2520action%2520generation%252C%2520we%2520introduce%2520V-L-A%2520Coupled%2520Attention%250A%2528CAtten%2529%252C%2520which%2520combines%2520causal%2520vision-language%2520attention%2520with%2520bidirectional%250Aaction%2520parallel%2520decoding.%2520Extensive%2520experiments%2520on%2520the%2520LIBERO%2520benchmark%2520and%250Areal-world%2520robotic%2520tasks%2520demonstrate%2520that%2520CogVLA%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520success%2520rates%2520of%252097.4%2525%2520and%252070.0%2525%252C%2520respectively%252C%2520while%2520reducing%250Atraining%2520costs%2520by%25202.5-fold%2520and%2520decreasing%2520inference%2520latency%2520by%25202.8-fold%250Acompared%2520to%2520OpenVLA.%2520CogVLA%2520is%2520open-sourced%2520and%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JiuTian-VL/CogVLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification&entry.906535625=Wei%20Li%20and%20Renshan%20Zhang%20and%20Rui%20Shao%20and%20Jie%20He%20and%20Liqiang%20Nie&entry.1292438233=%20%20Recent%20Vision-Language-Action%20%28VLA%29%20models%20built%20on%20pre-trained%0AVision-Language%20Models%20%28VLMs%29%20require%20extensive%20post-training%2C%20resulting%20in%0Ahigh%20computational%20overhead%20that%20limits%20scalability%20and%20deployment.We%20propose%0ACogVLA%2C%20a%20Cognition-Aligned%20Vision-Language-Action%20framework%20that%20leverages%0Ainstruction-driven%20routing%20and%20sparsification%20to%20improve%20both%20efficiency%20and%0Aperformance.%20CogVLA%20draws%20inspiration%20from%20human%20multimodal%20coordination%20and%0Aintroduces%20a%203-stage%20progressive%20architecture.%201%29%20Encoder-FiLM%20based%0AAggregation%20Routing%20%28EFA-Routing%29%20injects%20instruction%20information%20into%20the%0Avision%20encoder%20to%20selectively%20aggregate%20and%20compress%20dual-stream%20visual%20tokens%2C%0Aforming%20a%20instruction-aware%20latent%20representation.%202%29%20Building%20upon%20this%0Acompact%20visual%20encoding%2C%20LLM-FiLM%20based%20Pruning%20Routing%20%28LFP-Routing%29%0Aintroduces%20action%20intent%20into%20the%20language%20model%20by%20pruning%0Ainstruction-irrelevant%20visually%20grounded%20tokens%2C%20thereby%20achieving%20token-level%0Asparsity.%203%29%20To%20ensure%20that%20compressed%20perception%20inputs%20can%20still%20support%0Aaccurate%20and%20coherent%20action%20generation%2C%20we%20introduce%20V-L-A%20Coupled%20Attention%0A%28CAtten%29%2C%20which%20combines%20causal%20vision-language%20attention%20with%20bidirectional%0Aaction%20parallel%20decoding.%20Extensive%20experiments%20on%20the%20LIBERO%20benchmark%20and%0Areal-world%20robotic%20tasks%20demonstrate%20that%20CogVLA%20achieves%20state-of-the-art%0Aperformance%20with%20success%20rates%20of%2097.4%25%20and%2070.0%25%2C%20respectively%2C%20while%20reducing%0Atraining%20costs%20by%202.5-fold%20and%20decreasing%20inference%20latency%20by%202.8-fold%0Acompared%20to%20OpenVLA.%20CogVLA%20is%20open-sourced%20and%20publicly%20available%20at%0Ahttps%3A//github.com/JiuTian-VL/CogVLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21046v1&entry.124074799=Read"},
{"title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs", "author": "Junpeng Ma and Qizhe Zhang and Ming Lu and Zhibin Wang and Qiang Zhou and Jun Song and Shanghang Zhang", "abstract": "  Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.\n", "link": "http://arxiv.org/abs/2508.21044v1", "date": "2025-08-28", "relevancy": 2.8279, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMG-Vid%3A%20Maximizing%20Marginal%20Gains%20at%20Segment-level%20and%20Token-level%20for%0A%20%20Efficient%20Video%20LLMs&body=Title%3A%20MMG-Vid%3A%20Maximizing%20Marginal%20Gains%20at%20Segment-level%20and%20Token-level%20for%0A%20%20Efficient%20Video%20LLMs%0AAuthor%3A%20Junpeng%20Ma%20and%20Qizhe%20Zhang%20and%20Ming%20Lu%20and%20Zhibin%20Wang%20and%20Qiang%20Zhou%20and%20Jun%20Song%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28VLLMs%29%20excel%20in%20video%20understanding%2C%20but%20their%0Aexcessive%20visual%20tokens%20pose%20a%20significant%20computational%20challenge%20for%0Areal-world%20applications.%20Current%20methods%20aim%20to%20enhance%20inference%20efficiency%20by%0Avisual%20token%20pruning.%20However%2C%20they%20do%20not%20consider%20the%20dynamic%20characteristics%0Aand%20temporal%20dependencies%20of%20video%20frames%2C%20as%20they%20perceive%20video%20understanding%0Aas%20a%20multi-frame%20task.%20To%20address%20these%20challenges%2C%20we%20propose%20MMG-Vid%2C%20a%20novel%0Atraining-free%20visual%20token%20pruning%20framework%20that%20removes%20redundancy%20by%0AMaximizing%20Marginal%20Gains%20at%20both%20segment-level%20and%20token-level.%20Specifically%2C%0Awe%20first%20divide%20the%20video%20into%20segments%20based%20on%20frame%20similarity%2C%20and%20then%0Adynamically%20allocate%20the%20token%20budget%20for%20each%20segment%20to%20maximize%20the%20marginal%0Again%20of%20each%20segment.%20Subsequently%2C%20we%20propose%20a%20temporal-guided%20DPC%20algorithm%0Athat%20jointly%20models%20inter-frame%20uniqueness%20and%20intra-frame%20diversity%2C%20thereby%0Amaximizing%20the%20marginal%20gain%20of%20each%20token.%20By%20combining%20both%20stages%2C%20MMG-Vid%0Acan%20maximize%20the%20utilization%20of%20the%20limited%20token%20budget%2C%20significantly%0Aimproving%20efficiency%20while%20maintaining%20strong%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20MMG-Vid%20can%20maintain%20over%2099.5%25%20of%20the%20original%0Aperformance%2C%20while%20effectively%20reducing%2075%25%20visual%20tokens%20and%20accelerating%20the%0Aprefilling%20stage%20by%203.9x%20on%20LLaVA-OneVision-7B.%20Code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMG-Vid%253A%2520Maximizing%2520Marginal%2520Gains%2520at%2520Segment-level%2520and%2520Token-level%2520for%250A%2520%2520Efficient%2520Video%2520LLMs%26entry.906535625%3DJunpeng%2520Ma%2520and%2520Qizhe%2520Zhang%2520and%2520Ming%2520Lu%2520and%2520Zhibin%2520Wang%2520and%2520Qiang%2520Zhou%2520and%2520Jun%2520Song%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520excel%2520in%2520video%2520understanding%252C%2520but%2520their%250Aexcessive%2520visual%2520tokens%2520pose%2520a%2520significant%2520computational%2520challenge%2520for%250Areal-world%2520applications.%2520Current%2520methods%2520aim%2520to%2520enhance%2520inference%2520efficiency%2520by%250Avisual%2520token%2520pruning.%2520However%252C%2520they%2520do%2520not%2520consider%2520the%2520dynamic%2520characteristics%250Aand%2520temporal%2520dependencies%2520of%2520video%2520frames%252C%2520as%2520they%2520perceive%2520video%2520understanding%250Aas%2520a%2520multi-frame%2520task.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MMG-Vid%252C%2520a%2520novel%250Atraining-free%2520visual%2520token%2520pruning%2520framework%2520that%2520removes%2520redundancy%2520by%250AMaximizing%2520Marginal%2520Gains%2520at%2520both%2520segment-level%2520and%2520token-level.%2520Specifically%252C%250Awe%2520first%2520divide%2520the%2520video%2520into%2520segments%2520based%2520on%2520frame%2520similarity%252C%2520and%2520then%250Adynamically%2520allocate%2520the%2520token%2520budget%2520for%2520each%2520segment%2520to%2520maximize%2520the%2520marginal%250Again%2520of%2520each%2520segment.%2520Subsequently%252C%2520we%2520propose%2520a%2520temporal-guided%2520DPC%2520algorithm%250Athat%2520jointly%2520models%2520inter-frame%2520uniqueness%2520and%2520intra-frame%2520diversity%252C%2520thereby%250Amaximizing%2520the%2520marginal%2520gain%2520of%2520each%2520token.%2520By%2520combining%2520both%2520stages%252C%2520MMG-Vid%250Acan%2520maximize%2520the%2520utilization%2520of%2520the%2520limited%2520token%2520budget%252C%2520significantly%250Aimproving%2520efficiency%2520while%2520maintaining%2520strong%2520performance.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520MMG-Vid%2520can%2520maintain%2520over%252099.5%2525%2520of%2520the%2520original%250Aperformance%252C%2520while%2520effectively%2520reducing%252075%2525%2520visual%2520tokens%2520and%2520accelerating%2520the%250Aprefilling%2520stage%2520by%25203.9x%2520on%2520LLaVA-OneVision-7B.%2520Code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMG-Vid%3A%20Maximizing%20Marginal%20Gains%20at%20Segment-level%20and%20Token-level%20for%0A%20%20Efficient%20Video%20LLMs&entry.906535625=Junpeng%20Ma%20and%20Qizhe%20Zhang%20and%20Ming%20Lu%20and%20Zhibin%20Wang%20and%20Qiang%20Zhou%20and%20Jun%20Song%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28VLLMs%29%20excel%20in%20video%20understanding%2C%20but%20their%0Aexcessive%20visual%20tokens%20pose%20a%20significant%20computational%20challenge%20for%0Areal-world%20applications.%20Current%20methods%20aim%20to%20enhance%20inference%20efficiency%20by%0Avisual%20token%20pruning.%20However%2C%20they%20do%20not%20consider%20the%20dynamic%20characteristics%0Aand%20temporal%20dependencies%20of%20video%20frames%2C%20as%20they%20perceive%20video%20understanding%0Aas%20a%20multi-frame%20task.%20To%20address%20these%20challenges%2C%20we%20propose%20MMG-Vid%2C%20a%20novel%0Atraining-free%20visual%20token%20pruning%20framework%20that%20removes%20redundancy%20by%0AMaximizing%20Marginal%20Gains%20at%20both%20segment-level%20and%20token-level.%20Specifically%2C%0Awe%20first%20divide%20the%20video%20into%20segments%20based%20on%20frame%20similarity%2C%20and%20then%0Adynamically%20allocate%20the%20token%20budget%20for%20each%20segment%20to%20maximize%20the%20marginal%0Again%20of%20each%20segment.%20Subsequently%2C%20we%20propose%20a%20temporal-guided%20DPC%20algorithm%0Athat%20jointly%20models%20inter-frame%20uniqueness%20and%20intra-frame%20diversity%2C%20thereby%0Amaximizing%20the%20marginal%20gain%20of%20each%20token.%20By%20combining%20both%20stages%2C%20MMG-Vid%0Acan%20maximize%20the%20utilization%20of%20the%20limited%20token%20budget%2C%20significantly%0Aimproving%20efficiency%20while%20maintaining%20strong%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20MMG-Vid%20can%20maintain%20over%2099.5%25%20of%20the%20original%0Aperformance%2C%20while%20effectively%20reducing%2075%25%20visual%20tokens%20and%20accelerating%20the%0Aprefilling%20stage%20by%203.9x%20on%20LLaVA-OneVision-7B.%20Code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21044v1&entry.124074799=Read"},
{"title": "Understanding and evaluating computer vision models through the lens of\n  counterfactuals", "author": "Pushkar Shukla", "abstract": "  Counterfactual reasoning -- the practice of asking ``what if'' by varying\ninputs and observing changes in model behavior -- has become central to\ninterpretable and fair AI. This thesis develops frameworks that use\ncounterfactuals to explain, audit, and mitigate bias in vision classifiers and\ngenerative models. By systematically altering semantically meaningful\nattributes while holding others fixed, these methods uncover spurious\ncorrelations, probe causal dependencies, and help build more robust systems.\n  The first part addresses vision classifiers. CAVLI integrates attribution\n(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions\nrely on human-interpretable concepts. With localized heatmaps and a Concept\nDependency Score, CAVLI shows when models depend on irrelevant cues like\nbackgrounds. Extending this, ASAC introduces adversarial counterfactuals that\nperturb protected attributes while preserving semantics. Through curriculum\nlearning, ASAC fine-tunes biased models for improved fairness and accuracy\nwhile avoiding stereotype-laden artifacts.\n  The second part targets generative Text-to-Image (TTI) models. TIBET provides\na scalable pipeline for evaluating prompt-sensitive biases by varying\nidentity-related terms, enabling causal auditing of how race, gender, and age\naffect image generation. To capture interactions, BiasConnect builds causal\ngraphs diagnosing intersectional biases. Finally, InterMit offers a modular,\ntraining-free algorithm that mitigates intersectional bias via causal\nsensitivity scores and user-defined fairness goals.\n  Together, these contributions show counterfactuals as a unifying lens for\ninterpretability, fairness, and causality in both discriminative and generative\nmodels, establishing principled, scalable methods for socially responsible bias\nevaluation and mitigation.\n", "link": "http://arxiv.org/abs/2508.20881v1", "date": "2025-08-28", "relevancy": 2.8251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20evaluating%20computer%20vision%20models%20through%20the%20lens%20of%0A%20%20counterfactuals&body=Title%3A%20Understanding%20and%20evaluating%20computer%20vision%20models%20through%20the%20lens%20of%0A%20%20counterfactuals%0AAuthor%3A%20Pushkar%20Shukla%0AAbstract%3A%20%20%20Counterfactual%20reasoning%20--%20the%20practice%20of%20asking%20%60%60what%20if%27%27%20by%20varying%0Ainputs%20and%20observing%20changes%20in%20model%20behavior%20--%20has%20become%20central%20to%0Ainterpretable%20and%20fair%20AI.%20This%20thesis%20develops%20frameworks%20that%20use%0Acounterfactuals%20to%20explain%2C%20audit%2C%20and%20mitigate%20bias%20in%20vision%20classifiers%20and%0Agenerative%20models.%20By%20systematically%20altering%20semantically%20meaningful%0Aattributes%20while%20holding%20others%20fixed%2C%20these%20methods%20uncover%20spurious%0Acorrelations%2C%20probe%20causal%20dependencies%2C%20and%20help%20build%20more%20robust%20systems.%0A%20%20The%20first%20part%20addresses%20vision%20classifiers.%20CAVLI%20integrates%20attribution%0A%28LIME%29%20with%20concept-level%20analysis%20%28TCAV%29%20to%20quantify%20how%20strongly%20decisions%0Arely%20on%20human-interpretable%20concepts.%20With%20localized%20heatmaps%20and%20a%20Concept%0ADependency%20Score%2C%20CAVLI%20shows%20when%20models%20depend%20on%20irrelevant%20cues%20like%0Abackgrounds.%20Extending%20this%2C%20ASAC%20introduces%20adversarial%20counterfactuals%20that%0Aperturb%20protected%20attributes%20while%20preserving%20semantics.%20Through%20curriculum%0Alearning%2C%20ASAC%20fine-tunes%20biased%20models%20for%20improved%20fairness%20and%20accuracy%0Awhile%20avoiding%20stereotype-laden%20artifacts.%0A%20%20The%20second%20part%20targets%20generative%20Text-to-Image%20%28TTI%29%20models.%20TIBET%20provides%0Aa%20scalable%20pipeline%20for%20evaluating%20prompt-sensitive%20biases%20by%20varying%0Aidentity-related%20terms%2C%20enabling%20causal%20auditing%20of%20how%20race%2C%20gender%2C%20and%20age%0Aaffect%20image%20generation.%20To%20capture%20interactions%2C%20BiasConnect%20builds%20causal%0Agraphs%20diagnosing%20intersectional%20biases.%20Finally%2C%20InterMit%20offers%20a%20modular%2C%0Atraining-free%20algorithm%20that%20mitigates%20intersectional%20bias%20via%20causal%0Asensitivity%20scores%20and%20user-defined%20fairness%20goals.%0A%20%20Together%2C%20these%20contributions%20show%20counterfactuals%20as%20a%20unifying%20lens%20for%0Ainterpretability%2C%20fairness%2C%20and%20causality%20in%20both%20discriminative%20and%20generative%0Amodels%2C%20establishing%20principled%2C%20scalable%20methods%20for%20socially%20responsible%20bias%0Aevaluation%20and%20mitigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520evaluating%2520computer%2520vision%2520models%2520through%2520the%2520lens%2520of%250A%2520%2520counterfactuals%26entry.906535625%3DPushkar%2520Shukla%26entry.1292438233%3D%2520%2520Counterfactual%2520reasoning%2520--%2520the%2520practice%2520of%2520asking%2520%2560%2560what%2520if%2527%2527%2520by%2520varying%250Ainputs%2520and%2520observing%2520changes%2520in%2520model%2520behavior%2520--%2520has%2520become%2520central%2520to%250Ainterpretable%2520and%2520fair%2520AI.%2520This%2520thesis%2520develops%2520frameworks%2520that%2520use%250Acounterfactuals%2520to%2520explain%252C%2520audit%252C%2520and%2520mitigate%2520bias%2520in%2520vision%2520classifiers%2520and%250Agenerative%2520models.%2520By%2520systematically%2520altering%2520semantically%2520meaningful%250Aattributes%2520while%2520holding%2520others%2520fixed%252C%2520these%2520methods%2520uncover%2520spurious%250Acorrelations%252C%2520probe%2520causal%2520dependencies%252C%2520and%2520help%2520build%2520more%2520robust%2520systems.%250A%2520%2520The%2520first%2520part%2520addresses%2520vision%2520classifiers.%2520CAVLI%2520integrates%2520attribution%250A%2528LIME%2529%2520with%2520concept-level%2520analysis%2520%2528TCAV%2529%2520to%2520quantify%2520how%2520strongly%2520decisions%250Arely%2520on%2520human-interpretable%2520concepts.%2520With%2520localized%2520heatmaps%2520and%2520a%2520Concept%250ADependency%2520Score%252C%2520CAVLI%2520shows%2520when%2520models%2520depend%2520on%2520irrelevant%2520cues%2520like%250Abackgrounds.%2520Extending%2520this%252C%2520ASAC%2520introduces%2520adversarial%2520counterfactuals%2520that%250Aperturb%2520protected%2520attributes%2520while%2520preserving%2520semantics.%2520Through%2520curriculum%250Alearning%252C%2520ASAC%2520fine-tunes%2520biased%2520models%2520for%2520improved%2520fairness%2520and%2520accuracy%250Awhile%2520avoiding%2520stereotype-laden%2520artifacts.%250A%2520%2520The%2520second%2520part%2520targets%2520generative%2520Text-to-Image%2520%2528TTI%2529%2520models.%2520TIBET%2520provides%250Aa%2520scalable%2520pipeline%2520for%2520evaluating%2520prompt-sensitive%2520biases%2520by%2520varying%250Aidentity-related%2520terms%252C%2520enabling%2520causal%2520auditing%2520of%2520how%2520race%252C%2520gender%252C%2520and%2520age%250Aaffect%2520image%2520generation.%2520To%2520capture%2520interactions%252C%2520BiasConnect%2520builds%2520causal%250Agraphs%2520diagnosing%2520intersectional%2520biases.%2520Finally%252C%2520InterMit%2520offers%2520a%2520modular%252C%250Atraining-free%2520algorithm%2520that%2520mitigates%2520intersectional%2520bias%2520via%2520causal%250Asensitivity%2520scores%2520and%2520user-defined%2520fairness%2520goals.%250A%2520%2520Together%252C%2520these%2520contributions%2520show%2520counterfactuals%2520as%2520a%2520unifying%2520lens%2520for%250Ainterpretability%252C%2520fairness%252C%2520and%2520causality%2520in%2520both%2520discriminative%2520and%2520generative%250Amodels%252C%2520establishing%2520principled%252C%2520scalable%2520methods%2520for%2520socially%2520responsible%2520bias%250Aevaluation%2520and%2520mitigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20evaluating%20computer%20vision%20models%20through%20the%20lens%20of%0A%20%20counterfactuals&entry.906535625=Pushkar%20Shukla&entry.1292438233=%20%20Counterfactual%20reasoning%20--%20the%20practice%20of%20asking%20%60%60what%20if%27%27%20by%20varying%0Ainputs%20and%20observing%20changes%20in%20model%20behavior%20--%20has%20become%20central%20to%0Ainterpretable%20and%20fair%20AI.%20This%20thesis%20develops%20frameworks%20that%20use%0Acounterfactuals%20to%20explain%2C%20audit%2C%20and%20mitigate%20bias%20in%20vision%20classifiers%20and%0Agenerative%20models.%20By%20systematically%20altering%20semantically%20meaningful%0Aattributes%20while%20holding%20others%20fixed%2C%20these%20methods%20uncover%20spurious%0Acorrelations%2C%20probe%20causal%20dependencies%2C%20and%20help%20build%20more%20robust%20systems.%0A%20%20The%20first%20part%20addresses%20vision%20classifiers.%20CAVLI%20integrates%20attribution%0A%28LIME%29%20with%20concept-level%20analysis%20%28TCAV%29%20to%20quantify%20how%20strongly%20decisions%0Arely%20on%20human-interpretable%20concepts.%20With%20localized%20heatmaps%20and%20a%20Concept%0ADependency%20Score%2C%20CAVLI%20shows%20when%20models%20depend%20on%20irrelevant%20cues%20like%0Abackgrounds.%20Extending%20this%2C%20ASAC%20introduces%20adversarial%20counterfactuals%20that%0Aperturb%20protected%20attributes%20while%20preserving%20semantics.%20Through%20curriculum%0Alearning%2C%20ASAC%20fine-tunes%20biased%20models%20for%20improved%20fairness%20and%20accuracy%0Awhile%20avoiding%20stereotype-laden%20artifacts.%0A%20%20The%20second%20part%20targets%20generative%20Text-to-Image%20%28TTI%29%20models.%20TIBET%20provides%0Aa%20scalable%20pipeline%20for%20evaluating%20prompt-sensitive%20biases%20by%20varying%0Aidentity-related%20terms%2C%20enabling%20causal%20auditing%20of%20how%20race%2C%20gender%2C%20and%20age%0Aaffect%20image%20generation.%20To%20capture%20interactions%2C%20BiasConnect%20builds%20causal%0Agraphs%20diagnosing%20intersectional%20biases.%20Finally%2C%20InterMit%20offers%20a%20modular%2C%0Atraining-free%20algorithm%20that%20mitigates%20intersectional%20bias%20via%20causal%0Asensitivity%20scores%20and%20user-defined%20fairness%20goals.%0A%20%20Together%2C%20these%20contributions%20show%20counterfactuals%20as%20a%20unifying%20lens%20for%0Ainterpretability%2C%20fairness%2C%20and%20causality%20in%20both%20discriminative%20and%20generative%0Amodels%2C%20establishing%20principled%2C%20scalable%20methods%20for%20socially%20responsible%20bias%0Aevaluation%20and%20mitigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20881v1&entry.124074799=Read"},
{"title": "Occlusion Robustness of CLIP for Military Vehicle Classification", "author": "Jan Erik van Woerden and Gertjan Burghouts and Lotte Nijskens and Alma M. Liezenga and Sabina van Rooij and Frank Ruis and Hugo J. Kuijf", "abstract": "  Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP.\n", "link": "http://arxiv.org/abs/2508.20760v1", "date": "2025-08-28", "relevancy": 2.8145, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion%20Robustness%20of%20CLIP%20for%20Military%20Vehicle%20Classification&body=Title%3A%20Occlusion%20Robustness%20of%20CLIP%20for%20Military%20Vehicle%20Classification%0AAuthor%3A%20Jan%20Erik%20van%20Woerden%20and%20Gertjan%20Burghouts%20and%20Lotte%20Nijskens%20and%20Alma%20M.%20Liezenga%20and%20Sabina%20van%20Rooij%20and%20Frank%20Ruis%20and%20Hugo%20J.%20Kuijf%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20enable%20zero-shot%20classification%20by%0Aaligning%20images%20and%20text%20in%20a%20shared%20embedding%20space%2C%20offering%20advantages%20for%0Adefense%20applications%20with%20scarce%20labeled%20data.%20However%2C%20CLIP%27s%20robustness%20in%0Achallenging%20military%20environments%2C%20with%20partial%20occlusion%20and%20degraded%0Asignal-to-noise%20ratio%20%28SNR%29%2C%20remains%20underexplored.%20We%20investigate%20CLIP%0Avariants%27%20robustness%20to%20occlusion%20using%20a%20custom%20dataset%20of%2018%20military%20vehicle%0Aclasses%20and%20evaluate%20using%20Normalized%20Area%20Under%20the%20Curve%20%28NAUC%29%20across%0Aocclusion%20percentages.%20Four%20key%20insights%20emerge%3A%20%281%29%20Transformer-based%20CLIP%0Amodels%20consistently%20outperform%20CNNs%2C%20%282%29%20fine-grained%2C%20dispersed%20occlusions%0Adegrade%20performance%20more%20than%20larger%20contiguous%20occlusions%2C%20%283%29%20despite%0Aimproved%20accuracy%2C%20performance%20of%20linear-probed%20models%20sharply%20drops%20at%20around%0A35%25%20occlusion%2C%20%284%29%20by%20finetuning%20the%20model%27s%20backbone%2C%20this%20performance%20drop%0Aoccurs%20at%20more%20than%2060%25%20occlusion.%20These%20results%20underscore%20the%20importance%20of%0Aocclusion-specific%20augmentations%20during%20training%20and%20the%20need%20for%20further%0Aexploration%20into%20patch-level%20sensitivity%20and%20architectural%20resilience%20for%0Areal-world%20deployment%20of%20CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion%2520Robustness%2520of%2520CLIP%2520for%2520Military%2520Vehicle%2520Classification%26entry.906535625%3DJan%2520Erik%2520van%2520Woerden%2520and%2520Gertjan%2520Burghouts%2520and%2520Lotte%2520Nijskens%2520and%2520Alma%2520M.%2520Liezenga%2520and%2520Sabina%2520van%2520Rooij%2520and%2520Frank%2520Ruis%2520and%2520Hugo%2520J.%2520Kuijf%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520enable%2520zero-shot%2520classification%2520by%250Aaligning%2520images%2520and%2520text%2520in%2520a%2520shared%2520embedding%2520space%252C%2520offering%2520advantages%2520for%250Adefense%2520applications%2520with%2520scarce%2520labeled%2520data.%2520However%252C%2520CLIP%2527s%2520robustness%2520in%250Achallenging%2520military%2520environments%252C%2520with%2520partial%2520occlusion%2520and%2520degraded%250Asignal-to-noise%2520ratio%2520%2528SNR%2529%252C%2520remains%2520underexplored.%2520We%2520investigate%2520CLIP%250Avariants%2527%2520robustness%2520to%2520occlusion%2520using%2520a%2520custom%2520dataset%2520of%252018%2520military%2520vehicle%250Aclasses%2520and%2520evaluate%2520using%2520Normalized%2520Area%2520Under%2520the%2520Curve%2520%2528NAUC%2529%2520across%250Aocclusion%2520percentages.%2520Four%2520key%2520insights%2520emerge%253A%2520%25281%2529%2520Transformer-based%2520CLIP%250Amodels%2520consistently%2520outperform%2520CNNs%252C%2520%25282%2529%2520fine-grained%252C%2520dispersed%2520occlusions%250Adegrade%2520performance%2520more%2520than%2520larger%2520contiguous%2520occlusions%252C%2520%25283%2529%2520despite%250Aimproved%2520accuracy%252C%2520performance%2520of%2520linear-probed%2520models%2520sharply%2520drops%2520at%2520around%250A35%2525%2520occlusion%252C%2520%25284%2529%2520by%2520finetuning%2520the%2520model%2527s%2520backbone%252C%2520this%2520performance%2520drop%250Aoccurs%2520at%2520more%2520than%252060%2525%2520occlusion.%2520These%2520results%2520underscore%2520the%2520importance%2520of%250Aocclusion-specific%2520augmentations%2520during%2520training%2520and%2520the%2520need%2520for%2520further%250Aexploration%2520into%2520patch-level%2520sensitivity%2520and%2520architectural%2520resilience%2520for%250Areal-world%2520deployment%2520of%2520CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion%20Robustness%20of%20CLIP%20for%20Military%20Vehicle%20Classification&entry.906535625=Jan%20Erik%20van%20Woerden%20and%20Gertjan%20Burghouts%20and%20Lotte%20Nijskens%20and%20Alma%20M.%20Liezenga%20and%20Sabina%20van%20Rooij%20and%20Frank%20Ruis%20and%20Hugo%20J.%20Kuijf&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20enable%20zero-shot%20classification%20by%0Aaligning%20images%20and%20text%20in%20a%20shared%20embedding%20space%2C%20offering%20advantages%20for%0Adefense%20applications%20with%20scarce%20labeled%20data.%20However%2C%20CLIP%27s%20robustness%20in%0Achallenging%20military%20environments%2C%20with%20partial%20occlusion%20and%20degraded%0Asignal-to-noise%20ratio%20%28SNR%29%2C%20remains%20underexplored.%20We%20investigate%20CLIP%0Avariants%27%20robustness%20to%20occlusion%20using%20a%20custom%20dataset%20of%2018%20military%20vehicle%0Aclasses%20and%20evaluate%20using%20Normalized%20Area%20Under%20the%20Curve%20%28NAUC%29%20across%0Aocclusion%20percentages.%20Four%20key%20insights%20emerge%3A%20%281%29%20Transformer-based%20CLIP%0Amodels%20consistently%20outperform%20CNNs%2C%20%282%29%20fine-grained%2C%20dispersed%20occlusions%0Adegrade%20performance%20more%20than%20larger%20contiguous%20occlusions%2C%20%283%29%20despite%0Aimproved%20accuracy%2C%20performance%20of%20linear-probed%20models%20sharply%20drops%20at%20around%0A35%25%20occlusion%2C%20%284%29%20by%20finetuning%20the%20model%27s%20backbone%2C%20this%20performance%20drop%0Aoccurs%20at%20more%20than%2060%25%20occlusion.%20These%20results%20underscore%20the%20importance%20of%0Aocclusion-specific%20augmentations%20during%20training%20and%20the%20need%20for%20further%0Aexploration%20into%20patch-level%20sensitivity%20and%20architectural%20resilience%20for%0Areal-world%20deployment%20of%20CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20760v1&entry.124074799=Read"},
{"title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from\n  Distracting Videos for Reinforcement Learning", "author": "Qi Wang and Zhipeng Zhang and Baao Xie and Xin Jin and Yunbo Wang and Shiyu Wang and Liaomo Zheng and Xiaokang Yang and Wenjun Zeng", "abstract": "  Training visual reinforcement learning (RL) in practical scenarios presents a\nsignificant challenge, $\\textit{i.e.,}$ RL agents suffer from low sample\nefficiency in environments with variations. While various approaches have\nattempted to alleviate this issue by disentangled representation learning,\nthese methods usually start learning from scratch without prior knowledge of\nthe world. This paper, in contrast, tries to learn and understand underlying\nsemantic variations from distracting videos via offline-to-online latent\ndistillation and flexible disentanglement constraints. To enable effective\ncross-domain semantic knowledge transfer, we introduce an interpretable\nmodel-based RL framework, dubbed Disentangled World Models (DisWM).\nSpecifically, we pretrain the action-free video prediction model offline with\ndisentanglement regularization to extract semantic knowledge from distracting\nvideos. The disentanglement capability of the pretrained model is then\ntransferred to the world model through latent distillation. For finetuning in\nthe online environment, we exploit the knowledge from the pretrained model and\nintroduce a disentanglement constraint to the world model. During the\nadaptation phase, the incorporation of actions and rewards from online\nenvironment interactions enriches the diversity of the data, which in turn\nstrengthens the disentangled representation learning. Experimental results\nvalidate the superiority of our approach on various benchmarks.\n", "link": "http://arxiv.org/abs/2503.08751v2", "date": "2025-08-28", "relevancy": 2.7814, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20World%20Models%3A%20Learning%20to%20Transfer%20Semantic%20Knowledge%20from%0A%20%20Distracting%20Videos%20for%20Reinforcement%20Learning&body=Title%3A%20Disentangled%20World%20Models%3A%20Learning%20to%20Transfer%20Semantic%20Knowledge%20from%0A%20%20Distracting%20Videos%20for%20Reinforcement%20Learning%0AAuthor%3A%20Qi%20Wang%20and%20Zhipeng%20Zhang%20and%20Baao%20Xie%20and%20Xin%20Jin%20and%20Yunbo%20Wang%20and%20Shiyu%20Wang%20and%20Liaomo%20Zheng%20and%20Xiaokang%20Yang%20and%20Wenjun%20Zeng%0AAbstract%3A%20%20%20Training%20visual%20reinforcement%20learning%20%28RL%29%20in%20practical%20scenarios%20presents%20a%0Asignificant%20challenge%2C%20%24%5Ctextit%7Bi.e.%2C%7D%24%20RL%20agents%20suffer%20from%20low%20sample%0Aefficiency%20in%20environments%20with%20variations.%20While%20various%20approaches%20have%0Aattempted%20to%20alleviate%20this%20issue%20by%20disentangled%20representation%20learning%2C%0Athese%20methods%20usually%20start%20learning%20from%20scratch%20without%20prior%20knowledge%20of%0Athe%20world.%20This%20paper%2C%20in%20contrast%2C%20tries%20to%20learn%20and%20understand%20underlying%0Asemantic%20variations%20from%20distracting%20videos%20via%20offline-to-online%20latent%0Adistillation%20and%20flexible%20disentanglement%20constraints.%20To%20enable%20effective%0Across-domain%20semantic%20knowledge%20transfer%2C%20we%20introduce%20an%20interpretable%0Amodel-based%20RL%20framework%2C%20dubbed%20Disentangled%20World%20Models%20%28DisWM%29.%0ASpecifically%2C%20we%20pretrain%20the%20action-free%20video%20prediction%20model%20offline%20with%0Adisentanglement%20regularization%20to%20extract%20semantic%20knowledge%20from%20distracting%0Avideos.%20The%20disentanglement%20capability%20of%20the%20pretrained%20model%20is%20then%0Atransferred%20to%20the%20world%20model%20through%20latent%20distillation.%20For%20finetuning%20in%0Athe%20online%20environment%2C%20we%20exploit%20the%20knowledge%20from%20the%20pretrained%20model%20and%0Aintroduce%20a%20disentanglement%20constraint%20to%20the%20world%20model.%20During%20the%0Aadaptation%20phase%2C%20the%20incorporation%20of%20actions%20and%20rewards%20from%20online%0Aenvironment%20interactions%20enriches%20the%20diversity%20of%20the%20data%2C%20which%20in%20turn%0Astrengthens%20the%20disentangled%20representation%20learning.%20Experimental%20results%0Avalidate%20the%20superiority%20of%20our%20approach%20on%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520World%2520Models%253A%2520Learning%2520to%2520Transfer%2520Semantic%2520Knowledge%2520from%250A%2520%2520Distracting%2520Videos%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DQi%2520Wang%2520and%2520Zhipeng%2520Zhang%2520and%2520Baao%2520Xie%2520and%2520Xin%2520Jin%2520and%2520Yunbo%2520Wang%2520and%2520Shiyu%2520Wang%2520and%2520Liaomo%2520Zheng%2520and%2520Xiaokang%2520Yang%2520and%2520Wenjun%2520Zeng%26entry.1292438233%3D%2520%2520Training%2520visual%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520practical%2520scenarios%2520presents%2520a%250Asignificant%2520challenge%252C%2520%2524%255Ctextit%257Bi.e.%252C%257D%2524%2520RL%2520agents%2520suffer%2520from%2520low%2520sample%250Aefficiency%2520in%2520environments%2520with%2520variations.%2520While%2520various%2520approaches%2520have%250Aattempted%2520to%2520alleviate%2520this%2520issue%2520by%2520disentangled%2520representation%2520learning%252C%250Athese%2520methods%2520usually%2520start%2520learning%2520from%2520scratch%2520without%2520prior%2520knowledge%2520of%250Athe%2520world.%2520This%2520paper%252C%2520in%2520contrast%252C%2520tries%2520to%2520learn%2520and%2520understand%2520underlying%250Asemantic%2520variations%2520from%2520distracting%2520videos%2520via%2520offline-to-online%2520latent%250Adistillation%2520and%2520flexible%2520disentanglement%2520constraints.%2520To%2520enable%2520effective%250Across-domain%2520semantic%2520knowledge%2520transfer%252C%2520we%2520introduce%2520an%2520interpretable%250Amodel-based%2520RL%2520framework%252C%2520dubbed%2520Disentangled%2520World%2520Models%2520%2528DisWM%2529.%250ASpecifically%252C%2520we%2520pretrain%2520the%2520action-free%2520video%2520prediction%2520model%2520offline%2520with%250Adisentanglement%2520regularization%2520to%2520extract%2520semantic%2520knowledge%2520from%2520distracting%250Avideos.%2520The%2520disentanglement%2520capability%2520of%2520the%2520pretrained%2520model%2520is%2520then%250Atransferred%2520to%2520the%2520world%2520model%2520through%2520latent%2520distillation.%2520For%2520finetuning%2520in%250Athe%2520online%2520environment%252C%2520we%2520exploit%2520the%2520knowledge%2520from%2520the%2520pretrained%2520model%2520and%250Aintroduce%2520a%2520disentanglement%2520constraint%2520to%2520the%2520world%2520model.%2520During%2520the%250Aadaptation%2520phase%252C%2520the%2520incorporation%2520of%2520actions%2520and%2520rewards%2520from%2520online%250Aenvironment%2520interactions%2520enriches%2520the%2520diversity%2520of%2520the%2520data%252C%2520which%2520in%2520turn%250Astrengthens%2520the%2520disentangled%2520representation%2520learning.%2520Experimental%2520results%250Avalidate%2520the%2520superiority%2520of%2520our%2520approach%2520on%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20World%20Models%3A%20Learning%20to%20Transfer%20Semantic%20Knowledge%20from%0A%20%20Distracting%20Videos%20for%20Reinforcement%20Learning&entry.906535625=Qi%20Wang%20and%20Zhipeng%20Zhang%20and%20Baao%20Xie%20and%20Xin%20Jin%20and%20Yunbo%20Wang%20and%20Shiyu%20Wang%20and%20Liaomo%20Zheng%20and%20Xiaokang%20Yang%20and%20Wenjun%20Zeng&entry.1292438233=%20%20Training%20visual%20reinforcement%20learning%20%28RL%29%20in%20practical%20scenarios%20presents%20a%0Asignificant%20challenge%2C%20%24%5Ctextit%7Bi.e.%2C%7D%24%20RL%20agents%20suffer%20from%20low%20sample%0Aefficiency%20in%20environments%20with%20variations.%20While%20various%20approaches%20have%0Aattempted%20to%20alleviate%20this%20issue%20by%20disentangled%20representation%20learning%2C%0Athese%20methods%20usually%20start%20learning%20from%20scratch%20without%20prior%20knowledge%20of%0Athe%20world.%20This%20paper%2C%20in%20contrast%2C%20tries%20to%20learn%20and%20understand%20underlying%0Asemantic%20variations%20from%20distracting%20videos%20via%20offline-to-online%20latent%0Adistillation%20and%20flexible%20disentanglement%20constraints.%20To%20enable%20effective%0Across-domain%20semantic%20knowledge%20transfer%2C%20we%20introduce%20an%20interpretable%0Amodel-based%20RL%20framework%2C%20dubbed%20Disentangled%20World%20Models%20%28DisWM%29.%0ASpecifically%2C%20we%20pretrain%20the%20action-free%20video%20prediction%20model%20offline%20with%0Adisentanglement%20regularization%20to%20extract%20semantic%20knowledge%20from%20distracting%0Avideos.%20The%20disentanglement%20capability%20of%20the%20pretrained%20model%20is%20then%0Atransferred%20to%20the%20world%20model%20through%20latent%20distillation.%20For%20finetuning%20in%0Athe%20online%20environment%2C%20we%20exploit%20the%20knowledge%20from%20the%20pretrained%20model%20and%0Aintroduce%20a%20disentanglement%20constraint%20to%20the%20world%20model.%20During%20the%0Aadaptation%20phase%2C%20the%20incorporation%20of%20actions%20and%20rewards%20from%20online%0Aenvironment%20interactions%20enriches%20the%20diversity%20of%20the%20data%2C%20which%20in%20turn%0Astrengthens%20the%20disentangled%20representation%20learning.%20Experimental%20results%0Avalidate%20the%20superiority%20of%20our%20approach%20on%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08751v2&entry.124074799=Read"},
{"title": "SEAL: Structure and Element Aware Learning to Improve Long Structured\n  Document Retrieval", "author": "Xinhao Huang and Zhibo Ren and Yipeng Yu and Ying Zhou and Zulong Chen and Zeyi Wen", "abstract": "  In long structured document retrieval, existing methods typically fine-tune\npre-trained language models (PLMs) using contrastive learning on datasets\nlacking explicit structural information. This practice suffers from two\ncritical issues: 1) current methods fail to leverage structural features and\nelement-level semantics effectively, and 2) the lack of datasets containing\nstructural metadata. To bridge these gaps, we propose \\our, a novel contrastive\nlearning framework. It leverages structure-aware learning to preserve semantic\nhierarchies and masked element alignment for fine-grained semantic\ndiscrimination. Furthermore, we release \\dataset, a long structured document\nretrieval dataset with rich structural annotations. Extensive experiments on\nboth released and industrial datasets across various modern PLMs, along with\nonline A/B testing, demonstrate consistent performance improvements, boosting\nNDCG@10 from 73.96\\% to 77.84\\% on BGE-M3. The resources are available at\nhttps://github.com/xinhaoH/SEAL.\n", "link": "http://arxiv.org/abs/2508.20778v1", "date": "2025-08-28", "relevancy": 2.6691, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Structure%20and%20Element%20Aware%20Learning%20to%20Improve%20Long%20Structured%0A%20%20Document%20Retrieval&body=Title%3A%20SEAL%3A%20Structure%20and%20Element%20Aware%20Learning%20to%20Improve%20Long%20Structured%0A%20%20Document%20Retrieval%0AAuthor%3A%20Xinhao%20Huang%20and%20Zhibo%20Ren%20and%20Yipeng%20Yu%20and%20Ying%20Zhou%20and%20Zulong%20Chen%20and%20Zeyi%20Wen%0AAbstract%3A%20%20%20In%20long%20structured%20document%20retrieval%2C%20existing%20methods%20typically%20fine-tune%0Apre-trained%20language%20models%20%28PLMs%29%20using%20contrastive%20learning%20on%20datasets%0Alacking%20explicit%20structural%20information.%20This%20practice%20suffers%20from%20two%0Acritical%20issues%3A%201%29%20current%20methods%20fail%20to%20leverage%20structural%20features%20and%0Aelement-level%20semantics%20effectively%2C%20and%202%29%20the%20lack%20of%20datasets%20containing%0Astructural%20metadata.%20To%20bridge%20these%20gaps%2C%20we%20propose%20%5Cour%2C%20a%20novel%20contrastive%0Alearning%20framework.%20It%20leverages%20structure-aware%20learning%20to%20preserve%20semantic%0Ahierarchies%20and%20masked%20element%20alignment%20for%20fine-grained%20semantic%0Adiscrimination.%20Furthermore%2C%20we%20release%20%5Cdataset%2C%20a%20long%20structured%20document%0Aretrieval%20dataset%20with%20rich%20structural%20annotations.%20Extensive%20experiments%20on%0Aboth%20released%20and%20industrial%20datasets%20across%20various%20modern%20PLMs%2C%20along%20with%0Aonline%20A/B%20testing%2C%20demonstrate%20consistent%20performance%20improvements%2C%20boosting%0ANDCG%4010%20from%2073.96%5C%25%20to%2077.84%5C%25%20on%20BGE-M3.%20The%20resources%20are%20available%20at%0Ahttps%3A//github.com/xinhaoH/SEAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Structure%2520and%2520Element%2520Aware%2520Learning%2520to%2520Improve%2520Long%2520Structured%250A%2520%2520Document%2520Retrieval%26entry.906535625%3DXinhao%2520Huang%2520and%2520Zhibo%2520Ren%2520and%2520Yipeng%2520Yu%2520and%2520Ying%2520Zhou%2520and%2520Zulong%2520Chen%2520and%2520Zeyi%2520Wen%26entry.1292438233%3D%2520%2520In%2520long%2520structured%2520document%2520retrieval%252C%2520existing%2520methods%2520typically%2520fine-tune%250Apre-trained%2520language%2520models%2520%2528PLMs%2529%2520using%2520contrastive%2520learning%2520on%2520datasets%250Alacking%2520explicit%2520structural%2520information.%2520This%2520practice%2520suffers%2520from%2520two%250Acritical%2520issues%253A%25201%2529%2520current%2520methods%2520fail%2520to%2520leverage%2520structural%2520features%2520and%250Aelement-level%2520semantics%2520effectively%252C%2520and%25202%2529%2520the%2520lack%2520of%2520datasets%2520containing%250Astructural%2520metadata.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520propose%2520%255Cour%252C%2520a%2520novel%2520contrastive%250Alearning%2520framework.%2520It%2520leverages%2520structure-aware%2520learning%2520to%2520preserve%2520semantic%250Ahierarchies%2520and%2520masked%2520element%2520alignment%2520for%2520fine-grained%2520semantic%250Adiscrimination.%2520Furthermore%252C%2520we%2520release%2520%255Cdataset%252C%2520a%2520long%2520structured%2520document%250Aretrieval%2520dataset%2520with%2520rich%2520structural%2520annotations.%2520Extensive%2520experiments%2520on%250Aboth%2520released%2520and%2520industrial%2520datasets%2520across%2520various%2520modern%2520PLMs%252C%2520along%2520with%250Aonline%2520A/B%2520testing%252C%2520demonstrate%2520consistent%2520performance%2520improvements%252C%2520boosting%250ANDCG%254010%2520from%252073.96%255C%2525%2520to%252077.84%255C%2525%2520on%2520BGE-M3.%2520The%2520resources%2520are%2520available%2520at%250Ahttps%253A//github.com/xinhaoH/SEAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Structure%20and%20Element%20Aware%20Learning%20to%20Improve%20Long%20Structured%0A%20%20Document%20Retrieval&entry.906535625=Xinhao%20Huang%20and%20Zhibo%20Ren%20and%20Yipeng%20Yu%20and%20Ying%20Zhou%20and%20Zulong%20Chen%20and%20Zeyi%20Wen&entry.1292438233=%20%20In%20long%20structured%20document%20retrieval%2C%20existing%20methods%20typically%20fine-tune%0Apre-trained%20language%20models%20%28PLMs%29%20using%20contrastive%20learning%20on%20datasets%0Alacking%20explicit%20structural%20information.%20This%20practice%20suffers%20from%20two%0Acritical%20issues%3A%201%29%20current%20methods%20fail%20to%20leverage%20structural%20features%20and%0Aelement-level%20semantics%20effectively%2C%20and%202%29%20the%20lack%20of%20datasets%20containing%0Astructural%20metadata.%20To%20bridge%20these%20gaps%2C%20we%20propose%20%5Cour%2C%20a%20novel%20contrastive%0Alearning%20framework.%20It%20leverages%20structure-aware%20learning%20to%20preserve%20semantic%0Ahierarchies%20and%20masked%20element%20alignment%20for%20fine-grained%20semantic%0Adiscrimination.%20Furthermore%2C%20we%20release%20%5Cdataset%2C%20a%20long%20structured%20document%0Aretrieval%20dataset%20with%20rich%20structural%20annotations.%20Extensive%20experiments%20on%0Aboth%20released%20and%20industrial%20datasets%20across%20various%20modern%20PLMs%2C%20along%20with%0Aonline%20A/B%20testing%2C%20demonstrate%20consistent%20performance%20improvements%2C%20boosting%0ANDCG%4010%20from%2073.96%5C%25%20to%2077.84%5C%25%20on%20BGE-M3.%20The%20resources%20are%20available%20at%0Ahttps%3A//github.com/xinhaoH/SEAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20778v1&entry.124074799=Read"},
{"title": "PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for\n  Point Cloud Classification", "author": "Hao Yang and Qianyu Zhou and Haijia Sun and Xiangtai Li and Xuequan Lu and Lizhuang Ma and Shuicheng Yan", "abstract": "  Domain Generalization (DG) has been recently explored to enhance the\ngeneralizability of Point Cloud Classification (PCC) models toward unseen\ndomains. Prior works are based on convolutional networks, Transformer or Mamba\narchitectures, either suffering from limited receptive fields or high\ncomputational cost, or insufficient long-range dependency modeling. RWKV, as an\nemerging architecture, possesses superior linear complexity, global receptive\nfields, and long-range dependency. In this paper, we present the first work\nthat studies the generalizability of RWKV models in DG PCC. We find that\ndirectly applying RWKV to DG PCC encounters two significant challenges: RWKV's\nfixed direction token shift methods, like Q-Shift, introduce spatial\ndistortions when applied to unstructured point clouds, weakening local\ngeometric modeling and reducing robustness. In addition, the Bi-WKV attention\nin RWKV amplifies slight cross-domain differences in key distributions through\nexponential weighting, leading to attention shifts and degraded generalization.\nTo this end, we propose PointDGRWKV, the first RWKV-based framework tailored\nfor DG PCC. It introduces two key modules to enhance spatial modeling and\ncross-domain robustness, while maintaining RWKV's linear efficiency. In\nparticular, we present Adaptive Geometric Token Shift to model local\nneighborhood structures to improve geometric context awareness. In addition,\nCross-Domain key feature Distribution Alignment is designed to mitigate\nattention drift by aligning key feature distributions across domains. Extensive\nexperiments on multiple benchmarks demonstrate that PointDGRWKV achieves\nstate-of-the-art performance on DG PCC.\n", "link": "http://arxiv.org/abs/2508.20835v1", "date": "2025-08-28", "relevancy": 2.6436, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5202}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointDGRWKV%3A%20Generalizing%20RWKV-like%20Architecture%20to%20Unseen%20Domains%20for%0A%20%20Point%20Cloud%20Classification&body=Title%3A%20PointDGRWKV%3A%20Generalizing%20RWKV-like%20Architecture%20to%20Unseen%20Domains%20for%0A%20%20Point%20Cloud%20Classification%0AAuthor%3A%20Hao%20Yang%20and%20Qianyu%20Zhou%20and%20Haijia%20Sun%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%20has%20been%20recently%20explored%20to%20enhance%20the%0Ageneralizability%20of%20Point%20Cloud%20Classification%20%28PCC%29%20models%20toward%20unseen%0Adomains.%20Prior%20works%20are%20based%20on%20convolutional%20networks%2C%20Transformer%20or%20Mamba%0Aarchitectures%2C%20either%20suffering%20from%20limited%20receptive%20fields%20or%20high%0Acomputational%20cost%2C%20or%20insufficient%20long-range%20dependency%20modeling.%20RWKV%2C%20as%20an%0Aemerging%20architecture%2C%20possesses%20superior%20linear%20complexity%2C%20global%20receptive%0Afields%2C%20and%20long-range%20dependency.%20In%20this%20paper%2C%20we%20present%20the%20first%20work%0Athat%20studies%20the%20generalizability%20of%20RWKV%20models%20in%20DG%20PCC.%20We%20find%20that%0Adirectly%20applying%20RWKV%20to%20DG%20PCC%20encounters%20two%20significant%20challenges%3A%20RWKV%27s%0Afixed%20direction%20token%20shift%20methods%2C%20like%20Q-Shift%2C%20introduce%20spatial%0Adistortions%20when%20applied%20to%20unstructured%20point%20clouds%2C%20weakening%20local%0Ageometric%20modeling%20and%20reducing%20robustness.%20In%20addition%2C%20the%20Bi-WKV%20attention%0Ain%20RWKV%20amplifies%20slight%20cross-domain%20differences%20in%20key%20distributions%20through%0Aexponential%20weighting%2C%20leading%20to%20attention%20shifts%20and%20degraded%20generalization.%0ATo%20this%20end%2C%20we%20propose%20PointDGRWKV%2C%20the%20first%20RWKV-based%20framework%20tailored%0Afor%20DG%20PCC.%20It%20introduces%20two%20key%20modules%20to%20enhance%20spatial%20modeling%20and%0Across-domain%20robustness%2C%20while%20maintaining%20RWKV%27s%20linear%20efficiency.%20In%0Aparticular%2C%20we%20present%20Adaptive%20Geometric%20Token%20Shift%20to%20model%20local%0Aneighborhood%20structures%20to%20improve%20geometric%20context%20awareness.%20In%20addition%2C%0ACross-Domain%20key%20feature%20Distribution%20Alignment%20is%20designed%20to%20mitigate%0Aattention%20drift%20by%20aligning%20key%20feature%20distributions%20across%20domains.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20demonstrate%20that%20PointDGRWKV%20achieves%0Astate-of-the-art%20performance%20on%20DG%20PCC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointDGRWKV%253A%2520Generalizing%2520RWKV-like%2520Architecture%2520to%2520Unseen%2520Domains%2520for%250A%2520%2520Point%2520Cloud%2520Classification%26entry.906535625%3DHao%2520Yang%2520and%2520Qianyu%2520Zhou%2520and%2520Haijia%2520Sun%2520and%2520Xiangtai%2520Li%2520and%2520Xuequan%2520Lu%2520and%2520Lizhuang%2520Ma%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%2520has%2520been%2520recently%2520explored%2520to%2520enhance%2520the%250Ageneralizability%2520of%2520Point%2520Cloud%2520Classification%2520%2528PCC%2529%2520models%2520toward%2520unseen%250Adomains.%2520Prior%2520works%2520are%2520based%2520on%2520convolutional%2520networks%252C%2520Transformer%2520or%2520Mamba%250Aarchitectures%252C%2520either%2520suffering%2520from%2520limited%2520receptive%2520fields%2520or%2520high%250Acomputational%2520cost%252C%2520or%2520insufficient%2520long-range%2520dependency%2520modeling.%2520RWKV%252C%2520as%2520an%250Aemerging%2520architecture%252C%2520possesses%2520superior%2520linear%2520complexity%252C%2520global%2520receptive%250Afields%252C%2520and%2520long-range%2520dependency.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520work%250Athat%2520studies%2520the%2520generalizability%2520of%2520RWKV%2520models%2520in%2520DG%2520PCC.%2520We%2520find%2520that%250Adirectly%2520applying%2520RWKV%2520to%2520DG%2520PCC%2520encounters%2520two%2520significant%2520challenges%253A%2520RWKV%2527s%250Afixed%2520direction%2520token%2520shift%2520methods%252C%2520like%2520Q-Shift%252C%2520introduce%2520spatial%250Adistortions%2520when%2520applied%2520to%2520unstructured%2520point%2520clouds%252C%2520weakening%2520local%250Ageometric%2520modeling%2520and%2520reducing%2520robustness.%2520In%2520addition%252C%2520the%2520Bi-WKV%2520attention%250Ain%2520RWKV%2520amplifies%2520slight%2520cross-domain%2520differences%2520in%2520key%2520distributions%2520through%250Aexponential%2520weighting%252C%2520leading%2520to%2520attention%2520shifts%2520and%2520degraded%2520generalization.%250ATo%2520this%2520end%252C%2520we%2520propose%2520PointDGRWKV%252C%2520the%2520first%2520RWKV-based%2520framework%2520tailored%250Afor%2520DG%2520PCC.%2520It%2520introduces%2520two%2520key%2520modules%2520to%2520enhance%2520spatial%2520modeling%2520and%250Across-domain%2520robustness%252C%2520while%2520maintaining%2520RWKV%2527s%2520linear%2520efficiency.%2520In%250Aparticular%252C%2520we%2520present%2520Adaptive%2520Geometric%2520Token%2520Shift%2520to%2520model%2520local%250Aneighborhood%2520structures%2520to%2520improve%2520geometric%2520context%2520awareness.%2520In%2520addition%252C%250ACross-Domain%2520key%2520feature%2520Distribution%2520Alignment%2520is%2520designed%2520to%2520mitigate%250Aattention%2520drift%2520by%2520aligning%2520key%2520feature%2520distributions%2520across%2520domains.%2520Extensive%250Aexperiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520PointDGRWKV%2520achieves%250Astate-of-the-art%2520performance%2520on%2520DG%2520PCC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointDGRWKV%3A%20Generalizing%20RWKV-like%20Architecture%20to%20Unseen%20Domains%20for%0A%20%20Point%20Cloud%20Classification&entry.906535625=Hao%20Yang%20and%20Qianyu%20Zhou%20and%20Haijia%20Sun%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%20has%20been%20recently%20explored%20to%20enhance%20the%0Ageneralizability%20of%20Point%20Cloud%20Classification%20%28PCC%29%20models%20toward%20unseen%0Adomains.%20Prior%20works%20are%20based%20on%20convolutional%20networks%2C%20Transformer%20or%20Mamba%0Aarchitectures%2C%20either%20suffering%20from%20limited%20receptive%20fields%20or%20high%0Acomputational%20cost%2C%20or%20insufficient%20long-range%20dependency%20modeling.%20RWKV%2C%20as%20an%0Aemerging%20architecture%2C%20possesses%20superior%20linear%20complexity%2C%20global%20receptive%0Afields%2C%20and%20long-range%20dependency.%20In%20this%20paper%2C%20we%20present%20the%20first%20work%0Athat%20studies%20the%20generalizability%20of%20RWKV%20models%20in%20DG%20PCC.%20We%20find%20that%0Adirectly%20applying%20RWKV%20to%20DG%20PCC%20encounters%20two%20significant%20challenges%3A%20RWKV%27s%0Afixed%20direction%20token%20shift%20methods%2C%20like%20Q-Shift%2C%20introduce%20spatial%0Adistortions%20when%20applied%20to%20unstructured%20point%20clouds%2C%20weakening%20local%0Ageometric%20modeling%20and%20reducing%20robustness.%20In%20addition%2C%20the%20Bi-WKV%20attention%0Ain%20RWKV%20amplifies%20slight%20cross-domain%20differences%20in%20key%20distributions%20through%0Aexponential%20weighting%2C%20leading%20to%20attention%20shifts%20and%20degraded%20generalization.%0ATo%20this%20end%2C%20we%20propose%20PointDGRWKV%2C%20the%20first%20RWKV-based%20framework%20tailored%0Afor%20DG%20PCC.%20It%20introduces%20two%20key%20modules%20to%20enhance%20spatial%20modeling%20and%0Across-domain%20robustness%2C%20while%20maintaining%20RWKV%27s%20linear%20efficiency.%20In%0Aparticular%2C%20we%20present%20Adaptive%20Geometric%20Token%20Shift%20to%20model%20local%0Aneighborhood%20structures%20to%20improve%20geometric%20context%20awareness.%20In%20addition%2C%0ACross-Domain%20key%20feature%20Distribution%20Alignment%20is%20designed%20to%20mitigate%0Aattention%20drift%20by%20aligning%20key%20feature%20distributions%20across%20domains.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20demonstrate%20that%20PointDGRWKV%20achieves%0Astate-of-the-art%20performance%20on%20DG%20PCC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20835v1&entry.124074799=Read"},
{"title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset", "author": "Jeongkyun Park and Jung-Wook Hwang and Kwanghee Choi and Seung-Hyun Lee and Jun Hwan Ahn and Rae-Hong Park and Hyung-Min Park", "abstract": "  Inspired by humans comprehending speech in a multi-modal manner, various\naudio-visual datasets have been constructed. However, most existing datasets\nfocus on English, induce dependencies with various prediction models during\ndataset preparation, and have only a small number of multi-view videos. To\nmitigate the limitations, we recently developed the Open Large-scale Korean\nAudio-Visual Speech (OLKAVS) dataset, which is the largest among publicly\navailable audio-visual speech datasets. The dataset contains 1,150 hours of\ntranscribed audio from 1,107 Korean speakers in a studio setup with nine\ndifferent viewpoints and various noise situations. We also provide the\npre-trained baseline models for two tasks, audio-visual speech recognition and\nlip reading. We conducted experiments based on the models to verify the\neffectiveness of multi-modal and multi-view training over uni-modal and\nfrontal-view-only training. We expect the OLKAVS dataset to facilitate\nmulti-modal research in broader areas such as Korean speech recognition,\nspeaker recognition, pronunciation level classification, and mouth motion\nanalysis.\n", "link": "http://arxiv.org/abs/2301.06375v2", "date": "2025-08-28", "relevancy": 2.6151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLKAVS%3A%20An%20Open%20Large-Scale%20Korean%20Audio-Visual%20Speech%20Dataset&body=Title%3A%20OLKAVS%3A%20An%20Open%20Large-Scale%20Korean%20Audio-Visual%20Speech%20Dataset%0AAuthor%3A%20Jeongkyun%20Park%20and%20Jung-Wook%20Hwang%20and%20Kwanghee%20Choi%20and%20Seung-Hyun%20Lee%20and%20Jun%20Hwan%20Ahn%20and%20Rae-Hong%20Park%20and%20Hyung-Min%20Park%0AAbstract%3A%20%20%20Inspired%20by%20humans%20comprehending%20speech%20in%20a%20multi-modal%20manner%2C%20various%0Aaudio-visual%20datasets%20have%20been%20constructed.%20However%2C%20most%20existing%20datasets%0Afocus%20on%20English%2C%20induce%20dependencies%20with%20various%20prediction%20models%20during%0Adataset%20preparation%2C%20and%20have%20only%20a%20small%20number%20of%20multi-view%20videos.%20To%0Amitigate%20the%20limitations%2C%20we%20recently%20developed%20the%20Open%20Large-scale%20Korean%0AAudio-Visual%20Speech%20%28OLKAVS%29%20dataset%2C%20which%20is%20the%20largest%20among%20publicly%0Aavailable%20audio-visual%20speech%20datasets.%20The%20dataset%20contains%201%2C150%20hours%20of%0Atranscribed%20audio%20from%201%2C107%20Korean%20speakers%20in%20a%20studio%20setup%20with%20nine%0Adifferent%20viewpoints%20and%20various%20noise%20situations.%20We%20also%20provide%20the%0Apre-trained%20baseline%20models%20for%20two%20tasks%2C%20audio-visual%20speech%20recognition%20and%0Alip%20reading.%20We%20conducted%20experiments%20based%20on%20the%20models%20to%20verify%20the%0Aeffectiveness%20of%20multi-modal%20and%20multi-view%20training%20over%20uni-modal%20and%0Afrontal-view-only%20training.%20We%20expect%20the%20OLKAVS%20dataset%20to%20facilitate%0Amulti-modal%20research%20in%20broader%20areas%20such%20as%20Korean%20speech%20recognition%2C%0Aspeaker%20recognition%2C%20pronunciation%20level%20classification%2C%20and%20mouth%20motion%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.06375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLKAVS%253A%2520An%2520Open%2520Large-Scale%2520Korean%2520Audio-Visual%2520Speech%2520Dataset%26entry.906535625%3DJeongkyun%2520Park%2520and%2520Jung-Wook%2520Hwang%2520and%2520Kwanghee%2520Choi%2520and%2520Seung-Hyun%2520Lee%2520and%2520Jun%2520Hwan%2520Ahn%2520and%2520Rae-Hong%2520Park%2520and%2520Hyung-Min%2520Park%26entry.1292438233%3D%2520%2520Inspired%2520by%2520humans%2520comprehending%2520speech%2520in%2520a%2520multi-modal%2520manner%252C%2520various%250Aaudio-visual%2520datasets%2520have%2520been%2520constructed.%2520However%252C%2520most%2520existing%2520datasets%250Afocus%2520on%2520English%252C%2520induce%2520dependencies%2520with%2520various%2520prediction%2520models%2520during%250Adataset%2520preparation%252C%2520and%2520have%2520only%2520a%2520small%2520number%2520of%2520multi-view%2520videos.%2520To%250Amitigate%2520the%2520limitations%252C%2520we%2520recently%2520developed%2520the%2520Open%2520Large-scale%2520Korean%250AAudio-Visual%2520Speech%2520%2528OLKAVS%2529%2520dataset%252C%2520which%2520is%2520the%2520largest%2520among%2520publicly%250Aavailable%2520audio-visual%2520speech%2520datasets.%2520The%2520dataset%2520contains%25201%252C150%2520hours%2520of%250Atranscribed%2520audio%2520from%25201%252C107%2520Korean%2520speakers%2520in%2520a%2520studio%2520setup%2520with%2520nine%250Adifferent%2520viewpoints%2520and%2520various%2520noise%2520situations.%2520We%2520also%2520provide%2520the%250Apre-trained%2520baseline%2520models%2520for%2520two%2520tasks%252C%2520audio-visual%2520speech%2520recognition%2520and%250Alip%2520reading.%2520We%2520conducted%2520experiments%2520based%2520on%2520the%2520models%2520to%2520verify%2520the%250Aeffectiveness%2520of%2520multi-modal%2520and%2520multi-view%2520training%2520over%2520uni-modal%2520and%250Afrontal-view-only%2520training.%2520We%2520expect%2520the%2520OLKAVS%2520dataset%2520to%2520facilitate%250Amulti-modal%2520research%2520in%2520broader%2520areas%2520such%2520as%2520Korean%2520speech%2520recognition%252C%250Aspeaker%2520recognition%252C%2520pronunciation%2520level%2520classification%252C%2520and%2520mouth%2520motion%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.06375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLKAVS%3A%20An%20Open%20Large-Scale%20Korean%20Audio-Visual%20Speech%20Dataset&entry.906535625=Jeongkyun%20Park%20and%20Jung-Wook%20Hwang%20and%20Kwanghee%20Choi%20and%20Seung-Hyun%20Lee%20and%20Jun%20Hwan%20Ahn%20and%20Rae-Hong%20Park%20and%20Hyung-Min%20Park&entry.1292438233=%20%20Inspired%20by%20humans%20comprehending%20speech%20in%20a%20multi-modal%20manner%2C%20various%0Aaudio-visual%20datasets%20have%20been%20constructed.%20However%2C%20most%20existing%20datasets%0Afocus%20on%20English%2C%20induce%20dependencies%20with%20various%20prediction%20models%20during%0Adataset%20preparation%2C%20and%20have%20only%20a%20small%20number%20of%20multi-view%20videos.%20To%0Amitigate%20the%20limitations%2C%20we%20recently%20developed%20the%20Open%20Large-scale%20Korean%0AAudio-Visual%20Speech%20%28OLKAVS%29%20dataset%2C%20which%20is%20the%20largest%20among%20publicly%0Aavailable%20audio-visual%20speech%20datasets.%20The%20dataset%20contains%201%2C150%20hours%20of%0Atranscribed%20audio%20from%201%2C107%20Korean%20speakers%20in%20a%20studio%20setup%20with%20nine%0Adifferent%20viewpoints%20and%20various%20noise%20situations.%20We%20also%20provide%20the%0Apre-trained%20baseline%20models%20for%20two%20tasks%2C%20audio-visual%20speech%20recognition%20and%0Alip%20reading.%20We%20conducted%20experiments%20based%20on%20the%20models%20to%20verify%20the%0Aeffectiveness%20of%20multi-modal%20and%20multi-view%20training%20over%20uni-modal%20and%0Afrontal-view-only%20training.%20We%20expect%20the%20OLKAVS%20dataset%20to%20facilitate%0Amulti-modal%20research%20in%20broader%20areas%20such%20as%20Korean%20speech%20recognition%2C%0Aspeaker%20recognition%2C%20pronunciation%20level%20classification%2C%20and%20mouth%20motion%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.06375v2&entry.124074799=Read"},
{"title": "Improving Quantization with Post-Training Model Expansion", "author": "Giuseppe Franco and Pablo Monteagudo-Lago and Ian Colbert and Nicholas Fraser and Michaela Blott", "abstract": "  The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the gap to full-precision perplexity by an average of 9% relative\nto both QuaRot and SpinQuant with only 5% more parameters, which is still a\n3.8% reduction in volume relative to a BF16 reference model.\n", "link": "http://arxiv.org/abs/2503.17513v2", "date": "2025-08-28", "relevancy": 2.6117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Quantization%20with%20Post-Training%20Model%20Expansion&body=Title%3A%20Improving%20Quantization%20with%20Post-Training%20Model%20Expansion%0AAuthor%3A%20Giuseppe%20Franco%20and%20Pablo%20Monteagudo-Lago%20and%20Ian%20Colbert%20and%20Nicholas%20Fraser%20and%20Michaela%20Blott%0AAbstract%3A%20%20%20The%20size%20of%20a%20model%20has%20been%20a%20strong%20predictor%20of%20its%20quality%2C%20as%20well%20as%0Aits%20cost.%20As%20such%2C%20the%20trade-off%20between%20model%20cost%20and%20quality%20has%20been%0Awell-studied.%20Post-training%20optimizations%20like%20quantization%20and%20pruning%20have%0Atypically%20focused%20on%20reducing%20the%20overall%20volume%20of%20pre-trained%20models%20to%0Areduce%20inference%20costs%20while%20maintaining%20model%20quality.%20However%2C%20recent%0Aadvancements%20have%20introduced%20optimization%20techniques%20that%2C%20interestingly%2C%0Aexpand%20models%20post-training%2C%20increasing%20model%20size%20to%20improve%20quality%20when%0Areducing%20volume.%20For%20instance%2C%20to%20enable%204-bit%20weight%20and%20activation%0Aquantization%2C%20incoherence%20processing%20often%20necessitates%20inserting%20online%0AHadamard%20rotations%20in%20the%20compute%20graph%2C%20and%20preserving%20highly%20sensitive%0Aweights%20often%20calls%20for%20additional%20higher%20precision%20computations.%20However%2C%20if%0Aapplication%20requirements%20cannot%20be%20met%2C%20the%20prevailing%20solution%20is%20to%20relax%0Aquantization%20constraints.%20In%20contrast%2C%20we%20demonstrate%20post-training%20model%0Aexpansion%20is%20a%20viable%20strategy%20to%20improve%20model%20quality%20within%20a%20quantization%0Aco-design%20space%2C%20and%20provide%20theoretical%20justification.%20We%20show%20it%20is%20possible%0Ato%20progressively%20and%20selectively%20expand%20the%20size%20of%20a%20pre-trained%20large%0Alanguage%20model%20%28LLM%29%20to%20improve%20model%20quality%20without%20end-to-end%20retraining.%20In%0Aparticular%2C%20when%20quantizing%20the%20weights%20and%20activations%20to%204%20bits%20for%20Llama3%0A1B%2C%20we%20reduce%20the%20gap%20to%20full-precision%20perplexity%20by%20an%20average%20of%209%25%20relative%0Ato%20both%20QuaRot%20and%20SpinQuant%20with%20only%205%25%20more%20parameters%2C%20which%20is%20still%20a%0A3.8%25%20reduction%20in%20volume%20relative%20to%20a%20BF16%20reference%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Quantization%2520with%2520Post-Training%2520Model%2520Expansion%26entry.906535625%3DGiuseppe%2520Franco%2520and%2520Pablo%2520Monteagudo-Lago%2520and%2520Ian%2520Colbert%2520and%2520Nicholas%2520Fraser%2520and%2520Michaela%2520Blott%26entry.1292438233%3D%2520%2520The%2520size%2520of%2520a%2520model%2520has%2520been%2520a%2520strong%2520predictor%2520of%2520its%2520quality%252C%2520as%2520well%2520as%250Aits%2520cost.%2520As%2520such%252C%2520the%2520trade-off%2520between%2520model%2520cost%2520and%2520quality%2520has%2520been%250Awell-studied.%2520Post-training%2520optimizations%2520like%2520quantization%2520and%2520pruning%2520have%250Atypically%2520focused%2520on%2520reducing%2520the%2520overall%2520volume%2520of%2520pre-trained%2520models%2520to%250Areduce%2520inference%2520costs%2520while%2520maintaining%2520model%2520quality.%2520However%252C%2520recent%250Aadvancements%2520have%2520introduced%2520optimization%2520techniques%2520that%252C%2520interestingly%252C%250Aexpand%2520models%2520post-training%252C%2520increasing%2520model%2520size%2520to%2520improve%2520quality%2520when%250Areducing%2520volume.%2520For%2520instance%252C%2520to%2520enable%25204-bit%2520weight%2520and%2520activation%250Aquantization%252C%2520incoherence%2520processing%2520often%2520necessitates%2520inserting%2520online%250AHadamard%2520rotations%2520in%2520the%2520compute%2520graph%252C%2520and%2520preserving%2520highly%2520sensitive%250Aweights%2520often%2520calls%2520for%2520additional%2520higher%2520precision%2520computations.%2520However%252C%2520if%250Aapplication%2520requirements%2520cannot%2520be%2520met%252C%2520the%2520prevailing%2520solution%2520is%2520to%2520relax%250Aquantization%2520constraints.%2520In%2520contrast%252C%2520we%2520demonstrate%2520post-training%2520model%250Aexpansion%2520is%2520a%2520viable%2520strategy%2520to%2520improve%2520model%2520quality%2520within%2520a%2520quantization%250Aco-design%2520space%252C%2520and%2520provide%2520theoretical%2520justification.%2520We%2520show%2520it%2520is%2520possible%250Ato%2520progressively%2520and%2520selectively%2520expand%2520the%2520size%2520of%2520a%2520pre-trained%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520to%2520improve%2520model%2520quality%2520without%2520end-to-end%2520retraining.%2520In%250Aparticular%252C%2520when%2520quantizing%2520the%2520weights%2520and%2520activations%2520to%25204%2520bits%2520for%2520Llama3%250A1B%252C%2520we%2520reduce%2520the%2520gap%2520to%2520full-precision%2520perplexity%2520by%2520an%2520average%2520of%25209%2525%2520relative%250Ato%2520both%2520QuaRot%2520and%2520SpinQuant%2520with%2520only%25205%2525%2520more%2520parameters%252C%2520which%2520is%2520still%2520a%250A3.8%2525%2520reduction%2520in%2520volume%2520relative%2520to%2520a%2520BF16%2520reference%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Quantization%20with%20Post-Training%20Model%20Expansion&entry.906535625=Giuseppe%20Franco%20and%20Pablo%20Monteagudo-Lago%20and%20Ian%20Colbert%20and%20Nicholas%20Fraser%20and%20Michaela%20Blott&entry.1292438233=%20%20The%20size%20of%20a%20model%20has%20been%20a%20strong%20predictor%20of%20its%20quality%2C%20as%20well%20as%0Aits%20cost.%20As%20such%2C%20the%20trade-off%20between%20model%20cost%20and%20quality%20has%20been%0Awell-studied.%20Post-training%20optimizations%20like%20quantization%20and%20pruning%20have%0Atypically%20focused%20on%20reducing%20the%20overall%20volume%20of%20pre-trained%20models%20to%0Areduce%20inference%20costs%20while%20maintaining%20model%20quality.%20However%2C%20recent%0Aadvancements%20have%20introduced%20optimization%20techniques%20that%2C%20interestingly%2C%0Aexpand%20models%20post-training%2C%20increasing%20model%20size%20to%20improve%20quality%20when%0Areducing%20volume.%20For%20instance%2C%20to%20enable%204-bit%20weight%20and%20activation%0Aquantization%2C%20incoherence%20processing%20often%20necessitates%20inserting%20online%0AHadamard%20rotations%20in%20the%20compute%20graph%2C%20and%20preserving%20highly%20sensitive%0Aweights%20often%20calls%20for%20additional%20higher%20precision%20computations.%20However%2C%20if%0Aapplication%20requirements%20cannot%20be%20met%2C%20the%20prevailing%20solution%20is%20to%20relax%0Aquantization%20constraints.%20In%20contrast%2C%20we%20demonstrate%20post-training%20model%0Aexpansion%20is%20a%20viable%20strategy%20to%20improve%20model%20quality%20within%20a%20quantization%0Aco-design%20space%2C%20and%20provide%20theoretical%20justification.%20We%20show%20it%20is%20possible%0Ato%20progressively%20and%20selectively%20expand%20the%20size%20of%20a%20pre-trained%20large%0Alanguage%20model%20%28LLM%29%20to%20improve%20model%20quality%20without%20end-to-end%20retraining.%20In%0Aparticular%2C%20when%20quantizing%20the%20weights%20and%20activations%20to%204%20bits%20for%20Llama3%0A1B%2C%20we%20reduce%20the%20gap%20to%20full-precision%20perplexity%20by%20an%20average%20of%209%25%20relative%0Ato%20both%20QuaRot%20and%20SpinQuant%20with%20only%205%25%20more%20parameters%2C%20which%20is%20still%20a%0A3.8%25%20reduction%20in%20volume%20relative%20to%20a%20BF16%20reference%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17513v2&entry.124074799=Read"},
{"title": "L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved\n  Visible-Infrared Person Re-Identification", "author": "Yan Jiang and Hao Yu and Mengting Wei and Zhaodong Sun and Haoyu Chen and Xu Cheng and Guoying Zhao", "abstract": "  Visible-infrared person re-identification (VI-ReID) is a challenging task\nthat aims to match pedestrian images captured under varying lighting\nconditions, which has drawn intensive research attention and achieved promising\nresults. However, existing methods adopt the centralized training, ignoring the\npotential privacy concerns as the data is distributed across multiple devices\nor entities in reality. In this paper, we propose L2RW+, a benchmark that\nbrings VI-ReID closer to real-world applications. The core rationale behind\nL2RW+ is that incorporating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing constrains.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we simulate the training\nunder real-world data conditions that: 1) data from each camera is completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy restrictions, which is closer to real-world conditions.\nComprehensive experiments show the feasibility and potential of decentralized\nVI-ReID training at both image and video levels. In particular, with increasing\ndata scales, the performance gap between decentralized and centralized training\ndecreases, especially in video-level VI-ReID. In unseen domains, decentralized\ntraining even achieves performance comparable to SOTA centralized methods. This\nwork offers a novel research entry for deploying VI-ReID into real-world\nscenarios and can benefit the community. Code is available at:\nhttps://github.com/Joey623/L2RW.\n", "link": "http://arxiv.org/abs/2503.12232v2", "date": "2025-08-28", "relevancy": 2.5754, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5405}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L2RW%2B%3A%20A%20Comprehensive%20Benchmark%20Towards%20Privacy-Preserved%0A%20%20Visible-Infrared%20Person%20Re-Identification&body=Title%3A%20L2RW%2B%3A%20A%20Comprehensive%20Benchmark%20Towards%20Privacy-Preserved%0A%20%20Visible-Infrared%20Person%20Re-Identification%0AAuthor%3A%20Yan%20Jiang%20and%20Hao%20Yu%20and%20Mengting%20Wei%20and%20Zhaodong%20Sun%20and%20Haoyu%20Chen%20and%20Xu%20Cheng%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20is%20a%20challenging%20task%0Athat%20aims%20to%20match%20pedestrian%20images%20captured%20under%20varying%20lighting%0Aconditions%2C%20which%20has%20drawn%20intensive%20research%20attention%20and%20achieved%20promising%0Aresults.%20However%2C%20existing%20methods%20adopt%20the%20centralized%20training%2C%20ignoring%20the%0Apotential%20privacy%20concerns%20as%20the%20data%20is%20distributed%20across%20multiple%20devices%0Aor%20entities%20in%20reality.%20In%20this%20paper%2C%20we%20propose%20L2RW%2B%2C%20a%20benchmark%20that%0Abrings%20VI-ReID%20closer%20to%20real-world%20applications.%20The%20core%20rationale%20behind%0AL2RW%2B%20is%20that%20incorporating%20decentralized%20training%20into%20VI-ReID%20can%20address%0Aprivacy%20concerns%20in%20scenarios%20with%20limited%20data-sharing%20constrains.%0ASpecifically%2C%20we%20design%20protocols%20and%20corresponding%20algorithms%20for%20different%0Aprivacy%20sensitivity%20levels.%20In%20our%20new%20benchmark%2C%20we%20simulate%20the%20training%0Aunder%20real-world%20data%20conditions%20that%3A%201%29%20data%20from%20each%20camera%20is%20completely%0Aisolated%2C%20or%202%29%20different%20data%20entities%20%28e.g.%2C%20data%20controllers%20of%20a%20certain%0Aregion%29%20can%20selectively%20share%20the%20data.%20In%20this%20way%2C%20we%20simulate%20scenarios%20with%0Astrict%20privacy%20restrictions%2C%20which%20is%20closer%20to%20real-world%20conditions.%0AComprehensive%20experiments%20show%20the%20feasibility%20and%20potential%20of%20decentralized%0AVI-ReID%20training%20at%20both%20image%20and%20video%20levels.%20In%20particular%2C%20with%20increasing%0Adata%20scales%2C%20the%20performance%20gap%20between%20decentralized%20and%20centralized%20training%0Adecreases%2C%20especially%20in%20video-level%20VI-ReID.%20In%20unseen%20domains%2C%20decentralized%0Atraining%20even%20achieves%20performance%20comparable%20to%20SOTA%20centralized%20methods.%20This%0Awork%20offers%20a%20novel%20research%20entry%20for%20deploying%20VI-ReID%20into%20real-world%0Ascenarios%20and%20can%20benefit%20the%20community.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Joey623/L2RW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL2RW%252B%253A%2520A%2520Comprehensive%2520Benchmark%2520Towards%2520Privacy-Preserved%250A%2520%2520Visible-Infrared%2520Person%2520Re-Identification%26entry.906535625%3DYan%2520Jiang%2520and%2520Hao%2520Yu%2520and%2520Mengting%2520Wei%2520and%2520Zhaodong%2520Sun%2520and%2520Haoyu%2520Chen%2520and%2520Xu%2520Cheng%2520and%2520Guoying%2520Zhao%26entry.1292438233%3D%2520%2520Visible-infrared%2520person%2520re-identification%2520%2528VI-ReID%2529%2520is%2520a%2520challenging%2520task%250Athat%2520aims%2520to%2520match%2520pedestrian%2520images%2520captured%2520under%2520varying%2520lighting%250Aconditions%252C%2520which%2520has%2520drawn%2520intensive%2520research%2520attention%2520and%2520achieved%2520promising%250Aresults.%2520However%252C%2520existing%2520methods%2520adopt%2520the%2520centralized%2520training%252C%2520ignoring%2520the%250Apotential%2520privacy%2520concerns%2520as%2520the%2520data%2520is%2520distributed%2520across%2520multiple%2520devices%250Aor%2520entities%2520in%2520reality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520L2RW%252B%252C%2520a%2520benchmark%2520that%250Abrings%2520VI-ReID%2520closer%2520to%2520real-world%2520applications.%2520The%2520core%2520rationale%2520behind%250AL2RW%252B%2520is%2520that%2520incorporating%2520decentralized%2520training%2520into%2520VI-ReID%2520can%2520address%250Aprivacy%2520concerns%2520in%2520scenarios%2520with%2520limited%2520data-sharing%2520constrains.%250ASpecifically%252C%2520we%2520design%2520protocols%2520and%2520corresponding%2520algorithms%2520for%2520different%250Aprivacy%2520sensitivity%2520levels.%2520In%2520our%2520new%2520benchmark%252C%2520we%2520simulate%2520the%2520training%250Aunder%2520real-world%2520data%2520conditions%2520that%253A%25201%2529%2520data%2520from%2520each%2520camera%2520is%2520completely%250Aisolated%252C%2520or%25202%2529%2520different%2520data%2520entities%2520%2528e.g.%252C%2520data%2520controllers%2520of%2520a%2520certain%250Aregion%2529%2520can%2520selectively%2520share%2520the%2520data.%2520In%2520this%2520way%252C%2520we%2520simulate%2520scenarios%2520with%250Astrict%2520privacy%2520restrictions%252C%2520which%2520is%2520closer%2520to%2520real-world%2520conditions.%250AComprehensive%2520experiments%2520show%2520the%2520feasibility%2520and%2520potential%2520of%2520decentralized%250AVI-ReID%2520training%2520at%2520both%2520image%2520and%2520video%2520levels.%2520In%2520particular%252C%2520with%2520increasing%250Adata%2520scales%252C%2520the%2520performance%2520gap%2520between%2520decentralized%2520and%2520centralized%2520training%250Adecreases%252C%2520especially%2520in%2520video-level%2520VI-ReID.%2520In%2520unseen%2520domains%252C%2520decentralized%250Atraining%2520even%2520achieves%2520performance%2520comparable%2520to%2520SOTA%2520centralized%2520methods.%2520This%250Awork%2520offers%2520a%2520novel%2520research%2520entry%2520for%2520deploying%2520VI-ReID%2520into%2520real-world%250Ascenarios%2520and%2520can%2520benefit%2520the%2520community.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Joey623/L2RW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L2RW%2B%3A%20A%20Comprehensive%20Benchmark%20Towards%20Privacy-Preserved%0A%20%20Visible-Infrared%20Person%20Re-Identification&entry.906535625=Yan%20Jiang%20and%20Hao%20Yu%20and%20Mengting%20Wei%20and%20Zhaodong%20Sun%20and%20Haoyu%20Chen%20and%20Xu%20Cheng%20and%20Guoying%20Zhao&entry.1292438233=%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20is%20a%20challenging%20task%0Athat%20aims%20to%20match%20pedestrian%20images%20captured%20under%20varying%20lighting%0Aconditions%2C%20which%20has%20drawn%20intensive%20research%20attention%20and%20achieved%20promising%0Aresults.%20However%2C%20existing%20methods%20adopt%20the%20centralized%20training%2C%20ignoring%20the%0Apotential%20privacy%20concerns%20as%20the%20data%20is%20distributed%20across%20multiple%20devices%0Aor%20entities%20in%20reality.%20In%20this%20paper%2C%20we%20propose%20L2RW%2B%2C%20a%20benchmark%20that%0Abrings%20VI-ReID%20closer%20to%20real-world%20applications.%20The%20core%20rationale%20behind%0AL2RW%2B%20is%20that%20incorporating%20decentralized%20training%20into%20VI-ReID%20can%20address%0Aprivacy%20concerns%20in%20scenarios%20with%20limited%20data-sharing%20constrains.%0ASpecifically%2C%20we%20design%20protocols%20and%20corresponding%20algorithms%20for%20different%0Aprivacy%20sensitivity%20levels.%20In%20our%20new%20benchmark%2C%20we%20simulate%20the%20training%0Aunder%20real-world%20data%20conditions%20that%3A%201%29%20data%20from%20each%20camera%20is%20completely%0Aisolated%2C%20or%202%29%20different%20data%20entities%20%28e.g.%2C%20data%20controllers%20of%20a%20certain%0Aregion%29%20can%20selectively%20share%20the%20data.%20In%20this%20way%2C%20we%20simulate%20scenarios%20with%0Astrict%20privacy%20restrictions%2C%20which%20is%20closer%20to%20real-world%20conditions.%0AComprehensive%20experiments%20show%20the%20feasibility%20and%20potential%20of%20decentralized%0AVI-ReID%20training%20at%20both%20image%20and%20video%20levels.%20In%20particular%2C%20with%20increasing%0Adata%20scales%2C%20the%20performance%20gap%20between%20decentralized%20and%20centralized%20training%0Adecreases%2C%20especially%20in%20video-level%20VI-ReID.%20In%20unseen%20domains%2C%20decentralized%0Atraining%20even%20achieves%20performance%20comparable%20to%20SOTA%20centralized%20methods.%20This%0Awork%20offers%20a%20novel%20research%20entry%20for%20deploying%20VI-ReID%20into%20real-world%0Ascenarios%20and%20can%20benefit%20the%20community.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Joey623/L2RW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12232v2&entry.124074799=Read"},
{"title": "A multi-task neural network for atypical mitosis recognition under\n  domain shift", "author": "Gennaro Percannella and Mattia Sarno and Francesco Tortorella and Mario Vento", "abstract": "  Recognizing atypical mitotic figures in histopathology images allows\nphysicians to correctly assess tumor aggressiveness. Although machine learning\nmodels could be exploited for automatically performing such a task, under\ndomain shift these models suffer from significative performance drops. In this\nwork, an approach based on multi-task learning is proposed for addressing this\nproblem. By exploiting auxiliary tasks, correlated to the main classification\ntask, the proposed approach, submitted to the track 2 of the MItosis DOmain\nGeneralization (MIDOG) challenge, aims to aid the model to focus only on the\nobject to classify, ignoring the domain varying background of the image. The\nproposed approach shows promising performance in a preliminary evaluation\nconducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training\nSet, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25\nchallenge.\n", "link": "http://arxiv.org/abs/2508.21035v1", "date": "2025-08-28", "relevancy": 2.5546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5088}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift&body=Title%3A%20A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift%0AAuthor%3A%20Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento%0AAbstract%3A%20%20%20Recognizing%20atypical%20mitotic%20figures%20in%20histopathology%20images%20allows%0Aphysicians%20to%20correctly%20assess%20tumor%20aggressiveness.%20Although%20machine%20learning%0Amodels%20could%20be%20exploited%20for%20automatically%20performing%20such%20a%20task%2C%20under%0Adomain%20shift%20these%20models%20suffer%20from%20significative%20performance%20drops.%20In%20this%0Awork%2C%20an%20approach%20based%20on%20multi-task%20learning%20is%20proposed%20for%20addressing%20this%0Aproblem.%20By%20exploiting%20auxiliary%20tasks%2C%20correlated%20to%20the%20main%20classification%0Atask%2C%20the%20proposed%20approach%2C%20submitted%20to%20the%20track%202%20of%20the%20MItosis%20DOmain%0AGeneralization%20%28MIDOG%29%20challenge%2C%20aims%20to%20aid%20the%20model%20to%20focus%20only%20on%20the%0Aobject%20to%20classify%2C%20ignoring%20the%20domain%20varying%20background%20of%20the%20image.%20The%0Aproposed%20approach%20shows%20promising%20performance%20in%20a%20preliminary%20evaluation%0Aconducted%20on%20three%20distinct%20datasets%2C%20i.e.%2C%20the%20MIDOG%202025%20Atypical%20Training%0ASet%2C%20the%20Ami-Br%20dataset%2C%20as%20well%20as%20the%20preliminary%20test%20set%20of%20the%20MIDOG25%0Achallenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520multi-task%2520neural%2520network%2520for%2520atypical%2520mitosis%2520recognition%2520under%250A%2520%2520domain%2520shift%26entry.906535625%3DGennaro%2520Percannella%2520and%2520Mattia%2520Sarno%2520and%2520Francesco%2520Tortorella%2520and%2520Mario%2520Vento%26entry.1292438233%3D%2520%2520Recognizing%2520atypical%2520mitotic%2520figures%2520in%2520histopathology%2520images%2520allows%250Aphysicians%2520to%2520correctly%2520assess%2520tumor%2520aggressiveness.%2520Although%2520machine%2520learning%250Amodels%2520could%2520be%2520exploited%2520for%2520automatically%2520performing%2520such%2520a%2520task%252C%2520under%250Adomain%2520shift%2520these%2520models%2520suffer%2520from%2520significative%2520performance%2520drops.%2520In%2520this%250Awork%252C%2520an%2520approach%2520based%2520on%2520multi-task%2520learning%2520is%2520proposed%2520for%2520addressing%2520this%250Aproblem.%2520By%2520exploiting%2520auxiliary%2520tasks%252C%2520correlated%2520to%2520the%2520main%2520classification%250Atask%252C%2520the%2520proposed%2520approach%252C%2520submitted%2520to%2520the%2520track%25202%2520of%2520the%2520MItosis%2520DOmain%250AGeneralization%2520%2528MIDOG%2529%2520challenge%252C%2520aims%2520to%2520aid%2520the%2520model%2520to%2520focus%2520only%2520on%2520the%250Aobject%2520to%2520classify%252C%2520ignoring%2520the%2520domain%2520varying%2520background%2520of%2520the%2520image.%2520The%250Aproposed%2520approach%2520shows%2520promising%2520performance%2520in%2520a%2520preliminary%2520evaluation%250Aconducted%2520on%2520three%2520distinct%2520datasets%252C%2520i.e.%252C%2520the%2520MIDOG%25202025%2520Atypical%2520Training%250ASet%252C%2520the%2520Ami-Br%2520dataset%252C%2520as%2520well%2520as%2520the%2520preliminary%2520test%2520set%2520of%2520the%2520MIDOG25%250Achallenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift&entry.906535625=Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento&entry.1292438233=%20%20Recognizing%20atypical%20mitotic%20figures%20in%20histopathology%20images%20allows%0Aphysicians%20to%20correctly%20assess%20tumor%20aggressiveness.%20Although%20machine%20learning%0Amodels%20could%20be%20exploited%20for%20automatically%20performing%20such%20a%20task%2C%20under%0Adomain%20shift%20these%20models%20suffer%20from%20significative%20performance%20drops.%20In%20this%0Awork%2C%20an%20approach%20based%20on%20multi-task%20learning%20is%20proposed%20for%20addressing%20this%0Aproblem.%20By%20exploiting%20auxiliary%20tasks%2C%20correlated%20to%20the%20main%20classification%0Atask%2C%20the%20proposed%20approach%2C%20submitted%20to%20the%20track%202%20of%20the%20MItosis%20DOmain%0AGeneralization%20%28MIDOG%29%20challenge%2C%20aims%20to%20aid%20the%20model%20to%20focus%20only%20on%20the%0Aobject%20to%20classify%2C%20ignoring%20the%20domain%20varying%20background%20of%20the%20image.%20The%0Aproposed%20approach%20shows%20promising%20performance%20in%20a%20preliminary%20evaluation%0Aconducted%20on%20three%20distinct%20datasets%2C%20i.e.%2C%20the%20MIDOG%202025%20Atypical%20Training%0ASet%2C%20the%20Ami-Br%20dataset%2C%20as%20well%20as%20the%20preliminary%20test%20set%20of%20the%20MIDOG25%0Achallenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21035v1&entry.124074799=Read"},
{"title": "Multi-View 3D Point Tracking", "author": "Frano Raji\u010d and Haofei Xu and Marko Mihajlovic and Siyuan Li and Irem Demir and Emircan G\u00fcndo\u011fdu and Lei Ke and Sergey Prokudin and Marc Pollefeys and Siyu Tang", "abstract": "  We introduce the first data-driven multi-view 3D point tracker, designed to\ntrack arbitrary points in dynamic scenes using multiple camera views. Unlike\nexisting monocular trackers, which struggle with depth ambiguities and\nocclusion, or prior multi-camera methods that require over 20 cameras and\ntedious per-sequence optimization, our feed-forward model directly predicts 3D\ncorrespondences using a practical number of cameras (e.g., four), enabling\nrobust and accurate online tracking. Given known camera poses and either\nsensor-based or estimated multi-view depth, our tracker fuses multi-view\nfeatures into a unified point cloud and applies k-nearest-neighbors correlation\nalongside a transformer-based update to reliably estimate long-range 3D\ncorrespondences, even under occlusion. We train on 5K synthetic multi-view\nKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and\nDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.\nOur method generalizes well to diverse camera setups of 1-8 views with varying\nvantage points and video lengths of 24-150 frames. By releasing our tracker\nalongside training and evaluation datasets, we aim to set a new standard for\nmulti-view 3D tracking research and provide a practical tool for real-world\napplications. Project page available at https://ethz-vlg.github.io/mvtracker.\n", "link": "http://arxiv.org/abs/2508.21060v1", "date": "2025-08-28", "relevancy": 2.5539, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6449}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6372}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%203D%20Point%20Tracking&body=Title%3A%20Multi-View%203D%20Point%20Tracking%0AAuthor%3A%20Frano%20Raji%C4%8D%20and%20Haofei%20Xu%20and%20Marko%20Mihajlovic%20and%20Siyuan%20Li%20and%20Irem%20Demir%20and%20Emircan%20G%C3%BCndo%C4%9Fdu%20and%20Lei%20Ke%20and%20Sergey%20Prokudin%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20data-driven%20multi-view%203D%20point%20tracker%2C%20designed%20to%0Atrack%20arbitrary%20points%20in%20dynamic%20scenes%20using%20multiple%20camera%20views.%20Unlike%0Aexisting%20monocular%20trackers%2C%20which%20struggle%20with%20depth%20ambiguities%20and%0Aocclusion%2C%20or%20prior%20multi-camera%20methods%20that%20require%20over%2020%20cameras%20and%0Atedious%20per-sequence%20optimization%2C%20our%20feed-forward%20model%20directly%20predicts%203D%0Acorrespondences%20using%20a%20practical%20number%20of%20cameras%20%28e.g.%2C%20four%29%2C%20enabling%0Arobust%20and%20accurate%20online%20tracking.%20Given%20known%20camera%20poses%20and%20either%0Asensor-based%20or%20estimated%20multi-view%20depth%2C%20our%20tracker%20fuses%20multi-view%0Afeatures%20into%20a%20unified%20point%20cloud%20and%20applies%20k-nearest-neighbors%20correlation%0Aalongside%20a%20transformer-based%20update%20to%20reliably%20estimate%20long-range%203D%0Acorrespondences%2C%20even%20under%20occlusion.%20We%20train%20on%205K%20synthetic%20multi-view%0AKubric%20sequences%20and%20evaluate%20on%20two%20real-world%20benchmarks%3A%20Panoptic%20Studio%20and%0ADexYCB%2C%20achieving%20median%20trajectory%20errors%20of%203.1%20cm%20and%202.0%20cm%2C%20respectively.%0AOur%20method%20generalizes%20well%20to%20diverse%20camera%20setups%20of%201-8%20views%20with%20varying%0Avantage%20points%20and%20video%20lengths%20of%2024-150%20frames.%20By%20releasing%20our%20tracker%0Aalongside%20training%20and%20evaluation%20datasets%2C%20we%20aim%20to%20set%20a%20new%20standard%20for%0Amulti-view%203D%20tracking%20research%20and%20provide%20a%20practical%20tool%20for%20real-world%0Aapplications.%20Project%20page%20available%20at%20https%3A//ethz-vlg.github.io/mvtracker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%25203D%2520Point%2520Tracking%26entry.906535625%3DFrano%2520Raji%25C4%258D%2520and%2520Haofei%2520Xu%2520and%2520Marko%2520Mihajlovic%2520and%2520Siyuan%2520Li%2520and%2520Irem%2520Demir%2520and%2520Emircan%2520G%25C3%25BCndo%25C4%259Fdu%2520and%2520Lei%2520Ke%2520and%2520Sergey%2520Prokudin%2520and%2520Marc%2520Pollefeys%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520data-driven%2520multi-view%25203D%2520point%2520tracker%252C%2520designed%2520to%250Atrack%2520arbitrary%2520points%2520in%2520dynamic%2520scenes%2520using%2520multiple%2520camera%2520views.%2520Unlike%250Aexisting%2520monocular%2520trackers%252C%2520which%2520struggle%2520with%2520depth%2520ambiguities%2520and%250Aocclusion%252C%2520or%2520prior%2520multi-camera%2520methods%2520that%2520require%2520over%252020%2520cameras%2520and%250Atedious%2520per-sequence%2520optimization%252C%2520our%2520feed-forward%2520model%2520directly%2520predicts%25203D%250Acorrespondences%2520using%2520a%2520practical%2520number%2520of%2520cameras%2520%2528e.g.%252C%2520four%2529%252C%2520enabling%250Arobust%2520and%2520accurate%2520online%2520tracking.%2520Given%2520known%2520camera%2520poses%2520and%2520either%250Asensor-based%2520or%2520estimated%2520multi-view%2520depth%252C%2520our%2520tracker%2520fuses%2520multi-view%250Afeatures%2520into%2520a%2520unified%2520point%2520cloud%2520and%2520applies%2520k-nearest-neighbors%2520correlation%250Aalongside%2520a%2520transformer-based%2520update%2520to%2520reliably%2520estimate%2520long-range%25203D%250Acorrespondences%252C%2520even%2520under%2520occlusion.%2520We%2520train%2520on%25205K%2520synthetic%2520multi-view%250AKubric%2520sequences%2520and%2520evaluate%2520on%2520two%2520real-world%2520benchmarks%253A%2520Panoptic%2520Studio%2520and%250ADexYCB%252C%2520achieving%2520median%2520trajectory%2520errors%2520of%25203.1%2520cm%2520and%25202.0%2520cm%252C%2520respectively.%250AOur%2520method%2520generalizes%2520well%2520to%2520diverse%2520camera%2520setups%2520of%25201-8%2520views%2520with%2520varying%250Avantage%2520points%2520and%2520video%2520lengths%2520of%252024-150%2520frames.%2520By%2520releasing%2520our%2520tracker%250Aalongside%2520training%2520and%2520evaluation%2520datasets%252C%2520we%2520aim%2520to%2520set%2520a%2520new%2520standard%2520for%250Amulti-view%25203D%2520tracking%2520research%2520and%2520provide%2520a%2520practical%2520tool%2520for%2520real-world%250Aapplications.%2520Project%2520page%2520available%2520at%2520https%253A//ethz-vlg.github.io/mvtracker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%203D%20Point%20Tracking&entry.906535625=Frano%20Raji%C4%8D%20and%20Haofei%20Xu%20and%20Marko%20Mihajlovic%20and%20Siyuan%20Li%20and%20Irem%20Demir%20and%20Emircan%20G%C3%BCndo%C4%9Fdu%20and%20Lei%20Ke%20and%20Sergey%20Prokudin%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang&entry.1292438233=%20%20We%20introduce%20the%20first%20data-driven%20multi-view%203D%20point%20tracker%2C%20designed%20to%0Atrack%20arbitrary%20points%20in%20dynamic%20scenes%20using%20multiple%20camera%20views.%20Unlike%0Aexisting%20monocular%20trackers%2C%20which%20struggle%20with%20depth%20ambiguities%20and%0Aocclusion%2C%20or%20prior%20multi-camera%20methods%20that%20require%20over%2020%20cameras%20and%0Atedious%20per-sequence%20optimization%2C%20our%20feed-forward%20model%20directly%20predicts%203D%0Acorrespondences%20using%20a%20practical%20number%20of%20cameras%20%28e.g.%2C%20four%29%2C%20enabling%0Arobust%20and%20accurate%20online%20tracking.%20Given%20known%20camera%20poses%20and%20either%0Asensor-based%20or%20estimated%20multi-view%20depth%2C%20our%20tracker%20fuses%20multi-view%0Afeatures%20into%20a%20unified%20point%20cloud%20and%20applies%20k-nearest-neighbors%20correlation%0Aalongside%20a%20transformer-based%20update%20to%20reliably%20estimate%20long-range%203D%0Acorrespondences%2C%20even%20under%20occlusion.%20We%20train%20on%205K%20synthetic%20multi-view%0AKubric%20sequences%20and%20evaluate%20on%20two%20real-world%20benchmarks%3A%20Panoptic%20Studio%20and%0ADexYCB%2C%20achieving%20median%20trajectory%20errors%20of%203.1%20cm%20and%202.0%20cm%2C%20respectively.%0AOur%20method%20generalizes%20well%20to%20diverse%20camera%20setups%20of%201-8%20views%20with%20varying%0Avantage%20points%20and%20video%20lengths%20of%2024-150%20frames.%20By%20releasing%20our%20tracker%0Aalongside%20training%20and%20evaluation%20datasets%2C%20we%20aim%20to%20set%20a%20new%20standard%20for%0Amulti-view%203D%20tracking%20research%20and%20provide%20a%20practical%20tool%20for%20real-world%0Aapplications.%20Project%20page%20available%20at%20https%3A//ethz-vlg.github.io/mvtracker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21060v1&entry.124074799=Read"},
{"title": "FusionCounting: Robust visible-infrared image fusion guided by crowd\n  counting via multi-task learning", "author": "He Li and Xinyu Liu and Weihang Kong and Xingchen Zhang", "abstract": "  Most visible and infrared image fusion (VIF) methods focus primarily on\noptimizing fused image quality. Recent studies have begun incorporating\ndownstream tasks, such as semantic segmentation and object detection, to\nprovide semantic guidance for VIF. However, semantic segmentation requires\nextensive annotations, while object detection, despite reducing annotation\nefforts compared with segmentation, faces challenges in highly crowded scenes\ndue to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd\ncounting has gained increasing attention in recent years, no studies have\nintegrated VIF and crowd counting into a unified framework. To address these\nchallenges, we propose FusionCounting, a novel multi-task learning framework\nthat integrates crowd counting into the VIF process. Crowd counting provides a\ndirect quantitative measure of population density with minimal annotation,\nmaking it particularly suitable for dense scenes. Our framework leverages both\ninput images and population density information in a mutually beneficial\nmulti-task design. To accelerate convergence and balance tasks contributions,\nwe introduce a dynamic loss function weighting strategy. Furthermore, we\nincorporate adversarial training to enhance the robustness of both VIF and\ncrowd counting, improving the model's stability and resilience to adversarial\nattacks. Experimental results on public datasets demonstrate that\nFusionCounting not only enhances image fusion quality but also achieves\nsuperior crowd counting performance.\n", "link": "http://arxiv.org/abs/2508.20817v1", "date": "2025-08-28", "relevancy": 2.5486, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5221}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5049}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionCounting%3A%20Robust%20visible-infrared%20image%20fusion%20guided%20by%20crowd%0A%20%20counting%20via%20multi-task%20learning&body=Title%3A%20FusionCounting%3A%20Robust%20visible-infrared%20image%20fusion%20guided%20by%20crowd%0A%20%20counting%20via%20multi-task%20learning%0AAuthor%3A%20He%20Li%20and%20Xinyu%20Liu%20and%20Weihang%20Kong%20and%20Xingchen%20Zhang%0AAbstract%3A%20%20%20Most%20visible%20and%20infrared%20image%20fusion%20%28VIF%29%20methods%20focus%20primarily%20on%0Aoptimizing%20fused%20image%20quality.%20Recent%20studies%20have%20begun%20incorporating%0Adownstream%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20object%20detection%2C%20to%0Aprovide%20semantic%20guidance%20for%20VIF.%20However%2C%20semantic%20segmentation%20requires%0Aextensive%20annotations%2C%20while%20object%20detection%2C%20despite%20reducing%20annotation%0Aefforts%20compared%20with%20segmentation%2C%20faces%20challenges%20in%20highly%20crowded%20scenes%0Adue%20to%20overlapping%20bounding%20boxes%20and%20occlusion.%20Moreover%2C%20although%20RGB-T%20crowd%0Acounting%20has%20gained%20increasing%20attention%20in%20recent%20years%2C%20no%20studies%20have%0Aintegrated%20VIF%20and%20crowd%20counting%20into%20a%20unified%20framework.%20To%20address%20these%0Achallenges%2C%20we%20propose%20FusionCounting%2C%20a%20novel%20multi-task%20learning%20framework%0Athat%20integrates%20crowd%20counting%20into%20the%20VIF%20process.%20Crowd%20counting%20provides%20a%0Adirect%20quantitative%20measure%20of%20population%20density%20with%20minimal%20annotation%2C%0Amaking%20it%20particularly%20suitable%20for%20dense%20scenes.%20Our%20framework%20leverages%20both%0Ainput%20images%20and%20population%20density%20information%20in%20a%20mutually%20beneficial%0Amulti-task%20design.%20To%20accelerate%20convergence%20and%20balance%20tasks%20contributions%2C%0Awe%20introduce%20a%20dynamic%20loss%20function%20weighting%20strategy.%20Furthermore%2C%20we%0Aincorporate%20adversarial%20training%20to%20enhance%20the%20robustness%20of%20both%20VIF%20and%0Acrowd%20counting%2C%20improving%20the%20model%27s%20stability%20and%20resilience%20to%20adversarial%0Aattacks.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%0AFusionCounting%20not%20only%20enhances%20image%20fusion%20quality%20but%20also%20achieves%0Asuperior%20crowd%20counting%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionCounting%253A%2520Robust%2520visible-infrared%2520image%2520fusion%2520guided%2520by%2520crowd%250A%2520%2520counting%2520via%2520multi-task%2520learning%26entry.906535625%3DHe%2520Li%2520and%2520Xinyu%2520Liu%2520and%2520Weihang%2520Kong%2520and%2520Xingchen%2520Zhang%26entry.1292438233%3D%2520%2520Most%2520visible%2520and%2520infrared%2520image%2520fusion%2520%2528VIF%2529%2520methods%2520focus%2520primarily%2520on%250Aoptimizing%2520fused%2520image%2520quality.%2520Recent%2520studies%2520have%2520begun%2520incorporating%250Adownstream%2520tasks%252C%2520such%2520as%2520semantic%2520segmentation%2520and%2520object%2520detection%252C%2520to%250Aprovide%2520semantic%2520guidance%2520for%2520VIF.%2520However%252C%2520semantic%2520segmentation%2520requires%250Aextensive%2520annotations%252C%2520while%2520object%2520detection%252C%2520despite%2520reducing%2520annotation%250Aefforts%2520compared%2520with%2520segmentation%252C%2520faces%2520challenges%2520in%2520highly%2520crowded%2520scenes%250Adue%2520to%2520overlapping%2520bounding%2520boxes%2520and%2520occlusion.%2520Moreover%252C%2520although%2520RGB-T%2520crowd%250Acounting%2520has%2520gained%2520increasing%2520attention%2520in%2520recent%2520years%252C%2520no%2520studies%2520have%250Aintegrated%2520VIF%2520and%2520crowd%2520counting%2520into%2520a%2520unified%2520framework.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520FusionCounting%252C%2520a%2520novel%2520multi-task%2520learning%2520framework%250Athat%2520integrates%2520crowd%2520counting%2520into%2520the%2520VIF%2520process.%2520Crowd%2520counting%2520provides%2520a%250Adirect%2520quantitative%2520measure%2520of%2520population%2520density%2520with%2520minimal%2520annotation%252C%250Amaking%2520it%2520particularly%2520suitable%2520for%2520dense%2520scenes.%2520Our%2520framework%2520leverages%2520both%250Ainput%2520images%2520and%2520population%2520density%2520information%2520in%2520a%2520mutually%2520beneficial%250Amulti-task%2520design.%2520To%2520accelerate%2520convergence%2520and%2520balance%2520tasks%2520contributions%252C%250Awe%2520introduce%2520a%2520dynamic%2520loss%2520function%2520weighting%2520strategy.%2520Furthermore%252C%2520we%250Aincorporate%2520adversarial%2520training%2520to%2520enhance%2520the%2520robustness%2520of%2520both%2520VIF%2520and%250Acrowd%2520counting%252C%2520improving%2520the%2520model%2527s%2520stability%2520and%2520resilience%2520to%2520adversarial%250Aattacks.%2520Experimental%2520results%2520on%2520public%2520datasets%2520demonstrate%2520that%250AFusionCounting%2520not%2520only%2520enhances%2520image%2520fusion%2520quality%2520but%2520also%2520achieves%250Asuperior%2520crowd%2520counting%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionCounting%3A%20Robust%20visible-infrared%20image%20fusion%20guided%20by%20crowd%0A%20%20counting%20via%20multi-task%20learning&entry.906535625=He%20Li%20and%20Xinyu%20Liu%20and%20Weihang%20Kong%20and%20Xingchen%20Zhang&entry.1292438233=%20%20Most%20visible%20and%20infrared%20image%20fusion%20%28VIF%29%20methods%20focus%20primarily%20on%0Aoptimizing%20fused%20image%20quality.%20Recent%20studies%20have%20begun%20incorporating%0Adownstream%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20object%20detection%2C%20to%0Aprovide%20semantic%20guidance%20for%20VIF.%20However%2C%20semantic%20segmentation%20requires%0Aextensive%20annotations%2C%20while%20object%20detection%2C%20despite%20reducing%20annotation%0Aefforts%20compared%20with%20segmentation%2C%20faces%20challenges%20in%20highly%20crowded%20scenes%0Adue%20to%20overlapping%20bounding%20boxes%20and%20occlusion.%20Moreover%2C%20although%20RGB-T%20crowd%0Acounting%20has%20gained%20increasing%20attention%20in%20recent%20years%2C%20no%20studies%20have%0Aintegrated%20VIF%20and%20crowd%20counting%20into%20a%20unified%20framework.%20To%20address%20these%0Achallenges%2C%20we%20propose%20FusionCounting%2C%20a%20novel%20multi-task%20learning%20framework%0Athat%20integrates%20crowd%20counting%20into%20the%20VIF%20process.%20Crowd%20counting%20provides%20a%0Adirect%20quantitative%20measure%20of%20population%20density%20with%20minimal%20annotation%2C%0Amaking%20it%20particularly%20suitable%20for%20dense%20scenes.%20Our%20framework%20leverages%20both%0Ainput%20images%20and%20population%20density%20information%20in%20a%20mutually%20beneficial%0Amulti-task%20design.%20To%20accelerate%20convergence%20and%20balance%20tasks%20contributions%2C%0Awe%20introduce%20a%20dynamic%20loss%20function%20weighting%20strategy.%20Furthermore%2C%20we%0Aincorporate%20adversarial%20training%20to%20enhance%20the%20robustness%20of%20both%20VIF%20and%0Acrowd%20counting%2C%20improving%20the%20model%27s%20stability%20and%20resilience%20to%20adversarial%0Aattacks.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%0AFusionCounting%20not%20only%20enhances%20image%20fusion%20quality%20but%20also%20achieves%0Asuperior%20crowd%20counting%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20817v1&entry.124074799=Read"},
{"title": "Learning Robust Spatial Representations from Binaural Audio through\n  Feature Distillation", "author": "Holger Severin Bovbjerg and Jan \u00d8stergaard and Jesper Jensen and Shinji Watanabe and Zheng-Hua Tan", "abstract": "  Recently, deep representation learning has shown strong performance in\nmultiple audio tasks. However, its use for learning spatial representations\nfrom multichannel audio is underexplored. We investigate the use of a\npretraining stage based on feature distillation to learn a robust spatial\nrepresentation of binaural speech without the need for data labels. In this\nframework, spatial features are computed from clean binaural speech samples to\nform prediction labels. These clean features are then predicted from\ncorresponding augmented speech using a neural network. After pretraining, we\nthrow away the spatial feature predictor and use the learned encoder weights to\ninitialize a DoA estimation model which we fine-tune for DoA estimation. Our\nexperiments demonstrate that the pretrained models show improved performance in\nnoisy and reverberant environments after fine-tuning for direction-of-arrival\nestimation, when compared to fully supervised models and classic signal\nprocessing methods.\n", "link": "http://arxiv.org/abs/2508.20914v1", "date": "2025-08-28", "relevancy": 2.546, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5182}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5095}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robust%20Spatial%20Representations%20from%20Binaural%20Audio%20through%0A%20%20Feature%20Distillation&body=Title%3A%20Learning%20Robust%20Spatial%20Representations%20from%20Binaural%20Audio%20through%0A%20%20Feature%20Distillation%0AAuthor%3A%20Holger%20Severin%20Bovbjerg%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Shinji%20Watanabe%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Recently%2C%20deep%20representation%20learning%20has%20shown%20strong%20performance%20in%0Amultiple%20audio%20tasks.%20However%2C%20its%20use%20for%20learning%20spatial%20representations%0Afrom%20multichannel%20audio%20is%20underexplored.%20We%20investigate%20the%20use%20of%20a%0Apretraining%20stage%20based%20on%20feature%20distillation%20to%20learn%20a%20robust%20spatial%0Arepresentation%20of%20binaural%20speech%20without%20the%20need%20for%20data%20labels.%20In%20this%0Aframework%2C%20spatial%20features%20are%20computed%20from%20clean%20binaural%20speech%20samples%20to%0Aform%20prediction%20labels.%20These%20clean%20features%20are%20then%20predicted%20from%0Acorresponding%20augmented%20speech%20using%20a%20neural%20network.%20After%20pretraining%2C%20we%0Athrow%20away%20the%20spatial%20feature%20predictor%20and%20use%20the%20learned%20encoder%20weights%20to%0Ainitialize%20a%20DoA%20estimation%20model%20which%20we%20fine-tune%20for%20DoA%20estimation.%20Our%0Aexperiments%20demonstrate%20that%20the%20pretrained%20models%20show%20improved%20performance%20in%0Anoisy%20and%20reverberant%20environments%20after%20fine-tuning%20for%20direction-of-arrival%0Aestimation%2C%20when%20compared%20to%20fully%20supervised%20models%20and%20classic%20signal%0Aprocessing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robust%2520Spatial%2520Representations%2520from%2520Binaural%2520Audio%2520through%250A%2520%2520Feature%2520Distillation%26entry.906535625%3DHolger%2520Severin%2520Bovbjerg%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Jesper%2520Jensen%2520and%2520Shinji%2520Watanabe%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520representation%2520learning%2520has%2520shown%2520strong%2520performance%2520in%250Amultiple%2520audio%2520tasks.%2520However%252C%2520its%2520use%2520for%2520learning%2520spatial%2520representations%250Afrom%2520multichannel%2520audio%2520is%2520underexplored.%2520We%2520investigate%2520the%2520use%2520of%2520a%250Apretraining%2520stage%2520based%2520on%2520feature%2520distillation%2520to%2520learn%2520a%2520robust%2520spatial%250Arepresentation%2520of%2520binaural%2520speech%2520without%2520the%2520need%2520for%2520data%2520labels.%2520In%2520this%250Aframework%252C%2520spatial%2520features%2520are%2520computed%2520from%2520clean%2520binaural%2520speech%2520samples%2520to%250Aform%2520prediction%2520labels.%2520These%2520clean%2520features%2520are%2520then%2520predicted%2520from%250Acorresponding%2520augmented%2520speech%2520using%2520a%2520neural%2520network.%2520After%2520pretraining%252C%2520we%250Athrow%2520away%2520the%2520spatial%2520feature%2520predictor%2520and%2520use%2520the%2520learned%2520encoder%2520weights%2520to%250Ainitialize%2520a%2520DoA%2520estimation%2520model%2520which%2520we%2520fine-tune%2520for%2520DoA%2520estimation.%2520Our%250Aexperiments%2520demonstrate%2520that%2520the%2520pretrained%2520models%2520show%2520improved%2520performance%2520in%250Anoisy%2520and%2520reverberant%2520environments%2520after%2520fine-tuning%2520for%2520direction-of-arrival%250Aestimation%252C%2520when%2520compared%2520to%2520fully%2520supervised%2520models%2520and%2520classic%2520signal%250Aprocessing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robust%20Spatial%20Representations%20from%20Binaural%20Audio%20through%0A%20%20Feature%20Distillation&entry.906535625=Holger%20Severin%20Bovbjerg%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Shinji%20Watanabe%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Recently%2C%20deep%20representation%20learning%20has%20shown%20strong%20performance%20in%0Amultiple%20audio%20tasks.%20However%2C%20its%20use%20for%20learning%20spatial%20representations%0Afrom%20multichannel%20audio%20is%20underexplored.%20We%20investigate%20the%20use%20of%20a%0Apretraining%20stage%20based%20on%20feature%20distillation%20to%20learn%20a%20robust%20spatial%0Arepresentation%20of%20binaural%20speech%20without%20the%20need%20for%20data%20labels.%20In%20this%0Aframework%2C%20spatial%20features%20are%20computed%20from%20clean%20binaural%20speech%20samples%20to%0Aform%20prediction%20labels.%20These%20clean%20features%20are%20then%20predicted%20from%0Acorresponding%20augmented%20speech%20using%20a%20neural%20network.%20After%20pretraining%2C%20we%0Athrow%20away%20the%20spatial%20feature%20predictor%20and%20use%20the%20learned%20encoder%20weights%20to%0Ainitialize%20a%20DoA%20estimation%20model%20which%20we%20fine-tune%20for%20DoA%20estimation.%20Our%0Aexperiments%20demonstrate%20that%20the%20pretrained%20models%20show%20improved%20performance%20in%0Anoisy%20and%20reverberant%20environments%20after%20fine-tuning%20for%20direction-of-arrival%0Aestimation%2C%20when%20compared%20to%20fully%20supervised%20models%20and%20classic%20signal%0Aprocessing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20914v1&entry.124074799=Read"},
{"title": "ADAGE: Active Defenses Against GNN Extraction", "author": "Jing Xu and Franziska Boenisch and Adam Dziedzic", "abstract": "  Graph Neural Networks (GNNs) achieve high performance in various real-world\napplications, such as drug discovery, traffic states prediction, and\nrecommendation systems. The fact that building powerful GNNs requires a large\namount of training data, powerful computing resources, and human expertise\nturns the models into lucrative targets for model stealing attacks. Prior work\nhas revealed that the threat vector of stealing attacks against GNNs is large\nand diverse, as an attacker can leverage various heterogeneous signals ranging\nfrom node labels to high-dimensional node embeddings to create a local copy of\nthe target GNN at a fraction of the original training costs. This diversity in\nthe threat vector renders the design of effective and general defenses\nchallenging and existing defenses usually focus on one particular stealing\nsetup. Additionally, they solely provide means to identify stolen model copies\nrather than preventing the attack. To close this gap, we propose the first and\ngeneral Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the\nobservation that stealing a model's full functionality requires highly diverse\nqueries to leak its behavior across the input space. Our defense monitors this\nquery diversity and progressively perturbs outputs as the accumulated leakage\ngrows. In contrast to prior work, ADAGE can prevent stealing across all common\nattack setups. Our extensive experimental evaluation using six benchmark\ndatasets, four GNN models, and three types of adaptive attackers shows that\nADAGE penalizes attackers to the degree of rendering stealing impossible,\nwhilst preserving predictive performance on downstream tasks. ADAGE, thereby,\ncontributes towards securely sharing valuable GNNs in the future.\n", "link": "http://arxiv.org/abs/2503.00065v3", "date": "2025-08-28", "relevancy": 2.5309, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADAGE%3A%20Active%20Defenses%20Against%20GNN%20Extraction&body=Title%3A%20ADAGE%3A%20Active%20Defenses%20Against%20GNN%20Extraction%0AAuthor%3A%20Jing%20Xu%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20achieve%20high%20performance%20in%20various%20real-world%0Aapplications%2C%20such%20as%20drug%20discovery%2C%20traffic%20states%20prediction%2C%20and%0Arecommendation%20systems.%20The%20fact%20that%20building%20powerful%20GNNs%20requires%20a%20large%0Aamount%20of%20training%20data%2C%20powerful%20computing%20resources%2C%20and%20human%20expertise%0Aturns%20the%20models%20into%20lucrative%20targets%20for%20model%20stealing%20attacks.%20Prior%20work%0Ahas%20revealed%20that%20the%20threat%20vector%20of%20stealing%20attacks%20against%20GNNs%20is%20large%0Aand%20diverse%2C%20as%20an%20attacker%20can%20leverage%20various%20heterogeneous%20signals%20ranging%0Afrom%20node%20labels%20to%20high-dimensional%20node%20embeddings%20to%20create%20a%20local%20copy%20of%0Athe%20target%20GNN%20at%20a%20fraction%20of%20the%20original%20training%20costs.%20This%20diversity%20in%0Athe%20threat%20vector%20renders%20the%20design%20of%20effective%20and%20general%20defenses%0Achallenging%20and%20existing%20defenses%20usually%20focus%20on%20one%20particular%20stealing%0Asetup.%20Additionally%2C%20they%20solely%20provide%20means%20to%20identify%20stolen%20model%20copies%0Arather%20than%20preventing%20the%20attack.%20To%20close%20this%20gap%2C%20we%20propose%20the%20first%20and%0Ageneral%20Active%20Defense%20Against%20GNN%20Extraction%20%28ADAGE%29.%20ADAGE%20builds%20on%20the%0Aobservation%20that%20stealing%20a%20model%27s%20full%20functionality%20requires%20highly%20diverse%0Aqueries%20to%20leak%20its%20behavior%20across%20the%20input%20space.%20Our%20defense%20monitors%20this%0Aquery%20diversity%20and%20progressively%20perturbs%20outputs%20as%20the%20accumulated%20leakage%0Agrows.%20In%20contrast%20to%20prior%20work%2C%20ADAGE%20can%20prevent%20stealing%20across%20all%20common%0Aattack%20setups.%20Our%20extensive%20experimental%20evaluation%20using%20six%20benchmark%0Adatasets%2C%20four%20GNN%20models%2C%20and%20three%20types%20of%20adaptive%20attackers%20shows%20that%0AADAGE%20penalizes%20attackers%20to%20the%20degree%20of%20rendering%20stealing%20impossible%2C%0Awhilst%20preserving%20predictive%20performance%20on%20downstream%20tasks.%20ADAGE%2C%20thereby%2C%0Acontributes%20towards%20securely%20sharing%20valuable%20GNNs%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00065v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADAGE%253A%2520Active%2520Defenses%2520Against%2520GNN%2520Extraction%26entry.906535625%3DJing%2520Xu%2520and%2520Franziska%2520Boenisch%2520and%2520Adam%2520Dziedzic%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520achieve%2520high%2520performance%2520in%2520various%2520real-world%250Aapplications%252C%2520such%2520as%2520drug%2520discovery%252C%2520traffic%2520states%2520prediction%252C%2520and%250Arecommendation%2520systems.%2520The%2520fact%2520that%2520building%2520powerful%2520GNNs%2520requires%2520a%2520large%250Aamount%2520of%2520training%2520data%252C%2520powerful%2520computing%2520resources%252C%2520and%2520human%2520expertise%250Aturns%2520the%2520models%2520into%2520lucrative%2520targets%2520for%2520model%2520stealing%2520attacks.%2520Prior%2520work%250Ahas%2520revealed%2520that%2520the%2520threat%2520vector%2520of%2520stealing%2520attacks%2520against%2520GNNs%2520is%2520large%250Aand%2520diverse%252C%2520as%2520an%2520attacker%2520can%2520leverage%2520various%2520heterogeneous%2520signals%2520ranging%250Afrom%2520node%2520labels%2520to%2520high-dimensional%2520node%2520embeddings%2520to%2520create%2520a%2520local%2520copy%2520of%250Athe%2520target%2520GNN%2520at%2520a%2520fraction%2520of%2520the%2520original%2520training%2520costs.%2520This%2520diversity%2520in%250Athe%2520threat%2520vector%2520renders%2520the%2520design%2520of%2520effective%2520and%2520general%2520defenses%250Achallenging%2520and%2520existing%2520defenses%2520usually%2520focus%2520on%2520one%2520particular%2520stealing%250Asetup.%2520Additionally%252C%2520they%2520solely%2520provide%2520means%2520to%2520identify%2520stolen%2520model%2520copies%250Arather%2520than%2520preventing%2520the%2520attack.%2520To%2520close%2520this%2520gap%252C%2520we%2520propose%2520the%2520first%2520and%250Ageneral%2520Active%2520Defense%2520Against%2520GNN%2520Extraction%2520%2528ADAGE%2529.%2520ADAGE%2520builds%2520on%2520the%250Aobservation%2520that%2520stealing%2520a%2520model%2527s%2520full%2520functionality%2520requires%2520highly%2520diverse%250Aqueries%2520to%2520leak%2520its%2520behavior%2520across%2520the%2520input%2520space.%2520Our%2520defense%2520monitors%2520this%250Aquery%2520diversity%2520and%2520progressively%2520perturbs%2520outputs%2520as%2520the%2520accumulated%2520leakage%250Agrows.%2520In%2520contrast%2520to%2520prior%2520work%252C%2520ADAGE%2520can%2520prevent%2520stealing%2520across%2520all%2520common%250Aattack%2520setups.%2520Our%2520extensive%2520experimental%2520evaluation%2520using%2520six%2520benchmark%250Adatasets%252C%2520four%2520GNN%2520models%252C%2520and%2520three%2520types%2520of%2520adaptive%2520attackers%2520shows%2520that%250AADAGE%2520penalizes%2520attackers%2520to%2520the%2520degree%2520of%2520rendering%2520stealing%2520impossible%252C%250Awhilst%2520preserving%2520predictive%2520performance%2520on%2520downstream%2520tasks.%2520ADAGE%252C%2520thereby%252C%250Acontributes%2520towards%2520securely%2520sharing%2520valuable%2520GNNs%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00065v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADAGE%3A%20Active%20Defenses%20Against%20GNN%20Extraction&entry.906535625=Jing%20Xu%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20achieve%20high%20performance%20in%20various%20real-world%0Aapplications%2C%20such%20as%20drug%20discovery%2C%20traffic%20states%20prediction%2C%20and%0Arecommendation%20systems.%20The%20fact%20that%20building%20powerful%20GNNs%20requires%20a%20large%0Aamount%20of%20training%20data%2C%20powerful%20computing%20resources%2C%20and%20human%20expertise%0Aturns%20the%20models%20into%20lucrative%20targets%20for%20model%20stealing%20attacks.%20Prior%20work%0Ahas%20revealed%20that%20the%20threat%20vector%20of%20stealing%20attacks%20against%20GNNs%20is%20large%0Aand%20diverse%2C%20as%20an%20attacker%20can%20leverage%20various%20heterogeneous%20signals%20ranging%0Afrom%20node%20labels%20to%20high-dimensional%20node%20embeddings%20to%20create%20a%20local%20copy%20of%0Athe%20target%20GNN%20at%20a%20fraction%20of%20the%20original%20training%20costs.%20This%20diversity%20in%0Athe%20threat%20vector%20renders%20the%20design%20of%20effective%20and%20general%20defenses%0Achallenging%20and%20existing%20defenses%20usually%20focus%20on%20one%20particular%20stealing%0Asetup.%20Additionally%2C%20they%20solely%20provide%20means%20to%20identify%20stolen%20model%20copies%0Arather%20than%20preventing%20the%20attack.%20To%20close%20this%20gap%2C%20we%20propose%20the%20first%20and%0Ageneral%20Active%20Defense%20Against%20GNN%20Extraction%20%28ADAGE%29.%20ADAGE%20builds%20on%20the%0Aobservation%20that%20stealing%20a%20model%27s%20full%20functionality%20requires%20highly%20diverse%0Aqueries%20to%20leak%20its%20behavior%20across%20the%20input%20space.%20Our%20defense%20monitors%20this%0Aquery%20diversity%20and%20progressively%20perturbs%20outputs%20as%20the%20accumulated%20leakage%0Agrows.%20In%20contrast%20to%20prior%20work%2C%20ADAGE%20can%20prevent%20stealing%20across%20all%20common%0Aattack%20setups.%20Our%20extensive%20experimental%20evaluation%20using%20six%20benchmark%0Adatasets%2C%20four%20GNN%20models%2C%20and%20three%20types%20of%20adaptive%20attackers%20shows%20that%0AADAGE%20penalizes%20attackers%20to%20the%20degree%20of%20rendering%20stealing%20impossible%2C%0Awhilst%20preserving%20predictive%20performance%20on%20downstream%20tasks.%20ADAGE%2C%20thereby%2C%0Acontributes%20towards%20securely%20sharing%20valuable%20GNNs%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00065v3&entry.124074799=Read"},
{"title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview", "author": "Jun-Kun Chen and Aayush Bansal and Minh Phuoc Vo and Yu-Xiong Wang", "abstract": "  We present Dress&Dance, a video diffusion framework that generates high\nquality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a\nuser wearing desired garments while moving in accordance with a given reference\nvideo. Our approach requires a single user image and supports a range of tops,\nbottoms, and one-piece garments, as well as simultaneous tops and bottoms\ntry-on in a single pass. Key to our framework is CondNet, a novel conditioning\nnetwork that leverages attention to unify multi-modal inputs (text, images, and\nvideos), thereby enhancing garment registration and motion fidelity. CondNet is\ntrained on heterogeneous training data, combining limited video data and a\nlarger, more readily available image dataset, in a multistage progressive\nmanner. Dress&Dance outperforms existing open source and commercial solutions\nand enables a high quality and flexible try-on experience.\n", "link": "http://arxiv.org/abs/2508.21070v1", "date": "2025-08-28", "relevancy": 2.5221, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.644}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6237}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dress%26Dance%3A%20Dress%20up%20and%20Dance%20as%20You%20Like%20It%20-%20Technical%20Preview&body=Title%3A%20Dress%26Dance%3A%20Dress%20up%20and%20Dance%20as%20You%20Like%20It%20-%20Technical%20Preview%0AAuthor%3A%20Jun-Kun%20Chen%20and%20Aayush%20Bansal%20and%20Minh%20Phuoc%20Vo%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20We%20present%20Dress%26Dance%2C%20a%20video%20diffusion%20framework%20that%20generates%20high%0Aquality%205-second-long%2024%20FPS%20virtual%20try-on%20videos%20at%201152x720%20resolution%20of%20a%0Auser%20wearing%20desired%20garments%20while%20moving%20in%20accordance%20with%20a%20given%20reference%0Avideo.%20Our%20approach%20requires%20a%20single%20user%20image%20and%20supports%20a%20range%20of%20tops%2C%0Abottoms%2C%20and%20one-piece%20garments%2C%20as%20well%20as%20simultaneous%20tops%20and%20bottoms%0Atry-on%20in%20a%20single%20pass.%20Key%20to%20our%20framework%20is%20CondNet%2C%20a%20novel%20conditioning%0Anetwork%20that%20leverages%20attention%20to%20unify%20multi-modal%20inputs%20%28text%2C%20images%2C%20and%0Avideos%29%2C%20thereby%20enhancing%20garment%20registration%20and%20motion%20fidelity.%20CondNet%20is%0Atrained%20on%20heterogeneous%20training%20data%2C%20combining%20limited%20video%20data%20and%20a%0Alarger%2C%20more%20readily%20available%20image%20dataset%2C%20in%20a%20multistage%20progressive%0Amanner.%20Dress%26Dance%20outperforms%20existing%20open%20source%20and%20commercial%20solutions%0Aand%20enables%20a%20high%20quality%20and%20flexible%20try-on%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDress%2526Dance%253A%2520Dress%2520up%2520and%2520Dance%2520as%2520You%2520Like%2520It%2520-%2520Technical%2520Preview%26entry.906535625%3DJun-Kun%2520Chen%2520and%2520Aayush%2520Bansal%2520and%2520Minh%2520Phuoc%2520Vo%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520Dress%2526Dance%252C%2520a%2520video%2520diffusion%2520framework%2520that%2520generates%2520high%250Aquality%25205-second-long%252024%2520FPS%2520virtual%2520try-on%2520videos%2520at%25201152x720%2520resolution%2520of%2520a%250Auser%2520wearing%2520desired%2520garments%2520while%2520moving%2520in%2520accordance%2520with%2520a%2520given%2520reference%250Avideo.%2520Our%2520approach%2520requires%2520a%2520single%2520user%2520image%2520and%2520supports%2520a%2520range%2520of%2520tops%252C%250Abottoms%252C%2520and%2520one-piece%2520garments%252C%2520as%2520well%2520as%2520simultaneous%2520tops%2520and%2520bottoms%250Atry-on%2520in%2520a%2520single%2520pass.%2520Key%2520to%2520our%2520framework%2520is%2520CondNet%252C%2520a%2520novel%2520conditioning%250Anetwork%2520that%2520leverages%2520attention%2520to%2520unify%2520multi-modal%2520inputs%2520%2528text%252C%2520images%252C%2520and%250Avideos%2529%252C%2520thereby%2520enhancing%2520garment%2520registration%2520and%2520motion%2520fidelity.%2520CondNet%2520is%250Atrained%2520on%2520heterogeneous%2520training%2520data%252C%2520combining%2520limited%2520video%2520data%2520and%2520a%250Alarger%252C%2520more%2520readily%2520available%2520image%2520dataset%252C%2520in%2520a%2520multistage%2520progressive%250Amanner.%2520Dress%2526Dance%2520outperforms%2520existing%2520open%2520source%2520and%2520commercial%2520solutions%250Aand%2520enables%2520a%2520high%2520quality%2520and%2520flexible%2520try-on%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dress%26Dance%3A%20Dress%20up%20and%20Dance%20as%20You%20Like%20It%20-%20Technical%20Preview&entry.906535625=Jun-Kun%20Chen%20and%20Aayush%20Bansal%20and%20Minh%20Phuoc%20Vo%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20We%20present%20Dress%26Dance%2C%20a%20video%20diffusion%20framework%20that%20generates%20high%0Aquality%205-second-long%2024%20FPS%20virtual%20try-on%20videos%20at%201152x720%20resolution%20of%20a%0Auser%20wearing%20desired%20garments%20while%20moving%20in%20accordance%20with%20a%20given%20reference%0Avideo.%20Our%20approach%20requires%20a%20single%20user%20image%20and%20supports%20a%20range%20of%20tops%2C%0Abottoms%2C%20and%20one-piece%20garments%2C%20as%20well%20as%20simultaneous%20tops%20and%20bottoms%0Atry-on%20in%20a%20single%20pass.%20Key%20to%20our%20framework%20is%20CondNet%2C%20a%20novel%20conditioning%0Anetwork%20that%20leverages%20attention%20to%20unify%20multi-modal%20inputs%20%28text%2C%20images%2C%20and%0Avideos%29%2C%20thereby%20enhancing%20garment%20registration%20and%20motion%20fidelity.%20CondNet%20is%0Atrained%20on%20heterogeneous%20training%20data%2C%20combining%20limited%20video%20data%20and%20a%0Alarger%2C%20more%20readily%20available%20image%20dataset%2C%20in%20a%20multistage%20progressive%0Amanner.%20Dress%26Dance%20outperforms%20existing%20open%20source%20and%20commercial%20solutions%0Aand%20enables%20a%20high%20quality%20and%20flexible%20try-on%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21070v1&entry.124074799=Read"},
{"title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for\n  Zero-Shot 3D Visual Grounding", "author": "Jiawen Lin and Shiran Bian and Yihang Zhu and Wenbin Tan and Yachao Zhang and Yuan Xie and Yanyun Qu", "abstract": "  3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using\nnatural language descriptions. Although supervised methods achieve higher\naccuracy in constrained settings, zero-shot 3DVG holds greater promise for\nreal-world applications since eliminating scene-specific training requirements.\nHowever, existing zero-shot methods face challenges of spatial-limited\nreasoning due to reliance on single-view localization, and contextual omissions\nor detail degradation. To address these issues, we propose SeqVLM, a novel\nzero-shot 3DVG framework that leverages multi-view real-world scene images with\nspatial information for target object reasoning. Specifically, SeqVLM first\ngenerates 3D instance proposals via a 3D semantic segmentation network and\nrefines them through semantic filtering, retaining only semantic-relevant\ncandidates. A proposal-guided multi-view projection strategy then projects\nthese candidate proposals onto real scene image sequences, preserving spatial\nrelationships and contextual details in the conversion process of 3D point\ncloud to images. Furthermore, to mitigate VLM computational overload, we\nimplement a dynamic scheduling mechanism that iteratively processes\nsequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to\nidentify textually specified objects. Experiments on the ScanRefer and Nr3D\nbenchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores\nof 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,\nrespectively, which advance 3DVG toward greater generalization and real-world\napplicability. The code is available at https://github.com/JiawLin/SeqVLM.\n", "link": "http://arxiv.org/abs/2508.20758v1", "date": "2025-08-28", "relevancy": 2.5126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6355}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeqVLM%3A%20Proposal-Guided%20Multi-View%20Sequences%20Reasoning%20via%20VLM%20for%0A%20%20Zero-Shot%203D%20Visual%20Grounding&body=Title%3A%20SeqVLM%3A%20Proposal-Guided%20Multi-View%20Sequences%20Reasoning%20via%20VLM%20for%0A%20%20Zero-Shot%203D%20Visual%20Grounding%0AAuthor%3A%20Jiawen%20Lin%20and%20Shiran%20Bian%20and%20Yihang%20Zhu%20and%20Wenbin%20Tan%20and%20Yachao%20Zhang%20and%20Yuan%20Xie%20and%20Yanyun%20Qu%0AAbstract%3A%20%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20localize%20objects%20in%203D%20scenes%20using%0Anatural%20language%20descriptions.%20Although%20supervised%20methods%20achieve%20higher%0Aaccuracy%20in%20constrained%20settings%2C%20zero-shot%203DVG%20holds%20greater%20promise%20for%0Areal-world%20applications%20since%20eliminating%20scene-specific%20training%20requirements.%0AHowever%2C%20existing%20zero-shot%20methods%20face%20challenges%20of%20spatial-limited%0Areasoning%20due%20to%20reliance%20on%20single-view%20localization%2C%20and%20contextual%20omissions%0Aor%20detail%20degradation.%20To%20address%20these%20issues%2C%20we%20propose%20SeqVLM%2C%20a%20novel%0Azero-shot%203DVG%20framework%20that%20leverages%20multi-view%20real-world%20scene%20images%20with%0Aspatial%20information%20for%20target%20object%20reasoning.%20Specifically%2C%20SeqVLM%20first%0Agenerates%203D%20instance%20proposals%20via%20a%203D%20semantic%20segmentation%20network%20and%0Arefines%20them%20through%20semantic%20filtering%2C%20retaining%20only%20semantic-relevant%0Acandidates.%20A%20proposal-guided%20multi-view%20projection%20strategy%20then%20projects%0Athese%20candidate%20proposals%20onto%20real%20scene%20image%20sequences%2C%20preserving%20spatial%0Arelationships%20and%20contextual%20details%20in%20the%20conversion%20process%20of%203D%20point%0Acloud%20to%20images.%20Furthermore%2C%20to%20mitigate%20VLM%20computational%20overload%2C%20we%0Aimplement%20a%20dynamic%20scheduling%20mechanism%20that%20iteratively%20processes%0Asequances-query%20prompts%2C%20leveraging%20VLM%27s%20cross-modal%20reasoning%20capabilities%20to%0Aidentify%20textually%20specified%20objects.%20Experiments%20on%20the%20ScanRefer%20and%20Nr3D%0Abenchmarks%20demonstrate%20state-of-the-art%20performance%2C%20achieving%20Acc%400.25%20scores%0Aof%2055.6%25%20and%2053.2%25%2C%20surpassing%20previous%20zero-shot%20methods%20by%204.0%25%20and%205.2%25%2C%0Arespectively%2C%20which%20advance%203DVG%20toward%20greater%20generalization%20and%20real-world%0Aapplicability.%20The%20code%20is%20available%20at%20https%3A//github.com/JiawLin/SeqVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeqVLM%253A%2520Proposal-Guided%2520Multi-View%2520Sequences%2520Reasoning%2520via%2520VLM%2520for%250A%2520%2520Zero-Shot%25203D%2520Visual%2520Grounding%26entry.906535625%3DJiawen%2520Lin%2520and%2520Shiran%2520Bian%2520and%2520Yihang%2520Zhu%2520and%2520Wenbin%2520Tan%2520and%2520Yachao%2520Zhang%2520and%2520Yuan%2520Xie%2520and%2520Yanyun%2520Qu%26entry.1292438233%3D%2520%25203D%2520Visual%2520Grounding%2520%25283DVG%2529%2520aims%2520to%2520localize%2520objects%2520in%25203D%2520scenes%2520using%250Anatural%2520language%2520descriptions.%2520Although%2520supervised%2520methods%2520achieve%2520higher%250Aaccuracy%2520in%2520constrained%2520settings%252C%2520zero-shot%25203DVG%2520holds%2520greater%2520promise%2520for%250Areal-world%2520applications%2520since%2520eliminating%2520scene-specific%2520training%2520requirements.%250AHowever%252C%2520existing%2520zero-shot%2520methods%2520face%2520challenges%2520of%2520spatial-limited%250Areasoning%2520due%2520to%2520reliance%2520on%2520single-view%2520localization%252C%2520and%2520contextual%2520omissions%250Aor%2520detail%2520degradation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520SeqVLM%252C%2520a%2520novel%250Azero-shot%25203DVG%2520framework%2520that%2520leverages%2520multi-view%2520real-world%2520scene%2520images%2520with%250Aspatial%2520information%2520for%2520target%2520object%2520reasoning.%2520Specifically%252C%2520SeqVLM%2520first%250Agenerates%25203D%2520instance%2520proposals%2520via%2520a%25203D%2520semantic%2520segmentation%2520network%2520and%250Arefines%2520them%2520through%2520semantic%2520filtering%252C%2520retaining%2520only%2520semantic-relevant%250Acandidates.%2520A%2520proposal-guided%2520multi-view%2520projection%2520strategy%2520then%2520projects%250Athese%2520candidate%2520proposals%2520onto%2520real%2520scene%2520image%2520sequences%252C%2520preserving%2520spatial%250Arelationships%2520and%2520contextual%2520details%2520in%2520the%2520conversion%2520process%2520of%25203D%2520point%250Acloud%2520to%2520images.%2520Furthermore%252C%2520to%2520mitigate%2520VLM%2520computational%2520overload%252C%2520we%250Aimplement%2520a%2520dynamic%2520scheduling%2520mechanism%2520that%2520iteratively%2520processes%250Asequances-query%2520prompts%252C%2520leveraging%2520VLM%2527s%2520cross-modal%2520reasoning%2520capabilities%2520to%250Aidentify%2520textually%2520specified%2520objects.%2520Experiments%2520on%2520the%2520ScanRefer%2520and%2520Nr3D%250Abenchmarks%2520demonstrate%2520state-of-the-art%2520performance%252C%2520achieving%2520Acc%25400.25%2520scores%250Aof%252055.6%2525%2520and%252053.2%2525%252C%2520surpassing%2520previous%2520zero-shot%2520methods%2520by%25204.0%2525%2520and%25205.2%2525%252C%250Arespectively%252C%2520which%2520advance%25203DVG%2520toward%2520greater%2520generalization%2520and%2520real-world%250Aapplicability.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/JiawLin/SeqVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeqVLM%3A%20Proposal-Guided%20Multi-View%20Sequences%20Reasoning%20via%20VLM%20for%0A%20%20Zero-Shot%203D%20Visual%20Grounding&entry.906535625=Jiawen%20Lin%20and%20Shiran%20Bian%20and%20Yihang%20Zhu%20and%20Wenbin%20Tan%20and%20Yachao%20Zhang%20and%20Yuan%20Xie%20and%20Yanyun%20Qu&entry.1292438233=%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20localize%20objects%20in%203D%20scenes%20using%0Anatural%20language%20descriptions.%20Although%20supervised%20methods%20achieve%20higher%0Aaccuracy%20in%20constrained%20settings%2C%20zero-shot%203DVG%20holds%20greater%20promise%20for%0Areal-world%20applications%20since%20eliminating%20scene-specific%20training%20requirements.%0AHowever%2C%20existing%20zero-shot%20methods%20face%20challenges%20of%20spatial-limited%0Areasoning%20due%20to%20reliance%20on%20single-view%20localization%2C%20and%20contextual%20omissions%0Aor%20detail%20degradation.%20To%20address%20these%20issues%2C%20we%20propose%20SeqVLM%2C%20a%20novel%0Azero-shot%203DVG%20framework%20that%20leverages%20multi-view%20real-world%20scene%20images%20with%0Aspatial%20information%20for%20target%20object%20reasoning.%20Specifically%2C%20SeqVLM%20first%0Agenerates%203D%20instance%20proposals%20via%20a%203D%20semantic%20segmentation%20network%20and%0Arefines%20them%20through%20semantic%20filtering%2C%20retaining%20only%20semantic-relevant%0Acandidates.%20A%20proposal-guided%20multi-view%20projection%20strategy%20then%20projects%0Athese%20candidate%20proposals%20onto%20real%20scene%20image%20sequences%2C%20preserving%20spatial%0Arelationships%20and%20contextual%20details%20in%20the%20conversion%20process%20of%203D%20point%0Acloud%20to%20images.%20Furthermore%2C%20to%20mitigate%20VLM%20computational%20overload%2C%20we%0Aimplement%20a%20dynamic%20scheduling%20mechanism%20that%20iteratively%20processes%0Asequances-query%20prompts%2C%20leveraging%20VLM%27s%20cross-modal%20reasoning%20capabilities%20to%0Aidentify%20textually%20specified%20objects.%20Experiments%20on%20the%20ScanRefer%20and%20Nr3D%0Abenchmarks%20demonstrate%20state-of-the-art%20performance%2C%20achieving%20Acc%400.25%20scores%0Aof%2055.6%25%20and%2053.2%25%2C%20surpassing%20previous%20zero-shot%20methods%20by%204.0%25%20and%205.2%25%2C%0Arespectively%2C%20which%20advance%203DVG%20toward%20greater%20generalization%20and%20real-world%0Aapplicability.%20The%20code%20is%20available%20at%20https%3A//github.com/JiawLin/SeqVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20758v1&entry.124074799=Read"},
{"title": "On the Theoretical Limitations of Embedding-Based Retrieval", "author": "Orion Weller and Michael Boratko and Iftekhar Naim and Jinhyuk Lee", "abstract": "  Vector embeddings have been tasked with an ever-increasing set of retrieval\ntasks over the years, with a nascent rise in using them for reasoning,\ninstruction-following, coding, and more. These new benchmarks push embeddings\nto work for any query and any notion of relevance that could be given. While\nprior works have pointed out theoretical limitations of vector embeddings,\nthere is a common assumption that these difficulties are exclusively due to\nunrealistic queries, and those that are not can be overcome with better\ntraining data and larger models. In this work, we demonstrate that we may\nencounter these theoretical limitations in realistic settings with extremely\nsimple queries. We connect known results in learning theory, showing that the\nnumber of top-k subsets of documents capable of being returned as the result of\nsome query is limited by the dimension of the embedding. We empirically show\nthat this holds true even if we restrict to k=2, and directly optimize on the\ntest set with free parameterized embeddings. We then create a realistic dataset\ncalled LIMIT that stress tests models based on these theoretical results, and\nobserve that even state-of-the-art models fail on this dataset despite the\nsimple nature of the task. Our work shows the limits of embedding models under\nthe existing single vector paradigm and calls for future research to develop\nmethods that can resolve this fundamental limitation.\n", "link": "http://arxiv.org/abs/2508.21038v1", "date": "2025-08-28", "relevancy": 2.4982, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Theoretical%20Limitations%20of%20Embedding-Based%20Retrieval&body=Title%3A%20On%20the%20Theoretical%20Limitations%20of%20Embedding-Based%20Retrieval%0AAuthor%3A%20Orion%20Weller%20and%20Michael%20Boratko%20and%20Iftekhar%20Naim%20and%20Jinhyuk%20Lee%0AAbstract%3A%20%20%20Vector%20embeddings%20have%20been%20tasked%20with%20an%20ever-increasing%20set%20of%20retrieval%0Atasks%20over%20the%20years%2C%20with%20a%20nascent%20rise%20in%20using%20them%20for%20reasoning%2C%0Ainstruction-following%2C%20coding%2C%20and%20more.%20These%20new%20benchmarks%20push%20embeddings%0Ato%20work%20for%20any%20query%20and%20any%20notion%20of%20relevance%20that%20could%20be%20given.%20While%0Aprior%20works%20have%20pointed%20out%20theoretical%20limitations%20of%20vector%20embeddings%2C%0Athere%20is%20a%20common%20assumption%20that%20these%20difficulties%20are%20exclusively%20due%20to%0Aunrealistic%20queries%2C%20and%20those%20that%20are%20not%20can%20be%20overcome%20with%20better%0Atraining%20data%20and%20larger%20models.%20In%20this%20work%2C%20we%20demonstrate%20that%20we%20may%0Aencounter%20these%20theoretical%20limitations%20in%20realistic%20settings%20with%20extremely%0Asimple%20queries.%20We%20connect%20known%20results%20in%20learning%20theory%2C%20showing%20that%20the%0Anumber%20of%20top-k%20subsets%20of%20documents%20capable%20of%20being%20returned%20as%20the%20result%20of%0Asome%20query%20is%20limited%20by%20the%20dimension%20of%20the%20embedding.%20We%20empirically%20show%0Athat%20this%20holds%20true%20even%20if%20we%20restrict%20to%20k%3D2%2C%20and%20directly%20optimize%20on%20the%0Atest%20set%20with%20free%20parameterized%20embeddings.%20We%20then%20create%20a%20realistic%20dataset%0Acalled%20LIMIT%20that%20stress%20tests%20models%20based%20on%20these%20theoretical%20results%2C%20and%0Aobserve%20that%20even%20state-of-the-art%20models%20fail%20on%20this%20dataset%20despite%20the%0Asimple%20nature%20of%20the%20task.%20Our%20work%20shows%20the%20limits%20of%20embedding%20models%20under%0Athe%20existing%20single%20vector%20paradigm%20and%20calls%20for%20future%20research%20to%20develop%0Amethods%20that%20can%20resolve%20this%20fundamental%20limitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Theoretical%2520Limitations%2520of%2520Embedding-Based%2520Retrieval%26entry.906535625%3DOrion%2520Weller%2520and%2520Michael%2520Boratko%2520and%2520Iftekhar%2520Naim%2520and%2520Jinhyuk%2520Lee%26entry.1292438233%3D%2520%2520Vector%2520embeddings%2520have%2520been%2520tasked%2520with%2520an%2520ever-increasing%2520set%2520of%2520retrieval%250Atasks%2520over%2520the%2520years%252C%2520with%2520a%2520nascent%2520rise%2520in%2520using%2520them%2520for%2520reasoning%252C%250Ainstruction-following%252C%2520coding%252C%2520and%2520more.%2520These%2520new%2520benchmarks%2520push%2520embeddings%250Ato%2520work%2520for%2520any%2520query%2520and%2520any%2520notion%2520of%2520relevance%2520that%2520could%2520be%2520given.%2520While%250Aprior%2520works%2520have%2520pointed%2520out%2520theoretical%2520limitations%2520of%2520vector%2520embeddings%252C%250Athere%2520is%2520a%2520common%2520assumption%2520that%2520these%2520difficulties%2520are%2520exclusively%2520due%2520to%250Aunrealistic%2520queries%252C%2520and%2520those%2520that%2520are%2520not%2520can%2520be%2520overcome%2520with%2520better%250Atraining%2520data%2520and%2520larger%2520models.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520we%2520may%250Aencounter%2520these%2520theoretical%2520limitations%2520in%2520realistic%2520settings%2520with%2520extremely%250Asimple%2520queries.%2520We%2520connect%2520known%2520results%2520in%2520learning%2520theory%252C%2520showing%2520that%2520the%250Anumber%2520of%2520top-k%2520subsets%2520of%2520documents%2520capable%2520of%2520being%2520returned%2520as%2520the%2520result%2520of%250Asome%2520query%2520is%2520limited%2520by%2520the%2520dimension%2520of%2520the%2520embedding.%2520We%2520empirically%2520show%250Athat%2520this%2520holds%2520true%2520even%2520if%2520we%2520restrict%2520to%2520k%253D2%252C%2520and%2520directly%2520optimize%2520on%2520the%250Atest%2520set%2520with%2520free%2520parameterized%2520embeddings.%2520We%2520then%2520create%2520a%2520realistic%2520dataset%250Acalled%2520LIMIT%2520that%2520stress%2520tests%2520models%2520based%2520on%2520these%2520theoretical%2520results%252C%2520and%250Aobserve%2520that%2520even%2520state-of-the-art%2520models%2520fail%2520on%2520this%2520dataset%2520despite%2520the%250Asimple%2520nature%2520of%2520the%2520task.%2520Our%2520work%2520shows%2520the%2520limits%2520of%2520embedding%2520models%2520under%250Athe%2520existing%2520single%2520vector%2520paradigm%2520and%2520calls%2520for%2520future%2520research%2520to%2520develop%250Amethods%2520that%2520can%2520resolve%2520this%2520fundamental%2520limitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Theoretical%20Limitations%20of%20Embedding-Based%20Retrieval&entry.906535625=Orion%20Weller%20and%20Michael%20Boratko%20and%20Iftekhar%20Naim%20and%20Jinhyuk%20Lee&entry.1292438233=%20%20Vector%20embeddings%20have%20been%20tasked%20with%20an%20ever-increasing%20set%20of%20retrieval%0Atasks%20over%20the%20years%2C%20with%20a%20nascent%20rise%20in%20using%20them%20for%20reasoning%2C%0Ainstruction-following%2C%20coding%2C%20and%20more.%20These%20new%20benchmarks%20push%20embeddings%0Ato%20work%20for%20any%20query%20and%20any%20notion%20of%20relevance%20that%20could%20be%20given.%20While%0Aprior%20works%20have%20pointed%20out%20theoretical%20limitations%20of%20vector%20embeddings%2C%0Athere%20is%20a%20common%20assumption%20that%20these%20difficulties%20are%20exclusively%20due%20to%0Aunrealistic%20queries%2C%20and%20those%20that%20are%20not%20can%20be%20overcome%20with%20better%0Atraining%20data%20and%20larger%20models.%20In%20this%20work%2C%20we%20demonstrate%20that%20we%20may%0Aencounter%20these%20theoretical%20limitations%20in%20realistic%20settings%20with%20extremely%0Asimple%20queries.%20We%20connect%20known%20results%20in%20learning%20theory%2C%20showing%20that%20the%0Anumber%20of%20top-k%20subsets%20of%20documents%20capable%20of%20being%20returned%20as%20the%20result%20of%0Asome%20query%20is%20limited%20by%20the%20dimension%20of%20the%20embedding.%20We%20empirically%20show%0Athat%20this%20holds%20true%20even%20if%20we%20restrict%20to%20k%3D2%2C%20and%20directly%20optimize%20on%20the%0Atest%20set%20with%20free%20parameterized%20embeddings.%20We%20then%20create%20a%20realistic%20dataset%0Acalled%20LIMIT%20that%20stress%20tests%20models%20based%20on%20these%20theoretical%20results%2C%20and%0Aobserve%20that%20even%20state-of-the-art%20models%20fail%20on%20this%20dataset%20despite%20the%0Asimple%20nature%20of%20the%20task.%20Our%20work%20shows%20the%20limits%20of%20embedding%20models%20under%0Athe%20existing%20single%20vector%20paradigm%20and%20calls%20for%20future%20research%20to%20develop%0Amethods%20that%20can%20resolve%20this%20fundamental%20limitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21038v1&entry.124074799=Read"},
{"title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal\n  Understanding and Reasoning", "author": "Nannan Zhu and Yonghao Dong and Teng Wang and Xueqian Li and Shengjun Deng and Yijia Wang and Zheng Hong and Tiantian Geng and Guo Niu and Hanyan Huang and Xiongfei Yao and Shuaiwei Jiao", "abstract": "  While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs. The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.\n", "link": "http://arxiv.org/abs/2508.19542v2", "date": "2025-08-28", "relevancy": 2.4954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVBench%3A%20Evaluating%20Cross-Video%20Synergies%20for%20Complex%20Multimodal%0A%20%20Understanding%20and%20Reasoning&body=Title%3A%20CVBench%3A%20Evaluating%20Cross-Video%20Synergies%20for%20Complex%20Multimodal%0A%20%20Understanding%20and%20Reasoning%0AAuthor%3A%20Nannan%20Zhu%20and%20Yonghao%20Dong%20and%20Teng%20Wang%20and%20Xueqian%20Li%20and%20Shengjun%20Deng%20and%20Yijia%20Wang%20and%20Zheng%20Hong%20and%20Tiantian%20Geng%20and%20Guo%20Niu%20and%20Hanyan%20Huang%20and%20Xiongfei%20Yao%20and%20Shuaiwei%20Jiao%0AAbstract%3A%20%20%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20exhibit%20strong%20performance%20on%0Asingle-video%20tasks%20%28e.g.%2C%20video%20question%20answering%29%2C%20their%20ability%20across%0Amultiple%20videos%20remains%20critically%20underexplored.%20However%2C%20this%20capability%20is%0Aessential%20for%20real-world%20applications%2C%20including%20multi-camera%20surveillance%20and%0Across-video%20procedural%20learning.%20To%20bridge%20this%20gap%2C%20we%20present%20CVBench%2C%20the%0Afirst%20comprehensive%20benchmark%20designed%20to%20assess%20cross-video%20relational%0Areasoning%20rigorously.%20CVBench%20comprises%201%2C000%20question-answer%20pairs%20spanning%0Athree%20hierarchical%20tiers%3A%20cross-video%20object%20association%20%28identifying%20shared%0Aentities%29%2C%20cross-video%20event%20association%20%28linking%20temporal%20or%20causal%20event%0Achains%29%2C%20and%20cross-video%20complex%20reasoning%20%28integrating%20commonsense%20and%20domain%0Aknowledge%29.%20Built%20from%20five%20domain-diverse%20video%20clusters%20%28e.g.%2C%20sports%2C%20life%0Arecords%29%2C%20the%20benchmark%20challenges%20models%20to%20synthesise%20information%20across%0Adynamic%20visual%20contexts.%20Extensive%20evaluation%20of%2010%2B%20leading%20MLLMs%20%28including%0AGPT-4o%2C%20Gemini-2.0-flash%2C%20Qwen2.5-VL%29%20under%20zero-shot%20or%20chain-of-thought%0Aprompting%20paradigms.%20Key%20findings%20reveal%20stark%20performance%20gaps%3A%20even%20top%0Amodels%2C%20such%20as%20GPT-4o%2C%20achieve%20only%2060%25%20accuracy%20on%20causal%20reasoning%20tasks%2C%0Acompared%20to%20the%2091%25%20accuracy%20of%20human%20performance.%20Crucially%2C%20our%20analysis%0Areveals%20fundamental%20bottlenecks%20inherent%20in%20current%20MLLM%20architectures%2C%20notably%0Adeficient%20inter-video%20context%20retention%20and%20poor%20disambiguation%20of%20overlapping%0Aentities.%20CVBench%20establishes%20a%20rigorous%20framework%20for%20diagnosing%20and%20advancing%0Amulti-video%20reasoning%2C%20offering%20architectural%20insights%20for%20next-generation%0AMLLMs.%20The%20data%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//github.com/Hokhim2/CVBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVBench%253A%2520Evaluating%2520Cross-Video%2520Synergies%2520for%2520Complex%2520Multimodal%250A%2520%2520Understanding%2520and%2520Reasoning%26entry.906535625%3DNannan%2520Zhu%2520and%2520Yonghao%2520Dong%2520and%2520Teng%2520Wang%2520and%2520Xueqian%2520Li%2520and%2520Shengjun%2520Deng%2520and%2520Yijia%2520Wang%2520and%2520Zheng%2520Hong%2520and%2520Tiantian%2520Geng%2520and%2520Guo%2520Niu%2520and%2520Hanyan%2520Huang%2520and%2520Xiongfei%2520Yao%2520and%2520Shuaiwei%2520Jiao%26entry.1292438233%3D%2520%2520While%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520exhibit%2520strong%2520performance%2520on%250Asingle-video%2520tasks%2520%2528e.g.%252C%2520video%2520question%2520answering%2529%252C%2520their%2520ability%2520across%250Amultiple%2520videos%2520remains%2520critically%2520underexplored.%2520However%252C%2520this%2520capability%2520is%250Aessential%2520for%2520real-world%2520applications%252C%2520including%2520multi-camera%2520surveillance%2520and%250Across-video%2520procedural%2520learning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520CVBench%252C%2520the%250Afirst%2520comprehensive%2520benchmark%2520designed%2520to%2520assess%2520cross-video%2520relational%250Areasoning%2520rigorously.%2520CVBench%2520comprises%25201%252C000%2520question-answer%2520pairs%2520spanning%250Athree%2520hierarchical%2520tiers%253A%2520cross-video%2520object%2520association%2520%2528identifying%2520shared%250Aentities%2529%252C%2520cross-video%2520event%2520association%2520%2528linking%2520temporal%2520or%2520causal%2520event%250Achains%2529%252C%2520and%2520cross-video%2520complex%2520reasoning%2520%2528integrating%2520commonsense%2520and%2520domain%250Aknowledge%2529.%2520Built%2520from%2520five%2520domain-diverse%2520video%2520clusters%2520%2528e.g.%252C%2520sports%252C%2520life%250Arecords%2529%252C%2520the%2520benchmark%2520challenges%2520models%2520to%2520synthesise%2520information%2520across%250Adynamic%2520visual%2520contexts.%2520Extensive%2520evaluation%2520of%252010%252B%2520leading%2520MLLMs%2520%2528including%250AGPT-4o%252C%2520Gemini-2.0-flash%252C%2520Qwen2.5-VL%2529%2520under%2520zero-shot%2520or%2520chain-of-thought%250Aprompting%2520paradigms.%2520Key%2520findings%2520reveal%2520stark%2520performance%2520gaps%253A%2520even%2520top%250Amodels%252C%2520such%2520as%2520GPT-4o%252C%2520achieve%2520only%252060%2525%2520accuracy%2520on%2520causal%2520reasoning%2520tasks%252C%250Acompared%2520to%2520the%252091%2525%2520accuracy%2520of%2520human%2520performance.%2520Crucially%252C%2520our%2520analysis%250Areveals%2520fundamental%2520bottlenecks%2520inherent%2520in%2520current%2520MLLM%2520architectures%252C%2520notably%250Adeficient%2520inter-video%2520context%2520retention%2520and%2520poor%2520disambiguation%2520of%2520overlapping%250Aentities.%2520CVBench%2520establishes%2520a%2520rigorous%2520framework%2520for%2520diagnosing%2520and%2520advancing%250Amulti-video%2520reasoning%252C%2520offering%2520architectural%2520insights%2520for%2520next-generation%250AMLLMs.%2520The%2520data%2520and%2520evaluation%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Hokhim2/CVBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVBench%3A%20Evaluating%20Cross-Video%20Synergies%20for%20Complex%20Multimodal%0A%20%20Understanding%20and%20Reasoning&entry.906535625=Nannan%20Zhu%20and%20Yonghao%20Dong%20and%20Teng%20Wang%20and%20Xueqian%20Li%20and%20Shengjun%20Deng%20and%20Yijia%20Wang%20and%20Zheng%20Hong%20and%20Tiantian%20Geng%20and%20Guo%20Niu%20and%20Hanyan%20Huang%20and%20Xiongfei%20Yao%20and%20Shuaiwei%20Jiao&entry.1292438233=%20%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20exhibit%20strong%20performance%20on%0Asingle-video%20tasks%20%28e.g.%2C%20video%20question%20answering%29%2C%20their%20ability%20across%0Amultiple%20videos%20remains%20critically%20underexplored.%20However%2C%20this%20capability%20is%0Aessential%20for%20real-world%20applications%2C%20including%20multi-camera%20surveillance%20and%0Across-video%20procedural%20learning.%20To%20bridge%20this%20gap%2C%20we%20present%20CVBench%2C%20the%0Afirst%20comprehensive%20benchmark%20designed%20to%20assess%20cross-video%20relational%0Areasoning%20rigorously.%20CVBench%20comprises%201%2C000%20question-answer%20pairs%20spanning%0Athree%20hierarchical%20tiers%3A%20cross-video%20object%20association%20%28identifying%20shared%0Aentities%29%2C%20cross-video%20event%20association%20%28linking%20temporal%20or%20causal%20event%0Achains%29%2C%20and%20cross-video%20complex%20reasoning%20%28integrating%20commonsense%20and%20domain%0Aknowledge%29.%20Built%20from%20five%20domain-diverse%20video%20clusters%20%28e.g.%2C%20sports%2C%20life%0Arecords%29%2C%20the%20benchmark%20challenges%20models%20to%20synthesise%20information%20across%0Adynamic%20visual%20contexts.%20Extensive%20evaluation%20of%2010%2B%20leading%20MLLMs%20%28including%0AGPT-4o%2C%20Gemini-2.0-flash%2C%20Qwen2.5-VL%29%20under%20zero-shot%20or%20chain-of-thought%0Aprompting%20paradigms.%20Key%20findings%20reveal%20stark%20performance%20gaps%3A%20even%20top%0Amodels%2C%20such%20as%20GPT-4o%2C%20achieve%20only%2060%25%20accuracy%20on%20causal%20reasoning%20tasks%2C%0Acompared%20to%20the%2091%25%20accuracy%20of%20human%20performance.%20Crucially%2C%20our%20analysis%0Areveals%20fundamental%20bottlenecks%20inherent%20in%20current%20MLLM%20architectures%2C%20notably%0Adeficient%20inter-video%20context%20retention%20and%20poor%20disambiguation%20of%20overlapping%0Aentities.%20CVBench%20establishes%20a%20rigorous%20framework%20for%20diagnosing%20and%20advancing%0Amulti-video%20reasoning%2C%20offering%20architectural%20insights%20for%20next-generation%0AMLLMs.%20The%20data%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//github.com/Hokhim2/CVBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19542v2&entry.124074799=Read"},
{"title": "InSQuAD: In-Context Learning for Efficient Retrieval via Submodular\n  Mutual Information to Enforce Quality and Diversity", "author": "Souradeep Nanda and Anay Majee and Rishabh Iyer", "abstract": "  In this paper, we introduce InSQuAD, designed to enhance the performance of\nIn-Context Learning (ICL) models through Submodular Mutual Information} (SMI)\nenforcing Quality and Diversity among in-context exemplars. InSQuAD achieves\nthis through two principal strategies: First, we model the ICL task as a\ntargeted selection problem and introduce a unified selection strategy based on\nSMIs which mines relevant yet diverse in-context examples encapsulating the\nnotions of quality and diversity. Secondly, we address a common pitfall in\nexisting retrieval models which model query relevance, often overlooking\ndiversity, critical for ICL. InSQuAD introduces a combinatorial training\nparadigm which learns the parameters of an SMI function to enforce both quality\nand diversity in the retrieval model through a novel likelihood-based loss. To\nfurther aid the learning process we augment an existing multi-hop question\nanswering dataset with synthetically generated paraphrases. Adopting the\nretrieval model trained using this strategy alongside the novel targeted\nselection formulation for ICL on nine benchmark datasets shows significant\nimprovements validating the efficacy of our approach.\n", "link": "http://arxiv.org/abs/2508.21003v1", "date": "2025-08-28", "relevancy": 2.4833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InSQuAD%3A%20In-Context%20Learning%20for%20Efficient%20Retrieval%20via%20Submodular%0A%20%20Mutual%20Information%20to%20Enforce%20Quality%20and%20Diversity&body=Title%3A%20InSQuAD%3A%20In-Context%20Learning%20for%20Efficient%20Retrieval%20via%20Submodular%0A%20%20Mutual%20Information%20to%20Enforce%20Quality%20and%20Diversity%0AAuthor%3A%20Souradeep%20Nanda%20and%20Anay%20Majee%20and%20Rishabh%20Iyer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20InSQuAD%2C%20designed%20to%20enhance%20the%20performance%20of%0AIn-Context%20Learning%20%28ICL%29%20models%20through%20Submodular%20Mutual%20Information%7D%20%28SMI%29%0Aenforcing%20Quality%20and%20Diversity%20among%20in-context%20exemplars.%20InSQuAD%20achieves%0Athis%20through%20two%20principal%20strategies%3A%20First%2C%20we%20model%20the%20ICL%20task%20as%20a%0Atargeted%20selection%20problem%20and%20introduce%20a%20unified%20selection%20strategy%20based%20on%0ASMIs%20which%20mines%20relevant%20yet%20diverse%20in-context%20examples%20encapsulating%20the%0Anotions%20of%20quality%20and%20diversity.%20Secondly%2C%20we%20address%20a%20common%20pitfall%20in%0Aexisting%20retrieval%20models%20which%20model%20query%20relevance%2C%20often%20overlooking%0Adiversity%2C%20critical%20for%20ICL.%20InSQuAD%20introduces%20a%20combinatorial%20training%0Aparadigm%20which%20learns%20the%20parameters%20of%20an%20SMI%20function%20to%20enforce%20both%20quality%0Aand%20diversity%20in%20the%20retrieval%20model%20through%20a%20novel%20likelihood-based%20loss.%20To%0Afurther%20aid%20the%20learning%20process%20we%20augment%20an%20existing%20multi-hop%20question%0Aanswering%20dataset%20with%20synthetically%20generated%20paraphrases.%20Adopting%20the%0Aretrieval%20model%20trained%20using%20this%20strategy%20alongside%20the%20novel%20targeted%0Aselection%20formulation%20for%20ICL%20on%20nine%20benchmark%20datasets%20shows%20significant%0Aimprovements%20validating%20the%20efficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInSQuAD%253A%2520In-Context%2520Learning%2520for%2520Efficient%2520Retrieval%2520via%2520Submodular%250A%2520%2520Mutual%2520Information%2520to%2520Enforce%2520Quality%2520and%2520Diversity%26entry.906535625%3DSouradeep%2520Nanda%2520and%2520Anay%2520Majee%2520and%2520Rishabh%2520Iyer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520InSQuAD%252C%2520designed%2520to%2520enhance%2520the%2520performance%2520of%250AIn-Context%2520Learning%2520%2528ICL%2529%2520models%2520through%2520Submodular%2520Mutual%2520Information%257D%2520%2528SMI%2529%250Aenforcing%2520Quality%2520and%2520Diversity%2520among%2520in-context%2520exemplars.%2520InSQuAD%2520achieves%250Athis%2520through%2520two%2520principal%2520strategies%253A%2520First%252C%2520we%2520model%2520the%2520ICL%2520task%2520as%2520a%250Atargeted%2520selection%2520problem%2520and%2520introduce%2520a%2520unified%2520selection%2520strategy%2520based%2520on%250ASMIs%2520which%2520mines%2520relevant%2520yet%2520diverse%2520in-context%2520examples%2520encapsulating%2520the%250Anotions%2520of%2520quality%2520and%2520diversity.%2520Secondly%252C%2520we%2520address%2520a%2520common%2520pitfall%2520in%250Aexisting%2520retrieval%2520models%2520which%2520model%2520query%2520relevance%252C%2520often%2520overlooking%250Adiversity%252C%2520critical%2520for%2520ICL.%2520InSQuAD%2520introduces%2520a%2520combinatorial%2520training%250Aparadigm%2520which%2520learns%2520the%2520parameters%2520of%2520an%2520SMI%2520function%2520to%2520enforce%2520both%2520quality%250Aand%2520diversity%2520in%2520the%2520retrieval%2520model%2520through%2520a%2520novel%2520likelihood-based%2520loss.%2520To%250Afurther%2520aid%2520the%2520learning%2520process%2520we%2520augment%2520an%2520existing%2520multi-hop%2520question%250Aanswering%2520dataset%2520with%2520synthetically%2520generated%2520paraphrases.%2520Adopting%2520the%250Aretrieval%2520model%2520trained%2520using%2520this%2520strategy%2520alongside%2520the%2520novel%2520targeted%250Aselection%2520formulation%2520for%2520ICL%2520on%2520nine%2520benchmark%2520datasets%2520shows%2520significant%250Aimprovements%2520validating%2520the%2520efficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InSQuAD%3A%20In-Context%20Learning%20for%20Efficient%20Retrieval%20via%20Submodular%0A%20%20Mutual%20Information%20to%20Enforce%20Quality%20and%20Diversity&entry.906535625=Souradeep%20Nanda%20and%20Anay%20Majee%20and%20Rishabh%20Iyer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20InSQuAD%2C%20designed%20to%20enhance%20the%20performance%20of%0AIn-Context%20Learning%20%28ICL%29%20models%20through%20Submodular%20Mutual%20Information%7D%20%28SMI%29%0Aenforcing%20Quality%20and%20Diversity%20among%20in-context%20exemplars.%20InSQuAD%20achieves%0Athis%20through%20two%20principal%20strategies%3A%20First%2C%20we%20model%20the%20ICL%20task%20as%20a%0Atargeted%20selection%20problem%20and%20introduce%20a%20unified%20selection%20strategy%20based%20on%0ASMIs%20which%20mines%20relevant%20yet%20diverse%20in-context%20examples%20encapsulating%20the%0Anotions%20of%20quality%20and%20diversity.%20Secondly%2C%20we%20address%20a%20common%20pitfall%20in%0Aexisting%20retrieval%20models%20which%20model%20query%20relevance%2C%20often%20overlooking%0Adiversity%2C%20critical%20for%20ICL.%20InSQuAD%20introduces%20a%20combinatorial%20training%0Aparadigm%20which%20learns%20the%20parameters%20of%20an%20SMI%20function%20to%20enforce%20both%20quality%0Aand%20diversity%20in%20the%20retrieval%20model%20through%20a%20novel%20likelihood-based%20loss.%20To%0Afurther%20aid%20the%20learning%20process%20we%20augment%20an%20existing%20multi-hop%20question%0Aanswering%20dataset%20with%20synthetically%20generated%20paraphrases.%20Adopting%20the%0Aretrieval%20model%20trained%20using%20this%20strategy%20alongside%20the%20novel%20targeted%0Aselection%20formulation%20for%20ICL%20on%20nine%20benchmark%20datasets%20shows%20significant%0Aimprovements%20validating%20the%20efficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21003v1&entry.124074799=Read"},
{"title": "Omni-Perception: Omnidirectional Collision Avoidance for Legged\n  Locomotion in Dynamic Environments", "author": "Zifan Wang and Teli Ma and Yufei Jia and Xun Yang and Jiaming Zhou and Wenlong Ouyang and Qiang Zhang and Junwei Liang", "abstract": "  Agile locomotion in complex 3D environments requires robust spatial awareness\nto safely avoid diverse obstacles such as aerial clutter, uneven terrain, and\ndynamic agents. Depth-based perception approaches often struggle with sensor\nnoise, lighting variability, computational overhead from intermediate\nrepresentations (e.g., elevation maps), and difficulties with non-planar\nobstacles, limiting performance in unstructured environments. In contrast,\ndirect integration of LiDAR sensing into end-to-end learning for legged\nlocomotion remains underexplored. We propose Omni-Perception, an end-to-end\nlocomotion policy that achieves 3D spatial awareness and omnidirectional\ncollision avoidance by directly processing raw LiDAR point clouds. At its core\nis PD-RiskNet (Proximal-Distal Risk-Aware Hierarchical Network), a novel\nperception module that interprets spatio-temporal LiDAR data for environmental\nrisk assessment. To facilitate efficient policy learning, we develop a\nhigh-fidelity LiDAR simulation toolkit with realistic noise modeling and fast\nraycasting, compatible with platforms such as Isaac Gym, Genesis, and MuJoCo,\nenabling scalable training and effective sim-to-real transfer. Learning\nreactive control policies directly from raw LiDAR data enables the robot to\nnavigate complex environments with static and dynamic obstacles more robustly\nthan approaches relying on intermediate maps or limited sensing. We validate\nOmni-Perception through real-world experiments and extensive simulation,\ndemonstrating strong omnidirectional avoidance capabilities and superior\nlocomotion performance in highly dynamic environments.\n", "link": "http://arxiv.org/abs/2505.19214v2", "date": "2025-08-28", "relevancy": 2.4831, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6445}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Perception%3A%20Omnidirectional%20Collision%20Avoidance%20for%20Legged%0A%20%20Locomotion%20in%20Dynamic%20Environments&body=Title%3A%20Omni-Perception%3A%20Omnidirectional%20Collision%20Avoidance%20for%20Legged%0A%20%20Locomotion%20in%20Dynamic%20Environments%0AAuthor%3A%20Zifan%20Wang%20and%20Teli%20Ma%20and%20Yufei%20Jia%20and%20Xun%20Yang%20and%20Jiaming%20Zhou%20and%20Wenlong%20Ouyang%20and%20Qiang%20Zhang%20and%20Junwei%20Liang%0AAbstract%3A%20%20%20Agile%20locomotion%20in%20complex%203D%20environments%20requires%20robust%20spatial%20awareness%0Ato%20safely%20avoid%20diverse%20obstacles%20such%20as%20aerial%20clutter%2C%20uneven%20terrain%2C%20and%0Adynamic%20agents.%20Depth-based%20perception%20approaches%20often%20struggle%20with%20sensor%0Anoise%2C%20lighting%20variability%2C%20computational%20overhead%20from%20intermediate%0Arepresentations%20%28e.g.%2C%20elevation%20maps%29%2C%20and%20difficulties%20with%20non-planar%0Aobstacles%2C%20limiting%20performance%20in%20unstructured%20environments.%20In%20contrast%2C%0Adirect%20integration%20of%20LiDAR%20sensing%20into%20end-to-end%20learning%20for%20legged%0Alocomotion%20remains%20underexplored.%20We%20propose%20Omni-Perception%2C%20an%20end-to-end%0Alocomotion%20policy%20that%20achieves%203D%20spatial%20awareness%20and%20omnidirectional%0Acollision%20avoidance%20by%20directly%20processing%20raw%20LiDAR%20point%20clouds.%20At%20its%20core%0Ais%20PD-RiskNet%20%28Proximal-Distal%20Risk-Aware%20Hierarchical%20Network%29%2C%20a%20novel%0Aperception%20module%20that%20interprets%20spatio-temporal%20LiDAR%20data%20for%20environmental%0Arisk%20assessment.%20To%20facilitate%20efficient%20policy%20learning%2C%20we%20develop%20a%0Ahigh-fidelity%20LiDAR%20simulation%20toolkit%20with%20realistic%20noise%20modeling%20and%20fast%0Araycasting%2C%20compatible%20with%20platforms%20such%20as%20Isaac%20Gym%2C%20Genesis%2C%20and%20MuJoCo%2C%0Aenabling%20scalable%20training%20and%20effective%20sim-to-real%20transfer.%20Learning%0Areactive%20control%20policies%20directly%20from%20raw%20LiDAR%20data%20enables%20the%20robot%20to%0Anavigate%20complex%20environments%20with%20static%20and%20dynamic%20obstacles%20more%20robustly%0Athan%20approaches%20relying%20on%20intermediate%20maps%20or%20limited%20sensing.%20We%20validate%0AOmni-Perception%20through%20real-world%20experiments%20and%20extensive%20simulation%2C%0Ademonstrating%20strong%20omnidirectional%20avoidance%20capabilities%20and%20superior%0Alocomotion%20performance%20in%20highly%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Perception%253A%2520Omnidirectional%2520Collision%2520Avoidance%2520for%2520Legged%250A%2520%2520Locomotion%2520in%2520Dynamic%2520Environments%26entry.906535625%3DZifan%2520Wang%2520and%2520Teli%2520Ma%2520and%2520Yufei%2520Jia%2520and%2520Xun%2520Yang%2520and%2520Jiaming%2520Zhou%2520and%2520Wenlong%2520Ouyang%2520and%2520Qiang%2520Zhang%2520and%2520Junwei%2520Liang%26entry.1292438233%3D%2520%2520Agile%2520locomotion%2520in%2520complex%25203D%2520environments%2520requires%2520robust%2520spatial%2520awareness%250Ato%2520safely%2520avoid%2520diverse%2520obstacles%2520such%2520as%2520aerial%2520clutter%252C%2520uneven%2520terrain%252C%2520and%250Adynamic%2520agents.%2520Depth-based%2520perception%2520approaches%2520often%2520struggle%2520with%2520sensor%250Anoise%252C%2520lighting%2520variability%252C%2520computational%2520overhead%2520from%2520intermediate%250Arepresentations%2520%2528e.g.%252C%2520elevation%2520maps%2529%252C%2520and%2520difficulties%2520with%2520non-planar%250Aobstacles%252C%2520limiting%2520performance%2520in%2520unstructured%2520environments.%2520In%2520contrast%252C%250Adirect%2520integration%2520of%2520LiDAR%2520sensing%2520into%2520end-to-end%2520learning%2520for%2520legged%250Alocomotion%2520remains%2520underexplored.%2520We%2520propose%2520Omni-Perception%252C%2520an%2520end-to-end%250Alocomotion%2520policy%2520that%2520achieves%25203D%2520spatial%2520awareness%2520and%2520omnidirectional%250Acollision%2520avoidance%2520by%2520directly%2520processing%2520raw%2520LiDAR%2520point%2520clouds.%2520At%2520its%2520core%250Ais%2520PD-RiskNet%2520%2528Proximal-Distal%2520Risk-Aware%2520Hierarchical%2520Network%2529%252C%2520a%2520novel%250Aperception%2520module%2520that%2520interprets%2520spatio-temporal%2520LiDAR%2520data%2520for%2520environmental%250Arisk%2520assessment.%2520To%2520facilitate%2520efficient%2520policy%2520learning%252C%2520we%2520develop%2520a%250Ahigh-fidelity%2520LiDAR%2520simulation%2520toolkit%2520with%2520realistic%2520noise%2520modeling%2520and%2520fast%250Araycasting%252C%2520compatible%2520with%2520platforms%2520such%2520as%2520Isaac%2520Gym%252C%2520Genesis%252C%2520and%2520MuJoCo%252C%250Aenabling%2520scalable%2520training%2520and%2520effective%2520sim-to-real%2520transfer.%2520Learning%250Areactive%2520control%2520policies%2520directly%2520from%2520raw%2520LiDAR%2520data%2520enables%2520the%2520robot%2520to%250Anavigate%2520complex%2520environments%2520with%2520static%2520and%2520dynamic%2520obstacles%2520more%2520robustly%250Athan%2520approaches%2520relying%2520on%2520intermediate%2520maps%2520or%2520limited%2520sensing.%2520We%2520validate%250AOmni-Perception%2520through%2520real-world%2520experiments%2520and%2520extensive%2520simulation%252C%250Ademonstrating%2520strong%2520omnidirectional%2520avoidance%2520capabilities%2520and%2520superior%250Alocomotion%2520performance%2520in%2520highly%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Perception%3A%20Omnidirectional%20Collision%20Avoidance%20for%20Legged%0A%20%20Locomotion%20in%20Dynamic%20Environments&entry.906535625=Zifan%20Wang%20and%20Teli%20Ma%20and%20Yufei%20Jia%20and%20Xun%20Yang%20and%20Jiaming%20Zhou%20and%20Wenlong%20Ouyang%20and%20Qiang%20Zhang%20and%20Junwei%20Liang&entry.1292438233=%20%20Agile%20locomotion%20in%20complex%203D%20environments%20requires%20robust%20spatial%20awareness%0Ato%20safely%20avoid%20diverse%20obstacles%20such%20as%20aerial%20clutter%2C%20uneven%20terrain%2C%20and%0Adynamic%20agents.%20Depth-based%20perception%20approaches%20often%20struggle%20with%20sensor%0Anoise%2C%20lighting%20variability%2C%20computational%20overhead%20from%20intermediate%0Arepresentations%20%28e.g.%2C%20elevation%20maps%29%2C%20and%20difficulties%20with%20non-planar%0Aobstacles%2C%20limiting%20performance%20in%20unstructured%20environments.%20In%20contrast%2C%0Adirect%20integration%20of%20LiDAR%20sensing%20into%20end-to-end%20learning%20for%20legged%0Alocomotion%20remains%20underexplored.%20We%20propose%20Omni-Perception%2C%20an%20end-to-end%0Alocomotion%20policy%20that%20achieves%203D%20spatial%20awareness%20and%20omnidirectional%0Acollision%20avoidance%20by%20directly%20processing%20raw%20LiDAR%20point%20clouds.%20At%20its%20core%0Ais%20PD-RiskNet%20%28Proximal-Distal%20Risk-Aware%20Hierarchical%20Network%29%2C%20a%20novel%0Aperception%20module%20that%20interprets%20spatio-temporal%20LiDAR%20data%20for%20environmental%0Arisk%20assessment.%20To%20facilitate%20efficient%20policy%20learning%2C%20we%20develop%20a%0Ahigh-fidelity%20LiDAR%20simulation%20toolkit%20with%20realistic%20noise%20modeling%20and%20fast%0Araycasting%2C%20compatible%20with%20platforms%20such%20as%20Isaac%20Gym%2C%20Genesis%2C%20and%20MuJoCo%2C%0Aenabling%20scalable%20training%20and%20effective%20sim-to-real%20transfer.%20Learning%0Areactive%20control%20policies%20directly%20from%20raw%20LiDAR%20data%20enables%20the%20robot%20to%0Anavigate%20complex%20environments%20with%20static%20and%20dynamic%20obstacles%20more%20robustly%0Athan%20approaches%20relying%20on%20intermediate%20maps%20or%20limited%20sensing.%20We%20validate%0AOmni-Perception%20through%20real-world%20experiments%20and%20extensive%20simulation%2C%0Ademonstrating%20strong%20omnidirectional%20avoidance%20capabilities%20and%20superior%0Alocomotion%20performance%20in%20highly%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19214v2&entry.124074799=Read"},
{"title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for\n  Video Understanding", "author": "Gowreesh Mago and Pascal Mettes and Stevan Rudinac", "abstract": "  The automatic understanding of video content is advancing rapidly. Empowered\nby deeper neural networks and large datasets, machines are increasingly capable\nof understanding what is concretely visible in video frames, whether it be\nobjects, actions, events, or scenes. In comparison, humans retain a unique\nability to also look beyond concrete entities and recognize abstract concepts\nlike justice, freedom, and togetherness. Abstract concept recognition forms a\ncrucial open challenge in video understanding, where reasoning on multiple\nsemantic levels based on contextual information is key. In this paper, we argue\nthat the recent advances in foundation models make for an ideal setting to\naddress abstract concept understanding in videos. Automated understanding of\nhigh-level abstract concepts is imperative as it enables models to be more\naligned with human reasoning and values. In this survey, we study different\ntasks and datasets used to understand abstract concepts in video content. We\nobserve that, periodically and over a long period, researchers have attempted\nto solve these tasks, making the best use of the tools available at their\ndisposal. We advocate that drawing on decades of community experience will help\nus shed light on this important open grand challenge and avoid ``re-inventing\nthe wheel'' as we start revisiting it in the era of multi-modal foundation\nmodels.\n", "link": "http://arxiv.org/abs/2508.20765v1", "date": "2025-08-28", "relevancy": 2.4765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6347}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20Beyond%20the%20Obvious%3A%20A%20Survey%20on%20Abstract%20Concept%20Recognition%20for%0A%20%20Video%20Understanding&body=Title%3A%20Looking%20Beyond%20the%20Obvious%3A%20A%20Survey%20on%20Abstract%20Concept%20Recognition%20for%0A%20%20Video%20Understanding%0AAuthor%3A%20Gowreesh%20Mago%20and%20Pascal%20Mettes%20and%20Stevan%20Rudinac%0AAbstract%3A%20%20%20The%20automatic%20understanding%20of%20video%20content%20is%20advancing%20rapidly.%20Empowered%0Aby%20deeper%20neural%20networks%20and%20large%20datasets%2C%20machines%20are%20increasingly%20capable%0Aof%20understanding%20what%20is%20concretely%20visible%20in%20video%20frames%2C%20whether%20it%20be%0Aobjects%2C%20actions%2C%20events%2C%20or%20scenes.%20In%20comparison%2C%20humans%20retain%20a%20unique%0Aability%20to%20also%20look%20beyond%20concrete%20entities%20and%20recognize%20abstract%20concepts%0Alike%20justice%2C%20freedom%2C%20and%20togetherness.%20Abstract%20concept%20recognition%20forms%20a%0Acrucial%20open%20challenge%20in%20video%20understanding%2C%20where%20reasoning%20on%20multiple%0Asemantic%20levels%20based%20on%20contextual%20information%20is%20key.%20In%20this%20paper%2C%20we%20argue%0Athat%20the%20recent%20advances%20in%20foundation%20models%20make%20for%20an%20ideal%20setting%20to%0Aaddress%20abstract%20concept%20understanding%20in%20videos.%20Automated%20understanding%20of%0Ahigh-level%20abstract%20concepts%20is%20imperative%20as%20it%20enables%20models%20to%20be%20more%0Aaligned%20with%20human%20reasoning%20and%20values.%20In%20this%20survey%2C%20we%20study%20different%0Atasks%20and%20datasets%20used%20to%20understand%20abstract%20concepts%20in%20video%20content.%20We%0Aobserve%20that%2C%20periodically%20and%20over%20a%20long%20period%2C%20researchers%20have%20attempted%0Ato%20solve%20these%20tasks%2C%20making%20the%20best%20use%20of%20the%20tools%20available%20at%20their%0Adisposal.%20We%20advocate%20that%20drawing%20on%20decades%20of%20community%20experience%20will%20help%0Aus%20shed%20light%20on%20this%20important%20open%20grand%20challenge%20and%20avoid%20%60%60re-inventing%0Athe%20wheel%27%27%20as%20we%20start%20revisiting%20it%20in%20the%20era%20of%20multi-modal%20foundation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520Beyond%2520the%2520Obvious%253A%2520A%2520Survey%2520on%2520Abstract%2520Concept%2520Recognition%2520for%250A%2520%2520Video%2520Understanding%26entry.906535625%3DGowreesh%2520Mago%2520and%2520Pascal%2520Mettes%2520and%2520Stevan%2520Rudinac%26entry.1292438233%3D%2520%2520The%2520automatic%2520understanding%2520of%2520video%2520content%2520is%2520advancing%2520rapidly.%2520Empowered%250Aby%2520deeper%2520neural%2520networks%2520and%2520large%2520datasets%252C%2520machines%2520are%2520increasingly%2520capable%250Aof%2520understanding%2520what%2520is%2520concretely%2520visible%2520in%2520video%2520frames%252C%2520whether%2520it%2520be%250Aobjects%252C%2520actions%252C%2520events%252C%2520or%2520scenes.%2520In%2520comparison%252C%2520humans%2520retain%2520a%2520unique%250Aability%2520to%2520also%2520look%2520beyond%2520concrete%2520entities%2520and%2520recognize%2520abstract%2520concepts%250Alike%2520justice%252C%2520freedom%252C%2520and%2520togetherness.%2520Abstract%2520concept%2520recognition%2520forms%2520a%250Acrucial%2520open%2520challenge%2520in%2520video%2520understanding%252C%2520where%2520reasoning%2520on%2520multiple%250Asemantic%2520levels%2520based%2520on%2520contextual%2520information%2520is%2520key.%2520In%2520this%2520paper%252C%2520we%2520argue%250Athat%2520the%2520recent%2520advances%2520in%2520foundation%2520models%2520make%2520for%2520an%2520ideal%2520setting%2520to%250Aaddress%2520abstract%2520concept%2520understanding%2520in%2520videos.%2520Automated%2520understanding%2520of%250Ahigh-level%2520abstract%2520concepts%2520is%2520imperative%2520as%2520it%2520enables%2520models%2520to%2520be%2520more%250Aaligned%2520with%2520human%2520reasoning%2520and%2520values.%2520In%2520this%2520survey%252C%2520we%2520study%2520different%250Atasks%2520and%2520datasets%2520used%2520to%2520understand%2520abstract%2520concepts%2520in%2520video%2520content.%2520We%250Aobserve%2520that%252C%2520periodically%2520and%2520over%2520a%2520long%2520period%252C%2520researchers%2520have%2520attempted%250Ato%2520solve%2520these%2520tasks%252C%2520making%2520the%2520best%2520use%2520of%2520the%2520tools%2520available%2520at%2520their%250Adisposal.%2520We%2520advocate%2520that%2520drawing%2520on%2520decades%2520of%2520community%2520experience%2520will%2520help%250Aus%2520shed%2520light%2520on%2520this%2520important%2520open%2520grand%2520challenge%2520and%2520avoid%2520%2560%2560re-inventing%250Athe%2520wheel%2527%2527%2520as%2520we%2520start%2520revisiting%2520it%2520in%2520the%2520era%2520of%2520multi-modal%2520foundation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20Beyond%20the%20Obvious%3A%20A%20Survey%20on%20Abstract%20Concept%20Recognition%20for%0A%20%20Video%20Understanding&entry.906535625=Gowreesh%20Mago%20and%20Pascal%20Mettes%20and%20Stevan%20Rudinac&entry.1292438233=%20%20The%20automatic%20understanding%20of%20video%20content%20is%20advancing%20rapidly.%20Empowered%0Aby%20deeper%20neural%20networks%20and%20large%20datasets%2C%20machines%20are%20increasingly%20capable%0Aof%20understanding%20what%20is%20concretely%20visible%20in%20video%20frames%2C%20whether%20it%20be%0Aobjects%2C%20actions%2C%20events%2C%20or%20scenes.%20In%20comparison%2C%20humans%20retain%20a%20unique%0Aability%20to%20also%20look%20beyond%20concrete%20entities%20and%20recognize%20abstract%20concepts%0Alike%20justice%2C%20freedom%2C%20and%20togetherness.%20Abstract%20concept%20recognition%20forms%20a%0Acrucial%20open%20challenge%20in%20video%20understanding%2C%20where%20reasoning%20on%20multiple%0Asemantic%20levels%20based%20on%20contextual%20information%20is%20key.%20In%20this%20paper%2C%20we%20argue%0Athat%20the%20recent%20advances%20in%20foundation%20models%20make%20for%20an%20ideal%20setting%20to%0Aaddress%20abstract%20concept%20understanding%20in%20videos.%20Automated%20understanding%20of%0Ahigh-level%20abstract%20concepts%20is%20imperative%20as%20it%20enables%20models%20to%20be%20more%0Aaligned%20with%20human%20reasoning%20and%20values.%20In%20this%20survey%2C%20we%20study%20different%0Atasks%20and%20datasets%20used%20to%20understand%20abstract%20concepts%20in%20video%20content.%20We%0Aobserve%20that%2C%20periodically%20and%20over%20a%20long%20period%2C%20researchers%20have%20attempted%0Ato%20solve%20these%20tasks%2C%20making%20the%20best%20use%20of%20the%20tools%20available%20at%20their%0Adisposal.%20We%20advocate%20that%20drawing%20on%20decades%20of%20community%20experience%20will%20help%0Aus%20shed%20light%20on%20this%20important%20open%20grand%20challenge%20and%20avoid%20%60%60re-inventing%0Athe%20wheel%27%27%20as%20we%20start%20revisiting%20it%20in%20the%20era%20of%20multi-modal%20foundation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20765v1&entry.124074799=Read"},
{"title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition\n  Models", "author": "Huong Ngo and Matt Deitke and Martijn Bartelds and Sarah Pratt and Josh Gardner and Matt Jordan and Ludwig Schmidt", "abstract": "  Improvements in training data scale and quality have led to significant\nadvances, yet its influence in speech recognition remains underexplored. In\nthis paper, we present a large-scale dataset, OLMoASR-Pool, and series of\nmodels, OLMoASR, to study and develop robust zero-shot speech recognition\nmodels. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio\nand 17M transcripts, we design text heuristic filters to remove low-quality or\nmistranscribed data. Our curation pipeline produces a new dataset containing 1M\nhours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use\nOLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M\n(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR\nachieves comparable average performance to OpenAI's Whisper on short and\nlong-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a\n12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest\nEnglish-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and\nlong-form recognition respectively (at equivalent parameter count).\nOLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will\nbe made publicly available to further research on robust speech processing.\n", "link": "http://arxiv.org/abs/2508.20869v1", "date": "2025-08-28", "relevancy": 2.45, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLMoASR%3A%20Open%20Models%20and%20Data%20for%20Training%20Robust%20Speech%20Recognition%0A%20%20Models&body=Title%3A%20OLMoASR%3A%20Open%20Models%20and%20Data%20for%20Training%20Robust%20Speech%20Recognition%0A%20%20Models%0AAuthor%3A%20Huong%20Ngo%20and%20Matt%20Deitke%20and%20Martijn%20Bartelds%20and%20Sarah%20Pratt%20and%20Josh%20Gardner%20and%20Matt%20Jordan%20and%20Ludwig%20Schmidt%0AAbstract%3A%20%20%20Improvements%20in%20training%20data%20scale%20and%20quality%20have%20led%20to%20significant%0Aadvances%2C%20yet%20its%20influence%20in%20speech%20recognition%20remains%20underexplored.%20In%0Athis%20paper%2C%20we%20present%20a%20large-scale%20dataset%2C%20OLMoASR-Pool%2C%20and%20series%20of%0Amodels%2C%20OLMoASR%2C%20to%20study%20and%20develop%20robust%20zero-shot%20speech%20recognition%0Amodels.%20Beginning%20from%20OLMoASR-Pool%2C%20a%20collection%20of%203M%20hours%20of%20English%20audio%0Aand%2017M%20transcripts%2C%20we%20design%20text%20heuristic%20filters%20to%20remove%20low-quality%20or%0Amistranscribed%20data.%20Our%20curation%20pipeline%20produces%20a%20new%20dataset%20containing%201M%0Ahours%20of%20high-quality%20audio-transcript%20pairs%2C%20which%20we%20call%20OLMoASR-Mix.%20We%20use%0AOLMoASR-Mix%20to%20train%20the%20OLMoASR-Mix%20suite%20of%20models%2C%20ranging%20from%2039M%0A%28tiny.en%29%20to%201.5B%20%28large.en%29%20parameters.%20Across%20all%20model%20scales%2C%20OLMoASR%0Aachieves%20comparable%20average%20performance%20to%20OpenAI%27s%20Whisper%20on%20short%20and%0Along-form%20speech%20recognition%20benchmarks.%20Notably%2C%20OLMoASR-medium.en%20attains%20a%0A12.8%5C%25%20and%2011.0%5C%25%20word%20error%20rate%20%28WER%29%20that%20is%20on%20par%20with%20Whisper%27s%20largest%0AEnglish-only%20model%20Whisper-medium.en%27s%2012.4%5C%25%20and%2010.5%5C%25%20WER%20for%20short%20and%0Along-form%20recognition%20respectively%20%28at%20equivalent%20parameter%20count%29.%0AOLMoASR-Pool%2C%20OLMoASR%20models%2C%20and%20filtering%2C%20training%20and%20evaluation%20code%20will%0Abe%20made%20publicly%20available%20to%20further%20research%20on%20robust%20speech%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLMoASR%253A%2520Open%2520Models%2520and%2520Data%2520for%2520Training%2520Robust%2520Speech%2520Recognition%250A%2520%2520Models%26entry.906535625%3DHuong%2520Ngo%2520and%2520Matt%2520Deitke%2520and%2520Martijn%2520Bartelds%2520and%2520Sarah%2520Pratt%2520and%2520Josh%2520Gardner%2520and%2520Matt%2520Jordan%2520and%2520Ludwig%2520Schmidt%26entry.1292438233%3D%2520%2520Improvements%2520in%2520training%2520data%2520scale%2520and%2520quality%2520have%2520led%2520to%2520significant%250Aadvances%252C%2520yet%2520its%2520influence%2520in%2520speech%2520recognition%2520remains%2520underexplored.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520large-scale%2520dataset%252C%2520OLMoASR-Pool%252C%2520and%2520series%2520of%250Amodels%252C%2520OLMoASR%252C%2520to%2520study%2520and%2520develop%2520robust%2520zero-shot%2520speech%2520recognition%250Amodels.%2520Beginning%2520from%2520OLMoASR-Pool%252C%2520a%2520collection%2520of%25203M%2520hours%2520of%2520English%2520audio%250Aand%252017M%2520transcripts%252C%2520we%2520design%2520text%2520heuristic%2520filters%2520to%2520remove%2520low-quality%2520or%250Amistranscribed%2520data.%2520Our%2520curation%2520pipeline%2520produces%2520a%2520new%2520dataset%2520containing%25201M%250Ahours%2520of%2520high-quality%2520audio-transcript%2520pairs%252C%2520which%2520we%2520call%2520OLMoASR-Mix.%2520We%2520use%250AOLMoASR-Mix%2520to%2520train%2520the%2520OLMoASR-Mix%2520suite%2520of%2520models%252C%2520ranging%2520from%252039M%250A%2528tiny.en%2529%2520to%25201.5B%2520%2528large.en%2529%2520parameters.%2520Across%2520all%2520model%2520scales%252C%2520OLMoASR%250Aachieves%2520comparable%2520average%2520performance%2520to%2520OpenAI%2527s%2520Whisper%2520on%2520short%2520and%250Along-form%2520speech%2520recognition%2520benchmarks.%2520Notably%252C%2520OLMoASR-medium.en%2520attains%2520a%250A12.8%255C%2525%2520and%252011.0%255C%2525%2520word%2520error%2520rate%2520%2528WER%2529%2520that%2520is%2520on%2520par%2520with%2520Whisper%2527s%2520largest%250AEnglish-only%2520model%2520Whisper-medium.en%2527s%252012.4%255C%2525%2520and%252010.5%255C%2525%2520WER%2520for%2520short%2520and%250Along-form%2520recognition%2520respectively%2520%2528at%2520equivalent%2520parameter%2520count%2529.%250AOLMoASR-Pool%252C%2520OLMoASR%2520models%252C%2520and%2520filtering%252C%2520training%2520and%2520evaluation%2520code%2520will%250Abe%2520made%2520publicly%2520available%2520to%2520further%2520research%2520on%2520robust%2520speech%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLMoASR%3A%20Open%20Models%20and%20Data%20for%20Training%20Robust%20Speech%20Recognition%0A%20%20Models&entry.906535625=Huong%20Ngo%20and%20Matt%20Deitke%20and%20Martijn%20Bartelds%20and%20Sarah%20Pratt%20and%20Josh%20Gardner%20and%20Matt%20Jordan%20and%20Ludwig%20Schmidt&entry.1292438233=%20%20Improvements%20in%20training%20data%20scale%20and%20quality%20have%20led%20to%20significant%0Aadvances%2C%20yet%20its%20influence%20in%20speech%20recognition%20remains%20underexplored.%20In%0Athis%20paper%2C%20we%20present%20a%20large-scale%20dataset%2C%20OLMoASR-Pool%2C%20and%20series%20of%0Amodels%2C%20OLMoASR%2C%20to%20study%20and%20develop%20robust%20zero-shot%20speech%20recognition%0Amodels.%20Beginning%20from%20OLMoASR-Pool%2C%20a%20collection%20of%203M%20hours%20of%20English%20audio%0Aand%2017M%20transcripts%2C%20we%20design%20text%20heuristic%20filters%20to%20remove%20low-quality%20or%0Amistranscribed%20data.%20Our%20curation%20pipeline%20produces%20a%20new%20dataset%20containing%201M%0Ahours%20of%20high-quality%20audio-transcript%20pairs%2C%20which%20we%20call%20OLMoASR-Mix.%20We%20use%0AOLMoASR-Mix%20to%20train%20the%20OLMoASR-Mix%20suite%20of%20models%2C%20ranging%20from%2039M%0A%28tiny.en%29%20to%201.5B%20%28large.en%29%20parameters.%20Across%20all%20model%20scales%2C%20OLMoASR%0Aachieves%20comparable%20average%20performance%20to%20OpenAI%27s%20Whisper%20on%20short%20and%0Along-form%20speech%20recognition%20benchmarks.%20Notably%2C%20OLMoASR-medium.en%20attains%20a%0A12.8%5C%25%20and%2011.0%5C%25%20word%20error%20rate%20%28WER%29%20that%20is%20on%20par%20with%20Whisper%27s%20largest%0AEnglish-only%20model%20Whisper-medium.en%27s%2012.4%5C%25%20and%2010.5%5C%25%20WER%20for%20short%20and%0Along-form%20recognition%20respectively%20%28at%20equivalent%20parameter%20count%29.%0AOLMoASR-Pool%2C%20OLMoASR%20models%2C%20and%20filtering%2C%20training%20and%20evaluation%20code%20will%0Abe%20made%20publicly%20available%20to%20further%20research%20on%20robust%20speech%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20869v1&entry.124074799=Read"},
{"title": "Provable Benefits of In-Tool Learning for Large Language Models", "author": "Sam Houliston and Ambroise Odonnat and Charles Arnal and Vivien Cabannes", "abstract": "  Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.\n", "link": "http://arxiv.org/abs/2508.20755v1", "date": "2025-08-28", "relevancy": 2.4112, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Benefits%20of%20In-Tool%20Learning%20for%20Large%20Language%20Models&body=Title%3A%20Provable%20Benefits%20of%20In-Tool%20Learning%20for%20Large%20Language%20Models%0AAuthor%3A%20Sam%20Houliston%20and%20Ambroise%20Odonnat%20and%20Charles%20Arnal%20and%20Vivien%20Cabannes%0AAbstract%3A%20%20%20Tool-augmented%20language%20models%2C%20equipped%20with%20retrieval%2C%20memory%2C%20or%20external%0AAPIs%2C%20are%20reshaping%20AI%2C%20yet%20their%20theoretical%20advantages%20remain%20underexplored.%0AIn%20this%20paper%2C%20we%20address%20this%20question%20by%20demonstrating%20the%20benefits%20of%0Ain-tool%20learning%20%28external%20retrieval%29%20over%20in-weight%20learning%20%28memorization%29%0Afor%20factual%20recall.%20We%20show%20that%20the%20number%20of%20facts%20a%20model%20can%20memorize%0Asolely%20in%20its%20weights%20is%20fundamentally%20limited%20by%20its%20parameter%20count.%20In%0Acontrast%2C%20we%20prove%20that%20tool-use%20enables%20unbounded%20factual%20recall%20via%20a%20simple%0Aand%20efficient%20circuit%20construction.%20These%20results%20are%20validated%20in%20controlled%0Aexperiments%2C%20where%20tool-using%20models%20consistently%20outperform%20memorizing%20ones.%0AWe%20further%20show%20that%20for%20pretrained%20large%20language%20models%2C%20teaching%20tool-use%0Aand%20general%20rules%20is%20more%20effective%20than%20finetuning%20facts%20into%20memory.%20Our%20work%0Aprovides%20both%20a%20theoretical%20and%20empirical%20foundation%2C%20establishing%20why%0Atool-augmented%20workflows%20are%20not%20just%20practical%2C%20but%20provably%20more%20scalable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Benefits%2520of%2520In-Tool%2520Learning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSam%2520Houliston%2520and%2520Ambroise%2520Odonnat%2520and%2520Charles%2520Arnal%2520and%2520Vivien%2520Cabannes%26entry.1292438233%3D%2520%2520Tool-augmented%2520language%2520models%252C%2520equipped%2520with%2520retrieval%252C%2520memory%252C%2520or%2520external%250AAPIs%252C%2520are%2520reshaping%2520AI%252C%2520yet%2520their%2520theoretical%2520advantages%2520remain%2520underexplored.%250AIn%2520this%2520paper%252C%2520we%2520address%2520this%2520question%2520by%2520demonstrating%2520the%2520benefits%2520of%250Ain-tool%2520learning%2520%2528external%2520retrieval%2529%2520over%2520in-weight%2520learning%2520%2528memorization%2529%250Afor%2520factual%2520recall.%2520We%2520show%2520that%2520the%2520number%2520of%2520facts%2520a%2520model%2520can%2520memorize%250Asolely%2520in%2520its%2520weights%2520is%2520fundamentally%2520limited%2520by%2520its%2520parameter%2520count.%2520In%250Acontrast%252C%2520we%2520prove%2520that%2520tool-use%2520enables%2520unbounded%2520factual%2520recall%2520via%2520a%2520simple%250Aand%2520efficient%2520circuit%2520construction.%2520These%2520results%2520are%2520validated%2520in%2520controlled%250Aexperiments%252C%2520where%2520tool-using%2520models%2520consistently%2520outperform%2520memorizing%2520ones.%250AWe%2520further%2520show%2520that%2520for%2520pretrained%2520large%2520language%2520models%252C%2520teaching%2520tool-use%250Aand%2520general%2520rules%2520is%2520more%2520effective%2520than%2520finetuning%2520facts%2520into%2520memory.%2520Our%2520work%250Aprovides%2520both%2520a%2520theoretical%2520and%2520empirical%2520foundation%252C%2520establishing%2520why%250Atool-augmented%2520workflows%2520are%2520not%2520just%2520practical%252C%2520but%2520provably%2520more%2520scalable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Benefits%20of%20In-Tool%20Learning%20for%20Large%20Language%20Models&entry.906535625=Sam%20Houliston%20and%20Ambroise%20Odonnat%20and%20Charles%20Arnal%20and%20Vivien%20Cabannes&entry.1292438233=%20%20Tool-augmented%20language%20models%2C%20equipped%20with%20retrieval%2C%20memory%2C%20or%20external%0AAPIs%2C%20are%20reshaping%20AI%2C%20yet%20their%20theoretical%20advantages%20remain%20underexplored.%0AIn%20this%20paper%2C%20we%20address%20this%20question%20by%20demonstrating%20the%20benefits%20of%0Ain-tool%20learning%20%28external%20retrieval%29%20over%20in-weight%20learning%20%28memorization%29%0Afor%20factual%20recall.%20We%20show%20that%20the%20number%20of%20facts%20a%20model%20can%20memorize%0Asolely%20in%20its%20weights%20is%20fundamentally%20limited%20by%20its%20parameter%20count.%20In%0Acontrast%2C%20we%20prove%20that%20tool-use%20enables%20unbounded%20factual%20recall%20via%20a%20simple%0Aand%20efficient%20circuit%20construction.%20These%20results%20are%20validated%20in%20controlled%0Aexperiments%2C%20where%20tool-using%20models%20consistently%20outperform%20memorizing%20ones.%0AWe%20further%20show%20that%20for%20pretrained%20large%20language%20models%2C%20teaching%20tool-use%0Aand%20general%20rules%20is%20more%20effective%20than%20finetuning%20facts%20into%20memory.%20Our%20work%0Aprovides%20both%20a%20theoretical%20and%20empirical%20foundation%2C%20establishing%20why%0Atool-augmented%20workflows%20are%20not%20just%20practical%2C%20but%20provably%20more%20scalable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20755v1&entry.124074799=Read"},
{"title": "To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle\n  Software", "author": "Lo\u00efc Stratil and Felix Fent and Esteban Rivera and Markus Lienkamp", "abstract": "  Autonomous vehicle perception typically relies on modular pipelines that\ndecompose the task into detection, tracking, and prediction. While\ninterpretable, these pipelines suffer from error accumulation and limited\ninter-task synergy. Unified perception has emerged as a promising paradigm that\nintegrates these sub-tasks within a shared architecture, potentially improving\nrobustness, contextual reasoning, and efficiency while retaining interpretable\noutputs. In this survey, we provide a comprehensive overview of unified\nperception, introducing a holistic and systemic taxonomy that categorizes\nmethods along task integration, tracking formulation, and representation flow.\nWe define three paradigms -Early, Late, and Full Unified Perception- and\nsystematically review existing methods, their architectures, training\nstrategies, datasets used, and open-source availability, while highlighting\nfuture research directions. This work establishes the first comprehensive\nframework for understanding and advancing unified perception, consolidates\nfragmented efforts, and guides future research toward more robust,\ngeneralizable, and interpretable perception.\n", "link": "http://arxiv.org/abs/2508.20892v1", "date": "2025-08-28", "relevancy": 2.4077, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20New%20Beginnings%3A%20A%20Survey%20of%20Unified%20Perception%20in%20Autonomous%20Vehicle%0A%20%20Software&body=Title%3A%20To%20New%20Beginnings%3A%20A%20Survey%20of%20Unified%20Perception%20in%20Autonomous%20Vehicle%0A%20%20Software%0AAuthor%3A%20Lo%C3%AFc%20Stratil%20and%20Felix%20Fent%20and%20Esteban%20Rivera%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20Autonomous%20vehicle%20perception%20typically%20relies%20on%20modular%20pipelines%20that%0Adecompose%20the%20task%20into%20detection%2C%20tracking%2C%20and%20prediction.%20While%0Ainterpretable%2C%20these%20pipelines%20suffer%20from%20error%20accumulation%20and%20limited%0Ainter-task%20synergy.%20Unified%20perception%20has%20emerged%20as%20a%20promising%20paradigm%20that%0Aintegrates%20these%20sub-tasks%20within%20a%20shared%20architecture%2C%20potentially%20improving%0Arobustness%2C%20contextual%20reasoning%2C%20and%20efficiency%20while%20retaining%20interpretable%0Aoutputs.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20unified%0Aperception%2C%20introducing%20a%20holistic%20and%20systemic%20taxonomy%20that%20categorizes%0Amethods%20along%20task%20integration%2C%20tracking%20formulation%2C%20and%20representation%20flow.%0AWe%20define%20three%20paradigms%20-Early%2C%20Late%2C%20and%20Full%20Unified%20Perception-%20and%0Asystematically%20review%20existing%20methods%2C%20their%20architectures%2C%20training%0Astrategies%2C%20datasets%20used%2C%20and%20open-source%20availability%2C%20while%20highlighting%0Afuture%20research%20directions.%20This%20work%20establishes%20the%20first%20comprehensive%0Aframework%20for%20understanding%20and%20advancing%20unified%20perception%2C%20consolidates%0Afragmented%20efforts%2C%20and%20guides%20future%20research%20toward%20more%20robust%2C%0Ageneralizable%2C%20and%20interpretable%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520New%2520Beginnings%253A%2520A%2520Survey%2520of%2520Unified%2520Perception%2520in%2520Autonomous%2520Vehicle%250A%2520%2520Software%26entry.906535625%3DLo%25C3%25AFc%2520Stratil%2520and%2520Felix%2520Fent%2520and%2520Esteban%2520Rivera%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520Autonomous%2520vehicle%2520perception%2520typically%2520relies%2520on%2520modular%2520pipelines%2520that%250Adecompose%2520the%2520task%2520into%2520detection%252C%2520tracking%252C%2520and%2520prediction.%2520While%250Ainterpretable%252C%2520these%2520pipelines%2520suffer%2520from%2520error%2520accumulation%2520and%2520limited%250Ainter-task%2520synergy.%2520Unified%2520perception%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520that%250Aintegrates%2520these%2520sub-tasks%2520within%2520a%2520shared%2520architecture%252C%2520potentially%2520improving%250Arobustness%252C%2520contextual%2520reasoning%252C%2520and%2520efficiency%2520while%2520retaining%2520interpretable%250Aoutputs.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520overview%2520of%2520unified%250Aperception%252C%2520introducing%2520a%2520holistic%2520and%2520systemic%2520taxonomy%2520that%2520categorizes%250Amethods%2520along%2520task%2520integration%252C%2520tracking%2520formulation%252C%2520and%2520representation%2520flow.%250AWe%2520define%2520three%2520paradigms%2520-Early%252C%2520Late%252C%2520and%2520Full%2520Unified%2520Perception-%2520and%250Asystematically%2520review%2520existing%2520methods%252C%2520their%2520architectures%252C%2520training%250Astrategies%252C%2520datasets%2520used%252C%2520and%2520open-source%2520availability%252C%2520while%2520highlighting%250Afuture%2520research%2520directions.%2520This%2520work%2520establishes%2520the%2520first%2520comprehensive%250Aframework%2520for%2520understanding%2520and%2520advancing%2520unified%2520perception%252C%2520consolidates%250Afragmented%2520efforts%252C%2520and%2520guides%2520future%2520research%2520toward%2520more%2520robust%252C%250Ageneralizable%252C%2520and%2520interpretable%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20New%20Beginnings%3A%20A%20Survey%20of%20Unified%20Perception%20in%20Autonomous%20Vehicle%0A%20%20Software&entry.906535625=Lo%C3%AFc%20Stratil%20and%20Felix%20Fent%20and%20Esteban%20Rivera%20and%20Markus%20Lienkamp&entry.1292438233=%20%20Autonomous%20vehicle%20perception%20typically%20relies%20on%20modular%20pipelines%20that%0Adecompose%20the%20task%20into%20detection%2C%20tracking%2C%20and%20prediction.%20While%0Ainterpretable%2C%20these%20pipelines%20suffer%20from%20error%20accumulation%20and%20limited%0Ainter-task%20synergy.%20Unified%20perception%20has%20emerged%20as%20a%20promising%20paradigm%20that%0Aintegrates%20these%20sub-tasks%20within%20a%20shared%20architecture%2C%20potentially%20improving%0Arobustness%2C%20contextual%20reasoning%2C%20and%20efficiency%20while%20retaining%20interpretable%0Aoutputs.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20unified%0Aperception%2C%20introducing%20a%20holistic%20and%20systemic%20taxonomy%20that%20categorizes%0Amethods%20along%20task%20integration%2C%20tracking%20formulation%2C%20and%20representation%20flow.%0AWe%20define%20three%20paradigms%20-Early%2C%20Late%2C%20and%20Full%20Unified%20Perception-%20and%0Asystematically%20review%20existing%20methods%2C%20their%20architectures%2C%20training%0Astrategies%2C%20datasets%20used%2C%20and%20open-source%20availability%2C%20while%20highlighting%0Afuture%20research%20directions.%20This%20work%20establishes%20the%20first%20comprehensive%0Aframework%20for%20understanding%20and%20advancing%20unified%20perception%2C%20consolidates%0Afragmented%20efforts%2C%20and%20guides%20future%20research%20toward%20more%20robust%2C%0Ageneralizable%2C%20and%20interpretable%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20892v1&entry.124074799=Read"},
{"title": "High-Dimensional Gaussian Process Regression with Soft Kernel\n  Interpolation", "author": "Chris Cama\u00f1o and Daniel Huang", "abstract": "  We introduce Soft Kernel Interpolation (SoftKI), a method that combines\naspects of Structured Kernel Interpolation (SKI) and variational inducing point\nmethods, to achieve scalable Gaussian Process (GP) regression on\nhigh-dimensional datasets. SoftKI approximates a kernel via softmax\ninterpolation from a smaller number of interpolation points learned by\noptimizing a combination of the SoftKI marginal log-likelihood (MLL), and when\nneeded, an approximate MLL for improved numerical stability. Consequently, it\ncan overcome the dimensionality scaling challenges that SKI faces when\ninterpolating from a dense and static lattice while retaining the flexibility\nof variational methods to adapt inducing points to the dataset. We demonstrate\nthe effectiveness of SoftKI across various examples and show that it is\ncompetitive with other approximated GP methods when the data dimensionality is\nmodest (around 10).\n", "link": "http://arxiv.org/abs/2410.21419v3", "date": "2025-08-28", "relevancy": 2.3756, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4879}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4781}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Gaussian%20Process%20Regression%20with%20Soft%20Kernel%0A%20%20Interpolation&body=Title%3A%20High-Dimensional%20Gaussian%20Process%20Regression%20with%20Soft%20Kernel%0A%20%20Interpolation%0AAuthor%3A%20Chris%20Cama%C3%B1o%20and%20Daniel%20Huang%0AAbstract%3A%20%20%20We%20introduce%20Soft%20Kernel%20Interpolation%20%28SoftKI%29%2C%20a%20method%20that%20combines%0Aaspects%20of%20Structured%20Kernel%20Interpolation%20%28SKI%29%20and%20variational%20inducing%20point%0Amethods%2C%20to%20achieve%20scalable%20Gaussian%20Process%20%28GP%29%20regression%20on%0Ahigh-dimensional%20datasets.%20SoftKI%20approximates%20a%20kernel%20via%20softmax%0Ainterpolation%20from%20a%20smaller%20number%20of%20interpolation%20points%20learned%20by%0Aoptimizing%20a%20combination%20of%20the%20SoftKI%20marginal%20log-likelihood%20%28MLL%29%2C%20and%20when%0Aneeded%2C%20an%20approximate%20MLL%20for%20improved%20numerical%20stability.%20Consequently%2C%20it%0Acan%20overcome%20the%20dimensionality%20scaling%20challenges%20that%20SKI%20faces%20when%0Ainterpolating%20from%20a%20dense%20and%20static%20lattice%20while%20retaining%20the%20flexibility%0Aof%20variational%20methods%20to%20adapt%20inducing%20points%20to%20the%20dataset.%20We%20demonstrate%0Athe%20effectiveness%20of%20SoftKI%20across%20various%20examples%20and%20show%20that%20it%20is%0Acompetitive%20with%20other%20approximated%20GP%20methods%20when%20the%20data%20dimensionality%20is%0Amodest%20%28around%2010%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21419v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Gaussian%2520Process%2520Regression%2520with%2520Soft%2520Kernel%250A%2520%2520Interpolation%26entry.906535625%3DChris%2520Cama%25C3%25B1o%2520and%2520Daniel%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Soft%2520Kernel%2520Interpolation%2520%2528SoftKI%2529%252C%2520a%2520method%2520that%2520combines%250Aaspects%2520of%2520Structured%2520Kernel%2520Interpolation%2520%2528SKI%2529%2520and%2520variational%2520inducing%2520point%250Amethods%252C%2520to%2520achieve%2520scalable%2520Gaussian%2520Process%2520%2528GP%2529%2520regression%2520on%250Ahigh-dimensional%2520datasets.%2520SoftKI%2520approximates%2520a%2520kernel%2520via%2520softmax%250Ainterpolation%2520from%2520a%2520smaller%2520number%2520of%2520interpolation%2520points%2520learned%2520by%250Aoptimizing%2520a%2520combination%2520of%2520the%2520SoftKI%2520marginal%2520log-likelihood%2520%2528MLL%2529%252C%2520and%2520when%250Aneeded%252C%2520an%2520approximate%2520MLL%2520for%2520improved%2520numerical%2520stability.%2520Consequently%252C%2520it%250Acan%2520overcome%2520the%2520dimensionality%2520scaling%2520challenges%2520that%2520SKI%2520faces%2520when%250Ainterpolating%2520from%2520a%2520dense%2520and%2520static%2520lattice%2520while%2520retaining%2520the%2520flexibility%250Aof%2520variational%2520methods%2520to%2520adapt%2520inducing%2520points%2520to%2520the%2520dataset.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520SoftKI%2520across%2520various%2520examples%2520and%2520show%2520that%2520it%2520is%250Acompetitive%2520with%2520other%2520approximated%2520GP%2520methods%2520when%2520the%2520data%2520dimensionality%2520is%250Amodest%2520%2528around%252010%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21419v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Gaussian%20Process%20Regression%20with%20Soft%20Kernel%0A%20%20Interpolation&entry.906535625=Chris%20Cama%C3%B1o%20and%20Daniel%20Huang&entry.1292438233=%20%20We%20introduce%20Soft%20Kernel%20Interpolation%20%28SoftKI%29%2C%20a%20method%20that%20combines%0Aaspects%20of%20Structured%20Kernel%20Interpolation%20%28SKI%29%20and%20variational%20inducing%20point%0Amethods%2C%20to%20achieve%20scalable%20Gaussian%20Process%20%28GP%29%20regression%20on%0Ahigh-dimensional%20datasets.%20SoftKI%20approximates%20a%20kernel%20via%20softmax%0Ainterpolation%20from%20a%20smaller%20number%20of%20interpolation%20points%20learned%20by%0Aoptimizing%20a%20combination%20of%20the%20SoftKI%20marginal%20log-likelihood%20%28MLL%29%2C%20and%20when%0Aneeded%2C%20an%20approximate%20MLL%20for%20improved%20numerical%20stability.%20Consequently%2C%20it%0Acan%20overcome%20the%20dimensionality%20scaling%20challenges%20that%20SKI%20faces%20when%0Ainterpolating%20from%20a%20dense%20and%20static%20lattice%20while%20retaining%20the%20flexibility%0Aof%20variational%20methods%20to%20adapt%20inducing%20points%20to%20the%20dataset.%20We%20demonstrate%0Athe%20effectiveness%20of%20SoftKI%20across%20various%20examples%20and%20show%20that%20it%20is%0Acompetitive%20with%20other%20approximated%20GP%20methods%20when%20the%20data%20dimensionality%20is%0Amodest%20%28around%2010%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21419v3&entry.124074799=Read"},
{"title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models", "author": "Beth Pearson and Bilal Boulbarss and Michael Wray and Martha Lewis", "abstract": "  A fundamental aspect of the semantics of natural language is that novel\nmeanings can be formed from the composition of previously known parts.\nVision-language models (VLMs) have made significant progress in recent years,\nhowever, there is evidence that they are unable to perform this kind of\ncomposition. For example, given an image of a red cube and a blue cylinder, a\nVLM such as CLIP is likely to incorrectly label the image as a red cylinder or\na blue cube, indicating it represents the image as a `bag-of-words' and fails\nto capture compositional semantics. Diffusion models have recently gained\nsignificant attention for their impressive generative abilities, and zero-shot\nclassifiers based on diffusion models have been shown to perform competitively\nwith CLIP in certain compositional tasks. In this work we explore whether the\ngenerative Diffusion Classifier has improved compositional generalisation\nabilities compared to discriminative models. We assess three models --\nDiffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with\nattributes and relations in both zero-shot learning (ZSL) and generalised\nzero-shot learning (GZSL) settings. Our results show that the Diffusion\nClassifier and ViLT perform well at concept binding tasks, but that all models\nstruggle significantly with the relational GZSL task, underscoring the broader\nchallenges VLMs face with relational reasoning. Analysis of CLIP embeddings\nsuggests that the difficulty may stem from overly similar representations of\nrelational concepts such as left and right. Code and dataset are available at:\nhttps://github.com/otmive/diffusion_classifier_clip\n", "link": "http://arxiv.org/abs/2508.20783v1", "date": "2025-08-28", "relevancy": 2.3587, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Compositional%20Generalisation%20in%20VLMs%20and%20Diffusion%20Models&body=Title%3A%20Evaluating%20Compositional%20Generalisation%20in%20VLMs%20and%20Diffusion%20Models%0AAuthor%3A%20Beth%20Pearson%20and%20Bilal%20Boulbarss%20and%20Michael%20Wray%20and%20Martha%20Lewis%0AAbstract%3A%20%20%20A%20fundamental%20aspect%20of%20the%20semantics%20of%20natural%20language%20is%20that%20novel%0Ameanings%20can%20be%20formed%20from%20the%20composition%20of%20previously%20known%20parts.%0AVision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%20years%2C%0Ahowever%2C%20there%20is%20evidence%20that%20they%20are%20unable%20to%20perform%20this%20kind%20of%0Acomposition.%20For%20example%2C%20given%20an%20image%20of%20a%20red%20cube%20and%20a%20blue%20cylinder%2C%20a%0AVLM%20such%20as%20CLIP%20is%20likely%20to%20incorrectly%20label%20the%20image%20as%20a%20red%20cylinder%20or%0Aa%20blue%20cube%2C%20indicating%20it%20represents%20the%20image%20as%20a%20%60bag-of-words%27%20and%20fails%0Ato%20capture%20compositional%20semantics.%20Diffusion%20models%20have%20recently%20gained%0Asignificant%20attention%20for%20their%20impressive%20generative%20abilities%2C%20and%20zero-shot%0Aclassifiers%20based%20on%20diffusion%20models%20have%20been%20shown%20to%20perform%20competitively%0Awith%20CLIP%20in%20certain%20compositional%20tasks.%20In%20this%20work%20we%20explore%20whether%20the%0Agenerative%20Diffusion%20Classifier%20has%20improved%20compositional%20generalisation%0Aabilities%20compared%20to%20discriminative%20models.%20We%20assess%20three%20models%20--%0ADiffusion%20Classifier%2C%20CLIP%2C%20and%20ViLT%20--%20on%20their%20ability%20to%20bind%20objects%20with%0Aattributes%20and%20relations%20in%20both%20zero-shot%20learning%20%28ZSL%29%20and%20generalised%0Azero-shot%20learning%20%28GZSL%29%20settings.%20Our%20results%20show%20that%20the%20Diffusion%0AClassifier%20and%20ViLT%20perform%20well%20at%20concept%20binding%20tasks%2C%20but%20that%20all%20models%0Astruggle%20significantly%20with%20the%20relational%20GZSL%20task%2C%20underscoring%20the%20broader%0Achallenges%20VLMs%20face%20with%20relational%20reasoning.%20Analysis%20of%20CLIP%20embeddings%0Asuggests%20that%20the%20difficulty%20may%20stem%20from%20overly%20similar%20representations%20of%0Arelational%20concepts%20such%20as%20left%20and%20right.%20Code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/otmive/diffusion_classifier_clip%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Compositional%2520Generalisation%2520in%2520VLMs%2520and%2520Diffusion%2520Models%26entry.906535625%3DBeth%2520Pearson%2520and%2520Bilal%2520Boulbarss%2520and%2520Michael%2520Wray%2520and%2520Martha%2520Lewis%26entry.1292438233%3D%2520%2520A%2520fundamental%2520aspect%2520of%2520the%2520semantics%2520of%2520natural%2520language%2520is%2520that%2520novel%250Ameanings%2520can%2520be%2520formed%2520from%2520the%2520composition%2520of%2520previously%2520known%2520parts.%250AVision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520recent%2520years%252C%250Ahowever%252C%2520there%2520is%2520evidence%2520that%2520they%2520are%2520unable%2520to%2520perform%2520this%2520kind%2520of%250Acomposition.%2520For%2520example%252C%2520given%2520an%2520image%2520of%2520a%2520red%2520cube%2520and%2520a%2520blue%2520cylinder%252C%2520a%250AVLM%2520such%2520as%2520CLIP%2520is%2520likely%2520to%2520incorrectly%2520label%2520the%2520image%2520as%2520a%2520red%2520cylinder%2520or%250Aa%2520blue%2520cube%252C%2520indicating%2520it%2520represents%2520the%2520image%2520as%2520a%2520%2560bag-of-words%2527%2520and%2520fails%250Ato%2520capture%2520compositional%2520semantics.%2520Diffusion%2520models%2520have%2520recently%2520gained%250Asignificant%2520attention%2520for%2520their%2520impressive%2520generative%2520abilities%252C%2520and%2520zero-shot%250Aclassifiers%2520based%2520on%2520diffusion%2520models%2520have%2520been%2520shown%2520to%2520perform%2520competitively%250Awith%2520CLIP%2520in%2520certain%2520compositional%2520tasks.%2520In%2520this%2520work%2520we%2520explore%2520whether%2520the%250Agenerative%2520Diffusion%2520Classifier%2520has%2520improved%2520compositional%2520generalisation%250Aabilities%2520compared%2520to%2520discriminative%2520models.%2520We%2520assess%2520three%2520models%2520--%250ADiffusion%2520Classifier%252C%2520CLIP%252C%2520and%2520ViLT%2520--%2520on%2520their%2520ability%2520to%2520bind%2520objects%2520with%250Aattributes%2520and%2520relations%2520in%2520both%2520zero-shot%2520learning%2520%2528ZSL%2529%2520and%2520generalised%250Azero-shot%2520learning%2520%2528GZSL%2529%2520settings.%2520Our%2520results%2520show%2520that%2520the%2520Diffusion%250AClassifier%2520and%2520ViLT%2520perform%2520well%2520at%2520concept%2520binding%2520tasks%252C%2520but%2520that%2520all%2520models%250Astruggle%2520significantly%2520with%2520the%2520relational%2520GZSL%2520task%252C%2520underscoring%2520the%2520broader%250Achallenges%2520VLMs%2520face%2520with%2520relational%2520reasoning.%2520Analysis%2520of%2520CLIP%2520embeddings%250Asuggests%2520that%2520the%2520difficulty%2520may%2520stem%2520from%2520overly%2520similar%2520representations%2520of%250Arelational%2520concepts%2520such%2520as%2520left%2520and%2520right.%2520Code%2520and%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/otmive/diffusion_classifier_clip%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Compositional%20Generalisation%20in%20VLMs%20and%20Diffusion%20Models&entry.906535625=Beth%20Pearson%20and%20Bilal%20Boulbarss%20and%20Michael%20Wray%20and%20Martha%20Lewis&entry.1292438233=%20%20A%20fundamental%20aspect%20of%20the%20semantics%20of%20natural%20language%20is%20that%20novel%0Ameanings%20can%20be%20formed%20from%20the%20composition%20of%20previously%20known%20parts.%0AVision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%20years%2C%0Ahowever%2C%20there%20is%20evidence%20that%20they%20are%20unable%20to%20perform%20this%20kind%20of%0Acomposition.%20For%20example%2C%20given%20an%20image%20of%20a%20red%20cube%20and%20a%20blue%20cylinder%2C%20a%0AVLM%20such%20as%20CLIP%20is%20likely%20to%20incorrectly%20label%20the%20image%20as%20a%20red%20cylinder%20or%0Aa%20blue%20cube%2C%20indicating%20it%20represents%20the%20image%20as%20a%20%60bag-of-words%27%20and%20fails%0Ato%20capture%20compositional%20semantics.%20Diffusion%20models%20have%20recently%20gained%0Asignificant%20attention%20for%20their%20impressive%20generative%20abilities%2C%20and%20zero-shot%0Aclassifiers%20based%20on%20diffusion%20models%20have%20been%20shown%20to%20perform%20competitively%0Awith%20CLIP%20in%20certain%20compositional%20tasks.%20In%20this%20work%20we%20explore%20whether%20the%0Agenerative%20Diffusion%20Classifier%20has%20improved%20compositional%20generalisation%0Aabilities%20compared%20to%20discriminative%20models.%20We%20assess%20three%20models%20--%0ADiffusion%20Classifier%2C%20CLIP%2C%20and%20ViLT%20--%20on%20their%20ability%20to%20bind%20objects%20with%0Aattributes%20and%20relations%20in%20both%20zero-shot%20learning%20%28ZSL%29%20and%20generalised%0Azero-shot%20learning%20%28GZSL%29%20settings.%20Our%20results%20show%20that%20the%20Diffusion%0AClassifier%20and%20ViLT%20perform%20well%20at%20concept%20binding%20tasks%2C%20but%20that%20all%20models%0Astruggle%20significantly%20with%20the%20relational%20GZSL%20task%2C%20underscoring%20the%20broader%0Achallenges%20VLMs%20face%20with%20relational%20reasoning.%20Analysis%20of%20CLIP%20embeddings%0Asuggests%20that%20the%20difficulty%20may%20stem%20from%20overly%20similar%20representations%20of%0Arelational%20concepts%20such%20as%20left%20and%20right.%20Code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/otmive/diffusion_classifier_clip%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20783v1&entry.124074799=Read"},
{"title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human\n  Preference Learning", "author": "Yuan Gong and Xionghui Wang and Jie Wu and Shiyin Wang and Yitong Wang and Xinglong Wu", "abstract": "  In this paper, we introduce OneReward, a unified reinforcement learning\nframework that enhances the model's generative capabilities across multiple\ntasks under different evaluation criteria using only \\textit{One Reward} model.\nBy employing a single vision-language model (VLM) as the generative reward\nmodel, which can distinguish the winner and loser for a given task and a given\nevaluation criterion, it can be effectively applied to multi-task generation\nmodels, particularly in contexts with varied data and diverse task objectives.\nWe utilize OneReward for mask-guided image generation, which can be further\ndivided into several sub-tasks such as image fill, image extend, object\nremoval, and text rendering, involving a binary mask as the edit area. Although\nthese domain-specific tasks share same conditioning paradigm, they differ\nsignificantly in underlying data distributions and evaluation metrics. Existing\nmethods often rely on task-specific supervised fine-tuning (SFT), which limits\ngeneralization and training efficiency. Building on OneReward, we develop\nSeedream 3.0 Fill, a mask-guided generation model trained via multi-task\nreinforcement learning directly on a pre-trained base model, eliminating the\nneed for task-specific SFT. Experimental results demonstrate that our unified\nedit model consistently outperforms both commercial and open-source\ncompetitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across\nmultiple evaluation dimensions. Code and model are available at:\nhttps://one-reward.github.io\n", "link": "http://arxiv.org/abs/2508.21066v1", "date": "2025-08-28", "relevancy": 2.3517, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6192}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5833}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneReward%3A%20Unified%20Mask-Guided%20Image%20Generation%20via%20Multi-Task%20Human%0A%20%20Preference%20Learning&body=Title%3A%20OneReward%3A%20Unified%20Mask-Guided%20Image%20Generation%20via%20Multi-Task%20Human%0A%20%20Preference%20Learning%0AAuthor%3A%20Yuan%20Gong%20and%20Xionghui%20Wang%20and%20Jie%20Wu%20and%20Shiyin%20Wang%20and%20Yitong%20Wang%20and%20Xinglong%20Wu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20OneReward%2C%20a%20unified%20reinforcement%20learning%0Aframework%20that%20enhances%20the%20model%27s%20generative%20capabilities%20across%20multiple%0Atasks%20under%20different%20evaluation%20criteria%20using%20only%20%5Ctextit%7BOne%20Reward%7D%20model.%0ABy%20employing%20a%20single%20vision-language%20model%20%28VLM%29%20as%20the%20generative%20reward%0Amodel%2C%20which%20can%20distinguish%20the%20winner%20and%20loser%20for%20a%20given%20task%20and%20a%20given%0Aevaluation%20criterion%2C%20it%20can%20be%20effectively%20applied%20to%20multi-task%20generation%0Amodels%2C%20particularly%20in%20contexts%20with%20varied%20data%20and%20diverse%20task%20objectives.%0AWe%20utilize%20OneReward%20for%20mask-guided%20image%20generation%2C%20which%20can%20be%20further%0Adivided%20into%20several%20sub-tasks%20such%20as%20image%20fill%2C%20image%20extend%2C%20object%0Aremoval%2C%20and%20text%20rendering%2C%20involving%20a%20binary%20mask%20as%20the%20edit%20area.%20Although%0Athese%20domain-specific%20tasks%20share%20same%20conditioning%20paradigm%2C%20they%20differ%0Asignificantly%20in%20underlying%20data%20distributions%20and%20evaluation%20metrics.%20Existing%0Amethods%20often%20rely%20on%20task-specific%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20limits%0Ageneralization%20and%20training%20efficiency.%20Building%20on%20OneReward%2C%20we%20develop%0ASeedream%203.0%20Fill%2C%20a%20mask-guided%20generation%20model%20trained%20via%20multi-task%0Areinforcement%20learning%20directly%20on%20a%20pre-trained%20base%20model%2C%20eliminating%20the%0Aneed%20for%20task-specific%20SFT.%20Experimental%20results%20demonstrate%20that%20our%20unified%0Aedit%20model%20consistently%20outperforms%20both%20commercial%20and%20open-source%0Acompetitors%2C%20such%20as%20Ideogram%2C%20Adobe%20Photoshop%2C%20and%20FLUX%20Fill%20%5BPro%5D%2C%20across%0Amultiple%20evaluation%20dimensions.%20Code%20and%20model%20are%20available%20at%3A%0Ahttps%3A//one-reward.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneReward%253A%2520Unified%2520Mask-Guided%2520Image%2520Generation%2520via%2520Multi-Task%2520Human%250A%2520%2520Preference%2520Learning%26entry.906535625%3DYuan%2520Gong%2520and%2520Xionghui%2520Wang%2520and%2520Jie%2520Wu%2520and%2520Shiyin%2520Wang%2520and%2520Yitong%2520Wang%2520and%2520Xinglong%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OneReward%252C%2520a%2520unified%2520reinforcement%2520learning%250Aframework%2520that%2520enhances%2520the%2520model%2527s%2520generative%2520capabilities%2520across%2520multiple%250Atasks%2520under%2520different%2520evaluation%2520criteria%2520using%2520only%2520%255Ctextit%257BOne%2520Reward%257D%2520model.%250ABy%2520employing%2520a%2520single%2520vision-language%2520model%2520%2528VLM%2529%2520as%2520the%2520generative%2520reward%250Amodel%252C%2520which%2520can%2520distinguish%2520the%2520winner%2520and%2520loser%2520for%2520a%2520given%2520task%2520and%2520a%2520given%250Aevaluation%2520criterion%252C%2520it%2520can%2520be%2520effectively%2520applied%2520to%2520multi-task%2520generation%250Amodels%252C%2520particularly%2520in%2520contexts%2520with%2520varied%2520data%2520and%2520diverse%2520task%2520objectives.%250AWe%2520utilize%2520OneReward%2520for%2520mask-guided%2520image%2520generation%252C%2520which%2520can%2520be%2520further%250Adivided%2520into%2520several%2520sub-tasks%2520such%2520as%2520image%2520fill%252C%2520image%2520extend%252C%2520object%250Aremoval%252C%2520and%2520text%2520rendering%252C%2520involving%2520a%2520binary%2520mask%2520as%2520the%2520edit%2520area.%2520Although%250Athese%2520domain-specific%2520tasks%2520share%2520same%2520conditioning%2520paradigm%252C%2520they%2520differ%250Asignificantly%2520in%2520underlying%2520data%2520distributions%2520and%2520evaluation%2520metrics.%2520Existing%250Amethods%2520often%2520rely%2520on%2520task-specific%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520which%2520limits%250Ageneralization%2520and%2520training%2520efficiency.%2520Building%2520on%2520OneReward%252C%2520we%2520develop%250ASeedream%25203.0%2520Fill%252C%2520a%2520mask-guided%2520generation%2520model%2520trained%2520via%2520multi-task%250Areinforcement%2520learning%2520directly%2520on%2520a%2520pre-trained%2520base%2520model%252C%2520eliminating%2520the%250Aneed%2520for%2520task-specific%2520SFT.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520unified%250Aedit%2520model%2520consistently%2520outperforms%2520both%2520commercial%2520and%2520open-source%250Acompetitors%252C%2520such%2520as%2520Ideogram%252C%2520Adobe%2520Photoshop%252C%2520and%2520FLUX%2520Fill%2520%255BPro%255D%252C%2520across%250Amultiple%2520evaluation%2520dimensions.%2520Code%2520and%2520model%2520are%2520available%2520at%253A%250Ahttps%253A//one-reward.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneReward%3A%20Unified%20Mask-Guided%20Image%20Generation%20via%20Multi-Task%20Human%0A%20%20Preference%20Learning&entry.906535625=Yuan%20Gong%20and%20Xionghui%20Wang%20and%20Jie%20Wu%20and%20Shiyin%20Wang%20and%20Yitong%20Wang%20and%20Xinglong%20Wu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20OneReward%2C%20a%20unified%20reinforcement%20learning%0Aframework%20that%20enhances%20the%20model%27s%20generative%20capabilities%20across%20multiple%0Atasks%20under%20different%20evaluation%20criteria%20using%20only%20%5Ctextit%7BOne%20Reward%7D%20model.%0ABy%20employing%20a%20single%20vision-language%20model%20%28VLM%29%20as%20the%20generative%20reward%0Amodel%2C%20which%20can%20distinguish%20the%20winner%20and%20loser%20for%20a%20given%20task%20and%20a%20given%0Aevaluation%20criterion%2C%20it%20can%20be%20effectively%20applied%20to%20multi-task%20generation%0Amodels%2C%20particularly%20in%20contexts%20with%20varied%20data%20and%20diverse%20task%20objectives.%0AWe%20utilize%20OneReward%20for%20mask-guided%20image%20generation%2C%20which%20can%20be%20further%0Adivided%20into%20several%20sub-tasks%20such%20as%20image%20fill%2C%20image%20extend%2C%20object%0Aremoval%2C%20and%20text%20rendering%2C%20involving%20a%20binary%20mask%20as%20the%20edit%20area.%20Although%0Athese%20domain-specific%20tasks%20share%20same%20conditioning%20paradigm%2C%20they%20differ%0Asignificantly%20in%20underlying%20data%20distributions%20and%20evaluation%20metrics.%20Existing%0Amethods%20often%20rely%20on%20task-specific%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20limits%0Ageneralization%20and%20training%20efficiency.%20Building%20on%20OneReward%2C%20we%20develop%0ASeedream%203.0%20Fill%2C%20a%20mask-guided%20generation%20model%20trained%20via%20multi-task%0Areinforcement%20learning%20directly%20on%20a%20pre-trained%20base%20model%2C%20eliminating%20the%0Aneed%20for%20task-specific%20SFT.%20Experimental%20results%20demonstrate%20that%20our%20unified%0Aedit%20model%20consistently%20outperforms%20both%20commercial%20and%20open-source%0Acompetitors%2C%20such%20as%20Ideogram%2C%20Adobe%20Photoshop%2C%20and%20FLUX%20Fill%20%5BPro%5D%2C%20across%0Amultiple%20evaluation%20dimensions.%20Code%20and%20model%20are%20available%20at%3A%0Ahttps%3A//one-reward.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21066v1&entry.124074799=Read"},
{"title": "DanceGRPO: Unleashing GRPO on Visual Generation", "author": "Zeyue Xue and Jie Wu and Yu Gao and Fangyuan Kong and Lingting Zhu and Mengzhao Chen and Zhiheng Liu and Wei Liu and Qiushan Guo and Weilin Huang and Ping Luo", "abstract": "  Recent advances in generative AI have revolutionized visual content creation,\nyet aligning model outputs with human preferences remains a critical challenge.\nWhile Reinforcement Learning (RL) has emerged as a promising approach for\nfine-tuning generative models, existing methods like DDPO and DPOK face\nfundamental limitations - particularly their inability to maintain stable\noptimization when scaling to large and diverse prompt sets, severely\nrestricting their practical utility. This paper presents DanceGRPO, a framework\nthat addresses these limitations through an innovative adaptation of Group\nRelative Policy Optimization (GRPO) for visual generation tasks. Our key\ninsight is that GRPO's inherent stability mechanisms uniquely position it to\novercome the optimization challenges that plague prior RL-based approaches on\nvisual generation. DanceGRPO establishes several significant advances: First,\nit demonstrates consistent and stable policy optimization across multiple\nmodern generative paradigms, including both diffusion models and rectified\nflows. Second, it maintains robust performance when scaling to complex,\nreal-world scenarios encompassing three key tasks and four foundation models.\nThird, it shows remarkable versatility in optimizing for diverse human\npreferences as captured by five distinct reward models assessing image/video\naesthetics, text-image alignment, video motion quality, and binary feedback.\nOur comprehensive experiments reveal that DanceGRPO outperforms baseline\nmethods by up to 181\\% across multiple established benchmarks, including\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO\nas a robust and versatile solution for scaling Reinforcement Learning from\nHuman Feedback (RLHF) tasks in visual generation, offering new insights into\nharmonizing reinforcement learning and visual synthesis.\n", "link": "http://arxiv.org/abs/2505.07818v4", "date": "2025-08-28", "relevancy": 2.337, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5996}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5917}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation&body=Title%3A%20DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation%0AAuthor%3A%20Zeyue%20Xue%20and%20Jie%20Wu%20and%20Yu%20Gao%20and%20Fangyuan%20Kong%20and%20Lingting%20Zhu%20and%20Mengzhao%20Chen%20and%20Zhiheng%20Liu%20and%20Wei%20Liu%20and%20Qiushan%20Guo%20and%20Weilin%20Huang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20AI%20have%20revolutionized%20visual%20content%20creation%2C%0Ayet%20aligning%20model%20outputs%20with%20human%20preferences%20remains%20a%20critical%20challenge.%0AWhile%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20approach%20for%0Afine-tuning%20generative%20models%2C%20existing%20methods%20like%20DDPO%20and%20DPOK%20face%0Afundamental%20limitations%20-%20particularly%20their%20inability%20to%20maintain%20stable%0Aoptimization%20when%20scaling%20to%20large%20and%20diverse%20prompt%20sets%2C%20severely%0Arestricting%20their%20practical%20utility.%20This%20paper%20presents%20DanceGRPO%2C%20a%20framework%0Athat%20addresses%20these%20limitations%20through%20an%20innovative%20adaptation%20of%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20for%20visual%20generation%20tasks.%20Our%20key%0Ainsight%20is%20that%20GRPO%27s%20inherent%20stability%20mechanisms%20uniquely%20position%20it%20to%0Aovercome%20the%20optimization%20challenges%20that%20plague%20prior%20RL-based%20approaches%20on%0Avisual%20generation.%20DanceGRPO%20establishes%20several%20significant%20advances%3A%20First%2C%0Ait%20demonstrates%20consistent%20and%20stable%20policy%20optimization%20across%20multiple%0Amodern%20generative%20paradigms%2C%20including%20both%20diffusion%20models%20and%20rectified%0Aflows.%20Second%2C%20it%20maintains%20robust%20performance%20when%20scaling%20to%20complex%2C%0Areal-world%20scenarios%20encompassing%20three%20key%20tasks%20and%20four%20foundation%20models.%0AThird%2C%20it%20shows%20remarkable%20versatility%20in%20optimizing%20for%20diverse%20human%0Apreferences%20as%20captured%20by%20five%20distinct%20reward%20models%20assessing%20image/video%0Aaesthetics%2C%20text-image%20alignment%2C%20video%20motion%20quality%2C%20and%20binary%20feedback.%0AOur%20comprehensive%20experiments%20reveal%20that%20DanceGRPO%20outperforms%20baseline%0Amethods%20by%20up%20to%20181%5C%25%20across%20multiple%20established%20benchmarks%2C%20including%0AHPS-v2.1%2C%20CLIP%20Score%2C%20VideoAlign%2C%20and%20GenEval.%20Our%20results%20establish%20DanceGRPO%0Aas%20a%20robust%20and%20versatile%20solution%20for%20scaling%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20tasks%20in%20visual%20generation%2C%20offering%20new%20insights%20into%0Aharmonizing%20reinforcement%20learning%20and%20visual%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07818v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceGRPO%253A%2520Unleashing%2520GRPO%2520on%2520Visual%2520Generation%26entry.906535625%3DZeyue%2520Xue%2520and%2520Jie%2520Wu%2520and%2520Yu%2520Gao%2520and%2520Fangyuan%2520Kong%2520and%2520Lingting%2520Zhu%2520and%2520Mengzhao%2520Chen%2520and%2520Zhiheng%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Qiushan%2520Guo%2520and%2520Weilin%2520Huang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520AI%2520have%2520revolutionized%2520visual%2520content%2520creation%252C%250Ayet%2520aligning%2520model%2520outputs%2520with%2520human%2520preferences%2520remains%2520a%2520critical%2520challenge.%250AWhile%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Afine-tuning%2520generative%2520models%252C%2520existing%2520methods%2520like%2520DDPO%2520and%2520DPOK%2520face%250Afundamental%2520limitations%2520-%2520particularly%2520their%2520inability%2520to%2520maintain%2520stable%250Aoptimization%2520when%2520scaling%2520to%2520large%2520and%2520diverse%2520prompt%2520sets%252C%2520severely%250Arestricting%2520their%2520practical%2520utility.%2520This%2520paper%2520presents%2520DanceGRPO%252C%2520a%2520framework%250Athat%2520addresses%2520these%2520limitations%2520through%2520an%2520innovative%2520adaptation%2520of%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520for%2520visual%2520generation%2520tasks.%2520Our%2520key%250Ainsight%2520is%2520that%2520GRPO%2527s%2520inherent%2520stability%2520mechanisms%2520uniquely%2520position%2520it%2520to%250Aovercome%2520the%2520optimization%2520challenges%2520that%2520plague%2520prior%2520RL-based%2520approaches%2520on%250Avisual%2520generation.%2520DanceGRPO%2520establishes%2520several%2520significant%2520advances%253A%2520First%252C%250Ait%2520demonstrates%2520consistent%2520and%2520stable%2520policy%2520optimization%2520across%2520multiple%250Amodern%2520generative%2520paradigms%252C%2520including%2520both%2520diffusion%2520models%2520and%2520rectified%250Aflows.%2520Second%252C%2520it%2520maintains%2520robust%2520performance%2520when%2520scaling%2520to%2520complex%252C%250Areal-world%2520scenarios%2520encompassing%2520three%2520key%2520tasks%2520and%2520four%2520foundation%2520models.%250AThird%252C%2520it%2520shows%2520remarkable%2520versatility%2520in%2520optimizing%2520for%2520diverse%2520human%250Apreferences%2520as%2520captured%2520by%2520five%2520distinct%2520reward%2520models%2520assessing%2520image/video%250Aaesthetics%252C%2520text-image%2520alignment%252C%2520video%2520motion%2520quality%252C%2520and%2520binary%2520feedback.%250AOur%2520comprehensive%2520experiments%2520reveal%2520that%2520DanceGRPO%2520outperforms%2520baseline%250Amethods%2520by%2520up%2520to%2520181%255C%2525%2520across%2520multiple%2520established%2520benchmarks%252C%2520including%250AHPS-v2.1%252C%2520CLIP%2520Score%252C%2520VideoAlign%252C%2520and%2520GenEval.%2520Our%2520results%2520establish%2520DanceGRPO%250Aas%2520a%2520robust%2520and%2520versatile%2520solution%2520for%2520scaling%2520Reinforcement%2520Learning%2520from%250AHuman%2520Feedback%2520%2528RLHF%2529%2520tasks%2520in%2520visual%2520generation%252C%2520offering%2520new%2520insights%2520into%250Aharmonizing%2520reinforcement%2520learning%2520and%2520visual%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07818v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation&entry.906535625=Zeyue%20Xue%20and%20Jie%20Wu%20and%20Yu%20Gao%20and%20Fangyuan%20Kong%20and%20Lingting%20Zhu%20and%20Mengzhao%20Chen%20and%20Zhiheng%20Liu%20and%20Wei%20Liu%20and%20Qiushan%20Guo%20and%20Weilin%20Huang%20and%20Ping%20Luo&entry.1292438233=%20%20Recent%20advances%20in%20generative%20AI%20have%20revolutionized%20visual%20content%20creation%2C%0Ayet%20aligning%20model%20outputs%20with%20human%20preferences%20remains%20a%20critical%20challenge.%0AWhile%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20approach%20for%0Afine-tuning%20generative%20models%2C%20existing%20methods%20like%20DDPO%20and%20DPOK%20face%0Afundamental%20limitations%20-%20particularly%20their%20inability%20to%20maintain%20stable%0Aoptimization%20when%20scaling%20to%20large%20and%20diverse%20prompt%20sets%2C%20severely%0Arestricting%20their%20practical%20utility.%20This%20paper%20presents%20DanceGRPO%2C%20a%20framework%0Athat%20addresses%20these%20limitations%20through%20an%20innovative%20adaptation%20of%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20for%20visual%20generation%20tasks.%20Our%20key%0Ainsight%20is%20that%20GRPO%27s%20inherent%20stability%20mechanisms%20uniquely%20position%20it%20to%0Aovercome%20the%20optimization%20challenges%20that%20plague%20prior%20RL-based%20approaches%20on%0Avisual%20generation.%20DanceGRPO%20establishes%20several%20significant%20advances%3A%20First%2C%0Ait%20demonstrates%20consistent%20and%20stable%20policy%20optimization%20across%20multiple%0Amodern%20generative%20paradigms%2C%20including%20both%20diffusion%20models%20and%20rectified%0Aflows.%20Second%2C%20it%20maintains%20robust%20performance%20when%20scaling%20to%20complex%2C%0Areal-world%20scenarios%20encompassing%20three%20key%20tasks%20and%20four%20foundation%20models.%0AThird%2C%20it%20shows%20remarkable%20versatility%20in%20optimizing%20for%20diverse%20human%0Apreferences%20as%20captured%20by%20five%20distinct%20reward%20models%20assessing%20image/video%0Aaesthetics%2C%20text-image%20alignment%2C%20video%20motion%20quality%2C%20and%20binary%20feedback.%0AOur%20comprehensive%20experiments%20reveal%20that%20DanceGRPO%20outperforms%20baseline%0Amethods%20by%20up%20to%20181%5C%25%20across%20multiple%20established%20benchmarks%2C%20including%0AHPS-v2.1%2C%20CLIP%20Score%2C%20VideoAlign%2C%20and%20GenEval.%20Our%20results%20establish%20DanceGRPO%0Aas%20a%20robust%20and%20versatile%20solution%20for%20scaling%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20tasks%20in%20visual%20generation%2C%20offering%20new%20insights%20into%0Aharmonizing%20reinforcement%20learning%20and%20visual%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07818v4&entry.124074799=Read"},
{"title": "Random Feature Representation Boosting", "author": "Nikita Zozoulenko and Thomas Cass and Lukas Gonon", "abstract": "  We introduce Random Feature Representation Boosting (RFRBoost), a novel\nmethod for constructing deep residual random feature neural networks (RFNNs)\nusing boosting theory. RFRBoost uses random features at each layer to learn the\nfunctional gradient of the network representation, enhancing performance while\npreserving the convex optimization benefits of RFNNs. In the case of MSE loss,\nwe obtain closed-form solutions to greedy layer-wise boosting with random\nfeatures. For general loss functions, we show that fitting random feature\nresidual blocks reduces to solving a quadratically constrained least squares\nproblem. Through extensive numerical experiments on tabular datasets for both\nregression and classification, we show that RFRBoost significantly outperforms\nRFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime\nwhere RFNNs are typically applied. Moreover, RFRBoost offers substantial\ncomputational benefits, and theoretical guarantees stemming from boosting\ntheory.\n", "link": "http://arxiv.org/abs/2501.18283v4", "date": "2025-08-28", "relevancy": 2.3346, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4432}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Feature%20Representation%20Boosting&body=Title%3A%20Random%20Feature%20Representation%20Boosting%0AAuthor%3A%20Nikita%20Zozoulenko%20and%20Thomas%20Cass%20and%20Lukas%20Gonon%0AAbstract%3A%20%20%20We%20introduce%20Random%20Feature%20Representation%20Boosting%20%28RFRBoost%29%2C%20a%20novel%0Amethod%20for%20constructing%20deep%20residual%20random%20feature%20neural%20networks%20%28RFNNs%29%0Ausing%20boosting%20theory.%20RFRBoost%20uses%20random%20features%20at%20each%20layer%20to%20learn%20the%0Afunctional%20gradient%20of%20the%20network%20representation%2C%20enhancing%20performance%20while%0Apreserving%20the%20convex%20optimization%20benefits%20of%20RFNNs.%20In%20the%20case%20of%20MSE%20loss%2C%0Awe%20obtain%20closed-form%20solutions%20to%20greedy%20layer-wise%20boosting%20with%20random%0Afeatures.%20For%20general%20loss%20functions%2C%20we%20show%20that%20fitting%20random%20feature%0Aresidual%20blocks%20reduces%20to%20solving%20a%20quadratically%20constrained%20least%20squares%0Aproblem.%20Through%20extensive%20numerical%20experiments%20on%20tabular%20datasets%20for%20both%0Aregression%20and%20classification%2C%20we%20show%20that%20RFRBoost%20significantly%20outperforms%0ARFNNs%20and%20end-to-end%20trained%20MLP%20ResNets%20in%20the%20small-%20to%20medium-scale%20regime%0Awhere%20RFNNs%20are%20typically%20applied.%20Moreover%2C%20RFRBoost%20offers%20substantial%0Acomputational%20benefits%2C%20and%20theoretical%20guarantees%20stemming%20from%20boosting%0Atheory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18283v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Feature%2520Representation%2520Boosting%26entry.906535625%3DNikita%2520Zozoulenko%2520and%2520Thomas%2520Cass%2520and%2520Lukas%2520Gonon%26entry.1292438233%3D%2520%2520We%2520introduce%2520Random%2520Feature%2520Representation%2520Boosting%2520%2528RFRBoost%2529%252C%2520a%2520novel%250Amethod%2520for%2520constructing%2520deep%2520residual%2520random%2520feature%2520neural%2520networks%2520%2528RFNNs%2529%250Ausing%2520boosting%2520theory.%2520RFRBoost%2520uses%2520random%2520features%2520at%2520each%2520layer%2520to%2520learn%2520the%250Afunctional%2520gradient%2520of%2520the%2520network%2520representation%252C%2520enhancing%2520performance%2520while%250Apreserving%2520the%2520convex%2520optimization%2520benefits%2520of%2520RFNNs.%2520In%2520the%2520case%2520of%2520MSE%2520loss%252C%250Awe%2520obtain%2520closed-form%2520solutions%2520to%2520greedy%2520layer-wise%2520boosting%2520with%2520random%250Afeatures.%2520For%2520general%2520loss%2520functions%252C%2520we%2520show%2520that%2520fitting%2520random%2520feature%250Aresidual%2520blocks%2520reduces%2520to%2520solving%2520a%2520quadratically%2520constrained%2520least%2520squares%250Aproblem.%2520Through%2520extensive%2520numerical%2520experiments%2520on%2520tabular%2520datasets%2520for%2520both%250Aregression%2520and%2520classification%252C%2520we%2520show%2520that%2520RFRBoost%2520significantly%2520outperforms%250ARFNNs%2520and%2520end-to-end%2520trained%2520MLP%2520ResNets%2520in%2520the%2520small-%2520to%2520medium-scale%2520regime%250Awhere%2520RFNNs%2520are%2520typically%2520applied.%2520Moreover%252C%2520RFRBoost%2520offers%2520substantial%250Acomputational%2520benefits%252C%2520and%2520theoretical%2520guarantees%2520stemming%2520from%2520boosting%250Atheory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18283v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Feature%20Representation%20Boosting&entry.906535625=Nikita%20Zozoulenko%20and%20Thomas%20Cass%20and%20Lukas%20Gonon&entry.1292438233=%20%20We%20introduce%20Random%20Feature%20Representation%20Boosting%20%28RFRBoost%29%2C%20a%20novel%0Amethod%20for%20constructing%20deep%20residual%20random%20feature%20neural%20networks%20%28RFNNs%29%0Ausing%20boosting%20theory.%20RFRBoost%20uses%20random%20features%20at%20each%20layer%20to%20learn%20the%0Afunctional%20gradient%20of%20the%20network%20representation%2C%20enhancing%20performance%20while%0Apreserving%20the%20convex%20optimization%20benefits%20of%20RFNNs.%20In%20the%20case%20of%20MSE%20loss%2C%0Awe%20obtain%20closed-form%20solutions%20to%20greedy%20layer-wise%20boosting%20with%20random%0Afeatures.%20For%20general%20loss%20functions%2C%20we%20show%20that%20fitting%20random%20feature%0Aresidual%20blocks%20reduces%20to%20solving%20a%20quadratically%20constrained%20least%20squares%0Aproblem.%20Through%20extensive%20numerical%20experiments%20on%20tabular%20datasets%20for%20both%0Aregression%20and%20classification%2C%20we%20show%20that%20RFRBoost%20significantly%20outperforms%0ARFNNs%20and%20end-to-end%20trained%20MLP%20ResNets%20in%20the%20small-%20to%20medium-scale%20regime%0Awhere%20RFNNs%20are%20typically%20applied.%20Moreover%2C%20RFRBoost%20offers%20substantial%0Acomputational%20benefits%2C%20and%20theoretical%20guarantees%20stemming%20from%20boosting%0Atheory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18283v4&entry.124074799=Read"},
{"title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase\n  Refinement", "author": "Amir Jmal and Chaima Chtourou and Mahdi Louati and Abdelaziz Kallel and Houda Khmila", "abstract": "  In the context of proven climate change, maintaining olive biodiversity\nthrough early anomaly detection and treatment using remote sensing technology\nis crucial, offering effective management solutions. This paper presents an\ninnovative approach to olive tree segmentation from satellite images. By\nleveraging foundational models and advanced segmentation techniques, the study\nintegrates the Segment Anything Model (SAM) to accurately identify and segment\nolive trees in agricultural plots. The methodology includes SAM segmentation\nand corrections based on trees alignement in the field and a learanble\nconstraint about the shape and the size. Our approach achieved a 98\\% accuracy\nrate, significantly surpassing the initial SAM performance of 82\\%.\n", "link": "http://arxiv.org/abs/2508.20954v1", "date": "2025-08-28", "relevancy": 2.3277, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Olive%20Tree%20Satellite%20Image%20Segmentation%20Based%20On%20SAM%20and%20Multi-Phase%0A%20%20Refinement&body=Title%3A%20Olive%20Tree%20Satellite%20Image%20Segmentation%20Based%20On%20SAM%20and%20Multi-Phase%0A%20%20Refinement%0AAuthor%3A%20Amir%20Jmal%20and%20Chaima%20Chtourou%20and%20Mahdi%20Louati%20and%20Abdelaziz%20Kallel%20and%20Houda%20Khmila%0AAbstract%3A%20%20%20In%20the%20context%20of%20proven%20climate%20change%2C%20maintaining%20olive%20biodiversity%0Athrough%20early%20anomaly%20detection%20and%20treatment%20using%20remote%20sensing%20technology%0Ais%20crucial%2C%20offering%20effective%20management%20solutions.%20This%20paper%20presents%20an%0Ainnovative%20approach%20to%20olive%20tree%20segmentation%20from%20satellite%20images.%20By%0Aleveraging%20foundational%20models%20and%20advanced%20segmentation%20techniques%2C%20the%20study%0Aintegrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20accurately%20identify%20and%20segment%0Aolive%20trees%20in%20agricultural%20plots.%20The%20methodology%20includes%20SAM%20segmentation%0Aand%20corrections%20based%20on%20trees%20alignement%20in%20the%20field%20and%20a%20learanble%0Aconstraint%20about%20the%20shape%20and%20the%20size.%20Our%20approach%20achieved%20a%2098%5C%25%20accuracy%0Arate%2C%20significantly%20surpassing%20the%20initial%20SAM%20performance%20of%2082%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOlive%2520Tree%2520Satellite%2520Image%2520Segmentation%2520Based%2520On%2520SAM%2520and%2520Multi-Phase%250A%2520%2520Refinement%26entry.906535625%3DAmir%2520Jmal%2520and%2520Chaima%2520Chtourou%2520and%2520Mahdi%2520Louati%2520and%2520Abdelaziz%2520Kallel%2520and%2520Houda%2520Khmila%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520proven%2520climate%2520change%252C%2520maintaining%2520olive%2520biodiversity%250Athrough%2520early%2520anomaly%2520detection%2520and%2520treatment%2520using%2520remote%2520sensing%2520technology%250Ais%2520crucial%252C%2520offering%2520effective%2520management%2520solutions.%2520This%2520paper%2520presents%2520an%250Ainnovative%2520approach%2520to%2520olive%2520tree%2520segmentation%2520from%2520satellite%2520images.%2520By%250Aleveraging%2520foundational%2520models%2520and%2520advanced%2520segmentation%2520techniques%252C%2520the%2520study%250Aintegrates%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520accurately%2520identify%2520and%2520segment%250Aolive%2520trees%2520in%2520agricultural%2520plots.%2520The%2520methodology%2520includes%2520SAM%2520segmentation%250Aand%2520corrections%2520based%2520on%2520trees%2520alignement%2520in%2520the%2520field%2520and%2520a%2520learanble%250Aconstraint%2520about%2520the%2520shape%2520and%2520the%2520size.%2520Our%2520approach%2520achieved%2520a%252098%255C%2525%2520accuracy%250Arate%252C%2520significantly%2520surpassing%2520the%2520initial%2520SAM%2520performance%2520of%252082%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Olive%20Tree%20Satellite%20Image%20Segmentation%20Based%20On%20SAM%20and%20Multi-Phase%0A%20%20Refinement&entry.906535625=Amir%20Jmal%20and%20Chaima%20Chtourou%20and%20Mahdi%20Louati%20and%20Abdelaziz%20Kallel%20and%20Houda%20Khmila&entry.1292438233=%20%20In%20the%20context%20of%20proven%20climate%20change%2C%20maintaining%20olive%20biodiversity%0Athrough%20early%20anomaly%20detection%20and%20treatment%20using%20remote%20sensing%20technology%0Ais%20crucial%2C%20offering%20effective%20management%20solutions.%20This%20paper%20presents%20an%0Ainnovative%20approach%20to%20olive%20tree%20segmentation%20from%20satellite%20images.%20By%0Aleveraging%20foundational%20models%20and%20advanced%20segmentation%20techniques%2C%20the%20study%0Aintegrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20accurately%20identify%20and%20segment%0Aolive%20trees%20in%20agricultural%20plots.%20The%20methodology%20includes%20SAM%20segmentation%0Aand%20corrections%20based%20on%20trees%20alignement%20in%20the%20field%20and%20a%20learanble%0Aconstraint%20about%20the%20shape%20and%20the%20size.%20Our%20approach%20achieved%20a%2098%5C%25%20accuracy%0Arate%2C%20significantly%20surpassing%20the%20initial%20SAM%20performance%20of%2082%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20954v1&entry.124074799=Read"},
{"title": "ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial\n  Transaction Networks", "author": "Zeyue Zhang and Lin Song and Erkang Bao and Xiaoling Lv and Xinyue Wang", "abstract": "  Financial fraud detection is essential to safeguard billions of dollars, yet\nthe intertwined entities and fast-changing transaction behaviors in modern\nfinancial systems routinely defeat conventional machine learning models. Recent\ngraph-based detectors make headway by representing transactions as networks,\nbut they still overlook two fraud hallmarks rooted in time: (1) temporal\nmotifs--recurring, telltale subgraphs that reveal suspicious money flows as\nthey unfold--and (2) account-specific intervals of anomalous activity, when\nfraud surfaces only in short bursts unique to each entity. To exploit both\nsignals, we introduce ATM-GAD, an adaptive graph neural network that leverages\ntemporal motifs for financial anomaly detection. A Temporal Motif Extractor\ncondenses each account's transaction history into the most informative motifs,\npreserving both topology and temporal patterns. These motifs are then analyzed\nby dual-attention blocks: IntraA reasons over interactions within a single\nmotif, while InterA aggregates evidence across motifs to expose multi-step\nfraud schemes. In parallel, a differentiable Adaptive Time-Window Learner\ntailors the observation window for every node, allowing the model to focus\nprecisely on the most revealing time slices. Experiments on four real-world\ndatasets show that ATM-GAD consistently outperforms seven strong\nanomaly-detection baselines, uncovering fraud patterns missed by earlier\nmethods.\n", "link": "http://arxiv.org/abs/2508.20829v1", "date": "2025-08-28", "relevancy": 2.3239, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4955}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATM-GAD%3A%20Adaptive%20Temporal%20Motif%20Graph%20Anomaly%20Detection%20for%20Financial%0A%20%20Transaction%20Networks&body=Title%3A%20ATM-GAD%3A%20Adaptive%20Temporal%20Motif%20Graph%20Anomaly%20Detection%20for%20Financial%0A%20%20Transaction%20Networks%0AAuthor%3A%20Zeyue%20Zhang%20and%20Lin%20Song%20and%20Erkang%20Bao%20and%20Xiaoling%20Lv%20and%20Xinyue%20Wang%0AAbstract%3A%20%20%20Financial%20fraud%20detection%20is%20essential%20to%20safeguard%20billions%20of%20dollars%2C%20yet%0Athe%20intertwined%20entities%20and%20fast-changing%20transaction%20behaviors%20in%20modern%0Afinancial%20systems%20routinely%20defeat%20conventional%20machine%20learning%20models.%20Recent%0Agraph-based%20detectors%20make%20headway%20by%20representing%20transactions%20as%20networks%2C%0Abut%20they%20still%20overlook%20two%20fraud%20hallmarks%20rooted%20in%20time%3A%20%281%29%20temporal%0Amotifs--recurring%2C%20telltale%20subgraphs%20that%20reveal%20suspicious%20money%20flows%20as%0Athey%20unfold--and%20%282%29%20account-specific%20intervals%20of%20anomalous%20activity%2C%20when%0Afraud%20surfaces%20only%20in%20short%20bursts%20unique%20to%20each%20entity.%20To%20exploit%20both%0Asignals%2C%20we%20introduce%20ATM-GAD%2C%20an%20adaptive%20graph%20neural%20network%20that%20leverages%0Atemporal%20motifs%20for%20financial%20anomaly%20detection.%20A%20Temporal%20Motif%20Extractor%0Acondenses%20each%20account%27s%20transaction%20history%20into%20the%20most%20informative%20motifs%2C%0Apreserving%20both%20topology%20and%20temporal%20patterns.%20These%20motifs%20are%20then%20analyzed%0Aby%20dual-attention%20blocks%3A%20IntraA%20reasons%20over%20interactions%20within%20a%20single%0Amotif%2C%20while%20InterA%20aggregates%20evidence%20across%20motifs%20to%20expose%20multi-step%0Afraud%20schemes.%20In%20parallel%2C%20a%20differentiable%20Adaptive%20Time-Window%20Learner%0Atailors%20the%20observation%20window%20for%20every%20node%2C%20allowing%20the%20model%20to%20focus%0Aprecisely%20on%20the%20most%20revealing%20time%20slices.%20Experiments%20on%20four%20real-world%0Adatasets%20show%20that%20ATM-GAD%20consistently%20outperforms%20seven%20strong%0Aanomaly-detection%20baselines%2C%20uncovering%20fraud%20patterns%20missed%20by%20earlier%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATM-GAD%253A%2520Adaptive%2520Temporal%2520Motif%2520Graph%2520Anomaly%2520Detection%2520for%2520Financial%250A%2520%2520Transaction%2520Networks%26entry.906535625%3DZeyue%2520Zhang%2520and%2520Lin%2520Song%2520and%2520Erkang%2520Bao%2520and%2520Xiaoling%2520Lv%2520and%2520Xinyue%2520Wang%26entry.1292438233%3D%2520%2520Financial%2520fraud%2520detection%2520is%2520essential%2520to%2520safeguard%2520billions%2520of%2520dollars%252C%2520yet%250Athe%2520intertwined%2520entities%2520and%2520fast-changing%2520transaction%2520behaviors%2520in%2520modern%250Afinancial%2520systems%2520routinely%2520defeat%2520conventional%2520machine%2520learning%2520models.%2520Recent%250Agraph-based%2520detectors%2520make%2520headway%2520by%2520representing%2520transactions%2520as%2520networks%252C%250Abut%2520they%2520still%2520overlook%2520two%2520fraud%2520hallmarks%2520rooted%2520in%2520time%253A%2520%25281%2529%2520temporal%250Amotifs--recurring%252C%2520telltale%2520subgraphs%2520that%2520reveal%2520suspicious%2520money%2520flows%2520as%250Athey%2520unfold--and%2520%25282%2529%2520account-specific%2520intervals%2520of%2520anomalous%2520activity%252C%2520when%250Afraud%2520surfaces%2520only%2520in%2520short%2520bursts%2520unique%2520to%2520each%2520entity.%2520To%2520exploit%2520both%250Asignals%252C%2520we%2520introduce%2520ATM-GAD%252C%2520an%2520adaptive%2520graph%2520neural%2520network%2520that%2520leverages%250Atemporal%2520motifs%2520for%2520financial%2520anomaly%2520detection.%2520A%2520Temporal%2520Motif%2520Extractor%250Acondenses%2520each%2520account%2527s%2520transaction%2520history%2520into%2520the%2520most%2520informative%2520motifs%252C%250Apreserving%2520both%2520topology%2520and%2520temporal%2520patterns.%2520These%2520motifs%2520are%2520then%2520analyzed%250Aby%2520dual-attention%2520blocks%253A%2520IntraA%2520reasons%2520over%2520interactions%2520within%2520a%2520single%250Amotif%252C%2520while%2520InterA%2520aggregates%2520evidence%2520across%2520motifs%2520to%2520expose%2520multi-step%250Afraud%2520schemes.%2520In%2520parallel%252C%2520a%2520differentiable%2520Adaptive%2520Time-Window%2520Learner%250Atailors%2520the%2520observation%2520window%2520for%2520every%2520node%252C%2520allowing%2520the%2520model%2520to%2520focus%250Aprecisely%2520on%2520the%2520most%2520revealing%2520time%2520slices.%2520Experiments%2520on%2520four%2520real-world%250Adatasets%2520show%2520that%2520ATM-GAD%2520consistently%2520outperforms%2520seven%2520strong%250Aanomaly-detection%2520baselines%252C%2520uncovering%2520fraud%2520patterns%2520missed%2520by%2520earlier%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATM-GAD%3A%20Adaptive%20Temporal%20Motif%20Graph%20Anomaly%20Detection%20for%20Financial%0A%20%20Transaction%20Networks&entry.906535625=Zeyue%20Zhang%20and%20Lin%20Song%20and%20Erkang%20Bao%20and%20Xiaoling%20Lv%20and%20Xinyue%20Wang&entry.1292438233=%20%20Financial%20fraud%20detection%20is%20essential%20to%20safeguard%20billions%20of%20dollars%2C%20yet%0Athe%20intertwined%20entities%20and%20fast-changing%20transaction%20behaviors%20in%20modern%0Afinancial%20systems%20routinely%20defeat%20conventional%20machine%20learning%20models.%20Recent%0Agraph-based%20detectors%20make%20headway%20by%20representing%20transactions%20as%20networks%2C%0Abut%20they%20still%20overlook%20two%20fraud%20hallmarks%20rooted%20in%20time%3A%20%281%29%20temporal%0Amotifs--recurring%2C%20telltale%20subgraphs%20that%20reveal%20suspicious%20money%20flows%20as%0Athey%20unfold--and%20%282%29%20account-specific%20intervals%20of%20anomalous%20activity%2C%20when%0Afraud%20surfaces%20only%20in%20short%20bursts%20unique%20to%20each%20entity.%20To%20exploit%20both%0Asignals%2C%20we%20introduce%20ATM-GAD%2C%20an%20adaptive%20graph%20neural%20network%20that%20leverages%0Atemporal%20motifs%20for%20financial%20anomaly%20detection.%20A%20Temporal%20Motif%20Extractor%0Acondenses%20each%20account%27s%20transaction%20history%20into%20the%20most%20informative%20motifs%2C%0Apreserving%20both%20topology%20and%20temporal%20patterns.%20These%20motifs%20are%20then%20analyzed%0Aby%20dual-attention%20blocks%3A%20IntraA%20reasons%20over%20interactions%20within%20a%20single%0Amotif%2C%20while%20InterA%20aggregates%20evidence%20across%20motifs%20to%20expose%20multi-step%0Afraud%20schemes.%20In%20parallel%2C%20a%20differentiable%20Adaptive%20Time-Window%20Learner%0Atailors%20the%20observation%20window%20for%20every%20node%2C%20allowing%20the%20model%20to%20focus%0Aprecisely%20on%20the%20most%20revealing%20time%20slices.%20Experiments%20on%20four%20real-world%0Adatasets%20show%20that%20ATM-GAD%20consistently%20outperforms%20seven%20strong%0Aanomaly-detection%20baselines%2C%20uncovering%20fraud%20patterns%20missed%20by%20earlier%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20829v1&entry.124074799=Read"},
{"title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition", "author": "Azam Nouri", "abstract": "  We revisit the classical Sobel operator to ask a simple question: Are\nfirst-order edge maps sufficient to drive an all-dense multilayer perceptron\n(MLP) for handwritten character recognition (HCR), as an alternative to\nconvolutional neural networks (CNNs)? Using only horizontal and vertical Sobel\nderivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its\nextreme simplicity, the resulting network reaches 98% accuracy on MNIST digits\nand 92% on EMNIST letters -- approaching CNNs while offering a smaller memory\nfootprint and transparent features. Our findings highlight that much of the\nclass-discriminative information in handwritten character images is already\ncaptured by first-order gradients, making edge-aware MLPs a compelling option\nfor HCR.\n", "link": "http://arxiv.org/abs/2508.11902v3", "date": "2025-08-28", "relevancy": 2.3102, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4692}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4685}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sobel-Gradient%20MLP%20Baseline%20for%20Handwritten%20Character%20Recognition&body=Title%3A%20A%20Sobel-Gradient%20MLP%20Baseline%20for%20Handwritten%20Character%20Recognition%0AAuthor%3A%20Azam%20Nouri%0AAbstract%3A%20%20%20We%20revisit%20the%20classical%20Sobel%20operator%20to%20ask%20a%20simple%20question%3A%20Are%0Afirst-order%20edge%20maps%20sufficient%20to%20drive%20an%20all-dense%20multilayer%20perceptron%0A%28MLP%29%20for%20handwritten%20character%20recognition%20%28HCR%29%2C%20as%20an%20alternative%20to%0Aconvolutional%20neural%20networks%20%28CNNs%29%3F%20Using%20only%20horizontal%20and%20vertical%20Sobel%0Aderivatives%20as%20input%2C%20we%20train%20an%20MLP%20on%20MNIST%20and%20EMNIST%20Letters.%20Despite%20its%0Aextreme%20simplicity%2C%20the%20resulting%20network%20reaches%2098%25%20accuracy%20on%20MNIST%20digits%0Aand%2092%25%20on%20EMNIST%20letters%20--%20approaching%20CNNs%20while%20offering%20a%20smaller%20memory%0Afootprint%20and%20transparent%20features.%20Our%20findings%20highlight%20that%20much%20of%20the%0Aclass-discriminative%20information%20in%20handwritten%20character%20images%20is%20already%0Acaptured%20by%20first-order%20gradients%2C%20making%20edge-aware%20MLPs%20a%20compelling%20option%0Afor%20HCR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11902v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sobel-Gradient%2520MLP%2520Baseline%2520for%2520Handwritten%2520Character%2520Recognition%26entry.906535625%3DAzam%2520Nouri%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520classical%2520Sobel%2520operator%2520to%2520ask%2520a%2520simple%2520question%253A%2520Are%250Afirst-order%2520edge%2520maps%2520sufficient%2520to%2520drive%2520an%2520all-dense%2520multilayer%2520perceptron%250A%2528MLP%2529%2520for%2520handwritten%2520character%2520recognition%2520%2528HCR%2529%252C%2520as%2520an%2520alternative%2520to%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%253F%2520Using%2520only%2520horizontal%2520and%2520vertical%2520Sobel%250Aderivatives%2520as%2520input%252C%2520we%2520train%2520an%2520MLP%2520on%2520MNIST%2520and%2520EMNIST%2520Letters.%2520Despite%2520its%250Aextreme%2520simplicity%252C%2520the%2520resulting%2520network%2520reaches%252098%2525%2520accuracy%2520on%2520MNIST%2520digits%250Aand%252092%2525%2520on%2520EMNIST%2520letters%2520--%2520approaching%2520CNNs%2520while%2520offering%2520a%2520smaller%2520memory%250Afootprint%2520and%2520transparent%2520features.%2520Our%2520findings%2520highlight%2520that%2520much%2520of%2520the%250Aclass-discriminative%2520information%2520in%2520handwritten%2520character%2520images%2520is%2520already%250Acaptured%2520by%2520first-order%2520gradients%252C%2520making%2520edge-aware%2520MLPs%2520a%2520compelling%2520option%250Afor%2520HCR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11902v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sobel-Gradient%20MLP%20Baseline%20for%20Handwritten%20Character%20Recognition&entry.906535625=Azam%20Nouri&entry.1292438233=%20%20We%20revisit%20the%20classical%20Sobel%20operator%20to%20ask%20a%20simple%20question%3A%20Are%0Afirst-order%20edge%20maps%20sufficient%20to%20drive%20an%20all-dense%20multilayer%20perceptron%0A%28MLP%29%20for%20handwritten%20character%20recognition%20%28HCR%29%2C%20as%20an%20alternative%20to%0Aconvolutional%20neural%20networks%20%28CNNs%29%3F%20Using%20only%20horizontal%20and%20vertical%20Sobel%0Aderivatives%20as%20input%2C%20we%20train%20an%20MLP%20on%20MNIST%20and%20EMNIST%20Letters.%20Despite%20its%0Aextreme%20simplicity%2C%20the%20resulting%20network%20reaches%2098%25%20accuracy%20on%20MNIST%20digits%0Aand%2092%25%20on%20EMNIST%20letters%20--%20approaching%20CNNs%20while%20offering%20a%20smaller%20memory%0Afootprint%20and%20transparent%20features.%20Our%20findings%20highlight%20that%20much%20of%20the%0Aclass-discriminative%20information%20in%20handwritten%20character%20images%20is%20already%0Acaptured%20by%20first-order%20gradients%2C%20making%20edge-aware%20MLPs%20a%20compelling%20option%0Afor%20HCR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11902v3&entry.124074799=Read"},
{"title": "Webly-Supervised Image Manipulation Localization via Category-Aware\n  Auto-Annotation", "author": "Chenfan Qu and Yiwu Zhong and Bin Li and Lianwen Jin", "abstract": "  Images manipulated using image editing tools can mislead viewers and pose\nsignificant risks to social security. However, accurately localizing the\nmanipulated regions within an image remains a challenging problem. One of the\nmain barriers in this area is the high cost of data acquisition and the severe\nlack of high-quality annotated datasets. To address this challenge, we\nintroduce novel methods that mitigate data scarcity by leveraging readily\navailable web data. We utilize a large collection of manually forged images\nfrom the web, as well as automatically generated annotations derived from a\nsimpler auxiliary task, constrained image manipulation localization.\nSpecifically, we introduce a new paradigm CAAAv2, which automatically and\naccurately annotates manipulated regions at the pixel level. To further improve\nannotation quality, we propose a novel metric, QES, which filters out\nunreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a\nlarge-scale, diverse, and high-quality dataset containing 246,212 manually\nforged images with pixel-level mask annotations. This is over 120x larger than\nexisting handcrafted datasets like IMD20. Additionally, we introduce Object\nJitter, a technique that further enhances model training by generating\nhigh-quality manipulation artifacts. Building on these advances, we develop a\nnew model, Web-IML, designed to effectively leverage web-scale supervision for\nthe image manipulation localization task. Extensive experiments demonstrate\nthat our approach substantially alleviates the data scarcity problem and\nsignificantly improves the performance of various models on multiple real-world\nforgery benchmarks. With the proposed web supervision, Web-IML achieves a\nstriking performance gain of 31% and surpasses previous SOTA TruFor by 24.1\naverage IoU points. The dataset and code will be made publicly available at\nhttps://github.com/qcf-568/MIML.\n", "link": "http://arxiv.org/abs/2508.20987v1", "date": "2025-08-28", "relevancy": 2.3045, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6138}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Webly-Supervised%20Image%20Manipulation%20Localization%20via%20Category-Aware%0A%20%20Auto-Annotation&body=Title%3A%20Webly-Supervised%20Image%20Manipulation%20Localization%20via%20Category-Aware%0A%20%20Auto-Annotation%0AAuthor%3A%20Chenfan%20Qu%20and%20Yiwu%20Zhong%20and%20Bin%20Li%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Images%20manipulated%20using%20image%20editing%20tools%20can%20mislead%20viewers%20and%20pose%0Asignificant%20risks%20to%20social%20security.%20However%2C%20accurately%20localizing%20the%0Amanipulated%20regions%20within%20an%20image%20remains%20a%20challenging%20problem.%20One%20of%20the%0Amain%20barriers%20in%20this%20area%20is%20the%20high%20cost%20of%20data%20acquisition%20and%20the%20severe%0Alack%20of%20high-quality%20annotated%20datasets.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20novel%20methods%20that%20mitigate%20data%20scarcity%20by%20leveraging%20readily%0Aavailable%20web%20data.%20We%20utilize%20a%20large%20collection%20of%20manually%20forged%20images%0Afrom%20the%20web%2C%20as%20well%20as%20automatically%20generated%20annotations%20derived%20from%20a%0Asimpler%20auxiliary%20task%2C%20constrained%20image%20manipulation%20localization.%0ASpecifically%2C%20we%20introduce%20a%20new%20paradigm%20CAAAv2%2C%20which%20automatically%20and%0Aaccurately%20annotates%20manipulated%20regions%20at%20the%20pixel%20level.%20To%20further%20improve%0Aannotation%20quality%2C%20we%20propose%20a%20novel%20metric%2C%20QES%2C%20which%20filters%20out%0Aunreliable%20annotations.%20Through%20CAAA%20v2%20and%20QES%2C%20we%20construct%20MIMLv2%2C%20a%0Alarge-scale%2C%20diverse%2C%20and%20high-quality%20dataset%20containing%20246%2C212%20manually%0Aforged%20images%20with%20pixel-level%20mask%20annotations.%20This%20is%20over%20120x%20larger%20than%0Aexisting%20handcrafted%20datasets%20like%20IMD20.%20Additionally%2C%20we%20introduce%20Object%0AJitter%2C%20a%20technique%20that%20further%20enhances%20model%20training%20by%20generating%0Ahigh-quality%20manipulation%20artifacts.%20Building%20on%20these%20advances%2C%20we%20develop%20a%0Anew%20model%2C%20Web-IML%2C%20designed%20to%20effectively%20leverage%20web-scale%20supervision%20for%0Athe%20image%20manipulation%20localization%20task.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20substantially%20alleviates%20the%20data%20scarcity%20problem%20and%0Asignificantly%20improves%20the%20performance%20of%20various%20models%20on%20multiple%20real-world%0Aforgery%20benchmarks.%20With%20the%20proposed%20web%20supervision%2C%20Web-IML%20achieves%20a%0Astriking%20performance%20gain%20of%2031%25%20and%20surpasses%20previous%20SOTA%20TruFor%20by%2024.1%0Aaverage%20IoU%20points.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/qcf-568/MIML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebly-Supervised%2520Image%2520Manipulation%2520Localization%2520via%2520Category-Aware%250A%2520%2520Auto-Annotation%26entry.906535625%3DChenfan%2520Qu%2520and%2520Yiwu%2520Zhong%2520and%2520Bin%2520Li%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Images%2520manipulated%2520using%2520image%2520editing%2520tools%2520can%2520mislead%2520viewers%2520and%2520pose%250Asignificant%2520risks%2520to%2520social%2520security.%2520However%252C%2520accurately%2520localizing%2520the%250Amanipulated%2520regions%2520within%2520an%2520image%2520remains%2520a%2520challenging%2520problem.%2520One%2520of%2520the%250Amain%2520barriers%2520in%2520this%2520area%2520is%2520the%2520high%2520cost%2520of%2520data%2520acquisition%2520and%2520the%2520severe%250Alack%2520of%2520high-quality%2520annotated%2520datasets.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520novel%2520methods%2520that%2520mitigate%2520data%2520scarcity%2520by%2520leveraging%2520readily%250Aavailable%2520web%2520data.%2520We%2520utilize%2520a%2520large%2520collection%2520of%2520manually%2520forged%2520images%250Afrom%2520the%2520web%252C%2520as%2520well%2520as%2520automatically%2520generated%2520annotations%2520derived%2520from%2520a%250Asimpler%2520auxiliary%2520task%252C%2520constrained%2520image%2520manipulation%2520localization.%250ASpecifically%252C%2520we%2520introduce%2520a%2520new%2520paradigm%2520CAAAv2%252C%2520which%2520automatically%2520and%250Aaccurately%2520annotates%2520manipulated%2520regions%2520at%2520the%2520pixel%2520level.%2520To%2520further%2520improve%250Aannotation%2520quality%252C%2520we%2520propose%2520a%2520novel%2520metric%252C%2520QES%252C%2520which%2520filters%2520out%250Aunreliable%2520annotations.%2520Through%2520CAAA%2520v2%2520and%2520QES%252C%2520we%2520construct%2520MIMLv2%252C%2520a%250Alarge-scale%252C%2520diverse%252C%2520and%2520high-quality%2520dataset%2520containing%2520246%252C212%2520manually%250Aforged%2520images%2520with%2520pixel-level%2520mask%2520annotations.%2520This%2520is%2520over%2520120x%2520larger%2520than%250Aexisting%2520handcrafted%2520datasets%2520like%2520IMD20.%2520Additionally%252C%2520we%2520introduce%2520Object%250AJitter%252C%2520a%2520technique%2520that%2520further%2520enhances%2520model%2520training%2520by%2520generating%250Ahigh-quality%2520manipulation%2520artifacts.%2520Building%2520on%2520these%2520advances%252C%2520we%2520develop%2520a%250Anew%2520model%252C%2520Web-IML%252C%2520designed%2520to%2520effectively%2520leverage%2520web-scale%2520supervision%2520for%250Athe%2520image%2520manipulation%2520localization%2520task.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520approach%2520substantially%2520alleviates%2520the%2520data%2520scarcity%2520problem%2520and%250Asignificantly%2520improves%2520the%2520performance%2520of%2520various%2520models%2520on%2520multiple%2520real-world%250Aforgery%2520benchmarks.%2520With%2520the%2520proposed%2520web%2520supervision%252C%2520Web-IML%2520achieves%2520a%250Astriking%2520performance%2520gain%2520of%252031%2525%2520and%2520surpasses%2520previous%2520SOTA%2520TruFor%2520by%252024.1%250Aaverage%2520IoU%2520points.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/qcf-568/MIML.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Webly-Supervised%20Image%20Manipulation%20Localization%20via%20Category-Aware%0A%20%20Auto-Annotation&entry.906535625=Chenfan%20Qu%20and%20Yiwu%20Zhong%20and%20Bin%20Li%20and%20Lianwen%20Jin&entry.1292438233=%20%20Images%20manipulated%20using%20image%20editing%20tools%20can%20mislead%20viewers%20and%20pose%0Asignificant%20risks%20to%20social%20security.%20However%2C%20accurately%20localizing%20the%0Amanipulated%20regions%20within%20an%20image%20remains%20a%20challenging%20problem.%20One%20of%20the%0Amain%20barriers%20in%20this%20area%20is%20the%20high%20cost%20of%20data%20acquisition%20and%20the%20severe%0Alack%20of%20high-quality%20annotated%20datasets.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20novel%20methods%20that%20mitigate%20data%20scarcity%20by%20leveraging%20readily%0Aavailable%20web%20data.%20We%20utilize%20a%20large%20collection%20of%20manually%20forged%20images%0Afrom%20the%20web%2C%20as%20well%20as%20automatically%20generated%20annotations%20derived%20from%20a%0Asimpler%20auxiliary%20task%2C%20constrained%20image%20manipulation%20localization.%0ASpecifically%2C%20we%20introduce%20a%20new%20paradigm%20CAAAv2%2C%20which%20automatically%20and%0Aaccurately%20annotates%20manipulated%20regions%20at%20the%20pixel%20level.%20To%20further%20improve%0Aannotation%20quality%2C%20we%20propose%20a%20novel%20metric%2C%20QES%2C%20which%20filters%20out%0Aunreliable%20annotations.%20Through%20CAAA%20v2%20and%20QES%2C%20we%20construct%20MIMLv2%2C%20a%0Alarge-scale%2C%20diverse%2C%20and%20high-quality%20dataset%20containing%20246%2C212%20manually%0Aforged%20images%20with%20pixel-level%20mask%20annotations.%20This%20is%20over%20120x%20larger%20than%0Aexisting%20handcrafted%20datasets%20like%20IMD20.%20Additionally%2C%20we%20introduce%20Object%0AJitter%2C%20a%20technique%20that%20further%20enhances%20model%20training%20by%20generating%0Ahigh-quality%20manipulation%20artifacts.%20Building%20on%20these%20advances%2C%20we%20develop%20a%0Anew%20model%2C%20Web-IML%2C%20designed%20to%20effectively%20leverage%20web-scale%20supervision%20for%0Athe%20image%20manipulation%20localization%20task.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20substantially%20alleviates%20the%20data%20scarcity%20problem%20and%0Asignificantly%20improves%20the%20performance%20of%20various%20models%20on%20multiple%20real-world%0Aforgery%20benchmarks.%20With%20the%20proposed%20web%20supervision%2C%20Web-IML%20achieves%20a%0Astriking%20performance%20gain%20of%2031%25%20and%20surpasses%20previous%20SOTA%20TruFor%20by%2024.1%0Aaverage%20IoU%20points.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/qcf-568/MIML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20987v1&entry.124074799=Read"},
{"title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human\n  Robot Interaction through Probabilistic Motion Forecasting", "author": "Lorenzo Busellato and Federico Cunico and Diego Dall'Alba and Marco Emporio and Andrea Giachetti and Riccardo Muradore and Marco Cristani", "abstract": "  To enable flexible, high-throughput automation in settings where people and\nrobots share workspaces, collaborative robotic cells must reconcile stringent\nsafety guarantees with the need for responsive and effective behavior. A\ndynamic obstacle is the stochastic, task-dependent variability of human motion:\nwhen robots fall back on purely reactive or worst-case envelopes, they brake\nunnecessarily, stall task progress, and tamper with the fluidity that true\nHuman-Robot Interaction demands. In recent years, learning-based human-motion\nprediction has rapidly advanced, although most approaches produce worst-case\nscenario forecasts that often do not treat prediction uncertainty in a\nwell-structured way, resulting in over-conservative planning algorithms,\nlimiting their flexibility. We introduce Uncertainty-Aware Predictive Control\nBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistic\nhuman hand motion forecasting with the formal safety guarantees of Control\nBarrier Functions. In contrast to other variants, our framework allows for\ndynamic adjustment of the safety margin thanks to the human motion uncertainty\nestimation provided by a forecasting module. Thanks to uncertainty estimation,\nUA-PCBFs empower collaborative robots with a deeper understanding of future\nhuman states, facilitating more fluid and intelligent interactions through\ninformed motion planning. We validate UA-PCBFs through comprehensive real-world\nexperiments with an increasing level of realism, including automated setups (to\nperform exactly repeatable motions) with a robotic hand and direct human-robot\ninteractions (to validate promptness, usability, and human confidence).\nRelative to state-of-the-art HRI architectures, UA-PCBFs show better\nperformance in task-critical metrics, significantly reducing the number of\nviolations of the robot's safe space during interaction with respect to the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2508.20812v1", "date": "2025-08-28", "relevancy": 2.3016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6412}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5686}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Aware-Predictive%20Control%20Barrier%20Functions%3A%20Safer%20Human%0A%20%20Robot%20Interaction%20through%20Probabilistic%20Motion%20Forecasting&body=Title%3A%20Uncertainty%20Aware-Predictive%20Control%20Barrier%20Functions%3A%20Safer%20Human%0A%20%20Robot%20Interaction%20through%20Probabilistic%20Motion%20Forecasting%0AAuthor%3A%20Lorenzo%20Busellato%20and%20Federico%20Cunico%20and%20Diego%20Dall%27Alba%20and%20Marco%20Emporio%20and%20Andrea%20Giachetti%20and%20Riccardo%20Muradore%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20To%20enable%20flexible%2C%20high-throughput%20automation%20in%20settings%20where%20people%20and%0Arobots%20share%20workspaces%2C%20collaborative%20robotic%20cells%20must%20reconcile%20stringent%0Asafety%20guarantees%20with%20the%20need%20for%20responsive%20and%20effective%20behavior.%20A%0Adynamic%20obstacle%20is%20the%20stochastic%2C%20task-dependent%20variability%20of%20human%20motion%3A%0Awhen%20robots%20fall%20back%20on%20purely%20reactive%20or%20worst-case%20envelopes%2C%20they%20brake%0Aunnecessarily%2C%20stall%20task%20progress%2C%20and%20tamper%20with%20the%20fluidity%20that%20true%0AHuman-Robot%20Interaction%20demands.%20In%20recent%20years%2C%20learning-based%20human-motion%0Aprediction%20has%20rapidly%20advanced%2C%20although%20most%20approaches%20produce%20worst-case%0Ascenario%20forecasts%20that%20often%20do%20not%20treat%20prediction%20uncertainty%20in%20a%0Awell-structured%20way%2C%20resulting%20in%20over-conservative%20planning%20algorithms%2C%0Alimiting%20their%20flexibility.%20We%20introduce%20Uncertainty-Aware%20Predictive%20Control%0ABarrier%20Functions%20%28UA-PCBFs%29%2C%20a%20unified%20framework%20that%20fuses%20probabilistic%0Ahuman%20hand%20motion%20forecasting%20with%20the%20formal%20safety%20guarantees%20of%20Control%0ABarrier%20Functions.%20In%20contrast%20to%20other%20variants%2C%20our%20framework%20allows%20for%0Adynamic%20adjustment%20of%20the%20safety%20margin%20thanks%20to%20the%20human%20motion%20uncertainty%0Aestimation%20provided%20by%20a%20forecasting%20module.%20Thanks%20to%20uncertainty%20estimation%2C%0AUA-PCBFs%20empower%20collaborative%20robots%20with%20a%20deeper%20understanding%20of%20future%0Ahuman%20states%2C%20facilitating%20more%20fluid%20and%20intelligent%20interactions%20through%0Ainformed%20motion%20planning.%20We%20validate%20UA-PCBFs%20through%20comprehensive%20real-world%0Aexperiments%20with%20an%20increasing%20level%20of%20realism%2C%20including%20automated%20setups%20%28to%0Aperform%20exactly%20repeatable%20motions%29%20with%20a%20robotic%20hand%20and%20direct%20human-robot%0Ainteractions%20%28to%20validate%20promptness%2C%20usability%2C%20and%20human%20confidence%29.%0ARelative%20to%20state-of-the-art%20HRI%20architectures%2C%20UA-PCBFs%20show%20better%0Aperformance%20in%20task-critical%20metrics%2C%20significantly%20reducing%20the%20number%20of%0Aviolations%20of%20the%20robot%27s%20safe%20space%20during%20interaction%20with%20respect%20to%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Aware-Predictive%2520Control%2520Barrier%2520Functions%253A%2520Safer%2520Human%250A%2520%2520Robot%2520Interaction%2520through%2520Probabilistic%2520Motion%2520Forecasting%26entry.906535625%3DLorenzo%2520Busellato%2520and%2520Federico%2520Cunico%2520and%2520Diego%2520Dall%2527Alba%2520and%2520Marco%2520Emporio%2520and%2520Andrea%2520Giachetti%2520and%2520Riccardo%2520Muradore%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520To%2520enable%2520flexible%252C%2520high-throughput%2520automation%2520in%2520settings%2520where%2520people%2520and%250Arobots%2520share%2520workspaces%252C%2520collaborative%2520robotic%2520cells%2520must%2520reconcile%2520stringent%250Asafety%2520guarantees%2520with%2520the%2520need%2520for%2520responsive%2520and%2520effective%2520behavior.%2520A%250Adynamic%2520obstacle%2520is%2520the%2520stochastic%252C%2520task-dependent%2520variability%2520of%2520human%2520motion%253A%250Awhen%2520robots%2520fall%2520back%2520on%2520purely%2520reactive%2520or%2520worst-case%2520envelopes%252C%2520they%2520brake%250Aunnecessarily%252C%2520stall%2520task%2520progress%252C%2520and%2520tamper%2520with%2520the%2520fluidity%2520that%2520true%250AHuman-Robot%2520Interaction%2520demands.%2520In%2520recent%2520years%252C%2520learning-based%2520human-motion%250Aprediction%2520has%2520rapidly%2520advanced%252C%2520although%2520most%2520approaches%2520produce%2520worst-case%250Ascenario%2520forecasts%2520that%2520often%2520do%2520not%2520treat%2520prediction%2520uncertainty%2520in%2520a%250Awell-structured%2520way%252C%2520resulting%2520in%2520over-conservative%2520planning%2520algorithms%252C%250Alimiting%2520their%2520flexibility.%2520We%2520introduce%2520Uncertainty-Aware%2520Predictive%2520Control%250ABarrier%2520Functions%2520%2528UA-PCBFs%2529%252C%2520a%2520unified%2520framework%2520that%2520fuses%2520probabilistic%250Ahuman%2520hand%2520motion%2520forecasting%2520with%2520the%2520formal%2520safety%2520guarantees%2520of%2520Control%250ABarrier%2520Functions.%2520In%2520contrast%2520to%2520other%2520variants%252C%2520our%2520framework%2520allows%2520for%250Adynamic%2520adjustment%2520of%2520the%2520safety%2520margin%2520thanks%2520to%2520the%2520human%2520motion%2520uncertainty%250Aestimation%2520provided%2520by%2520a%2520forecasting%2520module.%2520Thanks%2520to%2520uncertainty%2520estimation%252C%250AUA-PCBFs%2520empower%2520collaborative%2520robots%2520with%2520a%2520deeper%2520understanding%2520of%2520future%250Ahuman%2520states%252C%2520facilitating%2520more%2520fluid%2520and%2520intelligent%2520interactions%2520through%250Ainformed%2520motion%2520planning.%2520We%2520validate%2520UA-PCBFs%2520through%2520comprehensive%2520real-world%250Aexperiments%2520with%2520an%2520increasing%2520level%2520of%2520realism%252C%2520including%2520automated%2520setups%2520%2528to%250Aperform%2520exactly%2520repeatable%2520motions%2529%2520with%2520a%2520robotic%2520hand%2520and%2520direct%2520human-robot%250Ainteractions%2520%2528to%2520validate%2520promptness%252C%2520usability%252C%2520and%2520human%2520confidence%2529.%250ARelative%2520to%2520state-of-the-art%2520HRI%2520architectures%252C%2520UA-PCBFs%2520show%2520better%250Aperformance%2520in%2520task-critical%2520metrics%252C%2520significantly%2520reducing%2520the%2520number%2520of%250Aviolations%2520of%2520the%2520robot%2527s%2520safe%2520space%2520during%2520interaction%2520with%2520respect%2520to%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Aware-Predictive%20Control%20Barrier%20Functions%3A%20Safer%20Human%0A%20%20Robot%20Interaction%20through%20Probabilistic%20Motion%20Forecasting&entry.906535625=Lorenzo%20Busellato%20and%20Federico%20Cunico%20and%20Diego%20Dall%27Alba%20and%20Marco%20Emporio%20and%20Andrea%20Giachetti%20and%20Riccardo%20Muradore%20and%20Marco%20Cristani&entry.1292438233=%20%20To%20enable%20flexible%2C%20high-throughput%20automation%20in%20settings%20where%20people%20and%0Arobots%20share%20workspaces%2C%20collaborative%20robotic%20cells%20must%20reconcile%20stringent%0Asafety%20guarantees%20with%20the%20need%20for%20responsive%20and%20effective%20behavior.%20A%0Adynamic%20obstacle%20is%20the%20stochastic%2C%20task-dependent%20variability%20of%20human%20motion%3A%0Awhen%20robots%20fall%20back%20on%20purely%20reactive%20or%20worst-case%20envelopes%2C%20they%20brake%0Aunnecessarily%2C%20stall%20task%20progress%2C%20and%20tamper%20with%20the%20fluidity%20that%20true%0AHuman-Robot%20Interaction%20demands.%20In%20recent%20years%2C%20learning-based%20human-motion%0Aprediction%20has%20rapidly%20advanced%2C%20although%20most%20approaches%20produce%20worst-case%0Ascenario%20forecasts%20that%20often%20do%20not%20treat%20prediction%20uncertainty%20in%20a%0Awell-structured%20way%2C%20resulting%20in%20over-conservative%20planning%20algorithms%2C%0Alimiting%20their%20flexibility.%20We%20introduce%20Uncertainty-Aware%20Predictive%20Control%0ABarrier%20Functions%20%28UA-PCBFs%29%2C%20a%20unified%20framework%20that%20fuses%20probabilistic%0Ahuman%20hand%20motion%20forecasting%20with%20the%20formal%20safety%20guarantees%20of%20Control%0ABarrier%20Functions.%20In%20contrast%20to%20other%20variants%2C%20our%20framework%20allows%20for%0Adynamic%20adjustment%20of%20the%20safety%20margin%20thanks%20to%20the%20human%20motion%20uncertainty%0Aestimation%20provided%20by%20a%20forecasting%20module.%20Thanks%20to%20uncertainty%20estimation%2C%0AUA-PCBFs%20empower%20collaborative%20robots%20with%20a%20deeper%20understanding%20of%20future%0Ahuman%20states%2C%20facilitating%20more%20fluid%20and%20intelligent%20interactions%20through%0Ainformed%20motion%20planning.%20We%20validate%20UA-PCBFs%20through%20comprehensive%20real-world%0Aexperiments%20with%20an%20increasing%20level%20of%20realism%2C%20including%20automated%20setups%20%28to%0Aperform%20exactly%20repeatable%20motions%29%20with%20a%20robotic%20hand%20and%20direct%20human-robot%0Ainteractions%20%28to%20validate%20promptness%2C%20usability%2C%20and%20human%20confidence%29.%0ARelative%20to%20state-of-the-art%20HRI%20architectures%2C%20UA-PCBFs%20show%20better%0Aperformance%20in%20task-critical%20metrics%2C%20significantly%20reducing%20the%20number%20of%0Aviolations%20of%20the%20robot%27s%20safe%20space%20during%20interaction%20with%20respect%20to%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20812v1&entry.124074799=Read"},
{"title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents", "author": "Tianjian Liu and Fanqi Wan and Jiajian Guo and Xiaojun Quan", "abstract": "  Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.\n", "link": "http://arxiv.org/abs/2508.20973v1", "date": "2025-08-28", "relevancy": 2.2989, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProactiveEval%3A%20A%20Unified%20Evaluation%20Framework%20for%20Proactive%20Dialogue%0A%20%20Agents&body=Title%3A%20ProactiveEval%3A%20A%20Unified%20Evaluation%20Framework%20for%20Proactive%20Dialogue%0A%20%20Agents%0AAuthor%3A%20Tianjian%20Liu%20and%20Fanqi%20Wan%20and%20Jiajian%20Guo%20and%20Xiaojun%20Quan%0AAbstract%3A%20%20%20Proactive%20dialogue%20has%20emerged%20as%20a%20critical%20and%20challenging%20research%20problem%0Ain%20advancing%20large%20language%20models%20%28LLMs%29.%20Existing%20works%20predominantly%20focus%0Aon%20domain-specific%20or%20task-oriented%20scenarios%2C%20which%20leads%20to%20fragmented%0Aevaluations%20and%20limits%20the%20comprehensive%20exploration%20of%20models%27%20proactive%0Aconversation%20abilities.%20In%20this%20work%2C%20we%20propose%20ProactiveEval%2C%20a%20unified%0Aframework%20designed%20for%20evaluating%20proactive%20dialogue%20capabilities%20of%20LLMs.%20This%0Aframework%20decomposes%20proactive%20dialogue%20into%20target%20planning%20and%20dialogue%0Aguidance%2C%20establishing%20evaluation%20metrics%20across%20various%20domains.%20Moreover%2C%20it%0Aalso%20enables%20the%20automatic%20generation%20of%20diverse%20and%20challenging%20evaluation%0Adata.%20Based%20on%20the%20proposed%20framework%2C%20we%20develop%20328%20evaluation%20environments%0Aspanning%206%20distinct%20domains.%20Through%20experiments%20with%2022%20different%20types%20of%0ALLMs%2C%20we%20show%20that%20DeepSeek-R1%20and%20Claude-3.7-Sonnet%20exhibit%20exceptional%0Aperformance%20on%20target%20planning%20and%20dialogue%20guidance%20tasks%2C%20respectively.%0AFinally%2C%20we%20investigate%20how%20reasoning%20capabilities%20influence%20proactive%0Abehaviors%20and%20discuss%20their%20implications%20for%20future%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactiveEval%253A%2520A%2520Unified%2520Evaluation%2520Framework%2520for%2520Proactive%2520Dialogue%250A%2520%2520Agents%26entry.906535625%3DTianjian%2520Liu%2520and%2520Fanqi%2520Wan%2520and%2520Jiajian%2520Guo%2520and%2520Xiaojun%2520Quan%26entry.1292438233%3D%2520%2520Proactive%2520dialogue%2520has%2520emerged%2520as%2520a%2520critical%2520and%2520challenging%2520research%2520problem%250Ain%2520advancing%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Existing%2520works%2520predominantly%2520focus%250Aon%2520domain-specific%2520or%2520task-oriented%2520scenarios%252C%2520which%2520leads%2520to%2520fragmented%250Aevaluations%2520and%2520limits%2520the%2520comprehensive%2520exploration%2520of%2520models%2527%2520proactive%250Aconversation%2520abilities.%2520In%2520this%2520work%252C%2520we%2520propose%2520ProactiveEval%252C%2520a%2520unified%250Aframework%2520designed%2520for%2520evaluating%2520proactive%2520dialogue%2520capabilities%2520of%2520LLMs.%2520This%250Aframework%2520decomposes%2520proactive%2520dialogue%2520into%2520target%2520planning%2520and%2520dialogue%250Aguidance%252C%2520establishing%2520evaluation%2520metrics%2520across%2520various%2520domains.%2520Moreover%252C%2520it%250Aalso%2520enables%2520the%2520automatic%2520generation%2520of%2520diverse%2520and%2520challenging%2520evaluation%250Adata.%2520Based%2520on%2520the%2520proposed%2520framework%252C%2520we%2520develop%2520328%2520evaluation%2520environments%250Aspanning%25206%2520distinct%2520domains.%2520Through%2520experiments%2520with%252022%2520different%2520types%2520of%250ALLMs%252C%2520we%2520show%2520that%2520DeepSeek-R1%2520and%2520Claude-3.7-Sonnet%2520exhibit%2520exceptional%250Aperformance%2520on%2520target%2520planning%2520and%2520dialogue%2520guidance%2520tasks%252C%2520respectively.%250AFinally%252C%2520we%2520investigate%2520how%2520reasoning%2520capabilities%2520influence%2520proactive%250Abehaviors%2520and%2520discuss%2520their%2520implications%2520for%2520future%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProactiveEval%3A%20A%20Unified%20Evaluation%20Framework%20for%20Proactive%20Dialogue%0A%20%20Agents&entry.906535625=Tianjian%20Liu%20and%20Fanqi%20Wan%20and%20Jiajian%20Guo%20and%20Xiaojun%20Quan&entry.1292438233=%20%20Proactive%20dialogue%20has%20emerged%20as%20a%20critical%20and%20challenging%20research%20problem%0Ain%20advancing%20large%20language%20models%20%28LLMs%29.%20Existing%20works%20predominantly%20focus%0Aon%20domain-specific%20or%20task-oriented%20scenarios%2C%20which%20leads%20to%20fragmented%0Aevaluations%20and%20limits%20the%20comprehensive%20exploration%20of%20models%27%20proactive%0Aconversation%20abilities.%20In%20this%20work%2C%20we%20propose%20ProactiveEval%2C%20a%20unified%0Aframework%20designed%20for%20evaluating%20proactive%20dialogue%20capabilities%20of%20LLMs.%20This%0Aframework%20decomposes%20proactive%20dialogue%20into%20target%20planning%20and%20dialogue%0Aguidance%2C%20establishing%20evaluation%20metrics%20across%20various%20domains.%20Moreover%2C%20it%0Aalso%20enables%20the%20automatic%20generation%20of%20diverse%20and%20challenging%20evaluation%0Adata.%20Based%20on%20the%20proposed%20framework%2C%20we%20develop%20328%20evaluation%20environments%0Aspanning%206%20distinct%20domains.%20Through%20experiments%20with%2022%20different%20types%20of%0ALLMs%2C%20we%20show%20that%20DeepSeek-R1%20and%20Claude-3.7-Sonnet%20exhibit%20exceptional%0Aperformance%20on%20target%20planning%20and%20dialogue%20guidance%20tasks%2C%20respectively.%0AFinally%2C%20we%20investigate%20how%20reasoning%20capabilities%20influence%20proactive%0Abehaviors%20and%20discuss%20their%20implications%20for%20future%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20973v1&entry.124074799=Read"},
{"title": "Surfel-based 3D Registration with Equivariant SE(3) Features", "author": "Xueyang Kang and Hang Zhao and Kourosh Khoshelham and Patrick Vandewalle", "abstract": "  Point cloud registration is crucial for ensuring 3D alignment consistency of\nmultiple local point clouds in 3D reconstruction for remote sensing or digital\nheritage. While various point cloud-based registration methods exist, both\nnon-learning and learning-based, they ignore point orientations and point\nuncertainties, making the model susceptible to noisy input and aggressive\nrotations of the input point cloud like orthogonal transformation; thus, it\nnecessitates extensive training point clouds with transformation augmentations.\nTo address these issues, we propose a novel surfel-based pose learning\nregression approach. Our method can initialize surfels from Lidar point cloud\nusing virtual perspective camera parameters, and learns explicit\n$\\mathbf{SE(3)}$ equivariant features, including both position and rotation\nthrough $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative\ntransformation between source and target scans. The model comprises an\nequivariant convolutional encoder, a cross-attention mechanism for similarity\ncomputation, a fully-connected decoder, and a non-linear Huber loss.\nExperimental results on indoor and outdoor datasets demonstrate our model\nsuperiority and robust performance on real point-cloud scans compared to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.20789v1", "date": "2025-08-28", "relevancy": 2.2756, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5972}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5538}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surfel-based%203D%20Registration%20with%20Equivariant%20SE%283%29%20Features&body=Title%3A%20Surfel-based%203D%20Registration%20with%20Equivariant%20SE%283%29%20Features%0AAuthor%3A%20Xueyang%20Kang%20and%20Hang%20Zhao%20and%20Kourosh%20Khoshelham%20and%20Patrick%20Vandewalle%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20crucial%20for%20ensuring%203D%20alignment%20consistency%20of%0Amultiple%20local%20point%20clouds%20in%203D%20reconstruction%20for%20remote%20sensing%20or%20digital%0Aheritage.%20While%20various%20point%20cloud-based%20registration%20methods%20exist%2C%20both%0Anon-learning%20and%20learning-based%2C%20they%20ignore%20point%20orientations%20and%20point%0Auncertainties%2C%20making%20the%20model%20susceptible%20to%20noisy%20input%20and%20aggressive%0Arotations%20of%20the%20input%20point%20cloud%20like%20orthogonal%20transformation%3B%20thus%2C%20it%0Anecessitates%20extensive%20training%20point%20clouds%20with%20transformation%20augmentations.%0ATo%20address%20these%20issues%2C%20we%20propose%20a%20novel%20surfel-based%20pose%20learning%0Aregression%20approach.%20Our%20method%20can%20initialize%20surfels%20from%20Lidar%20point%20cloud%0Ausing%20virtual%20perspective%20camera%20parameters%2C%20and%20learns%20explicit%0A%24%5Cmathbf%7BSE%283%29%7D%24%20equivariant%20features%2C%20including%20both%20position%20and%20rotation%0Athrough%20%24%5Cmathbf%7BSE%283%29%7D%24%20equivariant%20convolutional%20kernels%20to%20predict%20relative%0Atransformation%20between%20source%20and%20target%20scans.%20The%20model%20comprises%20an%0Aequivariant%20convolutional%20encoder%2C%20a%20cross-attention%20mechanism%20for%20similarity%0Acomputation%2C%20a%20fully-connected%20decoder%2C%20and%20a%20non-linear%20Huber%20loss.%0AExperimental%20results%20on%20indoor%20and%20outdoor%20datasets%20demonstrate%20our%20model%0Asuperiority%20and%20robust%20performance%20on%20real%20point-cloud%20scans%20compared%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfel-based%25203D%2520Registration%2520with%2520Equivariant%2520SE%25283%2529%2520Features%26entry.906535625%3DXueyang%2520Kang%2520and%2520Hang%2520Zhao%2520and%2520Kourosh%2520Khoshelham%2520and%2520Patrick%2520Vandewalle%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520crucial%2520for%2520ensuring%25203D%2520alignment%2520consistency%2520of%250Amultiple%2520local%2520point%2520clouds%2520in%25203D%2520reconstruction%2520for%2520remote%2520sensing%2520or%2520digital%250Aheritage.%2520While%2520various%2520point%2520cloud-based%2520registration%2520methods%2520exist%252C%2520both%250Anon-learning%2520and%2520learning-based%252C%2520they%2520ignore%2520point%2520orientations%2520and%2520point%250Auncertainties%252C%2520making%2520the%2520model%2520susceptible%2520to%2520noisy%2520input%2520and%2520aggressive%250Arotations%2520of%2520the%2520input%2520point%2520cloud%2520like%2520orthogonal%2520transformation%253B%2520thus%252C%2520it%250Anecessitates%2520extensive%2520training%2520point%2520clouds%2520with%2520transformation%2520augmentations.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520surfel-based%2520pose%2520learning%250Aregression%2520approach.%2520Our%2520method%2520can%2520initialize%2520surfels%2520from%2520Lidar%2520point%2520cloud%250Ausing%2520virtual%2520perspective%2520camera%2520parameters%252C%2520and%2520learns%2520explicit%250A%2524%255Cmathbf%257BSE%25283%2529%257D%2524%2520equivariant%2520features%252C%2520including%2520both%2520position%2520and%2520rotation%250Athrough%2520%2524%255Cmathbf%257BSE%25283%2529%257D%2524%2520equivariant%2520convolutional%2520kernels%2520to%2520predict%2520relative%250Atransformation%2520between%2520source%2520and%2520target%2520scans.%2520The%2520model%2520comprises%2520an%250Aequivariant%2520convolutional%2520encoder%252C%2520a%2520cross-attention%2520mechanism%2520for%2520similarity%250Acomputation%252C%2520a%2520fully-connected%2520decoder%252C%2520and%2520a%2520non-linear%2520Huber%2520loss.%250AExperimental%2520results%2520on%2520indoor%2520and%2520outdoor%2520datasets%2520demonstrate%2520our%2520model%250Asuperiority%2520and%2520robust%2520performance%2520on%2520real%2520point-cloud%2520scans%2520compared%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surfel-based%203D%20Registration%20with%20Equivariant%20SE%283%29%20Features&entry.906535625=Xueyang%20Kang%20and%20Hang%20Zhao%20and%20Kourosh%20Khoshelham%20and%20Patrick%20Vandewalle&entry.1292438233=%20%20Point%20cloud%20registration%20is%20crucial%20for%20ensuring%203D%20alignment%20consistency%20of%0Amultiple%20local%20point%20clouds%20in%203D%20reconstruction%20for%20remote%20sensing%20or%20digital%0Aheritage.%20While%20various%20point%20cloud-based%20registration%20methods%20exist%2C%20both%0Anon-learning%20and%20learning-based%2C%20they%20ignore%20point%20orientations%20and%20point%0Auncertainties%2C%20making%20the%20model%20susceptible%20to%20noisy%20input%20and%20aggressive%0Arotations%20of%20the%20input%20point%20cloud%20like%20orthogonal%20transformation%3B%20thus%2C%20it%0Anecessitates%20extensive%20training%20point%20clouds%20with%20transformation%20augmentations.%0ATo%20address%20these%20issues%2C%20we%20propose%20a%20novel%20surfel-based%20pose%20learning%0Aregression%20approach.%20Our%20method%20can%20initialize%20surfels%20from%20Lidar%20point%20cloud%0Ausing%20virtual%20perspective%20camera%20parameters%2C%20and%20learns%20explicit%0A%24%5Cmathbf%7BSE%283%29%7D%24%20equivariant%20features%2C%20including%20both%20position%20and%20rotation%0Athrough%20%24%5Cmathbf%7BSE%283%29%7D%24%20equivariant%20convolutional%20kernels%20to%20predict%20relative%0Atransformation%20between%20source%20and%20target%20scans.%20The%20model%20comprises%20an%0Aequivariant%20convolutional%20encoder%2C%20a%20cross-attention%20mechanism%20for%20similarity%0Acomputation%2C%20a%20fully-connected%20decoder%2C%20and%20a%20non-linear%20Huber%20loss.%0AExperimental%20results%20on%20indoor%20and%20outdoor%20datasets%20demonstrate%20our%20model%0Asuperiority%20and%20robust%20performance%20on%20real%20point-cloud%20scans%20compared%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20789v1&entry.124074799=Read"},
{"title": "Mixture of Contexts for Long Video Generation", "author": "Shengqu Cai and Ceyuan Yang and Lvmin Zhang and Yuwei Guo and Junfei Xiao and Ziyan Yang and Yinghao Xu and Zhenheng Yang and Alan Yuille and Leonidas Guibas and Maneesh Agrawala and Lu Jiang and Gordon Wetzstein", "abstract": "  Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.\n", "link": "http://arxiv.org/abs/2508.21058v1", "date": "2025-08-28", "relevancy": 2.2661, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5884}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5841}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Contexts%20for%20Long%20Video%20Generation&body=Title%3A%20Mixture%20of%20Contexts%20for%20Long%20Video%20Generation%0AAuthor%3A%20Shengqu%20Cai%20and%20Ceyuan%20Yang%20and%20Lvmin%20Zhang%20and%20Yuwei%20Guo%20and%20Junfei%20Xiao%20and%20Ziyan%20Yang%20and%20Yinghao%20Xu%20and%20Zhenheng%20Yang%20and%20Alan%20Yuille%20and%20Leonidas%20Guibas%20and%20Maneesh%20Agrawala%20and%20Lu%20Jiang%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20Long%20video%20generation%20is%20fundamentally%20a%20long%20context%20memory%20problem%3A%20models%0Amust%20retain%20and%20retrieve%20salient%20events%20across%20a%20long%20range%20without%20collapsing%0Aor%20drifting.%20However%2C%20scaling%20diffusion%20transformers%20to%20generate%20long-context%0Avideos%20is%20fundamentally%20limited%20by%20the%20quadratic%20cost%20of%20self-attention%2C%20which%0Amakes%20memory%20and%20computation%20intractable%20and%20difficult%20to%20optimize%20for%20long%0Asequences.%20We%20recast%20long-context%20video%20generation%20as%20an%20internal%20information%0Aretrieval%20task%20and%20propose%20a%20simple%2C%20learnable%20sparse%20attention%20routing%20module%2C%0AMixture%20of%20Contexts%20%28MoC%29%2C%20as%20an%20effective%20long-term%20memory%20retrieval%20engine.%0AIn%20MoC%2C%20each%20query%20dynamically%20selects%20a%20few%20informative%20chunks%20plus%20mandatory%0Aanchors%20%28caption%2C%20local%20windows%29%20to%20attend%20to%2C%20with%20causal%20routing%20that%0Aprevents%20loop%20closures.%20As%20we%20scale%20the%20data%20and%20gradually%20sparsify%20the%0Arouting%2C%20the%20model%20allocates%20compute%20to%20salient%20history%2C%20preserving%20identities%2C%0Aactions%2C%20and%20scenes%20over%20minutes%20of%20content.%20Efficiency%20follows%20as%20a%20byproduct%0Aof%20retrieval%20%28near-linear%20scaling%29%2C%20which%20enables%20practical%20training%20and%0Asynthesis%2C%20and%20the%20emergence%20of%20memory%20and%20consistency%20at%20the%20scale%20of%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Contexts%2520for%2520Long%2520Video%2520Generation%26entry.906535625%3DShengqu%2520Cai%2520and%2520Ceyuan%2520Yang%2520and%2520Lvmin%2520Zhang%2520and%2520Yuwei%2520Guo%2520and%2520Junfei%2520Xiao%2520and%2520Ziyan%2520Yang%2520and%2520Yinghao%2520Xu%2520and%2520Zhenheng%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Leonidas%2520Guibas%2520and%2520Maneesh%2520Agrawala%2520and%2520Lu%2520Jiang%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520Long%2520video%2520generation%2520is%2520fundamentally%2520a%2520long%2520context%2520memory%2520problem%253A%2520models%250Amust%2520retain%2520and%2520retrieve%2520salient%2520events%2520across%2520a%2520long%2520range%2520without%2520collapsing%250Aor%2520drifting.%2520However%252C%2520scaling%2520diffusion%2520transformers%2520to%2520generate%2520long-context%250Avideos%2520is%2520fundamentally%2520limited%2520by%2520the%2520quadratic%2520cost%2520of%2520self-attention%252C%2520which%250Amakes%2520memory%2520and%2520computation%2520intractable%2520and%2520difficult%2520to%2520optimize%2520for%2520long%250Asequences.%2520We%2520recast%2520long-context%2520video%2520generation%2520as%2520an%2520internal%2520information%250Aretrieval%2520task%2520and%2520propose%2520a%2520simple%252C%2520learnable%2520sparse%2520attention%2520routing%2520module%252C%250AMixture%2520of%2520Contexts%2520%2528MoC%2529%252C%2520as%2520an%2520effective%2520long-term%2520memory%2520retrieval%2520engine.%250AIn%2520MoC%252C%2520each%2520query%2520dynamically%2520selects%2520a%2520few%2520informative%2520chunks%2520plus%2520mandatory%250Aanchors%2520%2528caption%252C%2520local%2520windows%2529%2520to%2520attend%2520to%252C%2520with%2520causal%2520routing%2520that%250Aprevents%2520loop%2520closures.%2520As%2520we%2520scale%2520the%2520data%2520and%2520gradually%2520sparsify%2520the%250Arouting%252C%2520the%2520model%2520allocates%2520compute%2520to%2520salient%2520history%252C%2520preserving%2520identities%252C%250Aactions%252C%2520and%2520scenes%2520over%2520minutes%2520of%2520content.%2520Efficiency%2520follows%2520as%2520a%2520byproduct%250Aof%2520retrieval%2520%2528near-linear%2520scaling%2529%252C%2520which%2520enables%2520practical%2520training%2520and%250Asynthesis%252C%2520and%2520the%2520emergence%2520of%2520memory%2520and%2520consistency%2520at%2520the%2520scale%2520of%2520minutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Contexts%20for%20Long%20Video%20Generation&entry.906535625=Shengqu%20Cai%20and%20Ceyuan%20Yang%20and%20Lvmin%20Zhang%20and%20Yuwei%20Guo%20and%20Junfei%20Xiao%20and%20Ziyan%20Yang%20and%20Yinghao%20Xu%20and%20Zhenheng%20Yang%20and%20Alan%20Yuille%20and%20Leonidas%20Guibas%20and%20Maneesh%20Agrawala%20and%20Lu%20Jiang%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20Long%20video%20generation%20is%20fundamentally%20a%20long%20context%20memory%20problem%3A%20models%0Amust%20retain%20and%20retrieve%20salient%20events%20across%20a%20long%20range%20without%20collapsing%0Aor%20drifting.%20However%2C%20scaling%20diffusion%20transformers%20to%20generate%20long-context%0Avideos%20is%20fundamentally%20limited%20by%20the%20quadratic%20cost%20of%20self-attention%2C%20which%0Amakes%20memory%20and%20computation%20intractable%20and%20difficult%20to%20optimize%20for%20long%0Asequences.%20We%20recast%20long-context%20video%20generation%20as%20an%20internal%20information%0Aretrieval%20task%20and%20propose%20a%20simple%2C%20learnable%20sparse%20attention%20routing%20module%2C%0AMixture%20of%20Contexts%20%28MoC%29%2C%20as%20an%20effective%20long-term%20memory%20retrieval%20engine.%0AIn%20MoC%2C%20each%20query%20dynamically%20selects%20a%20few%20informative%20chunks%20plus%20mandatory%0Aanchors%20%28caption%2C%20local%20windows%29%20to%20attend%20to%2C%20with%20causal%20routing%20that%0Aprevents%20loop%20closures.%20As%20we%20scale%20the%20data%20and%20gradually%20sparsify%20the%0Arouting%2C%20the%20model%20allocates%20compute%20to%20salient%20history%2C%20preserving%20identities%2C%0Aactions%2C%20and%20scenes%20over%20minutes%20of%20content.%20Efficiency%20follows%20as%20a%20byproduct%0Aof%20retrieval%20%28near-linear%20scaling%29%2C%20which%20enables%20practical%20training%20and%0Asynthesis%2C%20and%20the%20emergence%20of%20memory%20and%20consistency%20at%20the%20scale%20of%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21058v1&entry.124074799=Read"},
{"title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "author": "Carter Blum and Katja Filippova and Ann Yuan and Asma Ghandeharioun and Julian Zimmert and Fred Zhang and Jessica Hoffmann and Tal Linzen and Martin Wattenberg and Lucas Dixon and Mor Geva", "abstract": "  Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.\n", "link": "http://arxiv.org/abs/2508.11017v2", "date": "2025-08-28", "relevancy": 2.216, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Rosetta%20Stone%3A%20Unification%20Forces%20in%20Generalization%20Dynamics&body=Title%3A%20Beyond%20the%20Rosetta%20Stone%3A%20Unification%20Forces%20in%20Generalization%20Dynamics%0AAuthor%3A%20Carter%20Blum%20and%20Katja%20Filippova%20and%20Ann%20Yuan%20and%20Asma%20Ghandeharioun%20and%20Julian%20Zimmert%20and%20Fred%20Zhang%20and%20Jessica%20Hoffmann%20and%20Tal%20Linzen%20and%20Martin%20Wattenberg%20and%20Lucas%20Dixon%20and%20Mor%20Geva%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20struggle%20with%20cross-lingual%20knowledge%20transfer%3A%0Athey%20hallucinate%20when%20asked%20in%20one%20language%20about%20facts%20expressed%20in%20a%0Adifferent%20language%20during%20training.%20This%20work%20introduces%20a%20controlled%20setting%0Ato%20study%20the%20causes%20and%20dynamics%20of%20this%20phenomenon%20by%20training%20small%0ATransformer%20models%20from%20scratch%20on%20synthetic%20multilingual%20datasets.%20We%20identify%0Aa%20learning%20phase%20wherein%20a%20model%20develops%20either%20separate%20or%20unified%0Arepresentations%20of%20the%20same%20facts%20across%20languages%2C%20and%20show%20that%20unification%0Ais%20essential%20for%20cross-lingual%20transfer.%20We%20also%20show%20that%20the%20degree%20of%0Aunification%20depends%20on%20mutual%20information%20between%20facts%20and%20training%20data%0Alanguage%2C%20and%20on%20how%20easy%20it%20is%20to%20extract%20that%20language.%20Based%20on%20these%0Ainsights%2C%20we%20develop%20methods%20to%20modulate%20the%20level%20of%20cross-lingual%20transfer%20by%0Amanipulating%20data%20distribution%20and%20tokenization%2C%20and%20we%20introduce%20metrics%20and%0Avisualizations%20to%20formally%20characterize%20their%20effects%20on%20unification.%20Our%20work%0Ashows%20how%20controlled%20settings%20can%20shed%20light%20on%20pre-training%20dynamics%20and%0Asuggests%20new%20directions%20for%20improving%20cross-lingual%20transfer%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Rosetta%2520Stone%253A%2520Unification%2520Forces%2520in%2520Generalization%2520Dynamics%26entry.906535625%3DCarter%2520Blum%2520and%2520Katja%2520Filippova%2520and%2520Ann%2520Yuan%2520and%2520Asma%2520Ghandeharioun%2520and%2520Julian%2520Zimmert%2520and%2520Fred%2520Zhang%2520and%2520Jessica%2520Hoffmann%2520and%2520Tal%2520Linzen%2520and%2520Martin%2520Wattenberg%2520and%2520Lucas%2520Dixon%2520and%2520Mor%2520Geva%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520struggle%2520with%2520cross-lingual%2520knowledge%2520transfer%253A%250Athey%2520hallucinate%2520when%2520asked%2520in%2520one%2520language%2520about%2520facts%2520expressed%2520in%2520a%250Adifferent%2520language%2520during%2520training.%2520This%2520work%2520introduces%2520a%2520controlled%2520setting%250Ato%2520study%2520the%2520causes%2520and%2520dynamics%2520of%2520this%2520phenomenon%2520by%2520training%2520small%250ATransformer%2520models%2520from%2520scratch%2520on%2520synthetic%2520multilingual%2520datasets.%2520We%2520identify%250Aa%2520learning%2520phase%2520wherein%2520a%2520model%2520develops%2520either%2520separate%2520or%2520unified%250Arepresentations%2520of%2520the%2520same%2520facts%2520across%2520languages%252C%2520and%2520show%2520that%2520unification%250Ais%2520essential%2520for%2520cross-lingual%2520transfer.%2520We%2520also%2520show%2520that%2520the%2520degree%2520of%250Aunification%2520depends%2520on%2520mutual%2520information%2520between%2520facts%2520and%2520training%2520data%250Alanguage%252C%2520and%2520on%2520how%2520easy%2520it%2520is%2520to%2520extract%2520that%2520language.%2520Based%2520on%2520these%250Ainsights%252C%2520we%2520develop%2520methods%2520to%2520modulate%2520the%2520level%2520of%2520cross-lingual%2520transfer%2520by%250Amanipulating%2520data%2520distribution%2520and%2520tokenization%252C%2520and%2520we%2520introduce%2520metrics%2520and%250Avisualizations%2520to%2520formally%2520characterize%2520their%2520effects%2520on%2520unification.%2520Our%2520work%250Ashows%2520how%2520controlled%2520settings%2520can%2520shed%2520light%2520on%2520pre-training%2520dynamics%2520and%250Asuggests%2520new%2520directions%2520for%2520improving%2520cross-lingual%2520transfer%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Rosetta%20Stone%3A%20Unification%20Forces%20in%20Generalization%20Dynamics&entry.906535625=Carter%20Blum%20and%20Katja%20Filippova%20and%20Ann%20Yuan%20and%20Asma%20Ghandeharioun%20and%20Julian%20Zimmert%20and%20Fred%20Zhang%20and%20Jessica%20Hoffmann%20and%20Tal%20Linzen%20and%20Martin%20Wattenberg%20and%20Lucas%20Dixon%20and%20Mor%20Geva&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20struggle%20with%20cross-lingual%20knowledge%20transfer%3A%0Athey%20hallucinate%20when%20asked%20in%20one%20language%20about%20facts%20expressed%20in%20a%0Adifferent%20language%20during%20training.%20This%20work%20introduces%20a%20controlled%20setting%0Ato%20study%20the%20causes%20and%20dynamics%20of%20this%20phenomenon%20by%20training%20small%0ATransformer%20models%20from%20scratch%20on%20synthetic%20multilingual%20datasets.%20We%20identify%0Aa%20learning%20phase%20wherein%20a%20model%20develops%20either%20separate%20or%20unified%0Arepresentations%20of%20the%20same%20facts%20across%20languages%2C%20and%20show%20that%20unification%0Ais%20essential%20for%20cross-lingual%20transfer.%20We%20also%20show%20that%20the%20degree%20of%0Aunification%20depends%20on%20mutual%20information%20between%20facts%20and%20training%20data%0Alanguage%2C%20and%20on%20how%20easy%20it%20is%20to%20extract%20that%20language.%20Based%20on%20these%0Ainsights%2C%20we%20develop%20methods%20to%20modulate%20the%20level%20of%20cross-lingual%20transfer%20by%0Amanipulating%20data%20distribution%20and%20tokenization%2C%20and%20we%20introduce%20metrics%20and%0Avisualizations%20to%20formally%20characterize%20their%20effects%20on%20unification.%20Our%20work%0Ashows%20how%20controlled%20settings%20can%20shed%20light%20on%20pre-training%20dynamics%20and%0Asuggests%20new%20directions%20for%20improving%20cross-lingual%20transfer%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11017v2&entry.124074799=Read"},
{"title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian\n  Recurrent Deep Network", "author": "Reza Akbari Movahed and Abuzar Rezaee and Arezoo Zakeri and Colin Berry and Edmond S. L. Ho and Ali Gooya", "abstract": "  Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)\nimages is vital for assessing cardiac function and detecting its abnormalities.\nExisting methods often struggle to capture heart motion accurately because they\nrely on intensity-based image registration similarity losses that may overlook\ncardiac anatomical regions. To address this, we propose CardioMorphNet, a\nrecurrent Bayesian deep learning framework for 3D cardiac shape-guided\ndeformable registration using short-axis (SAX) CMR images. It employs a\nrecurrent variational autoencoder to model spatio-temporal dependencies over\nthe cardiac cycle and two posterior models for bi-ventricular segmentation and\nmotion estimation. The derived loss function from the Bayesian formulation\nguides the framework to focus on anatomical regions by recursively registering\nsegmentation maps without using intensity-based image registration similarity\nloss, while leveraging sequential SAX volumes and spatio-temporal features. The\nBayesian modelling also enables computation of uncertainty maps for the\nestimated motion fields. Validated on the UK Biobank dataset by comparing\nwarped mask shapes with ground truth masks, CardioMorphNet demonstrates\nsuperior performance in cardiac motion estimation, outperforming\nstate-of-the-art methods. Uncertainty assessment shows that it also yields\nlower uncertainty values for estimated motion fields in the cardiac region\ncompared with other probabilistic-based cardiac registration methods,\nindicating higher confidence in its predictions.\n", "link": "http://arxiv.org/abs/2508.20734v1", "date": "2025-08-28", "relevancy": 2.1923, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardioMorphNet%3A%20Cardiac%20Motion%20Prediction%20Using%20a%20Shape-Guided%20Bayesian%0A%20%20Recurrent%20Deep%20Network&body=Title%3A%20CardioMorphNet%3A%20Cardiac%20Motion%20Prediction%20Using%20a%20Shape-Guided%20Bayesian%0A%20%20Recurrent%20Deep%20Network%0AAuthor%3A%20Reza%20Akbari%20Movahed%20and%20Abuzar%20Rezaee%20and%20Arezoo%20Zakeri%20and%20Colin%20Berry%20and%20Edmond%20S.%20L.%20Ho%20and%20Ali%20Gooya%0AAbstract%3A%20%20%20Accurate%20cardiac%20motion%20estimation%20from%20cine%20cardiac%20magnetic%20resonance%20%28CMR%29%0Aimages%20is%20vital%20for%20assessing%20cardiac%20function%20and%20detecting%20its%20abnormalities.%0AExisting%20methods%20often%20struggle%20to%20capture%20heart%20motion%20accurately%20because%20they%0Arely%20on%20intensity-based%20image%20registration%20similarity%20losses%20that%20may%20overlook%0Acardiac%20anatomical%20regions.%20To%20address%20this%2C%20we%20propose%20CardioMorphNet%2C%20a%0Arecurrent%20Bayesian%20deep%20learning%20framework%20for%203D%20cardiac%20shape-guided%0Adeformable%20registration%20using%20short-axis%20%28SAX%29%20CMR%20images.%20It%20employs%20a%0Arecurrent%20variational%20autoencoder%20to%20model%20spatio-temporal%20dependencies%20over%0Athe%20cardiac%20cycle%20and%20two%20posterior%20models%20for%20bi-ventricular%20segmentation%20and%0Amotion%20estimation.%20The%20derived%20loss%20function%20from%20the%20Bayesian%20formulation%0Aguides%20the%20framework%20to%20focus%20on%20anatomical%20regions%20by%20recursively%20registering%0Asegmentation%20maps%20without%20using%20intensity-based%20image%20registration%20similarity%0Aloss%2C%20while%20leveraging%20sequential%20SAX%20volumes%20and%20spatio-temporal%20features.%20The%0ABayesian%20modelling%20also%20enables%20computation%20of%20uncertainty%20maps%20for%20the%0Aestimated%20motion%20fields.%20Validated%20on%20the%20UK%20Biobank%20dataset%20by%20comparing%0Awarped%20mask%20shapes%20with%20ground%20truth%20masks%2C%20CardioMorphNet%20demonstrates%0Asuperior%20performance%20in%20cardiac%20motion%20estimation%2C%20outperforming%0Astate-of-the-art%20methods.%20Uncertainty%20assessment%20shows%20that%20it%20also%20yields%0Alower%20uncertainty%20values%20for%20estimated%20motion%20fields%20in%20the%20cardiac%20region%0Acompared%20with%20other%20probabilistic-based%20cardiac%20registration%20methods%2C%0Aindicating%20higher%20confidence%20in%20its%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardioMorphNet%253A%2520Cardiac%2520Motion%2520Prediction%2520Using%2520a%2520Shape-Guided%2520Bayesian%250A%2520%2520Recurrent%2520Deep%2520Network%26entry.906535625%3DReza%2520Akbari%2520Movahed%2520and%2520Abuzar%2520Rezaee%2520and%2520Arezoo%2520Zakeri%2520and%2520Colin%2520Berry%2520and%2520Edmond%2520S.%2520L.%2520Ho%2520and%2520Ali%2520Gooya%26entry.1292438233%3D%2520%2520Accurate%2520cardiac%2520motion%2520estimation%2520from%2520cine%2520cardiac%2520magnetic%2520resonance%2520%2528CMR%2529%250Aimages%2520is%2520vital%2520for%2520assessing%2520cardiac%2520function%2520and%2520detecting%2520its%2520abnormalities.%250AExisting%2520methods%2520often%2520struggle%2520to%2520capture%2520heart%2520motion%2520accurately%2520because%2520they%250Arely%2520on%2520intensity-based%2520image%2520registration%2520similarity%2520losses%2520that%2520may%2520overlook%250Acardiac%2520anatomical%2520regions.%2520To%2520address%2520this%252C%2520we%2520propose%2520CardioMorphNet%252C%2520a%250Arecurrent%2520Bayesian%2520deep%2520learning%2520framework%2520for%25203D%2520cardiac%2520shape-guided%250Adeformable%2520registration%2520using%2520short-axis%2520%2528SAX%2529%2520CMR%2520images.%2520It%2520employs%2520a%250Arecurrent%2520variational%2520autoencoder%2520to%2520model%2520spatio-temporal%2520dependencies%2520over%250Athe%2520cardiac%2520cycle%2520and%2520two%2520posterior%2520models%2520for%2520bi-ventricular%2520segmentation%2520and%250Amotion%2520estimation.%2520The%2520derived%2520loss%2520function%2520from%2520the%2520Bayesian%2520formulation%250Aguides%2520the%2520framework%2520to%2520focus%2520on%2520anatomical%2520regions%2520by%2520recursively%2520registering%250Asegmentation%2520maps%2520without%2520using%2520intensity-based%2520image%2520registration%2520similarity%250Aloss%252C%2520while%2520leveraging%2520sequential%2520SAX%2520volumes%2520and%2520spatio-temporal%2520features.%2520The%250ABayesian%2520modelling%2520also%2520enables%2520computation%2520of%2520uncertainty%2520maps%2520for%2520the%250Aestimated%2520motion%2520fields.%2520Validated%2520on%2520the%2520UK%2520Biobank%2520dataset%2520by%2520comparing%250Awarped%2520mask%2520shapes%2520with%2520ground%2520truth%2520masks%252C%2520CardioMorphNet%2520demonstrates%250Asuperior%2520performance%2520in%2520cardiac%2520motion%2520estimation%252C%2520outperforming%250Astate-of-the-art%2520methods.%2520Uncertainty%2520assessment%2520shows%2520that%2520it%2520also%2520yields%250Alower%2520uncertainty%2520values%2520for%2520estimated%2520motion%2520fields%2520in%2520the%2520cardiac%2520region%250Acompared%2520with%2520other%2520probabilistic-based%2520cardiac%2520registration%2520methods%252C%250Aindicating%2520higher%2520confidence%2520in%2520its%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioMorphNet%3A%20Cardiac%20Motion%20Prediction%20Using%20a%20Shape-Guided%20Bayesian%0A%20%20Recurrent%20Deep%20Network&entry.906535625=Reza%20Akbari%20Movahed%20and%20Abuzar%20Rezaee%20and%20Arezoo%20Zakeri%20and%20Colin%20Berry%20and%20Edmond%20S.%20L.%20Ho%20and%20Ali%20Gooya&entry.1292438233=%20%20Accurate%20cardiac%20motion%20estimation%20from%20cine%20cardiac%20magnetic%20resonance%20%28CMR%29%0Aimages%20is%20vital%20for%20assessing%20cardiac%20function%20and%20detecting%20its%20abnormalities.%0AExisting%20methods%20often%20struggle%20to%20capture%20heart%20motion%20accurately%20because%20they%0Arely%20on%20intensity-based%20image%20registration%20similarity%20losses%20that%20may%20overlook%0Acardiac%20anatomical%20regions.%20To%20address%20this%2C%20we%20propose%20CardioMorphNet%2C%20a%0Arecurrent%20Bayesian%20deep%20learning%20framework%20for%203D%20cardiac%20shape-guided%0Adeformable%20registration%20using%20short-axis%20%28SAX%29%20CMR%20images.%20It%20employs%20a%0Arecurrent%20variational%20autoencoder%20to%20model%20spatio-temporal%20dependencies%20over%0Athe%20cardiac%20cycle%20and%20two%20posterior%20models%20for%20bi-ventricular%20segmentation%20and%0Amotion%20estimation.%20The%20derived%20loss%20function%20from%20the%20Bayesian%20formulation%0Aguides%20the%20framework%20to%20focus%20on%20anatomical%20regions%20by%20recursively%20registering%0Asegmentation%20maps%20without%20using%20intensity-based%20image%20registration%20similarity%0Aloss%2C%20while%20leveraging%20sequential%20SAX%20volumes%20and%20spatio-temporal%20features.%20The%0ABayesian%20modelling%20also%20enables%20computation%20of%20uncertainty%20maps%20for%20the%0Aestimated%20motion%20fields.%20Validated%20on%20the%20UK%20Biobank%20dataset%20by%20comparing%0Awarped%20mask%20shapes%20with%20ground%20truth%20masks%2C%20CardioMorphNet%20demonstrates%0Asuperior%20performance%20in%20cardiac%20motion%20estimation%2C%20outperforming%0Astate-of-the-art%20methods.%20Uncertainty%20assessment%20shows%20that%20it%20also%20yields%0Alower%20uncertainty%20values%20for%20estimated%20motion%20fields%20in%20the%20cardiac%20region%0Acompared%20with%20other%20probabilistic-based%20cardiac%20registration%20methods%2C%0Aindicating%20higher%20confidence%20in%20its%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20734v1&entry.124074799=Read"},
{"title": "Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous\n  Driving", "author": "Dianzhao Li and Ostap Okhrin", "abstract": "  Autonomous vehicles hold great promise for reducing traffic fatalities and\nimproving transportation efficiency, yet their widespread adoption hinges on\nembedding robust ethical reasoning into routine and emergency maneuvers,\nparticularly to protect vulnerable road users (VRUs) such as pedestrians and\ncyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL)\nframework that explicitly integrates moral considerations with standard driving\nobjectives. At the decision level, a Safe RL agent is trained using a composite\nethical risk cost, combining collision probability and harm severity, to\ngenerate high-level motion targets. A dynamic Prioritized Experience Replay\nmechanism amplifies learning from rare but critical, high-risk events. At the\nexecution level, polynomial path planning coupled with\nProportional-Integral-Derivative (PID) and Stanley controllers translates these\ntargets into smooth, feasible trajectories, ensuring both accuracy and comfort.\nWe train and validate our approach on rich, real-world traffic datasets\nencompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that\nit outperforms baseline methods in reducing ethical risk and maintaining\ndriving performance. To our knowledge, this is the first study of ethical\ndecision-making for autonomous vehicles via Safe RL evaluated on real-world,\nhuman-mixed traffic scenarios. Our results highlight the potential of combining\nformal control theory and data-driven learning to advance ethically accountable\nautonomy that explicitly protects those most at risk in urban traffic\nenvironments.\n", "link": "http://arxiv.org/abs/2508.14926v2", "date": "2025-08-28", "relevancy": 2.1891, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Drive%20Ethically%3A%20Embedding%20Moral%20Reasoning%20into%20Autonomous%0A%20%20Driving&body=Title%3A%20Learning%20to%20Drive%20Ethically%3A%20Embedding%20Moral%20Reasoning%20into%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Dianzhao%20Li%20and%20Ostap%20Okhrin%0AAbstract%3A%20%20%20Autonomous%20vehicles%20hold%20great%20promise%20for%20reducing%20traffic%20fatalities%20and%0Aimproving%20transportation%20efficiency%2C%20yet%20their%20widespread%20adoption%20hinges%20on%0Aembedding%20robust%20ethical%20reasoning%20into%20routine%20and%20emergency%20maneuvers%2C%0Aparticularly%20to%20protect%20vulnerable%20road%20users%20%28VRUs%29%20such%20as%20pedestrians%20and%0Acyclists.%20Here%2C%20we%20present%20a%20hierarchical%20Safe%20Reinforcement%20Learning%20%28Safe%20RL%29%0Aframework%20that%20explicitly%20integrates%20moral%20considerations%20with%20standard%20driving%0Aobjectives.%20At%20the%20decision%20level%2C%20a%20Safe%20RL%20agent%20is%20trained%20using%20a%20composite%0Aethical%20risk%20cost%2C%20combining%20collision%20probability%20and%20harm%20severity%2C%20to%0Agenerate%20high-level%20motion%20targets.%20A%20dynamic%20Prioritized%20Experience%20Replay%0Amechanism%20amplifies%20learning%20from%20rare%20but%20critical%2C%20high-risk%20events.%20At%20the%0Aexecution%20level%2C%20polynomial%20path%20planning%20coupled%20with%0AProportional-Integral-Derivative%20%28PID%29%20and%20Stanley%20controllers%20translates%20these%0Atargets%20into%20smooth%2C%20feasible%20trajectories%2C%20ensuring%20both%20accuracy%20and%20comfort.%0AWe%20train%20and%20validate%20our%20approach%20on%20rich%2C%20real-world%20traffic%20datasets%0Aencompassing%20diverse%20vehicles%2C%20cyclists%2C%20and%20pedestrians%2C%20and%20demonstrate%20that%0Ait%20outperforms%20baseline%20methods%20in%20reducing%20ethical%20risk%20and%20maintaining%0Adriving%20performance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20study%20of%20ethical%0Adecision-making%20for%20autonomous%20vehicles%20via%20Safe%20RL%20evaluated%20on%20real-world%2C%0Ahuman-mixed%20traffic%20scenarios.%20Our%20results%20highlight%20the%20potential%20of%20combining%0Aformal%20control%20theory%20and%20data-driven%20learning%20to%20advance%20ethically%20accountable%0Aautonomy%20that%20explicitly%20protects%20those%20most%20at%20risk%20in%20urban%20traffic%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Drive%2520Ethically%253A%2520Embedding%2520Moral%2520Reasoning%2520into%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DDianzhao%2520Li%2520and%2520Ostap%2520Okhrin%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520hold%2520great%2520promise%2520for%2520reducing%2520traffic%2520fatalities%2520and%250Aimproving%2520transportation%2520efficiency%252C%2520yet%2520their%2520widespread%2520adoption%2520hinges%2520on%250Aembedding%2520robust%2520ethical%2520reasoning%2520into%2520routine%2520and%2520emergency%2520maneuvers%252C%250Aparticularly%2520to%2520protect%2520vulnerable%2520road%2520users%2520%2528VRUs%2529%2520such%2520as%2520pedestrians%2520and%250Acyclists.%2520Here%252C%2520we%2520present%2520a%2520hierarchical%2520Safe%2520Reinforcement%2520Learning%2520%2528Safe%2520RL%2529%250Aframework%2520that%2520explicitly%2520integrates%2520moral%2520considerations%2520with%2520standard%2520driving%250Aobjectives.%2520At%2520the%2520decision%2520level%252C%2520a%2520Safe%2520RL%2520agent%2520is%2520trained%2520using%2520a%2520composite%250Aethical%2520risk%2520cost%252C%2520combining%2520collision%2520probability%2520and%2520harm%2520severity%252C%2520to%250Agenerate%2520high-level%2520motion%2520targets.%2520A%2520dynamic%2520Prioritized%2520Experience%2520Replay%250Amechanism%2520amplifies%2520learning%2520from%2520rare%2520but%2520critical%252C%2520high-risk%2520events.%2520At%2520the%250Aexecution%2520level%252C%2520polynomial%2520path%2520planning%2520coupled%2520with%250AProportional-Integral-Derivative%2520%2528PID%2529%2520and%2520Stanley%2520controllers%2520translates%2520these%250Atargets%2520into%2520smooth%252C%2520feasible%2520trajectories%252C%2520ensuring%2520both%2520accuracy%2520and%2520comfort.%250AWe%2520train%2520and%2520validate%2520our%2520approach%2520on%2520rich%252C%2520real-world%2520traffic%2520datasets%250Aencompassing%2520diverse%2520vehicles%252C%2520cyclists%252C%2520and%2520pedestrians%252C%2520and%2520demonstrate%2520that%250Ait%2520outperforms%2520baseline%2520methods%2520in%2520reducing%2520ethical%2520risk%2520and%2520maintaining%250Adriving%2520performance.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520of%2520ethical%250Adecision-making%2520for%2520autonomous%2520vehicles%2520via%2520Safe%2520RL%2520evaluated%2520on%2520real-world%252C%250Ahuman-mixed%2520traffic%2520scenarios.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520combining%250Aformal%2520control%2520theory%2520and%2520data-driven%2520learning%2520to%2520advance%2520ethically%2520accountable%250Aautonomy%2520that%2520explicitly%2520protects%2520those%2520most%2520at%2520risk%2520in%2520urban%2520traffic%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Drive%20Ethically%3A%20Embedding%20Moral%20Reasoning%20into%20Autonomous%0A%20%20Driving&entry.906535625=Dianzhao%20Li%20and%20Ostap%20Okhrin&entry.1292438233=%20%20Autonomous%20vehicles%20hold%20great%20promise%20for%20reducing%20traffic%20fatalities%20and%0Aimproving%20transportation%20efficiency%2C%20yet%20their%20widespread%20adoption%20hinges%20on%0Aembedding%20robust%20ethical%20reasoning%20into%20routine%20and%20emergency%20maneuvers%2C%0Aparticularly%20to%20protect%20vulnerable%20road%20users%20%28VRUs%29%20such%20as%20pedestrians%20and%0Acyclists.%20Here%2C%20we%20present%20a%20hierarchical%20Safe%20Reinforcement%20Learning%20%28Safe%20RL%29%0Aframework%20that%20explicitly%20integrates%20moral%20considerations%20with%20standard%20driving%0Aobjectives.%20At%20the%20decision%20level%2C%20a%20Safe%20RL%20agent%20is%20trained%20using%20a%20composite%0Aethical%20risk%20cost%2C%20combining%20collision%20probability%20and%20harm%20severity%2C%20to%0Agenerate%20high-level%20motion%20targets.%20A%20dynamic%20Prioritized%20Experience%20Replay%0Amechanism%20amplifies%20learning%20from%20rare%20but%20critical%2C%20high-risk%20events.%20At%20the%0Aexecution%20level%2C%20polynomial%20path%20planning%20coupled%20with%0AProportional-Integral-Derivative%20%28PID%29%20and%20Stanley%20controllers%20translates%20these%0Atargets%20into%20smooth%2C%20feasible%20trajectories%2C%20ensuring%20both%20accuracy%20and%20comfort.%0AWe%20train%20and%20validate%20our%20approach%20on%20rich%2C%20real-world%20traffic%20datasets%0Aencompassing%20diverse%20vehicles%2C%20cyclists%2C%20and%20pedestrians%2C%20and%20demonstrate%20that%0Ait%20outperforms%20baseline%20methods%20in%20reducing%20ethical%20risk%20and%20maintaining%0Adriving%20performance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20study%20of%20ethical%0Adecision-making%20for%20autonomous%20vehicles%20via%20Safe%20RL%20evaluated%20on%20real-world%2C%0Ahuman-mixed%20traffic%20scenarios.%20Our%20results%20highlight%20the%20potential%20of%20combining%0Aformal%20control%20theory%20and%20data-driven%20learning%20to%20advance%20ethically%20accountable%0Aautonomy%20that%20explicitly%20protects%20those%20most%20at%20risk%20in%20urban%20traffic%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14926v2&entry.124074799=Read"},
{"title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression\n  via Dynamic Neural Network", "author": "Chenhao Zhang and Wei Gao", "abstract": "  Neural Video Compression (NVC) has achieved remarkable performance in recent\nyears. However, precise rate control remains a challenge due to the inherent\nlimitations of learning-based codecs. To solve this issue, we propose a dynamic\nvideo compression framework designed for variable bitrate scenarios. First, to\nachieve variable bitrate implementation, we propose the Dynamic-Route\nAutoencoder with variable coding routes, each occupying partial computational\ncomplexity of the whole network and navigating to a distinct RD trade-off.\nSecond, to approach the target bitrate, the Rate Control Agent estimates the\nbitrate of each route and adjusts the coding route of DRA at run time. To\nencompass a broad spectrum of variable bitrates while preserving overall RD\nperformance, we employ the Joint-Routes Optimization strategy, achieving\ncollaborative training of various routes. Extensive experiments on the HEVC and\nUVG datasets show that the proposed method achieves an average BD-Rate\nreduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods\nwhile maintaining an average bitrate error of 1.66%, achieving\nRate-Distortion-Complexity Optimization (RDCO) for various bitrate and\nbitrate-constrained applications. Our code is available at\nhttps://git.openi.org.cn/OpenAICoding/DynamicDVC.\n", "link": "http://arxiv.org/abs/2508.20709v1", "date": "2025-08-28", "relevancy": 2.1867, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5597}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5391}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Rate%20Control%20for%20Frame-Level%20Adaptive%20Neural%20Video%20Compression%0A%20%20via%20Dynamic%20Neural%20Network&body=Title%3A%20Learned%20Rate%20Control%20for%20Frame-Level%20Adaptive%20Neural%20Video%20Compression%0A%20%20via%20Dynamic%20Neural%20Network%0AAuthor%3A%20Chenhao%20Zhang%20and%20Wei%20Gao%0AAbstract%3A%20%20%20Neural%20Video%20Compression%20%28NVC%29%20has%20achieved%20remarkable%20performance%20in%20recent%0Ayears.%20However%2C%20precise%20rate%20control%20remains%20a%20challenge%20due%20to%20the%20inherent%0Alimitations%20of%20learning-based%20codecs.%20To%20solve%20this%20issue%2C%20we%20propose%20a%20dynamic%0Avideo%20compression%20framework%20designed%20for%20variable%20bitrate%20scenarios.%20First%2C%20to%0Aachieve%20variable%20bitrate%20implementation%2C%20we%20propose%20the%20Dynamic-Route%0AAutoencoder%20with%20variable%20coding%20routes%2C%20each%20occupying%20partial%20computational%0Acomplexity%20of%20the%20whole%20network%20and%20navigating%20to%20a%20distinct%20RD%20trade-off.%0ASecond%2C%20to%20approach%20the%20target%20bitrate%2C%20the%20Rate%20Control%20Agent%20estimates%20the%0Abitrate%20of%20each%20route%20and%20adjusts%20the%20coding%20route%20of%20DRA%20at%20run%20time.%20To%0Aencompass%20a%20broad%20spectrum%20of%20variable%20bitrates%20while%20preserving%20overall%20RD%0Aperformance%2C%20we%20employ%20the%20Joint-Routes%20Optimization%20strategy%2C%20achieving%0Acollaborative%20training%20of%20various%20routes.%20Extensive%20experiments%20on%20the%20HEVC%20and%0AUVG%20datasets%20show%20that%20the%20proposed%20method%20achieves%20an%20average%20BD-Rate%0Areduction%20of%2014.8%25%20and%20BD-PSNR%20gain%20of%200.47dB%20over%20state-of-the-art%20methods%0Awhile%20maintaining%20an%20average%20bitrate%20error%20of%201.66%25%2C%20achieving%0ARate-Distortion-Complexity%20Optimization%20%28RDCO%29%20for%20various%20bitrate%20and%0Abitrate-constrained%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//git.openi.org.cn/OpenAICoding/DynamicDVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Rate%2520Control%2520for%2520Frame-Level%2520Adaptive%2520Neural%2520Video%2520Compression%250A%2520%2520via%2520Dynamic%2520Neural%2520Network%26entry.906535625%3DChenhao%2520Zhang%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520Neural%2520Video%2520Compression%2520%2528NVC%2529%2520has%2520achieved%2520remarkable%2520performance%2520in%2520recent%250Ayears.%2520However%252C%2520precise%2520rate%2520control%2520remains%2520a%2520challenge%2520due%2520to%2520the%2520inherent%250Alimitations%2520of%2520learning-based%2520codecs.%2520To%2520solve%2520this%2520issue%252C%2520we%2520propose%2520a%2520dynamic%250Avideo%2520compression%2520framework%2520designed%2520for%2520variable%2520bitrate%2520scenarios.%2520First%252C%2520to%250Aachieve%2520variable%2520bitrate%2520implementation%252C%2520we%2520propose%2520the%2520Dynamic-Route%250AAutoencoder%2520with%2520variable%2520coding%2520routes%252C%2520each%2520occupying%2520partial%2520computational%250Acomplexity%2520of%2520the%2520whole%2520network%2520and%2520navigating%2520to%2520a%2520distinct%2520RD%2520trade-off.%250ASecond%252C%2520to%2520approach%2520the%2520target%2520bitrate%252C%2520the%2520Rate%2520Control%2520Agent%2520estimates%2520the%250Abitrate%2520of%2520each%2520route%2520and%2520adjusts%2520the%2520coding%2520route%2520of%2520DRA%2520at%2520run%2520time.%2520To%250Aencompass%2520a%2520broad%2520spectrum%2520of%2520variable%2520bitrates%2520while%2520preserving%2520overall%2520RD%250Aperformance%252C%2520we%2520employ%2520the%2520Joint-Routes%2520Optimization%2520strategy%252C%2520achieving%250Acollaborative%2520training%2520of%2520various%2520routes.%2520Extensive%2520experiments%2520on%2520the%2520HEVC%2520and%250AUVG%2520datasets%2520show%2520that%2520the%2520proposed%2520method%2520achieves%2520an%2520average%2520BD-Rate%250Areduction%2520of%252014.8%2525%2520and%2520BD-PSNR%2520gain%2520of%25200.47dB%2520over%2520state-of-the-art%2520methods%250Awhile%2520maintaining%2520an%2520average%2520bitrate%2520error%2520of%25201.66%2525%252C%2520achieving%250ARate-Distortion-Complexity%2520Optimization%2520%2528RDCO%2529%2520for%2520various%2520bitrate%2520and%250Abitrate-constrained%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//git.openi.org.cn/OpenAICoding/DynamicDVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Rate%20Control%20for%20Frame-Level%20Adaptive%20Neural%20Video%20Compression%0A%20%20via%20Dynamic%20Neural%20Network&entry.906535625=Chenhao%20Zhang%20and%20Wei%20Gao&entry.1292438233=%20%20Neural%20Video%20Compression%20%28NVC%29%20has%20achieved%20remarkable%20performance%20in%20recent%0Ayears.%20However%2C%20precise%20rate%20control%20remains%20a%20challenge%20due%20to%20the%20inherent%0Alimitations%20of%20learning-based%20codecs.%20To%20solve%20this%20issue%2C%20we%20propose%20a%20dynamic%0Avideo%20compression%20framework%20designed%20for%20variable%20bitrate%20scenarios.%20First%2C%20to%0Aachieve%20variable%20bitrate%20implementation%2C%20we%20propose%20the%20Dynamic-Route%0AAutoencoder%20with%20variable%20coding%20routes%2C%20each%20occupying%20partial%20computational%0Acomplexity%20of%20the%20whole%20network%20and%20navigating%20to%20a%20distinct%20RD%20trade-off.%0ASecond%2C%20to%20approach%20the%20target%20bitrate%2C%20the%20Rate%20Control%20Agent%20estimates%20the%0Abitrate%20of%20each%20route%20and%20adjusts%20the%20coding%20route%20of%20DRA%20at%20run%20time.%20To%0Aencompass%20a%20broad%20spectrum%20of%20variable%20bitrates%20while%20preserving%20overall%20RD%0Aperformance%2C%20we%20employ%20the%20Joint-Routes%20Optimization%20strategy%2C%20achieving%0Acollaborative%20training%20of%20various%20routes.%20Extensive%20experiments%20on%20the%20HEVC%20and%0AUVG%20datasets%20show%20that%20the%20proposed%20method%20achieves%20an%20average%20BD-Rate%0Areduction%20of%2014.8%25%20and%20BD-PSNR%20gain%20of%200.47dB%20over%20state-of-the-art%20methods%0Awhile%20maintaining%20an%20average%20bitrate%20error%20of%201.66%25%2C%20achieving%0ARate-Distortion-Complexity%20Optimization%20%28RDCO%29%20for%20various%20bitrate%20and%0Abitrate-constrained%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//git.openi.org.cn/OpenAICoding/DynamicDVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20709v1&entry.124074799=Read"},
{"title": "E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with\n  Cross-Stage Partial Connections", "author": "Fang Wang and Huitao Li and Wenhan Chao and Zheng Zhuo and Yiran Ji and Chang Peng and Yupeng Sun", "abstract": "  Many high-performance networks were not designed with lightweight application\nscenarios in mind from the outset, which has greatly restricted their scope of\napplication. This paper takes ConvNeXt as the research object and significantly\nreduces the parameter scale and network complexity of ConvNeXt by integrating\nthe Cross Stage Partial Connections mechanism and a series of optimized\ndesigns. The new network is named E-ConvNeXt, which can maintain high accuracy\nperformance under different complexity configurations. The three core\ninnovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network\n(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the\nmodel's network complexity by up to 80%; (2) Optimizing the Stem and Block\nstructures to enhance the model's feature expression capability and operational\nefficiency; (3) Replacing Layer Scale with channel attention. Experimental\nvalidation on ImageNet classification demonstrates E-ConvNeXt's superior\naccuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at\n0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer\nlearning tests on object detection tasks further confirm its generalization\ncapability.\n", "link": "http://arxiv.org/abs/2508.20955v1", "date": "2025-08-28", "relevancy": 2.1756, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5886}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5438}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-ConvNeXt%3A%20A%20Lightweight%20and%20Efficient%20ConvNeXt%20Variant%20with%0A%20%20Cross-Stage%20Partial%20Connections&body=Title%3A%20E-ConvNeXt%3A%20A%20Lightweight%20and%20Efficient%20ConvNeXt%20Variant%20with%0A%20%20Cross-Stage%20Partial%20Connections%0AAuthor%3A%20Fang%20Wang%20and%20Huitao%20Li%20and%20Wenhan%20Chao%20and%20Zheng%20Zhuo%20and%20Yiran%20Ji%20and%20Chang%20Peng%20and%20Yupeng%20Sun%0AAbstract%3A%20%20%20Many%20high-performance%20networks%20were%20not%20designed%20with%20lightweight%20application%0Ascenarios%20in%20mind%20from%20the%20outset%2C%20which%20has%20greatly%20restricted%20their%20scope%20of%0Aapplication.%20This%20paper%20takes%20ConvNeXt%20as%20the%20research%20object%20and%20significantly%0Areduces%20the%20parameter%20scale%20and%20network%20complexity%20of%20ConvNeXt%20by%20integrating%0Athe%20Cross%20Stage%20Partial%20Connections%20mechanism%20and%20a%20series%20of%20optimized%0Adesigns.%20The%20new%20network%20is%20named%20E-ConvNeXt%2C%20which%20can%20maintain%20high%20accuracy%0Aperformance%20under%20different%20complexity%20configurations.%20The%20three%20core%0Ainnovations%20of%20E-ConvNeXt%20are%20%3A%20%281%29%20integrating%20the%20Cross%20Stage%20Partial%20Network%0A%28CSPNet%29%20with%20ConvNeXt%20and%20adjusting%20the%20network%20structure%2C%20which%20reduces%20the%0Amodel%27s%20network%20complexity%20by%20up%20to%2080%25%3B%20%282%29%20Optimizing%20the%20Stem%20and%20Block%0Astructures%20to%20enhance%20the%20model%27s%20feature%20expression%20capability%20and%20operational%0Aefficiency%3B%20%283%29%20Replacing%20Layer%20Scale%20with%20channel%20attention.%20Experimental%0Avalidation%20on%20ImageNet%20classification%20demonstrates%20E-ConvNeXt%27s%20superior%0Aaccuracy-efficiency%20balance%3A%20E-ConvNeXt-mini%20reaches%2078.3%25%20Top-1%20accuracy%20at%0A0.9GFLOPs.%20E-ConvNeXt-small%20reaches%2081.9%25%20Top-1%20accuracy%20at%203.1GFLOPs.%20Transfer%0Alearning%20tests%20on%20object%20detection%20tasks%20further%20confirm%20its%20generalization%0Acapability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-ConvNeXt%253A%2520A%2520Lightweight%2520and%2520Efficient%2520ConvNeXt%2520Variant%2520with%250A%2520%2520Cross-Stage%2520Partial%2520Connections%26entry.906535625%3DFang%2520Wang%2520and%2520Huitao%2520Li%2520and%2520Wenhan%2520Chao%2520and%2520Zheng%2520Zhuo%2520and%2520Yiran%2520Ji%2520and%2520Chang%2520Peng%2520and%2520Yupeng%2520Sun%26entry.1292438233%3D%2520%2520Many%2520high-performance%2520networks%2520were%2520not%2520designed%2520with%2520lightweight%2520application%250Ascenarios%2520in%2520mind%2520from%2520the%2520outset%252C%2520which%2520has%2520greatly%2520restricted%2520their%2520scope%2520of%250Aapplication.%2520This%2520paper%2520takes%2520ConvNeXt%2520as%2520the%2520research%2520object%2520and%2520significantly%250Areduces%2520the%2520parameter%2520scale%2520and%2520network%2520complexity%2520of%2520ConvNeXt%2520by%2520integrating%250Athe%2520Cross%2520Stage%2520Partial%2520Connections%2520mechanism%2520and%2520a%2520series%2520of%2520optimized%250Adesigns.%2520The%2520new%2520network%2520is%2520named%2520E-ConvNeXt%252C%2520which%2520can%2520maintain%2520high%2520accuracy%250Aperformance%2520under%2520different%2520complexity%2520configurations.%2520The%2520three%2520core%250Ainnovations%2520of%2520E-ConvNeXt%2520are%2520%253A%2520%25281%2529%2520integrating%2520the%2520Cross%2520Stage%2520Partial%2520Network%250A%2528CSPNet%2529%2520with%2520ConvNeXt%2520and%2520adjusting%2520the%2520network%2520structure%252C%2520which%2520reduces%2520the%250Amodel%2527s%2520network%2520complexity%2520by%2520up%2520to%252080%2525%253B%2520%25282%2529%2520Optimizing%2520the%2520Stem%2520and%2520Block%250Astructures%2520to%2520enhance%2520the%2520model%2527s%2520feature%2520expression%2520capability%2520and%2520operational%250Aefficiency%253B%2520%25283%2529%2520Replacing%2520Layer%2520Scale%2520with%2520channel%2520attention.%2520Experimental%250Avalidation%2520on%2520ImageNet%2520classification%2520demonstrates%2520E-ConvNeXt%2527s%2520superior%250Aaccuracy-efficiency%2520balance%253A%2520E-ConvNeXt-mini%2520reaches%252078.3%2525%2520Top-1%2520accuracy%2520at%250A0.9GFLOPs.%2520E-ConvNeXt-small%2520reaches%252081.9%2525%2520Top-1%2520accuracy%2520at%25203.1GFLOPs.%2520Transfer%250Alearning%2520tests%2520on%2520object%2520detection%2520tasks%2520further%2520confirm%2520its%2520generalization%250Acapability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-ConvNeXt%3A%20A%20Lightweight%20and%20Efficient%20ConvNeXt%20Variant%20with%0A%20%20Cross-Stage%20Partial%20Connections&entry.906535625=Fang%20Wang%20and%20Huitao%20Li%20and%20Wenhan%20Chao%20and%20Zheng%20Zhuo%20and%20Yiran%20Ji%20and%20Chang%20Peng%20and%20Yupeng%20Sun&entry.1292438233=%20%20Many%20high-performance%20networks%20were%20not%20designed%20with%20lightweight%20application%0Ascenarios%20in%20mind%20from%20the%20outset%2C%20which%20has%20greatly%20restricted%20their%20scope%20of%0Aapplication.%20This%20paper%20takes%20ConvNeXt%20as%20the%20research%20object%20and%20significantly%0Areduces%20the%20parameter%20scale%20and%20network%20complexity%20of%20ConvNeXt%20by%20integrating%0Athe%20Cross%20Stage%20Partial%20Connections%20mechanism%20and%20a%20series%20of%20optimized%0Adesigns.%20The%20new%20network%20is%20named%20E-ConvNeXt%2C%20which%20can%20maintain%20high%20accuracy%0Aperformance%20under%20different%20complexity%20configurations.%20The%20three%20core%0Ainnovations%20of%20E-ConvNeXt%20are%20%3A%20%281%29%20integrating%20the%20Cross%20Stage%20Partial%20Network%0A%28CSPNet%29%20with%20ConvNeXt%20and%20adjusting%20the%20network%20structure%2C%20which%20reduces%20the%0Amodel%27s%20network%20complexity%20by%20up%20to%2080%25%3B%20%282%29%20Optimizing%20the%20Stem%20and%20Block%0Astructures%20to%20enhance%20the%20model%27s%20feature%20expression%20capability%20and%20operational%0Aefficiency%3B%20%283%29%20Replacing%20Layer%20Scale%20with%20channel%20attention.%20Experimental%0Avalidation%20on%20ImageNet%20classification%20demonstrates%20E-ConvNeXt%27s%20superior%0Aaccuracy-efficiency%20balance%3A%20E-ConvNeXt-mini%20reaches%2078.3%25%20Top-1%20accuracy%20at%0A0.9GFLOPs.%20E-ConvNeXt-small%20reaches%2081.9%25%20Top-1%20accuracy%20at%203.1GFLOPs.%20Transfer%0Alearning%20tests%20on%20object%20detection%20tasks%20further%20confirm%20its%20generalization%0Acapability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20955v1&entry.124074799=Read"},
{"title": "Adapting Foundation Model for Dental Caries Detection with Dual-View\n  Co-Training", "author": "Tao Luo and Han Wu and Tong Yang and Dinggang Shen and Zhiming Cui", "abstract": "  Accurate dental caries detection from panoramic X-rays plays a pivotal role\nin preventing lesion progression. However, current detection methods often\nyield suboptimal accuracy due to subtle contrast variations and diverse lesion\nmorphology of dental caries. In this work, inspired by the clinical workflow\nwhere dentists systematically combine whole-image screening with detailed\ntooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training\nnetwork for accurate dental caries detection. Our DVCTNet starts with employing\nautomated tooth detection to establish two complementary views: a global view\nfrom panoramic X-ray images and a local view from cropped tooth images. We then\npretrain two vision foundation models separately on the two views. The\nglobal-view foundation model serves as the detection backbone, generating\nregion proposals and global features, while the local-view model extracts\ndetailed features from corresponding cropped tooth patches matched by the\nregion proposals. To effectively integrate information from both views, we\nintroduce a Gated Cross-View Attention (GCV-Atten) module that dynamically\nfuses dual-view features, enhancing the detection pipeline by integrating the\nfused features back into the detection model for final caries detection. To\nrigorously evaluate our DVCTNet, we test it on a public dataset and further\nvalidate its performance on a newly curated, high-precision dental caries\ndetection dataset, annotated using both intra-oral images and panoramic X-rays\nfor double verification. Experimental results demonstrate DVCTNet's superior\nperformance against existing state-of-the-art (SOTA) methods on both datasets,\nindicating the clinical applicability of our method. Our code and labeled\ndataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.\n", "link": "http://arxiv.org/abs/2508.20813v1", "date": "2025-08-28", "relevancy": 2.1682, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Foundation%20Model%20for%20Dental%20Caries%20Detection%20with%20Dual-View%0A%20%20Co-Training&body=Title%3A%20Adapting%20Foundation%20Model%20for%20Dental%20Caries%20Detection%20with%20Dual-View%0A%20%20Co-Training%0AAuthor%3A%20Tao%20Luo%20and%20Han%20Wu%20and%20Tong%20Yang%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui%0AAbstract%3A%20%20%20Accurate%20dental%20caries%20detection%20from%20panoramic%20X-rays%20plays%20a%20pivotal%20role%0Ain%20preventing%20lesion%20progression.%20However%2C%20current%20detection%20methods%20often%0Ayield%20suboptimal%20accuracy%20due%20to%20subtle%20contrast%20variations%20and%20diverse%20lesion%0Amorphology%20of%20dental%20caries.%20In%20this%20work%2C%20inspired%20by%20the%20clinical%20workflow%0Awhere%20dentists%20systematically%20combine%20whole-image%20screening%20with%20detailed%0Atooth-level%20inspection%2C%20we%20present%20DVCTNet%2C%20a%20novel%20Dual-View%20Co-Training%0Anetwork%20for%20accurate%20dental%20caries%20detection.%20Our%20DVCTNet%20starts%20with%20employing%0Aautomated%20tooth%20detection%20to%20establish%20two%20complementary%20views%3A%20a%20global%20view%0Afrom%20panoramic%20X-ray%20images%20and%20a%20local%20view%20from%20cropped%20tooth%20images.%20We%20then%0Apretrain%20two%20vision%20foundation%20models%20separately%20on%20the%20two%20views.%20The%0Aglobal-view%20foundation%20model%20serves%20as%20the%20detection%20backbone%2C%20generating%0Aregion%20proposals%20and%20global%20features%2C%20while%20the%20local-view%20model%20extracts%0Adetailed%20features%20from%20corresponding%20cropped%20tooth%20patches%20matched%20by%20the%0Aregion%20proposals.%20To%20effectively%20integrate%20information%20from%20both%20views%2C%20we%0Aintroduce%20a%20Gated%20Cross-View%20Attention%20%28GCV-Atten%29%20module%20that%20dynamically%0Afuses%20dual-view%20features%2C%20enhancing%20the%20detection%20pipeline%20by%20integrating%20the%0Afused%20features%20back%20into%20the%20detection%20model%20for%20final%20caries%20detection.%20To%0Arigorously%20evaluate%20our%20DVCTNet%2C%20we%20test%20it%20on%20a%20public%20dataset%20and%20further%0Avalidate%20its%20performance%20on%20a%20newly%20curated%2C%20high-precision%20dental%20caries%0Adetection%20dataset%2C%20annotated%20using%20both%20intra-oral%20images%20and%20panoramic%20X-rays%0Afor%20double%20verification.%20Experimental%20results%20demonstrate%20DVCTNet%27s%20superior%0Aperformance%20against%20existing%20state-of-the-art%20%28SOTA%29%20methods%20on%20both%20datasets%2C%0Aindicating%20the%20clinical%20applicability%20of%20our%20method.%20Our%20code%20and%20labeled%0Adataset%20are%20available%20at%20https%3A//github.com/ShanghaiTech-IMPACT/DVCTNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Foundation%2520Model%2520for%2520Dental%2520Caries%2520Detection%2520with%2520Dual-View%250A%2520%2520Co-Training%26entry.906535625%3DTao%2520Luo%2520and%2520Han%2520Wu%2520and%2520Tong%2520Yang%2520and%2520Dinggang%2520Shen%2520and%2520Zhiming%2520Cui%26entry.1292438233%3D%2520%2520Accurate%2520dental%2520caries%2520detection%2520from%2520panoramic%2520X-rays%2520plays%2520a%2520pivotal%2520role%250Ain%2520preventing%2520lesion%2520progression.%2520However%252C%2520current%2520detection%2520methods%2520often%250Ayield%2520suboptimal%2520accuracy%2520due%2520to%2520subtle%2520contrast%2520variations%2520and%2520diverse%2520lesion%250Amorphology%2520of%2520dental%2520caries.%2520In%2520this%2520work%252C%2520inspired%2520by%2520the%2520clinical%2520workflow%250Awhere%2520dentists%2520systematically%2520combine%2520whole-image%2520screening%2520with%2520detailed%250Atooth-level%2520inspection%252C%2520we%2520present%2520DVCTNet%252C%2520a%2520novel%2520Dual-View%2520Co-Training%250Anetwork%2520for%2520accurate%2520dental%2520caries%2520detection.%2520Our%2520DVCTNet%2520starts%2520with%2520employing%250Aautomated%2520tooth%2520detection%2520to%2520establish%2520two%2520complementary%2520views%253A%2520a%2520global%2520view%250Afrom%2520panoramic%2520X-ray%2520images%2520and%2520a%2520local%2520view%2520from%2520cropped%2520tooth%2520images.%2520We%2520then%250Apretrain%2520two%2520vision%2520foundation%2520models%2520separately%2520on%2520the%2520two%2520views.%2520The%250Aglobal-view%2520foundation%2520model%2520serves%2520as%2520the%2520detection%2520backbone%252C%2520generating%250Aregion%2520proposals%2520and%2520global%2520features%252C%2520while%2520the%2520local-view%2520model%2520extracts%250Adetailed%2520features%2520from%2520corresponding%2520cropped%2520tooth%2520patches%2520matched%2520by%2520the%250Aregion%2520proposals.%2520To%2520effectively%2520integrate%2520information%2520from%2520both%2520views%252C%2520we%250Aintroduce%2520a%2520Gated%2520Cross-View%2520Attention%2520%2528GCV-Atten%2529%2520module%2520that%2520dynamically%250Afuses%2520dual-view%2520features%252C%2520enhancing%2520the%2520detection%2520pipeline%2520by%2520integrating%2520the%250Afused%2520features%2520back%2520into%2520the%2520detection%2520model%2520for%2520final%2520caries%2520detection.%2520To%250Arigorously%2520evaluate%2520our%2520DVCTNet%252C%2520we%2520test%2520it%2520on%2520a%2520public%2520dataset%2520and%2520further%250Avalidate%2520its%2520performance%2520on%2520a%2520newly%2520curated%252C%2520high-precision%2520dental%2520caries%250Adetection%2520dataset%252C%2520annotated%2520using%2520both%2520intra-oral%2520images%2520and%2520panoramic%2520X-rays%250Afor%2520double%2520verification.%2520Experimental%2520results%2520demonstrate%2520DVCTNet%2527s%2520superior%250Aperformance%2520against%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520both%2520datasets%252C%250Aindicating%2520the%2520clinical%2520applicability%2520of%2520our%2520method.%2520Our%2520code%2520and%2520labeled%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/ShanghaiTech-IMPACT/DVCTNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Foundation%20Model%20for%20Dental%20Caries%20Detection%20with%20Dual-View%0A%20%20Co-Training&entry.906535625=Tao%20Luo%20and%20Han%20Wu%20and%20Tong%20Yang%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui&entry.1292438233=%20%20Accurate%20dental%20caries%20detection%20from%20panoramic%20X-rays%20plays%20a%20pivotal%20role%0Ain%20preventing%20lesion%20progression.%20However%2C%20current%20detection%20methods%20often%0Ayield%20suboptimal%20accuracy%20due%20to%20subtle%20contrast%20variations%20and%20diverse%20lesion%0Amorphology%20of%20dental%20caries.%20In%20this%20work%2C%20inspired%20by%20the%20clinical%20workflow%0Awhere%20dentists%20systematically%20combine%20whole-image%20screening%20with%20detailed%0Atooth-level%20inspection%2C%20we%20present%20DVCTNet%2C%20a%20novel%20Dual-View%20Co-Training%0Anetwork%20for%20accurate%20dental%20caries%20detection.%20Our%20DVCTNet%20starts%20with%20employing%0Aautomated%20tooth%20detection%20to%20establish%20two%20complementary%20views%3A%20a%20global%20view%0Afrom%20panoramic%20X-ray%20images%20and%20a%20local%20view%20from%20cropped%20tooth%20images.%20We%20then%0Apretrain%20two%20vision%20foundation%20models%20separately%20on%20the%20two%20views.%20The%0Aglobal-view%20foundation%20model%20serves%20as%20the%20detection%20backbone%2C%20generating%0Aregion%20proposals%20and%20global%20features%2C%20while%20the%20local-view%20model%20extracts%0Adetailed%20features%20from%20corresponding%20cropped%20tooth%20patches%20matched%20by%20the%0Aregion%20proposals.%20To%20effectively%20integrate%20information%20from%20both%20views%2C%20we%0Aintroduce%20a%20Gated%20Cross-View%20Attention%20%28GCV-Atten%29%20module%20that%20dynamically%0Afuses%20dual-view%20features%2C%20enhancing%20the%20detection%20pipeline%20by%20integrating%20the%0Afused%20features%20back%20into%20the%20detection%20model%20for%20final%20caries%20detection.%20To%0Arigorously%20evaluate%20our%20DVCTNet%2C%20we%20test%20it%20on%20a%20public%20dataset%20and%20further%0Avalidate%20its%20performance%20on%20a%20newly%20curated%2C%20high-precision%20dental%20caries%0Adetection%20dataset%2C%20annotated%20using%20both%20intra-oral%20images%20and%20panoramic%20X-rays%0Afor%20double%20verification.%20Experimental%20results%20demonstrate%20DVCTNet%27s%20superior%0Aperformance%20against%20existing%20state-of-the-art%20%28SOTA%29%20methods%20on%20both%20datasets%2C%0Aindicating%20the%20clinical%20applicability%20of%20our%20method.%20Our%20code%20and%20labeled%0Adataset%20are%20available%20at%20https%3A//github.com/ShanghaiTech-IMPACT/DVCTNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20813v1&entry.124074799=Read"},
{"title": "PathMR: Multimodal Visual Reasoning for Interpretable Pathology\n  Diagnosis", "author": "Ye Zhang and Yu Zhou and Jingwen Qi and Yongbing Zhang and Simon Puettmann and Finn Wichmann and Larissa Pereira Ferreira and Lara Sichward and Julius Keyl and Sylvia Hartmann and Shuo Zhao and Hongxiao Wang and Xiaowei Xu and Jianxu Chen", "abstract": "  Deep learning based automated pathological diagnosis has markedly improved\ndiagnostic efficiency and reduced variability between observers, yet its\nclinical adoption remains limited by opaque model decisions and a lack of\ntraceable rationale. To address this, recent multimodal visual reasoning\narchitectures provide a unified framework that generates segmentation masks at\nthe pixel level alongside semantically aligned textual explanations. By\nlocalizing lesion regions and producing expert style diagnostic narratives,\nthese models deliver the transparent and interpretable insights necessary for\ndependable AI assisted pathology. Building on these advancements, we propose\nPathMR, a cell-level Multimodal visual Reasoning framework for Pathological\nimage analysis. Given a pathological image and a textual query, PathMR\ngenerates expert-level diagnostic explanations while simultaneously predicting\ncell distribution patterns. To benchmark its performance, we evaluated our\napproach on the publicly available PathGen dataset as well as on our newly\ndeveloped GADVR dataset. Extensive experiments on these two datasets\ndemonstrate that PathMR consistently outperforms state-of-the-art visual\nreasoning methods in text generation quality, segmentation accuracy, and\ncross-modal alignment. These results highlight the potential of PathMR for\nimproving interpretability in AI-driven pathological diagnosis. The code will\nbe publicly available in https://github.com/zhangye-zoe/PathMR.\n", "link": "http://arxiv.org/abs/2508.20851v1", "date": "2025-08-28", "relevancy": 2.1626, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathMR%3A%20Multimodal%20Visual%20Reasoning%20for%20Interpretable%20Pathology%0A%20%20Diagnosis&body=Title%3A%20PathMR%3A%20Multimodal%20Visual%20Reasoning%20for%20Interpretable%20Pathology%0A%20%20Diagnosis%0AAuthor%3A%20Ye%20Zhang%20and%20Yu%20Zhou%20and%20Jingwen%20Qi%20and%20Yongbing%20Zhang%20and%20Simon%20Puettmann%20and%20Finn%20Wichmann%20and%20Larissa%20Pereira%20Ferreira%20and%20Lara%20Sichward%20and%20Julius%20Keyl%20and%20Sylvia%20Hartmann%20and%20Shuo%20Zhao%20and%20Hongxiao%20Wang%20and%20Xiaowei%20Xu%20and%20Jianxu%20Chen%0AAbstract%3A%20%20%20Deep%20learning%20based%20automated%20pathological%20diagnosis%20has%20markedly%20improved%0Adiagnostic%20efficiency%20and%20reduced%20variability%20between%20observers%2C%20yet%20its%0Aclinical%20adoption%20remains%20limited%20by%20opaque%20model%20decisions%20and%20a%20lack%20of%0Atraceable%20rationale.%20To%20address%20this%2C%20recent%20multimodal%20visual%20reasoning%0Aarchitectures%20provide%20a%20unified%20framework%20that%20generates%20segmentation%20masks%20at%0Athe%20pixel%20level%20alongside%20semantically%20aligned%20textual%20explanations.%20By%0Alocalizing%20lesion%20regions%20and%20producing%20expert%20style%20diagnostic%20narratives%2C%0Athese%20models%20deliver%20the%20transparent%20and%20interpretable%20insights%20necessary%20for%0Adependable%20AI%20assisted%20pathology.%20Building%20on%20these%20advancements%2C%20we%20propose%0APathMR%2C%20a%20cell-level%20Multimodal%20visual%20Reasoning%20framework%20for%20Pathological%0Aimage%20analysis.%20Given%20a%20pathological%20image%20and%20a%20textual%20query%2C%20PathMR%0Agenerates%20expert-level%20diagnostic%20explanations%20while%20simultaneously%20predicting%0Acell%20distribution%20patterns.%20To%20benchmark%20its%20performance%2C%20we%20evaluated%20our%0Aapproach%20on%20the%20publicly%20available%20PathGen%20dataset%20as%20well%20as%20on%20our%20newly%0Adeveloped%20GADVR%20dataset.%20Extensive%20experiments%20on%20these%20two%20datasets%0Ademonstrate%20that%20PathMR%20consistently%20outperforms%20state-of-the-art%20visual%0Areasoning%20methods%20in%20text%20generation%20quality%2C%20segmentation%20accuracy%2C%20and%0Across-modal%20alignment.%20These%20results%20highlight%20the%20potential%20of%20PathMR%20for%0Aimproving%20interpretability%20in%20AI-driven%20pathological%20diagnosis.%20The%20code%20will%0Abe%20publicly%20available%20in%20https%3A//github.com/zhangye-zoe/PathMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathMR%253A%2520Multimodal%2520Visual%2520Reasoning%2520for%2520Interpretable%2520Pathology%250A%2520%2520Diagnosis%26entry.906535625%3DYe%2520Zhang%2520and%2520Yu%2520Zhou%2520and%2520Jingwen%2520Qi%2520and%2520Yongbing%2520Zhang%2520and%2520Simon%2520Puettmann%2520and%2520Finn%2520Wichmann%2520and%2520Larissa%2520Pereira%2520Ferreira%2520and%2520Lara%2520Sichward%2520and%2520Julius%2520Keyl%2520and%2520Sylvia%2520Hartmann%2520and%2520Shuo%2520Zhao%2520and%2520Hongxiao%2520Wang%2520and%2520Xiaowei%2520Xu%2520and%2520Jianxu%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520learning%2520based%2520automated%2520pathological%2520diagnosis%2520has%2520markedly%2520improved%250Adiagnostic%2520efficiency%2520and%2520reduced%2520variability%2520between%2520observers%252C%2520yet%2520its%250Aclinical%2520adoption%2520remains%2520limited%2520by%2520opaque%2520model%2520decisions%2520and%2520a%2520lack%2520of%250Atraceable%2520rationale.%2520To%2520address%2520this%252C%2520recent%2520multimodal%2520visual%2520reasoning%250Aarchitectures%2520provide%2520a%2520unified%2520framework%2520that%2520generates%2520segmentation%2520masks%2520at%250Athe%2520pixel%2520level%2520alongside%2520semantically%2520aligned%2520textual%2520explanations.%2520By%250Alocalizing%2520lesion%2520regions%2520and%2520producing%2520expert%2520style%2520diagnostic%2520narratives%252C%250Athese%2520models%2520deliver%2520the%2520transparent%2520and%2520interpretable%2520insights%2520necessary%2520for%250Adependable%2520AI%2520assisted%2520pathology.%2520Building%2520on%2520these%2520advancements%252C%2520we%2520propose%250APathMR%252C%2520a%2520cell-level%2520Multimodal%2520visual%2520Reasoning%2520framework%2520for%2520Pathological%250Aimage%2520analysis.%2520Given%2520a%2520pathological%2520image%2520and%2520a%2520textual%2520query%252C%2520PathMR%250Agenerates%2520expert-level%2520diagnostic%2520explanations%2520while%2520simultaneously%2520predicting%250Acell%2520distribution%2520patterns.%2520To%2520benchmark%2520its%2520performance%252C%2520we%2520evaluated%2520our%250Aapproach%2520on%2520the%2520publicly%2520available%2520PathGen%2520dataset%2520as%2520well%2520as%2520on%2520our%2520newly%250Adeveloped%2520GADVR%2520dataset.%2520Extensive%2520experiments%2520on%2520these%2520two%2520datasets%250Ademonstrate%2520that%2520PathMR%2520consistently%2520outperforms%2520state-of-the-art%2520visual%250Areasoning%2520methods%2520in%2520text%2520generation%2520quality%252C%2520segmentation%2520accuracy%252C%2520and%250Across-modal%2520alignment.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520PathMR%2520for%250Aimproving%2520interpretability%2520in%2520AI-driven%2520pathological%2520diagnosis.%2520The%2520code%2520will%250Abe%2520publicly%2520available%2520in%2520https%253A//github.com/zhangye-zoe/PathMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathMR%3A%20Multimodal%20Visual%20Reasoning%20for%20Interpretable%20Pathology%0A%20%20Diagnosis&entry.906535625=Ye%20Zhang%20and%20Yu%20Zhou%20and%20Jingwen%20Qi%20and%20Yongbing%20Zhang%20and%20Simon%20Puettmann%20and%20Finn%20Wichmann%20and%20Larissa%20Pereira%20Ferreira%20and%20Lara%20Sichward%20and%20Julius%20Keyl%20and%20Sylvia%20Hartmann%20and%20Shuo%20Zhao%20and%20Hongxiao%20Wang%20and%20Xiaowei%20Xu%20and%20Jianxu%20Chen&entry.1292438233=%20%20Deep%20learning%20based%20automated%20pathological%20diagnosis%20has%20markedly%20improved%0Adiagnostic%20efficiency%20and%20reduced%20variability%20between%20observers%2C%20yet%20its%0Aclinical%20adoption%20remains%20limited%20by%20opaque%20model%20decisions%20and%20a%20lack%20of%0Atraceable%20rationale.%20To%20address%20this%2C%20recent%20multimodal%20visual%20reasoning%0Aarchitectures%20provide%20a%20unified%20framework%20that%20generates%20segmentation%20masks%20at%0Athe%20pixel%20level%20alongside%20semantically%20aligned%20textual%20explanations.%20By%0Alocalizing%20lesion%20regions%20and%20producing%20expert%20style%20diagnostic%20narratives%2C%0Athese%20models%20deliver%20the%20transparent%20and%20interpretable%20insights%20necessary%20for%0Adependable%20AI%20assisted%20pathology.%20Building%20on%20these%20advancements%2C%20we%20propose%0APathMR%2C%20a%20cell-level%20Multimodal%20visual%20Reasoning%20framework%20for%20Pathological%0Aimage%20analysis.%20Given%20a%20pathological%20image%20and%20a%20textual%20query%2C%20PathMR%0Agenerates%20expert-level%20diagnostic%20explanations%20while%20simultaneously%20predicting%0Acell%20distribution%20patterns.%20To%20benchmark%20its%20performance%2C%20we%20evaluated%20our%0Aapproach%20on%20the%20publicly%20available%20PathGen%20dataset%20as%20well%20as%20on%20our%20newly%0Adeveloped%20GADVR%20dataset.%20Extensive%20experiments%20on%20these%20two%20datasets%0Ademonstrate%20that%20PathMR%20consistently%20outperforms%20state-of-the-art%20visual%0Areasoning%20methods%20in%20text%20generation%20quality%2C%20segmentation%20accuracy%2C%20and%0Across-modal%20alignment.%20These%20results%20highlight%20the%20potential%20of%20PathMR%20for%0Aimproving%20interpretability%20in%20AI-driven%20pathological%20diagnosis.%20The%20code%20will%0Abe%20publicly%20available%20in%20https%3A//github.com/zhangye-zoe/PathMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20851v1&entry.124074799=Read"},
{"title": "ODES: Domain Adaptation with Expert Guidance for Online Medical Image\n  Segmentation", "author": "Md Shazid Islam and Sayak Nag and Arindam Dutta and Miraj Ahmed and Fahim Faisal Niloy and Shreyangshu Bera and Amit K. Roy-Chowdhury", "abstract": "  Unsupervised domain adaptive segmentation typically relies on self-training\nusing pseudo labels predicted by a pre-trained network on an unlabeled target\ndataset. However, the noisy nature of such pseudo-labels presents a major\nbottleneck in adapting a network to the distribution shift between source and\ntarget datasets. This challenge is exaggerated when the network encounters an\nincoming data stream in online fashion, where the network is constrained to\nadapt to incoming streams of target domain data in exactly one round of forward\nand backward passes. In this scenario, relying solely on inaccurate\npseudo-labels can lead to low-quality segmentation, which is detrimental to\nmedical image analysis where accuracy and precision are of utmost priority. We\nhypothesize that a small amount of pixel-level annotation obtained from an\nexpert can address this problem, thereby enhancing the performance of domain\nadaptation of online streaming data, even in the absence of dedicated training\ndata. We call our method ODES: Domain Adaptation with Expert Guidance for\nOnline Medical Image Segmentation that adapts to each incoming data batch in an\nonline setup, incorporating feedback from an expert through active learning.\nThrough active learning, the most informative pixels in each image can be\nselected for expert annotation. However, the acquisition of pixel-level\nannotations across all images in a batch often leads to redundant information\nwhile increasing temporal overhead in online learning. To reduce the annotation\nacquisition time and make the adaptation process more online-friendly, we\nfurther propose a novel image-pruning strategy that selects the most useful\nsubset of images from the current batch for active learning. Our proposed\napproach outperforms existing online adaptation approaches and produces\ncompetitive results compared to offline domain adaptive active learning\nmethods.\n", "link": "http://arxiv.org/abs/2312.05407v4", "date": "2025-08-28", "relevancy": 2.1511, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Md%20Shazid%20Islam%20and%20Sayak%20Nag%20and%20Arindam%20Dutta%20and%20Miraj%20Ahmed%20and%20Fahim%20Faisal%20Niloy%20and%20Shreyangshu%20Bera%20and%20Amit%20K.%20Roy-Chowdhury%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptive%20segmentation%20typically%20relies%20on%20self-training%0Ausing%20pseudo%20labels%20predicted%20by%20a%20pre-trained%20network%20on%20an%20unlabeled%20target%0Adataset.%20However%2C%20the%20noisy%20nature%20of%20such%20pseudo-labels%20presents%20a%20major%0Abottleneck%20in%20adapting%20a%20network%20to%20the%20distribution%20shift%20between%20source%20and%0Atarget%20datasets.%20This%20challenge%20is%20exaggerated%20when%20the%20network%20encounters%20an%0Aincoming%20data%20stream%20in%20online%20fashion%2C%20where%20the%20network%20is%20constrained%20to%0Aadapt%20to%20incoming%20streams%20of%20target%20domain%20data%20in%20exactly%20one%20round%20of%20forward%0Aand%20backward%20passes.%20In%20this%20scenario%2C%20relying%20solely%20on%20inaccurate%0Apseudo-labels%20can%20lead%20to%20low-quality%20segmentation%2C%20which%20is%20detrimental%20to%0Amedical%20image%20analysis%20where%20accuracy%20and%20precision%20are%20of%20utmost%20priority.%20We%0Ahypothesize%20that%20a%20small%20amount%20of%20pixel-level%20annotation%20obtained%20from%20an%0Aexpert%20can%20address%20this%20problem%2C%20thereby%20enhancing%20the%20performance%20of%20domain%0Aadaptation%20of%20online%20streaming%20data%2C%20even%20in%20the%20absence%20of%20dedicated%20training%0Adata.%20We%20call%20our%20method%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%0AOnline%20Medical%20Image%20Segmentation%20that%20adapts%20to%20each%20incoming%20data%20batch%20in%20an%0Aonline%20setup%2C%20incorporating%20feedback%20from%20an%20expert%20through%20active%20learning.%0AThrough%20active%20learning%2C%20the%20most%20informative%20pixels%20in%20each%20image%20can%20be%0Aselected%20for%20expert%20annotation.%20However%2C%20the%20acquisition%20of%20pixel-level%0Aannotations%20across%20all%20images%20in%20a%20batch%20often%20leads%20to%20redundant%20information%0Awhile%20increasing%20temporal%20overhead%20in%20online%20learning.%20To%20reduce%20the%20annotation%0Aacquisition%20time%20and%20make%20the%20adaptation%20process%20more%20online-friendly%2C%20we%0Afurther%20propose%20a%20novel%20image-pruning%20strategy%20that%20selects%20the%20most%20useful%0Asubset%20of%20images%20from%20the%20current%20batch%20for%20active%20learning.%20Our%20proposed%0Aapproach%20outperforms%20existing%20online%20adaptation%20approaches%20and%20produces%0Acompetitive%20results%20compared%20to%20offline%20domain%20adaptive%20active%20learning%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05407v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODES%253A%2520Domain%2520Adaptation%2520with%2520Expert%2520Guidance%2520for%2520Online%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DMd%2520Shazid%2520Islam%2520and%2520Sayak%2520Nag%2520and%2520Arindam%2520Dutta%2520and%2520Miraj%2520Ahmed%2520and%2520Fahim%2520Faisal%2520Niloy%2520and%2520Shreyangshu%2520Bera%2520and%2520Amit%2520K.%2520Roy-Chowdhury%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptive%2520segmentation%2520typically%2520relies%2520on%2520self-training%250Ausing%2520pseudo%2520labels%2520predicted%2520by%2520a%2520pre-trained%2520network%2520on%2520an%2520unlabeled%2520target%250Adataset.%2520However%252C%2520the%2520noisy%2520nature%2520of%2520such%2520pseudo-labels%2520presents%2520a%2520major%250Abottleneck%2520in%2520adapting%2520a%2520network%2520to%2520the%2520distribution%2520shift%2520between%2520source%2520and%250Atarget%2520datasets.%2520This%2520challenge%2520is%2520exaggerated%2520when%2520the%2520network%2520encounters%2520an%250Aincoming%2520data%2520stream%2520in%2520online%2520fashion%252C%2520where%2520the%2520network%2520is%2520constrained%2520to%250Aadapt%2520to%2520incoming%2520streams%2520of%2520target%2520domain%2520data%2520in%2520exactly%2520one%2520round%2520of%2520forward%250Aand%2520backward%2520passes.%2520In%2520this%2520scenario%252C%2520relying%2520solely%2520on%2520inaccurate%250Apseudo-labels%2520can%2520lead%2520to%2520low-quality%2520segmentation%252C%2520which%2520is%2520detrimental%2520to%250Amedical%2520image%2520analysis%2520where%2520accuracy%2520and%2520precision%2520are%2520of%2520utmost%2520priority.%2520We%250Ahypothesize%2520that%2520a%2520small%2520amount%2520of%2520pixel-level%2520annotation%2520obtained%2520from%2520an%250Aexpert%2520can%2520address%2520this%2520problem%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%2520domain%250Aadaptation%2520of%2520online%2520streaming%2520data%252C%2520even%2520in%2520the%2520absence%2520of%2520dedicated%2520training%250Adata.%2520We%2520call%2520our%2520method%2520ODES%253A%2520Domain%2520Adaptation%2520with%2520Expert%2520Guidance%2520for%250AOnline%2520Medical%2520Image%2520Segmentation%2520that%2520adapts%2520to%2520each%2520incoming%2520data%2520batch%2520in%2520an%250Aonline%2520setup%252C%2520incorporating%2520feedback%2520from%2520an%2520expert%2520through%2520active%2520learning.%250AThrough%2520active%2520learning%252C%2520the%2520most%2520informative%2520pixels%2520in%2520each%2520image%2520can%2520be%250Aselected%2520for%2520expert%2520annotation.%2520However%252C%2520the%2520acquisition%2520of%2520pixel-level%250Aannotations%2520across%2520all%2520images%2520in%2520a%2520batch%2520often%2520leads%2520to%2520redundant%2520information%250Awhile%2520increasing%2520temporal%2520overhead%2520in%2520online%2520learning.%2520To%2520reduce%2520the%2520annotation%250Aacquisition%2520time%2520and%2520make%2520the%2520adaptation%2520process%2520more%2520online-friendly%252C%2520we%250Afurther%2520propose%2520a%2520novel%2520image-pruning%2520strategy%2520that%2520selects%2520the%2520most%2520useful%250Asubset%2520of%2520images%2520from%2520the%2520current%2520batch%2520for%2520active%2520learning.%2520Our%2520proposed%250Aapproach%2520outperforms%2520existing%2520online%2520adaptation%2520approaches%2520and%2520produces%250Acompetitive%2520results%2520compared%2520to%2520offline%2520domain%2520adaptive%2520active%2520learning%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05407v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Md%20Shazid%20Islam%20and%20Sayak%20Nag%20and%20Arindam%20Dutta%20and%20Miraj%20Ahmed%20and%20Fahim%20Faisal%20Niloy%20and%20Shreyangshu%20Bera%20and%20Amit%20K.%20Roy-Chowdhury&entry.1292438233=%20%20Unsupervised%20domain%20adaptive%20segmentation%20typically%20relies%20on%20self-training%0Ausing%20pseudo%20labels%20predicted%20by%20a%20pre-trained%20network%20on%20an%20unlabeled%20target%0Adataset.%20However%2C%20the%20noisy%20nature%20of%20such%20pseudo-labels%20presents%20a%20major%0Abottleneck%20in%20adapting%20a%20network%20to%20the%20distribution%20shift%20between%20source%20and%0Atarget%20datasets.%20This%20challenge%20is%20exaggerated%20when%20the%20network%20encounters%20an%0Aincoming%20data%20stream%20in%20online%20fashion%2C%20where%20the%20network%20is%20constrained%20to%0Aadapt%20to%20incoming%20streams%20of%20target%20domain%20data%20in%20exactly%20one%20round%20of%20forward%0Aand%20backward%20passes.%20In%20this%20scenario%2C%20relying%20solely%20on%20inaccurate%0Apseudo-labels%20can%20lead%20to%20low-quality%20segmentation%2C%20which%20is%20detrimental%20to%0Amedical%20image%20analysis%20where%20accuracy%20and%20precision%20are%20of%20utmost%20priority.%20We%0Ahypothesize%20that%20a%20small%20amount%20of%20pixel-level%20annotation%20obtained%20from%20an%0Aexpert%20can%20address%20this%20problem%2C%20thereby%20enhancing%20the%20performance%20of%20domain%0Aadaptation%20of%20online%20streaming%20data%2C%20even%20in%20the%20absence%20of%20dedicated%20training%0Adata.%20We%20call%20our%20method%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%0AOnline%20Medical%20Image%20Segmentation%20that%20adapts%20to%20each%20incoming%20data%20batch%20in%20an%0Aonline%20setup%2C%20incorporating%20feedback%20from%20an%20expert%20through%20active%20learning.%0AThrough%20active%20learning%2C%20the%20most%20informative%20pixels%20in%20each%20image%20can%20be%0Aselected%20for%20expert%20annotation.%20However%2C%20the%20acquisition%20of%20pixel-level%0Aannotations%20across%20all%20images%20in%20a%20batch%20often%20leads%20to%20redundant%20information%0Awhile%20increasing%20temporal%20overhead%20in%20online%20learning.%20To%20reduce%20the%20annotation%0Aacquisition%20time%20and%20make%20the%20adaptation%20process%20more%20online-friendly%2C%20we%0Afurther%20propose%20a%20novel%20image-pruning%20strategy%20that%20selects%20the%20most%20useful%0Asubset%20of%20images%20from%20the%20current%20batch%20for%20active%20learning.%20Our%20proposed%0Aapproach%20outperforms%20existing%20online%20adaptation%20approaches%20and%20produces%0Acompetitive%20results%20compared%20to%20offline%20domain%20adaptive%20active%20learning%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05407v4&entry.124074799=Read"},
{"title": "GPT-FT: An Efficient Automated Feature Transformation Using GPT for\n  Sequence Reconstruction and Performance Enhancement", "author": "Yang Gao and Dongjie Wang and Scott Piersall and Ye Zhang and Liqiang Wang", "abstract": "  Feature transformation plays a critical role in enhancing machine learning\nmodel performance by optimizing data representations. Recent state-of-the-art\napproaches address this task as a continuous embedding optimization problem,\nconverting discrete search into a learnable process. Although effective, these\nmethods often rely on sequential encoder-decoder structures that cause high\ncomputational costs and parameter requirements, limiting scalability and\nefficiency. To address these limitations, we propose a novel framework that\naccomplishes automated feature transformation through four steps:\ntransformation records collection, embedding space construction with a revised\nGenerative Pre-trained Transformer (GPT) model, gradient-ascent search, and\nautoregressive reconstruction. In our approach, the revised GPT model serves\ntwo primary functions: (a) feature transformation sequence reconstruction and\n(b) model performance estimation and enhancement for downstream tasks by\nconstructing the embedding space. Such a multi-objective optimization framework\nreduces parameter size and accelerates transformation processes. Experimental\nresults on benchmark datasets show that the proposed framework matches or\nexceeds baseline performance, with significant gains in computational\nefficiency. This work highlights the potential of transformer-based\narchitectures for scalable, high-performance automated feature transformation.\n", "link": "http://arxiv.org/abs/2508.20824v1", "date": "2025-08-28", "relevancy": 2.136, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5735}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5385}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT-FT%3A%20An%20Efficient%20Automated%20Feature%20Transformation%20Using%20GPT%20for%0A%20%20Sequence%20Reconstruction%20and%20Performance%20Enhancement&body=Title%3A%20GPT-FT%3A%20An%20Efficient%20Automated%20Feature%20Transformation%20Using%20GPT%20for%0A%20%20Sequence%20Reconstruction%20and%20Performance%20Enhancement%0AAuthor%3A%20Yang%20Gao%20and%20Dongjie%20Wang%20and%20Scott%20Piersall%20and%20Ye%20Zhang%20and%20Liqiang%20Wang%0AAbstract%3A%20%20%20Feature%20transformation%20plays%20a%20critical%20role%20in%20enhancing%20machine%20learning%0Amodel%20performance%20by%20optimizing%20data%20representations.%20Recent%20state-of-the-art%0Aapproaches%20address%20this%20task%20as%20a%20continuous%20embedding%20optimization%20problem%2C%0Aconverting%20discrete%20search%20into%20a%20learnable%20process.%20Although%20effective%2C%20these%0Amethods%20often%20rely%20on%20sequential%20encoder-decoder%20structures%20that%20cause%20high%0Acomputational%20costs%20and%20parameter%20requirements%2C%20limiting%20scalability%20and%0Aefficiency.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20that%0Aaccomplishes%20automated%20feature%20transformation%20through%20four%20steps%3A%0Atransformation%20records%20collection%2C%20embedding%20space%20construction%20with%20a%20revised%0AGenerative%20Pre-trained%20Transformer%20%28GPT%29%20model%2C%20gradient-ascent%20search%2C%20and%0Aautoregressive%20reconstruction.%20In%20our%20approach%2C%20the%20revised%20GPT%20model%20serves%0Atwo%20primary%20functions%3A%20%28a%29%20feature%20transformation%20sequence%20reconstruction%20and%0A%28b%29%20model%20performance%20estimation%20and%20enhancement%20for%20downstream%20tasks%20by%0Aconstructing%20the%20embedding%20space.%20Such%20a%20multi-objective%20optimization%20framework%0Areduces%20parameter%20size%20and%20accelerates%20transformation%20processes.%20Experimental%0Aresults%20on%20benchmark%20datasets%20show%20that%20the%20proposed%20framework%20matches%20or%0Aexceeds%20baseline%20performance%2C%20with%20significant%20gains%20in%20computational%0Aefficiency.%20This%20work%20highlights%20the%20potential%20of%20transformer-based%0Aarchitectures%20for%20scalable%2C%20high-performance%20automated%20feature%20transformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT-FT%253A%2520An%2520Efficient%2520Automated%2520Feature%2520Transformation%2520Using%2520GPT%2520for%250A%2520%2520Sequence%2520Reconstruction%2520and%2520Performance%2520Enhancement%26entry.906535625%3DYang%2520Gao%2520and%2520Dongjie%2520Wang%2520and%2520Scott%2520Piersall%2520and%2520Ye%2520Zhang%2520and%2520Liqiang%2520Wang%26entry.1292438233%3D%2520%2520Feature%2520transformation%2520plays%2520a%2520critical%2520role%2520in%2520enhancing%2520machine%2520learning%250Amodel%2520performance%2520by%2520optimizing%2520data%2520representations.%2520Recent%2520state-of-the-art%250Aapproaches%2520address%2520this%2520task%2520as%2520a%2520continuous%2520embedding%2520optimization%2520problem%252C%250Aconverting%2520discrete%2520search%2520into%2520a%2520learnable%2520process.%2520Although%2520effective%252C%2520these%250Amethods%2520often%2520rely%2520on%2520sequential%2520encoder-decoder%2520structures%2520that%2520cause%2520high%250Acomputational%2520costs%2520and%2520parameter%2520requirements%252C%2520limiting%2520scalability%2520and%250Aefficiency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%250Aaccomplishes%2520automated%2520feature%2520transformation%2520through%2520four%2520steps%253A%250Atransformation%2520records%2520collection%252C%2520embedding%2520space%2520construction%2520with%2520a%2520revised%250AGenerative%2520Pre-trained%2520Transformer%2520%2528GPT%2529%2520model%252C%2520gradient-ascent%2520search%252C%2520and%250Aautoregressive%2520reconstruction.%2520In%2520our%2520approach%252C%2520the%2520revised%2520GPT%2520model%2520serves%250Atwo%2520primary%2520functions%253A%2520%2528a%2529%2520feature%2520transformation%2520sequence%2520reconstruction%2520and%250A%2528b%2529%2520model%2520performance%2520estimation%2520and%2520enhancement%2520for%2520downstream%2520tasks%2520by%250Aconstructing%2520the%2520embedding%2520space.%2520Such%2520a%2520multi-objective%2520optimization%2520framework%250Areduces%2520parameter%2520size%2520and%2520accelerates%2520transformation%2520processes.%2520Experimental%250Aresults%2520on%2520benchmark%2520datasets%2520show%2520that%2520the%2520proposed%2520framework%2520matches%2520or%250Aexceeds%2520baseline%2520performance%252C%2520with%2520significant%2520gains%2520in%2520computational%250Aefficiency.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520transformer-based%250Aarchitectures%2520for%2520scalable%252C%2520high-performance%2520automated%2520feature%2520transformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT-FT%3A%20An%20Efficient%20Automated%20Feature%20Transformation%20Using%20GPT%20for%0A%20%20Sequence%20Reconstruction%20and%20Performance%20Enhancement&entry.906535625=Yang%20Gao%20and%20Dongjie%20Wang%20and%20Scott%20Piersall%20and%20Ye%20Zhang%20and%20Liqiang%20Wang&entry.1292438233=%20%20Feature%20transformation%20plays%20a%20critical%20role%20in%20enhancing%20machine%20learning%0Amodel%20performance%20by%20optimizing%20data%20representations.%20Recent%20state-of-the-art%0Aapproaches%20address%20this%20task%20as%20a%20continuous%20embedding%20optimization%20problem%2C%0Aconverting%20discrete%20search%20into%20a%20learnable%20process.%20Although%20effective%2C%20these%0Amethods%20often%20rely%20on%20sequential%20encoder-decoder%20structures%20that%20cause%20high%0Acomputational%20costs%20and%20parameter%20requirements%2C%20limiting%20scalability%20and%0Aefficiency.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20that%0Aaccomplishes%20automated%20feature%20transformation%20through%20four%20steps%3A%0Atransformation%20records%20collection%2C%20embedding%20space%20construction%20with%20a%20revised%0AGenerative%20Pre-trained%20Transformer%20%28GPT%29%20model%2C%20gradient-ascent%20search%2C%20and%0Aautoregressive%20reconstruction.%20In%20our%20approach%2C%20the%20revised%20GPT%20model%20serves%0Atwo%20primary%20functions%3A%20%28a%29%20feature%20transformation%20sequence%20reconstruction%20and%0A%28b%29%20model%20performance%20estimation%20and%20enhancement%20for%20downstream%20tasks%20by%0Aconstructing%20the%20embedding%20space.%20Such%20a%20multi-objective%20optimization%20framework%0Areduces%20parameter%20size%20and%20accelerates%20transformation%20processes.%20Experimental%0Aresults%20on%20benchmark%20datasets%20show%20that%20the%20proposed%20framework%20matches%20or%0Aexceeds%20baseline%20performance%2C%20with%20significant%20gains%20in%20computational%0Aefficiency.%20This%20work%20highlights%20the%20potential%20of%20transformer-based%0Aarchitectures%20for%20scalable%2C%20high-performance%20automated%20feature%20transformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20824v1&entry.124074799=Read"},
{"title": "Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models\n  with Low-Rank Adaptation", "author": "Krit Duangprom and Tryphon Lambrou and Binod Bhattarai", "abstract": "  This paper presents a novel pipeline for 2D keypoint estima- tion of surgical\ntools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank\nadjusting (LoRA) technique. Unlike traditional Convolutional Neural Network\n(CNN) or Transformer-based approaches, which often suffer from overfitting in\nsmall-scale medical datasets, our method harnesses the generalization\ncapabilities of pre-trained VLMs. We carefully design prompts to create an\ninstruction-tuning dataset and use them to align visual features with semantic\nkeypoint descriptions. Experimental results show that with only two epochs of\nfine tuning, the adapted VLM outperforms the baseline models, demonstrating the\nef- fectiveness of LoRA in low-resource scenarios. This approach not only\nimproves keypoint detection performance, but also paves the way for future work\nin 3D surgical hands and tools pose estimation.\n", "link": "http://arxiv.org/abs/2508.20830v1", "date": "2025-08-28", "relevancy": 2.1248, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5457}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%202D%20Keypoints%20of%20Surgical%20Tools%20Using%20Vision-Language%20Models%0A%20%20with%20Low-Rank%20Adaptation&body=Title%3A%20Estimating%202D%20Keypoints%20of%20Surgical%20Tools%20Using%20Vision-Language%20Models%0A%20%20with%20Low-Rank%20Adaptation%0AAuthor%3A%20Krit%20Duangprom%20and%20Tryphon%20Lambrou%20and%20Binod%20Bhattarai%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20pipeline%20for%202D%20keypoint%20estima-%20tion%20of%20surgical%0Atools%20by%20leveraging%20Vision%20Language%20Models%20%28VLMs%29%20fine-%20tuned%20using%20a%20low%20rank%0Aadjusting%20%28LoRA%29%20technique.%20Unlike%20traditional%20Convolutional%20Neural%20Network%0A%28CNN%29%20or%20Transformer-based%20approaches%2C%20which%20often%20suffer%20from%20overfitting%20in%0Asmall-scale%20medical%20datasets%2C%20our%20method%20harnesses%20the%20generalization%0Acapabilities%20of%20pre-trained%20VLMs.%20We%20carefully%20design%20prompts%20to%20create%20an%0Ainstruction-tuning%20dataset%20and%20use%20them%20to%20align%20visual%20features%20with%20semantic%0Akeypoint%20descriptions.%20Experimental%20results%20show%20that%20with%20only%20two%20epochs%20of%0Afine%20tuning%2C%20the%20adapted%20VLM%20outperforms%20the%20baseline%20models%2C%20demonstrating%20the%0Aef-%20fectiveness%20of%20LoRA%20in%20low-resource%20scenarios.%20This%20approach%20not%20only%0Aimproves%20keypoint%20detection%20performance%2C%20but%20also%20paves%20the%20way%20for%20future%20work%0Ain%203D%20surgical%20hands%20and%20tools%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%25202D%2520Keypoints%2520of%2520Surgical%2520Tools%2520Using%2520Vision-Language%2520Models%250A%2520%2520with%2520Low-Rank%2520Adaptation%26entry.906535625%3DKrit%2520Duangprom%2520and%2520Tryphon%2520Lambrou%2520and%2520Binod%2520Bhattarai%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520pipeline%2520for%25202D%2520keypoint%2520estima-%2520tion%2520of%2520surgical%250Atools%2520by%2520leveraging%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520fine-%2520tuned%2520using%2520a%2520low%2520rank%250Aadjusting%2520%2528LoRA%2529%2520technique.%2520Unlike%2520traditional%2520Convolutional%2520Neural%2520Network%250A%2528CNN%2529%2520or%2520Transformer-based%2520approaches%252C%2520which%2520often%2520suffer%2520from%2520overfitting%2520in%250Asmall-scale%2520medical%2520datasets%252C%2520our%2520method%2520harnesses%2520the%2520generalization%250Acapabilities%2520of%2520pre-trained%2520VLMs.%2520We%2520carefully%2520design%2520prompts%2520to%2520create%2520an%250Ainstruction-tuning%2520dataset%2520and%2520use%2520them%2520to%2520align%2520visual%2520features%2520with%2520semantic%250Akeypoint%2520descriptions.%2520Experimental%2520results%2520show%2520that%2520with%2520only%2520two%2520epochs%2520of%250Afine%2520tuning%252C%2520the%2520adapted%2520VLM%2520outperforms%2520the%2520baseline%2520models%252C%2520demonstrating%2520the%250Aef-%2520fectiveness%2520of%2520LoRA%2520in%2520low-resource%2520scenarios.%2520This%2520approach%2520not%2520only%250Aimproves%2520keypoint%2520detection%2520performance%252C%2520but%2520also%2520paves%2520the%2520way%2520for%2520future%2520work%250Ain%25203D%2520surgical%2520hands%2520and%2520tools%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%202D%20Keypoints%20of%20Surgical%20Tools%20Using%20Vision-Language%20Models%0A%20%20with%20Low-Rank%20Adaptation&entry.906535625=Krit%20Duangprom%20and%20Tryphon%20Lambrou%20and%20Binod%20Bhattarai&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20pipeline%20for%202D%20keypoint%20estima-%20tion%20of%20surgical%0Atools%20by%20leveraging%20Vision%20Language%20Models%20%28VLMs%29%20fine-%20tuned%20using%20a%20low%20rank%0Aadjusting%20%28LoRA%29%20technique.%20Unlike%20traditional%20Convolutional%20Neural%20Network%0A%28CNN%29%20or%20Transformer-based%20approaches%2C%20which%20often%20suffer%20from%20overfitting%20in%0Asmall-scale%20medical%20datasets%2C%20our%20method%20harnesses%20the%20generalization%0Acapabilities%20of%20pre-trained%20VLMs.%20We%20carefully%20design%20prompts%20to%20create%20an%0Ainstruction-tuning%20dataset%20and%20use%20them%20to%20align%20visual%20features%20with%20semantic%0Akeypoint%20descriptions.%20Experimental%20results%20show%20that%20with%20only%20two%20epochs%20of%0Afine%20tuning%2C%20the%20adapted%20VLM%20outperforms%20the%20baseline%20models%2C%20demonstrating%20the%0Aef-%20fectiveness%20of%20LoRA%20in%20low-resource%20scenarios.%20This%20approach%20not%20only%0Aimproves%20keypoint%20detection%20performance%2C%20but%20also%20paves%20the%20way%20for%20future%20work%0Ain%203D%20surgical%20hands%20and%20tools%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20830v1&entry.124074799=Read"},
{"title": "SMARTe-VR: Student Monitoring and Adaptive Response Technology for\n  e-Learning in Virtual Reality", "author": "Roberto Daza and Lin Shengkai and Aythami Morales and Julian Fierrez and Katashi Nagao", "abstract": "  This work introduces SMARTe-VR, a platform for student monitoring in an\nimmersive virtual reality environment designed for online education. SMARTe-VR\naims to collect data for adaptive learning, focusing on facial biometrics and\nlearning metadata. The platform allows instructors to create customized\nlearning sessions with video lectures, featuring an interface with an AutoQA\nsystem to evaluate understanding, interaction tools (for example, textbook\nhighlighting and lecture tagging), and real-time feedback. Furthermore, we\nreleased a dataset that contains 5 research challenges with data from 10 users\nin VR-based TOEIC sessions. This data set, which spans more than 25 hours,\nincludes facial features, learning metadata, 450 responses, difficulty levels\nof the questions, concept tags, and understanding labels. Alongside the\ndatabase, we present preliminary experiments using Item Response Theory models,\nadapted for understanding detection using facial features. Two architectures\nwere explored: a Temporal Convolutional Network for local features and a\nMultilayer Perceptron for global features.\n", "link": "http://arxiv.org/abs/2501.10977v3", "date": "2025-08-28", "relevancy": 2.1029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5125}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMARTe-VR%3A%20Student%20Monitoring%20and%20Adaptive%20Response%20Technology%20for%0A%20%20e-Learning%20in%20Virtual%20Reality&body=Title%3A%20SMARTe-VR%3A%20Student%20Monitoring%20and%20Adaptive%20Response%20Technology%20for%0A%20%20e-Learning%20in%20Virtual%20Reality%0AAuthor%3A%20Roberto%20Daza%20and%20Lin%20Shengkai%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Katashi%20Nagao%0AAbstract%3A%20%20%20This%20work%20introduces%20SMARTe-VR%2C%20a%20platform%20for%20student%20monitoring%20in%20an%0Aimmersive%20virtual%20reality%20environment%20designed%20for%20online%20education.%20SMARTe-VR%0Aaims%20to%20collect%20data%20for%20adaptive%20learning%2C%20focusing%20on%20facial%20biometrics%20and%0Alearning%20metadata.%20The%20platform%20allows%20instructors%20to%20create%20customized%0Alearning%20sessions%20with%20video%20lectures%2C%20featuring%20an%20interface%20with%20an%20AutoQA%0Asystem%20to%20evaluate%20understanding%2C%20interaction%20tools%20%28for%20example%2C%20textbook%0Ahighlighting%20and%20lecture%20tagging%29%2C%20and%20real-time%20feedback.%20Furthermore%2C%20we%0Areleased%20a%20dataset%20that%20contains%205%20research%20challenges%20with%20data%20from%2010%20users%0Ain%20VR-based%20TOEIC%20sessions.%20This%20data%20set%2C%20which%20spans%20more%20than%2025%20hours%2C%0Aincludes%20facial%20features%2C%20learning%20metadata%2C%20450%20responses%2C%20difficulty%20levels%0Aof%20the%20questions%2C%20concept%20tags%2C%20and%20understanding%20labels.%20Alongside%20the%0Adatabase%2C%20we%20present%20preliminary%20experiments%20using%20Item%20Response%20Theory%20models%2C%0Aadapted%20for%20understanding%20detection%20using%20facial%20features.%20Two%20architectures%0Awere%20explored%3A%20a%20Temporal%20Convolutional%20Network%20for%20local%20features%20and%20a%0AMultilayer%20Perceptron%20for%20global%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMARTe-VR%253A%2520Student%2520Monitoring%2520and%2520Adaptive%2520Response%2520Technology%2520for%250A%2520%2520e-Learning%2520in%2520Virtual%2520Reality%26entry.906535625%3DRoberto%2520Daza%2520and%2520Lin%2520Shengkai%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Katashi%2520Nagao%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520SMARTe-VR%252C%2520a%2520platform%2520for%2520student%2520monitoring%2520in%2520an%250Aimmersive%2520virtual%2520reality%2520environment%2520designed%2520for%2520online%2520education.%2520SMARTe-VR%250Aaims%2520to%2520collect%2520data%2520for%2520adaptive%2520learning%252C%2520focusing%2520on%2520facial%2520biometrics%2520and%250Alearning%2520metadata.%2520The%2520platform%2520allows%2520instructors%2520to%2520create%2520customized%250Alearning%2520sessions%2520with%2520video%2520lectures%252C%2520featuring%2520an%2520interface%2520with%2520an%2520AutoQA%250Asystem%2520to%2520evaluate%2520understanding%252C%2520interaction%2520tools%2520%2528for%2520example%252C%2520textbook%250Ahighlighting%2520and%2520lecture%2520tagging%2529%252C%2520and%2520real-time%2520feedback.%2520Furthermore%252C%2520we%250Areleased%2520a%2520dataset%2520that%2520contains%25205%2520research%2520challenges%2520with%2520data%2520from%252010%2520users%250Ain%2520VR-based%2520TOEIC%2520sessions.%2520This%2520data%2520set%252C%2520which%2520spans%2520more%2520than%252025%2520hours%252C%250Aincludes%2520facial%2520features%252C%2520learning%2520metadata%252C%2520450%2520responses%252C%2520difficulty%2520levels%250Aof%2520the%2520questions%252C%2520concept%2520tags%252C%2520and%2520understanding%2520labels.%2520Alongside%2520the%250Adatabase%252C%2520we%2520present%2520preliminary%2520experiments%2520using%2520Item%2520Response%2520Theory%2520models%252C%250Aadapted%2520for%2520understanding%2520detection%2520using%2520facial%2520features.%2520Two%2520architectures%250Awere%2520explored%253A%2520a%2520Temporal%2520Convolutional%2520Network%2520for%2520local%2520features%2520and%2520a%250AMultilayer%2520Perceptron%2520for%2520global%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMARTe-VR%3A%20Student%20Monitoring%20and%20Adaptive%20Response%20Technology%20for%0A%20%20e-Learning%20in%20Virtual%20Reality&entry.906535625=Roberto%20Daza%20and%20Lin%20Shengkai%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Katashi%20Nagao&entry.1292438233=%20%20This%20work%20introduces%20SMARTe-VR%2C%20a%20platform%20for%20student%20monitoring%20in%20an%0Aimmersive%20virtual%20reality%20environment%20designed%20for%20online%20education.%20SMARTe-VR%0Aaims%20to%20collect%20data%20for%20adaptive%20learning%2C%20focusing%20on%20facial%20biometrics%20and%0Alearning%20metadata.%20The%20platform%20allows%20instructors%20to%20create%20customized%0Alearning%20sessions%20with%20video%20lectures%2C%20featuring%20an%20interface%20with%20an%20AutoQA%0Asystem%20to%20evaluate%20understanding%2C%20interaction%20tools%20%28for%20example%2C%20textbook%0Ahighlighting%20and%20lecture%20tagging%29%2C%20and%20real-time%20feedback.%20Furthermore%2C%20we%0Areleased%20a%20dataset%20that%20contains%205%20research%20challenges%20with%20data%20from%2010%20users%0Ain%20VR-based%20TOEIC%20sessions.%20This%20data%20set%2C%20which%20spans%20more%20than%2025%20hours%2C%0Aincludes%20facial%20features%2C%20learning%20metadata%2C%20450%20responses%2C%20difficulty%20levels%0Aof%20the%20questions%2C%20concept%20tags%2C%20and%20understanding%20labels.%20Alongside%20the%0Adatabase%2C%20we%20present%20preliminary%20experiments%20using%20Item%20Response%20Theory%20models%2C%0Aadapted%20for%20understanding%20detection%20using%20facial%20features.%20Two%20architectures%0Awere%20explored%3A%20a%20Temporal%20Convolutional%20Network%20for%20local%20features%20and%20a%0AMultilayer%20Perceptron%20for%20global%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10977v3&entry.124074799=Read"},
{"title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language\n  Models via Marine Mammal Vocalizations", "author": "Jaeyeon Kim and Heeseung Yun and Sang Hoon Woo and Chao-Han Huck Yang and Gunhee Kim", "abstract": "  Large audio language models (LALMs) extend language understanding into the\nauditory domain, yet their ability to perform low-level listening, such as\npitch and duration detection, remains underexplored. However, low-level\nlistening is critical for real-world, out-of-distribution tasks where models\nmust reason about unfamiliar sounds based on fine-grained acoustic cues. To\naddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to\nevaluate low-level auditory perception and cognition using marine mammal\nvocalizations. WoW-bench is composed of a Perception benchmark for categorizing\nnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess\nthe abilities to remember, understand, apply, and analyze sound events. For the\nCognition benchmark, we additionally introduce distractor questions to evaluate\nwhether models are truly solving problems through listening rather than relying\non other heuristics. Experiments with state-of-the-art LALMs show performance\nfar below human levels, indicating a need for stronger auditory grounding in\nLALMs.\n", "link": "http://arxiv.org/abs/2508.20976v1", "date": "2025-08-28", "relevancy": 2.102, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WoW-Bench%3A%20Evaluating%20Fine-Grained%20Acoustic%20Perception%20in%20Audio-Language%0A%20%20Models%20via%20Marine%20Mammal%20Vocalizations&body=Title%3A%20WoW-Bench%3A%20Evaluating%20Fine-Grained%20Acoustic%20Perception%20in%20Audio-Language%0A%20%20Models%20via%20Marine%20Mammal%20Vocalizations%0AAuthor%3A%20Jaeyeon%20Kim%20and%20Heeseung%20Yun%20and%20Sang%20Hoon%20Woo%20and%20Chao-Han%20Huck%20Yang%20and%20Gunhee%20Kim%0AAbstract%3A%20%20%20Large%20audio%20language%20models%20%28LALMs%29%20extend%20language%20understanding%20into%20the%0Aauditory%20domain%2C%20yet%20their%20ability%20to%20perform%20low-level%20listening%2C%20such%20as%0Apitch%20and%20duration%20detection%2C%20remains%20underexplored.%20However%2C%20low-level%0Alistening%20is%20critical%20for%20real-world%2C%20out-of-distribution%20tasks%20where%20models%0Amust%20reason%20about%20unfamiliar%20sounds%20based%20on%20fine-grained%20acoustic%20cues.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20World-of-Whale%20benchmark%20%28WoW-Bench%29%20to%0Aevaluate%20low-level%20auditory%20perception%20and%20cognition%20using%20marine%20mammal%0Avocalizations.%20WoW-bench%20is%20composed%20of%20a%20Perception%20benchmark%20for%20categorizing%0Anovel%20sounds%20and%20a%20Cognition%20benchmark%2C%20inspired%20by%20Bloom%27s%20taxonomy%2C%20to%20assess%0Athe%20abilities%20to%20remember%2C%20understand%2C%20apply%2C%20and%20analyze%20sound%20events.%20For%20the%0ACognition%20benchmark%2C%20we%20additionally%20introduce%20distractor%20questions%20to%20evaluate%0Awhether%20models%20are%20truly%20solving%20problems%20through%20listening%20rather%20than%20relying%0Aon%20other%20heuristics.%20Experiments%20with%20state-of-the-art%20LALMs%20show%20performance%0Afar%20below%20human%20levels%2C%20indicating%20a%20need%20for%20stronger%20auditory%20grounding%20in%0ALALMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWoW-Bench%253A%2520Evaluating%2520Fine-Grained%2520Acoustic%2520Perception%2520in%2520Audio-Language%250A%2520%2520Models%2520via%2520Marine%2520Mammal%2520Vocalizations%26entry.906535625%3DJaeyeon%2520Kim%2520and%2520Heeseung%2520Yun%2520and%2520Sang%2520Hoon%2520Woo%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Gunhee%2520Kim%26entry.1292438233%3D%2520%2520Large%2520audio%2520language%2520models%2520%2528LALMs%2529%2520extend%2520language%2520understanding%2520into%2520the%250Aauditory%2520domain%252C%2520yet%2520their%2520ability%2520to%2520perform%2520low-level%2520listening%252C%2520such%2520as%250Apitch%2520and%2520duration%2520detection%252C%2520remains%2520underexplored.%2520However%252C%2520low-level%250Alistening%2520is%2520critical%2520for%2520real-world%252C%2520out-of-distribution%2520tasks%2520where%2520models%250Amust%2520reason%2520about%2520unfamiliar%2520sounds%2520based%2520on%2520fine-grained%2520acoustic%2520cues.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520the%2520World-of-Whale%2520benchmark%2520%2528WoW-Bench%2529%2520to%250Aevaluate%2520low-level%2520auditory%2520perception%2520and%2520cognition%2520using%2520marine%2520mammal%250Avocalizations.%2520WoW-bench%2520is%2520composed%2520of%2520a%2520Perception%2520benchmark%2520for%2520categorizing%250Anovel%2520sounds%2520and%2520a%2520Cognition%2520benchmark%252C%2520inspired%2520by%2520Bloom%2527s%2520taxonomy%252C%2520to%2520assess%250Athe%2520abilities%2520to%2520remember%252C%2520understand%252C%2520apply%252C%2520and%2520analyze%2520sound%2520events.%2520For%2520the%250ACognition%2520benchmark%252C%2520we%2520additionally%2520introduce%2520distractor%2520questions%2520to%2520evaluate%250Awhether%2520models%2520are%2520truly%2520solving%2520problems%2520through%2520listening%2520rather%2520than%2520relying%250Aon%2520other%2520heuristics.%2520Experiments%2520with%2520state-of-the-art%2520LALMs%2520show%2520performance%250Afar%2520below%2520human%2520levels%252C%2520indicating%2520a%2520need%2520for%2520stronger%2520auditory%2520grounding%2520in%250ALALMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WoW-Bench%3A%20Evaluating%20Fine-Grained%20Acoustic%20Perception%20in%20Audio-Language%0A%20%20Models%20via%20Marine%20Mammal%20Vocalizations&entry.906535625=Jaeyeon%20Kim%20and%20Heeseung%20Yun%20and%20Sang%20Hoon%20Woo%20and%20Chao-Han%20Huck%20Yang%20and%20Gunhee%20Kim&entry.1292438233=%20%20Large%20audio%20language%20models%20%28LALMs%29%20extend%20language%20understanding%20into%20the%0Aauditory%20domain%2C%20yet%20their%20ability%20to%20perform%20low-level%20listening%2C%20such%20as%0Apitch%20and%20duration%20detection%2C%20remains%20underexplored.%20However%2C%20low-level%0Alistening%20is%20critical%20for%20real-world%2C%20out-of-distribution%20tasks%20where%20models%0Amust%20reason%20about%20unfamiliar%20sounds%20based%20on%20fine-grained%20acoustic%20cues.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20World-of-Whale%20benchmark%20%28WoW-Bench%29%20to%0Aevaluate%20low-level%20auditory%20perception%20and%20cognition%20using%20marine%20mammal%0Avocalizations.%20WoW-bench%20is%20composed%20of%20a%20Perception%20benchmark%20for%20categorizing%0Anovel%20sounds%20and%20a%20Cognition%20benchmark%2C%20inspired%20by%20Bloom%27s%20taxonomy%2C%20to%20assess%0Athe%20abilities%20to%20remember%2C%20understand%2C%20apply%2C%20and%20analyze%20sound%20events.%20For%20the%0ACognition%20benchmark%2C%20we%20additionally%20introduce%20distractor%20questions%20to%20evaluate%0Awhether%20models%20are%20truly%20solving%20problems%20through%20listening%20rather%20than%20relying%0Aon%20other%20heuristics.%20Experiments%20with%20state-of-the-art%20LALMs%20show%20performance%0Afar%20below%20human%20levels%2C%20indicating%20a%20need%20for%20stronger%20auditory%20grounding%20in%0ALALMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20976v1&entry.124074799=Read"},
{"title": "Unified Multi-task Learning for Voice-Based Detection of Diverse\n  Clinical Conditions", "author": "Ran Piao and Yuan Lu and Hareld Kemps and Tong Xia and Aaqib Saeed", "abstract": "  Voice-based health assessment offers unprecedented opportunities for\nscalable, non-invasive disease screening, yet existing approaches typically\nfocus on single conditions and fail to leverage the rich, multi-faceted\ninformation embedded in speech. We present MARVEL (Multi-task Acoustic\nRepresentations for Voice-based Health Analysis), a privacy-conscious multitask\nlearning framework that simultaneously detects nine distinct neurological,\nrespiratory, and voice disorders using only derived acoustic features,\neliminating the need for raw audio transmission. Our dual-branch architecture\nemploys specialized encoders with task-specific heads sharing a common acoustic\nbackbone, enabling effective cross-condition knowledge transfer. Evaluated on\nthe large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC\nof 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),\nparticularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).\nOur framework consistently outperforms single-modal baselines by 5-19% and\nsurpasses state-of-the-art self-supervised models on 7 of 9 tasks, while\ncorrelation analysis reveals that the learned representations exhibit\nmeaningful similarities with established acoustic features, indicating that the\nmodel's internal representations are consistent with clinically recognized\nacoustic patterns. By demonstrating that a single unified model can effectively\nscreen for diverse conditions, this work establishes a foundation for\ndeployable voice-based diagnostics in resource-constrained and remote\nhealthcare settings.\n", "link": "http://arxiv.org/abs/2508.20717v1", "date": "2025-08-28", "relevancy": 2.0927, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multi-task%20Learning%20for%20Voice-Based%20Detection%20of%20Diverse%0A%20%20Clinical%20Conditions&body=Title%3A%20Unified%20Multi-task%20Learning%20for%20Voice-Based%20Detection%20of%20Diverse%0A%20%20Clinical%20Conditions%0AAuthor%3A%20Ran%20Piao%20and%20Yuan%20Lu%20and%20Hareld%20Kemps%20and%20Tong%20Xia%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Voice-based%20health%20assessment%20offers%20unprecedented%20opportunities%20for%0Ascalable%2C%20non-invasive%20disease%20screening%2C%20yet%20existing%20approaches%20typically%0Afocus%20on%20single%20conditions%20and%20fail%20to%20leverage%20the%20rich%2C%20multi-faceted%0Ainformation%20embedded%20in%20speech.%20We%20present%20MARVEL%20%28Multi-task%20Acoustic%0ARepresentations%20for%20Voice-based%20Health%20Analysis%29%2C%20a%20privacy-conscious%20multitask%0Alearning%20framework%20that%20simultaneously%20detects%20nine%20distinct%20neurological%2C%0Arespiratory%2C%20and%20voice%20disorders%20using%20only%20derived%20acoustic%20features%2C%0Aeliminating%20the%20need%20for%20raw%20audio%20transmission.%20Our%20dual-branch%20architecture%0Aemploys%20specialized%20encoders%20with%20task-specific%20heads%20sharing%20a%20common%20acoustic%0Abackbone%2C%20enabling%20effective%20cross-condition%20knowledge%20transfer.%20Evaluated%20on%0Athe%20large-scale%20Bridge2AI-Voice%20v2.0%20dataset%2C%20MARVEL%20achieves%20an%20overall%20AUROC%0Aof%200.78%2C%20with%20exceptional%20performance%20on%20neurological%20disorders%20%28AUROC%20%3D%200.89%29%2C%0Aparticularly%20for%20Alzheimer%27s%20disease/mild%20cognitive%20impairment%20%28AUROC%20%3D%200.97%29.%0AOur%20framework%20consistently%20outperforms%20single-modal%20baselines%20by%205-19%25%20and%0Asurpasses%20state-of-the-art%20self-supervised%20models%20on%207%20of%209%20tasks%2C%20while%0Acorrelation%20analysis%20reveals%20that%20the%20learned%20representations%20exhibit%0Ameaningful%20similarities%20with%20established%20acoustic%20features%2C%20indicating%20that%20the%0Amodel%27s%20internal%20representations%20are%20consistent%20with%20clinically%20recognized%0Aacoustic%20patterns.%20By%20demonstrating%20that%20a%20single%20unified%20model%20can%20effectively%0Ascreen%20for%20diverse%20conditions%2C%20this%20work%20establishes%20a%20foundation%20for%0Adeployable%20voice-based%20diagnostics%20in%20resource-constrained%20and%20remote%0Ahealthcare%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multi-task%2520Learning%2520for%2520Voice-Based%2520Detection%2520of%2520Diverse%250A%2520%2520Clinical%2520Conditions%26entry.906535625%3DRan%2520Piao%2520and%2520Yuan%2520Lu%2520and%2520Hareld%2520Kemps%2520and%2520Tong%2520Xia%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Voice-based%2520health%2520assessment%2520offers%2520unprecedented%2520opportunities%2520for%250Ascalable%252C%2520non-invasive%2520disease%2520screening%252C%2520yet%2520existing%2520approaches%2520typically%250Afocus%2520on%2520single%2520conditions%2520and%2520fail%2520to%2520leverage%2520the%2520rich%252C%2520multi-faceted%250Ainformation%2520embedded%2520in%2520speech.%2520We%2520present%2520MARVEL%2520%2528Multi-task%2520Acoustic%250ARepresentations%2520for%2520Voice-based%2520Health%2520Analysis%2529%252C%2520a%2520privacy-conscious%2520multitask%250Alearning%2520framework%2520that%2520simultaneously%2520detects%2520nine%2520distinct%2520neurological%252C%250Arespiratory%252C%2520and%2520voice%2520disorders%2520using%2520only%2520derived%2520acoustic%2520features%252C%250Aeliminating%2520the%2520need%2520for%2520raw%2520audio%2520transmission.%2520Our%2520dual-branch%2520architecture%250Aemploys%2520specialized%2520encoders%2520with%2520task-specific%2520heads%2520sharing%2520a%2520common%2520acoustic%250Abackbone%252C%2520enabling%2520effective%2520cross-condition%2520knowledge%2520transfer.%2520Evaluated%2520on%250Athe%2520large-scale%2520Bridge2AI-Voice%2520v2.0%2520dataset%252C%2520MARVEL%2520achieves%2520an%2520overall%2520AUROC%250Aof%25200.78%252C%2520with%2520exceptional%2520performance%2520on%2520neurological%2520disorders%2520%2528AUROC%2520%253D%25200.89%2529%252C%250Aparticularly%2520for%2520Alzheimer%2527s%2520disease/mild%2520cognitive%2520impairment%2520%2528AUROC%2520%253D%25200.97%2529.%250AOur%2520framework%2520consistently%2520outperforms%2520single-modal%2520baselines%2520by%25205-19%2525%2520and%250Asurpasses%2520state-of-the-art%2520self-supervised%2520models%2520on%25207%2520of%25209%2520tasks%252C%2520while%250Acorrelation%2520analysis%2520reveals%2520that%2520the%2520learned%2520representations%2520exhibit%250Ameaningful%2520similarities%2520with%2520established%2520acoustic%2520features%252C%2520indicating%2520that%2520the%250Amodel%2527s%2520internal%2520representations%2520are%2520consistent%2520with%2520clinically%2520recognized%250Aacoustic%2520patterns.%2520By%2520demonstrating%2520that%2520a%2520single%2520unified%2520model%2520can%2520effectively%250Ascreen%2520for%2520diverse%2520conditions%252C%2520this%2520work%2520establishes%2520a%2520foundation%2520for%250Adeployable%2520voice-based%2520diagnostics%2520in%2520resource-constrained%2520and%2520remote%250Ahealthcare%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multi-task%20Learning%20for%20Voice-Based%20Detection%20of%20Diverse%0A%20%20Clinical%20Conditions&entry.906535625=Ran%20Piao%20and%20Yuan%20Lu%20and%20Hareld%20Kemps%20and%20Tong%20Xia%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Voice-based%20health%20assessment%20offers%20unprecedented%20opportunities%20for%0Ascalable%2C%20non-invasive%20disease%20screening%2C%20yet%20existing%20approaches%20typically%0Afocus%20on%20single%20conditions%20and%20fail%20to%20leverage%20the%20rich%2C%20multi-faceted%0Ainformation%20embedded%20in%20speech.%20We%20present%20MARVEL%20%28Multi-task%20Acoustic%0ARepresentations%20for%20Voice-based%20Health%20Analysis%29%2C%20a%20privacy-conscious%20multitask%0Alearning%20framework%20that%20simultaneously%20detects%20nine%20distinct%20neurological%2C%0Arespiratory%2C%20and%20voice%20disorders%20using%20only%20derived%20acoustic%20features%2C%0Aeliminating%20the%20need%20for%20raw%20audio%20transmission.%20Our%20dual-branch%20architecture%0Aemploys%20specialized%20encoders%20with%20task-specific%20heads%20sharing%20a%20common%20acoustic%0Abackbone%2C%20enabling%20effective%20cross-condition%20knowledge%20transfer.%20Evaluated%20on%0Athe%20large-scale%20Bridge2AI-Voice%20v2.0%20dataset%2C%20MARVEL%20achieves%20an%20overall%20AUROC%0Aof%200.78%2C%20with%20exceptional%20performance%20on%20neurological%20disorders%20%28AUROC%20%3D%200.89%29%2C%0Aparticularly%20for%20Alzheimer%27s%20disease/mild%20cognitive%20impairment%20%28AUROC%20%3D%200.97%29.%0AOur%20framework%20consistently%20outperforms%20single-modal%20baselines%20by%205-19%25%20and%0Asurpasses%20state-of-the-art%20self-supervised%20models%20on%207%20of%209%20tasks%2C%20while%0Acorrelation%20analysis%20reveals%20that%20the%20learned%20representations%20exhibit%0Ameaningful%20similarities%20with%20established%20acoustic%20features%2C%20indicating%20that%20the%0Amodel%27s%20internal%20representations%20are%20consistent%20with%20clinically%20recognized%0Aacoustic%20patterns.%20By%20demonstrating%20that%20a%20single%20unified%20model%20can%20effectively%0Ascreen%20for%20diverse%20conditions%2C%20this%20work%20establishes%20a%20foundation%20for%0Adeployable%20voice-based%20diagnostics%20in%20resource-constrained%20and%20remote%0Ahealthcare%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20717v1&entry.124074799=Read"},
{"title": "UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for\n  Enhanced Robotic Perception", "author": "Junhao Gong and Kit-Wa Sou and Shoujie Li and Changqing Guo and Yan Huang and Chuqiao Lyu and Ziwu Song and Wenbo Ding", "abstract": "  Visuotactile sensors provide high-resolution tactile information but are\nincapable of perceiving the material features of objects. We present UltraTac,\nan integrated sensor that combines visuotactile imaging with ultrasound sensing\nthrough a coaxial optoacoustic architecture. The design shares structural\ncomponents and achieves consistent sensing regions for both modalities.\nAdditionally, we incorporate acoustic matching into the traditional\nvisuotactile sensor structure, enabling integration of the ultrasound sensing\nmodality without compromising visuotactile performance. Through tactile\nfeedback, we dynamically adjust the operating state of the ultrasound module to\nachieve flexible functional coordination. Systematic experiments demonstrate\nthree key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),\nmaterial classification (average accuracy: 99.20%), and texture-material\ndual-mode object recognition achieving 92.11% accuracy on a 15-class task.\nFinally, we integrate the sensor into a robotic manipulation system to\nconcurrently detect container surface patterns and internal content, which\nverifies its potential for advanced human-machine interaction and precise\nrobotic manipulation.\n", "link": "http://arxiv.org/abs/2508.20982v1", "date": "2025-08-28", "relevancy": 2.0894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraTac%3A%20Integrated%20Ultrasound-Augmented%20Visuotactile%20Sensor%20for%0A%20%20Enhanced%20Robotic%20Perception&body=Title%3A%20UltraTac%3A%20Integrated%20Ultrasound-Augmented%20Visuotactile%20Sensor%20for%0A%20%20Enhanced%20Robotic%20Perception%0AAuthor%3A%20Junhao%20Gong%20and%20Kit-Wa%20Sou%20and%20Shoujie%20Li%20and%20Changqing%20Guo%20and%20Yan%20Huang%20and%20Chuqiao%20Lyu%20and%20Ziwu%20Song%20and%20Wenbo%20Ding%0AAbstract%3A%20%20%20Visuotactile%20sensors%20provide%20high-resolution%20tactile%20information%20but%20are%0Aincapable%20of%20perceiving%20the%20material%20features%20of%20objects.%20We%20present%20UltraTac%2C%0Aan%20integrated%20sensor%20that%20combines%20visuotactile%20imaging%20with%20ultrasound%20sensing%0Athrough%20a%20coaxial%20optoacoustic%20architecture.%20The%20design%20shares%20structural%0Acomponents%20and%20achieves%20consistent%20sensing%20regions%20for%20both%20modalities.%0AAdditionally%2C%20we%20incorporate%20acoustic%20matching%20into%20the%20traditional%0Avisuotactile%20sensor%20structure%2C%20enabling%20integration%20of%20the%20ultrasound%20sensing%0Amodality%20without%20compromising%20visuotactile%20performance.%20Through%20tactile%0Afeedback%2C%20we%20dynamically%20adjust%20the%20operating%20state%20of%20the%20ultrasound%20module%20to%0Aachieve%20flexible%20functional%20coordination.%20Systematic%20experiments%20demonstrate%0Athree%20key%20capabilities%3A%20proximity%20sensing%20in%20the%203-8%20cm%20range%20%28%24R%5E2%3D0.90%24%29%2C%0Amaterial%20classification%20%28average%20accuracy%3A%2099.20%25%29%2C%20and%20texture-material%0Adual-mode%20object%20recognition%20achieving%2092.11%25%20accuracy%20on%20a%2015-class%20task.%0AFinally%2C%20we%20integrate%20the%20sensor%20into%20a%20robotic%20manipulation%20system%20to%0Aconcurrently%20detect%20container%20surface%20patterns%20and%20internal%20content%2C%20which%0Averifies%20its%20potential%20for%20advanced%20human-machine%20interaction%20and%20precise%0Arobotic%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraTac%253A%2520Integrated%2520Ultrasound-Augmented%2520Visuotactile%2520Sensor%2520for%250A%2520%2520Enhanced%2520Robotic%2520Perception%26entry.906535625%3DJunhao%2520Gong%2520and%2520Kit-Wa%2520Sou%2520and%2520Shoujie%2520Li%2520and%2520Changqing%2520Guo%2520and%2520Yan%2520Huang%2520and%2520Chuqiao%2520Lyu%2520and%2520Ziwu%2520Song%2520and%2520Wenbo%2520Ding%26entry.1292438233%3D%2520%2520Visuotactile%2520sensors%2520provide%2520high-resolution%2520tactile%2520information%2520but%2520are%250Aincapable%2520of%2520perceiving%2520the%2520material%2520features%2520of%2520objects.%2520We%2520present%2520UltraTac%252C%250Aan%2520integrated%2520sensor%2520that%2520combines%2520visuotactile%2520imaging%2520with%2520ultrasound%2520sensing%250Athrough%2520a%2520coaxial%2520optoacoustic%2520architecture.%2520The%2520design%2520shares%2520structural%250Acomponents%2520and%2520achieves%2520consistent%2520sensing%2520regions%2520for%2520both%2520modalities.%250AAdditionally%252C%2520we%2520incorporate%2520acoustic%2520matching%2520into%2520the%2520traditional%250Avisuotactile%2520sensor%2520structure%252C%2520enabling%2520integration%2520of%2520the%2520ultrasound%2520sensing%250Amodality%2520without%2520compromising%2520visuotactile%2520performance.%2520Through%2520tactile%250Afeedback%252C%2520we%2520dynamically%2520adjust%2520the%2520operating%2520state%2520of%2520the%2520ultrasound%2520module%2520to%250Aachieve%2520flexible%2520functional%2520coordination.%2520Systematic%2520experiments%2520demonstrate%250Athree%2520key%2520capabilities%253A%2520proximity%2520sensing%2520in%2520the%25203-8%2520cm%2520range%2520%2528%2524R%255E2%253D0.90%2524%2529%252C%250Amaterial%2520classification%2520%2528average%2520accuracy%253A%252099.20%2525%2529%252C%2520and%2520texture-material%250Adual-mode%2520object%2520recognition%2520achieving%252092.11%2525%2520accuracy%2520on%2520a%252015-class%2520task.%250AFinally%252C%2520we%2520integrate%2520the%2520sensor%2520into%2520a%2520robotic%2520manipulation%2520system%2520to%250Aconcurrently%2520detect%2520container%2520surface%2520patterns%2520and%2520internal%2520content%252C%2520which%250Averifies%2520its%2520potential%2520for%2520advanced%2520human-machine%2520interaction%2520and%2520precise%250Arobotic%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraTac%3A%20Integrated%20Ultrasound-Augmented%20Visuotactile%20Sensor%20for%0A%20%20Enhanced%20Robotic%20Perception&entry.906535625=Junhao%20Gong%20and%20Kit-Wa%20Sou%20and%20Shoujie%20Li%20and%20Changqing%20Guo%20and%20Yan%20Huang%20and%20Chuqiao%20Lyu%20and%20Ziwu%20Song%20and%20Wenbo%20Ding&entry.1292438233=%20%20Visuotactile%20sensors%20provide%20high-resolution%20tactile%20information%20but%20are%0Aincapable%20of%20perceiving%20the%20material%20features%20of%20objects.%20We%20present%20UltraTac%2C%0Aan%20integrated%20sensor%20that%20combines%20visuotactile%20imaging%20with%20ultrasound%20sensing%0Athrough%20a%20coaxial%20optoacoustic%20architecture.%20The%20design%20shares%20structural%0Acomponents%20and%20achieves%20consistent%20sensing%20regions%20for%20both%20modalities.%0AAdditionally%2C%20we%20incorporate%20acoustic%20matching%20into%20the%20traditional%0Avisuotactile%20sensor%20structure%2C%20enabling%20integration%20of%20the%20ultrasound%20sensing%0Amodality%20without%20compromising%20visuotactile%20performance.%20Through%20tactile%0Afeedback%2C%20we%20dynamically%20adjust%20the%20operating%20state%20of%20the%20ultrasound%20module%20to%0Aachieve%20flexible%20functional%20coordination.%20Systematic%20experiments%20demonstrate%0Athree%20key%20capabilities%3A%20proximity%20sensing%20in%20the%203-8%20cm%20range%20%28%24R%5E2%3D0.90%24%29%2C%0Amaterial%20classification%20%28average%20accuracy%3A%2099.20%25%29%2C%20and%20texture-material%0Adual-mode%20object%20recognition%20achieving%2092.11%25%20accuracy%20on%20a%2015-class%20task.%0AFinally%2C%20we%20integrate%20the%20sensor%20into%20a%20robotic%20manipulation%20system%20to%0Aconcurrently%20detect%20container%20surface%20patterns%20and%20internal%20content%2C%20which%0Averifies%20its%20potential%20for%20advanced%20human-machine%20interaction%20and%20precise%0Arobotic%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20982v1&entry.124074799=Read"},
{"title": "ChainReaction! Structured Approach with Causal Chains as Intermediate\n  Representations for Improved and Explainable Causal Video Question Answering", "author": "Paritosh Parmar and Eric Peh and Basura Fernando", "abstract": "  Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/\n", "link": "http://arxiv.org/abs/2508.21010v1", "date": "2025-08-28", "relevancy": 2.0861, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChainReaction%21%20Structured%20Approach%20with%20Causal%20Chains%20as%20Intermediate%0A%20%20Representations%20for%20Improved%20and%20Explainable%20Causal%20Video%20Question%20Answering&body=Title%3A%20ChainReaction%21%20Structured%20Approach%20with%20Causal%20Chains%20as%20Intermediate%0A%20%20Representations%20for%20Improved%20and%20Explainable%20Causal%20Video%20Question%20Answering%0AAuthor%3A%20Paritosh%20Parmar%20and%20Eric%20Peh%20and%20Basura%20Fernando%0AAbstract%3A%20%20%20Existing%20Causal-Why%20Video%20Question%20Answering%20%28VideoQA%29%20models%20often%20struggle%0Awith%20higher-order%20reasoning%2C%20relying%20on%20opaque%2C%20monolithic%20pipelines%20that%0Aentangle%20video%20understanding%2C%20causal%20inference%2C%20and%20answer%20generation.%20These%0Ablack-box%20approaches%20offer%20limited%20interpretability%20and%20tend%20to%20depend%20on%0Ashallow%20heuristics.%20We%20propose%20a%20novel%2C%20modular%20framework%20that%20explicitly%0Adecouples%20causal%20reasoning%20from%20answer%20generation%2C%20introducing%20natural%20language%0Acausal%20chains%20as%20interpretable%20intermediate%20representations.%20Inspired%20by%20human%0Acognitive%20models%2C%20these%20structured%20cause-effect%20sequences%20bridge%20low-level%0Avideo%20content%20with%20high-level%20causal%20reasoning%2C%20enabling%20transparent%20and%0Alogically%20coherent%20inference.%20Our%20two-stage%20architecture%20comprises%20a%20Causal%0AChain%20Extractor%20%28CCE%29%20that%20generates%20causal%20chains%20from%20video-question%20pairs%2C%0Aand%20a%20Causal%20Chain-Driven%20Answerer%20%28CCDA%29%20that%20produces%20answers%20grounded%20in%0Athese%20chains.%20To%20address%20the%20lack%20of%20annotated%20reasoning%20traces%2C%20we%20introduce%20a%0Ascalable%20method%20for%20generating%20high-quality%20causal%20chains%20from%20existing%0Adatasets%20using%20large%20language%20models.%20We%20also%20propose%20CauCo%2C%20a%20new%20evaluation%0Ametric%20for%20causality-oriented%20captioning.%20Experiments%20on%20three%20large-scale%0Abenchmarks%20demonstrate%20that%20our%20approach%20not%20only%20outperforms%20state-of-the-art%0Amodels%2C%20but%20also%20yields%20substantial%20gains%20in%20explainability%2C%20user%20trust%2C%20and%0Ageneralization%20--%20positioning%20the%20CCE%20as%20a%20reusable%20causal%20reasoning%20engine%0Aacross%20diverse%20domains.%20Project%20page%3A%0Ahttps%3A//paritoshparmar.github.io/chainreaction/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChainReaction%2521%2520Structured%2520Approach%2520with%2520Causal%2520Chains%2520as%2520Intermediate%250A%2520%2520Representations%2520for%2520Improved%2520and%2520Explainable%2520Causal%2520Video%2520Question%2520Answering%26entry.906535625%3DParitosh%2520Parmar%2520and%2520Eric%2520Peh%2520and%2520Basura%2520Fernando%26entry.1292438233%3D%2520%2520Existing%2520Causal-Why%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%2520models%2520often%2520struggle%250Awith%2520higher-order%2520reasoning%252C%2520relying%2520on%2520opaque%252C%2520monolithic%2520pipelines%2520that%250Aentangle%2520video%2520understanding%252C%2520causal%2520inference%252C%2520and%2520answer%2520generation.%2520These%250Ablack-box%2520approaches%2520offer%2520limited%2520interpretability%2520and%2520tend%2520to%2520depend%2520on%250Ashallow%2520heuristics.%2520We%2520propose%2520a%2520novel%252C%2520modular%2520framework%2520that%2520explicitly%250Adecouples%2520causal%2520reasoning%2520from%2520answer%2520generation%252C%2520introducing%2520natural%2520language%250Acausal%2520chains%2520as%2520interpretable%2520intermediate%2520representations.%2520Inspired%2520by%2520human%250Acognitive%2520models%252C%2520these%2520structured%2520cause-effect%2520sequences%2520bridge%2520low-level%250Avideo%2520content%2520with%2520high-level%2520causal%2520reasoning%252C%2520enabling%2520transparent%2520and%250Alogically%2520coherent%2520inference.%2520Our%2520two-stage%2520architecture%2520comprises%2520a%2520Causal%250AChain%2520Extractor%2520%2528CCE%2529%2520that%2520generates%2520causal%2520chains%2520from%2520video-question%2520pairs%252C%250Aand%2520a%2520Causal%2520Chain-Driven%2520Answerer%2520%2528CCDA%2529%2520that%2520produces%2520answers%2520grounded%2520in%250Athese%2520chains.%2520To%2520address%2520the%2520lack%2520of%2520annotated%2520reasoning%2520traces%252C%2520we%2520introduce%2520a%250Ascalable%2520method%2520for%2520generating%2520high-quality%2520causal%2520chains%2520from%2520existing%250Adatasets%2520using%2520large%2520language%2520models.%2520We%2520also%2520propose%2520CauCo%252C%2520a%2520new%2520evaluation%250Ametric%2520for%2520causality-oriented%2520captioning.%2520Experiments%2520on%2520three%2520large-scale%250Abenchmarks%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520outperforms%2520state-of-the-art%250Amodels%252C%2520but%2520also%2520yields%2520substantial%2520gains%2520in%2520explainability%252C%2520user%2520trust%252C%2520and%250Ageneralization%2520--%2520positioning%2520the%2520CCE%2520as%2520a%2520reusable%2520causal%2520reasoning%2520engine%250Aacross%2520diverse%2520domains.%2520Project%2520page%253A%250Ahttps%253A//paritoshparmar.github.io/chainreaction/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChainReaction%21%20Structured%20Approach%20with%20Causal%20Chains%20as%20Intermediate%0A%20%20Representations%20for%20Improved%20and%20Explainable%20Causal%20Video%20Question%20Answering&entry.906535625=Paritosh%20Parmar%20and%20Eric%20Peh%20and%20Basura%20Fernando&entry.1292438233=%20%20Existing%20Causal-Why%20Video%20Question%20Answering%20%28VideoQA%29%20models%20often%20struggle%0Awith%20higher-order%20reasoning%2C%20relying%20on%20opaque%2C%20monolithic%20pipelines%20that%0Aentangle%20video%20understanding%2C%20causal%20inference%2C%20and%20answer%20generation.%20These%0Ablack-box%20approaches%20offer%20limited%20interpretability%20and%20tend%20to%20depend%20on%0Ashallow%20heuristics.%20We%20propose%20a%20novel%2C%20modular%20framework%20that%20explicitly%0Adecouples%20causal%20reasoning%20from%20answer%20generation%2C%20introducing%20natural%20language%0Acausal%20chains%20as%20interpretable%20intermediate%20representations.%20Inspired%20by%20human%0Acognitive%20models%2C%20these%20structured%20cause-effect%20sequences%20bridge%20low-level%0Avideo%20content%20with%20high-level%20causal%20reasoning%2C%20enabling%20transparent%20and%0Alogically%20coherent%20inference.%20Our%20two-stage%20architecture%20comprises%20a%20Causal%0AChain%20Extractor%20%28CCE%29%20that%20generates%20causal%20chains%20from%20video-question%20pairs%2C%0Aand%20a%20Causal%20Chain-Driven%20Answerer%20%28CCDA%29%20that%20produces%20answers%20grounded%20in%0Athese%20chains.%20To%20address%20the%20lack%20of%20annotated%20reasoning%20traces%2C%20we%20introduce%20a%0Ascalable%20method%20for%20generating%20high-quality%20causal%20chains%20from%20existing%0Adatasets%20using%20large%20language%20models.%20We%20also%20propose%20CauCo%2C%20a%20new%20evaluation%0Ametric%20for%20causality-oriented%20captioning.%20Experiments%20on%20three%20large-scale%0Abenchmarks%20demonstrate%20that%20our%20approach%20not%20only%20outperforms%20state-of-the-art%0Amodels%2C%20but%20also%20yields%20substantial%20gains%20in%20explainability%2C%20user%20trust%2C%20and%0Ageneralization%20--%20positioning%20the%20CCE%20as%20a%20reusable%20causal%20reasoning%20engine%0Aacross%20diverse%20domains.%20Project%20page%3A%0Ahttps%3A//paritoshparmar.github.io/chainreaction/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21010v1&entry.124074799=Read"},
{"title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for\n  Atypical Mitotic Figure Classification in MIDOG 2025", "author": "Guillaume Balezo and Rapha\u00ebl Bourgade and Thomas Walter", "abstract": "  Atypical mitotic figures (AMFs) are markers of abnormal cell division\nassociated with poor prognosis, yet their detection remains difficult due to\nlow prevalence, subtle morphology, and inter-observer variability. The MIDOG\n2025 challenge introduces a benchmark for AMF classification across multiple\ndomains. In this work, we evaluate the recently published DINOv3-H+ vision\ntransformer, pretrained on natural images, which we fine-tuned using low-rank\nadaptation (LoRA, 650k trainable parameters) and extensive augmentation.\nDespite the domain gap, DINOv3 transfers effectively to histopathology,\nachieving a balanced accuracy of 0.8871 on the preliminary test set. These\nresults highlight the robustness of DINOv3 pretraining and show that, when\ncombined with parameter-efficient fine-tuning, it provides a strong baseline\nfor atypical mitosis classification in MIDOG 2025.\n", "link": "http://arxiv.org/abs/2508.21041v1", "date": "2025-08-28", "relevancy": 2.0744, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5236}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025&body=Title%3A%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025%0AAuthor%3A%20Guillaume%20Balezo%20and%20Rapha%C3%ABl%20Bourgade%20and%20Thomas%20Walter%0AAbstract%3A%20%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20are%20markers%20of%20abnormal%20cell%20division%0Aassociated%20with%20poor%20prognosis%2C%20yet%20their%20detection%20remains%20difficult%20due%20to%0Alow%20prevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%0A2025%20challenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20evaluate%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20which%20we%20fine-tuned%20using%20low-rank%0Aadaptation%20%28LoRA%2C%20650k%20trainable%20parameters%29%20and%20extensive%20augmentation.%0ADespite%20the%20domain%20gap%2C%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%0Aachieving%20a%20balanced%20accuracy%20of%200.8871%20on%20the%20preliminary%20test%20set.%20These%0Aresults%20highlight%20the%20robustness%20of%20DINOv3%20pretraining%20and%20show%20that%2C%20when%0Acombined%20with%20parameter-efficient%20fine-tuning%2C%20it%20provides%20a%20strong%20baseline%0Afor%20atypical%20mitosis%20classification%20in%20MIDOG%202025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Fine-Tuning%2520of%2520DINOv3%2520Pretrained%2520on%2520Natural%2520Images%2520for%250A%2520%2520Atypical%2520Mitotic%2520Figure%2520Classification%2520in%2520MIDOG%25202025%26entry.906535625%3DGuillaume%2520Balezo%2520and%2520Rapha%25C3%25ABl%2520Bourgade%2520and%2520Thomas%2520Walter%26entry.1292438233%3D%2520%2520Atypical%2520mitotic%2520figures%2520%2528AMFs%2529%2520are%2520markers%2520of%2520abnormal%2520cell%2520division%250Aassociated%2520with%2520poor%2520prognosis%252C%2520yet%2520their%2520detection%2520remains%2520difficult%2520due%2520to%250Alow%2520prevalence%252C%2520subtle%2520morphology%252C%2520and%2520inter-observer%2520variability.%2520The%2520MIDOG%250A2025%2520challenge%2520introduces%2520a%2520benchmark%2520for%2520AMF%2520classification%2520across%2520multiple%250Adomains.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%2520recently%2520published%2520DINOv3-H%252B%2520vision%250Atransformer%252C%2520pretrained%2520on%2520natural%2520images%252C%2520which%2520we%2520fine-tuned%2520using%2520low-rank%250Aadaptation%2520%2528LoRA%252C%2520650k%2520trainable%2520parameters%2529%2520and%2520extensive%2520augmentation.%250ADespite%2520the%2520domain%2520gap%252C%2520DINOv3%2520transfers%2520effectively%2520to%2520histopathology%252C%250Aachieving%2520a%2520balanced%2520accuracy%2520of%25200.8871%2520on%2520the%2520preliminary%2520test%2520set.%2520These%250Aresults%2520highlight%2520the%2520robustness%2520of%2520DINOv3%2520pretraining%2520and%2520show%2520that%252C%2520when%250Acombined%2520with%2520parameter-efficient%2520fine-tuning%252C%2520it%2520provides%2520a%2520strong%2520baseline%250Afor%2520atypical%2520mitosis%2520classification%2520in%2520MIDOG%25202025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025&entry.906535625=Guillaume%20Balezo%20and%20Rapha%C3%ABl%20Bourgade%20and%20Thomas%20Walter&entry.1292438233=%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20are%20markers%20of%20abnormal%20cell%20division%0Aassociated%20with%20poor%20prognosis%2C%20yet%20their%20detection%20remains%20difficult%20due%20to%0Alow%20prevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%0A2025%20challenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20evaluate%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20which%20we%20fine-tuned%20using%20low-rank%0Aadaptation%20%28LoRA%2C%20650k%20trainable%20parameters%29%20and%20extensive%20augmentation.%0ADespite%20the%20domain%20gap%2C%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%0Aachieving%20a%20balanced%20accuracy%20of%200.8871%20on%20the%20preliminary%20test%20set.%20These%0Aresults%20highlight%20the%20robustness%20of%20DINOv3%20pretraining%20and%20show%20that%2C%20when%0Acombined%20with%20parameter-efficient%20fine-tuning%2C%20it%20provides%20a%20strong%20baseline%0Afor%20atypical%20mitosis%20classification%20in%20MIDOG%202025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21041v1&entry.124074799=Read"},
{"title": "Fast Convergence Rates for Subsampled Natural Gradient Algorithms on\n  Quadratic Model Problems", "author": "Gil Goldshlager and Jiang Hu and Lin Lin", "abstract": "  Subsampled natural gradient descent (SNGD) has shown impressive results for\nparametric optimization tasks in scientific machine learning, such as neural\nnetwork wavefunctions and physics-informed neural networks, but it has lacked a\ntheoretical explanation. We address this gap by analyzing the convergence of\nSNGD and its accelerated variant, SPRING, for idealized parametric optimization\nproblems where the model is linear and the loss function is strongly convex and\nquadratic. In the special case of a least-squares loss, namely the standard\nlinear least-squares problem, we prove that SNGD is equivalent to a regularized\nKaczmarz method while SPRING is equivalent to an accelerated regularized\nKaczmarz method. As a result, by leveraging existing analyses we obtain under\nmild conditions (i) the first fast convergence rate for SNGD, (ii) the first\nconvergence guarantee for SPRING in any setting, and (iii) the first proof that\nSPRING can accelerate SNGD. In the case of a general strongly convex quadratic\nloss, we extend the analysis of the regularized Kaczmarz method to obtain a\nfast convergence rate for SNGD under stronger conditions, providing the first\nexplanation for the effectiveness of SNGD outside of the least-squares setting.\nOverall, our results illustrate how tools from randomized linear algebra can\nshed new light on the interplay between subsampling and curvature-aware\noptimization strategies.\n", "link": "http://arxiv.org/abs/2508.21022v1", "date": "2025-08-28", "relevancy": 2.0733, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5292}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5165}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Convergence%20Rates%20for%20Subsampled%20Natural%20Gradient%20Algorithms%20on%0A%20%20Quadratic%20Model%20Problems&body=Title%3A%20Fast%20Convergence%20Rates%20for%20Subsampled%20Natural%20Gradient%20Algorithms%20on%0A%20%20Quadratic%20Model%20Problems%0AAuthor%3A%20Gil%20Goldshlager%20and%20Jiang%20Hu%20and%20Lin%20Lin%0AAbstract%3A%20%20%20Subsampled%20natural%20gradient%20descent%20%28SNGD%29%20has%20shown%20impressive%20results%20for%0Aparametric%20optimization%20tasks%20in%20scientific%20machine%20learning%2C%20such%20as%20neural%0Anetwork%20wavefunctions%20and%20physics-informed%20neural%20networks%2C%20but%20it%20has%20lacked%20a%0Atheoretical%20explanation.%20We%20address%20this%20gap%20by%20analyzing%20the%20convergence%20of%0ASNGD%20and%20its%20accelerated%20variant%2C%20SPRING%2C%20for%20idealized%20parametric%20optimization%0Aproblems%20where%20the%20model%20is%20linear%20and%20the%20loss%20function%20is%20strongly%20convex%20and%0Aquadratic.%20In%20the%20special%20case%20of%20a%20least-squares%20loss%2C%20namely%20the%20standard%0Alinear%20least-squares%20problem%2C%20we%20prove%20that%20SNGD%20is%20equivalent%20to%20a%20regularized%0AKaczmarz%20method%20while%20SPRING%20is%20equivalent%20to%20an%20accelerated%20regularized%0AKaczmarz%20method.%20As%20a%20result%2C%20by%20leveraging%20existing%20analyses%20we%20obtain%20under%0Amild%20conditions%20%28i%29%20the%20first%20fast%20convergence%20rate%20for%20SNGD%2C%20%28ii%29%20the%20first%0Aconvergence%20guarantee%20for%20SPRING%20in%20any%20setting%2C%20and%20%28iii%29%20the%20first%20proof%20that%0ASPRING%20can%20accelerate%20SNGD.%20In%20the%20case%20of%20a%20general%20strongly%20convex%20quadratic%0Aloss%2C%20we%20extend%20the%20analysis%20of%20the%20regularized%20Kaczmarz%20method%20to%20obtain%20a%0Afast%20convergence%20rate%20for%20SNGD%20under%20stronger%20conditions%2C%20providing%20the%20first%0Aexplanation%20for%20the%20effectiveness%20of%20SNGD%20outside%20of%20the%20least-squares%20setting.%0AOverall%2C%20our%20results%20illustrate%20how%20tools%20from%20randomized%20linear%20algebra%20can%0Ashed%20new%20light%20on%20the%20interplay%20between%20subsampling%20and%20curvature-aware%0Aoptimization%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Convergence%2520Rates%2520for%2520Subsampled%2520Natural%2520Gradient%2520Algorithms%2520on%250A%2520%2520Quadratic%2520Model%2520Problems%26entry.906535625%3DGil%2520Goldshlager%2520and%2520Jiang%2520Hu%2520and%2520Lin%2520Lin%26entry.1292438233%3D%2520%2520Subsampled%2520natural%2520gradient%2520descent%2520%2528SNGD%2529%2520has%2520shown%2520impressive%2520results%2520for%250Aparametric%2520optimization%2520tasks%2520in%2520scientific%2520machine%2520learning%252C%2520such%2520as%2520neural%250Anetwork%2520wavefunctions%2520and%2520physics-informed%2520neural%2520networks%252C%2520but%2520it%2520has%2520lacked%2520a%250Atheoretical%2520explanation.%2520We%2520address%2520this%2520gap%2520by%2520analyzing%2520the%2520convergence%2520of%250ASNGD%2520and%2520its%2520accelerated%2520variant%252C%2520SPRING%252C%2520for%2520idealized%2520parametric%2520optimization%250Aproblems%2520where%2520the%2520model%2520is%2520linear%2520and%2520the%2520loss%2520function%2520is%2520strongly%2520convex%2520and%250Aquadratic.%2520In%2520the%2520special%2520case%2520of%2520a%2520least-squares%2520loss%252C%2520namely%2520the%2520standard%250Alinear%2520least-squares%2520problem%252C%2520we%2520prove%2520that%2520SNGD%2520is%2520equivalent%2520to%2520a%2520regularized%250AKaczmarz%2520method%2520while%2520SPRING%2520is%2520equivalent%2520to%2520an%2520accelerated%2520regularized%250AKaczmarz%2520method.%2520As%2520a%2520result%252C%2520by%2520leveraging%2520existing%2520analyses%2520we%2520obtain%2520under%250Amild%2520conditions%2520%2528i%2529%2520the%2520first%2520fast%2520convergence%2520rate%2520for%2520SNGD%252C%2520%2528ii%2529%2520the%2520first%250Aconvergence%2520guarantee%2520for%2520SPRING%2520in%2520any%2520setting%252C%2520and%2520%2528iii%2529%2520the%2520first%2520proof%2520that%250ASPRING%2520can%2520accelerate%2520SNGD.%2520In%2520the%2520case%2520of%2520a%2520general%2520strongly%2520convex%2520quadratic%250Aloss%252C%2520we%2520extend%2520the%2520analysis%2520of%2520the%2520regularized%2520Kaczmarz%2520method%2520to%2520obtain%2520a%250Afast%2520convergence%2520rate%2520for%2520SNGD%2520under%2520stronger%2520conditions%252C%2520providing%2520the%2520first%250Aexplanation%2520for%2520the%2520effectiveness%2520of%2520SNGD%2520outside%2520of%2520the%2520least-squares%2520setting.%250AOverall%252C%2520our%2520results%2520illustrate%2520how%2520tools%2520from%2520randomized%2520linear%2520algebra%2520can%250Ashed%2520new%2520light%2520on%2520the%2520interplay%2520between%2520subsampling%2520and%2520curvature-aware%250Aoptimization%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Convergence%20Rates%20for%20Subsampled%20Natural%20Gradient%20Algorithms%20on%0A%20%20Quadratic%20Model%20Problems&entry.906535625=Gil%20Goldshlager%20and%20Jiang%20Hu%20and%20Lin%20Lin&entry.1292438233=%20%20Subsampled%20natural%20gradient%20descent%20%28SNGD%29%20has%20shown%20impressive%20results%20for%0Aparametric%20optimization%20tasks%20in%20scientific%20machine%20learning%2C%20such%20as%20neural%0Anetwork%20wavefunctions%20and%20physics-informed%20neural%20networks%2C%20but%20it%20has%20lacked%20a%0Atheoretical%20explanation.%20We%20address%20this%20gap%20by%20analyzing%20the%20convergence%20of%0ASNGD%20and%20its%20accelerated%20variant%2C%20SPRING%2C%20for%20idealized%20parametric%20optimization%0Aproblems%20where%20the%20model%20is%20linear%20and%20the%20loss%20function%20is%20strongly%20convex%20and%0Aquadratic.%20In%20the%20special%20case%20of%20a%20least-squares%20loss%2C%20namely%20the%20standard%0Alinear%20least-squares%20problem%2C%20we%20prove%20that%20SNGD%20is%20equivalent%20to%20a%20regularized%0AKaczmarz%20method%20while%20SPRING%20is%20equivalent%20to%20an%20accelerated%20regularized%0AKaczmarz%20method.%20As%20a%20result%2C%20by%20leveraging%20existing%20analyses%20we%20obtain%20under%0Amild%20conditions%20%28i%29%20the%20first%20fast%20convergence%20rate%20for%20SNGD%2C%20%28ii%29%20the%20first%0Aconvergence%20guarantee%20for%20SPRING%20in%20any%20setting%2C%20and%20%28iii%29%20the%20first%20proof%20that%0ASPRING%20can%20accelerate%20SNGD.%20In%20the%20case%20of%20a%20general%20strongly%20convex%20quadratic%0Aloss%2C%20we%20extend%20the%20analysis%20of%20the%20regularized%20Kaczmarz%20method%20to%20obtain%20a%0Afast%20convergence%20rate%20for%20SNGD%20under%20stronger%20conditions%2C%20providing%20the%20first%0Aexplanation%20for%20the%20effectiveness%20of%20SNGD%20outside%20of%20the%20least-squares%20setting.%0AOverall%2C%20our%20results%20illustrate%20how%20tools%20from%20randomized%20linear%20algebra%20can%0Ashed%20new%20light%20on%20the%20interplay%20between%20subsampling%20and%20curvature-aware%0Aoptimization%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21022v1&entry.124074799=Read"},
{"title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis", "author": "Dennis Slobodzian and Karissa Tilbury and Amir Kordijazi", "abstract": "  Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications.\n", "link": "http://arxiv.org/abs/2508.20877v1", "date": "2025-08-28", "relevancy": 2.0727, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Framework%20for%20Early%20Detection%20of%20Pancreatic%20Cancer%20Using%0A%20%20Multi-Modal%20Medical%20Imaging%20Analysis&body=Title%3A%20Deep%20Learning%20Framework%20for%20Early%20Detection%20of%20Pancreatic%20Cancer%20Using%0A%20%20Multi-Modal%20Medical%20Imaging%20Analysis%0AAuthor%3A%20Dennis%20Slobodzian%20and%20Karissa%20Tilbury%20and%20Amir%20Kordijazi%0AAbstract%3A%20%20%20Pacreatic%20ductal%20adenocarcinoma%20%28PDAC%29%20remains%20one%20of%20the%20most%20lethal%20forms%0Aof%20cancer%2C%20with%20a%20five-year%20survival%20rate%20below%2010%25%20primarily%20due%20to%20late%0Adetection.%20This%20research%20develops%20and%20validates%20a%20deep%20learning%20framework%20for%0Aearly%20PDAC%20detection%20through%20analysis%20of%20dual-modality%20imaging%3A%0Aautofluorescence%20and%20second%20harmonic%20generation%20%28SHG%29.%20We%20analyzed%2040%20unique%0Apatient%20samples%20to%20create%20a%20specialized%20neural%20network%20capable%20of%0Adistinguishing%20between%20normal%2C%20fibrotic%2C%20and%20cancerous%20tissue.%20Our%20methodology%0Aevaluated%20six%20distinct%20deep%20learning%20architectures%2C%20comparing%20traditional%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20with%20modern%20Vision%20Transformers%20%28ViTs%29.%0AThrough%20systematic%20experimentation%2C%20we%20identified%20and%20overcome%20significant%0Achallenges%20in%20medical%20image%20analysis%2C%20including%20limited%20dataset%20size%20and%20class%0Aimbalance.%20The%20final%20optimized%20framework%2C%20based%20on%20a%20modified%20ResNet%0Aarchitecture%20with%20frozen%20pre-trained%20layers%20and%20class-weighted%20training%2C%0Aachieved%20over%2090%25%20accuracy%20in%20cancer%20detection.%20This%20represents%20a%20significant%0Aimprovement%20over%20current%20manual%20analysis%20methods%20an%20demonstrates%20potential%20for%0Aclinical%20deployment.%20This%20work%20establishes%20a%20robust%20pipeline%20for%20automated%20PDAC%0Adetection%20that%20can%20augment%20pathologists%27%20capabilities%20while%20providing%20a%0Afoundation%20for%20future%20expansion%20to%20other%20cancer%20types.%20The%20developed%0Amethodology%20also%20offers%20valuable%20insights%20for%20applying%20deep%20learning%20to%0Alimited-size%20medical%20imaging%20datasets%2C%20a%20common%20challenge%20in%20clinical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Framework%2520for%2520Early%2520Detection%2520of%2520Pancreatic%2520Cancer%2520Using%250A%2520%2520Multi-Modal%2520Medical%2520Imaging%2520Analysis%26entry.906535625%3DDennis%2520Slobodzian%2520and%2520Karissa%2520Tilbury%2520and%2520Amir%2520Kordijazi%26entry.1292438233%3D%2520%2520Pacreatic%2520ductal%2520adenocarcinoma%2520%2528PDAC%2529%2520remains%2520one%2520of%2520the%2520most%2520lethal%2520forms%250Aof%2520cancer%252C%2520with%2520a%2520five-year%2520survival%2520rate%2520below%252010%2525%2520primarily%2520due%2520to%2520late%250Adetection.%2520This%2520research%2520develops%2520and%2520validates%2520a%2520deep%2520learning%2520framework%2520for%250Aearly%2520PDAC%2520detection%2520through%2520analysis%2520of%2520dual-modality%2520imaging%253A%250Aautofluorescence%2520and%2520second%2520harmonic%2520generation%2520%2528SHG%2529.%2520We%2520analyzed%252040%2520unique%250Apatient%2520samples%2520to%2520create%2520a%2520specialized%2520neural%2520network%2520capable%2520of%250Adistinguishing%2520between%2520normal%252C%2520fibrotic%252C%2520and%2520cancerous%2520tissue.%2520Our%2520methodology%250Aevaluated%2520six%2520distinct%2520deep%2520learning%2520architectures%252C%2520comparing%2520traditional%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520with%2520modern%2520Vision%2520Transformers%2520%2528ViTs%2529.%250AThrough%2520systematic%2520experimentation%252C%2520we%2520identified%2520and%2520overcome%2520significant%250Achallenges%2520in%2520medical%2520image%2520analysis%252C%2520including%2520limited%2520dataset%2520size%2520and%2520class%250Aimbalance.%2520The%2520final%2520optimized%2520framework%252C%2520based%2520on%2520a%2520modified%2520ResNet%250Aarchitecture%2520with%2520frozen%2520pre-trained%2520layers%2520and%2520class-weighted%2520training%252C%250Aachieved%2520over%252090%2525%2520accuracy%2520in%2520cancer%2520detection.%2520This%2520represents%2520a%2520significant%250Aimprovement%2520over%2520current%2520manual%2520analysis%2520methods%2520an%2520demonstrates%2520potential%2520for%250Aclinical%2520deployment.%2520This%2520work%2520establishes%2520a%2520robust%2520pipeline%2520for%2520automated%2520PDAC%250Adetection%2520that%2520can%2520augment%2520pathologists%2527%2520capabilities%2520while%2520providing%2520a%250Afoundation%2520for%2520future%2520expansion%2520to%2520other%2520cancer%2520types.%2520The%2520developed%250Amethodology%2520also%2520offers%2520valuable%2520insights%2520for%2520applying%2520deep%2520learning%2520to%250Alimited-size%2520medical%2520imaging%2520datasets%252C%2520a%2520common%2520challenge%2520in%2520clinical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Framework%20for%20Early%20Detection%20of%20Pancreatic%20Cancer%20Using%0A%20%20Multi-Modal%20Medical%20Imaging%20Analysis&entry.906535625=Dennis%20Slobodzian%20and%20Karissa%20Tilbury%20and%20Amir%20Kordijazi&entry.1292438233=%20%20Pacreatic%20ductal%20adenocarcinoma%20%28PDAC%29%20remains%20one%20of%20the%20most%20lethal%20forms%0Aof%20cancer%2C%20with%20a%20five-year%20survival%20rate%20below%2010%25%20primarily%20due%20to%20late%0Adetection.%20This%20research%20develops%20and%20validates%20a%20deep%20learning%20framework%20for%0Aearly%20PDAC%20detection%20through%20analysis%20of%20dual-modality%20imaging%3A%0Aautofluorescence%20and%20second%20harmonic%20generation%20%28SHG%29.%20We%20analyzed%2040%20unique%0Apatient%20samples%20to%20create%20a%20specialized%20neural%20network%20capable%20of%0Adistinguishing%20between%20normal%2C%20fibrotic%2C%20and%20cancerous%20tissue.%20Our%20methodology%0Aevaluated%20six%20distinct%20deep%20learning%20architectures%2C%20comparing%20traditional%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20with%20modern%20Vision%20Transformers%20%28ViTs%29.%0AThrough%20systematic%20experimentation%2C%20we%20identified%20and%20overcome%20significant%0Achallenges%20in%20medical%20image%20analysis%2C%20including%20limited%20dataset%20size%20and%20class%0Aimbalance.%20The%20final%20optimized%20framework%2C%20based%20on%20a%20modified%20ResNet%0Aarchitecture%20with%20frozen%20pre-trained%20layers%20and%20class-weighted%20training%2C%0Aachieved%20over%2090%25%20accuracy%20in%20cancer%20detection.%20This%20represents%20a%20significant%0Aimprovement%20over%20current%20manual%20analysis%20methods%20an%20demonstrates%20potential%20for%0Aclinical%20deployment.%20This%20work%20establishes%20a%20robust%20pipeline%20for%20automated%20PDAC%0Adetection%20that%20can%20augment%20pathologists%27%20capabilities%20while%20providing%20a%0Afoundation%20for%20future%20expansion%20to%20other%20cancer%20types.%20The%20developed%0Amethodology%20also%20offers%20valuable%20insights%20for%20applying%20deep%20learning%20to%0Alimited-size%20medical%20imaging%20datasets%2C%20a%20common%20challenge%20in%20clinical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20877v1&entry.124074799=Read"},
{"title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation", "author": "Ruixuan Liu and Philip Huang and Ava Pun and Kangle Deng and Shobhit Aggarwal and Kevin Tang and Michelle Liu and Deva Ramanan and Jun-Yan Zhu and Jiaoyang Li and Changliu Liu", "abstract": "  Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.\n", "link": "http://arxiv.org/abs/2508.21063v1", "date": "2025-08-28", "relevancy": 2.056, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5339}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.521}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-to-Product%3A%20Generative%20Assembly%20via%20Bimanual%20Manipulation&body=Title%3A%20Prompt-to-Product%3A%20Generative%20Assembly%20via%20Bimanual%20Manipulation%0AAuthor%3A%20Ruixuan%20Liu%20and%20Philip%20Huang%20and%20Ava%20Pun%20and%20Kangle%20Deng%20and%20Shobhit%20Aggarwal%20and%20Kevin%20Tang%20and%20Michelle%20Liu%20and%20Deva%20Ramanan%20and%20Jun-Yan%20Zhu%20and%20Jiaoyang%20Li%20and%20Changliu%20Liu%0AAbstract%3A%20%20%20Creating%20assembly%20products%20demands%20significant%20manual%20effort%20and%20expert%0Aknowledge%20in%201%29%20designing%20the%20assembly%20and%202%29%20constructing%20the%20product.%20This%0Apaper%20introduces%20Prompt-to-Product%2C%20an%20automated%20pipeline%20that%20generates%0Areal-world%20assembly%20products%20from%20natural%20language%20prompts.%20Specifically%2C%20we%0Aleverage%20LEGO%20bricks%20as%20the%20assembly%20platform%20and%20automate%20the%20process%20of%0Acreating%20brick%20assembly%20structures.%20Given%20the%20user%20design%20requirements%2C%0APrompt-to-Product%20generates%20physically%20buildable%20brick%20designs%2C%20and%20then%0Aleverages%20a%20bimanual%20robotic%20system%20to%20construct%20the%20real%20assembly%20products%2C%0Abringing%20user%20imaginations%20into%20the%20real%20world.%20We%20conduct%20a%20comprehensive%20user%0Astudy%2C%20and%20the%20results%20demonstrate%20that%20Prompt-to-Product%20significantly%20lowers%0Athe%20barrier%20and%20reduces%20manual%20effort%20in%20creating%20assembly%20products%20from%0Aimaginative%20ideas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-to-Product%253A%2520Generative%2520Assembly%2520via%2520Bimanual%2520Manipulation%26entry.906535625%3DRuixuan%2520Liu%2520and%2520Philip%2520Huang%2520and%2520Ava%2520Pun%2520and%2520Kangle%2520Deng%2520and%2520Shobhit%2520Aggarwal%2520and%2520Kevin%2520Tang%2520and%2520Michelle%2520Liu%2520and%2520Deva%2520Ramanan%2520and%2520Jun-Yan%2520Zhu%2520and%2520Jiaoyang%2520Li%2520and%2520Changliu%2520Liu%26entry.1292438233%3D%2520%2520Creating%2520assembly%2520products%2520demands%2520significant%2520manual%2520effort%2520and%2520expert%250Aknowledge%2520in%25201%2529%2520designing%2520the%2520assembly%2520and%25202%2529%2520constructing%2520the%2520product.%2520This%250Apaper%2520introduces%2520Prompt-to-Product%252C%2520an%2520automated%2520pipeline%2520that%2520generates%250Areal-world%2520assembly%2520products%2520from%2520natural%2520language%2520prompts.%2520Specifically%252C%2520we%250Aleverage%2520LEGO%2520bricks%2520as%2520the%2520assembly%2520platform%2520and%2520automate%2520the%2520process%2520of%250Acreating%2520brick%2520assembly%2520structures.%2520Given%2520the%2520user%2520design%2520requirements%252C%250APrompt-to-Product%2520generates%2520physically%2520buildable%2520brick%2520designs%252C%2520and%2520then%250Aleverages%2520a%2520bimanual%2520robotic%2520system%2520to%2520construct%2520the%2520real%2520assembly%2520products%252C%250Abringing%2520user%2520imaginations%2520into%2520the%2520real%2520world.%2520We%2520conduct%2520a%2520comprehensive%2520user%250Astudy%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520Prompt-to-Product%2520significantly%2520lowers%250Athe%2520barrier%2520and%2520reduces%2520manual%2520effort%2520in%2520creating%2520assembly%2520products%2520from%250Aimaginative%2520ideas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-to-Product%3A%20Generative%20Assembly%20via%20Bimanual%20Manipulation&entry.906535625=Ruixuan%20Liu%20and%20Philip%20Huang%20and%20Ava%20Pun%20and%20Kangle%20Deng%20and%20Shobhit%20Aggarwal%20and%20Kevin%20Tang%20and%20Michelle%20Liu%20and%20Deva%20Ramanan%20and%20Jun-Yan%20Zhu%20and%20Jiaoyang%20Li%20and%20Changliu%20Liu&entry.1292438233=%20%20Creating%20assembly%20products%20demands%20significant%20manual%20effort%20and%20expert%0Aknowledge%20in%201%29%20designing%20the%20assembly%20and%202%29%20constructing%20the%20product.%20This%0Apaper%20introduces%20Prompt-to-Product%2C%20an%20automated%20pipeline%20that%20generates%0Areal-world%20assembly%20products%20from%20natural%20language%20prompts.%20Specifically%2C%20we%0Aleverage%20LEGO%20bricks%20as%20the%20assembly%20platform%20and%20automate%20the%20process%20of%0Acreating%20brick%20assembly%20structures.%20Given%20the%20user%20design%20requirements%2C%0APrompt-to-Product%20generates%20physically%20buildable%20brick%20designs%2C%20and%20then%0Aleverages%20a%20bimanual%20robotic%20system%20to%20construct%20the%20real%20assembly%20products%2C%0Abringing%20user%20imaginations%20into%20the%20real%20world.%20We%20conduct%20a%20comprehensive%20user%0Astudy%2C%20and%20the%20results%20demonstrate%20that%20Prompt-to-Product%20significantly%20lowers%0Athe%20barrier%20and%20reduces%20manual%20effort%20in%20creating%20assembly%20products%20from%0Aimaginative%20ideas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21063v1&entry.124074799=Read"},
{"title": "Expert Routing with Synthetic Data for Continual Learning", "author": "Yewon Byun and Sanket Vaibhav Mehta and Saurabh Garg and Emma Strubell and Michael Oberst and Bryan Wilder and Zachary C. Lipton", "abstract": "  In many real-world settings, regulations and economic incentives permit the\nsharing of models but not data across institutional boundaries. In such\nscenarios, practitioners might hope to adapt models to new domains, without\nlosing performance on previous domains (so-called catastrophic forgetting).\nWhile any single model may struggle to achieve this goal, learning an ensemble\nof domain-specific experts offers the potential to adapt more closely to each\nindividual institution. However, a core challenge in this context is\ndetermining which expert to deploy at test time. In this paper, we propose\nGenerate to Discriminate (G2D), a domain-incremental continual learning method\nthat leverages synthetic data to train a domain-discriminator that routes\nsamples at inference time to the appropriate expert. Surprisingly, we find that\nleveraging synthetic data in this capacity is more effective than using the\nsamples to \\textit{directly} train the downstream classifier (the more common\napproach to leveraging synthetic data in the lifelong learning literature). We\nobserve that G2D outperforms competitive domain-incremental learning methods on\ntasks in both vision and language modalities, providing a new perspective on\nthe use of synthetic data in the lifelong learning literature.\n", "link": "http://arxiv.org/abs/2412.17009v3", "date": "2025-08-28", "relevancy": 2.055, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.514}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert%20Routing%20with%20Synthetic%20Data%20for%20Continual%20Learning&body=Title%3A%20Expert%20Routing%20with%20Synthetic%20Data%20for%20Continual%20Learning%0AAuthor%3A%20Yewon%20Byun%20and%20Sanket%20Vaibhav%20Mehta%20and%20Saurabh%20Garg%20and%20Emma%20Strubell%20and%20Michael%20Oberst%20and%20Bryan%20Wilder%20and%20Zachary%20C.%20Lipton%0AAbstract%3A%20%20%20In%20many%20real-world%20settings%2C%20regulations%20and%20economic%20incentives%20permit%20the%0Asharing%20of%20models%20but%20not%20data%20across%20institutional%20boundaries.%20In%20such%0Ascenarios%2C%20practitioners%20might%20hope%20to%20adapt%20models%20to%20new%20domains%2C%20without%0Alosing%20performance%20on%20previous%20domains%20%28so-called%20catastrophic%20forgetting%29.%0AWhile%20any%20single%20model%20may%20struggle%20to%20achieve%20this%20goal%2C%20learning%20an%20ensemble%0Aof%20domain-specific%20experts%20offers%20the%20potential%20to%20adapt%20more%20closely%20to%20each%0Aindividual%20institution.%20However%2C%20a%20core%20challenge%20in%20this%20context%20is%0Adetermining%20which%20expert%20to%20deploy%20at%20test%20time.%20In%20this%20paper%2C%20we%20propose%0AGenerate%20to%20Discriminate%20%28G2D%29%2C%20a%20domain-incremental%20continual%20learning%20method%0Athat%20leverages%20synthetic%20data%20to%20train%20a%20domain-discriminator%20that%20routes%0Asamples%20at%20inference%20time%20to%20the%20appropriate%20expert.%20Surprisingly%2C%20we%20find%20that%0Aleveraging%20synthetic%20data%20in%20this%20capacity%20is%20more%20effective%20than%20using%20the%0Asamples%20to%20%5Ctextit%7Bdirectly%7D%20train%20the%20downstream%20classifier%20%28the%20more%20common%0Aapproach%20to%20leveraging%20synthetic%20data%20in%20the%20lifelong%20learning%20literature%29.%20We%0Aobserve%20that%20G2D%20outperforms%20competitive%20domain-incremental%20learning%20methods%20on%0Atasks%20in%20both%20vision%20and%20language%20modalities%2C%20providing%20a%20new%20perspective%20on%0Athe%20use%20of%20synthetic%20data%20in%20the%20lifelong%20learning%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert%2520Routing%2520with%2520Synthetic%2520Data%2520for%2520Continual%2520Learning%26entry.906535625%3DYewon%2520Byun%2520and%2520Sanket%2520Vaibhav%2520Mehta%2520and%2520Saurabh%2520Garg%2520and%2520Emma%2520Strubell%2520and%2520Michael%2520Oberst%2520and%2520Bryan%2520Wilder%2520and%2520Zachary%2520C.%2520Lipton%26entry.1292438233%3D%2520%2520In%2520many%2520real-world%2520settings%252C%2520regulations%2520and%2520economic%2520incentives%2520permit%2520the%250Asharing%2520of%2520models%2520but%2520not%2520data%2520across%2520institutional%2520boundaries.%2520In%2520such%250Ascenarios%252C%2520practitioners%2520might%2520hope%2520to%2520adapt%2520models%2520to%2520new%2520domains%252C%2520without%250Alosing%2520performance%2520on%2520previous%2520domains%2520%2528so-called%2520catastrophic%2520forgetting%2529.%250AWhile%2520any%2520single%2520model%2520may%2520struggle%2520to%2520achieve%2520this%2520goal%252C%2520learning%2520an%2520ensemble%250Aof%2520domain-specific%2520experts%2520offers%2520the%2520potential%2520to%2520adapt%2520more%2520closely%2520to%2520each%250Aindividual%2520institution.%2520However%252C%2520a%2520core%2520challenge%2520in%2520this%2520context%2520is%250Adetermining%2520which%2520expert%2520to%2520deploy%2520at%2520test%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%250AGenerate%2520to%2520Discriminate%2520%2528G2D%2529%252C%2520a%2520domain-incremental%2520continual%2520learning%2520method%250Athat%2520leverages%2520synthetic%2520data%2520to%2520train%2520a%2520domain-discriminator%2520that%2520routes%250Asamples%2520at%2520inference%2520time%2520to%2520the%2520appropriate%2520expert.%2520Surprisingly%252C%2520we%2520find%2520that%250Aleveraging%2520synthetic%2520data%2520in%2520this%2520capacity%2520is%2520more%2520effective%2520than%2520using%2520the%250Asamples%2520to%2520%255Ctextit%257Bdirectly%257D%2520train%2520the%2520downstream%2520classifier%2520%2528the%2520more%2520common%250Aapproach%2520to%2520leveraging%2520synthetic%2520data%2520in%2520the%2520lifelong%2520learning%2520literature%2529.%2520We%250Aobserve%2520that%2520G2D%2520outperforms%2520competitive%2520domain-incremental%2520learning%2520methods%2520on%250Atasks%2520in%2520both%2520vision%2520and%2520language%2520modalities%252C%2520providing%2520a%2520new%2520perspective%2520on%250Athe%2520use%2520of%2520synthetic%2520data%2520in%2520the%2520lifelong%2520learning%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert%20Routing%20with%20Synthetic%20Data%20for%20Continual%20Learning&entry.906535625=Yewon%20Byun%20and%20Sanket%20Vaibhav%20Mehta%20and%20Saurabh%20Garg%20and%20Emma%20Strubell%20and%20Michael%20Oberst%20and%20Bryan%20Wilder%20and%20Zachary%20C.%20Lipton&entry.1292438233=%20%20In%20many%20real-world%20settings%2C%20regulations%20and%20economic%20incentives%20permit%20the%0Asharing%20of%20models%20but%20not%20data%20across%20institutional%20boundaries.%20In%20such%0Ascenarios%2C%20practitioners%20might%20hope%20to%20adapt%20models%20to%20new%20domains%2C%20without%0Alosing%20performance%20on%20previous%20domains%20%28so-called%20catastrophic%20forgetting%29.%0AWhile%20any%20single%20model%20may%20struggle%20to%20achieve%20this%20goal%2C%20learning%20an%20ensemble%0Aof%20domain-specific%20experts%20offers%20the%20potential%20to%20adapt%20more%20closely%20to%20each%0Aindividual%20institution.%20However%2C%20a%20core%20challenge%20in%20this%20context%20is%0Adetermining%20which%20expert%20to%20deploy%20at%20test%20time.%20In%20this%20paper%2C%20we%20propose%0AGenerate%20to%20Discriminate%20%28G2D%29%2C%20a%20domain-incremental%20continual%20learning%20method%0Athat%20leverages%20synthetic%20data%20to%20train%20a%20domain-discriminator%20that%20routes%0Asamples%20at%20inference%20time%20to%20the%20appropriate%20expert.%20Surprisingly%2C%20we%20find%20that%0Aleveraging%20synthetic%20data%20in%20this%20capacity%20is%20more%20effective%20than%20using%20the%0Asamples%20to%20%5Ctextit%7Bdirectly%7D%20train%20the%20downstream%20classifier%20%28the%20more%20common%0Aapproach%20to%20leveraging%20synthetic%20data%20in%20the%20lifelong%20learning%20literature%29.%20We%0Aobserve%20that%20G2D%20outperforms%20competitive%20domain-incremental%20learning%20methods%20on%0Atasks%20in%20both%20vision%20and%20language%20modalities%2C%20providing%20a%20new%20perspective%20on%0Athe%20use%20of%20synthetic%20data%20in%20the%20lifelong%20learning%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17009v3&entry.124074799=Read"},
{"title": "PLUME: Procedural Layer Underground Modeling Engine", "author": "Gabriel Manuel Garcia and Antoine Richard and Miguel Olivares-Mendez", "abstract": "  As space exploration advances, underground environments are becoming\nincreasingly attractive due to their potential to provide shelter, easier\naccess to resources, and enhanced scientific opportunities. Although such\nenvironments exist on Earth, they are often not easily accessible and do not\naccurately represent the diversity of underground environments found throughout\nthe solar system. This paper presents PLUME, a procedural generation framework\naimed at easily creating 3D underground environments. Its flexible structure\nallows for the continuous enhancement of various underground features, aligning\nwith our expanding understanding of the solar system. The environments\ngenerated using PLUME can be used for AI training, evaluating robotics\nalgorithms, 3D rendering, and facilitating rapid iteration on developed\nexploration algorithms. In this paper, it is demonstrated that PLUME has been\nused along with a robotic simulator. PLUME is open source and has been released\non Github. https://github.com/Gabryss/P.L.U.M.E\n", "link": "http://arxiv.org/abs/2508.20926v1", "date": "2025-08-28", "relevancy": 2.052, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5275}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5127}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLUME%3A%20Procedural%20Layer%20Underground%20Modeling%20Engine&body=Title%3A%20PLUME%3A%20Procedural%20Layer%20Underground%20Modeling%20Engine%0AAuthor%3A%20Gabriel%20Manuel%20Garcia%20and%20Antoine%20Richard%20and%20Miguel%20Olivares-Mendez%0AAbstract%3A%20%20%20As%20space%20exploration%20advances%2C%20underground%20environments%20are%20becoming%0Aincreasingly%20attractive%20due%20to%20their%20potential%20to%20provide%20shelter%2C%20easier%0Aaccess%20to%20resources%2C%20and%20enhanced%20scientific%20opportunities.%20Although%20such%0Aenvironments%20exist%20on%20Earth%2C%20they%20are%20often%20not%20easily%20accessible%20and%20do%20not%0Aaccurately%20represent%20the%20diversity%20of%20underground%20environments%20found%20throughout%0Athe%20solar%20system.%20This%20paper%20presents%20PLUME%2C%20a%20procedural%20generation%20framework%0Aaimed%20at%20easily%20creating%203D%20underground%20environments.%20Its%20flexible%20structure%0Aallows%20for%20the%20continuous%20enhancement%20of%20various%20underground%20features%2C%20aligning%0Awith%20our%20expanding%20understanding%20of%20the%20solar%20system.%20The%20environments%0Agenerated%20using%20PLUME%20can%20be%20used%20for%20AI%20training%2C%20evaluating%20robotics%0Aalgorithms%2C%203D%20rendering%2C%20and%20facilitating%20rapid%20iteration%20on%20developed%0Aexploration%20algorithms.%20In%20this%20paper%2C%20it%20is%20demonstrated%20that%20PLUME%20has%20been%0Aused%20along%20with%20a%20robotic%20simulator.%20PLUME%20is%20open%20source%20and%20has%20been%20released%0Aon%20Github.%20https%3A//github.com/Gabryss/P.L.U.M.E%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLUME%253A%2520Procedural%2520Layer%2520Underground%2520Modeling%2520Engine%26entry.906535625%3DGabriel%2520Manuel%2520Garcia%2520and%2520Antoine%2520Richard%2520and%2520Miguel%2520Olivares-Mendez%26entry.1292438233%3D%2520%2520As%2520space%2520exploration%2520advances%252C%2520underground%2520environments%2520are%2520becoming%250Aincreasingly%2520attractive%2520due%2520to%2520their%2520potential%2520to%2520provide%2520shelter%252C%2520easier%250Aaccess%2520to%2520resources%252C%2520and%2520enhanced%2520scientific%2520opportunities.%2520Although%2520such%250Aenvironments%2520exist%2520on%2520Earth%252C%2520they%2520are%2520often%2520not%2520easily%2520accessible%2520and%2520do%2520not%250Aaccurately%2520represent%2520the%2520diversity%2520of%2520underground%2520environments%2520found%2520throughout%250Athe%2520solar%2520system.%2520This%2520paper%2520presents%2520PLUME%252C%2520a%2520procedural%2520generation%2520framework%250Aaimed%2520at%2520easily%2520creating%25203D%2520underground%2520environments.%2520Its%2520flexible%2520structure%250Aallows%2520for%2520the%2520continuous%2520enhancement%2520of%2520various%2520underground%2520features%252C%2520aligning%250Awith%2520our%2520expanding%2520understanding%2520of%2520the%2520solar%2520system.%2520The%2520environments%250Agenerated%2520using%2520PLUME%2520can%2520be%2520used%2520for%2520AI%2520training%252C%2520evaluating%2520robotics%250Aalgorithms%252C%25203D%2520rendering%252C%2520and%2520facilitating%2520rapid%2520iteration%2520on%2520developed%250Aexploration%2520algorithms.%2520In%2520this%2520paper%252C%2520it%2520is%2520demonstrated%2520that%2520PLUME%2520has%2520been%250Aused%2520along%2520with%2520a%2520robotic%2520simulator.%2520PLUME%2520is%2520open%2520source%2520and%2520has%2520been%2520released%250Aon%2520Github.%2520https%253A//github.com/Gabryss/P.L.U.M.E%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLUME%3A%20Procedural%20Layer%20Underground%20Modeling%20Engine&entry.906535625=Gabriel%20Manuel%20Garcia%20and%20Antoine%20Richard%20and%20Miguel%20Olivares-Mendez&entry.1292438233=%20%20As%20space%20exploration%20advances%2C%20underground%20environments%20are%20becoming%0Aincreasingly%20attractive%20due%20to%20their%20potential%20to%20provide%20shelter%2C%20easier%0Aaccess%20to%20resources%2C%20and%20enhanced%20scientific%20opportunities.%20Although%20such%0Aenvironments%20exist%20on%20Earth%2C%20they%20are%20often%20not%20easily%20accessible%20and%20do%20not%0Aaccurately%20represent%20the%20diversity%20of%20underground%20environments%20found%20throughout%0Athe%20solar%20system.%20This%20paper%20presents%20PLUME%2C%20a%20procedural%20generation%20framework%0Aaimed%20at%20easily%20creating%203D%20underground%20environments.%20Its%20flexible%20structure%0Aallows%20for%20the%20continuous%20enhancement%20of%20various%20underground%20features%2C%20aligning%0Awith%20our%20expanding%20understanding%20of%20the%20solar%20system.%20The%20environments%0Agenerated%20using%20PLUME%20can%20be%20used%20for%20AI%20training%2C%20evaluating%20robotics%0Aalgorithms%2C%203D%20rendering%2C%20and%20facilitating%20rapid%20iteration%20on%20developed%0Aexploration%20algorithms.%20In%20this%20paper%2C%20it%20is%20demonstrated%20that%20PLUME%20has%20been%0Aused%20along%20with%20a%20robotic%20simulator.%20PLUME%20is%20open%20source%20and%20has%20been%20released%0Aon%20Github.%20https%3A//github.com/Gabryss/P.L.U.M.E%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20926v1&entry.124074799=Read"},
{"title": "Program Semantic Inequivalence Game with Large Language Models", "author": "Antonio Valerio Miceli-Barone and Vaishak Belle and Ali Payani", "abstract": "  Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs.\n", "link": "http://arxiv.org/abs/2505.03818v2", "date": "2025-08-28", "relevancy": 2.0374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Program%20Semantic%20Inequivalence%20Game%20with%20Large%20Language%20Models&body=Title%3A%20Program%20Semantic%20Inequivalence%20Game%20with%20Large%20Language%20Models%0AAuthor%3A%20Antonio%20Valerio%20Miceli-Barone%20and%20Vaishak%20Belle%20and%20Ali%20Payani%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20achieve%20strong%20performance%20on%20everyday%0Acoding%20tasks%2C%20but%20they%20can%20fail%20on%20complex%20tasks%20that%20require%20non-trivial%0Areasoning%20about%20program%20semantics.%20Finding%20training%20examples%20to%20teach%20LLMs%20to%0Asolve%20these%20tasks%20can%20be%20challenging.%0A%20%20In%20this%20work%2C%20we%20explore%20a%20method%20to%20synthetically%20generate%20code%20reasoning%0Atraining%20data%20based%20on%20a%20semantic%20inequivalence%20game%20SInQ%3A%20a%20generator%20agent%0Acreates%20program%20variants%20that%20are%20semantically%20distinct%2C%20derived%20from%20a%20dataset%0Aof%20real-world%20programming%20tasks%2C%20while%20an%20evaluator%20agent%20has%20to%20identify%20input%0Aexamples%20that%20cause%20the%20original%20programs%20and%20the%20generated%20variants%20to%20diverge%0Ain%20their%20behaviour%2C%20with%20the%20agents%20training%20each%20other%20semi-adversarially.%20We%0Aprove%20that%20this%20setup%20enables%20theoretically%20unlimited%20improvement%20through%0Aself-play%20in%20the%20limit%20of%20infinite%20computational%20resources.%0A%20%20We%20evaluated%20our%20approach%20on%20multiple%20code%20generation%20and%20understanding%0Abenchmarks%2C%20including%20cross-language%20vulnerability%20detection%20%28Lu%20et%20al.%2C%202021%29%2C%0Awhere%20our%20method%20improves%20vulnerability%20detection%20in%20C/C%2B%2B%20code%20despite%20being%0Atrained%20exclusively%20on%20Python%20code%2C%20and%20the%20challenging%20Python%20builtin%0Aidentifier%20swap%20benchmark%20%28Miceli-Barone%20et%20al.%2C%202023%29%2C%20showing%20that%20whereas%0Amodern%20LLMs%20still%20struggle%20with%20this%20benchmark%2C%20our%20approach%20yields%20substantial%0Aimprovements.%0A%20%20We%20release%20the%20code%20needed%20to%20replicate%20the%20experiments%2C%20as%20well%20as%20the%0Agenerated%20synthetic%20data%2C%20which%20can%20be%20used%20to%20fine-tune%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgram%2520Semantic%2520Inequivalence%2520Game%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DAntonio%2520Valerio%2520Miceli-Barone%2520and%2520Vaishak%2520Belle%2520and%2520Ali%2520Payani%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520achieve%2520strong%2520performance%2520on%2520everyday%250Acoding%2520tasks%252C%2520but%2520they%2520can%2520fail%2520on%2520complex%2520tasks%2520that%2520require%2520non-trivial%250Areasoning%2520about%2520program%2520semantics.%2520Finding%2520training%2520examples%2520to%2520teach%2520LLMs%2520to%250Asolve%2520these%2520tasks%2520can%2520be%2520challenging.%250A%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520method%2520to%2520synthetically%2520generate%2520code%2520reasoning%250Atraining%2520data%2520based%2520on%2520a%2520semantic%2520inequivalence%2520game%2520SInQ%253A%2520a%2520generator%2520agent%250Acreates%2520program%2520variants%2520that%2520are%2520semantically%2520distinct%252C%2520derived%2520from%2520a%2520dataset%250Aof%2520real-world%2520programming%2520tasks%252C%2520while%2520an%2520evaluator%2520agent%2520has%2520to%2520identify%2520input%250Aexamples%2520that%2520cause%2520the%2520original%2520programs%2520and%2520the%2520generated%2520variants%2520to%2520diverge%250Ain%2520their%2520behaviour%252C%2520with%2520the%2520agents%2520training%2520each%2520other%2520semi-adversarially.%2520We%250Aprove%2520that%2520this%2520setup%2520enables%2520theoretically%2520unlimited%2520improvement%2520through%250Aself-play%2520in%2520the%2520limit%2520of%2520infinite%2520computational%2520resources.%250A%2520%2520We%2520evaluated%2520our%2520approach%2520on%2520multiple%2520code%2520generation%2520and%2520understanding%250Abenchmarks%252C%2520including%2520cross-language%2520vulnerability%2520detection%2520%2528Lu%2520et%2520al.%252C%25202021%2529%252C%250Awhere%2520our%2520method%2520improves%2520vulnerability%2520detection%2520in%2520C/C%252B%252B%2520code%2520despite%2520being%250Atrained%2520exclusively%2520on%2520Python%2520code%252C%2520and%2520the%2520challenging%2520Python%2520builtin%250Aidentifier%2520swap%2520benchmark%2520%2528Miceli-Barone%2520et%2520al.%252C%25202023%2529%252C%2520showing%2520that%2520whereas%250Amodern%2520LLMs%2520still%2520struggle%2520with%2520this%2520benchmark%252C%2520our%2520approach%2520yields%2520substantial%250Aimprovements.%250A%2520%2520We%2520release%2520the%2520code%2520needed%2520to%2520replicate%2520the%2520experiments%252C%2520as%2520well%2520as%2520the%250Agenerated%2520synthetic%2520data%252C%2520which%2520can%2520be%2520used%2520to%2520fine-tune%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Program%20Semantic%20Inequivalence%20Game%20with%20Large%20Language%20Models&entry.906535625=Antonio%20Valerio%20Miceli-Barone%20and%20Vaishak%20Belle%20and%20Ali%20Payani&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20achieve%20strong%20performance%20on%20everyday%0Acoding%20tasks%2C%20but%20they%20can%20fail%20on%20complex%20tasks%20that%20require%20non-trivial%0Areasoning%20about%20program%20semantics.%20Finding%20training%20examples%20to%20teach%20LLMs%20to%0Asolve%20these%20tasks%20can%20be%20challenging.%0A%20%20In%20this%20work%2C%20we%20explore%20a%20method%20to%20synthetically%20generate%20code%20reasoning%0Atraining%20data%20based%20on%20a%20semantic%20inequivalence%20game%20SInQ%3A%20a%20generator%20agent%0Acreates%20program%20variants%20that%20are%20semantically%20distinct%2C%20derived%20from%20a%20dataset%0Aof%20real-world%20programming%20tasks%2C%20while%20an%20evaluator%20agent%20has%20to%20identify%20input%0Aexamples%20that%20cause%20the%20original%20programs%20and%20the%20generated%20variants%20to%20diverge%0Ain%20their%20behaviour%2C%20with%20the%20agents%20training%20each%20other%20semi-adversarially.%20We%0Aprove%20that%20this%20setup%20enables%20theoretically%20unlimited%20improvement%20through%0Aself-play%20in%20the%20limit%20of%20infinite%20computational%20resources.%0A%20%20We%20evaluated%20our%20approach%20on%20multiple%20code%20generation%20and%20understanding%0Abenchmarks%2C%20including%20cross-language%20vulnerability%20detection%20%28Lu%20et%20al.%2C%202021%29%2C%0Awhere%20our%20method%20improves%20vulnerability%20detection%20in%20C/C%2B%2B%20code%20despite%20being%0Atrained%20exclusively%20on%20Python%20code%2C%20and%20the%20challenging%20Python%20builtin%0Aidentifier%20swap%20benchmark%20%28Miceli-Barone%20et%20al.%2C%202023%29%2C%20showing%20that%20whereas%0Amodern%20LLMs%20still%20struggle%20with%20this%20benchmark%2C%20our%20approach%20yields%20substantial%0Aimprovements.%0A%20%20We%20release%20the%20code%20needed%20to%20replicate%20the%20experiments%2C%20as%20well%20as%20the%0Agenerated%20synthetic%20data%2C%20which%20can%20be%20used%20to%20fine-tune%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03818v2&entry.124074799=Read"},
{"title": "FakeParts: a New Family of AI-Generated DeepFakes", "author": "Gaetan Brison and Soobash Daiboo and Samy Aimeur and Awais Hussain Sani and Xi Wang and Gianni Franchi and Vicky Kalogeiton", "abstract": "  We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.\n", "link": "http://arxiv.org/abs/2508.21052v1", "date": "2025-08-28", "relevancy": 2.0356, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5197}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5073}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes&body=Title%3A%20FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes%0AAuthor%3A%20Gaetan%20Brison%20and%20Soobash%20Daiboo%20and%20Samy%20Aimeur%20and%20Awais%20Hussain%20Sani%20and%20Xi%20Wang%20and%20Gianni%20Franchi%20and%20Vicky%20Kalogeiton%0AAbstract%3A%20%20%20We%20introduce%20FakeParts%2C%20a%20new%20class%20of%20deepfakes%20characterized%20by%20subtle%2C%0Alocalized%20manipulations%20to%20specific%20spatial%20regions%20or%20temporal%20segments%20of%0Aotherwise%20authentic%20videos.%20Unlike%20fully%20synthetic%20content%2C%20these%20partial%0Amanipulations%2C%20ranging%20from%20altered%20facial%20expressions%20to%20object%20substitutions%0Aand%20background%20modifications%2C%20blend%20seamlessly%20with%20real%20elements%2C%20making%20them%0Aparticularly%20deceptive%20and%20difficult%20to%20detect.%20To%20address%20the%20critical%20gap%20in%0Adetection%20capabilities%2C%20we%20present%20FakePartsBench%2C%20the%20first%20large-scale%0Abenchmark%20dataset%20specifically%20designed%20to%20capture%20the%20full%20spectrum%20of%20partial%0Adeepfakes.%20Comprising%20over%2025K%20videos%20with%20pixel-level%20and%20frame-level%0Amanipulation%20annotations%2C%20our%20dataset%20enables%20comprehensive%20evaluation%20of%0Adetection%20methods.%20Our%20user%20studies%20demonstrate%20that%20FakeParts%20reduces%20human%0Adetection%20accuracy%20by%20over%2030%25%20compared%20to%20traditional%20deepfakes%2C%20with%20similar%0Aperformance%20degradation%20observed%20in%20state-of-the-art%20detection%20models.%20This%0Awork%20identifies%20an%20urgent%20vulnerability%20in%20current%20deepfake%20detection%0Aapproaches%20and%20provides%20the%20necessary%20resources%20to%20develop%20more%20robust%20methods%0Afor%20partial%20video%20manipulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeParts%253A%2520a%2520New%2520Family%2520of%2520AI-Generated%2520DeepFakes%26entry.906535625%3DGaetan%2520Brison%2520and%2520Soobash%2520Daiboo%2520and%2520Samy%2520Aimeur%2520and%2520Awais%2520Hussain%2520Sani%2520and%2520Xi%2520Wang%2520and%2520Gianni%2520Franchi%2520and%2520Vicky%2520Kalogeiton%26entry.1292438233%3D%2520%2520We%2520introduce%2520FakeParts%252C%2520a%2520new%2520class%2520of%2520deepfakes%2520characterized%2520by%2520subtle%252C%250Alocalized%2520manipulations%2520to%2520specific%2520spatial%2520regions%2520or%2520temporal%2520segments%2520of%250Aotherwise%2520authentic%2520videos.%2520Unlike%2520fully%2520synthetic%2520content%252C%2520these%2520partial%250Amanipulations%252C%2520ranging%2520from%2520altered%2520facial%2520expressions%2520to%2520object%2520substitutions%250Aand%2520background%2520modifications%252C%2520blend%2520seamlessly%2520with%2520real%2520elements%252C%2520making%2520them%250Aparticularly%2520deceptive%2520and%2520difficult%2520to%2520detect.%2520To%2520address%2520the%2520critical%2520gap%2520in%250Adetection%2520capabilities%252C%2520we%2520present%2520FakePartsBench%252C%2520the%2520first%2520large-scale%250Abenchmark%2520dataset%2520specifically%2520designed%2520to%2520capture%2520the%2520full%2520spectrum%2520of%2520partial%250Adeepfakes.%2520Comprising%2520over%252025K%2520videos%2520with%2520pixel-level%2520and%2520frame-level%250Amanipulation%2520annotations%252C%2520our%2520dataset%2520enables%2520comprehensive%2520evaluation%2520of%250Adetection%2520methods.%2520Our%2520user%2520studies%2520demonstrate%2520that%2520FakeParts%2520reduces%2520human%250Adetection%2520accuracy%2520by%2520over%252030%2525%2520compared%2520to%2520traditional%2520deepfakes%252C%2520with%2520similar%250Aperformance%2520degradation%2520observed%2520in%2520state-of-the-art%2520detection%2520models.%2520This%250Awork%2520identifies%2520an%2520urgent%2520vulnerability%2520in%2520current%2520deepfake%2520detection%250Aapproaches%2520and%2520provides%2520the%2520necessary%2520resources%2520to%2520develop%2520more%2520robust%2520methods%250Afor%2520partial%2520video%2520manipulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes&entry.906535625=Gaetan%20Brison%20and%20Soobash%20Daiboo%20and%20Samy%20Aimeur%20and%20Awais%20Hussain%20Sani%20and%20Xi%20Wang%20and%20Gianni%20Franchi%20and%20Vicky%20Kalogeiton&entry.1292438233=%20%20We%20introduce%20FakeParts%2C%20a%20new%20class%20of%20deepfakes%20characterized%20by%20subtle%2C%0Alocalized%20manipulations%20to%20specific%20spatial%20regions%20or%20temporal%20segments%20of%0Aotherwise%20authentic%20videos.%20Unlike%20fully%20synthetic%20content%2C%20these%20partial%0Amanipulations%2C%20ranging%20from%20altered%20facial%20expressions%20to%20object%20substitutions%0Aand%20background%20modifications%2C%20blend%20seamlessly%20with%20real%20elements%2C%20making%20them%0Aparticularly%20deceptive%20and%20difficult%20to%20detect.%20To%20address%20the%20critical%20gap%20in%0Adetection%20capabilities%2C%20we%20present%20FakePartsBench%2C%20the%20first%20large-scale%0Abenchmark%20dataset%20specifically%20designed%20to%20capture%20the%20full%20spectrum%20of%20partial%0Adeepfakes.%20Comprising%20over%2025K%20videos%20with%20pixel-level%20and%20frame-level%0Amanipulation%20annotations%2C%20our%20dataset%20enables%20comprehensive%20evaluation%20of%0Adetection%20methods.%20Our%20user%20studies%20demonstrate%20that%20FakeParts%20reduces%20human%0Adetection%20accuracy%20by%20over%2030%25%20compared%20to%20traditional%20deepfakes%2C%20with%20similar%0Aperformance%20degradation%20observed%20in%20state-of-the-art%20detection%20models.%20This%0Awork%20identifies%20an%20urgent%20vulnerability%20in%20current%20deepfake%20detection%0Aapproaches%20and%20provides%20the%20necessary%20resources%20to%20develop%20more%20robust%20methods%0Afor%20partial%20video%20manipulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21052v1&entry.124074799=Read"},
{"title": "TacCompress: A Benchmark for Multi-Point Tactile Data Compression in\n  Dexterous Hand", "author": "Yan Zhao and Yang Li and Zhengxue Cheng and Hengdi Zhang and Li Song", "abstract": "  Though robotic dexterous manipulation has progressed substantially recently,\nchallenges like in-hand occlusion still necessitate fine-grained tactile\nperception, leading to the integration of more tactile sensors into robotic\nhands. Consequently, the increased data volume imposes substantial bandwidth\npressure on signal transmission from the hand's controller. However, the\nacquisition and compression of multi-point tactile signals based on the\ndexterous hands' physical structures have not been thoroughly explored. In this\npaper, our contributions are twofold. First, we introduce a Multi-Point Tactile\nDataset for Dexterous Hand Grasping (Dex-MPTD). This dataset captures tactile\nsignals from multiple contact sensors across various objects and grasping\nposes, offering a comprehensive benchmark for advancing dexterous robotic\nmanipulation research. Second, we investigate both lossless and lossy\ncompression on Dex-MPTD by converting tactile data into images and applying six\nlossless and five lossy image codecs for efficient compression. Experimental\nresults demonstrate that tactile data can be losslessly compressed to as low as\n0.0364 bits per sub-sample (bpss), achieving approximately 200$\\times$\ncompression ratio compared to the raw tactile data. Efficient lossy compressors\nlike HM and VTM can achieve about 1000$\\times$ data reductions while preserving\nacceptable data fidelity. The exploration of lossy compression also reveals\nthat screen-content-targeted coding tools outperform general-purpose codecs in\ncompressing tactile data.\n", "link": "http://arxiv.org/abs/2505.16289v2", "date": "2025-08-28", "relevancy": 2.0304, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TacCompress%3A%20A%20Benchmark%20for%20Multi-Point%20Tactile%20Data%20Compression%20in%0A%20%20Dexterous%20Hand&body=Title%3A%20TacCompress%3A%20A%20Benchmark%20for%20Multi-Point%20Tactile%20Data%20Compression%20in%0A%20%20Dexterous%20Hand%0AAuthor%3A%20Yan%20Zhao%20and%20Yang%20Li%20and%20Zhengxue%20Cheng%20and%20Hengdi%20Zhang%20and%20Li%20Song%0AAbstract%3A%20%20%20Though%20robotic%20dexterous%20manipulation%20has%20progressed%20substantially%20recently%2C%0Achallenges%20like%20in-hand%20occlusion%20still%20necessitate%20fine-grained%20tactile%0Aperception%2C%20leading%20to%20the%20integration%20of%20more%20tactile%20sensors%20into%20robotic%0Ahands.%20Consequently%2C%20the%20increased%20data%20volume%20imposes%20substantial%20bandwidth%0Apressure%20on%20signal%20transmission%20from%20the%20hand%27s%20controller.%20However%2C%20the%0Aacquisition%20and%20compression%20of%20multi-point%20tactile%20signals%20based%20on%20the%0Adexterous%20hands%27%20physical%20structures%20have%20not%20been%20thoroughly%20explored.%20In%20this%0Apaper%2C%20our%20contributions%20are%20twofold.%20First%2C%20we%20introduce%20a%20Multi-Point%20Tactile%0ADataset%20for%20Dexterous%20Hand%20Grasping%20%28Dex-MPTD%29.%20This%20dataset%20captures%20tactile%0Asignals%20from%20multiple%20contact%20sensors%20across%20various%20objects%20and%20grasping%0Aposes%2C%20offering%20a%20comprehensive%20benchmark%20for%20advancing%20dexterous%20robotic%0Amanipulation%20research.%20Second%2C%20we%20investigate%20both%20lossless%20and%20lossy%0Acompression%20on%20Dex-MPTD%20by%20converting%20tactile%20data%20into%20images%20and%20applying%20six%0Alossless%20and%20five%20lossy%20image%20codecs%20for%20efficient%20compression.%20Experimental%0Aresults%20demonstrate%20that%20tactile%20data%20can%20be%20losslessly%20compressed%20to%20as%20low%20as%0A0.0364%20bits%20per%20sub-sample%20%28bpss%29%2C%20achieving%20approximately%20200%24%5Ctimes%24%0Acompression%20ratio%20compared%20to%20the%20raw%20tactile%20data.%20Efficient%20lossy%20compressors%0Alike%20HM%20and%20VTM%20can%20achieve%20about%201000%24%5Ctimes%24%20data%20reductions%20while%20preserving%0Aacceptable%20data%20fidelity.%20The%20exploration%20of%20lossy%20compression%20also%20reveals%0Athat%20screen-content-targeted%20coding%20tools%20outperform%20general-purpose%20codecs%20in%0Acompressing%20tactile%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTacCompress%253A%2520A%2520Benchmark%2520for%2520Multi-Point%2520Tactile%2520Data%2520Compression%2520in%250A%2520%2520Dexterous%2520Hand%26entry.906535625%3DYan%2520Zhao%2520and%2520Yang%2520Li%2520and%2520Zhengxue%2520Cheng%2520and%2520Hengdi%2520Zhang%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520Though%2520robotic%2520dexterous%2520manipulation%2520has%2520progressed%2520substantially%2520recently%252C%250Achallenges%2520like%2520in-hand%2520occlusion%2520still%2520necessitate%2520fine-grained%2520tactile%250Aperception%252C%2520leading%2520to%2520the%2520integration%2520of%2520more%2520tactile%2520sensors%2520into%2520robotic%250Ahands.%2520Consequently%252C%2520the%2520increased%2520data%2520volume%2520imposes%2520substantial%2520bandwidth%250Apressure%2520on%2520signal%2520transmission%2520from%2520the%2520hand%2527s%2520controller.%2520However%252C%2520the%250Aacquisition%2520and%2520compression%2520of%2520multi-point%2520tactile%2520signals%2520based%2520on%2520the%250Adexterous%2520hands%2527%2520physical%2520structures%2520have%2520not%2520been%2520thoroughly%2520explored.%2520In%2520this%250Apaper%252C%2520our%2520contributions%2520are%2520twofold.%2520First%252C%2520we%2520introduce%2520a%2520Multi-Point%2520Tactile%250ADataset%2520for%2520Dexterous%2520Hand%2520Grasping%2520%2528Dex-MPTD%2529.%2520This%2520dataset%2520captures%2520tactile%250Asignals%2520from%2520multiple%2520contact%2520sensors%2520across%2520various%2520objects%2520and%2520grasping%250Aposes%252C%2520offering%2520a%2520comprehensive%2520benchmark%2520for%2520advancing%2520dexterous%2520robotic%250Amanipulation%2520research.%2520Second%252C%2520we%2520investigate%2520both%2520lossless%2520and%2520lossy%250Acompression%2520on%2520Dex-MPTD%2520by%2520converting%2520tactile%2520data%2520into%2520images%2520and%2520applying%2520six%250Alossless%2520and%2520five%2520lossy%2520image%2520codecs%2520for%2520efficient%2520compression.%2520Experimental%250Aresults%2520demonstrate%2520that%2520tactile%2520data%2520can%2520be%2520losslessly%2520compressed%2520to%2520as%2520low%2520as%250A0.0364%2520bits%2520per%2520sub-sample%2520%2528bpss%2529%252C%2520achieving%2520approximately%2520200%2524%255Ctimes%2524%250Acompression%2520ratio%2520compared%2520to%2520the%2520raw%2520tactile%2520data.%2520Efficient%2520lossy%2520compressors%250Alike%2520HM%2520and%2520VTM%2520can%2520achieve%2520about%25201000%2524%255Ctimes%2524%2520data%2520reductions%2520while%2520preserving%250Aacceptable%2520data%2520fidelity.%2520The%2520exploration%2520of%2520lossy%2520compression%2520also%2520reveals%250Athat%2520screen-content-targeted%2520coding%2520tools%2520outperform%2520general-purpose%2520codecs%2520in%250Acompressing%2520tactile%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TacCompress%3A%20A%20Benchmark%20for%20Multi-Point%20Tactile%20Data%20Compression%20in%0A%20%20Dexterous%20Hand&entry.906535625=Yan%20Zhao%20and%20Yang%20Li%20and%20Zhengxue%20Cheng%20and%20Hengdi%20Zhang%20and%20Li%20Song&entry.1292438233=%20%20Though%20robotic%20dexterous%20manipulation%20has%20progressed%20substantially%20recently%2C%0Achallenges%20like%20in-hand%20occlusion%20still%20necessitate%20fine-grained%20tactile%0Aperception%2C%20leading%20to%20the%20integration%20of%20more%20tactile%20sensors%20into%20robotic%0Ahands.%20Consequently%2C%20the%20increased%20data%20volume%20imposes%20substantial%20bandwidth%0Apressure%20on%20signal%20transmission%20from%20the%20hand%27s%20controller.%20However%2C%20the%0Aacquisition%20and%20compression%20of%20multi-point%20tactile%20signals%20based%20on%20the%0Adexterous%20hands%27%20physical%20structures%20have%20not%20been%20thoroughly%20explored.%20In%20this%0Apaper%2C%20our%20contributions%20are%20twofold.%20First%2C%20we%20introduce%20a%20Multi-Point%20Tactile%0ADataset%20for%20Dexterous%20Hand%20Grasping%20%28Dex-MPTD%29.%20This%20dataset%20captures%20tactile%0Asignals%20from%20multiple%20contact%20sensors%20across%20various%20objects%20and%20grasping%0Aposes%2C%20offering%20a%20comprehensive%20benchmark%20for%20advancing%20dexterous%20robotic%0Amanipulation%20research.%20Second%2C%20we%20investigate%20both%20lossless%20and%20lossy%0Acompression%20on%20Dex-MPTD%20by%20converting%20tactile%20data%20into%20images%20and%20applying%20six%0Alossless%20and%20five%20lossy%20image%20codecs%20for%20efficient%20compression.%20Experimental%0Aresults%20demonstrate%20that%20tactile%20data%20can%20be%20losslessly%20compressed%20to%20as%20low%20as%0A0.0364%20bits%20per%20sub-sample%20%28bpss%29%2C%20achieving%20approximately%20200%24%5Ctimes%24%0Acompression%20ratio%20compared%20to%20the%20raw%20tactile%20data.%20Efficient%20lossy%20compressors%0Alike%20HM%20and%20VTM%20can%20achieve%20about%201000%24%5Ctimes%24%20data%20reductions%20while%20preserving%0Aacceptable%20data%20fidelity.%20The%20exploration%20of%20lossy%20compression%20also%20reveals%0Athat%20screen-content-targeted%20coding%20tools%20outperform%20general-purpose%20codecs%20in%0Acompressing%20tactile%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16289v2&entry.124074799=Read"},
{"title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and\n  Navigation Using Skip Stage Swin Transformer", "author": "Fachri Najm Noer Kartiman and  Rasim and Yaya Wihardi and Nurul Hasanah and Oskar Natan and Bambang Wahono and Taufik Ibnu Salim", "abstract": "  Focusing on the development of an end-to-end autonomous vehicle model with\npixel-to-pixel context awareness, this research proposes the SKGE-Swin\narchitecture. This architecture utilizes the Swin Transformer with a skip-stage\nmechanism to broaden feature representation globally and at various network\nlevels. This approach enables the model to extract information from distant\npixels by leveraging the Swin Transformer's Shifted Window-based Multi-head\nSelf-Attention (SW-MSA) mechanism and to retain critical information from the\ninitial to the final stages of feature extraction, thereby enhancing its\ncapability to comprehend complex patterns in the vehicle's surroundings. The\nmodel is evaluated on the CARLA platform using adversarial scenarios to\nsimulate real-world conditions. Experimental results demonstrate that the\nSKGE-Swin architecture achieves a superior Driving Score compared to previous\nmethods. Furthermore, an ablation study will be conducted to evaluate the\ncontribution of each architectural component, including the influence of skip\nconnections and the use of the Swin Transformer, in improving model\nperformance.\n", "link": "http://arxiv.org/abs/2508.20762v1", "date": "2025-08-28", "relevancy": 2.0278, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5115}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5073}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKGE-SWIN%3A%20End-To-End%20Autonomous%20Vehicle%20Waypoint%20Prediction%20and%0A%20%20Navigation%20Using%20Skip%20Stage%20Swin%20Transformer&body=Title%3A%20SKGE-SWIN%3A%20End-To-End%20Autonomous%20Vehicle%20Waypoint%20Prediction%20and%0A%20%20Navigation%20Using%20Skip%20Stage%20Swin%20Transformer%0AAuthor%3A%20Fachri%20Najm%20Noer%20Kartiman%20and%20%20Rasim%20and%20Yaya%20Wihardi%20and%20Nurul%20Hasanah%20and%20Oskar%20Natan%20and%20Bambang%20Wahono%20and%20Taufik%20Ibnu%20Salim%0AAbstract%3A%20%20%20Focusing%20on%20the%20development%20of%20an%20end-to-end%20autonomous%20vehicle%20model%20with%0Apixel-to-pixel%20context%20awareness%2C%20this%20research%20proposes%20the%20SKGE-Swin%0Aarchitecture.%20This%20architecture%20utilizes%20the%20Swin%20Transformer%20with%20a%20skip-stage%0Amechanism%20to%20broaden%20feature%20representation%20globally%20and%20at%20various%20network%0Alevels.%20This%20approach%20enables%20the%20model%20to%20extract%20information%20from%20distant%0Apixels%20by%20leveraging%20the%20Swin%20Transformer%27s%20Shifted%20Window-based%20Multi-head%0ASelf-Attention%20%28SW-MSA%29%20mechanism%20and%20to%20retain%20critical%20information%20from%20the%0Ainitial%20to%20the%20final%20stages%20of%20feature%20extraction%2C%20thereby%20enhancing%20its%0Acapability%20to%20comprehend%20complex%20patterns%20in%20the%20vehicle%27s%20surroundings.%20The%0Amodel%20is%20evaluated%20on%20the%20CARLA%20platform%20using%20adversarial%20scenarios%20to%0Asimulate%20real-world%20conditions.%20Experimental%20results%20demonstrate%20that%20the%0ASKGE-Swin%20architecture%20achieves%20a%20superior%20Driving%20Score%20compared%20to%20previous%0Amethods.%20Furthermore%2C%20an%20ablation%20study%20will%20be%20conducted%20to%20evaluate%20the%0Acontribution%20of%20each%20architectural%20component%2C%20including%20the%20influence%20of%20skip%0Aconnections%20and%20the%20use%20of%20the%20Swin%20Transformer%2C%20in%20improving%20model%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKGE-SWIN%253A%2520End-To-End%2520Autonomous%2520Vehicle%2520Waypoint%2520Prediction%2520and%250A%2520%2520Navigation%2520Using%2520Skip%2520Stage%2520Swin%2520Transformer%26entry.906535625%3DFachri%2520Najm%2520Noer%2520Kartiman%2520and%2520%2520Rasim%2520and%2520Yaya%2520Wihardi%2520and%2520Nurul%2520Hasanah%2520and%2520Oskar%2520Natan%2520and%2520Bambang%2520Wahono%2520and%2520Taufik%2520Ibnu%2520Salim%26entry.1292438233%3D%2520%2520Focusing%2520on%2520the%2520development%2520of%2520an%2520end-to-end%2520autonomous%2520vehicle%2520model%2520with%250Apixel-to-pixel%2520context%2520awareness%252C%2520this%2520research%2520proposes%2520the%2520SKGE-Swin%250Aarchitecture.%2520This%2520architecture%2520utilizes%2520the%2520Swin%2520Transformer%2520with%2520a%2520skip-stage%250Amechanism%2520to%2520broaden%2520feature%2520representation%2520globally%2520and%2520at%2520various%2520network%250Alevels.%2520This%2520approach%2520enables%2520the%2520model%2520to%2520extract%2520information%2520from%2520distant%250Apixels%2520by%2520leveraging%2520the%2520Swin%2520Transformer%2527s%2520Shifted%2520Window-based%2520Multi-head%250ASelf-Attention%2520%2528SW-MSA%2529%2520mechanism%2520and%2520to%2520retain%2520critical%2520information%2520from%2520the%250Ainitial%2520to%2520the%2520final%2520stages%2520of%2520feature%2520extraction%252C%2520thereby%2520enhancing%2520its%250Acapability%2520to%2520comprehend%2520complex%2520patterns%2520in%2520the%2520vehicle%2527s%2520surroundings.%2520The%250Amodel%2520is%2520evaluated%2520on%2520the%2520CARLA%2520platform%2520using%2520adversarial%2520scenarios%2520to%250Asimulate%2520real-world%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250ASKGE-Swin%2520architecture%2520achieves%2520a%2520superior%2520Driving%2520Score%2520compared%2520to%2520previous%250Amethods.%2520Furthermore%252C%2520an%2520ablation%2520study%2520will%2520be%2520conducted%2520to%2520evaluate%2520the%250Acontribution%2520of%2520each%2520architectural%2520component%252C%2520including%2520the%2520influence%2520of%2520skip%250Aconnections%2520and%2520the%2520use%2520of%2520the%2520Swin%2520Transformer%252C%2520in%2520improving%2520model%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKGE-SWIN%3A%20End-To-End%20Autonomous%20Vehicle%20Waypoint%20Prediction%20and%0A%20%20Navigation%20Using%20Skip%20Stage%20Swin%20Transformer&entry.906535625=Fachri%20Najm%20Noer%20Kartiman%20and%20%20Rasim%20and%20Yaya%20Wihardi%20and%20Nurul%20Hasanah%20and%20Oskar%20Natan%20and%20Bambang%20Wahono%20and%20Taufik%20Ibnu%20Salim&entry.1292438233=%20%20Focusing%20on%20the%20development%20of%20an%20end-to-end%20autonomous%20vehicle%20model%20with%0Apixel-to-pixel%20context%20awareness%2C%20this%20research%20proposes%20the%20SKGE-Swin%0Aarchitecture.%20This%20architecture%20utilizes%20the%20Swin%20Transformer%20with%20a%20skip-stage%0Amechanism%20to%20broaden%20feature%20representation%20globally%20and%20at%20various%20network%0Alevels.%20This%20approach%20enables%20the%20model%20to%20extract%20information%20from%20distant%0Apixels%20by%20leveraging%20the%20Swin%20Transformer%27s%20Shifted%20Window-based%20Multi-head%0ASelf-Attention%20%28SW-MSA%29%20mechanism%20and%20to%20retain%20critical%20information%20from%20the%0Ainitial%20to%20the%20final%20stages%20of%20feature%20extraction%2C%20thereby%20enhancing%20its%0Acapability%20to%20comprehend%20complex%20patterns%20in%20the%20vehicle%27s%20surroundings.%20The%0Amodel%20is%20evaluated%20on%20the%20CARLA%20platform%20using%20adversarial%20scenarios%20to%0Asimulate%20real-world%20conditions.%20Experimental%20results%20demonstrate%20that%20the%0ASKGE-Swin%20architecture%20achieves%20a%20superior%20Driving%20Score%20compared%20to%20previous%0Amethods.%20Furthermore%2C%20an%20ablation%20study%20will%20be%20conducted%20to%20evaluate%20the%0Acontribution%20of%20each%20architectural%20component%2C%20including%20the%20influence%20of%20skip%0Aconnections%20and%20the%20use%20of%20the%20Swin%20Transformer%2C%20in%20improving%20model%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20762v1&entry.124074799=Read"},
{"title": "Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic\n  Programming Heuristics", "author": "Liding Zhang and Kuanqi Cai and Zhenshan Bing and Chaoqun Wang and Alois Knoll", "abstract": "  Optimal path planning involves finding a feasible state sequence between a\nstart and a goal that optimizes an objective. This process relies on heuristic\nfunctions to guide the search direction. While a robust function can improve\nsearch efficiency and solution quality, current methods often overlook\navailable environmental data and simplify the function structure due to the\ncomplexity of information relationships. This study introduces Genetic Informed\nTrees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a\nwider array of environmental data, such as repulsive forces from obstacles and\nthe dynamic importance of vertices, to refine heuristic functions for better\nguidance. Furthermore, we integrated reinforced genetic programming (RGP),\nwhich combines genetic programming with reward system feedback to mutate\ngenotype-generative heuristic functions for GIT*. RGP leverages a multitude of\ndata types, thereby improving computational efficiency and solution quality\nwithin a set timeframe. Comparative analyses demonstrate that GIT* surpasses\nexisting single-query, sampling-based planners in problems ranging from R^4 to\nR^16 and was tested on a real-world mobile manipulation task. A video\nshowcasing our experimental results is available at\nhttps://youtu.be/URjXbc_BiYg\n", "link": "http://arxiv.org/abs/2508.20871v1", "date": "2025-08-28", "relevancy": 2.027, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5306}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5119}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Genetic%20Informed%20Trees%20%28GIT%2A%29%3A%20Path%20Planning%20via%20Reinforced%20Genetic%0A%20%20Programming%20Heuristics&body=Title%3A%20Genetic%20Informed%20Trees%20%28GIT%2A%29%3A%20Path%20Planning%20via%20Reinforced%20Genetic%0A%20%20Programming%20Heuristics%0AAuthor%3A%20Liding%20Zhang%20and%20Kuanqi%20Cai%20and%20Zhenshan%20Bing%20and%20Chaoqun%20Wang%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Optimal%20path%20planning%20involves%20finding%20a%20feasible%20state%20sequence%20between%20a%0Astart%20and%20a%20goal%20that%20optimizes%20an%20objective.%20This%20process%20relies%20on%20heuristic%0Afunctions%20to%20guide%20the%20search%20direction.%20While%20a%20robust%20function%20can%20improve%0Asearch%20efficiency%20and%20solution%20quality%2C%20current%20methods%20often%20overlook%0Aavailable%20environmental%20data%20and%20simplify%20the%20function%20structure%20due%20to%20the%0Acomplexity%20of%20information%20relationships.%20This%20study%20introduces%20Genetic%20Informed%0ATrees%20%28GIT%2A%29%2C%20which%20improves%20upon%20Effort%20Informed%20Trees%20%28EIT%2A%29%20by%20integrating%20a%0Awider%20array%20of%20environmental%20data%2C%20such%20as%20repulsive%20forces%20from%20obstacles%20and%0Athe%20dynamic%20importance%20of%20vertices%2C%20to%20refine%20heuristic%20functions%20for%20better%0Aguidance.%20Furthermore%2C%20we%20integrated%20reinforced%20genetic%20programming%20%28RGP%29%2C%0Awhich%20combines%20genetic%20programming%20with%20reward%20system%20feedback%20to%20mutate%0Agenotype-generative%20heuristic%20functions%20for%20GIT%2A.%20RGP%20leverages%20a%20multitude%20of%0Adata%20types%2C%20thereby%20improving%20computational%20efficiency%20and%20solution%20quality%0Awithin%20a%20set%20timeframe.%20Comparative%20analyses%20demonstrate%20that%20GIT%2A%20surpasses%0Aexisting%20single-query%2C%20sampling-based%20planners%20in%20problems%20ranging%20from%20R%5E4%20to%0AR%5E16%20and%20was%20tested%20on%20a%20real-world%20mobile%20manipulation%20task.%20A%20video%0Ashowcasing%20our%20experimental%20results%20is%20available%20at%0Ahttps%3A//youtu.be/URjXbc_BiYg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenetic%2520Informed%2520Trees%2520%2528GIT%252A%2529%253A%2520Path%2520Planning%2520via%2520Reinforced%2520Genetic%250A%2520%2520Programming%2520Heuristics%26entry.906535625%3DLiding%2520Zhang%2520and%2520Kuanqi%2520Cai%2520and%2520Zhenshan%2520Bing%2520and%2520Chaoqun%2520Wang%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Optimal%2520path%2520planning%2520involves%2520finding%2520a%2520feasible%2520state%2520sequence%2520between%2520a%250Astart%2520and%2520a%2520goal%2520that%2520optimizes%2520an%2520objective.%2520This%2520process%2520relies%2520on%2520heuristic%250Afunctions%2520to%2520guide%2520the%2520search%2520direction.%2520While%2520a%2520robust%2520function%2520can%2520improve%250Asearch%2520efficiency%2520and%2520solution%2520quality%252C%2520current%2520methods%2520often%2520overlook%250Aavailable%2520environmental%2520data%2520and%2520simplify%2520the%2520function%2520structure%2520due%2520to%2520the%250Acomplexity%2520of%2520information%2520relationships.%2520This%2520study%2520introduces%2520Genetic%2520Informed%250ATrees%2520%2528GIT%252A%2529%252C%2520which%2520improves%2520upon%2520Effort%2520Informed%2520Trees%2520%2528EIT%252A%2529%2520by%2520integrating%2520a%250Awider%2520array%2520of%2520environmental%2520data%252C%2520such%2520as%2520repulsive%2520forces%2520from%2520obstacles%2520and%250Athe%2520dynamic%2520importance%2520of%2520vertices%252C%2520to%2520refine%2520heuristic%2520functions%2520for%2520better%250Aguidance.%2520Furthermore%252C%2520we%2520integrated%2520reinforced%2520genetic%2520programming%2520%2528RGP%2529%252C%250Awhich%2520combines%2520genetic%2520programming%2520with%2520reward%2520system%2520feedback%2520to%2520mutate%250Agenotype-generative%2520heuristic%2520functions%2520for%2520GIT%252A.%2520RGP%2520leverages%2520a%2520multitude%2520of%250Adata%2520types%252C%2520thereby%2520improving%2520computational%2520efficiency%2520and%2520solution%2520quality%250Awithin%2520a%2520set%2520timeframe.%2520Comparative%2520analyses%2520demonstrate%2520that%2520GIT%252A%2520surpasses%250Aexisting%2520single-query%252C%2520sampling-based%2520planners%2520in%2520problems%2520ranging%2520from%2520R%255E4%2520to%250AR%255E16%2520and%2520was%2520tested%2520on%2520a%2520real-world%2520mobile%2520manipulation%2520task.%2520A%2520video%250Ashowcasing%2520our%2520experimental%2520results%2520is%2520available%2520at%250Ahttps%253A//youtu.be/URjXbc_BiYg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genetic%20Informed%20Trees%20%28GIT%2A%29%3A%20Path%20Planning%20via%20Reinforced%20Genetic%0A%20%20Programming%20Heuristics&entry.906535625=Liding%20Zhang%20and%20Kuanqi%20Cai%20and%20Zhenshan%20Bing%20and%20Chaoqun%20Wang%20and%20Alois%20Knoll&entry.1292438233=%20%20Optimal%20path%20planning%20involves%20finding%20a%20feasible%20state%20sequence%20between%20a%0Astart%20and%20a%20goal%20that%20optimizes%20an%20objective.%20This%20process%20relies%20on%20heuristic%0Afunctions%20to%20guide%20the%20search%20direction.%20While%20a%20robust%20function%20can%20improve%0Asearch%20efficiency%20and%20solution%20quality%2C%20current%20methods%20often%20overlook%0Aavailable%20environmental%20data%20and%20simplify%20the%20function%20structure%20due%20to%20the%0Acomplexity%20of%20information%20relationships.%20This%20study%20introduces%20Genetic%20Informed%0ATrees%20%28GIT%2A%29%2C%20which%20improves%20upon%20Effort%20Informed%20Trees%20%28EIT%2A%29%20by%20integrating%20a%0Awider%20array%20of%20environmental%20data%2C%20such%20as%20repulsive%20forces%20from%20obstacles%20and%0Athe%20dynamic%20importance%20of%20vertices%2C%20to%20refine%20heuristic%20functions%20for%20better%0Aguidance.%20Furthermore%2C%20we%20integrated%20reinforced%20genetic%20programming%20%28RGP%29%2C%0Awhich%20combines%20genetic%20programming%20with%20reward%20system%20feedback%20to%20mutate%0Agenotype-generative%20heuristic%20functions%20for%20GIT%2A.%20RGP%20leverages%20a%20multitude%20of%0Adata%20types%2C%20thereby%20improving%20computational%20efficiency%20and%20solution%20quality%0Awithin%20a%20set%20timeframe.%20Comparative%20analyses%20demonstrate%20that%20GIT%2A%20surpasses%0Aexisting%20single-query%2C%20sampling-based%20planners%20in%20problems%20ranging%20from%20R%5E4%20to%0AR%5E16%20and%20was%20tested%20on%20a%20real-world%20mobile%20manipulation%20task.%20A%20video%0Ashowcasing%20our%20experimental%20results%20is%20available%20at%0Ahttps%3A//youtu.be/URjXbc_BiYg%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20871v1&entry.124074799=Read"},
{"title": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis\n  Classification", "author": "Kaustubh Atey and Sameer Anand Jha and Gouranga Bala and Amit Sethi", "abstract": "  Atypical mitotic figures (AMFs) are important histopathological markers yet\nremain challenging to identify consistently, particularly under domain shift\nstemming from scanner, stain, and acquisition differences. We present a simple\ntraining-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.\nThe approach (i) increases feature diversity via style perturbations inserted\nat early and mid backbone stages, (ii) aligns attention-refined features across\nsites using weak domain labels (Scanner, Origin, Species, Tumor) through an\nauxiliary alignment loss, and (iii) stabilizes predictions by distilling from\nan exponential moving average (EMA) teacher with temperature-scaled KL\ndivergence. On the organizer-run preliminary leaderboard for atypical mitosis\nclassification, our submission attains balanced accuracy of 0.8762, sensitivity\nof 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs\nnegligible inference-time overhead, relies only on coarse domain metadata, and\ndelivers strong, balanced performance, positioning it as a competitive\nsubmission for the MIDOG 2025 challenge.\n", "link": "http://arxiv.org/abs/2508.20745v1", "date": "2025-08-28", "relevancy": 2.0109, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4989}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mix%2C%20Align%2C%20Distil%3A%20Reliable%20Cross-Domain%20Atypical%20Mitosis%0A%20%20Classification&body=Title%3A%20Mix%2C%20Align%2C%20Distil%3A%20Reliable%20Cross-Domain%20Atypical%20Mitosis%0A%20%20Classification%0AAuthor%3A%20Kaustubh%20Atey%20and%20Sameer%20Anand%20Jha%20and%20Gouranga%20Bala%20and%20Amit%20Sethi%0AAbstract%3A%20%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20are%20important%20histopathological%20markers%20yet%0Aremain%20challenging%20to%20identify%20consistently%2C%20particularly%20under%20domain%20shift%0Astemming%20from%20scanner%2C%20stain%2C%20and%20acquisition%20differences.%20We%20present%20a%20simple%0Atraining-time%20recipe%20for%20domain-robust%20AMF%20classification%20in%20MIDOG%202025%20Task%202.%0AThe%20approach%20%28i%29%20increases%20feature%20diversity%20via%20style%20perturbations%20inserted%0Aat%20early%20and%20mid%20backbone%20stages%2C%20%28ii%29%20aligns%20attention-refined%20features%20across%0Asites%20using%20weak%20domain%20labels%20%28Scanner%2C%20Origin%2C%20Species%2C%20Tumor%29%20through%20an%0Aauxiliary%20alignment%20loss%2C%20and%20%28iii%29%20stabilizes%20predictions%20by%20distilling%20from%0Aan%20exponential%20moving%20average%20%28EMA%29%20teacher%20with%20temperature-scaled%20KL%0Adivergence.%20On%20the%20organizer-run%20preliminary%20leaderboard%20for%20atypical%20mitosis%0Aclassification%2C%20our%20submission%20attains%20balanced%20accuracy%20of%200.8762%2C%20sensitivity%0Aof%200.8873%2C%20specificity%20of%200.8651%2C%20and%20ROC%20AUC%20of%200.9499.%20The%20method%20incurs%0Anegligible%20inference-time%20overhead%2C%20relies%20only%20on%20coarse%20domain%20metadata%2C%20and%0Adelivers%20strong%2C%20balanced%20performance%2C%20positioning%20it%20as%20a%20competitive%0Asubmission%20for%20the%20MIDOG%202025%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMix%252C%2520Align%252C%2520Distil%253A%2520Reliable%2520Cross-Domain%2520Atypical%2520Mitosis%250A%2520%2520Classification%26entry.906535625%3DKaustubh%2520Atey%2520and%2520Sameer%2520Anand%2520Jha%2520and%2520Gouranga%2520Bala%2520and%2520Amit%2520Sethi%26entry.1292438233%3D%2520%2520Atypical%2520mitotic%2520figures%2520%2528AMFs%2529%2520are%2520important%2520histopathological%2520markers%2520yet%250Aremain%2520challenging%2520to%2520identify%2520consistently%252C%2520particularly%2520under%2520domain%2520shift%250Astemming%2520from%2520scanner%252C%2520stain%252C%2520and%2520acquisition%2520differences.%2520We%2520present%2520a%2520simple%250Atraining-time%2520recipe%2520for%2520domain-robust%2520AMF%2520classification%2520in%2520MIDOG%25202025%2520Task%25202.%250AThe%2520approach%2520%2528i%2529%2520increases%2520feature%2520diversity%2520via%2520style%2520perturbations%2520inserted%250Aat%2520early%2520and%2520mid%2520backbone%2520stages%252C%2520%2528ii%2529%2520aligns%2520attention-refined%2520features%2520across%250Asites%2520using%2520weak%2520domain%2520labels%2520%2528Scanner%252C%2520Origin%252C%2520Species%252C%2520Tumor%2529%2520through%2520an%250Aauxiliary%2520alignment%2520loss%252C%2520and%2520%2528iii%2529%2520stabilizes%2520predictions%2520by%2520distilling%2520from%250Aan%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520teacher%2520with%2520temperature-scaled%2520KL%250Adivergence.%2520On%2520the%2520organizer-run%2520preliminary%2520leaderboard%2520for%2520atypical%2520mitosis%250Aclassification%252C%2520our%2520submission%2520attains%2520balanced%2520accuracy%2520of%25200.8762%252C%2520sensitivity%250Aof%25200.8873%252C%2520specificity%2520of%25200.8651%252C%2520and%2520ROC%2520AUC%2520of%25200.9499.%2520The%2520method%2520incurs%250Anegligible%2520inference-time%2520overhead%252C%2520relies%2520only%2520on%2520coarse%2520domain%2520metadata%252C%2520and%250Adelivers%2520strong%252C%2520balanced%2520performance%252C%2520positioning%2520it%2520as%2520a%2520competitive%250Asubmission%2520for%2520the%2520MIDOG%25202025%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mix%2C%20Align%2C%20Distil%3A%20Reliable%20Cross-Domain%20Atypical%20Mitosis%0A%20%20Classification&entry.906535625=Kaustubh%20Atey%20and%20Sameer%20Anand%20Jha%20and%20Gouranga%20Bala%20and%20Amit%20Sethi&entry.1292438233=%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20are%20important%20histopathological%20markers%20yet%0Aremain%20challenging%20to%20identify%20consistently%2C%20particularly%20under%20domain%20shift%0Astemming%20from%20scanner%2C%20stain%2C%20and%20acquisition%20differences.%20We%20present%20a%20simple%0Atraining-time%20recipe%20for%20domain-robust%20AMF%20classification%20in%20MIDOG%202025%20Task%202.%0AThe%20approach%20%28i%29%20increases%20feature%20diversity%20via%20style%20perturbations%20inserted%0Aat%20early%20and%20mid%20backbone%20stages%2C%20%28ii%29%20aligns%20attention-refined%20features%20across%0Asites%20using%20weak%20domain%20labels%20%28Scanner%2C%20Origin%2C%20Species%2C%20Tumor%29%20through%20an%0Aauxiliary%20alignment%20loss%2C%20and%20%28iii%29%20stabilizes%20predictions%20by%20distilling%20from%0Aan%20exponential%20moving%20average%20%28EMA%29%20teacher%20with%20temperature-scaled%20KL%0Adivergence.%20On%20the%20organizer-run%20preliminary%20leaderboard%20for%20atypical%20mitosis%0Aclassification%2C%20our%20submission%20attains%20balanced%20accuracy%20of%200.8762%2C%20sensitivity%0Aof%200.8873%2C%20specificity%20of%200.8651%2C%20and%20ROC%20AUC%20of%200.9499.%20The%20method%20incurs%0Anegligible%20inference-time%20overhead%2C%20relies%20only%20on%20coarse%20domain%20metadata%2C%20and%0Adelivers%20strong%2C%20balanced%20performance%2C%20positioning%20it%20as%20a%20competitive%0Asubmission%20for%20the%20MIDOG%202025%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20745v1&entry.124074799=Read"},
{"title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "author": "Joseph Raj Vishal and Divesh Basina and Rutuja Patil and Manas Srinivas Gowda and Katha Naik and Yezhou Yang and Bharatesh Chakravarthi", "abstract": "  Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA\n", "link": "http://arxiv.org/abs/2507.14743v3", "date": "2025-08-28", "relevancy": 1.9939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterAct-Video%3A%20Reasoning-Rich%20Video%20QA%20for%20Urban%20Traffic&body=Title%3A%20InterAct-Video%3A%20Reasoning-Rich%20Video%20QA%20for%20Urban%20Traffic%0AAuthor%3A%20Joseph%20Raj%20Vishal%20and%20Divesh%20Basina%20and%20Rutuja%20Patil%20and%20Manas%20Srinivas%20Gowda%20and%20Katha%20Naik%20and%20Yezhou%20Yang%20and%20Bharatesh%20Chakravarthi%0AAbstract%3A%20%20%20Traffic%20monitoring%20is%20crucial%20for%20urban%20mobility%2C%20road%20safety%2C%20and%0Aintelligent%20transportation%20systems%20%28ITS%29.%20Deep%20learning%20has%20advanced%0Avideo-based%20traffic%20monitoring%20through%20video%20question%20answering%20%28VideoQA%29%0Amodels%2C%20enabling%20structured%20insight%20extraction%20from%20traffic%20videos.%20However%2C%0Aexisting%20VideoQA%20models%20struggle%20with%20the%20complexity%20of%20real-world%20traffic%0Ascenes%2C%20where%20multiple%20concurrent%20events%20unfold%20across%20spatiotemporal%0Adimensions.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20%5Ctextbf%7BInterAct%0AVideoQA%7D%2C%20a%20curated%20dataset%20designed%20to%20benchmark%20and%20enhance%20VideoQA%20models%0Afor%20traffic%20monitoring%20tasks.%20The%20InterAct%20VideoQA%20dataset%20comprises%208%20hours%20of%0Areal-world%20traffic%20footage%20collected%20from%20diverse%20intersections%2C%20segmented%20into%0A10-second%20video%20clips%2C%20with%20over%2025%2C000%20question-answer%20%28QA%29%20pairs%20covering%0Aspatiotemporal%20dynamics%2C%20vehicle%20interactions%2C%20incident%20detection%2C%20and%20other%0Acritical%20traffic%20attributes.%20State-of-the-art%20VideoQA%20models%20are%20evaluated%20on%0AInterAct%20VideoQA%2C%20exposing%20challenges%20in%20reasoning%20over%20fine-grained%0Aspatiotemporal%20dependencies%20within%20complex%20traffic%20scenarios.%20Additionally%2C%0Afine-tuning%20these%20models%20on%20InterAct%20VideoQA%20yields%20notable%20performance%0Aimprovements%2C%20demonstrating%20the%20necessity%20of%20domain-specific%20datasets%20for%0AVideoQA.%20InterAct%20VideoQA%20is%20publicly%20available%20as%20a%20benchmark%20dataset%20to%0Afacilitate%20future%20research%20in%20real-world%20deployable%20VideoQA%20models%20for%0Aintelligent%20transportation%20systems.%20GitHub%20Repo%3A%0Ahttps%3A//github.com/joe-rabbit/InterAct_VideoQA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterAct-Video%253A%2520Reasoning-Rich%2520Video%2520QA%2520for%2520Urban%2520Traffic%26entry.906535625%3DJoseph%2520Raj%2520Vishal%2520and%2520Divesh%2520Basina%2520and%2520Rutuja%2520Patil%2520and%2520Manas%2520Srinivas%2520Gowda%2520and%2520Katha%2520Naik%2520and%2520Yezhou%2520Yang%2520and%2520Bharatesh%2520Chakravarthi%26entry.1292438233%3D%2520%2520Traffic%2520monitoring%2520is%2520crucial%2520for%2520urban%2520mobility%252C%2520road%2520safety%252C%2520and%250Aintelligent%2520transportation%2520systems%2520%2528ITS%2529.%2520Deep%2520learning%2520has%2520advanced%250Avideo-based%2520traffic%2520monitoring%2520through%2520video%2520question%2520answering%2520%2528VideoQA%2529%250Amodels%252C%2520enabling%2520structured%2520insight%2520extraction%2520from%2520traffic%2520videos.%2520However%252C%250Aexisting%2520VideoQA%2520models%2520struggle%2520with%2520the%2520complexity%2520of%2520real-world%2520traffic%250Ascenes%252C%2520where%2520multiple%2520concurrent%2520events%2520unfold%2520across%2520spatiotemporal%250Adimensions.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520%255Ctextbf%257BInterAct%250AVideoQA%257D%252C%2520a%2520curated%2520dataset%2520designed%2520to%2520benchmark%2520and%2520enhance%2520VideoQA%2520models%250Afor%2520traffic%2520monitoring%2520tasks.%2520The%2520InterAct%2520VideoQA%2520dataset%2520comprises%25208%2520hours%2520of%250Areal-world%2520traffic%2520footage%2520collected%2520from%2520diverse%2520intersections%252C%2520segmented%2520into%250A10-second%2520video%2520clips%252C%2520with%2520over%252025%252C000%2520question-answer%2520%2528QA%2529%2520pairs%2520covering%250Aspatiotemporal%2520dynamics%252C%2520vehicle%2520interactions%252C%2520incident%2520detection%252C%2520and%2520other%250Acritical%2520traffic%2520attributes.%2520State-of-the-art%2520VideoQA%2520models%2520are%2520evaluated%2520on%250AInterAct%2520VideoQA%252C%2520exposing%2520challenges%2520in%2520reasoning%2520over%2520fine-grained%250Aspatiotemporal%2520dependencies%2520within%2520complex%2520traffic%2520scenarios.%2520Additionally%252C%250Afine-tuning%2520these%2520models%2520on%2520InterAct%2520VideoQA%2520yields%2520notable%2520performance%250Aimprovements%252C%2520demonstrating%2520the%2520necessity%2520of%2520domain-specific%2520datasets%2520for%250AVideoQA.%2520InterAct%2520VideoQA%2520is%2520publicly%2520available%2520as%2520a%2520benchmark%2520dataset%2520to%250Afacilitate%2520future%2520research%2520in%2520real-world%2520deployable%2520VideoQA%2520models%2520for%250Aintelligent%2520transportation%2520systems.%2520GitHub%2520Repo%253A%250Ahttps%253A//github.com/joe-rabbit/InterAct_VideoQA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterAct-Video%3A%20Reasoning-Rich%20Video%20QA%20for%20Urban%20Traffic&entry.906535625=Joseph%20Raj%20Vishal%20and%20Divesh%20Basina%20and%20Rutuja%20Patil%20and%20Manas%20Srinivas%20Gowda%20and%20Katha%20Naik%20and%20Yezhou%20Yang%20and%20Bharatesh%20Chakravarthi&entry.1292438233=%20%20Traffic%20monitoring%20is%20crucial%20for%20urban%20mobility%2C%20road%20safety%2C%20and%0Aintelligent%20transportation%20systems%20%28ITS%29.%20Deep%20learning%20has%20advanced%0Avideo-based%20traffic%20monitoring%20through%20video%20question%20answering%20%28VideoQA%29%0Amodels%2C%20enabling%20structured%20insight%20extraction%20from%20traffic%20videos.%20However%2C%0Aexisting%20VideoQA%20models%20struggle%20with%20the%20complexity%20of%20real-world%20traffic%0Ascenes%2C%20where%20multiple%20concurrent%20events%20unfold%20across%20spatiotemporal%0Adimensions.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20%5Ctextbf%7BInterAct%0AVideoQA%7D%2C%20a%20curated%20dataset%20designed%20to%20benchmark%20and%20enhance%20VideoQA%20models%0Afor%20traffic%20monitoring%20tasks.%20The%20InterAct%20VideoQA%20dataset%20comprises%208%20hours%20of%0Areal-world%20traffic%20footage%20collected%20from%20diverse%20intersections%2C%20segmented%20into%0A10-second%20video%20clips%2C%20with%20over%2025%2C000%20question-answer%20%28QA%29%20pairs%20covering%0Aspatiotemporal%20dynamics%2C%20vehicle%20interactions%2C%20incident%20detection%2C%20and%20other%0Acritical%20traffic%20attributes.%20State-of-the-art%20VideoQA%20models%20are%20evaluated%20on%0AInterAct%20VideoQA%2C%20exposing%20challenges%20in%20reasoning%20over%20fine-grained%0Aspatiotemporal%20dependencies%20within%20complex%20traffic%20scenarios.%20Additionally%2C%0Afine-tuning%20these%20models%20on%20InterAct%20VideoQA%20yields%20notable%20performance%0Aimprovements%2C%20demonstrating%20the%20necessity%20of%20domain-specific%20datasets%20for%0AVideoQA.%20InterAct%20VideoQA%20is%20publicly%20available%20as%20a%20benchmark%20dataset%20to%0Afacilitate%20future%20research%20in%20real-world%20deployable%20VideoQA%20models%20for%0Aintelligent%20transportation%20systems.%20GitHub%20Repo%3A%0Ahttps%3A//github.com/joe-rabbit/InterAct_VideoQA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14743v3&entry.124074799=Read"},
{"title": "Speech Emotion Recognition via Entropy-Aware Score Selection", "author": "ChenYi Chua and JunKai Wong and Chengxin Chen and Xiaoxiao Miao", "abstract": "  In this paper, we propose a multimodal framework for speech emotion\nrecognition that leverages entropy-aware score selection to combine speech and\ntextual predictions. The proposed method integrates a primary pipeline that\nconsists of an acoustic model based on wav2vec2.0 and a secondary pipeline that\nconsists of a sentiment analysis model using RoBERTa-XLM, with transcriptions\ngenerated via Whisper-large-v3. We propose a late score fusion approach based\non entropy and varentropy thresholds to overcome the confidence constraints of\nprimary pipeline predictions. A sentiment mapping strategy translates three\nsentiment categories into four target emotion classes, enabling coherent\nintegration of multimodal predictions. The results on the IEMOCAP and\nMSP-IMPROV datasets show that the proposed method offers a practical and\nreliable enhancement over traditional single-modality systems.\n", "link": "http://arxiv.org/abs/2508.20796v1", "date": "2025-08-28", "relevancy": 1.9925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5159}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech%20Emotion%20Recognition%20via%20Entropy-Aware%20Score%20Selection&body=Title%3A%20Speech%20Emotion%20Recognition%20via%20Entropy-Aware%20Score%20Selection%0AAuthor%3A%20ChenYi%20Chua%20and%20JunKai%20Wong%20and%20Chengxin%20Chen%20and%20Xiaoxiao%20Miao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20framework%20for%20speech%20emotion%0Arecognition%20that%20leverages%20entropy-aware%20score%20selection%20to%20combine%20speech%20and%0Atextual%20predictions.%20The%20proposed%20method%20integrates%20a%20primary%20pipeline%20that%0Aconsists%20of%20an%20acoustic%20model%20based%20on%20wav2vec2.0%20and%20a%20secondary%20pipeline%20that%0Aconsists%20of%20a%20sentiment%20analysis%20model%20using%20RoBERTa-XLM%2C%20with%20transcriptions%0Agenerated%20via%20Whisper-large-v3.%20We%20propose%20a%20late%20score%20fusion%20approach%20based%0Aon%20entropy%20and%20varentropy%20thresholds%20to%20overcome%20the%20confidence%20constraints%20of%0Aprimary%20pipeline%20predictions.%20A%20sentiment%20mapping%20strategy%20translates%20three%0Asentiment%20categories%20into%20four%20target%20emotion%20classes%2C%20enabling%20coherent%0Aintegration%20of%20multimodal%20predictions.%20The%20results%20on%20the%20IEMOCAP%20and%0AMSP-IMPROV%20datasets%20show%20that%20the%20proposed%20method%20offers%20a%20practical%20and%0Areliable%20enhancement%20over%20traditional%20single-modality%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech%2520Emotion%2520Recognition%2520via%2520Entropy-Aware%2520Score%2520Selection%26entry.906535625%3DChenYi%2520Chua%2520and%2520JunKai%2520Wong%2520and%2520Chengxin%2520Chen%2520and%2520Xiaoxiao%2520Miao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multimodal%2520framework%2520for%2520speech%2520emotion%250Arecognition%2520that%2520leverages%2520entropy-aware%2520score%2520selection%2520to%2520combine%2520speech%2520and%250Atextual%2520predictions.%2520The%2520proposed%2520method%2520integrates%2520a%2520primary%2520pipeline%2520that%250Aconsists%2520of%2520an%2520acoustic%2520model%2520based%2520on%2520wav2vec2.0%2520and%2520a%2520secondary%2520pipeline%2520that%250Aconsists%2520of%2520a%2520sentiment%2520analysis%2520model%2520using%2520RoBERTa-XLM%252C%2520with%2520transcriptions%250Agenerated%2520via%2520Whisper-large-v3.%2520We%2520propose%2520a%2520late%2520score%2520fusion%2520approach%2520based%250Aon%2520entropy%2520and%2520varentropy%2520thresholds%2520to%2520overcome%2520the%2520confidence%2520constraints%2520of%250Aprimary%2520pipeline%2520predictions.%2520A%2520sentiment%2520mapping%2520strategy%2520translates%2520three%250Asentiment%2520categories%2520into%2520four%2520target%2520emotion%2520classes%252C%2520enabling%2520coherent%250Aintegration%2520of%2520multimodal%2520predictions.%2520The%2520results%2520on%2520the%2520IEMOCAP%2520and%250AMSP-IMPROV%2520datasets%2520show%2520that%2520the%2520proposed%2520method%2520offers%2520a%2520practical%2520and%250Areliable%2520enhancement%2520over%2520traditional%2520single-modality%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech%20Emotion%20Recognition%20via%20Entropy-Aware%20Score%20Selection&entry.906535625=ChenYi%20Chua%20and%20JunKai%20Wong%20and%20Chengxin%20Chen%20and%20Xiaoxiao%20Miao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20framework%20for%20speech%20emotion%0Arecognition%20that%20leverages%20entropy-aware%20score%20selection%20to%20combine%20speech%20and%0Atextual%20predictions.%20The%20proposed%20method%20integrates%20a%20primary%20pipeline%20that%0Aconsists%20of%20an%20acoustic%20model%20based%20on%20wav2vec2.0%20and%20a%20secondary%20pipeline%20that%0Aconsists%20of%20a%20sentiment%20analysis%20model%20using%20RoBERTa-XLM%2C%20with%20transcriptions%0Agenerated%20via%20Whisper-large-v3.%20We%20propose%20a%20late%20score%20fusion%20approach%20based%0Aon%20entropy%20and%20varentropy%20thresholds%20to%20overcome%20the%20confidence%20constraints%20of%0Aprimary%20pipeline%20predictions.%20A%20sentiment%20mapping%20strategy%20translates%20three%0Asentiment%20categories%20into%20four%20target%20emotion%20classes%2C%20enabling%20coherent%0Aintegration%20of%20multimodal%20predictions.%20The%20results%20on%20the%20IEMOCAP%20and%0AMSP-IMPROV%20datasets%20show%20that%20the%20proposed%20method%20offers%20a%20practical%20and%0Areliable%20enhancement%20over%20traditional%20single-modality%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20796v1&entry.124074799=Read"},
{"title": "A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation", "author": "Rui Chen and Domenico Chiaradia and Antonio Frisoli and Daniele Leonardis", "abstract": "  This paper presents a novel fabric-based thermal-haptic interface for virtual\nreality and teleoperation. It integrates pneumatic actuation and conductive\nfabric with an innovative ultra-lightweight design, achieving only 2~g for each\nfinger unit. By embedding heating elements within textile pneumatic chambers,\nthe system delivers modulated pressure and thermal stimuli to fingerpads\nthrough a fully soft, wearable interface.\n  Comprehensive characterization demonstrates rapid thermal modulation with\nheating rates up to 3$^{\\circ}$C/s, enabling dynamic thermal feedback for\nvirtual or teleoperation interactions. The pneumatic subsystem generates forces\nup to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance\nenhances cooling efficiency with minimal force reduction. Experimental\nvalidation conducted with two different user studies shows high temperature\nidentification accuracy (0.98 overall) across three thermal levels, and\nsignificant manipulation improvements in a virtual pick-and-place tasks.\nResults show enhanced success rates (88.5\\% to 96.4\\%, p = 0.029) and improved\nforce control precision (p = 0.013) when haptic feedback is enabled, validating\nthe effectiveness of the integrated thermal-haptic approach for advanced\nhuman-machine interaction applications.\n", "link": "http://arxiv.org/abs/2508.20831v1", "date": "2025-08-28", "relevancy": 1.9794, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5143}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4882}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Soft%20Fabric-Based%20Thermal%20Haptic%20Device%20for%20VR%20and%20Teleoperation&body=Title%3A%20A%20Soft%20Fabric-Based%20Thermal%20Haptic%20Device%20for%20VR%20and%20Teleoperation%0AAuthor%3A%20Rui%20Chen%20and%20Domenico%20Chiaradia%20and%20Antonio%20Frisoli%20and%20Daniele%20Leonardis%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20fabric-based%20thermal-haptic%20interface%20for%20virtual%0Areality%20and%20teleoperation.%20It%20integrates%20pneumatic%20actuation%20and%20conductive%0Afabric%20with%20an%20innovative%20ultra-lightweight%20design%2C%20achieving%20only%202~g%20for%20each%0Afinger%20unit.%20By%20embedding%20heating%20elements%20within%20textile%20pneumatic%20chambers%2C%0Athe%20system%20delivers%20modulated%20pressure%20and%20thermal%20stimuli%20to%20fingerpads%0Athrough%20a%20fully%20soft%2C%20wearable%20interface.%0A%20%20Comprehensive%20characterization%20demonstrates%20rapid%20thermal%20modulation%20with%0Aheating%20rates%20up%20to%203%24%5E%7B%5Ccirc%7D%24C/s%2C%20enabling%20dynamic%20thermal%20feedback%20for%0Avirtual%20or%20teleoperation%20interactions.%20The%20pneumatic%20subsystem%20generates%20forces%0Aup%20to%208.93~N%20at%2050~kPa%2C%20while%20optimization%20of%20fingerpad-actuator%20clearance%0Aenhances%20cooling%20efficiency%20with%20minimal%20force%20reduction.%20Experimental%0Avalidation%20conducted%20with%20two%20different%20user%20studies%20shows%20high%20temperature%0Aidentification%20accuracy%20%280.98%20overall%29%20across%20three%20thermal%20levels%2C%20and%0Asignificant%20manipulation%20improvements%20in%20a%20virtual%20pick-and-place%20tasks.%0AResults%20show%20enhanced%20success%20rates%20%2888.5%5C%25%20to%2096.4%5C%25%2C%20p%20%3D%200.029%29%20and%20improved%0Aforce%20control%20precision%20%28p%20%3D%200.013%29%20when%20haptic%20feedback%20is%20enabled%2C%20validating%0Athe%20effectiveness%20of%20the%20integrated%20thermal-haptic%20approach%20for%20advanced%0Ahuman-machine%20interaction%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Soft%2520Fabric-Based%2520Thermal%2520Haptic%2520Device%2520for%2520VR%2520and%2520Teleoperation%26entry.906535625%3DRui%2520Chen%2520and%2520Domenico%2520Chiaradia%2520and%2520Antonio%2520Frisoli%2520and%2520Daniele%2520Leonardis%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520fabric-based%2520thermal-haptic%2520interface%2520for%2520virtual%250Areality%2520and%2520teleoperation.%2520It%2520integrates%2520pneumatic%2520actuation%2520and%2520conductive%250Afabric%2520with%2520an%2520innovative%2520ultra-lightweight%2520design%252C%2520achieving%2520only%25202~g%2520for%2520each%250Afinger%2520unit.%2520By%2520embedding%2520heating%2520elements%2520within%2520textile%2520pneumatic%2520chambers%252C%250Athe%2520system%2520delivers%2520modulated%2520pressure%2520and%2520thermal%2520stimuli%2520to%2520fingerpads%250Athrough%2520a%2520fully%2520soft%252C%2520wearable%2520interface.%250A%2520%2520Comprehensive%2520characterization%2520demonstrates%2520rapid%2520thermal%2520modulation%2520with%250Aheating%2520rates%2520up%2520to%25203%2524%255E%257B%255Ccirc%257D%2524C/s%252C%2520enabling%2520dynamic%2520thermal%2520feedback%2520for%250Avirtual%2520or%2520teleoperation%2520interactions.%2520The%2520pneumatic%2520subsystem%2520generates%2520forces%250Aup%2520to%25208.93~N%2520at%252050~kPa%252C%2520while%2520optimization%2520of%2520fingerpad-actuator%2520clearance%250Aenhances%2520cooling%2520efficiency%2520with%2520minimal%2520force%2520reduction.%2520Experimental%250Avalidation%2520conducted%2520with%2520two%2520different%2520user%2520studies%2520shows%2520high%2520temperature%250Aidentification%2520accuracy%2520%25280.98%2520overall%2529%2520across%2520three%2520thermal%2520levels%252C%2520and%250Asignificant%2520manipulation%2520improvements%2520in%2520a%2520virtual%2520pick-and-place%2520tasks.%250AResults%2520show%2520enhanced%2520success%2520rates%2520%252888.5%255C%2525%2520to%252096.4%255C%2525%252C%2520p%2520%253D%25200.029%2529%2520and%2520improved%250Aforce%2520control%2520precision%2520%2528p%2520%253D%25200.013%2529%2520when%2520haptic%2520feedback%2520is%2520enabled%252C%2520validating%250Athe%2520effectiveness%2520of%2520the%2520integrated%2520thermal-haptic%2520approach%2520for%2520advanced%250Ahuman-machine%2520interaction%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Soft%20Fabric-Based%20Thermal%20Haptic%20Device%20for%20VR%20and%20Teleoperation&entry.906535625=Rui%20Chen%20and%20Domenico%20Chiaradia%20and%20Antonio%20Frisoli%20and%20Daniele%20Leonardis&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20fabric-based%20thermal-haptic%20interface%20for%20virtual%0Areality%20and%20teleoperation.%20It%20integrates%20pneumatic%20actuation%20and%20conductive%0Afabric%20with%20an%20innovative%20ultra-lightweight%20design%2C%20achieving%20only%202~g%20for%20each%0Afinger%20unit.%20By%20embedding%20heating%20elements%20within%20textile%20pneumatic%20chambers%2C%0Athe%20system%20delivers%20modulated%20pressure%20and%20thermal%20stimuli%20to%20fingerpads%0Athrough%20a%20fully%20soft%2C%20wearable%20interface.%0A%20%20Comprehensive%20characterization%20demonstrates%20rapid%20thermal%20modulation%20with%0Aheating%20rates%20up%20to%203%24%5E%7B%5Ccirc%7D%24C/s%2C%20enabling%20dynamic%20thermal%20feedback%20for%0Avirtual%20or%20teleoperation%20interactions.%20The%20pneumatic%20subsystem%20generates%20forces%0Aup%20to%208.93~N%20at%2050~kPa%2C%20while%20optimization%20of%20fingerpad-actuator%20clearance%0Aenhances%20cooling%20efficiency%20with%20minimal%20force%20reduction.%20Experimental%0Avalidation%20conducted%20with%20two%20different%20user%20studies%20shows%20high%20temperature%0Aidentification%20accuracy%20%280.98%20overall%29%20across%20three%20thermal%20levels%2C%20and%0Asignificant%20manipulation%20improvements%20in%20a%20virtual%20pick-and-place%20tasks.%0AResults%20show%20enhanced%20success%20rates%20%2888.5%5C%25%20to%2096.4%5C%25%2C%20p%20%3D%200.029%29%20and%20improved%0Aforce%20control%20precision%20%28p%20%3D%200.013%29%20when%20haptic%20feedback%20is%20enabled%2C%20validating%0Athe%20effectiveness%20of%20the%20integrated%20thermal-haptic%20approach%20for%20advanced%0Ahuman-machine%20interaction%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20831v1&entry.124074799=Read"},
{"title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation\n  of Image Sets", "author": "Dale Decatur and Thibault Groueix and Wang Yifan and Rana Hanocka and Vladimir Kim and Matheus Gadelha", "abstract": "  Text-to-image diffusion models enable high-quality image generation but are\ncomputationally expensive. While prior work optimizes per-inference efficiency,\nwe explore an orthogonal approach: reducing redundancy across correlated\nprompts. Our method leverages the coarse-to-fine nature of diffusion models,\nwhere early denoising steps capture shared structures among similar prompts. We\npropose a training-free approach that clusters prompts based on semantic\nsimilarity and shares computation in early diffusion steps. Experiments show\nthat for models trained conditioned on image embeddings, our approach\nsignificantly reduces compute cost while improving image quality. By leveraging\nUnClip's text-to-image prior, we enhance diffusion step allocation for greater\nefficiency. Our method seamlessly integrates with existing pipelines, scales\nwith prompt sets, and reduces the environmental and financial burden of\nlarge-scale text-to-image generation. Project page:\nhttps://ddecatur.github.io/hierarchical-diffusion/\n", "link": "http://arxiv.org/abs/2508.21032v1", "date": "2025-08-28", "relevancy": 1.974, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6753}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reusing%20Computation%20in%20Text-to-Image%20Diffusion%20for%20Efficient%20Generation%0A%20%20of%20Image%20Sets&body=Title%3A%20Reusing%20Computation%20in%20Text-to-Image%20Diffusion%20for%20Efficient%20Generation%0A%20%20of%20Image%20Sets%0AAuthor%3A%20Dale%20Decatur%20and%20Thibault%20Groueix%20and%20Wang%20Yifan%20and%20Rana%20Hanocka%20and%20Vladimir%20Kim%20and%20Matheus%20Gadelha%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20enable%20high-quality%20image%20generation%20but%20are%0Acomputationally%20expensive.%20While%20prior%20work%20optimizes%20per-inference%20efficiency%2C%0Awe%20explore%20an%20orthogonal%20approach%3A%20reducing%20redundancy%20across%20correlated%0Aprompts.%20Our%20method%20leverages%20the%20coarse-to-fine%20nature%20of%20diffusion%20models%2C%0Awhere%20early%20denoising%20steps%20capture%20shared%20structures%20among%20similar%20prompts.%20We%0Apropose%20a%20training-free%20approach%20that%20clusters%20prompts%20based%20on%20semantic%0Asimilarity%20and%20shares%20computation%20in%20early%20diffusion%20steps.%20Experiments%20show%0Athat%20for%20models%20trained%20conditioned%20on%20image%20embeddings%2C%20our%20approach%0Asignificantly%20reduces%20compute%20cost%20while%20improving%20image%20quality.%20By%20leveraging%0AUnClip%27s%20text-to-image%20prior%2C%20we%20enhance%20diffusion%20step%20allocation%20for%20greater%0Aefficiency.%20Our%20method%20seamlessly%20integrates%20with%20existing%20pipelines%2C%20scales%0Awith%20prompt%20sets%2C%20and%20reduces%20the%20environmental%20and%20financial%20burden%20of%0Alarge-scale%20text-to-image%20generation.%20Project%20page%3A%0Ahttps%3A//ddecatur.github.io/hierarchical-diffusion/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReusing%2520Computation%2520in%2520Text-to-Image%2520Diffusion%2520for%2520Efficient%2520Generation%250A%2520%2520of%2520Image%2520Sets%26entry.906535625%3DDale%2520Decatur%2520and%2520Thibault%2520Groueix%2520and%2520Wang%2520Yifan%2520and%2520Rana%2520Hanocka%2520and%2520Vladimir%2520Kim%2520and%2520Matheus%2520Gadelha%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520enable%2520high-quality%2520image%2520generation%2520but%2520are%250Acomputationally%2520expensive.%2520While%2520prior%2520work%2520optimizes%2520per-inference%2520efficiency%252C%250Awe%2520explore%2520an%2520orthogonal%2520approach%253A%2520reducing%2520redundancy%2520across%2520correlated%250Aprompts.%2520Our%2520method%2520leverages%2520the%2520coarse-to-fine%2520nature%2520of%2520diffusion%2520models%252C%250Awhere%2520early%2520denoising%2520steps%2520capture%2520shared%2520structures%2520among%2520similar%2520prompts.%2520We%250Apropose%2520a%2520training-free%2520approach%2520that%2520clusters%2520prompts%2520based%2520on%2520semantic%250Asimilarity%2520and%2520shares%2520computation%2520in%2520early%2520diffusion%2520steps.%2520Experiments%2520show%250Athat%2520for%2520models%2520trained%2520conditioned%2520on%2520image%2520embeddings%252C%2520our%2520approach%250Asignificantly%2520reduces%2520compute%2520cost%2520while%2520improving%2520image%2520quality.%2520By%2520leveraging%250AUnClip%2527s%2520text-to-image%2520prior%252C%2520we%2520enhance%2520diffusion%2520step%2520allocation%2520for%2520greater%250Aefficiency.%2520Our%2520method%2520seamlessly%2520integrates%2520with%2520existing%2520pipelines%252C%2520scales%250Awith%2520prompt%2520sets%252C%2520and%2520reduces%2520the%2520environmental%2520and%2520financial%2520burden%2520of%250Alarge-scale%2520text-to-image%2520generation.%2520Project%2520page%253A%250Ahttps%253A//ddecatur.github.io/hierarchical-diffusion/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reusing%20Computation%20in%20Text-to-Image%20Diffusion%20for%20Efficient%20Generation%0A%20%20of%20Image%20Sets&entry.906535625=Dale%20Decatur%20and%20Thibault%20Groueix%20and%20Wang%20Yifan%20and%20Rana%20Hanocka%20and%20Vladimir%20Kim%20and%20Matheus%20Gadelha&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20enable%20high-quality%20image%20generation%20but%20are%0Acomputationally%20expensive.%20While%20prior%20work%20optimizes%20per-inference%20efficiency%2C%0Awe%20explore%20an%20orthogonal%20approach%3A%20reducing%20redundancy%20across%20correlated%0Aprompts.%20Our%20method%20leverages%20the%20coarse-to-fine%20nature%20of%20diffusion%20models%2C%0Awhere%20early%20denoising%20steps%20capture%20shared%20structures%20among%20similar%20prompts.%20We%0Apropose%20a%20training-free%20approach%20that%20clusters%20prompts%20based%20on%20semantic%0Asimilarity%20and%20shares%20computation%20in%20early%20diffusion%20steps.%20Experiments%20show%0Athat%20for%20models%20trained%20conditioned%20on%20image%20embeddings%2C%20our%20approach%0Asignificantly%20reduces%20compute%20cost%20while%20improving%20image%20quality.%20By%20leveraging%0AUnClip%27s%20text-to-image%20prior%2C%20we%20enhance%20diffusion%20step%20allocation%20for%20greater%0Aefficiency.%20Our%20method%20seamlessly%20integrates%20with%20existing%20pipelines%2C%20scales%0Awith%20prompt%20sets%2C%20and%20reduces%20the%20environmental%20and%20financial%20burden%20of%0Alarge-scale%20text-to-image%20generation.%20Project%20page%3A%0Ahttps%3A//ddecatur.github.io/hierarchical-diffusion/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21032v1&entry.124074799=Read"},
{"title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP\n  Generator", "author": "Huynh Tong Dang Khoa and Dang Hoai Nam and Vo Nguyen Le Duy", "abstract": "  Labeled handwriting data is often scarce, limiting the effectiveness of\nrecognition systems that require diverse, style-consistent training samples.\nHandwriting synthesis offers a promising solution by generating artificial data\nto augment training. However, current methods face two major limitations.\nFirst, most are built on conventional convolutional architectures, which\nstruggle to model long-range dependencies and complex stroke patterns. Second,\nthey largely ignore the crucial role of frequency information, which is\nessential for capturing fine-grained stylistic and structural details in\nhandwriting. To address these challenges, we propose FW-GAN, a one-shot\nhandwriting synthesis framework that generates realistic, writer-consistent\ntext from a single example. Our generator integrates a phase-aware Wave-MLP to\nbetter capture spatial relationships while preserving subtle stylistic cues. We\nfurther introduce a frequency-guided discriminator that leverages\nhigh-frequency components to enhance the authenticity detection of generated\nsamples. Additionally, we introduce a novel Frequency Distribution Loss that\naligns the frequency characteristics of synthetic and real handwriting, thereby\nenhancing visual fidelity. Experiments on Vietnamese and English handwriting\ndatasets demonstrate that FW-GAN generates high-quality, style-consistent\nhandwriting, making it a valuable tool for augmenting data in low-resource\nhandwriting recognition (HTR) pipelines. Official implementation is available\nat https://github.com/DAIR-Group/FW-GAN\n", "link": "http://arxiv.org/abs/2508.21040v1", "date": "2025-08-28", "relevancy": 1.9615, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5047}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4906}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FW-GAN%3A%20Frequency-Driven%20Handwriting%20Synthesis%20with%20Wave-Modulated%20MLP%0A%20%20Generator&body=Title%3A%20FW-GAN%3A%20Frequency-Driven%20Handwriting%20Synthesis%20with%20Wave-Modulated%20MLP%0A%20%20Generator%0AAuthor%3A%20Huynh%20Tong%20Dang%20Khoa%20and%20Dang%20Hoai%20Nam%20and%20Vo%20Nguyen%20Le%20Duy%0AAbstract%3A%20%20%20Labeled%20handwriting%20data%20is%20often%20scarce%2C%20limiting%20the%20effectiveness%20of%0Arecognition%20systems%20that%20require%20diverse%2C%20style-consistent%20training%20samples.%0AHandwriting%20synthesis%20offers%20a%20promising%20solution%20by%20generating%20artificial%20data%0Ato%20augment%20training.%20However%2C%20current%20methods%20face%20two%20major%20limitations.%0AFirst%2C%20most%20are%20built%20on%20conventional%20convolutional%20architectures%2C%20which%0Astruggle%20to%20model%20long-range%20dependencies%20and%20complex%20stroke%20patterns.%20Second%2C%0Athey%20largely%20ignore%20the%20crucial%20role%20of%20frequency%20information%2C%20which%20is%0Aessential%20for%20capturing%20fine-grained%20stylistic%20and%20structural%20details%20in%0Ahandwriting.%20To%20address%20these%20challenges%2C%20we%20propose%20FW-GAN%2C%20a%20one-shot%0Ahandwriting%20synthesis%20framework%20that%20generates%20realistic%2C%20writer-consistent%0Atext%20from%20a%20single%20example.%20Our%20generator%20integrates%20a%20phase-aware%20Wave-MLP%20to%0Abetter%20capture%20spatial%20relationships%20while%20preserving%20subtle%20stylistic%20cues.%20We%0Afurther%20introduce%20a%20frequency-guided%20discriminator%20that%20leverages%0Ahigh-frequency%20components%20to%20enhance%20the%20authenticity%20detection%20of%20generated%0Asamples.%20Additionally%2C%20we%20introduce%20a%20novel%20Frequency%20Distribution%20Loss%20that%0Aaligns%20the%20frequency%20characteristics%20of%20synthetic%20and%20real%20handwriting%2C%20thereby%0Aenhancing%20visual%20fidelity.%20Experiments%20on%20Vietnamese%20and%20English%20handwriting%0Adatasets%20demonstrate%20that%20FW-GAN%20generates%20high-quality%2C%20style-consistent%0Ahandwriting%2C%20making%20it%20a%20valuable%20tool%20for%20augmenting%20data%20in%20low-resource%0Ahandwriting%20recognition%20%28HTR%29%20pipelines.%20Official%20implementation%20is%20available%0Aat%20https%3A//github.com/DAIR-Group/FW-GAN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFW-GAN%253A%2520Frequency-Driven%2520Handwriting%2520Synthesis%2520with%2520Wave-Modulated%2520MLP%250A%2520%2520Generator%26entry.906535625%3DHuynh%2520Tong%2520Dang%2520Khoa%2520and%2520Dang%2520Hoai%2520Nam%2520and%2520Vo%2520Nguyen%2520Le%2520Duy%26entry.1292438233%3D%2520%2520Labeled%2520handwriting%2520data%2520is%2520often%2520scarce%252C%2520limiting%2520the%2520effectiveness%2520of%250Arecognition%2520systems%2520that%2520require%2520diverse%252C%2520style-consistent%2520training%2520samples.%250AHandwriting%2520synthesis%2520offers%2520a%2520promising%2520solution%2520by%2520generating%2520artificial%2520data%250Ato%2520augment%2520training.%2520However%252C%2520current%2520methods%2520face%2520two%2520major%2520limitations.%250AFirst%252C%2520most%2520are%2520built%2520on%2520conventional%2520convolutional%2520architectures%252C%2520which%250Astruggle%2520to%2520model%2520long-range%2520dependencies%2520and%2520complex%2520stroke%2520patterns.%2520Second%252C%250Athey%2520largely%2520ignore%2520the%2520crucial%2520role%2520of%2520frequency%2520information%252C%2520which%2520is%250Aessential%2520for%2520capturing%2520fine-grained%2520stylistic%2520and%2520structural%2520details%2520in%250Ahandwriting.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FW-GAN%252C%2520a%2520one-shot%250Ahandwriting%2520synthesis%2520framework%2520that%2520generates%2520realistic%252C%2520writer-consistent%250Atext%2520from%2520a%2520single%2520example.%2520Our%2520generator%2520integrates%2520a%2520phase-aware%2520Wave-MLP%2520to%250Abetter%2520capture%2520spatial%2520relationships%2520while%2520preserving%2520subtle%2520stylistic%2520cues.%2520We%250Afurther%2520introduce%2520a%2520frequency-guided%2520discriminator%2520that%2520leverages%250Ahigh-frequency%2520components%2520to%2520enhance%2520the%2520authenticity%2520detection%2520of%2520generated%250Asamples.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520Frequency%2520Distribution%2520Loss%2520that%250Aaligns%2520the%2520frequency%2520characteristics%2520of%2520synthetic%2520and%2520real%2520handwriting%252C%2520thereby%250Aenhancing%2520visual%2520fidelity.%2520Experiments%2520on%2520Vietnamese%2520and%2520English%2520handwriting%250Adatasets%2520demonstrate%2520that%2520FW-GAN%2520generates%2520high-quality%252C%2520style-consistent%250Ahandwriting%252C%2520making%2520it%2520a%2520valuable%2520tool%2520for%2520augmenting%2520data%2520in%2520low-resource%250Ahandwriting%2520recognition%2520%2528HTR%2529%2520pipelines.%2520Official%2520implementation%2520is%2520available%250Aat%2520https%253A//github.com/DAIR-Group/FW-GAN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FW-GAN%3A%20Frequency-Driven%20Handwriting%20Synthesis%20with%20Wave-Modulated%20MLP%0A%20%20Generator&entry.906535625=Huynh%20Tong%20Dang%20Khoa%20and%20Dang%20Hoai%20Nam%20and%20Vo%20Nguyen%20Le%20Duy&entry.1292438233=%20%20Labeled%20handwriting%20data%20is%20often%20scarce%2C%20limiting%20the%20effectiveness%20of%0Arecognition%20systems%20that%20require%20diverse%2C%20style-consistent%20training%20samples.%0AHandwriting%20synthesis%20offers%20a%20promising%20solution%20by%20generating%20artificial%20data%0Ato%20augment%20training.%20However%2C%20current%20methods%20face%20two%20major%20limitations.%0AFirst%2C%20most%20are%20built%20on%20conventional%20convolutional%20architectures%2C%20which%0Astruggle%20to%20model%20long-range%20dependencies%20and%20complex%20stroke%20patterns.%20Second%2C%0Athey%20largely%20ignore%20the%20crucial%20role%20of%20frequency%20information%2C%20which%20is%0Aessential%20for%20capturing%20fine-grained%20stylistic%20and%20structural%20details%20in%0Ahandwriting.%20To%20address%20these%20challenges%2C%20we%20propose%20FW-GAN%2C%20a%20one-shot%0Ahandwriting%20synthesis%20framework%20that%20generates%20realistic%2C%20writer-consistent%0Atext%20from%20a%20single%20example.%20Our%20generator%20integrates%20a%20phase-aware%20Wave-MLP%20to%0Abetter%20capture%20spatial%20relationships%20while%20preserving%20subtle%20stylistic%20cues.%20We%0Afurther%20introduce%20a%20frequency-guided%20discriminator%20that%20leverages%0Ahigh-frequency%20components%20to%20enhance%20the%20authenticity%20detection%20of%20generated%0Asamples.%20Additionally%2C%20we%20introduce%20a%20novel%20Frequency%20Distribution%20Loss%20that%0Aaligns%20the%20frequency%20characteristics%20of%20synthetic%20and%20real%20handwriting%2C%20thereby%0Aenhancing%20visual%20fidelity.%20Experiments%20on%20Vietnamese%20and%20English%20handwriting%0Adatasets%20demonstrate%20that%20FW-GAN%20generates%20high-quality%2C%20style-consistent%0Ahandwriting%2C%20making%20it%20a%20valuable%20tool%20for%20augmenting%20data%20in%20low-resource%0Ahandwriting%20recognition%20%28HTR%29%20pipelines.%20Official%20implementation%20is%20available%0Aat%20https%3A//github.com/DAIR-Group/FW-GAN%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21040v1&entry.124074799=Read"},
{"title": "Graph-Based Feature Augmentation for Predictive Tasks on Relational\n  Datasets", "author": "Lianpeng Qiao and Ziqi Cao and Kaiyu Feng and Ye Yuan and Guoren Wang", "abstract": "  Data has become a foundational asset driving innovation across domains such\nas finance, healthcare, and e-commerce. In these areas, predictive modeling\nover relational tables is commonly employed, with increasing emphasis on\nreducing manual effort through automated machine learning (AutoML) techniques.\nThis raises an interesting question: can feature augmentation itself be\nautomated and identify and utilize task-related relational signals?\n  To address this challenge, we propose an end-to-end automated feature\naugmentation framework, ReCoGNN, which enhances initial datasets using features\nextracted from multiple relational tables to support predictive tasks. ReCoGNN\nfirst captures semantic dependencies within each table by modeling intra-table\nattribute relationships, enabling it to partition tables into structured,\nsemantically coherent segments. It then constructs a heterogeneous weighted\ngraph that represents inter-row relationships across all segments. Finally,\nReCoGNN leverages message-passing graph neural networks to propagate\ninformation through the graph, guiding feature selection and augmenting the\noriginal dataset. Extensive experiments conducted on ten real-life and\nsynthetic datasets demonstrate that ReCoGNN consistently outperforms existing\nmethods on both classification and regression tasks.\n", "link": "http://arxiv.org/abs/2508.20986v1", "date": "2025-08-28", "relevancy": 1.9523, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4869}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Feature%20Augmentation%20for%20Predictive%20Tasks%20on%20Relational%0A%20%20Datasets&body=Title%3A%20Graph-Based%20Feature%20Augmentation%20for%20Predictive%20Tasks%20on%20Relational%0A%20%20Datasets%0AAuthor%3A%20Lianpeng%20Qiao%20and%20Ziqi%20Cao%20and%20Kaiyu%20Feng%20and%20Ye%20Yuan%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Data%20has%20become%20a%20foundational%20asset%20driving%20innovation%20across%20domains%20such%0Aas%20finance%2C%20healthcare%2C%20and%20e-commerce.%20In%20these%20areas%2C%20predictive%20modeling%0Aover%20relational%20tables%20is%20commonly%20employed%2C%20with%20increasing%20emphasis%20on%0Areducing%20manual%20effort%20through%20automated%20machine%20learning%20%28AutoML%29%20techniques.%0AThis%20raises%20an%20interesting%20question%3A%20can%20feature%20augmentation%20itself%20be%0Aautomated%20and%20identify%20and%20utilize%20task-related%20relational%20signals%3F%0A%20%20To%20address%20this%20challenge%2C%20we%20propose%20an%20end-to-end%20automated%20feature%0Aaugmentation%20framework%2C%20ReCoGNN%2C%20which%20enhances%20initial%20datasets%20using%20features%0Aextracted%20from%20multiple%20relational%20tables%20to%20support%20predictive%20tasks.%20ReCoGNN%0Afirst%20captures%20semantic%20dependencies%20within%20each%20table%20by%20modeling%20intra-table%0Aattribute%20relationships%2C%20enabling%20it%20to%20partition%20tables%20into%20structured%2C%0Asemantically%20coherent%20segments.%20It%20then%20constructs%20a%20heterogeneous%20weighted%0Agraph%20that%20represents%20inter-row%20relationships%20across%20all%20segments.%20Finally%2C%0AReCoGNN%20leverages%20message-passing%20graph%20neural%20networks%20to%20propagate%0Ainformation%20through%20the%20graph%2C%20guiding%20feature%20selection%20and%20augmenting%20the%0Aoriginal%20dataset.%20Extensive%20experiments%20conducted%20on%20ten%20real-life%20and%0Asynthetic%20datasets%20demonstrate%20that%20ReCoGNN%20consistently%20outperforms%20existing%0Amethods%20on%20both%20classification%20and%20regression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Feature%2520Augmentation%2520for%2520Predictive%2520Tasks%2520on%2520Relational%250A%2520%2520Datasets%26entry.906535625%3DLianpeng%2520Qiao%2520and%2520Ziqi%2520Cao%2520and%2520Kaiyu%2520Feng%2520and%2520Ye%2520Yuan%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Data%2520has%2520become%2520a%2520foundational%2520asset%2520driving%2520innovation%2520across%2520domains%2520such%250Aas%2520finance%252C%2520healthcare%252C%2520and%2520e-commerce.%2520In%2520these%2520areas%252C%2520predictive%2520modeling%250Aover%2520relational%2520tables%2520is%2520commonly%2520employed%252C%2520with%2520increasing%2520emphasis%2520on%250Areducing%2520manual%2520effort%2520through%2520automated%2520machine%2520learning%2520%2528AutoML%2529%2520techniques.%250AThis%2520raises%2520an%2520interesting%2520question%253A%2520can%2520feature%2520augmentation%2520itself%2520be%250Aautomated%2520and%2520identify%2520and%2520utilize%2520task-related%2520relational%2520signals%253F%250A%2520%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520end-to-end%2520automated%2520feature%250Aaugmentation%2520framework%252C%2520ReCoGNN%252C%2520which%2520enhances%2520initial%2520datasets%2520using%2520features%250Aextracted%2520from%2520multiple%2520relational%2520tables%2520to%2520support%2520predictive%2520tasks.%2520ReCoGNN%250Afirst%2520captures%2520semantic%2520dependencies%2520within%2520each%2520table%2520by%2520modeling%2520intra-table%250Aattribute%2520relationships%252C%2520enabling%2520it%2520to%2520partition%2520tables%2520into%2520structured%252C%250Asemantically%2520coherent%2520segments.%2520It%2520then%2520constructs%2520a%2520heterogeneous%2520weighted%250Agraph%2520that%2520represents%2520inter-row%2520relationships%2520across%2520all%2520segments.%2520Finally%252C%250AReCoGNN%2520leverages%2520message-passing%2520graph%2520neural%2520networks%2520to%2520propagate%250Ainformation%2520through%2520the%2520graph%252C%2520guiding%2520feature%2520selection%2520and%2520augmenting%2520the%250Aoriginal%2520dataset.%2520Extensive%2520experiments%2520conducted%2520on%2520ten%2520real-life%2520and%250Asynthetic%2520datasets%2520demonstrate%2520that%2520ReCoGNN%2520consistently%2520outperforms%2520existing%250Amethods%2520on%2520both%2520classification%2520and%2520regression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Feature%20Augmentation%20for%20Predictive%20Tasks%20on%20Relational%0A%20%20Datasets&entry.906535625=Lianpeng%20Qiao%20and%20Ziqi%20Cao%20and%20Kaiyu%20Feng%20and%20Ye%20Yuan%20and%20Guoren%20Wang&entry.1292438233=%20%20Data%20has%20become%20a%20foundational%20asset%20driving%20innovation%20across%20domains%20such%0Aas%20finance%2C%20healthcare%2C%20and%20e-commerce.%20In%20these%20areas%2C%20predictive%20modeling%0Aover%20relational%20tables%20is%20commonly%20employed%2C%20with%20increasing%20emphasis%20on%0Areducing%20manual%20effort%20through%20automated%20machine%20learning%20%28AutoML%29%20techniques.%0AThis%20raises%20an%20interesting%20question%3A%20can%20feature%20augmentation%20itself%20be%0Aautomated%20and%20identify%20and%20utilize%20task-related%20relational%20signals%3F%0A%20%20To%20address%20this%20challenge%2C%20we%20propose%20an%20end-to-end%20automated%20feature%0Aaugmentation%20framework%2C%20ReCoGNN%2C%20which%20enhances%20initial%20datasets%20using%20features%0Aextracted%20from%20multiple%20relational%20tables%20to%20support%20predictive%20tasks.%20ReCoGNN%0Afirst%20captures%20semantic%20dependencies%20within%20each%20table%20by%20modeling%20intra-table%0Aattribute%20relationships%2C%20enabling%20it%20to%20partition%20tables%20into%20structured%2C%0Asemantically%20coherent%20segments.%20It%20then%20constructs%20a%20heterogeneous%20weighted%0Agraph%20that%20represents%20inter-row%20relationships%20across%20all%20segments.%20Finally%2C%0AReCoGNN%20leverages%20message-passing%20graph%20neural%20networks%20to%20propagate%0Ainformation%20through%20the%20graph%2C%20guiding%20feature%20selection%20and%20augmenting%20the%0Aoriginal%20dataset.%20Extensive%20experiments%20conducted%20on%20ten%20real-life%20and%0Asynthetic%20datasets%20demonstrate%20that%20ReCoGNN%20consistently%20outperforms%20existing%0Amethods%20on%20both%20classification%20and%20regression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20986v1&entry.124074799=Read"},
{"title": "Understanding, Protecting, and Augmenting Human Cognition with\n  Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop", "author": "Lev Tankelevitch and Elena L. Glassman and Jessica He and Aniket Kittur and Mina Lee and Srishti Palani and Advait Sarkar and Gonzalo Ramos and Yvonne Rogers and Hari Subramonyam", "abstract": "  Generative AI (GenAI) radically expands the scope and capability of\nautomation for work, education, and everyday tasks, a transformation posing\nboth risks and opportunities for human cognition. How will human cognition\nchange, and what opportunities are there for GenAI to augment it? Which\ntheories, metrics, and other tools are needed to address these questions? The\nCHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of\nhow the use of GenAI affects human thought, from metacognition to critical\nthinking, memory, and creativity, with an emerging design practice for building\nGenAI tools that both protect and augment human thought. Fifty-six researchers,\ndesigners, and thinkers from across disciplines as well as industry and\nacademia, along with 34 papers and portfolios, seeded a day of discussion,\nideation, and community-building. We synthesize this material here to begin\nmapping the space of research and design opportunities and to catalyze a\nmultidisciplinary community around this pressing area of research.\n", "link": "http://arxiv.org/abs/2508.21036v1", "date": "2025-08-28", "relevancy": 1.9504, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5169}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4824}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%2C%20Protecting%2C%20and%20Augmenting%20Human%20Cognition%20with%0A%20%20Generative%20AI%3A%20A%20Synthesis%20of%20the%20CHI%202025%20Tools%20for%20Thought%20Workshop&body=Title%3A%20Understanding%2C%20Protecting%2C%20and%20Augmenting%20Human%20Cognition%20with%0A%20%20Generative%20AI%3A%20A%20Synthesis%20of%20the%20CHI%202025%20Tools%20for%20Thought%20Workshop%0AAuthor%3A%20Lev%20Tankelevitch%20and%20Elena%20L.%20Glassman%20and%20Jessica%20He%20and%20Aniket%20Kittur%20and%20Mina%20Lee%20and%20Srishti%20Palani%20and%20Advait%20Sarkar%20and%20Gonzalo%20Ramos%20and%20Yvonne%20Rogers%20and%20Hari%20Subramonyam%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20radically%20expands%20the%20scope%20and%20capability%20of%0Aautomation%20for%20work%2C%20education%2C%20and%20everyday%20tasks%2C%20a%20transformation%20posing%0Aboth%20risks%20and%20opportunities%20for%20human%20cognition.%20How%20will%20human%20cognition%0Achange%2C%20and%20what%20opportunities%20are%20there%20for%20GenAI%20to%20augment%20it%3F%20Which%0Atheories%2C%20metrics%2C%20and%20other%20tools%20are%20needed%20to%20address%20these%20questions%3F%20The%0ACHI%202025%20workshop%20on%20Tools%20for%20Thought%20aimed%20to%20bridge%20an%20emerging%20science%20of%0Ahow%20the%20use%20of%20GenAI%20affects%20human%20thought%2C%20from%20metacognition%20to%20critical%0Athinking%2C%20memory%2C%20and%20creativity%2C%20with%20an%20emerging%20design%20practice%20for%20building%0AGenAI%20tools%20that%20both%20protect%20and%20augment%20human%20thought.%20Fifty-six%20researchers%2C%0Adesigners%2C%20and%20thinkers%20from%20across%20disciplines%20as%20well%20as%20industry%20and%0Aacademia%2C%20along%20with%2034%20papers%20and%20portfolios%2C%20seeded%20a%20day%20of%20discussion%2C%0Aideation%2C%20and%20community-building.%20We%20synthesize%20this%20material%20here%20to%20begin%0Amapping%20the%20space%20of%20research%20and%20design%20opportunities%20and%20to%20catalyze%20a%0Amultidisciplinary%20community%20around%20this%20pressing%20area%20of%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%252C%2520Protecting%252C%2520and%2520Augmenting%2520Human%2520Cognition%2520with%250A%2520%2520Generative%2520AI%253A%2520A%2520Synthesis%2520of%2520the%2520CHI%25202025%2520Tools%2520for%2520Thought%2520Workshop%26entry.906535625%3DLev%2520Tankelevitch%2520and%2520Elena%2520L.%2520Glassman%2520and%2520Jessica%2520He%2520and%2520Aniket%2520Kittur%2520and%2520Mina%2520Lee%2520and%2520Srishti%2520Palani%2520and%2520Advait%2520Sarkar%2520and%2520Gonzalo%2520Ramos%2520and%2520Yvonne%2520Rogers%2520and%2520Hari%2520Subramonyam%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520radically%2520expands%2520the%2520scope%2520and%2520capability%2520of%250Aautomation%2520for%2520work%252C%2520education%252C%2520and%2520everyday%2520tasks%252C%2520a%2520transformation%2520posing%250Aboth%2520risks%2520and%2520opportunities%2520for%2520human%2520cognition.%2520How%2520will%2520human%2520cognition%250Achange%252C%2520and%2520what%2520opportunities%2520are%2520there%2520for%2520GenAI%2520to%2520augment%2520it%253F%2520Which%250Atheories%252C%2520metrics%252C%2520and%2520other%2520tools%2520are%2520needed%2520to%2520address%2520these%2520questions%253F%2520The%250ACHI%25202025%2520workshop%2520on%2520Tools%2520for%2520Thought%2520aimed%2520to%2520bridge%2520an%2520emerging%2520science%2520of%250Ahow%2520the%2520use%2520of%2520GenAI%2520affects%2520human%2520thought%252C%2520from%2520metacognition%2520to%2520critical%250Athinking%252C%2520memory%252C%2520and%2520creativity%252C%2520with%2520an%2520emerging%2520design%2520practice%2520for%2520building%250AGenAI%2520tools%2520that%2520both%2520protect%2520and%2520augment%2520human%2520thought.%2520Fifty-six%2520researchers%252C%250Adesigners%252C%2520and%2520thinkers%2520from%2520across%2520disciplines%2520as%2520well%2520as%2520industry%2520and%250Aacademia%252C%2520along%2520with%252034%2520papers%2520and%2520portfolios%252C%2520seeded%2520a%2520day%2520of%2520discussion%252C%250Aideation%252C%2520and%2520community-building.%2520We%2520synthesize%2520this%2520material%2520here%2520to%2520begin%250Amapping%2520the%2520space%2520of%2520research%2520and%2520design%2520opportunities%2520and%2520to%2520catalyze%2520a%250Amultidisciplinary%2520community%2520around%2520this%2520pressing%2520area%2520of%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%2C%20Protecting%2C%20and%20Augmenting%20Human%20Cognition%20with%0A%20%20Generative%20AI%3A%20A%20Synthesis%20of%20the%20CHI%202025%20Tools%20for%20Thought%20Workshop&entry.906535625=Lev%20Tankelevitch%20and%20Elena%20L.%20Glassman%20and%20Jessica%20He%20and%20Aniket%20Kittur%20and%20Mina%20Lee%20and%20Srishti%20Palani%20and%20Advait%20Sarkar%20and%20Gonzalo%20Ramos%20and%20Yvonne%20Rogers%20and%20Hari%20Subramonyam&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20radically%20expands%20the%20scope%20and%20capability%20of%0Aautomation%20for%20work%2C%20education%2C%20and%20everyday%20tasks%2C%20a%20transformation%20posing%0Aboth%20risks%20and%20opportunities%20for%20human%20cognition.%20How%20will%20human%20cognition%0Achange%2C%20and%20what%20opportunities%20are%20there%20for%20GenAI%20to%20augment%20it%3F%20Which%0Atheories%2C%20metrics%2C%20and%20other%20tools%20are%20needed%20to%20address%20these%20questions%3F%20The%0ACHI%202025%20workshop%20on%20Tools%20for%20Thought%20aimed%20to%20bridge%20an%20emerging%20science%20of%0Ahow%20the%20use%20of%20GenAI%20affects%20human%20thought%2C%20from%20metacognition%20to%20critical%0Athinking%2C%20memory%2C%20and%20creativity%2C%20with%20an%20emerging%20design%20practice%20for%20building%0AGenAI%20tools%20that%20both%20protect%20and%20augment%20human%20thought.%20Fifty-six%20researchers%2C%0Adesigners%2C%20and%20thinkers%20from%20across%20disciplines%20as%20well%20as%20industry%20and%0Aacademia%2C%20along%20with%2034%20papers%20and%20portfolios%2C%20seeded%20a%20day%20of%20discussion%2C%0Aideation%2C%20and%20community-building.%20We%20synthesize%20this%20material%20here%20to%20begin%0Amapping%20the%20space%20of%20research%20and%20design%20opportunities%20and%20to%20catalyze%20a%0Amultidisciplinary%20community%20around%20this%20pressing%20area%20of%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21036v1&entry.124074799=Read"},
{"title": "Multilingual Dataset Integration Strategies for Robust Audio Deepfake\n  Detection: A SAFE Challenge System", "author": "Hashim Ali and Surya Subramani and Lekha Bollinani and Nithin Sai Adupa and Sali El-Loh and Hafiz Malik", "abstract": "  The SAFE Challenge evaluates synthetic speech detection across three tasks:\nunmodified audio, processed audio with compression artifacts, and laundered\naudio designed to evade detection. We systematically explore self-supervised\nlearning (SSL) front-ends, training data compositions, and audio length\nconfigurations for robust deepfake detection. Our AASIST-based approach\nincorporates WavLM large frontend with RawBoost augmentation, trained on a\nmultilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS\nsystems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.\nThrough extensive experimentation with different SSL front-ends, three training\ndata versions, and two audio lengths, we achieved second place in both Task 1\n(unmodified audio detection) and Task 3 (laundered audio detection),\ndemonstrating strong generalization and robustness.\n", "link": "http://arxiv.org/abs/2508.20983v1", "date": "2025-08-28", "relevancy": 1.9437, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.514}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4865}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Dataset%20Integration%20Strategies%20for%20Robust%20Audio%20Deepfake%0A%20%20Detection%3A%20A%20SAFE%20Challenge%20System&body=Title%3A%20Multilingual%20Dataset%20Integration%20Strategies%20for%20Robust%20Audio%20Deepfake%0A%20%20Detection%3A%20A%20SAFE%20Challenge%20System%0AAuthor%3A%20Hashim%20Ali%20and%20Surya%20Subramani%20and%20Lekha%20Bollinani%20and%20Nithin%20Sai%20Adupa%20and%20Sali%20El-Loh%20and%20Hafiz%20Malik%0AAbstract%3A%20%20%20The%20SAFE%20Challenge%20evaluates%20synthetic%20speech%20detection%20across%20three%20tasks%3A%0Aunmodified%20audio%2C%20processed%20audio%20with%20compression%20artifacts%2C%20and%20laundered%0Aaudio%20designed%20to%20evade%20detection.%20We%20systematically%20explore%20self-supervised%0Alearning%20%28SSL%29%20front-ends%2C%20training%20data%20compositions%2C%20and%20audio%20length%0Aconfigurations%20for%20robust%20deepfake%20detection.%20Our%20AASIST-based%20approach%0Aincorporates%20WavLM%20large%20frontend%20with%20RawBoost%20augmentation%2C%20trained%20on%20a%0Amultilingual%20dataset%20of%20256%2C600%20samples%20spanning%209%20languages%20and%20over%2070%20TTS%0Asystems%20from%20CodecFake%2C%20MLAAD%20v5%2C%20SpoofCeleb%2C%20Famous%20Figures%2C%20and%20MAILABS.%0AThrough%20extensive%20experimentation%20with%20different%20SSL%20front-ends%2C%20three%20training%0Adata%20versions%2C%20and%20two%20audio%20lengths%2C%20we%20achieved%20second%20place%20in%20both%20Task%201%0A%28unmodified%20audio%20detection%29%20and%20Task%203%20%28laundered%20audio%20detection%29%2C%0Ademonstrating%20strong%20generalization%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Dataset%2520Integration%2520Strategies%2520for%2520Robust%2520Audio%2520Deepfake%250A%2520%2520Detection%253A%2520A%2520SAFE%2520Challenge%2520System%26entry.906535625%3DHashim%2520Ali%2520and%2520Surya%2520Subramani%2520and%2520Lekha%2520Bollinani%2520and%2520Nithin%2520Sai%2520Adupa%2520and%2520Sali%2520El-Loh%2520and%2520Hafiz%2520Malik%26entry.1292438233%3D%2520%2520The%2520SAFE%2520Challenge%2520evaluates%2520synthetic%2520speech%2520detection%2520across%2520three%2520tasks%253A%250Aunmodified%2520audio%252C%2520processed%2520audio%2520with%2520compression%2520artifacts%252C%2520and%2520laundered%250Aaudio%2520designed%2520to%2520evade%2520detection.%2520We%2520systematically%2520explore%2520self-supervised%250Alearning%2520%2528SSL%2529%2520front-ends%252C%2520training%2520data%2520compositions%252C%2520and%2520audio%2520length%250Aconfigurations%2520for%2520robust%2520deepfake%2520detection.%2520Our%2520AASIST-based%2520approach%250Aincorporates%2520WavLM%2520large%2520frontend%2520with%2520RawBoost%2520augmentation%252C%2520trained%2520on%2520a%250Amultilingual%2520dataset%2520of%2520256%252C600%2520samples%2520spanning%25209%2520languages%2520and%2520over%252070%2520TTS%250Asystems%2520from%2520CodecFake%252C%2520MLAAD%2520v5%252C%2520SpoofCeleb%252C%2520Famous%2520Figures%252C%2520and%2520MAILABS.%250AThrough%2520extensive%2520experimentation%2520with%2520different%2520SSL%2520front-ends%252C%2520three%2520training%250Adata%2520versions%252C%2520and%2520two%2520audio%2520lengths%252C%2520we%2520achieved%2520second%2520place%2520in%2520both%2520Task%25201%250A%2528unmodified%2520audio%2520detection%2529%2520and%2520Task%25203%2520%2528laundered%2520audio%2520detection%2529%252C%250Ademonstrating%2520strong%2520generalization%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Dataset%20Integration%20Strategies%20for%20Robust%20Audio%20Deepfake%0A%20%20Detection%3A%20A%20SAFE%20Challenge%20System&entry.906535625=Hashim%20Ali%20and%20Surya%20Subramani%20and%20Lekha%20Bollinani%20and%20Nithin%20Sai%20Adupa%20and%20Sali%20El-Loh%20and%20Hafiz%20Malik&entry.1292438233=%20%20The%20SAFE%20Challenge%20evaluates%20synthetic%20speech%20detection%20across%20three%20tasks%3A%0Aunmodified%20audio%2C%20processed%20audio%20with%20compression%20artifacts%2C%20and%20laundered%0Aaudio%20designed%20to%20evade%20detection.%20We%20systematically%20explore%20self-supervised%0Alearning%20%28SSL%29%20front-ends%2C%20training%20data%20compositions%2C%20and%20audio%20length%0Aconfigurations%20for%20robust%20deepfake%20detection.%20Our%20AASIST-based%20approach%0Aincorporates%20WavLM%20large%20frontend%20with%20RawBoost%20augmentation%2C%20trained%20on%20a%0Amultilingual%20dataset%20of%20256%2C600%20samples%20spanning%209%20languages%20and%20over%2070%20TTS%0Asystems%20from%20CodecFake%2C%20MLAAD%20v5%2C%20SpoofCeleb%2C%20Famous%20Figures%2C%20and%20MAILABS.%0AThrough%20extensive%20experimentation%20with%20different%20SSL%20front-ends%2C%20three%20training%0Adata%20versions%2C%20and%20two%20audio%20lengths%2C%20we%20achieved%20second%20place%20in%20both%20Task%201%0A%28unmodified%20audio%20detection%29%20and%20Task%203%20%28laundered%20audio%20detection%29%2C%0Ademonstrating%20strong%20generalization%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20983v1&entry.124074799=Read"},
{"title": "Steering Towards Fairness: Mitigating Political Bias in LLMs", "author": "Afrozah Nadeem and Mark Dras and Usman Naseem", "abstract": "  Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases along political\nand economic dimensions. In this paper, we employ a framework for probing and\nmitigating such biases in decoder-based LLMs through analysis of internal model\nrepresentations. Grounded in the Political Compass Test (PCT), this method uses\ncontrastive pairs to extract and compare hidden layer activations from models\nlike Mistral and DeepSeek. We introduce a comprehensive activation extraction\npipeline capable of layer-wise analysis across multiple ideological axes,\nrevealing meaningful disparities linked to political framing. Our results show\nthat decoder LLMs systematically encode representational bias across layers,\nwhich can be leveraged for effective steering vector-based mitigation. This\nwork provides new insights into how political bias is encoded in LLMs and\noffers a principled approach to debiasing beyond surface-level output\ninterventions.\n", "link": "http://arxiv.org/abs/2508.08846v2", "date": "2025-08-28", "relevancy": 1.934, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Towards%20Fairness%3A%20Mitigating%20Political%20Bias%20in%20LLMs&body=Title%3A%20Steering%20Towards%20Fairness%3A%20Mitigating%20Political%20Bias%20in%20LLMs%0AAuthor%3A%20Afrozah%20Nadeem%20and%20Mark%20Dras%20and%20Usman%20Naseem%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%0Awidespread%20use%20across%20diverse%20real-world%20applications.%20However%2C%20concerns%20remain%0Aabout%20their%20tendency%20to%20encode%20and%20reproduce%20ideological%20biases%20along%20political%0Aand%20economic%20dimensions.%20In%20this%20paper%2C%20we%20employ%20a%20framework%20for%20probing%20and%0Amitigating%20such%20biases%20in%20decoder-based%20LLMs%20through%20analysis%20of%20internal%20model%0Arepresentations.%20Grounded%20in%20the%20Political%20Compass%20Test%20%28PCT%29%2C%20this%20method%20uses%0Acontrastive%20pairs%20to%20extract%20and%20compare%20hidden%20layer%20activations%20from%20models%0Alike%20Mistral%20and%20DeepSeek.%20We%20introduce%20a%20comprehensive%20activation%20extraction%0Apipeline%20capable%20of%20layer-wise%20analysis%20across%20multiple%20ideological%20axes%2C%0Arevealing%20meaningful%20disparities%20linked%20to%20political%20framing.%20Our%20results%20show%0Athat%20decoder%20LLMs%20systematically%20encode%20representational%20bias%20across%20layers%2C%0Awhich%20can%20be%20leveraged%20for%20effective%20steering%20vector-based%20mitigation.%20This%0Awork%20provides%20new%20insights%20into%20how%20political%20bias%20is%20encoded%20in%20LLMs%20and%0Aoffers%20a%20principled%20approach%20to%20debiasing%20beyond%20surface-level%20output%0Ainterventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Towards%2520Fairness%253A%2520Mitigating%2520Political%2520Bias%2520in%2520LLMs%26entry.906535625%3DAfrozah%2520Nadeem%2520and%2520Mark%2520Dras%2520and%2520Usman%2520Naseem%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520their%250Awidespread%2520use%2520across%2520diverse%2520real-world%2520applications.%2520However%252C%2520concerns%2520remain%250Aabout%2520their%2520tendency%2520to%2520encode%2520and%2520reproduce%2520ideological%2520biases%2520along%2520political%250Aand%2520economic%2520dimensions.%2520In%2520this%2520paper%252C%2520we%2520employ%2520a%2520framework%2520for%2520probing%2520and%250Amitigating%2520such%2520biases%2520in%2520decoder-based%2520LLMs%2520through%2520analysis%2520of%2520internal%2520model%250Arepresentations.%2520Grounded%2520in%2520the%2520Political%2520Compass%2520Test%2520%2528PCT%2529%252C%2520this%2520method%2520uses%250Acontrastive%2520pairs%2520to%2520extract%2520and%2520compare%2520hidden%2520layer%2520activations%2520from%2520models%250Alike%2520Mistral%2520and%2520DeepSeek.%2520We%2520introduce%2520a%2520comprehensive%2520activation%2520extraction%250Apipeline%2520capable%2520of%2520layer-wise%2520analysis%2520across%2520multiple%2520ideological%2520axes%252C%250Arevealing%2520meaningful%2520disparities%2520linked%2520to%2520political%2520framing.%2520Our%2520results%2520show%250Athat%2520decoder%2520LLMs%2520systematically%2520encode%2520representational%2520bias%2520across%2520layers%252C%250Awhich%2520can%2520be%2520leveraged%2520for%2520effective%2520steering%2520vector-based%2520mitigation.%2520This%250Awork%2520provides%2520new%2520insights%2520into%2520how%2520political%2520bias%2520is%2520encoded%2520in%2520LLMs%2520and%250Aoffers%2520a%2520principled%2520approach%2520to%2520debiasing%2520beyond%2520surface-level%2520output%250Ainterventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Towards%20Fairness%3A%20Mitigating%20Political%20Bias%20in%20LLMs&entry.906535625=Afrozah%20Nadeem%20and%20Mark%20Dras%20and%20Usman%20Naseem&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%0Awidespread%20use%20across%20diverse%20real-world%20applications.%20However%2C%20concerns%20remain%0Aabout%20their%20tendency%20to%20encode%20and%20reproduce%20ideological%20biases%20along%20political%0Aand%20economic%20dimensions.%20In%20this%20paper%2C%20we%20employ%20a%20framework%20for%20probing%20and%0Amitigating%20such%20biases%20in%20decoder-based%20LLMs%20through%20analysis%20of%20internal%20model%0Arepresentations.%20Grounded%20in%20the%20Political%20Compass%20Test%20%28PCT%29%2C%20this%20method%20uses%0Acontrastive%20pairs%20to%20extract%20and%20compare%20hidden%20layer%20activations%20from%20models%0Alike%20Mistral%20and%20DeepSeek.%20We%20introduce%20a%20comprehensive%20activation%20extraction%0Apipeline%20capable%20of%20layer-wise%20analysis%20across%20multiple%20ideological%20axes%2C%0Arevealing%20meaningful%20disparities%20linked%20to%20political%20framing.%20Our%20results%20show%0Athat%20decoder%20LLMs%20systematically%20encode%20representational%20bias%20across%20layers%2C%0Awhich%20can%20be%20leveraged%20for%20effective%20steering%20vector-based%20mitigation.%20This%0Awork%20provides%20new%20insights%20into%20how%20political%20bias%20is%20encoded%20in%20LLMs%20and%0Aoffers%20a%20principled%20approach%20to%20debiasing%20beyond%20surface-level%20output%0Ainterventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08846v2&entry.124074799=Read"},
{"title": "Improving Fine-Grained Control via Aggregation of Multiple Diffusion\n  Models", "author": "Conghan Yue and Zhengwei Peng and Shiyan Du and Zhi Ji and Chuangjian Cai and Le Wan and Dongyu Zhang", "abstract": "  While many diffusion models perform well when controlling particular aspects\nsuch as style, character, and interaction, they struggle with fine-grained\ncontrol due to dataset limitations and intricate model architecture design.\nThis paper introduces a novel training-free algorithm, independent of denoising\nnetwork architectures, for fine-grained generation, called Aggregation of\nMultiple Diffusion Models (AMDM). The algorithm integrates features from\nmultiple diffusion models into a specified model to activate particular\nfeatures and enable fine-grained control. Experimental results demonstrate that\nAMDM significantly improves fine-grained control without training, validating\nits effectiveness. Additionally, it reveals that diffusion models initially\nfocus on features such as position, attributes, and style, with later stages\nimproving generation quality and consistency. AMDM offers a new perspective for\ntackling the challenges of fine-grained conditional generation in diffusion\nmodels. Specifically, it allows us to fully utilize existing or develop new\nconditional diffusion models that control specific aspects, and then aggregate\nthem using the AMDM algorithm. This eliminates the need for constructing\ncomplex datasets, designing intricate model architectures, and incurring high\ntraining costs. Code is available at: https://github.com/Hammour-steak/AMDM.\n", "link": "http://arxiv.org/abs/2410.01262v4", "date": "2025-08-28", "relevancy": 1.9153, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6577}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6338}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Fine-Grained%20Control%20via%20Aggregation%20of%20Multiple%20Diffusion%0A%20%20Models&body=Title%3A%20Improving%20Fine-Grained%20Control%20via%20Aggregation%20of%20Multiple%20Diffusion%0A%20%20Models%0AAuthor%3A%20Conghan%20Yue%20and%20Zhengwei%20Peng%20and%20Shiyan%20Du%20and%20Zhi%20Ji%20and%20Chuangjian%20Cai%20and%20Le%20Wan%20and%20Dongyu%20Zhang%0AAbstract%3A%20%20%20While%20many%20diffusion%20models%20perform%20well%20when%20controlling%20particular%20aspects%0Asuch%20as%20style%2C%20character%2C%20and%20interaction%2C%20they%20struggle%20with%20fine-grained%0Acontrol%20due%20to%20dataset%20limitations%20and%20intricate%20model%20architecture%20design.%0AThis%20paper%20introduces%20a%20novel%20training-free%20algorithm%2C%20independent%20of%20denoising%0Anetwork%20architectures%2C%20for%20fine-grained%20generation%2C%20called%20Aggregation%20of%0AMultiple%20Diffusion%20Models%20%28AMDM%29.%20The%20algorithm%20integrates%20features%20from%0Amultiple%20diffusion%20models%20into%20a%20specified%20model%20to%20activate%20particular%0Afeatures%20and%20enable%20fine-grained%20control.%20Experimental%20results%20demonstrate%20that%0AAMDM%20significantly%20improves%20fine-grained%20control%20without%20training%2C%20validating%0Aits%20effectiveness.%20Additionally%2C%20it%20reveals%20that%20diffusion%20models%20initially%0Afocus%20on%20features%20such%20as%20position%2C%20attributes%2C%20and%20style%2C%20with%20later%20stages%0Aimproving%20generation%20quality%20and%20consistency.%20AMDM%20offers%20a%20new%20perspective%20for%0Atackling%20the%20challenges%20of%20fine-grained%20conditional%20generation%20in%20diffusion%0Amodels.%20Specifically%2C%20it%20allows%20us%20to%20fully%20utilize%20existing%20or%20develop%20new%0Aconditional%20diffusion%20models%20that%20control%20specific%20aspects%2C%20and%20then%20aggregate%0Athem%20using%20the%20AMDM%20algorithm.%20This%20eliminates%20the%20need%20for%20constructing%0Acomplex%20datasets%2C%20designing%20intricate%20model%20architectures%2C%20and%20incurring%20high%0Atraining%20costs.%20Code%20is%20available%20at%3A%20https%3A//github.com/Hammour-steak/AMDM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01262v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Fine-Grained%2520Control%2520via%2520Aggregation%2520of%2520Multiple%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DConghan%2520Yue%2520and%2520Zhengwei%2520Peng%2520and%2520Shiyan%2520Du%2520and%2520Zhi%2520Ji%2520and%2520Chuangjian%2520Cai%2520and%2520Le%2520Wan%2520and%2520Dongyu%2520Zhang%26entry.1292438233%3D%2520%2520While%2520many%2520diffusion%2520models%2520perform%2520well%2520when%2520controlling%2520particular%2520aspects%250Asuch%2520as%2520style%252C%2520character%252C%2520and%2520interaction%252C%2520they%2520struggle%2520with%2520fine-grained%250Acontrol%2520due%2520to%2520dataset%2520limitations%2520and%2520intricate%2520model%2520architecture%2520design.%250AThis%2520paper%2520introduces%2520a%2520novel%2520training-free%2520algorithm%252C%2520independent%2520of%2520denoising%250Anetwork%2520architectures%252C%2520for%2520fine-grained%2520generation%252C%2520called%2520Aggregation%2520of%250AMultiple%2520Diffusion%2520Models%2520%2528AMDM%2529.%2520The%2520algorithm%2520integrates%2520features%2520from%250Amultiple%2520diffusion%2520models%2520into%2520a%2520specified%2520model%2520to%2520activate%2520particular%250Afeatures%2520and%2520enable%2520fine-grained%2520control.%2520Experimental%2520results%2520demonstrate%2520that%250AAMDM%2520significantly%2520improves%2520fine-grained%2520control%2520without%2520training%252C%2520validating%250Aits%2520effectiveness.%2520Additionally%252C%2520it%2520reveals%2520that%2520diffusion%2520models%2520initially%250Afocus%2520on%2520features%2520such%2520as%2520position%252C%2520attributes%252C%2520and%2520style%252C%2520with%2520later%2520stages%250Aimproving%2520generation%2520quality%2520and%2520consistency.%2520AMDM%2520offers%2520a%2520new%2520perspective%2520for%250Atackling%2520the%2520challenges%2520of%2520fine-grained%2520conditional%2520generation%2520in%2520diffusion%250Amodels.%2520Specifically%252C%2520it%2520allows%2520us%2520to%2520fully%2520utilize%2520existing%2520or%2520develop%2520new%250Aconditional%2520diffusion%2520models%2520that%2520control%2520specific%2520aspects%252C%2520and%2520then%2520aggregate%250Athem%2520using%2520the%2520AMDM%2520algorithm.%2520This%2520eliminates%2520the%2520need%2520for%2520constructing%250Acomplex%2520datasets%252C%2520designing%2520intricate%2520model%2520architectures%252C%2520and%2520incurring%2520high%250Atraining%2520costs.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/Hammour-steak/AMDM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01262v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Fine-Grained%20Control%20via%20Aggregation%20of%20Multiple%20Diffusion%0A%20%20Models&entry.906535625=Conghan%20Yue%20and%20Zhengwei%20Peng%20and%20Shiyan%20Du%20and%20Zhi%20Ji%20and%20Chuangjian%20Cai%20and%20Le%20Wan%20and%20Dongyu%20Zhang&entry.1292438233=%20%20While%20many%20diffusion%20models%20perform%20well%20when%20controlling%20particular%20aspects%0Asuch%20as%20style%2C%20character%2C%20and%20interaction%2C%20they%20struggle%20with%20fine-grained%0Acontrol%20due%20to%20dataset%20limitations%20and%20intricate%20model%20architecture%20design.%0AThis%20paper%20introduces%20a%20novel%20training-free%20algorithm%2C%20independent%20of%20denoising%0Anetwork%20architectures%2C%20for%20fine-grained%20generation%2C%20called%20Aggregation%20of%0AMultiple%20Diffusion%20Models%20%28AMDM%29.%20The%20algorithm%20integrates%20features%20from%0Amultiple%20diffusion%20models%20into%20a%20specified%20model%20to%20activate%20particular%0Afeatures%20and%20enable%20fine-grained%20control.%20Experimental%20results%20demonstrate%20that%0AAMDM%20significantly%20improves%20fine-grained%20control%20without%20training%2C%20validating%0Aits%20effectiveness.%20Additionally%2C%20it%20reveals%20that%20diffusion%20models%20initially%0Afocus%20on%20features%20such%20as%20position%2C%20attributes%2C%20and%20style%2C%20with%20later%20stages%0Aimproving%20generation%20quality%20and%20consistency.%20AMDM%20offers%20a%20new%20perspective%20for%0Atackling%20the%20challenges%20of%20fine-grained%20conditional%20generation%20in%20diffusion%0Amodels.%20Specifically%2C%20it%20allows%20us%20to%20fully%20utilize%20existing%20or%20develop%20new%0Aconditional%20diffusion%20models%20that%20control%20specific%20aspects%2C%20and%20then%20aggregate%0Athem%20using%20the%20AMDM%20algorithm.%20This%20eliminates%20the%20need%20for%20constructing%0Acomplex%20datasets%2C%20designing%20intricate%20model%20architectures%2C%20and%20incurring%20high%0Atraining%20costs.%20Code%20is%20available%20at%3A%20https%3A//github.com/Hammour-steak/AMDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01262v4&entry.124074799=Read"},
{"title": "Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with\n  Nonstationary Rewards", "author": "Katherine B. Adams and Justin J. Boutilier and Qinyang He and Yonatan Mintz", "abstract": "  We study a sequential resource allocation problem where a decision maker\nselects subsets of agents at each period to maximize overall outcomes without\nprior knowledge of individual-level effects. Our framework applies to settings\nsuch as community health interventions, targeted digital advertising, and\nworkforce retention programs, where intervention effects evolve dynamically.\nAgents may exhibit habituation (diminished response from frequent selection) or\nrecovery (enhanced response from infrequent selection). The technical challenge\ncenters on nonstationary reward distributions that lead to changing\nintervention effects over time. The problem requires balancing two key\ncompeting objectives: heterogeneous individual rewards and the\nexploration-exploitation tradeoff in terms of learning for improved future\ndecisions as opposed to maximizing immediate outcomes. Our contribution\nintroduces the first framework incorporating this form of nonstationary rewards\nin the combinatorial multi-armed bandit literature. We develop algorithms with\ntheoretical guarantees on dynamic regret and demonstrate practical efficacy\nthrough a diabetes intervention case study. Our personalized community\nintervention algorithm achieved up to three times as much improvement in\nprogram enrollment compared to baseline approaches, validating the framework's\npotential for real-world applications. This work bridges theoretical advances\nin adaptive learning with practical challenges in population-level behavioral\nchange interventions.\n", "link": "http://arxiv.org/abs/2508.20923v1", "date": "2025-08-28", "relevancy": 1.9086, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5177}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4759}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite-Time%20Guarantees%20for%20Multi-Agent%20Combinatorial%20Bandits%20with%0A%20%20Nonstationary%20Rewards&body=Title%3A%20Finite-Time%20Guarantees%20for%20Multi-Agent%20Combinatorial%20Bandits%20with%0A%20%20Nonstationary%20Rewards%0AAuthor%3A%20Katherine%20B.%20Adams%20and%20Justin%20J.%20Boutilier%20and%20Qinyang%20He%20and%20Yonatan%20Mintz%0AAbstract%3A%20%20%20We%20study%20a%20sequential%20resource%20allocation%20problem%20where%20a%20decision%20maker%0Aselects%20subsets%20of%20agents%20at%20each%20period%20to%20maximize%20overall%20outcomes%20without%0Aprior%20knowledge%20of%20individual-level%20effects.%20Our%20framework%20applies%20to%20settings%0Asuch%20as%20community%20health%20interventions%2C%20targeted%20digital%20advertising%2C%20and%0Aworkforce%20retention%20programs%2C%20where%20intervention%20effects%20evolve%20dynamically.%0AAgents%20may%20exhibit%20habituation%20%28diminished%20response%20from%20frequent%20selection%29%20or%0Arecovery%20%28enhanced%20response%20from%20infrequent%20selection%29.%20The%20technical%20challenge%0Acenters%20on%20nonstationary%20reward%20distributions%20that%20lead%20to%20changing%0Aintervention%20effects%20over%20time.%20The%20problem%20requires%20balancing%20two%20key%0Acompeting%20objectives%3A%20heterogeneous%20individual%20rewards%20and%20the%0Aexploration-exploitation%20tradeoff%20in%20terms%20of%20learning%20for%20improved%20future%0Adecisions%20as%20opposed%20to%20maximizing%20immediate%20outcomes.%20Our%20contribution%0Aintroduces%20the%20first%20framework%20incorporating%20this%20form%20of%20nonstationary%20rewards%0Ain%20the%20combinatorial%20multi-armed%20bandit%20literature.%20We%20develop%20algorithms%20with%0Atheoretical%20guarantees%20on%20dynamic%20regret%20and%20demonstrate%20practical%20efficacy%0Athrough%20a%20diabetes%20intervention%20case%20study.%20Our%20personalized%20community%0Aintervention%20algorithm%20achieved%20up%20to%20three%20times%20as%20much%20improvement%20in%0Aprogram%20enrollment%20compared%20to%20baseline%20approaches%2C%20validating%20the%20framework%27s%0Apotential%20for%20real-world%20applications.%20This%20work%20bridges%20theoretical%20advances%0Ain%20adaptive%20learning%20with%20practical%20challenges%20in%20population-level%20behavioral%0Achange%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite-Time%2520Guarantees%2520for%2520Multi-Agent%2520Combinatorial%2520Bandits%2520with%250A%2520%2520Nonstationary%2520Rewards%26entry.906535625%3DKatherine%2520B.%2520Adams%2520and%2520Justin%2520J.%2520Boutilier%2520and%2520Qinyang%2520He%2520and%2520Yonatan%2520Mintz%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520sequential%2520resource%2520allocation%2520problem%2520where%2520a%2520decision%2520maker%250Aselects%2520subsets%2520of%2520agents%2520at%2520each%2520period%2520to%2520maximize%2520overall%2520outcomes%2520without%250Aprior%2520knowledge%2520of%2520individual-level%2520effects.%2520Our%2520framework%2520applies%2520to%2520settings%250Asuch%2520as%2520community%2520health%2520interventions%252C%2520targeted%2520digital%2520advertising%252C%2520and%250Aworkforce%2520retention%2520programs%252C%2520where%2520intervention%2520effects%2520evolve%2520dynamically.%250AAgents%2520may%2520exhibit%2520habituation%2520%2528diminished%2520response%2520from%2520frequent%2520selection%2529%2520or%250Arecovery%2520%2528enhanced%2520response%2520from%2520infrequent%2520selection%2529.%2520The%2520technical%2520challenge%250Acenters%2520on%2520nonstationary%2520reward%2520distributions%2520that%2520lead%2520to%2520changing%250Aintervention%2520effects%2520over%2520time.%2520The%2520problem%2520requires%2520balancing%2520two%2520key%250Acompeting%2520objectives%253A%2520heterogeneous%2520individual%2520rewards%2520and%2520the%250Aexploration-exploitation%2520tradeoff%2520in%2520terms%2520of%2520learning%2520for%2520improved%2520future%250Adecisions%2520as%2520opposed%2520to%2520maximizing%2520immediate%2520outcomes.%2520Our%2520contribution%250Aintroduces%2520the%2520first%2520framework%2520incorporating%2520this%2520form%2520of%2520nonstationary%2520rewards%250Ain%2520the%2520combinatorial%2520multi-armed%2520bandit%2520literature.%2520We%2520develop%2520algorithms%2520with%250Atheoretical%2520guarantees%2520on%2520dynamic%2520regret%2520and%2520demonstrate%2520practical%2520efficacy%250Athrough%2520a%2520diabetes%2520intervention%2520case%2520study.%2520Our%2520personalized%2520community%250Aintervention%2520algorithm%2520achieved%2520up%2520to%2520three%2520times%2520as%2520much%2520improvement%2520in%250Aprogram%2520enrollment%2520compared%2520to%2520baseline%2520approaches%252C%2520validating%2520the%2520framework%2527s%250Apotential%2520for%2520real-world%2520applications.%2520This%2520work%2520bridges%2520theoretical%2520advances%250Ain%2520adaptive%2520learning%2520with%2520practical%2520challenges%2520in%2520population-level%2520behavioral%250Achange%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite-Time%20Guarantees%20for%20Multi-Agent%20Combinatorial%20Bandits%20with%0A%20%20Nonstationary%20Rewards&entry.906535625=Katherine%20B.%20Adams%20and%20Justin%20J.%20Boutilier%20and%20Qinyang%20He%20and%20Yonatan%20Mintz&entry.1292438233=%20%20We%20study%20a%20sequential%20resource%20allocation%20problem%20where%20a%20decision%20maker%0Aselects%20subsets%20of%20agents%20at%20each%20period%20to%20maximize%20overall%20outcomes%20without%0Aprior%20knowledge%20of%20individual-level%20effects.%20Our%20framework%20applies%20to%20settings%0Asuch%20as%20community%20health%20interventions%2C%20targeted%20digital%20advertising%2C%20and%0Aworkforce%20retention%20programs%2C%20where%20intervention%20effects%20evolve%20dynamically.%0AAgents%20may%20exhibit%20habituation%20%28diminished%20response%20from%20frequent%20selection%29%20or%0Arecovery%20%28enhanced%20response%20from%20infrequent%20selection%29.%20The%20technical%20challenge%0Acenters%20on%20nonstationary%20reward%20distributions%20that%20lead%20to%20changing%0Aintervention%20effects%20over%20time.%20The%20problem%20requires%20balancing%20two%20key%0Acompeting%20objectives%3A%20heterogeneous%20individual%20rewards%20and%20the%0Aexploration-exploitation%20tradeoff%20in%20terms%20of%20learning%20for%20improved%20future%0Adecisions%20as%20opposed%20to%20maximizing%20immediate%20outcomes.%20Our%20contribution%0Aintroduces%20the%20first%20framework%20incorporating%20this%20form%20of%20nonstationary%20rewards%0Ain%20the%20combinatorial%20multi-armed%20bandit%20literature.%20We%20develop%20algorithms%20with%0Atheoretical%20guarantees%20on%20dynamic%20regret%20and%20demonstrate%20practical%20efficacy%0Athrough%20a%20diabetes%20intervention%20case%20study.%20Our%20personalized%20community%0Aintervention%20algorithm%20achieved%20up%20to%20three%20times%20as%20much%20improvement%20in%0Aprogram%20enrollment%20compared%20to%20baseline%20approaches%2C%20validating%20the%20framework%27s%0Apotential%20for%20real-world%20applications.%20This%20work%20bridges%20theoretical%20advances%0Ain%20adaptive%20learning%20with%20practical%20challenges%20in%20population-level%20behavioral%0Achange%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20923v1&entry.124074799=Read"},
{"title": "Learning Primitive Embodied World Models: Towards Scalable Robotic\n  Learning", "author": "Qiao Sun and Liujia Yang and Wei Tang and Wei Huang and Kaixin Xu and Yongchao Chen and Mingyu Liu and Jiange Yang and Haoyi Zhu and Yating Wang and Tong He and Yilun Chen and Xili Dai and Nanyang Ye and Qinying Gu", "abstract": "  While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.\n", "link": "http://arxiv.org/abs/2508.20840v1", "date": "2025-08-28", "relevancy": 1.9048, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6448}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Primitive%20Embodied%20World%20Models%3A%20Towards%20Scalable%20Robotic%0A%20%20Learning&body=Title%3A%20Learning%20Primitive%20Embodied%20World%20Models%3A%20Towards%20Scalable%20Robotic%0A%20%20Learning%0AAuthor%3A%20Qiao%20Sun%20and%20Liujia%20Yang%20and%20Wei%20Tang%20and%20Wei%20Huang%20and%20Kaixin%20Xu%20and%20Yongchao%20Chen%20and%20Mingyu%20Liu%20and%20Jiange%20Yang%20and%20Haoyi%20Zhu%20and%20Yating%20Wang%20and%20Tong%20He%20and%20Yilun%20Chen%20and%20Xili%20Dai%20and%20Nanyang%20Ye%20and%20Qinying%20Gu%0AAbstract%3A%20%20%20While%20video-generation-based%20embodied%20world%20models%20have%20gained%20increasing%0Aattention%2C%20their%20reliance%20on%20large-scale%20embodied%20interaction%20data%20remains%20a%0Akey%20bottleneck.%20The%20scarcity%2C%20difficulty%20of%20collection%2C%20and%20high%20dimensionality%0Aof%20embodied%20data%20fundamentally%20limit%20the%20alignment%20granularity%20between%20language%0Aand%20actions%20and%20exacerbate%20the%20challenge%20of%20long-horizon%20video%0Ageneration--hindering%20generative%20models%20from%20achieving%20a%20%22GPT%20moment%22%20in%20the%0Aembodied%20domain.%20There%20is%20a%20naive%20observation%3A%20the%20diversity%20of%20embodied%20data%0Afar%20exceeds%20the%20relatively%20small%20space%20of%20possible%20primitive%20motions.%20Based%20on%0Athis%20insight%2C%20we%20propose%20a%20novel%20paradigm%20for%20world%20modeling--Primitive%0AEmbodied%20World%20Models%20%28PEWM%29.%20By%20restricting%20video%20generation%20to%20fixed%20short%0Ahorizons%2C%20our%20approach%201%29%20enables%20fine-grained%20alignment%20between%20linguistic%0Aconcepts%20and%20visual%20representations%20of%20robotic%20actions%2C%202%29%20reduces%20learning%0Acomplexity%2C%203%29%20improves%20data%20efficiency%20in%20embodied%20data%20collection%2C%20and%204%29%0Adecreases%20inference%20latency.%20By%20equipping%20with%20a%20modular%20Vision-Language%20Model%0A%28VLM%29%20planner%20and%20a%20Start-Goal%20heatmap%20Guidance%20mechanism%20%28SGG%29%2C%20PEWM%20further%0Aenables%20flexible%20closed-loop%20control%20and%20supports%20compositional%20generalization%0Aof%20primitive-level%20policies%20over%20extended%2C%20complex%20tasks.%20Our%20framework%0Aleverages%20the%20spatiotemporal%20vision%20priors%20in%20video%20models%20and%20the%20semantic%0Aawareness%20of%20VLMs%20to%20bridge%20the%20gap%20between%20fine-grained%20physical%20interaction%0Aand%20high-level%20reasoning%2C%20paving%20the%20way%20toward%20scalable%2C%20interpretable%2C%20and%0Ageneral-purpose%20embodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Primitive%2520Embodied%2520World%2520Models%253A%2520Towards%2520Scalable%2520Robotic%250A%2520%2520Learning%26entry.906535625%3DQiao%2520Sun%2520and%2520Liujia%2520Yang%2520and%2520Wei%2520Tang%2520and%2520Wei%2520Huang%2520and%2520Kaixin%2520Xu%2520and%2520Yongchao%2520Chen%2520and%2520Mingyu%2520Liu%2520and%2520Jiange%2520Yang%2520and%2520Haoyi%2520Zhu%2520and%2520Yating%2520Wang%2520and%2520Tong%2520He%2520and%2520Yilun%2520Chen%2520and%2520Xili%2520Dai%2520and%2520Nanyang%2520Ye%2520and%2520Qinying%2520Gu%26entry.1292438233%3D%2520%2520While%2520video-generation-based%2520embodied%2520world%2520models%2520have%2520gained%2520increasing%250Aattention%252C%2520their%2520reliance%2520on%2520large-scale%2520embodied%2520interaction%2520data%2520remains%2520a%250Akey%2520bottleneck.%2520The%2520scarcity%252C%2520difficulty%2520of%2520collection%252C%2520and%2520high%2520dimensionality%250Aof%2520embodied%2520data%2520fundamentally%2520limit%2520the%2520alignment%2520granularity%2520between%2520language%250Aand%2520actions%2520and%2520exacerbate%2520the%2520challenge%2520of%2520long-horizon%2520video%250Ageneration--hindering%2520generative%2520models%2520from%2520achieving%2520a%2520%2522GPT%2520moment%2522%2520in%2520the%250Aembodied%2520domain.%2520There%2520is%2520a%2520naive%2520observation%253A%2520the%2520diversity%2520of%2520embodied%2520data%250Afar%2520exceeds%2520the%2520relatively%2520small%2520space%2520of%2520possible%2520primitive%2520motions.%2520Based%2520on%250Athis%2520insight%252C%2520we%2520propose%2520a%2520novel%2520paradigm%2520for%2520world%2520modeling--Primitive%250AEmbodied%2520World%2520Models%2520%2528PEWM%2529.%2520By%2520restricting%2520video%2520generation%2520to%2520fixed%2520short%250Ahorizons%252C%2520our%2520approach%25201%2529%2520enables%2520fine-grained%2520alignment%2520between%2520linguistic%250Aconcepts%2520and%2520visual%2520representations%2520of%2520robotic%2520actions%252C%25202%2529%2520reduces%2520learning%250Acomplexity%252C%25203%2529%2520improves%2520data%2520efficiency%2520in%2520embodied%2520data%2520collection%252C%2520and%25204%2529%250Adecreases%2520inference%2520latency.%2520By%2520equipping%2520with%2520a%2520modular%2520Vision-Language%2520Model%250A%2528VLM%2529%2520planner%2520and%2520a%2520Start-Goal%2520heatmap%2520Guidance%2520mechanism%2520%2528SGG%2529%252C%2520PEWM%2520further%250Aenables%2520flexible%2520closed-loop%2520control%2520and%2520supports%2520compositional%2520generalization%250Aof%2520primitive-level%2520policies%2520over%2520extended%252C%2520complex%2520tasks.%2520Our%2520framework%250Aleverages%2520the%2520spatiotemporal%2520vision%2520priors%2520in%2520video%2520models%2520and%2520the%2520semantic%250Aawareness%2520of%2520VLMs%2520to%2520bridge%2520the%2520gap%2520between%2520fine-grained%2520physical%2520interaction%250Aand%2520high-level%2520reasoning%252C%2520paving%2520the%2520way%2520toward%2520scalable%252C%2520interpretable%252C%2520and%250Ageneral-purpose%2520embodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Primitive%20Embodied%20World%20Models%3A%20Towards%20Scalable%20Robotic%0A%20%20Learning&entry.906535625=Qiao%20Sun%20and%20Liujia%20Yang%20and%20Wei%20Tang%20and%20Wei%20Huang%20and%20Kaixin%20Xu%20and%20Yongchao%20Chen%20and%20Mingyu%20Liu%20and%20Jiange%20Yang%20and%20Haoyi%20Zhu%20and%20Yating%20Wang%20and%20Tong%20He%20and%20Yilun%20Chen%20and%20Xili%20Dai%20and%20Nanyang%20Ye%20and%20Qinying%20Gu&entry.1292438233=%20%20While%20video-generation-based%20embodied%20world%20models%20have%20gained%20increasing%0Aattention%2C%20their%20reliance%20on%20large-scale%20embodied%20interaction%20data%20remains%20a%0Akey%20bottleneck.%20The%20scarcity%2C%20difficulty%20of%20collection%2C%20and%20high%20dimensionality%0Aof%20embodied%20data%20fundamentally%20limit%20the%20alignment%20granularity%20between%20language%0Aand%20actions%20and%20exacerbate%20the%20challenge%20of%20long-horizon%20video%0Ageneration--hindering%20generative%20models%20from%20achieving%20a%20%22GPT%20moment%22%20in%20the%0Aembodied%20domain.%20There%20is%20a%20naive%20observation%3A%20the%20diversity%20of%20embodied%20data%0Afar%20exceeds%20the%20relatively%20small%20space%20of%20possible%20primitive%20motions.%20Based%20on%0Athis%20insight%2C%20we%20propose%20a%20novel%20paradigm%20for%20world%20modeling--Primitive%0AEmbodied%20World%20Models%20%28PEWM%29.%20By%20restricting%20video%20generation%20to%20fixed%20short%0Ahorizons%2C%20our%20approach%201%29%20enables%20fine-grained%20alignment%20between%20linguistic%0Aconcepts%20and%20visual%20representations%20of%20robotic%20actions%2C%202%29%20reduces%20learning%0Acomplexity%2C%203%29%20improves%20data%20efficiency%20in%20embodied%20data%20collection%2C%20and%204%29%0Adecreases%20inference%20latency.%20By%20equipping%20with%20a%20modular%20Vision-Language%20Model%0A%28VLM%29%20planner%20and%20a%20Start-Goal%20heatmap%20Guidance%20mechanism%20%28SGG%29%2C%20PEWM%20further%0Aenables%20flexible%20closed-loop%20control%20and%20supports%20compositional%20generalization%0Aof%20primitive-level%20policies%20over%20extended%2C%20complex%20tasks.%20Our%20framework%0Aleverages%20the%20spatiotemporal%20vision%20priors%20in%20video%20models%20and%20the%20semantic%0Aawareness%20of%20VLMs%20to%20bridge%20the%20gap%20between%20fine-grained%20physical%20interaction%0Aand%20high-level%20reasoning%2C%20paving%20the%20way%20toward%20scalable%2C%20interpretable%2C%20and%0Ageneral-purpose%20embodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20840v1&entry.124074799=Read"},
{"title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models", "author": "Adam Coscia and Shunan Guo and Eunyee Koh and Alex Endert", "abstract": "  As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.\n", "link": "http://arxiv.org/abs/2508.21061v1", "date": "2025-08-28", "relevancy": 1.8966, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OnGoal%3A%20Tracking%20and%20Visualizing%20Conversational%20Goals%20in%20Multi-Turn%0A%20%20Dialogue%20with%20Large%20Language%20Models&body=Title%3A%20OnGoal%3A%20Tracking%20and%20Visualizing%20Conversational%20Goals%20in%20Multi-Turn%0A%20%20Dialogue%20with%20Large%20Language%20Models%0AAuthor%3A%20Adam%20Coscia%20and%20Shunan%20Guo%20and%20Eunyee%20Koh%20and%20Alex%20Endert%0AAbstract%3A%20%20%20As%20multi-turn%20dialogues%20with%20large%20language%20models%20%28LLMs%29%20grow%20longer%20and%0Amore%20complex%2C%20how%20can%20users%20better%20evaluate%20and%20review%20progress%20on%20their%0Aconversational%20goals%3F%20We%20present%20OnGoal%2C%20an%20LLM%20chat%20interface%20that%20helps%20users%0Abetter%20manage%20goal%20progress.%20OnGoal%20provides%20real-time%20feedback%20on%20goal%0Aalignment%20through%20LLM-assisted%20evaluation%2C%20explanations%20for%20evaluation%20results%0Awith%20examples%2C%20and%20overviews%20of%20goal%20progression%20over%20time%2C%20enabling%20users%20to%0Anavigate%20complex%20dialogues%20more%20effectively.%20Through%20a%20study%20with%2020%0Aparticipants%20on%20a%20writing%20task%2C%20we%20evaluate%20OnGoal%20against%20a%20baseline%20chat%0Ainterface%20without%20goal%20tracking.%20Using%20OnGoal%2C%20participants%20spent%20less%20time%20and%0Aeffort%20to%20achieve%20their%20goals%20while%20exploring%20new%20prompting%20strategies%20to%0Aovercome%20miscommunication%2C%20suggesting%20tracking%20and%20visualizing%20goals%20can%0Aenhance%20engagement%20and%20resilience%20in%20LLM%20dialogues.%20Our%20findings%20inspired%0Adesign%20implications%20for%20future%20LLM%20chat%20interfaces%20that%20improve%20goal%0Acommunication%2C%20reduce%20cognitive%20load%2C%20enhance%20interactivity%2C%20and%20enable%0Afeedback%20to%20improve%20LLM%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnGoal%253A%2520Tracking%2520and%2520Visualizing%2520Conversational%2520Goals%2520in%2520Multi-Turn%250A%2520%2520Dialogue%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DAdam%2520Coscia%2520and%2520Shunan%2520Guo%2520and%2520Eunyee%2520Koh%2520and%2520Alex%2520Endert%26entry.1292438233%3D%2520%2520As%2520multi-turn%2520dialogues%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520grow%2520longer%2520and%250Amore%2520complex%252C%2520how%2520can%2520users%2520better%2520evaluate%2520and%2520review%2520progress%2520on%2520their%250Aconversational%2520goals%253F%2520We%2520present%2520OnGoal%252C%2520an%2520LLM%2520chat%2520interface%2520that%2520helps%2520users%250Abetter%2520manage%2520goal%2520progress.%2520OnGoal%2520provides%2520real-time%2520feedback%2520on%2520goal%250Aalignment%2520through%2520LLM-assisted%2520evaluation%252C%2520explanations%2520for%2520evaluation%2520results%250Awith%2520examples%252C%2520and%2520overviews%2520of%2520goal%2520progression%2520over%2520time%252C%2520enabling%2520users%2520to%250Anavigate%2520complex%2520dialogues%2520more%2520effectively.%2520Through%2520a%2520study%2520with%252020%250Aparticipants%2520on%2520a%2520writing%2520task%252C%2520we%2520evaluate%2520OnGoal%2520against%2520a%2520baseline%2520chat%250Ainterface%2520without%2520goal%2520tracking.%2520Using%2520OnGoal%252C%2520participants%2520spent%2520less%2520time%2520and%250Aeffort%2520to%2520achieve%2520their%2520goals%2520while%2520exploring%2520new%2520prompting%2520strategies%2520to%250Aovercome%2520miscommunication%252C%2520suggesting%2520tracking%2520and%2520visualizing%2520goals%2520can%250Aenhance%2520engagement%2520and%2520resilience%2520in%2520LLM%2520dialogues.%2520Our%2520findings%2520inspired%250Adesign%2520implications%2520for%2520future%2520LLM%2520chat%2520interfaces%2520that%2520improve%2520goal%250Acommunication%252C%2520reduce%2520cognitive%2520load%252C%2520enhance%2520interactivity%252C%2520and%2520enable%250Afeedback%2520to%2520improve%2520LLM%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OnGoal%3A%20Tracking%20and%20Visualizing%20Conversational%20Goals%20in%20Multi-Turn%0A%20%20Dialogue%20with%20Large%20Language%20Models&entry.906535625=Adam%20Coscia%20and%20Shunan%20Guo%20and%20Eunyee%20Koh%20and%20Alex%20Endert&entry.1292438233=%20%20As%20multi-turn%20dialogues%20with%20large%20language%20models%20%28LLMs%29%20grow%20longer%20and%0Amore%20complex%2C%20how%20can%20users%20better%20evaluate%20and%20review%20progress%20on%20their%0Aconversational%20goals%3F%20We%20present%20OnGoal%2C%20an%20LLM%20chat%20interface%20that%20helps%20users%0Abetter%20manage%20goal%20progress.%20OnGoal%20provides%20real-time%20feedback%20on%20goal%0Aalignment%20through%20LLM-assisted%20evaluation%2C%20explanations%20for%20evaluation%20results%0Awith%20examples%2C%20and%20overviews%20of%20goal%20progression%20over%20time%2C%20enabling%20users%20to%0Anavigate%20complex%20dialogues%20more%20effectively.%20Through%20a%20study%20with%2020%0Aparticipants%20on%20a%20writing%20task%2C%20we%20evaluate%20OnGoal%20against%20a%20baseline%20chat%0Ainterface%20without%20goal%20tracking.%20Using%20OnGoal%2C%20participants%20spent%20less%20time%20and%0Aeffort%20to%20achieve%20their%20goals%20while%20exploring%20new%20prompting%20strategies%20to%0Aovercome%20miscommunication%2C%20suggesting%20tracking%20and%20visualizing%20goals%20can%0Aenhance%20engagement%20and%20resilience%20in%20LLM%20dialogues.%20Our%20findings%20inspired%0Adesign%20implications%20for%20future%20LLM%20chat%20interfaces%20that%20improve%20goal%0Acommunication%2C%20reduce%20cognitive%20load%2C%20enhance%20interactivity%2C%20and%20enable%0Afeedback%20to%20improve%20LLM%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21061v1&entry.124074799=Read"},
{"title": "Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments", "author": "Liding Zhang and Zeqi Li and Kuanqi Cai and Qian Huang and Zhenshan Bing and Alois Knoll", "abstract": "  Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS\n", "link": "http://arxiv.org/abs/2508.20899v1", "date": "2025-08-28", "relevancy": 1.8955, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6226}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Enhanced%20Mobile%20Manipulation%20for%20Efficient%20Object%20Search%20in%0A%20%20Indoor%20Environments&body=Title%3A%20Language-Enhanced%20Mobile%20Manipulation%20for%20Efficient%20Object%20Search%20in%0A%20%20Indoor%20Environments%0AAuthor%3A%20Liding%20Zhang%20and%20Zeqi%20Li%20and%20Kuanqi%20Cai%20and%20Qian%20Huang%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Enabling%20robots%20to%20efficiently%20search%20for%20and%20identify%20objects%20in%20complex%2C%0Aunstructured%20environments%20is%20critical%20for%20diverse%20applications%20ranging%20from%0Ahousehold%20assistance%20to%20industrial%20automation.%20However%2C%20traditional%20scene%0Arepresentations%20typically%20capture%20only%20static%20semantics%20and%20lack%20interpretable%0Acontextual%20reasoning%2C%20limiting%20their%20ability%20to%20guide%20object%20search%20in%0Acompletely%20unfamiliar%20settings.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Alanguage-enhanced%20hierarchical%20navigation%20framework%20that%20tightly%20integrates%0Asemantic%20perception%20and%20spatial%20reasoning.%20Our%20method%2C%20Goal-Oriented%0ADynamically%20Heuristic-Guided%20Hierarchical%20Search%20%28GODHS%29%2C%20leverages%20large%0Alanguage%20models%20%28LLMs%29%20to%20infer%20scene%20semantics%20and%20guide%20the%20search%20process%0Athrough%20a%20multi-level%20decision%20hierarchy.%20Reliability%20in%20reasoning%20is%20achieved%0Athrough%20the%20use%20of%20structured%20prompts%20and%20logical%20constraints%20applied%20at%20each%0Astage%20of%20the%20hierarchy.%20For%20the%20specific%20challenges%20of%20mobile%20manipulation%2C%20we%0Aintroduce%20a%20heuristic-based%20motion%20planner%20that%20combines%20polar%20angle%20sorting%0Awith%20distance%20prioritization%20to%20efficiently%20generate%20exploration%20paths.%0AComprehensive%20evaluations%20in%20Isaac%20Sim%20demonstrate%20the%20feasibility%20of%20our%0Aframework%2C%20showing%20that%20GODHS%20can%20locate%20target%20objects%20with%20higher%20search%0Aefficiency%20compared%20to%20conventional%2C%20non-semantic%20search%20strategies.%20Website%0Aand%20Video%20are%20available%20at%3A%20https%3A//drapandiger.github.io/GODHS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Enhanced%2520Mobile%2520Manipulation%2520for%2520Efficient%2520Object%2520Search%2520in%250A%2520%2520Indoor%2520Environments%26entry.906535625%3DLiding%2520Zhang%2520and%2520Zeqi%2520Li%2520and%2520Kuanqi%2520Cai%2520and%2520Qian%2520Huang%2520and%2520Zhenshan%2520Bing%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Enabling%2520robots%2520to%2520efficiently%2520search%2520for%2520and%2520identify%2520objects%2520in%2520complex%252C%250Aunstructured%2520environments%2520is%2520critical%2520for%2520diverse%2520applications%2520ranging%2520from%250Ahousehold%2520assistance%2520to%2520industrial%2520automation.%2520However%252C%2520traditional%2520scene%250Arepresentations%2520typically%2520capture%2520only%2520static%2520semantics%2520and%2520lack%2520interpretable%250Acontextual%2520reasoning%252C%2520limiting%2520their%2520ability%2520to%2520guide%2520object%2520search%2520in%250Acompletely%2520unfamiliar%2520settings.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Alanguage-enhanced%2520hierarchical%2520navigation%2520framework%2520that%2520tightly%2520integrates%250Asemantic%2520perception%2520and%2520spatial%2520reasoning.%2520Our%2520method%252C%2520Goal-Oriented%250ADynamically%2520Heuristic-Guided%2520Hierarchical%2520Search%2520%2528GODHS%2529%252C%2520leverages%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520infer%2520scene%2520semantics%2520and%2520guide%2520the%2520search%2520process%250Athrough%2520a%2520multi-level%2520decision%2520hierarchy.%2520Reliability%2520in%2520reasoning%2520is%2520achieved%250Athrough%2520the%2520use%2520of%2520structured%2520prompts%2520and%2520logical%2520constraints%2520applied%2520at%2520each%250Astage%2520of%2520the%2520hierarchy.%2520For%2520the%2520specific%2520challenges%2520of%2520mobile%2520manipulation%252C%2520we%250Aintroduce%2520a%2520heuristic-based%2520motion%2520planner%2520that%2520combines%2520polar%2520angle%2520sorting%250Awith%2520distance%2520prioritization%2520to%2520efficiently%2520generate%2520exploration%2520paths.%250AComprehensive%2520evaluations%2520in%2520Isaac%2520Sim%2520demonstrate%2520the%2520feasibility%2520of%2520our%250Aframework%252C%2520showing%2520that%2520GODHS%2520can%2520locate%2520target%2520objects%2520with%2520higher%2520search%250Aefficiency%2520compared%2520to%2520conventional%252C%2520non-semantic%2520search%2520strategies.%2520Website%250Aand%2520Video%2520are%2520available%2520at%253A%2520https%253A//drapandiger.github.io/GODHS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Enhanced%20Mobile%20Manipulation%20for%20Efficient%20Object%20Search%20in%0A%20%20Indoor%20Environments&entry.906535625=Liding%20Zhang%20and%20Zeqi%20Li%20and%20Kuanqi%20Cai%20and%20Qian%20Huang%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll&entry.1292438233=%20%20Enabling%20robots%20to%20efficiently%20search%20for%20and%20identify%20objects%20in%20complex%2C%0Aunstructured%20environments%20is%20critical%20for%20diverse%20applications%20ranging%20from%0Ahousehold%20assistance%20to%20industrial%20automation.%20However%2C%20traditional%20scene%0Arepresentations%20typically%20capture%20only%20static%20semantics%20and%20lack%20interpretable%0Acontextual%20reasoning%2C%20limiting%20their%20ability%20to%20guide%20object%20search%20in%0Acompletely%20unfamiliar%20settings.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Alanguage-enhanced%20hierarchical%20navigation%20framework%20that%20tightly%20integrates%0Asemantic%20perception%20and%20spatial%20reasoning.%20Our%20method%2C%20Goal-Oriented%0ADynamically%20Heuristic-Guided%20Hierarchical%20Search%20%28GODHS%29%2C%20leverages%20large%0Alanguage%20models%20%28LLMs%29%20to%20infer%20scene%20semantics%20and%20guide%20the%20search%20process%0Athrough%20a%20multi-level%20decision%20hierarchy.%20Reliability%20in%20reasoning%20is%20achieved%0Athrough%20the%20use%20of%20structured%20prompts%20and%20logical%20constraints%20applied%20at%20each%0Astage%20of%20the%20hierarchy.%20For%20the%20specific%20challenges%20of%20mobile%20manipulation%2C%20we%0Aintroduce%20a%20heuristic-based%20motion%20planner%20that%20combines%20polar%20angle%20sorting%0Awith%20distance%20prioritization%20to%20efficiently%20generate%20exploration%20paths.%0AComprehensive%20evaluations%20in%20Isaac%20Sim%20demonstrate%20the%20feasibility%20of%20our%0Aframework%2C%20showing%20that%20GODHS%20can%20locate%20target%20objects%20with%20higher%20search%0Aefficiency%20compared%20to%20conventional%2C%20non-semantic%20search%20strategies.%20Website%0Aand%20Video%20are%20available%20at%3A%20https%3A//drapandiger.github.io/GODHS%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20899v1&entry.124074799=Read"},
{"title": "LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for\n  Atomistic Modeling", "author": "Ali Ramlaoui and Martin Siron and Inel Djafar and Joseph Musielewicz and Amandine Rossello and Victor Schmidt and Alexandre Duval", "abstract": "  The development of accurate machine learning interatomic potentials (MLIPs)\nis limited by the fragmented availability and inconsistent formatting of\nquantum mechanical trajectory datasets derived from Density Functional Theory\n(DFT). These datasets are expensive to generate yet difficult to combine due to\nvariations in format, metadata, and accessibility. To address this, we\nintroduce LeMat-Traj, a curated dataset comprising over 120 million atomic\nconfigurations aggregated from large-scale repositories, including the\nMaterials Project, Alexandria, and OQMD. LeMat-Traj standardizes data\nrepresentation, harmonizes results and filters for high-quality configurations\nacross widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It\nsignificantly lowers the barrier for training transferrable and accurate MLIPs.\nLeMat-Traj spans both relaxed low-energy states and high-energy, high-force\nstructures, complementing molecular dynamics and active learning datasets. By\nfine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a\nsignificant reduction in force prediction errors on relaxation tasks. We also\npresent LeMaterial-Fetcher, a modular and extensible open-source library\ndeveloped for this work, designed to provide a reproducible framework for the\ncommunity to easily incorporate new data sources and ensure the continued\nevolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher\nare publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj\nand https://github.com/LeMaterial/lematerial-fetcher.\n", "link": "http://arxiv.org/abs/2508.20875v1", "date": "2025-08-28", "relevancy": 1.8931, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling&body=Title%3A%20LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling%0AAuthor%3A%20Ali%20Ramlaoui%20and%20Martin%20Siron%20and%20Inel%20Djafar%20and%20Joseph%20Musielewicz%20and%20Amandine%20Rossello%20and%20Victor%20Schmidt%20and%20Alexandre%20Duval%0AAbstract%3A%20%20%20The%20development%20of%20accurate%20machine%20learning%20interatomic%20potentials%20%28MLIPs%29%0Ais%20limited%20by%20the%20fragmented%20availability%20and%20inconsistent%20formatting%20of%0Aquantum%20mechanical%20trajectory%20datasets%20derived%20from%20Density%20Functional%20Theory%0A%28DFT%29.%20These%20datasets%20are%20expensive%20to%20generate%20yet%20difficult%20to%20combine%20due%20to%0Avariations%20in%20format%2C%20metadata%2C%20and%20accessibility.%20To%20address%20this%2C%20we%0Aintroduce%20LeMat-Traj%2C%20a%20curated%20dataset%20comprising%20over%20120%20million%20atomic%0Aconfigurations%20aggregated%20from%20large-scale%20repositories%2C%20including%20the%0AMaterials%20Project%2C%20Alexandria%2C%20and%20OQMD.%20LeMat-Traj%20standardizes%20data%0Arepresentation%2C%20harmonizes%20results%20and%20filters%20for%20high-quality%20configurations%0Aacross%20widely%20used%20DFT%20functionals%20%28PBE%2C%20PBESol%2C%20SCAN%2C%20r2SCAN%29.%20It%0Asignificantly%20lowers%20the%20barrier%20for%20training%20transferrable%20and%20accurate%20MLIPs.%0ALeMat-Traj%20spans%20both%20relaxed%20low-energy%20states%20and%20high-energy%2C%20high-force%0Astructures%2C%20complementing%20molecular%20dynamics%20and%20active%20learning%20datasets.%20By%0Afine-tuning%20models%20pre-trained%20on%20high-force%20data%20with%20LeMat-Traj%2C%20we%20achieve%20a%0Asignificant%20reduction%20in%20force%20prediction%20errors%20on%20relaxation%20tasks.%20We%20also%0Apresent%20LeMaterial-Fetcher%2C%20a%20modular%20and%20extensible%20open-source%20library%0Adeveloped%20for%20this%20work%2C%20designed%20to%20provide%20a%20reproducible%20framework%20for%20the%0Acommunity%20to%20easily%20incorporate%20new%20data%20sources%20and%20ensure%20the%20continued%0Aevolution%20of%20large-scale%20materials%20datasets.%20LeMat-Traj%20and%20LeMaterial-Fetcher%0Aare%20publicly%20available%20at%20https%3A//huggingface.co/datasets/LeMaterial/LeMat-Traj%0Aand%20https%3A//github.com/LeMaterial/lematerial-fetcher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeMat-Traj%253A%2520A%2520Scalable%2520and%2520Unified%2520Dataset%2520of%2520Materials%2520Trajectories%2520for%250A%2520%2520Atomistic%2520Modeling%26entry.906535625%3DAli%2520Ramlaoui%2520and%2520Martin%2520Siron%2520and%2520Inel%2520Djafar%2520and%2520Joseph%2520Musielewicz%2520and%2520Amandine%2520Rossello%2520and%2520Victor%2520Schmidt%2520and%2520Alexandre%2520Duval%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520accurate%2520machine%2520learning%2520interatomic%2520potentials%2520%2528MLIPs%2529%250Ais%2520limited%2520by%2520the%2520fragmented%2520availability%2520and%2520inconsistent%2520formatting%2520of%250Aquantum%2520mechanical%2520trajectory%2520datasets%2520derived%2520from%2520Density%2520Functional%2520Theory%250A%2528DFT%2529.%2520These%2520datasets%2520are%2520expensive%2520to%2520generate%2520yet%2520difficult%2520to%2520combine%2520due%2520to%250Avariations%2520in%2520format%252C%2520metadata%252C%2520and%2520accessibility.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520LeMat-Traj%252C%2520a%2520curated%2520dataset%2520comprising%2520over%2520120%2520million%2520atomic%250Aconfigurations%2520aggregated%2520from%2520large-scale%2520repositories%252C%2520including%2520the%250AMaterials%2520Project%252C%2520Alexandria%252C%2520and%2520OQMD.%2520LeMat-Traj%2520standardizes%2520data%250Arepresentation%252C%2520harmonizes%2520results%2520and%2520filters%2520for%2520high-quality%2520configurations%250Aacross%2520widely%2520used%2520DFT%2520functionals%2520%2528PBE%252C%2520PBESol%252C%2520SCAN%252C%2520r2SCAN%2529.%2520It%250Asignificantly%2520lowers%2520the%2520barrier%2520for%2520training%2520transferrable%2520and%2520accurate%2520MLIPs.%250ALeMat-Traj%2520spans%2520both%2520relaxed%2520low-energy%2520states%2520and%2520high-energy%252C%2520high-force%250Astructures%252C%2520complementing%2520molecular%2520dynamics%2520and%2520active%2520learning%2520datasets.%2520By%250Afine-tuning%2520models%2520pre-trained%2520on%2520high-force%2520data%2520with%2520LeMat-Traj%252C%2520we%2520achieve%2520a%250Asignificant%2520reduction%2520in%2520force%2520prediction%2520errors%2520on%2520relaxation%2520tasks.%2520We%2520also%250Apresent%2520LeMaterial-Fetcher%252C%2520a%2520modular%2520and%2520extensible%2520open-source%2520library%250Adeveloped%2520for%2520this%2520work%252C%2520designed%2520to%2520provide%2520a%2520reproducible%2520framework%2520for%2520the%250Acommunity%2520to%2520easily%2520incorporate%2520new%2520data%2520sources%2520and%2520ensure%2520the%2520continued%250Aevolution%2520of%2520large-scale%2520materials%2520datasets.%2520LeMat-Traj%2520and%2520LeMaterial-Fetcher%250Aare%2520publicly%2520available%2520at%2520https%253A//huggingface.co/datasets/LeMaterial/LeMat-Traj%250Aand%2520https%253A//github.com/LeMaterial/lematerial-fetcher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling&entry.906535625=Ali%20Ramlaoui%20and%20Martin%20Siron%20and%20Inel%20Djafar%20and%20Joseph%20Musielewicz%20and%20Amandine%20Rossello%20and%20Victor%20Schmidt%20and%20Alexandre%20Duval&entry.1292438233=%20%20The%20development%20of%20accurate%20machine%20learning%20interatomic%20potentials%20%28MLIPs%29%0Ais%20limited%20by%20the%20fragmented%20availability%20and%20inconsistent%20formatting%20of%0Aquantum%20mechanical%20trajectory%20datasets%20derived%20from%20Density%20Functional%20Theory%0A%28DFT%29.%20These%20datasets%20are%20expensive%20to%20generate%20yet%20difficult%20to%20combine%20due%20to%0Avariations%20in%20format%2C%20metadata%2C%20and%20accessibility.%20To%20address%20this%2C%20we%0Aintroduce%20LeMat-Traj%2C%20a%20curated%20dataset%20comprising%20over%20120%20million%20atomic%0Aconfigurations%20aggregated%20from%20large-scale%20repositories%2C%20including%20the%0AMaterials%20Project%2C%20Alexandria%2C%20and%20OQMD.%20LeMat-Traj%20standardizes%20data%0Arepresentation%2C%20harmonizes%20results%20and%20filters%20for%20high-quality%20configurations%0Aacross%20widely%20used%20DFT%20functionals%20%28PBE%2C%20PBESol%2C%20SCAN%2C%20r2SCAN%29.%20It%0Asignificantly%20lowers%20the%20barrier%20for%20training%20transferrable%20and%20accurate%20MLIPs.%0ALeMat-Traj%20spans%20both%20relaxed%20low-energy%20states%20and%20high-energy%2C%20high-force%0Astructures%2C%20complementing%20molecular%20dynamics%20and%20active%20learning%20datasets.%20By%0Afine-tuning%20models%20pre-trained%20on%20high-force%20data%20with%20LeMat-Traj%2C%20we%20achieve%20a%0Asignificant%20reduction%20in%20force%20prediction%20errors%20on%20relaxation%20tasks.%20We%20also%0Apresent%20LeMaterial-Fetcher%2C%20a%20modular%20and%20extensible%20open-source%20library%0Adeveloped%20for%20this%20work%2C%20designed%20to%20provide%20a%20reproducible%20framework%20for%20the%0Acommunity%20to%20easily%20incorporate%20new%20data%20sources%20and%20ensure%20the%20continued%0Aevolution%20of%20large-scale%20materials%20datasets.%20LeMat-Traj%20and%20LeMaterial-Fetcher%0Aare%20publicly%20available%20at%20https%3A//huggingface.co/datasets/LeMaterial/LeMat-Traj%0Aand%20https%3A//github.com/LeMaterial/lematerial-fetcher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20875v1&entry.124074799=Read"},
{"title": "Encoding Tactile Stimuli for Organoid Intelligence in Braille\n  Recognition", "author": "Tianyi Liu and Hemma Philamore and Benjamin Ward-Cherrier", "abstract": "  This study proposes a generalizable encoding strategy that maps tactile\nsensor data to electrical stimulation patterns, enabling neural organoids to\nperform an open-loop artificial tactile Braille classification task. Human\nforebrain organoids cultured on a low-density microelectrode array (MEA) are\nsystematically stimulated to characterize the relationship between electrical\nstimulation parameters (number of pulse, phase amplitude, phase duration, and\ntrigger delay) and organoid responses, measured as spike activity and spatial\ndisplacement of the center of activity. Implemented on event-based tactile\ninputs recorded from the Evetac sensor, our system achieved an average Braille\nletter classification accuracy of 61 percent with a single organoid, which\nincreased significantly to 83 percent when responses from a three-organoid\nensemble were combined. Additionally, the multi-organoid configuration\ndemonstrated enhanced robustness against various types of artificially\nintroduced noise. This research demonstrates the potential of organoids as\nlow-power, adaptive bio-hybrid computational elements and provides a\nfoundational encoding framework for future scalable bio-hybrid computing\narchitectures.\n", "link": "http://arxiv.org/abs/2508.20850v1", "date": "2025-08-28", "relevancy": 1.892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5097}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encoding%20Tactile%20Stimuli%20for%20Organoid%20Intelligence%20in%20Braille%0A%20%20Recognition&body=Title%3A%20Encoding%20Tactile%20Stimuli%20for%20Organoid%20Intelligence%20in%20Braille%0A%20%20Recognition%0AAuthor%3A%20Tianyi%20Liu%20and%20Hemma%20Philamore%20and%20Benjamin%20Ward-Cherrier%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20generalizable%20encoding%20strategy%20that%20maps%20tactile%0Asensor%20data%20to%20electrical%20stimulation%20patterns%2C%20enabling%20neural%20organoids%20to%0Aperform%20an%20open-loop%20artificial%20tactile%20Braille%20classification%20task.%20Human%0Aforebrain%20organoids%20cultured%20on%20a%20low-density%20microelectrode%20array%20%28MEA%29%20are%0Asystematically%20stimulated%20to%20characterize%20the%20relationship%20between%20electrical%0Astimulation%20parameters%20%28number%20of%20pulse%2C%20phase%20amplitude%2C%20phase%20duration%2C%20and%0Atrigger%20delay%29%20and%20organoid%20responses%2C%20measured%20as%20spike%20activity%20and%20spatial%0Adisplacement%20of%20the%20center%20of%20activity.%20Implemented%20on%20event-based%20tactile%0Ainputs%20recorded%20from%20the%20Evetac%20sensor%2C%20our%20system%20achieved%20an%20average%20Braille%0Aletter%20classification%20accuracy%20of%2061%20percent%20with%20a%20single%20organoid%2C%20which%0Aincreased%20significantly%20to%2083%20percent%20when%20responses%20from%20a%20three-organoid%0Aensemble%20were%20combined.%20Additionally%2C%20the%20multi-organoid%20configuration%0Ademonstrated%20enhanced%20robustness%20against%20various%20types%20of%20artificially%0Aintroduced%20noise.%20This%20research%20demonstrates%20the%20potential%20of%20organoids%20as%0Alow-power%2C%20adaptive%20bio-hybrid%20computational%20elements%20and%20provides%20a%0Afoundational%20encoding%20framework%20for%20future%20scalable%20bio-hybrid%20computing%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncoding%2520Tactile%2520Stimuli%2520for%2520Organoid%2520Intelligence%2520in%2520Braille%250A%2520%2520Recognition%26entry.906535625%3DTianyi%2520Liu%2520and%2520Hemma%2520Philamore%2520and%2520Benjamin%2520Ward-Cherrier%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520a%2520generalizable%2520encoding%2520strategy%2520that%2520maps%2520tactile%250Asensor%2520data%2520to%2520electrical%2520stimulation%2520patterns%252C%2520enabling%2520neural%2520organoids%2520to%250Aperform%2520an%2520open-loop%2520artificial%2520tactile%2520Braille%2520classification%2520task.%2520Human%250Aforebrain%2520organoids%2520cultured%2520on%2520a%2520low-density%2520microelectrode%2520array%2520%2528MEA%2529%2520are%250Asystematically%2520stimulated%2520to%2520characterize%2520the%2520relationship%2520between%2520electrical%250Astimulation%2520parameters%2520%2528number%2520of%2520pulse%252C%2520phase%2520amplitude%252C%2520phase%2520duration%252C%2520and%250Atrigger%2520delay%2529%2520and%2520organoid%2520responses%252C%2520measured%2520as%2520spike%2520activity%2520and%2520spatial%250Adisplacement%2520of%2520the%2520center%2520of%2520activity.%2520Implemented%2520on%2520event-based%2520tactile%250Ainputs%2520recorded%2520from%2520the%2520Evetac%2520sensor%252C%2520our%2520system%2520achieved%2520an%2520average%2520Braille%250Aletter%2520classification%2520accuracy%2520of%252061%2520percent%2520with%2520a%2520single%2520organoid%252C%2520which%250Aincreased%2520significantly%2520to%252083%2520percent%2520when%2520responses%2520from%2520a%2520three-organoid%250Aensemble%2520were%2520combined.%2520Additionally%252C%2520the%2520multi-organoid%2520configuration%250Ademonstrated%2520enhanced%2520robustness%2520against%2520various%2520types%2520of%2520artificially%250Aintroduced%2520noise.%2520This%2520research%2520demonstrates%2520the%2520potential%2520of%2520organoids%2520as%250Alow-power%252C%2520adaptive%2520bio-hybrid%2520computational%2520elements%2520and%2520provides%2520a%250Afoundational%2520encoding%2520framework%2520for%2520future%2520scalable%2520bio-hybrid%2520computing%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encoding%20Tactile%20Stimuli%20for%20Organoid%20Intelligence%20in%20Braille%0A%20%20Recognition&entry.906535625=Tianyi%20Liu%20and%20Hemma%20Philamore%20and%20Benjamin%20Ward-Cherrier&entry.1292438233=%20%20This%20study%20proposes%20a%20generalizable%20encoding%20strategy%20that%20maps%20tactile%0Asensor%20data%20to%20electrical%20stimulation%20patterns%2C%20enabling%20neural%20organoids%20to%0Aperform%20an%20open-loop%20artificial%20tactile%20Braille%20classification%20task.%20Human%0Aforebrain%20organoids%20cultured%20on%20a%20low-density%20microelectrode%20array%20%28MEA%29%20are%0Asystematically%20stimulated%20to%20characterize%20the%20relationship%20between%20electrical%0Astimulation%20parameters%20%28number%20of%20pulse%2C%20phase%20amplitude%2C%20phase%20duration%2C%20and%0Atrigger%20delay%29%20and%20organoid%20responses%2C%20measured%20as%20spike%20activity%20and%20spatial%0Adisplacement%20of%20the%20center%20of%20activity.%20Implemented%20on%20event-based%20tactile%0Ainputs%20recorded%20from%20the%20Evetac%20sensor%2C%20our%20system%20achieved%20an%20average%20Braille%0Aletter%20classification%20accuracy%20of%2061%20percent%20with%20a%20single%20organoid%2C%20which%0Aincreased%20significantly%20to%2083%20percent%20when%20responses%20from%20a%20three-organoid%0Aensemble%20were%20combined.%20Additionally%2C%20the%20multi-organoid%20configuration%0Ademonstrated%20enhanced%20robustness%20against%20various%20types%20of%20artificially%0Aintroduced%20noise.%20This%20research%20demonstrates%20the%20potential%20of%20organoids%20as%0Alow-power%2C%20adaptive%20bio-hybrid%20computational%20elements%20and%20provides%20a%0Afoundational%20encoding%20framework%20for%20future%20scalable%20bio-hybrid%20computing%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20850v1&entry.124074799=Read"},
{"title": "Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble\n  Learning and Rule Based Refinement", "author": "Sara Krauss and Ellena Spie\u00df and Daniel Hieber and Frank Kramer and Johannes Schobel and Dominik M\u00fcller", "abstract": "  Mitotic figures (MFs) are relevant biomarkers in tumor grading.\nDifferentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,\nas manual annotation is time-consuming and subjective. In this work an ensemble\nof ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based\nrefinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble\nachieved a balanced accuracy of 84.02%. While the RBR increased specificity, it\nreduced sensitivity and overall performance. The results show that deep\nensembles perform well for AMF classification. RBR can increase specific\nmetrics but requires further research.\n", "link": "http://arxiv.org/abs/2508.20919v1", "date": "2025-08-28", "relevancy": 1.8855, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4654}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classifying%20Mitotic%20Figures%20in%20the%20MIDOG25%20Challenge%20with%20Deep%20Ensemble%0A%20%20Learning%20and%20Rule%20Based%20Refinement&body=Title%3A%20Classifying%20Mitotic%20Figures%20in%20the%20MIDOG25%20Challenge%20with%20Deep%20Ensemble%0A%20%20Learning%20and%20Rule%20Based%20Refinement%0AAuthor%3A%20Sara%20Krauss%20and%20Ellena%20Spie%C3%9F%20and%20Daniel%20Hieber%20and%20Frank%20Kramer%20and%20Johannes%20Schobel%20and%20Dominik%20M%C3%BCller%0AAbstract%3A%20%20%20Mitotic%20figures%20%28MFs%29%20are%20relevant%20biomarkers%20in%20tumor%20grading.%0ADifferentiating%20atypical%20MFs%20%28AMFs%29%20from%20normal%20MFs%20%28NMFs%29%20remains%20difficult%2C%0Aas%20manual%20annotation%20is%20time-consuming%20and%20subjective.%20In%20this%20work%20an%20ensemble%0Aof%20ConvNeXtBase%20models%20was%20trained%20with%20AUCMEDI%20and%20extend%20with%20a%20rule-based%0Arefinement%20%28RBR%29%20module.%20On%20the%20MIDOG25%20preliminary%20test%20set%2C%20the%20ensemble%0Aachieved%20a%20balanced%20accuracy%20of%2084.02%25.%20While%20the%20RBR%20increased%20specificity%2C%20it%0Areduced%20sensitivity%20and%20overall%20performance.%20The%20results%20show%20that%20deep%0Aensembles%20perform%20well%20for%20AMF%20classification.%20RBR%20can%20increase%20specific%0Ametrics%20but%20requires%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassifying%2520Mitotic%2520Figures%2520in%2520the%2520MIDOG25%2520Challenge%2520with%2520Deep%2520Ensemble%250A%2520%2520Learning%2520and%2520Rule%2520Based%2520Refinement%26entry.906535625%3DSara%2520Krauss%2520and%2520Ellena%2520Spie%25C3%259F%2520and%2520Daniel%2520Hieber%2520and%2520Frank%2520Kramer%2520and%2520Johannes%2520Schobel%2520and%2520Dominik%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520Mitotic%2520figures%2520%2528MFs%2529%2520are%2520relevant%2520biomarkers%2520in%2520tumor%2520grading.%250ADifferentiating%2520atypical%2520MFs%2520%2528AMFs%2529%2520from%2520normal%2520MFs%2520%2528NMFs%2529%2520remains%2520difficult%252C%250Aas%2520manual%2520annotation%2520is%2520time-consuming%2520and%2520subjective.%2520In%2520this%2520work%2520an%2520ensemble%250Aof%2520ConvNeXtBase%2520models%2520was%2520trained%2520with%2520AUCMEDI%2520and%2520extend%2520with%2520a%2520rule-based%250Arefinement%2520%2528RBR%2529%2520module.%2520On%2520the%2520MIDOG25%2520preliminary%2520test%2520set%252C%2520the%2520ensemble%250Aachieved%2520a%2520balanced%2520accuracy%2520of%252084.02%2525.%2520While%2520the%2520RBR%2520increased%2520specificity%252C%2520it%250Areduced%2520sensitivity%2520and%2520overall%2520performance.%2520The%2520results%2520show%2520that%2520deep%250Aensembles%2520perform%2520well%2520for%2520AMF%2520classification.%2520RBR%2520can%2520increase%2520specific%250Ametrics%2520but%2520requires%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifying%20Mitotic%20Figures%20in%20the%20MIDOG25%20Challenge%20with%20Deep%20Ensemble%0A%20%20Learning%20and%20Rule%20Based%20Refinement&entry.906535625=Sara%20Krauss%20and%20Ellena%20Spie%C3%9F%20and%20Daniel%20Hieber%20and%20Frank%20Kramer%20and%20Johannes%20Schobel%20and%20Dominik%20M%C3%BCller&entry.1292438233=%20%20Mitotic%20figures%20%28MFs%29%20are%20relevant%20biomarkers%20in%20tumor%20grading.%0ADifferentiating%20atypical%20MFs%20%28AMFs%29%20from%20normal%20MFs%20%28NMFs%29%20remains%20difficult%2C%0Aas%20manual%20annotation%20is%20time-consuming%20and%20subjective.%20In%20this%20work%20an%20ensemble%0Aof%20ConvNeXtBase%20models%20was%20trained%20with%20AUCMEDI%20and%20extend%20with%20a%20rule-based%0Arefinement%20%28RBR%29%20module.%20On%20the%20MIDOG25%20preliminary%20test%20set%2C%20the%20ensemble%0Aachieved%20a%20balanced%20accuracy%20of%2084.02%25.%20While%20the%20RBR%20increased%20specificity%2C%20it%0Areduced%20sensitivity%20and%20overall%20performance.%20The%20results%20show%20that%20deep%0Aensembles%20perform%20well%20for%20AMF%20classification.%20RBR%20can%20increase%20specific%0Ametrics%20but%20requires%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20919v1&entry.124074799=Read"},
{"title": "First-Place Solution to NeurIPS 2024 Invisible Watermark Removal\n  Challenge", "author": "Fahad Shamshad and Tameem Bakr and Yahia Shaaban and Noor Hussein and Karthik Nandakumar and Nils Lukas", "abstract": "  Content watermarking is an important tool for the authentication and\ncopyright protection of digital media. However, it is unclear whether existing\nwatermarks are robust against adversarial attacks. We present the winning\nsolution to the NeurIPS 2024 Erasing the Invisible challenge, which\nstress-tests watermark robustness under varying degrees of adversary knowledge.\nThe challenge consisted of two tracks: a black-box and beige-box track,\ndepending on whether the adversary knows which watermarking method was used by\nthe provider. For the beige-box track, we leverage an adaptive VAE-based\nevasion attack, with a test-time optimization and color-contrast restoration in\nCIELAB space to preserve the image's quality. For the black-box track, we first\ncluster images based on their artifacts in the spatial or frequency-domain.\nThen, we apply image-to-image diffusion models with controlled noise injection\nand semantic priors from ChatGPT-generated captions to each cluster with\noptimized parameter settings. Empirical evaluations demonstrate that our method\nsuccessfully achieves near-perfect watermark removal (95.7%) with negligible\nimpact on the residual image's quality. We hope that our attacks inspire the\ndevelopment of more robust image watermarking methods.\n", "link": "http://arxiv.org/abs/2508.21072v1", "date": "2025-08-28", "relevancy": 1.8816, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4761}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.467}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First-Place%20Solution%20to%20NeurIPS%202024%20Invisible%20Watermark%20Removal%0A%20%20Challenge&body=Title%3A%20First-Place%20Solution%20to%20NeurIPS%202024%20Invisible%20Watermark%20Removal%0A%20%20Challenge%0AAuthor%3A%20Fahad%20Shamshad%20and%20Tameem%20Bakr%20and%20Yahia%20Shaaban%20and%20Noor%20Hussein%20and%20Karthik%20Nandakumar%20and%20Nils%20Lukas%0AAbstract%3A%20%20%20Content%20watermarking%20is%20an%20important%20tool%20for%20the%20authentication%20and%0Acopyright%20protection%20of%20digital%20media.%20However%2C%20it%20is%20unclear%20whether%20existing%0Awatermarks%20are%20robust%20against%20adversarial%20attacks.%20We%20present%20the%20winning%0Asolution%20to%20the%20NeurIPS%202024%20Erasing%20the%20Invisible%20challenge%2C%20which%0Astress-tests%20watermark%20robustness%20under%20varying%20degrees%20of%20adversary%20knowledge.%0AThe%20challenge%20consisted%20of%20two%20tracks%3A%20a%20black-box%20and%20beige-box%20track%2C%0Adepending%20on%20whether%20the%20adversary%20knows%20which%20watermarking%20method%20was%20used%20by%0Athe%20provider.%20For%20the%20beige-box%20track%2C%20we%20leverage%20an%20adaptive%20VAE-based%0Aevasion%20attack%2C%20with%20a%20test-time%20optimization%20and%20color-contrast%20restoration%20in%0ACIELAB%20space%20to%20preserve%20the%20image%27s%20quality.%20For%20the%20black-box%20track%2C%20we%20first%0Acluster%20images%20based%20on%20their%20artifacts%20in%20the%20spatial%20or%20frequency-domain.%0AThen%2C%20we%20apply%20image-to-image%20diffusion%20models%20with%20controlled%20noise%20injection%0Aand%20semantic%20priors%20from%20ChatGPT-generated%20captions%20to%20each%20cluster%20with%0Aoptimized%20parameter%20settings.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%0Asuccessfully%20achieves%20near-perfect%20watermark%20removal%20%2895.7%25%29%20with%20negligible%0Aimpact%20on%20the%20residual%20image%27s%20quality.%20We%20hope%20that%20our%20attacks%20inspire%20the%0Adevelopment%20of%20more%20robust%20image%20watermarking%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst-Place%2520Solution%2520to%2520NeurIPS%25202024%2520Invisible%2520Watermark%2520Removal%250A%2520%2520Challenge%26entry.906535625%3DFahad%2520Shamshad%2520and%2520Tameem%2520Bakr%2520and%2520Yahia%2520Shaaban%2520and%2520Noor%2520Hussein%2520and%2520Karthik%2520Nandakumar%2520and%2520Nils%2520Lukas%26entry.1292438233%3D%2520%2520Content%2520watermarking%2520is%2520an%2520important%2520tool%2520for%2520the%2520authentication%2520and%250Acopyright%2520protection%2520of%2520digital%2520media.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520existing%250Awatermarks%2520are%2520robust%2520against%2520adversarial%2520attacks.%2520We%2520present%2520the%2520winning%250Asolution%2520to%2520the%2520NeurIPS%25202024%2520Erasing%2520the%2520Invisible%2520challenge%252C%2520which%250Astress-tests%2520watermark%2520robustness%2520under%2520varying%2520degrees%2520of%2520adversary%2520knowledge.%250AThe%2520challenge%2520consisted%2520of%2520two%2520tracks%253A%2520a%2520black-box%2520and%2520beige-box%2520track%252C%250Adepending%2520on%2520whether%2520the%2520adversary%2520knows%2520which%2520watermarking%2520method%2520was%2520used%2520by%250Athe%2520provider.%2520For%2520the%2520beige-box%2520track%252C%2520we%2520leverage%2520an%2520adaptive%2520VAE-based%250Aevasion%2520attack%252C%2520with%2520a%2520test-time%2520optimization%2520and%2520color-contrast%2520restoration%2520in%250ACIELAB%2520space%2520to%2520preserve%2520the%2520image%2527s%2520quality.%2520For%2520the%2520black-box%2520track%252C%2520we%2520first%250Acluster%2520images%2520based%2520on%2520their%2520artifacts%2520in%2520the%2520spatial%2520or%2520frequency-domain.%250AThen%252C%2520we%2520apply%2520image-to-image%2520diffusion%2520models%2520with%2520controlled%2520noise%2520injection%250Aand%2520semantic%2520priors%2520from%2520ChatGPT-generated%2520captions%2520to%2520each%2520cluster%2520with%250Aoptimized%2520parameter%2520settings.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%2520method%250Asuccessfully%2520achieves%2520near-perfect%2520watermark%2520removal%2520%252895.7%2525%2529%2520with%2520negligible%250Aimpact%2520on%2520the%2520residual%2520image%2527s%2520quality.%2520We%2520hope%2520that%2520our%2520attacks%2520inspire%2520the%250Adevelopment%2520of%2520more%2520robust%2520image%2520watermarking%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First-Place%20Solution%20to%20NeurIPS%202024%20Invisible%20Watermark%20Removal%0A%20%20Challenge&entry.906535625=Fahad%20Shamshad%20and%20Tameem%20Bakr%20and%20Yahia%20Shaaban%20and%20Noor%20Hussein%20and%20Karthik%20Nandakumar%20and%20Nils%20Lukas&entry.1292438233=%20%20Content%20watermarking%20is%20an%20important%20tool%20for%20the%20authentication%20and%0Acopyright%20protection%20of%20digital%20media.%20However%2C%20it%20is%20unclear%20whether%20existing%0Awatermarks%20are%20robust%20against%20adversarial%20attacks.%20We%20present%20the%20winning%0Asolution%20to%20the%20NeurIPS%202024%20Erasing%20the%20Invisible%20challenge%2C%20which%0Astress-tests%20watermark%20robustness%20under%20varying%20degrees%20of%20adversary%20knowledge.%0AThe%20challenge%20consisted%20of%20two%20tracks%3A%20a%20black-box%20and%20beige-box%20track%2C%0Adepending%20on%20whether%20the%20adversary%20knows%20which%20watermarking%20method%20was%20used%20by%0Athe%20provider.%20For%20the%20beige-box%20track%2C%20we%20leverage%20an%20adaptive%20VAE-based%0Aevasion%20attack%2C%20with%20a%20test-time%20optimization%20and%20color-contrast%20restoration%20in%0ACIELAB%20space%20to%20preserve%20the%20image%27s%20quality.%20For%20the%20black-box%20track%2C%20we%20first%0Acluster%20images%20based%20on%20their%20artifacts%20in%20the%20spatial%20or%20frequency-domain.%0AThen%2C%20we%20apply%20image-to-image%20diffusion%20models%20with%20controlled%20noise%20injection%0Aand%20semantic%20priors%20from%20ChatGPT-generated%20captions%20to%20each%20cluster%20with%0Aoptimized%20parameter%20settings.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%0Asuccessfully%20achieves%20near-perfect%20watermark%20removal%20%2895.7%25%29%20with%20negligible%0Aimpact%20on%20the%20residual%20image%27s%20quality.%20We%20hope%20that%20our%20attacks%20inspire%20the%0Adevelopment%20of%20more%20robust%20image%20watermarking%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21072v1&entry.124074799=Read"},
{"title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models", "author": "Jiaxiang Cheng and Bing Ma and Xuhua Ren and Hongyi Jin and Kai Yu and Peng Zhang and Wenyue Li and Yuan Zhou and Tianxiang Zheng and Qinglin Lu", "abstract": "  The field of video diffusion generation faces critical bottlenecks in\nsampling efficiency, especially for large-scale models and long sequences.\nExisting video acceleration methods adopt image-based techniques but suffer\nfrom fundamental limitations: they neither model the temporal coherence of\nvideo frames nor provide single-step distillation for large-scale video models.\nTo bridge this gap, we propose POSE (Phased One-Step Equilibrium), a\ndistillation framework that reduces the sampling steps of large-scale video\ndiffusion models, enabling the generation of high-quality videos in a single\nstep. POSE employs a carefully designed two-phase process to distill video\nmodels:(i) stability priming: a warm-up mechanism to stabilize adversarial\ndistillation that adapts the high-quality trajectory of the one-step generator\nfrom high to low signal-to-noise ratio regimes, optimizing the video quality of\nsingle-step mappings near the endpoints of flow trajectories. (ii) unified\nadversarial equilibrium: a flexible self-adversarial distillation mechanism\nthat promotes stable single-step adversarial training towards a Nash\nequilibrium within the Gaussian noise space, generating realistic single-step\nvideos close to real videos. For conditional video generation, we propose (iii)\nconditional adversarial consistency, a method to improve both semantic\nconsistency and frame consistency between conditional frames and generated\nframes. Comprehensive experiments demonstrate that POSE outperforms other\nacceleration methods on VBench-I2V by average 7.15% in semantic alignment,\ntemporal conference and frame quality, reducing the latency of the pre-trained\nmodel by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining\ncompetitive performance.\n", "link": "http://arxiv.org/abs/2508.21019v1", "date": "2025-08-28", "relevancy": 1.8797, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6362}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6257}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POSE%3A%20Phased%20One-Step%20Adversarial%20Equilibrium%20for%20Video%20Diffusion%20Models&body=Title%3A%20POSE%3A%20Phased%20One-Step%20Adversarial%20Equilibrium%20for%20Video%20Diffusion%20Models%0AAuthor%3A%20Jiaxiang%20Cheng%20and%20Bing%20Ma%20and%20Xuhua%20Ren%20and%20Hongyi%20Jin%20and%20Kai%20Yu%20and%20Peng%20Zhang%20and%20Wenyue%20Li%20and%20Yuan%20Zhou%20and%20Tianxiang%20Zheng%20and%20Qinglin%20Lu%0AAbstract%3A%20%20%20The%20field%20of%20video%20diffusion%20generation%20faces%20critical%20bottlenecks%20in%0Asampling%20efficiency%2C%20especially%20for%20large-scale%20models%20and%20long%20sequences.%0AExisting%20video%20acceleration%20methods%20adopt%20image-based%20techniques%20but%20suffer%0Afrom%20fundamental%20limitations%3A%20they%20neither%20model%20the%20temporal%20coherence%20of%0Avideo%20frames%20nor%20provide%20single-step%20distillation%20for%20large-scale%20video%20models.%0ATo%20bridge%20this%20gap%2C%20we%20propose%20POSE%20%28Phased%20One-Step%20Equilibrium%29%2C%20a%0Adistillation%20framework%20that%20reduces%20the%20sampling%20steps%20of%20large-scale%20video%0Adiffusion%20models%2C%20enabling%20the%20generation%20of%20high-quality%20videos%20in%20a%20single%0Astep.%20POSE%20employs%20a%20carefully%20designed%20two-phase%20process%20to%20distill%20video%0Amodels%3A%28i%29%20stability%20priming%3A%20a%20warm-up%20mechanism%20to%20stabilize%20adversarial%0Adistillation%20that%20adapts%20the%20high-quality%20trajectory%20of%20the%20one-step%20generator%0Afrom%20high%20to%20low%20signal-to-noise%20ratio%20regimes%2C%20optimizing%20the%20video%20quality%20of%0Asingle-step%20mappings%20near%20the%20endpoints%20of%20flow%20trajectories.%20%28ii%29%20unified%0Aadversarial%20equilibrium%3A%20a%20flexible%20self-adversarial%20distillation%20mechanism%0Athat%20promotes%20stable%20single-step%20adversarial%20training%20towards%20a%20Nash%0Aequilibrium%20within%20the%20Gaussian%20noise%20space%2C%20generating%20realistic%20single-step%0Avideos%20close%20to%20real%20videos.%20For%20conditional%20video%20generation%2C%20we%20propose%20%28iii%29%0Aconditional%20adversarial%20consistency%2C%20a%20method%20to%20improve%20both%20semantic%0Aconsistency%20and%20frame%20consistency%20between%20conditional%20frames%20and%20generated%0Aframes.%20Comprehensive%20experiments%20demonstrate%20that%20POSE%20outperforms%20other%0Aacceleration%20methods%20on%20VBench-I2V%20by%20average%207.15%25%20in%20semantic%20alignment%2C%0Atemporal%20conference%20and%20frame%20quality%2C%20reducing%20the%20latency%20of%20the%20pre-trained%0Amodel%20by%20100%24%5Ctimes%24%2C%20from%201000%20seconds%20to%2010%20seconds%2C%20while%20maintaining%0Acompetitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOSE%253A%2520Phased%2520One-Step%2520Adversarial%2520Equilibrium%2520for%2520Video%2520Diffusion%2520Models%26entry.906535625%3DJiaxiang%2520Cheng%2520and%2520Bing%2520Ma%2520and%2520Xuhua%2520Ren%2520and%2520Hongyi%2520Jin%2520and%2520Kai%2520Yu%2520and%2520Peng%2520Zhang%2520and%2520Wenyue%2520Li%2520and%2520Yuan%2520Zhou%2520and%2520Tianxiang%2520Zheng%2520and%2520Qinglin%2520Lu%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520video%2520diffusion%2520generation%2520faces%2520critical%2520bottlenecks%2520in%250Asampling%2520efficiency%252C%2520especially%2520for%2520large-scale%2520models%2520and%2520long%2520sequences.%250AExisting%2520video%2520acceleration%2520methods%2520adopt%2520image-based%2520techniques%2520but%2520suffer%250Afrom%2520fundamental%2520limitations%253A%2520they%2520neither%2520model%2520the%2520temporal%2520coherence%2520of%250Avideo%2520frames%2520nor%2520provide%2520single-step%2520distillation%2520for%2520large-scale%2520video%2520models.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520POSE%2520%2528Phased%2520One-Step%2520Equilibrium%2529%252C%2520a%250Adistillation%2520framework%2520that%2520reduces%2520the%2520sampling%2520steps%2520of%2520large-scale%2520video%250Adiffusion%2520models%252C%2520enabling%2520the%2520generation%2520of%2520high-quality%2520videos%2520in%2520a%2520single%250Astep.%2520POSE%2520employs%2520a%2520carefully%2520designed%2520two-phase%2520process%2520to%2520distill%2520video%250Amodels%253A%2528i%2529%2520stability%2520priming%253A%2520a%2520warm-up%2520mechanism%2520to%2520stabilize%2520adversarial%250Adistillation%2520that%2520adapts%2520the%2520high-quality%2520trajectory%2520of%2520the%2520one-step%2520generator%250Afrom%2520high%2520to%2520low%2520signal-to-noise%2520ratio%2520regimes%252C%2520optimizing%2520the%2520video%2520quality%2520of%250Asingle-step%2520mappings%2520near%2520the%2520endpoints%2520of%2520flow%2520trajectories.%2520%2528ii%2529%2520unified%250Aadversarial%2520equilibrium%253A%2520a%2520flexible%2520self-adversarial%2520distillation%2520mechanism%250Athat%2520promotes%2520stable%2520single-step%2520adversarial%2520training%2520towards%2520a%2520Nash%250Aequilibrium%2520within%2520the%2520Gaussian%2520noise%2520space%252C%2520generating%2520realistic%2520single-step%250Avideos%2520close%2520to%2520real%2520videos.%2520For%2520conditional%2520video%2520generation%252C%2520we%2520propose%2520%2528iii%2529%250Aconditional%2520adversarial%2520consistency%252C%2520a%2520method%2520to%2520improve%2520both%2520semantic%250Aconsistency%2520and%2520frame%2520consistency%2520between%2520conditional%2520frames%2520and%2520generated%250Aframes.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520POSE%2520outperforms%2520other%250Aacceleration%2520methods%2520on%2520VBench-I2V%2520by%2520average%25207.15%2525%2520in%2520semantic%2520alignment%252C%250Atemporal%2520conference%2520and%2520frame%2520quality%252C%2520reducing%2520the%2520latency%2520of%2520the%2520pre-trained%250Amodel%2520by%2520100%2524%255Ctimes%2524%252C%2520from%25201000%2520seconds%2520to%252010%2520seconds%252C%2520while%2520maintaining%250Acompetitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POSE%3A%20Phased%20One-Step%20Adversarial%20Equilibrium%20for%20Video%20Diffusion%20Models&entry.906535625=Jiaxiang%20Cheng%20and%20Bing%20Ma%20and%20Xuhua%20Ren%20and%20Hongyi%20Jin%20and%20Kai%20Yu%20and%20Peng%20Zhang%20and%20Wenyue%20Li%20and%20Yuan%20Zhou%20and%20Tianxiang%20Zheng%20and%20Qinglin%20Lu&entry.1292438233=%20%20The%20field%20of%20video%20diffusion%20generation%20faces%20critical%20bottlenecks%20in%0Asampling%20efficiency%2C%20especially%20for%20large-scale%20models%20and%20long%20sequences.%0AExisting%20video%20acceleration%20methods%20adopt%20image-based%20techniques%20but%20suffer%0Afrom%20fundamental%20limitations%3A%20they%20neither%20model%20the%20temporal%20coherence%20of%0Avideo%20frames%20nor%20provide%20single-step%20distillation%20for%20large-scale%20video%20models.%0ATo%20bridge%20this%20gap%2C%20we%20propose%20POSE%20%28Phased%20One-Step%20Equilibrium%29%2C%20a%0Adistillation%20framework%20that%20reduces%20the%20sampling%20steps%20of%20large-scale%20video%0Adiffusion%20models%2C%20enabling%20the%20generation%20of%20high-quality%20videos%20in%20a%20single%0Astep.%20POSE%20employs%20a%20carefully%20designed%20two-phase%20process%20to%20distill%20video%0Amodels%3A%28i%29%20stability%20priming%3A%20a%20warm-up%20mechanism%20to%20stabilize%20adversarial%0Adistillation%20that%20adapts%20the%20high-quality%20trajectory%20of%20the%20one-step%20generator%0Afrom%20high%20to%20low%20signal-to-noise%20ratio%20regimes%2C%20optimizing%20the%20video%20quality%20of%0Asingle-step%20mappings%20near%20the%20endpoints%20of%20flow%20trajectories.%20%28ii%29%20unified%0Aadversarial%20equilibrium%3A%20a%20flexible%20self-adversarial%20distillation%20mechanism%0Athat%20promotes%20stable%20single-step%20adversarial%20training%20towards%20a%20Nash%0Aequilibrium%20within%20the%20Gaussian%20noise%20space%2C%20generating%20realistic%20single-step%0Avideos%20close%20to%20real%20videos.%20For%20conditional%20video%20generation%2C%20we%20propose%20%28iii%29%0Aconditional%20adversarial%20consistency%2C%20a%20method%20to%20improve%20both%20semantic%0Aconsistency%20and%20frame%20consistency%20between%20conditional%20frames%20and%20generated%0Aframes.%20Comprehensive%20experiments%20demonstrate%20that%20POSE%20outperforms%20other%0Aacceleration%20methods%20on%20VBench-I2V%20by%20average%207.15%25%20in%20semantic%20alignment%2C%0Atemporal%20conference%20and%20frame%20quality%2C%20reducing%20the%20latency%20of%20the%20pre-trained%0Amodel%20by%20100%24%5Ctimes%24%2C%20from%201000%20seconds%20to%2010%20seconds%2C%20while%20maintaining%0Acompetitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21019v1&entry.124074799=Read"},
{"title": "Diagonal Symmetrization of Neural Network Solvers for the Many-Electron\n  Schr\u00f6dinger Equation", "author": "Kevin Han Huang and Ni Zhan and Elif Ertekin and Peter Orbanz and Ryan P. Adams", "abstract": "  Incorporating group symmetries into neural networks has been a cornerstone of\nsuccess in many AI-for-science applications. Diagonal groups of isometries,\nwhich describe the invariance under a simultaneous movement of multiple\nobjects, arise naturally in many-body quantum problems. Despite their\nimportance, diagonal groups have received relatively little attention, as they\nlack a natural choice of invariant maps except in special cases. We study\ndifferent ways of incorporating diagonal invariance in neural network ans\\\"atze\ntrained via variational Monte Carlo methods, and consider specifically data\naugmentation, group averaging and canonicalization. We show that, contrary to\nstandard ML setups, in-training symmetrization destabilizes training and can\nlead to worse performance. Our theoretical and numerical results indicate that\nthis unexpected behavior may arise from a unique computational-statistical\ntradeoff not found in standard ML analyses of symmetrization. Meanwhile, we\ndemonstrate that post hoc averaging is less sensitive to such tradeoffs and\nemerges as a simple, flexible and effective method for improving neural network\nsolvers.\n", "link": "http://arxiv.org/abs/2502.05318v2", "date": "2025-08-28", "relevancy": 1.8735, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4749}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagonal%20Symmetrization%20of%20Neural%20Network%20Solvers%20for%20the%20Many-Electron%0A%20%20Schr%C3%B6dinger%20Equation&body=Title%3A%20Diagonal%20Symmetrization%20of%20Neural%20Network%20Solvers%20for%20the%20Many-Electron%0A%20%20Schr%C3%B6dinger%20Equation%0AAuthor%3A%20Kevin%20Han%20Huang%20and%20Ni%20Zhan%20and%20Elif%20Ertekin%20and%20Peter%20Orbanz%20and%20Ryan%20P.%20Adams%0AAbstract%3A%20%20%20Incorporating%20group%20symmetries%20into%20neural%20networks%20has%20been%20a%20cornerstone%20of%0Asuccess%20in%20many%20AI-for-science%20applications.%20Diagonal%20groups%20of%20isometries%2C%0Awhich%20describe%20the%20invariance%20under%20a%20simultaneous%20movement%20of%20multiple%0Aobjects%2C%20arise%20naturally%20in%20many-body%20quantum%20problems.%20Despite%20their%0Aimportance%2C%20diagonal%20groups%20have%20received%20relatively%20little%20attention%2C%20as%20they%0Alack%20a%20natural%20choice%20of%20invariant%20maps%20except%20in%20special%20cases.%20We%20study%0Adifferent%20ways%20of%20incorporating%20diagonal%20invariance%20in%20neural%20network%20ans%5C%22atze%0Atrained%20via%20variational%20Monte%20Carlo%20methods%2C%20and%20consider%20specifically%20data%0Aaugmentation%2C%20group%20averaging%20and%20canonicalization.%20We%20show%20that%2C%20contrary%20to%0Astandard%20ML%20setups%2C%20in-training%20symmetrization%20destabilizes%20training%20and%20can%0Alead%20to%20worse%20performance.%20Our%20theoretical%20and%20numerical%20results%20indicate%20that%0Athis%20unexpected%20behavior%20may%20arise%20from%20a%20unique%20computational-statistical%0Atradeoff%20not%20found%20in%20standard%20ML%20analyses%20of%20symmetrization.%20Meanwhile%2C%20we%0Ademonstrate%20that%20post%20hoc%20averaging%20is%20less%20sensitive%20to%20such%20tradeoffs%20and%0Aemerges%20as%20a%20simple%2C%20flexible%20and%20effective%20method%20for%20improving%20neural%20network%0Asolvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagonal%2520Symmetrization%2520of%2520Neural%2520Network%2520Solvers%2520for%2520the%2520Many-Electron%250A%2520%2520Schr%25C3%25B6dinger%2520Equation%26entry.906535625%3DKevin%2520Han%2520Huang%2520and%2520Ni%2520Zhan%2520and%2520Elif%2520Ertekin%2520and%2520Peter%2520Orbanz%2520and%2520Ryan%2520P.%2520Adams%26entry.1292438233%3D%2520%2520Incorporating%2520group%2520symmetries%2520into%2520neural%2520networks%2520has%2520been%2520a%2520cornerstone%2520of%250Asuccess%2520in%2520many%2520AI-for-science%2520applications.%2520Diagonal%2520groups%2520of%2520isometries%252C%250Awhich%2520describe%2520the%2520invariance%2520under%2520a%2520simultaneous%2520movement%2520of%2520multiple%250Aobjects%252C%2520arise%2520naturally%2520in%2520many-body%2520quantum%2520problems.%2520Despite%2520their%250Aimportance%252C%2520diagonal%2520groups%2520have%2520received%2520relatively%2520little%2520attention%252C%2520as%2520they%250Alack%2520a%2520natural%2520choice%2520of%2520invariant%2520maps%2520except%2520in%2520special%2520cases.%2520We%2520study%250Adifferent%2520ways%2520of%2520incorporating%2520diagonal%2520invariance%2520in%2520neural%2520network%2520ans%255C%2522atze%250Atrained%2520via%2520variational%2520Monte%2520Carlo%2520methods%252C%2520and%2520consider%2520specifically%2520data%250Aaugmentation%252C%2520group%2520averaging%2520and%2520canonicalization.%2520We%2520show%2520that%252C%2520contrary%2520to%250Astandard%2520ML%2520setups%252C%2520in-training%2520symmetrization%2520destabilizes%2520training%2520and%2520can%250Alead%2520to%2520worse%2520performance.%2520Our%2520theoretical%2520and%2520numerical%2520results%2520indicate%2520that%250Athis%2520unexpected%2520behavior%2520may%2520arise%2520from%2520a%2520unique%2520computational-statistical%250Atradeoff%2520not%2520found%2520in%2520standard%2520ML%2520analyses%2520of%2520symmetrization.%2520Meanwhile%252C%2520we%250Ademonstrate%2520that%2520post%2520hoc%2520averaging%2520is%2520less%2520sensitive%2520to%2520such%2520tradeoffs%2520and%250Aemerges%2520as%2520a%2520simple%252C%2520flexible%2520and%2520effective%2520method%2520for%2520improving%2520neural%2520network%250Asolvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagonal%20Symmetrization%20of%20Neural%20Network%20Solvers%20for%20the%20Many-Electron%0A%20%20Schr%C3%B6dinger%20Equation&entry.906535625=Kevin%20Han%20Huang%20and%20Ni%20Zhan%20and%20Elif%20Ertekin%20and%20Peter%20Orbanz%20and%20Ryan%20P.%20Adams&entry.1292438233=%20%20Incorporating%20group%20symmetries%20into%20neural%20networks%20has%20been%20a%20cornerstone%20of%0Asuccess%20in%20many%20AI-for-science%20applications.%20Diagonal%20groups%20of%20isometries%2C%0Awhich%20describe%20the%20invariance%20under%20a%20simultaneous%20movement%20of%20multiple%0Aobjects%2C%20arise%20naturally%20in%20many-body%20quantum%20problems.%20Despite%20their%0Aimportance%2C%20diagonal%20groups%20have%20received%20relatively%20little%20attention%2C%20as%20they%0Alack%20a%20natural%20choice%20of%20invariant%20maps%20except%20in%20special%20cases.%20We%20study%0Adifferent%20ways%20of%20incorporating%20diagonal%20invariance%20in%20neural%20network%20ans%5C%22atze%0Atrained%20via%20variational%20Monte%20Carlo%20methods%2C%20and%20consider%20specifically%20data%0Aaugmentation%2C%20group%20averaging%20and%20canonicalization.%20We%20show%20that%2C%20contrary%20to%0Astandard%20ML%20setups%2C%20in-training%20symmetrization%20destabilizes%20training%20and%20can%0Alead%20to%20worse%20performance.%20Our%20theoretical%20and%20numerical%20results%20indicate%20that%0Athis%20unexpected%20behavior%20may%20arise%20from%20a%20unique%20computational-statistical%0Atradeoff%20not%20found%20in%20standard%20ML%20analyses%20of%20symmetrization.%20Meanwhile%2C%20we%0Ademonstrate%20that%20post%20hoc%20averaging%20is%20less%20sensitive%20to%20such%20tradeoffs%20and%0Aemerges%20as%20a%20simple%2C%20flexible%20and%20effective%20method%20for%20improving%20neural%20network%0Asolvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05318v2&entry.124074799=Read"},
{"title": "Transfer Learning for Classification under Decision Rule Drift with\n  Application to Optimal Individualized Treatment Rule Estimation", "author": "Xiaohan Wang and Yang Ning", "abstract": "  In this paper, we extend the transfer learning classification framework from\nregression function-based methods to decision rules. We propose a novel\nmethodology for modeling posterior drift through Bayes decision rules. By\nexploiting the geometric transformation of the Bayes decision boundary, our\nmethod reformulates the problem as a low-dimensional empirical risk\nminimization problem. Under mild regularity conditions, we establish the\nconsistency of our estimators and derive the risk bounds. Moreover, we\nillustrate the broad applicability of our method by adapting it to the\nestimation of optimal individualized treatment rules. Extensive simulation\nstudies and analyses of real-world data further demonstrate both superior\nperformance and robustness of our approach.\n", "link": "http://arxiv.org/abs/2508.20942v1", "date": "2025-08-28", "relevancy": 1.8698, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.467}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20for%20Classification%20under%20Decision%20Rule%20Drift%20with%0A%20%20Application%20to%20Optimal%20Individualized%20Treatment%20Rule%20Estimation&body=Title%3A%20Transfer%20Learning%20for%20Classification%20under%20Decision%20Rule%20Drift%20with%0A%20%20Application%20to%20Optimal%20Individualized%20Treatment%20Rule%20Estimation%0AAuthor%3A%20Xiaohan%20Wang%20and%20Yang%20Ning%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20extend%20the%20transfer%20learning%20classification%20framework%20from%0Aregression%20function-based%20methods%20to%20decision%20rules.%20We%20propose%20a%20novel%0Amethodology%20for%20modeling%20posterior%20drift%20through%20Bayes%20decision%20rules.%20By%0Aexploiting%20the%20geometric%20transformation%20of%20the%20Bayes%20decision%20boundary%2C%20our%0Amethod%20reformulates%20the%20problem%20as%20a%20low-dimensional%20empirical%20risk%0Aminimization%20problem.%20Under%20mild%20regularity%20conditions%2C%20we%20establish%20the%0Aconsistency%20of%20our%20estimators%20and%20derive%20the%20risk%20bounds.%20Moreover%2C%20we%0Aillustrate%20the%20broad%20applicability%20of%20our%20method%20by%20adapting%20it%20to%20the%0Aestimation%20of%20optimal%20individualized%20treatment%20rules.%20Extensive%20simulation%0Astudies%20and%20analyses%20of%20real-world%20data%20further%20demonstrate%20both%20superior%0Aperformance%20and%20robustness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520for%2520Classification%2520under%2520Decision%2520Rule%2520Drift%2520with%250A%2520%2520Application%2520to%2520Optimal%2520Individualized%2520Treatment%2520Rule%2520Estimation%26entry.906535625%3DXiaohan%2520Wang%2520and%2520Yang%2520Ning%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520transfer%2520learning%2520classification%2520framework%2520from%250Aregression%2520function-based%2520methods%2520to%2520decision%2520rules.%2520We%2520propose%2520a%2520novel%250Amethodology%2520for%2520modeling%2520posterior%2520drift%2520through%2520Bayes%2520decision%2520rules.%2520By%250Aexploiting%2520the%2520geometric%2520transformation%2520of%2520the%2520Bayes%2520decision%2520boundary%252C%2520our%250Amethod%2520reformulates%2520the%2520problem%2520as%2520a%2520low-dimensional%2520empirical%2520risk%250Aminimization%2520problem.%2520Under%2520mild%2520regularity%2520conditions%252C%2520we%2520establish%2520the%250Aconsistency%2520of%2520our%2520estimators%2520and%2520derive%2520the%2520risk%2520bounds.%2520Moreover%252C%2520we%250Aillustrate%2520the%2520broad%2520applicability%2520of%2520our%2520method%2520by%2520adapting%2520it%2520to%2520the%250Aestimation%2520of%2520optimal%2520individualized%2520treatment%2520rules.%2520Extensive%2520simulation%250Astudies%2520and%2520analyses%2520of%2520real-world%2520data%2520further%2520demonstrate%2520both%2520superior%250Aperformance%2520and%2520robustness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20for%20Classification%20under%20Decision%20Rule%20Drift%20with%0A%20%20Application%20to%20Optimal%20Individualized%20Treatment%20Rule%20Estimation&entry.906535625=Xiaohan%20Wang%20and%20Yang%20Ning&entry.1292438233=%20%20In%20this%20paper%2C%20we%20extend%20the%20transfer%20learning%20classification%20framework%20from%0Aregression%20function-based%20methods%20to%20decision%20rules.%20We%20propose%20a%20novel%0Amethodology%20for%20modeling%20posterior%20drift%20through%20Bayes%20decision%20rules.%20By%0Aexploiting%20the%20geometric%20transformation%20of%20the%20Bayes%20decision%20boundary%2C%20our%0Amethod%20reformulates%20the%20problem%20as%20a%20low-dimensional%20empirical%20risk%0Aminimization%20problem.%20Under%20mild%20regularity%20conditions%2C%20we%20establish%20the%0Aconsistency%20of%20our%20estimators%20and%20derive%20the%20risk%20bounds.%20Moreover%2C%20we%0Aillustrate%20the%20broad%20applicability%20of%20our%20method%20by%20adapting%20it%20to%20the%0Aestimation%20of%20optimal%20individualized%20treatment%20rules.%20Extensive%20simulation%0Astudies%20and%20analyses%20of%20real-world%20data%20further%20demonstrate%20both%20superior%0Aperformance%20and%20robustness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20942v1&entry.124074799=Read"},
{"title": "Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal\n  Robot Motion Planning", "author": "Liding Zhang and Qiyang Zong and Yu Zhang and Zhenshan Bing and Alois Knoll", "abstract": "  Efficient motion planning algorithms are essential in robotics. Optimizing\nessential parameters, such as batch size and nearest neighbor selection in\nsampling-based methods, can enhance performance in the planning process.\nHowever, existing approaches often lack environmental adaptability. Inspired by\nthe method of the deep fuzzy neural networks, this work introduces\nLearning-based Informed Trees (LIT*), a sampling-based deep fuzzy\nlearning-based planner that dynamically adjusts batch size and nearest neighbor\nparameters to obstacle distributions in the configuration spaces. By encoding\nboth global and local ratios via valid and invalid states, LIT* differentiates\nbetween obstacle-sparse and obstacle-dense regions, leading to lower-cost paths\nand reduced computation time. Experimental results in high-dimensional spaces\ndemonstrate that LIT* achieves faster convergence and improved solution\nquality. It outperforms state-of-the-art single-query, sampling-based planners\nin environments ranging from R^8 to R^14 and is successfully validated on a\ndual-arm robot manipulation task. A video showcasing our experimental results\nis available at: https://youtu.be/NrNs9zebWWk\n", "link": "http://arxiv.org/abs/2508.20884v1", "date": "2025-08-28", "relevancy": 1.5921, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.549}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Fuzzy%20Optimization%20for%20Batch-Size%20and%20Nearest%20Neighbors%20in%20Optimal%0A%20%20Robot%20Motion%20Planning&body=Title%3A%20Deep%20Fuzzy%20Optimization%20for%20Batch-Size%20and%20Nearest%20Neighbors%20in%20Optimal%0A%20%20Robot%20Motion%20Planning%0AAuthor%3A%20Liding%20Zhang%20and%20Qiyang%20Zong%20and%20Yu%20Zhang%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Efficient%20motion%20planning%20algorithms%20are%20essential%20in%20robotics.%20Optimizing%0Aessential%20parameters%2C%20such%20as%20batch%20size%20and%20nearest%20neighbor%20selection%20in%0Asampling-based%20methods%2C%20can%20enhance%20performance%20in%20the%20planning%20process.%0AHowever%2C%20existing%20approaches%20often%20lack%20environmental%20adaptability.%20Inspired%20by%0Athe%20method%20of%20the%20deep%20fuzzy%20neural%20networks%2C%20this%20work%20introduces%0ALearning-based%20Informed%20Trees%20%28LIT%2A%29%2C%20a%20sampling-based%20deep%20fuzzy%0Alearning-based%20planner%20that%20dynamically%20adjusts%20batch%20size%20and%20nearest%20neighbor%0Aparameters%20to%20obstacle%20distributions%20in%20the%20configuration%20spaces.%20By%20encoding%0Aboth%20global%20and%20local%20ratios%20via%20valid%20and%20invalid%20states%2C%20LIT%2A%20differentiates%0Abetween%20obstacle-sparse%20and%20obstacle-dense%20regions%2C%20leading%20to%20lower-cost%20paths%0Aand%20reduced%20computation%20time.%20Experimental%20results%20in%20high-dimensional%20spaces%0Ademonstrate%20that%20LIT%2A%20achieves%20faster%20convergence%20and%20improved%20solution%0Aquality.%20It%20outperforms%20state-of-the-art%20single-query%2C%20sampling-based%20planners%0Ain%20environments%20ranging%20from%20R%5E8%20to%20R%5E14%20and%20is%20successfully%20validated%20on%20a%0Adual-arm%20robot%20manipulation%20task.%20A%20video%20showcasing%20our%20experimental%20results%0Ais%20available%20at%3A%20https%3A//youtu.be/NrNs9zebWWk%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Fuzzy%2520Optimization%2520for%2520Batch-Size%2520and%2520Nearest%2520Neighbors%2520in%2520Optimal%250A%2520%2520Robot%2520Motion%2520Planning%26entry.906535625%3DLiding%2520Zhang%2520and%2520Qiyang%2520Zong%2520and%2520Yu%2520Zhang%2520and%2520Zhenshan%2520Bing%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Efficient%2520motion%2520planning%2520algorithms%2520are%2520essential%2520in%2520robotics.%2520Optimizing%250Aessential%2520parameters%252C%2520such%2520as%2520batch%2520size%2520and%2520nearest%2520neighbor%2520selection%2520in%250Asampling-based%2520methods%252C%2520can%2520enhance%2520performance%2520in%2520the%2520planning%2520process.%250AHowever%252C%2520existing%2520approaches%2520often%2520lack%2520environmental%2520adaptability.%2520Inspired%2520by%250Athe%2520method%2520of%2520the%2520deep%2520fuzzy%2520neural%2520networks%252C%2520this%2520work%2520introduces%250ALearning-based%2520Informed%2520Trees%2520%2528LIT%252A%2529%252C%2520a%2520sampling-based%2520deep%2520fuzzy%250Alearning-based%2520planner%2520that%2520dynamically%2520adjusts%2520batch%2520size%2520and%2520nearest%2520neighbor%250Aparameters%2520to%2520obstacle%2520distributions%2520in%2520the%2520configuration%2520spaces.%2520By%2520encoding%250Aboth%2520global%2520and%2520local%2520ratios%2520via%2520valid%2520and%2520invalid%2520states%252C%2520LIT%252A%2520differentiates%250Abetween%2520obstacle-sparse%2520and%2520obstacle-dense%2520regions%252C%2520leading%2520to%2520lower-cost%2520paths%250Aand%2520reduced%2520computation%2520time.%2520Experimental%2520results%2520in%2520high-dimensional%2520spaces%250Ademonstrate%2520that%2520LIT%252A%2520achieves%2520faster%2520convergence%2520and%2520improved%2520solution%250Aquality.%2520It%2520outperforms%2520state-of-the-art%2520single-query%252C%2520sampling-based%2520planners%250Ain%2520environments%2520ranging%2520from%2520R%255E8%2520to%2520R%255E14%2520and%2520is%2520successfully%2520validated%2520on%2520a%250Adual-arm%2520robot%2520manipulation%2520task.%2520A%2520video%2520showcasing%2520our%2520experimental%2520results%250Ais%2520available%2520at%253A%2520https%253A//youtu.be/NrNs9zebWWk%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Fuzzy%20Optimization%20for%20Batch-Size%20and%20Nearest%20Neighbors%20in%20Optimal%0A%20%20Robot%20Motion%20Planning&entry.906535625=Liding%20Zhang%20and%20Qiyang%20Zong%20and%20Yu%20Zhang%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll&entry.1292438233=%20%20Efficient%20motion%20planning%20algorithms%20are%20essential%20in%20robotics.%20Optimizing%0Aessential%20parameters%2C%20such%20as%20batch%20size%20and%20nearest%20neighbor%20selection%20in%0Asampling-based%20methods%2C%20can%20enhance%20performance%20in%20the%20planning%20process.%0AHowever%2C%20existing%20approaches%20often%20lack%20environmental%20adaptability.%20Inspired%20by%0Athe%20method%20of%20the%20deep%20fuzzy%20neural%20networks%2C%20this%20work%20introduces%0ALearning-based%20Informed%20Trees%20%28LIT%2A%29%2C%20a%20sampling-based%20deep%20fuzzy%0Alearning-based%20planner%20that%20dynamically%20adjusts%20batch%20size%20and%20nearest%20neighbor%0Aparameters%20to%20obstacle%20distributions%20in%20the%20configuration%20spaces.%20By%20encoding%0Aboth%20global%20and%20local%20ratios%20via%20valid%20and%20invalid%20states%2C%20LIT%2A%20differentiates%0Abetween%20obstacle-sparse%20and%20obstacle-dense%20regions%2C%20leading%20to%20lower-cost%20paths%0Aand%20reduced%20computation%20time.%20Experimental%20results%20in%20high-dimensional%20spaces%0Ademonstrate%20that%20LIT%2A%20achieves%20faster%20convergence%20and%20improved%20solution%0Aquality.%20It%20outperforms%20state-of-the-art%20single-query%2C%20sampling-based%20planners%0Ain%20environments%20ranging%20from%20R%5E8%20to%20R%5E14%20and%20is%20successfully%20validated%20on%20a%0Adual-arm%20robot%20manipulation%20task.%20A%20video%20showcasing%20our%20experimental%20results%0Ais%20available%20at%3A%20https%3A//youtu.be/NrNs9zebWWk%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20884v1&entry.124074799=Read"},
{"title": "Investigating the Robustness of Counterfactual Learning to Rank Models:\n  A Reproducibility Study", "author": "Zechun Niu and Zhilin Zhang and Jiaxin Mao and Qingyao Ai and Ji-Rong Wen", "abstract": "  Counterfactual learning to rank (CLTR) has attracted extensive attention in\nthe IR community for its ability to leverage massive logged user interaction\ndata to train ranking models. While the CLTR models can be theoretically\nunbiased when the user behavior assumption is correct and the propensity\nestimation is accurate, their effectiveness is usually empirically evaluated\nvia simulation-based experiments due to a lack of widely available,\nlarge-scale, real click logs. However, many previous simulation-based\nexperiments are somewhat limited because they may have one or more of the\nfollowing deficiencies: 1) using a weak production ranker to generate initial\nranked lists, 2) relying on a simplified user simulation model to simulate user\nclicks, and 3) generating a fixed number of synthetic click logs. As a result,\nthe robustness of CLTR models in complex and diverse situations is largely\nunknown and needs further investigation.\n  To address this problem, in this paper, we aim to investigate the robustness\nof existing CLTR models in a reproducibility study with extensive\nsimulation-based experiments that (1) use production rankers with different\nranking performance, (2) leverage multiple user simulation models with\ndifferent user behavior assumptions, and (3) generate different numbers of\nsynthetic sessions for the training queries. We find that the IPS-DCM, DLA-PBM,\nand UPE models show better robustness under various simulation settings than\nother CLTR models. Moreover, existing CLTR models often fail to outperform\nnaive click baselines when the production ranker is strong and the number of\ntraining sessions is limited, indicating a pressing need for new CLTR\nalgorithms tailored to these conditions.\n", "link": "http://arxiv.org/abs/2404.03707v2", "date": "2025-08-28", "relevancy": 1.3461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Robustness%20of%20Counterfactual%20Learning%20to%20Rank%20Models%3A%0A%20%20A%20Reproducibility%20Study&body=Title%3A%20Investigating%20the%20Robustness%20of%20Counterfactual%20Learning%20to%20Rank%20Models%3A%0A%20%20A%20Reproducibility%20Study%0AAuthor%3A%20Zechun%20Niu%20and%20Zhilin%20Zhang%20and%20Jiaxin%20Mao%20and%20Qingyao%20Ai%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Counterfactual%20learning%20to%20rank%20%28CLTR%29%20has%20attracted%20extensive%20attention%20in%0Athe%20IR%20community%20for%20its%20ability%20to%20leverage%20massive%20logged%20user%20interaction%0Adata%20to%20train%20ranking%20models.%20While%20the%20CLTR%20models%20can%20be%20theoretically%0Aunbiased%20when%20the%20user%20behavior%20assumption%20is%20correct%20and%20the%20propensity%0Aestimation%20is%20accurate%2C%20their%20effectiveness%20is%20usually%20empirically%20evaluated%0Avia%20simulation-based%20experiments%20due%20to%20a%20lack%20of%20widely%20available%2C%0Alarge-scale%2C%20real%20click%20logs.%20However%2C%20many%20previous%20simulation-based%0Aexperiments%20are%20somewhat%20limited%20because%20they%20may%20have%20one%20or%20more%20of%20the%0Afollowing%20deficiencies%3A%201%29%20using%20a%20weak%20production%20ranker%20to%20generate%20initial%0Aranked%20lists%2C%202%29%20relying%20on%20a%20simplified%20user%20simulation%20model%20to%20simulate%20user%0Aclicks%2C%20and%203%29%20generating%20a%20fixed%20number%20of%20synthetic%20click%20logs.%20As%20a%20result%2C%0Athe%20robustness%20of%20CLTR%20models%20in%20complex%20and%20diverse%20situations%20is%20largely%0Aunknown%20and%20needs%20further%20investigation.%0A%20%20To%20address%20this%20problem%2C%20in%20this%20paper%2C%20we%20aim%20to%20investigate%20the%20robustness%0Aof%20existing%20CLTR%20models%20in%20a%20reproducibility%20study%20with%20extensive%0Asimulation-based%20experiments%20that%20%281%29%20use%20production%20rankers%20with%20different%0Aranking%20performance%2C%20%282%29%20leverage%20multiple%20user%20simulation%20models%20with%0Adifferent%20user%20behavior%20assumptions%2C%20and%20%283%29%20generate%20different%20numbers%20of%0Asynthetic%20sessions%20for%20the%20training%20queries.%20We%20find%20that%20the%20IPS-DCM%2C%20DLA-PBM%2C%0Aand%20UPE%20models%20show%20better%20robustness%20under%20various%20simulation%20settings%20than%0Aother%20CLTR%20models.%20Moreover%2C%20existing%20CLTR%20models%20often%20fail%20to%20outperform%0Anaive%20click%20baselines%20when%20the%20production%20ranker%20is%20strong%20and%20the%20number%20of%0Atraining%20sessions%20is%20limited%2C%20indicating%20a%20pressing%20need%20for%20new%20CLTR%0Aalgorithms%20tailored%20to%20these%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Robustness%2520of%2520Counterfactual%2520Learning%2520to%2520Rank%2520Models%253A%250A%2520%2520A%2520Reproducibility%2520Study%26entry.906535625%3DZechun%2520Niu%2520and%2520Zhilin%2520Zhang%2520and%2520Jiaxin%2520Mao%2520and%2520Qingyao%2520Ai%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Counterfactual%2520learning%2520to%2520rank%2520%2528CLTR%2529%2520has%2520attracted%2520extensive%2520attention%2520in%250Athe%2520IR%2520community%2520for%2520its%2520ability%2520to%2520leverage%2520massive%2520logged%2520user%2520interaction%250Adata%2520to%2520train%2520ranking%2520models.%2520While%2520the%2520CLTR%2520models%2520can%2520be%2520theoretically%250Aunbiased%2520when%2520the%2520user%2520behavior%2520assumption%2520is%2520correct%2520and%2520the%2520propensity%250Aestimation%2520is%2520accurate%252C%2520their%2520effectiveness%2520is%2520usually%2520empirically%2520evaluated%250Avia%2520simulation-based%2520experiments%2520due%2520to%2520a%2520lack%2520of%2520widely%2520available%252C%250Alarge-scale%252C%2520real%2520click%2520logs.%2520However%252C%2520many%2520previous%2520simulation-based%250Aexperiments%2520are%2520somewhat%2520limited%2520because%2520they%2520may%2520have%2520one%2520or%2520more%2520of%2520the%250Afollowing%2520deficiencies%253A%25201%2529%2520using%2520a%2520weak%2520production%2520ranker%2520to%2520generate%2520initial%250Aranked%2520lists%252C%25202%2529%2520relying%2520on%2520a%2520simplified%2520user%2520simulation%2520model%2520to%2520simulate%2520user%250Aclicks%252C%2520and%25203%2529%2520generating%2520a%2520fixed%2520number%2520of%2520synthetic%2520click%2520logs.%2520As%2520a%2520result%252C%250Athe%2520robustness%2520of%2520CLTR%2520models%2520in%2520complex%2520and%2520diverse%2520situations%2520is%2520largely%250Aunknown%2520and%2520needs%2520further%2520investigation.%250A%2520%2520To%2520address%2520this%2520problem%252C%2520in%2520this%2520paper%252C%2520we%2520aim%2520to%2520investigate%2520the%2520robustness%250Aof%2520existing%2520CLTR%2520models%2520in%2520a%2520reproducibility%2520study%2520with%2520extensive%250Asimulation-based%2520experiments%2520that%2520%25281%2529%2520use%2520production%2520rankers%2520with%2520different%250Aranking%2520performance%252C%2520%25282%2529%2520leverage%2520multiple%2520user%2520simulation%2520models%2520with%250Adifferent%2520user%2520behavior%2520assumptions%252C%2520and%2520%25283%2529%2520generate%2520different%2520numbers%2520of%250Asynthetic%2520sessions%2520for%2520the%2520training%2520queries.%2520We%2520find%2520that%2520the%2520IPS-DCM%252C%2520DLA-PBM%252C%250Aand%2520UPE%2520models%2520show%2520better%2520robustness%2520under%2520various%2520simulation%2520settings%2520than%250Aother%2520CLTR%2520models.%2520Moreover%252C%2520existing%2520CLTR%2520models%2520often%2520fail%2520to%2520outperform%250Anaive%2520click%2520baselines%2520when%2520the%2520production%2520ranker%2520is%2520strong%2520and%2520the%2520number%2520of%250Atraining%2520sessions%2520is%2520limited%252C%2520indicating%2520a%2520pressing%2520need%2520for%2520new%2520CLTR%250Aalgorithms%2520tailored%2520to%2520these%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Robustness%20of%20Counterfactual%20Learning%20to%20Rank%20Models%3A%0A%20%20A%20Reproducibility%20Study&entry.906535625=Zechun%20Niu%20and%20Zhilin%20Zhang%20and%20Jiaxin%20Mao%20and%20Qingyao%20Ai%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Counterfactual%20learning%20to%20rank%20%28CLTR%29%20has%20attracted%20extensive%20attention%20in%0Athe%20IR%20community%20for%20its%20ability%20to%20leverage%20massive%20logged%20user%20interaction%0Adata%20to%20train%20ranking%20models.%20While%20the%20CLTR%20models%20can%20be%20theoretically%0Aunbiased%20when%20the%20user%20behavior%20assumption%20is%20correct%20and%20the%20propensity%0Aestimation%20is%20accurate%2C%20their%20effectiveness%20is%20usually%20empirically%20evaluated%0Avia%20simulation-based%20experiments%20due%20to%20a%20lack%20of%20widely%20available%2C%0Alarge-scale%2C%20real%20click%20logs.%20However%2C%20many%20previous%20simulation-based%0Aexperiments%20are%20somewhat%20limited%20because%20they%20may%20have%20one%20or%20more%20of%20the%0Afollowing%20deficiencies%3A%201%29%20using%20a%20weak%20production%20ranker%20to%20generate%20initial%0Aranked%20lists%2C%202%29%20relying%20on%20a%20simplified%20user%20simulation%20model%20to%20simulate%20user%0Aclicks%2C%20and%203%29%20generating%20a%20fixed%20number%20of%20synthetic%20click%20logs.%20As%20a%20result%2C%0Athe%20robustness%20of%20CLTR%20models%20in%20complex%20and%20diverse%20situations%20is%20largely%0Aunknown%20and%20needs%20further%20investigation.%0A%20%20To%20address%20this%20problem%2C%20in%20this%20paper%2C%20we%20aim%20to%20investigate%20the%20robustness%0Aof%20existing%20CLTR%20models%20in%20a%20reproducibility%20study%20with%20extensive%0Asimulation-based%20experiments%20that%20%281%29%20use%20production%20rankers%20with%20different%0Aranking%20performance%2C%20%282%29%20leverage%20multiple%20user%20simulation%20models%20with%0Adifferent%20user%20behavior%20assumptions%2C%20and%20%283%29%20generate%20different%20numbers%20of%0Asynthetic%20sessions%20for%20the%20training%20queries.%20We%20find%20that%20the%20IPS-DCM%2C%20DLA-PBM%2C%0Aand%20UPE%20models%20show%20better%20robustness%20under%20various%20simulation%20settings%20than%0Aother%20CLTR%20models.%20Moreover%2C%20existing%20CLTR%20models%20often%20fail%20to%20outperform%0Anaive%20click%20baselines%20when%20the%20production%20ranker%20is%20strong%20and%20the%20number%20of%0Atraining%20sessions%20is%20limited%2C%20indicating%20a%20pressing%20need%20for%20new%20CLTR%0Aalgorithms%20tailored%20to%20these%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03707v2&entry.124074799=Read"},
{"title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via\n  Real-World Unsupervised Quality-Diversity", "author": "Luca Grillotti and Lisa Coiffard and Oscar Pang and Maxence Faldor and Antoine Cully", "abstract": "  Autonomous skill discovery aims to enable robots to acquire diverse behaviors\nwithout explicit supervision. Learning such behaviors directly on physical\nhardware remains challenging due to safety and data efficiency constraints.\nExisting methods, including Quality-Diversity Actor-Critic (QDAC), require\nmanually defined skill spaces and carefully tuned heuristics, limiting\nreal-world applicability. We propose Unsupervised Real-world Skill Acquisition\n(URSA), an extension of QDAC that enables robots to autonomously discover and\nmaster diverse, high-performing skills directly in the real world. We\ndemonstrate that URSA successfully discovers diverse locomotion skills on a\nUnitree A1 quadruped in both simulation and the real world. Our approach\nsupports both heuristic-driven skill discovery and fully unsupervised settings.\nWe also show that the learned skill repertoire can be reused for downstream\ntasks such as real-world damage adaptation, where URSA outperforms all\nbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.\nOur results establish a new framework for real-world robot learning that\nenables continuous skill discovery with limited human intervention,\nrepresenting a significant step toward more autonomous and adaptable robotic\nsystems. Demonstration videos are available at\nhttps://adaptive-intelligent-robotics.github.io/URSA.\n", "link": "http://arxiv.org/abs/2508.19172v3", "date": "2025-08-28", "relevancy": 1.7349, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5759}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Tabula%20Rasa%20to%20Emergent%20Abilities%3A%20Discovering%20Robot%20Skills%20via%0A%20%20Real-World%20Unsupervised%20Quality-Diversity&body=Title%3A%20From%20Tabula%20Rasa%20to%20Emergent%20Abilities%3A%20Discovering%20Robot%20Skills%20via%0A%20%20Real-World%20Unsupervised%20Quality-Diversity%0AAuthor%3A%20Luca%20Grillotti%20and%20Lisa%20Coiffard%20and%20Oscar%20Pang%20and%20Maxence%20Faldor%20and%20Antoine%20Cully%0AAbstract%3A%20%20%20Autonomous%20skill%20discovery%20aims%20to%20enable%20robots%20to%20acquire%20diverse%20behaviors%0Awithout%20explicit%20supervision.%20Learning%20such%20behaviors%20directly%20on%20physical%0Ahardware%20remains%20challenging%20due%20to%20safety%20and%20data%20efficiency%20constraints.%0AExisting%20methods%2C%20including%20Quality-Diversity%20Actor-Critic%20%28QDAC%29%2C%20require%0Amanually%20defined%20skill%20spaces%20and%20carefully%20tuned%20heuristics%2C%20limiting%0Areal-world%20applicability.%20We%20propose%20Unsupervised%20Real-world%20Skill%20Acquisition%0A%28URSA%29%2C%20an%20extension%20of%20QDAC%20that%20enables%20robots%20to%20autonomously%20discover%20and%0Amaster%20diverse%2C%20high-performing%20skills%20directly%20in%20the%20real%20world.%20We%0Ademonstrate%20that%20URSA%20successfully%20discovers%20diverse%20locomotion%20skills%20on%20a%0AUnitree%20A1%20quadruped%20in%20both%20simulation%20and%20the%20real%20world.%20Our%20approach%0Asupports%20both%20heuristic-driven%20skill%20discovery%20and%20fully%20unsupervised%20settings.%0AWe%20also%20show%20that%20the%20learned%20skill%20repertoire%20can%20be%20reused%20for%20downstream%0Atasks%20such%20as%20real-world%20damage%20adaptation%2C%20where%20URSA%20outperforms%20all%0Abaselines%20in%205%20out%20of%209%20simulated%20and%203%20out%20of%205%20real-world%20damage%20scenarios.%0AOur%20results%20establish%20a%20new%20framework%20for%20real-world%20robot%20learning%20that%0Aenables%20continuous%20skill%20discovery%20with%20limited%20human%20intervention%2C%0Arepresenting%20a%20significant%20step%20toward%20more%20autonomous%20and%20adaptable%20robotic%0Asystems.%20Demonstration%20videos%20are%20available%20at%0Ahttps%3A//adaptive-intelligent-robotics.github.io/URSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19172v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Tabula%2520Rasa%2520to%2520Emergent%2520Abilities%253A%2520Discovering%2520Robot%2520Skills%2520via%250A%2520%2520Real-World%2520Unsupervised%2520Quality-Diversity%26entry.906535625%3DLuca%2520Grillotti%2520and%2520Lisa%2520Coiffard%2520and%2520Oscar%2520Pang%2520and%2520Maxence%2520Faldor%2520and%2520Antoine%2520Cully%26entry.1292438233%3D%2520%2520Autonomous%2520skill%2520discovery%2520aims%2520to%2520enable%2520robots%2520to%2520acquire%2520diverse%2520behaviors%250Awithout%2520explicit%2520supervision.%2520Learning%2520such%2520behaviors%2520directly%2520on%2520physical%250Ahardware%2520remains%2520challenging%2520due%2520to%2520safety%2520and%2520data%2520efficiency%2520constraints.%250AExisting%2520methods%252C%2520including%2520Quality-Diversity%2520Actor-Critic%2520%2528QDAC%2529%252C%2520require%250Amanually%2520defined%2520skill%2520spaces%2520and%2520carefully%2520tuned%2520heuristics%252C%2520limiting%250Areal-world%2520applicability.%2520We%2520propose%2520Unsupervised%2520Real-world%2520Skill%2520Acquisition%250A%2528URSA%2529%252C%2520an%2520extension%2520of%2520QDAC%2520that%2520enables%2520robots%2520to%2520autonomously%2520discover%2520and%250Amaster%2520diverse%252C%2520high-performing%2520skills%2520directly%2520in%2520the%2520real%2520world.%2520We%250Ademonstrate%2520that%2520URSA%2520successfully%2520discovers%2520diverse%2520locomotion%2520skills%2520on%2520a%250AUnitree%2520A1%2520quadruped%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world.%2520Our%2520approach%250Asupports%2520both%2520heuristic-driven%2520skill%2520discovery%2520and%2520fully%2520unsupervised%2520settings.%250AWe%2520also%2520show%2520that%2520the%2520learned%2520skill%2520repertoire%2520can%2520be%2520reused%2520for%2520downstream%250Atasks%2520such%2520as%2520real-world%2520damage%2520adaptation%252C%2520where%2520URSA%2520outperforms%2520all%250Abaselines%2520in%25205%2520out%2520of%25209%2520simulated%2520and%25203%2520out%2520of%25205%2520real-world%2520damage%2520scenarios.%250AOur%2520results%2520establish%2520a%2520new%2520framework%2520for%2520real-world%2520robot%2520learning%2520that%250Aenables%2520continuous%2520skill%2520discovery%2520with%2520limited%2520human%2520intervention%252C%250Arepresenting%2520a%2520significant%2520step%2520toward%2520more%2520autonomous%2520and%2520adaptable%2520robotic%250Asystems.%2520Demonstration%2520videos%2520are%2520available%2520at%250Ahttps%253A//adaptive-intelligent-robotics.github.io/URSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19172v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Tabula%20Rasa%20to%20Emergent%20Abilities%3A%20Discovering%20Robot%20Skills%20via%0A%20%20Real-World%20Unsupervised%20Quality-Diversity&entry.906535625=Luca%20Grillotti%20and%20Lisa%20Coiffard%20and%20Oscar%20Pang%20and%20Maxence%20Faldor%20and%20Antoine%20Cully&entry.1292438233=%20%20Autonomous%20skill%20discovery%20aims%20to%20enable%20robots%20to%20acquire%20diverse%20behaviors%0Awithout%20explicit%20supervision.%20Learning%20such%20behaviors%20directly%20on%20physical%0Ahardware%20remains%20challenging%20due%20to%20safety%20and%20data%20efficiency%20constraints.%0AExisting%20methods%2C%20including%20Quality-Diversity%20Actor-Critic%20%28QDAC%29%2C%20require%0Amanually%20defined%20skill%20spaces%20and%20carefully%20tuned%20heuristics%2C%20limiting%0Areal-world%20applicability.%20We%20propose%20Unsupervised%20Real-world%20Skill%20Acquisition%0A%28URSA%29%2C%20an%20extension%20of%20QDAC%20that%20enables%20robots%20to%20autonomously%20discover%20and%0Amaster%20diverse%2C%20high-performing%20skills%20directly%20in%20the%20real%20world.%20We%0Ademonstrate%20that%20URSA%20successfully%20discovers%20diverse%20locomotion%20skills%20on%20a%0AUnitree%20A1%20quadruped%20in%20both%20simulation%20and%20the%20real%20world.%20Our%20approach%0Asupports%20both%20heuristic-driven%20skill%20discovery%20and%20fully%20unsupervised%20settings.%0AWe%20also%20show%20that%20the%20learned%20skill%20repertoire%20can%20be%20reused%20for%20downstream%0Atasks%20such%20as%20real-world%20damage%20adaptation%2C%20where%20URSA%20outperforms%20all%0Abaselines%20in%205%20out%20of%209%20simulated%20and%203%20out%20of%205%20real-world%20damage%20scenarios.%0AOur%20results%20establish%20a%20new%20framework%20for%20real-world%20robot%20learning%20that%0Aenables%20continuous%20skill%20discovery%20with%20limited%20human%20intervention%2C%0Arepresenting%20a%20significant%20step%20toward%20more%20autonomous%20and%20adaptable%20robotic%0Asystems.%20Demonstration%20videos%20are%20available%20at%0Ahttps%3A//adaptive-intelligent-robotics.github.io/URSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19172v3&entry.124074799=Read"},
{"title": "Mitosis detection in domain shift scenarios: a Mamba-based approach", "author": "Gennaro Percannella and Mattia Sarno and Francesco Tortorella and Mario Vento", "abstract": "  Mitosis detection in histopathology images plays a key role in tumor\nassessment. Although machine learning algorithms could be exploited for aiding\nphysicians in accurately performing such a task, these algorithms suffer from\nsignificative performance drop when evaluated on images coming from domains\nthat are different from the training ones. In this work, we propose a\nMamba-based approach for mitosis detection under domain shift, inspired by the\npromising performance demonstrated by Mamba in medical imaging segmentation\ntasks. Specifically, our approach exploits a VM-UNet architecture for carrying\nout the addressed task, as well as stain augmentation operations for further\nimproving model robustness against domain shift. Our approach has been\nsubmitted to the track 1 of the MItosis DOmain Generalization (MIDOG)\nchallenge. Preliminary experiments, conducted on the MIDOG++ dataset, show\nlarge room for improvement for the proposed method.\n", "link": "http://arxiv.org/abs/2508.21033v1", "date": "2025-08-28", "relevancy": 1.4838, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4965}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4945}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitosis%20detection%20in%20domain%20shift%20scenarios%3A%20a%20Mamba-based%20approach&body=Title%3A%20Mitosis%20detection%20in%20domain%20shift%20scenarios%3A%20a%20Mamba-based%20approach%0AAuthor%3A%20Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento%0AAbstract%3A%20%20%20Mitosis%20detection%20in%20histopathology%20images%20plays%20a%20key%20role%20in%20tumor%0Aassessment.%20Although%20machine%20learning%20algorithms%20could%20be%20exploited%20for%20aiding%0Aphysicians%20in%20accurately%20performing%20such%20a%20task%2C%20these%20algorithms%20suffer%20from%0Asignificative%20performance%20drop%20when%20evaluated%20on%20images%20coming%20from%20domains%0Athat%20are%20different%20from%20the%20training%20ones.%20In%20this%20work%2C%20we%20propose%20a%0AMamba-based%20approach%20for%20mitosis%20detection%20under%20domain%20shift%2C%20inspired%20by%20the%0Apromising%20performance%20demonstrated%20by%20Mamba%20in%20medical%20imaging%20segmentation%0Atasks.%20Specifically%2C%20our%20approach%20exploits%20a%20VM-UNet%20architecture%20for%20carrying%0Aout%20the%20addressed%20task%2C%20as%20well%20as%20stain%20augmentation%20operations%20for%20further%0Aimproving%20model%20robustness%20against%20domain%20shift.%20Our%20approach%20has%20been%0Asubmitted%20to%20the%20track%201%20of%20the%20MItosis%20DOmain%20Generalization%20%28MIDOG%29%0Achallenge.%20Preliminary%20experiments%2C%20conducted%20on%20the%20MIDOG%2B%2B%20dataset%2C%20show%0Alarge%20room%20for%20improvement%20for%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitosis%2520detection%2520in%2520domain%2520shift%2520scenarios%253A%2520a%2520Mamba-based%2520approach%26entry.906535625%3DGennaro%2520Percannella%2520and%2520Mattia%2520Sarno%2520and%2520Francesco%2520Tortorella%2520and%2520Mario%2520Vento%26entry.1292438233%3D%2520%2520Mitosis%2520detection%2520in%2520histopathology%2520images%2520plays%2520a%2520key%2520role%2520in%2520tumor%250Aassessment.%2520Although%2520machine%2520learning%2520algorithms%2520could%2520be%2520exploited%2520for%2520aiding%250Aphysicians%2520in%2520accurately%2520performing%2520such%2520a%2520task%252C%2520these%2520algorithms%2520suffer%2520from%250Asignificative%2520performance%2520drop%2520when%2520evaluated%2520on%2520images%2520coming%2520from%2520domains%250Athat%2520are%2520different%2520from%2520the%2520training%2520ones.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250AMamba-based%2520approach%2520for%2520mitosis%2520detection%2520under%2520domain%2520shift%252C%2520inspired%2520by%2520the%250Apromising%2520performance%2520demonstrated%2520by%2520Mamba%2520in%2520medical%2520imaging%2520segmentation%250Atasks.%2520Specifically%252C%2520our%2520approach%2520exploits%2520a%2520VM-UNet%2520architecture%2520for%2520carrying%250Aout%2520the%2520addressed%2520task%252C%2520as%2520well%2520as%2520stain%2520augmentation%2520operations%2520for%2520further%250Aimproving%2520model%2520robustness%2520against%2520domain%2520shift.%2520Our%2520approach%2520has%2520been%250Asubmitted%2520to%2520the%2520track%25201%2520of%2520the%2520MItosis%2520DOmain%2520Generalization%2520%2528MIDOG%2529%250Achallenge.%2520Preliminary%2520experiments%252C%2520conducted%2520on%2520the%2520MIDOG%252B%252B%2520dataset%252C%2520show%250Alarge%2520room%2520for%2520improvement%2520for%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitosis%20detection%20in%20domain%20shift%20scenarios%3A%20a%20Mamba-based%20approach&entry.906535625=Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento&entry.1292438233=%20%20Mitosis%20detection%20in%20histopathology%20images%20plays%20a%20key%20role%20in%20tumor%0Aassessment.%20Although%20machine%20learning%20algorithms%20could%20be%20exploited%20for%20aiding%0Aphysicians%20in%20accurately%20performing%20such%20a%20task%2C%20these%20algorithms%20suffer%20from%0Asignificative%20performance%20drop%20when%20evaluated%20on%20images%20coming%20from%20domains%0Athat%20are%20different%20from%20the%20training%20ones.%20In%20this%20work%2C%20we%20propose%20a%0AMamba-based%20approach%20for%20mitosis%20detection%20under%20domain%20shift%2C%20inspired%20by%20the%0Apromising%20performance%20demonstrated%20by%20Mamba%20in%20medical%20imaging%20segmentation%0Atasks.%20Specifically%2C%20our%20approach%20exploits%20a%20VM-UNet%20architecture%20for%20carrying%0Aout%20the%20addressed%20task%2C%20as%20well%20as%20stain%20augmentation%20operations%20for%20further%0Aimproving%20model%20robustness%20against%20domain%20shift.%20Our%20approach%20has%20been%0Asubmitted%20to%20the%20track%201%20of%20the%20MItosis%20DOmain%20Generalization%20%28MIDOG%29%0Achallenge.%20Preliminary%20experiments%2C%20conducted%20on%20the%20MIDOG%2B%2B%20dataset%2C%20show%0Alarge%20room%20for%20improvement%20for%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21033v1&entry.124074799=Read"},
{"title": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of\n  Local Nash Interactions", "author": "Zhouyu Zhang and Chih-Yuan Chiu and Glen Chou", "abstract": "  We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.\n", "link": "http://arxiv.org/abs/2508.19945v2", "date": "2025-08-28", "relevancy": 1.5489, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5076}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraint%20Learning%20in%20Multi-Agent%20Dynamic%20Games%20from%20Demonstrations%20of%0A%20%20Local%20Nash%20Interactions&body=Title%3A%20Constraint%20Learning%20in%20Multi-Agent%20Dynamic%20Games%20from%20Demonstrations%20of%0A%20%20Local%20Nash%20Interactions%0AAuthor%3A%20Zhouyu%20Zhang%20and%20Chih-Yuan%20Chiu%20and%20Glen%20Chou%0AAbstract%3A%20%20%20We%20present%20an%20inverse%20dynamic%20game-based%20algorithm%20to%20learn%20parametric%0Aconstraints%20from%20a%20given%20dataset%20of%20local%20generalized%20Nash%20equilibrium%0Ainteractions%20between%20multiple%20agents.%20Specifically%2C%20we%20introduce%20mixed-integer%0Alinear%20programs%20%28MILP%29%20encoding%20the%20Karush-Kuhn-Tucker%20%28KKT%29%20conditions%20of%20the%0Ainteracting%20agents%2C%20which%20recover%20constraints%20consistent%20with%20the%20Nash%0Astationarity%20of%20the%20interaction%20demonstrations.%20We%20establish%20theoretical%0Aguarantees%20that%20our%20method%20learns%20inner%20approximations%20of%20the%20true%20safe%20and%0Aunsafe%20sets%2C%20as%20well%20as%20limitations%20of%20constraint%20learnability%20from%0Ademonstrations%20of%20Nash%20equilibrium%20interactions.%20We%20also%20use%20the%20interaction%0Aconstraints%20recovered%20by%20our%20method%20to%20design%20motion%20plans%20that%20robustly%0Asatisfy%20the%20underlying%20constraints.%20Across%20simulations%20and%20hardware%0Aexperiments%2C%20our%20methods%20proved%20capable%20of%20inferring%20constraints%20and%20designing%0Ainteractive%20motion%20plans%20for%20various%20classes%20of%20constraints%2C%20both%20convex%20and%0Anon-convex%2C%20from%20interaction%20demonstrations%20of%20agents%20with%20nonlinear%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraint%2520Learning%2520in%2520Multi-Agent%2520Dynamic%2520Games%2520from%2520Demonstrations%2520of%250A%2520%2520Local%2520Nash%2520Interactions%26entry.906535625%3DZhouyu%2520Zhang%2520and%2520Chih-Yuan%2520Chiu%2520and%2520Glen%2520Chou%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520inverse%2520dynamic%2520game-based%2520algorithm%2520to%2520learn%2520parametric%250Aconstraints%2520from%2520a%2520given%2520dataset%2520of%2520local%2520generalized%2520Nash%2520equilibrium%250Ainteractions%2520between%2520multiple%2520agents.%2520Specifically%252C%2520we%2520introduce%2520mixed-integer%250Alinear%2520programs%2520%2528MILP%2529%2520encoding%2520the%2520Karush-Kuhn-Tucker%2520%2528KKT%2529%2520conditions%2520of%2520the%250Ainteracting%2520agents%252C%2520which%2520recover%2520constraints%2520consistent%2520with%2520the%2520Nash%250Astationarity%2520of%2520the%2520interaction%2520demonstrations.%2520We%2520establish%2520theoretical%250Aguarantees%2520that%2520our%2520method%2520learns%2520inner%2520approximations%2520of%2520the%2520true%2520safe%2520and%250Aunsafe%2520sets%252C%2520as%2520well%2520as%2520limitations%2520of%2520constraint%2520learnability%2520from%250Ademonstrations%2520of%2520Nash%2520equilibrium%2520interactions.%2520We%2520also%2520use%2520the%2520interaction%250Aconstraints%2520recovered%2520by%2520our%2520method%2520to%2520design%2520motion%2520plans%2520that%2520robustly%250Asatisfy%2520the%2520underlying%2520constraints.%2520Across%2520simulations%2520and%2520hardware%250Aexperiments%252C%2520our%2520methods%2520proved%2520capable%2520of%2520inferring%2520constraints%2520and%2520designing%250Ainteractive%2520motion%2520plans%2520for%2520various%2520classes%2520of%2520constraints%252C%2520both%2520convex%2520and%250Anon-convex%252C%2520from%2520interaction%2520demonstrations%2520of%2520agents%2520with%2520nonlinear%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraint%20Learning%20in%20Multi-Agent%20Dynamic%20Games%20from%20Demonstrations%20of%0A%20%20Local%20Nash%20Interactions&entry.906535625=Zhouyu%20Zhang%20and%20Chih-Yuan%20Chiu%20and%20Glen%20Chou&entry.1292438233=%20%20We%20present%20an%20inverse%20dynamic%20game-based%20algorithm%20to%20learn%20parametric%0Aconstraints%20from%20a%20given%20dataset%20of%20local%20generalized%20Nash%20equilibrium%0Ainteractions%20between%20multiple%20agents.%20Specifically%2C%20we%20introduce%20mixed-integer%0Alinear%20programs%20%28MILP%29%20encoding%20the%20Karush-Kuhn-Tucker%20%28KKT%29%20conditions%20of%20the%0Ainteracting%20agents%2C%20which%20recover%20constraints%20consistent%20with%20the%20Nash%0Astationarity%20of%20the%20interaction%20demonstrations.%20We%20establish%20theoretical%0Aguarantees%20that%20our%20method%20learns%20inner%20approximations%20of%20the%20true%20safe%20and%0Aunsafe%20sets%2C%20as%20well%20as%20limitations%20of%20constraint%20learnability%20from%0Ademonstrations%20of%20Nash%20equilibrium%20interactions.%20We%20also%20use%20the%20interaction%0Aconstraints%20recovered%20by%20our%20method%20to%20design%20motion%20plans%20that%20robustly%0Asatisfy%20the%20underlying%20constraints.%20Across%20simulations%20and%20hardware%0Aexperiments%2C%20our%20methods%20proved%20capable%20of%20inferring%20constraints%20and%20designing%0Ainteractive%20motion%20plans%20for%20various%20classes%20of%20constraints%2C%20both%20convex%20and%0Anon-convex%2C%20from%20interaction%20demonstrations%20of%20agents%20with%20nonlinear%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19945v2&entry.124074799=Read"},
{"title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control", "author": "Yifan Zhang", "abstract": "  Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced.\n", "link": "http://arxiv.org/abs/2508.20784v1", "date": "2025-08-28", "relevancy": 1.5706, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5221}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Agent%20Robust%20Deep%20Reinforcement%20Learning%20for%20Bus%20Fleet%20Control&body=Title%3A%20Single%20Agent%20Robust%20Deep%20Reinforcement%20Learning%20for%20Bus%20Fleet%20Control%0AAuthor%3A%20Yifan%20Zhang%0AAbstract%3A%20%20%20Bus%20bunching%20remains%20a%20challenge%20for%20urban%20transit%20due%20to%20stochastic%20traffic%0Aand%20passenger%20demand.%20Traditional%20solutions%20rely%20on%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20in%20loop-line%20settings%2C%20which%20overlook%20realistic%20operations%0Acharacterized%20by%20heterogeneous%20routes%2C%20timetables%2C%20fluctuating%20demand%2C%20and%0Avarying%20fleet%20sizes.%20We%20propose%20a%20novel%20single-agent%20reinforcement%20learning%0A%28RL%29%20framework%20for%20bus%20holding%20control%20that%20avoids%20the%20data%20imbalance%20and%0Aconvergence%20issues%20of%20MARL%20under%20near-realistic%20simulation.%20A%20bidirectional%0Atimetabled%20network%20with%20dynamic%20passenger%20demand%20is%20constructed.%20The%20key%0Ainnovation%20is%20reformulating%20the%20multi-agent%20problem%20into%20a%20single-agent%20one%20by%0Aaugmenting%20the%20state%20space%20with%20categorical%20identifiers%20%28vehicle%20ID%2C%20station%0AID%2C%20time%20period%29%20in%20addition%20to%20numerical%20features%20%28headway%2C%20occupancy%2C%0Avelocity%29.%20This%20high-dimensional%20encoding%20enables%20single-agent%20policies%20to%0Acapture%20inter-agent%20dependencies%2C%20analogous%20to%20projecting%20non-separable%20inputs%0Ainto%20a%20higher-dimensional%20space.%20We%20further%20design%20a%20structured%20reward%20function%0Aaligned%20with%20operational%20goals%3A%20instead%20of%20exponential%20penalties%20on%20headway%0Adeviations%2C%20a%20ridge-shaped%20reward%20balances%20uniform%20headways%20and%20schedule%0Aadherence.%20Experiments%20show%20that%20our%20modified%20soft%20actor-critic%20%28SAC%29%20achieves%0Amore%20stable%20and%20superior%20performance%20than%20benchmarks%2C%20including%20MADDPG%20%28e.g.%2C%0A-430k%20vs.%20-530k%20under%20stochastic%20conditions%29.%20These%20results%20demonstrate%20that%0Asingle-agent%20deep%20RL%2C%20when%20enhanced%20with%20categorical%20structuring%20and%0Aschedule-aware%20rewards%2C%20can%20effectively%20manage%20bus%20holding%20in%20non-loop%2C%0Areal-world%20contexts.%20This%20paradigm%20offers%20a%20robust%2C%20scalable%20alternative%20to%0AMARL%20frameworks%2C%20particularly%20where%20agent-specific%20experiences%20are%20imbalanced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Agent%2520Robust%2520Deep%2520Reinforcement%2520Learning%2520for%2520Bus%2520Fleet%2520Control%26entry.906535625%3DYifan%2520Zhang%26entry.1292438233%3D%2520%2520Bus%2520bunching%2520remains%2520a%2520challenge%2520for%2520urban%2520transit%2520due%2520to%2520stochastic%2520traffic%250Aand%2520passenger%2520demand.%2520Traditional%2520solutions%2520rely%2520on%2520multi-agent%2520reinforcement%250Alearning%2520%2528MARL%2529%2520in%2520loop-line%2520settings%252C%2520which%2520overlook%2520realistic%2520operations%250Acharacterized%2520by%2520heterogeneous%2520routes%252C%2520timetables%252C%2520fluctuating%2520demand%252C%2520and%250Avarying%2520fleet%2520sizes.%2520We%2520propose%2520a%2520novel%2520single-agent%2520reinforcement%2520learning%250A%2528RL%2529%2520framework%2520for%2520bus%2520holding%2520control%2520that%2520avoids%2520the%2520data%2520imbalance%2520and%250Aconvergence%2520issues%2520of%2520MARL%2520under%2520near-realistic%2520simulation.%2520A%2520bidirectional%250Atimetabled%2520network%2520with%2520dynamic%2520passenger%2520demand%2520is%2520constructed.%2520The%2520key%250Ainnovation%2520is%2520reformulating%2520the%2520multi-agent%2520problem%2520into%2520a%2520single-agent%2520one%2520by%250Aaugmenting%2520the%2520state%2520space%2520with%2520categorical%2520identifiers%2520%2528vehicle%2520ID%252C%2520station%250AID%252C%2520time%2520period%2529%2520in%2520addition%2520to%2520numerical%2520features%2520%2528headway%252C%2520occupancy%252C%250Avelocity%2529.%2520This%2520high-dimensional%2520encoding%2520enables%2520single-agent%2520policies%2520to%250Acapture%2520inter-agent%2520dependencies%252C%2520analogous%2520to%2520projecting%2520non-separable%2520inputs%250Ainto%2520a%2520higher-dimensional%2520space.%2520We%2520further%2520design%2520a%2520structured%2520reward%2520function%250Aaligned%2520with%2520operational%2520goals%253A%2520instead%2520of%2520exponential%2520penalties%2520on%2520headway%250Adeviations%252C%2520a%2520ridge-shaped%2520reward%2520balances%2520uniform%2520headways%2520and%2520schedule%250Aadherence.%2520Experiments%2520show%2520that%2520our%2520modified%2520soft%2520actor-critic%2520%2528SAC%2529%2520achieves%250Amore%2520stable%2520and%2520superior%2520performance%2520than%2520benchmarks%252C%2520including%2520MADDPG%2520%2528e.g.%252C%250A-430k%2520vs.%2520-530k%2520under%2520stochastic%2520conditions%2529.%2520These%2520results%2520demonstrate%2520that%250Asingle-agent%2520deep%2520RL%252C%2520when%2520enhanced%2520with%2520categorical%2520structuring%2520and%250Aschedule-aware%2520rewards%252C%2520can%2520effectively%2520manage%2520bus%2520holding%2520in%2520non-loop%252C%250Areal-world%2520contexts.%2520This%2520paradigm%2520offers%2520a%2520robust%252C%2520scalable%2520alternative%2520to%250AMARL%2520frameworks%252C%2520particularly%2520where%2520agent-specific%2520experiences%2520are%2520imbalanced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Agent%20Robust%20Deep%20Reinforcement%20Learning%20for%20Bus%20Fleet%20Control&entry.906535625=Yifan%20Zhang&entry.1292438233=%20%20Bus%20bunching%20remains%20a%20challenge%20for%20urban%20transit%20due%20to%20stochastic%20traffic%0Aand%20passenger%20demand.%20Traditional%20solutions%20rely%20on%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20in%20loop-line%20settings%2C%20which%20overlook%20realistic%20operations%0Acharacterized%20by%20heterogeneous%20routes%2C%20timetables%2C%20fluctuating%20demand%2C%20and%0Avarying%20fleet%20sizes.%20We%20propose%20a%20novel%20single-agent%20reinforcement%20learning%0A%28RL%29%20framework%20for%20bus%20holding%20control%20that%20avoids%20the%20data%20imbalance%20and%0Aconvergence%20issues%20of%20MARL%20under%20near-realistic%20simulation.%20A%20bidirectional%0Atimetabled%20network%20with%20dynamic%20passenger%20demand%20is%20constructed.%20The%20key%0Ainnovation%20is%20reformulating%20the%20multi-agent%20problem%20into%20a%20single-agent%20one%20by%0Aaugmenting%20the%20state%20space%20with%20categorical%20identifiers%20%28vehicle%20ID%2C%20station%0AID%2C%20time%20period%29%20in%20addition%20to%20numerical%20features%20%28headway%2C%20occupancy%2C%0Avelocity%29.%20This%20high-dimensional%20encoding%20enables%20single-agent%20policies%20to%0Acapture%20inter-agent%20dependencies%2C%20analogous%20to%20projecting%20non-separable%20inputs%0Ainto%20a%20higher-dimensional%20space.%20We%20further%20design%20a%20structured%20reward%20function%0Aaligned%20with%20operational%20goals%3A%20instead%20of%20exponential%20penalties%20on%20headway%0Adeviations%2C%20a%20ridge-shaped%20reward%20balances%20uniform%20headways%20and%20schedule%0Aadherence.%20Experiments%20show%20that%20our%20modified%20soft%20actor-critic%20%28SAC%29%20achieves%0Amore%20stable%20and%20superior%20performance%20than%20benchmarks%2C%20including%20MADDPG%20%28e.g.%2C%0A-430k%20vs.%20-530k%20under%20stochastic%20conditions%29.%20These%20results%20demonstrate%20that%0Asingle-agent%20deep%20RL%2C%20when%20enhanced%20with%20categorical%20structuring%20and%0Aschedule-aware%20rewards%2C%20can%20effectively%20manage%20bus%20holding%20in%20non-loop%2C%0Areal-world%20contexts.%20This%20paradigm%20offers%20a%20robust%2C%20scalable%20alternative%20to%0AMARL%20frameworks%2C%20particularly%20where%20agent-specific%20experiences%20are%20imbalanced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20784v1&entry.124074799=Read"},
{"title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning", "author": "Hao Tan and Jun Lan and Zichang Tan and Ajian Liu and Chuanbiao Song and Senyuan Shi and Huijia Zhu and Weiqiang Wang and Jun Wan and Zhen Lei", "abstract": "  Deepfake detection remains a formidable challenge due to the complex and\nevolving nature of fake content in real-world scenarios. However, existing\nacademic benchmarks suffer from severe discrepancies from industrial practice,\ntypically featuring homogeneous training sources and low-quality testing\nimages, which hinder the practical deployments of current detectors. To\nmitigate this gap, we introduce HydraFake, a dataset that simulates real-world\nchallenges with hierarchical generalization testing. Specifically, HydraFake\ninvolves diversified deepfake techniques and in-the-wild forgeries, along with\nrigorous training and evaluation protocol, covering unseen model architectures,\nemerging forgery techniques and novel data domains. Building on this resource,\nwe propose Veritas, a multi-modal large language model (MLLM) based deepfake\ndetector. Different from vanilla chain-of-thought (CoT), we introduce\npattern-aware reasoning that involves critical reasoning patterns such as\n\"planning\" and \"self-reflection\" to emulate human forensic process. We further\npropose a two-stage training pipeline to seamlessly internalize such deepfake\nreasoning capacities into current MLLMs. Experiments on HydraFake dataset\nreveal that although previous detectors show great generalization on\ncross-model scenarios, they fall short on unseen forgeries and data domains.\nOur Veritas achieves significant gains across different OOD scenarios, and is\ncapable of delivering transparent and faithful detection outputs.\n", "link": "http://arxiv.org/abs/2508.21048v1", "date": "2025-08-28", "relevancy": 1.5259, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5041}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Veritas%3A%20Generalizable%20Deepfake%20Detection%20via%20Pattern-Aware%20Reasoning&body=Title%3A%20Veritas%3A%20Generalizable%20Deepfake%20Detection%20via%20Pattern-Aware%20Reasoning%0AAuthor%3A%20Hao%20Tan%20and%20Jun%20Lan%20and%20Zichang%20Tan%20and%20Ajian%20Liu%20and%20Chuanbiao%20Song%20and%20Senyuan%20Shi%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jun%20Wan%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Deepfake%20detection%20remains%20a%20formidable%20challenge%20due%20to%20the%20complex%20and%0Aevolving%20nature%20of%20fake%20content%20in%20real-world%20scenarios.%20However%2C%20existing%0Aacademic%20benchmarks%20suffer%20from%20severe%20discrepancies%20from%20industrial%20practice%2C%0Atypically%20featuring%20homogeneous%20training%20sources%20and%20low-quality%20testing%0Aimages%2C%20which%20hinder%20the%20practical%20deployments%20of%20current%20detectors.%20To%0Amitigate%20this%20gap%2C%20we%20introduce%20HydraFake%2C%20a%20dataset%20that%20simulates%20real-world%0Achallenges%20with%20hierarchical%20generalization%20testing.%20Specifically%2C%20HydraFake%0Ainvolves%20diversified%20deepfake%20techniques%20and%20in-the-wild%20forgeries%2C%20along%20with%0Arigorous%20training%20and%20evaluation%20protocol%2C%20covering%20unseen%20model%20architectures%2C%0Aemerging%20forgery%20techniques%20and%20novel%20data%20domains.%20Building%20on%20this%20resource%2C%0Awe%20propose%20Veritas%2C%20a%20multi-modal%20large%20language%20model%20%28MLLM%29%20based%20deepfake%0Adetector.%20Different%20from%20vanilla%20chain-of-thought%20%28CoT%29%2C%20we%20introduce%0Apattern-aware%20reasoning%20that%20involves%20critical%20reasoning%20patterns%20such%20as%0A%22planning%22%20and%20%22self-reflection%22%20to%20emulate%20human%20forensic%20process.%20We%20further%0Apropose%20a%20two-stage%20training%20pipeline%20to%20seamlessly%20internalize%20such%20deepfake%0Areasoning%20capacities%20into%20current%20MLLMs.%20Experiments%20on%20HydraFake%20dataset%0Areveal%20that%20although%20previous%20detectors%20show%20great%20generalization%20on%0Across-model%20scenarios%2C%20they%20fall%20short%20on%20unseen%20forgeries%20and%20data%20domains.%0AOur%20Veritas%20achieves%20significant%20gains%20across%20different%20OOD%20scenarios%2C%20and%20is%0Acapable%20of%20delivering%20transparent%20and%20faithful%20detection%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeritas%253A%2520Generalizable%2520Deepfake%2520Detection%2520via%2520Pattern-Aware%2520Reasoning%26entry.906535625%3DHao%2520Tan%2520and%2520Jun%2520Lan%2520and%2520Zichang%2520Tan%2520and%2520Ajian%2520Liu%2520and%2520Chuanbiao%2520Song%2520and%2520Senyuan%2520Shi%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Jun%2520Wan%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520Deepfake%2520detection%2520remains%2520a%2520formidable%2520challenge%2520due%2520to%2520the%2520complex%2520and%250Aevolving%2520nature%2520of%2520fake%2520content%2520in%2520real-world%2520scenarios.%2520However%252C%2520existing%250Aacademic%2520benchmarks%2520suffer%2520from%2520severe%2520discrepancies%2520from%2520industrial%2520practice%252C%250Atypically%2520featuring%2520homogeneous%2520training%2520sources%2520and%2520low-quality%2520testing%250Aimages%252C%2520which%2520hinder%2520the%2520practical%2520deployments%2520of%2520current%2520detectors.%2520To%250Amitigate%2520this%2520gap%252C%2520we%2520introduce%2520HydraFake%252C%2520a%2520dataset%2520that%2520simulates%2520real-world%250Achallenges%2520with%2520hierarchical%2520generalization%2520testing.%2520Specifically%252C%2520HydraFake%250Ainvolves%2520diversified%2520deepfake%2520techniques%2520and%2520in-the-wild%2520forgeries%252C%2520along%2520with%250Arigorous%2520training%2520and%2520evaluation%2520protocol%252C%2520covering%2520unseen%2520model%2520architectures%252C%250Aemerging%2520forgery%2520techniques%2520and%2520novel%2520data%2520domains.%2520Building%2520on%2520this%2520resource%252C%250Awe%2520propose%2520Veritas%252C%2520a%2520multi-modal%2520large%2520language%2520model%2520%2528MLLM%2529%2520based%2520deepfake%250Adetector.%2520Different%2520from%2520vanilla%2520chain-of-thought%2520%2528CoT%2529%252C%2520we%2520introduce%250Apattern-aware%2520reasoning%2520that%2520involves%2520critical%2520reasoning%2520patterns%2520such%2520as%250A%2522planning%2522%2520and%2520%2522self-reflection%2522%2520to%2520emulate%2520human%2520forensic%2520process.%2520We%2520further%250Apropose%2520a%2520two-stage%2520training%2520pipeline%2520to%2520seamlessly%2520internalize%2520such%2520deepfake%250Areasoning%2520capacities%2520into%2520current%2520MLLMs.%2520Experiments%2520on%2520HydraFake%2520dataset%250Areveal%2520that%2520although%2520previous%2520detectors%2520show%2520great%2520generalization%2520on%250Across-model%2520scenarios%252C%2520they%2520fall%2520short%2520on%2520unseen%2520forgeries%2520and%2520data%2520domains.%250AOur%2520Veritas%2520achieves%2520significant%2520gains%2520across%2520different%2520OOD%2520scenarios%252C%2520and%2520is%250Acapable%2520of%2520delivering%2520transparent%2520and%2520faithful%2520detection%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Veritas%3A%20Generalizable%20Deepfake%20Detection%20via%20Pattern-Aware%20Reasoning&entry.906535625=Hao%20Tan%20and%20Jun%20Lan%20and%20Zichang%20Tan%20and%20Ajian%20Liu%20and%20Chuanbiao%20Song%20and%20Senyuan%20Shi%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jun%20Wan%20and%20Zhen%20Lei&entry.1292438233=%20%20Deepfake%20detection%20remains%20a%20formidable%20challenge%20due%20to%20the%20complex%20and%0Aevolving%20nature%20of%20fake%20content%20in%20real-world%20scenarios.%20However%2C%20existing%0Aacademic%20benchmarks%20suffer%20from%20severe%20discrepancies%20from%20industrial%20practice%2C%0Atypically%20featuring%20homogeneous%20training%20sources%20and%20low-quality%20testing%0Aimages%2C%20which%20hinder%20the%20practical%20deployments%20of%20current%20detectors.%20To%0Amitigate%20this%20gap%2C%20we%20introduce%20HydraFake%2C%20a%20dataset%20that%20simulates%20real-world%0Achallenges%20with%20hierarchical%20generalization%20testing.%20Specifically%2C%20HydraFake%0Ainvolves%20diversified%20deepfake%20techniques%20and%20in-the-wild%20forgeries%2C%20along%20with%0Arigorous%20training%20and%20evaluation%20protocol%2C%20covering%20unseen%20model%20architectures%2C%0Aemerging%20forgery%20techniques%20and%20novel%20data%20domains.%20Building%20on%20this%20resource%2C%0Awe%20propose%20Veritas%2C%20a%20multi-modal%20large%20language%20model%20%28MLLM%29%20based%20deepfake%0Adetector.%20Different%20from%20vanilla%20chain-of-thought%20%28CoT%29%2C%20we%20introduce%0Apattern-aware%20reasoning%20that%20involves%20critical%20reasoning%20patterns%20such%20as%0A%22planning%22%20and%20%22self-reflection%22%20to%20emulate%20human%20forensic%20process.%20We%20further%0Apropose%20a%20two-stage%20training%20pipeline%20to%20seamlessly%20internalize%20such%20deepfake%0Areasoning%20capacities%20into%20current%20MLLMs.%20Experiments%20on%20HydraFake%20dataset%0Areveal%20that%20although%20previous%20detectors%20show%20great%20generalization%20on%0Across-model%20scenarios%2C%20they%20fall%20short%20on%20unseen%20forgeries%20and%20data%20domains.%0AOur%20Veritas%20achieves%20significant%20gains%20across%20different%20OOD%20scenarios%2C%20and%20is%0Acapable%20of%20delivering%20transparent%20and%20faithful%20detection%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21048v1&entry.124074799=Read"},
{"title": "Automatic Inspection Based on Switch Sounds of Electric Point Machines", "author": "Ayano Shibata and Toshiki Gunji and Mitsuaki Tsuda and Takashi Endo and Kota Dohi and Tomoya Nishida and Satoko Nomoto", "abstract": "  Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to\nreplace human inspections with IoT-based monitoring. The purpose is\nLabor-saving required for equipment inspections and provide appropriate\npreventive maintenance. As an alternative to visual inspection, it has been\ndifficult to substitute electrical characteristic monitoring, and the\nintroduction of new high-performance sensors has been costly. In 2019, we\nimplemented cameras and microphones in an ``NS'' electric point machines to\nreduce downtime from equipment failures, allowing for remote monitoring of\nlock-piece conditions. This method for detecting turnout switching errors based\non sound information was proposed, and the expected test results were obtained.\nThe proposed method will make it possible to detect equipment failures in real\ntime, thereby reducing the need for visual inspections. This paper presents the\nresults of our technical studies aimed at automating the inspection of\nelectronic point machines using sound, specifically focusing on ``switch\nsound'' beginning in 2019.\n", "link": "http://arxiv.org/abs/2508.20870v1", "date": "2025-08-28", "relevancy": 1.1763, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4097}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3906}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Inspection%20Based%20on%20Switch%20Sounds%20of%20Electric%20Point%20Machines&body=Title%3A%20Automatic%20Inspection%20Based%20on%20Switch%20Sounds%20of%20Electric%20Point%20Machines%0AAuthor%3A%20Ayano%20Shibata%20and%20Toshiki%20Gunji%20and%20Mitsuaki%20Tsuda%20and%20Takashi%20Endo%20and%20Kota%20Dohi%20and%20Tomoya%20Nishida%20and%20Satoko%20Nomoto%0AAbstract%3A%20%20%20Since%202018%2C%20East%20Japan%20Railway%20Company%20and%20Hitachi%2C%20Ltd.%20have%20been%20working%20to%0Areplace%20human%20inspections%20with%20IoT-based%20monitoring.%20The%20purpose%20is%0ALabor-saving%20required%20for%20equipment%20inspections%20and%20provide%20appropriate%0Apreventive%20maintenance.%20As%20an%20alternative%20to%20visual%20inspection%2C%20it%20has%20been%0Adifficult%20to%20substitute%20electrical%20characteristic%20monitoring%2C%20and%20the%0Aintroduction%20of%20new%20high-performance%20sensors%20has%20been%20costly.%20In%202019%2C%20we%0Aimplemented%20cameras%20and%20microphones%20in%20an%20%60%60NS%27%27%20electric%20point%20machines%20to%0Areduce%20downtime%20from%20equipment%20failures%2C%20allowing%20for%20remote%20monitoring%20of%0Alock-piece%20conditions.%20This%20method%20for%20detecting%20turnout%20switching%20errors%20based%0Aon%20sound%20information%20was%20proposed%2C%20and%20the%20expected%20test%20results%20were%20obtained.%0AThe%20proposed%20method%20will%20make%20it%20possible%20to%20detect%20equipment%20failures%20in%20real%0Atime%2C%20thereby%20reducing%20the%20need%20for%20visual%20inspections.%20This%20paper%20presents%20the%0Aresults%20of%20our%20technical%20studies%20aimed%20at%20automating%20the%20inspection%20of%0Aelectronic%20point%20machines%20using%20sound%2C%20specifically%20focusing%20on%20%60%60switch%0Asound%27%27%20beginning%20in%202019.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Inspection%2520Based%2520on%2520Switch%2520Sounds%2520of%2520Electric%2520Point%2520Machines%26entry.906535625%3DAyano%2520Shibata%2520and%2520Toshiki%2520Gunji%2520and%2520Mitsuaki%2520Tsuda%2520and%2520Takashi%2520Endo%2520and%2520Kota%2520Dohi%2520and%2520Tomoya%2520Nishida%2520and%2520Satoko%2520Nomoto%26entry.1292438233%3D%2520%2520Since%25202018%252C%2520East%2520Japan%2520Railway%2520Company%2520and%2520Hitachi%252C%2520Ltd.%2520have%2520been%2520working%2520to%250Areplace%2520human%2520inspections%2520with%2520IoT-based%2520monitoring.%2520The%2520purpose%2520is%250ALabor-saving%2520required%2520for%2520equipment%2520inspections%2520and%2520provide%2520appropriate%250Apreventive%2520maintenance.%2520As%2520an%2520alternative%2520to%2520visual%2520inspection%252C%2520it%2520has%2520been%250Adifficult%2520to%2520substitute%2520electrical%2520characteristic%2520monitoring%252C%2520and%2520the%250Aintroduction%2520of%2520new%2520high-performance%2520sensors%2520has%2520been%2520costly.%2520In%25202019%252C%2520we%250Aimplemented%2520cameras%2520and%2520microphones%2520in%2520an%2520%2560%2560NS%2527%2527%2520electric%2520point%2520machines%2520to%250Areduce%2520downtime%2520from%2520equipment%2520failures%252C%2520allowing%2520for%2520remote%2520monitoring%2520of%250Alock-piece%2520conditions.%2520This%2520method%2520for%2520detecting%2520turnout%2520switching%2520errors%2520based%250Aon%2520sound%2520information%2520was%2520proposed%252C%2520and%2520the%2520expected%2520test%2520results%2520were%2520obtained.%250AThe%2520proposed%2520method%2520will%2520make%2520it%2520possible%2520to%2520detect%2520equipment%2520failures%2520in%2520real%250Atime%252C%2520thereby%2520reducing%2520the%2520need%2520for%2520visual%2520inspections.%2520This%2520paper%2520presents%2520the%250Aresults%2520of%2520our%2520technical%2520studies%2520aimed%2520at%2520automating%2520the%2520inspection%2520of%250Aelectronic%2520point%2520machines%2520using%2520sound%252C%2520specifically%2520focusing%2520on%2520%2560%2560switch%250Asound%2527%2527%2520beginning%2520in%25202019.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Inspection%20Based%20on%20Switch%20Sounds%20of%20Electric%20Point%20Machines&entry.906535625=Ayano%20Shibata%20and%20Toshiki%20Gunji%20and%20Mitsuaki%20Tsuda%20and%20Takashi%20Endo%20and%20Kota%20Dohi%20and%20Tomoya%20Nishida%20and%20Satoko%20Nomoto&entry.1292438233=%20%20Since%202018%2C%20East%20Japan%20Railway%20Company%20and%20Hitachi%2C%20Ltd.%20have%20been%20working%20to%0Areplace%20human%20inspections%20with%20IoT-based%20monitoring.%20The%20purpose%20is%0ALabor-saving%20required%20for%20equipment%20inspections%20and%20provide%20appropriate%0Apreventive%20maintenance.%20As%20an%20alternative%20to%20visual%20inspection%2C%20it%20has%20been%0Adifficult%20to%20substitute%20electrical%20characteristic%20monitoring%2C%20and%20the%0Aintroduction%20of%20new%20high-performance%20sensors%20has%20been%20costly.%20In%202019%2C%20we%0Aimplemented%20cameras%20and%20microphones%20in%20an%20%60%60NS%27%27%20electric%20point%20machines%20to%0Areduce%20downtime%20from%20equipment%20failures%2C%20allowing%20for%20remote%20monitoring%20of%0Alock-piece%20conditions.%20This%20method%20for%20detecting%20turnout%20switching%20errors%20based%0Aon%20sound%20information%20was%20proposed%2C%20and%20the%20expected%20test%20results%20were%20obtained.%0AThe%20proposed%20method%20will%20make%20it%20possible%20to%20detect%20equipment%20failures%20in%20real%0Atime%2C%20thereby%20reducing%20the%20need%20for%20visual%20inspections.%20This%20paper%20presents%20the%0Aresults%20of%20our%20technical%20studies%20aimed%20at%20automating%20the%20inspection%20of%0Aelectronic%20point%20machines%20using%20sound%2C%20specifically%20focusing%20on%20%60%60switch%0Asound%27%27%20beginning%20in%202019.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20870v1&entry.124074799=Read"},
{"title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery", "author": "Junda Wang and Zonghai Yao and Zhichao Yang and Lingxi Li and Junhui Qian and Hong Yu", "abstract": "  Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.\n", "link": "http://arxiv.org/abs/2508.20996v1", "date": "2025-08-28", "relevancy": 1.3803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatThero%3A%20An%20LLM-Supported%20Chatbot%20for%20Behavior%20Change%20and%20Therapeutic%0A%20%20Support%20in%20Addiction%20Recovery&body=Title%3A%20ChatThero%3A%20An%20LLM-Supported%20Chatbot%20for%20Behavior%20Change%20and%20Therapeutic%0A%20%20Support%20in%20Addiction%20Recovery%0AAuthor%3A%20Junda%20Wang%20and%20Zonghai%20Yao%20and%20Zhichao%20Yang%20and%20Lingxi%20Li%20and%20Junhui%20Qian%20and%20Hong%20Yu%0AAbstract%3A%20%20%20Substance%20use%20disorders%20%28SUDs%29%20affect%20over%2036%20million%20people%20worldwide%2C%20yet%0Afew%20receive%20effective%20care%20due%20to%20stigma%2C%20motivational%20barriers%2C%20and%20limited%0Apersonalized%20support.%20Although%20large%20language%20models%20%28LLMs%29%20show%20promise%20for%0Amental-health%20assistance%2C%20most%20systems%20lack%20tight%20integration%20with%20clinically%0Avalidated%20strategies%2C%20reducing%20effectiveness%20in%20addiction%20recovery.%20We%20present%0AChatThero%2C%20a%20multi-agent%20conversational%20framework%20that%20couples%20dynamic%20patient%0Amodeling%20with%20context-sensitive%20therapeutic%20dialogue%20and%20adaptive%20persuasive%0Astrategies%20grounded%20in%20cognitive%20behavioral%20therapy%20%28CBT%29%20and%20motivational%0Ainterviewing%20%28MI%29.%20We%20build%20a%20high-fidelity%20synthetic%20benchmark%20spanning%20Easy%2C%0AMedium%2C%20and%20Hard%20resistance%20levels%2C%20and%20train%20ChatThero%20with%20a%20two-stage%0Apipeline%20comprising%20supervised%20fine-tuning%20%28SFT%29%20followed%20by%20direct%20preference%0Aoptimization%20%28DPO%29.%20In%20evaluation%2C%20ChatThero%20yields%20a%2041.5%5C%25%20average%20gain%20in%0Apatient%20motivation%2C%20a%200.49%5C%25%20increase%20in%20treatment%20confidence%2C%20and%20resolves%0Ahard%20cases%20with%2026%5C%25%20fewer%20turns%20than%20GPT-4o%2C%20and%20both%20automated%20and%20human%0Aclinical%20assessments%20rate%20it%20higher%20in%20empathy%2C%20responsiveness%2C%20and%20behavioral%0Arealism.%20The%20framework%20supports%20rigorous%2C%20privacy-preserving%20study%20of%0Atherapeutic%20conversation%20and%20provides%20a%20robust%2C%20replicable%20basis%20for%20research%0Aand%20clinical%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatThero%253A%2520An%2520LLM-Supported%2520Chatbot%2520for%2520Behavior%2520Change%2520and%2520Therapeutic%250A%2520%2520Support%2520in%2520Addiction%2520Recovery%26entry.906535625%3DJunda%2520Wang%2520and%2520Zonghai%2520Yao%2520and%2520Zhichao%2520Yang%2520and%2520Lingxi%2520Li%2520and%2520Junhui%2520Qian%2520and%2520Hong%2520Yu%26entry.1292438233%3D%2520%2520Substance%2520use%2520disorders%2520%2528SUDs%2529%2520affect%2520over%252036%2520million%2520people%2520worldwide%252C%2520yet%250Afew%2520receive%2520effective%2520care%2520due%2520to%2520stigma%252C%2520motivational%2520barriers%252C%2520and%2520limited%250Apersonalized%2520support.%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520promise%2520for%250Amental-health%2520assistance%252C%2520most%2520systems%2520lack%2520tight%2520integration%2520with%2520clinically%250Avalidated%2520strategies%252C%2520reducing%2520effectiveness%2520in%2520addiction%2520recovery.%2520We%2520present%250AChatThero%252C%2520a%2520multi-agent%2520conversational%2520framework%2520that%2520couples%2520dynamic%2520patient%250Amodeling%2520with%2520context-sensitive%2520therapeutic%2520dialogue%2520and%2520adaptive%2520persuasive%250Astrategies%2520grounded%2520in%2520cognitive%2520behavioral%2520therapy%2520%2528CBT%2529%2520and%2520motivational%250Ainterviewing%2520%2528MI%2529.%2520We%2520build%2520a%2520high-fidelity%2520synthetic%2520benchmark%2520spanning%2520Easy%252C%250AMedium%252C%2520and%2520Hard%2520resistance%2520levels%252C%2520and%2520train%2520ChatThero%2520with%2520a%2520two-stage%250Apipeline%2520comprising%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520followed%2520by%2520direct%2520preference%250Aoptimization%2520%2528DPO%2529.%2520In%2520evaluation%252C%2520ChatThero%2520yields%2520a%252041.5%255C%2525%2520average%2520gain%2520in%250Apatient%2520motivation%252C%2520a%25200.49%255C%2525%2520increase%2520in%2520treatment%2520confidence%252C%2520and%2520resolves%250Ahard%2520cases%2520with%252026%255C%2525%2520fewer%2520turns%2520than%2520GPT-4o%252C%2520and%2520both%2520automated%2520and%2520human%250Aclinical%2520assessments%2520rate%2520it%2520higher%2520in%2520empathy%252C%2520responsiveness%252C%2520and%2520behavioral%250Arealism.%2520The%2520framework%2520supports%2520rigorous%252C%2520privacy-preserving%2520study%2520of%250Atherapeutic%2520conversation%2520and%2520provides%2520a%2520robust%252C%2520replicable%2520basis%2520for%2520research%250Aand%2520clinical%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatThero%3A%20An%20LLM-Supported%20Chatbot%20for%20Behavior%20Change%20and%20Therapeutic%0A%20%20Support%20in%20Addiction%20Recovery&entry.906535625=Junda%20Wang%20and%20Zonghai%20Yao%20and%20Zhichao%20Yang%20and%20Lingxi%20Li%20and%20Junhui%20Qian%20and%20Hong%20Yu&entry.1292438233=%20%20Substance%20use%20disorders%20%28SUDs%29%20affect%20over%2036%20million%20people%20worldwide%2C%20yet%0Afew%20receive%20effective%20care%20due%20to%20stigma%2C%20motivational%20barriers%2C%20and%20limited%0Apersonalized%20support.%20Although%20large%20language%20models%20%28LLMs%29%20show%20promise%20for%0Amental-health%20assistance%2C%20most%20systems%20lack%20tight%20integration%20with%20clinically%0Avalidated%20strategies%2C%20reducing%20effectiveness%20in%20addiction%20recovery.%20We%20present%0AChatThero%2C%20a%20multi-agent%20conversational%20framework%20that%20couples%20dynamic%20patient%0Amodeling%20with%20context-sensitive%20therapeutic%20dialogue%20and%20adaptive%20persuasive%0Astrategies%20grounded%20in%20cognitive%20behavioral%20therapy%20%28CBT%29%20and%20motivational%0Ainterviewing%20%28MI%29.%20We%20build%20a%20high-fidelity%20synthetic%20benchmark%20spanning%20Easy%2C%0AMedium%2C%20and%20Hard%20resistance%20levels%2C%20and%20train%20ChatThero%20with%20a%20two-stage%0Apipeline%20comprising%20supervised%20fine-tuning%20%28SFT%29%20followed%20by%20direct%20preference%0Aoptimization%20%28DPO%29.%20In%20evaluation%2C%20ChatThero%20yields%20a%2041.5%5C%25%20average%20gain%20in%0Apatient%20motivation%2C%20a%200.49%5C%25%20increase%20in%20treatment%20confidence%2C%20and%20resolves%0Ahard%20cases%20with%2026%5C%25%20fewer%20turns%20than%20GPT-4o%2C%20and%20both%20automated%20and%20human%0Aclinical%20assessments%20rate%20it%20higher%20in%20empathy%2C%20responsiveness%2C%20and%20behavioral%0Arealism.%20The%20framework%20supports%20rigorous%2C%20privacy-preserving%20study%20of%0Atherapeutic%20conversation%20and%20provides%20a%20robust%2C%20replicable%20basis%20for%20research%0Aand%20clinical%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20996v1&entry.124074799=Read"},
{"title": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection", "author": "Javier Si Zhao Hong and Timothy Zoe Delaya and Sherwyn Chan Yin Kit and Pai Chet Ng and Xiaoxiao Miao", "abstract": "  This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction.\n", "link": "http://arxiv.org/abs/2508.20805v1", "date": "2025-08-28", "relevancy": 1.5128, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5267}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Machine%20Learning%20and%20Language%20Models%20for%20Multimodal%20Depression%0A%20%20Detection&body=Title%3A%20Exploring%20Machine%20Learning%20and%20Language%20Models%20for%20Multimodal%20Depression%0A%20%20Detection%0AAuthor%3A%20Javier%20Si%20Zhao%20Hong%20and%20Timothy%20Zoe%20Delaya%20and%20Sherwyn%20Chan%20Yin%20Kit%20and%20Pai%20Chet%20Ng%20and%20Xiaoxiao%20Miao%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20approach%20to%20the%20first%20Multimodal%20Personality-Aware%0ADepression%20Detection%20Challenge%2C%20focusing%20on%20multimodal%20depression%20detection%0Ausing%20machine%20learning%20and%20deep%20learning%20models.%20We%20explore%20and%20compare%20the%0Aperformance%20of%20XGBoost%2C%20transformer-based%20architectures%2C%20and%20large%20language%0Amodels%20%28LLMs%29%20on%20audio%2C%20video%2C%20and%20text%20features.%20Our%20results%20highlight%20the%0Astrengths%20and%20limitations%20of%20each%20type%20of%20model%20in%20capturing%20depression-related%0Asignals%20across%20modalities%2C%20offering%20insights%20into%20effective%20multimodal%0Arepresentation%20strategies%20for%20mental%20health%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Machine%2520Learning%2520and%2520Language%2520Models%2520for%2520Multimodal%2520Depression%250A%2520%2520Detection%26entry.906535625%3DJavier%2520Si%2520Zhao%2520Hong%2520and%2520Timothy%2520Zoe%2520Delaya%2520and%2520Sherwyn%2520Chan%2520Yin%2520Kit%2520and%2520Pai%2520Chet%2520Ng%2520and%2520Xiaoxiao%2520Miao%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520approach%2520to%2520the%2520first%2520Multimodal%2520Personality-Aware%250ADepression%2520Detection%2520Challenge%252C%2520focusing%2520on%2520multimodal%2520depression%2520detection%250Ausing%2520machine%2520learning%2520and%2520deep%2520learning%2520models.%2520We%2520explore%2520and%2520compare%2520the%250Aperformance%2520of%2520XGBoost%252C%2520transformer-based%2520architectures%252C%2520and%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520on%2520audio%252C%2520video%252C%2520and%2520text%2520features.%2520Our%2520results%2520highlight%2520the%250Astrengths%2520and%2520limitations%2520of%2520each%2520type%2520of%2520model%2520in%2520capturing%2520depression-related%250Asignals%2520across%2520modalities%252C%2520offering%2520insights%2520into%2520effective%2520multimodal%250Arepresentation%2520strategies%2520for%2520mental%2520health%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Machine%20Learning%20and%20Language%20Models%20for%20Multimodal%20Depression%0A%20%20Detection&entry.906535625=Javier%20Si%20Zhao%20Hong%20and%20Timothy%20Zoe%20Delaya%20and%20Sherwyn%20Chan%20Yin%20Kit%20and%20Pai%20Chet%20Ng%20and%20Xiaoxiao%20Miao&entry.1292438233=%20%20This%20paper%20presents%20our%20approach%20to%20the%20first%20Multimodal%20Personality-Aware%0ADepression%20Detection%20Challenge%2C%20focusing%20on%20multimodal%20depression%20detection%0Ausing%20machine%20learning%20and%20deep%20learning%20models.%20We%20explore%20and%20compare%20the%0Aperformance%20of%20XGBoost%2C%20transformer-based%20architectures%2C%20and%20large%20language%0Amodels%20%28LLMs%29%20on%20audio%2C%20video%2C%20and%20text%20features.%20Our%20results%20highlight%20the%0Astrengths%20and%20limitations%20of%20each%20type%20of%20model%20in%20capturing%20depression-related%0Asignals%20across%20modalities%2C%20offering%20insights%20into%20effective%20multimodal%0Arepresentation%20strategies%20for%20mental%20health%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20805v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


