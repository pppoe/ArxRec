<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251030.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "The Impact and Outlook of 3D Gaussian Splatting", "author": "Bernhard Kerbl", "abstract": "  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.\n", "link": "http://arxiv.org/abs/2510.26694v1", "date": "2025-10-30", "relevancy": 3.3792, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6935}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6784}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20and%20Outlook%20of%203D%20Gaussian%20Splatting&body=Title%3A%20The%20Impact%20and%20Outlook%20of%203D%20Gaussian%20Splatting%0AAuthor%3A%20Bernhard%20Kerbl%0AAbstract%3A%20%20%20Since%20its%20introduction%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20rapidly%20transformed%0Athe%20landscape%20of%203D%20scene%20representations%2C%20inspiring%20an%20extensive%20body%20of%0Aassociated%20research.%20Follow-up%20work%20includes%20analyses%20and%20contributions%20that%0Aenhance%20the%20efficiency%2C%20scalability%2C%20and%20real-world%20applicability%20of%203DGS.%20In%0Athis%20summary%2C%20we%20present%20an%20overview%20of%20several%20key%20directions%20that%20have%0Aemerged%20in%20the%20wake%20of%203DGS.%20We%20highlight%20advances%20enabling%20resource-efficient%0Atraining%20and%20rendering%2C%20the%20evolution%20toward%20dynamic%20%28or%20four-dimensional%2C%0A4DGS%29%20representations%2C%20and%20deeper%20exploration%20of%20the%20mathematical%20foundations%0Aunderlying%20its%20appearance%20modeling%20and%20rendering%20process.%20Furthermore%2C%20we%0Aexamine%20efforts%20to%20bring%203DGS%20to%20mobile%20and%20virtual%20reality%20platforms%2C%20its%0Aextension%20to%20massive-scale%20environments%2C%20and%20recent%20progress%20toward%0Anear-instant%20radiance%20field%20reconstruction%20via%20feed-forward%20or%20distributed%0Acomputation.%20Collectively%2C%20these%20developments%20illustrate%20how%203DGS%20has%20evolved%0Afrom%20a%20breakthrough%20representation%20into%20a%20versatile%20and%20foundational%20tool%20for%0A3D%20vision%20and%20graphics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520and%2520Outlook%2520of%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DBernhard%2520Kerbl%26entry.1292438233%3D%2520%2520Since%2520its%2520introduction%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520rapidly%2520transformed%250Athe%2520landscape%2520of%25203D%2520scene%2520representations%252C%2520inspiring%2520an%2520extensive%2520body%2520of%250Aassociated%2520research.%2520Follow-up%2520work%2520includes%2520analyses%2520and%2520contributions%2520that%250Aenhance%2520the%2520efficiency%252C%2520scalability%252C%2520and%2520real-world%2520applicability%2520of%25203DGS.%2520In%250Athis%2520summary%252C%2520we%2520present%2520an%2520overview%2520of%2520several%2520key%2520directions%2520that%2520have%250Aemerged%2520in%2520the%2520wake%2520of%25203DGS.%2520We%2520highlight%2520advances%2520enabling%2520resource-efficient%250Atraining%2520and%2520rendering%252C%2520the%2520evolution%2520toward%2520dynamic%2520%2528or%2520four-dimensional%252C%250A4DGS%2529%2520representations%252C%2520and%2520deeper%2520exploration%2520of%2520the%2520mathematical%2520foundations%250Aunderlying%2520its%2520appearance%2520modeling%2520and%2520rendering%2520process.%2520Furthermore%252C%2520we%250Aexamine%2520efforts%2520to%2520bring%25203DGS%2520to%2520mobile%2520and%2520virtual%2520reality%2520platforms%252C%2520its%250Aextension%2520to%2520massive-scale%2520environments%252C%2520and%2520recent%2520progress%2520toward%250Anear-instant%2520radiance%2520field%2520reconstruction%2520via%2520feed-forward%2520or%2520distributed%250Acomputation.%2520Collectively%252C%2520these%2520developments%2520illustrate%2520how%25203DGS%2520has%2520evolved%250Afrom%2520a%2520breakthrough%2520representation%2520into%2520a%2520versatile%2520and%2520foundational%2520tool%2520for%250A3D%2520vision%2520and%2520graphics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20and%20Outlook%20of%203D%20Gaussian%20Splatting&entry.906535625=Bernhard%20Kerbl&entry.1292438233=%20%20Since%20its%20introduction%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20rapidly%20transformed%0Athe%20landscape%20of%203D%20scene%20representations%2C%20inspiring%20an%20extensive%20body%20of%0Aassociated%20research.%20Follow-up%20work%20includes%20analyses%20and%20contributions%20that%0Aenhance%20the%20efficiency%2C%20scalability%2C%20and%20real-world%20applicability%20of%203DGS.%20In%0Athis%20summary%2C%20we%20present%20an%20overview%20of%20several%20key%20directions%20that%20have%0Aemerged%20in%20the%20wake%20of%203DGS.%20We%20highlight%20advances%20enabling%20resource-efficient%0Atraining%20and%20rendering%2C%20the%20evolution%20toward%20dynamic%20%28or%20four-dimensional%2C%0A4DGS%29%20representations%2C%20and%20deeper%20exploration%20of%20the%20mathematical%20foundations%0Aunderlying%20its%20appearance%20modeling%20and%20rendering%20process.%20Furthermore%2C%20we%0Aexamine%20efforts%20to%20bring%203DGS%20to%20mobile%20and%20virtual%20reality%20platforms%2C%20its%0Aextension%20to%20massive-scale%20environments%2C%20and%20recent%20progress%20toward%0Anear-instant%20radiance%20field%20reconstruction%20via%20feed-forward%20or%20distributed%0Acomputation.%20Collectively%2C%20these%20developments%20illustrate%20how%203DGS%20has%20evolved%0Afrom%20a%20breakthrough%20representation%20into%20a%20versatile%20and%20foundational%20tool%20for%0A3D%20vision%20and%20graphics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26694v1&entry.124074799=Read"},
{"title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation", "author": "Jing Lin and Ruisi Wang and Junzhe Lu and Ziqi Huang and Guorui Song and Ailing Zeng and Xian Liu and Chen Wei and Wanqi Yin and Qingping Sun and Zhongang Cai and Lei Yang and Ziwei Liu", "abstract": "  Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.\n", "link": "http://arxiv.org/abs/2510.26794v1", "date": "2025-10-30", "relevancy": 3.0968, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6447}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6072}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Quest%20for%20Generalizable%20Motion%20Generation%3A%20Data%2C%20Model%2C%20and%0A%20%20Evaluation&body=Title%3A%20The%20Quest%20for%20Generalizable%20Motion%20Generation%3A%20Data%2C%20Model%2C%20and%0A%20%20Evaluation%0AAuthor%3A%20Jing%20Lin%20and%20Ruisi%20Wang%20and%20Junzhe%20Lu%20and%20Ziqi%20Huang%20and%20Guorui%20Song%20and%20Ailing%20Zeng%20and%20Xian%20Liu%20and%20Chen%20Wei%20and%20Wanqi%20Yin%20and%20Qingping%20Sun%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%203D%20human%20motion%20generation%20%28MoGen%29%20on%20standard%0Abenchmarks%2C%20existing%20models%20still%20face%20a%20fundamental%20bottleneck%20in%20their%0Ageneralization%20capability.%20In%20contrast%2C%20adjacent%20generative%20fields%2C%20most%0Anotably%20video%20generation%20%28ViGen%29%2C%20have%20demonstrated%20remarkable%20generalization%0Ain%20modeling%20human%20behaviors%2C%20highlighting%20transferable%20insights%20that%20MoGen%20can%0Aleverage.%20Motivated%20by%20this%20observation%2C%20we%20present%20a%20comprehensive%20framework%0Athat%20systematically%20transfers%20knowledge%20from%20ViGen%20to%20MoGen%20across%20three%20key%0Apillars%3A%20data%2C%20modeling%2C%20and%20evaluation.%20First%2C%20we%20introduce%20ViMoGen-228K%2C%20a%0Alarge-scale%20dataset%20comprising%20228%2C000%20high-quality%20motion%20samples%20that%0Aintegrates%20high-fidelity%20optical%20MoCap%20data%20with%20semantically%20annotated%20motions%0Afrom%20web%20videos%20and%20synthesized%20samples%20generated%20by%20state-of-the-art%20ViGen%0Amodels.%20The%20dataset%20includes%20both%20text-motion%20pairs%20and%20text-video-motion%0Atriplets%2C%20substantially%20expanding%20semantic%20diversity.%20Second%2C%20we%20propose%0AViMoGen%2C%20a%20flow-matching-based%20diffusion%20transformer%20that%20unifies%20priors%20from%0AMoCap%20data%20and%20ViGen%20models%20through%20gated%20multimodal%20conditioning.%20To%20enhance%0Aefficiency%2C%20we%20further%20develop%20ViMoGen-light%2C%20a%20distilled%20variant%20that%0Aeliminates%20video%20generation%20dependencies%20while%20preserving%20strong%0Ageneralization.%20Finally%2C%20we%20present%20MBench%2C%20a%20hierarchical%20benchmark%20designed%0Afor%20fine-grained%20evaluation%20across%20motion%20quality%2C%20prompt%20fidelity%2C%20and%0Ageneralization%20ability.%20Extensive%20experiments%20show%20that%20our%20framework%0Asignificantly%20outperforms%20existing%20approaches%20in%20both%20automatic%20and%20human%0Aevaluations.%20The%20code%2C%20data%2C%20and%20benchmark%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Quest%2520for%2520Generalizable%2520Motion%2520Generation%253A%2520Data%252C%2520Model%252C%2520and%250A%2520%2520Evaluation%26entry.906535625%3DJing%2520Lin%2520and%2520Ruisi%2520Wang%2520and%2520Junzhe%2520Lu%2520and%2520Ziqi%2520Huang%2520and%2520Guorui%2520Song%2520and%2520Ailing%2520Zeng%2520and%2520Xian%2520Liu%2520and%2520Chen%2520Wei%2520and%2520Wanqi%2520Yin%2520and%2520Qingping%2520Sun%2520and%2520Zhongang%2520Cai%2520and%2520Lei%2520Yang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%25203D%2520human%2520motion%2520generation%2520%2528MoGen%2529%2520on%2520standard%250Abenchmarks%252C%2520existing%2520models%2520still%2520face%2520a%2520fundamental%2520bottleneck%2520in%2520their%250Ageneralization%2520capability.%2520In%2520contrast%252C%2520adjacent%2520generative%2520fields%252C%2520most%250Anotably%2520video%2520generation%2520%2528ViGen%2529%252C%2520have%2520demonstrated%2520remarkable%2520generalization%250Ain%2520modeling%2520human%2520behaviors%252C%2520highlighting%2520transferable%2520insights%2520that%2520MoGen%2520can%250Aleverage.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520present%2520a%2520comprehensive%2520framework%250Athat%2520systematically%2520transfers%2520knowledge%2520from%2520ViGen%2520to%2520MoGen%2520across%2520three%2520key%250Apillars%253A%2520data%252C%2520modeling%252C%2520and%2520evaluation.%2520First%252C%2520we%2520introduce%2520ViMoGen-228K%252C%2520a%250Alarge-scale%2520dataset%2520comprising%2520228%252C000%2520high-quality%2520motion%2520samples%2520that%250Aintegrates%2520high-fidelity%2520optical%2520MoCap%2520data%2520with%2520semantically%2520annotated%2520motions%250Afrom%2520web%2520videos%2520and%2520synthesized%2520samples%2520generated%2520by%2520state-of-the-art%2520ViGen%250Amodels.%2520The%2520dataset%2520includes%2520both%2520text-motion%2520pairs%2520and%2520text-video-motion%250Atriplets%252C%2520substantially%2520expanding%2520semantic%2520diversity.%2520Second%252C%2520we%2520propose%250AViMoGen%252C%2520a%2520flow-matching-based%2520diffusion%2520transformer%2520that%2520unifies%2520priors%2520from%250AMoCap%2520data%2520and%2520ViGen%2520models%2520through%2520gated%2520multimodal%2520conditioning.%2520To%2520enhance%250Aefficiency%252C%2520we%2520further%2520develop%2520ViMoGen-light%252C%2520a%2520distilled%2520variant%2520that%250Aeliminates%2520video%2520generation%2520dependencies%2520while%2520preserving%2520strong%250Ageneralization.%2520Finally%252C%2520we%2520present%2520MBench%252C%2520a%2520hierarchical%2520benchmark%2520designed%250Afor%2520fine-grained%2520evaluation%2520across%2520motion%2520quality%252C%2520prompt%2520fidelity%252C%2520and%250Ageneralization%2520ability.%2520Extensive%2520experiments%2520show%2520that%2520our%2520framework%250Asignificantly%2520outperforms%2520existing%2520approaches%2520in%2520both%2520automatic%2520and%2520human%250Aevaluations.%2520The%2520code%252C%2520data%252C%2520and%2520benchmark%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Quest%20for%20Generalizable%20Motion%20Generation%3A%20Data%2C%20Model%2C%20and%0A%20%20Evaluation&entry.906535625=Jing%20Lin%20and%20Ruisi%20Wang%20and%20Junzhe%20Lu%20and%20Ziqi%20Huang%20and%20Guorui%20Song%20and%20Ailing%20Zeng%20and%20Xian%20Liu%20and%20Chen%20Wei%20and%20Wanqi%20Yin%20and%20Qingping%20Sun%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Ziwei%20Liu&entry.1292438233=%20%20Despite%20recent%20advances%20in%203D%20human%20motion%20generation%20%28MoGen%29%20on%20standard%0Abenchmarks%2C%20existing%20models%20still%20face%20a%20fundamental%20bottleneck%20in%20their%0Ageneralization%20capability.%20In%20contrast%2C%20adjacent%20generative%20fields%2C%20most%0Anotably%20video%20generation%20%28ViGen%29%2C%20have%20demonstrated%20remarkable%20generalization%0Ain%20modeling%20human%20behaviors%2C%20highlighting%20transferable%20insights%20that%20MoGen%20can%0Aleverage.%20Motivated%20by%20this%20observation%2C%20we%20present%20a%20comprehensive%20framework%0Athat%20systematically%20transfers%20knowledge%20from%20ViGen%20to%20MoGen%20across%20three%20key%0Apillars%3A%20data%2C%20modeling%2C%20and%20evaluation.%20First%2C%20we%20introduce%20ViMoGen-228K%2C%20a%0Alarge-scale%20dataset%20comprising%20228%2C000%20high-quality%20motion%20samples%20that%0Aintegrates%20high-fidelity%20optical%20MoCap%20data%20with%20semantically%20annotated%20motions%0Afrom%20web%20videos%20and%20synthesized%20samples%20generated%20by%20state-of-the-art%20ViGen%0Amodels.%20The%20dataset%20includes%20both%20text-motion%20pairs%20and%20text-video-motion%0Atriplets%2C%20substantially%20expanding%20semantic%20diversity.%20Second%2C%20we%20propose%0AViMoGen%2C%20a%20flow-matching-based%20diffusion%20transformer%20that%20unifies%20priors%20from%0AMoCap%20data%20and%20ViGen%20models%20through%20gated%20multimodal%20conditioning.%20To%20enhance%0Aefficiency%2C%20we%20further%20develop%20ViMoGen-light%2C%20a%20distilled%20variant%20that%0Aeliminates%20video%20generation%20dependencies%20while%20preserving%20strong%0Ageneralization.%20Finally%2C%20we%20present%20MBench%2C%20a%20hierarchical%20benchmark%20designed%0Afor%20fine-grained%20evaluation%20across%20motion%20quality%2C%20prompt%20fidelity%2C%20and%0Ageneralization%20ability.%20Extensive%20experiments%20show%20that%20our%20framework%0Asignificantly%20outperforms%20existing%20approaches%20in%20both%20automatic%20and%20human%0Aevaluations.%20The%20code%2C%20data%2C%20and%20benchmark%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26794v1&entry.124074799=Read"},
{"title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis", "author": "Shiyu Liu and Kui Jiang and Xianming Liu and Hongxun Yao and Xiaocheng Feng", "abstract": "  Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.\n", "link": "http://arxiv.org/abs/2508.10566v2", "date": "2025-10-30", "relevancy": 3.068, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6395}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6007}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis&body=Title%3A%20HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis%0AAuthor%3A%20Shiyu%20Liu%20and%20Kui%20Jiang%20and%20Xianming%20Liu%20and%20Hongxun%20Yao%20and%20Xiaocheng%20Feng%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20video%20generation%20enhances%20user%20engagement%20in%0Ahuman-computer%20interaction.%20However%2C%20current%20methods%20frequently%20produce%20videos%0Awith%20motion%20blur%20and%20lip%20jitter%2C%20primarily%20due%20to%20their%20reliance%20on%20implicit%0Amodeling%20of%20audio-facial%20motion%20correlations--an%20approach%20lacking%20explicit%0Aarticulatory%20priors%20%28i.e.%2C%20anatomical%20guidance%20for%20speech-related%20facial%0Amovements%29.%20To%20overcome%20this%20limitation%2C%20we%20propose%20HM-Talker%2C%20a%20novel%0Aframework%20for%20generating%20high-fidelity%2C%20temporally%20coherent%20talking%20heads.%0AHM-Talker%20leverages%20a%20hybrid%20motion%20representation%20combining%20both%20implicit%20and%0Aexplicit%20motion%20cues.%20Explicit%20cues%20use%20Action%20Units%20%28AUs%29%2C%20anatomically%0Adefined%20facial%20muscle%20movements%2C%20alongside%20implicit%20features%20to%20minimize%0Aphoneme-viseme%20misalignment.%20Specifically%2C%20our%20Cross-Modal%20Disentanglement%0AModule%20%28CMDM%29%20extracts%20complementary%20implicit/explicit%20motion%20features%20while%0Apredicting%20AUs%20directly%20from%20audio%20input%20aligned%20to%20visual%20cues.%20To%20mitigate%0Aidentity-dependent%20biases%20in%20explicit%20features%20and%20enhance%20cross-subject%0Ageneralization%2C%20we%20introduce%20the%20Hybrid%20Motion%20Modeling%20Module%20%28HMMM%29.%20This%0Amodule%20dynamically%20merges%20randomly%20paired%20implicit/explicit%20features%2C%20enforcing%0Aidentity-agnostic%20learning.%20Together%2C%20these%20components%20enable%20robust%20lip%0Asynchronization%20across%20diverse%20identities%2C%20advancing%20personalized%20talking%20head%0Asynthesis.%20Extensive%20experiments%20demonstrate%20HM-Talker%27s%20superiority%20over%0Astate-of-the-art%20methods%20in%20visual%20quality%20and%20lip-sync%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10566v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHM-Talker%253A%2520Hybrid%2520Motion%2520Modeling%2520for%2520High-Fidelity%2520Talking%2520Head%250A%2520%2520Synthesis%26entry.906535625%3DShiyu%2520Liu%2520and%2520Kui%2520Jiang%2520and%2520Xianming%2520Liu%2520and%2520Hongxun%2520Yao%2520and%2520Xiaocheng%2520Feng%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520video%2520generation%2520enhances%2520user%2520engagement%2520in%250Ahuman-computer%2520interaction.%2520However%252C%2520current%2520methods%2520frequently%2520produce%2520videos%250Awith%2520motion%2520blur%2520and%2520lip%2520jitter%252C%2520primarily%2520due%2520to%2520their%2520reliance%2520on%2520implicit%250Amodeling%2520of%2520audio-facial%2520motion%2520correlations--an%2520approach%2520lacking%2520explicit%250Aarticulatory%2520priors%2520%2528i.e.%252C%2520anatomical%2520guidance%2520for%2520speech-related%2520facial%250Amovements%2529.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520HM-Talker%252C%2520a%2520novel%250Aframework%2520for%2520generating%2520high-fidelity%252C%2520temporally%2520coherent%2520talking%2520heads.%250AHM-Talker%2520leverages%2520a%2520hybrid%2520motion%2520representation%2520combining%2520both%2520implicit%2520and%250Aexplicit%2520motion%2520cues.%2520Explicit%2520cues%2520use%2520Action%2520Units%2520%2528AUs%2529%252C%2520anatomically%250Adefined%2520facial%2520muscle%2520movements%252C%2520alongside%2520implicit%2520features%2520to%2520minimize%250Aphoneme-viseme%2520misalignment.%2520Specifically%252C%2520our%2520Cross-Modal%2520Disentanglement%250AModule%2520%2528CMDM%2529%2520extracts%2520complementary%2520implicit/explicit%2520motion%2520features%2520while%250Apredicting%2520AUs%2520directly%2520from%2520audio%2520input%2520aligned%2520to%2520visual%2520cues.%2520To%2520mitigate%250Aidentity-dependent%2520biases%2520in%2520explicit%2520features%2520and%2520enhance%2520cross-subject%250Ageneralization%252C%2520we%2520introduce%2520the%2520Hybrid%2520Motion%2520Modeling%2520Module%2520%2528HMMM%2529.%2520This%250Amodule%2520dynamically%2520merges%2520randomly%2520paired%2520implicit/explicit%2520features%252C%2520enforcing%250Aidentity-agnostic%2520learning.%2520Together%252C%2520these%2520components%2520enable%2520robust%2520lip%250Asynchronization%2520across%2520diverse%2520identities%252C%2520advancing%2520personalized%2520talking%2520head%250Asynthesis.%2520Extensive%2520experiments%2520demonstrate%2520HM-Talker%2527s%2520superiority%2520over%250Astate-of-the-art%2520methods%2520in%2520visual%2520quality%2520and%2520lip-sync%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10566v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis&entry.906535625=Shiyu%20Liu%20and%20Kui%20Jiang%20and%20Xianming%20Liu%20and%20Hongxun%20Yao%20and%20Xiaocheng%20Feng&entry.1292438233=%20%20Audio-driven%20talking%20head%20video%20generation%20enhances%20user%20engagement%20in%0Ahuman-computer%20interaction.%20However%2C%20current%20methods%20frequently%20produce%20videos%0Awith%20motion%20blur%20and%20lip%20jitter%2C%20primarily%20due%20to%20their%20reliance%20on%20implicit%0Amodeling%20of%20audio-facial%20motion%20correlations--an%20approach%20lacking%20explicit%0Aarticulatory%20priors%20%28i.e.%2C%20anatomical%20guidance%20for%20speech-related%20facial%0Amovements%29.%20To%20overcome%20this%20limitation%2C%20we%20propose%20HM-Talker%2C%20a%20novel%0Aframework%20for%20generating%20high-fidelity%2C%20temporally%20coherent%20talking%20heads.%0AHM-Talker%20leverages%20a%20hybrid%20motion%20representation%20combining%20both%20implicit%20and%0Aexplicit%20motion%20cues.%20Explicit%20cues%20use%20Action%20Units%20%28AUs%29%2C%20anatomically%0Adefined%20facial%20muscle%20movements%2C%20alongside%20implicit%20features%20to%20minimize%0Aphoneme-viseme%20misalignment.%20Specifically%2C%20our%20Cross-Modal%20Disentanglement%0AModule%20%28CMDM%29%20extracts%20complementary%20implicit/explicit%20motion%20features%20while%0Apredicting%20AUs%20directly%20from%20audio%20input%20aligned%20to%20visual%20cues.%20To%20mitigate%0Aidentity-dependent%20biases%20in%20explicit%20features%20and%20enhance%20cross-subject%0Ageneralization%2C%20we%20introduce%20the%20Hybrid%20Motion%20Modeling%20Module%20%28HMMM%29.%20This%0Amodule%20dynamically%20merges%20randomly%20paired%20implicit/explicit%20features%2C%20enforcing%0Aidentity-agnostic%20learning.%20Together%2C%20these%20components%20enable%20robust%20lip%0Asynchronization%20across%20diverse%20identities%2C%20advancing%20personalized%20talking%20head%0Asynthesis.%20Extensive%20experiments%20demonstrate%20HM-Talker%27s%20superiority%20over%0Astate-of-the-art%20methods%20in%20visual%20quality%20and%20lip-sync%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10566v2&entry.124074799=Read"},
{"title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark", "author": "Ziyu Guo and Xinyan Chen and Renrui Zhang and Ruichuan An and Yu Qi and Dongzhi Jiang and Xiangtai Li and Manyuan Zhang and Hongsheng Li and Pheng-Ann Heng", "abstract": "  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n", "link": "http://arxiv.org/abs/2510.26802v1", "date": "2025-10-30", "relevancy": 2.9749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6227}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Video%20Models%20Ready%20as%20Zero-Shot%20Reasoners%3F%20An%20Empirical%20Study%20with%0A%20%20the%20MME-CoF%20Benchmark&body=Title%3A%20Are%20Video%20Models%20Ready%20as%20Zero-Shot%20Reasoners%3F%20An%20Empirical%20Study%20with%0A%20%20the%20MME-CoF%20Benchmark%0AAuthor%3A%20Ziyu%20Guo%20and%20Xinyan%20Chen%20and%20Renrui%20Zhang%20and%20Ruichuan%20An%20and%20Yu%20Qi%20and%20Dongzhi%20Jiang%20and%20Xiangtai%20Li%20and%20Manyuan%20Zhang%20and%20Hongsheng%20Li%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20Recent%20video%20generation%20models%20can%20produce%20high-fidelity%2C%20temporally%20coherent%0Avideos%2C%20indicating%20that%20they%20may%20encode%20substantial%20world%20knowledge.%20Beyond%0Arealistic%20synthesis%2C%20they%20also%20exhibit%20emerging%20behaviors%20indicative%20of%20visual%0Aperception%2C%20modeling%2C%20and%20manipulation.%20Yet%2C%20an%20important%20question%20still%0Aremains%3A%20Are%20video%20models%20ready%20to%20serve%20as%20zero-shot%20reasoners%20in%20challenging%0Avisual%20reasoning%20scenarios%3F%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20study%20to%0Acomprehensively%20investigate%20this%20question%2C%20focusing%20on%20the%20leading%20and%20popular%0AVeo-3.%20We%20evaluate%20its%20reasoning%20behavior%20across%2012%20dimensions%2C%20including%0Aspatial%2C%20geometric%2C%20physical%2C%20temporal%2C%20and%20embodied%20logic%2C%20systematically%0Acharacterizing%20both%20its%20strengths%20and%20failure%20modes.%20To%20standardize%20this%20study%2C%0Awe%20curate%20the%20evaluation%20data%20into%20MME-CoF%2C%20a%20compact%20benchmark%20that%20enables%0Ain-depth%20and%20thorough%20assessment%20of%20Chain-of-Frame%20%28CoF%29%20reasoning.%20Our%0Afindings%20reveal%20that%20while%20current%20video%20models%20demonstrate%20promising%20reasoning%0Apatterns%20on%20short-horizon%20spatial%20coherence%2C%20fine-grained%20grounding%2C%20and%0Alocally%20consistent%20dynamics%2C%20they%20remain%20limited%20in%20long-horizon%20causal%0Areasoning%2C%20strict%20geometric%20constraints%2C%20and%20abstract%20logic.%20Overall%2C%20they%20are%0Anot%20yet%20reliable%20as%20standalone%20zero-shot%20reasoners%2C%20but%20exhibit%20encouraging%0Asigns%20as%20complementary%20visual%20engines%20alongside%20dedicated%20reasoning%20models.%0AProject%20page%3A%20https%3A//video-cof.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Video%2520Models%2520Ready%2520as%2520Zero-Shot%2520Reasoners%253F%2520An%2520Empirical%2520Study%2520with%250A%2520%2520the%2520MME-CoF%2520Benchmark%26entry.906535625%3DZiyu%2520Guo%2520and%2520Xinyan%2520Chen%2520and%2520Renrui%2520Zhang%2520and%2520Ruichuan%2520An%2520and%2520Yu%2520Qi%2520and%2520Dongzhi%2520Jiang%2520and%2520Xiangtai%2520Li%2520and%2520Manyuan%2520Zhang%2520and%2520Hongsheng%2520Li%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520Recent%2520video%2520generation%2520models%2520can%2520produce%2520high-fidelity%252C%2520temporally%2520coherent%250Avideos%252C%2520indicating%2520that%2520they%2520may%2520encode%2520substantial%2520world%2520knowledge.%2520Beyond%250Arealistic%2520synthesis%252C%2520they%2520also%2520exhibit%2520emerging%2520behaviors%2520indicative%2520of%2520visual%250Aperception%252C%2520modeling%252C%2520and%2520manipulation.%2520Yet%252C%2520an%2520important%2520question%2520still%250Aremains%253A%2520Are%2520video%2520models%2520ready%2520to%2520serve%2520as%2520zero-shot%2520reasoners%2520in%2520challenging%250Avisual%2520reasoning%2520scenarios%253F%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520to%250Acomprehensively%2520investigate%2520this%2520question%252C%2520focusing%2520on%2520the%2520leading%2520and%2520popular%250AVeo-3.%2520We%2520evaluate%2520its%2520reasoning%2520behavior%2520across%252012%2520dimensions%252C%2520including%250Aspatial%252C%2520geometric%252C%2520physical%252C%2520temporal%252C%2520and%2520embodied%2520logic%252C%2520systematically%250Acharacterizing%2520both%2520its%2520strengths%2520and%2520failure%2520modes.%2520To%2520standardize%2520this%2520study%252C%250Awe%2520curate%2520the%2520evaluation%2520data%2520into%2520MME-CoF%252C%2520a%2520compact%2520benchmark%2520that%2520enables%250Ain-depth%2520and%2520thorough%2520assessment%2520of%2520Chain-of-Frame%2520%2528CoF%2529%2520reasoning.%2520Our%250Afindings%2520reveal%2520that%2520while%2520current%2520video%2520models%2520demonstrate%2520promising%2520reasoning%250Apatterns%2520on%2520short-horizon%2520spatial%2520coherence%252C%2520fine-grained%2520grounding%252C%2520and%250Alocally%2520consistent%2520dynamics%252C%2520they%2520remain%2520limited%2520in%2520long-horizon%2520causal%250Areasoning%252C%2520strict%2520geometric%2520constraints%252C%2520and%2520abstract%2520logic.%2520Overall%252C%2520they%2520are%250Anot%2520yet%2520reliable%2520as%2520standalone%2520zero-shot%2520reasoners%252C%2520but%2520exhibit%2520encouraging%250Asigns%2520as%2520complementary%2520visual%2520engines%2520alongside%2520dedicated%2520reasoning%2520models.%250AProject%2520page%253A%2520https%253A//video-cof.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Video%20Models%20Ready%20as%20Zero-Shot%20Reasoners%3F%20An%20Empirical%20Study%20with%0A%20%20the%20MME-CoF%20Benchmark&entry.906535625=Ziyu%20Guo%20and%20Xinyan%20Chen%20and%20Renrui%20Zhang%20and%20Ruichuan%20An%20and%20Yu%20Qi%20and%20Dongzhi%20Jiang%20and%20Xiangtai%20Li%20and%20Manyuan%20Zhang%20and%20Hongsheng%20Li%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20Recent%20video%20generation%20models%20can%20produce%20high-fidelity%2C%20temporally%20coherent%0Avideos%2C%20indicating%20that%20they%20may%20encode%20substantial%20world%20knowledge.%20Beyond%0Arealistic%20synthesis%2C%20they%20also%20exhibit%20emerging%20behaviors%20indicative%20of%20visual%0Aperception%2C%20modeling%2C%20and%20manipulation.%20Yet%2C%20an%20important%20question%20still%0Aremains%3A%20Are%20video%20models%20ready%20to%20serve%20as%20zero-shot%20reasoners%20in%20challenging%0Avisual%20reasoning%20scenarios%3F%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20study%20to%0Acomprehensively%20investigate%20this%20question%2C%20focusing%20on%20the%20leading%20and%20popular%0AVeo-3.%20We%20evaluate%20its%20reasoning%20behavior%20across%2012%20dimensions%2C%20including%0Aspatial%2C%20geometric%2C%20physical%2C%20temporal%2C%20and%20embodied%20logic%2C%20systematically%0Acharacterizing%20both%20its%20strengths%20and%20failure%20modes.%20To%20standardize%20this%20study%2C%0Awe%20curate%20the%20evaluation%20data%20into%20MME-CoF%2C%20a%20compact%20benchmark%20that%20enables%0Ain-depth%20and%20thorough%20assessment%20of%20Chain-of-Frame%20%28CoF%29%20reasoning.%20Our%0Afindings%20reveal%20that%20while%20current%20video%20models%20demonstrate%20promising%20reasoning%0Apatterns%20on%20short-horizon%20spatial%20coherence%2C%20fine-grained%20grounding%2C%20and%0Alocally%20consistent%20dynamics%2C%20they%20remain%20limited%20in%20long-horizon%20causal%0Areasoning%2C%20strict%20geometric%20constraints%2C%20and%20abstract%20logic.%20Overall%2C%20they%20are%0Anot%20yet%20reliable%20as%20standalone%20zero-shot%20reasoners%2C%20but%20exhibit%20encouraging%0Asigns%20as%20complementary%20visual%20engines%20alongside%20dedicated%20reasoning%20models.%0AProject%20page%3A%20https%3A//video-cof.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26802v1&entry.124074799=Read"},
{"title": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles", "author": "Sayed Pedram Haeri Boroujeni and Niloufar Mehrabi and Hazim Alzorgan and Ahmad Sarlak and Mahlagha Fazeli and Abolfazl Razi", "abstract": "  Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.\n", "link": "http://arxiv.org/abs/2510.26641v1", "date": "2025-10-30", "relevancy": 2.8508, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20You%20Need%20for%20Object%20Detection%3A%20From%20Pixels%2C%20Points%2C%20and%20Prompts%20to%0A%20%20Next-Gen%20Fusion%20and%20Multimodal%20LLMs/VLMs%20in%20Autonomous%20Vehicles&body=Title%3A%20All%20You%20Need%20for%20Object%20Detection%3A%20From%20Pixels%2C%20Points%2C%20and%20Prompts%20to%0A%20%20Next-Gen%20Fusion%20and%20Multimodal%20LLMs/VLMs%20in%20Autonomous%20Vehicles%0AAuthor%3A%20Sayed%20Pedram%20Haeri%20Boroujeni%20and%20Niloufar%20Mehrabi%20and%20Hazim%20Alzorgan%20and%20Ahmad%20Sarlak%20and%20Mahlagha%20Fazeli%20and%20Abolfazl%20Razi%0AAbstract%3A%20%20%20Autonomous%20Vehicles%20%28AVs%29%20are%20transforming%20the%20future%20of%20transportation%0Athrough%20advances%20in%20intelligent%20perception%2C%20decision-making%2C%20and%20control%0Asystems.%20However%2C%20their%20success%20is%20tied%20to%20one%20core%20capability%2C%20reliable%20object%0Adetection%20in%20complex%20and%20multimodal%20environments.%20While%20recent%20breakthroughs%20in%0AComputer%20Vision%20%28CV%29%20and%20Artificial%20Intelligence%20%28AI%29%20have%20driven%20remarkable%0Aprogress%2C%20the%20field%20still%20faces%20a%20critical%20challenge%20as%20knowledge%20remains%0Afragmented%20across%20multimodal%20perception%2C%20contextual%20reasoning%2C%20and%20cooperative%0Aintelligence.%20This%20survey%20bridges%20that%20gap%20by%20delivering%20a%20forward-looking%0Aanalysis%20of%20object%20detection%20in%20AVs%2C%20emphasizing%20emerging%20paradigms%20such%20as%0AVision-Language%20Models%20%28VLMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20Generative%20AI%0Arather%20than%20re-examining%20outdated%20techniques.%20We%20begin%20by%20systematically%0Areviewing%20the%20fundamental%20spectrum%20of%20AV%20sensors%20%28camera%2C%20ultrasonic%2C%20LiDAR%2C%0Aand%20Radar%29%20and%20their%20fusion%20strategies%2C%20highlighting%20not%20only%20their%0Acapabilities%20and%20limitations%20in%20dynamic%20driving%20environments%20but%20also%20their%0Apotential%20to%20integrate%20with%20recent%20advances%20in%20LLM/VLM-driven%20perception%0Aframeworks.%20Next%2C%20we%20introduce%20a%20structured%20categorization%20of%20AV%20datasets%20that%0Amoves%20beyond%20simple%20collections%2C%20positioning%20ego-vehicle%2C%20infrastructure-based%2C%0Aand%20cooperative%20datasets%20%28e.g.%2C%20V2V%2C%20V2I%2C%20V2X%2C%20I2I%29%2C%20followed%20by%20a%0Across-analysis%20of%20data%20structures%20and%20characteristics.%20Ultimately%2C%20we%20analyze%0Acutting-edge%20detection%20methodologies%2C%20ranging%20from%202D%20and%203D%20pipelines%20to%0Ahybrid%20sensor%20fusion%2C%20with%20particular%20attention%20to%20emerging%20transformer-driven%0Aapproaches%20powered%20by%20Vision%20Transformers%20%28ViTs%29%2C%20Large%20and%20Small%20Language%0AModels%20%28SLMs%29%2C%20and%20VLMs.%20By%20synthesizing%20these%20perspectives%2C%20our%20survey%0Adelivers%20a%20clear%20roadmap%20of%20current%20capabilities%2C%20open%20challenges%2C%20and%20future%0Aopportunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520You%2520Need%2520for%2520Object%2520Detection%253A%2520From%2520Pixels%252C%2520Points%252C%2520and%2520Prompts%2520to%250A%2520%2520Next-Gen%2520Fusion%2520and%2520Multimodal%2520LLMs/VLMs%2520in%2520Autonomous%2520Vehicles%26entry.906535625%3DSayed%2520Pedram%2520Haeri%2520Boroujeni%2520and%2520Niloufar%2520Mehrabi%2520and%2520Hazim%2520Alzorgan%2520and%2520Ahmad%2520Sarlak%2520and%2520Mahlagha%2520Fazeli%2520and%2520Abolfazl%2520Razi%26entry.1292438233%3D%2520%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520are%2520transforming%2520the%2520future%2520of%2520transportation%250Athrough%2520advances%2520in%2520intelligent%2520perception%252C%2520decision-making%252C%2520and%2520control%250Asystems.%2520However%252C%2520their%2520success%2520is%2520tied%2520to%2520one%2520core%2520capability%252C%2520reliable%2520object%250Adetection%2520in%2520complex%2520and%2520multimodal%2520environments.%2520While%2520recent%2520breakthroughs%2520in%250AComputer%2520Vision%2520%2528CV%2529%2520and%2520Artificial%2520Intelligence%2520%2528AI%2529%2520have%2520driven%2520remarkable%250Aprogress%252C%2520the%2520field%2520still%2520faces%2520a%2520critical%2520challenge%2520as%2520knowledge%2520remains%250Afragmented%2520across%2520multimodal%2520perception%252C%2520contextual%2520reasoning%252C%2520and%2520cooperative%250Aintelligence.%2520This%2520survey%2520bridges%2520that%2520gap%2520by%2520delivering%2520a%2520forward-looking%250Aanalysis%2520of%2520object%2520detection%2520in%2520AVs%252C%2520emphasizing%2520emerging%2520paradigms%2520such%2520as%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520Generative%2520AI%250Arather%2520than%2520re-examining%2520outdated%2520techniques.%2520We%2520begin%2520by%2520systematically%250Areviewing%2520the%2520fundamental%2520spectrum%2520of%2520AV%2520sensors%2520%2528camera%252C%2520ultrasonic%252C%2520LiDAR%252C%250Aand%2520Radar%2529%2520and%2520their%2520fusion%2520strategies%252C%2520highlighting%2520not%2520only%2520their%250Acapabilities%2520and%2520limitations%2520in%2520dynamic%2520driving%2520environments%2520but%2520also%2520their%250Apotential%2520to%2520integrate%2520with%2520recent%2520advances%2520in%2520LLM/VLM-driven%2520perception%250Aframeworks.%2520Next%252C%2520we%2520introduce%2520a%2520structured%2520categorization%2520of%2520AV%2520datasets%2520that%250Amoves%2520beyond%2520simple%2520collections%252C%2520positioning%2520ego-vehicle%252C%2520infrastructure-based%252C%250Aand%2520cooperative%2520datasets%2520%2528e.g.%252C%2520V2V%252C%2520V2I%252C%2520V2X%252C%2520I2I%2529%252C%2520followed%2520by%2520a%250Across-analysis%2520of%2520data%2520structures%2520and%2520characteristics.%2520Ultimately%252C%2520we%2520analyze%250Acutting-edge%2520detection%2520methodologies%252C%2520ranging%2520from%25202D%2520and%25203D%2520pipelines%2520to%250Ahybrid%2520sensor%2520fusion%252C%2520with%2520particular%2520attention%2520to%2520emerging%2520transformer-driven%250Aapproaches%2520powered%2520by%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520Large%2520and%2520Small%2520Language%250AModels%2520%2528SLMs%2529%252C%2520and%2520VLMs.%2520By%2520synthesizing%2520these%2520perspectives%252C%2520our%2520survey%250Adelivers%2520a%2520clear%2520roadmap%2520of%2520current%2520capabilities%252C%2520open%2520challenges%252C%2520and%2520future%250Aopportunities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20You%20Need%20for%20Object%20Detection%3A%20From%20Pixels%2C%20Points%2C%20and%20Prompts%20to%0A%20%20Next-Gen%20Fusion%20and%20Multimodal%20LLMs/VLMs%20in%20Autonomous%20Vehicles&entry.906535625=Sayed%20Pedram%20Haeri%20Boroujeni%20and%20Niloufar%20Mehrabi%20and%20Hazim%20Alzorgan%20and%20Ahmad%20Sarlak%20and%20Mahlagha%20Fazeli%20and%20Abolfazl%20Razi&entry.1292438233=%20%20Autonomous%20Vehicles%20%28AVs%29%20are%20transforming%20the%20future%20of%20transportation%0Athrough%20advances%20in%20intelligent%20perception%2C%20decision-making%2C%20and%20control%0Asystems.%20However%2C%20their%20success%20is%20tied%20to%20one%20core%20capability%2C%20reliable%20object%0Adetection%20in%20complex%20and%20multimodal%20environments.%20While%20recent%20breakthroughs%20in%0AComputer%20Vision%20%28CV%29%20and%20Artificial%20Intelligence%20%28AI%29%20have%20driven%20remarkable%0Aprogress%2C%20the%20field%20still%20faces%20a%20critical%20challenge%20as%20knowledge%20remains%0Afragmented%20across%20multimodal%20perception%2C%20contextual%20reasoning%2C%20and%20cooperative%0Aintelligence.%20This%20survey%20bridges%20that%20gap%20by%20delivering%20a%20forward-looking%0Aanalysis%20of%20object%20detection%20in%20AVs%2C%20emphasizing%20emerging%20paradigms%20such%20as%0AVision-Language%20Models%20%28VLMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20Generative%20AI%0Arather%20than%20re-examining%20outdated%20techniques.%20We%20begin%20by%20systematically%0Areviewing%20the%20fundamental%20spectrum%20of%20AV%20sensors%20%28camera%2C%20ultrasonic%2C%20LiDAR%2C%0Aand%20Radar%29%20and%20their%20fusion%20strategies%2C%20highlighting%20not%20only%20their%0Acapabilities%20and%20limitations%20in%20dynamic%20driving%20environments%20but%20also%20their%0Apotential%20to%20integrate%20with%20recent%20advances%20in%20LLM/VLM-driven%20perception%0Aframeworks.%20Next%2C%20we%20introduce%20a%20structured%20categorization%20of%20AV%20datasets%20that%0Amoves%20beyond%20simple%20collections%2C%20positioning%20ego-vehicle%2C%20infrastructure-based%2C%0Aand%20cooperative%20datasets%20%28e.g.%2C%20V2V%2C%20V2I%2C%20V2X%2C%20I2I%29%2C%20followed%20by%20a%0Across-analysis%20of%20data%20structures%20and%20characteristics.%20Ultimately%2C%20we%20analyze%0Acutting-edge%20detection%20methodologies%2C%20ranging%20from%202D%20and%203D%20pipelines%20to%0Ahybrid%20sensor%20fusion%2C%20with%20particular%20attention%20to%20emerging%20transformer-driven%0Aapproaches%20powered%20by%20Vision%20Transformers%20%28ViTs%29%2C%20Large%20and%20Small%20Language%0AModels%20%28SLMs%29%2C%20and%20VLMs.%20By%20synthesizing%20these%20perspectives%2C%20our%20survey%0Adelivers%20a%20clear%20roadmap%20of%20current%20capabilities%2C%20open%20challenges%2C%20and%20future%0Aopportunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26641v1&entry.124074799=Read"},
{"title": "Scaling Image Geo-Localization to Continent Level", "author": "Philipp Lindenberger and Paul-Edouard Sarlin and Jan Hosang and Matteo Balice and Marc Pollefeys and Simon Lynen and Eduard Trulls", "abstract": "  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n", "link": "http://arxiv.org/abs/2510.26795v1", "date": "2025-10-30", "relevancy": 2.6872, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5367}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Image%20Geo-Localization%20to%20Continent%20Level&body=Title%3A%20Scaling%20Image%20Geo-Localization%20to%20Continent%20Level%0AAuthor%3A%20Philipp%20Lindenberger%20and%20Paul-Edouard%20Sarlin%20and%20Jan%20Hosang%20and%20Matteo%20Balice%20and%20Marc%20Pollefeys%20and%20Simon%20Lynen%20and%20Eduard%20Trulls%0AAbstract%3A%20%20%20Determining%20the%20precise%20geographic%20location%20of%20an%20image%20at%20a%20global%20scale%0Aremains%20an%20unsolved%20challenge.%20Standard%20image%20retrieval%20techniques%20are%0Ainefficient%20due%20to%20the%20sheer%20volume%20of%20images%20%28%3E100M%29%20and%20fail%20when%20coverage%20is%0Ainsufficient.%20Scalable%20solutions%2C%20however%2C%20involve%20a%20trade-off%3A%20global%0Aclassification%20typically%20yields%20coarse%20results%20%2810%2B%20kilometers%29%2C%20while%0Across-view%20retrieval%20between%20ground%20and%20aerial%20imagery%20suffers%20from%20a%20domain%0Agap%20and%20has%20been%20primarily%20studied%20on%20smaller%20regions.%20This%20paper%20introduces%20a%0Ahybrid%20approach%20that%20achieves%20fine-grained%20geo-localization%20across%20a%20large%0Ageographic%20expanse%20the%20size%20of%20a%20continent.%20We%20leverage%20a%20proxy%20classification%0Atask%20during%20training%20to%20learn%20rich%20feature%20representations%20that%20implicitly%0Aencode%20precise%20location%20information.%20We%20combine%20these%20learned%20prototypes%20with%0Aembeddings%20of%20aerial%20imagery%20to%20increase%20robustness%20to%20the%20sparsity%20of%0Aground-level%20data.%20This%20enables%20direct%2C%20fine-grained%20retrieval%20over%20areas%0Aspanning%20multiple%20countries.%20Our%20extensive%20evaluation%20demonstrates%20that%20our%0Aapproach%20can%20localize%20within%20200m%20more%20than%2068%5C%25%20of%20queries%20of%20a%20dataset%0Acovering%20a%20large%20part%20of%20Europe.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//scaling-geoloc.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Image%2520Geo-Localization%2520to%2520Continent%2520Level%26entry.906535625%3DPhilipp%2520Lindenberger%2520and%2520Paul-Edouard%2520Sarlin%2520and%2520Jan%2520Hosang%2520and%2520Matteo%2520Balice%2520and%2520Marc%2520Pollefeys%2520and%2520Simon%2520Lynen%2520and%2520Eduard%2520Trulls%26entry.1292438233%3D%2520%2520Determining%2520the%2520precise%2520geographic%2520location%2520of%2520an%2520image%2520at%2520a%2520global%2520scale%250Aremains%2520an%2520unsolved%2520challenge.%2520Standard%2520image%2520retrieval%2520techniques%2520are%250Ainefficient%2520due%2520to%2520the%2520sheer%2520volume%2520of%2520images%2520%2528%253E100M%2529%2520and%2520fail%2520when%2520coverage%2520is%250Ainsufficient.%2520Scalable%2520solutions%252C%2520however%252C%2520involve%2520a%2520trade-off%253A%2520global%250Aclassification%2520typically%2520yields%2520coarse%2520results%2520%252810%252B%2520kilometers%2529%252C%2520while%250Across-view%2520retrieval%2520between%2520ground%2520and%2520aerial%2520imagery%2520suffers%2520from%2520a%2520domain%250Agap%2520and%2520has%2520been%2520primarily%2520studied%2520on%2520smaller%2520regions.%2520This%2520paper%2520introduces%2520a%250Ahybrid%2520approach%2520that%2520achieves%2520fine-grained%2520geo-localization%2520across%2520a%2520large%250Ageographic%2520expanse%2520the%2520size%2520of%2520a%2520continent.%2520We%2520leverage%2520a%2520proxy%2520classification%250Atask%2520during%2520training%2520to%2520learn%2520rich%2520feature%2520representations%2520that%2520implicitly%250Aencode%2520precise%2520location%2520information.%2520We%2520combine%2520these%2520learned%2520prototypes%2520with%250Aembeddings%2520of%2520aerial%2520imagery%2520to%2520increase%2520robustness%2520to%2520the%2520sparsity%2520of%250Aground-level%2520data.%2520This%2520enables%2520direct%252C%2520fine-grained%2520retrieval%2520over%2520areas%250Aspanning%2520multiple%2520countries.%2520Our%2520extensive%2520evaluation%2520demonstrates%2520that%2520our%250Aapproach%2520can%2520localize%2520within%2520200m%2520more%2520than%252068%255C%2525%2520of%2520queries%2520of%2520a%2520dataset%250Acovering%2520a%2520large%2520part%2520of%2520Europe.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//scaling-geoloc.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Image%20Geo-Localization%20to%20Continent%20Level&entry.906535625=Philipp%20Lindenberger%20and%20Paul-Edouard%20Sarlin%20and%20Jan%20Hosang%20and%20Matteo%20Balice%20and%20Marc%20Pollefeys%20and%20Simon%20Lynen%20and%20Eduard%20Trulls&entry.1292438233=%20%20Determining%20the%20precise%20geographic%20location%20of%20an%20image%20at%20a%20global%20scale%0Aremains%20an%20unsolved%20challenge.%20Standard%20image%20retrieval%20techniques%20are%0Ainefficient%20due%20to%20the%20sheer%20volume%20of%20images%20%28%3E100M%29%20and%20fail%20when%20coverage%20is%0Ainsufficient.%20Scalable%20solutions%2C%20however%2C%20involve%20a%20trade-off%3A%20global%0Aclassification%20typically%20yields%20coarse%20results%20%2810%2B%20kilometers%29%2C%20while%0Across-view%20retrieval%20between%20ground%20and%20aerial%20imagery%20suffers%20from%20a%20domain%0Agap%20and%20has%20been%20primarily%20studied%20on%20smaller%20regions.%20This%20paper%20introduces%20a%0Ahybrid%20approach%20that%20achieves%20fine-grained%20geo-localization%20across%20a%20large%0Ageographic%20expanse%20the%20size%20of%20a%20continent.%20We%20leverage%20a%20proxy%20classification%0Atask%20during%20training%20to%20learn%20rich%20feature%20representations%20that%20implicitly%0Aencode%20precise%20location%20information.%20We%20combine%20these%20learned%20prototypes%20with%0Aembeddings%20of%20aerial%20imagery%20to%20increase%20robustness%20to%20the%20sparsity%20of%0Aground-level%20data.%20This%20enables%20direct%2C%20fine-grained%20retrieval%20over%20areas%0Aspanning%20multiple%20countries.%20Our%20extensive%20evaluation%20demonstrates%20that%20our%0Aapproach%20can%20localize%20within%20200m%20more%20than%2068%5C%25%20of%20queries%20of%20a%20dataset%0Acovering%20a%20large%20part%20of%20Europe.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//scaling-geoloc.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26795v1&entry.124074799=Read"},
{"title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes", "author": "Yukun Huang and Jiwen Yu and Yanning Zhou and Jianan Wang and Xintao Wang and Pengfei Wan and Xihui Liu", "abstract": "  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n", "link": "http://arxiv.org/abs/2510.26800v1", "date": "2025-10-30", "relevancy": 2.6464, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6494}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniX%3A%20From%20Unified%20Panoramic%20Generation%20and%20Perception%20to%0A%20%20Graphics-Ready%203D%20Scenes&body=Title%3A%20OmniX%3A%20From%20Unified%20Panoramic%20Generation%20and%20Perception%20to%0A%20%20Graphics-Ready%203D%20Scenes%0AAuthor%3A%20Yukun%20Huang%20and%20Jiwen%20Yu%20and%20Yanning%20Zhou%20and%20Jianan%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20There%20are%20two%20prevalent%20ways%20to%20constructing%203D%20scenes%3A%20procedural%20generation%0Aand%202D%20lifting.%20Among%20them%2C%20panorama-based%202D%20lifting%20has%20emerged%20as%20a%0Apromising%20technique%2C%20leveraging%20powerful%202D%20generative%20priors%20to%20produce%0Aimmersive%2C%20realistic%2C%20and%20diverse%203D%20environments.%20In%20this%20work%2C%20we%20advance%0Athis%20technique%20to%20generate%20graphics-ready%203D%20scenes%20suitable%20for%20physically%0Abased%20rendering%20%28PBR%29%2C%20relighting%2C%20and%20simulation.%20Our%20key%20insight%20is%20to%0Arepurpose%202D%20generative%20models%20for%20panoramic%20perception%20of%20geometry%2C%20textures%2C%0Aand%20PBR%20materials.%20Unlike%20existing%202D%20lifting%20approaches%20that%20emphasize%0Aappearance%20generation%20and%20ignore%20the%20perception%20of%20intrinsic%20properties%2C%20we%0Apresent%20OmniX%2C%20a%20versatile%20and%20unified%20framework.%20Based%20on%20a%20lightweight%20and%0Aefficient%20cross-modal%20adapter%20structure%2C%20OmniX%20reuses%202D%20generative%20priors%20for%0Aa%20broad%20range%20of%20panoramic%20vision%20tasks%2C%20including%20panoramic%20perception%2C%0Ageneration%2C%20and%20completion.%20Furthermore%2C%20we%20construct%20a%20large-scale%20synthetic%0Apanorama%20dataset%20containing%20high-quality%20multimodal%20panoramas%20from%20diverse%0Aindoor%20and%20outdoor%20scenes.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20model%20in%20panoramic%20visual%20perception%20and%20graphics-ready%203D%20scene%0Ageneration%2C%20opening%20new%20possibilities%20for%20immersive%20and%20physically%20realistic%0Avirtual%20world%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniX%253A%2520From%2520Unified%2520Panoramic%2520Generation%2520and%2520Perception%2520to%250A%2520%2520Graphics-Ready%25203D%2520Scenes%26entry.906535625%3DYukun%2520Huang%2520and%2520Jiwen%2520Yu%2520and%2520Yanning%2520Zhou%2520and%2520Jianan%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520There%2520are%2520two%2520prevalent%2520ways%2520to%2520constructing%25203D%2520scenes%253A%2520procedural%2520generation%250Aand%25202D%2520lifting.%2520Among%2520them%252C%2520panorama-based%25202D%2520lifting%2520has%2520emerged%2520as%2520a%250Apromising%2520technique%252C%2520leveraging%2520powerful%25202D%2520generative%2520priors%2520to%2520produce%250Aimmersive%252C%2520realistic%252C%2520and%2520diverse%25203D%2520environments.%2520In%2520this%2520work%252C%2520we%2520advance%250Athis%2520technique%2520to%2520generate%2520graphics-ready%25203D%2520scenes%2520suitable%2520for%2520physically%250Abased%2520rendering%2520%2528PBR%2529%252C%2520relighting%252C%2520and%2520simulation.%2520Our%2520key%2520insight%2520is%2520to%250Arepurpose%25202D%2520generative%2520models%2520for%2520panoramic%2520perception%2520of%2520geometry%252C%2520textures%252C%250Aand%2520PBR%2520materials.%2520Unlike%2520existing%25202D%2520lifting%2520approaches%2520that%2520emphasize%250Aappearance%2520generation%2520and%2520ignore%2520the%2520perception%2520of%2520intrinsic%2520properties%252C%2520we%250Apresent%2520OmniX%252C%2520a%2520versatile%2520and%2520unified%2520framework.%2520Based%2520on%2520a%2520lightweight%2520and%250Aefficient%2520cross-modal%2520adapter%2520structure%252C%2520OmniX%2520reuses%25202D%2520generative%2520priors%2520for%250Aa%2520broad%2520range%2520of%2520panoramic%2520vision%2520tasks%252C%2520including%2520panoramic%2520perception%252C%250Ageneration%252C%2520and%2520completion.%2520Furthermore%252C%2520we%2520construct%2520a%2520large-scale%2520synthetic%250Apanorama%2520dataset%2520containing%2520high-quality%2520multimodal%2520panoramas%2520from%2520diverse%250Aindoor%2520and%2520outdoor%2520scenes.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520model%2520in%2520panoramic%2520visual%2520perception%2520and%2520graphics-ready%25203D%2520scene%250Ageneration%252C%2520opening%2520new%2520possibilities%2520for%2520immersive%2520and%2520physically%2520realistic%250Avirtual%2520world%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniX%3A%20From%20Unified%20Panoramic%20Generation%20and%20Perception%20to%0A%20%20Graphics-Ready%203D%20Scenes&entry.906535625=Yukun%20Huang%20and%20Jiwen%20Yu%20and%20Yanning%20Zhou%20and%20Jianan%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Xihui%20Liu&entry.1292438233=%20%20There%20are%20two%20prevalent%20ways%20to%20constructing%203D%20scenes%3A%20procedural%20generation%0Aand%202D%20lifting.%20Among%20them%2C%20panorama-based%202D%20lifting%20has%20emerged%20as%20a%0Apromising%20technique%2C%20leveraging%20powerful%202D%20generative%20priors%20to%20produce%0Aimmersive%2C%20realistic%2C%20and%20diverse%203D%20environments.%20In%20this%20work%2C%20we%20advance%0Athis%20technique%20to%20generate%20graphics-ready%203D%20scenes%20suitable%20for%20physically%0Abased%20rendering%20%28PBR%29%2C%20relighting%2C%20and%20simulation.%20Our%20key%20insight%20is%20to%0Arepurpose%202D%20generative%20models%20for%20panoramic%20perception%20of%20geometry%2C%20textures%2C%0Aand%20PBR%20materials.%20Unlike%20existing%202D%20lifting%20approaches%20that%20emphasize%0Aappearance%20generation%20and%20ignore%20the%20perception%20of%20intrinsic%20properties%2C%20we%0Apresent%20OmniX%2C%20a%20versatile%20and%20unified%20framework.%20Based%20on%20a%20lightweight%20and%0Aefficient%20cross-modal%20adapter%20structure%2C%20OmniX%20reuses%202D%20generative%20priors%20for%0Aa%20broad%20range%20of%20panoramic%20vision%20tasks%2C%20including%20panoramic%20perception%2C%0Ageneration%2C%20and%20completion.%20Furthermore%2C%20we%20construct%20a%20large-scale%20synthetic%0Apanorama%20dataset%20containing%20high-quality%20multimodal%20panoramas%20from%20diverse%0Aindoor%20and%20outdoor%20scenes.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20model%20in%20panoramic%20visual%20perception%20and%20graphics-ready%203D%20scene%0Ageneration%2C%20opening%20new%20possibilities%20for%20immersive%20and%20physically%20realistic%0Avirtual%20world%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26800v1&entry.124074799=Read"},
{"title": "MORE: Multi-Organ Medical Image REconstruction Dataset", "author": "Shaokai Wu and Yapan Guo and Yanbiao Ji and Jing Tong and Yuxiang Lu and Mei Li and Suizhi Huang and Yue Ding and Hongtao Lu", "abstract": "  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n", "link": "http://arxiv.org/abs/2510.26759v1", "date": "2025-10-30", "relevancy": 2.6132, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.534}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MORE%3A%20Multi-Organ%20Medical%20Image%20REconstruction%20Dataset&body=Title%3A%20MORE%3A%20Multi-Organ%20Medical%20Image%20REconstruction%20Dataset%0AAuthor%3A%20Shaokai%20Wu%20and%20Yapan%20Guo%20and%20Yanbiao%20Ji%20and%20Jing%20Tong%20and%20Yuxiang%20Lu%20and%20Mei%20Li%20and%20Suizhi%20Huang%20and%20Yue%20Ding%20and%20Hongtao%20Lu%0AAbstract%3A%20%20%20CT%20reconstruction%20provides%20radiologists%20with%20images%20for%20diagnosis%20and%0Atreatment%2C%20yet%20current%20deep%20learning%20methods%20are%20typically%20limited%20to%20specific%0Aanatomies%20and%20datasets%2C%20hindering%20generalization%20ability%20to%20unseen%20anatomies%0Aand%20lesions.%20To%20address%20this%2C%20we%20introduce%20the%20Multi-Organ%20medical%20image%0AREconstruction%20%28MORE%29%20dataset%2C%20comprising%20CT%20scans%20across%209%20diverse%20anatomies%0Awith%2015%20lesion%20types.%20This%20dataset%20serves%20two%20key%20purposes%3A%20%281%29%20enabling%20robust%0Atraining%20of%20deep%20learning%20models%20on%20extensive%2C%20heterogeneous%20data%2C%20and%20%282%29%0Afacilitating%20rigorous%20evaluation%20of%20model%20generalization%20for%20CT%20reconstruction.%0AWe%20further%20establish%20a%20strong%20baseline%20solution%20that%20outperforms%20prior%0Aapproaches%20under%20these%20challenging%20conditions.%20Our%20results%20demonstrate%20that%3A%0A%281%29%20a%20comprehensive%20dataset%20helps%20improve%20the%20generalization%20capability%20of%0Amodels%2C%20and%20%282%29%20optimization-based%20methods%20offer%20enhanced%20robustness%20for%20unseen%0Aanatomies.%20The%20MORE%20dataset%20is%20freely%20accessible%20under%20CC-BY-NC%204.0%20at%20our%0Aproject%20page%20https%3A//more-med.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMORE%253A%2520Multi-Organ%2520Medical%2520Image%2520REconstruction%2520Dataset%26entry.906535625%3DShaokai%2520Wu%2520and%2520Yapan%2520Guo%2520and%2520Yanbiao%2520Ji%2520and%2520Jing%2520Tong%2520and%2520Yuxiang%2520Lu%2520and%2520Mei%2520Li%2520and%2520Suizhi%2520Huang%2520and%2520Yue%2520Ding%2520and%2520Hongtao%2520Lu%26entry.1292438233%3D%2520%2520CT%2520reconstruction%2520provides%2520radiologists%2520with%2520images%2520for%2520diagnosis%2520and%250Atreatment%252C%2520yet%2520current%2520deep%2520learning%2520methods%2520are%2520typically%2520limited%2520to%2520specific%250Aanatomies%2520and%2520datasets%252C%2520hindering%2520generalization%2520ability%2520to%2520unseen%2520anatomies%250Aand%2520lesions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Multi-Organ%2520medical%2520image%250AREconstruction%2520%2528MORE%2529%2520dataset%252C%2520comprising%2520CT%2520scans%2520across%25209%2520diverse%2520anatomies%250Awith%252015%2520lesion%2520types.%2520This%2520dataset%2520serves%2520two%2520key%2520purposes%253A%2520%25281%2529%2520enabling%2520robust%250Atraining%2520of%2520deep%2520learning%2520models%2520on%2520extensive%252C%2520heterogeneous%2520data%252C%2520and%2520%25282%2529%250Afacilitating%2520rigorous%2520evaluation%2520of%2520model%2520generalization%2520for%2520CT%2520reconstruction.%250AWe%2520further%2520establish%2520a%2520strong%2520baseline%2520solution%2520that%2520outperforms%2520prior%250Aapproaches%2520under%2520these%2520challenging%2520conditions.%2520Our%2520results%2520demonstrate%2520that%253A%250A%25281%2529%2520a%2520comprehensive%2520dataset%2520helps%2520improve%2520the%2520generalization%2520capability%2520of%250Amodels%252C%2520and%2520%25282%2529%2520optimization-based%2520methods%2520offer%2520enhanced%2520robustness%2520for%2520unseen%250Aanatomies.%2520The%2520MORE%2520dataset%2520is%2520freely%2520accessible%2520under%2520CC-BY-NC%25204.0%2520at%2520our%250Aproject%2520page%2520https%253A//more-med.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MORE%3A%20Multi-Organ%20Medical%20Image%20REconstruction%20Dataset&entry.906535625=Shaokai%20Wu%20and%20Yapan%20Guo%20and%20Yanbiao%20Ji%20and%20Jing%20Tong%20and%20Yuxiang%20Lu%20and%20Mei%20Li%20and%20Suizhi%20Huang%20and%20Yue%20Ding%20and%20Hongtao%20Lu&entry.1292438233=%20%20CT%20reconstruction%20provides%20radiologists%20with%20images%20for%20diagnosis%20and%0Atreatment%2C%20yet%20current%20deep%20learning%20methods%20are%20typically%20limited%20to%20specific%0Aanatomies%20and%20datasets%2C%20hindering%20generalization%20ability%20to%20unseen%20anatomies%0Aand%20lesions.%20To%20address%20this%2C%20we%20introduce%20the%20Multi-Organ%20medical%20image%0AREconstruction%20%28MORE%29%20dataset%2C%20comprising%20CT%20scans%20across%209%20diverse%20anatomies%0Awith%2015%20lesion%20types.%20This%20dataset%20serves%20two%20key%20purposes%3A%20%281%29%20enabling%20robust%0Atraining%20of%20deep%20learning%20models%20on%20extensive%2C%20heterogeneous%20data%2C%20and%20%282%29%0Afacilitating%20rigorous%20evaluation%20of%20model%20generalization%20for%20CT%20reconstruction.%0AWe%20further%20establish%20a%20strong%20baseline%20solution%20that%20outperforms%20prior%0Aapproaches%20under%20these%20challenging%20conditions.%20Our%20results%20demonstrate%20that%3A%0A%281%29%20a%20comprehensive%20dataset%20helps%20improve%20the%20generalization%20capability%20of%0Amodels%2C%20and%20%282%29%20optimization-based%20methods%20offer%20enhanced%20robustness%20for%20unseen%0Aanatomies.%20The%20MORE%20dataset%20is%20freely%20accessible%20under%20CC-BY-NC%204.0%20at%20our%0Aproject%20page%20https%3A//more-med.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26759v1&entry.124074799=Read"},
{"title": "Deep sequence models tend to memorize geometrically; it is unclear why", "author": "Shahriar Noroozizadeh and Vaishnavh Nagarajan and Elan Rosenfeld and Sanjiv Kumar", "abstract": "  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n", "link": "http://arxiv.org/abs/2510.26745v1", "date": "2025-10-30", "relevancy": 2.5573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20sequence%20models%20tend%20to%20memorize%20geometrically%3B%20it%20is%20unclear%20why&body=Title%3A%20Deep%20sequence%20models%20tend%20to%20memorize%20geometrically%3B%20it%20is%20unclear%20why%0AAuthor%3A%20Shahriar%20Noroozizadeh%20and%20Vaishnavh%20Nagarajan%20and%20Elan%20Rosenfeld%20and%20Sanjiv%20Kumar%0AAbstract%3A%20%20%20In%20sequence%20modeling%2C%20the%20parametric%20memory%20of%20atomic%20facts%20has%20been%0Apredominantly%20abstracted%20as%20a%20brute-force%20lookup%20of%20co-occurrences%20between%0Aentities.%20We%20contrast%20this%20associative%20view%20against%20a%20geometric%20view%20of%20how%0Amemory%20is%20stored.%20We%20begin%20by%20isolating%20a%20clean%20and%20analyzable%20instance%20of%0ATransformer%20reasoning%20that%20is%20incompatible%20with%20memory%20as%20strictly%20a%20storage%20of%0Athe%20local%20co-occurrences%20specified%20during%20training.%20Instead%2C%20the%20model%20must%0Ahave%20somehow%20synthesized%20its%20own%20geometry%20of%20atomic%20facts%2C%20encoding%20global%0Arelationships%20between%20all%20entities%2C%20including%20non-co-occurring%20ones.%20This%20in%0Aturn%20has%20simplified%20a%20hard%20reasoning%20task%20involving%20an%20%24%5Cell%24-fold%20composition%0Ainto%20an%20easy-to-learn%201-step%20geometric%20task.%0A%20%20From%20this%20phenomenon%2C%20we%20extract%20fundamental%20aspects%20of%20neural%20embedding%0Ageometries%20that%20are%20hard%20to%20explain.%20We%20argue%20that%20the%20rise%20of%20such%20a%20geometry%2C%0Adespite%20optimizing%20over%20mere%20local%20associations%2C%20cannot%20be%20straightforwardly%0Aattributed%20to%20typical%20architectural%20or%20optimizational%20pressures.%0ACounterintuitively%2C%20an%20elegant%20geometry%20is%20learned%20even%20when%20it%20is%20not%20more%0Asuccinct%20than%20a%20brute-force%20lookup%20of%20associations.%0A%20%20Then%2C%20by%20analyzing%20a%20connection%20to%20Node2Vec%2C%20we%20demonstrate%20how%20the%20geometry%0Astems%20from%20a%20spectral%20bias%20that%20--%20in%20contrast%20to%20prevailing%20theories%20--%20indeed%0Aarises%20naturally%20despite%20the%20lack%20of%20various%20pressures.%20This%20analysis%20also%0Apoints%20to%20practitioners%20a%20visible%20headroom%20to%20make%20Transformer%20memory%20more%0Astrongly%20geometric.%20We%20hope%20the%20geometric%20view%20of%20parametric%20memory%20encourages%0Arevisiting%20the%20default%20intuitions%20that%20guide%20researchers%20in%20areas%20like%0Aknowledge%20acquisition%2C%20capacity%2C%20discovery%20and%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520sequence%2520models%2520tend%2520to%2520memorize%2520geometrically%253B%2520it%2520is%2520unclear%2520why%26entry.906535625%3DShahriar%2520Noroozizadeh%2520and%2520Vaishnavh%2520Nagarajan%2520and%2520Elan%2520Rosenfeld%2520and%2520Sanjiv%2520Kumar%26entry.1292438233%3D%2520%2520In%2520sequence%2520modeling%252C%2520the%2520parametric%2520memory%2520of%2520atomic%2520facts%2520has%2520been%250Apredominantly%2520abstracted%2520as%2520a%2520brute-force%2520lookup%2520of%2520co-occurrences%2520between%250Aentities.%2520We%2520contrast%2520this%2520associative%2520view%2520against%2520a%2520geometric%2520view%2520of%2520how%250Amemory%2520is%2520stored.%2520We%2520begin%2520by%2520isolating%2520a%2520clean%2520and%2520analyzable%2520instance%2520of%250ATransformer%2520reasoning%2520that%2520is%2520incompatible%2520with%2520memory%2520as%2520strictly%2520a%2520storage%2520of%250Athe%2520local%2520co-occurrences%2520specified%2520during%2520training.%2520Instead%252C%2520the%2520model%2520must%250Ahave%2520somehow%2520synthesized%2520its%2520own%2520geometry%2520of%2520atomic%2520facts%252C%2520encoding%2520global%250Arelationships%2520between%2520all%2520entities%252C%2520including%2520non-co-occurring%2520ones.%2520This%2520in%250Aturn%2520has%2520simplified%2520a%2520hard%2520reasoning%2520task%2520involving%2520an%2520%2524%255Cell%2524-fold%2520composition%250Ainto%2520an%2520easy-to-learn%25201-step%2520geometric%2520task.%250A%2520%2520From%2520this%2520phenomenon%252C%2520we%2520extract%2520fundamental%2520aspects%2520of%2520neural%2520embedding%250Ageometries%2520that%2520are%2520hard%2520to%2520explain.%2520We%2520argue%2520that%2520the%2520rise%2520of%2520such%2520a%2520geometry%252C%250Adespite%2520optimizing%2520over%2520mere%2520local%2520associations%252C%2520cannot%2520be%2520straightforwardly%250Aattributed%2520to%2520typical%2520architectural%2520or%2520optimizational%2520pressures.%250ACounterintuitively%252C%2520an%2520elegant%2520geometry%2520is%2520learned%2520even%2520when%2520it%2520is%2520not%2520more%250Asuccinct%2520than%2520a%2520brute-force%2520lookup%2520of%2520associations.%250A%2520%2520Then%252C%2520by%2520analyzing%2520a%2520connection%2520to%2520Node2Vec%252C%2520we%2520demonstrate%2520how%2520the%2520geometry%250Astems%2520from%2520a%2520spectral%2520bias%2520that%2520--%2520in%2520contrast%2520to%2520prevailing%2520theories%2520--%2520indeed%250Aarises%2520naturally%2520despite%2520the%2520lack%2520of%2520various%2520pressures.%2520This%2520analysis%2520also%250Apoints%2520to%2520practitioners%2520a%2520visible%2520headroom%2520to%2520make%2520Transformer%2520memory%2520more%250Astrongly%2520geometric.%2520We%2520hope%2520the%2520geometric%2520view%2520of%2520parametric%2520memory%2520encourages%250Arevisiting%2520the%2520default%2520intuitions%2520that%2520guide%2520researchers%2520in%2520areas%2520like%250Aknowledge%2520acquisition%252C%2520capacity%252C%2520discovery%2520and%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20sequence%20models%20tend%20to%20memorize%20geometrically%3B%20it%20is%20unclear%20why&entry.906535625=Shahriar%20Noroozizadeh%20and%20Vaishnavh%20Nagarajan%20and%20Elan%20Rosenfeld%20and%20Sanjiv%20Kumar&entry.1292438233=%20%20In%20sequence%20modeling%2C%20the%20parametric%20memory%20of%20atomic%20facts%20has%20been%0Apredominantly%20abstracted%20as%20a%20brute-force%20lookup%20of%20co-occurrences%20between%0Aentities.%20We%20contrast%20this%20associative%20view%20against%20a%20geometric%20view%20of%20how%0Amemory%20is%20stored.%20We%20begin%20by%20isolating%20a%20clean%20and%20analyzable%20instance%20of%0ATransformer%20reasoning%20that%20is%20incompatible%20with%20memory%20as%20strictly%20a%20storage%20of%0Athe%20local%20co-occurrences%20specified%20during%20training.%20Instead%2C%20the%20model%20must%0Ahave%20somehow%20synthesized%20its%20own%20geometry%20of%20atomic%20facts%2C%20encoding%20global%0Arelationships%20between%20all%20entities%2C%20including%20non-co-occurring%20ones.%20This%20in%0Aturn%20has%20simplified%20a%20hard%20reasoning%20task%20involving%20an%20%24%5Cell%24-fold%20composition%0Ainto%20an%20easy-to-learn%201-step%20geometric%20task.%0A%20%20From%20this%20phenomenon%2C%20we%20extract%20fundamental%20aspects%20of%20neural%20embedding%0Ageometries%20that%20are%20hard%20to%20explain.%20We%20argue%20that%20the%20rise%20of%20such%20a%20geometry%2C%0Adespite%20optimizing%20over%20mere%20local%20associations%2C%20cannot%20be%20straightforwardly%0Aattributed%20to%20typical%20architectural%20or%20optimizational%20pressures.%0ACounterintuitively%2C%20an%20elegant%20geometry%20is%20learned%20even%20when%20it%20is%20not%20more%0Asuccinct%20than%20a%20brute-force%20lookup%20of%20associations.%0A%20%20Then%2C%20by%20analyzing%20a%20connection%20to%20Node2Vec%2C%20we%20demonstrate%20how%20the%20geometry%0Astems%20from%20a%20spectral%20bias%20that%20--%20in%20contrast%20to%20prevailing%20theories%20--%20indeed%0Aarises%20naturally%20despite%20the%20lack%20of%20various%20pressures.%20This%20analysis%20also%0Apoints%20to%20practitioners%20a%20visible%20headroom%20to%20make%20Transformer%20memory%20more%0Astrongly%20geometric.%20We%20hope%20the%20geometric%20view%20of%20parametric%20memory%20encourages%0Arevisiting%20the%20default%20intuitions%20that%20guide%20researchers%20in%20areas%20like%0Aknowledge%20acquisition%2C%20capacity%2C%20discovery%20and%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26745v1&entry.124074799=Read"},
{"title": "Faithful and Fast Influence Function via Advanced Sampling", "author": "Jungyeon Koh and Hyeonsu Lyu and Jonggyu Jang and Hyun Jong Yang", "abstract": "  How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.\n", "link": "http://arxiv.org/abs/2510.26776v1", "date": "2025-10-30", "relevancy": 2.5014, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.51}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faithful%20and%20Fast%20Influence%20Function%20via%20Advanced%20Sampling&body=Title%3A%20Faithful%20and%20Fast%20Influence%20Function%20via%20Advanced%20Sampling%0AAuthor%3A%20Jungyeon%20Koh%20and%20Hyeonsu%20Lyu%20and%20Jonggyu%20Jang%20and%20Hyun%20Jong%20Yang%0AAbstract%3A%20%20%20How%20can%20we%20explain%20the%20influence%20of%20training%20data%20on%20black-box%20models%3F%0AInfluence%20functions%20%28IFs%29%20offer%20a%20post-hoc%20solution%20by%20utilizing%20gradients%20and%0AHessians.%20However%2C%20computing%20the%20Hessian%20for%20an%20entire%20dataset%20is%0Aresource-intensive%2C%20necessitating%20a%20feasible%20alternative.%20A%20common%20approach%0Ainvolves%20randomly%20sampling%20a%20small%20subset%20of%20the%20training%20data%2C%20but%20this%20method%0Aoften%20results%20in%20highly%20inconsistent%20IF%20estimates%20due%20to%20the%20high%20variance%20in%0Asample%20configurations.%20To%20address%20this%2C%20we%20propose%20two%20advanced%20sampling%0Atechniques%20based%20on%20features%20and%20logits.%20These%20samplers%20select%20a%20small%20yet%0Arepresentative%20subset%20of%20the%20entire%20dataset%20by%20considering%20the%20stochastic%0Adistribution%20of%20features%20or%20logits%2C%20thereby%20enhancing%20the%20accuracy%20of%20IF%0Aestimations.%20We%20validate%20our%20approach%20through%20class%20removal%20experiments%2C%20a%0Atypical%20application%20of%20IFs%2C%20using%20the%20F1-score%20to%20measure%20how%20effectively%20the%0Amodel%20forgets%20the%20removed%20class%20while%20maintaining%20inference%20consistency%20on%20the%0Aremaining%20classes.%20Our%20method%20reduces%20computation%20time%20by%2030.1%25%20and%20memory%0Ausage%20by%2042.2%25%2C%20or%20improves%20the%20F1-score%20by%202.5%25%20compared%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaithful%2520and%2520Fast%2520Influence%2520Function%2520via%2520Advanced%2520Sampling%26entry.906535625%3DJungyeon%2520Koh%2520and%2520Hyeonsu%2520Lyu%2520and%2520Jonggyu%2520Jang%2520and%2520Hyun%2520Jong%2520Yang%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520explain%2520the%2520influence%2520of%2520training%2520data%2520on%2520black-box%2520models%253F%250AInfluence%2520functions%2520%2528IFs%2529%2520offer%2520a%2520post-hoc%2520solution%2520by%2520utilizing%2520gradients%2520and%250AHessians.%2520However%252C%2520computing%2520the%2520Hessian%2520for%2520an%2520entire%2520dataset%2520is%250Aresource-intensive%252C%2520necessitating%2520a%2520feasible%2520alternative.%2520A%2520common%2520approach%250Ainvolves%2520randomly%2520sampling%2520a%2520small%2520subset%2520of%2520the%2520training%2520data%252C%2520but%2520this%2520method%250Aoften%2520results%2520in%2520highly%2520inconsistent%2520IF%2520estimates%2520due%2520to%2520the%2520high%2520variance%2520in%250Asample%2520configurations.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520advanced%2520sampling%250Atechniques%2520based%2520on%2520features%2520and%2520logits.%2520These%2520samplers%2520select%2520a%2520small%2520yet%250Arepresentative%2520subset%2520of%2520the%2520entire%2520dataset%2520by%2520considering%2520the%2520stochastic%250Adistribution%2520of%2520features%2520or%2520logits%252C%2520thereby%2520enhancing%2520the%2520accuracy%2520of%2520IF%250Aestimations.%2520We%2520validate%2520our%2520approach%2520through%2520class%2520removal%2520experiments%252C%2520a%250Atypical%2520application%2520of%2520IFs%252C%2520using%2520the%2520F1-score%2520to%2520measure%2520how%2520effectively%2520the%250Amodel%2520forgets%2520the%2520removed%2520class%2520while%2520maintaining%2520inference%2520consistency%2520on%2520the%250Aremaining%2520classes.%2520Our%2520method%2520reduces%2520computation%2520time%2520by%252030.1%2525%2520and%2520memory%250Ausage%2520by%252042.2%2525%252C%2520or%2520improves%2520the%2520F1-score%2520by%25202.5%2525%2520compared%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faithful%20and%20Fast%20Influence%20Function%20via%20Advanced%20Sampling&entry.906535625=Jungyeon%20Koh%20and%20Hyeonsu%20Lyu%20and%20Jonggyu%20Jang%20and%20Hyun%20Jong%20Yang&entry.1292438233=%20%20How%20can%20we%20explain%20the%20influence%20of%20training%20data%20on%20black-box%20models%3F%0AInfluence%20functions%20%28IFs%29%20offer%20a%20post-hoc%20solution%20by%20utilizing%20gradients%20and%0AHessians.%20However%2C%20computing%20the%20Hessian%20for%20an%20entire%20dataset%20is%0Aresource-intensive%2C%20necessitating%20a%20feasible%20alternative.%20A%20common%20approach%0Ainvolves%20randomly%20sampling%20a%20small%20subset%20of%20the%20training%20data%2C%20but%20this%20method%0Aoften%20results%20in%20highly%20inconsistent%20IF%20estimates%20due%20to%20the%20high%20variance%20in%0Asample%20configurations.%20To%20address%20this%2C%20we%20propose%20two%20advanced%20sampling%0Atechniques%20based%20on%20features%20and%20logits.%20These%20samplers%20select%20a%20small%20yet%0Arepresentative%20subset%20of%20the%20entire%20dataset%20by%20considering%20the%20stochastic%0Adistribution%20of%20features%20or%20logits%2C%20thereby%20enhancing%20the%20accuracy%20of%20IF%0Aestimations.%20We%20validate%20our%20approach%20through%20class%20removal%20experiments%2C%20a%0Atypical%20application%20of%20IFs%2C%20using%20the%20F1-score%20to%20measure%20how%20effectively%20the%0Amodel%20forgets%20the%20removed%20class%20while%20maintaining%20inference%20consistency%20on%20the%0Aremaining%20classes.%20Our%20method%20reduces%20computation%20time%20by%2030.1%25%20and%20memory%0Ausage%20by%2042.2%25%2C%20or%20improves%20the%20F1-score%20by%202.5%25%20compared%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26776v1&entry.124074799=Read"},
{"title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment", "author": "Aniruddh Bansal and Davit Soselia and Dang Nguyen and Tianyi Zhou", "abstract": "  Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.\n", "link": "http://arxiv.org/abs/2510.26781v1", "date": "2025-10-30", "relevancy": 2.497, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartAB%3A%20A%20Benchmark%20for%20Chart%20Grounding%20%26%20Dense%20Alignment&body=Title%3A%20ChartAB%3A%20A%20Benchmark%20for%20Chart%20Grounding%20%26%20Dense%20Alignment%0AAuthor%3A%20Aniruddh%20Bansal%20and%20Davit%20Soselia%20and%20Dang%20Nguyen%20and%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Charts%20play%20an%20important%20role%20in%20visualization%2C%20reasoning%2C%20data%20analysis%2C%20and%0Athe%20exchange%20of%20ideas%20among%20humans.%20However%2C%20existing%20vision-language%20models%0A%28VLMs%29%20still%20lack%20accurate%20perception%20of%20details%20and%20struggle%20to%20extract%0Afine-grained%20structures%20from%20charts.%20Such%20limitations%20in%20chart%20grounding%20also%0Ahinder%20their%20ability%20to%20compare%20multiple%20charts%20and%20reason%20over%20them.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20%22ChartAlign%20Benchmark%20%28ChartAB%29%22%20to%20provide%20a%0Acomprehensive%20evaluation%20of%20VLMs%20in%20chart%20grounding%20tasks%2C%20i.e.%2C%20extracting%0Atabular%20data%2C%20localizing%20visualization%20elements%2C%20and%20recognizing%20various%0Aattributes%20from%20charts%20of%20diverse%20types%20and%20complexities.%20We%20design%20a%20JSON%0Atemplate%20to%20facilitate%20the%20calculation%20of%20evaluation%20metrics%20specifically%0Atailored%20for%20each%20grounding%20task.%20By%20incorporating%20a%20novel%20two-stage%20inference%0Aworkflow%2C%20the%20benchmark%20can%20further%20evaluate%20VLMs%27%20capability%20to%20align%20and%0Acompare%20elements/attributes%20across%20two%20charts.%20Our%20analysis%20of%20evaluations%20on%0Aseveral%20recent%20VLMs%20reveals%20new%20insights%20into%20their%20perception%20biases%2C%0Aweaknesses%2C%20robustness%2C%20and%20hallucinations%20in%20chart%20understanding.%20These%0Afindings%20highlight%20the%20fine-grained%20discrepancies%20among%20VLMs%20in%20chart%0Aunderstanding%20tasks%20and%20point%20to%20specific%20skills%20that%20need%20to%20be%20strengthened%0Ain%20current%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartAB%253A%2520A%2520Benchmark%2520for%2520Chart%2520Grounding%2520%2526%2520Dense%2520Alignment%26entry.906535625%3DAniruddh%2520Bansal%2520and%2520Davit%2520Soselia%2520and%2520Dang%2520Nguyen%2520and%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Charts%2520play%2520an%2520important%2520role%2520in%2520visualization%252C%2520reasoning%252C%2520data%2520analysis%252C%2520and%250Athe%2520exchange%2520of%2520ideas%2520among%2520humans.%2520However%252C%2520existing%2520vision-language%2520models%250A%2528VLMs%2529%2520still%2520lack%2520accurate%2520perception%2520of%2520details%2520and%2520struggle%2520to%2520extract%250Afine-grained%2520structures%2520from%2520charts.%2520Such%2520limitations%2520in%2520chart%2520grounding%2520also%250Ahinder%2520their%2520ability%2520to%2520compare%2520multiple%2520charts%2520and%2520reason%2520over%2520them.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520%2522ChartAlign%2520Benchmark%2520%2528ChartAB%2529%2522%2520to%2520provide%2520a%250Acomprehensive%2520evaluation%2520of%2520VLMs%2520in%2520chart%2520grounding%2520tasks%252C%2520i.e.%252C%2520extracting%250Atabular%2520data%252C%2520localizing%2520visualization%2520elements%252C%2520and%2520recognizing%2520various%250Aattributes%2520from%2520charts%2520of%2520diverse%2520types%2520and%2520complexities.%2520We%2520design%2520a%2520JSON%250Atemplate%2520to%2520facilitate%2520the%2520calculation%2520of%2520evaluation%2520metrics%2520specifically%250Atailored%2520for%2520each%2520grounding%2520task.%2520By%2520incorporating%2520a%2520novel%2520two-stage%2520inference%250Aworkflow%252C%2520the%2520benchmark%2520can%2520further%2520evaluate%2520VLMs%2527%2520capability%2520to%2520align%2520and%250Acompare%2520elements/attributes%2520across%2520two%2520charts.%2520Our%2520analysis%2520of%2520evaluations%2520on%250Aseveral%2520recent%2520VLMs%2520reveals%2520new%2520insights%2520into%2520their%2520perception%2520biases%252C%250Aweaknesses%252C%2520robustness%252C%2520and%2520hallucinations%2520in%2520chart%2520understanding.%2520These%250Afindings%2520highlight%2520the%2520fine-grained%2520discrepancies%2520among%2520VLMs%2520in%2520chart%250Aunderstanding%2520tasks%2520and%2520point%2520to%2520specific%2520skills%2520that%2520need%2520to%2520be%2520strengthened%250Ain%2520current%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartAB%3A%20A%20Benchmark%20for%20Chart%20Grounding%20%26%20Dense%20Alignment&entry.906535625=Aniruddh%20Bansal%20and%20Davit%20Soselia%20and%20Dang%20Nguyen%20and%20Tianyi%20Zhou&entry.1292438233=%20%20Charts%20play%20an%20important%20role%20in%20visualization%2C%20reasoning%2C%20data%20analysis%2C%20and%0Athe%20exchange%20of%20ideas%20among%20humans.%20However%2C%20existing%20vision-language%20models%0A%28VLMs%29%20still%20lack%20accurate%20perception%20of%20details%20and%20struggle%20to%20extract%0Afine-grained%20structures%20from%20charts.%20Such%20limitations%20in%20chart%20grounding%20also%0Ahinder%20their%20ability%20to%20compare%20multiple%20charts%20and%20reason%20over%20them.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20%22ChartAlign%20Benchmark%20%28ChartAB%29%22%20to%20provide%20a%0Acomprehensive%20evaluation%20of%20VLMs%20in%20chart%20grounding%20tasks%2C%20i.e.%2C%20extracting%0Atabular%20data%2C%20localizing%20visualization%20elements%2C%20and%20recognizing%20various%0Aattributes%20from%20charts%20of%20diverse%20types%20and%20complexities.%20We%20design%20a%20JSON%0Atemplate%20to%20facilitate%20the%20calculation%20of%20evaluation%20metrics%20specifically%0Atailored%20for%20each%20grounding%20task.%20By%20incorporating%20a%20novel%20two-stage%20inference%0Aworkflow%2C%20the%20benchmark%20can%20further%20evaluate%20VLMs%27%20capability%20to%20align%20and%0Acompare%20elements/attributes%20across%20two%20charts.%20Our%20analysis%20of%20evaluations%20on%0Aseveral%20recent%20VLMs%20reveals%20new%20insights%20into%20their%20perception%20biases%2C%0Aweaknesses%2C%20robustness%2C%20and%20hallucinations%20in%20chart%20understanding.%20These%0Afindings%20highlight%20the%20fine-grained%20discrepancies%20among%20VLMs%20in%20chart%0Aunderstanding%20tasks%20and%20point%20to%20specific%20skills%20that%20need%20to%20be%20strengthened%0Ain%20current%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26781v1&entry.124074799=Read"},
{"title": "Advancing Local Clustering on Graphs via Compressive Sensing:\n  Semi-supervised and Unsupervised Methods", "author": "Zhaiming Shen and Sung Ha Kang", "abstract": "  Local clustering aims to identify specific substructures within a large graph\nwithout any additional structural information of the graph. These substructures\nare typically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data are given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes, and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate of the art results in the low-label rates regime.\n", "link": "http://arxiv.org/abs/2504.19419v2", "date": "2025-10-30", "relevancy": 2.472, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4704}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Local%20Clustering%20on%20Graphs%20via%20Compressive%20Sensing%3A%0A%20%20Semi-supervised%20and%20Unsupervised%20Methods&body=Title%3A%20Advancing%20Local%20Clustering%20on%20Graphs%20via%20Compressive%20Sensing%3A%0A%20%20Semi-supervised%20and%20Unsupervised%20Methods%0AAuthor%3A%20Zhaiming%20Shen%20and%20Sung%20Ha%20Kang%0AAbstract%3A%20%20%20Local%20clustering%20aims%20to%20identify%20specific%20substructures%20within%20a%20large%20graph%0Awithout%20any%20additional%20structural%20information%20of%20the%20graph.%20These%20substructures%0Aare%20typically%20small%20compared%20to%20the%20overall%20graph%2C%20enabling%20the%20problem%20to%20be%0Aapproached%20by%20finding%20a%20sparse%20solution%20to%20a%20linear%20system%20associated%20with%20the%0Agraph%20Laplacian.%20In%20this%20work%2C%20we%20first%20propose%20a%20method%20for%20identifying%0Aspecific%20local%20clusters%20when%20very%20few%20labeled%20data%20are%20given%2C%20which%20we%20term%0Asemi-supervised%20local%20clustering.%20We%20then%20extend%20this%20approach%20to%20the%0Aunsupervised%20setting%20when%20no%20prior%20information%20on%20labels%20is%20available.%20The%0Aproposed%20methods%20involve%20randomly%20sampling%20the%20graph%2C%20applying%20diffusion%0Athrough%20local%20cluster%20extraction%2C%20then%20examining%20the%20overlap%20among%20the%20results%0Ato%20find%20each%20cluster.%20We%20establish%20the%20co-membership%20conditions%20for%20any%20pair%20of%0Anodes%2C%20and%20rigorously%20prove%20the%20correctness%20of%20our%20methods.%20Additionally%2C%20we%0Aconduct%20extensive%20experiments%20to%20demonstrate%20that%20the%20proposed%20methods%20achieve%0Astate%20of%20the%20art%20results%20in%20the%20low-label%20rates%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Local%2520Clustering%2520on%2520Graphs%2520via%2520Compressive%2520Sensing%253A%250A%2520%2520Semi-supervised%2520and%2520Unsupervised%2520Methods%26entry.906535625%3DZhaiming%2520Shen%2520and%2520Sung%2520Ha%2520Kang%26entry.1292438233%3D%2520%2520Local%2520clustering%2520aims%2520to%2520identify%2520specific%2520substructures%2520within%2520a%2520large%2520graph%250Awithout%2520any%2520additional%2520structural%2520information%2520of%2520the%2520graph.%2520These%2520substructures%250Aare%2520typically%2520small%2520compared%2520to%2520the%2520overall%2520graph%252C%2520enabling%2520the%2520problem%2520to%2520be%250Aapproached%2520by%2520finding%2520a%2520sparse%2520solution%2520to%2520a%2520linear%2520system%2520associated%2520with%2520the%250Agraph%2520Laplacian.%2520In%2520this%2520work%252C%2520we%2520first%2520propose%2520a%2520method%2520for%2520identifying%250Aspecific%2520local%2520clusters%2520when%2520very%2520few%2520labeled%2520data%2520are%2520given%252C%2520which%2520we%2520term%250Asemi-supervised%2520local%2520clustering.%2520We%2520then%2520extend%2520this%2520approach%2520to%2520the%250Aunsupervised%2520setting%2520when%2520no%2520prior%2520information%2520on%2520labels%2520is%2520available.%2520The%250Aproposed%2520methods%2520involve%2520randomly%2520sampling%2520the%2520graph%252C%2520applying%2520diffusion%250Athrough%2520local%2520cluster%2520extraction%252C%2520then%2520examining%2520the%2520overlap%2520among%2520the%2520results%250Ato%2520find%2520each%2520cluster.%2520We%2520establish%2520the%2520co-membership%2520conditions%2520for%2520any%2520pair%2520of%250Anodes%252C%2520and%2520rigorously%2520prove%2520the%2520correctness%2520of%2520our%2520methods.%2520Additionally%252C%2520we%250Aconduct%2520extensive%2520experiments%2520to%2520demonstrate%2520that%2520the%2520proposed%2520methods%2520achieve%250Astate%2520of%2520the%2520art%2520results%2520in%2520the%2520low-label%2520rates%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Local%20Clustering%20on%20Graphs%20via%20Compressive%20Sensing%3A%0A%20%20Semi-supervised%20and%20Unsupervised%20Methods&entry.906535625=Zhaiming%20Shen%20and%20Sung%20Ha%20Kang&entry.1292438233=%20%20Local%20clustering%20aims%20to%20identify%20specific%20substructures%20within%20a%20large%20graph%0Awithout%20any%20additional%20structural%20information%20of%20the%20graph.%20These%20substructures%0Aare%20typically%20small%20compared%20to%20the%20overall%20graph%2C%20enabling%20the%20problem%20to%20be%0Aapproached%20by%20finding%20a%20sparse%20solution%20to%20a%20linear%20system%20associated%20with%20the%0Agraph%20Laplacian.%20In%20this%20work%2C%20we%20first%20propose%20a%20method%20for%20identifying%0Aspecific%20local%20clusters%20when%20very%20few%20labeled%20data%20are%20given%2C%20which%20we%20term%0Asemi-supervised%20local%20clustering.%20We%20then%20extend%20this%20approach%20to%20the%0Aunsupervised%20setting%20when%20no%20prior%20information%20on%20labels%20is%20available.%20The%0Aproposed%20methods%20involve%20randomly%20sampling%20the%20graph%2C%20applying%20diffusion%0Athrough%20local%20cluster%20extraction%2C%20then%20examining%20the%20overlap%20among%20the%20results%0Ato%20find%20each%20cluster.%20We%20establish%20the%20co-membership%20conditions%20for%20any%20pair%20of%0Anodes%2C%20and%20rigorously%20prove%20the%20correctness%20of%20our%20methods.%20Additionally%2C%20we%0Aconduct%20extensive%20experiments%20to%20demonstrate%20that%20the%20proposed%20methods%20achieve%0Astate%20of%20the%20art%20results%20in%20the%20low-label%20rates%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19419v2&entry.124074799=Read"},
{"title": "PepCompass: Navigating peptide embedding spaces using Riemannian\n  Geometry", "author": "Marcin Mo\u017cejko and Adam Bielecki and Jurand Pr\u0105dzy\u0144ski and Marcin Traskowski and Antoni Janowski and Hyun-Su Lee and Marcelo Der Torossian Torres and Micha\u0142 Kmicikiewicz and Paulina Szymczak and Karol Jurasz and Micha\u0142 Kucharczyk and Cesar de la Fuente-Nunez and Ewa Szczurek", "abstract": "  Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.\n", "link": "http://arxiv.org/abs/2510.01988v3", "date": "2025-10-30", "relevancy": 2.4337, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4811}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PepCompass%3A%20Navigating%20peptide%20embedding%20spaces%20using%20Riemannian%0A%20%20Geometry&body=Title%3A%20PepCompass%3A%20Navigating%20peptide%20embedding%20spaces%20using%20Riemannian%0A%20%20Geometry%0AAuthor%3A%20Marcin%20Mo%C5%BCejko%20and%20Adam%20Bielecki%20and%20Jurand%20Pr%C4%85dzy%C5%84ski%20and%20Marcin%20Traskowski%20and%20Antoni%20Janowski%20and%20Hyun-Su%20Lee%20and%20Marcelo%20Der%20Torossian%20Torres%20and%20Micha%C5%82%20Kmicikiewicz%20and%20Paulina%20Szymczak%20and%20Karol%20Jurasz%20and%20Micha%C5%82%20Kucharczyk%20and%20Cesar%20de%20la%20Fuente-Nunez%20and%20Ewa%20Szczurek%0AAbstract%3A%20%20%20Antimicrobial%20peptide%20discovery%20is%20challenged%20by%20the%20astronomical%20size%20of%0Apeptide%20space%20and%20the%20relative%20scarcity%20of%20active%20peptides.%20Generative%20models%0Aprovide%20continuous%20latent%20%22maps%22%20of%20peptide%20space%2C%20but%20conventionally%20ignore%0Adecoder-induced%20geometry%20and%20rely%20on%20flat%20Euclidean%20metrics%2C%20rendering%0Aexploration%20and%20optimization%20distorted%20and%20inefficient.%20Prior%20manifold-based%0Aremedies%20assume%20fixed%20intrinsic%20dimensionality%2C%20which%20critically%20fails%20in%0Apractice%20for%20peptide%20data.%20Here%2C%20we%20introduce%20PepCompass%2C%20a%20geometry-aware%0Aframework%20for%20peptide%20exploration%20and%20optimization.%20At%20its%20core%2C%20we%20define%20a%0AUnion%20of%20%24%5Ckappa%24-Stable%20Riemannian%20Manifolds%20%24%5Cmathbb%7BM%7D%5E%7B%5Ckappa%7D%24%2C%20a%20family%0Aof%20decoder-induced%20manifolds%20that%20captures%20local%20geometry%20while%20ensuring%0Acomputational%20stability.%20We%20propose%20two%20local%20exploration%20methods%3A%20Second-Order%0ARiemannian%20Brownian%20Efficient%20Sampling%2C%20which%20provides%20a%20convergent%0Asecond-order%20approximation%20to%20Riemannian%20Brownian%20motion%2C%20and%20Mutation%0AEnumeration%20in%20Tangent%20Space%2C%20which%20reinterprets%20tangent%20directions%20as%20discrete%0Aamino-acid%20substitutions.%20Combining%20these%20yields%20Local%20Enumeration%20Bayesian%0AOptimization%20%28LE-BO%29%2C%20an%20efficient%20algorithm%20for%20local%20activity%20optimization.%0AFinally%2C%20we%20introduce%20Potential-minimizing%20Geodesic%20Search%20%28PoGS%29%2C%20which%0Ainterpolates%20between%20prototype%20embeddings%20along%20property-enriched%20geodesics%2C%0Abiasing%20discovery%20toward%20seeds%2C%20i.e.%20peptides%20with%20favorable%20activity.%20In-vitro%0Avalidation%20confirms%20the%20effectiveness%20of%20PepCompass%3A%20PoGS%20yields%20four%20novel%0Aseeds%2C%20and%20subsequent%20optimization%20with%20LE-BO%20discovers%2025%20highly%20active%0Apeptides%20with%20broad-spectrum%20activity%2C%20including%20against%20resistant%20bacterial%0Astrains.%20These%20results%20demonstrate%20that%20geometry-informed%20exploration%20provides%0Aa%20powerful%20new%20paradigm%20for%20antimicrobial%20peptide%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01988v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPepCompass%253A%2520Navigating%2520peptide%2520embedding%2520spaces%2520using%2520Riemannian%250A%2520%2520Geometry%26entry.906535625%3DMarcin%2520Mo%25C5%25BCejko%2520and%2520Adam%2520Bielecki%2520and%2520Jurand%2520Pr%25C4%2585dzy%25C5%2584ski%2520and%2520Marcin%2520Traskowski%2520and%2520Antoni%2520Janowski%2520and%2520Hyun-Su%2520Lee%2520and%2520Marcelo%2520Der%2520Torossian%2520Torres%2520and%2520Micha%25C5%2582%2520Kmicikiewicz%2520and%2520Paulina%2520Szymczak%2520and%2520Karol%2520Jurasz%2520and%2520Micha%25C5%2582%2520Kucharczyk%2520and%2520Cesar%2520de%2520la%2520Fuente-Nunez%2520and%2520Ewa%2520Szczurek%26entry.1292438233%3D%2520%2520Antimicrobial%2520peptide%2520discovery%2520is%2520challenged%2520by%2520the%2520astronomical%2520size%2520of%250Apeptide%2520space%2520and%2520the%2520relative%2520scarcity%2520of%2520active%2520peptides.%2520Generative%2520models%250Aprovide%2520continuous%2520latent%2520%2522maps%2522%2520of%2520peptide%2520space%252C%2520but%2520conventionally%2520ignore%250Adecoder-induced%2520geometry%2520and%2520rely%2520on%2520flat%2520Euclidean%2520metrics%252C%2520rendering%250Aexploration%2520and%2520optimization%2520distorted%2520and%2520inefficient.%2520Prior%2520manifold-based%250Aremedies%2520assume%2520fixed%2520intrinsic%2520dimensionality%252C%2520which%2520critically%2520fails%2520in%250Apractice%2520for%2520peptide%2520data.%2520Here%252C%2520we%2520introduce%2520PepCompass%252C%2520a%2520geometry-aware%250Aframework%2520for%2520peptide%2520exploration%2520and%2520optimization.%2520At%2520its%2520core%252C%2520we%2520define%2520a%250AUnion%2520of%2520%2524%255Ckappa%2524-Stable%2520Riemannian%2520Manifolds%2520%2524%255Cmathbb%257BM%257D%255E%257B%255Ckappa%257D%2524%252C%2520a%2520family%250Aof%2520decoder-induced%2520manifolds%2520that%2520captures%2520local%2520geometry%2520while%2520ensuring%250Acomputational%2520stability.%2520We%2520propose%2520two%2520local%2520exploration%2520methods%253A%2520Second-Order%250ARiemannian%2520Brownian%2520Efficient%2520Sampling%252C%2520which%2520provides%2520a%2520convergent%250Asecond-order%2520approximation%2520to%2520Riemannian%2520Brownian%2520motion%252C%2520and%2520Mutation%250AEnumeration%2520in%2520Tangent%2520Space%252C%2520which%2520reinterprets%2520tangent%2520directions%2520as%2520discrete%250Aamino-acid%2520substitutions.%2520Combining%2520these%2520yields%2520Local%2520Enumeration%2520Bayesian%250AOptimization%2520%2528LE-BO%2529%252C%2520an%2520efficient%2520algorithm%2520for%2520local%2520activity%2520optimization.%250AFinally%252C%2520we%2520introduce%2520Potential-minimizing%2520Geodesic%2520Search%2520%2528PoGS%2529%252C%2520which%250Ainterpolates%2520between%2520prototype%2520embeddings%2520along%2520property-enriched%2520geodesics%252C%250Abiasing%2520discovery%2520toward%2520seeds%252C%2520i.e.%2520peptides%2520with%2520favorable%2520activity.%2520In-vitro%250Avalidation%2520confirms%2520the%2520effectiveness%2520of%2520PepCompass%253A%2520PoGS%2520yields%2520four%2520novel%250Aseeds%252C%2520and%2520subsequent%2520optimization%2520with%2520LE-BO%2520discovers%252025%2520highly%2520active%250Apeptides%2520with%2520broad-spectrum%2520activity%252C%2520including%2520against%2520resistant%2520bacterial%250Astrains.%2520These%2520results%2520demonstrate%2520that%2520geometry-informed%2520exploration%2520provides%250Aa%2520powerful%2520new%2520paradigm%2520for%2520antimicrobial%2520peptide%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01988v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PepCompass%3A%20Navigating%20peptide%20embedding%20spaces%20using%20Riemannian%0A%20%20Geometry&entry.906535625=Marcin%20Mo%C5%BCejko%20and%20Adam%20Bielecki%20and%20Jurand%20Pr%C4%85dzy%C5%84ski%20and%20Marcin%20Traskowski%20and%20Antoni%20Janowski%20and%20Hyun-Su%20Lee%20and%20Marcelo%20Der%20Torossian%20Torres%20and%20Micha%C5%82%20Kmicikiewicz%20and%20Paulina%20Szymczak%20and%20Karol%20Jurasz%20and%20Micha%C5%82%20Kucharczyk%20and%20Cesar%20de%20la%20Fuente-Nunez%20and%20Ewa%20Szczurek&entry.1292438233=%20%20Antimicrobial%20peptide%20discovery%20is%20challenged%20by%20the%20astronomical%20size%20of%0Apeptide%20space%20and%20the%20relative%20scarcity%20of%20active%20peptides.%20Generative%20models%0Aprovide%20continuous%20latent%20%22maps%22%20of%20peptide%20space%2C%20but%20conventionally%20ignore%0Adecoder-induced%20geometry%20and%20rely%20on%20flat%20Euclidean%20metrics%2C%20rendering%0Aexploration%20and%20optimization%20distorted%20and%20inefficient.%20Prior%20manifold-based%0Aremedies%20assume%20fixed%20intrinsic%20dimensionality%2C%20which%20critically%20fails%20in%0Apractice%20for%20peptide%20data.%20Here%2C%20we%20introduce%20PepCompass%2C%20a%20geometry-aware%0Aframework%20for%20peptide%20exploration%20and%20optimization.%20At%20its%20core%2C%20we%20define%20a%0AUnion%20of%20%24%5Ckappa%24-Stable%20Riemannian%20Manifolds%20%24%5Cmathbb%7BM%7D%5E%7B%5Ckappa%7D%24%2C%20a%20family%0Aof%20decoder-induced%20manifolds%20that%20captures%20local%20geometry%20while%20ensuring%0Acomputational%20stability.%20We%20propose%20two%20local%20exploration%20methods%3A%20Second-Order%0ARiemannian%20Brownian%20Efficient%20Sampling%2C%20which%20provides%20a%20convergent%0Asecond-order%20approximation%20to%20Riemannian%20Brownian%20motion%2C%20and%20Mutation%0AEnumeration%20in%20Tangent%20Space%2C%20which%20reinterprets%20tangent%20directions%20as%20discrete%0Aamino-acid%20substitutions.%20Combining%20these%20yields%20Local%20Enumeration%20Bayesian%0AOptimization%20%28LE-BO%29%2C%20an%20efficient%20algorithm%20for%20local%20activity%20optimization.%0AFinally%2C%20we%20introduce%20Potential-minimizing%20Geodesic%20Search%20%28PoGS%29%2C%20which%0Ainterpolates%20between%20prototype%20embeddings%20along%20property-enriched%20geodesics%2C%0Abiasing%20discovery%20toward%20seeds%2C%20i.e.%20peptides%20with%20favorable%20activity.%20In-vitro%0Avalidation%20confirms%20the%20effectiveness%20of%20PepCompass%3A%20PoGS%20yields%20four%20novel%0Aseeds%2C%20and%20subsequent%20optimization%20with%20LE-BO%20discovers%2025%20highly%20active%0Apeptides%20with%20broad-spectrum%20activity%2C%20including%20against%20resistant%20bacterial%0Astrains.%20These%20results%20demonstrate%20that%20geometry-informed%20exploration%20provides%0Aa%20powerful%20new%20paradigm%20for%20antimicrobial%20peptide%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01988v3&entry.124074799=Read"},
{"title": "Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for\n  Time Series Classification", "author": "Andreas Auer and Daniel Klotz and Sebastinan B\u00f6ck and Sepp Hochreiter", "abstract": "  Recent research on time series foundation models has primarily focused on\nforecasting, leaving it unclear how generalizable their learned representations\nare. In this study, we examine whether frozen pre-trained forecasting models\ncan provide effective representations for classification. To this end, we\ncompare different representation extraction strategies and introduce two\nmodel-agnostic embedding augmentations. Our experiments show that the best\nforecasting models achieve classification accuracy that matches or even\nsurpasses that of state-of-the-art models pre-trained specifically for\nclassification. Moreover, we observe a positive correlation between forecasting\nand classification performance. These findings challenge the assumption that\ntask-specific pre-training is necessary, and suggest that learning to forecast\nmay provide a powerful route toward constructing general-purpose time series\nfoundation models.\n", "link": "http://arxiv.org/abs/2510.26777v1", "date": "2025-10-30", "relevancy": 2.3817, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Forecasting%20Models%3A%20Strong%20Zero-Shot%20Feature%20Extractors%20for%0A%20%20Time%20Series%20Classification&body=Title%3A%20Pre-trained%20Forecasting%20Models%3A%20Strong%20Zero-Shot%20Feature%20Extractors%20for%0A%20%20Time%20Series%20Classification%0AAuthor%3A%20Andreas%20Auer%20and%20Daniel%20Klotz%20and%20Sebastinan%20B%C3%B6ck%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Recent%20research%20on%20time%20series%20foundation%20models%20has%20primarily%20focused%20on%0Aforecasting%2C%20leaving%20it%20unclear%20how%20generalizable%20their%20learned%20representations%0Aare.%20In%20this%20study%2C%20we%20examine%20whether%20frozen%20pre-trained%20forecasting%20models%0Acan%20provide%20effective%20representations%20for%20classification.%20To%20this%20end%2C%20we%0Acompare%20different%20representation%20extraction%20strategies%20and%20introduce%20two%0Amodel-agnostic%20embedding%20augmentations.%20Our%20experiments%20show%20that%20the%20best%0Aforecasting%20models%20achieve%20classification%20accuracy%20that%20matches%20or%20even%0Asurpasses%20that%20of%20state-of-the-art%20models%20pre-trained%20specifically%20for%0Aclassification.%20Moreover%2C%20we%20observe%20a%20positive%20correlation%20between%20forecasting%0Aand%20classification%20performance.%20These%20findings%20challenge%20the%20assumption%20that%0Atask-specific%20pre-training%20is%20necessary%2C%20and%20suggest%20that%20learning%20to%20forecast%0Amay%20provide%20a%20powerful%20route%20toward%20constructing%20general-purpose%20time%20series%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-trained%2520Forecasting%2520Models%253A%2520Strong%2520Zero-Shot%2520Feature%2520Extractors%2520for%250A%2520%2520Time%2520Series%2520Classification%26entry.906535625%3DAndreas%2520Auer%2520and%2520Daniel%2520Klotz%2520and%2520Sebastinan%2520B%25C3%25B6ck%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520time%2520series%2520foundation%2520models%2520has%2520primarily%2520focused%2520on%250Aforecasting%252C%2520leaving%2520it%2520unclear%2520how%2520generalizable%2520their%2520learned%2520representations%250Aare.%2520In%2520this%2520study%252C%2520we%2520examine%2520whether%2520frozen%2520pre-trained%2520forecasting%2520models%250Acan%2520provide%2520effective%2520representations%2520for%2520classification.%2520To%2520this%2520end%252C%2520we%250Acompare%2520different%2520representation%2520extraction%2520strategies%2520and%2520introduce%2520two%250Amodel-agnostic%2520embedding%2520augmentations.%2520Our%2520experiments%2520show%2520that%2520the%2520best%250Aforecasting%2520models%2520achieve%2520classification%2520accuracy%2520that%2520matches%2520or%2520even%250Asurpasses%2520that%2520of%2520state-of-the-art%2520models%2520pre-trained%2520specifically%2520for%250Aclassification.%2520Moreover%252C%2520we%2520observe%2520a%2520positive%2520correlation%2520between%2520forecasting%250Aand%2520classification%2520performance.%2520These%2520findings%2520challenge%2520the%2520assumption%2520that%250Atask-specific%2520pre-training%2520is%2520necessary%252C%2520and%2520suggest%2520that%2520learning%2520to%2520forecast%250Amay%2520provide%2520a%2520powerful%2520route%2520toward%2520constructing%2520general-purpose%2520time%2520series%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Forecasting%20Models%3A%20Strong%20Zero-Shot%20Feature%20Extractors%20for%0A%20%20Time%20Series%20Classification&entry.906535625=Andreas%20Auer%20and%20Daniel%20Klotz%20and%20Sebastinan%20B%C3%B6ck%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Recent%20research%20on%20time%20series%20foundation%20models%20has%20primarily%20focused%20on%0Aforecasting%2C%20leaving%20it%20unclear%20how%20generalizable%20their%20learned%20representations%0Aare.%20In%20this%20study%2C%20we%20examine%20whether%20frozen%20pre-trained%20forecasting%20models%0Acan%20provide%20effective%20representations%20for%20classification.%20To%20this%20end%2C%20we%0Acompare%20different%20representation%20extraction%20strategies%20and%20introduce%20two%0Amodel-agnostic%20embedding%20augmentations.%20Our%20experiments%20show%20that%20the%20best%0Aforecasting%20models%20achieve%20classification%20accuracy%20that%20matches%20or%20even%0Asurpasses%20that%20of%20state-of-the-art%20models%20pre-trained%20specifically%20for%0Aclassification.%20Moreover%2C%20we%20observe%20a%20positive%20correlation%20between%20forecasting%0Aand%20classification%20performance.%20These%20findings%20challenge%20the%20assumption%20that%0Atask-specific%20pre-training%20is%20necessary%2C%20and%20suggest%20that%20learning%20to%20forecast%0Amay%20provide%20a%20powerful%20route%20toward%20constructing%20general-purpose%20time%20series%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26777v1&entry.124074799=Read"},
{"title": "HEIR: Learning Graph-Based Motion Hierarchies", "author": "Cheng Zheng and William Koch and Baiang Li and Felix Heide", "abstract": "  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n", "link": "http://arxiv.org/abs/2510.26786v1", "date": "2025-10-30", "relevancy": 2.3637, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6447}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEIR%3A%20Learning%20Graph-Based%20Motion%20Hierarchies&body=Title%3A%20HEIR%3A%20Learning%20Graph-Based%20Motion%20Hierarchies%0AAuthor%3A%20Cheng%20Zheng%20and%20William%20Koch%20and%20Baiang%20Li%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Hierarchical%20structures%20of%20motion%20exist%20across%20research%20fields%2C%20including%0Acomputer%20vision%2C%20graphics%2C%20and%20robotics%2C%20where%20complex%20dynamics%20typically%20arise%0Afrom%20coordinated%20interactions%20among%20simpler%20motion%20components.%20Existing%20methods%0Ato%20model%20such%20dynamics%20typically%20rely%20on%20manually-defined%20or%20heuristic%0Ahierarchies%20with%20fixed%20motion%20primitives%2C%20limiting%20their%20generalizability%0Aacross%20different%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20general%20hierarchical%20motion%0Amodeling%20method%20that%20learns%20structured%2C%20interpretable%20motion%20relationships%0Adirectly%20from%20data.%20Our%20method%20represents%20observed%20motions%20using%20graph-based%0Ahierarchies%2C%20explicitly%20decomposing%20global%20absolute%20motions%20into%0Aparent-inherited%20patterns%20and%20local%20motion%20residuals.%20We%20formulate%20hierarchy%0Ainference%20as%20a%20differentiable%20graph%20learning%20problem%2C%20where%20vertices%20represent%0Aelemental%20motions%20and%20directed%20edges%20capture%20learned%20parent-child%20dependencies%0Athrough%20graph%20neural%20networks.%20We%20evaluate%20our%20hierarchical%20reconstruction%0Aapproach%20on%20three%20examples%3A%201D%20translational%20motion%2C%202D%20rotational%20motion%2C%20and%0Adynamic%203D%20scene%20deformation%20via%20Gaussian%20splatting.%20Experimental%20results%20show%0Athat%20our%20method%20reconstructs%20the%20intrinsic%20motion%20hierarchy%20in%201D%20and%202D%20cases%2C%0Aand%20produces%20more%20realistic%20and%20interpretable%20deformations%20compared%20to%20the%0Abaseline%20on%20dynamic%203D%20Gaussian%20splatting%20scenes.%20By%20providing%20an%20adaptable%2C%0Adata-driven%20hierarchical%20modeling%20paradigm%2C%20our%20method%20offers%20a%20formulation%0Aapplicable%20to%20a%20broad%20range%20of%20motion-centric%20tasks.%20Project%20Page%3A%0Ahttps%3A//light.princeton.edu/HEIR/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEIR%253A%2520Learning%2520Graph-Based%2520Motion%2520Hierarchies%26entry.906535625%3DCheng%2520Zheng%2520and%2520William%2520Koch%2520and%2520Baiang%2520Li%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Hierarchical%2520structures%2520of%2520motion%2520exist%2520across%2520research%2520fields%252C%2520including%250Acomputer%2520vision%252C%2520graphics%252C%2520and%2520robotics%252C%2520where%2520complex%2520dynamics%2520typically%2520arise%250Afrom%2520coordinated%2520interactions%2520among%2520simpler%2520motion%2520components.%2520Existing%2520methods%250Ato%2520model%2520such%2520dynamics%2520typically%2520rely%2520on%2520manually-defined%2520or%2520heuristic%250Ahierarchies%2520with%2520fixed%2520motion%2520primitives%252C%2520limiting%2520their%2520generalizability%250Aacross%2520different%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%2520hierarchical%2520motion%250Amodeling%2520method%2520that%2520learns%2520structured%252C%2520interpretable%2520motion%2520relationships%250Adirectly%2520from%2520data.%2520Our%2520method%2520represents%2520observed%2520motions%2520using%2520graph-based%250Ahierarchies%252C%2520explicitly%2520decomposing%2520global%2520absolute%2520motions%2520into%250Aparent-inherited%2520patterns%2520and%2520local%2520motion%2520residuals.%2520We%2520formulate%2520hierarchy%250Ainference%2520as%2520a%2520differentiable%2520graph%2520learning%2520problem%252C%2520where%2520vertices%2520represent%250Aelemental%2520motions%2520and%2520directed%2520edges%2520capture%2520learned%2520parent-child%2520dependencies%250Athrough%2520graph%2520neural%2520networks.%2520We%2520evaluate%2520our%2520hierarchical%2520reconstruction%250Aapproach%2520on%2520three%2520examples%253A%25201D%2520translational%2520motion%252C%25202D%2520rotational%2520motion%252C%2520and%250Adynamic%25203D%2520scene%2520deformation%2520via%2520Gaussian%2520splatting.%2520Experimental%2520results%2520show%250Athat%2520our%2520method%2520reconstructs%2520the%2520intrinsic%2520motion%2520hierarchy%2520in%25201D%2520and%25202D%2520cases%252C%250Aand%2520produces%2520more%2520realistic%2520and%2520interpretable%2520deformations%2520compared%2520to%2520the%250Abaseline%2520on%2520dynamic%25203D%2520Gaussian%2520splatting%2520scenes.%2520By%2520providing%2520an%2520adaptable%252C%250Adata-driven%2520hierarchical%2520modeling%2520paradigm%252C%2520our%2520method%2520offers%2520a%2520formulation%250Aapplicable%2520to%2520a%2520broad%2520range%2520of%2520motion-centric%2520tasks.%2520Project%2520Page%253A%250Ahttps%253A//light.princeton.edu/HEIR/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEIR%3A%20Learning%20Graph-Based%20Motion%20Hierarchies&entry.906535625=Cheng%20Zheng%20and%20William%20Koch%20and%20Baiang%20Li%20and%20Felix%20Heide&entry.1292438233=%20%20Hierarchical%20structures%20of%20motion%20exist%20across%20research%20fields%2C%20including%0Acomputer%20vision%2C%20graphics%2C%20and%20robotics%2C%20where%20complex%20dynamics%20typically%20arise%0Afrom%20coordinated%20interactions%20among%20simpler%20motion%20components.%20Existing%20methods%0Ato%20model%20such%20dynamics%20typically%20rely%20on%20manually-defined%20or%20heuristic%0Ahierarchies%20with%20fixed%20motion%20primitives%2C%20limiting%20their%20generalizability%0Aacross%20different%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20general%20hierarchical%20motion%0Amodeling%20method%20that%20learns%20structured%2C%20interpretable%20motion%20relationships%0Adirectly%20from%20data.%20Our%20method%20represents%20observed%20motions%20using%20graph-based%0Ahierarchies%2C%20explicitly%20decomposing%20global%20absolute%20motions%20into%0Aparent-inherited%20patterns%20and%20local%20motion%20residuals.%20We%20formulate%20hierarchy%0Ainference%20as%20a%20differentiable%20graph%20learning%20problem%2C%20where%20vertices%20represent%0Aelemental%20motions%20and%20directed%20edges%20capture%20learned%20parent-child%20dependencies%0Athrough%20graph%20neural%20networks.%20We%20evaluate%20our%20hierarchical%20reconstruction%0Aapproach%20on%20three%20examples%3A%201D%20translational%20motion%2C%202D%20rotational%20motion%2C%20and%0Adynamic%203D%20scene%20deformation%20via%20Gaussian%20splatting.%20Experimental%20results%20show%0Athat%20our%20method%20reconstructs%20the%20intrinsic%20motion%20hierarchy%20in%201D%20and%202D%20cases%2C%0Aand%20produces%20more%20realistic%20and%20interpretable%20deformations%20compared%20to%20the%0Abaseline%20on%20dynamic%203D%20Gaussian%20splatting%20scenes.%20By%20providing%20an%20adaptable%2C%0Adata-driven%20hierarchical%20modeling%20paradigm%2C%20our%20method%20offers%20a%20formulation%0Aapplicable%20to%20a%20broad%20range%20of%20motion-centric%20tasks.%20Project%20Page%3A%0Ahttps%3A//light.princeton.edu/HEIR/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26786v1&entry.124074799=Read"},
{"title": "Value Drifts: Tracing Value Alignment During LLM Post-Training", "author": "Mehar Bhatia and Shravan Nayak and Gaurav Kamath and Marius Mosbach and Karolina Sta\u0144czak and Vered Shwartz and Siva Reddy", "abstract": "  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n", "link": "http://arxiv.org/abs/2510.26707v1", "date": "2025-10-30", "relevancy": 2.3188, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Value%20Drifts%3A%20Tracing%20Value%20Alignment%20During%20LLM%20Post-Training&body=Title%3A%20Value%20Drifts%3A%20Tracing%20Value%20Alignment%20During%20LLM%20Post-Training%0AAuthor%3A%20Mehar%20Bhatia%20and%20Shravan%20Nayak%20and%20Gaurav%20Kamath%20and%20Marius%20Mosbach%20and%20Karolina%20Sta%C5%84czak%20and%20Vered%20Shwartz%20and%20Siva%20Reddy%0AAbstract%3A%20%20%20As%20LLMs%20occupy%20an%20increasingly%20important%20role%20in%20society%2C%20they%20are%20more%20and%0Amore%20confronted%20with%20questions%20that%20require%20them%20not%20only%20to%20draw%20on%20their%0Ageneral%20knowledge%20but%20also%20to%20align%20with%20certain%20human%20value%20systems.%0ATherefore%2C%20studying%20the%20alignment%20of%20LLMs%20with%20human%20values%20has%20become%20a%0Acrucial%20field%20of%20inquiry.%20Prior%20work%2C%20however%2C%20mostly%20focuses%20on%20evaluating%20the%0Aalignment%20of%20fully%20trained%20models%2C%20overlooking%20the%20training%20dynamics%20by%20which%0Amodels%20learn%20to%20express%20human%20values.%20In%20this%20work%2C%20we%20investigate%20how%20and%20at%0Awhich%20stage%20value%20alignment%20arises%20during%20the%20course%20of%20a%20model%27s%0Apost-training.%20Our%20analysis%20disentangles%20the%20effects%20of%20post-training%0Aalgorithms%20and%20datasets%2C%20measuring%20both%20the%20magnitude%20and%20time%20of%20value%20drifts%0Aduring%20training.%20Experimenting%20with%20Llama-3%20and%20Qwen-3%20models%20of%20different%0Asizes%20and%20popular%20supervised%20fine-tuning%20%28SFT%29%20and%20preference%20optimization%0Adatasets%20and%20algorithms%2C%20we%20find%20that%20the%20SFT%20phase%20generally%20establishes%20a%0Amodel%27s%20values%2C%20and%20subsequent%20preference%20optimization%20rarely%20re-aligns%20these%0Avalues.%20Furthermore%2C%20using%20a%20synthetic%20preference%20dataset%20that%20enables%0Acontrolled%20manipulation%20of%20values%2C%20we%20find%20that%20different%20preference%0Aoptimization%20algorithms%20lead%20to%20different%20value%20alignment%20outcomes%2C%20even%20when%0Apreference%20data%20is%20held%20constant.%20Our%20findings%20provide%20actionable%20insights%20into%0Ahow%20values%20are%20learned%20during%20post-training%20and%20help%20to%20inform%20data%20curation%2C%0Aas%20well%20as%20the%20selection%20of%20models%20and%20algorithms%20for%20preference%20optimization%0Ato%20improve%20model%20alignment%20to%20human%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValue%2520Drifts%253A%2520Tracing%2520Value%2520Alignment%2520During%2520LLM%2520Post-Training%26entry.906535625%3DMehar%2520Bhatia%2520and%2520Shravan%2520Nayak%2520and%2520Gaurav%2520Kamath%2520and%2520Marius%2520Mosbach%2520and%2520Karolina%2520Sta%25C5%2584czak%2520and%2520Vered%2520Shwartz%2520and%2520Siva%2520Reddy%26entry.1292438233%3D%2520%2520As%2520LLMs%2520occupy%2520an%2520increasingly%2520important%2520role%2520in%2520society%252C%2520they%2520are%2520more%2520and%250Amore%2520confronted%2520with%2520questions%2520that%2520require%2520them%2520not%2520only%2520to%2520draw%2520on%2520their%250Ageneral%2520knowledge%2520but%2520also%2520to%2520align%2520with%2520certain%2520human%2520value%2520systems.%250ATherefore%252C%2520studying%2520the%2520alignment%2520of%2520LLMs%2520with%2520human%2520values%2520has%2520become%2520a%250Acrucial%2520field%2520of%2520inquiry.%2520Prior%2520work%252C%2520however%252C%2520mostly%2520focuses%2520on%2520evaluating%2520the%250Aalignment%2520of%2520fully%2520trained%2520models%252C%2520overlooking%2520the%2520training%2520dynamics%2520by%2520which%250Amodels%2520learn%2520to%2520express%2520human%2520values.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520and%2520at%250Awhich%2520stage%2520value%2520alignment%2520arises%2520during%2520the%2520course%2520of%2520a%2520model%2527s%250Apost-training.%2520Our%2520analysis%2520disentangles%2520the%2520effects%2520of%2520post-training%250Aalgorithms%2520and%2520datasets%252C%2520measuring%2520both%2520the%2520magnitude%2520and%2520time%2520of%2520value%2520drifts%250Aduring%2520training.%2520Experimenting%2520with%2520Llama-3%2520and%2520Qwen-3%2520models%2520of%2520different%250Asizes%2520and%2520popular%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520preference%2520optimization%250Adatasets%2520and%2520algorithms%252C%2520we%2520find%2520that%2520the%2520SFT%2520phase%2520generally%2520establishes%2520a%250Amodel%2527s%2520values%252C%2520and%2520subsequent%2520preference%2520optimization%2520rarely%2520re-aligns%2520these%250Avalues.%2520Furthermore%252C%2520using%2520a%2520synthetic%2520preference%2520dataset%2520that%2520enables%250Acontrolled%2520manipulation%2520of%2520values%252C%2520we%2520find%2520that%2520different%2520preference%250Aoptimization%2520algorithms%2520lead%2520to%2520different%2520value%2520alignment%2520outcomes%252C%2520even%2520when%250Apreference%2520data%2520is%2520held%2520constant.%2520Our%2520findings%2520provide%2520actionable%2520insights%2520into%250Ahow%2520values%2520are%2520learned%2520during%2520post-training%2520and%2520help%2520to%2520inform%2520data%2520curation%252C%250Aas%2520well%2520as%2520the%2520selection%2520of%2520models%2520and%2520algorithms%2520for%2520preference%2520optimization%250Ato%2520improve%2520model%2520alignment%2520to%2520human%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Value%20Drifts%3A%20Tracing%20Value%20Alignment%20During%20LLM%20Post-Training&entry.906535625=Mehar%20Bhatia%20and%20Shravan%20Nayak%20and%20Gaurav%20Kamath%20and%20Marius%20Mosbach%20and%20Karolina%20Sta%C5%84czak%20and%20Vered%20Shwartz%20and%20Siva%20Reddy&entry.1292438233=%20%20As%20LLMs%20occupy%20an%20increasingly%20important%20role%20in%20society%2C%20they%20are%20more%20and%0Amore%20confronted%20with%20questions%20that%20require%20them%20not%20only%20to%20draw%20on%20their%0Ageneral%20knowledge%20but%20also%20to%20align%20with%20certain%20human%20value%20systems.%0ATherefore%2C%20studying%20the%20alignment%20of%20LLMs%20with%20human%20values%20has%20become%20a%0Acrucial%20field%20of%20inquiry.%20Prior%20work%2C%20however%2C%20mostly%20focuses%20on%20evaluating%20the%0Aalignment%20of%20fully%20trained%20models%2C%20overlooking%20the%20training%20dynamics%20by%20which%0Amodels%20learn%20to%20express%20human%20values.%20In%20this%20work%2C%20we%20investigate%20how%20and%20at%0Awhich%20stage%20value%20alignment%20arises%20during%20the%20course%20of%20a%20model%27s%0Apost-training.%20Our%20analysis%20disentangles%20the%20effects%20of%20post-training%0Aalgorithms%20and%20datasets%2C%20measuring%20both%20the%20magnitude%20and%20time%20of%20value%20drifts%0Aduring%20training.%20Experimenting%20with%20Llama-3%20and%20Qwen-3%20models%20of%20different%0Asizes%20and%20popular%20supervised%20fine-tuning%20%28SFT%29%20and%20preference%20optimization%0Adatasets%20and%20algorithms%2C%20we%20find%20that%20the%20SFT%20phase%20generally%20establishes%20a%0Amodel%27s%20values%2C%20and%20subsequent%20preference%20optimization%20rarely%20re-aligns%20these%0Avalues.%20Furthermore%2C%20using%20a%20synthetic%20preference%20dataset%20that%20enables%0Acontrolled%20manipulation%20of%20values%2C%20we%20find%20that%20different%20preference%0Aoptimization%20algorithms%20lead%20to%20different%20value%20alignment%20outcomes%2C%20even%20when%0Apreference%20data%20is%20held%20constant.%20Our%20findings%20provide%20actionable%20insights%20into%0Ahow%20values%20are%20learned%20during%20post-training%20and%20help%20to%20inform%20data%20curation%2C%0Aas%20well%20as%20the%20selection%20of%20models%20and%20algorithms%20for%20preference%20optimization%0Ato%20improve%20model%20alignment%20to%20human%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26707v1&entry.124074799=Read"},
{"title": "ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection", "author": "Paul F. R. Wilson and Mohamed Harmanani and Minh Nguyen Nhat To and Amoon Jamzad and Tarek Elghareb and Zhuoxin Guo and Adam Kinnaird and Brian Wodlinger and Purang Abolmaesumi and Parvin Mousavi", "abstract": "  Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols.\n", "link": "http://arxiv.org/abs/2510.26703v1", "date": "2025-10-30", "relevancy": 2.3079, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProstNFound%2B%3A%20A%20Prospective%20Study%20using%20Medical%20Foundation%20Models%20for%0A%20%20Prostate%20Cancer%20Detection&body=Title%3A%20ProstNFound%2B%3A%20A%20Prospective%20Study%20using%20Medical%20Foundation%20Models%20for%0A%20%20Prostate%20Cancer%20Detection%0AAuthor%3A%20Paul%20F.%20R.%20Wilson%20and%20Mohamed%20Harmanani%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Amoon%20Jamzad%20and%20Tarek%20Elghareb%20and%20Zhuoxin%20Guo%20and%20Adam%20Kinnaird%20and%20Brian%20Wodlinger%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi%0AAbstract%3A%20%20%20Purpose%3A%20Medical%20foundation%20models%20%28FMs%29%20offer%20a%20path%20to%20build%0Ahigh-performance%20diagnostic%20systems.%20However%2C%20their%20application%20to%20prostate%0Acancer%20%28PCa%29%20detection%20from%20micro-ultrasound%20%28%7B%5Cmu%7DUS%29%20remains%20untested%20in%0Aclinical%20settings.%20We%20present%20ProstNFound%2B%2C%20an%20adaptation%20of%20FMs%20for%20PCa%0Adetection%20from%20%7B%5Cmu%7DUS%2C%20along%20with%20its%20first%20prospective%20validation.%20Methods%3A%0AProstNFound%2B%20incorporates%20a%20medical%20FM%2C%20adapter%20tuning%2C%20and%20a%20custom%20prompt%0Aencoder%20that%20embeds%20PCa-specific%20clinical%20biomarkers.%20The%20model%20generates%20a%0Acancer%20heatmap%20and%20a%20risk%20score%20for%20clinically%20significant%20PCa.%20Following%0Atraining%20on%20multi-center%20retrospective%20data%2C%20the%20model%20is%20prospectively%0Aevaluated%20on%20data%20acquired%20five%20years%20later%20from%20a%20new%20clinical%20site.%20Model%0Apredictions%20are%20benchmarked%20against%20standard%20clinical%20scoring%20protocols%0A%28PRI-MUS%20and%20PI-RADS%29.%20Results%3A%20ProstNFound%2B%20shows%20strong%20generalization%20to%20the%0Aprospective%20data%2C%20with%20no%20performance%20degradation%20compared%20to%20retrospective%0Aevaluation.%20It%20aligns%20closely%20with%20clinical%20scores%20and%20produces%20interpretable%0Aheatmaps%20consistent%20with%20biopsy-confirmed%20lesions.%20Conclusion%3A%20The%20results%0Ahighlight%20its%20potential%20for%20clinical%20deployment%2C%20offering%20a%20scalable%20and%0Ainterpretable%20alternative%20to%20expert-driven%20protocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProstNFound%252B%253A%2520A%2520Prospective%2520Study%2520using%2520Medical%2520Foundation%2520Models%2520for%250A%2520%2520Prostate%2520Cancer%2520Detection%26entry.906535625%3DPaul%2520F.%2520R.%2520Wilson%2520and%2520Mohamed%2520Harmanani%2520and%2520Minh%2520Nguyen%2520Nhat%2520To%2520and%2520Amoon%2520Jamzad%2520and%2520Tarek%2520Elghareb%2520and%2520Zhuoxin%2520Guo%2520and%2520Adam%2520Kinnaird%2520and%2520Brian%2520Wodlinger%2520and%2520Purang%2520Abolmaesumi%2520and%2520Parvin%2520Mousavi%26entry.1292438233%3D%2520%2520Purpose%253A%2520Medical%2520foundation%2520models%2520%2528FMs%2529%2520offer%2520a%2520path%2520to%2520build%250Ahigh-performance%2520diagnostic%2520systems.%2520However%252C%2520their%2520application%2520to%2520prostate%250Acancer%2520%2528PCa%2529%2520detection%2520from%2520micro-ultrasound%2520%2528%257B%255Cmu%257DUS%2529%2520remains%2520untested%2520in%250Aclinical%2520settings.%2520We%2520present%2520ProstNFound%252B%252C%2520an%2520adaptation%2520of%2520FMs%2520for%2520PCa%250Adetection%2520from%2520%257B%255Cmu%257DUS%252C%2520along%2520with%2520its%2520first%2520prospective%2520validation.%2520Methods%253A%250AProstNFound%252B%2520incorporates%2520a%2520medical%2520FM%252C%2520adapter%2520tuning%252C%2520and%2520a%2520custom%2520prompt%250Aencoder%2520that%2520embeds%2520PCa-specific%2520clinical%2520biomarkers.%2520The%2520model%2520generates%2520a%250Acancer%2520heatmap%2520and%2520a%2520risk%2520score%2520for%2520clinically%2520significant%2520PCa.%2520Following%250Atraining%2520on%2520multi-center%2520retrospective%2520data%252C%2520the%2520model%2520is%2520prospectively%250Aevaluated%2520on%2520data%2520acquired%2520five%2520years%2520later%2520from%2520a%2520new%2520clinical%2520site.%2520Model%250Apredictions%2520are%2520benchmarked%2520against%2520standard%2520clinical%2520scoring%2520protocols%250A%2528PRI-MUS%2520and%2520PI-RADS%2529.%2520Results%253A%2520ProstNFound%252B%2520shows%2520strong%2520generalization%2520to%2520the%250Aprospective%2520data%252C%2520with%2520no%2520performance%2520degradation%2520compared%2520to%2520retrospective%250Aevaluation.%2520It%2520aligns%2520closely%2520with%2520clinical%2520scores%2520and%2520produces%2520interpretable%250Aheatmaps%2520consistent%2520with%2520biopsy-confirmed%2520lesions.%2520Conclusion%253A%2520The%2520results%250Ahighlight%2520its%2520potential%2520for%2520clinical%2520deployment%252C%2520offering%2520a%2520scalable%2520and%250Ainterpretable%2520alternative%2520to%2520expert-driven%2520protocols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProstNFound%2B%3A%20A%20Prospective%20Study%20using%20Medical%20Foundation%20Models%20for%0A%20%20Prostate%20Cancer%20Detection&entry.906535625=Paul%20F.%20R.%20Wilson%20and%20Mohamed%20Harmanani%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Amoon%20Jamzad%20and%20Tarek%20Elghareb%20and%20Zhuoxin%20Guo%20and%20Adam%20Kinnaird%20and%20Brian%20Wodlinger%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi&entry.1292438233=%20%20Purpose%3A%20Medical%20foundation%20models%20%28FMs%29%20offer%20a%20path%20to%20build%0Ahigh-performance%20diagnostic%20systems.%20However%2C%20their%20application%20to%20prostate%0Acancer%20%28PCa%29%20detection%20from%20micro-ultrasound%20%28%7B%5Cmu%7DUS%29%20remains%20untested%20in%0Aclinical%20settings.%20We%20present%20ProstNFound%2B%2C%20an%20adaptation%20of%20FMs%20for%20PCa%0Adetection%20from%20%7B%5Cmu%7DUS%2C%20along%20with%20its%20first%20prospective%20validation.%20Methods%3A%0AProstNFound%2B%20incorporates%20a%20medical%20FM%2C%20adapter%20tuning%2C%20and%20a%20custom%20prompt%0Aencoder%20that%20embeds%20PCa-specific%20clinical%20biomarkers.%20The%20model%20generates%20a%0Acancer%20heatmap%20and%20a%20risk%20score%20for%20clinically%20significant%20PCa.%20Following%0Atraining%20on%20multi-center%20retrospective%20data%2C%20the%20model%20is%20prospectively%0Aevaluated%20on%20data%20acquired%20five%20years%20later%20from%20a%20new%20clinical%20site.%20Model%0Apredictions%20are%20benchmarked%20against%20standard%20clinical%20scoring%20protocols%0A%28PRI-MUS%20and%20PI-RADS%29.%20Results%3A%20ProstNFound%2B%20shows%20strong%20generalization%20to%20the%0Aprospective%20data%2C%20with%20no%20performance%20degradation%20compared%20to%20retrospective%0Aevaluation.%20It%20aligns%20closely%20with%20clinical%20scores%20and%20produces%20interpretable%0Aheatmaps%20consistent%20with%20biopsy-confirmed%20lesions.%20Conclusion%3A%20The%20results%0Ahighlight%20its%20potential%20for%20clinical%20deployment%2C%20offering%20a%20scalable%20and%0Ainterpretable%20alternative%20to%20expert-driven%20protocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26703v1&entry.124074799=Read"},
{"title": "Improving Classification of Occluded Objects through Scene Context", "author": "Courtney M. King and Daniel D. Leeds and Damian Lyons and George Kalaitzis", "abstract": "  The presence of occlusions has provided substantial challenges to\ntypically-powerful object recognition algorithms. Additional sources of\ninformation can be extremely valuable to reduce errors caused by occlusions.\nScene context is known to aid in object recognition in biological vision. In\nthis work, we attempt to add robustness into existing Region Proposal\nNetwork-Deep Convolutional Neural Network (RPN-DCNN) object detection networks\nthrough two distinct scene-based information fusion techniques. We present one\nalgorithm under each methodology: the first operates prior to prediction,\nselecting a custom object network to use based on the identified background\nscene, and the second operates after detection, fusing scene knowledge into\ninitial object scores output by the RPN. We demonstrate our algorithms on\nchallenging datasets featuring partial occlusions, which show overall\nimprovement in both recall and precision against baseline methods. In addition,\nour experiments contrast multiple training methodologies for occlusion\nhandling, finding that training on a combination of both occluded and\nunoccluded images demonstrates an improvement over the others. Our method is\ninterpretable and can easily be adapted to other datasets, offering many future\ndirections for research and practical applications.\n", "link": "http://arxiv.org/abs/2510.26681v1", "date": "2025-10-30", "relevancy": 2.293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Classification%20of%20Occluded%20Objects%20through%20Scene%20Context&body=Title%3A%20Improving%20Classification%20of%20Occluded%20Objects%20through%20Scene%20Context%0AAuthor%3A%20Courtney%20M.%20King%20and%20Daniel%20D.%20Leeds%20and%20Damian%20Lyons%20and%20George%20Kalaitzis%0AAbstract%3A%20%20%20The%20presence%20of%20occlusions%20has%20provided%20substantial%20challenges%20to%0Atypically-powerful%20object%20recognition%20algorithms.%20Additional%20sources%20of%0Ainformation%20can%20be%20extremely%20valuable%20to%20reduce%20errors%20caused%20by%20occlusions.%0AScene%20context%20is%20known%20to%20aid%20in%20object%20recognition%20in%20biological%20vision.%20In%0Athis%20work%2C%20we%20attempt%20to%20add%20robustness%20into%20existing%20Region%20Proposal%0ANetwork-Deep%20Convolutional%20Neural%20Network%20%28RPN-DCNN%29%20object%20detection%20networks%0Athrough%20two%20distinct%20scene-based%20information%20fusion%20techniques.%20We%20present%20one%0Aalgorithm%20under%20each%20methodology%3A%20the%20first%20operates%20prior%20to%20prediction%2C%0Aselecting%20a%20custom%20object%20network%20to%20use%20based%20on%20the%20identified%20background%0Ascene%2C%20and%20the%20second%20operates%20after%20detection%2C%20fusing%20scene%20knowledge%20into%0Ainitial%20object%20scores%20output%20by%20the%20RPN.%20We%20demonstrate%20our%20algorithms%20on%0Achallenging%20datasets%20featuring%20partial%20occlusions%2C%20which%20show%20overall%0Aimprovement%20in%20both%20recall%20and%20precision%20against%20baseline%20methods.%20In%20addition%2C%0Aour%20experiments%20contrast%20multiple%20training%20methodologies%20for%20occlusion%0Ahandling%2C%20finding%20that%20training%20on%20a%20combination%20of%20both%20occluded%20and%0Aunoccluded%20images%20demonstrates%20an%20improvement%20over%20the%20others.%20Our%20method%20is%0Ainterpretable%20and%20can%20easily%20be%20adapted%20to%20other%20datasets%2C%20offering%20many%20future%0Adirections%20for%20research%20and%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Classification%2520of%2520Occluded%2520Objects%2520through%2520Scene%2520Context%26entry.906535625%3DCourtney%2520M.%2520King%2520and%2520Daniel%2520D.%2520Leeds%2520and%2520Damian%2520Lyons%2520and%2520George%2520Kalaitzis%26entry.1292438233%3D%2520%2520The%2520presence%2520of%2520occlusions%2520has%2520provided%2520substantial%2520challenges%2520to%250Atypically-powerful%2520object%2520recognition%2520algorithms.%2520Additional%2520sources%2520of%250Ainformation%2520can%2520be%2520extremely%2520valuable%2520to%2520reduce%2520errors%2520caused%2520by%2520occlusions.%250AScene%2520context%2520is%2520known%2520to%2520aid%2520in%2520object%2520recognition%2520in%2520biological%2520vision.%2520In%250Athis%2520work%252C%2520we%2520attempt%2520to%2520add%2520robustness%2520into%2520existing%2520Region%2520Proposal%250ANetwork-Deep%2520Convolutional%2520Neural%2520Network%2520%2528RPN-DCNN%2529%2520object%2520detection%2520networks%250Athrough%2520two%2520distinct%2520scene-based%2520information%2520fusion%2520techniques.%2520We%2520present%2520one%250Aalgorithm%2520under%2520each%2520methodology%253A%2520the%2520first%2520operates%2520prior%2520to%2520prediction%252C%250Aselecting%2520a%2520custom%2520object%2520network%2520to%2520use%2520based%2520on%2520the%2520identified%2520background%250Ascene%252C%2520and%2520the%2520second%2520operates%2520after%2520detection%252C%2520fusing%2520scene%2520knowledge%2520into%250Ainitial%2520object%2520scores%2520output%2520by%2520the%2520RPN.%2520We%2520demonstrate%2520our%2520algorithms%2520on%250Achallenging%2520datasets%2520featuring%2520partial%2520occlusions%252C%2520which%2520show%2520overall%250Aimprovement%2520in%2520both%2520recall%2520and%2520precision%2520against%2520baseline%2520methods.%2520In%2520addition%252C%250Aour%2520experiments%2520contrast%2520multiple%2520training%2520methodologies%2520for%2520occlusion%250Ahandling%252C%2520finding%2520that%2520training%2520on%2520a%2520combination%2520of%2520both%2520occluded%2520and%250Aunoccluded%2520images%2520demonstrates%2520an%2520improvement%2520over%2520the%2520others.%2520Our%2520method%2520is%250Ainterpretable%2520and%2520can%2520easily%2520be%2520adapted%2520to%2520other%2520datasets%252C%2520offering%2520many%2520future%250Adirections%2520for%2520research%2520and%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Classification%20of%20Occluded%20Objects%20through%20Scene%20Context&entry.906535625=Courtney%20M.%20King%20and%20Daniel%20D.%20Leeds%20and%20Damian%20Lyons%20and%20George%20Kalaitzis&entry.1292438233=%20%20The%20presence%20of%20occlusions%20has%20provided%20substantial%20challenges%20to%0Atypically-powerful%20object%20recognition%20algorithms.%20Additional%20sources%20of%0Ainformation%20can%20be%20extremely%20valuable%20to%20reduce%20errors%20caused%20by%20occlusions.%0AScene%20context%20is%20known%20to%20aid%20in%20object%20recognition%20in%20biological%20vision.%20In%0Athis%20work%2C%20we%20attempt%20to%20add%20robustness%20into%20existing%20Region%20Proposal%0ANetwork-Deep%20Convolutional%20Neural%20Network%20%28RPN-DCNN%29%20object%20detection%20networks%0Athrough%20two%20distinct%20scene-based%20information%20fusion%20techniques.%20We%20present%20one%0Aalgorithm%20under%20each%20methodology%3A%20the%20first%20operates%20prior%20to%20prediction%2C%0Aselecting%20a%20custom%20object%20network%20to%20use%20based%20on%20the%20identified%20background%0Ascene%2C%20and%20the%20second%20operates%20after%20detection%2C%20fusing%20scene%20knowledge%20into%0Ainitial%20object%20scores%20output%20by%20the%20RPN.%20We%20demonstrate%20our%20algorithms%20on%0Achallenging%20datasets%20featuring%20partial%20occlusions%2C%20which%20show%20overall%0Aimprovement%20in%20both%20recall%20and%20precision%20against%20baseline%20methods.%20In%20addition%2C%0Aour%20experiments%20contrast%20multiple%20training%20methodologies%20for%20occlusion%0Ahandling%2C%20finding%20that%20training%20on%20a%20combination%20of%20both%20occluded%20and%0Aunoccluded%20images%20demonstrates%20an%20improvement%20over%20the%20others.%20Our%20method%20is%0Ainterpretable%20and%20can%20easily%20be%20adapted%20to%20other%20datasets%2C%20offering%20many%20future%0Adirections%20for%20research%20and%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26681v1&entry.124074799=Read"},
{"title": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling", "author": "Hao Li and Shuai Yang and Yilun Chen and Xinyi Chen and Xiaoda Yang and Yang Tian and Hanqing Wang and Tai Wang and Dahua Lin and Feng Zhao and Jiangmiao Pang", "abstract": "  Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.\n", "link": "http://arxiv.org/abs/2506.19816v2", "date": "2025-10-30", "relevancy": 2.2905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CronusVLA%3A%20Towards%20Efficient%20and%20Robust%20Manipulation%20via%20Multi-Frame%0A%20%20Vision-Language-Action%20Modeling&body=Title%3A%20CronusVLA%3A%20Towards%20Efficient%20and%20Robust%20Manipulation%20via%20Multi-Frame%0A%20%20Vision-Language-Action%20Modeling%0AAuthor%3A%20Hao%20Li%20and%20Shuai%20Yang%20and%20Yilun%20Chen%20and%20Xinyi%20Chen%20and%20Xiaoda%20Yang%20and%20Yang%20Tian%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Dahua%20Lin%20and%20Feng%20Zhao%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Recent%20vision-language-action%20%28VLA%29%20models%20built%20on%20pretrained%0Avision-language%20models%20%28VLMs%29%20have%20demonstrated%20strong%20performance%20in%20robotic%0Amanipulation.%20However%2C%20these%20models%20remain%20constrained%20by%20the%20single-frame%0Aimage%20paradigm%20and%20fail%20to%20fully%20leverage%20the%20temporal%20information%20offered%20by%0Amulti-frame%20histories%2C%20as%20directly%20feeding%20multiple%20frames%20into%20VLM%20backbones%0Aincurs%20substantial%20computational%20overhead%20and%20inference%20latency.%20We%20propose%0ACronusVLA%2C%20a%20unified%20framework%20that%20extends%20single-frame%20VLA%20models%20to%20the%0Amulti-frame%20paradigm.%20CronusVLA%20follows%20a%20two-stage%20process%3A%20%281%29%20Single-frame%0Apretraining%20on%20large-scale%20embodied%20datasets%20with%20autoregressive%20prediction%20of%0Aaction%20tokens%2C%20establishing%20an%20effective%20embodied%20vision-language%20foundation%3B%0A%282%29%20Multi-frame%20post-training%2C%20which%20adapts%20the%20prediction%20of%20the%0Avision-language%20backbone%20from%20discrete%20tokens%20to%20learnable%20features%2C%20and%0Aaggregates%20historical%20information%20via%20feature%20chunking.%20CronusVLA%20effectively%0Aaddresses%20the%20existing%20challenges%20of%20multi-frame%20modeling%20while%20enhancing%0Aperformance%20and%20observational%20robustness.%20To%20evaluate%20the%20robustness%20under%0Atemporal%20and%20spatial%20disturbances%2C%20we%20introduce%20SimplerEnv-OR%2C%20a%20novel%0Abenchmark%20featuring%2024%20types%20of%20observational%20disturbances%20and%20120%20severity%0Alevels.%20Experiments%20across%20three%20embodiments%20in%20simulated%20and%20real-world%0Aenvironments%20demonstrate%20that%20CronusVLA%20achieves%20leading%20performance%20and%0Asuperior%20robustness%2C%20with%20a%2070.9%25%20success%20rate%20on%20SimplerEnv%2C%20a%2026.8%25%0Aimprovement%20over%20OpenVLA%20on%20LIBERO%2C%20and%20the%20highest%20robustness%20score%20on%0ASimplerEnv-OR.%20These%20results%20highlight%20the%20potential%20of%20efficient%20multi-frame%0Aadaptation%20in%20VLA%20models%20for%20more%20powerful%20and%20robust%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCronusVLA%253A%2520Towards%2520Efficient%2520and%2520Robust%2520Manipulation%2520via%2520Multi-Frame%250A%2520%2520Vision-Language-Action%2520Modeling%26entry.906535625%3DHao%2520Li%2520and%2520Shuai%2520Yang%2520and%2520Yilun%2520Chen%2520and%2520Xinyi%2520Chen%2520and%2520Xiaoda%2520Yang%2520and%2520Yang%2520Tian%2520and%2520Hanqing%2520Wang%2520and%2520Tai%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Feng%2520Zhao%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Recent%2520vision-language-action%2520%2528VLA%2529%2520models%2520built%2520on%2520pretrained%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520performance%2520in%2520robotic%250Amanipulation.%2520However%252C%2520these%2520models%2520remain%2520constrained%2520by%2520the%2520single-frame%250Aimage%2520paradigm%2520and%2520fail%2520to%2520fully%2520leverage%2520the%2520temporal%2520information%2520offered%2520by%250Amulti-frame%2520histories%252C%2520as%2520directly%2520feeding%2520multiple%2520frames%2520into%2520VLM%2520backbones%250Aincurs%2520substantial%2520computational%2520overhead%2520and%2520inference%2520latency.%2520We%2520propose%250ACronusVLA%252C%2520a%2520unified%2520framework%2520that%2520extends%2520single-frame%2520VLA%2520models%2520to%2520the%250Amulti-frame%2520paradigm.%2520CronusVLA%2520follows%2520a%2520two-stage%2520process%253A%2520%25281%2529%2520Single-frame%250Apretraining%2520on%2520large-scale%2520embodied%2520datasets%2520with%2520autoregressive%2520prediction%2520of%250Aaction%2520tokens%252C%2520establishing%2520an%2520effective%2520embodied%2520vision-language%2520foundation%253B%250A%25282%2529%2520Multi-frame%2520post-training%252C%2520which%2520adapts%2520the%2520prediction%2520of%2520the%250Avision-language%2520backbone%2520from%2520discrete%2520tokens%2520to%2520learnable%2520features%252C%2520and%250Aaggregates%2520historical%2520information%2520via%2520feature%2520chunking.%2520CronusVLA%2520effectively%250Aaddresses%2520the%2520existing%2520challenges%2520of%2520multi-frame%2520modeling%2520while%2520enhancing%250Aperformance%2520and%2520observational%2520robustness.%2520To%2520evaluate%2520the%2520robustness%2520under%250Atemporal%2520and%2520spatial%2520disturbances%252C%2520we%2520introduce%2520SimplerEnv-OR%252C%2520a%2520novel%250Abenchmark%2520featuring%252024%2520types%2520of%2520observational%2520disturbances%2520and%2520120%2520severity%250Alevels.%2520Experiments%2520across%2520three%2520embodiments%2520in%2520simulated%2520and%2520real-world%250Aenvironments%2520demonstrate%2520that%2520CronusVLA%2520achieves%2520leading%2520performance%2520and%250Asuperior%2520robustness%252C%2520with%2520a%252070.9%2525%2520success%2520rate%2520on%2520SimplerEnv%252C%2520a%252026.8%2525%250Aimprovement%2520over%2520OpenVLA%2520on%2520LIBERO%252C%2520and%2520the%2520highest%2520robustness%2520score%2520on%250ASimplerEnv-OR.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520efficient%2520multi-frame%250Aadaptation%2520in%2520VLA%2520models%2520for%2520more%2520powerful%2520and%2520robust%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CronusVLA%3A%20Towards%20Efficient%20and%20Robust%20Manipulation%20via%20Multi-Frame%0A%20%20Vision-Language-Action%20Modeling&entry.906535625=Hao%20Li%20and%20Shuai%20Yang%20and%20Yilun%20Chen%20and%20Xinyi%20Chen%20and%20Xiaoda%20Yang%20and%20Yang%20Tian%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Dahua%20Lin%20and%20Feng%20Zhao%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Recent%20vision-language-action%20%28VLA%29%20models%20built%20on%20pretrained%0Avision-language%20models%20%28VLMs%29%20have%20demonstrated%20strong%20performance%20in%20robotic%0Amanipulation.%20However%2C%20these%20models%20remain%20constrained%20by%20the%20single-frame%0Aimage%20paradigm%20and%20fail%20to%20fully%20leverage%20the%20temporal%20information%20offered%20by%0Amulti-frame%20histories%2C%20as%20directly%20feeding%20multiple%20frames%20into%20VLM%20backbones%0Aincurs%20substantial%20computational%20overhead%20and%20inference%20latency.%20We%20propose%0ACronusVLA%2C%20a%20unified%20framework%20that%20extends%20single-frame%20VLA%20models%20to%20the%0Amulti-frame%20paradigm.%20CronusVLA%20follows%20a%20two-stage%20process%3A%20%281%29%20Single-frame%0Apretraining%20on%20large-scale%20embodied%20datasets%20with%20autoregressive%20prediction%20of%0Aaction%20tokens%2C%20establishing%20an%20effective%20embodied%20vision-language%20foundation%3B%0A%282%29%20Multi-frame%20post-training%2C%20which%20adapts%20the%20prediction%20of%20the%0Avision-language%20backbone%20from%20discrete%20tokens%20to%20learnable%20features%2C%20and%0Aaggregates%20historical%20information%20via%20feature%20chunking.%20CronusVLA%20effectively%0Aaddresses%20the%20existing%20challenges%20of%20multi-frame%20modeling%20while%20enhancing%0Aperformance%20and%20observational%20robustness.%20To%20evaluate%20the%20robustness%20under%0Atemporal%20and%20spatial%20disturbances%2C%20we%20introduce%20SimplerEnv-OR%2C%20a%20novel%0Abenchmark%20featuring%2024%20types%20of%20observational%20disturbances%20and%20120%20severity%0Alevels.%20Experiments%20across%20three%20embodiments%20in%20simulated%20and%20real-world%0Aenvironments%20demonstrate%20that%20CronusVLA%20achieves%20leading%20performance%20and%0Asuperior%20robustness%2C%20with%20a%2070.9%25%20success%20rate%20on%20SimplerEnv%2C%20a%2026.8%25%0Aimprovement%20over%20OpenVLA%20on%20LIBERO%2C%20and%20the%20highest%20robustness%20score%20on%0ASimplerEnv-OR.%20These%20results%20highlight%20the%20potential%20of%20efficient%20multi-frame%0Aadaptation%20in%20VLA%20models%20for%20more%20powerful%20and%20robust%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19816v2&entry.124074799=Read"},
{"title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models\n  through Attention Key-Space Analysis", "author": "Xinhan Zheng and Huyu Wu and Xueting Wang and Haiyun Jiang", "abstract": "  Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.\n", "link": "http://arxiv.org/abs/2510.26721v1", "date": "2025-10-30", "relevancy": 2.2194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Intrinsic%20Text%20Bias%20in%20Multimodal%20Large%20Language%20Models%0A%20%20through%20Attention%20Key-Space%20Analysis&body=Title%3A%20Unveiling%20Intrinsic%20Text%20Bias%20in%20Multimodal%20Large%20Language%20Models%0A%20%20through%20Attention%20Key-Space%20Analysis%0AAuthor%3A%20Xinhan%20Zheng%20and%20Huyu%20Wu%20and%20Xueting%20Wang%20and%20Haiyun%20Jiang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20exhibit%20a%20pronounced%20preference%20for%0Atextual%20inputs%20when%20processing%20vision-language%20data%2C%20limiting%20their%20ability%20to%0Areason%20effectively%20from%20visual%20evidence.%20Unlike%20prior%20studies%20that%20attribute%0Athis%20text%20bias%20to%20external%20factors%20such%20as%20data%20imbalance%20or%20instruction%0Atuning%2C%20we%20propose%20that%20the%20bias%20originates%20from%20the%20model%27s%20internal%0Aarchitecture.%20Specifically%2C%20we%20hypothesize%20that%20visual%20key%20vectors%20%28Visual%0AKeys%29%20are%20out-of-distribution%20%28OOD%29%20relative%20to%20the%20text%20key%20space%20learned%0Aduring%20language-only%20pretraining.%20Consequently%2C%20these%20visual%20keys%20receive%0Asystematically%20lower%20similarity%20scores%20during%20attention%20computation%2C%20leading%20to%0Atheir%20under-utilization%20in%20the%20context%20representation.%20To%20validate%20this%0Ahypothesis%2C%20we%20extract%20key%20vectors%20from%20LLaVA%20and%20Qwen2.5-VL%20and%20analyze%20their%0Adistributional%20structures%20using%20qualitative%20%28t-SNE%29%20and%20quantitative%0A%28Jensen-Shannon%20divergence%29%20methods.%20The%20results%20provide%20direct%20evidence%20that%0Avisual%20and%20textual%20keys%20occupy%20markedly%20distinct%20subspaces%20within%20the%20attention%0Aspace.%20The%20inter-modal%20divergence%20is%20statistically%20significant%2C%20exceeding%0Aintra-modal%20variation%20by%20several%20orders%20of%20magnitude.%20These%20findings%20reveal%0Athat%20text%20bias%20arises%20from%20an%20intrinsic%20misalignment%20within%20the%20attention%20key%0Aspace%20rather%20than%20solely%20from%20external%20data%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Intrinsic%2520Text%2520Bias%2520in%2520Multimodal%2520Large%2520Language%2520Models%250A%2520%2520through%2520Attention%2520Key-Space%2520Analysis%26entry.906535625%3DXinhan%2520Zheng%2520and%2520Huyu%2520Wu%2520and%2520Xueting%2520Wang%2520and%2520Haiyun%2520Jiang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520exhibit%2520a%2520pronounced%2520preference%2520for%250Atextual%2520inputs%2520when%2520processing%2520vision-language%2520data%252C%2520limiting%2520their%2520ability%2520to%250Areason%2520effectively%2520from%2520visual%2520evidence.%2520Unlike%2520prior%2520studies%2520that%2520attribute%250Athis%2520text%2520bias%2520to%2520external%2520factors%2520such%2520as%2520data%2520imbalance%2520or%2520instruction%250Atuning%252C%2520we%2520propose%2520that%2520the%2520bias%2520originates%2520from%2520the%2520model%2527s%2520internal%250Aarchitecture.%2520Specifically%252C%2520we%2520hypothesize%2520that%2520visual%2520key%2520vectors%2520%2528Visual%250AKeys%2529%2520are%2520out-of-distribution%2520%2528OOD%2529%2520relative%2520to%2520the%2520text%2520key%2520space%2520learned%250Aduring%2520language-only%2520pretraining.%2520Consequently%252C%2520these%2520visual%2520keys%2520receive%250Asystematically%2520lower%2520similarity%2520scores%2520during%2520attention%2520computation%252C%2520leading%2520to%250Atheir%2520under-utilization%2520in%2520the%2520context%2520representation.%2520To%2520validate%2520this%250Ahypothesis%252C%2520we%2520extract%2520key%2520vectors%2520from%2520LLaVA%2520and%2520Qwen2.5-VL%2520and%2520analyze%2520their%250Adistributional%2520structures%2520using%2520qualitative%2520%2528t-SNE%2529%2520and%2520quantitative%250A%2528Jensen-Shannon%2520divergence%2529%2520methods.%2520The%2520results%2520provide%2520direct%2520evidence%2520that%250Avisual%2520and%2520textual%2520keys%2520occupy%2520markedly%2520distinct%2520subspaces%2520within%2520the%2520attention%250Aspace.%2520The%2520inter-modal%2520divergence%2520is%2520statistically%2520significant%252C%2520exceeding%250Aintra-modal%2520variation%2520by%2520several%2520orders%2520of%2520magnitude.%2520These%2520findings%2520reveal%250Athat%2520text%2520bias%2520arises%2520from%2520an%2520intrinsic%2520misalignment%2520within%2520the%2520attention%2520key%250Aspace%2520rather%2520than%2520solely%2520from%2520external%2520data%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Intrinsic%20Text%20Bias%20in%20Multimodal%20Large%20Language%20Models%0A%20%20through%20Attention%20Key-Space%20Analysis&entry.906535625=Xinhan%20Zheng%20and%20Huyu%20Wu%20and%20Xueting%20Wang%20and%20Haiyun%20Jiang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20exhibit%20a%20pronounced%20preference%20for%0Atextual%20inputs%20when%20processing%20vision-language%20data%2C%20limiting%20their%20ability%20to%0Areason%20effectively%20from%20visual%20evidence.%20Unlike%20prior%20studies%20that%20attribute%0Athis%20text%20bias%20to%20external%20factors%20such%20as%20data%20imbalance%20or%20instruction%0Atuning%2C%20we%20propose%20that%20the%20bias%20originates%20from%20the%20model%27s%20internal%0Aarchitecture.%20Specifically%2C%20we%20hypothesize%20that%20visual%20key%20vectors%20%28Visual%0AKeys%29%20are%20out-of-distribution%20%28OOD%29%20relative%20to%20the%20text%20key%20space%20learned%0Aduring%20language-only%20pretraining.%20Consequently%2C%20these%20visual%20keys%20receive%0Asystematically%20lower%20similarity%20scores%20during%20attention%20computation%2C%20leading%20to%0Atheir%20under-utilization%20in%20the%20context%20representation.%20To%20validate%20this%0Ahypothesis%2C%20we%20extract%20key%20vectors%20from%20LLaVA%20and%20Qwen2.5-VL%20and%20analyze%20their%0Adistributional%20structures%20using%20qualitative%20%28t-SNE%29%20and%20quantitative%0A%28Jensen-Shannon%20divergence%29%20methods.%20The%20results%20provide%20direct%20evidence%20that%0Avisual%20and%20textual%20keys%20occupy%20markedly%20distinct%20subspaces%20within%20the%20attention%0Aspace.%20The%20inter-modal%20divergence%20is%20statistically%20significant%2C%20exceeding%0Aintra-modal%20variation%20by%20several%20orders%20of%20magnitude.%20These%20findings%20reveal%0Athat%20text%20bias%20arises%20from%20an%20intrinsic%20misalignment%20within%20the%20attention%20key%0Aspace%20rather%20than%20solely%20from%20external%20data%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26721v1&entry.124074799=Read"},
{"title": "CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting", "author": "David Maria Schmidt and Raoul Schubert and Philipp Cimiano", "abstract": "  Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.\n", "link": "http://arxiv.org/abs/2507.21257v2", "date": "2025-10-30", "relevancy": 2.2054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompoST%3A%20A%20Benchmark%20for%20Analyzing%20the%20Ability%20of%20LLMs%20To%0A%20%20Compositionally%20Interpret%20Questions%20in%20a%20QALD%20Setting&body=Title%3A%20CompoST%3A%20A%20Benchmark%20for%20Analyzing%20the%20Ability%20of%20LLMs%20To%0A%20%20Compositionally%20Interpret%20Questions%20in%20a%20QALD%20Setting%0AAuthor%3A%20David%20Maria%20Schmidt%20and%20Raoul%20Schubert%20and%20Philipp%20Cimiano%0AAbstract%3A%20%20%20Language%20interpretation%20is%20a%20compositional%20process%2C%20in%20which%20the%20meaning%20of%0Amore%20complex%20linguistic%20structures%20is%20inferred%20from%20the%20meaning%20of%20their%20parts.%0ALarge%20language%20models%20possess%20remarkable%20language%20interpretation%20capabilities%0Aand%20have%20been%20successfully%20applied%20to%20interpret%20questions%20by%20mapping%20them%20to%0ASPARQL%20queries.%20An%20open%20question%20is%20how%20systematic%20this%20interpretation%20process%0Ais.%20Toward%20this%20question%2C%20in%20this%20paper%2C%20we%20propose%20a%20benchmark%20for%0Ainvestigating%20to%20what%20extent%20the%20abilities%20of%20LLMs%20to%20interpret%20questions%20are%0Aactually%20compositional.%20For%20this%2C%20we%20generate%20three%20datasets%20of%20varying%0Adifficulty%20based%20on%20graph%20patterns%20in%20DBpedia%2C%20relying%20on%20Lemon%20lexica%20for%0Averbalization.%20Our%20datasets%20are%20created%20in%20a%20very%20controlled%20fashion%20in%20order%0Ato%20test%20the%20ability%20of%20LLMs%20to%20interpret%20structurally%20complex%20questions%2C%20given%0Athat%20they%20have%20seen%20the%20atomic%20building%20blocks.%20This%20allows%20us%20to%20evaluate%20to%0Awhat%20degree%20LLMs%20are%20able%20to%20interpret%20complex%20questions%20for%20which%20they%0A%22understand%22%20the%20atomic%20parts.%20We%20conduct%20experiments%20with%20models%20of%20different%0Asizes%20using%20both%20various%20prompt%20and%20few-shot%20optimization%20techniques%20as%20well%20as%0Afine-tuning.%20Our%20results%20show%20that%20performance%20in%20terms%20of%20macro%20%24F_1%24%20degrades%0Afrom%20%240.45%24%20over%20%240.26%24%20down%20to%20%240.09%24%20with%20increasing%20deviation%20from%20the%0Asamples%20optimized%20on.%20Even%20when%20all%20necessary%20information%20was%20provided%20to%20the%0Amodel%20in%20the%20input%2C%20the%20%24F_1%24%20scores%20do%20not%20exceed%20%240.57%24%20for%20the%20dataset%20of%0Alowest%20complexity.%20We%20thus%20conclude%20that%20LLMs%20struggle%20to%20systematically%20and%0Acompositionally%20interpret%20questions%20and%20map%20them%20into%20SPARQL%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompoST%253A%2520A%2520Benchmark%2520for%2520Analyzing%2520the%2520Ability%2520of%2520LLMs%2520To%250A%2520%2520Compositionally%2520Interpret%2520Questions%2520in%2520a%2520QALD%2520Setting%26entry.906535625%3DDavid%2520Maria%2520Schmidt%2520and%2520Raoul%2520Schubert%2520and%2520Philipp%2520Cimiano%26entry.1292438233%3D%2520%2520Language%2520interpretation%2520is%2520a%2520compositional%2520process%252C%2520in%2520which%2520the%2520meaning%2520of%250Amore%2520complex%2520linguistic%2520structures%2520is%2520inferred%2520from%2520the%2520meaning%2520of%2520their%2520parts.%250ALarge%2520language%2520models%2520possess%2520remarkable%2520language%2520interpretation%2520capabilities%250Aand%2520have%2520been%2520successfully%2520applied%2520to%2520interpret%2520questions%2520by%2520mapping%2520them%2520to%250ASPARQL%2520queries.%2520An%2520open%2520question%2520is%2520how%2520systematic%2520this%2520interpretation%2520process%250Ais.%2520Toward%2520this%2520question%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520benchmark%2520for%250Ainvestigating%2520to%2520what%2520extent%2520the%2520abilities%2520of%2520LLMs%2520to%2520interpret%2520questions%2520are%250Aactually%2520compositional.%2520For%2520this%252C%2520we%2520generate%2520three%2520datasets%2520of%2520varying%250Adifficulty%2520based%2520on%2520graph%2520patterns%2520in%2520DBpedia%252C%2520relying%2520on%2520Lemon%2520lexica%2520for%250Averbalization.%2520Our%2520datasets%2520are%2520created%2520in%2520a%2520very%2520controlled%2520fashion%2520in%2520order%250Ato%2520test%2520the%2520ability%2520of%2520LLMs%2520to%2520interpret%2520structurally%2520complex%2520questions%252C%2520given%250Athat%2520they%2520have%2520seen%2520the%2520atomic%2520building%2520blocks.%2520This%2520allows%2520us%2520to%2520evaluate%2520to%250Awhat%2520degree%2520LLMs%2520are%2520able%2520to%2520interpret%2520complex%2520questions%2520for%2520which%2520they%250A%2522understand%2522%2520the%2520atomic%2520parts.%2520We%2520conduct%2520experiments%2520with%2520models%2520of%2520different%250Asizes%2520using%2520both%2520various%2520prompt%2520and%2520few-shot%2520optimization%2520techniques%2520as%2520well%2520as%250Afine-tuning.%2520Our%2520results%2520show%2520that%2520performance%2520in%2520terms%2520of%2520macro%2520%2524F_1%2524%2520degrades%250Afrom%2520%25240.45%2524%2520over%2520%25240.26%2524%2520down%2520to%2520%25240.09%2524%2520with%2520increasing%2520deviation%2520from%2520the%250Asamples%2520optimized%2520on.%2520Even%2520when%2520all%2520necessary%2520information%2520was%2520provided%2520to%2520the%250Amodel%2520in%2520the%2520input%252C%2520the%2520%2524F_1%2524%2520scores%2520do%2520not%2520exceed%2520%25240.57%2524%2520for%2520the%2520dataset%2520of%250Alowest%2520complexity.%2520We%2520thus%2520conclude%2520that%2520LLMs%2520struggle%2520to%2520systematically%2520and%250Acompositionally%2520interpret%2520questions%2520and%2520map%2520them%2520into%2520SPARQL%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompoST%3A%20A%20Benchmark%20for%20Analyzing%20the%20Ability%20of%20LLMs%20To%0A%20%20Compositionally%20Interpret%20Questions%20in%20a%20QALD%20Setting&entry.906535625=David%20Maria%20Schmidt%20and%20Raoul%20Schubert%20and%20Philipp%20Cimiano&entry.1292438233=%20%20Language%20interpretation%20is%20a%20compositional%20process%2C%20in%20which%20the%20meaning%20of%0Amore%20complex%20linguistic%20structures%20is%20inferred%20from%20the%20meaning%20of%20their%20parts.%0ALarge%20language%20models%20possess%20remarkable%20language%20interpretation%20capabilities%0Aand%20have%20been%20successfully%20applied%20to%20interpret%20questions%20by%20mapping%20them%20to%0ASPARQL%20queries.%20An%20open%20question%20is%20how%20systematic%20this%20interpretation%20process%0Ais.%20Toward%20this%20question%2C%20in%20this%20paper%2C%20we%20propose%20a%20benchmark%20for%0Ainvestigating%20to%20what%20extent%20the%20abilities%20of%20LLMs%20to%20interpret%20questions%20are%0Aactually%20compositional.%20For%20this%2C%20we%20generate%20three%20datasets%20of%20varying%0Adifficulty%20based%20on%20graph%20patterns%20in%20DBpedia%2C%20relying%20on%20Lemon%20lexica%20for%0Averbalization.%20Our%20datasets%20are%20created%20in%20a%20very%20controlled%20fashion%20in%20order%0Ato%20test%20the%20ability%20of%20LLMs%20to%20interpret%20structurally%20complex%20questions%2C%20given%0Athat%20they%20have%20seen%20the%20atomic%20building%20blocks.%20This%20allows%20us%20to%20evaluate%20to%0Awhat%20degree%20LLMs%20are%20able%20to%20interpret%20complex%20questions%20for%20which%20they%0A%22understand%22%20the%20atomic%20parts.%20We%20conduct%20experiments%20with%20models%20of%20different%0Asizes%20using%20both%20various%20prompt%20and%20few-shot%20optimization%20techniques%20as%20well%20as%0Afine-tuning.%20Our%20results%20show%20that%20performance%20in%20terms%20of%20macro%20%24F_1%24%20degrades%0Afrom%20%240.45%24%20over%20%240.26%24%20down%20to%20%240.09%24%20with%20increasing%20deviation%20from%20the%0Asamples%20optimized%20on.%20Even%20when%20all%20necessary%20information%20was%20provided%20to%20the%0Amodel%20in%20the%20input%2C%20the%20%24F_1%24%20scores%20do%20not%20exceed%20%240.57%24%20for%20the%20dataset%20of%0Alowest%20complexity.%20We%20thus%20conclude%20that%20LLMs%20struggle%20to%20systematically%20and%0Acompositionally%20interpret%20questions%20and%20map%20them%20into%20SPARQL%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21257v2&entry.124074799=Read"},
{"title": "Smoothing Slot Attention Iterations and Recurrences", "author": "Rongzhen Zhao and Wenyan Yang and Juho Kannala and Joni Pajarinen", "abstract": "  Slot Attention (SA) and its variants lie at the heart of mainstream\nObject-Centric Learning (OCL). Objects in an image can be aggregated into\nrespective slot vectors, by \\textit{iteratively} refining cold-start query\nvectors, typically three times, via SA on image features. For video, such\naggregation is \\textit{recurrently} shared across frames, with queries\ncold-started on the first frame while transitioned from the previous frame's\nslots on non-first frames. However, the cold-start queries lack sample-specific\ncues thus hinder precise aggregation on the image or video's first frame; Also,\nnon-first frames' queries are already sample-specific thus require transforms\ndifferent from the first frame's aggregation. We address these issues for the\nfirst time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image\nor video's first frame, we \\textit{preheat} the cold-start queries with rich\ninformation of input features, via a tiny module self-distilled inside OCL; (2)\nTo smooth SA recurrences across all video frames, we \\textit{differentiate} the\nhomogeneous transforms on the first and non-first frames, by using full and\nsingle iterations respectively. Comprehensive experiments on object discovery,\nrecognition and downstream benchmarks validate our method's effectiveness.\nFurther analyses intuitively illuminate how our method smooths SA iterations\nand recurrences. Our source code, model checkpoints and training logs are\navailable on https://github.com/Genera1Z/SmoothSA.\n", "link": "http://arxiv.org/abs/2508.05417v2", "date": "2025-10-30", "relevancy": 2.1953, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5415}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences&body=Title%3A%20Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Wenyan%20Yang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Slot%20Attention%20%28SA%29%20and%20its%20variants%20lie%20at%20the%20heart%20of%20mainstream%0AObject-Centric%20Learning%20%28OCL%29.%20Objects%20in%20an%20image%20can%20be%20aggregated%20into%0Arespective%20slot%20vectors%2C%20by%20%5Ctextit%7Biteratively%7D%20refining%20cold-start%20query%0Avectors%2C%20typically%20three%20times%2C%20via%20SA%20on%20image%20features.%20For%20video%2C%20such%0Aaggregation%20is%20%5Ctextit%7Brecurrently%7D%20shared%20across%20frames%2C%20with%20queries%0Acold-started%20on%20the%20first%20frame%20while%20transitioned%20from%20the%20previous%20frame%27s%0Aslots%20on%20non-first%20frames.%20However%2C%20the%20cold-start%20queries%20lack%20sample-specific%0Acues%20thus%20hinder%20precise%20aggregation%20on%20the%20image%20or%20video%27s%20first%20frame%3B%20Also%2C%0Anon-first%20frames%27%20queries%20are%20already%20sample-specific%20thus%20require%20transforms%0Adifferent%20from%20the%20first%20frame%27s%20aggregation.%20We%20address%20these%20issues%20for%20the%0Afirst%20time%20with%20our%20%5Ctextit%7BSmoothSA%7D%3A%20%281%29%20To%20smooth%20SA%20iterations%20on%20the%20image%0Aor%20video%27s%20first%20frame%2C%20we%20%5Ctextit%7Bpreheat%7D%20the%20cold-start%20queries%20with%20rich%0Ainformation%20of%20input%20features%2C%20via%20a%20tiny%20module%20self-distilled%20inside%20OCL%3B%20%282%29%0ATo%20smooth%20SA%20recurrences%20across%20all%20video%20frames%2C%20we%20%5Ctextit%7Bdifferentiate%7D%20the%0Ahomogeneous%20transforms%20on%20the%20first%20and%20non-first%20frames%2C%20by%20using%20full%20and%0Asingle%20iterations%20respectively.%20Comprehensive%20experiments%20on%20object%20discovery%2C%0Arecognition%20and%20downstream%20benchmarks%20validate%20our%20method%27s%20effectiveness.%0AFurther%20analyses%20intuitively%20illuminate%20how%20our%20method%20smooths%20SA%20iterations%0Aand%20recurrences.%20Our%20source%20code%2C%20model%20checkpoints%20and%20training%20logs%20are%0Aavailable%20on%20https%3A//github.com/Genera1Z/SmoothSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothing%2520Slot%2520Attention%2520Iterations%2520and%2520Recurrences%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Wenyan%2520Yang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Slot%2520Attention%2520%2528SA%2529%2520and%2520its%2520variants%2520lie%2520at%2520the%2520heart%2520of%2520mainstream%250AObject-Centric%2520Learning%2520%2528OCL%2529.%2520Objects%2520in%2520an%2520image%2520can%2520be%2520aggregated%2520into%250Arespective%2520slot%2520vectors%252C%2520by%2520%255Ctextit%257Biteratively%257D%2520refining%2520cold-start%2520query%250Avectors%252C%2520typically%2520three%2520times%252C%2520via%2520SA%2520on%2520image%2520features.%2520For%2520video%252C%2520such%250Aaggregation%2520is%2520%255Ctextit%257Brecurrently%257D%2520shared%2520across%2520frames%252C%2520with%2520queries%250Acold-started%2520on%2520the%2520first%2520frame%2520while%2520transitioned%2520from%2520the%2520previous%2520frame%2527s%250Aslots%2520on%2520non-first%2520frames.%2520However%252C%2520the%2520cold-start%2520queries%2520lack%2520sample-specific%250Acues%2520thus%2520hinder%2520precise%2520aggregation%2520on%2520the%2520image%2520or%2520video%2527s%2520first%2520frame%253B%2520Also%252C%250Anon-first%2520frames%2527%2520queries%2520are%2520already%2520sample-specific%2520thus%2520require%2520transforms%250Adifferent%2520from%2520the%2520first%2520frame%2527s%2520aggregation.%2520We%2520address%2520these%2520issues%2520for%2520the%250Afirst%2520time%2520with%2520our%2520%255Ctextit%257BSmoothSA%257D%253A%2520%25281%2529%2520To%2520smooth%2520SA%2520iterations%2520on%2520the%2520image%250Aor%2520video%2527s%2520first%2520frame%252C%2520we%2520%255Ctextit%257Bpreheat%257D%2520the%2520cold-start%2520queries%2520with%2520rich%250Ainformation%2520of%2520input%2520features%252C%2520via%2520a%2520tiny%2520module%2520self-distilled%2520inside%2520OCL%253B%2520%25282%2529%250ATo%2520smooth%2520SA%2520recurrences%2520across%2520all%2520video%2520frames%252C%2520we%2520%255Ctextit%257Bdifferentiate%257D%2520the%250Ahomogeneous%2520transforms%2520on%2520the%2520first%2520and%2520non-first%2520frames%252C%2520by%2520using%2520full%2520and%250Asingle%2520iterations%2520respectively.%2520Comprehensive%2520experiments%2520on%2520object%2520discovery%252C%250Arecognition%2520and%2520downstream%2520benchmarks%2520validate%2520our%2520method%2527s%2520effectiveness.%250AFurther%2520analyses%2520intuitively%2520illuminate%2520how%2520our%2520method%2520smooths%2520SA%2520iterations%250Aand%2520recurrences.%2520Our%2520source%2520code%252C%2520model%2520checkpoints%2520and%2520training%2520logs%2520are%250Aavailable%2520on%2520https%253A//github.com/Genera1Z/SmoothSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences&entry.906535625=Rongzhen%20Zhao%20and%20Wenyan%20Yang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Slot%20Attention%20%28SA%29%20and%20its%20variants%20lie%20at%20the%20heart%20of%20mainstream%0AObject-Centric%20Learning%20%28OCL%29.%20Objects%20in%20an%20image%20can%20be%20aggregated%20into%0Arespective%20slot%20vectors%2C%20by%20%5Ctextit%7Biteratively%7D%20refining%20cold-start%20query%0Avectors%2C%20typically%20three%20times%2C%20via%20SA%20on%20image%20features.%20For%20video%2C%20such%0Aaggregation%20is%20%5Ctextit%7Brecurrently%7D%20shared%20across%20frames%2C%20with%20queries%0Acold-started%20on%20the%20first%20frame%20while%20transitioned%20from%20the%20previous%20frame%27s%0Aslots%20on%20non-first%20frames.%20However%2C%20the%20cold-start%20queries%20lack%20sample-specific%0Acues%20thus%20hinder%20precise%20aggregation%20on%20the%20image%20or%20video%27s%20first%20frame%3B%20Also%2C%0Anon-first%20frames%27%20queries%20are%20already%20sample-specific%20thus%20require%20transforms%0Adifferent%20from%20the%20first%20frame%27s%20aggregation.%20We%20address%20these%20issues%20for%20the%0Afirst%20time%20with%20our%20%5Ctextit%7BSmoothSA%7D%3A%20%281%29%20To%20smooth%20SA%20iterations%20on%20the%20image%0Aor%20video%27s%20first%20frame%2C%20we%20%5Ctextit%7Bpreheat%7D%20the%20cold-start%20queries%20with%20rich%0Ainformation%20of%20input%20features%2C%20via%20a%20tiny%20module%20self-distilled%20inside%20OCL%3B%20%282%29%0ATo%20smooth%20SA%20recurrences%20across%20all%20video%20frames%2C%20we%20%5Ctextit%7Bdifferentiate%7D%20the%0Ahomogeneous%20transforms%20on%20the%20first%20and%20non-first%20frames%2C%20by%20using%20full%20and%0Asingle%20iterations%20respectively.%20Comprehensive%20experiments%20on%20object%20discovery%2C%0Arecognition%20and%20downstream%20benchmarks%20validate%20our%20method%27s%20effectiveness.%0AFurther%20analyses%20intuitively%20illuminate%20how%20our%20method%20smooths%20SA%20iterations%0Aand%20recurrences.%20Our%20source%20code%2C%20model%20checkpoints%20and%20training%20logs%20are%0Aavailable%20on%20https%3A//github.com/Genera1Z/SmoothSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05417v2&entry.124074799=Read"},
{"title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models", "author": "J. de Curt\u00f2 and I. de Zarz\u00e0 and Pablo Garc\u00eda and Jordi Cabot", "abstract": "  This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.\n", "link": "http://arxiv.org/abs/2510.26732v1", "date": "2025-10-30", "relevancy": 2.1729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Platform%20Evaluation%20of%20Reasoning%20Capabilities%20in%20Foundation%20Models&body=Title%3A%20Cross-Platform%20Evaluation%20of%20Reasoning%20Capabilities%20in%20Foundation%20Models%0AAuthor%3A%20J.%20de%20Curt%C3%B2%20and%20I.%20de%20Zarz%C3%A0%20and%20Pablo%20Garc%C3%ADa%20and%20Jordi%20Cabot%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20cross-platform%20evaluation%20of%20reasoning%0Acapabilities%20in%20contemporary%20foundation%20models%2C%20establishing%20an%0Ainfrastructure-agnostic%20benchmark%20across%20three%20computational%20paradigms%3A%20HPC%0Asupercomputing%20%28MareNostrum%205%29%2C%20cloud%20platforms%20%28Nebius%20AI%20Studio%29%2C%20and%0Auniversity%20clusters%20%28a%20node%20with%20eight%20H200%20GPUs%29.%0A%20%20We%20evaluate%2015%20foundation%20models%20across%2079%20problems%20spanning%20eight%20academic%0Adomains%20%28Physics%2C%20Mathematics%2C%20Chemistry%2C%20Economics%2C%20Biology%2C%20Statistics%2C%0ACalculus%2C%20and%20Optimization%29%20through%20three%20experimental%20phases%3A%20%281%29%20Baseline%0Aestablishment%3A%20Six%20models%20%28Mixtral-8x7B%2C%20Phi-3%2C%20LLaMA%203.1-8B%2C%20Gemma-2-9b%2C%0AMistral-7B%2C%20OLMo-7B%29%20evaluated%20on%2019%20problems%20using%20MareNostrum%205%2C%20establishing%0Amethodology%20and%20reference%20performance%3B%20%282%29%20Infrastructure%20validation%3A%20The%0A19-problem%20benchmark%20repeated%20on%20university%20cluster%20%28seven%20models%20including%0AFalcon-Mamba%20state-space%20architecture%29%20and%20Nebius%20AI%20Studio%20%28nine%0Astate-of-the-art%20models%3A%20Hermes-4%2070B/405B%2C%20LLaMA%203.1-405B/3.3-70B%2C%20Qwen3%0A30B/235B%2C%20DeepSeek-R1%2C%20GPT-OSS%2020B/120B%29%20to%20confirm%20infrastructure-agnostic%0Areproducibility%3B%20%283%29%20Extended%20evaluation%3A%20Full%2079-problem%20assessment%20on%20both%0Auniversity%20cluster%20and%20Nebius%20platforms%2C%20probing%20generalization%20at%20scale%20across%0Aarchitectural%20diversity.%0A%20%20The%20findings%20challenge%20conventional%20scaling%20assumptions%2C%20establish%20training%0Adata%20quality%20as%20more%20critical%20than%20model%20size%2C%20and%20provide%20actionable%0Aguidelines%20for%20model%20selection%20across%20educational%2C%20production%2C%20and%20research%0Acontexts.%20The%20tri-infrastructure%20methodology%20and%2079-problem%20benchmark%20enable%0Alongitudinal%20tracking%20of%20reasoning%20capabilities%20as%20foundation%20models%20evolve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Platform%2520Evaluation%2520of%2520Reasoning%2520Capabilities%2520in%2520Foundation%2520Models%26entry.906535625%3DJ.%2520de%2520Curt%25C3%25B2%2520and%2520I.%2520de%2520Zarz%25C3%25A0%2520and%2520Pablo%2520Garc%25C3%25ADa%2520and%2520Jordi%2520Cabot%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520cross-platform%2520evaluation%2520of%2520reasoning%250Acapabilities%2520in%2520contemporary%2520foundation%2520models%252C%2520establishing%2520an%250Ainfrastructure-agnostic%2520benchmark%2520across%2520three%2520computational%2520paradigms%253A%2520HPC%250Asupercomputing%2520%2528MareNostrum%25205%2529%252C%2520cloud%2520platforms%2520%2528Nebius%2520AI%2520Studio%2529%252C%2520and%250Auniversity%2520clusters%2520%2528a%2520node%2520with%2520eight%2520H200%2520GPUs%2529.%250A%2520%2520We%2520evaluate%252015%2520foundation%2520models%2520across%252079%2520problems%2520spanning%2520eight%2520academic%250Adomains%2520%2528Physics%252C%2520Mathematics%252C%2520Chemistry%252C%2520Economics%252C%2520Biology%252C%2520Statistics%252C%250ACalculus%252C%2520and%2520Optimization%2529%2520through%2520three%2520experimental%2520phases%253A%2520%25281%2529%2520Baseline%250Aestablishment%253A%2520Six%2520models%2520%2528Mixtral-8x7B%252C%2520Phi-3%252C%2520LLaMA%25203.1-8B%252C%2520Gemma-2-9b%252C%250AMistral-7B%252C%2520OLMo-7B%2529%2520evaluated%2520on%252019%2520problems%2520using%2520MareNostrum%25205%252C%2520establishing%250Amethodology%2520and%2520reference%2520performance%253B%2520%25282%2529%2520Infrastructure%2520validation%253A%2520The%250A19-problem%2520benchmark%2520repeated%2520on%2520university%2520cluster%2520%2528seven%2520models%2520including%250AFalcon-Mamba%2520state-space%2520architecture%2529%2520and%2520Nebius%2520AI%2520Studio%2520%2528nine%250Astate-of-the-art%2520models%253A%2520Hermes-4%252070B/405B%252C%2520LLaMA%25203.1-405B/3.3-70B%252C%2520Qwen3%250A30B/235B%252C%2520DeepSeek-R1%252C%2520GPT-OSS%252020B/120B%2529%2520to%2520confirm%2520infrastructure-agnostic%250Areproducibility%253B%2520%25283%2529%2520Extended%2520evaluation%253A%2520Full%252079-problem%2520assessment%2520on%2520both%250Auniversity%2520cluster%2520and%2520Nebius%2520platforms%252C%2520probing%2520generalization%2520at%2520scale%2520across%250Aarchitectural%2520diversity.%250A%2520%2520The%2520findings%2520challenge%2520conventional%2520scaling%2520assumptions%252C%2520establish%2520training%250Adata%2520quality%2520as%2520more%2520critical%2520than%2520model%2520size%252C%2520and%2520provide%2520actionable%250Aguidelines%2520for%2520model%2520selection%2520across%2520educational%252C%2520production%252C%2520and%2520research%250Acontexts.%2520The%2520tri-infrastructure%2520methodology%2520and%252079-problem%2520benchmark%2520enable%250Alongitudinal%2520tracking%2520of%2520reasoning%2520capabilities%2520as%2520foundation%2520models%2520evolve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Platform%20Evaluation%20of%20Reasoning%20Capabilities%20in%20Foundation%20Models&entry.906535625=J.%20de%20Curt%C3%B2%20and%20I.%20de%20Zarz%C3%A0%20and%20Pablo%20Garc%C3%ADa%20and%20Jordi%20Cabot&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20cross-platform%20evaluation%20of%20reasoning%0Acapabilities%20in%20contemporary%20foundation%20models%2C%20establishing%20an%0Ainfrastructure-agnostic%20benchmark%20across%20three%20computational%20paradigms%3A%20HPC%0Asupercomputing%20%28MareNostrum%205%29%2C%20cloud%20platforms%20%28Nebius%20AI%20Studio%29%2C%20and%0Auniversity%20clusters%20%28a%20node%20with%20eight%20H200%20GPUs%29.%0A%20%20We%20evaluate%2015%20foundation%20models%20across%2079%20problems%20spanning%20eight%20academic%0Adomains%20%28Physics%2C%20Mathematics%2C%20Chemistry%2C%20Economics%2C%20Biology%2C%20Statistics%2C%0ACalculus%2C%20and%20Optimization%29%20through%20three%20experimental%20phases%3A%20%281%29%20Baseline%0Aestablishment%3A%20Six%20models%20%28Mixtral-8x7B%2C%20Phi-3%2C%20LLaMA%203.1-8B%2C%20Gemma-2-9b%2C%0AMistral-7B%2C%20OLMo-7B%29%20evaluated%20on%2019%20problems%20using%20MareNostrum%205%2C%20establishing%0Amethodology%20and%20reference%20performance%3B%20%282%29%20Infrastructure%20validation%3A%20The%0A19-problem%20benchmark%20repeated%20on%20university%20cluster%20%28seven%20models%20including%0AFalcon-Mamba%20state-space%20architecture%29%20and%20Nebius%20AI%20Studio%20%28nine%0Astate-of-the-art%20models%3A%20Hermes-4%2070B/405B%2C%20LLaMA%203.1-405B/3.3-70B%2C%20Qwen3%0A30B/235B%2C%20DeepSeek-R1%2C%20GPT-OSS%2020B/120B%29%20to%20confirm%20infrastructure-agnostic%0Areproducibility%3B%20%283%29%20Extended%20evaluation%3A%20Full%2079-problem%20assessment%20on%20both%0Auniversity%20cluster%20and%20Nebius%20platforms%2C%20probing%20generalization%20at%20scale%20across%0Aarchitectural%20diversity.%0A%20%20The%20findings%20challenge%20conventional%20scaling%20assumptions%2C%20establish%20training%0Adata%20quality%20as%20more%20critical%20than%20model%20size%2C%20and%20provide%20actionable%0Aguidelines%20for%20model%20selection%20across%20educational%2C%20production%2C%20and%20research%0Acontexts.%20The%20tri-infrastructure%20methodology%20and%2079-problem%20benchmark%20enable%0Alongitudinal%20tracking%20of%20reasoning%20capabilities%20as%20foundation%20models%20evolve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26732v1&entry.124074799=Read"},
{"title": "Predicting Video Slot Attention Queries from Random Slot-Feature Pairs", "author": "Rongzhen Zhao and Jian Li and Juho Kannala and Joni Pajarinen", "abstract": "  Unsupervised video Object-Centric Learning (OCL) is promising as it enables\nobject-level scene representation and dynamics modeling as we humans do.\nMainstream video OCL methods adopt a recurrent architecture: An aggregator\naggregates current video frame into object features, termed slots, under some\nqueries; A transitioner transits current slots to queries for the next frame.\nThis is an effective architecture but all existing implementations both\n(\\textit{i1}) neglect to incorporate next frame features, the most informative\nsource for query prediction, and (\\textit{i2}) fail to learn transition\ndynamics, the knowledge essential for query prediction. To address these\nissues, we propose Random Slot-Feature pair for learning Query prediction\n(RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both\nslots and features, which provides more information for query prediction;\n(\\textit{t2}) We train the transitioner to predict queries from slot-feature\npairs randomly sampled from available recurrences, which drives it to learn\ntransition dynamics. Experiments on scene representation demonstrate that our\nmethod surpass existing video OCL methods significantly, e.g., up to 10 points\non object discovery, setting new state-of-the-art. Such superiority also\nbenefits downstream tasks like dynamics modeling. Our core source code, model\ncheckpoints and training logs are available on\nhttps://github.com/Genera1Z/RandSF.Q.\n", "link": "http://arxiv.org/abs/2508.01345v3", "date": "2025-10-30", "relevancy": 2.1607, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs&body=Title%3A%20Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Jian%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Unsupervised%20video%20Object-Centric%20Learning%20%28OCL%29%20is%20promising%20as%20it%20enables%0Aobject-level%20scene%20representation%20and%20dynamics%20modeling%20as%20we%20humans%20do.%0AMainstream%20video%20OCL%20methods%20adopt%20a%20recurrent%20architecture%3A%20An%20aggregator%0Aaggregates%20current%20video%20frame%20into%20object%20features%2C%20termed%20slots%2C%20under%20some%0Aqueries%3B%20A%20transitioner%20transits%20current%20slots%20to%20queries%20for%20the%20next%20frame.%0AThis%20is%20an%20effective%20architecture%20but%20all%20existing%20implementations%20both%0A%28%5Ctextit%7Bi1%7D%29%20neglect%20to%20incorporate%20next%20frame%20features%2C%20the%20most%20informative%0Asource%20for%20query%20prediction%2C%20and%20%28%5Ctextit%7Bi2%7D%29%20fail%20to%20learn%20transition%0Adynamics%2C%20the%20knowledge%20essential%20for%20query%20prediction.%20To%20address%20these%0Aissues%2C%20we%20propose%20Random%20Slot-Feature%20pair%20for%20learning%20Query%20prediction%0A%28RandSF.Q%29%3A%20%28%5Ctextit%7Bt1%7D%29%20We%20design%20a%20new%20transitioner%20to%20incorporate%20both%0Aslots%20and%20features%2C%20which%20provides%20more%20information%20for%20query%20prediction%3B%0A%28%5Ctextit%7Bt2%7D%29%20We%20train%20the%20transitioner%20to%20predict%20queries%20from%20slot-feature%0Apairs%20randomly%20sampled%20from%20available%20recurrences%2C%20which%20drives%20it%20to%20learn%0Atransition%20dynamics.%20Experiments%20on%20scene%20representation%20demonstrate%20that%20our%0Amethod%20surpass%20existing%20video%20OCL%20methods%20significantly%2C%20e.g.%2C%20up%20to%2010%20points%0Aon%20object%20discovery%2C%20setting%20new%20state-of-the-art.%20Such%20superiority%20also%0Abenefits%20downstream%20tasks%20like%20dynamics%20modeling.%20Our%20core%20source%20code%2C%20model%0Acheckpoints%20and%20training%20logs%20are%20available%20on%0Ahttps%3A//github.com/Genera1Z/RandSF.Q.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01345v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Video%2520Slot%2520Attention%2520Queries%2520from%2520Random%2520Slot-Feature%2520Pairs%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Jian%2520Li%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Unsupervised%2520video%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520is%2520promising%2520as%2520it%2520enables%250Aobject-level%2520scene%2520representation%2520and%2520dynamics%2520modeling%2520as%2520we%2520humans%2520do.%250AMainstream%2520video%2520OCL%2520methods%2520adopt%2520a%2520recurrent%2520architecture%253A%2520An%2520aggregator%250Aaggregates%2520current%2520video%2520frame%2520into%2520object%2520features%252C%2520termed%2520slots%252C%2520under%2520some%250Aqueries%253B%2520A%2520transitioner%2520transits%2520current%2520slots%2520to%2520queries%2520for%2520the%2520next%2520frame.%250AThis%2520is%2520an%2520effective%2520architecture%2520but%2520all%2520existing%2520implementations%2520both%250A%2528%255Ctextit%257Bi1%257D%2529%2520neglect%2520to%2520incorporate%2520next%2520frame%2520features%252C%2520the%2520most%2520informative%250Asource%2520for%2520query%2520prediction%252C%2520and%2520%2528%255Ctextit%257Bi2%257D%2529%2520fail%2520to%2520learn%2520transition%250Adynamics%252C%2520the%2520knowledge%2520essential%2520for%2520query%2520prediction.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Random%2520Slot-Feature%2520pair%2520for%2520learning%2520Query%2520prediction%250A%2528RandSF.Q%2529%253A%2520%2528%255Ctextit%257Bt1%257D%2529%2520We%2520design%2520a%2520new%2520transitioner%2520to%2520incorporate%2520both%250Aslots%2520and%2520features%252C%2520which%2520provides%2520more%2520information%2520for%2520query%2520prediction%253B%250A%2528%255Ctextit%257Bt2%257D%2529%2520We%2520train%2520the%2520transitioner%2520to%2520predict%2520queries%2520from%2520slot-feature%250Apairs%2520randomly%2520sampled%2520from%2520available%2520recurrences%252C%2520which%2520drives%2520it%2520to%2520learn%250Atransition%2520dynamics.%2520Experiments%2520on%2520scene%2520representation%2520demonstrate%2520that%2520our%250Amethod%2520surpass%2520existing%2520video%2520OCL%2520methods%2520significantly%252C%2520e.g.%252C%2520up%2520to%252010%2520points%250Aon%2520object%2520discovery%252C%2520setting%2520new%2520state-of-the-art.%2520Such%2520superiority%2520also%250Abenefits%2520downstream%2520tasks%2520like%2520dynamics%2520modeling.%2520Our%2520core%2520source%2520code%252C%2520model%250Acheckpoints%2520and%2520training%2520logs%2520are%2520available%2520on%250Ahttps%253A//github.com/Genera1Z/RandSF.Q.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01345v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs&entry.906535625=Rongzhen%20Zhao%20and%20Jian%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Unsupervised%20video%20Object-Centric%20Learning%20%28OCL%29%20is%20promising%20as%20it%20enables%0Aobject-level%20scene%20representation%20and%20dynamics%20modeling%20as%20we%20humans%20do.%0AMainstream%20video%20OCL%20methods%20adopt%20a%20recurrent%20architecture%3A%20An%20aggregator%0Aaggregates%20current%20video%20frame%20into%20object%20features%2C%20termed%20slots%2C%20under%20some%0Aqueries%3B%20A%20transitioner%20transits%20current%20slots%20to%20queries%20for%20the%20next%20frame.%0AThis%20is%20an%20effective%20architecture%20but%20all%20existing%20implementations%20both%0A%28%5Ctextit%7Bi1%7D%29%20neglect%20to%20incorporate%20next%20frame%20features%2C%20the%20most%20informative%0Asource%20for%20query%20prediction%2C%20and%20%28%5Ctextit%7Bi2%7D%29%20fail%20to%20learn%20transition%0Adynamics%2C%20the%20knowledge%20essential%20for%20query%20prediction.%20To%20address%20these%0Aissues%2C%20we%20propose%20Random%20Slot-Feature%20pair%20for%20learning%20Query%20prediction%0A%28RandSF.Q%29%3A%20%28%5Ctextit%7Bt1%7D%29%20We%20design%20a%20new%20transitioner%20to%20incorporate%20both%0Aslots%20and%20features%2C%20which%20provides%20more%20information%20for%20query%20prediction%3B%0A%28%5Ctextit%7Bt2%7D%29%20We%20train%20the%20transitioner%20to%20predict%20queries%20from%20slot-feature%0Apairs%20randomly%20sampled%20from%20available%20recurrences%2C%20which%20drives%20it%20to%20learn%0Atransition%20dynamics.%20Experiments%20on%20scene%20representation%20demonstrate%20that%20our%0Amethod%20surpass%20existing%20video%20OCL%20methods%20significantly%2C%20e.g.%2C%20up%20to%2010%20points%0Aon%20object%20discovery%2C%20setting%20new%20state-of-the-art.%20Such%20superiority%20also%0Abenefits%20downstream%20tasks%20like%20dynamics%20modeling.%20Our%20core%20source%20code%2C%20model%0Acheckpoints%20and%20training%20logs%20are%20available%20on%0Ahttps%3A//github.com/Genera1Z/RandSF.Q.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01345v3&entry.124074799=Read"},
{"title": "Resource Efficient Multi-stain Kidney Glomeruli Segmentation via\n  Self-supervision", "author": "Zeeshan Nisar and Friedrich Feuerhake and Thomas Lampert", "abstract": "  Semantic segmentation under domain shift remains a fundamental challenge in\ncomputer vision, particularly when labelled training data is scarce. This\nchallenge is particularly exemplified in histopathology image analysis, where\nthe same tissue structures must be segmented across images captured under\ndifferent imaging conditions (stains), each representing a distinct visual\ndomain. Traditional deep learning methods like UNet require extensive labels,\nwhich is both costly and time-consuming, particularly when dealing with\nmultiple domains (or stains). To mitigate this, various unsupervised domain\nadaptation based methods such as UDAGAN have been proposed, which reduce the\nneed for labels by requiring only one (source) stain to be labelled.\nNonetheless, obtaining source stain labels can still be challenging. This\narticle shows that through self-supervised pre-training -- including SimCLR,\nBYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation\nmethods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably,\nwith self-supervised pre-training and using only 5% labels, the performance\ndrops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains,\ncompared to their respective fully supervised counterparts (without\npre-training, using 100% labels). Furthermore, these findings are shown to\ngeneralise beyond their training distribution to public benchmark datasets.\nImplementations and pre-trained models are publicly available\n\\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.\n", "link": "http://arxiv.org/abs/2412.15389v3", "date": "2025-10-30", "relevancy": 2.1555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource%20Efficient%20Multi-stain%20Kidney%20Glomeruli%20Segmentation%20via%0A%20%20Self-supervision&body=Title%3A%20Resource%20Efficient%20Multi-stain%20Kidney%20Glomeruli%20Segmentation%20via%0A%20%20Self-supervision%0AAuthor%3A%20Zeeshan%20Nisar%20and%20Friedrich%20Feuerhake%20and%20Thomas%20Lampert%0AAbstract%3A%20%20%20Semantic%20segmentation%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision%2C%20particularly%20when%20labelled%20training%20data%20is%20scarce.%20This%0Achallenge%20is%20particularly%20exemplified%20in%20histopathology%20image%20analysis%2C%20where%0Athe%20same%20tissue%20structures%20must%20be%20segmented%20across%20images%20captured%20under%0Adifferent%20imaging%20conditions%20%28stains%29%2C%20each%20representing%20a%20distinct%20visual%0Adomain.%20Traditional%20deep%20learning%20methods%20like%20UNet%20require%20extensive%20labels%2C%0Awhich%20is%20both%20costly%20and%20time-consuming%2C%20particularly%20when%20dealing%20with%0Amultiple%20domains%20%28or%20stains%29.%20To%20mitigate%20this%2C%20various%20unsupervised%20domain%0Aadaptation%20based%20methods%20such%20as%20UDAGAN%20have%20been%20proposed%2C%20which%20reduce%20the%0Aneed%20for%20labels%20by%20requiring%20only%20one%20%28source%29%20stain%20to%20be%20labelled.%0ANonetheless%2C%20obtaining%20source%20stain%20labels%20can%20still%20be%20challenging.%20This%0Aarticle%20shows%20that%20through%20self-supervised%20pre-training%20--%20including%20SimCLR%2C%0ABYOL%2C%20and%20a%20novel%20approach%2C%20HR-CS-CO%20--%20the%20performance%20of%20these%20segmentation%0Amethods%20%28UNet%2C%20and%20UDAGAN%29%20can%20be%20retained%20even%20with%2095%25%20fewer%20labels.%20Notably%2C%0Awith%20self-supervised%20pre-training%20and%20using%20only%205%25%20labels%2C%20the%20performance%0Adrops%20are%20minimal%3A%205.9%25%20for%20UNet%20and%206.2%25%20for%20UDAGAN%2C%20averaged%20over%20all%20stains%2C%0Acompared%20to%20their%20respective%20fully%20supervised%20counterparts%20%28without%0Apre-training%2C%20using%20100%25%20labels%29.%20Furthermore%2C%20these%20findings%20are%20shown%20to%0Ageneralise%20beyond%20their%20training%20distribution%20to%20public%20benchmark%20datasets.%0AImplementations%20and%20pre-trained%20models%20are%20publicly%20available%0A%5Chref%7Bhttps%3A//github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git%7D%7Bonline%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource%2520Efficient%2520Multi-stain%2520Kidney%2520Glomeruli%2520Segmentation%2520via%250A%2520%2520Self-supervision%26entry.906535625%3DZeeshan%2520Nisar%2520and%2520Friedrich%2520Feuerhake%2520and%2520Thomas%2520Lampert%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520under%2520domain%2520shift%2520remains%2520a%2520fundamental%2520challenge%2520in%250Acomputer%2520vision%252C%2520particularly%2520when%2520labelled%2520training%2520data%2520is%2520scarce.%2520This%250Achallenge%2520is%2520particularly%2520exemplified%2520in%2520histopathology%2520image%2520analysis%252C%2520where%250Athe%2520same%2520tissue%2520structures%2520must%2520be%2520segmented%2520across%2520images%2520captured%2520under%250Adifferent%2520imaging%2520conditions%2520%2528stains%2529%252C%2520each%2520representing%2520a%2520distinct%2520visual%250Adomain.%2520Traditional%2520deep%2520learning%2520methods%2520like%2520UNet%2520require%2520extensive%2520labels%252C%250Awhich%2520is%2520both%2520costly%2520and%2520time-consuming%252C%2520particularly%2520when%2520dealing%2520with%250Amultiple%2520domains%2520%2528or%2520stains%2529.%2520To%2520mitigate%2520this%252C%2520various%2520unsupervised%2520domain%250Aadaptation%2520based%2520methods%2520such%2520as%2520UDAGAN%2520have%2520been%2520proposed%252C%2520which%2520reduce%2520the%250Aneed%2520for%2520labels%2520by%2520requiring%2520only%2520one%2520%2528source%2529%2520stain%2520to%2520be%2520labelled.%250ANonetheless%252C%2520obtaining%2520source%2520stain%2520labels%2520can%2520still%2520be%2520challenging.%2520This%250Aarticle%2520shows%2520that%2520through%2520self-supervised%2520pre-training%2520--%2520including%2520SimCLR%252C%250ABYOL%252C%2520and%2520a%2520novel%2520approach%252C%2520HR-CS-CO%2520--%2520the%2520performance%2520of%2520these%2520segmentation%250Amethods%2520%2528UNet%252C%2520and%2520UDAGAN%2529%2520can%2520be%2520retained%2520even%2520with%252095%2525%2520fewer%2520labels.%2520Notably%252C%250Awith%2520self-supervised%2520pre-training%2520and%2520using%2520only%25205%2525%2520labels%252C%2520the%2520performance%250Adrops%2520are%2520minimal%253A%25205.9%2525%2520for%2520UNet%2520and%25206.2%2525%2520for%2520UDAGAN%252C%2520averaged%2520over%2520all%2520stains%252C%250Acompared%2520to%2520their%2520respective%2520fully%2520supervised%2520counterparts%2520%2528without%250Apre-training%252C%2520using%2520100%2525%2520labels%2529.%2520Furthermore%252C%2520these%2520findings%2520are%2520shown%2520to%250Ageneralise%2520beyond%2520their%2520training%2520distribution%2520to%2520public%2520benchmark%2520datasets.%250AImplementations%2520and%2520pre-trained%2520models%2520are%2520publicly%2520available%250A%255Chref%257Bhttps%253A//github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git%257D%257Bonline%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource%20Efficient%20Multi-stain%20Kidney%20Glomeruli%20Segmentation%20via%0A%20%20Self-supervision&entry.906535625=Zeeshan%20Nisar%20and%20Friedrich%20Feuerhake%20and%20Thomas%20Lampert&entry.1292438233=%20%20Semantic%20segmentation%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision%2C%20particularly%20when%20labelled%20training%20data%20is%20scarce.%20This%0Achallenge%20is%20particularly%20exemplified%20in%20histopathology%20image%20analysis%2C%20where%0Athe%20same%20tissue%20structures%20must%20be%20segmented%20across%20images%20captured%20under%0Adifferent%20imaging%20conditions%20%28stains%29%2C%20each%20representing%20a%20distinct%20visual%0Adomain.%20Traditional%20deep%20learning%20methods%20like%20UNet%20require%20extensive%20labels%2C%0Awhich%20is%20both%20costly%20and%20time-consuming%2C%20particularly%20when%20dealing%20with%0Amultiple%20domains%20%28or%20stains%29.%20To%20mitigate%20this%2C%20various%20unsupervised%20domain%0Aadaptation%20based%20methods%20such%20as%20UDAGAN%20have%20been%20proposed%2C%20which%20reduce%20the%0Aneed%20for%20labels%20by%20requiring%20only%20one%20%28source%29%20stain%20to%20be%20labelled.%0ANonetheless%2C%20obtaining%20source%20stain%20labels%20can%20still%20be%20challenging.%20This%0Aarticle%20shows%20that%20through%20self-supervised%20pre-training%20--%20including%20SimCLR%2C%0ABYOL%2C%20and%20a%20novel%20approach%2C%20HR-CS-CO%20--%20the%20performance%20of%20these%20segmentation%0Amethods%20%28UNet%2C%20and%20UDAGAN%29%20can%20be%20retained%20even%20with%2095%25%20fewer%20labels.%20Notably%2C%0Awith%20self-supervised%20pre-training%20and%20using%20only%205%25%20labels%2C%20the%20performance%0Adrops%20are%20minimal%3A%205.9%25%20for%20UNet%20and%206.2%25%20for%20UDAGAN%2C%20averaged%20over%20all%20stains%2C%0Acompared%20to%20their%20respective%20fully%20supervised%20counterparts%20%28without%0Apre-training%2C%20using%20100%25%20labels%29.%20Furthermore%2C%20these%20findings%20are%20shown%20to%0Ageneralise%20beyond%20their%20training%20distribution%20to%20public%20benchmark%20datasets.%0AImplementations%20and%20pre-trained%20models%20are%20publicly%20available%0A%5Chref%7Bhttps%3A//github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git%7D%7Bonline%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15389v3&entry.124074799=Read"},
{"title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models", "author": "Mingchen Tu and Zhiqiang Liu and Juan Li and Liangyurui Liu and Junjie Wang and Lei Liang and Wen Zhang", "abstract": "  Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.\n", "link": "http://arxiv.org/abs/2510.26683v1", "date": "2025-10-30", "relevancy": 2.1094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evontree%3A%20Ontology%20Rule-Guided%20Self-Evolution%20of%20Large%20Language%20Models&body=Title%3A%20Evontree%3A%20Ontology%20Rule-Guided%20Self-Evolution%20of%20Large%20Language%20Models%0AAuthor%3A%20Mingchen%20Tu%20and%20Zhiqiang%20Liu%20and%20Juan%20Li%20and%20Liangyurui%20Liu%20and%20Junjie%20Wang%20and%20Lei%20Liang%20and%20Wen%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20capabilities%0Aacross%20multiple%20domains%20by%20leveraging%20massive%20pre-training%20and%20curated%0Afine-tuning%20data.%20However%2C%20in%20data-sensitive%20fields%20such%20as%20healthcare%2C%20the%0Alack%20of%20high-quality%2C%20domain-specific%20training%20corpus%20hinders%20LLMs%27%20adaptation%0Afor%20specialized%20applications.%20Meanwhile%2C%20domain%20experts%20have%20distilled%20domain%0Awisdom%20into%20ontology%20rules%2C%20which%20formalize%20relationships%20among%20concepts%20and%0Aensure%20the%20integrity%20of%20knowledge%20management%20repositories.%20Viewing%20LLMs%20as%0Aimplicit%20repositories%20of%20human%20knowledge%2C%20we%20propose%20Evontree%2C%20a%20novel%0Aframework%20that%20leverages%20a%20small%20set%20of%20high-quality%20ontology%20rules%20to%0Asystematically%20extract%2C%20validate%2C%20and%20enhance%20domain%20knowledge%20within%20LLMs%2C%0Awithout%20requiring%20extensive%20external%20datasets.%20Specifically%2C%20Evontree%20extracts%0Adomain%20ontology%20from%20raw%20models%2C%20detects%20inconsistencies%20using%20two%20core%0Aontology%20rules%2C%20and%20reinforces%20the%20refined%20knowledge%20via%20self-distilled%0Afine-tuning.%20Extensive%20experiments%20on%20medical%20QA%20benchmarks%20with%0ALlama3-8B-Instruct%20and%20Med42-v2%20demonstrate%20consistent%20outperformance%20over%20both%0Aunmodified%20models%20and%20leading%20supervised%20baselines%2C%20achieving%20up%20to%20a%203.7%25%0Aimprovement%20in%20accuracy.%20These%20results%20confirm%20the%20effectiveness%2C%20efficiency%2C%0Aand%20robustness%20of%20our%20approach%20for%20low-resource%20domain%20adaptation%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvontree%253A%2520Ontology%2520Rule-Guided%2520Self-Evolution%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMingchen%2520Tu%2520and%2520Zhiqiang%2520Liu%2520and%2520Juan%2520Li%2520and%2520Liangyurui%2520Liu%2520and%2520Junjie%2520Wang%2520and%2520Lei%2520Liang%2520and%2520Wen%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520capabilities%250Aacross%2520multiple%2520domains%2520by%2520leveraging%2520massive%2520pre-training%2520and%2520curated%250Afine-tuning%2520data.%2520However%252C%2520in%2520data-sensitive%2520fields%2520such%2520as%2520healthcare%252C%2520the%250Alack%2520of%2520high-quality%252C%2520domain-specific%2520training%2520corpus%2520hinders%2520LLMs%2527%2520adaptation%250Afor%2520specialized%2520applications.%2520Meanwhile%252C%2520domain%2520experts%2520have%2520distilled%2520domain%250Awisdom%2520into%2520ontology%2520rules%252C%2520which%2520formalize%2520relationships%2520among%2520concepts%2520and%250Aensure%2520the%2520integrity%2520of%2520knowledge%2520management%2520repositories.%2520Viewing%2520LLMs%2520as%250Aimplicit%2520repositories%2520of%2520human%2520knowledge%252C%2520we%2520propose%2520Evontree%252C%2520a%2520novel%250Aframework%2520that%2520leverages%2520a%2520small%2520set%2520of%2520high-quality%2520ontology%2520rules%2520to%250Asystematically%2520extract%252C%2520validate%252C%2520and%2520enhance%2520domain%2520knowledge%2520within%2520LLMs%252C%250Awithout%2520requiring%2520extensive%2520external%2520datasets.%2520Specifically%252C%2520Evontree%2520extracts%250Adomain%2520ontology%2520from%2520raw%2520models%252C%2520detects%2520inconsistencies%2520using%2520two%2520core%250Aontology%2520rules%252C%2520and%2520reinforces%2520the%2520refined%2520knowledge%2520via%2520self-distilled%250Afine-tuning.%2520Extensive%2520experiments%2520on%2520medical%2520QA%2520benchmarks%2520with%250ALlama3-8B-Instruct%2520and%2520Med42-v2%2520demonstrate%2520consistent%2520outperformance%2520over%2520both%250Aunmodified%2520models%2520and%2520leading%2520supervised%2520baselines%252C%2520achieving%2520up%2520to%2520a%25203.7%2525%250Aimprovement%2520in%2520accuracy.%2520These%2520results%2520confirm%2520the%2520effectiveness%252C%2520efficiency%252C%250Aand%2520robustness%2520of%2520our%2520approach%2520for%2520low-resource%2520domain%2520adaptation%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evontree%3A%20Ontology%20Rule-Guided%20Self-Evolution%20of%20Large%20Language%20Models&entry.906535625=Mingchen%20Tu%20and%20Zhiqiang%20Liu%20and%20Juan%20Li%20and%20Liangyurui%20Liu%20and%20Junjie%20Wang%20and%20Lei%20Liang%20and%20Wen%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20capabilities%0Aacross%20multiple%20domains%20by%20leveraging%20massive%20pre-training%20and%20curated%0Afine-tuning%20data.%20However%2C%20in%20data-sensitive%20fields%20such%20as%20healthcare%2C%20the%0Alack%20of%20high-quality%2C%20domain-specific%20training%20corpus%20hinders%20LLMs%27%20adaptation%0Afor%20specialized%20applications.%20Meanwhile%2C%20domain%20experts%20have%20distilled%20domain%0Awisdom%20into%20ontology%20rules%2C%20which%20formalize%20relationships%20among%20concepts%20and%0Aensure%20the%20integrity%20of%20knowledge%20management%20repositories.%20Viewing%20LLMs%20as%0Aimplicit%20repositories%20of%20human%20knowledge%2C%20we%20propose%20Evontree%2C%20a%20novel%0Aframework%20that%20leverages%20a%20small%20set%20of%20high-quality%20ontology%20rules%20to%0Asystematically%20extract%2C%20validate%2C%20and%20enhance%20domain%20knowledge%20within%20LLMs%2C%0Awithout%20requiring%20extensive%20external%20datasets.%20Specifically%2C%20Evontree%20extracts%0Adomain%20ontology%20from%20raw%20models%2C%20detects%20inconsistencies%20using%20two%20core%0Aontology%20rules%2C%20and%20reinforces%20the%20refined%20knowledge%20via%20self-distilled%0Afine-tuning.%20Extensive%20experiments%20on%20medical%20QA%20benchmarks%20with%0ALlama3-8B-Instruct%20and%20Med42-v2%20demonstrate%20consistent%20outperformance%20over%20both%0Aunmodified%20models%20and%20leading%20supervised%20baselines%2C%20achieving%20up%20to%20a%203.7%25%0Aimprovement%20in%20accuracy.%20These%20results%20confirm%20the%20effectiveness%2C%20efficiency%2C%0Aand%20robustness%20of%20our%20approach%20for%20low-resource%20domain%20adaptation%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26683v1&entry.124074799=Read"},
{"title": "Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras", "author": "Christoffer Koo \u00d8hrstr\u00f8m and Ronja G\u00fcldenring and Lazaros Nalpantidis", "abstract": "  We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.\n", "link": "http://arxiv.org/abs/2510.26614v1", "date": "2025-10-30", "relevancy": 2.0898, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5648}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4968}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Patches%3A%20Asynchronous%2C%20Sparse%2C%20and%20Efficient%20Tokens%20for%20Event%0A%20%20Cameras&body=Title%3A%20Spiking%20Patches%3A%20Asynchronous%2C%20Sparse%2C%20and%20Efficient%20Tokens%20for%20Event%0A%20%20Cameras%0AAuthor%3A%20Christoffer%20Koo%20%C3%98hrstr%C3%B8m%20and%20Ronja%20G%C3%BCldenring%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20%20%20We%20propose%20tokenization%20of%20events%20and%20present%20a%20tokenizer%2C%20Spiking%20Patches%2C%0Aspecifically%20designed%20for%20event%20cameras.%20Given%20a%20stream%20of%20asynchronous%20and%0Aspatially%20sparse%20events%2C%20our%20goal%20is%20to%20discover%20an%20event%20representation%20that%0Apreserves%20these%20properties.%20Prior%20works%20have%20represented%20events%20as%20frames%20or%20as%0Avoxels.%20However%2C%20while%20these%20representations%20yield%20high%20accuracy%2C%20both%20frames%0Aand%20voxels%20are%20synchronous%20and%20decrease%20the%20spatial%20sparsity.%20Spiking%20Patches%0Agives%20the%20means%20to%20preserve%20the%20unique%20properties%20of%20event%20cameras%20and%20we%20show%0Ain%20our%20experiments%20that%20this%20comes%20without%20sacrificing%20accuracy.%20We%20evaluate%0Aour%20tokenizer%20using%20a%20GNN%2C%20PCN%2C%20and%20a%20Transformer%20on%20gesture%20recognition%20and%0Aobject%20detection.%20Tokens%20from%20Spiking%20Patches%20yield%20inference%20times%20that%20are%20up%0Ato%203.4x%20faster%20than%20voxel-based%20tokens%20and%20up%20to%2010.4x%20faster%20than%20frames.%20We%0Aachieve%20this%20while%20matching%20their%20accuracy%20and%20even%20surpassing%20in%20some%20cases%0Awith%20absolute%20improvements%20up%20to%203.8%20for%20gesture%20recognition%20and%20up%20to%201.4%20for%0Aobject%20detection.%20Thus%2C%20tokenization%20constitutes%20a%20novel%20direction%20in%0Aevent-based%20vision%20and%20marks%20a%20step%20towards%20methods%20that%20preserve%20the%0Aproperties%20of%20event%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Patches%253A%2520Asynchronous%252C%2520Sparse%252C%2520and%2520Efficient%2520Tokens%2520for%2520Event%250A%2520%2520Cameras%26entry.906535625%3DChristoffer%2520Koo%2520%25C3%2598hrstr%25C3%25B8m%2520and%2520Ronja%2520G%25C3%25BCldenring%2520and%2520Lazaros%2520Nalpantidis%26entry.1292438233%3D%2520%2520We%2520propose%2520tokenization%2520of%2520events%2520and%2520present%2520a%2520tokenizer%252C%2520Spiking%2520Patches%252C%250Aspecifically%2520designed%2520for%2520event%2520cameras.%2520Given%2520a%2520stream%2520of%2520asynchronous%2520and%250Aspatially%2520sparse%2520events%252C%2520our%2520goal%2520is%2520to%2520discover%2520an%2520event%2520representation%2520that%250Apreserves%2520these%2520properties.%2520Prior%2520works%2520have%2520represented%2520events%2520as%2520frames%2520or%2520as%250Avoxels.%2520However%252C%2520while%2520these%2520representations%2520yield%2520high%2520accuracy%252C%2520both%2520frames%250Aand%2520voxels%2520are%2520synchronous%2520and%2520decrease%2520the%2520spatial%2520sparsity.%2520Spiking%2520Patches%250Agives%2520the%2520means%2520to%2520preserve%2520the%2520unique%2520properties%2520of%2520event%2520cameras%2520and%2520we%2520show%250Ain%2520our%2520experiments%2520that%2520this%2520comes%2520without%2520sacrificing%2520accuracy.%2520We%2520evaluate%250Aour%2520tokenizer%2520using%2520a%2520GNN%252C%2520PCN%252C%2520and%2520a%2520Transformer%2520on%2520gesture%2520recognition%2520and%250Aobject%2520detection.%2520Tokens%2520from%2520Spiking%2520Patches%2520yield%2520inference%2520times%2520that%2520are%2520up%250Ato%25203.4x%2520faster%2520than%2520voxel-based%2520tokens%2520and%2520up%2520to%252010.4x%2520faster%2520than%2520frames.%2520We%250Aachieve%2520this%2520while%2520matching%2520their%2520accuracy%2520and%2520even%2520surpassing%2520in%2520some%2520cases%250Awith%2520absolute%2520improvements%2520up%2520to%25203.8%2520for%2520gesture%2520recognition%2520and%2520up%2520to%25201.4%2520for%250Aobject%2520detection.%2520Thus%252C%2520tokenization%2520constitutes%2520a%2520novel%2520direction%2520in%250Aevent-based%2520vision%2520and%2520marks%2520a%2520step%2520towards%2520methods%2520that%2520preserve%2520the%250Aproperties%2520of%2520event%2520cameras.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Patches%3A%20Asynchronous%2C%20Sparse%2C%20and%20Efficient%20Tokens%20for%20Event%0A%20%20Cameras&entry.906535625=Christoffer%20Koo%20%C3%98hrstr%C3%B8m%20and%20Ronja%20G%C3%BCldenring%20and%20Lazaros%20Nalpantidis&entry.1292438233=%20%20We%20propose%20tokenization%20of%20events%20and%20present%20a%20tokenizer%2C%20Spiking%20Patches%2C%0Aspecifically%20designed%20for%20event%20cameras.%20Given%20a%20stream%20of%20asynchronous%20and%0Aspatially%20sparse%20events%2C%20our%20goal%20is%20to%20discover%20an%20event%20representation%20that%0Apreserves%20these%20properties.%20Prior%20works%20have%20represented%20events%20as%20frames%20or%20as%0Avoxels.%20However%2C%20while%20these%20representations%20yield%20high%20accuracy%2C%20both%20frames%0Aand%20voxels%20are%20synchronous%20and%20decrease%20the%20spatial%20sparsity.%20Spiking%20Patches%0Agives%20the%20means%20to%20preserve%20the%20unique%20properties%20of%20event%20cameras%20and%20we%20show%0Ain%20our%20experiments%20that%20this%20comes%20without%20sacrificing%20accuracy.%20We%20evaluate%0Aour%20tokenizer%20using%20a%20GNN%2C%20PCN%2C%20and%20a%20Transformer%20on%20gesture%20recognition%20and%0Aobject%20detection.%20Tokens%20from%20Spiking%20Patches%20yield%20inference%20times%20that%20are%20up%0Ato%203.4x%20faster%20than%20voxel-based%20tokens%20and%20up%20to%2010.4x%20faster%20than%20frames.%20We%0Aachieve%20this%20while%20matching%20their%20accuracy%20and%20even%20surpassing%20in%20some%20cases%0Awith%20absolute%20improvements%20up%20to%203.8%20for%20gesture%20recognition%20and%20up%20to%201.4%20for%0Aobject%20detection.%20Thus%2C%20tokenization%20constitutes%20a%20novel%20direction%20in%0Aevent-based%20vision%20and%20marks%20a%20step%20towards%20methods%20that%20preserve%20the%0Aproperties%20of%20event%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26614v1&entry.124074799=Read"},
{"title": "Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance", "author": "Valentyna Starodub and Mantas Luko\u0161evi\u010dius", "abstract": "  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n", "link": "http://arxiv.org/abs/2510.26778v1", "date": "2025-10-30", "relevancy": 2.0756, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5246}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surpassing%20state%20of%20the%20art%20on%20AMD%20area%20estimation%20from%20RGB%20fundus%0A%20%20images%20through%20careful%20selection%20of%20U-Net%20architectures%20and%20loss%20functions%0A%20%20for%20class%20imbalance&body=Title%3A%20Surpassing%20state%20of%20the%20art%20on%20AMD%20area%20estimation%20from%20RGB%20fundus%0A%20%20images%20through%20careful%20selection%20of%20U-Net%20architectures%20and%20loss%20functions%0A%20%20for%20class%20imbalance%0AAuthor%3A%20Valentyna%20Starodub%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius%0AAbstract%3A%20%20%20Age-related%20macular%20degeneration%20%28AMD%29%20is%20one%20of%20the%20leading%20causes%20of%0Airreversible%20vision%20impairment%20in%20people%20over%20the%20age%20of%2060.%20This%20research%0Afocuses%20on%20semantic%20segmentation%20for%20AMD%20lesion%20detection%20in%20RGB%20fundus%20images%2C%0Aa%20non-invasive%20and%20cost-effective%20imaging%20technique.%20The%20results%20of%20the%20ADAM%0Achallenge%20-%20the%20most%20comprehensive%20AMD%20detection%20from%20RGB%20fundus%20images%0Aresearch%20competition%20and%20open%20dataset%20to%20date%20-%20serve%20as%20a%20benchmark%20for%20our%0Aevaluation.%20Taking%20the%20U-Net%20connectivity%20as%20a%20base%20of%20our%20framework%2C%20we%0Aevaluate%20and%20compare%20several%20approaches%20to%20improve%20the%20segmentation%20model%27s%0Aarchitecture%20and%20training%20pipeline%2C%20including%20pre-processing%20techniques%2C%0Aencoder%20%28backbone%29%20deep%20network%20types%20of%20varying%20complexity%2C%20and%20specialized%0Aloss%20functions%20to%20mitigate%20class%20imbalances%20on%20image%20and%20pixel%20levels.%20The%20main%0Aoutcome%20of%20this%20research%20is%20the%20final%20configuration%20of%20the%20AMD%20detection%0Aframework%2C%20which%20outperforms%20all%20the%20prior%20ADAM%20challenge%20submissions%20on%20the%0Amulti-class%20segmentation%20of%20different%20AMD%20lesion%20types%20in%20non-invasive%20RGB%0Afundus%20images.%20The%20source%20code%20used%20to%20conduct%20the%20experiments%20presented%20in%0Athis%20paper%20is%20made%20freely%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurpassing%2520state%2520of%2520the%2520art%2520on%2520AMD%2520area%2520estimation%2520from%2520RGB%2520fundus%250A%2520%2520images%2520through%2520careful%2520selection%2520of%2520U-Net%2520architectures%2520and%2520loss%2520functions%250A%2520%2520for%2520class%2520imbalance%26entry.906535625%3DValentyna%2520Starodub%2520and%2520Mantas%2520Luko%25C5%25A1evi%25C4%258Dius%26entry.1292438233%3D%2520%2520Age-related%2520macular%2520degeneration%2520%2528AMD%2529%2520is%2520one%2520of%2520the%2520leading%2520causes%2520of%250Airreversible%2520vision%2520impairment%2520in%2520people%2520over%2520the%2520age%2520of%252060.%2520This%2520research%250Afocuses%2520on%2520semantic%2520segmentation%2520for%2520AMD%2520lesion%2520detection%2520in%2520RGB%2520fundus%2520images%252C%250Aa%2520non-invasive%2520and%2520cost-effective%2520imaging%2520technique.%2520The%2520results%2520of%2520the%2520ADAM%250Achallenge%2520-%2520the%2520most%2520comprehensive%2520AMD%2520detection%2520from%2520RGB%2520fundus%2520images%250Aresearch%2520competition%2520and%2520open%2520dataset%2520to%2520date%2520-%2520serve%2520as%2520a%2520benchmark%2520for%2520our%250Aevaluation.%2520Taking%2520the%2520U-Net%2520connectivity%2520as%2520a%2520base%2520of%2520our%2520framework%252C%2520we%250Aevaluate%2520and%2520compare%2520several%2520approaches%2520to%2520improve%2520the%2520segmentation%2520model%2527s%250Aarchitecture%2520and%2520training%2520pipeline%252C%2520including%2520pre-processing%2520techniques%252C%250Aencoder%2520%2528backbone%2529%2520deep%2520network%2520types%2520of%2520varying%2520complexity%252C%2520and%2520specialized%250Aloss%2520functions%2520to%2520mitigate%2520class%2520imbalances%2520on%2520image%2520and%2520pixel%2520levels.%2520The%2520main%250Aoutcome%2520of%2520this%2520research%2520is%2520the%2520final%2520configuration%2520of%2520the%2520AMD%2520detection%250Aframework%252C%2520which%2520outperforms%2520all%2520the%2520prior%2520ADAM%2520challenge%2520submissions%2520on%2520the%250Amulti-class%2520segmentation%2520of%2520different%2520AMD%2520lesion%2520types%2520in%2520non-invasive%2520RGB%250Afundus%2520images.%2520The%2520source%2520code%2520used%2520to%2520conduct%2520the%2520experiments%2520presented%2520in%250Athis%2520paper%2520is%2520made%2520freely%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surpassing%20state%20of%20the%20art%20on%20AMD%20area%20estimation%20from%20RGB%20fundus%0A%20%20images%20through%20careful%20selection%20of%20U-Net%20architectures%20and%20loss%20functions%0A%20%20for%20class%20imbalance&entry.906535625=Valentyna%20Starodub%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius&entry.1292438233=%20%20Age-related%20macular%20degeneration%20%28AMD%29%20is%20one%20of%20the%20leading%20causes%20of%0Airreversible%20vision%20impairment%20in%20people%20over%20the%20age%20of%2060.%20This%20research%0Afocuses%20on%20semantic%20segmentation%20for%20AMD%20lesion%20detection%20in%20RGB%20fundus%20images%2C%0Aa%20non-invasive%20and%20cost-effective%20imaging%20technique.%20The%20results%20of%20the%20ADAM%0Achallenge%20-%20the%20most%20comprehensive%20AMD%20detection%20from%20RGB%20fundus%20images%0Aresearch%20competition%20and%20open%20dataset%20to%20date%20-%20serve%20as%20a%20benchmark%20for%20our%0Aevaluation.%20Taking%20the%20U-Net%20connectivity%20as%20a%20base%20of%20our%20framework%2C%20we%0Aevaluate%20and%20compare%20several%20approaches%20to%20improve%20the%20segmentation%20model%27s%0Aarchitecture%20and%20training%20pipeline%2C%20including%20pre-processing%20techniques%2C%0Aencoder%20%28backbone%29%20deep%20network%20types%20of%20varying%20complexity%2C%20and%20specialized%0Aloss%20functions%20to%20mitigate%20class%20imbalances%20on%20image%20and%20pixel%20levels.%20The%20main%0Aoutcome%20of%20this%20research%20is%20the%20final%20configuration%20of%20the%20AMD%20detection%0Aframework%2C%20which%20outperforms%20all%20the%20prior%20ADAM%20challenge%20submissions%20on%20the%0Amulti-class%20segmentation%20of%20different%20AMD%20lesion%20types%20in%20non-invasive%20RGB%0Afundus%20images.%20The%20source%20code%20used%20to%20conduct%20the%20experiments%20presented%20in%0Athis%20paper%20is%20made%20freely%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26778v1&entry.124074799=Read"},
{"title": "SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models", "author": "Anushka Sivakumar and Andrew Zhang and Zaber Hakim and Chris Thomas", "abstract": "  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n", "link": "http://arxiv.org/abs/2510.26769v1", "date": "2025-10-30", "relevancy": 2.0744, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5229}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SteerVLM%3A%20Robust%20Model%20Control%20through%20Lightweight%20Activation%20Steering%0A%20%20for%20Vision%20Language%20Models&body=Title%3A%20SteerVLM%3A%20Robust%20Model%20Control%20through%20Lightweight%20Activation%20Steering%0A%20%20for%20Vision%20Language%20Models%0AAuthor%3A%20Anushka%20Sivakumar%20and%20Andrew%20Zhang%20and%20Zaber%20Hakim%20and%20Chris%20Thomas%0AAbstract%3A%20%20%20This%20work%20introduces%20SteerVLM%2C%20a%20lightweight%20steering%20module%20designed%20to%0Aguide%20Vision-Language%20Models%20%28VLMs%29%20towards%20outputs%20that%20better%20adhere%20to%0Adesired%20instructions.%20Our%20approach%20learns%20from%20the%20latent%20embeddings%20of%20paired%0Aprompts%20encoding%20target%20and%20converse%20behaviors%20to%20dynamically%20adjust%0Aactivations%20connecting%20the%20language%20modality%20with%20image%20context.%20This%20allows%0Afor%20fine-grained%2C%20inference-time%20control%20over%20complex%20output%20semantics%20without%0Amodifying%20model%20weights%20while%20preserving%20performance%20on%20off-target%20tasks.%20Our%0Asteering%20module%20requires%20learning%20parameters%20equal%20to%200.14%25%20of%20the%20original%0AVLM%27s%20size.%20Our%20steering%20module%20gains%20model%20control%20through%20dimension-wise%0Aactivation%20modulation%20and%20adaptive%20steering%20across%20layers%20without%20requiring%0Apre-extracted%20static%20vectors%20or%20manual%20tuning%20of%20intervention%20points.%0AFurthermore%2C%20we%20introduce%20VNIA%20%28Visual%20Narrative%20Intent%20Alignment%29%2C%20a%0Amultimodal%20dataset%20specifically%20created%20to%20facilitate%20the%20development%20and%0Aevaluation%20of%20VLM%20steering%20techniques.%20Our%20method%20outperforms%20existing%0Aintervention%20techniques%20on%20steering%20and%20hallucination%20mitigation%20benchmarks%20for%0AVLMs%20and%20proposes%20a%20robust%20solution%20for%20multimodal%20model%20control%20through%0Aactivation%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteerVLM%253A%2520Robust%2520Model%2520Control%2520through%2520Lightweight%2520Activation%2520Steering%250A%2520%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DAnushka%2520Sivakumar%2520and%2520Andrew%2520Zhang%2520and%2520Zaber%2520Hakim%2520and%2520Chris%2520Thomas%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520SteerVLM%252C%2520a%2520lightweight%2520steering%2520module%2520designed%2520to%250Aguide%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520towards%2520outputs%2520that%2520better%2520adhere%2520to%250Adesired%2520instructions.%2520Our%2520approach%2520learns%2520from%2520the%2520latent%2520embeddings%2520of%2520paired%250Aprompts%2520encoding%2520target%2520and%2520converse%2520behaviors%2520to%2520dynamically%2520adjust%250Aactivations%2520connecting%2520the%2520language%2520modality%2520with%2520image%2520context.%2520This%2520allows%250Afor%2520fine-grained%252C%2520inference-time%2520control%2520over%2520complex%2520output%2520semantics%2520without%250Amodifying%2520model%2520weights%2520while%2520preserving%2520performance%2520on%2520off-target%2520tasks.%2520Our%250Asteering%2520module%2520requires%2520learning%2520parameters%2520equal%2520to%25200.14%2525%2520of%2520the%2520original%250AVLM%2527s%2520size.%2520Our%2520steering%2520module%2520gains%2520model%2520control%2520through%2520dimension-wise%250Aactivation%2520modulation%2520and%2520adaptive%2520steering%2520across%2520layers%2520without%2520requiring%250Apre-extracted%2520static%2520vectors%2520or%2520manual%2520tuning%2520of%2520intervention%2520points.%250AFurthermore%252C%2520we%2520introduce%2520VNIA%2520%2528Visual%2520Narrative%2520Intent%2520Alignment%2529%252C%2520a%250Amultimodal%2520dataset%2520specifically%2520created%2520to%2520facilitate%2520the%2520development%2520and%250Aevaluation%2520of%2520VLM%2520steering%2520techniques.%2520Our%2520method%2520outperforms%2520existing%250Aintervention%2520techniques%2520on%2520steering%2520and%2520hallucination%2520mitigation%2520benchmarks%2520for%250AVLMs%2520and%2520proposes%2520a%2520robust%2520solution%2520for%2520multimodal%2520model%2520control%2520through%250Aactivation%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteerVLM%3A%20Robust%20Model%20Control%20through%20Lightweight%20Activation%20Steering%0A%20%20for%20Vision%20Language%20Models&entry.906535625=Anushka%20Sivakumar%20and%20Andrew%20Zhang%20and%20Zaber%20Hakim%20and%20Chris%20Thomas&entry.1292438233=%20%20This%20work%20introduces%20SteerVLM%2C%20a%20lightweight%20steering%20module%20designed%20to%0Aguide%20Vision-Language%20Models%20%28VLMs%29%20towards%20outputs%20that%20better%20adhere%20to%0Adesired%20instructions.%20Our%20approach%20learns%20from%20the%20latent%20embeddings%20of%20paired%0Aprompts%20encoding%20target%20and%20converse%20behaviors%20to%20dynamically%20adjust%0Aactivations%20connecting%20the%20language%20modality%20with%20image%20context.%20This%20allows%0Afor%20fine-grained%2C%20inference-time%20control%20over%20complex%20output%20semantics%20without%0Amodifying%20model%20weights%20while%20preserving%20performance%20on%20off-target%20tasks.%20Our%0Asteering%20module%20requires%20learning%20parameters%20equal%20to%200.14%25%20of%20the%20original%0AVLM%27s%20size.%20Our%20steering%20module%20gains%20model%20control%20through%20dimension-wise%0Aactivation%20modulation%20and%20adaptive%20steering%20across%20layers%20without%20requiring%0Apre-extracted%20static%20vectors%20or%20manual%20tuning%20of%20intervention%20points.%0AFurthermore%2C%20we%20introduce%20VNIA%20%28Visual%20Narrative%20Intent%20Alignment%29%2C%20a%0Amultimodal%20dataset%20specifically%20created%20to%20facilitate%20the%20development%20and%0Aevaluation%20of%20VLM%20steering%20techniques.%20Our%20method%20outperforms%20existing%0Aintervention%20techniques%20on%20steering%20and%20hallucination%20mitigation%20benchmarks%20for%0AVLMs%20and%20proposes%20a%20robust%20solution%20for%20multimodal%20model%20control%20through%0Aactivation%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26769v1&entry.124074799=Read"},
{"title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models", "author": "Amir Hossein Rahmati and Sanket Jantre and Weifeng Zhang and Yucheng Wang and Byung-Jun Yoon and Nathan M. Urban and Xiaoning Qian", "abstract": "  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.\n", "link": "http://arxiv.org/abs/2505.17773v3", "date": "2025-10-30", "relevancy": 2.0622, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Amir%20Hossein%20Rahmati%20and%20Sanket%20Jantre%20and%20Weifeng%20Zhang%20and%20Yucheng%20Wang%20and%20Byung-Jun%20Yoon%20and%20Nathan%20M.%20Urban%20and%20Xiaoning%20Qian%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20offers%20a%20cost-effective%20solution%20for%20fine-tuning%0Alarge%20language%20models%20%28LLMs%29%2C%20but%20it%20often%20produces%20overconfident%20predictions%0Ain%20data-scarce%20few-shot%20settings.%20To%20address%20this%20issue%2C%20several%20classical%0Astatistical%20learning%20approaches%20have%20been%20repurposed%20for%20scalable%0Auncertainty-aware%20LoRA%20fine-tuning.%20However%2C%20these%20approaches%20neglect%20how%20input%0Acharacteristics%20affect%20the%20predictive%20uncertainty%20estimates.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Contextual%20Low-Rank%20Adaptation%20%28C-LoRA%29%20as%20a%20novel%0Auncertainty-aware%20and%20parameter%20efficient%20fine-tuning%20approach%2C%20by%20developing%0Anew%20lightweight%20LoRA%20modules%20contextualized%20to%20each%20input%20data%20sample%20to%0Adynamically%20adapt%20uncertainty%20estimates.%20Incorporating%20data-driven%20contexts%0Ainto%20the%20parameter%20posteriors%2C%20C-LoRA%20mitigates%20overfitting%2C%20achieves%0Awell-calibrated%20uncertainties%2C%20and%20yields%20robust%20predictions.%20Extensive%0Aexperiments%20on%20LLaMA2-7B%20models%20demonstrate%20that%20C-LoRA%20consistently%0Aoutperforms%20the%20state-of-the-art%20uncertainty-aware%20LoRA%20methods%20in%20both%0Auncertainty%20quantification%20and%20model%20generalization.%20Ablation%20studies%20further%0Aconfirm%20the%20critical%20role%20of%20our%20contextual%20modules%20in%20capturing%0Asample-specific%20uncertainties.%20C-LoRA%20sets%20a%20new%20standard%20for%20robust%2C%0Auncertainty-aware%20LLM%20fine-tuning%20in%20few-shot%20regimes.%20Although%20our%20experiments%0Aare%20limited%20to%207B%20models%2C%20our%20method%20is%20architecture-agnostic%20and%2C%20in%0Aprinciple%2C%20applies%20beyond%20this%20scale%3B%20studying%20its%20scaling%20to%20larger%20models%0Aremains%20an%20open%20problem.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ahra99/c_lora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17773v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-LoRA%253A%2520Contextual%2520Low-Rank%2520Adaptation%2520for%2520Uncertainty%2520Estimation%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAmir%2520Hossein%2520Rahmati%2520and%2520Sanket%2520Jantre%2520and%2520Weifeng%2520Zhang%2520and%2520Yucheng%2520Wang%2520and%2520Byung-Jun%2520Yoon%2520and%2520Nathan%2520M.%2520Urban%2520and%2520Xiaoning%2520Qian%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520offers%2520a%2520cost-effective%2520solution%2520for%2520fine-tuning%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520but%2520it%2520often%2520produces%2520overconfident%2520predictions%250Ain%2520data-scarce%2520few-shot%2520settings.%2520To%2520address%2520this%2520issue%252C%2520several%2520classical%250Astatistical%2520learning%2520approaches%2520have%2520been%2520repurposed%2520for%2520scalable%250Auncertainty-aware%2520LoRA%2520fine-tuning.%2520However%252C%2520these%2520approaches%2520neglect%2520how%2520input%250Acharacteristics%2520affect%2520the%2520predictive%2520uncertainty%2520estimates.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520Contextual%2520Low-Rank%2520Adaptation%2520%2528C-LoRA%2529%2520as%2520a%2520novel%250Auncertainty-aware%2520and%2520parameter%2520efficient%2520fine-tuning%2520approach%252C%2520by%2520developing%250Anew%2520lightweight%2520LoRA%2520modules%2520contextualized%2520to%2520each%2520input%2520data%2520sample%2520to%250Adynamically%2520adapt%2520uncertainty%2520estimates.%2520Incorporating%2520data-driven%2520contexts%250Ainto%2520the%2520parameter%2520posteriors%252C%2520C-LoRA%2520mitigates%2520overfitting%252C%2520achieves%250Awell-calibrated%2520uncertainties%252C%2520and%2520yields%2520robust%2520predictions.%2520Extensive%250Aexperiments%2520on%2520LLaMA2-7B%2520models%2520demonstrate%2520that%2520C-LoRA%2520consistently%250Aoutperforms%2520the%2520state-of-the-art%2520uncertainty-aware%2520LoRA%2520methods%2520in%2520both%250Auncertainty%2520quantification%2520and%2520model%2520generalization.%2520Ablation%2520studies%2520further%250Aconfirm%2520the%2520critical%2520role%2520of%2520our%2520contextual%2520modules%2520in%2520capturing%250Asample-specific%2520uncertainties.%2520C-LoRA%2520sets%2520a%2520new%2520standard%2520for%2520robust%252C%250Auncertainty-aware%2520LLM%2520fine-tuning%2520in%2520few-shot%2520regimes.%2520Although%2520our%2520experiments%250Aare%2520limited%2520to%25207B%2520models%252C%2520our%2520method%2520is%2520architecture-agnostic%2520and%252C%2520in%250Aprinciple%252C%2520applies%2520beyond%2520this%2520scale%253B%2520studying%2520its%2520scaling%2520to%2520larger%2520models%250Aremains%2520an%2520open%2520problem.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ahra99/c_lora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17773v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models&entry.906535625=Amir%20Hossein%20Rahmati%20and%20Sanket%20Jantre%20and%20Weifeng%20Zhang%20and%20Yucheng%20Wang%20and%20Byung-Jun%20Yoon%20and%20Nathan%20M.%20Urban%20and%20Xiaoning%20Qian&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20offers%20a%20cost-effective%20solution%20for%20fine-tuning%0Alarge%20language%20models%20%28LLMs%29%2C%20but%20it%20often%20produces%20overconfident%20predictions%0Ain%20data-scarce%20few-shot%20settings.%20To%20address%20this%20issue%2C%20several%20classical%0Astatistical%20learning%20approaches%20have%20been%20repurposed%20for%20scalable%0Auncertainty-aware%20LoRA%20fine-tuning.%20However%2C%20these%20approaches%20neglect%20how%20input%0Acharacteristics%20affect%20the%20predictive%20uncertainty%20estimates.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Contextual%20Low-Rank%20Adaptation%20%28C-LoRA%29%20as%20a%20novel%0Auncertainty-aware%20and%20parameter%20efficient%20fine-tuning%20approach%2C%20by%20developing%0Anew%20lightweight%20LoRA%20modules%20contextualized%20to%20each%20input%20data%20sample%20to%0Adynamically%20adapt%20uncertainty%20estimates.%20Incorporating%20data-driven%20contexts%0Ainto%20the%20parameter%20posteriors%2C%20C-LoRA%20mitigates%20overfitting%2C%20achieves%0Awell-calibrated%20uncertainties%2C%20and%20yields%20robust%20predictions.%20Extensive%0Aexperiments%20on%20LLaMA2-7B%20models%20demonstrate%20that%20C-LoRA%20consistently%0Aoutperforms%20the%20state-of-the-art%20uncertainty-aware%20LoRA%20methods%20in%20both%0Auncertainty%20quantification%20and%20model%20generalization.%20Ablation%20studies%20further%0Aconfirm%20the%20critical%20role%20of%20our%20contextual%20modules%20in%20capturing%0Asample-specific%20uncertainties.%20C-LoRA%20sets%20a%20new%20standard%20for%20robust%2C%0Auncertainty-aware%20LLM%20fine-tuning%20in%20few-shot%20regimes.%20Although%20our%20experiments%0Aare%20limited%20to%207B%20models%2C%20our%20method%20is%20architecture-agnostic%20and%2C%20in%0Aprinciple%2C%20applies%20beyond%20this%20scale%3B%20studying%20its%20scaling%20to%20larger%20models%0Aremains%20an%20open%20problem.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ahra99/c_lora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17773v3&entry.124074799=Read"},
{"title": "LSM-MS2: A Foundation Model Bridging Spectral Identification and\n  Biological Interpretation", "author": "Gabriel Asher and Devesh Shah and Amy A. Caudy and Luke Ferro and Lea Amar and Ana S. H. Costa and Thomas Patton and Niall O'Connor and Jennifer M. Campbell and Jack Geremia", "abstract": "  A vast majority of mass spectrometry data remains uncharacterized, leaving\nmuch of its biological and chemical information untapped. Recent advances in\nmachine learning have begun to address this gap, particularly for tasks such as\nspectral identification in tandem mass spectrometry data. Here, we present the\nlatest generation of LSM-MS2, a large-scale deep learning foundation model\ntrained on millions of spectra to learn a semantic chemical space. LSM-MS2\nachieves state-of-the-art performance in spectral identification, improving on\nexisting methods by 30% in accuracy of identifying challenging isomeric\ncompounds, yielding 42% more correct identifications in complex biological\nsamples, and maintaining robustness under low-concentration conditions.\nFurthermore, LSM-MS2 produces rich spectral embeddings that enable direct\nbiological interpretation from minimal downstream data, successfully\ndifferentiating disease states and predicting clinical outcomes across diverse\ntranslational applications.\n", "link": "http://arxiv.org/abs/2510.26715v1", "date": "2025-10-30", "relevancy": 2.0596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSM-MS2%3A%20A%20Foundation%20Model%20Bridging%20Spectral%20Identification%20and%0A%20%20Biological%20Interpretation&body=Title%3A%20LSM-MS2%3A%20A%20Foundation%20Model%20Bridging%20Spectral%20Identification%20and%0A%20%20Biological%20Interpretation%0AAuthor%3A%20Gabriel%20Asher%20and%20Devesh%20Shah%20and%20Amy%20A.%20Caudy%20and%20Luke%20Ferro%20and%20Lea%20Amar%20and%20Ana%20S.%20H.%20Costa%20and%20Thomas%20Patton%20and%20Niall%20O%27Connor%20and%20Jennifer%20M.%20Campbell%20and%20Jack%20Geremia%0AAbstract%3A%20%20%20A%20vast%20majority%20of%20mass%20spectrometry%20data%20remains%20uncharacterized%2C%20leaving%0Amuch%20of%20its%20biological%20and%20chemical%20information%20untapped.%20Recent%20advances%20in%0Amachine%20learning%20have%20begun%20to%20address%20this%20gap%2C%20particularly%20for%20tasks%20such%20as%0Aspectral%20identification%20in%20tandem%20mass%20spectrometry%20data.%20Here%2C%20we%20present%20the%0Alatest%20generation%20of%20LSM-MS2%2C%20a%20large-scale%20deep%20learning%20foundation%20model%0Atrained%20on%20millions%20of%20spectra%20to%20learn%20a%20semantic%20chemical%20space.%20LSM-MS2%0Aachieves%20state-of-the-art%20performance%20in%20spectral%20identification%2C%20improving%20on%0Aexisting%20methods%20by%2030%25%20in%20accuracy%20of%20identifying%20challenging%20isomeric%0Acompounds%2C%20yielding%2042%25%20more%20correct%20identifications%20in%20complex%20biological%0Asamples%2C%20and%20maintaining%20robustness%20under%20low-concentration%20conditions.%0AFurthermore%2C%20LSM-MS2%20produces%20rich%20spectral%20embeddings%20that%20enable%20direct%0Abiological%20interpretation%20from%20minimal%20downstream%20data%2C%20successfully%0Adifferentiating%20disease%20states%20and%20predicting%20clinical%20outcomes%20across%20diverse%0Atranslational%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSM-MS2%253A%2520A%2520Foundation%2520Model%2520Bridging%2520Spectral%2520Identification%2520and%250A%2520%2520Biological%2520Interpretation%26entry.906535625%3DGabriel%2520Asher%2520and%2520Devesh%2520Shah%2520and%2520Amy%2520A.%2520Caudy%2520and%2520Luke%2520Ferro%2520and%2520Lea%2520Amar%2520and%2520Ana%2520S.%2520H.%2520Costa%2520and%2520Thomas%2520Patton%2520and%2520Niall%2520O%2527Connor%2520and%2520Jennifer%2520M.%2520Campbell%2520and%2520Jack%2520Geremia%26entry.1292438233%3D%2520%2520A%2520vast%2520majority%2520of%2520mass%2520spectrometry%2520data%2520remains%2520uncharacterized%252C%2520leaving%250Amuch%2520of%2520its%2520biological%2520and%2520chemical%2520information%2520untapped.%2520Recent%2520advances%2520in%250Amachine%2520learning%2520have%2520begun%2520to%2520address%2520this%2520gap%252C%2520particularly%2520for%2520tasks%2520such%2520as%250Aspectral%2520identification%2520in%2520tandem%2520mass%2520spectrometry%2520data.%2520Here%252C%2520we%2520present%2520the%250Alatest%2520generation%2520of%2520LSM-MS2%252C%2520a%2520large-scale%2520deep%2520learning%2520foundation%2520model%250Atrained%2520on%2520millions%2520of%2520spectra%2520to%2520learn%2520a%2520semantic%2520chemical%2520space.%2520LSM-MS2%250Aachieves%2520state-of-the-art%2520performance%2520in%2520spectral%2520identification%252C%2520improving%2520on%250Aexisting%2520methods%2520by%252030%2525%2520in%2520accuracy%2520of%2520identifying%2520challenging%2520isomeric%250Acompounds%252C%2520yielding%252042%2525%2520more%2520correct%2520identifications%2520in%2520complex%2520biological%250Asamples%252C%2520and%2520maintaining%2520robustness%2520under%2520low-concentration%2520conditions.%250AFurthermore%252C%2520LSM-MS2%2520produces%2520rich%2520spectral%2520embeddings%2520that%2520enable%2520direct%250Abiological%2520interpretation%2520from%2520minimal%2520downstream%2520data%252C%2520successfully%250Adifferentiating%2520disease%2520states%2520and%2520predicting%2520clinical%2520outcomes%2520across%2520diverse%250Atranslational%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSM-MS2%3A%20A%20Foundation%20Model%20Bridging%20Spectral%20Identification%20and%0A%20%20Biological%20Interpretation&entry.906535625=Gabriel%20Asher%20and%20Devesh%20Shah%20and%20Amy%20A.%20Caudy%20and%20Luke%20Ferro%20and%20Lea%20Amar%20and%20Ana%20S.%20H.%20Costa%20and%20Thomas%20Patton%20and%20Niall%20O%27Connor%20and%20Jennifer%20M.%20Campbell%20and%20Jack%20Geremia&entry.1292438233=%20%20A%20vast%20majority%20of%20mass%20spectrometry%20data%20remains%20uncharacterized%2C%20leaving%0Amuch%20of%20its%20biological%20and%20chemical%20information%20untapped.%20Recent%20advances%20in%0Amachine%20learning%20have%20begun%20to%20address%20this%20gap%2C%20particularly%20for%20tasks%20such%20as%0Aspectral%20identification%20in%20tandem%20mass%20spectrometry%20data.%20Here%2C%20we%20present%20the%0Alatest%20generation%20of%20LSM-MS2%2C%20a%20large-scale%20deep%20learning%20foundation%20model%0Atrained%20on%20millions%20of%20spectra%20to%20learn%20a%20semantic%20chemical%20space.%20LSM-MS2%0Aachieves%20state-of-the-art%20performance%20in%20spectral%20identification%2C%20improving%20on%0Aexisting%20methods%20by%2030%25%20in%20accuracy%20of%20identifying%20challenging%20isomeric%0Acompounds%2C%20yielding%2042%25%20more%20correct%20identifications%20in%20complex%20biological%0Asamples%2C%20and%20maintaining%20robustness%20under%20low-concentration%20conditions.%0AFurthermore%2C%20LSM-MS2%20produces%20rich%20spectral%20embeddings%20that%20enable%20direct%0Abiological%20interpretation%20from%20minimal%20downstream%20data%2C%20successfully%0Adifferentiating%20disease%20states%20and%20predicting%20clinical%20outcomes%20across%20diverse%0Atranslational%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26715v1&entry.124074799=Read"},
{"title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards", "author": "Zhilin Wang and Jiaqi Zeng and Olivier Delalleau and Ellie Evans and Daniel Egert and Hoo-Chang Shin and Felipe Soares and Yi Dong and Oleksii Kuchaiev", "abstract": "  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n", "link": "http://arxiv.org/abs/2509.21319v2", "date": "2025-10-30", "relevancy": 2.0557, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards&body=Title%3A%20RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards%0AAuthor%3A%20Zhilin%20Wang%20and%20Jiaqi%20Zeng%20and%20Olivier%20Delalleau%20and%20Ellie%20Evans%20and%20Daniel%20Egert%20and%20Hoo-Chang%20Shin%20and%20Felipe%20Soares%20and%20Yi%20Dong%20and%20Oleksii%20Kuchaiev%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20and%20Reinforcement%20Learning%0Awith%20Verifiable%20Rewards%20%28RLVR%29%20are%20the%20main%20RL%20paradigms%20used%20in%20LLM%0Apost-training%2C%20each%20offering%20distinct%20advantages.%20However%2C%20RLHF%20struggles%20with%0Ainterpretability%20and%20reward%20hacking%20because%20it%20relies%20on%20human%20judgments%20that%0Ausually%20lack%20explicit%20criteria%2C%20whereas%20RLVR%20is%20limited%20in%20scope%20by%20its%20focus%0Aon%20correctness-based%20verifiers.%20We%20propose%20Reinforcement%20Learning%20with%20Binary%0AFlexible%20Feedback%20%28RLBFF%29%2C%20which%20combines%20the%20versatility%20of%20human-driven%0Apreferences%20with%20the%20precision%20of%20rule-based%20verification%2C%20enabling%20reward%0Amodels%20to%20capture%20nuanced%20aspects%20of%20response%20quality%20beyond%20mere%20correctness.%0ARLBFF%20extracts%20principles%20that%20can%20be%20answered%20in%20a%20binary%20fashion%20%28e.g.%0Aaccuracy%20of%20information%3A%20yes%2C%20or%20code%20readability%3A%20no%29%20from%20natural%20language%0Afeedback.%20Such%20principles%20can%20then%20be%20used%20to%20ground%20Reward%20Model%20training%20as%0Aan%20entailment%20task%20%28response%20satisfies%20or%20does%20not%20satisfy%20an%20arbitrary%0Aprinciple%29.%20We%20show%20that%20Reward%20Models%20trained%20in%20this%20manner%20can%20outperform%0ABradley-Terry%20models%20when%20matched%20for%20data%20and%20achieve%20top%20performance%20on%0ARM-Bench%20%2886.2%25%29%20and%20JudgeBench%20%2881.4%25%2C%20%231%20on%20leaderboard%20as%20of%20September%2024%2C%0A2025%29.%20Additionally%2C%20users%20can%20specify%20principles%20of%20interest%20at%20inference%20time%0Ato%20customize%20the%20focus%20of%20our%20reward%20models%2C%20in%20contrast%20to%20Bradley-Terry%0Amodels.%20Finally%2C%20we%20present%20a%20fully%20open%20source%20recipe%20%28including%20data%29%20to%0Aalign%20Qwen3-32B%20using%20RLBFF%20and%20our%20Reward%20Model%2C%20to%20match%20or%20exceed%20the%0Aperformance%20of%20o3-mini%20and%20DeepSeek%20R1%20on%20general%20alignment%20benchmarks%20of%0AMT-Bench%2C%20WildBench%2C%20and%20Arena%20Hard%20v2%20%28at%20%3C5%25%20of%20the%20inference%20cost%29.%20Models%3A%0Ahttps%3A//huggingface.co/collections/nvidia/reward-models-10-2025%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLBFF%253A%2520Binary%2520Flexible%2520Feedback%2520to%2520bridge%2520between%2520Human%2520Feedback%2520%2526%250A%2520%2520Verifiable%2520Rewards%26entry.906535625%3DZhilin%2520Wang%2520and%2520Jiaqi%2520Zeng%2520and%2520Olivier%2520Delalleau%2520and%2520Ellie%2520Evans%2520and%2520Daniel%2520Egert%2520and%2520Hoo-Chang%2520Shin%2520and%2520Felipe%2520Soares%2520and%2520Yi%2520Dong%2520and%2520Oleksii%2520Kuchaiev%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Reinforcement%2520Learning%250Awith%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520are%2520the%2520main%2520RL%2520paradigms%2520used%2520in%2520LLM%250Apost-training%252C%2520each%2520offering%2520distinct%2520advantages.%2520However%252C%2520RLHF%2520struggles%2520with%250Ainterpretability%2520and%2520reward%2520hacking%2520because%2520it%2520relies%2520on%2520human%2520judgments%2520that%250Ausually%2520lack%2520explicit%2520criteria%252C%2520whereas%2520RLVR%2520is%2520limited%2520in%2520scope%2520by%2520its%2520focus%250Aon%2520correctness-based%2520verifiers.%2520We%2520propose%2520Reinforcement%2520Learning%2520with%2520Binary%250AFlexible%2520Feedback%2520%2528RLBFF%2529%252C%2520which%2520combines%2520the%2520versatility%2520of%2520human-driven%250Apreferences%2520with%2520the%2520precision%2520of%2520rule-based%2520verification%252C%2520enabling%2520reward%250Amodels%2520to%2520capture%2520nuanced%2520aspects%2520of%2520response%2520quality%2520beyond%2520mere%2520correctness.%250ARLBFF%2520extracts%2520principles%2520that%2520can%2520be%2520answered%2520in%2520a%2520binary%2520fashion%2520%2528e.g.%250Aaccuracy%2520of%2520information%253A%2520yes%252C%2520or%2520code%2520readability%253A%2520no%2529%2520from%2520natural%2520language%250Afeedback.%2520Such%2520principles%2520can%2520then%2520be%2520used%2520to%2520ground%2520Reward%2520Model%2520training%2520as%250Aan%2520entailment%2520task%2520%2528response%2520satisfies%2520or%2520does%2520not%2520satisfy%2520an%2520arbitrary%250Aprinciple%2529.%2520We%2520show%2520that%2520Reward%2520Models%2520trained%2520in%2520this%2520manner%2520can%2520outperform%250ABradley-Terry%2520models%2520when%2520matched%2520for%2520data%2520and%2520achieve%2520top%2520performance%2520on%250ARM-Bench%2520%252886.2%2525%2529%2520and%2520JudgeBench%2520%252881.4%2525%252C%2520%25231%2520on%2520leaderboard%2520as%2520of%2520September%252024%252C%250A2025%2529.%2520Additionally%252C%2520users%2520can%2520specify%2520principles%2520of%2520interest%2520at%2520inference%2520time%250Ato%2520customize%2520the%2520focus%2520of%2520our%2520reward%2520models%252C%2520in%2520contrast%2520to%2520Bradley-Terry%250Amodels.%2520Finally%252C%2520we%2520present%2520a%2520fully%2520open%2520source%2520recipe%2520%2528including%2520data%2529%2520to%250Aalign%2520Qwen3-32B%2520using%2520RLBFF%2520and%2520our%2520Reward%2520Model%252C%2520to%2520match%2520or%2520exceed%2520the%250Aperformance%2520of%2520o3-mini%2520and%2520DeepSeek%2520R1%2520on%2520general%2520alignment%2520benchmarks%2520of%250AMT-Bench%252C%2520WildBench%252C%2520and%2520Arena%2520Hard%2520v2%2520%2528at%2520%253C5%2525%2520of%2520the%2520inference%2520cost%2529.%2520Models%253A%250Ahttps%253A//huggingface.co/collections/nvidia/reward-models-10-2025%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards&entry.906535625=Zhilin%20Wang%20and%20Jiaqi%20Zeng%20and%20Olivier%20Delalleau%20and%20Ellie%20Evans%20and%20Daniel%20Egert%20and%20Hoo-Chang%20Shin%20and%20Felipe%20Soares%20and%20Yi%20Dong%20and%20Oleksii%20Kuchaiev&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20and%20Reinforcement%20Learning%0Awith%20Verifiable%20Rewards%20%28RLVR%29%20are%20the%20main%20RL%20paradigms%20used%20in%20LLM%0Apost-training%2C%20each%20offering%20distinct%20advantages.%20However%2C%20RLHF%20struggles%20with%0Ainterpretability%20and%20reward%20hacking%20because%20it%20relies%20on%20human%20judgments%20that%0Ausually%20lack%20explicit%20criteria%2C%20whereas%20RLVR%20is%20limited%20in%20scope%20by%20its%20focus%0Aon%20correctness-based%20verifiers.%20We%20propose%20Reinforcement%20Learning%20with%20Binary%0AFlexible%20Feedback%20%28RLBFF%29%2C%20which%20combines%20the%20versatility%20of%20human-driven%0Apreferences%20with%20the%20precision%20of%20rule-based%20verification%2C%20enabling%20reward%0Amodels%20to%20capture%20nuanced%20aspects%20of%20response%20quality%20beyond%20mere%20correctness.%0ARLBFF%20extracts%20principles%20that%20can%20be%20answered%20in%20a%20binary%20fashion%20%28e.g.%0Aaccuracy%20of%20information%3A%20yes%2C%20or%20code%20readability%3A%20no%29%20from%20natural%20language%0Afeedback.%20Such%20principles%20can%20then%20be%20used%20to%20ground%20Reward%20Model%20training%20as%0Aan%20entailment%20task%20%28response%20satisfies%20or%20does%20not%20satisfy%20an%20arbitrary%0Aprinciple%29.%20We%20show%20that%20Reward%20Models%20trained%20in%20this%20manner%20can%20outperform%0ABradley-Terry%20models%20when%20matched%20for%20data%20and%20achieve%20top%20performance%20on%0ARM-Bench%20%2886.2%25%29%20and%20JudgeBench%20%2881.4%25%2C%20%231%20on%20leaderboard%20as%20of%20September%2024%2C%0A2025%29.%20Additionally%2C%20users%20can%20specify%20principles%20of%20interest%20at%20inference%20time%0Ato%20customize%20the%20focus%20of%20our%20reward%20models%2C%20in%20contrast%20to%20Bradley-Terry%0Amodels.%20Finally%2C%20we%20present%20a%20fully%20open%20source%20recipe%20%28including%20data%29%20to%0Aalign%20Qwen3-32B%20using%20RLBFF%20and%20our%20Reward%20Model%2C%20to%20match%20or%20exceed%20the%0Aperformance%20of%20o3-mini%20and%20DeepSeek%20R1%20on%20general%20alignment%20benchmarks%20of%0AMT-Bench%2C%20WildBench%2C%20and%20Arena%20Hard%20v2%20%28at%20%3C5%25%20of%20the%20inference%20cost%29.%20Models%3A%0Ahttps%3A//huggingface.co/collections/nvidia/reward-models-10-2025%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21319v2&entry.124074799=Read"},
{"title": "Guided Model Merging for Hybrid Data Learning: Leveraging Centralized\n  Data to Refine Decentralized Models", "author": "Junyi Zhu and Ruicong Yao and Taha Ceritli and Savas Ozkan and Matthew B. Blaschko and Eunchung Noh and Jeongwon Min and Cho Jung Min and Mete Ozay", "abstract": "  Current network training paradigms primarily focus on either centralized or\ndecentralized data regimes. However, in practice, data availability often\nexhibits a hybrid nature, where both regimes coexist. This hybrid setting\npresents new opportunities for model training, as the two regimes offer\ncomplementary trade-offs: decentralized data is abundant but subject to\nheterogeneity and communication constraints, while centralized data, though\nlimited in volume and potentially unrepresentative, enables better curation and\nhigh-throughput access. Despite its potential, effectively combining these\nparadigms remains challenging, and few frameworks are tailored to hybrid data\nregimes. To address this, we propose a novel framework that constructs a model\natlas from decentralized models and leverages centralized data to refine a\nglobal model within this structured space. The refined model is then used to\nreinitialize the decentralized models. Our method synergizes federated learning\n(to exploit decentralized data) and model merging (to utilize centralized\ndata), enabling effective training under hybrid data availability.\nTheoretically, we show that our approach achieves faster convergence than\nmethods relying solely on decentralized data, due to variance reduction in the\nmerging process. Extensive experiments demonstrate that our framework\nconsistently outperforms purely centralized, purely decentralized, and existing\nhybrid-adaptable methods. Notably, our method remains robust even when the\ncentralized and decentralized data domains differ or when decentralized data\ncontains noise, significantly broadening its applicability.\n", "link": "http://arxiv.org/abs/2503.20138v2", "date": "2025-10-30", "relevancy": 2.0493, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5327}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5081}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Model%20Merging%20for%20Hybrid%20Data%20Learning%3A%20Leveraging%20Centralized%0A%20%20Data%20to%20Refine%20Decentralized%20Models&body=Title%3A%20Guided%20Model%20Merging%20for%20Hybrid%20Data%20Learning%3A%20Leveraging%20Centralized%0A%20%20Data%20to%20Refine%20Decentralized%20Models%0AAuthor%3A%20Junyi%20Zhu%20and%20Ruicong%20Yao%20and%20Taha%20Ceritli%20and%20Savas%20Ozkan%20and%20Matthew%20B.%20Blaschko%20and%20Eunchung%20Noh%20and%20Jeongwon%20Min%20and%20Cho%20Jung%20Min%20and%20Mete%20Ozay%0AAbstract%3A%20%20%20Current%20network%20training%20paradigms%20primarily%20focus%20on%20either%20centralized%20or%0Adecentralized%20data%20regimes.%20However%2C%20in%20practice%2C%20data%20availability%20often%0Aexhibits%20a%20hybrid%20nature%2C%20where%20both%20regimes%20coexist.%20This%20hybrid%20setting%0Apresents%20new%20opportunities%20for%20model%20training%2C%20as%20the%20two%20regimes%20offer%0Acomplementary%20trade-offs%3A%20decentralized%20data%20is%20abundant%20but%20subject%20to%0Aheterogeneity%20and%20communication%20constraints%2C%20while%20centralized%20data%2C%20though%0Alimited%20in%20volume%20and%20potentially%20unrepresentative%2C%20enables%20better%20curation%20and%0Ahigh-throughput%20access.%20Despite%20its%20potential%2C%20effectively%20combining%20these%0Aparadigms%20remains%20challenging%2C%20and%20few%20frameworks%20are%20tailored%20to%20hybrid%20data%0Aregimes.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20that%20constructs%20a%20model%0Aatlas%20from%20decentralized%20models%20and%20leverages%20centralized%20data%20to%20refine%20a%0Aglobal%20model%20within%20this%20structured%20space.%20The%20refined%20model%20is%20then%20used%20to%0Areinitialize%20the%20decentralized%20models.%20Our%20method%20synergizes%20federated%20learning%0A%28to%20exploit%20decentralized%20data%29%20and%20model%20merging%20%28to%20utilize%20centralized%0Adata%29%2C%20enabling%20effective%20training%20under%20hybrid%20data%20availability.%0ATheoretically%2C%20we%20show%20that%20our%20approach%20achieves%20faster%20convergence%20than%0Amethods%20relying%20solely%20on%20decentralized%20data%2C%20due%20to%20variance%20reduction%20in%20the%0Amerging%20process.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20purely%20centralized%2C%20purely%20decentralized%2C%20and%20existing%0Ahybrid-adaptable%20methods.%20Notably%2C%20our%20method%20remains%20robust%20even%20when%20the%0Acentralized%20and%20decentralized%20data%20domains%20differ%20or%20when%20decentralized%20data%0Acontains%20noise%2C%20significantly%20broadening%20its%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20138v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Model%2520Merging%2520for%2520Hybrid%2520Data%2520Learning%253A%2520Leveraging%2520Centralized%250A%2520%2520Data%2520to%2520Refine%2520Decentralized%2520Models%26entry.906535625%3DJunyi%2520Zhu%2520and%2520Ruicong%2520Yao%2520and%2520Taha%2520Ceritli%2520and%2520Savas%2520Ozkan%2520and%2520Matthew%2520B.%2520Blaschko%2520and%2520Eunchung%2520Noh%2520and%2520Jeongwon%2520Min%2520and%2520Cho%2520Jung%2520Min%2520and%2520Mete%2520Ozay%26entry.1292438233%3D%2520%2520Current%2520network%2520training%2520paradigms%2520primarily%2520focus%2520on%2520either%2520centralized%2520or%250Adecentralized%2520data%2520regimes.%2520However%252C%2520in%2520practice%252C%2520data%2520availability%2520often%250Aexhibits%2520a%2520hybrid%2520nature%252C%2520where%2520both%2520regimes%2520coexist.%2520This%2520hybrid%2520setting%250Apresents%2520new%2520opportunities%2520for%2520model%2520training%252C%2520as%2520the%2520two%2520regimes%2520offer%250Acomplementary%2520trade-offs%253A%2520decentralized%2520data%2520is%2520abundant%2520but%2520subject%2520to%250Aheterogeneity%2520and%2520communication%2520constraints%252C%2520while%2520centralized%2520data%252C%2520though%250Alimited%2520in%2520volume%2520and%2520potentially%2520unrepresentative%252C%2520enables%2520better%2520curation%2520and%250Ahigh-throughput%2520access.%2520Despite%2520its%2520potential%252C%2520effectively%2520combining%2520these%250Aparadigms%2520remains%2520challenging%252C%2520and%2520few%2520frameworks%2520are%2520tailored%2520to%2520hybrid%2520data%250Aregimes.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520constructs%2520a%2520model%250Aatlas%2520from%2520decentralized%2520models%2520and%2520leverages%2520centralized%2520data%2520to%2520refine%2520a%250Aglobal%2520model%2520within%2520this%2520structured%2520space.%2520The%2520refined%2520model%2520is%2520then%2520used%2520to%250Areinitialize%2520the%2520decentralized%2520models.%2520Our%2520method%2520synergizes%2520federated%2520learning%250A%2528to%2520exploit%2520decentralized%2520data%2529%2520and%2520model%2520merging%2520%2528to%2520utilize%2520centralized%250Adata%2529%252C%2520enabling%2520effective%2520training%2520under%2520hybrid%2520data%2520availability.%250ATheoretically%252C%2520we%2520show%2520that%2520our%2520approach%2520achieves%2520faster%2520convergence%2520than%250Amethods%2520relying%2520solely%2520on%2520decentralized%2520data%252C%2520due%2520to%2520variance%2520reduction%2520in%2520the%250Amerging%2520process.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%250Aconsistently%2520outperforms%2520purely%2520centralized%252C%2520purely%2520decentralized%252C%2520and%2520existing%250Ahybrid-adaptable%2520methods.%2520Notably%252C%2520our%2520method%2520remains%2520robust%2520even%2520when%2520the%250Acentralized%2520and%2520decentralized%2520data%2520domains%2520differ%2520or%2520when%2520decentralized%2520data%250Acontains%2520noise%252C%2520significantly%2520broadening%2520its%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20138v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Model%20Merging%20for%20Hybrid%20Data%20Learning%3A%20Leveraging%20Centralized%0A%20%20Data%20to%20Refine%20Decentralized%20Models&entry.906535625=Junyi%20Zhu%20and%20Ruicong%20Yao%20and%20Taha%20Ceritli%20and%20Savas%20Ozkan%20and%20Matthew%20B.%20Blaschko%20and%20Eunchung%20Noh%20and%20Jeongwon%20Min%20and%20Cho%20Jung%20Min%20and%20Mete%20Ozay&entry.1292438233=%20%20Current%20network%20training%20paradigms%20primarily%20focus%20on%20either%20centralized%20or%0Adecentralized%20data%20regimes.%20However%2C%20in%20practice%2C%20data%20availability%20often%0Aexhibits%20a%20hybrid%20nature%2C%20where%20both%20regimes%20coexist.%20This%20hybrid%20setting%0Apresents%20new%20opportunities%20for%20model%20training%2C%20as%20the%20two%20regimes%20offer%0Acomplementary%20trade-offs%3A%20decentralized%20data%20is%20abundant%20but%20subject%20to%0Aheterogeneity%20and%20communication%20constraints%2C%20while%20centralized%20data%2C%20though%0Alimited%20in%20volume%20and%20potentially%20unrepresentative%2C%20enables%20better%20curation%20and%0Ahigh-throughput%20access.%20Despite%20its%20potential%2C%20effectively%20combining%20these%0Aparadigms%20remains%20challenging%2C%20and%20few%20frameworks%20are%20tailored%20to%20hybrid%20data%0Aregimes.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20that%20constructs%20a%20model%0Aatlas%20from%20decentralized%20models%20and%20leverages%20centralized%20data%20to%20refine%20a%0Aglobal%20model%20within%20this%20structured%20space.%20The%20refined%20model%20is%20then%20used%20to%0Areinitialize%20the%20decentralized%20models.%20Our%20method%20synergizes%20federated%20learning%0A%28to%20exploit%20decentralized%20data%29%20and%20model%20merging%20%28to%20utilize%20centralized%0Adata%29%2C%20enabling%20effective%20training%20under%20hybrid%20data%20availability.%0ATheoretically%2C%20we%20show%20that%20our%20approach%20achieves%20faster%20convergence%20than%0Amethods%20relying%20solely%20on%20decentralized%20data%2C%20due%20to%20variance%20reduction%20in%20the%0Amerging%20process.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20purely%20centralized%2C%20purely%20decentralized%2C%20and%20existing%0Ahybrid-adaptable%20methods.%20Notably%2C%20our%20method%20remains%20robust%20even%20when%20the%0Acentralized%20and%20decentralized%20data%20domains%20differ%20or%20when%20decentralized%20data%0Acontains%20noise%2C%20significantly%20broadening%20its%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20138v2&entry.124074799=Read"},
{"title": "SAMRI: Segment Anything Model for MRI", "author": "Zhao Wang and Wei Dai and Thuy Thanh Dao and Steffen Bollmann and Hongfu Sun and Craig Engstrom and Shekhar S. Chandra", "abstract": "  Accurate magnetic resonance imaging (MRI) segmentation is crucial for\nclinical decision-making, but remains labor-intensive when performed manually.\nConvolutional neural network (CNN)-based methods can be accurate and efficient,\nbut often generalize poorly to MRI's variable contrast, intensity\ninhomogeneity, and protocols. Although the transformer-based Segment Anything\nModel (SAM) has demonstrated remarkable generalizability in natural images,\nexisting adaptations often treat MRI as another imaging modality, overlooking\nthese modality-specific challenges. We present SAMRI, an MRI-specialized SAM\ntrained and validated on 1.1 million labeled MR slices spanning whole-body\norgans and pathologies. We demonstrate that SAM can be effectively adapted to\nMRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing\ntraining time by 94% and trainable parameters by 96% versus full-model\nretraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice\nof 0.87, delivering state-of-the-art accuracy across anatomical regions and\nrobust generalization on unseen structures, particularly small and clinically\nimportant structures.\n", "link": "http://arxiv.org/abs/2510.26635v1", "date": "2025-10-30", "relevancy": 2.0315, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5035}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI&body=Title%3A%20SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI%0AAuthor%3A%20Zhao%20Wang%20and%20Wei%20Dai%20and%20Thuy%20Thanh%20Dao%20and%20Steffen%20Bollmann%20and%20Hongfu%20Sun%20and%20Craig%20Engstrom%20and%20Shekhar%20S.%20Chandra%0AAbstract%3A%20%20%20Accurate%20magnetic%20resonance%20imaging%20%28MRI%29%20segmentation%20is%20crucial%20for%0Aclinical%20decision-making%2C%20but%20remains%20labor-intensive%20when%20performed%20manually.%0AConvolutional%20neural%20network%20%28CNN%29-based%20methods%20can%20be%20accurate%20and%20efficient%2C%0Abut%20often%20generalize%20poorly%20to%20MRI%27s%20variable%20contrast%2C%20intensity%0Ainhomogeneity%2C%20and%20protocols.%20Although%20the%20transformer-based%20Segment%20Anything%0AModel%20%28SAM%29%20has%20demonstrated%20remarkable%20generalizability%20in%20natural%20images%2C%0Aexisting%20adaptations%20often%20treat%20MRI%20as%20another%20imaging%20modality%2C%20overlooking%0Athese%20modality-specific%20challenges.%20We%20present%20SAMRI%2C%20an%20MRI-specialized%20SAM%0Atrained%20and%20validated%20on%201.1%20million%20labeled%20MR%20slices%20spanning%20whole-body%0Aorgans%20and%20pathologies.%20We%20demonstrate%20that%20SAM%20can%20be%20effectively%20adapted%20to%0AMRI%20by%20simply%20fine-tuning%20its%20mask%20decoder%20using%20a%20two-stage%20strategy%2C%20reducing%0Atraining%20time%20by%2094%25%20and%20trainable%20parameters%20by%2096%25%20versus%20full-model%0Aretraining.%20Across%20diverse%20MRI%20segmentation%20tasks%2C%20SAMRI%20achieves%20a%20mean%20Dice%0Aof%200.87%2C%20delivering%20state-of-the-art%20accuracy%20across%20anatomical%20regions%20and%0Arobust%20generalization%20on%20unseen%20structures%2C%20particularly%20small%20and%20clinically%0Aimportant%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMRI%253A%2520Segment%2520Anything%2520Model%2520for%2520MRI%26entry.906535625%3DZhao%2520Wang%2520and%2520Wei%2520Dai%2520and%2520Thuy%2520Thanh%2520Dao%2520and%2520Steffen%2520Bollmann%2520and%2520Hongfu%2520Sun%2520and%2520Craig%2520Engstrom%2520and%2520Shekhar%2520S.%2520Chandra%26entry.1292438233%3D%2520%2520Accurate%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520segmentation%2520is%2520crucial%2520for%250Aclinical%2520decision-making%252C%2520but%2520remains%2520labor-intensive%2520when%2520performed%2520manually.%250AConvolutional%2520neural%2520network%2520%2528CNN%2529-based%2520methods%2520can%2520be%2520accurate%2520and%2520efficient%252C%250Abut%2520often%2520generalize%2520poorly%2520to%2520MRI%2527s%2520variable%2520contrast%252C%2520intensity%250Ainhomogeneity%252C%2520and%2520protocols.%2520Although%2520the%2520transformer-based%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%2520has%2520demonstrated%2520remarkable%2520generalizability%2520in%2520natural%2520images%252C%250Aexisting%2520adaptations%2520often%2520treat%2520MRI%2520as%2520another%2520imaging%2520modality%252C%2520overlooking%250Athese%2520modality-specific%2520challenges.%2520We%2520present%2520SAMRI%252C%2520an%2520MRI-specialized%2520SAM%250Atrained%2520and%2520validated%2520on%25201.1%2520million%2520labeled%2520MR%2520slices%2520spanning%2520whole-body%250Aorgans%2520and%2520pathologies.%2520We%2520demonstrate%2520that%2520SAM%2520can%2520be%2520effectively%2520adapted%2520to%250AMRI%2520by%2520simply%2520fine-tuning%2520its%2520mask%2520decoder%2520using%2520a%2520two-stage%2520strategy%252C%2520reducing%250Atraining%2520time%2520by%252094%2525%2520and%2520trainable%2520parameters%2520by%252096%2525%2520versus%2520full-model%250Aretraining.%2520Across%2520diverse%2520MRI%2520segmentation%2520tasks%252C%2520SAMRI%2520achieves%2520a%2520mean%2520Dice%250Aof%25200.87%252C%2520delivering%2520state-of-the-art%2520accuracy%2520across%2520anatomical%2520regions%2520and%250Arobust%2520generalization%2520on%2520unseen%2520structures%252C%2520particularly%2520small%2520and%2520clinically%250Aimportant%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI&entry.906535625=Zhao%20Wang%20and%20Wei%20Dai%20and%20Thuy%20Thanh%20Dao%20and%20Steffen%20Bollmann%20and%20Hongfu%20Sun%20and%20Craig%20Engstrom%20and%20Shekhar%20S.%20Chandra&entry.1292438233=%20%20Accurate%20magnetic%20resonance%20imaging%20%28MRI%29%20segmentation%20is%20crucial%20for%0Aclinical%20decision-making%2C%20but%20remains%20labor-intensive%20when%20performed%20manually.%0AConvolutional%20neural%20network%20%28CNN%29-based%20methods%20can%20be%20accurate%20and%20efficient%2C%0Abut%20often%20generalize%20poorly%20to%20MRI%27s%20variable%20contrast%2C%20intensity%0Ainhomogeneity%2C%20and%20protocols.%20Although%20the%20transformer-based%20Segment%20Anything%0AModel%20%28SAM%29%20has%20demonstrated%20remarkable%20generalizability%20in%20natural%20images%2C%0Aexisting%20adaptations%20often%20treat%20MRI%20as%20another%20imaging%20modality%2C%20overlooking%0Athese%20modality-specific%20challenges.%20We%20present%20SAMRI%2C%20an%20MRI-specialized%20SAM%0Atrained%20and%20validated%20on%201.1%20million%20labeled%20MR%20slices%20spanning%20whole-body%0Aorgans%20and%20pathologies.%20We%20demonstrate%20that%20SAM%20can%20be%20effectively%20adapted%20to%0AMRI%20by%20simply%20fine-tuning%20its%20mask%20decoder%20using%20a%20two-stage%20strategy%2C%20reducing%0Atraining%20time%20by%2094%25%20and%20trainable%20parameters%20by%2096%25%20versus%20full-model%0Aretraining.%20Across%20diverse%20MRI%20segmentation%20tasks%2C%20SAMRI%20achieves%20a%20mean%20Dice%0Aof%200.87%2C%20delivering%20state-of-the-art%20accuracy%20across%20anatomical%20regions%20and%0Arobust%20generalization%20on%20unseen%20structures%2C%20particularly%20small%20and%20clinically%0Aimportant%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26635v1&entry.124074799=Read"},
{"title": "The End of Manual Decoding: Towards Truly End-to-End Language Models", "author": "Zhichao Wang and Dongyang Ma and Xinting Huang and Deng Cai and Tian Lan and Jiahao Xu and Haitao Mi and Xiaoying Tang and Yan Wang", "abstract": "  The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.\n", "link": "http://arxiv.org/abs/2510.26697v1", "date": "2025-10-30", "relevancy": 1.9969, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models&body=Title%3A%20The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models%0AAuthor%3A%20Zhichao%20Wang%20and%20Dongyang%20Ma%20and%20Xinting%20Huang%20and%20Deng%20Cai%20and%20Tian%20Lan%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Xiaoying%20Tang%20and%20Yan%20Wang%0AAbstract%3A%20%20%20The%20%22end-to-end%22%20label%20for%20LLMs%20is%20a%20misnomer.%20In%20practice%2C%20they%20depend%20on%20a%0Anon-differentiable%20decoding%20process%20that%20requires%20laborious%2C%20hand-tuning%20of%0Ahyperparameters%20like%20temperature%20and%20top-p.%20This%20paper%20introduces%20AutoDeco%2C%20a%0Anovel%20architecture%20that%20enables%20truly%20%22end-to-end%22%20generation%20by%20learning%20to%0Acontrol%20its%20own%20decoding%20strategy.%20We%20augment%20the%20standard%20transformer%20with%0Alightweight%20heads%20that%2C%20at%20each%20step%2C%20dynamically%20predict%20context-specific%0Atemperature%20and%20top-p%20values%20alongside%20the%20next-token%20logits.%20This%20approach%0Atransforms%20decoding%20into%20a%20parametric%2C%20token-level%20process%2C%20allowing%20the%20model%0Ato%20self-regulate%20its%20sampling%20strategy%20within%20a%20single%20forward%20pass.%0A%20%20Through%20extensive%20experiments%20on%20eight%20benchmarks%2C%20we%20demonstrate%20that%0AAutoDeco%20not%20only%20significantly%20outperforms%20default%20decoding%20strategies%20but%0Aalso%20achieves%20performance%20comparable%20to%20an%20oracle-tuned%20baseline%20derived%20from%0A%22hacking%20the%20test%20set%22-a%20practical%20upper%20bound%20for%20any%20static%20method.%0ACrucially%2C%20we%20uncover%20an%20emergent%20capability%20for%20instruction-based%20decoding%0Acontrol%3A%20the%20model%20learns%20to%20interpret%20natural%20language%20commands%20%28e.g.%2C%0A%22generate%20with%20low%20randomness%22%29%20and%20adjusts%20its%20predicted%20temperature%20and%20top-p%0Aon%20a%20token-by-token%20basis%2C%20opening%20a%20new%20paradigm%20for%20steerable%20and%20interactive%0ALLM%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520End%2520of%2520Manual%2520Decoding%253A%2520Towards%2520Truly%2520End-to-End%2520Language%2520Models%26entry.906535625%3DZhichao%2520Wang%2520and%2520Dongyang%2520Ma%2520and%2520Xinting%2520Huang%2520and%2520Deng%2520Cai%2520and%2520Tian%2520Lan%2520and%2520Jiahao%2520Xu%2520and%2520Haitao%2520Mi%2520and%2520Xiaoying%2520Tang%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520The%2520%2522end-to-end%2522%2520label%2520for%2520LLMs%2520is%2520a%2520misnomer.%2520In%2520practice%252C%2520they%2520depend%2520on%2520a%250Anon-differentiable%2520decoding%2520process%2520that%2520requires%2520laborious%252C%2520hand-tuning%2520of%250Ahyperparameters%2520like%2520temperature%2520and%2520top-p.%2520This%2520paper%2520introduces%2520AutoDeco%252C%2520a%250Anovel%2520architecture%2520that%2520enables%2520truly%2520%2522end-to-end%2522%2520generation%2520by%2520learning%2520to%250Acontrol%2520its%2520own%2520decoding%2520strategy.%2520We%2520augment%2520the%2520standard%2520transformer%2520with%250Alightweight%2520heads%2520that%252C%2520at%2520each%2520step%252C%2520dynamically%2520predict%2520context-specific%250Atemperature%2520and%2520top-p%2520values%2520alongside%2520the%2520next-token%2520logits.%2520This%2520approach%250Atransforms%2520decoding%2520into%2520a%2520parametric%252C%2520token-level%2520process%252C%2520allowing%2520the%2520model%250Ato%2520self-regulate%2520its%2520sampling%2520strategy%2520within%2520a%2520single%2520forward%2520pass.%250A%2520%2520Through%2520extensive%2520experiments%2520on%2520eight%2520benchmarks%252C%2520we%2520demonstrate%2520that%250AAutoDeco%2520not%2520only%2520significantly%2520outperforms%2520default%2520decoding%2520strategies%2520but%250Aalso%2520achieves%2520performance%2520comparable%2520to%2520an%2520oracle-tuned%2520baseline%2520derived%2520from%250A%2522hacking%2520the%2520test%2520set%2522-a%2520practical%2520upper%2520bound%2520for%2520any%2520static%2520method.%250ACrucially%252C%2520we%2520uncover%2520an%2520emergent%2520capability%2520for%2520instruction-based%2520decoding%250Acontrol%253A%2520the%2520model%2520learns%2520to%2520interpret%2520natural%2520language%2520commands%2520%2528e.g.%252C%250A%2522generate%2520with%2520low%2520randomness%2522%2529%2520and%2520adjusts%2520its%2520predicted%2520temperature%2520and%2520top-p%250Aon%2520a%2520token-by-token%2520basis%252C%2520opening%2520a%2520new%2520paradigm%2520for%2520steerable%2520and%2520interactive%250ALLM%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models&entry.906535625=Zhichao%20Wang%20and%20Dongyang%20Ma%20and%20Xinting%20Huang%20and%20Deng%20Cai%20and%20Tian%20Lan%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Xiaoying%20Tang%20and%20Yan%20Wang&entry.1292438233=%20%20The%20%22end-to-end%22%20label%20for%20LLMs%20is%20a%20misnomer.%20In%20practice%2C%20they%20depend%20on%20a%0Anon-differentiable%20decoding%20process%20that%20requires%20laborious%2C%20hand-tuning%20of%0Ahyperparameters%20like%20temperature%20and%20top-p.%20This%20paper%20introduces%20AutoDeco%2C%20a%0Anovel%20architecture%20that%20enables%20truly%20%22end-to-end%22%20generation%20by%20learning%20to%0Acontrol%20its%20own%20decoding%20strategy.%20We%20augment%20the%20standard%20transformer%20with%0Alightweight%20heads%20that%2C%20at%20each%20step%2C%20dynamically%20predict%20context-specific%0Atemperature%20and%20top-p%20values%20alongside%20the%20next-token%20logits.%20This%20approach%0Atransforms%20decoding%20into%20a%20parametric%2C%20token-level%20process%2C%20allowing%20the%20model%0Ato%20self-regulate%20its%20sampling%20strategy%20within%20a%20single%20forward%20pass.%0A%20%20Through%20extensive%20experiments%20on%20eight%20benchmarks%2C%20we%20demonstrate%20that%0AAutoDeco%20not%20only%20significantly%20outperforms%20default%20decoding%20strategies%20but%0Aalso%20achieves%20performance%20comparable%20to%20an%20oracle-tuned%20baseline%20derived%20from%0A%22hacking%20the%20test%20set%22-a%20practical%20upper%20bound%20for%20any%20static%20method.%0ACrucially%2C%20we%20uncover%20an%20emergent%20capability%20for%20instruction-based%20decoding%0Acontrol%3A%20the%20model%20learns%20to%20interpret%20natural%20language%20commands%20%28e.g.%2C%0A%22generate%20with%20low%20randomness%22%29%20and%20adjusts%20its%20predicted%20temperature%20and%20top-p%0Aon%20a%20token-by-token%20basis%2C%20opening%20a%20new%20paradigm%20for%20steerable%20and%20interactive%0ALLM%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26697v1&entry.124074799=Read"},
{"title": "PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus", "author": "Bingcong Huo and Zhiming Wang", "abstract": "  To address the challenges in UAV object detection, such as complex\nbackgrounds, severe occlusion, dense small objects, and varying lighting\nconditions,this paper proposes PT-DETR based on RT-DETR, a novel detection\nalgorithm specifically designed for small objects in UAV imagery. In the\nbackbone network, we introduce the Partially-Aware Detail Focus (PADF) Module\nto enhance feature extraction for small objects. Additionally,we design the\nMedian-Frequency Feature Fusion (MFFF) module,which effectively improves the\nmodel's ability to capture small-object details and contextual information.\nFurthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box\nmatching capability and increase its sensitivity to small-object features,\nthereby further enhancing detection accuracy and robustness. Compared with\nRT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the\nVisDrone2019 dataset with lower computational complexity and fewer parameters,\ndemonstrating its robustness and feasibility for small-object detection tasks.\n", "link": "http://arxiv.org/abs/2510.26630v1", "date": "2025-10-30", "relevancy": 1.9899, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5021}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4942}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PT-DETR%3A%20Small%20Target%20Detection%20Based%20on%20Partially-Aware%20Detail%20Focus&body=Title%3A%20PT-DETR%3A%20Small%20Target%20Detection%20Based%20on%20Partially-Aware%20Detail%20Focus%0AAuthor%3A%20Bingcong%20Huo%20and%20Zhiming%20Wang%0AAbstract%3A%20%20%20To%20address%20the%20challenges%20in%20UAV%20object%20detection%2C%20such%20as%20complex%0Abackgrounds%2C%20severe%20occlusion%2C%20dense%20small%20objects%2C%20and%20varying%20lighting%0Aconditions%2Cthis%20paper%20proposes%20PT-DETR%20based%20on%20RT-DETR%2C%20a%20novel%20detection%0Aalgorithm%20specifically%20designed%20for%20small%20objects%20in%20UAV%20imagery.%20In%20the%0Abackbone%20network%2C%20we%20introduce%20the%20Partially-Aware%20Detail%20Focus%20%28PADF%29%20Module%0Ato%20enhance%20feature%20extraction%20for%20small%20objects.%20Additionally%2Cwe%20design%20the%0AMedian-Frequency%20Feature%20Fusion%20%28MFFF%29%20module%2Cwhich%20effectively%20improves%20the%0Amodel%27s%20ability%20to%20capture%20small-object%20details%20and%20contextual%20information.%0AFurthermore%2Cwe%20incorporate%20Focaler-SIoU%20to%20strengthen%20the%20model%27s%20bounding%20box%0Amatching%20capability%20and%20increase%20its%20sensitivity%20to%20small-object%20features%2C%0Athereby%20further%20enhancing%20detection%20accuracy%20and%20robustness.%20Compared%20with%0ART-DETR%2C%20our%20PT-DETR%20achieves%20mAP%20improvements%20of%201.6%25%20and%201.7%25%20on%20the%0AVisDrone2019%20dataset%20with%20lower%20computational%20complexity%20and%20fewer%20parameters%2C%0Ademonstrating%20its%20robustness%20and%20feasibility%20for%20small-object%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPT-DETR%253A%2520Small%2520Target%2520Detection%2520Based%2520on%2520Partially-Aware%2520Detail%2520Focus%26entry.906535625%3DBingcong%2520Huo%2520and%2520Zhiming%2520Wang%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520challenges%2520in%2520UAV%2520object%2520detection%252C%2520such%2520as%2520complex%250Abackgrounds%252C%2520severe%2520occlusion%252C%2520dense%2520small%2520objects%252C%2520and%2520varying%2520lighting%250Aconditions%252Cthis%2520paper%2520proposes%2520PT-DETR%2520based%2520on%2520RT-DETR%252C%2520a%2520novel%2520detection%250Aalgorithm%2520specifically%2520designed%2520for%2520small%2520objects%2520in%2520UAV%2520imagery.%2520In%2520the%250Abackbone%2520network%252C%2520we%2520introduce%2520the%2520Partially-Aware%2520Detail%2520Focus%2520%2528PADF%2529%2520Module%250Ato%2520enhance%2520feature%2520extraction%2520for%2520small%2520objects.%2520Additionally%252Cwe%2520design%2520the%250AMedian-Frequency%2520Feature%2520Fusion%2520%2528MFFF%2529%2520module%252Cwhich%2520effectively%2520improves%2520the%250Amodel%2527s%2520ability%2520to%2520capture%2520small-object%2520details%2520and%2520contextual%2520information.%250AFurthermore%252Cwe%2520incorporate%2520Focaler-SIoU%2520to%2520strengthen%2520the%2520model%2527s%2520bounding%2520box%250Amatching%2520capability%2520and%2520increase%2520its%2520sensitivity%2520to%2520small-object%2520features%252C%250Athereby%2520further%2520enhancing%2520detection%2520accuracy%2520and%2520robustness.%2520Compared%2520with%250ART-DETR%252C%2520our%2520PT-DETR%2520achieves%2520mAP%2520improvements%2520of%25201.6%2525%2520and%25201.7%2525%2520on%2520the%250AVisDrone2019%2520dataset%2520with%2520lower%2520computational%2520complexity%2520and%2520fewer%2520parameters%252C%250Ademonstrating%2520its%2520robustness%2520and%2520feasibility%2520for%2520small-object%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PT-DETR%3A%20Small%20Target%20Detection%20Based%20on%20Partially-Aware%20Detail%20Focus&entry.906535625=Bingcong%20Huo%20and%20Zhiming%20Wang&entry.1292438233=%20%20To%20address%20the%20challenges%20in%20UAV%20object%20detection%2C%20such%20as%20complex%0Abackgrounds%2C%20severe%20occlusion%2C%20dense%20small%20objects%2C%20and%20varying%20lighting%0Aconditions%2Cthis%20paper%20proposes%20PT-DETR%20based%20on%20RT-DETR%2C%20a%20novel%20detection%0Aalgorithm%20specifically%20designed%20for%20small%20objects%20in%20UAV%20imagery.%20In%20the%0Abackbone%20network%2C%20we%20introduce%20the%20Partially-Aware%20Detail%20Focus%20%28PADF%29%20Module%0Ato%20enhance%20feature%20extraction%20for%20small%20objects.%20Additionally%2Cwe%20design%20the%0AMedian-Frequency%20Feature%20Fusion%20%28MFFF%29%20module%2Cwhich%20effectively%20improves%20the%0Amodel%27s%20ability%20to%20capture%20small-object%20details%20and%20contextual%20information.%0AFurthermore%2Cwe%20incorporate%20Focaler-SIoU%20to%20strengthen%20the%20model%27s%20bounding%20box%0Amatching%20capability%20and%20increase%20its%20sensitivity%20to%20small-object%20features%2C%0Athereby%20further%20enhancing%20detection%20accuracy%20and%20robustness.%20Compared%20with%0ART-DETR%2C%20our%20PT-DETR%20achieves%20mAP%20improvements%20of%201.6%25%20and%201.7%25%20on%20the%0AVisDrone2019%20dataset%20with%20lower%20computational%20complexity%20and%20fewer%20parameters%2C%0Ademonstrating%20its%20robustness%20and%20feasibility%20for%20small-object%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26630v1&entry.124074799=Read"},
{"title": "Tight Differentially Private PCA via Matrix Coherence", "author": "Tommaso d'Orsi and Gleb Novikov", "abstract": "  We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.\n", "link": "http://arxiv.org/abs/2510.26679v1", "date": "2025-10-30", "relevancy": 1.9848, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4196}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3934}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20Differentially%20Private%20PCA%20via%20Matrix%20Coherence&body=Title%3A%20Tight%20Differentially%20Private%20PCA%20via%20Matrix%20Coherence%0AAuthor%3A%20Tommaso%20d%27Orsi%20and%20Gleb%20Novikov%0AAbstract%3A%20%20%20We%20revisit%20the%20task%20of%20computing%20the%20span%20of%20the%20top%20%24r%24%20singular%20vectors%0A%24u_1%2C%20%5Cldots%2C%20u_r%24%20of%20a%20matrix%20under%20differential%20privacy.%20We%20show%20that%20a%0Asimple%20and%20efficient%20algorithm%20--%20based%20on%20singular%20value%20decomposition%20and%0Astandard%20perturbation%20mechanisms%20--%20returns%20a%20private%20rank-%24r%24%20approximation%0Awhose%20error%20depends%20only%20on%20the%20%5Cemph%7Brank-%24r%24%20coherence%7D%20of%20%24u_1%2C%20%5Cldots%2C%20u_r%24%0Aand%20the%20spectral%20gap%20%24%5Csigma_r%20-%20%5Csigma_%7Br%2B1%7D%24.%20This%20resolves%20a%20question%20posed%0Aby%20Hardt%20and%20Roth~%5Ccite%7Bhardt2013beyond%7D.%20Our%20estimator%20outperforms%20the%20state%0Aof%20the%20art%20--%20significantly%20so%20in%20some%20regimes.%20In%20particular%2C%20we%20show%20that%20in%0Athe%20dense%20setting%2C%20it%20achieves%20the%20same%20guarantees%20for%20single-spike%20PCA%20in%20the%0AWishart%20model%20as%20those%20attained%20by%20optimal%20non-private%20algorithms%2C%20whereas%0Aprior%20private%20algorithms%20failed%20to%20do%20so.%0A%20%20In%20addition%2C%20we%20prove%20that%20%28rank-%24r%24%29%20coherence%20does%20not%20increase%20under%0AGaussian%20perturbations.%20This%20implies%20that%20any%20estimator%20based%20on%20the%20Gaussian%0Amechanism%20--%20including%20ours%20--%20preserves%20the%20coherence%20of%20the%20input.%20We%0Aconjecture%20that%20similar%20behavior%20holds%20for%20other%20structured%20models%2C%20including%0Aplanted%20problems%20in%20graphs.%0A%20%20We%20also%20explore%20applications%20of%20coherence%20to%20graph%20problems.%20In%20particular%2C%0Awe%20present%20a%20differentially%20private%20algorithm%20for%20Max-Cut%20and%20other%20constraint%0Asatisfaction%20problems%20under%20low%20coherence%20assumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520Differentially%2520Private%2520PCA%2520via%2520Matrix%2520Coherence%26entry.906535625%3DTommaso%2520d%2527Orsi%2520and%2520Gleb%2520Novikov%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520task%2520of%2520computing%2520the%2520span%2520of%2520the%2520top%2520%2524r%2524%2520singular%2520vectors%250A%2524u_1%252C%2520%255Cldots%252C%2520u_r%2524%2520of%2520a%2520matrix%2520under%2520differential%2520privacy.%2520We%2520show%2520that%2520a%250Asimple%2520and%2520efficient%2520algorithm%2520--%2520based%2520on%2520singular%2520value%2520decomposition%2520and%250Astandard%2520perturbation%2520mechanisms%2520--%2520returns%2520a%2520private%2520rank-%2524r%2524%2520approximation%250Awhose%2520error%2520depends%2520only%2520on%2520the%2520%255Cemph%257Brank-%2524r%2524%2520coherence%257D%2520of%2520%2524u_1%252C%2520%255Cldots%252C%2520u_r%2524%250Aand%2520the%2520spectral%2520gap%2520%2524%255Csigma_r%2520-%2520%255Csigma_%257Br%252B1%257D%2524.%2520This%2520resolves%2520a%2520question%2520posed%250Aby%2520Hardt%2520and%2520Roth~%255Ccite%257Bhardt2013beyond%257D.%2520Our%2520estimator%2520outperforms%2520the%2520state%250Aof%2520the%2520art%2520--%2520significantly%2520so%2520in%2520some%2520regimes.%2520In%2520particular%252C%2520we%2520show%2520that%2520in%250Athe%2520dense%2520setting%252C%2520it%2520achieves%2520the%2520same%2520guarantees%2520for%2520single-spike%2520PCA%2520in%2520the%250AWishart%2520model%2520as%2520those%2520attained%2520by%2520optimal%2520non-private%2520algorithms%252C%2520whereas%250Aprior%2520private%2520algorithms%2520failed%2520to%2520do%2520so.%250A%2520%2520In%2520addition%252C%2520we%2520prove%2520that%2520%2528rank-%2524r%2524%2529%2520coherence%2520does%2520not%2520increase%2520under%250AGaussian%2520perturbations.%2520This%2520implies%2520that%2520any%2520estimator%2520based%2520on%2520the%2520Gaussian%250Amechanism%2520--%2520including%2520ours%2520--%2520preserves%2520the%2520coherence%2520of%2520the%2520input.%2520We%250Aconjecture%2520that%2520similar%2520behavior%2520holds%2520for%2520other%2520structured%2520models%252C%2520including%250Aplanted%2520problems%2520in%2520graphs.%250A%2520%2520We%2520also%2520explore%2520applications%2520of%2520coherence%2520to%2520graph%2520problems.%2520In%2520particular%252C%250Awe%2520present%2520a%2520differentially%2520private%2520algorithm%2520for%2520Max-Cut%2520and%2520other%2520constraint%250Asatisfaction%2520problems%2520under%2520low%2520coherence%2520assumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20Differentially%20Private%20PCA%20via%20Matrix%20Coherence&entry.906535625=Tommaso%20d%27Orsi%20and%20Gleb%20Novikov&entry.1292438233=%20%20We%20revisit%20the%20task%20of%20computing%20the%20span%20of%20the%20top%20%24r%24%20singular%20vectors%0A%24u_1%2C%20%5Cldots%2C%20u_r%24%20of%20a%20matrix%20under%20differential%20privacy.%20We%20show%20that%20a%0Asimple%20and%20efficient%20algorithm%20--%20based%20on%20singular%20value%20decomposition%20and%0Astandard%20perturbation%20mechanisms%20--%20returns%20a%20private%20rank-%24r%24%20approximation%0Awhose%20error%20depends%20only%20on%20the%20%5Cemph%7Brank-%24r%24%20coherence%7D%20of%20%24u_1%2C%20%5Cldots%2C%20u_r%24%0Aand%20the%20spectral%20gap%20%24%5Csigma_r%20-%20%5Csigma_%7Br%2B1%7D%24.%20This%20resolves%20a%20question%20posed%0Aby%20Hardt%20and%20Roth~%5Ccite%7Bhardt2013beyond%7D.%20Our%20estimator%20outperforms%20the%20state%0Aof%20the%20art%20--%20significantly%20so%20in%20some%20regimes.%20In%20particular%2C%20we%20show%20that%20in%0Athe%20dense%20setting%2C%20it%20achieves%20the%20same%20guarantees%20for%20single-spike%20PCA%20in%20the%0AWishart%20model%20as%20those%20attained%20by%20optimal%20non-private%20algorithms%2C%20whereas%0Aprior%20private%20algorithms%20failed%20to%20do%20so.%0A%20%20In%20addition%2C%20we%20prove%20that%20%28rank-%24r%24%29%20coherence%20does%20not%20increase%20under%0AGaussian%20perturbations.%20This%20implies%20that%20any%20estimator%20based%20on%20the%20Gaussian%0Amechanism%20--%20including%20ours%20--%20preserves%20the%20coherence%20of%20the%20input.%20We%0Aconjecture%20that%20similar%20behavior%20holds%20for%20other%20structured%20models%2C%20including%0Aplanted%20problems%20in%20graphs.%0A%20%20We%20also%20explore%20applications%20of%20coherence%20to%20graph%20problems.%20In%20particular%2C%0Awe%20present%20a%20differentially%20private%20algorithm%20for%20Max-Cut%20and%20other%20constraint%0Asatisfaction%20problems%20under%20low%20coherence%20assumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26679v1&entry.124074799=Read"},
{"title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting", "author": "Dongyue Lu and Ao Liang and Tianxin Huang and Xiao Fu and Yuyang Zhao and Baorui Ma and Liang Pan and Wei Yin and Lingdong Kong and Wei Tsang Ooi and Ziwei Liu", "abstract": "  Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.\n", "link": "http://arxiv.org/abs/2510.26796v1", "date": "2025-10-30", "relevancy": 1.981, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6667}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6561}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEE4D%3A%20Pose-Free%204D%20Generation%20via%20Auto-Regressive%20Video%20Inpainting&body=Title%3A%20SEE4D%3A%20Pose-Free%204D%20Generation%20via%20Auto-Regressive%20Video%20Inpainting%0AAuthor%3A%20Dongyue%20Lu%20and%20Ao%20Liang%20and%20Tianxin%20Huang%20and%20Xiao%20Fu%20and%20Yuyang%20Zhao%20and%20Baorui%20Ma%20and%20Liang%20Pan%20and%20Wei%20Yin%20and%20Lingdong%20Kong%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Immersive%20applications%20call%20for%20synthesizing%20spatiotemporal%204D%20content%20from%0Acasual%20videos%20without%20costly%203D%20supervision.%20Existing%20video-to-4D%20methods%0Atypically%20rely%20on%20manually%20annotated%20camera%20poses%2C%20which%20are%20labor-intensive%0Aand%20brittle%20for%20in-the-wild%20footage.%20Recent%20warp-then-inpaint%20approaches%0Amitigate%20the%20need%20for%20pose%20labels%20by%20warping%20input%20frames%20along%20a%20novel%20camera%0Atrajectory%20and%20using%20an%20inpainting%20model%20to%20fill%20missing%20regions%2C%20thereby%0Adepicting%20the%204D%20scene%20from%20diverse%20viewpoints.%20However%2C%20this%0Atrajectory-to-trajectory%20formulation%20often%20entangles%20camera%20motion%20with%20scene%0Adynamics%20and%20complicates%20both%20modeling%20and%20inference.%20We%20introduce%20SEE4D%2C%20a%0Apose-free%2C%20trajectory-to-camera%20framework%20that%20replaces%20explicit%20trajectory%0Aprediction%20with%20rendering%20to%20a%20bank%20of%20fixed%20virtual%20cameras%2C%20thereby%0Aseparating%20camera%20control%20from%20scene%20modeling.%20A%20view-conditional%20video%0Ainpainting%20model%20is%20trained%20to%20learn%20a%20robust%20geometry%20prior%20by%20denoising%0Arealistically%20synthesized%20warped%20images%20and%20to%20inpaint%20occluded%20or%20missing%0Aregions%20across%20virtual%20viewpoints%2C%20eliminating%20the%20need%20for%20explicit%203D%0Aannotations.%20Building%20on%20this%20inpainting%20core%2C%20we%20design%20a%20spatiotemporal%0Aautoregressive%20inference%20pipeline%20that%20traverses%20virtual-camera%20splines%20and%0Aextends%20videos%20with%20overlapping%20windows%2C%20enabling%20coherent%20generation%20at%0Abounded%20per-step%20complexity.%20We%20validate%20See4D%20on%20cross-view%20video%20generation%0Aand%20sparse%20reconstruction%20benchmarks.%20Across%20quantitative%20metrics%20and%0Aqualitative%20assessments%2C%20our%20method%20achieves%20superior%20generalization%20and%0Aimproved%20performance%20relative%20to%20pose-%20or%20trajectory-conditioned%20baselines%2C%0Aadvancing%20practical%204D%20world%20modeling%20from%20casual%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEE4D%253A%2520Pose-Free%25204D%2520Generation%2520via%2520Auto-Regressive%2520Video%2520Inpainting%26entry.906535625%3DDongyue%2520Lu%2520and%2520Ao%2520Liang%2520and%2520Tianxin%2520Huang%2520and%2520Xiao%2520Fu%2520and%2520Yuyang%2520Zhao%2520and%2520Baorui%2520Ma%2520and%2520Liang%2520Pan%2520and%2520Wei%2520Yin%2520and%2520Lingdong%2520Kong%2520and%2520Wei%2520Tsang%2520Ooi%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Immersive%2520applications%2520call%2520for%2520synthesizing%2520spatiotemporal%25204D%2520content%2520from%250Acasual%2520videos%2520without%2520costly%25203D%2520supervision.%2520Existing%2520video-to-4D%2520methods%250Atypically%2520rely%2520on%2520manually%2520annotated%2520camera%2520poses%252C%2520which%2520are%2520labor-intensive%250Aand%2520brittle%2520for%2520in-the-wild%2520footage.%2520Recent%2520warp-then-inpaint%2520approaches%250Amitigate%2520the%2520need%2520for%2520pose%2520labels%2520by%2520warping%2520input%2520frames%2520along%2520a%2520novel%2520camera%250Atrajectory%2520and%2520using%2520an%2520inpainting%2520model%2520to%2520fill%2520missing%2520regions%252C%2520thereby%250Adepicting%2520the%25204D%2520scene%2520from%2520diverse%2520viewpoints.%2520However%252C%2520this%250Atrajectory-to-trajectory%2520formulation%2520often%2520entangles%2520camera%2520motion%2520with%2520scene%250Adynamics%2520and%2520complicates%2520both%2520modeling%2520and%2520inference.%2520We%2520introduce%2520SEE4D%252C%2520a%250Apose-free%252C%2520trajectory-to-camera%2520framework%2520that%2520replaces%2520explicit%2520trajectory%250Aprediction%2520with%2520rendering%2520to%2520a%2520bank%2520of%2520fixed%2520virtual%2520cameras%252C%2520thereby%250Aseparating%2520camera%2520control%2520from%2520scene%2520modeling.%2520A%2520view-conditional%2520video%250Ainpainting%2520model%2520is%2520trained%2520to%2520learn%2520a%2520robust%2520geometry%2520prior%2520by%2520denoising%250Arealistically%2520synthesized%2520warped%2520images%2520and%2520to%2520inpaint%2520occluded%2520or%2520missing%250Aregions%2520across%2520virtual%2520viewpoints%252C%2520eliminating%2520the%2520need%2520for%2520explicit%25203D%250Aannotations.%2520Building%2520on%2520this%2520inpainting%2520core%252C%2520we%2520design%2520a%2520spatiotemporal%250Aautoregressive%2520inference%2520pipeline%2520that%2520traverses%2520virtual-camera%2520splines%2520and%250Aextends%2520videos%2520with%2520overlapping%2520windows%252C%2520enabling%2520coherent%2520generation%2520at%250Abounded%2520per-step%2520complexity.%2520We%2520validate%2520See4D%2520on%2520cross-view%2520video%2520generation%250Aand%2520sparse%2520reconstruction%2520benchmarks.%2520Across%2520quantitative%2520metrics%2520and%250Aqualitative%2520assessments%252C%2520our%2520method%2520achieves%2520superior%2520generalization%2520and%250Aimproved%2520performance%2520relative%2520to%2520pose-%2520or%2520trajectory-conditioned%2520baselines%252C%250Aadvancing%2520practical%25204D%2520world%2520modeling%2520from%2520casual%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEE4D%3A%20Pose-Free%204D%20Generation%20via%20Auto-Regressive%20Video%20Inpainting&entry.906535625=Dongyue%20Lu%20and%20Ao%20Liang%20and%20Tianxin%20Huang%20and%20Xiao%20Fu%20and%20Yuyang%20Zhao%20and%20Baorui%20Ma%20and%20Liang%20Pan%20and%20Wei%20Yin%20and%20Lingdong%20Kong%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu&entry.1292438233=%20%20Immersive%20applications%20call%20for%20synthesizing%20spatiotemporal%204D%20content%20from%0Acasual%20videos%20without%20costly%203D%20supervision.%20Existing%20video-to-4D%20methods%0Atypically%20rely%20on%20manually%20annotated%20camera%20poses%2C%20which%20are%20labor-intensive%0Aand%20brittle%20for%20in-the-wild%20footage.%20Recent%20warp-then-inpaint%20approaches%0Amitigate%20the%20need%20for%20pose%20labels%20by%20warping%20input%20frames%20along%20a%20novel%20camera%0Atrajectory%20and%20using%20an%20inpainting%20model%20to%20fill%20missing%20regions%2C%20thereby%0Adepicting%20the%204D%20scene%20from%20diverse%20viewpoints.%20However%2C%20this%0Atrajectory-to-trajectory%20formulation%20often%20entangles%20camera%20motion%20with%20scene%0Adynamics%20and%20complicates%20both%20modeling%20and%20inference.%20We%20introduce%20SEE4D%2C%20a%0Apose-free%2C%20trajectory-to-camera%20framework%20that%20replaces%20explicit%20trajectory%0Aprediction%20with%20rendering%20to%20a%20bank%20of%20fixed%20virtual%20cameras%2C%20thereby%0Aseparating%20camera%20control%20from%20scene%20modeling.%20A%20view-conditional%20video%0Ainpainting%20model%20is%20trained%20to%20learn%20a%20robust%20geometry%20prior%20by%20denoising%0Arealistically%20synthesized%20warped%20images%20and%20to%20inpaint%20occluded%20or%20missing%0Aregions%20across%20virtual%20viewpoints%2C%20eliminating%20the%20need%20for%20explicit%203D%0Aannotations.%20Building%20on%20this%20inpainting%20core%2C%20we%20design%20a%20spatiotemporal%0Aautoregressive%20inference%20pipeline%20that%20traverses%20virtual-camera%20splines%20and%0Aextends%20videos%20with%20overlapping%20windows%2C%20enabling%20coherent%20generation%20at%0Abounded%20per-step%20complexity.%20We%20validate%20See4D%20on%20cross-view%20video%20generation%0Aand%20sparse%20reconstruction%20benchmarks.%20Across%20quantitative%20metrics%20and%0Aqualitative%20assessments%2C%20our%20method%20achieves%20superior%20generalization%20and%0Aimproved%20performance%20relative%20to%20pose-%20or%20trajectory-conditioned%20baselines%2C%0Aadvancing%20practical%204D%20world%20modeling%20from%20casual%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26796v1&entry.124074799=Read"},
{"title": "Kimi Linear: An Expressive, Efficient Attention Architecture", "author": " Kimi Team and Yu Zhang and Zongyu Lin and Xingcheng Yao and Jiaxi Hu and Fanqing Meng and Chengyin Liu and Xin Men and Songlin Yang and Zhiyuan Li and Wentao Li and Enzhe Lu and Weizhou Liu and Yanru Chen and Weixin Xu and Longhui Yu and Yejie Wang and Yu Fan and Longguang Zhong and Enming Yuan and Dehao Zhang and Yizhi Zhang and T. Y. Liu and Haiming Wang and Shengjun Fang and Weiran He and Shaowei Liu and Yiwei Li and Jianlin Su and Jiezhong Qiu and Bo Pang and Junjie Yan and Zhejun Jiang and Weixiao Huang and Bohong Yin and Jiacheng You and Chu Wei and Zhengtao Wang and Chao Hong and Yutian Chen and Guanduo Chen and Yucheng Wang and Huabin Zheng and Feng Wang and Yibo Liu and Mengnan Dong and Zheng Zhang and Siyuan Pan and Wenhao Wu and Yuhao Wu and Longyu Guan and Jiawen Tao and Guohong Fu and Xinran Xu and Yuzhi Wang and Guokun Lai and Yuxin Wu and Xinyu Zhou and Zhilin Yang and Yulun Du", "abstract": "  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n", "link": "http://arxiv.org/abs/2510.26692v1", "date": "2025-10-30", "relevancy": 1.9762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4905}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kimi%20Linear%3A%20An%20Expressive%2C%20Efficient%20Attention%20Architecture&body=Title%3A%20Kimi%20Linear%3A%20An%20Expressive%2C%20Efficient%20Attention%20Architecture%0AAuthor%3A%20%20Kimi%20Team%20and%20Yu%20Zhang%20and%20Zongyu%20Lin%20and%20Xingcheng%20Yao%20and%20Jiaxi%20Hu%20and%20Fanqing%20Meng%20and%20Chengyin%20Liu%20and%20Xin%20Men%20and%20Songlin%20Yang%20and%20Zhiyuan%20Li%20and%20Wentao%20Li%20and%20Enzhe%20Lu%20and%20Weizhou%20Liu%20and%20Yanru%20Chen%20and%20Weixin%20Xu%20and%20Longhui%20Yu%20and%20Yejie%20Wang%20and%20Yu%20Fan%20and%20Longguang%20Zhong%20and%20Enming%20Yuan%20and%20Dehao%20Zhang%20and%20Yizhi%20Zhang%20and%20T.%20Y.%20Liu%20and%20Haiming%20Wang%20and%20Shengjun%20Fang%20and%20Weiran%20He%20and%20Shaowei%20Liu%20and%20Yiwei%20Li%20and%20Jianlin%20Su%20and%20Jiezhong%20Qiu%20and%20Bo%20Pang%20and%20Junjie%20Yan%20and%20Zhejun%20Jiang%20and%20Weixiao%20Huang%20and%20Bohong%20Yin%20and%20Jiacheng%20You%20and%20Chu%20Wei%20and%20Zhengtao%20Wang%20and%20Chao%20Hong%20and%20Yutian%20Chen%20and%20Guanduo%20Chen%20and%20Yucheng%20Wang%20and%20Huabin%20Zheng%20and%20Feng%20Wang%20and%20Yibo%20Liu%20and%20Mengnan%20Dong%20and%20Zheng%20Zhang%20and%20Siyuan%20Pan%20and%20Wenhao%20Wu%20and%20Yuhao%20Wu%20and%20Longyu%20Guan%20and%20Jiawen%20Tao%20and%20Guohong%20Fu%20and%20Xinran%20Xu%20and%20Yuzhi%20Wang%20and%20Guokun%20Lai%20and%20Yuxin%20Wu%20and%20Xinyu%20Zhou%20and%20Zhilin%20Yang%20and%20Yulun%20Du%0AAbstract%3A%20%20%20We%20introduce%20Kimi%20Linear%2C%20a%20hybrid%20linear%20attention%20architecture%20that%2C%20for%0Athe%20first%20time%2C%20outperforms%20full%20attention%20under%20fair%20comparisons%20across%0Avarious%20scenarios%20--%20including%20short-context%2C%20long-context%2C%20and%20reinforcement%0Alearning%20%28RL%29%20scaling%20regimes.%20At%20its%20core%20lies%20Kimi%20Delta%20Attention%20%28KDA%29%2C%20an%0Aexpressive%20linear%20attention%20module%20that%20extends%20Gated%20DeltaNet%20with%20a%0Afiner-grained%20gating%20mechanism%2C%20enabling%20more%20effective%20use%20of%20limited%0Afinite-state%20RNN%20memory.%20Our%20bespoke%20chunkwise%20algorithm%20achieves%20high%20hardware%0Aefficiency%20through%20a%20specialized%20variant%20of%20the%20Diagonal-Plus-Low-Rank%20%28DPLR%29%0Atransition%20matrices%2C%20which%20substantially%20reduces%20computation%20compared%20to%20the%0Ageneral%20DPLR%20formulation%20while%20remaining%20more%20consistent%20with%20the%20classical%0Adelta%20rule.%0A%20%20We%20pretrain%20a%20Kimi%20Linear%20model%20with%203B%20activated%20parameters%20and%2048B%20total%0Aparameters%2C%20based%20on%20a%20layerwise%20hybrid%20of%20KDA%20and%20Multi-Head%20Latent%20Attention%0A%28MLA%29.%20Our%20experiments%20show%20that%20with%20an%20identical%20training%20recipe%2C%20Kimi%20Linear%0Aoutperforms%20full%20MLA%20with%20a%20sizeable%20margin%20across%20all%20evaluated%20tasks%2C%20while%0Areducing%20KV%20cache%20usage%20by%20up%20to%2075%25%20and%20achieving%20up%20to%206%20times%20decoding%0Athroughput%20for%20a%201M%20context.%20These%20results%20demonstrate%20that%20Kimi%20Linear%20can%20be%0Aa%20drop-in%20replacement%20for%20full%20attention%20architectures%20with%20superior%0Aperformance%20and%20efficiency%2C%20including%20tasks%20with%20longer%20input%20and%20output%0Alengths.%0A%20%20To%20support%20further%20research%2C%20we%20open-source%20the%20KDA%20kernel%20and%20vLLM%0Aimplementations%2C%20and%20release%20the%20pre-trained%20and%20instruction-tuned%20model%0Acheckpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKimi%2520Linear%253A%2520An%2520Expressive%252C%2520Efficient%2520Attention%2520Architecture%26entry.906535625%3D%2520Kimi%2520Team%2520and%2520Yu%2520Zhang%2520and%2520Zongyu%2520Lin%2520and%2520Xingcheng%2520Yao%2520and%2520Jiaxi%2520Hu%2520and%2520Fanqing%2520Meng%2520and%2520Chengyin%2520Liu%2520and%2520Xin%2520Men%2520and%2520Songlin%2520Yang%2520and%2520Zhiyuan%2520Li%2520and%2520Wentao%2520Li%2520and%2520Enzhe%2520Lu%2520and%2520Weizhou%2520Liu%2520and%2520Yanru%2520Chen%2520and%2520Weixin%2520Xu%2520and%2520Longhui%2520Yu%2520and%2520Yejie%2520Wang%2520and%2520Yu%2520Fan%2520and%2520Longguang%2520Zhong%2520and%2520Enming%2520Yuan%2520and%2520Dehao%2520Zhang%2520and%2520Yizhi%2520Zhang%2520and%2520T.%2520Y.%2520Liu%2520and%2520Haiming%2520Wang%2520and%2520Shengjun%2520Fang%2520and%2520Weiran%2520He%2520and%2520Shaowei%2520Liu%2520and%2520Yiwei%2520Li%2520and%2520Jianlin%2520Su%2520and%2520Jiezhong%2520Qiu%2520and%2520Bo%2520Pang%2520and%2520Junjie%2520Yan%2520and%2520Zhejun%2520Jiang%2520and%2520Weixiao%2520Huang%2520and%2520Bohong%2520Yin%2520and%2520Jiacheng%2520You%2520and%2520Chu%2520Wei%2520and%2520Zhengtao%2520Wang%2520and%2520Chao%2520Hong%2520and%2520Yutian%2520Chen%2520and%2520Guanduo%2520Chen%2520and%2520Yucheng%2520Wang%2520and%2520Huabin%2520Zheng%2520and%2520Feng%2520Wang%2520and%2520Yibo%2520Liu%2520and%2520Mengnan%2520Dong%2520and%2520Zheng%2520Zhang%2520and%2520Siyuan%2520Pan%2520and%2520Wenhao%2520Wu%2520and%2520Yuhao%2520Wu%2520and%2520Longyu%2520Guan%2520and%2520Jiawen%2520Tao%2520and%2520Guohong%2520Fu%2520and%2520Xinran%2520Xu%2520and%2520Yuzhi%2520Wang%2520and%2520Guokun%2520Lai%2520and%2520Yuxin%2520Wu%2520and%2520Xinyu%2520Zhou%2520and%2520Zhilin%2520Yang%2520and%2520Yulun%2520Du%26entry.1292438233%3D%2520%2520We%2520introduce%2520Kimi%2520Linear%252C%2520a%2520hybrid%2520linear%2520attention%2520architecture%2520that%252C%2520for%250Athe%2520first%2520time%252C%2520outperforms%2520full%2520attention%2520under%2520fair%2520comparisons%2520across%250Avarious%2520scenarios%2520--%2520including%2520short-context%252C%2520long-context%252C%2520and%2520reinforcement%250Alearning%2520%2528RL%2529%2520scaling%2520regimes.%2520At%2520its%2520core%2520lies%2520Kimi%2520Delta%2520Attention%2520%2528KDA%2529%252C%2520an%250Aexpressive%2520linear%2520attention%2520module%2520that%2520extends%2520Gated%2520DeltaNet%2520with%2520a%250Afiner-grained%2520gating%2520mechanism%252C%2520enabling%2520more%2520effective%2520use%2520of%2520limited%250Afinite-state%2520RNN%2520memory.%2520Our%2520bespoke%2520chunkwise%2520algorithm%2520achieves%2520high%2520hardware%250Aefficiency%2520through%2520a%2520specialized%2520variant%2520of%2520the%2520Diagonal-Plus-Low-Rank%2520%2528DPLR%2529%250Atransition%2520matrices%252C%2520which%2520substantially%2520reduces%2520computation%2520compared%2520to%2520the%250Ageneral%2520DPLR%2520formulation%2520while%2520remaining%2520more%2520consistent%2520with%2520the%2520classical%250Adelta%2520rule.%250A%2520%2520We%2520pretrain%2520a%2520Kimi%2520Linear%2520model%2520with%25203B%2520activated%2520parameters%2520and%252048B%2520total%250Aparameters%252C%2520based%2520on%2520a%2520layerwise%2520hybrid%2520of%2520KDA%2520and%2520Multi-Head%2520Latent%2520Attention%250A%2528MLA%2529.%2520Our%2520experiments%2520show%2520that%2520with%2520an%2520identical%2520training%2520recipe%252C%2520Kimi%2520Linear%250Aoutperforms%2520full%2520MLA%2520with%2520a%2520sizeable%2520margin%2520across%2520all%2520evaluated%2520tasks%252C%2520while%250Areducing%2520KV%2520cache%2520usage%2520by%2520up%2520to%252075%2525%2520and%2520achieving%2520up%2520to%25206%2520times%2520decoding%250Athroughput%2520for%2520a%25201M%2520context.%2520These%2520results%2520demonstrate%2520that%2520Kimi%2520Linear%2520can%2520be%250Aa%2520drop-in%2520replacement%2520for%2520full%2520attention%2520architectures%2520with%2520superior%250Aperformance%2520and%2520efficiency%252C%2520including%2520tasks%2520with%2520longer%2520input%2520and%2520output%250Alengths.%250A%2520%2520To%2520support%2520further%2520research%252C%2520we%2520open-source%2520the%2520KDA%2520kernel%2520and%2520vLLM%250Aimplementations%252C%2520and%2520release%2520the%2520pre-trained%2520and%2520instruction-tuned%2520model%250Acheckpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kimi%20Linear%3A%20An%20Expressive%2C%20Efficient%20Attention%20Architecture&entry.906535625=%20Kimi%20Team%20and%20Yu%20Zhang%20and%20Zongyu%20Lin%20and%20Xingcheng%20Yao%20and%20Jiaxi%20Hu%20and%20Fanqing%20Meng%20and%20Chengyin%20Liu%20and%20Xin%20Men%20and%20Songlin%20Yang%20and%20Zhiyuan%20Li%20and%20Wentao%20Li%20and%20Enzhe%20Lu%20and%20Weizhou%20Liu%20and%20Yanru%20Chen%20and%20Weixin%20Xu%20and%20Longhui%20Yu%20and%20Yejie%20Wang%20and%20Yu%20Fan%20and%20Longguang%20Zhong%20and%20Enming%20Yuan%20and%20Dehao%20Zhang%20and%20Yizhi%20Zhang%20and%20T.%20Y.%20Liu%20and%20Haiming%20Wang%20and%20Shengjun%20Fang%20and%20Weiran%20He%20and%20Shaowei%20Liu%20and%20Yiwei%20Li%20and%20Jianlin%20Su%20and%20Jiezhong%20Qiu%20and%20Bo%20Pang%20and%20Junjie%20Yan%20and%20Zhejun%20Jiang%20and%20Weixiao%20Huang%20and%20Bohong%20Yin%20and%20Jiacheng%20You%20and%20Chu%20Wei%20and%20Zhengtao%20Wang%20and%20Chao%20Hong%20and%20Yutian%20Chen%20and%20Guanduo%20Chen%20and%20Yucheng%20Wang%20and%20Huabin%20Zheng%20and%20Feng%20Wang%20and%20Yibo%20Liu%20and%20Mengnan%20Dong%20and%20Zheng%20Zhang%20and%20Siyuan%20Pan%20and%20Wenhao%20Wu%20and%20Yuhao%20Wu%20and%20Longyu%20Guan%20and%20Jiawen%20Tao%20and%20Guohong%20Fu%20and%20Xinran%20Xu%20and%20Yuzhi%20Wang%20and%20Guokun%20Lai%20and%20Yuxin%20Wu%20and%20Xinyu%20Zhou%20and%20Zhilin%20Yang%20and%20Yulun%20Du&entry.1292438233=%20%20We%20introduce%20Kimi%20Linear%2C%20a%20hybrid%20linear%20attention%20architecture%20that%2C%20for%0Athe%20first%20time%2C%20outperforms%20full%20attention%20under%20fair%20comparisons%20across%0Avarious%20scenarios%20--%20including%20short-context%2C%20long-context%2C%20and%20reinforcement%0Alearning%20%28RL%29%20scaling%20regimes.%20At%20its%20core%20lies%20Kimi%20Delta%20Attention%20%28KDA%29%2C%20an%0Aexpressive%20linear%20attention%20module%20that%20extends%20Gated%20DeltaNet%20with%20a%0Afiner-grained%20gating%20mechanism%2C%20enabling%20more%20effective%20use%20of%20limited%0Afinite-state%20RNN%20memory.%20Our%20bespoke%20chunkwise%20algorithm%20achieves%20high%20hardware%0Aefficiency%20through%20a%20specialized%20variant%20of%20the%20Diagonal-Plus-Low-Rank%20%28DPLR%29%0Atransition%20matrices%2C%20which%20substantially%20reduces%20computation%20compared%20to%20the%0Ageneral%20DPLR%20formulation%20while%20remaining%20more%20consistent%20with%20the%20classical%0Adelta%20rule.%0A%20%20We%20pretrain%20a%20Kimi%20Linear%20model%20with%203B%20activated%20parameters%20and%2048B%20total%0Aparameters%2C%20based%20on%20a%20layerwise%20hybrid%20of%20KDA%20and%20Multi-Head%20Latent%20Attention%0A%28MLA%29.%20Our%20experiments%20show%20that%20with%20an%20identical%20training%20recipe%2C%20Kimi%20Linear%0Aoutperforms%20full%20MLA%20with%20a%20sizeable%20margin%20across%20all%20evaluated%20tasks%2C%20while%0Areducing%20KV%20cache%20usage%20by%20up%20to%2075%25%20and%20achieving%20up%20to%206%20times%20decoding%0Athroughput%20for%20a%201M%20context.%20These%20results%20demonstrate%20that%20Kimi%20Linear%20can%20be%0Aa%20drop-in%20replacement%20for%20full%20attention%20architectures%20with%20superior%0Aperformance%20and%20efficiency%2C%20including%20tasks%20with%20longer%20input%20and%20output%0Alengths.%0A%20%20To%20support%20further%20research%2C%20we%20open-source%20the%20KDA%20kernel%20and%20vLLM%0Aimplementations%2C%20and%20release%20the%20pre-trained%20and%20instruction-tuned%20model%0Acheckpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26692v1&entry.124074799=Read"},
{"title": "Fit for Purpose? Deepfake Detection in the Real World", "author": "Guangyu Lin and Li Lin and Christina P. Walker and Daniel S. Schiff and Shu Hu", "abstract": "  The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.\n", "link": "http://arxiv.org/abs/2510.16556v2", "date": "2025-10-30", "relevancy": 1.9694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fit%20for%20Purpose%3F%20Deepfake%20Detection%20in%20the%20Real%20World&body=Title%3A%20Fit%20for%20Purpose%3F%20Deepfake%20Detection%20in%20the%20Real%20World%0AAuthor%3A%20Guangyu%20Lin%20and%20Li%20Lin%20and%20Christina%20P.%20Walker%20and%20Daniel%20S.%20Schiff%20and%20Shu%20Hu%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20AI-generated%20content%2C%20driven%20by%20advances%20in%0Agenerative%20adversarial%20networks%2C%20diffusion%20models%2C%20and%20multimodal%20large%0Alanguage%20models%2C%20has%20made%20the%20creation%20and%20dissemination%20of%20synthetic%20media%0Aeffortless%2C%20heightening%20the%20risks%20of%20misinformation%2C%20particularly%20political%0Adeepfakes%20that%20distort%20truth%20and%20undermine%20trust%20in%20political%20institutions.%20In%0Aturn%2C%20governments%2C%20research%20institutions%2C%20and%20industry%20have%20strongly%20promoted%0Adeepfake%20detection%20initiatives%20as%20solutions.%20Yet%2C%20most%20existing%20models%20are%0Atrained%20and%20validated%20on%20synthetic%2C%20laboratory-controlled%20datasets%2C%20limiting%0Atheir%20generalizability%20to%20the%20kinds%20of%20real-world%20political%20deepfakes%0Acirculating%20on%20social%20platforms%20that%20affect%20the%20public.%20In%20this%20work%2C%20we%0Aintroduce%20the%20first%20systematic%20benchmark%20based%20on%20the%20Political%20Deepfakes%0AIncident%20Database%2C%20a%20curated%20collection%20of%20real-world%20political%20deepfakes%0Ashared%20on%20social%20media%20since%202018.%20Our%20study%20includes%20a%20systematic%20evaluation%0Aof%20state-of-the-art%20deepfake%20detectors%20across%20academia%2C%20government%2C%20and%0Aindustry.%20We%20find%20that%20the%20detectors%20from%20academia%20and%20government%20perform%0Arelatively%20poorly.%20While%20paid%20detection%20tools%20achieve%20relatively%20higher%0Aperformance%20than%20free-access%20models%2C%20all%20evaluated%20detectors%20struggle%20to%0Ageneralize%20effectively%20to%20authentic%20political%20deepfakes%2C%20and%20are%20vulnerable%20to%0Asimple%20manipulations%2C%20especially%20in%20the%20video%20domain.%20Results%20urge%20the%20need%20for%0Apolitically%20contextualized%20deepfake%20detection%20frameworks%20to%20better%20safeguard%0Athe%20public%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFit%2520for%2520Purpose%253F%2520Deepfake%2520Detection%2520in%2520the%2520Real%2520World%26entry.906535625%3DGuangyu%2520Lin%2520and%2520Li%2520Lin%2520and%2520Christina%2520P.%2520Walker%2520and%2520Daniel%2520S.%2520Schiff%2520and%2520Shu%2520Hu%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520AI-generated%2520content%252C%2520driven%2520by%2520advances%2520in%250Agenerative%2520adversarial%2520networks%252C%2520diffusion%2520models%252C%2520and%2520multimodal%2520large%250Alanguage%2520models%252C%2520has%2520made%2520the%2520creation%2520and%2520dissemination%2520of%2520synthetic%2520media%250Aeffortless%252C%2520heightening%2520the%2520risks%2520of%2520misinformation%252C%2520particularly%2520political%250Adeepfakes%2520that%2520distort%2520truth%2520and%2520undermine%2520trust%2520in%2520political%2520institutions.%2520In%250Aturn%252C%2520governments%252C%2520research%2520institutions%252C%2520and%2520industry%2520have%2520strongly%2520promoted%250Adeepfake%2520detection%2520initiatives%2520as%2520solutions.%2520Yet%252C%2520most%2520existing%2520models%2520are%250Atrained%2520and%2520validated%2520on%2520synthetic%252C%2520laboratory-controlled%2520datasets%252C%2520limiting%250Atheir%2520generalizability%2520to%2520the%2520kinds%2520of%2520real-world%2520political%2520deepfakes%250Acirculating%2520on%2520social%2520platforms%2520that%2520affect%2520the%2520public.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520first%2520systematic%2520benchmark%2520based%2520on%2520the%2520Political%2520Deepfakes%250AIncident%2520Database%252C%2520a%2520curated%2520collection%2520of%2520real-world%2520political%2520deepfakes%250Ashared%2520on%2520social%2520media%2520since%25202018.%2520Our%2520study%2520includes%2520a%2520systematic%2520evaluation%250Aof%2520state-of-the-art%2520deepfake%2520detectors%2520across%2520academia%252C%2520government%252C%2520and%250Aindustry.%2520We%2520find%2520that%2520the%2520detectors%2520from%2520academia%2520and%2520government%2520perform%250Arelatively%2520poorly.%2520While%2520paid%2520detection%2520tools%2520achieve%2520relatively%2520higher%250Aperformance%2520than%2520free-access%2520models%252C%2520all%2520evaluated%2520detectors%2520struggle%2520to%250Ageneralize%2520effectively%2520to%2520authentic%2520political%2520deepfakes%252C%2520and%2520are%2520vulnerable%2520to%250Asimple%2520manipulations%252C%2520especially%2520in%2520the%2520video%2520domain.%2520Results%2520urge%2520the%2520need%2520for%250Apolitically%2520contextualized%2520deepfake%2520detection%2520frameworks%2520to%2520better%2520safeguard%250Athe%2520public%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fit%20for%20Purpose%3F%20Deepfake%20Detection%20in%20the%20Real%20World&entry.906535625=Guangyu%20Lin%20and%20Li%20Lin%20and%20Christina%20P.%20Walker%20and%20Daniel%20S.%20Schiff%20and%20Shu%20Hu&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20AI-generated%20content%2C%20driven%20by%20advances%20in%0Agenerative%20adversarial%20networks%2C%20diffusion%20models%2C%20and%20multimodal%20large%0Alanguage%20models%2C%20has%20made%20the%20creation%20and%20dissemination%20of%20synthetic%20media%0Aeffortless%2C%20heightening%20the%20risks%20of%20misinformation%2C%20particularly%20political%0Adeepfakes%20that%20distort%20truth%20and%20undermine%20trust%20in%20political%20institutions.%20In%0Aturn%2C%20governments%2C%20research%20institutions%2C%20and%20industry%20have%20strongly%20promoted%0Adeepfake%20detection%20initiatives%20as%20solutions.%20Yet%2C%20most%20existing%20models%20are%0Atrained%20and%20validated%20on%20synthetic%2C%20laboratory-controlled%20datasets%2C%20limiting%0Atheir%20generalizability%20to%20the%20kinds%20of%20real-world%20political%20deepfakes%0Acirculating%20on%20social%20platforms%20that%20affect%20the%20public.%20In%20this%20work%2C%20we%0Aintroduce%20the%20first%20systematic%20benchmark%20based%20on%20the%20Political%20Deepfakes%0AIncident%20Database%2C%20a%20curated%20collection%20of%20real-world%20political%20deepfakes%0Ashared%20on%20social%20media%20since%202018.%20Our%20study%20includes%20a%20systematic%20evaluation%0Aof%20state-of-the-art%20deepfake%20detectors%20across%20academia%2C%20government%2C%20and%0Aindustry.%20We%20find%20that%20the%20detectors%20from%20academia%20and%20government%20perform%0Arelatively%20poorly.%20While%20paid%20detection%20tools%20achieve%20relatively%20higher%0Aperformance%20than%20free-access%20models%2C%20all%20evaluated%20detectors%20struggle%20to%0Ageneralize%20effectively%20to%20authentic%20political%20deepfakes%2C%20and%20are%20vulnerable%20to%0Asimple%20manipulations%2C%20especially%20in%20the%20video%20domain.%20Results%20urge%20the%20need%20for%0Apolitically%20contextualized%20deepfake%20detection%20frameworks%20to%20better%20safeguard%0Athe%20public%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16556v2&entry.124074799=Read"},
{"title": "BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI", "author": "Alya Almsouti and Ainur Khamitova and Darya Taratynova and Mohammad Yaqub", "abstract": "  Assessing the severity of artifacts in pediatric brain Magnetic Resonance\nImaging (MRI) is critical for diagnostic accuracy, especially in low-field\nsystems where the signal-to-noise ratio is reduced. Manual quality assessment\nis time-consuming and subjective, motivating the need for robust automated\nsolutions. In this work, we propose BRIQA (Balanced Reweighting in Image\nQuality Assessment), which addresses class imbalance in artifact severity\nlevels. BRIQA uses gradient-based loss reweighting to dynamically adjust\nper-class contributions and employs a rotating batching scheme to ensure\nconsistent exposure to underrepresented classes. Through experiments, no single\narchitecture performs best across all artifact types, emphasizing the\nimportance of architectural diversity. The rotating batching configuration\nimproves performance across metrics by promoting balanced learning when\ncombined with cross-entropy loss. BRIQA improves average macro F1 score from\n0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),\nPositioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)\nartifact severity classification. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/BRIQA.\n", "link": "http://arxiv.org/abs/2510.26661v1", "date": "2025-10-30", "relevancy": 1.9639, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5019}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIQA%3A%20Balanced%20Reweighting%20in%20Image%20Quality%20Assessment%20of%20Pediatric%0A%20%20Brain%20MRI&body=Title%3A%20BRIQA%3A%20Balanced%20Reweighting%20in%20Image%20Quality%20Assessment%20of%20Pediatric%0A%20%20Brain%20MRI%0AAuthor%3A%20Alya%20Almsouti%20and%20Ainur%20Khamitova%20and%20Darya%20Taratynova%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Assessing%20the%20severity%20of%20artifacts%20in%20pediatric%20brain%20Magnetic%20Resonance%0AImaging%20%28MRI%29%20is%20critical%20for%20diagnostic%20accuracy%2C%20especially%20in%20low-field%0Asystems%20where%20the%20signal-to-noise%20ratio%20is%20reduced.%20Manual%20quality%20assessment%0Ais%20time-consuming%20and%20subjective%2C%20motivating%20the%20need%20for%20robust%20automated%0Asolutions.%20In%20this%20work%2C%20we%20propose%20BRIQA%20%28Balanced%20Reweighting%20in%20Image%0AQuality%20Assessment%29%2C%20which%20addresses%20class%20imbalance%20in%20artifact%20severity%0Alevels.%20BRIQA%20uses%20gradient-based%20loss%20reweighting%20to%20dynamically%20adjust%0Aper-class%20contributions%20and%20employs%20a%20rotating%20batching%20scheme%20to%20ensure%0Aconsistent%20exposure%20to%20underrepresented%20classes.%20Through%20experiments%2C%20no%20single%0Aarchitecture%20performs%20best%20across%20all%20artifact%20types%2C%20emphasizing%20the%0Aimportance%20of%20architectural%20diversity.%20The%20rotating%20batching%20configuration%0Aimproves%20performance%20across%20metrics%20by%20promoting%20balanced%20learning%20when%0Acombined%20with%20cross-entropy%20loss.%20BRIQA%20improves%20average%20macro%20F1%20score%20from%0A0.659%20to%200.706%2C%20with%20notable%20gains%20in%20Noise%20%280.430%29%2C%20Zipper%20%280.098%29%2C%0APositioning%20%280.097%29%2C%20Contrast%20%280.217%29%2C%20Motion%20%280.022%29%2C%20and%20Banding%20%280.012%29%0Aartifact%20severity%20classification.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/BRIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIQA%253A%2520Balanced%2520Reweighting%2520in%2520Image%2520Quality%2520Assessment%2520of%2520Pediatric%250A%2520%2520Brain%2520MRI%26entry.906535625%3DAlya%2520Almsouti%2520and%2520Ainur%2520Khamitova%2520and%2520Darya%2520Taratynova%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Assessing%2520the%2520severity%2520of%2520artifacts%2520in%2520pediatric%2520brain%2520Magnetic%2520Resonance%250AImaging%2520%2528MRI%2529%2520is%2520critical%2520for%2520diagnostic%2520accuracy%252C%2520especially%2520in%2520low-field%250Asystems%2520where%2520the%2520signal-to-noise%2520ratio%2520is%2520reduced.%2520Manual%2520quality%2520assessment%250Ais%2520time-consuming%2520and%2520subjective%252C%2520motivating%2520the%2520need%2520for%2520robust%2520automated%250Asolutions.%2520In%2520this%2520work%252C%2520we%2520propose%2520BRIQA%2520%2528Balanced%2520Reweighting%2520in%2520Image%250AQuality%2520Assessment%2529%252C%2520which%2520addresses%2520class%2520imbalance%2520in%2520artifact%2520severity%250Alevels.%2520BRIQA%2520uses%2520gradient-based%2520loss%2520reweighting%2520to%2520dynamically%2520adjust%250Aper-class%2520contributions%2520and%2520employs%2520a%2520rotating%2520batching%2520scheme%2520to%2520ensure%250Aconsistent%2520exposure%2520to%2520underrepresented%2520classes.%2520Through%2520experiments%252C%2520no%2520single%250Aarchitecture%2520performs%2520best%2520across%2520all%2520artifact%2520types%252C%2520emphasizing%2520the%250Aimportance%2520of%2520architectural%2520diversity.%2520The%2520rotating%2520batching%2520configuration%250Aimproves%2520performance%2520across%2520metrics%2520by%2520promoting%2520balanced%2520learning%2520when%250Acombined%2520with%2520cross-entropy%2520loss.%2520BRIQA%2520improves%2520average%2520macro%2520F1%2520score%2520from%250A0.659%2520to%25200.706%252C%2520with%2520notable%2520gains%2520in%2520Noise%2520%25280.430%2529%252C%2520Zipper%2520%25280.098%2529%252C%250APositioning%2520%25280.097%2529%252C%2520Contrast%2520%25280.217%2529%252C%2520Motion%2520%25280.022%2529%252C%2520and%2520Banding%2520%25280.012%2529%250Aartifact%2520severity%2520classification.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/BioMedIA-MBZUAI/BRIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIQA%3A%20Balanced%20Reweighting%20in%20Image%20Quality%20Assessment%20of%20Pediatric%0A%20%20Brain%20MRI&entry.906535625=Alya%20Almsouti%20and%20Ainur%20Khamitova%20and%20Darya%20Taratynova%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Assessing%20the%20severity%20of%20artifacts%20in%20pediatric%20brain%20Magnetic%20Resonance%0AImaging%20%28MRI%29%20is%20critical%20for%20diagnostic%20accuracy%2C%20especially%20in%20low-field%0Asystems%20where%20the%20signal-to-noise%20ratio%20is%20reduced.%20Manual%20quality%20assessment%0Ais%20time-consuming%20and%20subjective%2C%20motivating%20the%20need%20for%20robust%20automated%0Asolutions.%20In%20this%20work%2C%20we%20propose%20BRIQA%20%28Balanced%20Reweighting%20in%20Image%0AQuality%20Assessment%29%2C%20which%20addresses%20class%20imbalance%20in%20artifact%20severity%0Alevels.%20BRIQA%20uses%20gradient-based%20loss%20reweighting%20to%20dynamically%20adjust%0Aper-class%20contributions%20and%20employs%20a%20rotating%20batching%20scheme%20to%20ensure%0Aconsistent%20exposure%20to%20underrepresented%20classes.%20Through%20experiments%2C%20no%20single%0Aarchitecture%20performs%20best%20across%20all%20artifact%20types%2C%20emphasizing%20the%0Aimportance%20of%20architectural%20diversity.%20The%20rotating%20batching%20configuration%0Aimproves%20performance%20across%20metrics%20by%20promoting%20balanced%20learning%20when%0Acombined%20with%20cross-entropy%20loss.%20BRIQA%20improves%20average%20macro%20F1%20score%20from%0A0.659%20to%200.706%2C%20with%20notable%20gains%20in%20Noise%20%280.430%29%2C%20Zipper%20%280.098%29%2C%0APositioning%20%280.097%29%2C%20Contrast%20%280.217%29%2C%20Motion%20%280.022%29%2C%20and%20Banding%20%280.012%29%0Aartifact%20severity%20classification.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/BRIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26661v1&entry.124074799=Read"},
{"title": "Learning Pseudorandom Numbers with Transformers: Permuted Congruential\n  Generators, Curricula, and Interpretability", "author": "Tao Tao and Maissam Barkeshli", "abstract": "  We study the ability of Transformer models to learn sequences generated by\nPermuted Congruential Generators (PCGs), a widely used family of pseudo-random\nnumber generators (PRNGs). PCGs introduce substantial additional difficulty\nover linear congruential generators (LCGs) by applying a series of bit-wise\nshifts, XORs, rotations and truncations to the hidden state. We show that\nTransformers can nevertheless successfully perform in-context prediction on\nunseen sequences from diverse PCG variants, in tasks that are beyond published\nclassical attacks. In our experiments we scale moduli up to $2^{22}$ using up\nto $50$ million model parameters and datasets with up to $5$ billion tokens.\nSurprisingly, we find even when the output is truncated to a single bit, it can\nbe reliably predicted by the model. When multiple distinct PRNGs are presented\ntogether during training, the model can jointly learn them, identifying\nstructures from different permutations. We demonstrate a scaling law with\nmodulus $m$: the number of in-context sequence elements required for\nnear-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization\nenters extended stagnation phases; in our experiments, learning moduli $m \\geq\n2^{20}$ requires incorporating training data from smaller moduli, demonstrating\na critical necessity for curriculum learning. Finally, we analyze embedding\nlayers and uncover a novel clustering phenomenon: the model spontaneously\ngroups the integer inputs into bitwise rotationally-invariant clusters,\nrevealing how representations can transfer from smaller to larger moduli.\n", "link": "http://arxiv.org/abs/2510.26792v1", "date": "2025-10-30", "relevancy": 1.9579, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5697}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Pseudorandom%20Numbers%20with%20Transformers%3A%20Permuted%20Congruential%0A%20%20Generators%2C%20Curricula%2C%20and%20Interpretability&body=Title%3A%20Learning%20Pseudorandom%20Numbers%20with%20Transformers%3A%20Permuted%20Congruential%0A%20%20Generators%2C%20Curricula%2C%20and%20Interpretability%0AAuthor%3A%20Tao%20Tao%20and%20Maissam%20Barkeshli%0AAbstract%3A%20%20%20We%20study%20the%20ability%20of%20Transformer%20models%20to%20learn%20sequences%20generated%20by%0APermuted%20Congruential%20Generators%20%28PCGs%29%2C%20a%20widely%20used%20family%20of%20pseudo-random%0Anumber%20generators%20%28PRNGs%29.%20PCGs%20introduce%20substantial%20additional%20difficulty%0Aover%20linear%20congruential%20generators%20%28LCGs%29%20by%20applying%20a%20series%20of%20bit-wise%0Ashifts%2C%20XORs%2C%20rotations%20and%20truncations%20to%20the%20hidden%20state.%20We%20show%20that%0ATransformers%20can%20nevertheless%20successfully%20perform%20in-context%20prediction%20on%0Aunseen%20sequences%20from%20diverse%20PCG%20variants%2C%20in%20tasks%20that%20are%20beyond%20published%0Aclassical%20attacks.%20In%20our%20experiments%20we%20scale%20moduli%20up%20to%20%242%5E%7B22%7D%24%20using%20up%0Ato%20%2450%24%20million%20model%20parameters%20and%20datasets%20with%20up%20to%20%245%24%20billion%20tokens.%0ASurprisingly%2C%20we%20find%20even%20when%20the%20output%20is%20truncated%20to%20a%20single%20bit%2C%20it%20can%0Abe%20reliably%20predicted%20by%20the%20model.%20When%20multiple%20distinct%20PRNGs%20are%20presented%0Atogether%20during%20training%2C%20the%20model%20can%20jointly%20learn%20them%2C%20identifying%0Astructures%20from%20different%20permutations.%20We%20demonstrate%20a%20scaling%20law%20with%0Amodulus%20%24m%24%3A%20the%20number%20of%20in-context%20sequence%20elements%20required%20for%0Anear-perfect%20prediction%20grows%20as%20%24%5Csqrt%7Bm%7D%24.%20For%20larger%20moduli%2C%20optimization%0Aenters%20extended%20stagnation%20phases%3B%20in%20our%20experiments%2C%20learning%20moduli%20%24m%20%5Cgeq%0A2%5E%7B20%7D%24%20requires%20incorporating%20training%20data%20from%20smaller%20moduli%2C%20demonstrating%0Aa%20critical%20necessity%20for%20curriculum%20learning.%20Finally%2C%20we%20analyze%20embedding%0Alayers%20and%20uncover%20a%20novel%20clustering%20phenomenon%3A%20the%20model%20spontaneously%0Agroups%20the%20integer%20inputs%20into%20bitwise%20rotationally-invariant%20clusters%2C%0Arevealing%20how%20representations%20can%20transfer%20from%20smaller%20to%20larger%20moduli.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Pseudorandom%2520Numbers%2520with%2520Transformers%253A%2520Permuted%2520Congruential%250A%2520%2520Generators%252C%2520Curricula%252C%2520and%2520Interpretability%26entry.906535625%3DTao%2520Tao%2520and%2520Maissam%2520Barkeshli%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520ability%2520of%2520Transformer%2520models%2520to%2520learn%2520sequences%2520generated%2520by%250APermuted%2520Congruential%2520Generators%2520%2528PCGs%2529%252C%2520a%2520widely%2520used%2520family%2520of%2520pseudo-random%250Anumber%2520generators%2520%2528PRNGs%2529.%2520PCGs%2520introduce%2520substantial%2520additional%2520difficulty%250Aover%2520linear%2520congruential%2520generators%2520%2528LCGs%2529%2520by%2520applying%2520a%2520series%2520of%2520bit-wise%250Ashifts%252C%2520XORs%252C%2520rotations%2520and%2520truncations%2520to%2520the%2520hidden%2520state.%2520We%2520show%2520that%250ATransformers%2520can%2520nevertheless%2520successfully%2520perform%2520in-context%2520prediction%2520on%250Aunseen%2520sequences%2520from%2520diverse%2520PCG%2520variants%252C%2520in%2520tasks%2520that%2520are%2520beyond%2520published%250Aclassical%2520attacks.%2520In%2520our%2520experiments%2520we%2520scale%2520moduli%2520up%2520to%2520%25242%255E%257B22%257D%2524%2520using%2520up%250Ato%2520%252450%2524%2520million%2520model%2520parameters%2520and%2520datasets%2520with%2520up%2520to%2520%25245%2524%2520billion%2520tokens.%250ASurprisingly%252C%2520we%2520find%2520even%2520when%2520the%2520output%2520is%2520truncated%2520to%2520a%2520single%2520bit%252C%2520it%2520can%250Abe%2520reliably%2520predicted%2520by%2520the%2520model.%2520When%2520multiple%2520distinct%2520PRNGs%2520are%2520presented%250Atogether%2520during%2520training%252C%2520the%2520model%2520can%2520jointly%2520learn%2520them%252C%2520identifying%250Astructures%2520from%2520different%2520permutations.%2520We%2520demonstrate%2520a%2520scaling%2520law%2520with%250Amodulus%2520%2524m%2524%253A%2520the%2520number%2520of%2520in-context%2520sequence%2520elements%2520required%2520for%250Anear-perfect%2520prediction%2520grows%2520as%2520%2524%255Csqrt%257Bm%257D%2524.%2520For%2520larger%2520moduli%252C%2520optimization%250Aenters%2520extended%2520stagnation%2520phases%253B%2520in%2520our%2520experiments%252C%2520learning%2520moduli%2520%2524m%2520%255Cgeq%250A2%255E%257B20%257D%2524%2520requires%2520incorporating%2520training%2520data%2520from%2520smaller%2520moduli%252C%2520demonstrating%250Aa%2520critical%2520necessity%2520for%2520curriculum%2520learning.%2520Finally%252C%2520we%2520analyze%2520embedding%250Alayers%2520and%2520uncover%2520a%2520novel%2520clustering%2520phenomenon%253A%2520the%2520model%2520spontaneously%250Agroups%2520the%2520integer%2520inputs%2520into%2520bitwise%2520rotationally-invariant%2520clusters%252C%250Arevealing%2520how%2520representations%2520can%2520transfer%2520from%2520smaller%2520to%2520larger%2520moduli.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Pseudorandom%20Numbers%20with%20Transformers%3A%20Permuted%20Congruential%0A%20%20Generators%2C%20Curricula%2C%20and%20Interpretability&entry.906535625=Tao%20Tao%20and%20Maissam%20Barkeshli&entry.1292438233=%20%20We%20study%20the%20ability%20of%20Transformer%20models%20to%20learn%20sequences%20generated%20by%0APermuted%20Congruential%20Generators%20%28PCGs%29%2C%20a%20widely%20used%20family%20of%20pseudo-random%0Anumber%20generators%20%28PRNGs%29.%20PCGs%20introduce%20substantial%20additional%20difficulty%0Aover%20linear%20congruential%20generators%20%28LCGs%29%20by%20applying%20a%20series%20of%20bit-wise%0Ashifts%2C%20XORs%2C%20rotations%20and%20truncations%20to%20the%20hidden%20state.%20We%20show%20that%0ATransformers%20can%20nevertheless%20successfully%20perform%20in-context%20prediction%20on%0Aunseen%20sequences%20from%20diverse%20PCG%20variants%2C%20in%20tasks%20that%20are%20beyond%20published%0Aclassical%20attacks.%20In%20our%20experiments%20we%20scale%20moduli%20up%20to%20%242%5E%7B22%7D%24%20using%20up%0Ato%20%2450%24%20million%20model%20parameters%20and%20datasets%20with%20up%20to%20%245%24%20billion%20tokens.%0ASurprisingly%2C%20we%20find%20even%20when%20the%20output%20is%20truncated%20to%20a%20single%20bit%2C%20it%20can%0Abe%20reliably%20predicted%20by%20the%20model.%20When%20multiple%20distinct%20PRNGs%20are%20presented%0Atogether%20during%20training%2C%20the%20model%20can%20jointly%20learn%20them%2C%20identifying%0Astructures%20from%20different%20permutations.%20We%20demonstrate%20a%20scaling%20law%20with%0Amodulus%20%24m%24%3A%20the%20number%20of%20in-context%20sequence%20elements%20required%20for%0Anear-perfect%20prediction%20grows%20as%20%24%5Csqrt%7Bm%7D%24.%20For%20larger%20moduli%2C%20optimization%0Aenters%20extended%20stagnation%20phases%3B%20in%20our%20experiments%2C%20learning%20moduli%20%24m%20%5Cgeq%0A2%5E%7B20%7D%24%20requires%20incorporating%20training%20data%20from%20smaller%20moduli%2C%20demonstrating%0Aa%20critical%20necessity%20for%20curriculum%20learning.%20Finally%2C%20we%20analyze%20embedding%0Alayers%20and%20uncover%20a%20novel%20clustering%20phenomenon%3A%20the%20model%20spontaneously%0Agroups%20the%20integer%20inputs%20into%20bitwise%20rotationally-invariant%20clusters%2C%0Arevealing%20how%20representations%20can%20transfer%20from%20smaller%20to%20larger%20moduli.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26792v1&entry.124074799=Read"},
{"title": "UniSite: The First Cross-Structure Dataset and Learning Framework for\n  End-to-End Ligand Binding Site Detection", "author": "Jigang Fan and Quanlin Wu and Shengjie Luo and Liwei Wang", "abstract": "  The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.\n", "link": "http://arxiv.org/abs/2506.03237v2", "date": "2025-10-30", "relevancy": 1.9514, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniSite%3A%20The%20First%20Cross-Structure%20Dataset%20and%20Learning%20Framework%20for%0A%20%20End-to-End%20Ligand%20Binding%20Site%20Detection&body=Title%3A%20UniSite%3A%20The%20First%20Cross-Structure%20Dataset%20and%20Learning%20Framework%20for%0A%20%20End-to-End%20Ligand%20Binding%20Site%20Detection%0AAuthor%3A%20Jigang%20Fan%20and%20Quanlin%20Wu%20and%20Shengjie%20Luo%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20The%20detection%20of%20ligand%20binding%20sites%20for%20proteins%20is%20a%20fundamental%20step%20in%0AStructure-Based%20Drug%20Design.%20Despite%20notable%20advances%20in%20recent%20years%2C%20existing%0Amethods%2C%20datasets%2C%20and%20evaluation%20metrics%20are%20confronted%20with%20several%20key%0Achallenges%3A%20%281%29%20current%20datasets%20and%20methods%20are%20centered%20on%20individual%0Aprotein-ligand%20complexes%20and%20neglect%20that%20diverse%20binding%20sites%20may%20exist%0Aacross%20multiple%20complexes%20of%20the%20same%20protein%2C%20introducing%20significant%0Astatistical%20bias%3B%20%282%29%20ligand%20binding%20site%20detection%20is%20typically%20modeled%20as%20a%0Adiscontinuous%20workflow%2C%20employing%20binary%20segmentation%20and%20subsequent%20clustering%0Aalgorithms%3B%20%283%29%20traditional%20evaluation%20metrics%20do%20not%20adequately%20reflect%20the%0Aactual%20performance%20of%20different%20binding%20site%20prediction%20methods.%20To%20address%0Athese%20issues%2C%20we%20first%20introduce%20UniSite-DS%2C%20the%20first%20UniProt%20%28Unique%0AProtein%29-centric%20ligand%20binding%20site%20dataset%2C%20which%20contains%204.81%20times%20more%0Amulti-site%20data%20and%202.08%20times%20more%20overall%20data%20compared%20to%20the%20previously%0Amost%20widely%20used%20datasets.%20We%20then%20propose%20UniSite%2C%20the%20first%20end-to-end%20ligand%0Abinding%20site%20detection%20framework%20supervised%20by%20set%20prediction%20loss%20with%0Abijective%20matching.%20In%20addition%2C%20we%20introduce%20Average%20Precision%20based%20on%0AIntersection%20over%20Union%20%28IoU%29%20as%20a%20more%20accurate%20evaluation%20metric%20for%20ligand%0Abinding%20site%20prediction.%20Extensive%20experiments%20on%20UniSite-DS%20and%20several%0Arepresentative%20benchmark%20datasets%20demonstrate%20that%20IoU-based%20Average%20Precision%0Aprovides%20a%20more%20accurate%20reflection%20of%20prediction%20quality%2C%20and%20that%20UniSite%0Aoutperforms%20current%20state-of-the-art%20methods%20in%20ligand%20binding%20site%20detection.%0AThe%20dataset%20and%20codes%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/quanlin-wu/unisite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniSite%253A%2520The%2520First%2520Cross-Structure%2520Dataset%2520and%2520Learning%2520Framework%2520for%250A%2520%2520End-to-End%2520Ligand%2520Binding%2520Site%2520Detection%26entry.906535625%3DJigang%2520Fan%2520and%2520Quanlin%2520Wu%2520and%2520Shengjie%2520Luo%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520The%2520detection%2520of%2520ligand%2520binding%2520sites%2520for%2520proteins%2520is%2520a%2520fundamental%2520step%2520in%250AStructure-Based%2520Drug%2520Design.%2520Despite%2520notable%2520advances%2520in%2520recent%2520years%252C%2520existing%250Amethods%252C%2520datasets%252C%2520and%2520evaluation%2520metrics%2520are%2520confronted%2520with%2520several%2520key%250Achallenges%253A%2520%25281%2529%2520current%2520datasets%2520and%2520methods%2520are%2520centered%2520on%2520individual%250Aprotein-ligand%2520complexes%2520and%2520neglect%2520that%2520diverse%2520binding%2520sites%2520may%2520exist%250Aacross%2520multiple%2520complexes%2520of%2520the%2520same%2520protein%252C%2520introducing%2520significant%250Astatistical%2520bias%253B%2520%25282%2529%2520ligand%2520binding%2520site%2520detection%2520is%2520typically%2520modeled%2520as%2520a%250Adiscontinuous%2520workflow%252C%2520employing%2520binary%2520segmentation%2520and%2520subsequent%2520clustering%250Aalgorithms%253B%2520%25283%2529%2520traditional%2520evaluation%2520metrics%2520do%2520not%2520adequately%2520reflect%2520the%250Aactual%2520performance%2520of%2520different%2520binding%2520site%2520prediction%2520methods.%2520To%2520address%250Athese%2520issues%252C%2520we%2520first%2520introduce%2520UniSite-DS%252C%2520the%2520first%2520UniProt%2520%2528Unique%250AProtein%2529-centric%2520ligand%2520binding%2520site%2520dataset%252C%2520which%2520contains%25204.81%2520times%2520more%250Amulti-site%2520data%2520and%25202.08%2520times%2520more%2520overall%2520data%2520compared%2520to%2520the%2520previously%250Amost%2520widely%2520used%2520datasets.%2520We%2520then%2520propose%2520UniSite%252C%2520the%2520first%2520end-to-end%2520ligand%250Abinding%2520site%2520detection%2520framework%2520supervised%2520by%2520set%2520prediction%2520loss%2520with%250Abijective%2520matching.%2520In%2520addition%252C%2520we%2520introduce%2520Average%2520Precision%2520based%2520on%250AIntersection%2520over%2520Union%2520%2528IoU%2529%2520as%2520a%2520more%2520accurate%2520evaluation%2520metric%2520for%2520ligand%250Abinding%2520site%2520prediction.%2520Extensive%2520experiments%2520on%2520UniSite-DS%2520and%2520several%250Arepresentative%2520benchmark%2520datasets%2520demonstrate%2520that%2520IoU-based%2520Average%2520Precision%250Aprovides%2520a%2520more%2520accurate%2520reflection%2520of%2520prediction%2520quality%252C%2520and%2520that%2520UniSite%250Aoutperforms%2520current%2520state-of-the-art%2520methods%2520in%2520ligand%2520binding%2520site%2520detection.%250AThe%2520dataset%2520and%2520codes%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/quanlin-wu/unisite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniSite%3A%20The%20First%20Cross-Structure%20Dataset%20and%20Learning%20Framework%20for%0A%20%20End-to-End%20Ligand%20Binding%20Site%20Detection&entry.906535625=Jigang%20Fan%20and%20Quanlin%20Wu%20and%20Shengjie%20Luo%20and%20Liwei%20Wang&entry.1292438233=%20%20The%20detection%20of%20ligand%20binding%20sites%20for%20proteins%20is%20a%20fundamental%20step%20in%0AStructure-Based%20Drug%20Design.%20Despite%20notable%20advances%20in%20recent%20years%2C%20existing%0Amethods%2C%20datasets%2C%20and%20evaluation%20metrics%20are%20confronted%20with%20several%20key%0Achallenges%3A%20%281%29%20current%20datasets%20and%20methods%20are%20centered%20on%20individual%0Aprotein-ligand%20complexes%20and%20neglect%20that%20diverse%20binding%20sites%20may%20exist%0Aacross%20multiple%20complexes%20of%20the%20same%20protein%2C%20introducing%20significant%0Astatistical%20bias%3B%20%282%29%20ligand%20binding%20site%20detection%20is%20typically%20modeled%20as%20a%0Adiscontinuous%20workflow%2C%20employing%20binary%20segmentation%20and%20subsequent%20clustering%0Aalgorithms%3B%20%283%29%20traditional%20evaluation%20metrics%20do%20not%20adequately%20reflect%20the%0Aactual%20performance%20of%20different%20binding%20site%20prediction%20methods.%20To%20address%0Athese%20issues%2C%20we%20first%20introduce%20UniSite-DS%2C%20the%20first%20UniProt%20%28Unique%0AProtein%29-centric%20ligand%20binding%20site%20dataset%2C%20which%20contains%204.81%20times%20more%0Amulti-site%20data%20and%202.08%20times%20more%20overall%20data%20compared%20to%20the%20previously%0Amost%20widely%20used%20datasets.%20We%20then%20propose%20UniSite%2C%20the%20first%20end-to-end%20ligand%0Abinding%20site%20detection%20framework%20supervised%20by%20set%20prediction%20loss%20with%0Abijective%20matching.%20In%20addition%2C%20we%20introduce%20Average%20Precision%20based%20on%0AIntersection%20over%20Union%20%28IoU%29%20as%20a%20more%20accurate%20evaluation%20metric%20for%20ligand%0Abinding%20site%20prediction.%20Extensive%20experiments%20on%20UniSite-DS%20and%20several%0Arepresentative%20benchmark%20datasets%20demonstrate%20that%20IoU-based%20Average%20Precision%0Aprovides%20a%20more%20accurate%20reflection%20of%20prediction%20quality%2C%20and%20that%20UniSite%0Aoutperforms%20current%20state-of-the-art%20methods%20in%20ligand%20binding%20site%20detection.%0AThe%20dataset%20and%20codes%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/quanlin-wu/unisite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03237v2&entry.124074799=Read"},
{"title": "FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design", "author": "Jun Dai and Michael Rizvi-Martel and Guillaume Rabusseau", "abstract": "  Designing efficient quantum circuits is a central bottleneck to exploring the\npotential of quantum computing, particularly for noisy intermediate-scale\nquantum (NISQ) devices, where circuit efficiency and resilience to errors are\nparamount. The search space of gate sequences grows combinatorially, and\nhandcrafted templates often waste scarce qubit and depth budgets. We introduce\n\\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework\nfor automated quantum circuit synthesis based on Generative Flow Networks\n(GFlowNets). This framework learns a stochastic policy to construct circuits\nsequentially, sampling them in proportion to a flexible, user-defined reward\nfunction that can encode multiple design objectives such as performance, depth,\nand gate count. This approach uniquely enables the generation of a diverse\nensemble of high-quality circuits, moving beyond single-solution optimization.\nWe demonstrate the efficacy of \\textsc{FlowQ-Net} through an extensive set of\nsimulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz\ndesign for molecular ground state estimation, Max-Cut, and image\nclassification, key challenges in near-term quantum computing. Circuits\ndesigned by \\textsc{FlowQ-Net} achieve significant improvements, yielding\ncircuits that are 10$\\times$-30$\\times$ more compact in terms of parameters,\ngates, and depth compared to commonly used unitary baselines, without\ncompromising accuracy. This trend holds even when subjected to error profiles\nfrom real-world quantum devices. Our results underline the potential of\ngenerative models as a general-purpose methodology for automated quantum\ncircuit design, offering a promising path towards more efficient quantum\nalgorithms and accelerating scientific discovery in the quantum domain.\n", "link": "http://arxiv.org/abs/2510.26688v1", "date": "2025-10-30", "relevancy": 1.9514, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5659}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4766}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowQ-Net%3A%20A%20Generative%20Framework%20for%20Automated%20Quantum%20Circuit%20Design&body=Title%3A%20FlowQ-Net%3A%20A%20Generative%20Framework%20for%20Automated%20Quantum%20Circuit%20Design%0AAuthor%3A%20Jun%20Dai%20and%20Michael%20Rizvi-Martel%20and%20Guillaume%20Rabusseau%0AAbstract%3A%20%20%20Designing%20efficient%20quantum%20circuits%20is%20a%20central%20bottleneck%20to%20exploring%20the%0Apotential%20of%20quantum%20computing%2C%20particularly%20for%20noisy%20intermediate-scale%0Aquantum%20%28NISQ%29%20devices%2C%20where%20circuit%20efficiency%20and%20resilience%20to%20errors%20are%0Aparamount.%20The%20search%20space%20of%20gate%20sequences%20grows%20combinatorially%2C%20and%0Ahandcrafted%20templates%20often%20waste%20scarce%20qubit%20and%20depth%20budgets.%20We%20introduce%0A%5Ctextsc%7BFlowQ-Net%7D%20%28Flow-based%20Quantum%20design%20Network%29%2C%20a%20generative%20framework%0Afor%20automated%20quantum%20circuit%20synthesis%20based%20on%20Generative%20Flow%20Networks%0A%28GFlowNets%29.%20This%20framework%20learns%20a%20stochastic%20policy%20to%20construct%20circuits%0Asequentially%2C%20sampling%20them%20in%20proportion%20to%20a%20flexible%2C%20user-defined%20reward%0Afunction%20that%20can%20encode%20multiple%20design%20objectives%20such%20as%20performance%2C%20depth%2C%0Aand%20gate%20count.%20This%20approach%20uniquely%20enables%20the%20generation%20of%20a%20diverse%0Aensemble%20of%20high-quality%20circuits%2C%20moving%20beyond%20single-solution%20optimization.%0AWe%20demonstrate%20the%20efficacy%20of%20%5Ctextsc%7BFlowQ-Net%7D%20through%20an%20extensive%20set%20of%0Asimulations.%20We%20apply%20our%20method%20to%20Variational%20Quantum%20Algorithm%20%28VQA%29%20ansatz%0Adesign%20for%20molecular%20ground%20state%20estimation%2C%20Max-Cut%2C%20and%20image%0Aclassification%2C%20key%20challenges%20in%20near-term%20quantum%20computing.%20Circuits%0Adesigned%20by%20%5Ctextsc%7BFlowQ-Net%7D%20achieve%20significant%20improvements%2C%20yielding%0Acircuits%20that%20are%2010%24%5Ctimes%24-30%24%5Ctimes%24%20more%20compact%20in%20terms%20of%20parameters%2C%0Agates%2C%20and%20depth%20compared%20to%20commonly%20used%20unitary%20baselines%2C%20without%0Acompromising%20accuracy.%20This%20trend%20holds%20even%20when%20subjected%20to%20error%20profiles%0Afrom%20real-world%20quantum%20devices.%20Our%20results%20underline%20the%20potential%20of%0Agenerative%20models%20as%20a%20general-purpose%20methodology%20for%20automated%20quantum%0Acircuit%20design%2C%20offering%20a%20promising%20path%20towards%20more%20efficient%20quantum%0Aalgorithms%20and%20accelerating%20scientific%20discovery%20in%20the%20quantum%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowQ-Net%253A%2520A%2520Generative%2520Framework%2520for%2520Automated%2520Quantum%2520Circuit%2520Design%26entry.906535625%3DJun%2520Dai%2520and%2520Michael%2520Rizvi-Martel%2520and%2520Guillaume%2520Rabusseau%26entry.1292438233%3D%2520%2520Designing%2520efficient%2520quantum%2520circuits%2520is%2520a%2520central%2520bottleneck%2520to%2520exploring%2520the%250Apotential%2520of%2520quantum%2520computing%252C%2520particularly%2520for%2520noisy%2520intermediate-scale%250Aquantum%2520%2528NISQ%2529%2520devices%252C%2520where%2520circuit%2520efficiency%2520and%2520resilience%2520to%2520errors%2520are%250Aparamount.%2520The%2520search%2520space%2520of%2520gate%2520sequences%2520grows%2520combinatorially%252C%2520and%250Ahandcrafted%2520templates%2520often%2520waste%2520scarce%2520qubit%2520and%2520depth%2520budgets.%2520We%2520introduce%250A%255Ctextsc%257BFlowQ-Net%257D%2520%2528Flow-based%2520Quantum%2520design%2520Network%2529%252C%2520a%2520generative%2520framework%250Afor%2520automated%2520quantum%2520circuit%2520synthesis%2520based%2520on%2520Generative%2520Flow%2520Networks%250A%2528GFlowNets%2529.%2520This%2520framework%2520learns%2520a%2520stochastic%2520policy%2520to%2520construct%2520circuits%250Asequentially%252C%2520sampling%2520them%2520in%2520proportion%2520to%2520a%2520flexible%252C%2520user-defined%2520reward%250Afunction%2520that%2520can%2520encode%2520multiple%2520design%2520objectives%2520such%2520as%2520performance%252C%2520depth%252C%250Aand%2520gate%2520count.%2520This%2520approach%2520uniquely%2520enables%2520the%2520generation%2520of%2520a%2520diverse%250Aensemble%2520of%2520high-quality%2520circuits%252C%2520moving%2520beyond%2520single-solution%2520optimization.%250AWe%2520demonstrate%2520the%2520efficacy%2520of%2520%255Ctextsc%257BFlowQ-Net%257D%2520through%2520an%2520extensive%2520set%2520of%250Asimulations.%2520We%2520apply%2520our%2520method%2520to%2520Variational%2520Quantum%2520Algorithm%2520%2528VQA%2529%2520ansatz%250Adesign%2520for%2520molecular%2520ground%2520state%2520estimation%252C%2520Max-Cut%252C%2520and%2520image%250Aclassification%252C%2520key%2520challenges%2520in%2520near-term%2520quantum%2520computing.%2520Circuits%250Adesigned%2520by%2520%255Ctextsc%257BFlowQ-Net%257D%2520achieve%2520significant%2520improvements%252C%2520yielding%250Acircuits%2520that%2520are%252010%2524%255Ctimes%2524-30%2524%255Ctimes%2524%2520more%2520compact%2520in%2520terms%2520of%2520parameters%252C%250Agates%252C%2520and%2520depth%2520compared%2520to%2520commonly%2520used%2520unitary%2520baselines%252C%2520without%250Acompromising%2520accuracy.%2520This%2520trend%2520holds%2520even%2520when%2520subjected%2520to%2520error%2520profiles%250Afrom%2520real-world%2520quantum%2520devices.%2520Our%2520results%2520underline%2520the%2520potential%2520of%250Agenerative%2520models%2520as%2520a%2520general-purpose%2520methodology%2520for%2520automated%2520quantum%250Acircuit%2520design%252C%2520offering%2520a%2520promising%2520path%2520towards%2520more%2520efficient%2520quantum%250Aalgorithms%2520and%2520accelerating%2520scientific%2520discovery%2520in%2520the%2520quantum%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowQ-Net%3A%20A%20Generative%20Framework%20for%20Automated%20Quantum%20Circuit%20Design&entry.906535625=Jun%20Dai%20and%20Michael%20Rizvi-Martel%20and%20Guillaume%20Rabusseau&entry.1292438233=%20%20Designing%20efficient%20quantum%20circuits%20is%20a%20central%20bottleneck%20to%20exploring%20the%0Apotential%20of%20quantum%20computing%2C%20particularly%20for%20noisy%20intermediate-scale%0Aquantum%20%28NISQ%29%20devices%2C%20where%20circuit%20efficiency%20and%20resilience%20to%20errors%20are%0Aparamount.%20The%20search%20space%20of%20gate%20sequences%20grows%20combinatorially%2C%20and%0Ahandcrafted%20templates%20often%20waste%20scarce%20qubit%20and%20depth%20budgets.%20We%20introduce%0A%5Ctextsc%7BFlowQ-Net%7D%20%28Flow-based%20Quantum%20design%20Network%29%2C%20a%20generative%20framework%0Afor%20automated%20quantum%20circuit%20synthesis%20based%20on%20Generative%20Flow%20Networks%0A%28GFlowNets%29.%20This%20framework%20learns%20a%20stochastic%20policy%20to%20construct%20circuits%0Asequentially%2C%20sampling%20them%20in%20proportion%20to%20a%20flexible%2C%20user-defined%20reward%0Afunction%20that%20can%20encode%20multiple%20design%20objectives%20such%20as%20performance%2C%20depth%2C%0Aand%20gate%20count.%20This%20approach%20uniquely%20enables%20the%20generation%20of%20a%20diverse%0Aensemble%20of%20high-quality%20circuits%2C%20moving%20beyond%20single-solution%20optimization.%0AWe%20demonstrate%20the%20efficacy%20of%20%5Ctextsc%7BFlowQ-Net%7D%20through%20an%20extensive%20set%20of%0Asimulations.%20We%20apply%20our%20method%20to%20Variational%20Quantum%20Algorithm%20%28VQA%29%20ansatz%0Adesign%20for%20molecular%20ground%20state%20estimation%2C%20Max-Cut%2C%20and%20image%0Aclassification%2C%20key%20challenges%20in%20near-term%20quantum%20computing.%20Circuits%0Adesigned%20by%20%5Ctextsc%7BFlowQ-Net%7D%20achieve%20significant%20improvements%2C%20yielding%0Acircuits%20that%20are%2010%24%5Ctimes%24-30%24%5Ctimes%24%20more%20compact%20in%20terms%20of%20parameters%2C%0Agates%2C%20and%20depth%20compared%20to%20commonly%20used%20unitary%20baselines%2C%20without%0Acompromising%20accuracy.%20This%20trend%20holds%20even%20when%20subjected%20to%20error%20profiles%0Afrom%20real-world%20quantum%20devices.%20Our%20results%20underline%20the%20potential%20of%0Agenerative%20models%20as%20a%20general-purpose%20methodology%20for%20automated%20quantum%0Acircuit%20design%2C%20offering%20a%20promising%20path%20towards%20more%20efficient%20quantum%0Aalgorithms%20and%20accelerating%20scientific%20discovery%20in%20the%20quantum%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26688v1&entry.124074799=Read"},
{"title": "MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection", "author": "Emmanouil Sylligardos and John Paparrizos and Themis Palpanas and Pierre Senellart and Paul Boniol", "abstract": "  Anomaly detection is a fundamental task for time series analytics with\nimportant implications for the downstream performance of many applications.\nDespite increasing academic interest and the large number of methods proposed\nin the literature, recent benchmarks and evaluation studies demonstrated that\nno overall best anomaly detection methods exist when applied to very\nheterogeneous time series datasets. Therefore, the only scalable and viable\nsolution to solve anomaly detection over very different time series collected\nfrom diverse domains is to propose a model selection method that will select,\nbased on time series characteristics, the best anomaly detection methods to\nrun. Existing AutoML solutions are, unfortunately, not directly applicable to\ntime series anomaly detection, and no evaluation of time series-based\napproaches for model selection exists. Towards that direction, this paper\nstudies the performance of time series classification methods used as model\nselection for anomaly detection. In total, we evaluate 234 model configurations\nderived from 16 base classifiers across more than 1980 time series, and we\npropose the first extensive experimental evaluation of time series\nclassification as model selection for anomaly detection. Our results\ndemonstrate that model selection methods outperform every single anomaly\ndetection method while being in the same order of magnitude regarding execution\ntime. This evaluation is the first step to demonstrate the accuracy and\nefficiency of time series classification algorithms for anomaly detection, and\nrepresents a strong baseline that can then be used to guide the model selection\nstep in general AutoML pipelines. Preprint version of an article accepted at\nthe VLDB Journal.\n", "link": "http://arxiv.org/abs/2510.26643v1", "date": "2025-10-30", "relevancy": 1.9497, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5005}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4866}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSAD%3A%20A%20Deep%20Dive%20into%20Model%20Selection%20for%20Time%20series%20Anomaly%20Detection&body=Title%3A%20MSAD%3A%20A%20Deep%20Dive%20into%20Model%20Selection%20for%20Time%20series%20Anomaly%20Detection%0AAuthor%3A%20Emmanouil%20Sylligardos%20and%20John%20Paparrizos%20and%20Themis%20Palpanas%20and%20Pierre%20Senellart%20and%20Paul%20Boniol%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20a%20fundamental%20task%20for%20time%20series%20analytics%20with%0Aimportant%20implications%20for%20the%20downstream%20performance%20of%20many%20applications.%0ADespite%20increasing%20academic%20interest%20and%20the%20large%20number%20of%20methods%20proposed%0Ain%20the%20literature%2C%20recent%20benchmarks%20and%20evaluation%20studies%20demonstrated%20that%0Ano%20overall%20best%20anomaly%20detection%20methods%20exist%20when%20applied%20to%20very%0Aheterogeneous%20time%20series%20datasets.%20Therefore%2C%20the%20only%20scalable%20and%20viable%0Asolution%20to%20solve%20anomaly%20detection%20over%20very%20different%20time%20series%20collected%0Afrom%20diverse%20domains%20is%20to%20propose%20a%20model%20selection%20method%20that%20will%20select%2C%0Abased%20on%20time%20series%20characteristics%2C%20the%20best%20anomaly%20detection%20methods%20to%0Arun.%20Existing%20AutoML%20solutions%20are%2C%20unfortunately%2C%20not%20directly%20applicable%20to%0Atime%20series%20anomaly%20detection%2C%20and%20no%20evaluation%20of%20time%20series-based%0Aapproaches%20for%20model%20selection%20exists.%20Towards%20that%20direction%2C%20this%20paper%0Astudies%20the%20performance%20of%20time%20series%20classification%20methods%20used%20as%20model%0Aselection%20for%20anomaly%20detection.%20In%20total%2C%20we%20evaluate%20234%20model%20configurations%0Aderived%20from%2016%20base%20classifiers%20across%20more%20than%201980%20time%20series%2C%20and%20we%0Apropose%20the%20first%20extensive%20experimental%20evaluation%20of%20time%20series%0Aclassification%20as%20model%20selection%20for%20anomaly%20detection.%20Our%20results%0Ademonstrate%20that%20model%20selection%20methods%20outperform%20every%20single%20anomaly%0Adetection%20method%20while%20being%20in%20the%20same%20order%20of%20magnitude%20regarding%20execution%0Atime.%20This%20evaluation%20is%20the%20first%20step%20to%20demonstrate%20the%20accuracy%20and%0Aefficiency%20of%20time%20series%20classification%20algorithms%20for%20anomaly%20detection%2C%20and%0Arepresents%20a%20strong%20baseline%20that%20can%20then%20be%20used%20to%20guide%20the%20model%20selection%0Astep%20in%20general%20AutoML%20pipelines.%20Preprint%20version%20of%20an%20article%20accepted%20at%0Athe%20VLDB%20Journal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSAD%253A%2520A%2520Deep%2520Dive%2520into%2520Model%2520Selection%2520for%2520Time%2520series%2520Anomaly%2520Detection%26entry.906535625%3DEmmanouil%2520Sylligardos%2520and%2520John%2520Paparrizos%2520and%2520Themis%2520Palpanas%2520and%2520Pierre%2520Senellart%2520and%2520Paul%2520Boniol%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520a%2520fundamental%2520task%2520for%2520time%2520series%2520analytics%2520with%250Aimportant%2520implications%2520for%2520the%2520downstream%2520performance%2520of%2520many%2520applications.%250ADespite%2520increasing%2520academic%2520interest%2520and%2520the%2520large%2520number%2520of%2520methods%2520proposed%250Ain%2520the%2520literature%252C%2520recent%2520benchmarks%2520and%2520evaluation%2520studies%2520demonstrated%2520that%250Ano%2520overall%2520best%2520anomaly%2520detection%2520methods%2520exist%2520when%2520applied%2520to%2520very%250Aheterogeneous%2520time%2520series%2520datasets.%2520Therefore%252C%2520the%2520only%2520scalable%2520and%2520viable%250Asolution%2520to%2520solve%2520anomaly%2520detection%2520over%2520very%2520different%2520time%2520series%2520collected%250Afrom%2520diverse%2520domains%2520is%2520to%2520propose%2520a%2520model%2520selection%2520method%2520that%2520will%2520select%252C%250Abased%2520on%2520time%2520series%2520characteristics%252C%2520the%2520best%2520anomaly%2520detection%2520methods%2520to%250Arun.%2520Existing%2520AutoML%2520solutions%2520are%252C%2520unfortunately%252C%2520not%2520directly%2520applicable%2520to%250Atime%2520series%2520anomaly%2520detection%252C%2520and%2520no%2520evaluation%2520of%2520time%2520series-based%250Aapproaches%2520for%2520model%2520selection%2520exists.%2520Towards%2520that%2520direction%252C%2520this%2520paper%250Astudies%2520the%2520performance%2520of%2520time%2520series%2520classification%2520methods%2520used%2520as%2520model%250Aselection%2520for%2520anomaly%2520detection.%2520In%2520total%252C%2520we%2520evaluate%2520234%2520model%2520configurations%250Aderived%2520from%252016%2520base%2520classifiers%2520across%2520more%2520than%25201980%2520time%2520series%252C%2520and%2520we%250Apropose%2520the%2520first%2520extensive%2520experimental%2520evaluation%2520of%2520time%2520series%250Aclassification%2520as%2520model%2520selection%2520for%2520anomaly%2520detection.%2520Our%2520results%250Ademonstrate%2520that%2520model%2520selection%2520methods%2520outperform%2520every%2520single%2520anomaly%250Adetection%2520method%2520while%2520being%2520in%2520the%2520same%2520order%2520of%2520magnitude%2520regarding%2520execution%250Atime.%2520This%2520evaluation%2520is%2520the%2520first%2520step%2520to%2520demonstrate%2520the%2520accuracy%2520and%250Aefficiency%2520of%2520time%2520series%2520classification%2520algorithms%2520for%2520anomaly%2520detection%252C%2520and%250Arepresents%2520a%2520strong%2520baseline%2520that%2520can%2520then%2520be%2520used%2520to%2520guide%2520the%2520model%2520selection%250Astep%2520in%2520general%2520AutoML%2520pipelines.%2520Preprint%2520version%2520of%2520an%2520article%2520accepted%2520at%250Athe%2520VLDB%2520Journal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSAD%3A%20A%20Deep%20Dive%20into%20Model%20Selection%20for%20Time%20series%20Anomaly%20Detection&entry.906535625=Emmanouil%20Sylligardos%20and%20John%20Paparrizos%20and%20Themis%20Palpanas%20and%20Pierre%20Senellart%20and%20Paul%20Boniol&entry.1292438233=%20%20Anomaly%20detection%20is%20a%20fundamental%20task%20for%20time%20series%20analytics%20with%0Aimportant%20implications%20for%20the%20downstream%20performance%20of%20many%20applications.%0ADespite%20increasing%20academic%20interest%20and%20the%20large%20number%20of%20methods%20proposed%0Ain%20the%20literature%2C%20recent%20benchmarks%20and%20evaluation%20studies%20demonstrated%20that%0Ano%20overall%20best%20anomaly%20detection%20methods%20exist%20when%20applied%20to%20very%0Aheterogeneous%20time%20series%20datasets.%20Therefore%2C%20the%20only%20scalable%20and%20viable%0Asolution%20to%20solve%20anomaly%20detection%20over%20very%20different%20time%20series%20collected%0Afrom%20diverse%20domains%20is%20to%20propose%20a%20model%20selection%20method%20that%20will%20select%2C%0Abased%20on%20time%20series%20characteristics%2C%20the%20best%20anomaly%20detection%20methods%20to%0Arun.%20Existing%20AutoML%20solutions%20are%2C%20unfortunately%2C%20not%20directly%20applicable%20to%0Atime%20series%20anomaly%20detection%2C%20and%20no%20evaluation%20of%20time%20series-based%0Aapproaches%20for%20model%20selection%20exists.%20Towards%20that%20direction%2C%20this%20paper%0Astudies%20the%20performance%20of%20time%20series%20classification%20methods%20used%20as%20model%0Aselection%20for%20anomaly%20detection.%20In%20total%2C%20we%20evaluate%20234%20model%20configurations%0Aderived%20from%2016%20base%20classifiers%20across%20more%20than%201980%20time%20series%2C%20and%20we%0Apropose%20the%20first%20extensive%20experimental%20evaluation%20of%20time%20series%0Aclassification%20as%20model%20selection%20for%20anomaly%20detection.%20Our%20results%0Ademonstrate%20that%20model%20selection%20methods%20outperform%20every%20single%20anomaly%0Adetection%20method%20while%20being%20in%20the%20same%20order%20of%20magnitude%20regarding%20execution%0Atime.%20This%20evaluation%20is%20the%20first%20step%20to%20demonstrate%20the%20accuracy%20and%0Aefficiency%20of%20time%20series%20classification%20algorithms%20for%20anomaly%20detection%2C%20and%0Arepresents%20a%20strong%20baseline%20that%20can%20then%20be%20used%20to%20guide%20the%20model%20selection%0Astep%20in%20general%20AutoML%20pipelines.%20Preprint%20version%20of%20an%20article%20accepted%20at%0Athe%20VLDB%20Journal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26643v1&entry.124074799=Read"},
{"title": "When Kernels Multiply, Clusters Unify: Fusing Embeddings with the\n  Kronecker Product", "author": "Youqi Wu and Jingwei Zhang and Farzan Farnia", "abstract": "  State-of-the-art embeddings often capture distinct yet complementary\ndiscriminative features: For instance, one image embedding model may excel at\ndistinguishing fine-grained textures, while another focuses on object-level\nstructure. Motivated by this observation, we propose a principled approach to\nfuse such complementary representations through kernel multiplication.\nMultiplying the kernel similarity functions of two embeddings allows their\ndiscriminative structures to interact, producing a fused representation whose\nkernel encodes the union of the clusters identified by each parent embedding.\nThis formulation also provides a natural way to construct joint kernels for\npaired multi-modal data (e.g., image-text tuples), where the product of\nmodality-specific kernels inherits structure from both domains. We highlight\nthat this kernel product is mathematically realized via the Kronecker product\nof the embedding feature maps, yielding our proposed KrossFuse framework for\nembedding fusion. To address the computational cost of the resulting\nhigh-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable\nvariant that leverages random projections for efficient approximation. As a key\napplication, we use this framework to bridge the performance gap between\ncross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2,\nE5). Experiments show that RP-KrossFuse effectively integrates these models,\nenhancing modality-specific performance while preserving cross-modal alignment.\nThe project code is available at https://github.com/yokiwuuu/KrossFuse.\n", "link": "http://arxiv.org/abs/2506.08645v2", "date": "2025-10-30", "relevancy": 1.9429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Kernels%20Multiply%2C%20Clusters%20Unify%3A%20Fusing%20Embeddings%20with%20the%0A%20%20Kronecker%20Product&body=Title%3A%20When%20Kernels%20Multiply%2C%20Clusters%20Unify%3A%20Fusing%20Embeddings%20with%20the%0A%20%20Kronecker%20Product%0AAuthor%3A%20Youqi%20Wu%20and%20Jingwei%20Zhang%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20State-of-the-art%20embeddings%20often%20capture%20distinct%20yet%20complementary%0Adiscriminative%20features%3A%20For%20instance%2C%20one%20image%20embedding%20model%20may%20excel%20at%0Adistinguishing%20fine-grained%20textures%2C%20while%20another%20focuses%20on%20object-level%0Astructure.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20principled%20approach%20to%0Afuse%20such%20complementary%20representations%20through%20kernel%20multiplication.%0AMultiplying%20the%20kernel%20similarity%20functions%20of%20two%20embeddings%20allows%20their%0Adiscriminative%20structures%20to%20interact%2C%20producing%20a%20fused%20representation%20whose%0Akernel%20encodes%20the%20union%20of%20the%20clusters%20identified%20by%20each%20parent%20embedding.%0AThis%20formulation%20also%20provides%20a%20natural%20way%20to%20construct%20joint%20kernels%20for%0Apaired%20multi-modal%20data%20%28e.g.%2C%20image-text%20tuples%29%2C%20where%20the%20product%20of%0Amodality-specific%20kernels%20inherits%20structure%20from%20both%20domains.%20We%20highlight%0Athat%20this%20kernel%20product%20is%20mathematically%20realized%20via%20the%20Kronecker%20product%0Aof%20the%20embedding%20feature%20maps%2C%20yielding%20our%20proposed%20KrossFuse%20framework%20for%0Aembedding%20fusion.%20To%20address%20the%20computational%20cost%20of%20the%20resulting%0Ahigh-dimensional%20Kronecker%20space%2C%20we%20further%20develop%20RP-KrossFuse%2C%20a%20scalable%0Avariant%20that%20leverages%20random%20projections%20for%20efficient%20approximation.%20As%20a%20key%0Aapplication%2C%20we%20use%20this%20framework%20to%20bridge%20the%20performance%20gap%20between%0Across-modal%20embeddings%20%28e.g.%2C%20CLIP%2C%20BLIP%29%20and%20unimodal%20experts%20%28e.g.%2C%20DINOv2%2C%0AE5%29.%20Experiments%20show%20that%20RP-KrossFuse%20effectively%20integrates%20these%20models%2C%0Aenhancing%20modality-specific%20performance%20while%20preserving%20cross-modal%20alignment.%0AThe%20project%20code%20is%20available%20at%20https%3A//github.com/yokiwuuu/KrossFuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Kernels%2520Multiply%252C%2520Clusters%2520Unify%253A%2520Fusing%2520Embeddings%2520with%2520the%250A%2520%2520Kronecker%2520Product%26entry.906535625%3DYouqi%2520Wu%2520and%2520Jingwei%2520Zhang%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520State-of-the-art%2520embeddings%2520often%2520capture%2520distinct%2520yet%2520complementary%250Adiscriminative%2520features%253A%2520For%2520instance%252C%2520one%2520image%2520embedding%2520model%2520may%2520excel%2520at%250Adistinguishing%2520fine-grained%2520textures%252C%2520while%2520another%2520focuses%2520on%2520object-level%250Astructure.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520principled%2520approach%2520to%250Afuse%2520such%2520complementary%2520representations%2520through%2520kernel%2520multiplication.%250AMultiplying%2520the%2520kernel%2520similarity%2520functions%2520of%2520two%2520embeddings%2520allows%2520their%250Adiscriminative%2520structures%2520to%2520interact%252C%2520producing%2520a%2520fused%2520representation%2520whose%250Akernel%2520encodes%2520the%2520union%2520of%2520the%2520clusters%2520identified%2520by%2520each%2520parent%2520embedding.%250AThis%2520formulation%2520also%2520provides%2520a%2520natural%2520way%2520to%2520construct%2520joint%2520kernels%2520for%250Apaired%2520multi-modal%2520data%2520%2528e.g.%252C%2520image-text%2520tuples%2529%252C%2520where%2520the%2520product%2520of%250Amodality-specific%2520kernels%2520inherits%2520structure%2520from%2520both%2520domains.%2520We%2520highlight%250Athat%2520this%2520kernel%2520product%2520is%2520mathematically%2520realized%2520via%2520the%2520Kronecker%2520product%250Aof%2520the%2520embedding%2520feature%2520maps%252C%2520yielding%2520our%2520proposed%2520KrossFuse%2520framework%2520for%250Aembedding%2520fusion.%2520To%2520address%2520the%2520computational%2520cost%2520of%2520the%2520resulting%250Ahigh-dimensional%2520Kronecker%2520space%252C%2520we%2520further%2520develop%2520RP-KrossFuse%252C%2520a%2520scalable%250Avariant%2520that%2520leverages%2520random%2520projections%2520for%2520efficient%2520approximation.%2520As%2520a%2520key%250Aapplication%252C%2520we%2520use%2520this%2520framework%2520to%2520bridge%2520the%2520performance%2520gap%2520between%250Across-modal%2520embeddings%2520%2528e.g.%252C%2520CLIP%252C%2520BLIP%2529%2520and%2520unimodal%2520experts%2520%2528e.g.%252C%2520DINOv2%252C%250AE5%2529.%2520Experiments%2520show%2520that%2520RP-KrossFuse%2520effectively%2520integrates%2520these%2520models%252C%250Aenhancing%2520modality-specific%2520performance%2520while%2520preserving%2520cross-modal%2520alignment.%250AThe%2520project%2520code%2520is%2520available%2520at%2520https%253A//github.com/yokiwuuu/KrossFuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Kernels%20Multiply%2C%20Clusters%20Unify%3A%20Fusing%20Embeddings%20with%20the%0A%20%20Kronecker%20Product&entry.906535625=Youqi%20Wu%20and%20Jingwei%20Zhang%20and%20Farzan%20Farnia&entry.1292438233=%20%20State-of-the-art%20embeddings%20often%20capture%20distinct%20yet%20complementary%0Adiscriminative%20features%3A%20For%20instance%2C%20one%20image%20embedding%20model%20may%20excel%20at%0Adistinguishing%20fine-grained%20textures%2C%20while%20another%20focuses%20on%20object-level%0Astructure.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20principled%20approach%20to%0Afuse%20such%20complementary%20representations%20through%20kernel%20multiplication.%0AMultiplying%20the%20kernel%20similarity%20functions%20of%20two%20embeddings%20allows%20their%0Adiscriminative%20structures%20to%20interact%2C%20producing%20a%20fused%20representation%20whose%0Akernel%20encodes%20the%20union%20of%20the%20clusters%20identified%20by%20each%20parent%20embedding.%0AThis%20formulation%20also%20provides%20a%20natural%20way%20to%20construct%20joint%20kernels%20for%0Apaired%20multi-modal%20data%20%28e.g.%2C%20image-text%20tuples%29%2C%20where%20the%20product%20of%0Amodality-specific%20kernels%20inherits%20structure%20from%20both%20domains.%20We%20highlight%0Athat%20this%20kernel%20product%20is%20mathematically%20realized%20via%20the%20Kronecker%20product%0Aof%20the%20embedding%20feature%20maps%2C%20yielding%20our%20proposed%20KrossFuse%20framework%20for%0Aembedding%20fusion.%20To%20address%20the%20computational%20cost%20of%20the%20resulting%0Ahigh-dimensional%20Kronecker%20space%2C%20we%20further%20develop%20RP-KrossFuse%2C%20a%20scalable%0Avariant%20that%20leverages%20random%20projections%20for%20efficient%20approximation.%20As%20a%20key%0Aapplication%2C%20we%20use%20this%20framework%20to%20bridge%20the%20performance%20gap%20between%0Across-modal%20embeddings%20%28e.g.%2C%20CLIP%2C%20BLIP%29%20and%20unimodal%20experts%20%28e.g.%2C%20DINOv2%2C%0AE5%29.%20Experiments%20show%20that%20RP-KrossFuse%20effectively%20integrates%20these%20models%2C%0Aenhancing%20modality-specific%20performance%20while%20preserving%20cross-modal%20alignment.%0AThe%20project%20code%20is%20available%20at%20https%3A//github.com/yokiwuuu/KrossFuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08645v2&entry.124074799=Read"},
{"title": "GSE: Group-wise Sparse and Explainable Adversarial Attacks", "author": "Shpresim Sadiku and Moritz Wagner and Sebastian Pokutta", "abstract": "  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n", "link": "http://arxiv.org/abs/2311.17434v5", "date": "2025-10-30", "relevancy": 1.9315, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4885}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4847}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSE%3A%20Group-wise%20Sparse%20and%20Explainable%20Adversarial%20Attacks&body=Title%3A%20GSE%3A%20Group-wise%20Sparse%20and%20Explainable%20Adversarial%20Attacks%0AAuthor%3A%20Shpresim%20Sadiku%20and%20Moritz%20Wagner%20and%20Sebastian%20Pokutta%0AAbstract%3A%20%20%20Sparse%20adversarial%20attacks%20fool%20deep%20neural%20networks%20%28DNNs%29%20through%20minimal%0Apixel%20perturbations%2C%20often%20regularized%20by%20the%20%24%5Cell_0%24%20norm.%20Recent%20efforts%0Ahave%20replaced%20this%20norm%20with%20a%20structural%20sparsity%20regularizer%2C%20such%20as%20the%0Anuclear%20group%20norm%2C%20to%20craft%20group-wise%20sparse%20adversarial%20attacks.%20The%0Aresulting%20perturbations%20are%20thus%20explainable%20and%20hold%20significant%20practical%0Arelevance%2C%20shedding%20light%20on%20an%20even%20greater%20vulnerability%20of%20DNNs.%20However%2C%0Acrafting%20such%20attacks%20poses%20an%20optimization%20challenge%2C%20as%20it%20involves%20computing%0Anorms%20for%20groups%20of%20pixels%20within%20a%20non-convex%20objective.%20We%20address%20this%20by%0Apresenting%20a%20two-phase%20algorithm%20that%20generates%20group-wise%20sparse%20attacks%0Awithin%20semantically%20meaningful%20areas%20of%20an%20image.%20Initially%2C%20we%20optimize%20a%0Aquasinorm%20adversarial%20loss%20using%20the%20%241/2-%24quasinorm%20proximal%20operator%20tailored%0Afor%20non-convex%20programming.%20Subsequently%2C%20the%20algorithm%20transitions%20to%20a%0Aprojected%20Nesterov%27s%20accelerated%20gradient%20descent%20with%20%242-%24norm%20regularization%0Aapplied%20to%20perturbation%20magnitudes.%20Rigorous%20evaluations%20on%20CIFAR-10%20and%0AImageNet%20datasets%20demonstrate%20a%20remarkable%20increase%20in%20group-wise%20sparsity%2C%0Ae.g.%2C%20%2450.9%5C%25%24%20on%20CIFAR-10%20and%20%2438.4%5C%25%24%20on%20ImageNet%20%28average%20case%2C%20targeted%0Aattack%29.%20This%20performance%20improvement%20is%20accompanied%20by%20significantly%20faster%0Acomputation%20times%2C%20improved%20explainability%2C%20and%20a%20%24100%5C%25%24%20attack%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17434v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSE%253A%2520Group-wise%2520Sparse%2520and%2520Explainable%2520Adversarial%2520Attacks%26entry.906535625%3DShpresim%2520Sadiku%2520and%2520Moritz%2520Wagner%2520and%2520Sebastian%2520Pokutta%26entry.1292438233%3D%2520%2520Sparse%2520adversarial%2520attacks%2520fool%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520through%2520minimal%250Apixel%2520perturbations%252C%2520often%2520regularized%2520by%2520the%2520%2524%255Cell_0%2524%2520norm.%2520Recent%2520efforts%250Ahave%2520replaced%2520this%2520norm%2520with%2520a%2520structural%2520sparsity%2520regularizer%252C%2520such%2520as%2520the%250Anuclear%2520group%2520norm%252C%2520to%2520craft%2520group-wise%2520sparse%2520adversarial%2520attacks.%2520The%250Aresulting%2520perturbations%2520are%2520thus%2520explainable%2520and%2520hold%2520significant%2520practical%250Arelevance%252C%2520shedding%2520light%2520on%2520an%2520even%2520greater%2520vulnerability%2520of%2520DNNs.%2520However%252C%250Acrafting%2520such%2520attacks%2520poses%2520an%2520optimization%2520challenge%252C%2520as%2520it%2520involves%2520computing%250Anorms%2520for%2520groups%2520of%2520pixels%2520within%2520a%2520non-convex%2520objective.%2520We%2520address%2520this%2520by%250Apresenting%2520a%2520two-phase%2520algorithm%2520that%2520generates%2520group-wise%2520sparse%2520attacks%250Awithin%2520semantically%2520meaningful%2520areas%2520of%2520an%2520image.%2520Initially%252C%2520we%2520optimize%2520a%250Aquasinorm%2520adversarial%2520loss%2520using%2520the%2520%25241/2-%2524quasinorm%2520proximal%2520operator%2520tailored%250Afor%2520non-convex%2520programming.%2520Subsequently%252C%2520the%2520algorithm%2520transitions%2520to%2520a%250Aprojected%2520Nesterov%2527s%2520accelerated%2520gradient%2520descent%2520with%2520%25242-%2524norm%2520regularization%250Aapplied%2520to%2520perturbation%2520magnitudes.%2520Rigorous%2520evaluations%2520on%2520CIFAR-10%2520and%250AImageNet%2520datasets%2520demonstrate%2520a%2520remarkable%2520increase%2520in%2520group-wise%2520sparsity%252C%250Ae.g.%252C%2520%252450.9%255C%2525%2524%2520on%2520CIFAR-10%2520and%2520%252438.4%255C%2525%2524%2520on%2520ImageNet%2520%2528average%2520case%252C%2520targeted%250Aattack%2529.%2520This%2520performance%2520improvement%2520is%2520accompanied%2520by%2520significantly%2520faster%250Acomputation%2520times%252C%2520improved%2520explainability%252C%2520and%2520a%2520%2524100%255C%2525%2524%2520attack%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17434v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSE%3A%20Group-wise%20Sparse%20and%20Explainable%20Adversarial%20Attacks&entry.906535625=Shpresim%20Sadiku%20and%20Moritz%20Wagner%20and%20Sebastian%20Pokutta&entry.1292438233=%20%20Sparse%20adversarial%20attacks%20fool%20deep%20neural%20networks%20%28DNNs%29%20through%20minimal%0Apixel%20perturbations%2C%20often%20regularized%20by%20the%20%24%5Cell_0%24%20norm.%20Recent%20efforts%0Ahave%20replaced%20this%20norm%20with%20a%20structural%20sparsity%20regularizer%2C%20such%20as%20the%0Anuclear%20group%20norm%2C%20to%20craft%20group-wise%20sparse%20adversarial%20attacks.%20The%0Aresulting%20perturbations%20are%20thus%20explainable%20and%20hold%20significant%20practical%0Arelevance%2C%20shedding%20light%20on%20an%20even%20greater%20vulnerability%20of%20DNNs.%20However%2C%0Acrafting%20such%20attacks%20poses%20an%20optimization%20challenge%2C%20as%20it%20involves%20computing%0Anorms%20for%20groups%20of%20pixels%20within%20a%20non-convex%20objective.%20We%20address%20this%20by%0Apresenting%20a%20two-phase%20algorithm%20that%20generates%20group-wise%20sparse%20attacks%0Awithin%20semantically%20meaningful%20areas%20of%20an%20image.%20Initially%2C%20we%20optimize%20a%0Aquasinorm%20adversarial%20loss%20using%20the%20%241/2-%24quasinorm%20proximal%20operator%20tailored%0Afor%20non-convex%20programming.%20Subsequently%2C%20the%20algorithm%20transitions%20to%20a%0Aprojected%20Nesterov%27s%20accelerated%20gradient%20descent%20with%20%242-%24norm%20regularization%0Aapplied%20to%20perturbation%20magnitudes.%20Rigorous%20evaluations%20on%20CIFAR-10%20and%0AImageNet%20datasets%20demonstrate%20a%20remarkable%20increase%20in%20group-wise%20sparsity%2C%0Ae.g.%2C%20%2450.9%5C%25%24%20on%20CIFAR-10%20and%20%2438.4%5C%25%24%20on%20ImageNet%20%28average%20case%2C%20targeted%0Aattack%29.%20This%20performance%20improvement%20is%20accompanied%20by%20significantly%20faster%0Acomputation%20times%2C%20improved%20explainability%2C%20and%20a%20%24100%5C%25%24%20attack%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17434v5&entry.124074799=Read"},
{"title": "Partially-Supervised Neural Network Model For Quadratic Multiparametric\n  Programming", "author": "Fuat Can Beylunioglu and Mehrdad Pirnia and P. Robert Duimering", "abstract": "  Neural Networks (NN) with ReLU activation functions are used to model\nmultiparametric quadratic optimization problems (mp-QP) in diverse engineering\napplications. Researchers have suggested leveraging the piecewise affine\nproperty of deep NN models to solve mp-QP with linear constraints, which also\nexhibit piecewise affine behaviour. However, traditional deep NN applications\nto mp-QP fall short of providing optimal and feasible predictions, even when\ntrained on large datasets. This study proposes a partially-supervised NN (PSNN)\narchitecture that directly represents the mathematical structure of the global\nsolution function. In contrast to generic NN training approaches, the proposed\nPSNN method derives a large proportion of model weights directly from the\nmathematical properties of the optimization problem, producing more accurate\nsolutions despite significantly smaller training data sets. Many energy\nmanagement problems are formulated as QP, so we apply the proposed approach to\nenergy systems (specifically DC optimal power flow) to demonstrate proof of\nconcept. Model performance in terms of solution accuracy and speed of\npredictions was compared against a commercial solver and a generic Deep NN\nmodel based on classical training. Results show KKT sufficient conditions for\nPSNN consistently outperform generic NN architectures with classical training\nusing far less data, including when tested on extreme, out-of-training\ndistribution test data. Given its speed advantages over traditional solvers,\nthe PSNN model can quickly produce optimal and feasible solutions within a\nsecond for millions of input parameters sampled from a distribution of\nstochastic demands and renewable generator dispatches, which can be used for\nsimulations and long term planning.\n", "link": "http://arxiv.org/abs/2506.05567v2", "date": "2025-10-30", "relevancy": 1.9262, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4934}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partially-Supervised%20Neural%20Network%20Model%20For%20Quadratic%20Multiparametric%0A%20%20Programming&body=Title%3A%20Partially-Supervised%20Neural%20Network%20Model%20For%20Quadratic%20Multiparametric%0A%20%20Programming%0AAuthor%3A%20Fuat%20Can%20Beylunioglu%20and%20Mehrdad%20Pirnia%20and%20P.%20Robert%20Duimering%0AAbstract%3A%20%20%20Neural%20Networks%20%28NN%29%20with%20ReLU%20activation%20functions%20are%20used%20to%20model%0Amultiparametric%20quadratic%20optimization%20problems%20%28mp-QP%29%20in%20diverse%20engineering%0Aapplications.%20Researchers%20have%20suggested%20leveraging%20the%20piecewise%20affine%0Aproperty%20of%20deep%20NN%20models%20to%20solve%20mp-QP%20with%20linear%20constraints%2C%20which%20also%0Aexhibit%20piecewise%20affine%20behaviour.%20However%2C%20traditional%20deep%20NN%20applications%0Ato%20mp-QP%20fall%20short%20of%20providing%20optimal%20and%20feasible%20predictions%2C%20even%20when%0Atrained%20on%20large%20datasets.%20This%20study%20proposes%20a%20partially-supervised%20NN%20%28PSNN%29%0Aarchitecture%20that%20directly%20represents%20the%20mathematical%20structure%20of%20the%20global%0Asolution%20function.%20In%20contrast%20to%20generic%20NN%20training%20approaches%2C%20the%20proposed%0APSNN%20method%20derives%20a%20large%20proportion%20of%20model%20weights%20directly%20from%20the%0Amathematical%20properties%20of%20the%20optimization%20problem%2C%20producing%20more%20accurate%0Asolutions%20despite%20significantly%20smaller%20training%20data%20sets.%20Many%20energy%0Amanagement%20problems%20are%20formulated%20as%20QP%2C%20so%20we%20apply%20the%20proposed%20approach%20to%0Aenergy%20systems%20%28specifically%20DC%20optimal%20power%20flow%29%20to%20demonstrate%20proof%20of%0Aconcept.%20Model%20performance%20in%20terms%20of%20solution%20accuracy%20and%20speed%20of%0Apredictions%20was%20compared%20against%20a%20commercial%20solver%20and%20a%20generic%20Deep%20NN%0Amodel%20based%20on%20classical%20training.%20Results%20show%20KKT%20sufficient%20conditions%20for%0APSNN%20consistently%20outperform%20generic%20NN%20architectures%20with%20classical%20training%0Ausing%20far%20less%20data%2C%20including%20when%20tested%20on%20extreme%2C%20out-of-training%0Adistribution%20test%20data.%20Given%20its%20speed%20advantages%20over%20traditional%20solvers%2C%0Athe%20PSNN%20model%20can%20quickly%20produce%20optimal%20and%20feasible%20solutions%20within%20a%0Asecond%20for%20millions%20of%20input%20parameters%20sampled%20from%20a%20distribution%20of%0Astochastic%20demands%20and%20renewable%20generator%20dispatches%2C%20which%20can%20be%20used%20for%0Asimulations%20and%20long%20term%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartially-Supervised%2520Neural%2520Network%2520Model%2520For%2520Quadratic%2520Multiparametric%250A%2520%2520Programming%26entry.906535625%3DFuat%2520Can%2520Beylunioglu%2520and%2520Mehrdad%2520Pirnia%2520and%2520P.%2520Robert%2520Duimering%26entry.1292438233%3D%2520%2520Neural%2520Networks%2520%2528NN%2529%2520with%2520ReLU%2520activation%2520functions%2520are%2520used%2520to%2520model%250Amultiparametric%2520quadratic%2520optimization%2520problems%2520%2528mp-QP%2529%2520in%2520diverse%2520engineering%250Aapplications.%2520Researchers%2520have%2520suggested%2520leveraging%2520the%2520piecewise%2520affine%250Aproperty%2520of%2520deep%2520NN%2520models%2520to%2520solve%2520mp-QP%2520with%2520linear%2520constraints%252C%2520which%2520also%250Aexhibit%2520piecewise%2520affine%2520behaviour.%2520However%252C%2520traditional%2520deep%2520NN%2520applications%250Ato%2520mp-QP%2520fall%2520short%2520of%2520providing%2520optimal%2520and%2520feasible%2520predictions%252C%2520even%2520when%250Atrained%2520on%2520large%2520datasets.%2520This%2520study%2520proposes%2520a%2520partially-supervised%2520NN%2520%2528PSNN%2529%250Aarchitecture%2520that%2520directly%2520represents%2520the%2520mathematical%2520structure%2520of%2520the%2520global%250Asolution%2520function.%2520In%2520contrast%2520to%2520generic%2520NN%2520training%2520approaches%252C%2520the%2520proposed%250APSNN%2520method%2520derives%2520a%2520large%2520proportion%2520of%2520model%2520weights%2520directly%2520from%2520the%250Amathematical%2520properties%2520of%2520the%2520optimization%2520problem%252C%2520producing%2520more%2520accurate%250Asolutions%2520despite%2520significantly%2520smaller%2520training%2520data%2520sets.%2520Many%2520energy%250Amanagement%2520problems%2520are%2520formulated%2520as%2520QP%252C%2520so%2520we%2520apply%2520the%2520proposed%2520approach%2520to%250Aenergy%2520systems%2520%2528specifically%2520DC%2520optimal%2520power%2520flow%2529%2520to%2520demonstrate%2520proof%2520of%250Aconcept.%2520Model%2520performance%2520in%2520terms%2520of%2520solution%2520accuracy%2520and%2520speed%2520of%250Apredictions%2520was%2520compared%2520against%2520a%2520commercial%2520solver%2520and%2520a%2520generic%2520Deep%2520NN%250Amodel%2520based%2520on%2520classical%2520training.%2520Results%2520show%2520KKT%2520sufficient%2520conditions%2520for%250APSNN%2520consistently%2520outperform%2520generic%2520NN%2520architectures%2520with%2520classical%2520training%250Ausing%2520far%2520less%2520data%252C%2520including%2520when%2520tested%2520on%2520extreme%252C%2520out-of-training%250Adistribution%2520test%2520data.%2520Given%2520its%2520speed%2520advantages%2520over%2520traditional%2520solvers%252C%250Athe%2520PSNN%2520model%2520can%2520quickly%2520produce%2520optimal%2520and%2520feasible%2520solutions%2520within%2520a%250Asecond%2520for%2520millions%2520of%2520input%2520parameters%2520sampled%2520from%2520a%2520distribution%2520of%250Astochastic%2520demands%2520and%2520renewable%2520generator%2520dispatches%252C%2520which%2520can%2520be%2520used%2520for%250Asimulations%2520and%2520long%2520term%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partially-Supervised%20Neural%20Network%20Model%20For%20Quadratic%20Multiparametric%0A%20%20Programming&entry.906535625=Fuat%20Can%20Beylunioglu%20and%20Mehrdad%20Pirnia%20and%20P.%20Robert%20Duimering&entry.1292438233=%20%20Neural%20Networks%20%28NN%29%20with%20ReLU%20activation%20functions%20are%20used%20to%20model%0Amultiparametric%20quadratic%20optimization%20problems%20%28mp-QP%29%20in%20diverse%20engineering%0Aapplications.%20Researchers%20have%20suggested%20leveraging%20the%20piecewise%20affine%0Aproperty%20of%20deep%20NN%20models%20to%20solve%20mp-QP%20with%20linear%20constraints%2C%20which%20also%0Aexhibit%20piecewise%20affine%20behaviour.%20However%2C%20traditional%20deep%20NN%20applications%0Ato%20mp-QP%20fall%20short%20of%20providing%20optimal%20and%20feasible%20predictions%2C%20even%20when%0Atrained%20on%20large%20datasets.%20This%20study%20proposes%20a%20partially-supervised%20NN%20%28PSNN%29%0Aarchitecture%20that%20directly%20represents%20the%20mathematical%20structure%20of%20the%20global%0Asolution%20function.%20In%20contrast%20to%20generic%20NN%20training%20approaches%2C%20the%20proposed%0APSNN%20method%20derives%20a%20large%20proportion%20of%20model%20weights%20directly%20from%20the%0Amathematical%20properties%20of%20the%20optimization%20problem%2C%20producing%20more%20accurate%0Asolutions%20despite%20significantly%20smaller%20training%20data%20sets.%20Many%20energy%0Amanagement%20problems%20are%20formulated%20as%20QP%2C%20so%20we%20apply%20the%20proposed%20approach%20to%0Aenergy%20systems%20%28specifically%20DC%20optimal%20power%20flow%29%20to%20demonstrate%20proof%20of%0Aconcept.%20Model%20performance%20in%20terms%20of%20solution%20accuracy%20and%20speed%20of%0Apredictions%20was%20compared%20against%20a%20commercial%20solver%20and%20a%20generic%20Deep%20NN%0Amodel%20based%20on%20classical%20training.%20Results%20show%20KKT%20sufficient%20conditions%20for%0APSNN%20consistently%20outperform%20generic%20NN%20architectures%20with%20classical%20training%0Ausing%20far%20less%20data%2C%20including%20when%20tested%20on%20extreme%2C%20out-of-training%0Adistribution%20test%20data.%20Given%20its%20speed%20advantages%20over%20traditional%20solvers%2C%0Athe%20PSNN%20model%20can%20quickly%20produce%20optimal%20and%20feasible%20solutions%20within%20a%0Asecond%20for%20millions%20of%20input%20parameters%20sampled%20from%20a%20distribution%20of%0Astochastic%20demands%20and%20renewable%20generator%20dispatches%2C%20which%20can%20be%20used%20for%0Asimulations%20and%20long%20term%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05567v2&entry.124074799=Read"},
{"title": "Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning", "author": "Derin Cayir and Renjie Tao and Rashi Rungta and Kai Sun and Sean Chen and Haidar Khan and Minseok Kim and Julia Reinspach and Yue Liu", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.\n", "link": "http://arxiv.org/abs/2508.01543v2", "date": "2025-10-30", "relevancy": 1.925, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refine-n-Judge%3A%20Curating%20High-Quality%20Preference%20Chains%20for%0A%20%20LLM-Fine-Tuning&body=Title%3A%20Refine-n-Judge%3A%20Curating%20High-Quality%20Preference%20Chains%20for%0A%20%20LLM-Fine-Tuning%0AAuthor%3A%20Derin%20Cayir%20and%20Renjie%20Tao%20and%20Rashi%20Rungta%20and%20Kai%20Sun%20and%20Sean%20Chen%20and%20Haidar%20Khan%20and%20Minseok%20Kim%20and%20Julia%20Reinspach%20and%20Yue%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20progress%20through%0Apreference-based%20fine-tuning%2C%20which%20critically%20depends%20on%20the%20quality%20of%20the%0Aunderlying%20training%20data.%20While%20human%20feedback%20is%20essential%20for%20improving%20data%0Aquality%2C%20it%20is%20costly%20and%20does%20not%20scale%20well.%20In%20this%20paper%2C%20we%20introduce%0ARefine-n-Judge%2C%20an%20automated%20iterative%20approach%20that%20leverages%20a%20single%20LLM%20as%0Aboth%20a%20refiner%20and%20a%20judge%20to%20enhance%20dataset%20quality.%20Unlike%20existing%0Aiterative%20refinement%20methods%2C%20Refine-n-Judge%20employs%20an%20LLM%20to%20both%20generate%0Arefinements%20and%20explicitly%20evaluate%20each%20improvement%2C%20ensuring%20that%20every%0Aiteration%20meaningfully%20enhances%20the%20dataset%20without%20requiring%20additional%20human%0Aannotation%20or%20a%20separate%20reward%20model.%20At%20each%20step%2C%20the%20LLM%20refines%20a%20response%0Aand%20judges%20whether%20the%20refinement%20is%20an%20improvement%20over%20the%20previous%20answer.%0AThis%20process%20continues%20until%20the%20LLM%20prefers%20the%20initial%20answer%20over%20the%0Arefinement%2C%20indicating%20no%20further%20improvements.%20This%20produces%20sequences%20of%0Aincreasing%20quality%2C%20preference-labeled%20responses%20ideal%20for%20fine-tuning.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20Refine-n-Judge%20across%20a%20range%20of%20public%0Adatasets%20spanning%20five%20corpora%2C%20targeting%20tasks%20such%20as%20coding%2C%20math%2C%20and%0Aconversation.%20Models%20%28Llama%203.1-8B%20and%20Llama%203.3-70B%29%20fine-tuned%20on%0ARefine-n-Judge-enhanced%20datasets%20were%20preferred%20by%20LLM%20judges%20in%20over%2074%25%20of%0Acomparisons%20against%20models%20tuned%20on%20the%20original%20dataset%20by%20GPT-4.%0AAdditionally%2C%20we%20report%20performance%20gains%3A%20%2B5%25%20on%20AlpacaEval%20and%20AlpacaEval%0A2.0%2C%20and%20%2B19%25%20on%20MT-Bench.%20Our%20results%20indicate%20that%20Refine-n-Judge%20produces%0Ahigh-quality%20datasets%20and%20scalable%20model%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefine-n-Judge%253A%2520Curating%2520High-Quality%2520Preference%2520Chains%2520for%250A%2520%2520LLM-Fine-Tuning%26entry.906535625%3DDerin%2520Cayir%2520and%2520Renjie%2520Tao%2520and%2520Rashi%2520Rungta%2520and%2520Kai%2520Sun%2520and%2520Sean%2520Chen%2520and%2520Haidar%2520Khan%2520and%2520Minseok%2520Kim%2520and%2520Julia%2520Reinspach%2520and%2520Yue%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520progress%2520through%250Apreference-based%2520fine-tuning%252C%2520which%2520critically%2520depends%2520on%2520the%2520quality%2520of%2520the%250Aunderlying%2520training%2520data.%2520While%2520human%2520feedback%2520is%2520essential%2520for%2520improving%2520data%250Aquality%252C%2520it%2520is%2520costly%2520and%2520does%2520not%2520scale%2520well.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ARefine-n-Judge%252C%2520an%2520automated%2520iterative%2520approach%2520that%2520leverages%2520a%2520single%2520LLM%2520as%250Aboth%2520a%2520refiner%2520and%2520a%2520judge%2520to%2520enhance%2520dataset%2520quality.%2520Unlike%2520existing%250Aiterative%2520refinement%2520methods%252C%2520Refine-n-Judge%2520employs%2520an%2520LLM%2520to%2520both%2520generate%250Arefinements%2520and%2520explicitly%2520evaluate%2520each%2520improvement%252C%2520ensuring%2520that%2520every%250Aiteration%2520meaningfully%2520enhances%2520the%2520dataset%2520without%2520requiring%2520additional%2520human%250Aannotation%2520or%2520a%2520separate%2520reward%2520model.%2520At%2520each%2520step%252C%2520the%2520LLM%2520refines%2520a%2520response%250Aand%2520judges%2520whether%2520the%2520refinement%2520is%2520an%2520improvement%2520over%2520the%2520previous%2520answer.%250AThis%2520process%2520continues%2520until%2520the%2520LLM%2520prefers%2520the%2520initial%2520answer%2520over%2520the%250Arefinement%252C%2520indicating%2520no%2520further%2520improvements.%2520This%2520produces%2520sequences%2520of%250Aincreasing%2520quality%252C%2520preference-labeled%2520responses%2520ideal%2520for%2520fine-tuning.%250A%2520%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520Refine-n-Judge%2520across%2520a%2520range%2520of%2520public%250Adatasets%2520spanning%2520five%2520corpora%252C%2520targeting%2520tasks%2520such%2520as%2520coding%252C%2520math%252C%2520and%250Aconversation.%2520Models%2520%2528Llama%25203.1-8B%2520and%2520Llama%25203.3-70B%2529%2520fine-tuned%2520on%250ARefine-n-Judge-enhanced%2520datasets%2520were%2520preferred%2520by%2520LLM%2520judges%2520in%2520over%252074%2525%2520of%250Acomparisons%2520against%2520models%2520tuned%2520on%2520the%2520original%2520dataset%2520by%2520GPT-4.%250AAdditionally%252C%2520we%2520report%2520performance%2520gains%253A%2520%252B5%2525%2520on%2520AlpacaEval%2520and%2520AlpacaEval%250A2.0%252C%2520and%2520%252B19%2525%2520on%2520MT-Bench.%2520Our%2520results%2520indicate%2520that%2520Refine-n-Judge%2520produces%250Ahigh-quality%2520datasets%2520and%2520scalable%2520model%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refine-n-Judge%3A%20Curating%20High-Quality%20Preference%20Chains%20for%0A%20%20LLM-Fine-Tuning&entry.906535625=Derin%20Cayir%20and%20Renjie%20Tao%20and%20Rashi%20Rungta%20and%20Kai%20Sun%20and%20Sean%20Chen%20and%20Haidar%20Khan%20and%20Minseok%20Kim%20and%20Julia%20Reinspach%20and%20Yue%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20progress%20through%0Apreference-based%20fine-tuning%2C%20which%20critically%20depends%20on%20the%20quality%20of%20the%0Aunderlying%20training%20data.%20While%20human%20feedback%20is%20essential%20for%20improving%20data%0Aquality%2C%20it%20is%20costly%20and%20does%20not%20scale%20well.%20In%20this%20paper%2C%20we%20introduce%0ARefine-n-Judge%2C%20an%20automated%20iterative%20approach%20that%20leverages%20a%20single%20LLM%20as%0Aboth%20a%20refiner%20and%20a%20judge%20to%20enhance%20dataset%20quality.%20Unlike%20existing%0Aiterative%20refinement%20methods%2C%20Refine-n-Judge%20employs%20an%20LLM%20to%20both%20generate%0Arefinements%20and%20explicitly%20evaluate%20each%20improvement%2C%20ensuring%20that%20every%0Aiteration%20meaningfully%20enhances%20the%20dataset%20without%20requiring%20additional%20human%0Aannotation%20or%20a%20separate%20reward%20model.%20At%20each%20step%2C%20the%20LLM%20refines%20a%20response%0Aand%20judges%20whether%20the%20refinement%20is%20an%20improvement%20over%20the%20previous%20answer.%0AThis%20process%20continues%20until%20the%20LLM%20prefers%20the%20initial%20answer%20over%20the%0Arefinement%2C%20indicating%20no%20further%20improvements.%20This%20produces%20sequences%20of%0Aincreasing%20quality%2C%20preference-labeled%20responses%20ideal%20for%20fine-tuning.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20Refine-n-Judge%20across%20a%20range%20of%20public%0Adatasets%20spanning%20five%20corpora%2C%20targeting%20tasks%20such%20as%20coding%2C%20math%2C%20and%0Aconversation.%20Models%20%28Llama%203.1-8B%20and%20Llama%203.3-70B%29%20fine-tuned%20on%0ARefine-n-Judge-enhanced%20datasets%20were%20preferred%20by%20LLM%20judges%20in%20over%2074%25%20of%0Acomparisons%20against%20models%20tuned%20on%20the%20original%20dataset%20by%20GPT-4.%0AAdditionally%2C%20we%20report%20performance%20gains%3A%20%2B5%25%20on%20AlpacaEval%20and%20AlpacaEval%0A2.0%2C%20and%20%2B19%25%20on%20MT-Bench.%20Our%20results%20indicate%20that%20Refine-n-Judge%20produces%0Ahigh-quality%20datasets%20and%20scalable%20model%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01543v2&entry.124074799=Read"},
{"title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions", "author": "Shengnan An and Xunliang Cai and Xuezhi Cao and Xiaoyu Li and Yehao Lin and Junlin Liu and Xinxuan Lv and Dan Ma and Xuanlin Wang and Ziwen Wang and Shuang Zhou", "abstract": "  We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/\n", "link": "http://arxiv.org/abs/2510.26768v1", "date": "2025-10-30", "relevancy": 1.9149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMO-Bench%3A%20Large%20Language%20Models%20Still%20Struggle%20in%20High%20School%20Math%0A%20%20Competitions&body=Title%3A%20AMO-Bench%3A%20Large%20Language%20Models%20Still%20Struggle%20in%20High%20School%20Math%0A%20%20Competitions%0AAuthor%3A%20Shengnan%20An%20and%20Xunliang%20Cai%20and%20Xuezhi%20Cao%20and%20Xiaoyu%20Li%20and%20Yehao%20Lin%20and%20Junlin%20Liu%20and%20Xinxuan%20Lv%20and%20Dan%20Ma%20and%20Xuanlin%20Wang%20and%20Ziwen%20Wang%20and%20Shuang%20Zhou%0AAbstract%3A%20%20%20We%20present%20AMO-Bench%2C%20an%20Advanced%20Mathematical%20reasoning%20benchmark%20with%0AOlympiad%20level%20or%20even%20higher%20difficulty%2C%20comprising%2050%20human-crafted%20problems.%0AExisting%20benchmarks%20have%20widely%20leveraged%20high%20school%20math%20competitions%20for%0Aevaluating%20mathematical%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20many%20existing%20math%20competitions%20are%20becoming%20less%20effective%20for%0Aassessing%20top-tier%20LLMs%20due%20to%20performance%20saturation%20%28e.g.%2C%20AIME24/25%29.%20To%0Aaddress%20this%2C%20AMO-Bench%20introduces%20more%20rigorous%20challenges%20by%20ensuring%20all%2050%0Aproblems%20are%20%281%29%20cross-validated%20by%20experts%20to%20meet%20at%20least%20the%20International%0AMathematical%20Olympiad%20%28IMO%29%20difficulty%20standards%2C%20and%20%282%29%20entirely%20original%0Aproblems%20to%20prevent%20potential%20performance%20leakages%20from%20data%20memorization.%0AMoreover%2C%20each%20problem%20in%20AMO-Bench%20requires%20only%20a%20final%20answer%20rather%20than%20a%0Aproof%2C%20enabling%20automatic%20and%20robust%20grading%20for%20evaluation.%20Experimental%0Aresults%20across%2026%20LLMs%20on%20AMO-Bench%20show%20that%20even%20the%20best-performing%20model%0Aachieves%20only%2052.4%25%20accuracy%20on%20AMO-Bench%2C%20with%20most%20LLMs%20scoring%20below%2040%25.%0ABeyond%20these%20poor%20performances%2C%20our%20further%20analysis%20reveals%20a%20promising%0Ascaling%20trend%20with%20increasing%20test-time%20compute%20on%20AMO-Bench.%20These%20results%0Ahighlight%20the%20significant%20room%20for%20improving%20the%20mathematical%20reasoning%20in%0Acurrent%20LLMs.%20We%20release%20AMO-Bench%20to%20facilitate%20further%20research%20into%0Aadvancing%20the%20reasoning%20abilities%20of%20language%20models.%0Ahttps%3A//amo-bench.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMO-Bench%253A%2520Large%2520Language%2520Models%2520Still%2520Struggle%2520in%2520High%2520School%2520Math%250A%2520%2520Competitions%26entry.906535625%3DShengnan%2520An%2520and%2520Xunliang%2520Cai%2520and%2520Xuezhi%2520Cao%2520and%2520Xiaoyu%2520Li%2520and%2520Yehao%2520Lin%2520and%2520Junlin%2520Liu%2520and%2520Xinxuan%2520Lv%2520and%2520Dan%2520Ma%2520and%2520Xuanlin%2520Wang%2520and%2520Ziwen%2520Wang%2520and%2520Shuang%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520AMO-Bench%252C%2520an%2520Advanced%2520Mathematical%2520reasoning%2520benchmark%2520with%250AOlympiad%2520level%2520or%2520even%2520higher%2520difficulty%252C%2520comprising%252050%2520human-crafted%2520problems.%250AExisting%2520benchmarks%2520have%2520widely%2520leveraged%2520high%2520school%2520math%2520competitions%2520for%250Aevaluating%2520mathematical%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520many%2520existing%2520math%2520competitions%2520are%2520becoming%2520less%2520effective%2520for%250Aassessing%2520top-tier%2520LLMs%2520due%2520to%2520performance%2520saturation%2520%2528e.g.%252C%2520AIME24/25%2529.%2520To%250Aaddress%2520this%252C%2520AMO-Bench%2520introduces%2520more%2520rigorous%2520challenges%2520by%2520ensuring%2520all%252050%250Aproblems%2520are%2520%25281%2529%2520cross-validated%2520by%2520experts%2520to%2520meet%2520at%2520least%2520the%2520International%250AMathematical%2520Olympiad%2520%2528IMO%2529%2520difficulty%2520standards%252C%2520and%2520%25282%2529%2520entirely%2520original%250Aproblems%2520to%2520prevent%2520potential%2520performance%2520leakages%2520from%2520data%2520memorization.%250AMoreover%252C%2520each%2520problem%2520in%2520AMO-Bench%2520requires%2520only%2520a%2520final%2520answer%2520rather%2520than%2520a%250Aproof%252C%2520enabling%2520automatic%2520and%2520robust%2520grading%2520for%2520evaluation.%2520Experimental%250Aresults%2520across%252026%2520LLMs%2520on%2520AMO-Bench%2520show%2520that%2520even%2520the%2520best-performing%2520model%250Aachieves%2520only%252052.4%2525%2520accuracy%2520on%2520AMO-Bench%252C%2520with%2520most%2520LLMs%2520scoring%2520below%252040%2525.%250ABeyond%2520these%2520poor%2520performances%252C%2520our%2520further%2520analysis%2520reveals%2520a%2520promising%250Ascaling%2520trend%2520with%2520increasing%2520test-time%2520compute%2520on%2520AMO-Bench.%2520These%2520results%250Ahighlight%2520the%2520significant%2520room%2520for%2520improving%2520the%2520mathematical%2520reasoning%2520in%250Acurrent%2520LLMs.%2520We%2520release%2520AMO-Bench%2520to%2520facilitate%2520further%2520research%2520into%250Aadvancing%2520the%2520reasoning%2520abilities%2520of%2520language%2520models.%250Ahttps%253A//amo-bench.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMO-Bench%3A%20Large%20Language%20Models%20Still%20Struggle%20in%20High%20School%20Math%0A%20%20Competitions&entry.906535625=Shengnan%20An%20and%20Xunliang%20Cai%20and%20Xuezhi%20Cao%20and%20Xiaoyu%20Li%20and%20Yehao%20Lin%20and%20Junlin%20Liu%20and%20Xinxuan%20Lv%20and%20Dan%20Ma%20and%20Xuanlin%20Wang%20and%20Ziwen%20Wang%20and%20Shuang%20Zhou&entry.1292438233=%20%20We%20present%20AMO-Bench%2C%20an%20Advanced%20Mathematical%20reasoning%20benchmark%20with%0AOlympiad%20level%20or%20even%20higher%20difficulty%2C%20comprising%2050%20human-crafted%20problems.%0AExisting%20benchmarks%20have%20widely%20leveraged%20high%20school%20math%20competitions%20for%0Aevaluating%20mathematical%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20many%20existing%20math%20competitions%20are%20becoming%20less%20effective%20for%0Aassessing%20top-tier%20LLMs%20due%20to%20performance%20saturation%20%28e.g.%2C%20AIME24/25%29.%20To%0Aaddress%20this%2C%20AMO-Bench%20introduces%20more%20rigorous%20challenges%20by%20ensuring%20all%2050%0Aproblems%20are%20%281%29%20cross-validated%20by%20experts%20to%20meet%20at%20least%20the%20International%0AMathematical%20Olympiad%20%28IMO%29%20difficulty%20standards%2C%20and%20%282%29%20entirely%20original%0Aproblems%20to%20prevent%20potential%20performance%20leakages%20from%20data%20memorization.%0AMoreover%2C%20each%20problem%20in%20AMO-Bench%20requires%20only%20a%20final%20answer%20rather%20than%20a%0Aproof%2C%20enabling%20automatic%20and%20robust%20grading%20for%20evaluation.%20Experimental%0Aresults%20across%2026%20LLMs%20on%20AMO-Bench%20show%20that%20even%20the%20best-performing%20model%0Aachieves%20only%2052.4%25%20accuracy%20on%20AMO-Bench%2C%20with%20most%20LLMs%20scoring%20below%2040%25.%0ABeyond%20these%20poor%20performances%2C%20our%20further%20analysis%20reveals%20a%20promising%0Ascaling%20trend%20with%20increasing%20test-time%20compute%20on%20AMO-Bench.%20These%20results%0Ahighlight%20the%20significant%20room%20for%20improving%20the%20mathematical%20reasoning%20in%0Acurrent%20LLMs.%20We%20release%20AMO-Bench%20to%20facilitate%20further%20research%20into%0Aadvancing%20the%20reasoning%20abilities%20of%20language%20models.%0Ahttps%3A//amo-bench.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26768v1&entry.124074799=Read"},
{"title": "The Era of Agentic Organization: Learning to Organize with Language\n  Models", "author": "Zewen Chi and Li Dong and Qingxiu Dong and Yaru Hao and Xun Wu and Shaohan Huang and Furu Wei", "abstract": "  We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.\n", "link": "http://arxiv.org/abs/2510.26658v1", "date": "2025-10-30", "relevancy": 1.905, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.481}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Era%20of%20Agentic%20Organization%3A%20Learning%20to%20Organize%20with%20Language%0A%20%20Models&body=Title%3A%20The%20Era%20of%20Agentic%20Organization%3A%20Learning%20to%20Organize%20with%20Language%0A%20%20Models%0AAuthor%3A%20Zewen%20Chi%20and%20Li%20Dong%20and%20Qingxiu%20Dong%20and%20Yaru%20Hao%20and%20Xun%20Wu%20and%20Shaohan%20Huang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20We%20envision%20a%20new%20era%20of%20AI%2C%20termed%20agentic%20organization%2C%20where%20agents%20solve%0Acomplex%20problems%20by%20working%20collaboratively%20and%20concurrently%2C%20enabling%20outcomes%0Abeyond%20individual%20intelligence.%20To%20realize%20this%20vision%2C%20we%20introduce%0Aasynchronous%20thinking%20%28AsyncThink%29%20as%20a%20new%20paradigm%20of%20reasoning%20with%20large%0Alanguage%20models%2C%20which%20organizes%20the%20internal%20thinking%20process%20into%0Aconcurrently%20executable%20structures.%20Specifically%2C%20we%20propose%20a%20thinking%0Aprotocol%20where%20an%20organizer%20dynamically%20assigns%20sub-queries%20to%20workers%2C%20merges%0Aintermediate%20knowledge%2C%20and%20produces%20coherent%20solutions.%20More%20importantly%2C%20the%0Athinking%20structure%20in%20this%20protocol%20can%20be%20further%20optimized%20through%0Areinforcement%20learning.%20Experiments%20demonstrate%20that%20AsyncThink%20achieves%2028%25%0Alower%20inference%20latency%20compared%20to%20parallel%20thinking%20while%20improving%20accuracy%0Aon%20mathematical%20reasoning.%20Moreover%2C%20AsyncThink%20generalizes%20its%20learned%0Aasynchronous%20thinking%20capabilities%2C%20effectively%20tackling%20unseen%20tasks%20without%0Aadditional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Era%2520of%2520Agentic%2520Organization%253A%2520Learning%2520to%2520Organize%2520with%2520Language%250A%2520%2520Models%26entry.906535625%3DZewen%2520Chi%2520and%2520Li%2520Dong%2520and%2520Qingxiu%2520Dong%2520and%2520Yaru%2520Hao%2520and%2520Xun%2520Wu%2520and%2520Shaohan%2520Huang%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520We%2520envision%2520a%2520new%2520era%2520of%2520AI%252C%2520termed%2520agentic%2520organization%252C%2520where%2520agents%2520solve%250Acomplex%2520problems%2520by%2520working%2520collaboratively%2520and%2520concurrently%252C%2520enabling%2520outcomes%250Abeyond%2520individual%2520intelligence.%2520To%2520realize%2520this%2520vision%252C%2520we%2520introduce%250Aasynchronous%2520thinking%2520%2528AsyncThink%2529%2520as%2520a%2520new%2520paradigm%2520of%2520reasoning%2520with%2520large%250Alanguage%2520models%252C%2520which%2520organizes%2520the%2520internal%2520thinking%2520process%2520into%250Aconcurrently%2520executable%2520structures.%2520Specifically%252C%2520we%2520propose%2520a%2520thinking%250Aprotocol%2520where%2520an%2520organizer%2520dynamically%2520assigns%2520sub-queries%2520to%2520workers%252C%2520merges%250Aintermediate%2520knowledge%252C%2520and%2520produces%2520coherent%2520solutions.%2520More%2520importantly%252C%2520the%250Athinking%2520structure%2520in%2520this%2520protocol%2520can%2520be%2520further%2520optimized%2520through%250Areinforcement%2520learning.%2520Experiments%2520demonstrate%2520that%2520AsyncThink%2520achieves%252028%2525%250Alower%2520inference%2520latency%2520compared%2520to%2520parallel%2520thinking%2520while%2520improving%2520accuracy%250Aon%2520mathematical%2520reasoning.%2520Moreover%252C%2520AsyncThink%2520generalizes%2520its%2520learned%250Aasynchronous%2520thinking%2520capabilities%252C%2520effectively%2520tackling%2520unseen%2520tasks%2520without%250Aadditional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Era%20of%20Agentic%20Organization%3A%20Learning%20to%20Organize%20with%20Language%0A%20%20Models&entry.906535625=Zewen%20Chi%20and%20Li%20Dong%20and%20Qingxiu%20Dong%20and%20Yaru%20Hao%20and%20Xun%20Wu%20and%20Shaohan%20Huang%20and%20Furu%20Wei&entry.1292438233=%20%20We%20envision%20a%20new%20era%20of%20AI%2C%20termed%20agentic%20organization%2C%20where%20agents%20solve%0Acomplex%20problems%20by%20working%20collaboratively%20and%20concurrently%2C%20enabling%20outcomes%0Abeyond%20individual%20intelligence.%20To%20realize%20this%20vision%2C%20we%20introduce%0Aasynchronous%20thinking%20%28AsyncThink%29%20as%20a%20new%20paradigm%20of%20reasoning%20with%20large%0Alanguage%20models%2C%20which%20organizes%20the%20internal%20thinking%20process%20into%0Aconcurrently%20executable%20structures.%20Specifically%2C%20we%20propose%20a%20thinking%0Aprotocol%20where%20an%20organizer%20dynamically%20assigns%20sub-queries%20to%20workers%2C%20merges%0Aintermediate%20knowledge%2C%20and%20produces%20coherent%20solutions.%20More%20importantly%2C%20the%0Athinking%20structure%20in%20this%20protocol%20can%20be%20further%20optimized%20through%0Areinforcement%20learning.%20Experiments%20demonstrate%20that%20AsyncThink%20achieves%2028%25%0Alower%20inference%20latency%20compared%20to%20parallel%20thinking%20while%20improving%20accuracy%0Aon%20mathematical%20reasoning.%20Moreover%2C%20AsyncThink%20generalizes%20its%20learned%0Aasynchronous%20thinking%20capabilities%2C%20effectively%20tackling%20unseen%20tasks%20without%0Aadditional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26658v1&entry.124074799=Read"},
{"title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off", "author": "Muhammad Faraz Ul Abrar and Nicol\u00f2 Michelusi", "abstract": "  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n", "link": "http://arxiv.org/abs/2510.26722v1", "date": "2025-10-30", "relevancy": 1.8929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off&body=Title%3A%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off%0AAuthor%3A%20Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi%0AAbstract%3A%20%20%20Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%0Ascalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%0Amultiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%0AOTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%0A%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%0Aforcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%0Awireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%0Aand%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%0Alargely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%0Anon-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%0Adescent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%0Aheterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%0Atime-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%0Aderive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%0Anorm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%0Atrade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%0Aefficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%0Astatistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%0Aclassification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%0Aconvergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Convex%2520Over-the-Air%2520Heterogeneous%2520Federated%2520Learning%253A%2520A%250A%2520%2520Bias-Variance%2520Trade-off%26entry.906535625%3DMuhammad%2520Faraz%2520Ul%2520Abrar%2520and%2520Nicol%25C3%25B2%2520Michelusi%26entry.1292438233%3D%2520%2520Over-the-air%2520%2528OTA%2529%2520federated%2520learning%2520%2528FL%2529%2520has%2520been%2520well%2520recognized%2520as%2520a%250Ascalable%2520paradigm%2520that%2520exploits%2520the%2520waveform%2520superposition%2520of%2520the%2520wireless%250Amultiple-access%2520channel%2520to%2520aggregate%2520model%2520updates%2520in%2520a%2520single%2520use.%2520Existing%250AOTA-FL%2520designs%2520largely%2520enforce%2520zero-bias%2520model%2520updates%2520by%2520either%2520assuming%250A%255Cemph%257Bhomogeneous%257D%2520wireless%2520conditions%2520%2528equal%2520path%2520loss%2520across%2520devices%2529%2520or%250Aforcing%2520zero-bias%2520updates%2520to%2520guarantee%2520convergence.%2520Under%2520%255Cemph%257Bheterogeneous%257D%250Awireless%2520scenarios%252C%2520however%252C%2520such%2520designs%2520are%2520constrained%2520by%2520the%2520weakest%2520device%250Aand%2520inflate%2520the%2520update%2520variance.%2520Moreover%252C%2520prior%2520analyses%2520of%2520biased%2520OTA-FL%250Alargely%2520address%2520convex%2520objectives%252C%2520while%2520most%2520modern%2520AI%2520models%2520are%2520highly%250Anon-convex.%2520Motivated%2520by%2520these%2520gaps%252C%2520we%2520study%2520OTA-FL%2520with%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520for%2520general%2520smooth%2520non-convex%2520objectives%2520under%2520wireless%250Aheterogeneity.%2520We%2520develop%2520novel%2520OTA-FL%2520SGD%2520updates%2520that%2520allow%2520a%2520structured%252C%250Atime-invariant%2520model%2520bias%2520while%2520facilitating%2520reduced%2520variance%2520updates.%2520We%250Aderive%2520a%2520finite-time%2520stationarity%2520bound%2520%2528expected%2520time%2520average%2520squared%2520gradient%250Anorm%2529%2520that%2520explicitly%2520reveals%2520a%2520bias-variance%2520trade-off.%2520To%2520optimize%2520this%250Atrade-off%252C%2520we%2520pose%2520a%2520non-convex%2520joint%2520OTA%2520power-control%2520design%2520and%2520develop%2520an%250Aefficient%2520successive%2520convex%2520approximation%2520%2528SCA%2529%2520algorithm%2520that%2520requires%2520only%250Astatistical%2520CSI%2520at%2520the%2520base%2520station.%2520Experiments%2520on%2520a%2520non-convex%2520image%250Aclassification%2520task%2520validate%2520the%2520approach%253A%2520the%2520SCA-based%2520design%2520accelerates%250Aconvergence%2520via%2520an%2520optimized%2520bias%2520and%2520improves%2520generalization%2520over%2520prior%2520OTA-FL%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off&entry.906535625=Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi&entry.1292438233=%20%20Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%0Ascalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%0Amultiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%0AOTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%0A%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%0Aforcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%0Awireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%0Aand%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%0Alargely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%0Anon-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%0Adescent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%0Aheterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%0Atime-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%0Aderive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%0Anorm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%0Atrade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%0Aefficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%0Astatistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%0Aclassification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%0Aconvergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26722v1&entry.124074799=Read"},
{"title": "Understanding Generalization in Node and Link Prediction", "author": "Antonis Vasileiou and Timo Stoll and Christopher Morris", "abstract": "  Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.\n", "link": "http://arxiv.org/abs/2507.00927v3", "date": "2025-10-30", "relevancy": 1.883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4764}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction&body=Title%3A%20Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction%0AAuthor%3A%20Antonis%20Vasileiou%20and%20Timo%20Stoll%20and%20Christopher%20Morris%0AAbstract%3A%20%20%20Using%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20for%20node%20and%20link%0Aprediction%20is%20crucial%20in%20various%20scientific%20and%20industrial%20domains%2C%20which%20has%0Aled%20to%20the%20development%20of%20diverse%20MPNN%20architectures.%20Besides%20working%20well%20in%0Apractical%20settings%2C%20their%20ability%20to%20generalize%20beyond%20the%20training%20set%20remains%0Apoorly%20understood.%20While%20some%20studies%20have%20explored%20MPNNs%27%20generalization%20in%0Agraph-level%20prediction%20tasks%2C%20much%20less%20attention%20has%20been%20given%20to%20node-%20and%0Alink-level%20predictions.%20Existing%20works%20often%20rely%20on%20unrealistic%20i.i.d.%5C%40%0Aassumptions%2C%20overlooking%20possible%20correlations%20between%20nodes%20or%20links%2C%20and%0Aassuming%20fixed%20aggregation%20and%20impractical%20loss%20functions%20while%20neglecting%20the%0Ainfluence%20of%20graph%20structure.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20generalization%20properties%20of%20MPNNs%20in%20inductive%20and%20transductive%0Anode%20and%20link%20prediction%20settings%2C%20incorporating%20diverse%20architectural%0Aparameters%20and%20loss%20functions%20and%20quantifying%20the%20influence%20of%20graph%20structure.%0AAdditionally%2C%20our%20proposed%20generalization%20framework%20can%20be%20applied%20beyond%0Agraphs%20to%20any%20classification%20task%20under%20the%20inductive%20or%20transductive%20setting.%0AOur%20empirical%20study%20supports%20our%20theoretical%20insights%2C%20deepening%20our%0Aunderstanding%20of%20MPNNs%27%20generalization%20capabilities%20in%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Generalization%2520in%2520Node%2520and%2520Link%2520Prediction%26entry.906535625%3DAntonis%2520Vasileiou%2520and%2520Timo%2520Stoll%2520and%2520Christopher%2520Morris%26entry.1292438233%3D%2520%2520Using%2520message-passing%2520graph%2520neural%2520networks%2520%2528MPNNs%2529%2520for%2520node%2520and%2520link%250Aprediction%2520is%2520crucial%2520in%2520various%2520scientific%2520and%2520industrial%2520domains%252C%2520which%2520has%250Aled%2520to%2520the%2520development%2520of%2520diverse%2520MPNN%2520architectures.%2520Besides%2520working%2520well%2520in%250Apractical%2520settings%252C%2520their%2520ability%2520to%2520generalize%2520beyond%2520the%2520training%2520set%2520remains%250Apoorly%2520understood.%2520While%2520some%2520studies%2520have%2520explored%2520MPNNs%2527%2520generalization%2520in%250Agraph-level%2520prediction%2520tasks%252C%2520much%2520less%2520attention%2520has%2520been%2520given%2520to%2520node-%2520and%250Alink-level%2520predictions.%2520Existing%2520works%2520often%2520rely%2520on%2520unrealistic%2520i.i.d.%255C%2540%250Aassumptions%252C%2520overlooking%2520possible%2520correlations%2520between%2520nodes%2520or%2520links%252C%2520and%250Aassuming%2520fixed%2520aggregation%2520and%2520impractical%2520loss%2520functions%2520while%2520neglecting%2520the%250Ainfluence%2520of%2520graph%2520structure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520unified%2520framework%2520to%250Aanalyze%2520the%2520generalization%2520properties%2520of%2520MPNNs%2520in%2520inductive%2520and%2520transductive%250Anode%2520and%2520link%2520prediction%2520settings%252C%2520incorporating%2520diverse%2520architectural%250Aparameters%2520and%2520loss%2520functions%2520and%2520quantifying%2520the%2520influence%2520of%2520graph%2520structure.%250AAdditionally%252C%2520our%2520proposed%2520generalization%2520framework%2520can%2520be%2520applied%2520beyond%250Agraphs%2520to%2520any%2520classification%2520task%2520under%2520the%2520inductive%2520or%2520transductive%2520setting.%250AOur%2520empirical%2520study%2520supports%2520our%2520theoretical%2520insights%252C%2520deepening%2520our%250Aunderstanding%2520of%2520MPNNs%2527%2520generalization%2520capabilities%2520in%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction&entry.906535625=Antonis%20Vasileiou%20and%20Timo%20Stoll%20and%20Christopher%20Morris&entry.1292438233=%20%20Using%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20for%20node%20and%20link%0Aprediction%20is%20crucial%20in%20various%20scientific%20and%20industrial%20domains%2C%20which%20has%0Aled%20to%20the%20development%20of%20diverse%20MPNN%20architectures.%20Besides%20working%20well%20in%0Apractical%20settings%2C%20their%20ability%20to%20generalize%20beyond%20the%20training%20set%20remains%0Apoorly%20understood.%20While%20some%20studies%20have%20explored%20MPNNs%27%20generalization%20in%0Agraph-level%20prediction%20tasks%2C%20much%20less%20attention%20has%20been%20given%20to%20node-%20and%0Alink-level%20predictions.%20Existing%20works%20often%20rely%20on%20unrealistic%20i.i.d.%5C%40%0Aassumptions%2C%20overlooking%20possible%20correlations%20between%20nodes%20or%20links%2C%20and%0Aassuming%20fixed%20aggregation%20and%20impractical%20loss%20functions%20while%20neglecting%20the%0Ainfluence%20of%20graph%20structure.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20generalization%20properties%20of%20MPNNs%20in%20inductive%20and%20transductive%0Anode%20and%20link%20prediction%20settings%2C%20incorporating%20diverse%20architectural%0Aparameters%20and%20loss%20functions%20and%20quantifying%20the%20influence%20of%20graph%20structure.%0AAdditionally%2C%20our%20proposed%20generalization%20framework%20can%20be%20applied%20beyond%0Agraphs%20to%20any%20classification%20task%20under%20the%20inductive%20or%20transductive%20setting.%0AOur%20empirical%20study%20supports%20our%20theoretical%20insights%2C%20deepening%20our%0Aunderstanding%20of%20MPNNs%27%20generalization%20capabilities%20in%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00927v3&entry.124074799=Read"},
{"title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits", "author": "Amir Reza Mirzaei and Yuqiao Wen and Yanshuai Cao and Lili Mou", "abstract": "  Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.\n", "link": "http://arxiv.org/abs/2510.26690v1", "date": "2025-10-30", "relevancy": 1.8678, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4778}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRAQuant%3A%20Mixed-Precision%20Quantization%20of%20LoRA%20to%20Ultra-Low%20Bits&body=Title%3A%20LoRAQuant%3A%20Mixed-Precision%20Quantization%20of%20LoRA%20to%20Ultra-Low%20Bits%0AAuthor%3A%20Amir%20Reza%20Mirzaei%20and%20Yuqiao%20Wen%20and%20Yanshuai%20Cao%20and%20Lili%20Mou%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20popular%20technique%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29.%20In%20many%0Areal-world%20scenarios%2C%20multiple%20adapters%20are%20loaded%20simultaneously%20to%20enable%20LLM%0Acustomization%20for%20personalized%20user%20experiences%20or%20to%20support%20a%20diverse%20range%0Aof%20tasks.%20Although%20each%20adapter%20is%20lightweight%20in%20isolation%2C%20their%20aggregate%0Acost%20becomes%20substantial%20at%20scale.%20To%20address%20this%2C%20we%20propose%20LoRAQuant%2C%20a%0Amixed-precision%20post-training%20quantization%20method%20tailored%20to%20LoRA.%0ASpecifically%2C%20LoRAQuant%20reparameterizes%20each%20adapter%20by%20singular%20value%0Adecomposition%20%28SVD%29%20to%20concentrate%20the%20most%20important%20information%20into%20specific%0Arows%20and%20columns.%20This%20makes%20it%20possible%20to%20quantize%20the%20important%20components%0Ato%20higher%20precision%2C%20while%20quantizing%20the%20rest%20to%20ultra-low%20bitwidth.%20We%0Aconduct%20comprehensive%20experiments%20with%20LLaMA%202-7B%2C%20LLaMA%202-13B%2C%20and%20Mistral%207B%0Amodels%20on%20mathematical%20reasoning%2C%20coding%2C%20and%20summarization%20tasks.%20Results%20show%0Athat%20our%20LoRAQuant%20uses%20significantly%20lower%20bits%20than%20other%20quantization%0Amethods%2C%20but%20achieves%20comparable%20or%20even%20higher%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRAQuant%253A%2520Mixed-Precision%2520Quantization%2520of%2520LoRA%2520to%2520Ultra-Low%2520Bits%26entry.906535625%3DAmir%2520Reza%2520Mirzaei%2520and%2520Yuqiao%2520Wen%2520and%2520Yanshuai%2520Cao%2520and%2520Lili%2520Mou%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520become%2520a%2520popular%2520technique%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520In%2520many%250Areal-world%2520scenarios%252C%2520multiple%2520adapters%2520are%2520loaded%2520simultaneously%2520to%2520enable%2520LLM%250Acustomization%2520for%2520personalized%2520user%2520experiences%2520or%2520to%2520support%2520a%2520diverse%2520range%250Aof%2520tasks.%2520Although%2520each%2520adapter%2520is%2520lightweight%2520in%2520isolation%252C%2520their%2520aggregate%250Acost%2520becomes%2520substantial%2520at%2520scale.%2520To%2520address%2520this%252C%2520we%2520propose%2520LoRAQuant%252C%2520a%250Amixed-precision%2520post-training%2520quantization%2520method%2520tailored%2520to%2520LoRA.%250ASpecifically%252C%2520LoRAQuant%2520reparameterizes%2520each%2520adapter%2520by%2520singular%2520value%250Adecomposition%2520%2528SVD%2529%2520to%2520concentrate%2520the%2520most%2520important%2520information%2520into%2520specific%250Arows%2520and%2520columns.%2520This%2520makes%2520it%2520possible%2520to%2520quantize%2520the%2520important%2520components%250Ato%2520higher%2520precision%252C%2520while%2520quantizing%2520the%2520rest%2520to%2520ultra-low%2520bitwidth.%2520We%250Aconduct%2520comprehensive%2520experiments%2520with%2520LLaMA%25202-7B%252C%2520LLaMA%25202-13B%252C%2520and%2520Mistral%25207B%250Amodels%2520on%2520mathematical%2520reasoning%252C%2520coding%252C%2520and%2520summarization%2520tasks.%2520Results%2520show%250Athat%2520our%2520LoRAQuant%2520uses%2520significantly%2520lower%2520bits%2520than%2520other%2520quantization%250Amethods%252C%2520but%2520achieves%2520comparable%2520or%2520even%2520higher%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRAQuant%3A%20Mixed-Precision%20Quantization%20of%20LoRA%20to%20Ultra-Low%20Bits&entry.906535625=Amir%20Reza%20Mirzaei%20and%20Yuqiao%20Wen%20and%20Yanshuai%20Cao%20and%20Lili%20Mou&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20popular%20technique%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29.%20In%20many%0Areal-world%20scenarios%2C%20multiple%20adapters%20are%20loaded%20simultaneously%20to%20enable%20LLM%0Acustomization%20for%20personalized%20user%20experiences%20or%20to%20support%20a%20diverse%20range%0Aof%20tasks.%20Although%20each%20adapter%20is%20lightweight%20in%20isolation%2C%20their%20aggregate%0Acost%20becomes%20substantial%20at%20scale.%20To%20address%20this%2C%20we%20propose%20LoRAQuant%2C%20a%0Amixed-precision%20post-training%20quantization%20method%20tailored%20to%20LoRA.%0ASpecifically%2C%20LoRAQuant%20reparameterizes%20each%20adapter%20by%20singular%20value%0Adecomposition%20%28SVD%29%20to%20concentrate%20the%20most%20important%20information%20into%20specific%0Arows%20and%20columns.%20This%20makes%20it%20possible%20to%20quantize%20the%20important%20components%0Ato%20higher%20precision%2C%20while%20quantizing%20the%20rest%20to%20ultra-low%20bitwidth.%20We%0Aconduct%20comprehensive%20experiments%20with%20LLaMA%202-7B%2C%20LLaMA%202-13B%2C%20and%20Mistral%207B%0Amodels%20on%20mathematical%20reasoning%2C%20coding%2C%20and%20summarization%20tasks.%20Results%20show%0Athat%20our%20LoRAQuant%20uses%20significantly%20lower%20bits%20than%20other%20quantization%0Amethods%2C%20but%20achieves%20comparable%20or%20even%20higher%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26690v1&entry.124074799=Read"},
{"title": "Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes", "author": "Hossein Sholehrasa and Majid Jaberi-Douraki", "abstract": "  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model specifically designed for protein\nsequences, we generated embeddings that capture the functional and structural\nproperties of proteins. These embeddings were integrated with protein\nexpression levels to form enriched biological representations, which were\nanalyzed using machine learning methods, such as ensemble K-means for\nclustering and XGBoost for classification. Our approach enabled the successful\nclustering of patients into biologically distinct groups and accurately\npredicted clinical outcomes such as survival and biomarker status, achieving\nhigh performance metrics, notably an F1 score of 0.88 for survival and 0.87 for\nbiomarker status prediction. Feature importance analysis identified KMT2C,\nCLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal\nremodeling, and therapy resistance in hormone receptor-positive and\ntriple-negative breast cancer, with potential influence on breast cancer\nsubtype behavior and progression. Furthermore, protein-protein interaction\nnetworks and correlation analyses revealed functional interdependencies among\nproteins that may influence the behavior and progression of breast cancer\nsubtypes. These findings suggest that integrating protein sequence and\nexpression data provides valuable insights into tumor biology and has\nsignificant potential to enhance personalized treatment strategies in breast\ncancer care.\n", "link": "http://arxiv.org/abs/2410.01755v3", "date": "2025-10-30", "relevancy": 1.8624, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Protein%20Sequence%20and%20Expression%20Level%20to%20Analysis%20Molecular%0A%20%20Characterization%20of%20Breast%20Cancer%20Subtypes&body=Title%3A%20Integrating%20Protein%20Sequence%20and%20Expression%20Level%20to%20Analysis%20Molecular%0A%20%20Characterization%20of%20Breast%20Cancer%20Subtypes%0AAuthor%3A%20Hossein%20Sholehrasa%20and%20Majid%20Jaberi-Douraki%0AAbstract%3A%20%20%20Breast%20cancer%27s%20complexity%20and%20variability%20pose%20significant%20challenges%20in%0Aunderstanding%20its%20progression%20and%20guiding%20effective%20treatment.%20This%20study%20aims%0Ato%20integrate%20protein%20sequence%20data%20with%20expression%20levels%20to%20improve%20the%0Amolecular%20characterization%20of%20breast%20cancer%20subtypes%20and%20predict%20clinical%0Aoutcomes.%20Using%20ProtGPT2%2C%20a%20language%20model%20specifically%20designed%20for%20protein%0Asequences%2C%20we%20generated%20embeddings%20that%20capture%20the%20functional%20and%20structural%0Aproperties%20of%20proteins.%20These%20embeddings%20were%20integrated%20with%20protein%0Aexpression%20levels%20to%20form%20enriched%20biological%20representations%2C%20which%20were%0Aanalyzed%20using%20machine%20learning%20methods%2C%20such%20as%20ensemble%20K-means%20for%0Aclustering%20and%20XGBoost%20for%20classification.%20Our%20approach%20enabled%20the%20successful%0Aclustering%20of%20patients%20into%20biologically%20distinct%20groups%20and%20accurately%0Apredicted%20clinical%20outcomes%20such%20as%20survival%20and%20biomarker%20status%2C%20achieving%0Ahigh%20performance%20metrics%2C%20notably%20an%20F1%20score%20of%200.88%20for%20survival%20and%200.87%20for%0Abiomarker%20status%20prediction.%20Feature%20importance%20analysis%20identified%20KMT2C%2C%0ACLASP2%2C%20and%20MYO1B%20as%20key%20proteins%20involved%20in%20hormone%20signaling%2C%20cytoskeletal%0Aremodeling%2C%20and%20therapy%20resistance%20in%20hormone%20receptor-positive%20and%0Atriple-negative%20breast%20cancer%2C%20with%20potential%20influence%20on%20breast%20cancer%0Asubtype%20behavior%20and%20progression.%20Furthermore%2C%20protein-protein%20interaction%0Anetworks%20and%20correlation%20analyses%20revealed%20functional%20interdependencies%20among%0Aproteins%20that%20may%20influence%20the%20behavior%20and%20progression%20of%20breast%20cancer%0Asubtypes.%20These%20findings%20suggest%20that%20integrating%20protein%20sequence%20and%0Aexpression%20data%20provides%20valuable%20insights%20into%20tumor%20biology%20and%20has%0Asignificant%20potential%20to%20enhance%20personalized%20treatment%20strategies%20in%20breast%0Acancer%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Protein%2520Sequence%2520and%2520Expression%2520Level%2520to%2520Analysis%2520Molecular%250A%2520%2520Characterization%2520of%2520Breast%2520Cancer%2520Subtypes%26entry.906535625%3DHossein%2520Sholehrasa%2520and%2520Majid%2520Jaberi-Douraki%26entry.1292438233%3D%2520%2520Breast%2520cancer%2527s%2520complexity%2520and%2520variability%2520pose%2520significant%2520challenges%2520in%250Aunderstanding%2520its%2520progression%2520and%2520guiding%2520effective%2520treatment.%2520This%2520study%2520aims%250Ato%2520integrate%2520protein%2520sequence%2520data%2520with%2520expression%2520levels%2520to%2520improve%2520the%250Amolecular%2520characterization%2520of%2520breast%2520cancer%2520subtypes%2520and%2520predict%2520clinical%250Aoutcomes.%2520Using%2520ProtGPT2%252C%2520a%2520language%2520model%2520specifically%2520designed%2520for%2520protein%250Asequences%252C%2520we%2520generated%2520embeddings%2520that%2520capture%2520the%2520functional%2520and%2520structural%250Aproperties%2520of%2520proteins.%2520These%2520embeddings%2520were%2520integrated%2520with%2520protein%250Aexpression%2520levels%2520to%2520form%2520enriched%2520biological%2520representations%252C%2520which%2520were%250Aanalyzed%2520using%2520machine%2520learning%2520methods%252C%2520such%2520as%2520ensemble%2520K-means%2520for%250Aclustering%2520and%2520XGBoost%2520for%2520classification.%2520Our%2520approach%2520enabled%2520the%2520successful%250Aclustering%2520of%2520patients%2520into%2520biologically%2520distinct%2520groups%2520and%2520accurately%250Apredicted%2520clinical%2520outcomes%2520such%2520as%2520survival%2520and%2520biomarker%2520status%252C%2520achieving%250Ahigh%2520performance%2520metrics%252C%2520notably%2520an%2520F1%2520score%2520of%25200.88%2520for%2520survival%2520and%25200.87%2520for%250Abiomarker%2520status%2520prediction.%2520Feature%2520importance%2520analysis%2520identified%2520KMT2C%252C%250ACLASP2%252C%2520and%2520MYO1B%2520as%2520key%2520proteins%2520involved%2520in%2520hormone%2520signaling%252C%2520cytoskeletal%250Aremodeling%252C%2520and%2520therapy%2520resistance%2520in%2520hormone%2520receptor-positive%2520and%250Atriple-negative%2520breast%2520cancer%252C%2520with%2520potential%2520influence%2520on%2520breast%2520cancer%250Asubtype%2520behavior%2520and%2520progression.%2520Furthermore%252C%2520protein-protein%2520interaction%250Anetworks%2520and%2520correlation%2520analyses%2520revealed%2520functional%2520interdependencies%2520among%250Aproteins%2520that%2520may%2520influence%2520the%2520behavior%2520and%2520progression%2520of%2520breast%2520cancer%250Asubtypes.%2520These%2520findings%2520suggest%2520that%2520integrating%2520protein%2520sequence%2520and%250Aexpression%2520data%2520provides%2520valuable%2520insights%2520into%2520tumor%2520biology%2520and%2520has%250Asignificant%2520potential%2520to%2520enhance%2520personalized%2520treatment%2520strategies%2520in%2520breast%250Acancer%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Protein%20Sequence%20and%20Expression%20Level%20to%20Analysis%20Molecular%0A%20%20Characterization%20of%20Breast%20Cancer%20Subtypes&entry.906535625=Hossein%20Sholehrasa%20and%20Majid%20Jaberi-Douraki&entry.1292438233=%20%20Breast%20cancer%27s%20complexity%20and%20variability%20pose%20significant%20challenges%20in%0Aunderstanding%20its%20progression%20and%20guiding%20effective%20treatment.%20This%20study%20aims%0Ato%20integrate%20protein%20sequence%20data%20with%20expression%20levels%20to%20improve%20the%0Amolecular%20characterization%20of%20breast%20cancer%20subtypes%20and%20predict%20clinical%0Aoutcomes.%20Using%20ProtGPT2%2C%20a%20language%20model%20specifically%20designed%20for%20protein%0Asequences%2C%20we%20generated%20embeddings%20that%20capture%20the%20functional%20and%20structural%0Aproperties%20of%20proteins.%20These%20embeddings%20were%20integrated%20with%20protein%0Aexpression%20levels%20to%20form%20enriched%20biological%20representations%2C%20which%20were%0Aanalyzed%20using%20machine%20learning%20methods%2C%20such%20as%20ensemble%20K-means%20for%0Aclustering%20and%20XGBoost%20for%20classification.%20Our%20approach%20enabled%20the%20successful%0Aclustering%20of%20patients%20into%20biologically%20distinct%20groups%20and%20accurately%0Apredicted%20clinical%20outcomes%20such%20as%20survival%20and%20biomarker%20status%2C%20achieving%0Ahigh%20performance%20metrics%2C%20notably%20an%20F1%20score%20of%200.88%20for%20survival%20and%200.87%20for%0Abiomarker%20status%20prediction.%20Feature%20importance%20analysis%20identified%20KMT2C%2C%0ACLASP2%2C%20and%20MYO1B%20as%20key%20proteins%20involved%20in%20hormone%20signaling%2C%20cytoskeletal%0Aremodeling%2C%20and%20therapy%20resistance%20in%20hormone%20receptor-positive%20and%0Atriple-negative%20breast%20cancer%2C%20with%20potential%20influence%20on%20breast%20cancer%0Asubtype%20behavior%20and%20progression.%20Furthermore%2C%20protein-protein%20interaction%0Anetworks%20and%20correlation%20analyses%20revealed%20functional%20interdependencies%20among%0Aproteins%20that%20may%20influence%20the%20behavior%20and%20progression%20of%20breast%20cancer%0Asubtypes.%20These%20findings%20suggest%20that%20integrating%20protein%20sequence%20and%0Aexpression%20data%20provides%20valuable%20insights%20into%20tumor%20biology%20and%20has%0Asignificant%20potential%20to%20enhance%20personalized%20treatment%20strategies%20in%20breast%0Acancer%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01755v3&entry.124074799=Read"},
{"title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing", "author": "Shayan Nejadshamsi and Yuanyuan Zhang and Shadi Zaki and Brock Porth and Lysa Porth and Vahab Khoshdel", "abstract": "  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n", "link": "http://arxiv.org/abs/2510.26609v1", "date": "2025-10-30", "relevancy": 1.8525, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4688}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CYPRESS%3A%20Crop%20Yield%20Prediction%20via%20Regression%20on%20Prithvi%27s%20Encoder%20for%0A%20%20Satellite%20Sensing&body=Title%3A%20CYPRESS%3A%20Crop%20Yield%20Prediction%20via%20Regression%20on%20Prithvi%27s%20Encoder%20for%0A%20%20Satellite%20Sensing%0AAuthor%3A%20Shayan%20Nejadshamsi%20and%20Yuanyuan%20Zhang%20and%20Shadi%20Zaki%20and%20Brock%20Porth%20and%20Lysa%20Porth%20and%20Vahab%20Khoshdel%0AAbstract%3A%20%20%20Accurate%20and%20timely%20crop%20yield%20prediction%20is%20crucial%20for%20global%20food%20security%0Aand%20modern%20agricultural%20management.%20Traditional%20methods%20often%20lack%20the%0Ascalability%20and%20granularity%20required%20for%20precision%20farming.%20This%20paper%0Aintroduces%20CYPRESS%20%28Crop%20Yield%20Prediction%20via%20Regression%20on%20Prithvi%27s%20Encoder%0Afor%20Satellite%20Sensing%29%2C%20a%20deep%20learning%20model%20designed%20for%20high-resolution%2C%0Aintra-field%20canola%20yield%20prediction.%20CYPRESS%20leverages%20a%20pre-trained%2C%0Alarge-scale%20geospatial%20foundation%20model%20%28Prithvi-EO-2.0-600M%29%20and%20adapts%20it%20for%0Aa%20continuous%20regression%20task%2C%20transforming%20multi-temporal%20satellite%20imagery%0Ainto%20dense%2C%20pixel-level%20yield%20maps.%20Evaluated%20on%20a%20comprehensive%20dataset%20from%0Athe%20Canadian%20Prairies%2C%20CYPRESS%20demonstrates%20superior%20performance%20over%20existing%0Adeep%20learning-based%20yield%20prediction%20models%2C%20highlighting%20the%20effectiveness%20of%0Afine-tuning%20foundation%20models%20for%20specialized%20agricultural%20applications.%20By%0Aproviding%20a%20continuous%2C%20high-resolution%20output%2C%20CYPRESS%20offers%20a%20more%0Aactionable%20tool%20for%20precision%20agriculture%20than%20conventional%20classification%20or%0Acounty-level%20aggregation%20methods.%20This%20work%20validates%20a%20novel%20approach%20that%0Abridges%20the%20gap%20between%20large-scale%20Earth%20observation%20and%20on-farm%0Adecision-making%2C%20offering%20a%20scalable%20solution%20for%20detailed%20agricultural%0Amonitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCYPRESS%253A%2520Crop%2520Yield%2520Prediction%2520via%2520Regression%2520on%2520Prithvi%2527s%2520Encoder%2520for%250A%2520%2520Satellite%2520Sensing%26entry.906535625%3DShayan%2520Nejadshamsi%2520and%2520Yuanyuan%2520Zhang%2520and%2520Shadi%2520Zaki%2520and%2520Brock%2520Porth%2520and%2520Lysa%2520Porth%2520and%2520Vahab%2520Khoshdel%26entry.1292438233%3D%2520%2520Accurate%2520and%2520timely%2520crop%2520yield%2520prediction%2520is%2520crucial%2520for%2520global%2520food%2520security%250Aand%2520modern%2520agricultural%2520management.%2520Traditional%2520methods%2520often%2520lack%2520the%250Ascalability%2520and%2520granularity%2520required%2520for%2520precision%2520farming.%2520This%2520paper%250Aintroduces%2520CYPRESS%2520%2528Crop%2520Yield%2520Prediction%2520via%2520Regression%2520on%2520Prithvi%2527s%2520Encoder%250Afor%2520Satellite%2520Sensing%2529%252C%2520a%2520deep%2520learning%2520model%2520designed%2520for%2520high-resolution%252C%250Aintra-field%2520canola%2520yield%2520prediction.%2520CYPRESS%2520leverages%2520a%2520pre-trained%252C%250Alarge-scale%2520geospatial%2520foundation%2520model%2520%2528Prithvi-EO-2.0-600M%2529%2520and%2520adapts%2520it%2520for%250Aa%2520continuous%2520regression%2520task%252C%2520transforming%2520multi-temporal%2520satellite%2520imagery%250Ainto%2520dense%252C%2520pixel-level%2520yield%2520maps.%2520Evaluated%2520on%2520a%2520comprehensive%2520dataset%2520from%250Athe%2520Canadian%2520Prairies%252C%2520CYPRESS%2520demonstrates%2520superior%2520performance%2520over%2520existing%250Adeep%2520learning-based%2520yield%2520prediction%2520models%252C%2520highlighting%2520the%2520effectiveness%2520of%250Afine-tuning%2520foundation%2520models%2520for%2520specialized%2520agricultural%2520applications.%2520By%250Aproviding%2520a%2520continuous%252C%2520high-resolution%2520output%252C%2520CYPRESS%2520offers%2520a%2520more%250Aactionable%2520tool%2520for%2520precision%2520agriculture%2520than%2520conventional%2520classification%2520or%250Acounty-level%2520aggregation%2520methods.%2520This%2520work%2520validates%2520a%2520novel%2520approach%2520that%250Abridges%2520the%2520gap%2520between%2520large-scale%2520Earth%2520observation%2520and%2520on-farm%250Adecision-making%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520detailed%2520agricultural%250Amonitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CYPRESS%3A%20Crop%20Yield%20Prediction%20via%20Regression%20on%20Prithvi%27s%20Encoder%20for%0A%20%20Satellite%20Sensing&entry.906535625=Shayan%20Nejadshamsi%20and%20Yuanyuan%20Zhang%20and%20Shadi%20Zaki%20and%20Brock%20Porth%20and%20Lysa%20Porth%20and%20Vahab%20Khoshdel&entry.1292438233=%20%20Accurate%20and%20timely%20crop%20yield%20prediction%20is%20crucial%20for%20global%20food%20security%0Aand%20modern%20agricultural%20management.%20Traditional%20methods%20often%20lack%20the%0Ascalability%20and%20granularity%20required%20for%20precision%20farming.%20This%20paper%0Aintroduces%20CYPRESS%20%28Crop%20Yield%20Prediction%20via%20Regression%20on%20Prithvi%27s%20Encoder%0Afor%20Satellite%20Sensing%29%2C%20a%20deep%20learning%20model%20designed%20for%20high-resolution%2C%0Aintra-field%20canola%20yield%20prediction.%20CYPRESS%20leverages%20a%20pre-trained%2C%0Alarge-scale%20geospatial%20foundation%20model%20%28Prithvi-EO-2.0-600M%29%20and%20adapts%20it%20for%0Aa%20continuous%20regression%20task%2C%20transforming%20multi-temporal%20satellite%20imagery%0Ainto%20dense%2C%20pixel-level%20yield%20maps.%20Evaluated%20on%20a%20comprehensive%20dataset%20from%0Athe%20Canadian%20Prairies%2C%20CYPRESS%20demonstrates%20superior%20performance%20over%20existing%0Adeep%20learning-based%20yield%20prediction%20models%2C%20highlighting%20the%20effectiveness%20of%0Afine-tuning%20foundation%20models%20for%20specialized%20agricultural%20applications.%20By%0Aproviding%20a%20continuous%2C%20high-resolution%20output%2C%20CYPRESS%20offers%20a%20more%0Aactionable%20tool%20for%20precision%20agriculture%20than%20conventional%20classification%20or%0Acounty-level%20aggregation%20methods.%20This%20work%20validates%20a%20novel%20approach%20that%0Abridges%20the%20gap%20between%20large-scale%20Earth%20observation%20and%20on-farm%0Adecision-making%2C%20offering%20a%20scalable%20solution%20for%20detailed%20agricultural%0Amonitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26609v1&entry.124074799=Read"},
{"title": "Masked Diffusion Captioning for Visual Feature Learning", "author": "Chao Feng and Zihao Wei and Andrew Owens", "abstract": "  We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.\n", "link": "http://arxiv.org/abs/2510.26799v1", "date": "2025-10-30", "relevancy": 1.7848, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6097}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5937}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Diffusion%20Captioning%20for%20Visual%20Feature%20Learning&body=Title%3A%20Masked%20Diffusion%20Captioning%20for%20Visual%20Feature%20Learning%0AAuthor%3A%20Chao%20Feng%20and%20Zihao%20Wei%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20We%20learn%20visual%20features%20by%20captioning%20images%20with%20an%20image-conditioned%0Amasked%20diffusion%20language%20model%2C%20a%20formulation%20we%20call%20masked%20diffusion%0Acaptioning%20%28MDC%29.%20During%20training%2C%20text%20tokens%20in%20each%20image-caption%20pair%20are%0Amasked%20at%20a%20randomly%20chosen%20ratio%2C%20and%20a%20decoder%20conditioned%20on%20visual%20features%0Ais%20trained%20to%20reconstruct%20the%20original%20text.%20After%20training%2C%20the%20learned%20visual%0Afeatures%20can%20be%20applied%20to%20downstream%20vision%20tasks.%20Unlike%20autoregressive%0Acaptioning%2C%20the%20strength%20of%20the%20visual%20learning%20signal%20in%20MDC%20does%20not%20depend%0Aon%20each%20token%27s%20position%20in%20the%20sequence%2C%20reducing%20the%20need%20for%20auxiliary%0Aobjectives.%20Linear%20probing%20experiments%20across%20a%20variety%20of%20academic-scale%0Amodels%20and%20datasets%20show%20that%20the%20learned%20visual%20features%20are%20competitive%20with%0Athose%20produced%20by%20autoregressive%20and%20contrastive%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Diffusion%2520Captioning%2520for%2520Visual%2520Feature%2520Learning%26entry.906535625%3DChao%2520Feng%2520and%2520Zihao%2520Wei%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520We%2520learn%2520visual%2520features%2520by%2520captioning%2520images%2520with%2520an%2520image-conditioned%250Amasked%2520diffusion%2520language%2520model%252C%2520a%2520formulation%2520we%2520call%2520masked%2520diffusion%250Acaptioning%2520%2528MDC%2529.%2520During%2520training%252C%2520text%2520tokens%2520in%2520each%2520image-caption%2520pair%2520are%250Amasked%2520at%2520a%2520randomly%2520chosen%2520ratio%252C%2520and%2520a%2520decoder%2520conditioned%2520on%2520visual%2520features%250Ais%2520trained%2520to%2520reconstruct%2520the%2520original%2520text.%2520After%2520training%252C%2520the%2520learned%2520visual%250Afeatures%2520can%2520be%2520applied%2520to%2520downstream%2520vision%2520tasks.%2520Unlike%2520autoregressive%250Acaptioning%252C%2520the%2520strength%2520of%2520the%2520visual%2520learning%2520signal%2520in%2520MDC%2520does%2520not%2520depend%250Aon%2520each%2520token%2527s%2520position%2520in%2520the%2520sequence%252C%2520reducing%2520the%2520need%2520for%2520auxiliary%250Aobjectives.%2520Linear%2520probing%2520experiments%2520across%2520a%2520variety%2520of%2520academic-scale%250Amodels%2520and%2520datasets%2520show%2520that%2520the%2520learned%2520visual%2520features%2520are%2520competitive%2520with%250Athose%2520produced%2520by%2520autoregressive%2520and%2520contrastive%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Diffusion%20Captioning%20for%20Visual%20Feature%20Learning&entry.906535625=Chao%20Feng%20and%20Zihao%20Wei%20and%20Andrew%20Owens&entry.1292438233=%20%20We%20learn%20visual%20features%20by%20captioning%20images%20with%20an%20image-conditioned%0Amasked%20diffusion%20language%20model%2C%20a%20formulation%20we%20call%20masked%20diffusion%0Acaptioning%20%28MDC%29.%20During%20training%2C%20text%20tokens%20in%20each%20image-caption%20pair%20are%0Amasked%20at%20a%20randomly%20chosen%20ratio%2C%20and%20a%20decoder%20conditioned%20on%20visual%20features%0Ais%20trained%20to%20reconstruct%20the%20original%20text.%20After%20training%2C%20the%20learned%20visual%0Afeatures%20can%20be%20applied%20to%20downstream%20vision%20tasks.%20Unlike%20autoregressive%0Acaptioning%2C%20the%20strength%20of%20the%20visual%20learning%20signal%20in%20MDC%20does%20not%20depend%0Aon%20each%20token%27s%20position%20in%20the%20sequence%2C%20reducing%20the%20need%20for%20auxiliary%0Aobjectives.%20Linear%20probing%20experiments%20across%20a%20variety%20of%20academic-scale%0Amodels%20and%20datasets%20show%20that%20the%20learned%20visual%20features%20are%20competitive%20with%0Athose%20produced%20by%20autoregressive%20and%20contrastive%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26799v1&entry.124074799=Read"},
{"title": "Budgeted Multiple-Expert Deferral", "author": "Giulia DeSalvo and Clara Mohri and Mehryar Mohri and Yutao Zhong", "abstract": "  Learning to defer uncertain predictions to costly experts offers a powerful\nstrategy for improving the accuracy and efficiency of machine learning systems.\nHowever, standard training procedures for deferral algorithms typically require\nquerying all experts for every training instance, an approach that becomes\nprohibitively expensive when expert queries incur significant computational or\nresource costs. This undermines the core goal of deferral: to limit unnecessary\nexpert usage. To overcome this challenge, we introduce the budgeted deferral\nframework, which aims to train effective deferral algorithms while minimizing\nexpert query costs during training. We propose new algorithms for both\ntwo-stage and single-stage multiple-expert deferral settings that selectively\nquery only a subset of experts per training example. While inspired by active\nlearning, our setting is fundamentally different: labels are already known, and\nthe core challenge is to decide which experts to query in order to balance cost\nand predictive performance. We establish theoretical guarantees for both of our\nalgorithms, including generalization bounds and label complexity analyses.\nEmpirical results across several domains show that our algorithms substantially\nreduce training costs without sacrificing prediction accuracy, demonstrating\nthe practical value of our budget-aware deferral algorithms.\n", "link": "http://arxiv.org/abs/2510.26706v1", "date": "2025-10-30", "relevancy": 1.7782, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4531}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Budgeted%20Multiple-Expert%20Deferral&body=Title%3A%20Budgeted%20Multiple-Expert%20Deferral%0AAuthor%3A%20Giulia%20DeSalvo%20and%20Clara%20Mohri%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20Learning%20to%20defer%20uncertain%20predictions%20to%20costly%20experts%20offers%20a%20powerful%0Astrategy%20for%20improving%20the%20accuracy%20and%20efficiency%20of%20machine%20learning%20systems.%0AHowever%2C%20standard%20training%20procedures%20for%20deferral%20algorithms%20typically%20require%0Aquerying%20all%20experts%20for%20every%20training%20instance%2C%20an%20approach%20that%20becomes%0Aprohibitively%20expensive%20when%20expert%20queries%20incur%20significant%20computational%20or%0Aresource%20costs.%20This%20undermines%20the%20core%20goal%20of%20deferral%3A%20to%20limit%20unnecessary%0Aexpert%20usage.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20the%20budgeted%20deferral%0Aframework%2C%20which%20aims%20to%20train%20effective%20deferral%20algorithms%20while%20minimizing%0Aexpert%20query%20costs%20during%20training.%20We%20propose%20new%20algorithms%20for%20both%0Atwo-stage%20and%20single-stage%20multiple-expert%20deferral%20settings%20that%20selectively%0Aquery%20only%20a%20subset%20of%20experts%20per%20training%20example.%20While%20inspired%20by%20active%0Alearning%2C%20our%20setting%20is%20fundamentally%20different%3A%20labels%20are%20already%20known%2C%20and%0Athe%20core%20challenge%20is%20to%20decide%20which%20experts%20to%20query%20in%20order%20to%20balance%20cost%0Aand%20predictive%20performance.%20We%20establish%20theoretical%20guarantees%20for%20both%20of%20our%0Aalgorithms%2C%20including%20generalization%20bounds%20and%20label%20complexity%20analyses.%0AEmpirical%20results%20across%20several%20domains%20show%20that%20our%20algorithms%20substantially%0Areduce%20training%20costs%20without%20sacrificing%20prediction%20accuracy%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20budget-aware%20deferral%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBudgeted%2520Multiple-Expert%2520Deferral%26entry.906535625%3DGiulia%2520DeSalvo%2520and%2520Clara%2520Mohri%2520and%2520Mehryar%2520Mohri%2520and%2520Yutao%2520Zhong%26entry.1292438233%3D%2520%2520Learning%2520to%2520defer%2520uncertain%2520predictions%2520to%2520costly%2520experts%2520offers%2520a%2520powerful%250Astrategy%2520for%2520improving%2520the%2520accuracy%2520and%2520efficiency%2520of%2520machine%2520learning%2520systems.%250AHowever%252C%2520standard%2520training%2520procedures%2520for%2520deferral%2520algorithms%2520typically%2520require%250Aquerying%2520all%2520experts%2520for%2520every%2520training%2520instance%252C%2520an%2520approach%2520that%2520becomes%250Aprohibitively%2520expensive%2520when%2520expert%2520queries%2520incur%2520significant%2520computational%2520or%250Aresource%2520costs.%2520This%2520undermines%2520the%2520core%2520goal%2520of%2520deferral%253A%2520to%2520limit%2520unnecessary%250Aexpert%2520usage.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520the%2520budgeted%2520deferral%250Aframework%252C%2520which%2520aims%2520to%2520train%2520effective%2520deferral%2520algorithms%2520while%2520minimizing%250Aexpert%2520query%2520costs%2520during%2520training.%2520We%2520propose%2520new%2520algorithms%2520for%2520both%250Atwo-stage%2520and%2520single-stage%2520multiple-expert%2520deferral%2520settings%2520that%2520selectively%250Aquery%2520only%2520a%2520subset%2520of%2520experts%2520per%2520training%2520example.%2520While%2520inspired%2520by%2520active%250Alearning%252C%2520our%2520setting%2520is%2520fundamentally%2520different%253A%2520labels%2520are%2520already%2520known%252C%2520and%250Athe%2520core%2520challenge%2520is%2520to%2520decide%2520which%2520experts%2520to%2520query%2520in%2520order%2520to%2520balance%2520cost%250Aand%2520predictive%2520performance.%2520We%2520establish%2520theoretical%2520guarantees%2520for%2520both%2520of%2520our%250Aalgorithms%252C%2520including%2520generalization%2520bounds%2520and%2520label%2520complexity%2520analyses.%250AEmpirical%2520results%2520across%2520several%2520domains%2520show%2520that%2520our%2520algorithms%2520substantially%250Areduce%2520training%2520costs%2520without%2520sacrificing%2520prediction%2520accuracy%252C%2520demonstrating%250Athe%2520practical%2520value%2520of%2520our%2520budget-aware%2520deferral%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Budgeted%20Multiple-Expert%20Deferral&entry.906535625=Giulia%20DeSalvo%20and%20Clara%20Mohri%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20Learning%20to%20defer%20uncertain%20predictions%20to%20costly%20experts%20offers%20a%20powerful%0Astrategy%20for%20improving%20the%20accuracy%20and%20efficiency%20of%20machine%20learning%20systems.%0AHowever%2C%20standard%20training%20procedures%20for%20deferral%20algorithms%20typically%20require%0Aquerying%20all%20experts%20for%20every%20training%20instance%2C%20an%20approach%20that%20becomes%0Aprohibitively%20expensive%20when%20expert%20queries%20incur%20significant%20computational%20or%0Aresource%20costs.%20This%20undermines%20the%20core%20goal%20of%20deferral%3A%20to%20limit%20unnecessary%0Aexpert%20usage.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20the%20budgeted%20deferral%0Aframework%2C%20which%20aims%20to%20train%20effective%20deferral%20algorithms%20while%20minimizing%0Aexpert%20query%20costs%20during%20training.%20We%20propose%20new%20algorithms%20for%20both%0Atwo-stage%20and%20single-stage%20multiple-expert%20deferral%20settings%20that%20selectively%0Aquery%20only%20a%20subset%20of%20experts%20per%20training%20example.%20While%20inspired%20by%20active%0Alearning%2C%20our%20setting%20is%20fundamentally%20different%3A%20labels%20are%20already%20known%2C%20and%0Athe%20core%20challenge%20is%20to%20decide%20which%20experts%20to%20query%20in%20order%20to%20balance%20cost%0Aand%20predictive%20performance.%20We%20establish%20theoretical%20guarantees%20for%20both%20of%20our%0Aalgorithms%2C%20including%20generalization%20bounds%20and%20label%20complexity%20analyses.%0AEmpirical%20results%20across%20several%20domains%20show%20that%20our%20algorithms%20substantially%0Areduce%20training%20costs%20without%20sacrificing%20prediction%20accuracy%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20budget-aware%20deferral%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26706v1&entry.124074799=Read"},
{"title": "Action-Driven Processes for Continuous-Time Control", "author": "Ruimin He and Shaowei Lin", "abstract": "  At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.\n", "link": "http://arxiv.org/abs/2510.26672v1", "date": "2025-10-30", "relevancy": 1.7735, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action-Driven%20Processes%20for%20Continuous-Time%20Control&body=Title%3A%20Action-Driven%20Processes%20for%20Continuous-Time%20Control%0AAuthor%3A%20Ruimin%20He%20and%20Shaowei%20Lin%0AAbstract%3A%20%20%20At%20the%20heart%20of%20reinforcement%20learning%20are%20actions%20--%20decisions%20made%20in%0Aresponse%20to%20observations%20of%20the%20environment.%20Actions%20are%20equally%20fundamental%20in%0Athe%20modeling%20of%20stochastic%20processes%2C%20as%20they%20trigger%20discontinuous%20state%0Atransitions%20and%20enable%20the%20flow%20of%20information%20through%20large%2C%20complex%20systems.%0AIn%20this%20paper%2C%20we%20unify%20the%20perspectives%20of%20stochastic%20processes%20and%0Areinforcement%20learning%20through%20action-driven%20processes%2C%20and%20illustrate%20their%0Aapplication%20to%20spiking%20neural%20networks.%20Leveraging%20ideas%20from%0Acontrol-as-inference%2C%20we%20show%20that%20minimizing%20the%20Kullback-Leibler%20divergence%0Abetween%20a%20policy-driven%20true%20distribution%20and%20a%20reward-driven%20model%0Adistribution%20for%20a%20suitably%20defined%20action-driven%20process%20is%20equivalent%20to%0Amaximum%20entropy%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction-Driven%2520Processes%2520for%2520Continuous-Time%2520Control%26entry.906535625%3DRuimin%2520He%2520and%2520Shaowei%2520Lin%26entry.1292438233%3D%2520%2520At%2520the%2520heart%2520of%2520reinforcement%2520learning%2520are%2520actions%2520--%2520decisions%2520made%2520in%250Aresponse%2520to%2520observations%2520of%2520the%2520environment.%2520Actions%2520are%2520equally%2520fundamental%2520in%250Athe%2520modeling%2520of%2520stochastic%2520processes%252C%2520as%2520they%2520trigger%2520discontinuous%2520state%250Atransitions%2520and%2520enable%2520the%2520flow%2520of%2520information%2520through%2520large%252C%2520complex%2520systems.%250AIn%2520this%2520paper%252C%2520we%2520unify%2520the%2520perspectives%2520of%2520stochastic%2520processes%2520and%250Areinforcement%2520learning%2520through%2520action-driven%2520processes%252C%2520and%2520illustrate%2520their%250Aapplication%2520to%2520spiking%2520neural%2520networks.%2520Leveraging%2520ideas%2520from%250Acontrol-as-inference%252C%2520we%2520show%2520that%2520minimizing%2520the%2520Kullback-Leibler%2520divergence%250Abetween%2520a%2520policy-driven%2520true%2520distribution%2520and%2520a%2520reward-driven%2520model%250Adistribution%2520for%2520a%2520suitably%2520defined%2520action-driven%2520process%2520is%2520equivalent%2520to%250Amaximum%2520entropy%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action-Driven%20Processes%20for%20Continuous-Time%20Control&entry.906535625=Ruimin%20He%20and%20Shaowei%20Lin&entry.1292438233=%20%20At%20the%20heart%20of%20reinforcement%20learning%20are%20actions%20--%20decisions%20made%20in%0Aresponse%20to%20observations%20of%20the%20environment.%20Actions%20are%20equally%20fundamental%20in%0Athe%20modeling%20of%20stochastic%20processes%2C%20as%20they%20trigger%20discontinuous%20state%0Atransitions%20and%20enable%20the%20flow%20of%20information%20through%20large%2C%20complex%20systems.%0AIn%20this%20paper%2C%20we%20unify%20the%20perspectives%20of%20stochastic%20processes%20and%0Areinforcement%20learning%20through%20action-driven%20processes%2C%20and%20illustrate%20their%0Aapplication%20to%20spiking%20neural%20networks.%20Leveraging%20ideas%20from%0Acontrol-as-inference%2C%20we%20show%20that%20minimizing%20the%20Kullback-Leibler%20divergence%0Abetween%20a%20policy-driven%20true%20distribution%20and%20a%20reward-driven%20model%0Adistribution%20for%20a%20suitably%20defined%20action-driven%20process%20is%20equivalent%20to%0Amaximum%20entropy%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26672v1&entry.124074799=Read"},
{"title": "Gistify! Codebase-Level Understanding via Runtime Execution", "author": "Hyunji Lee and Minseon Kim and Chinmay Singh and Matheus Pereira and Atharv Sonwane and Isadora White and Elias Stengel-Eskin and Mohit Bansal and Zhengyan Shi and Alessandro Sordoni and Marc-Alexandre C\u00f4t\u00e9 and Xingdi Yuan and Lucas Caccia", "abstract": "  As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.\n", "link": "http://arxiv.org/abs/2510.26790v1", "date": "2025-10-30", "relevancy": 1.758, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4445}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4384}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gistify%21%20Codebase-Level%20Understanding%20via%20Runtime%20Execution&body=Title%3A%20Gistify%21%20Codebase-Level%20Understanding%20via%20Runtime%20Execution%0AAuthor%3A%20Hyunji%20Lee%20and%20Minseon%20Kim%20and%20Chinmay%20Singh%20and%20Matheus%20Pereira%20and%20Atharv%20Sonwane%20and%20Isadora%20White%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Zhengyan%20Shi%20and%20Alessandro%20Sordoni%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9%20and%20Xingdi%20Yuan%20and%20Lucas%20Caccia%0AAbstract%3A%20%20%20As%20coding%20agents%20are%20increasingly%20deployed%20in%20large%20codebases%2C%20the%20need%20to%0Aautomatically%20design%20challenging%2C%20codebase-level%20evaluation%20is%20central.%20We%0Apropose%20Gistify%2C%20a%20task%20where%20a%20coding%20LLM%20must%20create%20a%20single%2C%20minimal%2C%0Aself-contained%20file%20that%20can%20reproduce%20a%20specific%20functionality%20of%20a%20codebase.%0AThe%20coding%20LLM%20is%20given%20full%20access%20to%20a%20codebase%20along%20with%20a%20specific%0Aentrypoint%20%28e.g.%2C%20a%20python%20command%29%2C%20and%20the%20generated%20file%20must%20replicate%20the%0Aoutput%20of%20the%20same%20command%20ran%20under%20the%20full%20codebase%2C%20while%20containing%20only%0Athe%20essential%20components%20necessary%20to%20execute%20the%20provided%20command.%20Success%20on%0AGistify%20requires%20both%20structural%20understanding%20of%20the%20codebase%2C%20accurate%0Amodeling%20of%20its%20execution%20flow%20as%20well%20as%20the%20ability%20to%20produce%20potentially%0Alarge%20code%20patches.%20Our%20findings%20show%20that%20current%20state-of-the-art%20models%0Astruggle%20to%20reliably%20solve%20Gistify%20tasks%2C%20especially%20ones%20with%20long%20executions%0Atraces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGistify%2521%2520Codebase-Level%2520Understanding%2520via%2520Runtime%2520Execution%26entry.906535625%3DHyunji%2520Lee%2520and%2520Minseon%2520Kim%2520and%2520Chinmay%2520Singh%2520and%2520Matheus%2520Pereira%2520and%2520Atharv%2520Sonwane%2520and%2520Isadora%2520White%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%2520and%2520Zhengyan%2520Shi%2520and%2520Alessandro%2520Sordoni%2520and%2520Marc-Alexandre%2520C%25C3%25B4t%25C3%25A9%2520and%2520Xingdi%2520Yuan%2520and%2520Lucas%2520Caccia%26entry.1292438233%3D%2520%2520As%2520coding%2520agents%2520are%2520increasingly%2520deployed%2520in%2520large%2520codebases%252C%2520the%2520need%2520to%250Aautomatically%2520design%2520challenging%252C%2520codebase-level%2520evaluation%2520is%2520central.%2520We%250Apropose%2520Gistify%252C%2520a%2520task%2520where%2520a%2520coding%2520LLM%2520must%2520create%2520a%2520single%252C%2520minimal%252C%250Aself-contained%2520file%2520that%2520can%2520reproduce%2520a%2520specific%2520functionality%2520of%2520a%2520codebase.%250AThe%2520coding%2520LLM%2520is%2520given%2520full%2520access%2520to%2520a%2520codebase%2520along%2520with%2520a%2520specific%250Aentrypoint%2520%2528e.g.%252C%2520a%2520python%2520command%2529%252C%2520and%2520the%2520generated%2520file%2520must%2520replicate%2520the%250Aoutput%2520of%2520the%2520same%2520command%2520ran%2520under%2520the%2520full%2520codebase%252C%2520while%2520containing%2520only%250Athe%2520essential%2520components%2520necessary%2520to%2520execute%2520the%2520provided%2520command.%2520Success%2520on%250AGistify%2520requires%2520both%2520structural%2520understanding%2520of%2520the%2520codebase%252C%2520accurate%250Amodeling%2520of%2520its%2520execution%2520flow%2520as%2520well%2520as%2520the%2520ability%2520to%2520produce%2520potentially%250Alarge%2520code%2520patches.%2520Our%2520findings%2520show%2520that%2520current%2520state-of-the-art%2520models%250Astruggle%2520to%2520reliably%2520solve%2520Gistify%2520tasks%252C%2520especially%2520ones%2520with%2520long%2520executions%250Atraces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gistify%21%20Codebase-Level%20Understanding%20via%20Runtime%20Execution&entry.906535625=Hyunji%20Lee%20and%20Minseon%20Kim%20and%20Chinmay%20Singh%20and%20Matheus%20Pereira%20and%20Atharv%20Sonwane%20and%20Isadora%20White%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Zhengyan%20Shi%20and%20Alessandro%20Sordoni%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9%20and%20Xingdi%20Yuan%20and%20Lucas%20Caccia&entry.1292438233=%20%20As%20coding%20agents%20are%20increasingly%20deployed%20in%20large%20codebases%2C%20the%20need%20to%0Aautomatically%20design%20challenging%2C%20codebase-level%20evaluation%20is%20central.%20We%0Apropose%20Gistify%2C%20a%20task%20where%20a%20coding%20LLM%20must%20create%20a%20single%2C%20minimal%2C%0Aself-contained%20file%20that%20can%20reproduce%20a%20specific%20functionality%20of%20a%20codebase.%0AThe%20coding%20LLM%20is%20given%20full%20access%20to%20a%20codebase%20along%20with%20a%20specific%0Aentrypoint%20%28e.g.%2C%20a%20python%20command%29%2C%20and%20the%20generated%20file%20must%20replicate%20the%0Aoutput%20of%20the%20same%20command%20ran%20under%20the%20full%20codebase%2C%20while%20containing%20only%0Athe%20essential%20components%20necessary%20to%20execute%20the%20provided%20command.%20Success%20on%0AGistify%20requires%20both%20structural%20understanding%20of%20the%20codebase%2C%20accurate%0Amodeling%20of%20its%20execution%20flow%20as%20well%20as%20the%20ability%20to%20produce%20potentially%0Alarge%20code%20patches.%20Our%20findings%20show%20that%20current%20state-of-the-art%20models%0Astruggle%20to%20reliably%20solve%20Gistify%20tasks%2C%20especially%20ones%20with%20long%20executions%0Atraces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26790v1&entry.124074799=Read"},
{"title": "MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos", "author": "Gabriel Fiastre and Antoine Yang and Cordelia Schmid", "abstract": "  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n", "link": "http://arxiv.org/abs/2510.14904v2", "date": "2025-10-30", "relevancy": 1.7004, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5766}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.568}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskCaptioner%3A%20Learning%20to%20Jointly%20Segment%20and%20Caption%20Object%0A%20%20Trajectories%20in%20Videos&body=Title%3A%20MaskCaptioner%3A%20Learning%20to%20Jointly%20Segment%20and%20Caption%20Object%0A%20%20Trajectories%20in%20Videos%0AAuthor%3A%20Gabriel%20Fiastre%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid%0AAbstract%3A%20%20%20Dense%20Video%20Object%20Captioning%20%28DVOC%29%20is%20the%20task%20of%20jointly%20detecting%2C%0Atracking%2C%20and%20captioning%20object%20trajectories%20in%20a%20video%2C%20requiring%20the%20ability%0Ato%20understand%20spatio-temporal%20details%20and%20describe%20them%20in%20natural%20language.%0ADue%20to%20the%20complexity%20of%20the%20task%20and%20the%20high%20cost%20associated%20with%20manual%0Aannotation%2C%20previous%20approaches%20resort%20to%20disjoint%20training%20strategies%2C%0Apotentially%20leading%20to%20suboptimal%20performance.%20To%20circumvent%20this%20issue%2C%20we%0Apropose%20to%20generate%20captions%20about%20spatio-temporally%20localized%20entities%0Aleveraging%20a%20state-of-the-art%20VLM.%20By%20extending%20the%20LVIS%20and%20LV-VIS%20datasets%0Awith%20our%20synthetic%20captions%20%28LVISCap%20and%20LV-VISCap%29%2C%20we%20train%20MaskCaptioner%2C%20an%0Aend-to-end%20model%20capable%20of%20jointly%20detecting%2C%20segmenting%2C%20tracking%20and%0Acaptioning%20object%20trajectories.%20Moreover%2C%20with%20pretraining%20on%20LVISCap%20and%0ALV-VISCap%2C%20MaskCaptioner%20achieves%20state-of-the-art%20DVOC%20results%20on%20three%0Aexisting%20benchmarks%2C%20VidSTG%2C%20VLN%20and%20BenSMOT.%20The%20datasets%20and%20code%20are%0Aavailable%20at%20https%3A//www.gabriel.fiastre.fr/maskcaptioner/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskCaptioner%253A%2520Learning%2520to%2520Jointly%2520Segment%2520and%2520Caption%2520Object%250A%2520%2520Trajectories%2520in%2520Videos%26entry.906535625%3DGabriel%2520Fiastre%2520and%2520Antoine%2520Yang%2520and%2520Cordelia%2520Schmid%26entry.1292438233%3D%2520%2520Dense%2520Video%2520Object%2520Captioning%2520%2528DVOC%2529%2520is%2520the%2520task%2520of%2520jointly%2520detecting%252C%250Atracking%252C%2520and%2520captioning%2520object%2520trajectories%2520in%2520a%2520video%252C%2520requiring%2520the%2520ability%250Ato%2520understand%2520spatio-temporal%2520details%2520and%2520describe%2520them%2520in%2520natural%2520language.%250ADue%2520to%2520the%2520complexity%2520of%2520the%2520task%2520and%2520the%2520high%2520cost%2520associated%2520with%2520manual%250Aannotation%252C%2520previous%2520approaches%2520resort%2520to%2520disjoint%2520training%2520strategies%252C%250Apotentially%2520leading%2520to%2520suboptimal%2520performance.%2520To%2520circumvent%2520this%2520issue%252C%2520we%250Apropose%2520to%2520generate%2520captions%2520about%2520spatio-temporally%2520localized%2520entities%250Aleveraging%2520a%2520state-of-the-art%2520VLM.%2520By%2520extending%2520the%2520LVIS%2520and%2520LV-VIS%2520datasets%250Awith%2520our%2520synthetic%2520captions%2520%2528LVISCap%2520and%2520LV-VISCap%2529%252C%2520we%2520train%2520MaskCaptioner%252C%2520an%250Aend-to-end%2520model%2520capable%2520of%2520jointly%2520detecting%252C%2520segmenting%252C%2520tracking%2520and%250Acaptioning%2520object%2520trajectories.%2520Moreover%252C%2520with%2520pretraining%2520on%2520LVISCap%2520and%250ALV-VISCap%252C%2520MaskCaptioner%2520achieves%2520state-of-the-art%2520DVOC%2520results%2520on%2520three%250Aexisting%2520benchmarks%252C%2520VidSTG%252C%2520VLN%2520and%2520BenSMOT.%2520The%2520datasets%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//www.gabriel.fiastre.fr/maskcaptioner/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskCaptioner%3A%20Learning%20to%20Jointly%20Segment%20and%20Caption%20Object%0A%20%20Trajectories%20in%20Videos&entry.906535625=Gabriel%20Fiastre%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid&entry.1292438233=%20%20Dense%20Video%20Object%20Captioning%20%28DVOC%29%20is%20the%20task%20of%20jointly%20detecting%2C%0Atracking%2C%20and%20captioning%20object%20trajectories%20in%20a%20video%2C%20requiring%20the%20ability%0Ato%20understand%20spatio-temporal%20details%20and%20describe%20them%20in%20natural%20language.%0ADue%20to%20the%20complexity%20of%20the%20task%20and%20the%20high%20cost%20associated%20with%20manual%0Aannotation%2C%20previous%20approaches%20resort%20to%20disjoint%20training%20strategies%2C%0Apotentially%20leading%20to%20suboptimal%20performance.%20To%20circumvent%20this%20issue%2C%20we%0Apropose%20to%20generate%20captions%20about%20spatio-temporally%20localized%20entities%0Aleveraging%20a%20state-of-the-art%20VLM.%20By%20extending%20the%20LVIS%20and%20LV-VIS%20datasets%0Awith%20our%20synthetic%20captions%20%28LVISCap%20and%20LV-VISCap%29%2C%20we%20train%20MaskCaptioner%2C%20an%0Aend-to-end%20model%20capable%20of%20jointly%20detecting%2C%20segmenting%2C%20tracking%20and%0Acaptioning%20object%20trajectories.%20Moreover%2C%20with%20pretraining%20on%20LVISCap%20and%0ALV-VISCap%2C%20MaskCaptioner%20achieves%20state-of-the-art%20DVOC%20results%20on%20three%0Aexisting%20benchmarks%2C%20VidSTG%2C%20VLN%20and%20BenSMOT.%20The%20datasets%20and%20code%20are%0Aavailable%20at%20https%3A//www.gabriel.fiastre.fr/maskcaptioner/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14904v2&entry.124074799=Read"},
{"title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments", "author": "Xiaoyi He and Danggui Chen and Zhenshuo Zhang and Zimeng Bai", "abstract": "  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n", "link": "http://arxiv.org/abs/2510.26646v1", "date": "2025-10-30", "relevancy": 1.6974, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5521}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20DQN-TD3%20Reinforcement%20Learning%20for%20Autonomous%20Navigation%20in%0A%20%20Dynamic%20Environments&body=Title%3A%20Hybrid%20DQN-TD3%20Reinforcement%20Learning%20for%20Autonomous%20Navigation%20in%0A%20%20Dynamic%20Environments%0AAuthor%3A%20Xiaoyi%20He%20and%20Danggui%20Chen%20and%20Zhenshuo%20Zhang%20and%20Zimeng%20Bai%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20hierarchical%20path-planning%20and%20control%20framework%20that%0Acombines%20a%20high-level%20Deep%20Q-Network%20%28DQN%29%20for%20discrete%20sub-goal%20selection%20with%0Aa%20low-level%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20controller%0Afor%20continuous%20actuation.%20The%20high-level%20module%20selects%20behaviors%20and%0Asub-goals%3B%20the%20low-level%20module%20executes%20smooth%20velocity%20commands.%20We%20design%20a%0Apractical%20reward%20shaping%20scheme%20%28direction%2C%20distance%2C%20obstacle%20avoidance%2C%0Aaction%20smoothness%2C%20collision%20penalty%2C%20time%20penalty%2C%20and%20progress%29%2C%20together%0Awith%20a%20LiDAR-based%20safety%20gate%20that%20prevents%20unsafe%20motions.%20The%20system%20is%0Aimplemented%20in%20ROS%20%2B%20Gazebo%20%28TurtleBot3%29%20and%20evaluated%20with%20PathBench%20metrics%2C%0Aincluding%20success%20rate%2C%20collision%20rate%2C%20path%20efficiency%2C%20and%20re-planning%0Aefficiency%2C%20in%20dynamic%20and%20partially%20observable%20environments.%20Experiments%20show%0Aimproved%20success%20rate%20and%20sample%20efficiency%20over%20single-algorithm%20baselines%0A%28DQN%20or%20TD3%20alone%29%20and%20rule-based%20planners%2C%20with%20better%20generalization%20to%0Aunseen%20obstacle%20configurations%20and%20reduced%20abrupt%20control%20changes.%20Code%20and%0Aevaluation%20scripts%20are%20available%20at%20the%20project%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520DQN-TD3%2520Reinforcement%2520Learning%2520for%2520Autonomous%2520Navigation%2520in%250A%2520%2520Dynamic%2520Environments%26entry.906535625%3DXiaoyi%2520He%2520and%2520Danggui%2520Chen%2520and%2520Zhenshuo%2520Zhang%2520and%2520Zimeng%2520Bai%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520hierarchical%2520path-planning%2520and%2520control%2520framework%2520that%250Acombines%2520a%2520high-level%2520Deep%2520Q-Network%2520%2528DQN%2529%2520for%2520discrete%2520sub-goal%2520selection%2520with%250Aa%2520low-level%2520Twin%2520Delayed%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528TD3%2529%2520controller%250Afor%2520continuous%2520actuation.%2520The%2520high-level%2520module%2520selects%2520behaviors%2520and%250Asub-goals%253B%2520the%2520low-level%2520module%2520executes%2520smooth%2520velocity%2520commands.%2520We%2520design%2520a%250Apractical%2520reward%2520shaping%2520scheme%2520%2528direction%252C%2520distance%252C%2520obstacle%2520avoidance%252C%250Aaction%2520smoothness%252C%2520collision%2520penalty%252C%2520time%2520penalty%252C%2520and%2520progress%2529%252C%2520together%250Awith%2520a%2520LiDAR-based%2520safety%2520gate%2520that%2520prevents%2520unsafe%2520motions.%2520The%2520system%2520is%250Aimplemented%2520in%2520ROS%2520%252B%2520Gazebo%2520%2528TurtleBot3%2529%2520and%2520evaluated%2520with%2520PathBench%2520metrics%252C%250Aincluding%2520success%2520rate%252C%2520collision%2520rate%252C%2520path%2520efficiency%252C%2520and%2520re-planning%250Aefficiency%252C%2520in%2520dynamic%2520and%2520partially%2520observable%2520environments.%2520Experiments%2520show%250Aimproved%2520success%2520rate%2520and%2520sample%2520efficiency%2520over%2520single-algorithm%2520baselines%250A%2528DQN%2520or%2520TD3%2520alone%2529%2520and%2520rule-based%2520planners%252C%2520with%2520better%2520generalization%2520to%250Aunseen%2520obstacle%2520configurations%2520and%2520reduced%2520abrupt%2520control%2520changes.%2520Code%2520and%250Aevaluation%2520scripts%2520are%2520available%2520at%2520the%2520project%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20DQN-TD3%20Reinforcement%20Learning%20for%20Autonomous%20Navigation%20in%0A%20%20Dynamic%20Environments&entry.906535625=Xiaoyi%20He%20and%20Danggui%20Chen%20and%20Zhenshuo%20Zhang%20and%20Zimeng%20Bai&entry.1292438233=%20%20This%20paper%20presents%20a%20hierarchical%20path-planning%20and%20control%20framework%20that%0Acombines%20a%20high-level%20Deep%20Q-Network%20%28DQN%29%20for%20discrete%20sub-goal%20selection%20with%0Aa%20low-level%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20controller%0Afor%20continuous%20actuation.%20The%20high-level%20module%20selects%20behaviors%20and%0Asub-goals%3B%20the%20low-level%20module%20executes%20smooth%20velocity%20commands.%20We%20design%20a%0Apractical%20reward%20shaping%20scheme%20%28direction%2C%20distance%2C%20obstacle%20avoidance%2C%0Aaction%20smoothness%2C%20collision%20penalty%2C%20time%20penalty%2C%20and%20progress%29%2C%20together%0Awith%20a%20LiDAR-based%20safety%20gate%20that%20prevents%20unsafe%20motions.%20The%20system%20is%0Aimplemented%20in%20ROS%20%2B%20Gazebo%20%28TurtleBot3%29%20and%20evaluated%20with%20PathBench%20metrics%2C%0Aincluding%20success%20rate%2C%20collision%20rate%2C%20path%20efficiency%2C%20and%20re-planning%0Aefficiency%2C%20in%20dynamic%20and%20partially%20observable%20environments.%20Experiments%20show%0Aimproved%20success%20rate%20and%20sample%20efficiency%20over%20single-algorithm%20baselines%0A%28DQN%20or%20TD3%20alone%29%20and%20rule-based%20planners%2C%20with%20better%20generalization%20to%0Aunseen%20obstacle%20configurations%20and%20reduced%20abrupt%20control%20changes.%20Code%20and%0Aevaluation%20scripts%20are%20available%20at%20the%20project%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26646v1&entry.124074799=Read"},
{"title": "Running VLAs at Real-time Speed", "author": "Yunchao Ma and Yizhuang Zhou and Yunhuan Yang and Tiancai Wang and Haoqiang Fan", "abstract": "  In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.\n", "link": "http://arxiv.org/abs/2510.26742v1", "date": "2025-10-30", "relevancy": 1.6925, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Running%20VLAs%20at%20Real-time%20Speed&body=Title%3A%20Running%20VLAs%20at%20Real-time%20Speed%0AAuthor%3A%20Yunchao%20Ma%20and%20Yizhuang%20Zhou%20and%20Yunhuan%20Yang%20and%20Tiancai%20Wang%20and%20Haoqiang%20Fan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20how%20to%20run%20pi0-level%20multi-view%20VLA%20at%2030Hz%20frame%20rate%0Aand%20at%20most%20480Hz%20trajectory%20frequency%20using%20a%20single%20consumer%20GPU.%20This%0Aenables%20dynamic%20and%20real-time%20tasks%20that%20were%20previously%20believed%20to%20be%0Aunattainable%20by%20large%20VLA%20models.%20To%20achieve%20it%2C%20we%20introduce%20a%20bag%20of%0Astrategies%20to%20eliminate%20the%20overheads%20in%20model%20inference.%20The%20real-world%0Aexperiment%20shows%20that%20the%20pi0%20policy%20with%20our%20strategy%20achieves%20a%20100%25%20success%0Arate%20in%20grasping%20a%20falling%20pen%20task.%20Based%20on%20the%20results%2C%20we%20further%20propose%20a%0Afull%20streaming%20inference%20framework%20for%20real-time%20robot%20control%20of%20VLA.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Dexmal/realtime-vla.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRunning%2520VLAs%2520at%2520Real-time%2520Speed%26entry.906535625%3DYunchao%2520Ma%2520and%2520Yizhuang%2520Zhou%2520and%2520Yunhuan%2520Yang%2520and%2520Tiancai%2520Wang%2520and%2520Haoqiang%2520Fan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520to%2520run%2520pi0-level%2520multi-view%2520VLA%2520at%252030Hz%2520frame%2520rate%250Aand%2520at%2520most%2520480Hz%2520trajectory%2520frequency%2520using%2520a%2520single%2520consumer%2520GPU.%2520This%250Aenables%2520dynamic%2520and%2520real-time%2520tasks%2520that%2520were%2520previously%2520believed%2520to%2520be%250Aunattainable%2520by%2520large%2520VLA%2520models.%2520To%2520achieve%2520it%252C%2520we%2520introduce%2520a%2520bag%2520of%250Astrategies%2520to%2520eliminate%2520the%2520overheads%2520in%2520model%2520inference.%2520The%2520real-world%250Aexperiment%2520shows%2520that%2520the%2520pi0%2520policy%2520with%2520our%2520strategy%2520achieves%2520a%2520100%2525%2520success%250Arate%2520in%2520grasping%2520a%2520falling%2520pen%2520task.%2520Based%2520on%2520the%2520results%252C%2520we%2520further%2520propose%2520a%250Afull%2520streaming%2520inference%2520framework%2520for%2520real-time%2520robot%2520control%2520of%2520VLA.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/Dexmal/realtime-vla.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Running%20VLAs%20at%20Real-time%20Speed&entry.906535625=Yunchao%20Ma%20and%20Yizhuang%20Zhou%20and%20Yunhuan%20Yang%20and%20Tiancai%20Wang%20and%20Haoqiang%20Fan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20how%20to%20run%20pi0-level%20multi-view%20VLA%20at%2030Hz%20frame%20rate%0Aand%20at%20most%20480Hz%20trajectory%20frequency%20using%20a%20single%20consumer%20GPU.%20This%0Aenables%20dynamic%20and%20real-time%20tasks%20that%20were%20previously%20believed%20to%20be%0Aunattainable%20by%20large%20VLA%20models.%20To%20achieve%20it%2C%20we%20introduce%20a%20bag%20of%0Astrategies%20to%20eliminate%20the%20overheads%20in%20model%20inference.%20The%20real-world%0Aexperiment%20shows%20that%20the%20pi0%20policy%20with%20our%20strategy%20achieves%20a%20100%25%20success%0Arate%20in%20grasping%20a%20falling%20pen%20task.%20Based%20on%20the%20results%2C%20we%20further%20propose%20a%0Afull%20streaming%20inference%20framework%20for%20real-time%20robot%20control%20of%20VLA.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Dexmal/realtime-vla.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26742v1&entry.124074799=Read"},
{"title": "LLMs Process Lists With General Filter Heads", "author": "Arnab Sen Sharma and Giordano Rogers and Natalie Shapira and David Bau", "abstract": "  We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.\n", "link": "http://arxiv.org/abs/2510.26784v1", "date": "2025-10-30", "relevancy": 1.6852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Process%20Lists%20With%20General%20Filter%20Heads&body=Title%3A%20LLMs%20Process%20Lists%20With%20General%20Filter%20Heads%0AAuthor%3A%20Arnab%20Sen%20Sharma%20and%20Giordano%20Rogers%20and%20Natalie%20Shapira%20and%20David%20Bau%0AAbstract%3A%20%20%20We%20investigate%20the%20mechanisms%20underlying%20a%20range%20of%20list-processing%20tasks%20in%0ALLMs%2C%20and%20we%20find%20that%20LLMs%20have%20learned%20to%20encode%20a%20compact%2C%20causal%0Arepresentation%20of%20a%20general%20filtering%20operation%20that%20mirrors%20the%20generic%0A%22filter%22%20function%20of%20functional%20programming.%20Using%20causal%20mediation%20analysis%20on%0Aa%20diverse%20set%20of%20list-processing%20tasks%2C%20we%20find%20that%20a%20small%20number%20of%0Aattention%20heads%2C%20which%20we%20dub%20filter%20heads%2C%20encode%20a%20compact%20representation%20of%0Athe%20filtering%20predicate%20in%20their%20query%20states%20at%20certain%20tokens.%20We%20demonstrate%0Athat%20this%20predicate%20representation%20is%20general%20and%20portable%3A%20it%20can%20be%20extracted%0Aand%20reapplied%20to%20execute%20the%20same%20filtering%20operation%20on%20different%20collections%2C%0Apresented%20in%20different%20formats%2C%20languages%2C%20or%20even%20in%20tasks.%20However%2C%20we%20also%0Aidentify%20situations%20where%20transformer%20LMs%20can%20exploit%20a%20different%20strategy%20for%0Afiltering%3A%20eagerly%20evaluating%20if%20an%20item%20satisfies%20the%20predicate%20and%20storing%0Athis%20intermediate%20result%20as%20a%20flag%20directly%20in%20the%20item%20representations.%20Our%0Aresults%20reveal%20that%20transformer%20LMs%20can%20develop%20human-interpretable%0Aimplementations%20of%20abstract%20computational%20operations%20that%20generalize%20in%20ways%0Athat%20are%20surprisingly%20similar%20to%20strategies%20used%20in%20traditional%20functional%0Aprogramming%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Process%2520Lists%2520With%2520General%2520Filter%2520Heads%26entry.906535625%3DArnab%2520Sen%2520Sharma%2520and%2520Giordano%2520Rogers%2520and%2520Natalie%2520Shapira%2520and%2520David%2520Bau%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520mechanisms%2520underlying%2520a%2520range%2520of%2520list-processing%2520tasks%2520in%250ALLMs%252C%2520and%2520we%2520find%2520that%2520LLMs%2520have%2520learned%2520to%2520encode%2520a%2520compact%252C%2520causal%250Arepresentation%2520of%2520a%2520general%2520filtering%2520operation%2520that%2520mirrors%2520the%2520generic%250A%2522filter%2522%2520function%2520of%2520functional%2520programming.%2520Using%2520causal%2520mediation%2520analysis%2520on%250Aa%2520diverse%2520set%2520of%2520list-processing%2520tasks%252C%2520we%2520find%2520that%2520a%2520small%2520number%2520of%250Aattention%2520heads%252C%2520which%2520we%2520dub%2520filter%2520heads%252C%2520encode%2520a%2520compact%2520representation%2520of%250Athe%2520filtering%2520predicate%2520in%2520their%2520query%2520states%2520at%2520certain%2520tokens.%2520We%2520demonstrate%250Athat%2520this%2520predicate%2520representation%2520is%2520general%2520and%2520portable%253A%2520it%2520can%2520be%2520extracted%250Aand%2520reapplied%2520to%2520execute%2520the%2520same%2520filtering%2520operation%2520on%2520different%2520collections%252C%250Apresented%2520in%2520different%2520formats%252C%2520languages%252C%2520or%2520even%2520in%2520tasks.%2520However%252C%2520we%2520also%250Aidentify%2520situations%2520where%2520transformer%2520LMs%2520can%2520exploit%2520a%2520different%2520strategy%2520for%250Afiltering%253A%2520eagerly%2520evaluating%2520if%2520an%2520item%2520satisfies%2520the%2520predicate%2520and%2520storing%250Athis%2520intermediate%2520result%2520as%2520a%2520flag%2520directly%2520in%2520the%2520item%2520representations.%2520Our%250Aresults%2520reveal%2520that%2520transformer%2520LMs%2520can%2520develop%2520human-interpretable%250Aimplementations%2520of%2520abstract%2520computational%2520operations%2520that%2520generalize%2520in%2520ways%250Athat%2520are%2520surprisingly%2520similar%2520to%2520strategies%2520used%2520in%2520traditional%2520functional%250Aprogramming%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Process%20Lists%20With%20General%20Filter%20Heads&entry.906535625=Arnab%20Sen%20Sharma%20and%20Giordano%20Rogers%20and%20Natalie%20Shapira%20and%20David%20Bau&entry.1292438233=%20%20We%20investigate%20the%20mechanisms%20underlying%20a%20range%20of%20list-processing%20tasks%20in%0ALLMs%2C%20and%20we%20find%20that%20LLMs%20have%20learned%20to%20encode%20a%20compact%2C%20causal%0Arepresentation%20of%20a%20general%20filtering%20operation%20that%20mirrors%20the%20generic%0A%22filter%22%20function%20of%20functional%20programming.%20Using%20causal%20mediation%20analysis%20on%0Aa%20diverse%20set%20of%20list-processing%20tasks%2C%20we%20find%20that%20a%20small%20number%20of%0Aattention%20heads%2C%20which%20we%20dub%20filter%20heads%2C%20encode%20a%20compact%20representation%20of%0Athe%20filtering%20predicate%20in%20their%20query%20states%20at%20certain%20tokens.%20We%20demonstrate%0Athat%20this%20predicate%20representation%20is%20general%20and%20portable%3A%20it%20can%20be%20extracted%0Aand%20reapplied%20to%20execute%20the%20same%20filtering%20operation%20on%20different%20collections%2C%0Apresented%20in%20different%20formats%2C%20languages%2C%20or%20even%20in%20tasks.%20However%2C%20we%20also%0Aidentify%20situations%20where%20transformer%20LMs%20can%20exploit%20a%20different%20strategy%20for%0Afiltering%3A%20eagerly%20evaluating%20if%20an%20item%20satisfies%20the%20predicate%20and%20storing%0Athis%20intermediate%20result%20as%20a%20flag%20directly%20in%20the%20item%20representations.%20Our%0Aresults%20reveal%20that%20transformer%20LMs%20can%20develop%20human-interpretable%0Aimplementations%20of%20abstract%20computational%20operations%20that%20generalize%20in%20ways%0Athat%20are%20surprisingly%20similar%20to%20strategies%20used%20in%20traditional%20functional%0Aprogramming%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26784v1&entry.124074799=Read"},
{"title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models", "author": "Zaishuo Xia and Yukuan Lu and Xinyi Li and Yifan Xu and Yubei Chen", "abstract": "  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n", "link": "http://arxiv.org/abs/2510.26782v1", "date": "2025-10-30", "relevancy": 1.685, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5917}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5678}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clone%20Deterministic%203D%20Worlds%20with%20Geometrically-Regularized%20World%0A%20%20Models&body=Title%3A%20Clone%20Deterministic%203D%20Worlds%20with%20Geometrically-Regularized%20World%0A%20%20Models%0AAuthor%3A%20Zaishuo%20Xia%20and%20Yukuan%20Lu%20and%20Xinyi%20Li%20and%20Yifan%20Xu%20and%20Yubei%20Chen%0AAbstract%3A%20%20%20A%20world%20model%20is%20an%20internal%20model%20that%20simulates%20how%20the%20world%20evolves.%0AGiven%20past%20observations%20and%20actions%2C%20it%20predicts%20the%20future%20of%20both%20the%0Aembodied%20agent%20and%20its%20environment.%20Accurate%20world%20models%20are%20essential%20for%0Aenabling%20agents%20to%20think%2C%20plan%2C%20and%20reason%20effectively%20in%20complex%2C%20dynamic%0Asettings.%20Despite%20rapid%20progress%2C%20current%20world%20models%20remain%20brittle%20and%0Adegrade%20over%20long%20horizons.%20We%20argue%20that%20a%20central%20cause%20is%20representation%0Aquality%3A%20exteroceptive%20inputs%20%28e.g.%2C%20images%29%20are%20high-dimensional%2C%20and%20lossy%20or%0Aentangled%20latents%20make%20dynamics%20learning%20unnecessarily%20hard.%20We%20therefore%20ask%0Awhether%20improving%20representation%20learning%20alone%20can%20substantially%20improve%0Aworld-model%20performance.%20In%20this%20work%2C%20we%20take%20a%20step%20toward%20building%20a%20truly%0Aaccurate%20world%20model%20by%20addressing%20a%20fundamental%20yet%20open%20problem%3A%20constructing%0Aa%20model%20that%20can%20fully%20clone%20and%20overfit%20to%20a%20deterministic%203D%20world.%20We%0Apropose%20Geometrically-Regularized%20World%20Models%20%28GRWM%29%2C%20which%20enforces%20that%0Aconsecutive%20points%20along%20a%20natural%20sensory%20trajectory%20remain%20close%20in%20latent%0Arepresentation%20space.%20This%20approach%20yields%20significantly%20improved%20latent%0Arepresentations%20that%20align%20closely%20with%20the%20true%20topology%20of%20the%20environment.%0AGRWM%20is%20plug-and-play%2C%20requires%20only%20minimal%20architectural%20modification%2C%20scales%0Awith%20trajectory%20length%2C%20and%20is%20compatible%20with%20diverse%20latent%20generative%0Abackbones.%20Across%20deterministic%203D%20settings%20and%20long-horizon%20prediction%20tasks%2C%0AGRWM%20significantly%20increases%20rollout%20fidelity%20and%20stability.%20Analyses%20show%20that%0Aits%20benefits%20stem%20from%20learning%20a%20latent%20manifold%20with%20superior%20geometric%0Astructure.%20These%20findings%20support%20a%20clear%20takeaway%3A%20improving%20representation%0Alearning%20is%20a%20direct%20and%20useful%20path%20to%20robust%20world%20models%2C%20delivering%0Areliable%20long-horizon%20predictions%20without%20enlarging%20the%20dynamics%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClone%2520Deterministic%25203D%2520Worlds%2520with%2520Geometrically-Regularized%2520World%250A%2520%2520Models%26entry.906535625%3DZaishuo%2520Xia%2520and%2520Yukuan%2520Lu%2520and%2520Xinyi%2520Li%2520and%2520Yifan%2520Xu%2520and%2520Yubei%2520Chen%26entry.1292438233%3D%2520%2520A%2520world%2520model%2520is%2520an%2520internal%2520model%2520that%2520simulates%2520how%2520the%2520world%2520evolves.%250AGiven%2520past%2520observations%2520and%2520actions%252C%2520it%2520predicts%2520the%2520future%2520of%2520both%2520the%250Aembodied%2520agent%2520and%2520its%2520environment.%2520Accurate%2520world%2520models%2520are%2520essential%2520for%250Aenabling%2520agents%2520to%2520think%252C%2520plan%252C%2520and%2520reason%2520effectively%2520in%2520complex%252C%2520dynamic%250Asettings.%2520Despite%2520rapid%2520progress%252C%2520current%2520world%2520models%2520remain%2520brittle%2520and%250Adegrade%2520over%2520long%2520horizons.%2520We%2520argue%2520that%2520a%2520central%2520cause%2520is%2520representation%250Aquality%253A%2520exteroceptive%2520inputs%2520%2528e.g.%252C%2520images%2529%2520are%2520high-dimensional%252C%2520and%2520lossy%2520or%250Aentangled%2520latents%2520make%2520dynamics%2520learning%2520unnecessarily%2520hard.%2520We%2520therefore%2520ask%250Awhether%2520improving%2520representation%2520learning%2520alone%2520can%2520substantially%2520improve%250Aworld-model%2520performance.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520step%2520toward%2520building%2520a%2520truly%250Aaccurate%2520world%2520model%2520by%2520addressing%2520a%2520fundamental%2520yet%2520open%2520problem%253A%2520constructing%250Aa%2520model%2520that%2520can%2520fully%2520clone%2520and%2520overfit%2520to%2520a%2520deterministic%25203D%2520world.%2520We%250Apropose%2520Geometrically-Regularized%2520World%2520Models%2520%2528GRWM%2529%252C%2520which%2520enforces%2520that%250Aconsecutive%2520points%2520along%2520a%2520natural%2520sensory%2520trajectory%2520remain%2520close%2520in%2520latent%250Arepresentation%2520space.%2520This%2520approach%2520yields%2520significantly%2520improved%2520latent%250Arepresentations%2520that%2520align%2520closely%2520with%2520the%2520true%2520topology%2520of%2520the%2520environment.%250AGRWM%2520is%2520plug-and-play%252C%2520requires%2520only%2520minimal%2520architectural%2520modification%252C%2520scales%250Awith%2520trajectory%2520length%252C%2520and%2520is%2520compatible%2520with%2520diverse%2520latent%2520generative%250Abackbones.%2520Across%2520deterministic%25203D%2520settings%2520and%2520long-horizon%2520prediction%2520tasks%252C%250AGRWM%2520significantly%2520increases%2520rollout%2520fidelity%2520and%2520stability.%2520Analyses%2520show%2520that%250Aits%2520benefits%2520stem%2520from%2520learning%2520a%2520latent%2520manifold%2520with%2520superior%2520geometric%250Astructure.%2520These%2520findings%2520support%2520a%2520clear%2520takeaway%253A%2520improving%2520representation%250Alearning%2520is%2520a%2520direct%2520and%2520useful%2520path%2520to%2520robust%2520world%2520models%252C%2520delivering%250Areliable%2520long-horizon%2520predictions%2520without%2520enlarging%2520the%2520dynamics%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clone%20Deterministic%203D%20Worlds%20with%20Geometrically-Regularized%20World%0A%20%20Models&entry.906535625=Zaishuo%20Xia%20and%20Yukuan%20Lu%20and%20Xinyi%20Li%20and%20Yifan%20Xu%20and%20Yubei%20Chen&entry.1292438233=%20%20A%20world%20model%20is%20an%20internal%20model%20that%20simulates%20how%20the%20world%20evolves.%0AGiven%20past%20observations%20and%20actions%2C%20it%20predicts%20the%20future%20of%20both%20the%0Aembodied%20agent%20and%20its%20environment.%20Accurate%20world%20models%20are%20essential%20for%0Aenabling%20agents%20to%20think%2C%20plan%2C%20and%20reason%20effectively%20in%20complex%2C%20dynamic%0Asettings.%20Despite%20rapid%20progress%2C%20current%20world%20models%20remain%20brittle%20and%0Adegrade%20over%20long%20horizons.%20We%20argue%20that%20a%20central%20cause%20is%20representation%0Aquality%3A%20exteroceptive%20inputs%20%28e.g.%2C%20images%29%20are%20high-dimensional%2C%20and%20lossy%20or%0Aentangled%20latents%20make%20dynamics%20learning%20unnecessarily%20hard.%20We%20therefore%20ask%0Awhether%20improving%20representation%20learning%20alone%20can%20substantially%20improve%0Aworld-model%20performance.%20In%20this%20work%2C%20we%20take%20a%20step%20toward%20building%20a%20truly%0Aaccurate%20world%20model%20by%20addressing%20a%20fundamental%20yet%20open%20problem%3A%20constructing%0Aa%20model%20that%20can%20fully%20clone%20and%20overfit%20to%20a%20deterministic%203D%20world.%20We%0Apropose%20Geometrically-Regularized%20World%20Models%20%28GRWM%29%2C%20which%20enforces%20that%0Aconsecutive%20points%20along%20a%20natural%20sensory%20trajectory%20remain%20close%20in%20latent%0Arepresentation%20space.%20This%20approach%20yields%20significantly%20improved%20latent%0Arepresentations%20that%20align%20closely%20with%20the%20true%20topology%20of%20the%20environment.%0AGRWM%20is%20plug-and-play%2C%20requires%20only%20minimal%20architectural%20modification%2C%20scales%0Awith%20trajectory%20length%2C%20and%20is%20compatible%20with%20diverse%20latent%20generative%0Abackbones.%20Across%20deterministic%203D%20settings%20and%20long-horizon%20prediction%20tasks%2C%0AGRWM%20significantly%20increases%20rollout%20fidelity%20and%20stability.%20Analyses%20show%20that%0Aits%20benefits%20stem%20from%20learning%20a%20latent%20manifold%20with%20superior%20geometric%0Astructure.%20These%20findings%20support%20a%20clear%20takeaway%3A%20improving%20representation%0Alearning%20is%20a%20direct%20and%20useful%20path%20to%20robust%20world%20models%2C%20delivering%0Areliable%20long-horizon%20predictions%20without%20enlarging%20the%20dynamics%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26782v1&entry.124074799=Read"},
{"title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy", "author": "William Overman and Mohsen Bayati", "abstract": "  As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.\n", "link": "http://arxiv.org/abs/2510.26752v1", "date": "2025-10-30", "relevancy": 1.676, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5959}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Oversight%20Game%3A%20Learning%20to%20Cooperatively%20Balance%20an%20AI%20Agent%27s%0A%20%20Safety%20and%20Autonomy&body=Title%3A%20The%20Oversight%20Game%3A%20Learning%20to%20Cooperatively%20Balance%20an%20AI%20Agent%27s%0A%20%20Safety%20and%20Autonomy%0AAuthor%3A%20William%20Overman%20and%20Mohsen%20Bayati%0AAbstract%3A%20%20%20As%20increasingly%20capable%20agents%20are%20deployed%2C%20a%20central%20safety%20question%20is%20how%0Ato%20retain%20meaningful%20human%20control%20without%20modifying%20the%20underlying%20system.%20We%0Astudy%20a%20minimal%20control%20interface%20where%20an%20agent%20chooses%20whether%20to%20act%0Aautonomously%20%28play%29%20or%20defer%20%28ask%29%2C%20while%20a%20human%20simultaneously%20chooses%0Awhether%20to%20be%20permissive%20%28trust%29%20or%20to%20engage%20in%20oversight%20%28oversee%29.%20If%20the%0Aagent%20defers%2C%20the%20human%27s%20choice%20determines%20the%20outcome%2C%20potentially%20leading%20to%0Aa%20corrective%20action%20or%20a%20system%20shutdown.%20We%20model%20this%20interaction%20as%20a%0Atwo-player%20Markov%20Game.%20Our%20analysis%20focuses%20on%20cases%20where%20this%20game%20qualifies%0Aas%20a%20Markov%20Potential%20Game%20%28MPG%29%2C%20a%20class%20of%20games%20where%20we%20can%20provide%20an%0Aalignment%20guarantee%3A%20under%20a%20structural%20assumption%20on%20the%20human%27s%20value%0Afunction%2C%20any%20decision%20by%20the%20agent%20to%20act%20more%20autonomously%20that%20benefits%0Aitself%20cannot%20harm%20the%20human%27s%20value.%20We%20also%20analyze%20extensions%20to%20this%20MPG%0Aframework.%20Theoretically%2C%20this%20perspective%20provides%20conditions%20for%20a%20specific%0Aform%20of%20intrinsic%20alignment.%20If%20the%20reward%20structures%20of%20the%20human-agent%20game%0Ameet%20these%20conditions%2C%20we%20have%20a%20formal%20guarantee%20that%20the%20agent%20improving%20its%0Aown%20outcome%20will%20not%20harm%20the%20human%27s.%20Practically%2C%20this%20model%20motivates%20a%0Atransparent%20control%20layer%20with%20predictable%20incentives%20where%20the%20agent%20learns%20to%0Adefer%20when%20risky%20and%20act%20when%20safe%2C%20while%20its%20pretrained%20policy%20and%20the%0Aenvironment%27s%20reward%20structure%20remain%20untouched.%20Our%20gridworld%20simulation%20shows%0Athat%20through%20independent%20learning%2C%20the%20agent%20and%20human%20discover%20their%20optimal%0Aoversight%20roles.%20The%20agent%20learns%20to%20ask%20when%20uncertain%20and%20the%20human%20learns%0Awhen%20to%20oversee%2C%20leading%20to%20an%20emergent%20collaboration%20that%20avoids%20safety%0Aviolations%20introduced%20post-training.%20This%20demonstrates%20a%20practical%20method%20for%0Amaking%20misaligned%20models%20safer%20after%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Oversight%2520Game%253A%2520Learning%2520to%2520Cooperatively%2520Balance%2520an%2520AI%2520Agent%2527s%250A%2520%2520Safety%2520and%2520Autonomy%26entry.906535625%3DWilliam%2520Overman%2520and%2520Mohsen%2520Bayati%26entry.1292438233%3D%2520%2520As%2520increasingly%2520capable%2520agents%2520are%2520deployed%252C%2520a%2520central%2520safety%2520question%2520is%2520how%250Ato%2520retain%2520meaningful%2520human%2520control%2520without%2520modifying%2520the%2520underlying%2520system.%2520We%250Astudy%2520a%2520minimal%2520control%2520interface%2520where%2520an%2520agent%2520chooses%2520whether%2520to%2520act%250Aautonomously%2520%2528play%2529%2520or%2520defer%2520%2528ask%2529%252C%2520while%2520a%2520human%2520simultaneously%2520chooses%250Awhether%2520to%2520be%2520permissive%2520%2528trust%2529%2520or%2520to%2520engage%2520in%2520oversight%2520%2528oversee%2529.%2520If%2520the%250Aagent%2520defers%252C%2520the%2520human%2527s%2520choice%2520determines%2520the%2520outcome%252C%2520potentially%2520leading%2520to%250Aa%2520corrective%2520action%2520or%2520a%2520system%2520shutdown.%2520We%2520model%2520this%2520interaction%2520as%2520a%250Atwo-player%2520Markov%2520Game.%2520Our%2520analysis%2520focuses%2520on%2520cases%2520where%2520this%2520game%2520qualifies%250Aas%2520a%2520Markov%2520Potential%2520Game%2520%2528MPG%2529%252C%2520a%2520class%2520of%2520games%2520where%2520we%2520can%2520provide%2520an%250Aalignment%2520guarantee%253A%2520under%2520a%2520structural%2520assumption%2520on%2520the%2520human%2527s%2520value%250Afunction%252C%2520any%2520decision%2520by%2520the%2520agent%2520to%2520act%2520more%2520autonomously%2520that%2520benefits%250Aitself%2520cannot%2520harm%2520the%2520human%2527s%2520value.%2520We%2520also%2520analyze%2520extensions%2520to%2520this%2520MPG%250Aframework.%2520Theoretically%252C%2520this%2520perspective%2520provides%2520conditions%2520for%2520a%2520specific%250Aform%2520of%2520intrinsic%2520alignment.%2520If%2520the%2520reward%2520structures%2520of%2520the%2520human-agent%2520game%250Ameet%2520these%2520conditions%252C%2520we%2520have%2520a%2520formal%2520guarantee%2520that%2520the%2520agent%2520improving%2520its%250Aown%2520outcome%2520will%2520not%2520harm%2520the%2520human%2527s.%2520Practically%252C%2520this%2520model%2520motivates%2520a%250Atransparent%2520control%2520layer%2520with%2520predictable%2520incentives%2520where%2520the%2520agent%2520learns%2520to%250Adefer%2520when%2520risky%2520and%2520act%2520when%2520safe%252C%2520while%2520its%2520pretrained%2520policy%2520and%2520the%250Aenvironment%2527s%2520reward%2520structure%2520remain%2520untouched.%2520Our%2520gridworld%2520simulation%2520shows%250Athat%2520through%2520independent%2520learning%252C%2520the%2520agent%2520and%2520human%2520discover%2520their%2520optimal%250Aoversight%2520roles.%2520The%2520agent%2520learns%2520to%2520ask%2520when%2520uncertain%2520and%2520the%2520human%2520learns%250Awhen%2520to%2520oversee%252C%2520leading%2520to%2520an%2520emergent%2520collaboration%2520that%2520avoids%2520safety%250Aviolations%2520introduced%2520post-training.%2520This%2520demonstrates%2520a%2520practical%2520method%2520for%250Amaking%2520misaligned%2520models%2520safer%2520after%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Oversight%20Game%3A%20Learning%20to%20Cooperatively%20Balance%20an%20AI%20Agent%27s%0A%20%20Safety%20and%20Autonomy&entry.906535625=William%20Overman%20and%20Mohsen%20Bayati&entry.1292438233=%20%20As%20increasingly%20capable%20agents%20are%20deployed%2C%20a%20central%20safety%20question%20is%20how%0Ato%20retain%20meaningful%20human%20control%20without%20modifying%20the%20underlying%20system.%20We%0Astudy%20a%20minimal%20control%20interface%20where%20an%20agent%20chooses%20whether%20to%20act%0Aautonomously%20%28play%29%20or%20defer%20%28ask%29%2C%20while%20a%20human%20simultaneously%20chooses%0Awhether%20to%20be%20permissive%20%28trust%29%20or%20to%20engage%20in%20oversight%20%28oversee%29.%20If%20the%0Aagent%20defers%2C%20the%20human%27s%20choice%20determines%20the%20outcome%2C%20potentially%20leading%20to%0Aa%20corrective%20action%20or%20a%20system%20shutdown.%20We%20model%20this%20interaction%20as%20a%0Atwo-player%20Markov%20Game.%20Our%20analysis%20focuses%20on%20cases%20where%20this%20game%20qualifies%0Aas%20a%20Markov%20Potential%20Game%20%28MPG%29%2C%20a%20class%20of%20games%20where%20we%20can%20provide%20an%0Aalignment%20guarantee%3A%20under%20a%20structural%20assumption%20on%20the%20human%27s%20value%0Afunction%2C%20any%20decision%20by%20the%20agent%20to%20act%20more%20autonomously%20that%20benefits%0Aitself%20cannot%20harm%20the%20human%27s%20value.%20We%20also%20analyze%20extensions%20to%20this%20MPG%0Aframework.%20Theoretically%2C%20this%20perspective%20provides%20conditions%20for%20a%20specific%0Aform%20of%20intrinsic%20alignment.%20If%20the%20reward%20structures%20of%20the%20human-agent%20game%0Ameet%20these%20conditions%2C%20we%20have%20a%20formal%20guarantee%20that%20the%20agent%20improving%20its%0Aown%20outcome%20will%20not%20harm%20the%20human%27s.%20Practically%2C%20this%20model%20motivates%20a%0Atransparent%20control%20layer%20with%20predictable%20incentives%20where%20the%20agent%20learns%20to%0Adefer%20when%20risky%20and%20act%20when%20safe%2C%20while%20its%20pretrained%20policy%20and%20the%0Aenvironment%27s%20reward%20structure%20remain%20untouched.%20Our%20gridworld%20simulation%20shows%0Athat%20through%20independent%20learning%2C%20the%20agent%20and%20human%20discover%20their%20optimal%0Aoversight%20roles.%20The%20agent%20learns%20to%20ask%20when%20uncertain%20and%20the%20human%20learns%0Awhen%20to%20oversee%2C%20leading%20to%20an%20emergent%20collaboration%20that%20avoids%20safety%0Aviolations%20introduced%20post-training.%20This%20demonstrates%20a%20practical%20method%20for%0Amaking%20misaligned%20models%20safer%20after%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26752v1&entry.124074799=Read"},
{"title": "Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and\n  Real-Time Efficiency in Robotic Manipulation", "author": "Qianyou Zhao and Yuliang Shen and Xuanran Zhai and Ce Hao and Duidi Wu and Jin Qi and Jie Hu and Qiaojun Yu", "abstract": "  In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies.\n", "link": "http://arxiv.org/abs/2510.26670v1", "date": "2025-10-30", "relevancy": 1.67, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6091}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5457}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Consistency%20Policy%3A%20Decoupling%20Multi-Modal%20Diversity%20and%0A%20%20Real-Time%20Efficiency%20in%20Robotic%20Manipulation&body=Title%3A%20Hybrid%20Consistency%20Policy%3A%20Decoupling%20Multi-Modal%20Diversity%20and%0A%20%20Real-Time%20Efficiency%20in%20Robotic%20Manipulation%0AAuthor%3A%20Qianyou%20Zhao%20and%20Yuliang%20Shen%20and%20Xuanran%20Zhai%20and%20Ce%20Hao%20and%20Duidi%20Wu%20and%20Jin%20Qi%20and%20Jie%20Hu%20and%20Qiaojun%20Yu%0AAbstract%3A%20%20%20In%20visuomotor%20policy%20learning%2C%20diffusion-based%20imitation%20learning%20has%20become%0Awidely%20adopted%20for%20its%20ability%20to%20capture%20diverse%20behaviors.%20However%2C%0Aapproaches%20built%20on%20ordinary%20and%20stochastic%20denoising%20processes%20struggle%20to%0Ajointly%20achieve%20fast%20sampling%20and%20strong%20multi-modality.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Hybrid%20Consistency%20Policy%20%28HCP%29.%20HCP%20runs%20a%20short%0Astochastic%20prefix%20up%20to%20an%20adaptive%20switch%20time%2C%20and%20then%20applies%20a%20one-step%0Aconsistency%20jump%20to%20produce%20the%20final%20action.%20To%20align%20this%20one-jump%0Ageneration%2C%20HCP%20performs%20time-varying%20consistency%20distillation%20that%20combines%20a%0Atrajectory-consistency%20objective%20to%20keep%20neighboring%20predictions%20coherent%20and%20a%0Adenoising-matching%20objective%20to%20improve%20local%20fidelity.%20In%20both%20simulation%20and%0Aon%20a%20real%20robot%2C%20HCP%20with%2025%20SDE%20steps%20plus%20one%20jump%20approaches%20the%2080-step%0ADDPM%20teacher%20in%20accuracy%20and%20mode%20coverage%20while%20significantly%20reducing%0Alatency.%20These%20results%20show%20that%20multi-modality%20does%20not%20require%20slow%0Ainference%2C%20and%20a%20switch%20time%20decouples%20mode%20retention%20from%20speed.%20It%20yields%20a%0Apractical%20accuracy%20efficiency%20trade-off%20for%20robot%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Consistency%2520Policy%253A%2520Decoupling%2520Multi-Modal%2520Diversity%2520and%250A%2520%2520Real-Time%2520Efficiency%2520in%2520Robotic%2520Manipulation%26entry.906535625%3DQianyou%2520Zhao%2520and%2520Yuliang%2520Shen%2520and%2520Xuanran%2520Zhai%2520and%2520Ce%2520Hao%2520and%2520Duidi%2520Wu%2520and%2520Jin%2520Qi%2520and%2520Jie%2520Hu%2520and%2520Qiaojun%2520Yu%26entry.1292438233%3D%2520%2520In%2520visuomotor%2520policy%2520learning%252C%2520diffusion-based%2520imitation%2520learning%2520has%2520become%250Awidely%2520adopted%2520for%2520its%2520ability%2520to%2520capture%2520diverse%2520behaviors.%2520However%252C%250Aapproaches%2520built%2520on%2520ordinary%2520and%2520stochastic%2520denoising%2520processes%2520struggle%2520to%250Ajointly%2520achieve%2520fast%2520sampling%2520and%2520strong%2520multi-modality.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520Hybrid%2520Consistency%2520Policy%2520%2528HCP%2529.%2520HCP%2520runs%2520a%2520short%250Astochastic%2520prefix%2520up%2520to%2520an%2520adaptive%2520switch%2520time%252C%2520and%2520then%2520applies%2520a%2520one-step%250Aconsistency%2520jump%2520to%2520produce%2520the%2520final%2520action.%2520To%2520align%2520this%2520one-jump%250Ageneration%252C%2520HCP%2520performs%2520time-varying%2520consistency%2520distillation%2520that%2520combines%2520a%250Atrajectory-consistency%2520objective%2520to%2520keep%2520neighboring%2520predictions%2520coherent%2520and%2520a%250Adenoising-matching%2520objective%2520to%2520improve%2520local%2520fidelity.%2520In%2520both%2520simulation%2520and%250Aon%2520a%2520real%2520robot%252C%2520HCP%2520with%252025%2520SDE%2520steps%2520plus%2520one%2520jump%2520approaches%2520the%252080-step%250ADDPM%2520teacher%2520in%2520accuracy%2520and%2520mode%2520coverage%2520while%2520significantly%2520reducing%250Alatency.%2520These%2520results%2520show%2520that%2520multi-modality%2520does%2520not%2520require%2520slow%250Ainference%252C%2520and%2520a%2520switch%2520time%2520decouples%2520mode%2520retention%2520from%2520speed.%2520It%2520yields%2520a%250Apractical%2520accuracy%2520efficiency%2520trade-off%2520for%2520robot%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Consistency%20Policy%3A%20Decoupling%20Multi-Modal%20Diversity%20and%0A%20%20Real-Time%20Efficiency%20in%20Robotic%20Manipulation&entry.906535625=Qianyou%20Zhao%20and%20Yuliang%20Shen%20and%20Xuanran%20Zhai%20and%20Ce%20Hao%20and%20Duidi%20Wu%20and%20Jin%20Qi%20and%20Jie%20Hu%20and%20Qiaojun%20Yu&entry.1292438233=%20%20In%20visuomotor%20policy%20learning%2C%20diffusion-based%20imitation%20learning%20has%20become%0Awidely%20adopted%20for%20its%20ability%20to%20capture%20diverse%20behaviors.%20However%2C%0Aapproaches%20built%20on%20ordinary%20and%20stochastic%20denoising%20processes%20struggle%20to%0Ajointly%20achieve%20fast%20sampling%20and%20strong%20multi-modality.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Hybrid%20Consistency%20Policy%20%28HCP%29.%20HCP%20runs%20a%20short%0Astochastic%20prefix%20up%20to%20an%20adaptive%20switch%20time%2C%20and%20then%20applies%20a%20one-step%0Aconsistency%20jump%20to%20produce%20the%20final%20action.%20To%20align%20this%20one-jump%0Ageneration%2C%20HCP%20performs%20time-varying%20consistency%20distillation%20that%20combines%20a%0Atrajectory-consistency%20objective%20to%20keep%20neighboring%20predictions%20coherent%20and%20a%0Adenoising-matching%20objective%20to%20improve%20local%20fidelity.%20In%20both%20simulation%20and%0Aon%20a%20real%20robot%2C%20HCP%20with%2025%20SDE%20steps%20plus%20one%20jump%20approaches%20the%2080-step%0ADDPM%20teacher%20in%20accuracy%20and%20mode%20coverage%20while%20significantly%20reducing%0Alatency.%20These%20results%20show%20that%20multi-modality%20does%20not%20require%20slow%0Ainference%2C%20and%20a%20switch%20time%20decouples%20mode%20retention%20from%20speed.%20It%20yields%20a%0Apractical%20accuracy%20efficiency%20trade-off%20for%20robot%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26670v1&entry.124074799=Read"},
{"title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models", "author": "Chihan Huang and Hao Tang", "abstract": "  Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures.\n", "link": "http://arxiv.org/abs/2507.06078v2", "date": "2025-10-30", "relevancy": 1.6673, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5657}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5542}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models&body=Title%3A%20ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models%0AAuthor%3A%20Chihan%20Huang%20and%20Hao%20Tang%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20deep%20learning%20across%20various%20domains%2C%20it%20remains%0Avulnerable%20to%20adversarial%20attacks.%20Although%20many%20existing%20adversarial%20attack%0Amethods%20achieve%20high%20success%20rates%2C%20they%20typically%20rely%20on%20%24%5Cell_%7Bp%7D%24-norm%0Aperturbation%20constraints%2C%20which%20do%20not%20align%20with%20human%20perceptual%0Acapabilities.%20Consequently%2C%20researchers%20have%20shifted%20their%20focus%20toward%0Agenerating%20natural%2C%20unrestricted%20adversarial%20examples%20%28UAEs%29.%20GAN-based%0Aapproaches%20suffer%20from%20inherent%20limitations%2C%20such%20as%20poor%20image%20quality%20due%20to%0Ainstability%20and%20mode%20collapse.%20Meanwhile%2C%20diffusion%20models%20have%20been%20employed%0Afor%20UAE%20generation%2C%20but%20they%20still%20rely%20on%20iterative%20PGD%20perturbation%0Ainjection%2C%20without%20fully%20leveraging%20their%20central%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20generating%20UAEs%20based%20on%0Adiffusion%20models%2C%20named%20ScoreAdv.%20This%20method%20incorporates%20an%20interpretable%0Aadversarial%20guidance%20mechanism%20to%20gradually%20shift%20the%20sampling%20distribution%0Atowards%20the%20adversarial%20distribution%2C%20while%20using%20an%20interpretable%20saliency%20map%0Ato%20inject%20the%20visual%20information%20of%20a%20reference%20image%20into%20the%20generated%0Asamples.%20Notably%2C%20our%20method%20is%20capable%20of%20generating%20an%20unlimited%20number%20of%0Anatural%20adversarial%20examples%20and%20can%20attack%20not%20only%20classification%20models%20but%0Aalso%20retrieval%20models.%20We%20conduct%20extensive%20experiments%20on%20ImageNet%20and%20CelebA%0Adatasets%2C%20validating%20the%20performance%20of%20ScoreAdv%20across%20ten%20target%20models%20in%0Aboth%20black-box%20and%20white-box%20settings.%20Our%20results%20demonstrate%20that%20ScoreAdv%0Aachieves%20state-of-the-art%20attack%20success%20rates%20and%20image%20quality%2C%20while%0Amaintaining%20inference%20efficiency.%20Furthermore%2C%20the%20dynamic%20balance%20between%0Adenoising%20and%20adversarial%20perturbation%20enables%20ScoreAdv%20to%20remain%20robust%20even%0Aunder%20defensive%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScoreAdv%253A%2520Score-based%2520Targeted%2520Generation%2520of%2520Natural%2520Adversarial%250A%2520%2520Examples%2520via%2520Diffusion%2520Models%26entry.906535625%3DChihan%2520Huang%2520and%2520Hao%2520Tang%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520deep%2520learning%2520across%2520various%2520domains%252C%2520it%2520remains%250Avulnerable%2520to%2520adversarial%2520attacks.%2520Although%2520many%2520existing%2520adversarial%2520attack%250Amethods%2520achieve%2520high%2520success%2520rates%252C%2520they%2520typically%2520rely%2520on%2520%2524%255Cell_%257Bp%257D%2524-norm%250Aperturbation%2520constraints%252C%2520which%2520do%2520not%2520align%2520with%2520human%2520perceptual%250Acapabilities.%2520Consequently%252C%2520researchers%2520have%2520shifted%2520their%2520focus%2520toward%250Agenerating%2520natural%252C%2520unrestricted%2520adversarial%2520examples%2520%2528UAEs%2529.%2520GAN-based%250Aapproaches%2520suffer%2520from%2520inherent%2520limitations%252C%2520such%2520as%2520poor%2520image%2520quality%2520due%2520to%250Ainstability%2520and%2520mode%2520collapse.%2520Meanwhile%252C%2520diffusion%2520models%2520have%2520been%2520employed%250Afor%2520UAE%2520generation%252C%2520but%2520they%2520still%2520rely%2520on%2520iterative%2520PGD%2520perturbation%250Ainjection%252C%2520without%2520fully%2520leveraging%2520their%2520central%2520denoising%2520capabilities.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520generating%2520UAEs%2520based%2520on%250Adiffusion%2520models%252C%2520named%2520ScoreAdv.%2520This%2520method%2520incorporates%2520an%2520interpretable%250Aadversarial%2520guidance%2520mechanism%2520to%2520gradually%2520shift%2520the%2520sampling%2520distribution%250Atowards%2520the%2520adversarial%2520distribution%252C%2520while%2520using%2520an%2520interpretable%2520saliency%2520map%250Ato%2520inject%2520the%2520visual%2520information%2520of%2520a%2520reference%2520image%2520into%2520the%2520generated%250Asamples.%2520Notably%252C%2520our%2520method%2520is%2520capable%2520of%2520generating%2520an%2520unlimited%2520number%2520of%250Anatural%2520adversarial%2520examples%2520and%2520can%2520attack%2520not%2520only%2520classification%2520models%2520but%250Aalso%2520retrieval%2520models.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520ImageNet%2520and%2520CelebA%250Adatasets%252C%2520validating%2520the%2520performance%2520of%2520ScoreAdv%2520across%2520ten%2520target%2520models%2520in%250Aboth%2520black-box%2520and%2520white-box%2520settings.%2520Our%2520results%2520demonstrate%2520that%2520ScoreAdv%250Aachieves%2520state-of-the-art%2520attack%2520success%2520rates%2520and%2520image%2520quality%252C%2520while%250Amaintaining%2520inference%2520efficiency.%2520Furthermore%252C%2520the%2520dynamic%2520balance%2520between%250Adenoising%2520and%2520adversarial%2520perturbation%2520enables%2520ScoreAdv%2520to%2520remain%2520robust%2520even%250Aunder%2520defensive%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models&entry.906535625=Chihan%20Huang%20and%20Hao%20Tang&entry.1292438233=%20%20Despite%20the%20success%20of%20deep%20learning%20across%20various%20domains%2C%20it%20remains%0Avulnerable%20to%20adversarial%20attacks.%20Although%20many%20existing%20adversarial%20attack%0Amethods%20achieve%20high%20success%20rates%2C%20they%20typically%20rely%20on%20%24%5Cell_%7Bp%7D%24-norm%0Aperturbation%20constraints%2C%20which%20do%20not%20align%20with%20human%20perceptual%0Acapabilities.%20Consequently%2C%20researchers%20have%20shifted%20their%20focus%20toward%0Agenerating%20natural%2C%20unrestricted%20adversarial%20examples%20%28UAEs%29.%20GAN-based%0Aapproaches%20suffer%20from%20inherent%20limitations%2C%20such%20as%20poor%20image%20quality%20due%20to%0Ainstability%20and%20mode%20collapse.%20Meanwhile%2C%20diffusion%20models%20have%20been%20employed%0Afor%20UAE%20generation%2C%20but%20they%20still%20rely%20on%20iterative%20PGD%20perturbation%0Ainjection%2C%20without%20fully%20leveraging%20their%20central%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20generating%20UAEs%20based%20on%0Adiffusion%20models%2C%20named%20ScoreAdv.%20This%20method%20incorporates%20an%20interpretable%0Aadversarial%20guidance%20mechanism%20to%20gradually%20shift%20the%20sampling%20distribution%0Atowards%20the%20adversarial%20distribution%2C%20while%20using%20an%20interpretable%20saliency%20map%0Ato%20inject%20the%20visual%20information%20of%20a%20reference%20image%20into%20the%20generated%0Asamples.%20Notably%2C%20our%20method%20is%20capable%20of%20generating%20an%20unlimited%20number%20of%0Anatural%20adversarial%20examples%20and%20can%20attack%20not%20only%20classification%20models%20but%0Aalso%20retrieval%20models.%20We%20conduct%20extensive%20experiments%20on%20ImageNet%20and%20CelebA%0Adatasets%2C%20validating%20the%20performance%20of%20ScoreAdv%20across%20ten%20target%20models%20in%0Aboth%20black-box%20and%20white-box%20settings.%20Our%20results%20demonstrate%20that%20ScoreAdv%0Aachieves%20state-of-the-art%20attack%20success%20rates%20and%20image%20quality%2C%20while%0Amaintaining%20inference%20efficiency.%20Furthermore%2C%20the%20dynamic%20balance%20between%0Adenoising%20and%20adversarial%20perturbation%20enables%20ScoreAdv%20to%20remain%20robust%20even%0Aunder%20defensive%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06078v2&entry.124074799=Read"},
{"title": "REALMS2 -- Resilient Exploration And Lunar Mapping System 2 -- A\n  Comprehensive Approach", "author": "Dave van der Meer and Lo\u00efck P. Chovet and Gabriel M. Garcia and Abhishek Bera and Miguel A. Olivares-Mendez", "abstract": "  The European Space Agency (ESA) and the European Space Resources Innovation\nCentre (ESRIC) created the Space Resources Challenge to invite researchers and\ncompanies to propose innovative solutions for Multi-Robot Systems (MRS) space\nprospection. This paper proposes the Resilient Exploration And Lunar Mapping\nSystem 2 (REALMS2), a MRS framework for planetary prospection and mapping.\nBased on Robot Operating System version 2 (ROS 2) and enhanced with Visual\nSimultaneous Localisation And Mapping (vSLAM) for map generation, REALMS2 uses\na mesh network for a robust ad hoc network. A single graphical user interface\n(GUI) controls all the rovers, providing a simple overview of the robotic\nmission. This system is designed for heterogeneous multi-robot exploratory\nmissions, tackling the challenges presented by extraterrestrial environments.\nREALMS2 was used during the second field test of the ESA-ESRIC Challenge and\nallowed to map around 60% of the area, using three homogeneous rovers while\nhandling communication delays and blackouts.\n", "link": "http://arxiv.org/abs/2510.26638v1", "date": "2025-10-30", "relevancy": 1.6397, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5437}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REALMS2%20--%20Resilient%20Exploration%20And%20Lunar%20Mapping%20System%202%20--%20A%0A%20%20Comprehensive%20Approach&body=Title%3A%20REALMS2%20--%20Resilient%20Exploration%20And%20Lunar%20Mapping%20System%202%20--%20A%0A%20%20Comprehensive%20Approach%0AAuthor%3A%20Dave%20van%20der%20Meer%20and%20Lo%C3%AFck%20P.%20Chovet%20and%20Gabriel%20M.%20Garcia%20and%20Abhishek%20Bera%20and%20Miguel%20A.%20Olivares-Mendez%0AAbstract%3A%20%20%20The%20European%20Space%20Agency%20%28ESA%29%20and%20the%20European%20Space%20Resources%20Innovation%0ACentre%20%28ESRIC%29%20created%20the%20Space%20Resources%20Challenge%20to%20invite%20researchers%20and%0Acompanies%20to%20propose%20innovative%20solutions%20for%20Multi-Robot%20Systems%20%28MRS%29%20space%0Aprospection.%20This%20paper%20proposes%20the%20Resilient%20Exploration%20And%20Lunar%20Mapping%0ASystem%202%20%28REALMS2%29%2C%20a%20MRS%20framework%20for%20planetary%20prospection%20and%20mapping.%0ABased%20on%20Robot%20Operating%20System%20version%202%20%28ROS%202%29%20and%20enhanced%20with%20Visual%0ASimultaneous%20Localisation%20And%20Mapping%20%28vSLAM%29%20for%20map%20generation%2C%20REALMS2%20uses%0Aa%20mesh%20network%20for%20a%20robust%20ad%20hoc%20network.%20A%20single%20graphical%20user%20interface%0A%28GUI%29%20controls%20all%20the%20rovers%2C%20providing%20a%20simple%20overview%20of%20the%20robotic%0Amission.%20This%20system%20is%20designed%20for%20heterogeneous%20multi-robot%20exploratory%0Amissions%2C%20tackling%20the%20challenges%20presented%20by%20extraterrestrial%20environments.%0AREALMS2%20was%20used%20during%20the%20second%20field%20test%20of%20the%20ESA-ESRIC%20Challenge%20and%0Aallowed%20to%20map%20around%2060%25%20of%20the%20area%2C%20using%20three%20homogeneous%20rovers%20while%0Ahandling%20communication%20delays%20and%20blackouts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREALMS2%2520--%2520Resilient%2520Exploration%2520And%2520Lunar%2520Mapping%2520System%25202%2520--%2520A%250A%2520%2520Comprehensive%2520Approach%26entry.906535625%3DDave%2520van%2520der%2520Meer%2520and%2520Lo%25C3%25AFck%2520P.%2520Chovet%2520and%2520Gabriel%2520M.%2520Garcia%2520and%2520Abhishek%2520Bera%2520and%2520Miguel%2520A.%2520Olivares-Mendez%26entry.1292438233%3D%2520%2520The%2520European%2520Space%2520Agency%2520%2528ESA%2529%2520and%2520the%2520European%2520Space%2520Resources%2520Innovation%250ACentre%2520%2528ESRIC%2529%2520created%2520the%2520Space%2520Resources%2520Challenge%2520to%2520invite%2520researchers%2520and%250Acompanies%2520to%2520propose%2520innovative%2520solutions%2520for%2520Multi-Robot%2520Systems%2520%2528MRS%2529%2520space%250Aprospection.%2520This%2520paper%2520proposes%2520the%2520Resilient%2520Exploration%2520And%2520Lunar%2520Mapping%250ASystem%25202%2520%2528REALMS2%2529%252C%2520a%2520MRS%2520framework%2520for%2520planetary%2520prospection%2520and%2520mapping.%250ABased%2520on%2520Robot%2520Operating%2520System%2520version%25202%2520%2528ROS%25202%2529%2520and%2520enhanced%2520with%2520Visual%250ASimultaneous%2520Localisation%2520And%2520Mapping%2520%2528vSLAM%2529%2520for%2520map%2520generation%252C%2520REALMS2%2520uses%250Aa%2520mesh%2520network%2520for%2520a%2520robust%2520ad%2520hoc%2520network.%2520A%2520single%2520graphical%2520user%2520interface%250A%2528GUI%2529%2520controls%2520all%2520the%2520rovers%252C%2520providing%2520a%2520simple%2520overview%2520of%2520the%2520robotic%250Amission.%2520This%2520system%2520is%2520designed%2520for%2520heterogeneous%2520multi-robot%2520exploratory%250Amissions%252C%2520tackling%2520the%2520challenges%2520presented%2520by%2520extraterrestrial%2520environments.%250AREALMS2%2520was%2520used%2520during%2520the%2520second%2520field%2520test%2520of%2520the%2520ESA-ESRIC%2520Challenge%2520and%250Aallowed%2520to%2520map%2520around%252060%2525%2520of%2520the%2520area%252C%2520using%2520three%2520homogeneous%2520rovers%2520while%250Ahandling%2520communication%2520delays%2520and%2520blackouts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REALMS2%20--%20Resilient%20Exploration%20And%20Lunar%20Mapping%20System%202%20--%20A%0A%20%20Comprehensive%20Approach&entry.906535625=Dave%20van%20der%20Meer%20and%20Lo%C3%AFck%20P.%20Chovet%20and%20Gabriel%20M.%20Garcia%20and%20Abhishek%20Bera%20and%20Miguel%20A.%20Olivares-Mendez&entry.1292438233=%20%20The%20European%20Space%20Agency%20%28ESA%29%20and%20the%20European%20Space%20Resources%20Innovation%0ACentre%20%28ESRIC%29%20created%20the%20Space%20Resources%20Challenge%20to%20invite%20researchers%20and%0Acompanies%20to%20propose%20innovative%20solutions%20for%20Multi-Robot%20Systems%20%28MRS%29%20space%0Aprospection.%20This%20paper%20proposes%20the%20Resilient%20Exploration%20And%20Lunar%20Mapping%0ASystem%202%20%28REALMS2%29%2C%20a%20MRS%20framework%20for%20planetary%20prospection%20and%20mapping.%0ABased%20on%20Robot%20Operating%20System%20version%202%20%28ROS%202%29%20and%20enhanced%20with%20Visual%0ASimultaneous%20Localisation%20And%20Mapping%20%28vSLAM%29%20for%20map%20generation%2C%20REALMS2%20uses%0Aa%20mesh%20network%20for%20a%20robust%20ad%20hoc%20network.%20A%20single%20graphical%20user%20interface%0A%28GUI%29%20controls%20all%20the%20rovers%2C%20providing%20a%20simple%20overview%20of%20the%20robotic%0Amission.%20This%20system%20is%20designed%20for%20heterogeneous%20multi-robot%20exploratory%0Amissions%2C%20tackling%20the%20challenges%20presented%20by%20extraterrestrial%20environments.%0AREALMS2%20was%20used%20during%20the%20second%20field%20test%20of%20the%20ESA-ESRIC%20Challenge%20and%0Aallowed%20to%20map%20around%2060%25%20of%20the%20area%2C%20using%20three%20homogeneous%20rovers%20while%0Ahandling%20communication%20delays%20and%20blackouts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26638v1&entry.124074799=Read"},
{"title": "Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems", "author": "Georgios Kamaras and Craig Innes and Subramanian Ramamoorthy", "abstract": "  In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.\n", "link": "http://arxiv.org/abs/2510.26656v1", "date": "2025-10-30", "relevancy": 1.6357, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heuristic%20Adaptation%20of%20Potentially%20Misspecified%20Domain%20Support%20for%0A%20%20Likelihood-Free%20Inference%20in%20Stochastic%20Dynamical%20Systems&body=Title%3A%20Heuristic%20Adaptation%20of%20Potentially%20Misspecified%20Domain%20Support%20for%0A%20%20Likelihood-Free%20Inference%20in%20Stochastic%20Dynamical%20Systems%0AAuthor%3A%20Georgios%20Kamaras%20and%20Craig%20Innes%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20In%20robotics%2C%20likelihood-free%20inference%20%28LFI%29%20can%20provide%20the%20domain%0Adistribution%20that%20adapts%20a%20learnt%20agent%20in%20a%20parametric%20set%20of%20deployment%0Aconditions.%20LFI%20assumes%20an%20arbitrary%20support%20for%20sampling%2C%20which%20remains%0Aconstant%20as%20the%20initial%20generic%20prior%20is%20iteratively%20refined%20to%20more%0Adescriptive%20posteriors.%20However%2C%20a%20potentially%20misspecified%20support%20can%20lead%20to%0Asuboptimal%2C%20yet%20falsely%20certain%2C%20posteriors.%20To%20address%20this%20issue%2C%20we%20propose%0Athree%20heuristic%20LFI%20variants%3A%20EDGE%2C%20MODE%2C%20and%20CENTRE.%20Each%20interprets%20the%0Aposterior%20mode%20shift%20over%20inference%20steps%20in%20its%20own%20way%20and%2C%20when%20integrated%0Ainto%20an%20LFI%20step%2C%20adapts%20the%20support%20alongside%20posterior%20inference.%20We%20first%0Aexpose%20the%20support%20misspecification%20issue%20and%20evaluate%20our%20heuristics%20using%0Astochastic%20dynamical%20benchmarks.%20We%20then%20evaluate%20the%20impact%20of%20heuristic%0Asupport%20adaptation%20on%20parameter%20inference%20and%20policy%20learning%20for%20a%20dynamic%0Adeformable%20linear%20object%20%28DLO%29%20manipulation%20task.%20Inference%20results%20in%20a%20finer%0Alength%20and%20stiffness%20classification%20for%20a%20parametric%20set%20of%20DLOs.%20When%20the%0Aresulting%20posteriors%20are%20used%20as%20domain%20distributions%20for%20sim-based%20policy%0Alearning%2C%20they%20lead%20to%20more%20robust%20object-centric%20agent%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeuristic%2520Adaptation%2520of%2520Potentially%2520Misspecified%2520Domain%2520Support%2520for%250A%2520%2520Likelihood-Free%2520Inference%2520in%2520Stochastic%2520Dynamical%2520Systems%26entry.906535625%3DGeorgios%2520Kamaras%2520and%2520Craig%2520Innes%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520In%2520robotics%252C%2520likelihood-free%2520inference%2520%2528LFI%2529%2520can%2520provide%2520the%2520domain%250Adistribution%2520that%2520adapts%2520a%2520learnt%2520agent%2520in%2520a%2520parametric%2520set%2520of%2520deployment%250Aconditions.%2520LFI%2520assumes%2520an%2520arbitrary%2520support%2520for%2520sampling%252C%2520which%2520remains%250Aconstant%2520as%2520the%2520initial%2520generic%2520prior%2520is%2520iteratively%2520refined%2520to%2520more%250Adescriptive%2520posteriors.%2520However%252C%2520a%2520potentially%2520misspecified%2520support%2520can%2520lead%2520to%250Asuboptimal%252C%2520yet%2520falsely%2520certain%252C%2520posteriors.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250Athree%2520heuristic%2520LFI%2520variants%253A%2520EDGE%252C%2520MODE%252C%2520and%2520CENTRE.%2520Each%2520interprets%2520the%250Aposterior%2520mode%2520shift%2520over%2520inference%2520steps%2520in%2520its%2520own%2520way%2520and%252C%2520when%2520integrated%250Ainto%2520an%2520LFI%2520step%252C%2520adapts%2520the%2520support%2520alongside%2520posterior%2520inference.%2520We%2520first%250Aexpose%2520the%2520support%2520misspecification%2520issue%2520and%2520evaluate%2520our%2520heuristics%2520using%250Astochastic%2520dynamical%2520benchmarks.%2520We%2520then%2520evaluate%2520the%2520impact%2520of%2520heuristic%250Asupport%2520adaptation%2520on%2520parameter%2520inference%2520and%2520policy%2520learning%2520for%2520a%2520dynamic%250Adeformable%2520linear%2520object%2520%2528DLO%2529%2520manipulation%2520task.%2520Inference%2520results%2520in%2520a%2520finer%250Alength%2520and%2520stiffness%2520classification%2520for%2520a%2520parametric%2520set%2520of%2520DLOs.%2520When%2520the%250Aresulting%2520posteriors%2520are%2520used%2520as%2520domain%2520distributions%2520for%2520sim-based%2520policy%250Alearning%252C%2520they%2520lead%2520to%2520more%2520robust%2520object-centric%2520agent%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heuristic%20Adaptation%20of%20Potentially%20Misspecified%20Domain%20Support%20for%0A%20%20Likelihood-Free%20Inference%20in%20Stochastic%20Dynamical%20Systems&entry.906535625=Georgios%20Kamaras%20and%20Craig%20Innes%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20In%20robotics%2C%20likelihood-free%20inference%20%28LFI%29%20can%20provide%20the%20domain%0Adistribution%20that%20adapts%20a%20learnt%20agent%20in%20a%20parametric%20set%20of%20deployment%0Aconditions.%20LFI%20assumes%20an%20arbitrary%20support%20for%20sampling%2C%20which%20remains%0Aconstant%20as%20the%20initial%20generic%20prior%20is%20iteratively%20refined%20to%20more%0Adescriptive%20posteriors.%20However%2C%20a%20potentially%20misspecified%20support%20can%20lead%20to%0Asuboptimal%2C%20yet%20falsely%20certain%2C%20posteriors.%20To%20address%20this%20issue%2C%20we%20propose%0Athree%20heuristic%20LFI%20variants%3A%20EDGE%2C%20MODE%2C%20and%20CENTRE.%20Each%20interprets%20the%0Aposterior%20mode%20shift%20over%20inference%20steps%20in%20its%20own%20way%20and%2C%20when%20integrated%0Ainto%20an%20LFI%20step%2C%20adapts%20the%20support%20alongside%20posterior%20inference.%20We%20first%0Aexpose%20the%20support%20misspecification%20issue%20and%20evaluate%20our%20heuristics%20using%0Astochastic%20dynamical%20benchmarks.%20We%20then%20evaluate%20the%20impact%20of%20heuristic%0Asupport%20adaptation%20on%20parameter%20inference%20and%20policy%20learning%20for%20a%20dynamic%0Adeformable%20linear%20object%20%28DLO%29%20manipulation%20task.%20Inference%20results%20in%20a%20finer%0Alength%20and%20stiffness%20classification%20for%20a%20parametric%20set%20of%20DLOs.%20When%20the%0Aresulting%20posteriors%20are%20used%20as%20domain%20distributions%20for%20sim-based%20policy%0Alearning%2C%20they%20lead%20to%20more%20robust%20object-centric%20agent%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26656v1&entry.124074799=Read"},
{"title": "DDL: A Large-Scale Datasets for Deepfake Detection and Localization in\n  Diversified Real-World Scenarios", "author": "Changtao Miao and Yi Zhang and Weize Gao and Zhiya Tan and Weiwei Feng and Man Luo and Jianshu Li and Ajian Liu and Yunfeng Diao and Qi Chu and Tao Gong and Zhe Li and Weibin Yao and Joey Tianyi Zhou", "abstract": "  Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability.\nRecent studies have attempted to enhance the interpretability of classification\nresults by providing spatial manipulation masks or temporal forgery segments.\nHowever, due to the limitations of forgery datasets, the practical\neffectiveness of these methods remains suboptimal. The primary reason lies in\nthe fact that most existing deepfake datasets contain only binary labels, with\nlimited variety in forgery scenarios, insufficient diversity in deepfake types,\nand relatively small data scales, making them inadequate for complex real-world\nscenarios.To address this predicament, we construct a novel large-scale\ndeepfake detection and localization (\\textbf{DDL}) dataset containing over\n$\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct\ndeepfake methods. The DDL design incorporates four key innovations: (1)\n\\textbf{Comprehensive Deepfake Methods} (covering 7 different generation\narchitectures and a total of 80 methods), (2) \\textbf{Varied Manipulation\nModes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse\nForgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and\n(4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial\nmasks and 0.23M+ precise temporal segments).Through these improvements, our DDL\nnot only provides a more challenging benchmark for complex real-world forgeries\nbut also offers crucial support for building next-generation deepfake\ndetection, localization, and interpretability methods.\n", "link": "http://arxiv.org/abs/2506.23292v2", "date": "2025-10-30", "relevancy": 1.6339, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5597}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5438}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDL%3A%20A%20Large-Scale%20Datasets%20for%20Deepfake%20Detection%20and%20Localization%20in%0A%20%20Diversified%20Real-World%20Scenarios&body=Title%3A%20DDL%3A%20A%20Large-Scale%20Datasets%20for%20Deepfake%20Detection%20and%20Localization%20in%0A%20%20Diversified%20Real-World%20Scenarios%0AAuthor%3A%20Changtao%20Miao%20and%20Yi%20Zhang%20and%20Weize%20Gao%20and%20Zhiya%20Tan%20and%20Weiwei%20Feng%20and%20Man%20Luo%20and%20Jianshu%20Li%20and%20Ajian%20Liu%20and%20Yunfeng%20Diao%20and%20Qi%20Chu%20and%20Tao%20Gong%20and%20Zhe%20Li%20and%20Weibin%20Yao%20and%20Joey%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20AIGC%20have%20exacerbated%20the%20misuse%20of%20malicious%20deepfake%0Acontent%2C%20making%20the%20development%20of%20reliable%20deepfake%20detection%20methods%20an%0Aessential%20means%20to%20address%20this%20challenge.%20Although%20existing%20deepfake%20detection%0Amodels%20demonstrate%20outstanding%20performance%20in%20detection%20metrics%2C%20most%20methods%0Aonly%20provide%20simple%20binary%20classification%20results%2C%20lacking%20interpretability.%0ARecent%20studies%20have%20attempted%20to%20enhance%20the%20interpretability%20of%20classification%0Aresults%20by%20providing%20spatial%20manipulation%20masks%20or%20temporal%20forgery%20segments.%0AHowever%2C%20due%20to%20the%20limitations%20of%20forgery%20datasets%2C%20the%20practical%0Aeffectiveness%20of%20these%20methods%20remains%20suboptimal.%20The%20primary%20reason%20lies%20in%0Athe%20fact%20that%20most%20existing%20deepfake%20datasets%20contain%20only%20binary%20labels%2C%20with%0Alimited%20variety%20in%20forgery%20scenarios%2C%20insufficient%20diversity%20in%20deepfake%20types%2C%0Aand%20relatively%20small%20data%20scales%2C%20making%20them%20inadequate%20for%20complex%20real-world%0Ascenarios.To%20address%20this%20predicament%2C%20we%20construct%20a%20novel%20large-scale%0Adeepfake%20detection%20and%20localization%20%28%5Ctextbf%7BDDL%7D%29%20dataset%20containing%20over%0A%24%5Ctextbf%7B1.4M%2B%7D%24%20forged%20samples%20and%20encompassing%20up%20to%20%24%5Ctextbf%7B80%7D%24%20distinct%0Adeepfake%20methods.%20The%20DDL%20design%20incorporates%20four%20key%20innovations%3A%20%281%29%0A%5Ctextbf%7BComprehensive%20Deepfake%20Methods%7D%20%28covering%207%20different%20generation%0Aarchitectures%20and%20a%20total%20of%2080%20methods%29%2C%20%282%29%20%5Ctextbf%7BVaried%20Manipulation%0AModes%7D%20%28incorporating%207%20classic%20and%203%20novel%20forgery%20modes%29%2C%20%283%29%20%5Ctextbf%7BDiverse%0AForgery%20Scenarios%20and%20Modalities%7D%20%28including%203%20scenarios%20and%203%20modalities%29%2C%20and%0A%284%29%20%5Ctextbf%7BFine-grained%20Forgery%20Annotations%7D%20%28providing%201.18M%2B%20precise%20spatial%0Amasks%20and%200.23M%2B%20precise%20temporal%20segments%29.Through%20these%20improvements%2C%20our%20DDL%0Anot%20only%20provides%20a%20more%20challenging%20benchmark%20for%20complex%20real-world%20forgeries%0Abut%20also%20offers%20crucial%20support%20for%20building%20next-generation%20deepfake%0Adetection%2C%20localization%2C%20and%20interpretability%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDL%253A%2520A%2520Large-Scale%2520Datasets%2520for%2520Deepfake%2520Detection%2520and%2520Localization%2520in%250A%2520%2520Diversified%2520Real-World%2520Scenarios%26entry.906535625%3DChangtao%2520Miao%2520and%2520Yi%2520Zhang%2520and%2520Weize%2520Gao%2520and%2520Zhiya%2520Tan%2520and%2520Weiwei%2520Feng%2520and%2520Man%2520Luo%2520and%2520Jianshu%2520Li%2520and%2520Ajian%2520Liu%2520and%2520Yunfeng%2520Diao%2520and%2520Qi%2520Chu%2520and%2520Tao%2520Gong%2520and%2520Zhe%2520Li%2520and%2520Weibin%2520Yao%2520and%2520Joey%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AIGC%2520have%2520exacerbated%2520the%2520misuse%2520of%2520malicious%2520deepfake%250Acontent%252C%2520making%2520the%2520development%2520of%2520reliable%2520deepfake%2520detection%2520methods%2520an%250Aessential%2520means%2520to%2520address%2520this%2520challenge.%2520Although%2520existing%2520deepfake%2520detection%250Amodels%2520demonstrate%2520outstanding%2520performance%2520in%2520detection%2520metrics%252C%2520most%2520methods%250Aonly%2520provide%2520simple%2520binary%2520classification%2520results%252C%2520lacking%2520interpretability.%250ARecent%2520studies%2520have%2520attempted%2520to%2520enhance%2520the%2520interpretability%2520of%2520classification%250Aresults%2520by%2520providing%2520spatial%2520manipulation%2520masks%2520or%2520temporal%2520forgery%2520segments.%250AHowever%252C%2520due%2520to%2520the%2520limitations%2520of%2520forgery%2520datasets%252C%2520the%2520practical%250Aeffectiveness%2520of%2520these%2520methods%2520remains%2520suboptimal.%2520The%2520primary%2520reason%2520lies%2520in%250Athe%2520fact%2520that%2520most%2520existing%2520deepfake%2520datasets%2520contain%2520only%2520binary%2520labels%252C%2520with%250Alimited%2520variety%2520in%2520forgery%2520scenarios%252C%2520insufficient%2520diversity%2520in%2520deepfake%2520types%252C%250Aand%2520relatively%2520small%2520data%2520scales%252C%2520making%2520them%2520inadequate%2520for%2520complex%2520real-world%250Ascenarios.To%2520address%2520this%2520predicament%252C%2520we%2520construct%2520a%2520novel%2520large-scale%250Adeepfake%2520detection%2520and%2520localization%2520%2528%255Ctextbf%257BDDL%257D%2529%2520dataset%2520containing%2520over%250A%2524%255Ctextbf%257B1.4M%252B%257D%2524%2520forged%2520samples%2520and%2520encompassing%2520up%2520to%2520%2524%255Ctextbf%257B80%257D%2524%2520distinct%250Adeepfake%2520methods.%2520The%2520DDL%2520design%2520incorporates%2520four%2520key%2520innovations%253A%2520%25281%2529%250A%255Ctextbf%257BComprehensive%2520Deepfake%2520Methods%257D%2520%2528covering%25207%2520different%2520generation%250Aarchitectures%2520and%2520a%2520total%2520of%252080%2520methods%2529%252C%2520%25282%2529%2520%255Ctextbf%257BVaried%2520Manipulation%250AModes%257D%2520%2528incorporating%25207%2520classic%2520and%25203%2520novel%2520forgery%2520modes%2529%252C%2520%25283%2529%2520%255Ctextbf%257BDiverse%250AForgery%2520Scenarios%2520and%2520Modalities%257D%2520%2528including%25203%2520scenarios%2520and%25203%2520modalities%2529%252C%2520and%250A%25284%2529%2520%255Ctextbf%257BFine-grained%2520Forgery%2520Annotations%257D%2520%2528providing%25201.18M%252B%2520precise%2520spatial%250Amasks%2520and%25200.23M%252B%2520precise%2520temporal%2520segments%2529.Through%2520these%2520improvements%252C%2520our%2520DDL%250Anot%2520only%2520provides%2520a%2520more%2520challenging%2520benchmark%2520for%2520complex%2520real-world%2520forgeries%250Abut%2520also%2520offers%2520crucial%2520support%2520for%2520building%2520next-generation%2520deepfake%250Adetection%252C%2520localization%252C%2520and%2520interpretability%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDL%3A%20A%20Large-Scale%20Datasets%20for%20Deepfake%20Detection%20and%20Localization%20in%0A%20%20Diversified%20Real-World%20Scenarios&entry.906535625=Changtao%20Miao%20and%20Yi%20Zhang%20and%20Weize%20Gao%20and%20Zhiya%20Tan%20and%20Weiwei%20Feng%20and%20Man%20Luo%20and%20Jianshu%20Li%20and%20Ajian%20Liu%20and%20Yunfeng%20Diao%20and%20Qi%20Chu%20and%20Tao%20Gong%20and%20Zhe%20Li%20and%20Weibin%20Yao%20and%20Joey%20Tianyi%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20AIGC%20have%20exacerbated%20the%20misuse%20of%20malicious%20deepfake%0Acontent%2C%20making%20the%20development%20of%20reliable%20deepfake%20detection%20methods%20an%0Aessential%20means%20to%20address%20this%20challenge.%20Although%20existing%20deepfake%20detection%0Amodels%20demonstrate%20outstanding%20performance%20in%20detection%20metrics%2C%20most%20methods%0Aonly%20provide%20simple%20binary%20classification%20results%2C%20lacking%20interpretability.%0ARecent%20studies%20have%20attempted%20to%20enhance%20the%20interpretability%20of%20classification%0Aresults%20by%20providing%20spatial%20manipulation%20masks%20or%20temporal%20forgery%20segments.%0AHowever%2C%20due%20to%20the%20limitations%20of%20forgery%20datasets%2C%20the%20practical%0Aeffectiveness%20of%20these%20methods%20remains%20suboptimal.%20The%20primary%20reason%20lies%20in%0Athe%20fact%20that%20most%20existing%20deepfake%20datasets%20contain%20only%20binary%20labels%2C%20with%0Alimited%20variety%20in%20forgery%20scenarios%2C%20insufficient%20diversity%20in%20deepfake%20types%2C%0Aand%20relatively%20small%20data%20scales%2C%20making%20them%20inadequate%20for%20complex%20real-world%0Ascenarios.To%20address%20this%20predicament%2C%20we%20construct%20a%20novel%20large-scale%0Adeepfake%20detection%20and%20localization%20%28%5Ctextbf%7BDDL%7D%29%20dataset%20containing%20over%0A%24%5Ctextbf%7B1.4M%2B%7D%24%20forged%20samples%20and%20encompassing%20up%20to%20%24%5Ctextbf%7B80%7D%24%20distinct%0Adeepfake%20methods.%20The%20DDL%20design%20incorporates%20four%20key%20innovations%3A%20%281%29%0A%5Ctextbf%7BComprehensive%20Deepfake%20Methods%7D%20%28covering%207%20different%20generation%0Aarchitectures%20and%20a%20total%20of%2080%20methods%29%2C%20%282%29%20%5Ctextbf%7BVaried%20Manipulation%0AModes%7D%20%28incorporating%207%20classic%20and%203%20novel%20forgery%20modes%29%2C%20%283%29%20%5Ctextbf%7BDiverse%0AForgery%20Scenarios%20and%20Modalities%7D%20%28including%203%20scenarios%20and%203%20modalities%29%2C%20and%0A%284%29%20%5Ctextbf%7BFine-grained%20Forgery%20Annotations%7D%20%28providing%201.18M%2B%20precise%20spatial%0Amasks%20and%200.23M%2B%20precise%20temporal%20segments%29.Through%20these%20improvements%2C%20our%20DDL%0Anot%20only%20provides%20a%20more%20challenging%20benchmark%20for%20complex%20real-world%20forgeries%0Abut%20also%20offers%20crucial%20support%20for%20building%20next-generation%20deepfake%0Adetection%2C%20localization%2C%20and%20interpretability%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23292v2&entry.124074799=Read"},
{"title": "On Purely Private Covariance Estimation", "author": "Tommaso d'Orsi and Gleb Novikov", "abstract": "  We present a simple perturbation mechanism for the release of $d$-dimensional\ncovariance matrices $\\Sigma$ under pure differential privacy. For large\ndatasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers\nthe provably optimal Frobenius norm error guarantees of\n\\cite{nikolov2023private}, while simultaneously achieving best known error for\nall other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is\ninformation-theoretically optimal for all $p\\ge 2$, in particular, our\nmechanism is the first purely private covariance estimator that achieves\noptimal error in spectral norm.\n  For small datasets $n< d^2/\\varepsilon$, we further show that by projecting\nthe output onto the nuclear norm ball of appropriate radius, our algorithm\nachieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$,\nimproving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private}\nand ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of\n\\cite{dong2022differentially}.\n", "link": "http://arxiv.org/abs/2510.26717v1", "date": "2025-10-30", "relevancy": 1.6008, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4176}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3898}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Purely%20Private%20Covariance%20Estimation&body=Title%3A%20On%20Purely%20Private%20Covariance%20Estimation%0AAuthor%3A%20Tommaso%20d%27Orsi%20and%20Gleb%20Novikov%0AAbstract%3A%20%20%20We%20present%20a%20simple%20perturbation%20mechanism%20for%20the%20release%20of%20%24d%24-dimensional%0Acovariance%20matrices%20%24%5CSigma%24%20under%20pure%20differential%20privacy.%20For%20large%0Adatasets%20with%20at%20least%20%24n%5Cgeq%20d%5E2/%5Cvarepsilon%24%20elements%2C%20our%20mechanism%20recovers%0Athe%20provably%20optimal%20Frobenius%20norm%20error%20guarantees%20of%0A%5Ccite%7Bnikolov2023private%7D%2C%20while%20simultaneously%20achieving%20best%20known%20error%20for%0Aall%20other%20%24p%24-Schatten%20norms%2C%20with%20%24p%5Cin%20%5B1%2C%5Cinfty%5D%24.%20Our%20error%20is%0Ainformation-theoretically%20optimal%20for%20all%20%24p%5Cge%202%24%2C%20in%20particular%2C%20our%0Amechanism%20is%20the%20first%20purely%20private%20covariance%20estimator%20that%20achieves%0Aoptimal%20error%20in%20spectral%20norm.%0A%20%20For%20small%20datasets%20%24n%3C%20d%5E2/%5Cvarepsilon%24%2C%20we%20further%20show%20that%20by%20projecting%0Athe%20output%20onto%20the%20nuclear%20norm%20ball%20of%20appropriate%20radius%2C%20our%20algorithm%0Aachieves%20the%20optimal%20Frobenius%20norm%20error%20%24O%28%5Csqrt%7Bd%5C%3B%5Ctext%7BTr%7D%28%5CSigma%29%20/n%7D%29%24%2C%0Aimproving%20over%20the%20known%20bounds%20of%20%24O%28%5Csqrt%7Bd/n%7D%29%24%20of%20%5Ccite%7Bnikolov2023private%7D%0Aand%20%24%7BO%7D%5Cbig%28d%5E%7B3/4%7D%5Csqrt%7B%5Ctext%7BTr%7D%28%5CSigma%29/n%7D%5Cbig%29%24%20of%0A%5Ccite%7Bdong2022differentially%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Purely%2520Private%2520Covariance%2520Estimation%26entry.906535625%3DTommaso%2520d%2527Orsi%2520and%2520Gleb%2520Novikov%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520simple%2520perturbation%2520mechanism%2520for%2520the%2520release%2520of%2520%2524d%2524-dimensional%250Acovariance%2520matrices%2520%2524%255CSigma%2524%2520under%2520pure%2520differential%2520privacy.%2520For%2520large%250Adatasets%2520with%2520at%2520least%2520%2524n%255Cgeq%2520d%255E2/%255Cvarepsilon%2524%2520elements%252C%2520our%2520mechanism%2520recovers%250Athe%2520provably%2520optimal%2520Frobenius%2520norm%2520error%2520guarantees%2520of%250A%255Ccite%257Bnikolov2023private%257D%252C%2520while%2520simultaneously%2520achieving%2520best%2520known%2520error%2520for%250Aall%2520other%2520%2524p%2524-Schatten%2520norms%252C%2520with%2520%2524p%255Cin%2520%255B1%252C%255Cinfty%255D%2524.%2520Our%2520error%2520is%250Ainformation-theoretically%2520optimal%2520for%2520all%2520%2524p%255Cge%25202%2524%252C%2520in%2520particular%252C%2520our%250Amechanism%2520is%2520the%2520first%2520purely%2520private%2520covariance%2520estimator%2520that%2520achieves%250Aoptimal%2520error%2520in%2520spectral%2520norm.%250A%2520%2520For%2520small%2520datasets%2520%2524n%253C%2520d%255E2/%255Cvarepsilon%2524%252C%2520we%2520further%2520show%2520that%2520by%2520projecting%250Athe%2520output%2520onto%2520the%2520nuclear%2520norm%2520ball%2520of%2520appropriate%2520radius%252C%2520our%2520algorithm%250Aachieves%2520the%2520optimal%2520Frobenius%2520norm%2520error%2520%2524O%2528%255Csqrt%257Bd%255C%253B%255Ctext%257BTr%257D%2528%255CSigma%2529%2520/n%257D%2529%2524%252C%250Aimproving%2520over%2520the%2520known%2520bounds%2520of%2520%2524O%2528%255Csqrt%257Bd/n%257D%2529%2524%2520of%2520%255Ccite%257Bnikolov2023private%257D%250Aand%2520%2524%257BO%257D%255Cbig%2528d%255E%257B3/4%257D%255Csqrt%257B%255Ctext%257BTr%257D%2528%255CSigma%2529/n%257D%255Cbig%2529%2524%2520of%250A%255Ccite%257Bdong2022differentially%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Purely%20Private%20Covariance%20Estimation&entry.906535625=Tommaso%20d%27Orsi%20and%20Gleb%20Novikov&entry.1292438233=%20%20We%20present%20a%20simple%20perturbation%20mechanism%20for%20the%20release%20of%20%24d%24-dimensional%0Acovariance%20matrices%20%24%5CSigma%24%20under%20pure%20differential%20privacy.%20For%20large%0Adatasets%20with%20at%20least%20%24n%5Cgeq%20d%5E2/%5Cvarepsilon%24%20elements%2C%20our%20mechanism%20recovers%0Athe%20provably%20optimal%20Frobenius%20norm%20error%20guarantees%20of%0A%5Ccite%7Bnikolov2023private%7D%2C%20while%20simultaneously%20achieving%20best%20known%20error%20for%0Aall%20other%20%24p%24-Schatten%20norms%2C%20with%20%24p%5Cin%20%5B1%2C%5Cinfty%5D%24.%20Our%20error%20is%0Ainformation-theoretically%20optimal%20for%20all%20%24p%5Cge%202%24%2C%20in%20particular%2C%20our%0Amechanism%20is%20the%20first%20purely%20private%20covariance%20estimator%20that%20achieves%0Aoptimal%20error%20in%20spectral%20norm.%0A%20%20For%20small%20datasets%20%24n%3C%20d%5E2/%5Cvarepsilon%24%2C%20we%20further%20show%20that%20by%20projecting%0Athe%20output%20onto%20the%20nuclear%20norm%20ball%20of%20appropriate%20radius%2C%20our%20algorithm%0Aachieves%20the%20optimal%20Frobenius%20norm%20error%20%24O%28%5Csqrt%7Bd%5C%3B%5Ctext%7BTr%7D%28%5CSigma%29%20/n%7D%29%24%2C%0Aimproving%20over%20the%20known%20bounds%20of%20%24O%28%5Csqrt%7Bd/n%7D%29%24%20of%20%5Ccite%7Bnikolov2023private%7D%0Aand%20%24%7BO%7D%5Cbig%28d%5E%7B3/4%7D%5Csqrt%7B%5Ctext%7BTr%7D%28%5CSigma%29/n%7D%5Cbig%29%24%20of%0A%5Ccite%7Bdong2022differentially%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26717v1&entry.124074799=Read"},
{"title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet\n  Coefficient", "author": "Rinku Sebastian and Simon O'Keefe and Martin Trefzer", "abstract": "  Extracting features from the speech is the most critical process in speech\nsignal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most\nwidely used features in the majority of the speaker and speech recognition\napplications, as the filtering in this feature is similar to the filtering\ntaking place in the human ear. But the main drawback of this feature is that it\nprovides only the frequency information of the signal but does not provide the\ninformation about at what time which frequency is present. The wavelet\ntransform, with its flexible time-frequency window, provides time and frequency\ninformation of the signal and is an appropriate tool for the analysis of\nnon-stationary signals like speech. On the other hand, because of its uniform\nfrequency scaling, a typical wavelet transform may be less effective in\nanalysing speech signals, have poorer frequency resolution in low frequencies,\nand be less in line with human auditory perception. Hence, it is necessary to\ndevelop a feature that incorporates the merits of both MFCC and wavelet\ntransform. A great deal of studies are trying to combine both these features.\nThe present Wavelet Transform based Mel-scaled feature extraction methods\nrequire more computation when a wavelet transform is applied on top of\nMel-scale filtering, since it adds extra processing steps. Here we are\nproposing a method to extract Mel scale features in time domain combining the\nconcept of wavelet transform, thus reducing the computational burden of\ntime-frequency conversion and the complexity of wavelet extraction. Combining\nour proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique\nwith the reservoir computing methodology has significantly improved the\nefficiency of audio signal processing.\n", "link": "http://arxiv.org/abs/2510.24519v2", "date": "2025-10-30", "relevancy": 1.5811, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4102}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4046}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Signal%20Processing%20Using%20Time%20Domain%20Mel-Frequency%20Wavelet%0A%20%20Coefficient&body=Title%3A%20Audio%20Signal%20Processing%20Using%20Time%20Domain%20Mel-Frequency%20Wavelet%0A%20%20Coefficient%0AAuthor%3A%20Rinku%20Sebastian%20and%20Simon%20O%27Keefe%20and%20Martin%20Trefzer%0AAbstract%3A%20%20%20Extracting%20features%20from%20the%20speech%20is%20the%20most%20critical%20process%20in%20speech%0Asignal%20processing.%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20are%20the%20most%0Awidely%20used%20features%20in%20the%20majority%20of%20the%20speaker%20and%20speech%20recognition%0Aapplications%2C%20as%20the%20filtering%20in%20this%20feature%20is%20similar%20to%20the%20filtering%0Ataking%20place%20in%20the%20human%20ear.%20But%20the%20main%20drawback%20of%20this%20feature%20is%20that%20it%0Aprovides%20only%20the%20frequency%20information%20of%20the%20signal%20but%20does%20not%20provide%20the%0Ainformation%20about%20at%20what%20time%20which%20frequency%20is%20present.%20The%20wavelet%0Atransform%2C%20with%20its%20flexible%20time-frequency%20window%2C%20provides%20time%20and%20frequency%0Ainformation%20of%20the%20signal%20and%20is%20an%20appropriate%20tool%20for%20the%20analysis%20of%0Anon-stationary%20signals%20like%20speech.%20On%20the%20other%20hand%2C%20because%20of%20its%20uniform%0Afrequency%20scaling%2C%20a%20typical%20wavelet%20transform%20may%20be%20less%20effective%20in%0Aanalysing%20speech%20signals%2C%20have%20poorer%20frequency%20resolution%20in%20low%20frequencies%2C%0Aand%20be%20less%20in%20line%20with%20human%20auditory%20perception.%20Hence%2C%20it%20is%20necessary%20to%0Adevelop%20a%20feature%20that%20incorporates%20the%20merits%20of%20both%20MFCC%20and%20wavelet%0Atransform.%20A%20great%20deal%20of%20studies%20are%20trying%20to%20combine%20both%20these%20features.%0AThe%20present%20Wavelet%20Transform%20based%20Mel-scaled%20feature%20extraction%20methods%0Arequire%20more%20computation%20when%20a%20wavelet%20transform%20is%20applied%20on%20top%20of%0AMel-scale%20filtering%2C%20since%20it%20adds%20extra%20processing%20steps.%20Here%20we%20are%0Aproposing%20a%20method%20to%20extract%20Mel%20scale%20features%20in%20time%20domain%20combining%20the%0Aconcept%20of%20wavelet%20transform%2C%20thus%20reducing%20the%20computational%20burden%20of%0Atime-frequency%20conversion%20and%20the%20complexity%20of%20wavelet%20extraction.%20Combining%0Aour%20proposed%20Time%20domain%20Mel%20frequency%20Wavelet%20Coefficient%28TMFWC%29%20technique%0Awith%20the%20reservoir%20computing%20methodology%20has%20significantly%20improved%20the%0Aefficiency%20of%20audio%20signal%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.24519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Signal%2520Processing%2520Using%2520Time%2520Domain%2520Mel-Frequency%2520Wavelet%250A%2520%2520Coefficient%26entry.906535625%3DRinku%2520Sebastian%2520and%2520Simon%2520O%2527Keefe%2520and%2520Martin%2520Trefzer%26entry.1292438233%3D%2520%2520Extracting%2520features%2520from%2520the%2520speech%2520is%2520the%2520most%2520critical%2520process%2520in%2520speech%250Asignal%2520processing.%2520Mel%2520Frequency%2520Cepstral%2520Coefficients%2520%2528MFCC%2529%2520are%2520the%2520most%250Awidely%2520used%2520features%2520in%2520the%2520majority%2520of%2520the%2520speaker%2520and%2520speech%2520recognition%250Aapplications%252C%2520as%2520the%2520filtering%2520in%2520this%2520feature%2520is%2520similar%2520to%2520the%2520filtering%250Ataking%2520place%2520in%2520the%2520human%2520ear.%2520But%2520the%2520main%2520drawback%2520of%2520this%2520feature%2520is%2520that%2520it%250Aprovides%2520only%2520the%2520frequency%2520information%2520of%2520the%2520signal%2520but%2520does%2520not%2520provide%2520the%250Ainformation%2520about%2520at%2520what%2520time%2520which%2520frequency%2520is%2520present.%2520The%2520wavelet%250Atransform%252C%2520with%2520its%2520flexible%2520time-frequency%2520window%252C%2520provides%2520time%2520and%2520frequency%250Ainformation%2520of%2520the%2520signal%2520and%2520is%2520an%2520appropriate%2520tool%2520for%2520the%2520analysis%2520of%250Anon-stationary%2520signals%2520like%2520speech.%2520On%2520the%2520other%2520hand%252C%2520because%2520of%2520its%2520uniform%250Afrequency%2520scaling%252C%2520a%2520typical%2520wavelet%2520transform%2520may%2520be%2520less%2520effective%2520in%250Aanalysing%2520speech%2520signals%252C%2520have%2520poorer%2520frequency%2520resolution%2520in%2520low%2520frequencies%252C%250Aand%2520be%2520less%2520in%2520line%2520with%2520human%2520auditory%2520perception.%2520Hence%252C%2520it%2520is%2520necessary%2520to%250Adevelop%2520a%2520feature%2520that%2520incorporates%2520the%2520merits%2520of%2520both%2520MFCC%2520and%2520wavelet%250Atransform.%2520A%2520great%2520deal%2520of%2520studies%2520are%2520trying%2520to%2520combine%2520both%2520these%2520features.%250AThe%2520present%2520Wavelet%2520Transform%2520based%2520Mel-scaled%2520feature%2520extraction%2520methods%250Arequire%2520more%2520computation%2520when%2520a%2520wavelet%2520transform%2520is%2520applied%2520on%2520top%2520of%250AMel-scale%2520filtering%252C%2520since%2520it%2520adds%2520extra%2520processing%2520steps.%2520Here%2520we%2520are%250Aproposing%2520a%2520method%2520to%2520extract%2520Mel%2520scale%2520features%2520in%2520time%2520domain%2520combining%2520the%250Aconcept%2520of%2520wavelet%2520transform%252C%2520thus%2520reducing%2520the%2520computational%2520burden%2520of%250Atime-frequency%2520conversion%2520and%2520the%2520complexity%2520of%2520wavelet%2520extraction.%2520Combining%250Aour%2520proposed%2520Time%2520domain%2520Mel%2520frequency%2520Wavelet%2520Coefficient%2528TMFWC%2529%2520technique%250Awith%2520the%2520reservoir%2520computing%2520methodology%2520has%2520significantly%2520improved%2520the%250Aefficiency%2520of%2520audio%2520signal%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Signal%20Processing%20Using%20Time%20Domain%20Mel-Frequency%20Wavelet%0A%20%20Coefficient&entry.906535625=Rinku%20Sebastian%20and%20Simon%20O%27Keefe%20and%20Martin%20Trefzer&entry.1292438233=%20%20Extracting%20features%20from%20the%20speech%20is%20the%20most%20critical%20process%20in%20speech%0Asignal%20processing.%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20are%20the%20most%0Awidely%20used%20features%20in%20the%20majority%20of%20the%20speaker%20and%20speech%20recognition%0Aapplications%2C%20as%20the%20filtering%20in%20this%20feature%20is%20similar%20to%20the%20filtering%0Ataking%20place%20in%20the%20human%20ear.%20But%20the%20main%20drawback%20of%20this%20feature%20is%20that%20it%0Aprovides%20only%20the%20frequency%20information%20of%20the%20signal%20but%20does%20not%20provide%20the%0Ainformation%20about%20at%20what%20time%20which%20frequency%20is%20present.%20The%20wavelet%0Atransform%2C%20with%20its%20flexible%20time-frequency%20window%2C%20provides%20time%20and%20frequency%0Ainformation%20of%20the%20signal%20and%20is%20an%20appropriate%20tool%20for%20the%20analysis%20of%0Anon-stationary%20signals%20like%20speech.%20On%20the%20other%20hand%2C%20because%20of%20its%20uniform%0Afrequency%20scaling%2C%20a%20typical%20wavelet%20transform%20may%20be%20less%20effective%20in%0Aanalysing%20speech%20signals%2C%20have%20poorer%20frequency%20resolution%20in%20low%20frequencies%2C%0Aand%20be%20less%20in%20line%20with%20human%20auditory%20perception.%20Hence%2C%20it%20is%20necessary%20to%0Adevelop%20a%20feature%20that%20incorporates%20the%20merits%20of%20both%20MFCC%20and%20wavelet%0Atransform.%20A%20great%20deal%20of%20studies%20are%20trying%20to%20combine%20both%20these%20features.%0AThe%20present%20Wavelet%20Transform%20based%20Mel-scaled%20feature%20extraction%20methods%0Arequire%20more%20computation%20when%20a%20wavelet%20transform%20is%20applied%20on%20top%20of%0AMel-scale%20filtering%2C%20since%20it%20adds%20extra%20processing%20steps.%20Here%20we%20are%0Aproposing%20a%20method%20to%20extract%20Mel%20scale%20features%20in%20time%20domain%20combining%20the%0Aconcept%20of%20wavelet%20transform%2C%20thus%20reducing%20the%20computational%20burden%20of%0Atime-frequency%20conversion%20and%20the%20complexity%20of%20wavelet%20extraction.%20Combining%0Aour%20proposed%20Time%20domain%20Mel%20frequency%20Wavelet%20Coefficient%28TMFWC%29%20technique%0Awith%20the%20reservoir%20computing%20methodology%20has%20significantly%20improved%20the%0Aefficiency%20of%20audio%20signal%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.24519v2&entry.124074799=Read"},
{"title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization", "author": "Marco Federici and Riccardo Del Chiaro and Boris van Breugel and Paul Whatmough and Markus Nagel", "abstract": "  Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.\n", "link": "http://arxiv.org/abs/2510.26771v1", "date": "2025-10-30", "relevancy": 1.5685, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5357}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5279}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STaMP%3A%20Sequence%20Transformation%20and%20Mixed%20Precision%20for%20Low-Precision%0A%20%20Activation%20Quantization&body=Title%3A%20STaMP%3A%20Sequence%20Transformation%20and%20Mixed%20Precision%20for%20Low-Precision%0A%20%20Activation%20Quantization%0AAuthor%3A%20Marco%20Federici%20and%20Riccardo%20Del%20Chiaro%20and%20Boris%20van%20Breugel%20and%20Paul%20Whatmough%20and%20Markus%20Nagel%0AAbstract%3A%20%20%20Quantization%20is%20the%20key%20method%20for%20reducing%20inference%20latency%2C%20power%20and%0Amemory%20footprint%20of%20generative%20AI%20models.%20However%2C%20accuracy%20often%20degrades%0Asharply%20when%20activations%20are%20quantized%20below%20eight%20bits.%20Recent%20work%20suggests%0Athat%20invertible%20linear%20transformations%20%28e.g.%20rotations%29%20can%20aid%20quantization%2C%0Aby%20reparameterizing%20feature%20channels%20and%20weights.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextit%7BSequence%20Transformation%20and%20Mixed%20Precision%7D%20%28STaMP%29%20quantization%2C%20a%0Anovel%20strategy%20that%20applies%20linear%20transformations%20along%20the%20%5Ctextit%7Bsequence%7D%0Adimension%20to%20exploit%20the%20strong%20local%20correlation%20in%20language%20and%20visual%20data.%0ABy%20keeping%20a%20small%20number%20of%20tokens%20in%20each%20intermediate%20activation%20at%20higher%0Aprecision%2C%20we%20can%20maintain%20model%20accuracy%20at%20lower%20%28average%29%20activations%0Abit-widths.%20We%20evaluate%20STaMP%20on%20recent%20LVM%20and%20LLM%20architectures%2C%0Ademonstrating%20that%20it%20significantly%20improves%20low%20bit%20width%20activation%0Aquantization%20and%20complements%20established%20activation%20and%20weight%20quantization%0Amethods%20including%20recent%20feature%20transformations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTaMP%253A%2520Sequence%2520Transformation%2520and%2520Mixed%2520Precision%2520for%2520Low-Precision%250A%2520%2520Activation%2520Quantization%26entry.906535625%3DMarco%2520Federici%2520and%2520Riccardo%2520Del%2520Chiaro%2520and%2520Boris%2520van%2520Breugel%2520and%2520Paul%2520Whatmough%2520and%2520Markus%2520Nagel%26entry.1292438233%3D%2520%2520Quantization%2520is%2520the%2520key%2520method%2520for%2520reducing%2520inference%2520latency%252C%2520power%2520and%250Amemory%2520footprint%2520of%2520generative%2520AI%2520models.%2520However%252C%2520accuracy%2520often%2520degrades%250Asharply%2520when%2520activations%2520are%2520quantized%2520below%2520eight%2520bits.%2520Recent%2520work%2520suggests%250Athat%2520invertible%2520linear%2520transformations%2520%2528e.g.%2520rotations%2529%2520can%2520aid%2520quantization%252C%250Aby%2520reparameterizing%2520feature%2520channels%2520and%2520weights.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Ctextit%257BSequence%2520Transformation%2520and%2520Mixed%2520Precision%257D%2520%2528STaMP%2529%2520quantization%252C%2520a%250Anovel%2520strategy%2520that%2520applies%2520linear%2520transformations%2520along%2520the%2520%255Ctextit%257Bsequence%257D%250Adimension%2520to%2520exploit%2520the%2520strong%2520local%2520correlation%2520in%2520language%2520and%2520visual%2520data.%250ABy%2520keeping%2520a%2520small%2520number%2520of%2520tokens%2520in%2520each%2520intermediate%2520activation%2520at%2520higher%250Aprecision%252C%2520we%2520can%2520maintain%2520model%2520accuracy%2520at%2520lower%2520%2528average%2529%2520activations%250Abit-widths.%2520We%2520evaluate%2520STaMP%2520on%2520recent%2520LVM%2520and%2520LLM%2520architectures%252C%250Ademonstrating%2520that%2520it%2520significantly%2520improves%2520low%2520bit%2520width%2520activation%250Aquantization%2520and%2520complements%2520established%2520activation%2520and%2520weight%2520quantization%250Amethods%2520including%2520recent%2520feature%2520transformations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STaMP%3A%20Sequence%20Transformation%20and%20Mixed%20Precision%20for%20Low-Precision%0A%20%20Activation%20Quantization&entry.906535625=Marco%20Federici%20and%20Riccardo%20Del%20Chiaro%20and%20Boris%20van%20Breugel%20and%20Paul%20Whatmough%20and%20Markus%20Nagel&entry.1292438233=%20%20Quantization%20is%20the%20key%20method%20for%20reducing%20inference%20latency%2C%20power%20and%0Amemory%20footprint%20of%20generative%20AI%20models.%20However%2C%20accuracy%20often%20degrades%0Asharply%20when%20activations%20are%20quantized%20below%20eight%20bits.%20Recent%20work%20suggests%0Athat%20invertible%20linear%20transformations%20%28e.g.%20rotations%29%20can%20aid%20quantization%2C%0Aby%20reparameterizing%20feature%20channels%20and%20weights.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextit%7BSequence%20Transformation%20and%20Mixed%20Precision%7D%20%28STaMP%29%20quantization%2C%20a%0Anovel%20strategy%20that%20applies%20linear%20transformations%20along%20the%20%5Ctextit%7Bsequence%7D%0Adimension%20to%20exploit%20the%20strong%20local%20correlation%20in%20language%20and%20visual%20data.%0ABy%20keeping%20a%20small%20number%20of%20tokens%20in%20each%20intermediate%20activation%20at%20higher%0Aprecision%2C%20we%20can%20maintain%20model%20accuracy%20at%20lower%20%28average%29%20activations%0Abit-widths.%20We%20evaluate%20STaMP%20on%20recent%20LVM%20and%20LLM%20architectures%2C%0Ademonstrating%20that%20it%20significantly%20improves%20low%20bit%20width%20activation%0Aquantization%20and%20complements%20established%20activation%20and%20weight%20quantization%0Amethods%20including%20recent%20feature%20transformations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26771v1&entry.124074799=Read"},
{"title": "Curly Flow Matching for Learning Non-gradient Field Dynamics", "author": "Katarina Petrovi\u0107 and Lazar Atanackovic and Viggo Moro and Kacper Kapu\u015bniak and \u0130smail \u0130lkan Ceylan and Michael Bronstein and Avishek Joey Bose and Alexander Tong", "abstract": "  Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git\n", "link": "http://arxiv.org/abs/2510.26645v1", "date": "2025-10-30", "relevancy": 1.553, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5464}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5153}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curly%20Flow%20Matching%20for%20Learning%20Non-gradient%20Field%20Dynamics&body=Title%3A%20Curly%20Flow%20Matching%20for%20Learning%20Non-gradient%20Field%20Dynamics%0AAuthor%3A%20Katarina%20Petrovi%C4%87%20and%20Lazar%20Atanackovic%20and%20Viggo%20Moro%20and%20Kacper%20Kapu%C5%9Bniak%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Michael%20Bronstein%20and%20Avishek%20Joey%20Bose%20and%20Alexander%20Tong%0AAbstract%3A%20%20%20Modeling%20the%20transport%20dynamics%20of%20natural%20processes%20from%20population-level%0Aobservations%20is%20a%20ubiquitous%20problem%20in%20the%20natural%20sciences.%20Such%20models%20rely%0Aon%20key%20assumptions%20about%20the%20underlying%20process%20in%20order%20to%20enable%20faithful%0Alearning%20of%20governing%20dynamics%20that%20mimic%20the%20actual%20system%20behavior.%20The%20de%0Afacto%20assumption%20in%20current%20approaches%20relies%20on%20the%20principle%20of%20least%20action%0Athat%20results%20in%20gradient%20field%20dynamics%20and%20leads%20to%20trajectories%20minimizing%20an%0Aenergy%20functional%20between%20two%20probability%20measures.%20However%2C%20many%20real-world%0Asystems%2C%20such%20as%20cell%20cycles%20in%20single-cell%20RNA%2C%20are%20known%20to%20exhibit%0Anon-gradient%2C%20periodic%20behavior%2C%20which%20fundamentally%20cannot%20be%20captured%20by%0Acurrent%20state-of-the-art%20methods%20such%20as%20flow%20and%20bridge%20matching.%20In%20this%0Apaper%2C%20we%20introduce%20Curly%20Flow%20Matching%20%28Curly-FM%29%2C%20a%20novel%20approach%20that%20is%0Acapable%20of%20learning%20non-gradient%20field%20dynamics%20by%20designing%20and%20solving%20a%0ASchr%5C%22odinger%20bridge%20problem%20with%20a%20non-zero%20drift%20reference%20process%20--%20in%0Astark%20contrast%20to%20typical%20zero-drift%20reference%20processes%20--%20which%20is%0Aconstructed%20using%20inferred%20velocities%20in%20addition%20to%20population%20snapshot%20data.%0AWe%20showcase%20Curly-FM%20by%20solving%20the%20trajectory%20inference%20problems%20for%20single%0Acells%2C%20computational%20fluid%20dynamics%2C%20and%20ocean%20currents%20with%20approximate%0Avelocities.%20We%20demonstrate%20that%20Curly-FM%20can%20learn%20trajectories%20that%20better%0Amatch%20both%20the%20reference%20process%20and%20population%20marginals.%20Curly-FM%20expands%0Aflow%20matching%20models%20beyond%20the%20modeling%20of%20populations%20and%20towards%20the%0Amodeling%20of%20known%20periodic%20behavior%20in%20physical%20systems.%20Our%20code%20repository%20is%0Aaccessible%20at%3A%20https%3A//github.com/kpetrovicc/curly-flow-matching.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurly%2520Flow%2520Matching%2520for%2520Learning%2520Non-gradient%2520Field%2520Dynamics%26entry.906535625%3DKatarina%2520Petrovi%25C4%2587%2520and%2520Lazar%2520Atanackovic%2520and%2520Viggo%2520Moro%2520and%2520Kacper%2520Kapu%25C5%259Bniak%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Michael%2520Bronstein%2520and%2520Avishek%2520Joey%2520Bose%2520and%2520Alexander%2520Tong%26entry.1292438233%3D%2520%2520Modeling%2520the%2520transport%2520dynamics%2520of%2520natural%2520processes%2520from%2520population-level%250Aobservations%2520is%2520a%2520ubiquitous%2520problem%2520in%2520the%2520natural%2520sciences.%2520Such%2520models%2520rely%250Aon%2520key%2520assumptions%2520about%2520the%2520underlying%2520process%2520in%2520order%2520to%2520enable%2520faithful%250Alearning%2520of%2520governing%2520dynamics%2520that%2520mimic%2520the%2520actual%2520system%2520behavior.%2520The%2520de%250Afacto%2520assumption%2520in%2520current%2520approaches%2520relies%2520on%2520the%2520principle%2520of%2520least%2520action%250Athat%2520results%2520in%2520gradient%2520field%2520dynamics%2520and%2520leads%2520to%2520trajectories%2520minimizing%2520an%250Aenergy%2520functional%2520between%2520two%2520probability%2520measures.%2520However%252C%2520many%2520real-world%250Asystems%252C%2520such%2520as%2520cell%2520cycles%2520in%2520single-cell%2520RNA%252C%2520are%2520known%2520to%2520exhibit%250Anon-gradient%252C%2520periodic%2520behavior%252C%2520which%2520fundamentally%2520cannot%2520be%2520captured%2520by%250Acurrent%2520state-of-the-art%2520methods%2520such%2520as%2520flow%2520and%2520bridge%2520matching.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520Curly%2520Flow%2520Matching%2520%2528Curly-FM%2529%252C%2520a%2520novel%2520approach%2520that%2520is%250Acapable%2520of%2520learning%2520non-gradient%2520field%2520dynamics%2520by%2520designing%2520and%2520solving%2520a%250ASchr%255C%2522odinger%2520bridge%2520problem%2520with%2520a%2520non-zero%2520drift%2520reference%2520process%2520--%2520in%250Astark%2520contrast%2520to%2520typical%2520zero-drift%2520reference%2520processes%2520--%2520which%2520is%250Aconstructed%2520using%2520inferred%2520velocities%2520in%2520addition%2520to%2520population%2520snapshot%2520data.%250AWe%2520showcase%2520Curly-FM%2520by%2520solving%2520the%2520trajectory%2520inference%2520problems%2520for%2520single%250Acells%252C%2520computational%2520fluid%2520dynamics%252C%2520and%2520ocean%2520currents%2520with%2520approximate%250Avelocities.%2520We%2520demonstrate%2520that%2520Curly-FM%2520can%2520learn%2520trajectories%2520that%2520better%250Amatch%2520both%2520the%2520reference%2520process%2520and%2520population%2520marginals.%2520Curly-FM%2520expands%250Aflow%2520matching%2520models%2520beyond%2520the%2520modeling%2520of%2520populations%2520and%2520towards%2520the%250Amodeling%2520of%2520known%2520periodic%2520behavior%2520in%2520physical%2520systems.%2520Our%2520code%2520repository%2520is%250Aaccessible%2520at%253A%2520https%253A//github.com/kpetrovicc/curly-flow-matching.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curly%20Flow%20Matching%20for%20Learning%20Non-gradient%20Field%20Dynamics&entry.906535625=Katarina%20Petrovi%C4%87%20and%20Lazar%20Atanackovic%20and%20Viggo%20Moro%20and%20Kacper%20Kapu%C5%9Bniak%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Michael%20Bronstein%20and%20Avishek%20Joey%20Bose%20and%20Alexander%20Tong&entry.1292438233=%20%20Modeling%20the%20transport%20dynamics%20of%20natural%20processes%20from%20population-level%0Aobservations%20is%20a%20ubiquitous%20problem%20in%20the%20natural%20sciences.%20Such%20models%20rely%0Aon%20key%20assumptions%20about%20the%20underlying%20process%20in%20order%20to%20enable%20faithful%0Alearning%20of%20governing%20dynamics%20that%20mimic%20the%20actual%20system%20behavior.%20The%20de%0Afacto%20assumption%20in%20current%20approaches%20relies%20on%20the%20principle%20of%20least%20action%0Athat%20results%20in%20gradient%20field%20dynamics%20and%20leads%20to%20trajectories%20minimizing%20an%0Aenergy%20functional%20between%20two%20probability%20measures.%20However%2C%20many%20real-world%0Asystems%2C%20such%20as%20cell%20cycles%20in%20single-cell%20RNA%2C%20are%20known%20to%20exhibit%0Anon-gradient%2C%20periodic%20behavior%2C%20which%20fundamentally%20cannot%20be%20captured%20by%0Acurrent%20state-of-the-art%20methods%20such%20as%20flow%20and%20bridge%20matching.%20In%20this%0Apaper%2C%20we%20introduce%20Curly%20Flow%20Matching%20%28Curly-FM%29%2C%20a%20novel%20approach%20that%20is%0Acapable%20of%20learning%20non-gradient%20field%20dynamics%20by%20designing%20and%20solving%20a%0ASchr%5C%22odinger%20bridge%20problem%20with%20a%20non-zero%20drift%20reference%20process%20--%20in%0Astark%20contrast%20to%20typical%20zero-drift%20reference%20processes%20--%20which%20is%0Aconstructed%20using%20inferred%20velocities%20in%20addition%20to%20population%20snapshot%20data.%0AWe%20showcase%20Curly-FM%20by%20solving%20the%20trajectory%20inference%20problems%20for%20single%0Acells%2C%20computational%20fluid%20dynamics%2C%20and%20ocean%20currents%20with%20approximate%0Avelocities.%20We%20demonstrate%20that%20Curly-FM%20can%20learn%20trajectories%20that%20better%0Amatch%20both%20the%20reference%20process%20and%20population%20marginals.%20Curly-FM%20expands%0Aflow%20matching%20models%20beyond%20the%20modeling%20of%20populations%20and%20towards%20the%0Amodeling%20of%20known%20periodic%20behavior%20in%20physical%20systems.%20Our%20code%20repository%20is%0Aaccessible%20at%3A%20https%3A//github.com/kpetrovicc/curly-flow-matching.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26645v1&entry.124074799=Read"},
{"title": "Completion $\\neq$ Collaboration: Scaling Collaborative Effort with\n  Agents", "author": "Shannon Zejiang Shen and Valerie Chen and Ken Gu and Alexis Ross and Zixian Ma and Jillian Ross and Alex Gu and Chenglei Si and Wayne Chi and Andi Peng and Jocelyn J Shen and Ameet Talwalkar and Tongshuang Wu and David Sontag", "abstract": "  Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.\n", "link": "http://arxiv.org/abs/2510.25744v2", "date": "2025-10-30", "relevancy": 1.5158, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5197}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5047}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Completion%20%24%5Cneq%24%20Collaboration%3A%20Scaling%20Collaborative%20Effort%20with%0A%20%20Agents&body=Title%3A%20Completion%20%24%5Cneq%24%20Collaboration%3A%20Scaling%20Collaborative%20Effort%20with%0A%20%20Agents%0AAuthor%3A%20Shannon%20Zejiang%20Shen%20and%20Valerie%20Chen%20and%20Ken%20Gu%20and%20Alexis%20Ross%20and%20Zixian%20Ma%20and%20Jillian%20Ross%20and%20Alex%20Gu%20and%20Chenglei%20Si%20and%20Wayne%20Chi%20and%20Andi%20Peng%20and%20Jocelyn%20J%20Shen%20and%20Ameet%20Talwalkar%20and%20Tongshuang%20Wu%20and%20David%20Sontag%0AAbstract%3A%20%20%20Current%20evaluations%20of%20agents%20remain%20centered%20around%20one-shot%20task%0Acompletion%2C%20failing%20to%20account%20for%20the%20inherently%20iterative%20and%20collaborative%0Anature%20of%20many%20real-world%20problems%2C%20where%20human%20goals%20are%20often%20underspecified%0Aand%20evolve.%20We%20argue%20for%20a%20shift%20from%20building%20and%20assessing%20task%20completion%0Aagents%20to%20developing%20collaborative%20agents%2C%20assessed%20not%20only%20by%20the%20quality%20of%0Atheir%20final%20outputs%20but%20by%20how%20well%20they%20engage%20with%20and%20enhance%20human%20effort%0Athroughout%20the%20problem-solving%20process.%20To%20support%20this%20shift%2C%20we%20introduce%0Acollaborative%20effort%20scaling%2C%20a%20framework%20that%20captures%20how%20an%20agent%27s%20utility%0Agrows%20with%20increasing%20user%20involvement.%20Through%20case%20studies%20and%20simulated%0Aevaluations%2C%20we%20show%20that%20state-of-the-art%20agents%20often%20underperform%20in%0Amulti-turn%2C%20real-world%20scenarios%2C%20revealing%20a%20missing%20ingredient%20in%20agent%0Adesign%3A%20the%20ability%20to%20sustain%20engagement%20and%20scaffold%20user%20understanding.%0ACollaborative%20effort%20scaling%20offers%20a%20lens%20for%20diagnosing%20agent%20behavior%20and%0Aguiding%20development%20toward%20more%20effective%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompletion%2520%2524%255Cneq%2524%2520Collaboration%253A%2520Scaling%2520Collaborative%2520Effort%2520with%250A%2520%2520Agents%26entry.906535625%3DShannon%2520Zejiang%2520Shen%2520and%2520Valerie%2520Chen%2520and%2520Ken%2520Gu%2520and%2520Alexis%2520Ross%2520and%2520Zixian%2520Ma%2520and%2520Jillian%2520Ross%2520and%2520Alex%2520Gu%2520and%2520Chenglei%2520Si%2520and%2520Wayne%2520Chi%2520and%2520Andi%2520Peng%2520and%2520Jocelyn%2520J%2520Shen%2520and%2520Ameet%2520Talwalkar%2520and%2520Tongshuang%2520Wu%2520and%2520David%2520Sontag%26entry.1292438233%3D%2520%2520Current%2520evaluations%2520of%2520agents%2520remain%2520centered%2520around%2520one-shot%2520task%250Acompletion%252C%2520failing%2520to%2520account%2520for%2520the%2520inherently%2520iterative%2520and%2520collaborative%250Anature%2520of%2520many%2520real-world%2520problems%252C%2520where%2520human%2520goals%2520are%2520often%2520underspecified%250Aand%2520evolve.%2520We%2520argue%2520for%2520a%2520shift%2520from%2520building%2520and%2520assessing%2520task%2520completion%250Aagents%2520to%2520developing%2520collaborative%2520agents%252C%2520assessed%2520not%2520only%2520by%2520the%2520quality%2520of%250Atheir%2520final%2520outputs%2520but%2520by%2520how%2520well%2520they%2520engage%2520with%2520and%2520enhance%2520human%2520effort%250Athroughout%2520the%2520problem-solving%2520process.%2520To%2520support%2520this%2520shift%252C%2520we%2520introduce%250Acollaborative%2520effort%2520scaling%252C%2520a%2520framework%2520that%2520captures%2520how%2520an%2520agent%2527s%2520utility%250Agrows%2520with%2520increasing%2520user%2520involvement.%2520Through%2520case%2520studies%2520and%2520simulated%250Aevaluations%252C%2520we%2520show%2520that%2520state-of-the-art%2520agents%2520often%2520underperform%2520in%250Amulti-turn%252C%2520real-world%2520scenarios%252C%2520revealing%2520a%2520missing%2520ingredient%2520in%2520agent%250Adesign%253A%2520the%2520ability%2520to%2520sustain%2520engagement%2520and%2520scaffold%2520user%2520understanding.%250ACollaborative%2520effort%2520scaling%2520offers%2520a%2520lens%2520for%2520diagnosing%2520agent%2520behavior%2520and%250Aguiding%2520development%2520toward%2520more%2520effective%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Completion%20%24%5Cneq%24%20Collaboration%3A%20Scaling%20Collaborative%20Effort%20with%0A%20%20Agents&entry.906535625=Shannon%20Zejiang%20Shen%20and%20Valerie%20Chen%20and%20Ken%20Gu%20and%20Alexis%20Ross%20and%20Zixian%20Ma%20and%20Jillian%20Ross%20and%20Alex%20Gu%20and%20Chenglei%20Si%20and%20Wayne%20Chi%20and%20Andi%20Peng%20and%20Jocelyn%20J%20Shen%20and%20Ameet%20Talwalkar%20and%20Tongshuang%20Wu%20and%20David%20Sontag&entry.1292438233=%20%20Current%20evaluations%20of%20agents%20remain%20centered%20around%20one-shot%20task%0Acompletion%2C%20failing%20to%20account%20for%20the%20inherently%20iterative%20and%20collaborative%0Anature%20of%20many%20real-world%20problems%2C%20where%20human%20goals%20are%20often%20underspecified%0Aand%20evolve.%20We%20argue%20for%20a%20shift%20from%20building%20and%20assessing%20task%20completion%0Aagents%20to%20developing%20collaborative%20agents%2C%20assessed%20not%20only%20by%20the%20quality%20of%0Atheir%20final%20outputs%20but%20by%20how%20well%20they%20engage%20with%20and%20enhance%20human%20effort%0Athroughout%20the%20problem-solving%20process.%20To%20support%20this%20shift%2C%20we%20introduce%0Acollaborative%20effort%20scaling%2C%20a%20framework%20that%20captures%20how%20an%20agent%27s%20utility%0Agrows%20with%20increasing%20user%20involvement.%20Through%20case%20studies%20and%20simulated%0Aevaluations%2C%20we%20show%20that%20state-of-the-art%20agents%20often%20underperform%20in%0Amulti-turn%2C%20real-world%20scenarios%2C%20revealing%20a%20missing%20ingredient%20in%20agent%0Adesign%3A%20the%20ability%20to%20sustain%20engagement%20and%20scaffold%20user%20understanding.%0ACollaborative%20effort%20scaling%20offers%20a%20lens%20for%20diagnosing%20agent%20behavior%20and%0Aguiding%20development%20toward%20more%20effective%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25744v2&entry.124074799=Read"},
{"title": "Curriculum Abductive Learning", "author": "Wen-Chao Hu and Qi-Jie Li and Lin-Han Jia and Cunjing Ge and Yu-Feng Li and Yuan Jiang and Zhi-Hua Zhou", "abstract": "  Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.\n", "link": "http://arxiv.org/abs/2505.12275v2", "date": "2025-10-30", "relevancy": 1.4895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Abductive%20Learning&body=Title%3A%20Curriculum%20Abductive%20Learning%0AAuthor%3A%20Wen-Chao%20Hu%20and%20Qi-Jie%20Li%20and%20Lin-Han%20Jia%20and%20Cunjing%20Ge%20and%20Yu-Feng%20Li%20and%20Yuan%20Jiang%20and%20Zhi-Hua%20Zhou%0AAbstract%3A%20%20%20Abductive%20Learning%20%28ABL%29%20integrates%20machine%20learning%20with%20logical%20reasoning%0Ain%20a%20loop%3A%20a%20learning%20model%20predicts%20symbolic%20concept%20labels%20from%20raw%20inputs%2C%0Awhich%20are%20revised%20through%20abduction%20using%20domain%20knowledge%20and%20then%20fed%20back%0Afor%20retraining.%20However%2C%20due%20to%20the%20nondeterminism%20of%20abduction%2C%20the%20training%0Aprocess%20often%20suffers%20from%20instability%2C%20especially%20when%20the%20knowledge%20base%20is%0Alarge%20and%20complex%2C%20resulting%20in%20a%20prohibitively%20large%20abduction%20space.%20While%0Aprior%20works%20focus%20on%20improving%20candidate%20selection%20within%20this%20space%2C%20they%0Atypically%20treat%20the%20knowledge%20base%20as%20a%20static%20black%20box.%20In%20this%20work%2C%20we%0Apropose%20Curriculum%20Abductive%20Learning%20%28C-ABL%29%2C%20a%20method%20that%20explicitly%0Aleverages%20the%20internal%20structure%20of%20the%20knowledge%20base%20to%20address%20the%20ABL%0Atraining%20challenges.%20C-ABL%20partitions%20the%20knowledge%20base%20into%20a%20sequence%20of%0Asub-bases%2C%20progressively%20introduced%20during%20training.%20This%20reduces%20the%20abduction%0Aspace%20throughout%20training%20and%20enables%20the%20model%20to%20incorporate%20logic%20in%20a%0Astepwise%2C%20smooth%20way.%20Experiments%20across%20multiple%20tasks%20show%20that%20C-ABL%0Aoutperforms%20previous%20ABL%20implementations%2C%20significantly%20improves%20training%0Astability%2C%20convergence%20speed%2C%20and%20final%20accuracy%2C%20especially%20under%20complex%0Aknowledge%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Abductive%2520Learning%26entry.906535625%3DWen-Chao%2520Hu%2520and%2520Qi-Jie%2520Li%2520and%2520Lin-Han%2520Jia%2520and%2520Cunjing%2520Ge%2520and%2520Yu-Feng%2520Li%2520and%2520Yuan%2520Jiang%2520and%2520Zhi-Hua%2520Zhou%26entry.1292438233%3D%2520%2520Abductive%2520Learning%2520%2528ABL%2529%2520integrates%2520machine%2520learning%2520with%2520logical%2520reasoning%250Ain%2520a%2520loop%253A%2520a%2520learning%2520model%2520predicts%2520symbolic%2520concept%2520labels%2520from%2520raw%2520inputs%252C%250Awhich%2520are%2520revised%2520through%2520abduction%2520using%2520domain%2520knowledge%2520and%2520then%2520fed%2520back%250Afor%2520retraining.%2520However%252C%2520due%2520to%2520the%2520nondeterminism%2520of%2520abduction%252C%2520the%2520training%250Aprocess%2520often%2520suffers%2520from%2520instability%252C%2520especially%2520when%2520the%2520knowledge%2520base%2520is%250Alarge%2520and%2520complex%252C%2520resulting%2520in%2520a%2520prohibitively%2520large%2520abduction%2520space.%2520While%250Aprior%2520works%2520focus%2520on%2520improving%2520candidate%2520selection%2520within%2520this%2520space%252C%2520they%250Atypically%2520treat%2520the%2520knowledge%2520base%2520as%2520a%2520static%2520black%2520box.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Curriculum%2520Abductive%2520Learning%2520%2528C-ABL%2529%252C%2520a%2520method%2520that%2520explicitly%250Aleverages%2520the%2520internal%2520structure%2520of%2520the%2520knowledge%2520base%2520to%2520address%2520the%2520ABL%250Atraining%2520challenges.%2520C-ABL%2520partitions%2520the%2520knowledge%2520base%2520into%2520a%2520sequence%2520of%250Asub-bases%252C%2520progressively%2520introduced%2520during%2520training.%2520This%2520reduces%2520the%2520abduction%250Aspace%2520throughout%2520training%2520and%2520enables%2520the%2520model%2520to%2520incorporate%2520logic%2520in%2520a%250Astepwise%252C%2520smooth%2520way.%2520Experiments%2520across%2520multiple%2520tasks%2520show%2520that%2520C-ABL%250Aoutperforms%2520previous%2520ABL%2520implementations%252C%2520significantly%2520improves%2520training%250Astability%252C%2520convergence%2520speed%252C%2520and%2520final%2520accuracy%252C%2520especially%2520under%2520complex%250Aknowledge%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Abductive%20Learning&entry.906535625=Wen-Chao%20Hu%20and%20Qi-Jie%20Li%20and%20Lin-Han%20Jia%20and%20Cunjing%20Ge%20and%20Yu-Feng%20Li%20and%20Yuan%20Jiang%20and%20Zhi-Hua%20Zhou&entry.1292438233=%20%20Abductive%20Learning%20%28ABL%29%20integrates%20machine%20learning%20with%20logical%20reasoning%0Ain%20a%20loop%3A%20a%20learning%20model%20predicts%20symbolic%20concept%20labels%20from%20raw%20inputs%2C%0Awhich%20are%20revised%20through%20abduction%20using%20domain%20knowledge%20and%20then%20fed%20back%0Afor%20retraining.%20However%2C%20due%20to%20the%20nondeterminism%20of%20abduction%2C%20the%20training%0Aprocess%20often%20suffers%20from%20instability%2C%20especially%20when%20the%20knowledge%20base%20is%0Alarge%20and%20complex%2C%20resulting%20in%20a%20prohibitively%20large%20abduction%20space.%20While%0Aprior%20works%20focus%20on%20improving%20candidate%20selection%20within%20this%20space%2C%20they%0Atypically%20treat%20the%20knowledge%20base%20as%20a%20static%20black%20box.%20In%20this%20work%2C%20we%0Apropose%20Curriculum%20Abductive%20Learning%20%28C-ABL%29%2C%20a%20method%20that%20explicitly%0Aleverages%20the%20internal%20structure%20of%20the%20knowledge%20base%20to%20address%20the%20ABL%0Atraining%20challenges.%20C-ABL%20partitions%20the%20knowledge%20base%20into%20a%20sequence%20of%0Asub-bases%2C%20progressively%20introduced%20during%20training.%20This%20reduces%20the%20abduction%0Aspace%20throughout%20training%20and%20enables%20the%20model%20to%20incorporate%20logic%20in%20a%0Astepwise%2C%20smooth%20way.%20Experiments%20across%20multiple%20tasks%20show%20that%20C-ABL%0Aoutperforms%20previous%20ABL%20implementations%2C%20significantly%20improves%20training%0Astability%2C%20convergence%20speed%2C%20and%20final%20accuracy%2C%20especially%20under%20complex%0Aknowledge%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12275v2&entry.124074799=Read"},
{"title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference", "author": "Zixu Shen and Kexin Chu and Yifan Zhang and Dawei Xiang and Runxin Wu and Wei Zhang", "abstract": "  The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.\n", "link": "http://arxiv.org/abs/2510.26730v1", "date": "2025-10-30", "relevancy": 1.4641, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpertFlow%3A%20Adaptive%20Expert%20Scheduling%20and%20Memory%20Coordination%20for%0A%20%20Efficient%20MoE%20Inference&body=Title%3A%20ExpertFlow%3A%20Adaptive%20Expert%20Scheduling%20and%20Memory%20Coordination%20for%0A%20%20Efficient%20MoE%20Inference%0AAuthor%3A%20Zixu%20Shen%20and%20Kexin%20Chu%20and%20Yifan%20Zhang%20and%20Dawei%20Xiang%20and%20Runxin%20Wu%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20The%20expansion%20of%20large%20language%20models%20is%20increasingly%20limited%20by%20the%0Aconstrained%20memory%20capacity%20of%20modern%20GPUs.%20To%20mitigate%20this%2C%0AMixture-of-Experts%20%28MoE%29%20architectures%20activate%20only%20a%20small%20portion%20of%0Aparameters%20during%20inference%2C%20significantly%20lowering%20both%20memory%20demand%20and%0Acomputational%20overhead.%20However%2C%20conventional%20MoE%20inference%20approaches%2C%20which%0Aselect%20active%20experts%20independently%20at%20each%20layer%2C%20often%20introduce%20considerable%0Alatency%20because%20of%20frequent%20parameter%20transfers%20between%20host%20and%20GPU%20memory.%20In%0Aaddition%2C%20current%20cross-layer%20prediction%20strategies%2C%20which%20are%20typically%20based%0Aon%20fixed%20steps%2C%20lack%20adaptability%20across%20different%20hardware%20platforms%20and%0Aworkloads%2C%20thereby%20reducing%20their%20robustness%20and%20effectiveness.%0A%20%20To%20address%20these%20challenges%2C%20we%20present%20ExpertFlow%2C%20a%20runtime%20system%20for%20MoE%0Ainference%20that%20combines%20adaptive%20expert%20prefetching%20and%20cache-aware%20routing.%0AExpertFlow%20continuously%20adjusts%20its%20prediction%20horizon%20for%20expert%20activation%20by%0Aleveraging%20runtime%20statistics%20such%20as%20transfer%20bandwidth%2C%20parameter%0Adimensionality%2C%20and%20model%20feedback%20signals.%20Furthermore%2C%20it%20incorporates%20a%0Ahybrid%20cross-layer%20prediction%20scheme%20that%20fuses%20pregating%20information%20with%0Aintermediate%20computational%20states%20to%20anticipate%20future%20expert%20needs.%20By%0Aadaptively%20refining%20prefetching%20decisions%20and%20aligning%20them%20with%20actual%20usage%0Abehavior%2C%20ExpertFlow%20effectively%20decreases%20cache%20misses%20and%20removes%20latency%0Acaused%20by%20expert%20swap-ins.%20Our%20evaluation%20demonstrates%20that%20ExpertFlow%20reduces%0Amodel%20stall%20time%20to%20less%20than%200.1%25%20of%20the%20baseline%2C%20highlighting%20its%20capability%0Ato%20optimize%20MoE%20inference%20under%20stringent%20memory%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpertFlow%253A%2520Adaptive%2520Expert%2520Scheduling%2520and%2520Memory%2520Coordination%2520for%250A%2520%2520Efficient%2520MoE%2520Inference%26entry.906535625%3DZixu%2520Shen%2520and%2520Kexin%2520Chu%2520and%2520Yifan%2520Zhang%2520and%2520Dawei%2520Xiang%2520and%2520Runxin%2520Wu%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520expansion%2520of%2520large%2520language%2520models%2520is%2520increasingly%2520limited%2520by%2520the%250Aconstrained%2520memory%2520capacity%2520of%2520modern%2520GPUs.%2520To%2520mitigate%2520this%252C%250AMixture-of-Experts%2520%2528MoE%2529%2520architectures%2520activate%2520only%2520a%2520small%2520portion%2520of%250Aparameters%2520during%2520inference%252C%2520significantly%2520lowering%2520both%2520memory%2520demand%2520and%250Acomputational%2520overhead.%2520However%252C%2520conventional%2520MoE%2520inference%2520approaches%252C%2520which%250Aselect%2520active%2520experts%2520independently%2520at%2520each%2520layer%252C%2520often%2520introduce%2520considerable%250Alatency%2520because%2520of%2520frequent%2520parameter%2520transfers%2520between%2520host%2520and%2520GPU%2520memory.%2520In%250Aaddition%252C%2520current%2520cross-layer%2520prediction%2520strategies%252C%2520which%2520are%2520typically%2520based%250Aon%2520fixed%2520steps%252C%2520lack%2520adaptability%2520across%2520different%2520hardware%2520platforms%2520and%250Aworkloads%252C%2520thereby%2520reducing%2520their%2520robustness%2520and%2520effectiveness.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520ExpertFlow%252C%2520a%2520runtime%2520system%2520for%2520MoE%250Ainference%2520that%2520combines%2520adaptive%2520expert%2520prefetching%2520and%2520cache-aware%2520routing.%250AExpertFlow%2520continuously%2520adjusts%2520its%2520prediction%2520horizon%2520for%2520expert%2520activation%2520by%250Aleveraging%2520runtime%2520statistics%2520such%2520as%2520transfer%2520bandwidth%252C%2520parameter%250Adimensionality%252C%2520and%2520model%2520feedback%2520signals.%2520Furthermore%252C%2520it%2520incorporates%2520a%250Ahybrid%2520cross-layer%2520prediction%2520scheme%2520that%2520fuses%2520pregating%2520information%2520with%250Aintermediate%2520computational%2520states%2520to%2520anticipate%2520future%2520expert%2520needs.%2520By%250Aadaptively%2520refining%2520prefetching%2520decisions%2520and%2520aligning%2520them%2520with%2520actual%2520usage%250Abehavior%252C%2520ExpertFlow%2520effectively%2520decreases%2520cache%2520misses%2520and%2520removes%2520latency%250Acaused%2520by%2520expert%2520swap-ins.%2520Our%2520evaluation%2520demonstrates%2520that%2520ExpertFlow%2520reduces%250Amodel%2520stall%2520time%2520to%2520less%2520than%25200.1%2525%2520of%2520the%2520baseline%252C%2520highlighting%2520its%2520capability%250Ato%2520optimize%2520MoE%2520inference%2520under%2520stringent%2520memory%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpertFlow%3A%20Adaptive%20Expert%20Scheduling%20and%20Memory%20Coordination%20for%0A%20%20Efficient%20MoE%20Inference&entry.906535625=Zixu%20Shen%20and%20Kexin%20Chu%20and%20Yifan%20Zhang%20and%20Dawei%20Xiang%20and%20Runxin%20Wu%20and%20Wei%20Zhang&entry.1292438233=%20%20The%20expansion%20of%20large%20language%20models%20is%20increasingly%20limited%20by%20the%0Aconstrained%20memory%20capacity%20of%20modern%20GPUs.%20To%20mitigate%20this%2C%0AMixture-of-Experts%20%28MoE%29%20architectures%20activate%20only%20a%20small%20portion%20of%0Aparameters%20during%20inference%2C%20significantly%20lowering%20both%20memory%20demand%20and%0Acomputational%20overhead.%20However%2C%20conventional%20MoE%20inference%20approaches%2C%20which%0Aselect%20active%20experts%20independently%20at%20each%20layer%2C%20often%20introduce%20considerable%0Alatency%20because%20of%20frequent%20parameter%20transfers%20between%20host%20and%20GPU%20memory.%20In%0Aaddition%2C%20current%20cross-layer%20prediction%20strategies%2C%20which%20are%20typically%20based%0Aon%20fixed%20steps%2C%20lack%20adaptability%20across%20different%20hardware%20platforms%20and%0Aworkloads%2C%20thereby%20reducing%20their%20robustness%20and%20effectiveness.%0A%20%20To%20address%20these%20challenges%2C%20we%20present%20ExpertFlow%2C%20a%20runtime%20system%20for%20MoE%0Ainference%20that%20combines%20adaptive%20expert%20prefetching%20and%20cache-aware%20routing.%0AExpertFlow%20continuously%20adjusts%20its%20prediction%20horizon%20for%20expert%20activation%20by%0Aleveraging%20runtime%20statistics%20such%20as%20transfer%20bandwidth%2C%20parameter%0Adimensionality%2C%20and%20model%20feedback%20signals.%20Furthermore%2C%20it%20incorporates%20a%0Ahybrid%20cross-layer%20prediction%20scheme%20that%20fuses%20pregating%20information%20with%0Aintermediate%20computational%20states%20to%20anticipate%20future%20expert%20needs.%20By%0Aadaptively%20refining%20prefetching%20decisions%20and%20aligning%20them%20with%20actual%20usage%0Abehavior%2C%20ExpertFlow%20effectively%20decreases%20cache%20misses%20and%20removes%20latency%0Acaused%20by%20expert%20swap-ins.%20Our%20evaluation%20demonstrates%20that%20ExpertFlow%20reduces%0Amodel%20stall%20time%20to%20less%20than%200.1%25%20of%20the%20baseline%2C%20highlighting%20its%20capability%0Ato%20optimize%20MoE%20inference%20under%20stringent%20memory%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26730v1&entry.124074799=Read"},
{"title": "Delegated Authorization for Agents Constrained to Semantic Task-to-Scope\n  Matching", "author": "Majed El Helou and Chiara Troiani and Benjamin Ryder and Jean Diaconu and Herv\u00e9 Muyal and Marcelo Yannuzzi", "abstract": "  Authorizing Large Language Model driven agents to dynamically invoke tools\nand access protected resources introduces significant risks, since current\nmethods for delegating authorization grant overly broad permissions and give\naccess to tools allowing agents to operate beyond the intended task scope. We\nintroduce and assess a delegated authorization model enabling authorization\nservers to semantically inspect access requests to protected resources, and\nissue access tokens constrained to the minimal set of scopes necessary for the\nagents' assigned tasks. Given the unavailability of datasets centered on\ndelegated authorization flows, particularly including both semantically\nappropriate and inappropriate scope requests for a given task, we introduce\nASTRA, a dataset and data generation pipeline for benchmarking semantic\nmatching between tasks and scopes. Our experiments show both the potential and\ncurrent limitations of model-based matching, particularly as the number of\nscopes needed for task completion increases. Our results highlight the need for\nfurther research into semantic matching techniques enabling intent-aware\nauthorization for multi-agent and tool-augmented applications, including\nfine-grained control, such as Task-Based Access Control (TBAC).\n", "link": "http://arxiv.org/abs/2510.26702v1", "date": "2025-10-30", "relevancy": 1.4616, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5031}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5016}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delegated%20Authorization%20for%20Agents%20Constrained%20to%20Semantic%20Task-to-Scope%0A%20%20Matching&body=Title%3A%20Delegated%20Authorization%20for%20Agents%20Constrained%20to%20Semantic%20Task-to-Scope%0A%20%20Matching%0AAuthor%3A%20Majed%20El%20Helou%20and%20Chiara%20Troiani%20and%20Benjamin%20Ryder%20and%20Jean%20Diaconu%20and%20Herv%C3%A9%20Muyal%20and%20Marcelo%20Yannuzzi%0AAbstract%3A%20%20%20Authorizing%20Large%20Language%20Model%20driven%20agents%20to%20dynamically%20invoke%20tools%0Aand%20access%20protected%20resources%20introduces%20significant%20risks%2C%20since%20current%0Amethods%20for%20delegating%20authorization%20grant%20overly%20broad%20permissions%20and%20give%0Aaccess%20to%20tools%20allowing%20agents%20to%20operate%20beyond%20the%20intended%20task%20scope.%20We%0Aintroduce%20and%20assess%20a%20delegated%20authorization%20model%20enabling%20authorization%0Aservers%20to%20semantically%20inspect%20access%20requests%20to%20protected%20resources%2C%20and%0Aissue%20access%20tokens%20constrained%20to%20the%20minimal%20set%20of%20scopes%20necessary%20for%20the%0Aagents%27%20assigned%20tasks.%20Given%20the%20unavailability%20of%20datasets%20centered%20on%0Adelegated%20authorization%20flows%2C%20particularly%20including%20both%20semantically%0Aappropriate%20and%20inappropriate%20scope%20requests%20for%20a%20given%20task%2C%20we%20introduce%0AASTRA%2C%20a%20dataset%20and%20data%20generation%20pipeline%20for%20benchmarking%20semantic%0Amatching%20between%20tasks%20and%20scopes.%20Our%20experiments%20show%20both%20the%20potential%20and%0Acurrent%20limitations%20of%20model-based%20matching%2C%20particularly%20as%20the%20number%20of%0Ascopes%20needed%20for%20task%20completion%20increases.%20Our%20results%20highlight%20the%20need%20for%0Afurther%20research%20into%20semantic%20matching%20techniques%20enabling%20intent-aware%0Aauthorization%20for%20multi-agent%20and%20tool-augmented%20applications%2C%20including%0Afine-grained%20control%2C%20such%20as%20Task-Based%20Access%20Control%20%28TBAC%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelegated%2520Authorization%2520for%2520Agents%2520Constrained%2520to%2520Semantic%2520Task-to-Scope%250A%2520%2520Matching%26entry.906535625%3DMajed%2520El%2520Helou%2520and%2520Chiara%2520Troiani%2520and%2520Benjamin%2520Ryder%2520and%2520Jean%2520Diaconu%2520and%2520Herv%25C3%25A9%2520Muyal%2520and%2520Marcelo%2520Yannuzzi%26entry.1292438233%3D%2520%2520Authorizing%2520Large%2520Language%2520Model%2520driven%2520agents%2520to%2520dynamically%2520invoke%2520tools%250Aand%2520access%2520protected%2520resources%2520introduces%2520significant%2520risks%252C%2520since%2520current%250Amethods%2520for%2520delegating%2520authorization%2520grant%2520overly%2520broad%2520permissions%2520and%2520give%250Aaccess%2520to%2520tools%2520allowing%2520agents%2520to%2520operate%2520beyond%2520the%2520intended%2520task%2520scope.%2520We%250Aintroduce%2520and%2520assess%2520a%2520delegated%2520authorization%2520model%2520enabling%2520authorization%250Aservers%2520to%2520semantically%2520inspect%2520access%2520requests%2520to%2520protected%2520resources%252C%2520and%250Aissue%2520access%2520tokens%2520constrained%2520to%2520the%2520minimal%2520set%2520of%2520scopes%2520necessary%2520for%2520the%250Aagents%2527%2520assigned%2520tasks.%2520Given%2520the%2520unavailability%2520of%2520datasets%2520centered%2520on%250Adelegated%2520authorization%2520flows%252C%2520particularly%2520including%2520both%2520semantically%250Aappropriate%2520and%2520inappropriate%2520scope%2520requests%2520for%2520a%2520given%2520task%252C%2520we%2520introduce%250AASTRA%252C%2520a%2520dataset%2520and%2520data%2520generation%2520pipeline%2520for%2520benchmarking%2520semantic%250Amatching%2520between%2520tasks%2520and%2520scopes.%2520Our%2520experiments%2520show%2520both%2520the%2520potential%2520and%250Acurrent%2520limitations%2520of%2520model-based%2520matching%252C%2520particularly%2520as%2520the%2520number%2520of%250Ascopes%2520needed%2520for%2520task%2520completion%2520increases.%2520Our%2520results%2520highlight%2520the%2520need%2520for%250Afurther%2520research%2520into%2520semantic%2520matching%2520techniques%2520enabling%2520intent-aware%250Aauthorization%2520for%2520multi-agent%2520and%2520tool-augmented%2520applications%252C%2520including%250Afine-grained%2520control%252C%2520such%2520as%2520Task-Based%2520Access%2520Control%2520%2528TBAC%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delegated%20Authorization%20for%20Agents%20Constrained%20to%20Semantic%20Task-to-Scope%0A%20%20Matching&entry.906535625=Majed%20El%20Helou%20and%20Chiara%20Troiani%20and%20Benjamin%20Ryder%20and%20Jean%20Diaconu%20and%20Herv%C3%A9%20Muyal%20and%20Marcelo%20Yannuzzi&entry.1292438233=%20%20Authorizing%20Large%20Language%20Model%20driven%20agents%20to%20dynamically%20invoke%20tools%0Aand%20access%20protected%20resources%20introduces%20significant%20risks%2C%20since%20current%0Amethods%20for%20delegating%20authorization%20grant%20overly%20broad%20permissions%20and%20give%0Aaccess%20to%20tools%20allowing%20agents%20to%20operate%20beyond%20the%20intended%20task%20scope.%20We%0Aintroduce%20and%20assess%20a%20delegated%20authorization%20model%20enabling%20authorization%0Aservers%20to%20semantically%20inspect%20access%20requests%20to%20protected%20resources%2C%20and%0Aissue%20access%20tokens%20constrained%20to%20the%20minimal%20set%20of%20scopes%20necessary%20for%20the%0Aagents%27%20assigned%20tasks.%20Given%20the%20unavailability%20of%20datasets%20centered%20on%0Adelegated%20authorization%20flows%2C%20particularly%20including%20both%20semantically%0Aappropriate%20and%20inappropriate%20scope%20requests%20for%20a%20given%20task%2C%20we%20introduce%0AASTRA%2C%20a%20dataset%20and%20data%20generation%20pipeline%20for%20benchmarking%20semantic%0Amatching%20between%20tasks%20and%20scopes.%20Our%20experiments%20show%20both%20the%20potential%20and%0Acurrent%20limitations%20of%20model-based%20matching%2C%20particularly%20as%20the%20number%20of%0Ascopes%20needed%20for%20task%20completion%20increases.%20Our%20results%20highlight%20the%20need%20for%0Afurther%20research%20into%20semantic%20matching%20techniques%20enabling%20intent-aware%0Aauthorization%20for%20multi-agent%20and%20tool-augmented%20applications%2C%20including%0Afine-grained%20control%2C%20such%20as%20Task-Based%20Access%20Control%20%28TBAC%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26702v1&entry.124074799=Read"},
{"title": "RelP: Faithful and Efficient Circuit Discovery in Language Models via\n  Relevance Patching", "author": "Farnoush Rezaei Jafari and Oliver Eberle and Ashkan Khakzar and Neel Nanda", "abstract": "  Activation patching is a standard method in mechanistic interpretability for\nlocalizing the components of a model responsible for specific behaviors, but it\nis computationally expensive to apply at scale. Attribution patching offers a\nfaster, gradient-based approximation, yet suffers from noise and reduced\nreliability in deep, highly non-linear networks. In this work, we introduce\nRelevance Patching (RelP), which replaces the local gradients in attribution\npatching with propagation coefficients derived from Layer-wise Relevance\nPropagation (LRP). LRP propagates the network's output backward through the\nlayers, redistributing relevance to lower-level components according to local\npropagation rules that ensure properties such as relevance conservation or\nimproved signal-to-noise ratio. Like attribution patching, RelP requires only\ntwo forward passes and one backward pass, maintaining computational efficiency\nwhile improving faithfulness. We validate RelP across a range of models and\ntasks, showing that it more accurately approximates activation patching than\nstandard attribution patching, particularly when analyzing residual stream and\nMLP outputs in the Indirect Object Identification (IOI) task. For instance, for\nMLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation\nof 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by\nRelP. Additionally, we compare the faithfulness of sparse feature circuits\nidentified by RelP and Integrated Gradients (IG), showing that RelP achieves\ncomparable faithfulness without the extra computational cost associated with\nIG.\n", "link": "http://arxiv.org/abs/2508.21258v2", "date": "2025-10-30", "relevancy": 1.4596, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelP%3A%20Faithful%20and%20Efficient%20Circuit%20Discovery%20in%20Language%20Models%20via%0A%20%20Relevance%20Patching&body=Title%3A%20RelP%3A%20Faithful%20and%20Efficient%20Circuit%20Discovery%20in%20Language%20Models%20via%0A%20%20Relevance%20Patching%0AAuthor%3A%20Farnoush%20Rezaei%20Jafari%20and%20Oliver%20Eberle%20and%20Ashkan%20Khakzar%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Activation%20patching%20is%20a%20standard%20method%20in%20mechanistic%20interpretability%20for%0Alocalizing%20the%20components%20of%20a%20model%20responsible%20for%20specific%20behaviors%2C%20but%20it%0Ais%20computationally%20expensive%20to%20apply%20at%20scale.%20Attribution%20patching%20offers%20a%0Afaster%2C%20gradient-based%20approximation%2C%20yet%20suffers%20from%20noise%20and%20reduced%0Areliability%20in%20deep%2C%20highly%20non-linear%20networks.%20In%20this%20work%2C%20we%20introduce%0ARelevance%20Patching%20%28RelP%29%2C%20which%20replaces%20the%20local%20gradients%20in%20attribution%0Apatching%20with%20propagation%20coefficients%20derived%20from%20Layer-wise%20Relevance%0APropagation%20%28LRP%29.%20LRP%20propagates%20the%20network%27s%20output%20backward%20through%20the%0Alayers%2C%20redistributing%20relevance%20to%20lower-level%20components%20according%20to%20local%0Apropagation%20rules%20that%20ensure%20properties%20such%20as%20relevance%20conservation%20or%0Aimproved%20signal-to-noise%20ratio.%20Like%20attribution%20patching%2C%20RelP%20requires%20only%0Atwo%20forward%20passes%20and%20one%20backward%20pass%2C%20maintaining%20computational%20efficiency%0Awhile%20improving%20faithfulness.%20We%20validate%20RelP%20across%20a%20range%20of%20models%20and%0Atasks%2C%20showing%20that%20it%20more%20accurately%20approximates%20activation%20patching%20than%0Astandard%20attribution%20patching%2C%20particularly%20when%20analyzing%20residual%20stream%20and%0AMLP%20outputs%20in%20the%20Indirect%20Object%20Identification%20%28IOI%29%20task.%20For%20instance%2C%20for%0AMLP%20outputs%20in%20GPT-2%20Large%2C%20attribution%20patching%20achieves%20a%20Pearson%20correlation%0Aof%200.006%2C%20whereas%20RelP%20reaches%200.956%2C%20highlighting%20the%20improvement%20offered%20by%0ARelP.%20Additionally%2C%20we%20compare%20the%20faithfulness%20of%20sparse%20feature%20circuits%0Aidentified%20by%20RelP%20and%20Integrated%20Gradients%20%28IG%29%2C%20showing%20that%20RelP%20achieves%0Acomparable%20faithfulness%20without%20the%20extra%20computational%20cost%20associated%20with%0AIG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelP%253A%2520Faithful%2520and%2520Efficient%2520Circuit%2520Discovery%2520in%2520Language%2520Models%2520via%250A%2520%2520Relevance%2520Patching%26entry.906535625%3DFarnoush%2520Rezaei%2520Jafari%2520and%2520Oliver%2520Eberle%2520and%2520Ashkan%2520Khakzar%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Activation%2520patching%2520is%2520a%2520standard%2520method%2520in%2520mechanistic%2520interpretability%2520for%250Alocalizing%2520the%2520components%2520of%2520a%2520model%2520responsible%2520for%2520specific%2520behaviors%252C%2520but%2520it%250Ais%2520computationally%2520expensive%2520to%2520apply%2520at%2520scale.%2520Attribution%2520patching%2520offers%2520a%250Afaster%252C%2520gradient-based%2520approximation%252C%2520yet%2520suffers%2520from%2520noise%2520and%2520reduced%250Areliability%2520in%2520deep%252C%2520highly%2520non-linear%2520networks.%2520In%2520this%2520work%252C%2520we%2520introduce%250ARelevance%2520Patching%2520%2528RelP%2529%252C%2520which%2520replaces%2520the%2520local%2520gradients%2520in%2520attribution%250Apatching%2520with%2520propagation%2520coefficients%2520derived%2520from%2520Layer-wise%2520Relevance%250APropagation%2520%2528LRP%2529.%2520LRP%2520propagates%2520the%2520network%2527s%2520output%2520backward%2520through%2520the%250Alayers%252C%2520redistributing%2520relevance%2520to%2520lower-level%2520components%2520according%2520to%2520local%250Apropagation%2520rules%2520that%2520ensure%2520properties%2520such%2520as%2520relevance%2520conservation%2520or%250Aimproved%2520signal-to-noise%2520ratio.%2520Like%2520attribution%2520patching%252C%2520RelP%2520requires%2520only%250Atwo%2520forward%2520passes%2520and%2520one%2520backward%2520pass%252C%2520maintaining%2520computational%2520efficiency%250Awhile%2520improving%2520faithfulness.%2520We%2520validate%2520RelP%2520across%2520a%2520range%2520of%2520models%2520and%250Atasks%252C%2520showing%2520that%2520it%2520more%2520accurately%2520approximates%2520activation%2520patching%2520than%250Astandard%2520attribution%2520patching%252C%2520particularly%2520when%2520analyzing%2520residual%2520stream%2520and%250AMLP%2520outputs%2520in%2520the%2520Indirect%2520Object%2520Identification%2520%2528IOI%2529%2520task.%2520For%2520instance%252C%2520for%250AMLP%2520outputs%2520in%2520GPT-2%2520Large%252C%2520attribution%2520patching%2520achieves%2520a%2520Pearson%2520correlation%250Aof%25200.006%252C%2520whereas%2520RelP%2520reaches%25200.956%252C%2520highlighting%2520the%2520improvement%2520offered%2520by%250ARelP.%2520Additionally%252C%2520we%2520compare%2520the%2520faithfulness%2520of%2520sparse%2520feature%2520circuits%250Aidentified%2520by%2520RelP%2520and%2520Integrated%2520Gradients%2520%2528IG%2529%252C%2520showing%2520that%2520RelP%2520achieves%250Acomparable%2520faithfulness%2520without%2520the%2520extra%2520computational%2520cost%2520associated%2520with%250AIG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelP%3A%20Faithful%20and%20Efficient%20Circuit%20Discovery%20in%20Language%20Models%20via%0A%20%20Relevance%20Patching&entry.906535625=Farnoush%20Rezaei%20Jafari%20and%20Oliver%20Eberle%20and%20Ashkan%20Khakzar%20and%20Neel%20Nanda&entry.1292438233=%20%20Activation%20patching%20is%20a%20standard%20method%20in%20mechanistic%20interpretability%20for%0Alocalizing%20the%20components%20of%20a%20model%20responsible%20for%20specific%20behaviors%2C%20but%20it%0Ais%20computationally%20expensive%20to%20apply%20at%20scale.%20Attribution%20patching%20offers%20a%0Afaster%2C%20gradient-based%20approximation%2C%20yet%20suffers%20from%20noise%20and%20reduced%0Areliability%20in%20deep%2C%20highly%20non-linear%20networks.%20In%20this%20work%2C%20we%20introduce%0ARelevance%20Patching%20%28RelP%29%2C%20which%20replaces%20the%20local%20gradients%20in%20attribution%0Apatching%20with%20propagation%20coefficients%20derived%20from%20Layer-wise%20Relevance%0APropagation%20%28LRP%29.%20LRP%20propagates%20the%20network%27s%20output%20backward%20through%20the%0Alayers%2C%20redistributing%20relevance%20to%20lower-level%20components%20according%20to%20local%0Apropagation%20rules%20that%20ensure%20properties%20such%20as%20relevance%20conservation%20or%0Aimproved%20signal-to-noise%20ratio.%20Like%20attribution%20patching%2C%20RelP%20requires%20only%0Atwo%20forward%20passes%20and%20one%20backward%20pass%2C%20maintaining%20computational%20efficiency%0Awhile%20improving%20faithfulness.%20We%20validate%20RelP%20across%20a%20range%20of%20models%20and%0Atasks%2C%20showing%20that%20it%20more%20accurately%20approximates%20activation%20patching%20than%0Astandard%20attribution%20patching%2C%20particularly%20when%20analyzing%20residual%20stream%20and%0AMLP%20outputs%20in%20the%20Indirect%20Object%20Identification%20%28IOI%29%20task.%20For%20instance%2C%20for%0AMLP%20outputs%20in%20GPT-2%20Large%2C%20attribution%20patching%20achieves%20a%20Pearson%20correlation%0Aof%200.006%2C%20whereas%20RelP%20reaches%200.956%2C%20highlighting%20the%20improvement%20offered%20by%0ARelP.%20Additionally%2C%20we%20compare%20the%20faithfulness%20of%20sparse%20feature%20circuits%0Aidentified%20by%20RelP%20and%20Integrated%20Gradients%20%28IG%29%2C%20showing%20that%20RelP%20achieves%0Acomparable%20faithfulness%20without%20the%20extra%20computational%20cost%20associated%20with%0AIG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21258v2&entry.124074799=Read"},
{"title": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill", "author": "Vaibhav Kurrey and Sivakalyan Pujari and Gagan Raj Gupta", "abstract": "  We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.\n", "link": "http://arxiv.org/abs/2510.26684v1", "date": "2025-10-30", "relevancy": 1.4556, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.478}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process%20Integrated%20Computer%20Vision%20for%20Real-Time%20Failure%20Prediction%20in%0A%20%20Steel%20Rolling%20Mill&body=Title%3A%20Process%20Integrated%20Computer%20Vision%20for%20Real-Time%20Failure%20Prediction%20in%0A%20%20Steel%20Rolling%20Mill%0AAuthor%3A%20Vaibhav%20Kurrey%20and%20Sivakalyan%20Pujari%20and%20Gagan%20Raj%20Gupta%0AAbstract%3A%20%20%20We%20present%20a%20long-term%20deployment%20study%20of%20a%20machine%20vision-based%20anomaly%0Adetection%20system%20for%20failure%20prediction%20in%20a%20steel%20rolling%20mill.%20The%20system%0Aintegrates%20industrial%20cameras%20to%20monitor%20equipment%20operation%2C%20alignment%2C%20and%0Ahot%20bar%20motion%20in%20real%20time%20along%20the%20process%20line.%20Live%20video%20streams%20are%0Aprocessed%20on%20a%20centralized%20video%20server%20using%20deep%20learning%20models%2C%20enabling%0Aearly%20prediction%20of%20equipment%20failures%20and%20process%20interruptions%2C%20thereby%0Areducing%20unplanned%20breakdown%20costs.%20Server-based%20inference%20minimizes%20the%0Acomputational%20load%20on%20industrial%20process%20control%20systems%20%28PLCs%29%2C%20supporting%0Ascalable%20deployment%20across%20production%20lines%20with%20minimal%20additional%20resources.%0ABy%20jointly%20analyzing%20sensor%20data%20from%20data%20acquisition%20systems%20and%20visual%0Ainputs%2C%20the%20system%20identifies%20the%20location%20and%20probable%20root%20causes%20of%0Afailures%2C%20providing%20actionable%20insights%20for%20proactive%20maintenance.%20This%0Aintegrated%20approach%20enhances%20operational%20reliability%2C%20productivity%2C%20and%0Aprofitability%20in%20industrial%20manufacturing%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess%2520Integrated%2520Computer%2520Vision%2520for%2520Real-Time%2520Failure%2520Prediction%2520in%250A%2520%2520Steel%2520Rolling%2520Mill%26entry.906535625%3DVaibhav%2520Kurrey%2520and%2520Sivakalyan%2520Pujari%2520and%2520Gagan%2520Raj%2520Gupta%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520long-term%2520deployment%2520study%2520of%2520a%2520machine%2520vision-based%2520anomaly%250Adetection%2520system%2520for%2520failure%2520prediction%2520in%2520a%2520steel%2520rolling%2520mill.%2520The%2520system%250Aintegrates%2520industrial%2520cameras%2520to%2520monitor%2520equipment%2520operation%252C%2520alignment%252C%2520and%250Ahot%2520bar%2520motion%2520in%2520real%2520time%2520along%2520the%2520process%2520line.%2520Live%2520video%2520streams%2520are%250Aprocessed%2520on%2520a%2520centralized%2520video%2520server%2520using%2520deep%2520learning%2520models%252C%2520enabling%250Aearly%2520prediction%2520of%2520equipment%2520failures%2520and%2520process%2520interruptions%252C%2520thereby%250Areducing%2520unplanned%2520breakdown%2520costs.%2520Server-based%2520inference%2520minimizes%2520the%250Acomputational%2520load%2520on%2520industrial%2520process%2520control%2520systems%2520%2528PLCs%2529%252C%2520supporting%250Ascalable%2520deployment%2520across%2520production%2520lines%2520with%2520minimal%2520additional%2520resources.%250ABy%2520jointly%2520analyzing%2520sensor%2520data%2520from%2520data%2520acquisition%2520systems%2520and%2520visual%250Ainputs%252C%2520the%2520system%2520identifies%2520the%2520location%2520and%2520probable%2520root%2520causes%2520of%250Afailures%252C%2520providing%2520actionable%2520insights%2520for%2520proactive%2520maintenance.%2520This%250Aintegrated%2520approach%2520enhances%2520operational%2520reliability%252C%2520productivity%252C%2520and%250Aprofitability%2520in%2520industrial%2520manufacturing%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process%20Integrated%20Computer%20Vision%20for%20Real-Time%20Failure%20Prediction%20in%0A%20%20Steel%20Rolling%20Mill&entry.906535625=Vaibhav%20Kurrey%20and%20Sivakalyan%20Pujari%20and%20Gagan%20Raj%20Gupta&entry.1292438233=%20%20We%20present%20a%20long-term%20deployment%20study%20of%20a%20machine%20vision-based%20anomaly%0Adetection%20system%20for%20failure%20prediction%20in%20a%20steel%20rolling%20mill.%20The%20system%0Aintegrates%20industrial%20cameras%20to%20monitor%20equipment%20operation%2C%20alignment%2C%20and%0Ahot%20bar%20motion%20in%20real%20time%20along%20the%20process%20line.%20Live%20video%20streams%20are%0Aprocessed%20on%20a%20centralized%20video%20server%20using%20deep%20learning%20models%2C%20enabling%0Aearly%20prediction%20of%20equipment%20failures%20and%20process%20interruptions%2C%20thereby%0Areducing%20unplanned%20breakdown%20costs.%20Server-based%20inference%20minimizes%20the%0Acomputational%20load%20on%20industrial%20process%20control%20systems%20%28PLCs%29%2C%20supporting%0Ascalable%20deployment%20across%20production%20lines%20with%20minimal%20additional%20resources.%0ABy%20jointly%20analyzing%20sensor%20data%20from%20data%20acquisition%20systems%20and%20visual%0Ainputs%2C%20the%20system%20identifies%20the%20location%20and%20probable%20root%20causes%20of%0Afailures%2C%20providing%20actionable%20insights%20for%20proactive%20maintenance.%20This%0Aintegrated%20approach%20enhances%20operational%20reliability%2C%20productivity%2C%20and%0Aprofitability%20in%20industrial%20manufacturing%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26684v1&entry.124074799=Read"},
{"title": "Adversarial generalization of unfolding (model-based) networks", "author": "Vicky Kouni", "abstract": "  Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks.\n", "link": "http://arxiv.org/abs/2509.15370v3", "date": "2025-10-30", "relevancy": 1.4524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4963}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.47}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20generalization%20of%20unfolding%20%28model-based%29%20networks&body=Title%3A%20Adversarial%20generalization%20of%20unfolding%20%28model-based%29%20networks%0AAuthor%3A%20Vicky%20Kouni%0AAbstract%3A%20%20%20Unfolding%20networks%20are%20interpretable%20networks%20emerging%20from%20iterative%0Aalgorithms%2C%20incorporate%20prior%20knowledge%20of%20data%20structure%2C%20and%20are%20designed%20to%0Asolve%20inverse%20problems%20like%20compressed%20sensing%2C%20which%20deals%20with%20recovering%0Adata%20from%20noisy%2C%20missing%20observations.%20Compressed%20sensing%20finds%20applications%20in%0Acritical%20domains%2C%20from%20medical%20imaging%20to%20cryptography%2C%20where%20adversarial%0Arobustness%20is%20crucial%20to%20prevent%20catastrophic%20failures.%20However%2C%20a%20solid%0Atheoretical%20understanding%20of%20the%20performance%20of%20unfolding%20networks%20in%20the%0Apresence%20of%20adversarial%20attacks%20is%20still%20in%20its%20infancy.%20In%20this%20paper%2C%20we%0Astudy%20the%20adversarial%20generalization%20of%20unfolding%20networks%20when%20perturbed%20with%0A%24l_2%24-norm%20constrained%20attacks%2C%20generated%20by%20the%20fast%20gradient%20sign%20method.%0AParticularly%2C%20we%20choose%20a%20family%20of%20state-of-the-art%20overaparameterized%0Aunfolding%20networks%20and%20deploy%20a%20new%20framework%20to%20estimate%20their%20adversarial%0ARademacher%20complexity.%20Given%20this%20estimate%2C%20we%20provide%20adversarial%0Ageneralization%20error%20bounds%20for%20the%20networks%20under%20study%2C%20which%20are%20tight%20with%0Arespect%20to%20the%20attack%20level.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20theoretical%0Aanalysis%20on%20the%20adversarial%20generalization%20of%20unfolding%20networks.%20We%20further%0Apresent%20a%20series%20of%20experiments%20on%20real-world%20data%2C%20with%20results%20corroborating%0Aour%20derived%20theory%2C%20consistently%20for%20all%20data.%20Finally%2C%20we%20observe%20that%20the%0Afamily%27s%20overparameterization%20can%20be%20exploited%20to%20promote%20adversarial%0Arobustness%2C%20shedding%20light%20on%20how%20to%20efficiently%20robustify%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15370v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520generalization%2520of%2520unfolding%2520%2528model-based%2529%2520networks%26entry.906535625%3DVicky%2520Kouni%26entry.1292438233%3D%2520%2520Unfolding%2520networks%2520are%2520interpretable%2520networks%2520emerging%2520from%2520iterative%250Aalgorithms%252C%2520incorporate%2520prior%2520knowledge%2520of%2520data%2520structure%252C%2520and%2520are%2520designed%2520to%250Asolve%2520inverse%2520problems%2520like%2520compressed%2520sensing%252C%2520which%2520deals%2520with%2520recovering%250Adata%2520from%2520noisy%252C%2520missing%2520observations.%2520Compressed%2520sensing%2520finds%2520applications%2520in%250Acritical%2520domains%252C%2520from%2520medical%2520imaging%2520to%2520cryptography%252C%2520where%2520adversarial%250Arobustness%2520is%2520crucial%2520to%2520prevent%2520catastrophic%2520failures.%2520However%252C%2520a%2520solid%250Atheoretical%2520understanding%2520of%2520the%2520performance%2520of%2520unfolding%2520networks%2520in%2520the%250Apresence%2520of%2520adversarial%2520attacks%2520is%2520still%2520in%2520its%2520infancy.%2520In%2520this%2520paper%252C%2520we%250Astudy%2520the%2520adversarial%2520generalization%2520of%2520unfolding%2520networks%2520when%2520perturbed%2520with%250A%2524l_2%2524-norm%2520constrained%2520attacks%252C%2520generated%2520by%2520the%2520fast%2520gradient%2520sign%2520method.%250AParticularly%252C%2520we%2520choose%2520a%2520family%2520of%2520state-of-the-art%2520overaparameterized%250Aunfolding%2520networks%2520and%2520deploy%2520a%2520new%2520framework%2520to%2520estimate%2520their%2520adversarial%250ARademacher%2520complexity.%2520Given%2520this%2520estimate%252C%2520we%2520provide%2520adversarial%250Ageneralization%2520error%2520bounds%2520for%2520the%2520networks%2520under%2520study%252C%2520which%2520are%2520tight%2520with%250Arespect%2520to%2520the%2520attack%2520level.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520theoretical%250Aanalysis%2520on%2520the%2520adversarial%2520generalization%2520of%2520unfolding%2520networks.%2520We%2520further%250Apresent%2520a%2520series%2520of%2520experiments%2520on%2520real-world%2520data%252C%2520with%2520results%2520corroborating%250Aour%2520derived%2520theory%252C%2520consistently%2520for%2520all%2520data.%2520Finally%252C%2520we%2520observe%2520that%2520the%250Afamily%2527s%2520overparameterization%2520can%2520be%2520exploited%2520to%2520promote%2520adversarial%250Arobustness%252C%2520shedding%2520light%2520on%2520how%2520to%2520efficiently%2520robustify%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15370v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20generalization%20of%20unfolding%20%28model-based%29%20networks&entry.906535625=Vicky%20Kouni&entry.1292438233=%20%20Unfolding%20networks%20are%20interpretable%20networks%20emerging%20from%20iterative%0Aalgorithms%2C%20incorporate%20prior%20knowledge%20of%20data%20structure%2C%20and%20are%20designed%20to%0Asolve%20inverse%20problems%20like%20compressed%20sensing%2C%20which%20deals%20with%20recovering%0Adata%20from%20noisy%2C%20missing%20observations.%20Compressed%20sensing%20finds%20applications%20in%0Acritical%20domains%2C%20from%20medical%20imaging%20to%20cryptography%2C%20where%20adversarial%0Arobustness%20is%20crucial%20to%20prevent%20catastrophic%20failures.%20However%2C%20a%20solid%0Atheoretical%20understanding%20of%20the%20performance%20of%20unfolding%20networks%20in%20the%0Apresence%20of%20adversarial%20attacks%20is%20still%20in%20its%20infancy.%20In%20this%20paper%2C%20we%0Astudy%20the%20adversarial%20generalization%20of%20unfolding%20networks%20when%20perturbed%20with%0A%24l_2%24-norm%20constrained%20attacks%2C%20generated%20by%20the%20fast%20gradient%20sign%20method.%0AParticularly%2C%20we%20choose%20a%20family%20of%20state-of-the-art%20overaparameterized%0Aunfolding%20networks%20and%20deploy%20a%20new%20framework%20to%20estimate%20their%20adversarial%0ARademacher%20complexity.%20Given%20this%20estimate%2C%20we%20provide%20adversarial%0Ageneralization%20error%20bounds%20for%20the%20networks%20under%20study%2C%20which%20are%20tight%20with%0Arespect%20to%20the%20attack%20level.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20theoretical%0Aanalysis%20on%20the%20adversarial%20generalization%20of%20unfolding%20networks.%20We%20further%0Apresent%20a%20series%20of%20experiments%20on%20real-world%20data%2C%20with%20results%20corroborating%0Aour%20derived%20theory%2C%20consistently%20for%20all%20data.%20Finally%2C%20we%20observe%20that%20the%0Afamily%27s%20overparameterization%20can%20be%20exploited%20to%20promote%20adversarial%0Arobustness%2C%20shedding%20light%20on%20how%20to%20efficiently%20robustify%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15370v3&entry.124074799=Read"},
{"title": "An All-Reduce Compatible Top-K Compressor for Communication-Efficient\n  Distributed Learning", "author": "Chuyan Chen and Chenyang Ma and Zhangxin Li and Yutong He and Yanjie Dong and Kun Yuan", "abstract": "  Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.\n", "link": "http://arxiv.org/abs/2510.26709v1", "date": "2025-10-30", "relevancy": 1.4521, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4836}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20All-Reduce%20Compatible%20Top-K%20Compressor%20for%20Communication-Efficient%0A%20%20Distributed%20Learning&body=Title%3A%20An%20All-Reduce%20Compatible%20Top-K%20Compressor%20for%20Communication-Efficient%0A%20%20Distributed%20Learning%0AAuthor%3A%20Chuyan%20Chen%20and%20Chenyang%20Ma%20and%20Zhangxin%20Li%20and%20Yutong%20He%20and%20Yanjie%20Dong%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20Communication%20remains%20a%20central%20bottleneck%20in%20large-scale%20distributed%20machine%0Alearning%2C%20and%20gradient%20sparsification%20has%20emerged%20as%20a%20promising%20strategy%20to%0Aalleviate%20this%20challenge.%20However%2C%20existing%20gradient%20compressors%20face%20notable%0Alimitations%3A%20Rand-%24K%24%5C%20discards%20structural%20information%20and%20performs%20poorly%20in%0Apractice%2C%20while%20Top-%24K%24%5C%20preserves%20informative%20entries%20but%20loses%20the%0Acontraction%20property%20and%20requires%20costly%20All-Gather%20operations.%20In%20this%20paper%2C%0Awe%20propose%20ARC-Top-%24K%24%2C%20an%20%7BAll-Reduce%7D-Compatible%20Top-%24K%24%20compressor%20that%0Aaligns%20sparsity%20patterns%20across%20nodes%20using%20a%20lightweight%20sketch%20of%20the%0Agradient%2C%20enabling%20index-free%20All-Reduce%20while%20preserving%20globally%20significant%0Ainformation.%20ARC-Top-%24K%24%5C%20is%20provably%20contractive%20and%2C%20when%20combined%20with%0Amomentum%20error%20feedback%20%28EF21M%29%2C%20achieves%20linear%20speedup%20and%20sharper%0Aconvergence%20rates%20than%20the%20original%20EF21M%20under%20standard%20assumptions.%0AEmpirically%2C%20ARC-Top-%24K%24%5C%20matches%20the%20accuracy%20of%20Top-%24K%24%5C%20while%20reducing%0Awall-clock%20training%20time%20by%20up%20to%2060.7%5C%25%2C%20offering%20an%20efficient%20and%20scalable%0Asolution%20that%20combines%20the%20robustness%20of%20Rand-%24K%24%5C%20with%20the%20strong%20performance%0Aof%20Top-%24K%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520All-Reduce%2520Compatible%2520Top-K%2520Compressor%2520for%2520Communication-Efficient%250A%2520%2520Distributed%2520Learning%26entry.906535625%3DChuyan%2520Chen%2520and%2520Chenyang%2520Ma%2520and%2520Zhangxin%2520Li%2520and%2520Yutong%2520He%2520and%2520Yanjie%2520Dong%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520Communication%2520remains%2520a%2520central%2520bottleneck%2520in%2520large-scale%2520distributed%2520machine%250Alearning%252C%2520and%2520gradient%2520sparsification%2520has%2520emerged%2520as%2520a%2520promising%2520strategy%2520to%250Aalleviate%2520this%2520challenge.%2520However%252C%2520existing%2520gradient%2520compressors%2520face%2520notable%250Alimitations%253A%2520Rand-%2524K%2524%255C%2520discards%2520structural%2520information%2520and%2520performs%2520poorly%2520in%250Apractice%252C%2520while%2520Top-%2524K%2524%255C%2520preserves%2520informative%2520entries%2520but%2520loses%2520the%250Acontraction%2520property%2520and%2520requires%2520costly%2520All-Gather%2520operations.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520ARC-Top-%2524K%2524%252C%2520an%2520%257BAll-Reduce%257D-Compatible%2520Top-%2524K%2524%2520compressor%2520that%250Aaligns%2520sparsity%2520patterns%2520across%2520nodes%2520using%2520a%2520lightweight%2520sketch%2520of%2520the%250Agradient%252C%2520enabling%2520index-free%2520All-Reduce%2520while%2520preserving%2520globally%2520significant%250Ainformation.%2520ARC-Top-%2524K%2524%255C%2520is%2520provably%2520contractive%2520and%252C%2520when%2520combined%2520with%250Amomentum%2520error%2520feedback%2520%2528EF21M%2529%252C%2520achieves%2520linear%2520speedup%2520and%2520sharper%250Aconvergence%2520rates%2520than%2520the%2520original%2520EF21M%2520under%2520standard%2520assumptions.%250AEmpirically%252C%2520ARC-Top-%2524K%2524%255C%2520matches%2520the%2520accuracy%2520of%2520Top-%2524K%2524%255C%2520while%2520reducing%250Awall-clock%2520training%2520time%2520by%2520up%2520to%252060.7%255C%2525%252C%2520offering%2520an%2520efficient%2520and%2520scalable%250Asolution%2520that%2520combines%2520the%2520robustness%2520of%2520Rand-%2524K%2524%255C%2520with%2520the%2520strong%2520performance%250Aof%2520Top-%2524K%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20All-Reduce%20Compatible%20Top-K%20Compressor%20for%20Communication-Efficient%0A%20%20Distributed%20Learning&entry.906535625=Chuyan%20Chen%20and%20Chenyang%20Ma%20and%20Zhangxin%20Li%20and%20Yutong%20He%20and%20Yanjie%20Dong%20and%20Kun%20Yuan&entry.1292438233=%20%20Communication%20remains%20a%20central%20bottleneck%20in%20large-scale%20distributed%20machine%0Alearning%2C%20and%20gradient%20sparsification%20has%20emerged%20as%20a%20promising%20strategy%20to%0Aalleviate%20this%20challenge.%20However%2C%20existing%20gradient%20compressors%20face%20notable%0Alimitations%3A%20Rand-%24K%24%5C%20discards%20structural%20information%20and%20performs%20poorly%20in%0Apractice%2C%20while%20Top-%24K%24%5C%20preserves%20informative%20entries%20but%20loses%20the%0Acontraction%20property%20and%20requires%20costly%20All-Gather%20operations.%20In%20this%20paper%2C%0Awe%20propose%20ARC-Top-%24K%24%2C%20an%20%7BAll-Reduce%7D-Compatible%20Top-%24K%24%20compressor%20that%0Aaligns%20sparsity%20patterns%20across%20nodes%20using%20a%20lightweight%20sketch%20of%20the%0Agradient%2C%20enabling%20index-free%20All-Reduce%20while%20preserving%20globally%20significant%0Ainformation.%20ARC-Top-%24K%24%5C%20is%20provably%20contractive%20and%2C%20when%20combined%20with%0Amomentum%20error%20feedback%20%28EF21M%29%2C%20achieves%20linear%20speedup%20and%20sharper%0Aconvergence%20rates%20than%20the%20original%20EF21M%20under%20standard%20assumptions.%0AEmpirically%2C%20ARC-Top-%24K%24%5C%20matches%20the%20accuracy%20of%20Top-%24K%24%5C%20while%20reducing%0Awall-clock%20training%20time%20by%20up%20to%2060.7%5C%25%2C%20offering%20an%20efficient%20and%20scalable%0Asolution%20that%20combines%20the%20robustness%20of%20Rand-%24K%24%5C%20with%20the%20strong%20performance%0Aof%20Top-%24K%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26709v1&entry.124074799=Read"},
{"title": "A Sliding-Window Filter for Online Continuous-Time Continuum Robot State\n  Estimation", "author": "Spencer Teetaert and Sven Lilge and Jessica Burgner-Kahrs and Timothy D. Barfoot", "abstract": "  Stochastic state estimation methods for continuum robots (CRs) often struggle\nto balance accuracy and computational efficiency. While several recent works\nhave explored sliding-window formulations for CRs, these methods are limited to\nsimplified, discrete-time approximations and do not provide stochastic\nrepresentations. In contrast, current stochastic filter methods must run at the\nspeed of measurements, limiting their full potential. Recent works in\ncontinuous-time estimation techniques for CRs show a principled approach to\naddressing this runtime constraint, but are currently restricted to offline\noperation. In this work, we present a sliding-window filter (SWF) for\ncontinuous-time state estimation of CRs that improves upon the accuracy of a\nfilter approach while enabling continuous-time methods to operate online, all\nwhile running at faster-than-real-time speeds. This represents the first\nstochastic SWF specifically designed for CRs, providing a promising direction\nfor future research in this area.\n", "link": "http://arxiv.org/abs/2510.26623v1", "date": "2025-10-30", "relevancy": 1.4431, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5324}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4697}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sliding-Window%20Filter%20for%20Online%20Continuous-Time%20Continuum%20Robot%20State%0A%20%20Estimation&body=Title%3A%20A%20Sliding-Window%20Filter%20for%20Online%20Continuous-Time%20Continuum%20Robot%20State%0A%20%20Estimation%0AAuthor%3A%20Spencer%20Teetaert%20and%20Sven%20Lilge%20and%20Jessica%20Burgner-Kahrs%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Stochastic%20state%20estimation%20methods%20for%20continuum%20robots%20%28CRs%29%20often%20struggle%0Ato%20balance%20accuracy%20and%20computational%20efficiency.%20While%20several%20recent%20works%0Ahave%20explored%20sliding-window%20formulations%20for%20CRs%2C%20these%20methods%20are%20limited%20to%0Asimplified%2C%20discrete-time%20approximations%20and%20do%20not%20provide%20stochastic%0Arepresentations.%20In%20contrast%2C%20current%20stochastic%20filter%20methods%20must%20run%20at%20the%0Aspeed%20of%20measurements%2C%20limiting%20their%20full%20potential.%20Recent%20works%20in%0Acontinuous-time%20estimation%20techniques%20for%20CRs%20show%20a%20principled%20approach%20to%0Aaddressing%20this%20runtime%20constraint%2C%20but%20are%20currently%20restricted%20to%20offline%0Aoperation.%20In%20this%20work%2C%20we%20present%20a%20sliding-window%20filter%20%28SWF%29%20for%0Acontinuous-time%20state%20estimation%20of%20CRs%20that%20improves%20upon%20the%20accuracy%20of%20a%0Afilter%20approach%20while%20enabling%20continuous-time%20methods%20to%20operate%20online%2C%20all%0Awhile%20running%20at%20faster-than-real-time%20speeds.%20This%20represents%20the%20first%0Astochastic%20SWF%20specifically%20designed%20for%20CRs%2C%20providing%20a%20promising%20direction%0Afor%20future%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sliding-Window%2520Filter%2520for%2520Online%2520Continuous-Time%2520Continuum%2520Robot%2520State%250A%2520%2520Estimation%26entry.906535625%3DSpencer%2520Teetaert%2520and%2520Sven%2520Lilge%2520and%2520Jessica%2520Burgner-Kahrs%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Stochastic%2520state%2520estimation%2520methods%2520for%2520continuum%2520robots%2520%2528CRs%2529%2520often%2520struggle%250Ato%2520balance%2520accuracy%2520and%2520computational%2520efficiency.%2520While%2520several%2520recent%2520works%250Ahave%2520explored%2520sliding-window%2520formulations%2520for%2520CRs%252C%2520these%2520methods%2520are%2520limited%2520to%250Asimplified%252C%2520discrete-time%2520approximations%2520and%2520do%2520not%2520provide%2520stochastic%250Arepresentations.%2520In%2520contrast%252C%2520current%2520stochastic%2520filter%2520methods%2520must%2520run%2520at%2520the%250Aspeed%2520of%2520measurements%252C%2520limiting%2520their%2520full%2520potential.%2520Recent%2520works%2520in%250Acontinuous-time%2520estimation%2520techniques%2520for%2520CRs%2520show%2520a%2520principled%2520approach%2520to%250Aaddressing%2520this%2520runtime%2520constraint%252C%2520but%2520are%2520currently%2520restricted%2520to%2520offline%250Aoperation.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520sliding-window%2520filter%2520%2528SWF%2529%2520for%250Acontinuous-time%2520state%2520estimation%2520of%2520CRs%2520that%2520improves%2520upon%2520the%2520accuracy%2520of%2520a%250Afilter%2520approach%2520while%2520enabling%2520continuous-time%2520methods%2520to%2520operate%2520online%252C%2520all%250Awhile%2520running%2520at%2520faster-than-real-time%2520speeds.%2520This%2520represents%2520the%2520first%250Astochastic%2520SWF%2520specifically%2520designed%2520for%2520CRs%252C%2520providing%2520a%2520promising%2520direction%250Afor%2520future%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sliding-Window%20Filter%20for%20Online%20Continuous-Time%20Continuum%20Robot%20State%0A%20%20Estimation&entry.906535625=Spencer%20Teetaert%20and%20Sven%20Lilge%20and%20Jessica%20Burgner-Kahrs%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Stochastic%20state%20estimation%20methods%20for%20continuum%20robots%20%28CRs%29%20often%20struggle%0Ato%20balance%20accuracy%20and%20computational%20efficiency.%20While%20several%20recent%20works%0Ahave%20explored%20sliding-window%20formulations%20for%20CRs%2C%20these%20methods%20are%20limited%20to%0Asimplified%2C%20discrete-time%20approximations%20and%20do%20not%20provide%20stochastic%0Arepresentations.%20In%20contrast%2C%20current%20stochastic%20filter%20methods%20must%20run%20at%20the%0Aspeed%20of%20measurements%2C%20limiting%20their%20full%20potential.%20Recent%20works%20in%0Acontinuous-time%20estimation%20techniques%20for%20CRs%20show%20a%20principled%20approach%20to%0Aaddressing%20this%20runtime%20constraint%2C%20but%20are%20currently%20restricted%20to%20offline%0Aoperation.%20In%20this%20work%2C%20we%20present%20a%20sliding-window%20filter%20%28SWF%29%20for%0Acontinuous-time%20state%20estimation%20of%20CRs%20that%20improves%20upon%20the%20accuracy%20of%20a%0Afilter%20approach%20while%20enabling%20continuous-time%20methods%20to%20operate%20online%2C%20all%0Awhile%20running%20at%20faster-than-real-time%20speeds.%20This%20represents%20the%20first%0Astochastic%20SWF%20specifically%20designed%20for%20CRs%2C%20providing%20a%20promising%20direction%0Afor%20future%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26623v1&entry.124074799=Read"},
{"title": "Direct Debiased Machine Learning via Bregman Divergence Minimization", "author": "Masahiro Kato", "abstract": "  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n", "link": "http://arxiv.org/abs/2510.23534v2", "date": "2025-10-30", "relevancy": 1.4313, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Debiased%20Machine%20Learning%20via%20Bregman%20Divergence%20Minimization&body=Title%3A%20Direct%20Debiased%20Machine%20Learning%20via%20Bregman%20Divergence%20Minimization%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20We%20develop%20a%20direct%20debiased%20machine%20learning%20framework%20comprising%20Neyman%0Atargeted%20estimation%20and%20generalized%20Riesz%20regression.%20Our%20framework%20unifies%0ARiesz%20regression%20for%20automatic%20debiased%20machine%20learning%2C%20covariate%20balancing%2C%0Atargeted%20maximum%20likelihood%20estimation%20%28TMLE%29%2C%20and%20density-ratio%20estimation.%20In%0Amany%20problems%20involving%20causal%20effects%20or%20structural%20models%2C%20the%20parameters%20of%0Ainterest%20depend%20on%20regression%20functions.%20Plugging%20regression%20functions%0Aestimated%20by%20machine%20learning%20methods%20into%20the%20identifying%20equations%20can%20yield%0Apoor%20performance%20because%20of%20first-stage%20bias.%20To%20reduce%20such%20bias%2C%20debiased%0Amachine%20learning%20employs%20Neyman%20orthogonal%20estimating%20equations.%20Debiased%0Amachine%20learning%20typically%20requires%20estimation%20of%20the%20Riesz%20representer%20and%20the%0Aregression%20function.%20For%20this%20problem%2C%20we%20develop%20a%20direct%20debiased%20machine%0Alearning%20framework%20with%20an%20end-to-end%20algorithm.%20We%20formulate%20estimation%20of%20the%0Anuisance%20parameters%2C%20the%20regression%20function%20and%20the%20Riesz%20representer%2C%20as%0Aminimizing%20the%20discrepancy%20between%20Neyman%20orthogonal%20scores%20computed%20with%20known%0Aand%20unknown%20nuisance%20parameters%2C%20which%20we%20refer%20to%20as%20Neyman%20targeted%0Aestimation.%20Neyman%20targeted%20estimation%20includes%20Riesz%20representer%20estimation%2C%0Aand%20we%20measure%20discrepancies%20using%20the%20Bregman%20divergence.%20The%20Bregman%0Adivergence%20encompasses%20various%20loss%20functions%20as%20special%20cases%2C%20where%20the%0Asquared%20loss%20yields%20Riesz%20regression%20and%20the%20Kullback-Leibler%20divergence%20yields%0Aentropy%20balancing.%20We%20refer%20to%20this%20Riesz%20representer%20estimation%20as%20generalized%0ARiesz%20regression.%20Neyman%20targeted%20estimation%20also%20yields%20TMLE%20as%20a%20special%20case%0Afor%20regression%20function%20estimation.%20Furthermore%2C%20for%20specific%20pairs%20of%20models%0Aand%20Riesz%20representer%20estimation%20methods%2C%20we%20can%20automatically%20obtain%20the%0Acovariate%20balancing%20property%20without%20explicitly%20solving%20the%20covariate%20balancing%0Aobjective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Debiased%2520Machine%2520Learning%2520via%2520Bregman%2520Divergence%2520Minimization%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520direct%2520debiased%2520machine%2520learning%2520framework%2520comprising%2520Neyman%250Atargeted%2520estimation%2520and%2520generalized%2520Riesz%2520regression.%2520Our%2520framework%2520unifies%250ARiesz%2520regression%2520for%2520automatic%2520debiased%2520machine%2520learning%252C%2520covariate%2520balancing%252C%250Atargeted%2520maximum%2520likelihood%2520estimation%2520%2528TMLE%2529%252C%2520and%2520density-ratio%2520estimation.%2520In%250Amany%2520problems%2520involving%2520causal%2520effects%2520or%2520structural%2520models%252C%2520the%2520parameters%2520of%250Ainterest%2520depend%2520on%2520regression%2520functions.%2520Plugging%2520regression%2520functions%250Aestimated%2520by%2520machine%2520learning%2520methods%2520into%2520the%2520identifying%2520equations%2520can%2520yield%250Apoor%2520performance%2520because%2520of%2520first-stage%2520bias.%2520To%2520reduce%2520such%2520bias%252C%2520debiased%250Amachine%2520learning%2520employs%2520Neyman%2520orthogonal%2520estimating%2520equations.%2520Debiased%250Amachine%2520learning%2520typically%2520requires%2520estimation%2520of%2520the%2520Riesz%2520representer%2520and%2520the%250Aregression%2520function.%2520For%2520this%2520problem%252C%2520we%2520develop%2520a%2520direct%2520debiased%2520machine%250Alearning%2520framework%2520with%2520an%2520end-to-end%2520algorithm.%2520We%2520formulate%2520estimation%2520of%2520the%250Anuisance%2520parameters%252C%2520the%2520regression%2520function%2520and%2520the%2520Riesz%2520representer%252C%2520as%250Aminimizing%2520the%2520discrepancy%2520between%2520Neyman%2520orthogonal%2520scores%2520computed%2520with%2520known%250Aand%2520unknown%2520nuisance%2520parameters%252C%2520which%2520we%2520refer%2520to%2520as%2520Neyman%2520targeted%250Aestimation.%2520Neyman%2520targeted%2520estimation%2520includes%2520Riesz%2520representer%2520estimation%252C%250Aand%2520we%2520measure%2520discrepancies%2520using%2520the%2520Bregman%2520divergence.%2520The%2520Bregman%250Adivergence%2520encompasses%2520various%2520loss%2520functions%2520as%2520special%2520cases%252C%2520where%2520the%250Asquared%2520loss%2520yields%2520Riesz%2520regression%2520and%2520the%2520Kullback-Leibler%2520divergence%2520yields%250Aentropy%2520balancing.%2520We%2520refer%2520to%2520this%2520Riesz%2520representer%2520estimation%2520as%2520generalized%250ARiesz%2520regression.%2520Neyman%2520targeted%2520estimation%2520also%2520yields%2520TMLE%2520as%2520a%2520special%2520case%250Afor%2520regression%2520function%2520estimation.%2520Furthermore%252C%2520for%2520specific%2520pairs%2520of%2520models%250Aand%2520Riesz%2520representer%2520estimation%2520methods%252C%2520we%2520can%2520automatically%2520obtain%2520the%250Acovariate%2520balancing%2520property%2520without%2520explicitly%2520solving%2520the%2520covariate%2520balancing%250Aobjective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Debiased%20Machine%20Learning%20via%20Bregman%20Divergence%20Minimization&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20We%20develop%20a%20direct%20debiased%20machine%20learning%20framework%20comprising%20Neyman%0Atargeted%20estimation%20and%20generalized%20Riesz%20regression.%20Our%20framework%20unifies%0ARiesz%20regression%20for%20automatic%20debiased%20machine%20learning%2C%20covariate%20balancing%2C%0Atargeted%20maximum%20likelihood%20estimation%20%28TMLE%29%2C%20and%20density-ratio%20estimation.%20In%0Amany%20problems%20involving%20causal%20effects%20or%20structural%20models%2C%20the%20parameters%20of%0Ainterest%20depend%20on%20regression%20functions.%20Plugging%20regression%20functions%0Aestimated%20by%20machine%20learning%20methods%20into%20the%20identifying%20equations%20can%20yield%0Apoor%20performance%20because%20of%20first-stage%20bias.%20To%20reduce%20such%20bias%2C%20debiased%0Amachine%20learning%20employs%20Neyman%20orthogonal%20estimating%20equations.%20Debiased%0Amachine%20learning%20typically%20requires%20estimation%20of%20the%20Riesz%20representer%20and%20the%0Aregression%20function.%20For%20this%20problem%2C%20we%20develop%20a%20direct%20debiased%20machine%0Alearning%20framework%20with%20an%20end-to-end%20algorithm.%20We%20formulate%20estimation%20of%20the%0Anuisance%20parameters%2C%20the%20regression%20function%20and%20the%20Riesz%20representer%2C%20as%0Aminimizing%20the%20discrepancy%20between%20Neyman%20orthogonal%20scores%20computed%20with%20known%0Aand%20unknown%20nuisance%20parameters%2C%20which%20we%20refer%20to%20as%20Neyman%20targeted%0Aestimation.%20Neyman%20targeted%20estimation%20includes%20Riesz%20representer%20estimation%2C%0Aand%20we%20measure%20discrepancies%20using%20the%20Bregman%20divergence.%20The%20Bregman%0Adivergence%20encompasses%20various%20loss%20functions%20as%20special%20cases%2C%20where%20the%0Asquared%20loss%20yields%20Riesz%20regression%20and%20the%20Kullback-Leibler%20divergence%20yields%0Aentropy%20balancing.%20We%20refer%20to%20this%20Riesz%20representer%20estimation%20as%20generalized%0ARiesz%20regression.%20Neyman%20targeted%20estimation%20also%20yields%20TMLE%20as%20a%20special%20case%0Afor%20regression%20function%20estimation.%20Furthermore%2C%20for%20specific%20pairs%20of%20models%0Aand%20Riesz%20representer%20estimation%20methods%2C%20we%20can%20automatically%20obtain%20the%0Acovariate%20balancing%20property%20without%20explicitly%20solving%20the%20covariate%20balancing%0Aobjective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23534v2&entry.124074799=Read"},
{"title": "A General Incentives-Based Framework for Fairness in Multi-agent\n  Resource Allocation", "author": "Ashwin Kumar and William Yeoh", "abstract": "  We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems.\n", "link": "http://arxiv.org/abs/2510.26740v1", "date": "2025-10-30", "relevancy": 1.4277, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5027}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4691}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Incentives-Based%20Framework%20for%20Fairness%20in%20Multi-agent%0A%20%20Resource%20Allocation&body=Title%3A%20A%20General%20Incentives-Based%20Framework%20for%20Fairness%20in%20Multi-agent%0A%20%20Resource%20Allocation%0AAuthor%3A%20Ashwin%20Kumar%20and%20William%20Yeoh%0AAbstract%3A%20%20%20We%20introduce%20the%20General%20Incentives-based%20Framework%20for%20Fairness%20%28GIFF%29%2C%20a%0Anovel%20approach%20for%20fair%20multi-agent%20resource%20allocation%20that%20infers%20fair%0Adecision-making%20from%20standard%20value%20functions.%20In%20resource-constrained%0Asettings%2C%20agents%20optimizing%20for%20efficiency%20often%20create%20inequitable%20outcomes.%0AOur%20approach%20leverages%20the%20action-value%20%28Q-%29function%20to%20balance%20efficiency%20and%0Afairness%20without%20requiring%20additional%20training.%20Specifically%2C%20our%20method%0Acomputes%20a%20local%20fairness%20gain%20for%20each%20action%20and%20introduces%20a%20counterfactual%0Aadvantage%20correction%20term%20to%20discourage%20over-allocation%20to%20already%20well-off%0Aagents.%20This%20approach%20is%20formalized%20within%20a%20centralized%20control%20setting%2C%20where%0Aan%20arbitrator%20uses%20the%20GIFF-modified%20Q-values%20to%20solve%20an%20allocation%20problem.%0A%20%20Empirical%20evaluations%20across%20diverse%20domains%2C%20including%20dynamic%20ridesharing%2C%0Ahomelessness%20prevention%2C%20and%20a%20complex%20job%20allocation%20task-demonstrate%20that%20our%0Aframework%20consistently%20outperforms%20strong%20baselines%20and%20can%20discover%0Afar-sighted%2C%20equitable%20policies.%20The%20framework%27s%20effectiveness%20is%20supported%20by%0Aa%20theoretical%20foundation%3B%20we%20prove%20its%20fairness%20surrogate%20is%20a%20principled%20lower%0Abound%20on%20the%20true%20fairness%20improvement%20and%20that%20its%20trade-off%20parameter%20offers%0Amonotonic%20tuning.%20Our%20findings%20establish%20GIFF%20as%20a%20robust%20and%20principled%0Aframework%20for%20leveraging%20standard%20reinforcement%20learning%20components%20to%20achieve%0Amore%20equitable%20outcomes%20in%20complex%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Incentives-Based%2520Framework%2520for%2520Fairness%2520in%2520Multi-agent%250A%2520%2520Resource%2520Allocation%26entry.906535625%3DAshwin%2520Kumar%2520and%2520William%2520Yeoh%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520General%2520Incentives-based%2520Framework%2520for%2520Fairness%2520%2528GIFF%2529%252C%2520a%250Anovel%2520approach%2520for%2520fair%2520multi-agent%2520resource%2520allocation%2520that%2520infers%2520fair%250Adecision-making%2520from%2520standard%2520value%2520functions.%2520In%2520resource-constrained%250Asettings%252C%2520agents%2520optimizing%2520for%2520efficiency%2520often%2520create%2520inequitable%2520outcomes.%250AOur%2520approach%2520leverages%2520the%2520action-value%2520%2528Q-%2529function%2520to%2520balance%2520efficiency%2520and%250Afairness%2520without%2520requiring%2520additional%2520training.%2520Specifically%252C%2520our%2520method%250Acomputes%2520a%2520local%2520fairness%2520gain%2520for%2520each%2520action%2520and%2520introduces%2520a%2520counterfactual%250Aadvantage%2520correction%2520term%2520to%2520discourage%2520over-allocation%2520to%2520already%2520well-off%250Aagents.%2520This%2520approach%2520is%2520formalized%2520within%2520a%2520centralized%2520control%2520setting%252C%2520where%250Aan%2520arbitrator%2520uses%2520the%2520GIFF-modified%2520Q-values%2520to%2520solve%2520an%2520allocation%2520problem.%250A%2520%2520Empirical%2520evaluations%2520across%2520diverse%2520domains%252C%2520including%2520dynamic%2520ridesharing%252C%250Ahomelessness%2520prevention%252C%2520and%2520a%2520complex%2520job%2520allocation%2520task-demonstrate%2520that%2520our%250Aframework%2520consistently%2520outperforms%2520strong%2520baselines%2520and%2520can%2520discover%250Afar-sighted%252C%2520equitable%2520policies.%2520The%2520framework%2527s%2520effectiveness%2520is%2520supported%2520by%250Aa%2520theoretical%2520foundation%253B%2520we%2520prove%2520its%2520fairness%2520surrogate%2520is%2520a%2520principled%2520lower%250Abound%2520on%2520the%2520true%2520fairness%2520improvement%2520and%2520that%2520its%2520trade-off%2520parameter%2520offers%250Amonotonic%2520tuning.%2520Our%2520findings%2520establish%2520GIFF%2520as%2520a%2520robust%2520and%2520principled%250Aframework%2520for%2520leveraging%2520standard%2520reinforcement%2520learning%2520components%2520to%2520achieve%250Amore%2520equitable%2520outcomes%2520in%2520complex%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Incentives-Based%20Framework%20for%20Fairness%20in%20Multi-agent%0A%20%20Resource%20Allocation&entry.906535625=Ashwin%20Kumar%20and%20William%20Yeoh&entry.1292438233=%20%20We%20introduce%20the%20General%20Incentives-based%20Framework%20for%20Fairness%20%28GIFF%29%2C%20a%0Anovel%20approach%20for%20fair%20multi-agent%20resource%20allocation%20that%20infers%20fair%0Adecision-making%20from%20standard%20value%20functions.%20In%20resource-constrained%0Asettings%2C%20agents%20optimizing%20for%20efficiency%20often%20create%20inequitable%20outcomes.%0AOur%20approach%20leverages%20the%20action-value%20%28Q-%29function%20to%20balance%20efficiency%20and%0Afairness%20without%20requiring%20additional%20training.%20Specifically%2C%20our%20method%0Acomputes%20a%20local%20fairness%20gain%20for%20each%20action%20and%20introduces%20a%20counterfactual%0Aadvantage%20correction%20term%20to%20discourage%20over-allocation%20to%20already%20well-off%0Aagents.%20This%20approach%20is%20formalized%20within%20a%20centralized%20control%20setting%2C%20where%0Aan%20arbitrator%20uses%20the%20GIFF-modified%20Q-values%20to%20solve%20an%20allocation%20problem.%0A%20%20Empirical%20evaluations%20across%20diverse%20domains%2C%20including%20dynamic%20ridesharing%2C%0Ahomelessness%20prevention%2C%20and%20a%20complex%20job%20allocation%20task-demonstrate%20that%20our%0Aframework%20consistently%20outperforms%20strong%20baselines%20and%20can%20discover%0Afar-sighted%2C%20equitable%20policies.%20The%20framework%27s%20effectiveness%20is%20supported%20by%0Aa%20theoretical%20foundation%3B%20we%20prove%20its%20fairness%20surrogate%20is%20a%20principled%20lower%0Abound%20on%20the%20true%20fairness%20improvement%20and%20that%20its%20trade-off%20parameter%20offers%0Amonotonic%20tuning.%20Our%20findings%20establish%20GIFF%20as%20a%20robust%20and%20principled%0Aframework%20for%20leveraging%20standard%20reinforcement%20learning%20components%20to%20achieve%0Amore%20equitable%20outcomes%20in%20complex%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26740v1&entry.124074799=Read"},
{"title": "How Regularization Terms Make Invertible Neural Networks Bayesian Point\n  Estimators", "author": "Nick Heilenk\u00f6tter", "abstract": "  Can regularization terms in the training of invertible neural networks lead\nto known Bayesian point estimators in reconstruction? Invertible networks are\nattractive for inverse problems due to their inherent stability and\ninterpretability. Recently, optimization strategies for invertible neural\nnetworks that approximate either a reconstruction map or the forward operator\nhave been studied from a Bayesian perspective, but each has limitations. To\naddress this, we introduce and analyze two regularization terms for the network\ntraining that, upon inversion of the network, recover properties of classical\nBayesian point estimators: while the first can be connected to the posterior\nmean, the second resembles the MAP estimator. Our theoretical analysis\ncharacterizes how each loss shapes both the learned forward operator and its\ninverse reconstruction map. Numerical experiments support our findings and\ndemonstrate how these loss-term regularizers introduce data-dependence in a\nstable and interpretable way.\n", "link": "http://arxiv.org/abs/2510.26704v1", "date": "2025-10-30", "relevancy": 1.4218, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4683}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Regularization%20Terms%20Make%20Invertible%20Neural%20Networks%20Bayesian%20Point%0A%20%20Estimators&body=Title%3A%20How%20Regularization%20Terms%20Make%20Invertible%20Neural%20Networks%20Bayesian%20Point%0A%20%20Estimators%0AAuthor%3A%20Nick%20Heilenk%C3%B6tter%0AAbstract%3A%20%20%20Can%20regularization%20terms%20in%20the%20training%20of%20invertible%20neural%20networks%20lead%0Ato%20known%20Bayesian%20point%20estimators%20in%20reconstruction%3F%20Invertible%20networks%20are%0Aattractive%20for%20inverse%20problems%20due%20to%20their%20inherent%20stability%20and%0Ainterpretability.%20Recently%2C%20optimization%20strategies%20for%20invertible%20neural%0Anetworks%20that%20approximate%20either%20a%20reconstruction%20map%20or%20the%20forward%20operator%0Ahave%20been%20studied%20from%20a%20Bayesian%20perspective%2C%20but%20each%20has%20limitations.%20To%0Aaddress%20this%2C%20we%20introduce%20and%20analyze%20two%20regularization%20terms%20for%20the%20network%0Atraining%20that%2C%20upon%20inversion%20of%20the%20network%2C%20recover%20properties%20of%20classical%0ABayesian%20point%20estimators%3A%20while%20the%20first%20can%20be%20connected%20to%20the%20posterior%0Amean%2C%20the%20second%20resembles%20the%20MAP%20estimator.%20Our%20theoretical%20analysis%0Acharacterizes%20how%20each%20loss%20shapes%20both%20the%20learned%20forward%20operator%20and%20its%0Ainverse%20reconstruction%20map.%20Numerical%20experiments%20support%20our%20findings%20and%0Ademonstrate%20how%20these%20loss-term%20regularizers%20introduce%20data-dependence%20in%20a%0Astable%20and%20interpretable%20way.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Regularization%2520Terms%2520Make%2520Invertible%2520Neural%2520Networks%2520Bayesian%2520Point%250A%2520%2520Estimators%26entry.906535625%3DNick%2520Heilenk%25C3%25B6tter%26entry.1292438233%3D%2520%2520Can%2520regularization%2520terms%2520in%2520the%2520training%2520of%2520invertible%2520neural%2520networks%2520lead%250Ato%2520known%2520Bayesian%2520point%2520estimators%2520in%2520reconstruction%253F%2520Invertible%2520networks%2520are%250Aattractive%2520for%2520inverse%2520problems%2520due%2520to%2520their%2520inherent%2520stability%2520and%250Ainterpretability.%2520Recently%252C%2520optimization%2520strategies%2520for%2520invertible%2520neural%250Anetworks%2520that%2520approximate%2520either%2520a%2520reconstruction%2520map%2520or%2520the%2520forward%2520operator%250Ahave%2520been%2520studied%2520from%2520a%2520Bayesian%2520perspective%252C%2520but%2520each%2520has%2520limitations.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520and%2520analyze%2520two%2520regularization%2520terms%2520for%2520the%2520network%250Atraining%2520that%252C%2520upon%2520inversion%2520of%2520the%2520network%252C%2520recover%2520properties%2520of%2520classical%250ABayesian%2520point%2520estimators%253A%2520while%2520the%2520first%2520can%2520be%2520connected%2520to%2520the%2520posterior%250Amean%252C%2520the%2520second%2520resembles%2520the%2520MAP%2520estimator.%2520Our%2520theoretical%2520analysis%250Acharacterizes%2520how%2520each%2520loss%2520shapes%2520both%2520the%2520learned%2520forward%2520operator%2520and%2520its%250Ainverse%2520reconstruction%2520map.%2520Numerical%2520experiments%2520support%2520our%2520findings%2520and%250Ademonstrate%2520how%2520these%2520loss-term%2520regularizers%2520introduce%2520data-dependence%2520in%2520a%250Astable%2520and%2520interpretable%2520way.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Regularization%20Terms%20Make%20Invertible%20Neural%20Networks%20Bayesian%20Point%0A%20%20Estimators&entry.906535625=Nick%20Heilenk%C3%B6tter&entry.1292438233=%20%20Can%20regularization%20terms%20in%20the%20training%20of%20invertible%20neural%20networks%20lead%0Ato%20known%20Bayesian%20point%20estimators%20in%20reconstruction%3F%20Invertible%20networks%20are%0Aattractive%20for%20inverse%20problems%20due%20to%20their%20inherent%20stability%20and%0Ainterpretability.%20Recently%2C%20optimization%20strategies%20for%20invertible%20neural%0Anetworks%20that%20approximate%20either%20a%20reconstruction%20map%20or%20the%20forward%20operator%0Ahave%20been%20studied%20from%20a%20Bayesian%20perspective%2C%20but%20each%20has%20limitations.%20To%0Aaddress%20this%2C%20we%20introduce%20and%20analyze%20two%20regularization%20terms%20for%20the%20network%0Atraining%20that%2C%20upon%20inversion%20of%20the%20network%2C%20recover%20properties%20of%20classical%0ABayesian%20point%20estimators%3A%20while%20the%20first%20can%20be%20connected%20to%20the%20posterior%0Amean%2C%20the%20second%20resembles%20the%20MAP%20estimator.%20Our%20theoretical%20analysis%0Acharacterizes%20how%20each%20loss%20shapes%20both%20the%20learned%20forward%20operator%20and%20its%0Ainverse%20reconstruction%20map.%20Numerical%20experiments%20support%20our%20findings%20and%0Ademonstrate%20how%20these%20loss-term%20regularizers%20introduce%20data-dependence%20in%20a%0Astable%20and%20interpretable%20way.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26704v1&entry.124074799=Read"},
{"title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian\n  Optimization", "author": "Colin Doumont and Victor Picheny and Viacheslav Borovitskiy and Henry Moss", "abstract": "  Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.\n", "link": "http://arxiv.org/abs/2510.26633v1", "date": "2025-10-30", "relevancy": 1.4166, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omnipresent%20Yet%20Overlooked%3A%20Heat%20Kernels%20in%20Combinatorial%20Bayesian%0A%20%20Optimization&body=Title%3A%20Omnipresent%20Yet%20Overlooked%3A%20Heat%20Kernels%20in%20Combinatorial%20Bayesian%0A%20%20Optimization%0AAuthor%3A%20Colin%20Doumont%20and%20Victor%20Picheny%20and%20Viacheslav%20Borovitskiy%20and%20Henry%20Moss%0AAbstract%3A%20%20%20Bayesian%20Optimization%20%28BO%29%20has%20the%20potential%20to%20solve%20various%20combinatorial%0Atasks%2C%20ranging%20from%20materials%20science%20to%20neural%20architecture%20search.%20However%2C%0ABO%20requires%20specialized%20kernels%20to%20effectively%20model%20combinatorial%20domains.%0ARecent%20efforts%20have%20introduced%20several%20combinatorial%20kernels%2C%20but%20the%0Arelationships%20among%20them%20are%20not%20well%20understood.%20To%20bridge%20this%20gap%2C%20we%0Adevelop%20a%20unifying%20framework%20based%20on%20heat%20kernels%2C%20which%20we%20derive%20in%20a%0Asystematic%20way%20and%20express%20as%20simple%20closed-form%20expressions.%20Using%20this%0Aframework%2C%20we%20prove%20that%20many%20successful%20combinatorial%20kernels%20are%20either%0Arelated%20or%20equivalent%20to%20heat%20kernels%2C%20and%20validate%20this%20theoretical%20claim%20in%0Aour%20experiments.%20Moreover%2C%20our%20analysis%20confirms%20and%20extends%20the%20results%0Apresented%20in%20Bounce%3A%20certain%20algorithms%27%20performance%20decreases%20substantially%0Awhen%20the%20unknown%20optima%20of%20the%20function%20do%20not%20have%20a%20certain%20structure.%20In%0Acontrast%2C%20heat%20kernels%20are%20not%20sensitive%20to%20the%20location%20of%20the%20optima.%20Lastly%2C%0Awe%20show%20that%20a%20fast%20and%20simple%20pipeline%2C%20relying%20on%20heat%20kernels%2C%20is%20able%20to%0Aachieve%20state-of-the-art%20results%2C%20matching%20or%20even%20outperforming%20certain%20slow%0Aor%20complex%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmnipresent%2520Yet%2520Overlooked%253A%2520Heat%2520Kernels%2520in%2520Combinatorial%2520Bayesian%250A%2520%2520Optimization%26entry.906535625%3DColin%2520Doumont%2520and%2520Victor%2520Picheny%2520and%2520Viacheslav%2520Borovitskiy%2520and%2520Henry%2520Moss%26entry.1292438233%3D%2520%2520Bayesian%2520Optimization%2520%2528BO%2529%2520has%2520the%2520potential%2520to%2520solve%2520various%2520combinatorial%250Atasks%252C%2520ranging%2520from%2520materials%2520science%2520to%2520neural%2520architecture%2520search.%2520However%252C%250ABO%2520requires%2520specialized%2520kernels%2520to%2520effectively%2520model%2520combinatorial%2520domains.%250ARecent%2520efforts%2520have%2520introduced%2520several%2520combinatorial%2520kernels%252C%2520but%2520the%250Arelationships%2520among%2520them%2520are%2520not%2520well%2520understood.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Adevelop%2520a%2520unifying%2520framework%2520based%2520on%2520heat%2520kernels%252C%2520which%2520we%2520derive%2520in%2520a%250Asystematic%2520way%2520and%2520express%2520as%2520simple%2520closed-form%2520expressions.%2520Using%2520this%250Aframework%252C%2520we%2520prove%2520that%2520many%2520successful%2520combinatorial%2520kernels%2520are%2520either%250Arelated%2520or%2520equivalent%2520to%2520heat%2520kernels%252C%2520and%2520validate%2520this%2520theoretical%2520claim%2520in%250Aour%2520experiments.%2520Moreover%252C%2520our%2520analysis%2520confirms%2520and%2520extends%2520the%2520results%250Apresented%2520in%2520Bounce%253A%2520certain%2520algorithms%2527%2520performance%2520decreases%2520substantially%250Awhen%2520the%2520unknown%2520optima%2520of%2520the%2520function%2520do%2520not%2520have%2520a%2520certain%2520structure.%2520In%250Acontrast%252C%2520heat%2520kernels%2520are%2520not%2520sensitive%2520to%2520the%2520location%2520of%2520the%2520optima.%2520Lastly%252C%250Awe%2520show%2520that%2520a%2520fast%2520and%2520simple%2520pipeline%252C%2520relying%2520on%2520heat%2520kernels%252C%2520is%2520able%2520to%250Aachieve%2520state-of-the-art%2520results%252C%2520matching%2520or%2520even%2520outperforming%2520certain%2520slow%250Aor%2520complex%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omnipresent%20Yet%20Overlooked%3A%20Heat%20Kernels%20in%20Combinatorial%20Bayesian%0A%20%20Optimization&entry.906535625=Colin%20Doumont%20and%20Victor%20Picheny%20and%20Viacheslav%20Borovitskiy%20and%20Henry%20Moss&entry.1292438233=%20%20Bayesian%20Optimization%20%28BO%29%20has%20the%20potential%20to%20solve%20various%20combinatorial%0Atasks%2C%20ranging%20from%20materials%20science%20to%20neural%20architecture%20search.%20However%2C%0ABO%20requires%20specialized%20kernels%20to%20effectively%20model%20combinatorial%20domains.%0ARecent%20efforts%20have%20introduced%20several%20combinatorial%20kernels%2C%20but%20the%0Arelationships%20among%20them%20are%20not%20well%20understood.%20To%20bridge%20this%20gap%2C%20we%0Adevelop%20a%20unifying%20framework%20based%20on%20heat%20kernels%2C%20which%20we%20derive%20in%20a%0Asystematic%20way%20and%20express%20as%20simple%20closed-form%20expressions.%20Using%20this%0Aframework%2C%20we%20prove%20that%20many%20successful%20combinatorial%20kernels%20are%20either%0Arelated%20or%20equivalent%20to%20heat%20kernels%2C%20and%20validate%20this%20theoretical%20claim%20in%0Aour%20experiments.%20Moreover%2C%20our%20analysis%20confirms%20and%20extends%20the%20results%0Apresented%20in%20Bounce%3A%20certain%20algorithms%27%20performance%20decreases%20substantially%0Awhen%20the%20unknown%20optima%20of%20the%20function%20do%20not%20have%20a%20certain%20structure.%20In%0Acontrast%2C%20heat%20kernels%20are%20not%20sensitive%20to%20the%20location%20of%20the%20optima.%20Lastly%2C%0Awe%20show%20that%20a%20fast%20and%20simple%20pipeline%2C%20relying%20on%20heat%20kernels%2C%20is%20able%20to%0Aachieve%20state-of-the-art%20results%2C%20matching%20or%20even%20outperforming%20certain%20slow%0Aor%20complex%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26633v1&entry.124074799=Read"},
{"title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and\n  Information Environment Signals on Social Media", "author": "Soorya Ram Shimgekar and Ruining Zhao and Agam Goyal and Violeta J. Rodriguez and Paul A. Bloom and Hari Sundaram and Koustuv Saha", "abstract": "  On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.\n", "link": "http://arxiv.org/abs/2510.14889v2", "date": "2025-10-30", "relevancy": 1.3937, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Early%20and%20Implicit%20Suicidal%20Ideation%20via%20Longitudinal%20and%0A%20%20Information%20Environment%20Signals%20on%20Social%20Media&body=Title%3A%20Detecting%20Early%20and%20Implicit%20Suicidal%20Ideation%20via%20Longitudinal%20and%0A%20%20Information%20Environment%20Signals%20on%20Social%20Media%0AAuthor%3A%20Soorya%20Ram%20Shimgekar%20and%20Ruining%20Zhao%20and%20Agam%20Goyal%20and%20Violeta%20J.%20Rodriguez%20and%20Paul%20A.%20Bloom%20and%20Hari%20Sundaram%20and%20Koustuv%20Saha%0AAbstract%3A%20%20%20On%20social%20media%2C%20many%20individuals%20experiencing%20suicidal%20ideation%20%28SI%29%20do%20not%0Adisclose%20their%20distress%20explicitly.%20Instead%2C%20signs%20may%20surface%20indirectly%0Athrough%20everyday%20posts%20or%20peer%20interactions.%20Detecting%20such%20implicit%20signals%0Aearly%20is%20critical%20but%20remains%20challenging.%20We%20frame%20early%20and%20implicit%20SI%20as%20a%0Aforward-looking%20prediction%20task%20and%20develop%20a%20computational%20framework%20that%0Amodels%20a%20user%27s%20information%20environment%2C%20consisting%20of%20both%20their%20longitudinal%0Aposting%20histories%20as%20well%20as%20the%20discourse%20of%20their%20socially%20proximal%20peers.%20We%0Aadopted%20a%20composite%20network%20centrality%20measure%20to%20identify%20top%20neighbors%20of%20a%0Auser%2C%20and%20temporally%20aligned%20the%20user%27s%20and%20neighbors%27%20interactions%20--%0Aintegrating%20the%20multi-layered%20signals%20in%20a%20fine-tuned%20DeBERTa-v3%20model.%20In%20a%0AReddit%20study%20of%201%2C000%20%28500%20Case%20and%20500%20Control%29%20users%2C%20our%20approach%20improves%0Aearly%20and%20implicit%20SI%20detection%20by%2015%25%20over%20individual-only%20baselines.%20These%0Afindings%20highlight%20that%20peer%20interactions%20offer%20valuable%20predictive%20signals%20and%0Acarry%20broader%20implications%20for%20designing%20early%20detection%20systems%20that%20capture%0Aindirect%20as%20well%20as%20masked%20expressions%20of%20risk%20in%20online%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Early%2520and%2520Implicit%2520Suicidal%2520Ideation%2520via%2520Longitudinal%2520and%250A%2520%2520Information%2520Environment%2520Signals%2520on%2520Social%2520Media%26entry.906535625%3DSoorya%2520Ram%2520Shimgekar%2520and%2520Ruining%2520Zhao%2520and%2520Agam%2520Goyal%2520and%2520Violeta%2520J.%2520Rodriguez%2520and%2520Paul%2520A.%2520Bloom%2520and%2520Hari%2520Sundaram%2520and%2520Koustuv%2520Saha%26entry.1292438233%3D%2520%2520On%2520social%2520media%252C%2520many%2520individuals%2520experiencing%2520suicidal%2520ideation%2520%2528SI%2529%2520do%2520not%250Adisclose%2520their%2520distress%2520explicitly.%2520Instead%252C%2520signs%2520may%2520surface%2520indirectly%250Athrough%2520everyday%2520posts%2520or%2520peer%2520interactions.%2520Detecting%2520such%2520implicit%2520signals%250Aearly%2520is%2520critical%2520but%2520remains%2520challenging.%2520We%2520frame%2520early%2520and%2520implicit%2520SI%2520as%2520a%250Aforward-looking%2520prediction%2520task%2520and%2520develop%2520a%2520computational%2520framework%2520that%250Amodels%2520a%2520user%2527s%2520information%2520environment%252C%2520consisting%2520of%2520both%2520their%2520longitudinal%250Aposting%2520histories%2520as%2520well%2520as%2520the%2520discourse%2520of%2520their%2520socially%2520proximal%2520peers.%2520We%250Aadopted%2520a%2520composite%2520network%2520centrality%2520measure%2520to%2520identify%2520top%2520neighbors%2520of%2520a%250Auser%252C%2520and%2520temporally%2520aligned%2520the%2520user%2527s%2520and%2520neighbors%2527%2520interactions%2520--%250Aintegrating%2520the%2520multi-layered%2520signals%2520in%2520a%2520fine-tuned%2520DeBERTa-v3%2520model.%2520In%2520a%250AReddit%2520study%2520of%25201%252C000%2520%2528500%2520Case%2520and%2520500%2520Control%2529%2520users%252C%2520our%2520approach%2520improves%250Aearly%2520and%2520implicit%2520SI%2520detection%2520by%252015%2525%2520over%2520individual-only%2520baselines.%2520These%250Afindings%2520highlight%2520that%2520peer%2520interactions%2520offer%2520valuable%2520predictive%2520signals%2520and%250Acarry%2520broader%2520implications%2520for%2520designing%2520early%2520detection%2520systems%2520that%2520capture%250Aindirect%2520as%2520well%2520as%2520masked%2520expressions%2520of%2520risk%2520in%2520online%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Early%20and%20Implicit%20Suicidal%20Ideation%20via%20Longitudinal%20and%0A%20%20Information%20Environment%20Signals%20on%20Social%20Media&entry.906535625=Soorya%20Ram%20Shimgekar%20and%20Ruining%20Zhao%20and%20Agam%20Goyal%20and%20Violeta%20J.%20Rodriguez%20and%20Paul%20A.%20Bloom%20and%20Hari%20Sundaram%20and%20Koustuv%20Saha&entry.1292438233=%20%20On%20social%20media%2C%20many%20individuals%20experiencing%20suicidal%20ideation%20%28SI%29%20do%20not%0Adisclose%20their%20distress%20explicitly.%20Instead%2C%20signs%20may%20surface%20indirectly%0Athrough%20everyday%20posts%20or%20peer%20interactions.%20Detecting%20such%20implicit%20signals%0Aearly%20is%20critical%20but%20remains%20challenging.%20We%20frame%20early%20and%20implicit%20SI%20as%20a%0Aforward-looking%20prediction%20task%20and%20develop%20a%20computational%20framework%20that%0Amodels%20a%20user%27s%20information%20environment%2C%20consisting%20of%20both%20their%20longitudinal%0Aposting%20histories%20as%20well%20as%20the%20discourse%20of%20their%20socially%20proximal%20peers.%20We%0Aadopted%20a%20composite%20network%20centrality%20measure%20to%20identify%20top%20neighbors%20of%20a%0Auser%2C%20and%20temporally%20aligned%20the%20user%27s%20and%20neighbors%27%20interactions%20--%0Aintegrating%20the%20multi-layered%20signals%20in%20a%20fine-tuned%20DeBERTa-v3%20model.%20In%20a%0AReddit%20study%20of%201%2C000%20%28500%20Case%20and%20500%20Control%29%20users%2C%20our%20approach%20improves%0Aearly%20and%20implicit%20SI%20detection%20by%2015%25%20over%20individual-only%20baselines.%20These%0Afindings%20highlight%20that%20peer%20interactions%20offer%20valuable%20predictive%20signals%20and%0Acarry%20broader%20implications%20for%20designing%20early%20detection%20systems%20that%20capture%0Aindirect%20as%20well%20as%20masked%20expressions%20of%20risk%20in%20online%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14889v2&entry.124074799=Read"},
{"title": "Controlling Thinking Speed in Reasoning Models", "author": "Zhengkai Lin and Zhihang Fu and Ze Chen and Chao Chen and Liang Xie and Wenxiao Wang and Deng Cai and Zheng Wang and Jieping Ye", "abstract": "  Human cognition is theorized to operate in two modes: fast, intuitive System\n1 thinking and slow, deliberate System 2 thinking. While current Large\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\nfast thinking leads to high computational overhead and latency. In this work,\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\nadjust it for optimal performance. For the first question, we identify the\nsteering vector that governs slow-fast thinking transitions in LRMs'\nrepresentation space. Using this vector, we achieve the first representation\nediting-based test-time scaling effect, outperforming existing prompt-based\nscaling methods. For the second question, we apply real-time difficulty\nestimation to signal reasoning segments of varying complexity. Combining these\ntechniques, we propose the first reasoning strategy that enables fast\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\ntraining or additional cost, our plug-in module delivers an average +1.3%\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\nbenchmarks. All of our algorithms are implemented based on vLLM and are\nexpected to support broader applications and inspire future research.\n", "link": "http://arxiv.org/abs/2507.03704v2", "date": "2025-10-30", "relevancy": 1.3761, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Thinking%20Speed%20in%20Reasoning%20Models&body=Title%3A%20Controlling%20Thinking%20Speed%20in%20Reasoning%20Models%0AAuthor%3A%20Zhengkai%20Lin%20and%20Zhihang%20Fu%20and%20Ze%20Chen%20and%20Chao%20Chen%20and%20Liang%20Xie%20and%20Wenxiao%20Wang%20and%20Deng%20Cai%20and%20Zheng%20Wang%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20Human%20cognition%20is%20theorized%20to%20operate%20in%20two%20modes%3A%20fast%2C%20intuitive%20System%0A1%20thinking%20and%20slow%2C%20deliberate%20System%202%20thinking.%20While%20current%20Large%0AReasoning%20Models%20%28LRMs%29%20excel%20at%20System%202%20thinking%2C%20their%20inability%20to%20perform%0Afast%20thinking%20leads%20to%20high%20computational%20overhead%20and%20latency.%20In%20this%20work%2C%0Awe%20enable%20LRMs%20to%20approximate%20human%20intelligence%20through%20dynamic%20thinking%20speed%0Aadjustment%2C%20optimizing%20accuracy-efficiency%20trade-offs.%20Our%20approach%20addresses%0Atwo%20key%20questions%3A%20%281%29%20how%20to%20control%20thinking%20speed%20in%20LRMs%2C%20and%20%282%29%20when%20to%0Aadjust%20it%20for%20optimal%20performance.%20For%20the%20first%20question%2C%20we%20identify%20the%0Asteering%20vector%20that%20governs%20slow-fast%20thinking%20transitions%20in%20LRMs%27%0Arepresentation%20space.%20Using%20this%20vector%2C%20we%20achieve%20the%20first%20representation%0Aediting-based%20test-time%20scaling%20effect%2C%20outperforming%20existing%20prompt-based%0Ascaling%20methods.%20For%20the%20second%20question%2C%20we%20apply%20real-time%20difficulty%0Aestimation%20to%20signal%20reasoning%20segments%20of%20varying%20complexity.%20Combining%20these%0Atechniques%2C%20we%20propose%20the%20first%20reasoning%20strategy%20that%20enables%20fast%0Aprocessing%20of%20easy%20steps%20and%20deeper%20analysis%20for%20complex%20reasoning.%20Without%20any%0Atraining%20or%20additional%20cost%2C%20our%20plug-in%20module%20delivers%20an%20average%20%2B1.3%25%0Aaccuracy%20with%20-8.6%25%20token%20usage%20across%20leading%20LRMs%20and%20advanced%20reasoning%0Abenchmarks.%20All%20of%20our%20algorithms%20are%20implemented%20based%20on%20vLLM%20and%20are%0Aexpected%20to%20support%20broader%20applications%20and%20inspire%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Thinking%2520Speed%2520in%2520Reasoning%2520Models%26entry.906535625%3DZhengkai%2520Lin%2520and%2520Zhihang%2520Fu%2520and%2520Ze%2520Chen%2520and%2520Chao%2520Chen%2520and%2520Liang%2520Xie%2520and%2520Wenxiao%2520Wang%2520and%2520Deng%2520Cai%2520and%2520Zheng%2520Wang%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520Human%2520cognition%2520is%2520theorized%2520to%2520operate%2520in%2520two%2520modes%253A%2520fast%252C%2520intuitive%2520System%250A1%2520thinking%2520and%2520slow%252C%2520deliberate%2520System%25202%2520thinking.%2520While%2520current%2520Large%250AReasoning%2520Models%2520%2528LRMs%2529%2520excel%2520at%2520System%25202%2520thinking%252C%2520their%2520inability%2520to%2520perform%250Afast%2520thinking%2520leads%2520to%2520high%2520computational%2520overhead%2520and%2520latency.%2520In%2520this%2520work%252C%250Awe%2520enable%2520LRMs%2520to%2520approximate%2520human%2520intelligence%2520through%2520dynamic%2520thinking%2520speed%250Aadjustment%252C%2520optimizing%2520accuracy-efficiency%2520trade-offs.%2520Our%2520approach%2520addresses%250Atwo%2520key%2520questions%253A%2520%25281%2529%2520how%2520to%2520control%2520thinking%2520speed%2520in%2520LRMs%252C%2520and%2520%25282%2529%2520when%2520to%250Aadjust%2520it%2520for%2520optimal%2520performance.%2520For%2520the%2520first%2520question%252C%2520we%2520identify%2520the%250Asteering%2520vector%2520that%2520governs%2520slow-fast%2520thinking%2520transitions%2520in%2520LRMs%2527%250Arepresentation%2520space.%2520Using%2520this%2520vector%252C%2520we%2520achieve%2520the%2520first%2520representation%250Aediting-based%2520test-time%2520scaling%2520effect%252C%2520outperforming%2520existing%2520prompt-based%250Ascaling%2520methods.%2520For%2520the%2520second%2520question%252C%2520we%2520apply%2520real-time%2520difficulty%250Aestimation%2520to%2520signal%2520reasoning%2520segments%2520of%2520varying%2520complexity.%2520Combining%2520these%250Atechniques%252C%2520we%2520propose%2520the%2520first%2520reasoning%2520strategy%2520that%2520enables%2520fast%250Aprocessing%2520of%2520easy%2520steps%2520and%2520deeper%2520analysis%2520for%2520complex%2520reasoning.%2520Without%2520any%250Atraining%2520or%2520additional%2520cost%252C%2520our%2520plug-in%2520module%2520delivers%2520an%2520average%2520%252B1.3%2525%250Aaccuracy%2520with%2520-8.6%2525%2520token%2520usage%2520across%2520leading%2520LRMs%2520and%2520advanced%2520reasoning%250Abenchmarks.%2520All%2520of%2520our%2520algorithms%2520are%2520implemented%2520based%2520on%2520vLLM%2520and%2520are%250Aexpected%2520to%2520support%2520broader%2520applications%2520and%2520inspire%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Thinking%20Speed%20in%20Reasoning%20Models&entry.906535625=Zhengkai%20Lin%20and%20Zhihang%20Fu%20and%20Ze%20Chen%20and%20Chao%20Chen%20and%20Liang%20Xie%20and%20Wenxiao%20Wang%20and%20Deng%20Cai%20and%20Zheng%20Wang%20and%20Jieping%20Ye&entry.1292438233=%20%20Human%20cognition%20is%20theorized%20to%20operate%20in%20two%20modes%3A%20fast%2C%20intuitive%20System%0A1%20thinking%20and%20slow%2C%20deliberate%20System%202%20thinking.%20While%20current%20Large%0AReasoning%20Models%20%28LRMs%29%20excel%20at%20System%202%20thinking%2C%20their%20inability%20to%20perform%0Afast%20thinking%20leads%20to%20high%20computational%20overhead%20and%20latency.%20In%20this%20work%2C%0Awe%20enable%20LRMs%20to%20approximate%20human%20intelligence%20through%20dynamic%20thinking%20speed%0Aadjustment%2C%20optimizing%20accuracy-efficiency%20trade-offs.%20Our%20approach%20addresses%0Atwo%20key%20questions%3A%20%281%29%20how%20to%20control%20thinking%20speed%20in%20LRMs%2C%20and%20%282%29%20when%20to%0Aadjust%20it%20for%20optimal%20performance.%20For%20the%20first%20question%2C%20we%20identify%20the%0Asteering%20vector%20that%20governs%20slow-fast%20thinking%20transitions%20in%20LRMs%27%0Arepresentation%20space.%20Using%20this%20vector%2C%20we%20achieve%20the%20first%20representation%0Aediting-based%20test-time%20scaling%20effect%2C%20outperforming%20existing%20prompt-based%0Ascaling%20methods.%20For%20the%20second%20question%2C%20we%20apply%20real-time%20difficulty%0Aestimation%20to%20signal%20reasoning%20segments%20of%20varying%20complexity.%20Combining%20these%0Atechniques%2C%20we%20propose%20the%20first%20reasoning%20strategy%20that%20enables%20fast%0Aprocessing%20of%20easy%20steps%20and%20deeper%20analysis%20for%20complex%20reasoning.%20Without%20any%0Atraining%20or%20additional%20cost%2C%20our%20plug-in%20module%20delivers%20an%20average%20%2B1.3%25%0Aaccuracy%20with%20-8.6%25%20token%20usage%20across%20leading%20LRMs%20and%20advanced%20reasoning%0Abenchmarks.%20All%20of%20our%20algorithms%20are%20implemented%20based%20on%20vLLM%20and%20are%0Aexpected%20to%20support%20broader%20applications%20and%20inspire%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03704v2&entry.124074799=Read"},
{"title": "Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study", "author": "Gerard T. Portela and Jason B. Gibbons and Sebastian Schneeweiss and Rishi J. Desai", "abstract": "  Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference.\n", "link": "http://arxiv.org/abs/2510.26700v1", "date": "2025-10-30", "relevancy": 1.2314, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4179}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessment%20of%20the%20conditional%20exchangeability%20assumption%20in%20causal%0A%20%20machine%20learning%20models%3A%20a%20simulation%20study&body=Title%3A%20Assessment%20of%20the%20conditional%20exchangeability%20assumption%20in%20causal%0A%20%20machine%20learning%20models%3A%20a%20simulation%20study%0AAuthor%3A%20Gerard%20T.%20Portela%20and%20Jason%20B.%20Gibbons%20and%20Sebastian%20Schneeweiss%20and%20Rishi%20J.%20Desai%0AAbstract%3A%20%20%20Observational%20studies%20developing%20causal%20machine%20learning%20%28ML%29%20models%20for%20the%0Aprediction%20of%20individualized%20treatment%20effects%20%28ITEs%29%20seldom%20conduct%20empirical%0Aevaluations%20to%20assess%20the%20conditional%20exchangeability%20assumption.%20We%20aimed%20to%0Aevaluate%20the%20performance%20of%20these%20models%20under%20conditional%20exchangeability%0Aviolations%20and%20the%20utility%20of%20negative%20control%20outcomes%20%28NCOs%29%20as%20a%20diagnostic.%0AWe%20conducted%20a%20simulation%20study%20to%20examine%20confounding%20bias%20in%20ITE%20estimates%0Agenerated%20by%20causal%20forest%20and%20X-learner%20models%20under%20varying%20conditions%2C%0Aincluding%20the%20presence%20or%20absence%20of%20true%20heterogeneity.%20We%20simulated%20data%20to%0Areflect%20real-world%20scenarios%20with%20differing%20levels%20of%20confounding%2C%20sample%20size%2C%0Aand%20NCO%20confounding%20structures.%20We%20then%20estimated%20and%20compared%20subgroup-level%0Atreatment%20effects%20on%20the%20primary%20outcome%20and%20NCOs%20across%20settings%20with%20and%0Awithout%20unmeasured%20confounding.%20When%20conditional%20exchangeability%20was%20violated%2C%0Acausal%20forest%20and%20X-learner%20models%20failed%20to%20recover%20true%20treatment%20effect%0Aheterogeneity%20and%2C%20in%20some%20cases%2C%20falsely%20indicated%20heterogeneity%20when%20there%0Awas%20none.%20NCOs%20successfully%20identified%20subgroups%20affected%20by%20unmeasured%0Aconfounding.%20Even%20when%20NCOs%20did%20not%20perfectly%20satisfy%20its%20ideal%20assumptions%2C%20it%0Aremained%20informative%2C%20flagging%20potential%20bias%20in%20subgroup%20level%20estimates%2C%0Athough%20not%20always%20pinpointing%20the%20subgroup%20with%20the%20largest%20confounding.%0AViolations%20of%20conditional%20exchangeability%20substantially%20limit%20the%20validity%20of%0AITE%20estimates%20from%20causal%20ML%20models%20in%20routinely%20collected%20observational%20data.%0ANCOs%20serve%20a%20useful%20empirical%20diagnostic%20tool%20for%20detecting%20subgroup-specific%0Aunmeasured%20confounding%20and%20should%20be%20incorporated%20into%20causal%20ML%20workflows%20to%0Asupport%20the%20credibility%20of%20individualized%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessment%2520of%2520the%2520conditional%2520exchangeability%2520assumption%2520in%2520causal%250A%2520%2520machine%2520learning%2520models%253A%2520a%2520simulation%2520study%26entry.906535625%3DGerard%2520T.%2520Portela%2520and%2520Jason%2520B.%2520Gibbons%2520and%2520Sebastian%2520Schneeweiss%2520and%2520Rishi%2520J.%2520Desai%26entry.1292438233%3D%2520%2520Observational%2520studies%2520developing%2520causal%2520machine%2520learning%2520%2528ML%2529%2520models%2520for%2520the%250Aprediction%2520of%2520individualized%2520treatment%2520effects%2520%2528ITEs%2529%2520seldom%2520conduct%2520empirical%250Aevaluations%2520to%2520assess%2520the%2520conditional%2520exchangeability%2520assumption.%2520We%2520aimed%2520to%250Aevaluate%2520the%2520performance%2520of%2520these%2520models%2520under%2520conditional%2520exchangeability%250Aviolations%2520and%2520the%2520utility%2520of%2520negative%2520control%2520outcomes%2520%2528NCOs%2529%2520as%2520a%2520diagnostic.%250AWe%2520conducted%2520a%2520simulation%2520study%2520to%2520examine%2520confounding%2520bias%2520in%2520ITE%2520estimates%250Agenerated%2520by%2520causal%2520forest%2520and%2520X-learner%2520models%2520under%2520varying%2520conditions%252C%250Aincluding%2520the%2520presence%2520or%2520absence%2520of%2520true%2520heterogeneity.%2520We%2520simulated%2520data%2520to%250Areflect%2520real-world%2520scenarios%2520with%2520differing%2520levels%2520of%2520confounding%252C%2520sample%2520size%252C%250Aand%2520NCO%2520confounding%2520structures.%2520We%2520then%2520estimated%2520and%2520compared%2520subgroup-level%250Atreatment%2520effects%2520on%2520the%2520primary%2520outcome%2520and%2520NCOs%2520across%2520settings%2520with%2520and%250Awithout%2520unmeasured%2520confounding.%2520When%2520conditional%2520exchangeability%2520was%2520violated%252C%250Acausal%2520forest%2520and%2520X-learner%2520models%2520failed%2520to%2520recover%2520true%2520treatment%2520effect%250Aheterogeneity%2520and%252C%2520in%2520some%2520cases%252C%2520falsely%2520indicated%2520heterogeneity%2520when%2520there%250Awas%2520none.%2520NCOs%2520successfully%2520identified%2520subgroups%2520affected%2520by%2520unmeasured%250Aconfounding.%2520Even%2520when%2520NCOs%2520did%2520not%2520perfectly%2520satisfy%2520its%2520ideal%2520assumptions%252C%2520it%250Aremained%2520informative%252C%2520flagging%2520potential%2520bias%2520in%2520subgroup%2520level%2520estimates%252C%250Athough%2520not%2520always%2520pinpointing%2520the%2520subgroup%2520with%2520the%2520largest%2520confounding.%250AViolations%2520of%2520conditional%2520exchangeability%2520substantially%2520limit%2520the%2520validity%2520of%250AITE%2520estimates%2520from%2520causal%2520ML%2520models%2520in%2520routinely%2520collected%2520observational%2520data.%250ANCOs%2520serve%2520a%2520useful%2520empirical%2520diagnostic%2520tool%2520for%2520detecting%2520subgroup-specific%250Aunmeasured%2520confounding%2520and%2520should%2520be%2520incorporated%2520into%2520causal%2520ML%2520workflows%2520to%250Asupport%2520the%2520credibility%2520of%2520individualized%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessment%20of%20the%20conditional%20exchangeability%20assumption%20in%20causal%0A%20%20machine%20learning%20models%3A%20a%20simulation%20study&entry.906535625=Gerard%20T.%20Portela%20and%20Jason%20B.%20Gibbons%20and%20Sebastian%20Schneeweiss%20and%20Rishi%20J.%20Desai&entry.1292438233=%20%20Observational%20studies%20developing%20causal%20machine%20learning%20%28ML%29%20models%20for%20the%0Aprediction%20of%20individualized%20treatment%20effects%20%28ITEs%29%20seldom%20conduct%20empirical%0Aevaluations%20to%20assess%20the%20conditional%20exchangeability%20assumption.%20We%20aimed%20to%0Aevaluate%20the%20performance%20of%20these%20models%20under%20conditional%20exchangeability%0Aviolations%20and%20the%20utility%20of%20negative%20control%20outcomes%20%28NCOs%29%20as%20a%20diagnostic.%0AWe%20conducted%20a%20simulation%20study%20to%20examine%20confounding%20bias%20in%20ITE%20estimates%0Agenerated%20by%20causal%20forest%20and%20X-learner%20models%20under%20varying%20conditions%2C%0Aincluding%20the%20presence%20or%20absence%20of%20true%20heterogeneity.%20We%20simulated%20data%20to%0Areflect%20real-world%20scenarios%20with%20differing%20levels%20of%20confounding%2C%20sample%20size%2C%0Aand%20NCO%20confounding%20structures.%20We%20then%20estimated%20and%20compared%20subgroup-level%0Atreatment%20effects%20on%20the%20primary%20outcome%20and%20NCOs%20across%20settings%20with%20and%0Awithout%20unmeasured%20confounding.%20When%20conditional%20exchangeability%20was%20violated%2C%0Acausal%20forest%20and%20X-learner%20models%20failed%20to%20recover%20true%20treatment%20effect%0Aheterogeneity%20and%2C%20in%20some%20cases%2C%20falsely%20indicated%20heterogeneity%20when%20there%0Awas%20none.%20NCOs%20successfully%20identified%20subgroups%20affected%20by%20unmeasured%0Aconfounding.%20Even%20when%20NCOs%20did%20not%20perfectly%20satisfy%20its%20ideal%20assumptions%2C%20it%0Aremained%20informative%2C%20flagging%20potential%20bias%20in%20subgroup%20level%20estimates%2C%0Athough%20not%20always%20pinpointing%20the%20subgroup%20with%20the%20largest%20confounding.%0AViolations%20of%20conditional%20exchangeability%20substantially%20limit%20the%20validity%20of%0AITE%20estimates%20from%20causal%20ML%20models%20in%20routinely%20collected%20observational%20data.%0ANCOs%20serve%20a%20useful%20empirical%20diagnostic%20tool%20for%20detecting%20subgroup-specific%0Aunmeasured%20confounding%20and%20should%20be%20incorporated%20into%20causal%20ML%20workflows%20to%0Asupport%20the%20credibility%20of%20individualized%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26700v1&entry.124074799=Read"},
{"title": "On the limitation of evaluating machine unlearning using only a single\n  training seed", "author": "Jamie Lanyon and Axel Finke and Petros Andreou and Georgina Cosma", "abstract": "  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.\n", "link": "http://arxiv.org/abs/2510.26714v1", "date": "2025-10-30", "relevancy": 1.3179, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.428}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed&body=Title%3A%20On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed%0AAuthor%3A%20Jamie%20Lanyon%20and%20Axel%20Finke%20and%20Petros%20Andreou%20and%20Georgina%20Cosma%0AAbstract%3A%20%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20certain%20data%20points%0Afrom%20a%20trained%20model%20without%20costly%20retraining.%20Most%20practical%20MU%20algorithms%0Aare%20only%20approximate%20and%20their%20performance%20can%20only%20be%20assessed%20empirically.%0ACare%20must%20therefore%20be%20taken%20to%20make%20empirical%20comparisons%20as%20representative%20as%0Apossible.%20A%20common%20practice%20is%20to%20run%20the%20MU%20algorithm%20multiple%20times%0Aindependently%20starting%20from%20the%20same%20trained%20model.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20this%20practice%20can%20give%20highly%20non-representative%20results%0Abecause%20--%20even%20for%20the%20same%20architecture%20and%20same%20dataset%20--%20some%20MU%20methods%0Acan%20be%20highly%20sensitive%20to%20the%20choice%20of%20random%20number%20seed%20used%20for%20model%0Atraining.%20We%20therefore%20recommend%20that%20empirical%0Acomphttps%3A//info.arxiv.org/help/prep%23commentsarisons%20of%20MU%20algorithms%20should%0Aalso%20reflect%20the%20variability%20across%20different%20model%20training%20seeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520limitation%2520of%2520evaluating%2520machine%2520unlearning%2520using%2520only%2520a%2520single%250A%2520%2520training%2520seed%26entry.906535625%3DJamie%2520Lanyon%2520and%2520Axel%2520Finke%2520and%2520Petros%2520Andreou%2520and%2520Georgina%2520Cosma%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520%2528MU%2529%2520aims%2520to%2520remove%2520the%2520influence%2520of%2520certain%2520data%2520points%250Afrom%2520a%2520trained%2520model%2520without%2520costly%2520retraining.%2520Most%2520practical%2520MU%2520algorithms%250Aare%2520only%2520approximate%2520and%2520their%2520performance%2520can%2520only%2520be%2520assessed%2520empirically.%250ACare%2520must%2520therefore%2520be%2520taken%2520to%2520make%2520empirical%2520comparisons%2520as%2520representative%2520as%250Apossible.%2520A%2520common%2520practice%2520is%2520to%2520run%2520the%2520MU%2520algorithm%2520multiple%2520times%250Aindependently%2520starting%2520from%2520the%2520same%2520trained%2520model.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520this%2520practice%2520can%2520give%2520highly%2520non-representative%2520results%250Abecause%2520--%2520even%2520for%2520the%2520same%2520architecture%2520and%2520same%2520dataset%2520--%2520some%2520MU%2520methods%250Acan%2520be%2520highly%2520sensitive%2520to%2520the%2520choice%2520of%2520random%2520number%2520seed%2520used%2520for%2520model%250Atraining.%2520We%2520therefore%2520recommend%2520that%2520empirical%250Acomphttps%253A//info.arxiv.org/help/prep%2523commentsarisons%2520of%2520MU%2520algorithms%2520should%250Aalso%2520reflect%2520the%2520variability%2520across%2520different%2520model%2520training%2520seeds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed&entry.906535625=Jamie%20Lanyon%20and%20Axel%20Finke%20and%20Petros%20Andreou%20and%20Georgina%20Cosma&entry.1292438233=%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20certain%20data%20points%0Afrom%20a%20trained%20model%20without%20costly%20retraining.%20Most%20practical%20MU%20algorithms%0Aare%20only%20approximate%20and%20their%20performance%20can%20only%20be%20assessed%20empirically.%0ACare%20must%20therefore%20be%20taken%20to%20make%20empirical%20comparisons%20as%20representative%20as%0Apossible.%20A%20common%20practice%20is%20to%20run%20the%20MU%20algorithm%20multiple%20times%0Aindependently%20starting%20from%20the%20same%20trained%20model.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20this%20practice%20can%20give%20highly%20non-representative%20results%0Abecause%20--%20even%20for%20the%20same%20architecture%20and%20same%20dataset%20--%20some%20MU%20methods%0Acan%20be%20highly%20sensitive%20to%20the%20choice%20of%20random%20number%20seed%20used%20for%20model%0Atraining.%20We%20therefore%20recommend%20that%20empirical%0Acomphttps%3A//info.arxiv.org/help/prep%23commentsarisons%20of%20MU%20algorithms%20should%0Aalso%20reflect%20the%20variability%20across%20different%20model%20training%20seeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26714v1&entry.124074799=Read"},
{"title": "Bridging the Gap between Empirical Welfare Maximization and Conditional\n  Average Treatment Effect Estimation in Policy Learning", "author": "Masahiro Kato", "abstract": "  The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a novel regularization\nmethod for policy learning. Our findings yield a convex and computationally\nefficient training procedure that avoids the NP-hard combinatorial step\ntypically required in EWM.\n", "link": "http://arxiv.org/abs/2510.26723v1", "date": "2025-10-30", "relevancy": 1.2844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4215}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning&body=Title%3A%20Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20The%20goal%20of%20policy%20learning%20is%20to%20train%20a%20policy%20function%20that%20recommends%20a%0Atreatment%20given%20covariates%20to%20maximize%20population%20welfare.%20There%20are%20two%20major%0Aapproaches%20in%20policy%20learning%3A%20the%20empirical%20welfare%20maximization%20%28EWM%29%0Aapproach%20and%20the%20plug-in%20approach.%20The%20EWM%20approach%20is%20analogous%20to%20a%0Aclassification%20problem%2C%20where%20one%20first%20builds%20an%20estimator%20of%20the%20population%0Awelfare%2C%20which%20is%20a%20functional%20of%20policy%20functions%2C%20and%20then%20trains%20a%20policy%20by%0Amaximizing%20the%20estimated%20welfare.%20In%20contrast%2C%20the%20plug-in%20approach%20is%20based%20on%0Aregression%2C%20where%20one%20first%20estimates%20the%20conditional%20average%20treatment%20effect%0A%28CATE%29%20and%20then%20recommends%20the%20treatment%20with%20the%20highest%20estimated%20outcome.%0AThis%20study%20bridges%20the%20gap%20between%20the%20two%20approaches%20by%20showing%20that%20both%20are%0Abased%20on%20essentially%20the%20same%20optimization%20problem.%20In%20particular%2C%20we%20prove%20an%0Aexact%20equivalence%20between%20EWM%20and%20least%20squares%20over%20a%20reparameterization%20of%0Athe%20policy%20class.%20As%20a%20consequence%2C%20the%20two%20approaches%20are%20interchangeable%20in%0Aseveral%20respects%20and%20share%20the%20same%20theoretical%20guarantees%20under%20common%0Aconditions.%20Leveraging%20this%20equivalence%2C%20we%20propose%20a%20novel%20regularization%0Amethod%20for%20policy%20learning.%20Our%20findings%20yield%20a%20convex%20and%20computationally%0Aefficient%20training%20procedure%20that%20avoids%20the%20NP-hard%20combinatorial%20step%0Atypically%20required%20in%20EWM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Gap%2520between%2520Empirical%2520Welfare%2520Maximization%2520and%2520Conditional%250A%2520%2520Average%2520Treatment%2520Effect%2520Estimation%2520in%2520Policy%2520Learning%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520policy%2520learning%2520is%2520to%2520train%2520a%2520policy%2520function%2520that%2520recommends%2520a%250Atreatment%2520given%2520covariates%2520to%2520maximize%2520population%2520welfare.%2520There%2520are%2520two%2520major%250Aapproaches%2520in%2520policy%2520learning%253A%2520the%2520empirical%2520welfare%2520maximization%2520%2528EWM%2529%250Aapproach%2520and%2520the%2520plug-in%2520approach.%2520The%2520EWM%2520approach%2520is%2520analogous%2520to%2520a%250Aclassification%2520problem%252C%2520where%2520one%2520first%2520builds%2520an%2520estimator%2520of%2520the%2520population%250Awelfare%252C%2520which%2520is%2520a%2520functional%2520of%2520policy%2520functions%252C%2520and%2520then%2520trains%2520a%2520policy%2520by%250Amaximizing%2520the%2520estimated%2520welfare.%2520In%2520contrast%252C%2520the%2520plug-in%2520approach%2520is%2520based%2520on%250Aregression%252C%2520where%2520one%2520first%2520estimates%2520the%2520conditional%2520average%2520treatment%2520effect%250A%2528CATE%2529%2520and%2520then%2520recommends%2520the%2520treatment%2520with%2520the%2520highest%2520estimated%2520outcome.%250AThis%2520study%2520bridges%2520the%2520gap%2520between%2520the%2520two%2520approaches%2520by%2520showing%2520that%2520both%2520are%250Abased%2520on%2520essentially%2520the%2520same%2520optimization%2520problem.%2520In%2520particular%252C%2520we%2520prove%2520an%250Aexact%2520equivalence%2520between%2520EWM%2520and%2520least%2520squares%2520over%2520a%2520reparameterization%2520of%250Athe%2520policy%2520class.%2520As%2520a%2520consequence%252C%2520the%2520two%2520approaches%2520are%2520interchangeable%2520in%250Aseveral%2520respects%2520and%2520share%2520the%2520same%2520theoretical%2520guarantees%2520under%2520common%250Aconditions.%2520Leveraging%2520this%2520equivalence%252C%2520we%2520propose%2520a%2520novel%2520regularization%250Amethod%2520for%2520policy%2520learning.%2520Our%2520findings%2520yield%2520a%2520convex%2520and%2520computationally%250Aefficient%2520training%2520procedure%2520that%2520avoids%2520the%2520NP-hard%2520combinatorial%2520step%250Atypically%2520required%2520in%2520EWM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20The%20goal%20of%20policy%20learning%20is%20to%20train%20a%20policy%20function%20that%20recommends%20a%0Atreatment%20given%20covariates%20to%20maximize%20population%20welfare.%20There%20are%20two%20major%0Aapproaches%20in%20policy%20learning%3A%20the%20empirical%20welfare%20maximization%20%28EWM%29%0Aapproach%20and%20the%20plug-in%20approach.%20The%20EWM%20approach%20is%20analogous%20to%20a%0Aclassification%20problem%2C%20where%20one%20first%20builds%20an%20estimator%20of%20the%20population%0Awelfare%2C%20which%20is%20a%20functional%20of%20policy%20functions%2C%20and%20then%20trains%20a%20policy%20by%0Amaximizing%20the%20estimated%20welfare.%20In%20contrast%2C%20the%20plug-in%20approach%20is%20based%20on%0Aregression%2C%20where%20one%20first%20estimates%20the%20conditional%20average%20treatment%20effect%0A%28CATE%29%20and%20then%20recommends%20the%20treatment%20with%20the%20highest%20estimated%20outcome.%0AThis%20study%20bridges%20the%20gap%20between%20the%20two%20approaches%20by%20showing%20that%20both%20are%0Abased%20on%20essentially%20the%20same%20optimization%20problem.%20In%20particular%2C%20we%20prove%20an%0Aexact%20equivalence%20between%20EWM%20and%20least%20squares%20over%20a%20reparameterization%20of%0Athe%20policy%20class.%20As%20a%20consequence%2C%20the%20two%20approaches%20are%20interchangeable%20in%0Aseveral%20respects%20and%20share%20the%20same%20theoretical%20guarantees%20under%20common%0Aconditions.%20Leveraging%20this%20equivalence%2C%20we%20propose%20a%20novel%20regularization%0Amethod%20for%20policy%20learning.%20Our%20findings%20yield%20a%20convex%20and%20computationally%0Aefficient%20training%20procedure%20that%20avoids%20the%20NP-hard%20combinatorial%20step%0Atypically%20required%20in%20EWM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26723v1&entry.124074799=Read"},
{"title": "Locality in Image Diffusion Models Emerges from Data Statistics", "author": "Artem Lukoianov and Chenyang Yuan and Justin Solomon and Vincent Sitzmann", "abstract": "  Recent work has shown that the generalization ability of image diffusion\nmodels arises from the locality properties of the trained neural network. In\nparticular, when denoising a particular pixel, the model relies on a limited\nneighborhood of the input image around that pixel, which, according to the\nprevious work, is tightly related to the ability of these models to produce\nnovel images. Since locality is central to generalization, it is crucial to\nunderstand why diffusion models learn local behavior in the first place, as\nwell as the factors that govern the properties of locality patterns. In this\nwork, we present evidence that the locality in deep diffusion models emerges as\na statistical property of the image dataset and is not due to the inductive\nbias of convolutional neural networks, as suggested in previous work.\nSpecifically, we demonstrate that an optimal parametric linear denoiser\nexhibits similar locality properties to deep neural denoisers. We show, both\ntheoretically and experimentally, that this locality arises directly from pixel\ncorrelations present in the image datasets. Moreover, locality patterns are\ndrastically different on specialized datasets, approximating principal\ncomponents of the data's covariance. We use these insights to craft an\nanalytical denoiser that better matches scores predicted by a deep diffusion\nmodel than prior expert-crafted alternatives. Our key takeaway is that while\nneural network architectures influence generation quality, their primary role\nis to capture locality patterns inherent in the data.\n", "link": "http://arxiv.org/abs/2509.09672v2", "date": "2025-10-30", "relevancy": 1.1768, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6123}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5874}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality%20in%20Image%20Diffusion%20Models%20Emerges%20from%20Data%20Statistics&body=Title%3A%20Locality%20in%20Image%20Diffusion%20Models%20Emerges%20from%20Data%20Statistics%0AAuthor%3A%20Artem%20Lukoianov%20and%20Chenyang%20Yuan%20and%20Justin%20Solomon%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20the%20generalization%20ability%20of%20image%20diffusion%0Amodels%20arises%20from%20the%20locality%20properties%20of%20the%20trained%20neural%20network.%20In%0Aparticular%2C%20when%20denoising%20a%20particular%20pixel%2C%20the%20model%20relies%20on%20a%20limited%0Aneighborhood%20of%20the%20input%20image%20around%20that%20pixel%2C%20which%2C%20according%20to%20the%0Aprevious%20work%2C%20is%20tightly%20related%20to%20the%20ability%20of%20these%20models%20to%20produce%0Anovel%20images.%20Since%20locality%20is%20central%20to%20generalization%2C%20it%20is%20crucial%20to%0Aunderstand%20why%20diffusion%20models%20learn%20local%20behavior%20in%20the%20first%20place%2C%20as%0Awell%20as%20the%20factors%20that%20govern%20the%20properties%20of%20locality%20patterns.%20In%20this%0Awork%2C%20we%20present%20evidence%20that%20the%20locality%20in%20deep%20diffusion%20models%20emerges%20as%0Aa%20statistical%20property%20of%20the%20image%20dataset%20and%20is%20not%20due%20to%20the%20inductive%0Abias%20of%20convolutional%20neural%20networks%2C%20as%20suggested%20in%20previous%20work.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20optimal%20parametric%20linear%20denoiser%0Aexhibits%20similar%20locality%20properties%20to%20deep%20neural%20denoisers.%20We%20show%2C%20both%0Atheoretically%20and%20experimentally%2C%20that%20this%20locality%20arises%20directly%20from%20pixel%0Acorrelations%20present%20in%20the%20image%20datasets.%20Moreover%2C%20locality%20patterns%20are%0Adrastically%20different%20on%20specialized%20datasets%2C%20approximating%20principal%0Acomponents%20of%20the%20data%27s%20covariance.%20We%20use%20these%20insights%20to%20craft%20an%0Aanalytical%20denoiser%20that%20better%20matches%20scores%20predicted%20by%20a%20deep%20diffusion%0Amodel%20than%20prior%20expert-crafted%20alternatives.%20Our%20key%20takeaway%20is%20that%20while%0Aneural%20network%20architectures%20influence%20generation%20quality%2C%20their%20primary%20role%0Ais%20to%20capture%20locality%20patterns%20inherent%20in%20the%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality%2520in%2520Image%2520Diffusion%2520Models%2520Emerges%2520from%2520Data%2520Statistics%26entry.906535625%3DArtem%2520Lukoianov%2520and%2520Chenyang%2520Yuan%2520and%2520Justin%2520Solomon%2520and%2520Vincent%2520Sitzmann%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520the%2520generalization%2520ability%2520of%2520image%2520diffusion%250Amodels%2520arises%2520from%2520the%2520locality%2520properties%2520of%2520the%2520trained%2520neural%2520network.%2520In%250Aparticular%252C%2520when%2520denoising%2520a%2520particular%2520pixel%252C%2520the%2520model%2520relies%2520on%2520a%2520limited%250Aneighborhood%2520of%2520the%2520input%2520image%2520around%2520that%2520pixel%252C%2520which%252C%2520according%2520to%2520the%250Aprevious%2520work%252C%2520is%2520tightly%2520related%2520to%2520the%2520ability%2520of%2520these%2520models%2520to%2520produce%250Anovel%2520images.%2520Since%2520locality%2520is%2520central%2520to%2520generalization%252C%2520it%2520is%2520crucial%2520to%250Aunderstand%2520why%2520diffusion%2520models%2520learn%2520local%2520behavior%2520in%2520the%2520first%2520place%252C%2520as%250Awell%2520as%2520the%2520factors%2520that%2520govern%2520the%2520properties%2520of%2520locality%2520patterns.%2520In%2520this%250Awork%252C%2520we%2520present%2520evidence%2520that%2520the%2520locality%2520in%2520deep%2520diffusion%2520models%2520emerges%2520as%250Aa%2520statistical%2520property%2520of%2520the%2520image%2520dataset%2520and%2520is%2520not%2520due%2520to%2520the%2520inductive%250Abias%2520of%2520convolutional%2520neural%2520networks%252C%2520as%2520suggested%2520in%2520previous%2520work.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520an%2520optimal%2520parametric%2520linear%2520denoiser%250Aexhibits%2520similar%2520locality%2520properties%2520to%2520deep%2520neural%2520denoisers.%2520We%2520show%252C%2520both%250Atheoretically%2520and%2520experimentally%252C%2520that%2520this%2520locality%2520arises%2520directly%2520from%2520pixel%250Acorrelations%2520present%2520in%2520the%2520image%2520datasets.%2520Moreover%252C%2520locality%2520patterns%2520are%250Adrastically%2520different%2520on%2520specialized%2520datasets%252C%2520approximating%2520principal%250Acomponents%2520of%2520the%2520data%2527s%2520covariance.%2520We%2520use%2520these%2520insights%2520to%2520craft%2520an%250Aanalytical%2520denoiser%2520that%2520better%2520matches%2520scores%2520predicted%2520by%2520a%2520deep%2520diffusion%250Amodel%2520than%2520prior%2520expert-crafted%2520alternatives.%2520Our%2520key%2520takeaway%2520is%2520that%2520while%250Aneural%2520network%2520architectures%2520influence%2520generation%2520quality%252C%2520their%2520primary%2520role%250Ais%2520to%2520capture%2520locality%2520patterns%2520inherent%2520in%2520the%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality%20in%20Image%20Diffusion%20Models%20Emerges%20from%20Data%20Statistics&entry.906535625=Artem%20Lukoianov%20and%20Chenyang%20Yuan%20and%20Justin%20Solomon%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20the%20generalization%20ability%20of%20image%20diffusion%0Amodels%20arises%20from%20the%20locality%20properties%20of%20the%20trained%20neural%20network.%20In%0Aparticular%2C%20when%20denoising%20a%20particular%20pixel%2C%20the%20model%20relies%20on%20a%20limited%0Aneighborhood%20of%20the%20input%20image%20around%20that%20pixel%2C%20which%2C%20according%20to%20the%0Aprevious%20work%2C%20is%20tightly%20related%20to%20the%20ability%20of%20these%20models%20to%20produce%0Anovel%20images.%20Since%20locality%20is%20central%20to%20generalization%2C%20it%20is%20crucial%20to%0Aunderstand%20why%20diffusion%20models%20learn%20local%20behavior%20in%20the%20first%20place%2C%20as%0Awell%20as%20the%20factors%20that%20govern%20the%20properties%20of%20locality%20patterns.%20In%20this%0Awork%2C%20we%20present%20evidence%20that%20the%20locality%20in%20deep%20diffusion%20models%20emerges%20as%0Aa%20statistical%20property%20of%20the%20image%20dataset%20and%20is%20not%20due%20to%20the%20inductive%0Abias%20of%20convolutional%20neural%20networks%2C%20as%20suggested%20in%20previous%20work.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20optimal%20parametric%20linear%20denoiser%0Aexhibits%20similar%20locality%20properties%20to%20deep%20neural%20denoisers.%20We%20show%2C%20both%0Atheoretically%20and%20experimentally%2C%20that%20this%20locality%20arises%20directly%20from%20pixel%0Acorrelations%20present%20in%20the%20image%20datasets.%20Moreover%2C%20locality%20patterns%20are%0Adrastically%20different%20on%20specialized%20datasets%2C%20approximating%20principal%0Acomponents%20of%20the%20data%27s%20covariance.%20We%20use%20these%20insights%20to%20craft%20an%0Aanalytical%20denoiser%20that%20better%20matches%20scores%20predicted%20by%20a%20deep%20diffusion%0Amodel%20than%20prior%20expert-crafted%20alternatives.%20Our%20key%20takeaway%20is%20that%20while%0Aneural%20network%20architectures%20influence%20generation%20quality%2C%20their%20primary%20role%0Ais%20to%20capture%20locality%20patterns%20inherent%20in%20the%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09672v2&entry.124074799=Read"},
{"title": "Remote Labor Index: Measuring AI Automation of Remote Work", "author": "Mantas Mazeika and Alice Gatti and Cristina Menghini and Udari Madhushani Sehwag and Shivam Singhal and Yury Orlovskiy and Steven Basart and Manasi Sharma and Denis Peskoff and Elaine Lau and Jaehyuk Lim and Lachlan Carroll and Alice Blair and Vinaya Sivakumar and Sumana Basu and Brad Kenstler and Yuntao Ma and Julian Michael and Xiaoke Li and Oliver Ingebretsen and Aditya Mehta and Jean Mottola and John Teichmann and Kevin Yu and Zaina Shaik and Adam Khoja and Richard Ren and Jason Hausenloy and Long Phan and Ye Htet and Ankit Aich and Tahseen Rabbani and Vivswan Shah and Andriy Novykov and Felix Binder and Kirill Chugunov and Luis Ramirez and Matias Geralnik and Hern\u00e1n Mesura and Dean Lee and Ed-Yeremai Hernandez Cardona and Annette Diamond and Summer Yue and Alexandr Wang and Bing Liu and Ernesto Hernandez and Dan Hendrycks", "abstract": "  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n", "link": "http://arxiv.org/abs/2510.26787v1", "date": "2025-10-30", "relevancy": 0.8712, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4382}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remote%20Labor%20Index%3A%20Measuring%20AI%20Automation%20of%20Remote%20Work&body=Title%3A%20Remote%20Labor%20Index%3A%20Measuring%20AI%20Automation%20of%20Remote%20Work%0AAuthor%3A%20Mantas%20Mazeika%20and%20Alice%20Gatti%20and%20Cristina%20Menghini%20and%20Udari%20Madhushani%20Sehwag%20and%20Shivam%20Singhal%20and%20Yury%20Orlovskiy%20and%20Steven%20Basart%20and%20Manasi%20Sharma%20and%20Denis%20Peskoff%20and%20Elaine%20Lau%20and%20Jaehyuk%20Lim%20and%20Lachlan%20Carroll%20and%20Alice%20Blair%20and%20Vinaya%20Sivakumar%20and%20Sumana%20Basu%20and%20Brad%20Kenstler%20and%20Yuntao%20Ma%20and%20Julian%20Michael%20and%20Xiaoke%20Li%20and%20Oliver%20Ingebretsen%20and%20Aditya%20Mehta%20and%20Jean%20Mottola%20and%20John%20Teichmann%20and%20Kevin%20Yu%20and%20Zaina%20Shaik%20and%20Adam%20Khoja%20and%20Richard%20Ren%20and%20Jason%20Hausenloy%20and%20Long%20Phan%20and%20Ye%20Htet%20and%20Ankit%20Aich%20and%20Tahseen%20Rabbani%20and%20Vivswan%20Shah%20and%20Andriy%20Novykov%20and%20Felix%20Binder%20and%20Kirill%20Chugunov%20and%20Luis%20Ramirez%20and%20Matias%20Geralnik%20and%20Hern%C3%A1n%20Mesura%20and%20Dean%20Lee%20and%20Ed-Yeremai%20Hernandez%20Cardona%20and%20Annette%20Diamond%20and%20Summer%20Yue%20and%20Alexandr%20Wang%20and%20Bing%20Liu%20and%20Ernesto%20Hernandez%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20AIs%20have%20made%20rapid%20progress%20on%20research-oriented%20benchmarks%20of%20knowledge%20and%0Areasoning%2C%20but%20it%20remains%20unclear%20how%20these%20gains%20translate%20into%20economic%20value%0Aand%20automation.%20To%20measure%20this%2C%20we%20introduce%20the%20Remote%20Labor%20Index%20%28RLI%29%2C%20a%0Abroadly%20multi-sector%20benchmark%20comprising%20real-world%2C%20economically%20valuable%0Aprojects%20designed%20to%20evaluate%20end-to-end%20agent%20performance%20in%20practical%0Asettings.%20AI%20agents%20perform%20near%20the%20floor%20on%20RLI%2C%20with%20the%20highest-performing%0Aagent%20achieving%20an%20automation%20rate%20of%202.5%25.%20These%20results%20help%20ground%0Adiscussions%20of%20AI%20automation%20in%20empirical%20evidence%2C%20setting%20a%20common%20basis%20for%0Atracking%20AI%20impacts%20and%20enabling%20stakeholders%20to%20proactively%20navigate%20AI-driven%0Alabor%20automation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemote%2520Labor%2520Index%253A%2520Measuring%2520AI%2520Automation%2520of%2520Remote%2520Work%26entry.906535625%3DMantas%2520Mazeika%2520and%2520Alice%2520Gatti%2520and%2520Cristina%2520Menghini%2520and%2520Udari%2520Madhushani%2520Sehwag%2520and%2520Shivam%2520Singhal%2520and%2520Yury%2520Orlovskiy%2520and%2520Steven%2520Basart%2520and%2520Manasi%2520Sharma%2520and%2520Denis%2520Peskoff%2520and%2520Elaine%2520Lau%2520and%2520Jaehyuk%2520Lim%2520and%2520Lachlan%2520Carroll%2520and%2520Alice%2520Blair%2520and%2520Vinaya%2520Sivakumar%2520and%2520Sumana%2520Basu%2520and%2520Brad%2520Kenstler%2520and%2520Yuntao%2520Ma%2520and%2520Julian%2520Michael%2520and%2520Xiaoke%2520Li%2520and%2520Oliver%2520Ingebretsen%2520and%2520Aditya%2520Mehta%2520and%2520Jean%2520Mottola%2520and%2520John%2520Teichmann%2520and%2520Kevin%2520Yu%2520and%2520Zaina%2520Shaik%2520and%2520Adam%2520Khoja%2520and%2520Richard%2520Ren%2520and%2520Jason%2520Hausenloy%2520and%2520Long%2520Phan%2520and%2520Ye%2520Htet%2520and%2520Ankit%2520Aich%2520and%2520Tahseen%2520Rabbani%2520and%2520Vivswan%2520Shah%2520and%2520Andriy%2520Novykov%2520and%2520Felix%2520Binder%2520and%2520Kirill%2520Chugunov%2520and%2520Luis%2520Ramirez%2520and%2520Matias%2520Geralnik%2520and%2520Hern%25C3%25A1n%2520Mesura%2520and%2520Dean%2520Lee%2520and%2520Ed-Yeremai%2520Hernandez%2520Cardona%2520and%2520Annette%2520Diamond%2520and%2520Summer%2520Yue%2520and%2520Alexandr%2520Wang%2520and%2520Bing%2520Liu%2520and%2520Ernesto%2520Hernandez%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520AIs%2520have%2520made%2520rapid%2520progress%2520on%2520research-oriented%2520benchmarks%2520of%2520knowledge%2520and%250Areasoning%252C%2520but%2520it%2520remains%2520unclear%2520how%2520these%2520gains%2520translate%2520into%2520economic%2520value%250Aand%2520automation.%2520To%2520measure%2520this%252C%2520we%2520introduce%2520the%2520Remote%2520Labor%2520Index%2520%2528RLI%2529%252C%2520a%250Abroadly%2520multi-sector%2520benchmark%2520comprising%2520real-world%252C%2520economically%2520valuable%250Aprojects%2520designed%2520to%2520evaluate%2520end-to-end%2520agent%2520performance%2520in%2520practical%250Asettings.%2520AI%2520agents%2520perform%2520near%2520the%2520floor%2520on%2520RLI%252C%2520with%2520the%2520highest-performing%250Aagent%2520achieving%2520an%2520automation%2520rate%2520of%25202.5%2525.%2520These%2520results%2520help%2520ground%250Adiscussions%2520of%2520AI%2520automation%2520in%2520empirical%2520evidence%252C%2520setting%2520a%2520common%2520basis%2520for%250Atracking%2520AI%2520impacts%2520and%2520enabling%2520stakeholders%2520to%2520proactively%2520navigate%2520AI-driven%250Alabor%2520automation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remote%20Labor%20Index%3A%20Measuring%20AI%20Automation%20of%20Remote%20Work&entry.906535625=Mantas%20Mazeika%20and%20Alice%20Gatti%20and%20Cristina%20Menghini%20and%20Udari%20Madhushani%20Sehwag%20and%20Shivam%20Singhal%20and%20Yury%20Orlovskiy%20and%20Steven%20Basart%20and%20Manasi%20Sharma%20and%20Denis%20Peskoff%20and%20Elaine%20Lau%20and%20Jaehyuk%20Lim%20and%20Lachlan%20Carroll%20and%20Alice%20Blair%20and%20Vinaya%20Sivakumar%20and%20Sumana%20Basu%20and%20Brad%20Kenstler%20and%20Yuntao%20Ma%20and%20Julian%20Michael%20and%20Xiaoke%20Li%20and%20Oliver%20Ingebretsen%20and%20Aditya%20Mehta%20and%20Jean%20Mottola%20and%20John%20Teichmann%20and%20Kevin%20Yu%20and%20Zaina%20Shaik%20and%20Adam%20Khoja%20and%20Richard%20Ren%20and%20Jason%20Hausenloy%20and%20Long%20Phan%20and%20Ye%20Htet%20and%20Ankit%20Aich%20and%20Tahseen%20Rabbani%20and%20Vivswan%20Shah%20and%20Andriy%20Novykov%20and%20Felix%20Binder%20and%20Kirill%20Chugunov%20and%20Luis%20Ramirez%20and%20Matias%20Geralnik%20and%20Hern%C3%A1n%20Mesura%20and%20Dean%20Lee%20and%20Ed-Yeremai%20Hernandez%20Cardona%20and%20Annette%20Diamond%20and%20Summer%20Yue%20and%20Alexandr%20Wang%20and%20Bing%20Liu%20and%20Ernesto%20Hernandez%20and%20Dan%20Hendrycks&entry.1292438233=%20%20AIs%20have%20made%20rapid%20progress%20on%20research-oriented%20benchmarks%20of%20knowledge%20and%0Areasoning%2C%20but%20it%20remains%20unclear%20how%20these%20gains%20translate%20into%20economic%20value%0Aand%20automation.%20To%20measure%20this%2C%20we%20introduce%20the%20Remote%20Labor%20Index%20%28RLI%29%2C%20a%0Abroadly%20multi-sector%20benchmark%20comprising%20real-world%2C%20economically%20valuable%0Aprojects%20designed%20to%20evaluate%20end-to-end%20agent%20performance%20in%20practical%0Asettings.%20AI%20agents%20perform%20near%20the%20floor%20on%20RLI%2C%20with%20the%20highest-performing%0Aagent%20achieving%20an%20automation%20rate%20of%202.5%25.%20These%20results%20help%20ground%0Adiscussions%20of%20AI%20automation%20in%20empirical%20evidence%2C%20setting%20a%20common%20basis%20for%0Atracking%20AI%20impacts%20and%20enabling%20stakeholders%20to%20proactively%20navigate%20AI-driven%0Alabor%20automation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26787v1&entry.124074799=Read"},
{"title": "A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression", "author": "Masahiro Kato", "abstract": "  This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.\n", "link": "http://arxiv.org/abs/2510.26783v1", "date": "2025-10-30", "relevancy": 1.3119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Theory%20for%20Causal%20Inference%3A%20Direct%20Debiased%20Machine%20Learning%0A%20%20via%20Bregman-Riesz%20Regression&body=Title%3A%20A%20Unified%20Theory%20for%20Causal%20Inference%3A%20Direct%20Debiased%20Machine%20Learning%0A%20%20via%20Bregman-Riesz%20Regression%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20This%20note%20introduces%20a%20unified%20theory%20for%20causal%20inference%20that%20integrates%0ARiesz%20regression%2C%20covariate%20balancing%2C%20density-ratio%20estimation%20%28DRE%29%2C%20targeted%0Amaximum%20likelihood%20estimation%20%28TMLE%29%2C%20and%20the%20matching%20estimator%20in%20average%0Atreatment%20effect%20%28ATE%29%20estimation.%20In%20ATE%20estimation%2C%20the%20balancing%20weights%20and%0Athe%20regression%20functions%20of%20the%20outcome%20play%20important%20roles%2C%20where%20the%0Abalancing%20weights%20are%20referred%20to%20as%20the%20Riesz%20representer%2C%20bias-correction%0Aterm%2C%20and%20clever%20covariates%2C%20depending%20on%20the%20context.%20Riesz%20regression%2C%0Acovariate%20balancing%2C%20DRE%2C%20and%20the%20matching%20estimator%20are%20methods%20for%20estimating%0Athe%20balancing%20weights%2C%20where%20Riesz%20regression%20is%20essentially%20equivalent%20to%20DRE%0Ain%20the%20ATE%20context%2C%20the%20matching%20estimator%20is%20a%20special%20case%20of%20DRE%2C%20and%20DRE%20is%0Ain%20a%20dual%20relationship%20with%20covariate%20balancing.%20TMLE%20is%20a%20method%20for%0Aconstructing%20regression%20function%20estimators%20such%20that%20the%20leading%20bias%20term%0Abecomes%20zero.%20Nearest%20Neighbor%20Matching%20is%20equivalent%20to%20Least%20Squares%20Density%0ARatio%20Estimation%20and%20Riesz%20Regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Theory%2520for%2520Causal%2520Inference%253A%2520Direct%2520Debiased%2520Machine%2520Learning%250A%2520%2520via%2520Bregman-Riesz%2520Regression%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520This%2520note%2520introduces%2520a%2520unified%2520theory%2520for%2520causal%2520inference%2520that%2520integrates%250ARiesz%2520regression%252C%2520covariate%2520balancing%252C%2520density-ratio%2520estimation%2520%2528DRE%2529%252C%2520targeted%250Amaximum%2520likelihood%2520estimation%2520%2528TMLE%2529%252C%2520and%2520the%2520matching%2520estimator%2520in%2520average%250Atreatment%2520effect%2520%2528ATE%2529%2520estimation.%2520In%2520ATE%2520estimation%252C%2520the%2520balancing%2520weights%2520and%250Athe%2520regression%2520functions%2520of%2520the%2520outcome%2520play%2520important%2520roles%252C%2520where%2520the%250Abalancing%2520weights%2520are%2520referred%2520to%2520as%2520the%2520Riesz%2520representer%252C%2520bias-correction%250Aterm%252C%2520and%2520clever%2520covariates%252C%2520depending%2520on%2520the%2520context.%2520Riesz%2520regression%252C%250Acovariate%2520balancing%252C%2520DRE%252C%2520and%2520the%2520matching%2520estimator%2520are%2520methods%2520for%2520estimating%250Athe%2520balancing%2520weights%252C%2520where%2520Riesz%2520regression%2520is%2520essentially%2520equivalent%2520to%2520DRE%250Ain%2520the%2520ATE%2520context%252C%2520the%2520matching%2520estimator%2520is%2520a%2520special%2520case%2520of%2520DRE%252C%2520and%2520DRE%2520is%250Ain%2520a%2520dual%2520relationship%2520with%2520covariate%2520balancing.%2520TMLE%2520is%2520a%2520method%2520for%250Aconstructing%2520regression%2520function%2520estimators%2520such%2520that%2520the%2520leading%2520bias%2520term%250Abecomes%2520zero.%2520Nearest%2520Neighbor%2520Matching%2520is%2520equivalent%2520to%2520Least%2520Squares%2520Density%250ARatio%2520Estimation%2520and%2520Riesz%2520Regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Theory%20for%20Causal%20Inference%3A%20Direct%20Debiased%20Machine%20Learning%0A%20%20via%20Bregman-Riesz%20Regression&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20This%20note%20introduces%20a%20unified%20theory%20for%20causal%20inference%20that%20integrates%0ARiesz%20regression%2C%20covariate%20balancing%2C%20density-ratio%20estimation%20%28DRE%29%2C%20targeted%0Amaximum%20likelihood%20estimation%20%28TMLE%29%2C%20and%20the%20matching%20estimator%20in%20average%0Atreatment%20effect%20%28ATE%29%20estimation.%20In%20ATE%20estimation%2C%20the%20balancing%20weights%20and%0Athe%20regression%20functions%20of%20the%20outcome%20play%20important%20roles%2C%20where%20the%0Abalancing%20weights%20are%20referred%20to%20as%20the%20Riesz%20representer%2C%20bias-correction%0Aterm%2C%20and%20clever%20covariates%2C%20depending%20on%20the%20context.%20Riesz%20regression%2C%0Acovariate%20balancing%2C%20DRE%2C%20and%20the%20matching%20estimator%20are%20methods%20for%20estimating%0Athe%20balancing%20weights%2C%20where%20Riesz%20regression%20is%20essentially%20equivalent%20to%20DRE%0Ain%20the%20ATE%20context%2C%20the%20matching%20estimator%20is%20a%20special%20case%20of%20DRE%2C%20and%20DRE%20is%0Ain%20a%20dual%20relationship%20with%20covariate%20balancing.%20TMLE%20is%20a%20method%20for%0Aconstructing%20regression%20function%20estimators%20such%20that%20the%20leading%20bias%20term%0Abecomes%20zero.%20Nearest%20Neighbor%20Matching%20is%20equivalent%20to%20Least%20Squares%20Density%0ARatio%20Estimation%20and%20Riesz%20Regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26783v1&entry.124074799=Read"},
{"title": "Aeolus: A Multi-structural Flight Delay Dataset", "author": "Lin Xu and Xinyun Yuan and Yuxuan Liang and Suwan Yin and Yuankai Wu", "abstract": "  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data\n", "link": "http://arxiv.org/abs/2510.26616v1", "date": "2025-10-30", "relevancy": 1.2992, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4534}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4324}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aeolus%3A%20A%20Multi-structural%20Flight%20Delay%20Dataset&body=Title%3A%20Aeolus%3A%20A%20Multi-structural%20Flight%20Delay%20Dataset%0AAuthor%3A%20Lin%20Xu%20and%20Xinyun%20Yuan%20and%20Yuxuan%20Liang%20and%20Suwan%20Yin%20and%20Yuankai%20Wu%0AAbstract%3A%20%20%20We%20introduce%20Aeolus%2C%20a%20large-scale%20Multi-modal%20Flight%20Delay%20Dataset%20designed%0Ato%20advance%20research%20on%20flight%20delay%20prediction%20and%20support%20the%20development%20of%0Afoundation%20models%20for%20tabular%20data.%20Existing%20datasets%20in%20this%20domain%20are%0Atypically%20limited%20to%20flat%20tabular%20structures%20and%20fail%20to%20capture%20the%0Aspatiotemporal%20dynamics%20inherent%20in%20delay%20propagation.%20Aeolus%20addresses%20this%0Alimitation%20by%20providing%20three%20aligned%20modalities%3A%20%28i%29%20a%20tabular%20dataset%20with%0Arich%20operational%2C%20meteorological%2C%20and%20airportlevel%20features%20for%20over%2050%20million%0Aflights%3B%20%28ii%29%20a%20flight%20chain%20module%20that%20models%20delay%20propagation%20along%0Asequential%20flight%20legs%2C%20capturing%20upstream%20and%20downstream%20dependencies%3B%20and%0A%28iii%29%20a%20flight%20network%20graph%20that%20encodes%20shared%20aircraft%2C%20crew%2C%20and%20airport%0Aresource%20connections%2C%20enabling%20cross-flight%20relational%20reasoning.%20The%20dataset%0Ais%20carefully%20constructed%20with%20temporal%20splits%2C%20comprehensive%20features%2C%20and%0Astrict%20leakage%20prevention%20to%20support%20realistic%20and%20reproducible%20machine%0Alearning%20evaluation.%20Aeolus%20supports%20a%20broad%20range%20of%20tasks%2C%20including%0Aregression%2C%20classification%2C%20temporal%20structure%20modeling%2C%20and%20graph%20learning%2C%0Aserving%20as%20a%20unified%20benchmark%20across%20tabular%2C%20sequential%2C%20and%20graph%0Amodalities.%20We%20release%20baseline%20experiments%20and%20preprocessing%20tools%20to%0Afacilitate%20adoption.%20Aeolus%20fills%20a%20key%20gap%20for%20both%20domain-specific%20modeling%0Aand%20general-purpose%20structured%20data%20research.Our%20source%20code%20and%20data%20can%20be%0Aaccessed%20at%20https%3A//github.com/Flnny/Delay-data%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAeolus%253A%2520A%2520Multi-structural%2520Flight%2520Delay%2520Dataset%26entry.906535625%3DLin%2520Xu%2520and%2520Xinyun%2520Yuan%2520and%2520Yuxuan%2520Liang%2520and%2520Suwan%2520Yin%2520and%2520Yuankai%2520Wu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Aeolus%252C%2520a%2520large-scale%2520Multi-modal%2520Flight%2520Delay%2520Dataset%2520designed%250Ato%2520advance%2520research%2520on%2520flight%2520delay%2520prediction%2520and%2520support%2520the%2520development%2520of%250Afoundation%2520models%2520for%2520tabular%2520data.%2520Existing%2520datasets%2520in%2520this%2520domain%2520are%250Atypically%2520limited%2520to%2520flat%2520tabular%2520structures%2520and%2520fail%2520to%2520capture%2520the%250Aspatiotemporal%2520dynamics%2520inherent%2520in%2520delay%2520propagation.%2520Aeolus%2520addresses%2520this%250Alimitation%2520by%2520providing%2520three%2520aligned%2520modalities%253A%2520%2528i%2529%2520a%2520tabular%2520dataset%2520with%250Arich%2520operational%252C%2520meteorological%252C%2520and%2520airportlevel%2520features%2520for%2520over%252050%2520million%250Aflights%253B%2520%2528ii%2529%2520a%2520flight%2520chain%2520module%2520that%2520models%2520delay%2520propagation%2520along%250Asequential%2520flight%2520legs%252C%2520capturing%2520upstream%2520and%2520downstream%2520dependencies%253B%2520and%250A%2528iii%2529%2520a%2520flight%2520network%2520graph%2520that%2520encodes%2520shared%2520aircraft%252C%2520crew%252C%2520and%2520airport%250Aresource%2520connections%252C%2520enabling%2520cross-flight%2520relational%2520reasoning.%2520The%2520dataset%250Ais%2520carefully%2520constructed%2520with%2520temporal%2520splits%252C%2520comprehensive%2520features%252C%2520and%250Astrict%2520leakage%2520prevention%2520to%2520support%2520realistic%2520and%2520reproducible%2520machine%250Alearning%2520evaluation.%2520Aeolus%2520supports%2520a%2520broad%2520range%2520of%2520tasks%252C%2520including%250Aregression%252C%2520classification%252C%2520temporal%2520structure%2520modeling%252C%2520and%2520graph%2520learning%252C%250Aserving%2520as%2520a%2520unified%2520benchmark%2520across%2520tabular%252C%2520sequential%252C%2520and%2520graph%250Amodalities.%2520We%2520release%2520baseline%2520experiments%2520and%2520preprocessing%2520tools%2520to%250Afacilitate%2520adoption.%2520Aeolus%2520fills%2520a%2520key%2520gap%2520for%2520both%2520domain-specific%2520modeling%250Aand%2520general-purpose%2520structured%2520data%2520research.Our%2520source%2520code%2520and%2520data%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/Flnny/Delay-data%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aeolus%3A%20A%20Multi-structural%20Flight%20Delay%20Dataset&entry.906535625=Lin%20Xu%20and%20Xinyun%20Yuan%20and%20Yuxuan%20Liang%20and%20Suwan%20Yin%20and%20Yuankai%20Wu&entry.1292438233=%20%20We%20introduce%20Aeolus%2C%20a%20large-scale%20Multi-modal%20Flight%20Delay%20Dataset%20designed%0Ato%20advance%20research%20on%20flight%20delay%20prediction%20and%20support%20the%20development%20of%0Afoundation%20models%20for%20tabular%20data.%20Existing%20datasets%20in%20this%20domain%20are%0Atypically%20limited%20to%20flat%20tabular%20structures%20and%20fail%20to%20capture%20the%0Aspatiotemporal%20dynamics%20inherent%20in%20delay%20propagation.%20Aeolus%20addresses%20this%0Alimitation%20by%20providing%20three%20aligned%20modalities%3A%20%28i%29%20a%20tabular%20dataset%20with%0Arich%20operational%2C%20meteorological%2C%20and%20airportlevel%20features%20for%20over%2050%20million%0Aflights%3B%20%28ii%29%20a%20flight%20chain%20module%20that%20models%20delay%20propagation%20along%0Asequential%20flight%20legs%2C%20capturing%20upstream%20and%20downstream%20dependencies%3B%20and%0A%28iii%29%20a%20flight%20network%20graph%20that%20encodes%20shared%20aircraft%2C%20crew%2C%20and%20airport%0Aresource%20connections%2C%20enabling%20cross-flight%20relational%20reasoning.%20The%20dataset%0Ais%20carefully%20constructed%20with%20temporal%20splits%2C%20comprehensive%20features%2C%20and%0Astrict%20leakage%20prevention%20to%20support%20realistic%20and%20reproducible%20machine%0Alearning%20evaluation.%20Aeolus%20supports%20a%20broad%20range%20of%20tasks%2C%20including%0Aregression%2C%20classification%2C%20temporal%20structure%20modeling%2C%20and%20graph%20learning%2C%0Aserving%20as%20a%20unified%20benchmark%20across%20tabular%2C%20sequential%2C%20and%20graph%0Amodalities.%20We%20release%20baseline%20experiments%20and%20preprocessing%20tools%20to%0Afacilitate%20adoption.%20Aeolus%20fills%20a%20key%20gap%20for%20both%20domain-specific%20modeling%0Aand%20general-purpose%20structured%20data%20research.Our%20source%20code%20and%20data%20can%20be%0Aaccessed%20at%20https%3A//github.com/Flnny/Delay-data%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26616v1&entry.124074799=Read"},
{"title": "Defeating the Training-Inference Mismatch via FP16", "author": "Penghui Qi and Zichen Liu and Xiangxin Zhou and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin", "abstract": "  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n", "link": "http://arxiv.org/abs/2510.26788v1", "date": "2025-10-30", "relevancy": 1.3517, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4521}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defeating%20the%20Training-Inference%20Mismatch%20via%20FP16&body=Title%3A%20Defeating%20the%20Training-Inference%20Mismatch%20via%20FP16%0AAuthor%3A%20Penghui%20Qi%20and%20Zichen%20Liu%20and%20Xiangxin%20Zhou%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Wee%20Sun%20Lee%20and%20Min%20Lin%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20often%0Asuffers%20from%20instability%20due%20to%20the%20numerical%20mismatch%20between%20the%20training%20and%0Ainference%20policies.%20While%20prior%20work%20has%20attempted%20to%20mitigate%20this%20issue%0Athrough%20algorithmic%20corrections%20or%20engineering%20alignments%2C%20we%20show%20that%20its%0Aroot%20cause%20lies%20in%20the%20floating%20point%20precision%20itself.%20The%20widely%20adopted%0ABF16%2C%20despite%20its%20large%20dynamic%20range%2C%20introduces%20large%20rounding%20errors%20that%0Abreaks%20the%20consistency%20between%20training%20and%20inference.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20simply%20reverting%20to%20%5Ctextbf%7BFP16%7D%20effectively%20eliminates%20this%0Amismatch.%20The%20change%20is%20simple%2C%20fully%20supported%20by%20modern%20frameworks%20with%20only%0Aa%20few%20lines%20of%20code%20change%2C%20and%20requires%20no%20modification%20to%20the%20model%0Aarchitecture%20or%20learning%20algorithm.%20Our%20results%20suggest%20that%20using%20FP16%0Auniformly%20yields%20more%20stable%20optimization%2C%20faster%20convergence%2C%20and%20stronger%0Aperformance%20across%20diverse%20tasks%2C%20algorithms%20and%20frameworks.%20We%20hope%20these%0Afindings%20motivate%20a%20broader%20reconsideration%20of%20precision%20trade-offs%20in%20RL%0Afine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefeating%2520the%2520Training-Inference%2520Mismatch%2520via%2520FP16%26entry.906535625%3DPenghui%2520Qi%2520and%2520Zichen%2520Liu%2520and%2520Xiangxin%2520Zhou%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Wee%2520Sun%2520Lee%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%250Asuffers%2520from%2520instability%2520due%2520to%2520the%2520numerical%2520mismatch%2520between%2520the%2520training%2520and%250Ainference%2520policies.%2520While%2520prior%2520work%2520has%2520attempted%2520to%2520mitigate%2520this%2520issue%250Athrough%2520algorithmic%2520corrections%2520or%2520engineering%2520alignments%252C%2520we%2520show%2520that%2520its%250Aroot%2520cause%2520lies%2520in%2520the%2520floating%2520point%2520precision%2520itself.%2520The%2520widely%2520adopted%250ABF16%252C%2520despite%2520its%2520large%2520dynamic%2520range%252C%2520introduces%2520large%2520rounding%2520errors%2520that%250Abreaks%2520the%2520consistency%2520between%2520training%2520and%2520inference.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520simply%2520reverting%2520to%2520%255Ctextbf%257BFP16%257D%2520effectively%2520eliminates%2520this%250Amismatch.%2520The%2520change%2520is%2520simple%252C%2520fully%2520supported%2520by%2520modern%2520frameworks%2520with%2520only%250Aa%2520few%2520lines%2520of%2520code%2520change%252C%2520and%2520requires%2520no%2520modification%2520to%2520the%2520model%250Aarchitecture%2520or%2520learning%2520algorithm.%2520Our%2520results%2520suggest%2520that%2520using%2520FP16%250Auniformly%2520yields%2520more%2520stable%2520optimization%252C%2520faster%2520convergence%252C%2520and%2520stronger%250Aperformance%2520across%2520diverse%2520tasks%252C%2520algorithms%2520and%2520frameworks.%2520We%2520hope%2520these%250Afindings%2520motivate%2520a%2520broader%2520reconsideration%2520of%2520precision%2520trade-offs%2520in%2520RL%250Afine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defeating%20the%20Training-Inference%20Mismatch%20via%20FP16&entry.906535625=Penghui%20Qi%20and%20Zichen%20Liu%20and%20Xiangxin%20Zhou%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Wee%20Sun%20Lee%20and%20Min%20Lin&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20often%0Asuffers%20from%20instability%20due%20to%20the%20numerical%20mismatch%20between%20the%20training%20and%0Ainference%20policies.%20While%20prior%20work%20has%20attempted%20to%20mitigate%20this%20issue%0Athrough%20algorithmic%20corrections%20or%20engineering%20alignments%2C%20we%20show%20that%20its%0Aroot%20cause%20lies%20in%20the%20floating%20point%20precision%20itself.%20The%20widely%20adopted%0ABF16%2C%20despite%20its%20large%20dynamic%20range%2C%20introduces%20large%20rounding%20errors%20that%0Abreaks%20the%20consistency%20between%20training%20and%20inference.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20simply%20reverting%20to%20%5Ctextbf%7BFP16%7D%20effectively%20eliminates%20this%0Amismatch.%20The%20change%20is%20simple%2C%20fully%20supported%20by%20modern%20frameworks%20with%20only%0Aa%20few%20lines%20of%20code%20change%2C%20and%20requires%20no%20modification%20to%20the%20model%0Aarchitecture%20or%20learning%20algorithm.%20Our%20results%20suggest%20that%20using%20FP16%0Auniformly%20yields%20more%20stable%20optimization%2C%20faster%20convergence%2C%20and%20stronger%0Aperformance%20across%20diverse%20tasks%2C%20algorithms%20and%20frameworks.%20We%20hope%20these%0Afindings%20motivate%20a%20broader%20reconsideration%20of%20precision%20trade-offs%20in%20RL%0Afine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26788v1&entry.124074799=Read"},
{"title": "S-CFE: Simple Counterfactual Explanations", "author": "Shpresim Sadiku and Moritz Wagner and Sai Ganesh Nagarajan and Sebastian Pokutta", "abstract": "  We study the problem of finding optimal sparse, manifold-aligned\ncounterfactual explanations for classifiers. Canonically, this can be\nformulated as an optimization problem with multiple non-convex components,\nincluding classifier loss functions and manifold alignment (or\n\\emph{plausibility}) metrics. The added complexity of enforcing\n\\emph{sparsity}, or shorter explanations, complicates the problem further.\nExisting methods often focus on specific models and plausibility measures,\nrelying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we\ntackle the canonical formulation using the accelerated proximal gradient (APG)\nmethod, a simple yet efficient first-order procedure capable of handling smooth\nnon-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$)\nregularizers. This enables our approach to seamlessly incorporate various\nclassifiers and plausibility measures while producing sparser solutions. Our\nalgorithm only requires differentiable data-manifold regularizers and supports\nbox constraints for bounded feature ranges, ensuring the generated\ncounterfactuals remain \\emph{actionable}. Finally, experiments on real-world\ndatasets demonstrate that our approach effectively produces sparse,\nmanifold-aligned counterfactual explanations while maintaining proximity to the\nfactual data and computational efficiency.\n", "link": "http://arxiv.org/abs/2410.15723v7", "date": "2025-10-30", "relevancy": 1.3697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4579}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-CFE%3A%20Simple%20Counterfactual%20Explanations&body=Title%3A%20S-CFE%3A%20Simple%20Counterfactual%20Explanations%0AAuthor%3A%20Shpresim%20Sadiku%20and%20Moritz%20Wagner%20and%20Sai%20Ganesh%20Nagarajan%20and%20Sebastian%20Pokutta%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20finding%20optimal%20sparse%2C%20manifold-aligned%0Acounterfactual%20explanations%20for%20classifiers.%20Canonically%2C%20this%20can%20be%0Aformulated%20as%20an%20optimization%20problem%20with%20multiple%20non-convex%20components%2C%0Aincluding%20classifier%20loss%20functions%20and%20manifold%20alignment%20%28or%0A%5Cemph%7Bplausibility%7D%29%20metrics.%20The%20added%20complexity%20of%20enforcing%0A%5Cemph%7Bsparsity%7D%2C%20or%20shorter%20explanations%2C%20complicates%20the%20problem%20further.%0AExisting%20methods%20often%20focus%20on%20specific%20models%20and%20plausibility%20measures%2C%0Arelying%20on%20convex%20%24%5Cell_1%24%20regularizers%20to%20enforce%20sparsity.%20In%20this%20paper%2C%20we%0Atackle%20the%20canonical%20formulation%20using%20the%20accelerated%20proximal%20gradient%20%28APG%29%0Amethod%2C%20a%20simple%20yet%20efficient%20first-order%20procedure%20capable%20of%20handling%20smooth%0Anon-convex%20objectives%20and%20non-smooth%20%24%5Cell_p%24%20%28where%20%240%20%5Cleq%20p%20%3C%201%24%29%0Aregularizers.%20This%20enables%20our%20approach%20to%20seamlessly%20incorporate%20various%0Aclassifiers%20and%20plausibility%20measures%20while%20producing%20sparser%20solutions.%20Our%0Aalgorithm%20only%20requires%20differentiable%20data-manifold%20regularizers%20and%20supports%0Abox%20constraints%20for%20bounded%20feature%20ranges%2C%20ensuring%20the%20generated%0Acounterfactuals%20remain%20%5Cemph%7Bactionable%7D.%20Finally%2C%20experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20our%20approach%20effectively%20produces%20sparse%2C%0Amanifold-aligned%20counterfactual%20explanations%20while%20maintaining%20proximity%20to%20the%0Afactual%20data%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15723v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-CFE%253A%2520Simple%2520Counterfactual%2520Explanations%26entry.906535625%3DShpresim%2520Sadiku%2520and%2520Moritz%2520Wagner%2520and%2520Sai%2520Ganesh%2520Nagarajan%2520and%2520Sebastian%2520Pokutta%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520finding%2520optimal%2520sparse%252C%2520manifold-aligned%250Acounterfactual%2520explanations%2520for%2520classifiers.%2520Canonically%252C%2520this%2520can%2520be%250Aformulated%2520as%2520an%2520optimization%2520problem%2520with%2520multiple%2520non-convex%2520components%252C%250Aincluding%2520classifier%2520loss%2520functions%2520and%2520manifold%2520alignment%2520%2528or%250A%255Cemph%257Bplausibility%257D%2529%2520metrics.%2520The%2520added%2520complexity%2520of%2520enforcing%250A%255Cemph%257Bsparsity%257D%252C%2520or%2520shorter%2520explanations%252C%2520complicates%2520the%2520problem%2520further.%250AExisting%2520methods%2520often%2520focus%2520on%2520specific%2520models%2520and%2520plausibility%2520measures%252C%250Arelying%2520on%2520convex%2520%2524%255Cell_1%2524%2520regularizers%2520to%2520enforce%2520sparsity.%2520In%2520this%2520paper%252C%2520we%250Atackle%2520the%2520canonical%2520formulation%2520using%2520the%2520accelerated%2520proximal%2520gradient%2520%2528APG%2529%250Amethod%252C%2520a%2520simple%2520yet%2520efficient%2520first-order%2520procedure%2520capable%2520of%2520handling%2520smooth%250Anon-convex%2520objectives%2520and%2520non-smooth%2520%2524%255Cell_p%2524%2520%2528where%2520%25240%2520%255Cleq%2520p%2520%253C%25201%2524%2529%250Aregularizers.%2520This%2520enables%2520our%2520approach%2520to%2520seamlessly%2520incorporate%2520various%250Aclassifiers%2520and%2520plausibility%2520measures%2520while%2520producing%2520sparser%2520solutions.%2520Our%250Aalgorithm%2520only%2520requires%2520differentiable%2520data-manifold%2520regularizers%2520and%2520supports%250Abox%2520constraints%2520for%2520bounded%2520feature%2520ranges%252C%2520ensuring%2520the%2520generated%250Acounterfactuals%2520remain%2520%255Cemph%257Bactionable%257D.%2520Finally%252C%2520experiments%2520on%2520real-world%250Adatasets%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520produces%2520sparse%252C%250Amanifold-aligned%2520counterfactual%2520explanations%2520while%2520maintaining%2520proximity%2520to%2520the%250Afactual%2520data%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15723v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-CFE%3A%20Simple%20Counterfactual%20Explanations&entry.906535625=Shpresim%20Sadiku%20and%20Moritz%20Wagner%20and%20Sai%20Ganesh%20Nagarajan%20and%20Sebastian%20Pokutta&entry.1292438233=%20%20We%20study%20the%20problem%20of%20finding%20optimal%20sparse%2C%20manifold-aligned%0Acounterfactual%20explanations%20for%20classifiers.%20Canonically%2C%20this%20can%20be%0Aformulated%20as%20an%20optimization%20problem%20with%20multiple%20non-convex%20components%2C%0Aincluding%20classifier%20loss%20functions%20and%20manifold%20alignment%20%28or%0A%5Cemph%7Bplausibility%7D%29%20metrics.%20The%20added%20complexity%20of%20enforcing%0A%5Cemph%7Bsparsity%7D%2C%20or%20shorter%20explanations%2C%20complicates%20the%20problem%20further.%0AExisting%20methods%20often%20focus%20on%20specific%20models%20and%20plausibility%20measures%2C%0Arelying%20on%20convex%20%24%5Cell_1%24%20regularizers%20to%20enforce%20sparsity.%20In%20this%20paper%2C%20we%0Atackle%20the%20canonical%20formulation%20using%20the%20accelerated%20proximal%20gradient%20%28APG%29%0Amethod%2C%20a%20simple%20yet%20efficient%20first-order%20procedure%20capable%20of%20handling%20smooth%0Anon-convex%20objectives%20and%20non-smooth%20%24%5Cell_p%24%20%28where%20%240%20%5Cleq%20p%20%3C%201%24%29%0Aregularizers.%20This%20enables%20our%20approach%20to%20seamlessly%20incorporate%20various%0Aclassifiers%20and%20plausibility%20measures%20while%20producing%20sparser%20solutions.%20Our%0Aalgorithm%20only%20requires%20differentiable%20data-manifold%20regularizers%20and%20supports%0Abox%20constraints%20for%20bounded%20feature%20ranges%2C%20ensuring%20the%20generated%0Acounterfactuals%20remain%20%5Cemph%7Bactionable%7D.%20Finally%2C%20experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20our%20approach%20effectively%20produces%20sparse%2C%0Amanifold-aligned%20counterfactual%20explanations%20while%20maintaining%20proximity%20to%20the%0Afactual%20data%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15723v7&entry.124074799=Read"},
{"title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2", "author": "Daniela Martin and Joseph Gallego", "abstract": "  Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.\n", "link": "http://arxiv.org/abs/2510.26653v1", "date": "2025-10-30", "relevancy": 1.0188, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.515}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reliable%20Sea%20Ice%20Drift%20Estimation%20in%20the%20Arctic%20Deep%20Learning%0A%20%20Optical%20Flow%20on%20RADARSAT-2&body=Title%3A%20Towards%20Reliable%20Sea%20Ice%20Drift%20Estimation%20in%20the%20Arctic%20Deep%20Learning%0A%20%20Optical%20Flow%20on%20RADARSAT-2%0AAuthor%3A%20Daniela%20Martin%20and%20Joseph%20Gallego%0AAbstract%3A%20%20%20Accurate%20estimation%20of%20sea%20ice%20drift%20is%20critical%20for%20Arctic%20navigation%2C%0Aclimate%20research%2C%20and%20operational%20forecasting.%20While%20optical%20flow%2C%20a%20computer%0Avision%20technique%20for%20estimating%20pixel%20wise%20motion%20between%20consecutive%20images%2C%0Ahas%20advanced%20rapidly%20in%20computer%20vision%2C%20its%20applicability%20to%20geophysical%0Aproblems%20and%20to%20satellite%20SAR%20imagery%20remains%20underexplored.%20Classical%20optical%0Aflow%20methods%20rely%20on%20mathematical%20models%20and%20strong%20assumptions%20about%20motion%2C%0Awhich%20limit%20their%20accuracy%20in%20complex%20scenarios.%20Recent%20deep%20learning%20based%0Aapproaches%20have%20substantially%20improved%20performance%20and%20are%20now%20the%20standard%20in%0Acomputer%20vision%2C%20motivating%20their%20application%20to%20sea%20ice%20drift%20estimation.%20We%0Apresent%20the%20first%20large%20scale%20benchmark%20of%2048%20deep%20learning%20optical%20flow%20models%0Aon%20RADARSAT%202%20ScanSAR%20sea%20ice%20imagery%2C%20evaluated%20with%20endpoint%20error%20%28EPE%29%20and%0AFl%20all%20metrics%20against%20GNSS%20tracked%20buoys.%20Several%20models%20achieve%20sub%20kilometer%0Aaccuracy%20%28EPE%206%20to%208%20pixels%2C%20300%20to%20400%20m%29%2C%20a%20small%20error%20relative%20to%20the%0Aspatial%20scales%20of%20sea%20ice%20motion%20and%20typical%20navigation%20requirements%20in%20the%0AArctic.%20Our%20results%20demonstrate%20that%20the%20models%20are%20capable%20of%20capturing%0Aconsistent%20regional%20drift%20patterns%20and%20that%20recent%20deep%20learning%20based%20optical%0Aflow%20methods%2C%20which%20have%20substantially%20improved%20motion%20estimation%20accuracy%0Acompared%20to%20classical%20methods%2C%20can%20be%20effectively%20transferred%20to%20polar%20remote%0Asensing.%20Optical%20flow%20produces%20spatially%20continuous%20drift%20fields%2C%20providing%0Amotion%20estimates%20for%20every%20image%20pixel%20rather%20than%20at%20sparse%20buoy%20locations%2C%0Aoffering%20new%20opportunities%20for%20navigation%20and%20climate%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reliable%2520Sea%2520Ice%2520Drift%2520Estimation%2520in%2520the%2520Arctic%2520Deep%2520Learning%250A%2520%2520Optical%2520Flow%2520on%2520RADARSAT-2%26entry.906535625%3DDaniela%2520Martin%2520and%2520Joseph%2520Gallego%26entry.1292438233%3D%2520%2520Accurate%2520estimation%2520of%2520sea%2520ice%2520drift%2520is%2520critical%2520for%2520Arctic%2520navigation%252C%250Aclimate%2520research%252C%2520and%2520operational%2520forecasting.%2520While%2520optical%2520flow%252C%2520a%2520computer%250Avision%2520technique%2520for%2520estimating%2520pixel%2520wise%2520motion%2520between%2520consecutive%2520images%252C%250Ahas%2520advanced%2520rapidly%2520in%2520computer%2520vision%252C%2520its%2520applicability%2520to%2520geophysical%250Aproblems%2520and%2520to%2520satellite%2520SAR%2520imagery%2520remains%2520underexplored.%2520Classical%2520optical%250Aflow%2520methods%2520rely%2520on%2520mathematical%2520models%2520and%2520strong%2520assumptions%2520about%2520motion%252C%250Awhich%2520limit%2520their%2520accuracy%2520in%2520complex%2520scenarios.%2520Recent%2520deep%2520learning%2520based%250Aapproaches%2520have%2520substantially%2520improved%2520performance%2520and%2520are%2520now%2520the%2520standard%2520in%250Acomputer%2520vision%252C%2520motivating%2520their%2520application%2520to%2520sea%2520ice%2520drift%2520estimation.%2520We%250Apresent%2520the%2520first%2520large%2520scale%2520benchmark%2520of%252048%2520deep%2520learning%2520optical%2520flow%2520models%250Aon%2520RADARSAT%25202%2520ScanSAR%2520sea%2520ice%2520imagery%252C%2520evaluated%2520with%2520endpoint%2520error%2520%2528EPE%2529%2520and%250AFl%2520all%2520metrics%2520against%2520GNSS%2520tracked%2520buoys.%2520Several%2520models%2520achieve%2520sub%2520kilometer%250Aaccuracy%2520%2528EPE%25206%2520to%25208%2520pixels%252C%2520300%2520to%2520400%2520m%2529%252C%2520a%2520small%2520error%2520relative%2520to%2520the%250Aspatial%2520scales%2520of%2520sea%2520ice%2520motion%2520and%2520typical%2520navigation%2520requirements%2520in%2520the%250AArctic.%2520Our%2520results%2520demonstrate%2520that%2520the%2520models%2520are%2520capable%2520of%2520capturing%250Aconsistent%2520regional%2520drift%2520patterns%2520and%2520that%2520recent%2520deep%2520learning%2520based%2520optical%250Aflow%2520methods%252C%2520which%2520have%2520substantially%2520improved%2520motion%2520estimation%2520accuracy%250Acompared%2520to%2520classical%2520methods%252C%2520can%2520be%2520effectively%2520transferred%2520to%2520polar%2520remote%250Asensing.%2520Optical%2520flow%2520produces%2520spatially%2520continuous%2520drift%2520fields%252C%2520providing%250Amotion%2520estimates%2520for%2520every%2520image%2520pixel%2520rather%2520than%2520at%2520sparse%2520buoy%2520locations%252C%250Aoffering%2520new%2520opportunities%2520for%2520navigation%2520and%2520climate%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reliable%20Sea%20Ice%20Drift%20Estimation%20in%20the%20Arctic%20Deep%20Learning%0A%20%20Optical%20Flow%20on%20RADARSAT-2&entry.906535625=Daniela%20Martin%20and%20Joseph%20Gallego&entry.1292438233=%20%20Accurate%20estimation%20of%20sea%20ice%20drift%20is%20critical%20for%20Arctic%20navigation%2C%0Aclimate%20research%2C%20and%20operational%20forecasting.%20While%20optical%20flow%2C%20a%20computer%0Avision%20technique%20for%20estimating%20pixel%20wise%20motion%20between%20consecutive%20images%2C%0Ahas%20advanced%20rapidly%20in%20computer%20vision%2C%20its%20applicability%20to%20geophysical%0Aproblems%20and%20to%20satellite%20SAR%20imagery%20remains%20underexplored.%20Classical%20optical%0Aflow%20methods%20rely%20on%20mathematical%20models%20and%20strong%20assumptions%20about%20motion%2C%0Awhich%20limit%20their%20accuracy%20in%20complex%20scenarios.%20Recent%20deep%20learning%20based%0Aapproaches%20have%20substantially%20improved%20performance%20and%20are%20now%20the%20standard%20in%0Acomputer%20vision%2C%20motivating%20their%20application%20to%20sea%20ice%20drift%20estimation.%20We%0Apresent%20the%20first%20large%20scale%20benchmark%20of%2048%20deep%20learning%20optical%20flow%20models%0Aon%20RADARSAT%202%20ScanSAR%20sea%20ice%20imagery%2C%20evaluated%20with%20endpoint%20error%20%28EPE%29%20and%0AFl%20all%20metrics%20against%20GNSS%20tracked%20buoys.%20Several%20models%20achieve%20sub%20kilometer%0Aaccuracy%20%28EPE%206%20to%208%20pixels%2C%20300%20to%20400%20m%29%2C%20a%20small%20error%20relative%20to%20the%0Aspatial%20scales%20of%20sea%20ice%20motion%20and%20typical%20navigation%20requirements%20in%20the%0AArctic.%20Our%20results%20demonstrate%20that%20the%20models%20are%20capable%20of%20capturing%0Aconsistent%20regional%20drift%20patterns%20and%20that%20recent%20deep%20learning%20based%20optical%0Aflow%20methods%2C%20which%20have%20substantially%20improved%20motion%20estimation%20accuracy%0Acompared%20to%20classical%20methods%2C%20can%20be%20effectively%20transferred%20to%20polar%20remote%0Asensing.%20Optical%20flow%20produces%20spatially%20continuous%20drift%20fields%2C%20providing%0Amotion%20estimates%20for%20every%20image%20pixel%20rather%20than%20at%20sparse%20buoy%20locations%2C%0Aoffering%20new%20opportunities%20for%20navigation%20and%20climate%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26653v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


