<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241224.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RSGaussian:3D Gaussian Splatting with LiDAR for Aerial Remote Sensing\n  Novel View Synthesis", "author": "Yiling Yao and Wenjuan Zhang and Bing Zhang and Bocheng Li and Yaning Wang and Bowen Wang", "abstract": "  This study presents RSGaussian, an innovative novel view synthesis (NVS)\nmethod for aerial remote sensing scenes that incorporate LiDAR point cloud as\nconstraints into the 3D Gaussian Splatting method, which ensures that Gaussians\ngrow and split along geometric benchmarks, addressing the overgrowth and\nfloaters issues occurs. Additionally, the approach introduces coordinate\ntransformations with distortion parameters for camera models to achieve\npixel-level alignment between LiDAR point clouds and 2D images, facilitating\nheterogeneous data fusion and achieving the high-precision geo-alignment\nrequired in aerial remote sensing. Depth and plane consistency losses are\nincorporated into the loss function to guide Gaussians towards real depth and\nplane representations, significantly improving depth estimation accuracy.\nExperimental results indicate that our approach has achieved novel view\nsynthesis that balances photo-realistic visual quality and high-precision\ngeometric estimation under aerial remote sensing datasets. Finally, we have\nalso established and open-sourced a dense LiDAR point cloud dataset along with\nits corresponding aerial multi-view images, AIR-LONGYAN.\n", "link": "http://arxiv.org/abs/2412.18380v1", "date": "2024-12-24", "relevancy": 3.3554, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7416}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6376}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSGaussian%3A3D%20Gaussian%20Splatting%20with%20LiDAR%20for%20Aerial%20Remote%20Sensing%0A%20%20Novel%20View%20Synthesis&body=Title%3A%20RSGaussian%3A3D%20Gaussian%20Splatting%20with%20LiDAR%20for%20Aerial%20Remote%20Sensing%0A%20%20Novel%20View%20Synthesis%0AAuthor%3A%20Yiling%20Yao%20and%20Wenjuan%20Zhang%20and%20Bing%20Zhang%20and%20Bocheng%20Li%20and%20Yaning%20Wang%20and%20Bowen%20Wang%0AAbstract%3A%20%20%20This%20study%20presents%20RSGaussian%2C%20an%20innovative%20novel%20view%20synthesis%20%28NVS%29%0Amethod%20for%20aerial%20remote%20sensing%20scenes%20that%20incorporate%20LiDAR%20point%20cloud%20as%0Aconstraints%20into%20the%203D%20Gaussian%20Splatting%20method%2C%20which%20ensures%20that%20Gaussians%0Agrow%20and%20split%20along%20geometric%20benchmarks%2C%20addressing%20the%20overgrowth%20and%0Afloaters%20issues%20occurs.%20Additionally%2C%20the%20approach%20introduces%20coordinate%0Atransformations%20with%20distortion%20parameters%20for%20camera%20models%20to%20achieve%0Apixel-level%20alignment%20between%20LiDAR%20point%20clouds%20and%202D%20images%2C%20facilitating%0Aheterogeneous%20data%20fusion%20and%20achieving%20the%20high-precision%20geo-alignment%0Arequired%20in%20aerial%20remote%20sensing.%20Depth%20and%20plane%20consistency%20losses%20are%0Aincorporated%20into%20the%20loss%20function%20to%20guide%20Gaussians%20towards%20real%20depth%20and%0Aplane%20representations%2C%20significantly%20improving%20depth%20estimation%20accuracy.%0AExperimental%20results%20indicate%20that%20our%20approach%20has%20achieved%20novel%20view%0Asynthesis%20that%20balances%20photo-realistic%20visual%20quality%20and%20high-precision%0Ageometric%20estimation%20under%20aerial%20remote%20sensing%20datasets.%20Finally%2C%20we%20have%0Aalso%20established%20and%20open-sourced%20a%20dense%20LiDAR%20point%20cloud%20dataset%20along%20with%0Aits%20corresponding%20aerial%20multi-view%20images%2C%20AIR-LONGYAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSGaussian%253A3D%2520Gaussian%2520Splatting%2520with%2520LiDAR%2520for%2520Aerial%2520Remote%2520Sensing%250A%2520%2520Novel%2520View%2520Synthesis%26entry.906535625%3DYiling%2520Yao%2520and%2520Wenjuan%2520Zhang%2520and%2520Bing%2520Zhang%2520and%2520Bocheng%2520Li%2520and%2520Yaning%2520Wang%2520and%2520Bowen%2520Wang%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520RSGaussian%252C%2520an%2520innovative%2520novel%2520view%2520synthesis%2520%2528NVS%2529%250Amethod%2520for%2520aerial%2520remote%2520sensing%2520scenes%2520that%2520incorporate%2520LiDAR%2520point%2520cloud%2520as%250Aconstraints%2520into%2520the%25203D%2520Gaussian%2520Splatting%2520method%252C%2520which%2520ensures%2520that%2520Gaussians%250Agrow%2520and%2520split%2520along%2520geometric%2520benchmarks%252C%2520addressing%2520the%2520overgrowth%2520and%250Afloaters%2520issues%2520occurs.%2520Additionally%252C%2520the%2520approach%2520introduces%2520coordinate%250Atransformations%2520with%2520distortion%2520parameters%2520for%2520camera%2520models%2520to%2520achieve%250Apixel-level%2520alignment%2520between%2520LiDAR%2520point%2520clouds%2520and%25202D%2520images%252C%2520facilitating%250Aheterogeneous%2520data%2520fusion%2520and%2520achieving%2520the%2520high-precision%2520geo-alignment%250Arequired%2520in%2520aerial%2520remote%2520sensing.%2520Depth%2520and%2520plane%2520consistency%2520losses%2520are%250Aincorporated%2520into%2520the%2520loss%2520function%2520to%2520guide%2520Gaussians%2520towards%2520real%2520depth%2520and%250Aplane%2520representations%252C%2520significantly%2520improving%2520depth%2520estimation%2520accuracy.%250AExperimental%2520results%2520indicate%2520that%2520our%2520approach%2520has%2520achieved%2520novel%2520view%250Asynthesis%2520that%2520balances%2520photo-realistic%2520visual%2520quality%2520and%2520high-precision%250Ageometric%2520estimation%2520under%2520aerial%2520remote%2520sensing%2520datasets.%2520Finally%252C%2520we%2520have%250Aalso%2520established%2520and%2520open-sourced%2520a%2520dense%2520LiDAR%2520point%2520cloud%2520dataset%2520along%2520with%250Aits%2520corresponding%2520aerial%2520multi-view%2520images%252C%2520AIR-LONGYAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSGaussian%3A3D%20Gaussian%20Splatting%20with%20LiDAR%20for%20Aerial%20Remote%20Sensing%0A%20%20Novel%20View%20Synthesis&entry.906535625=Yiling%20Yao%20and%20Wenjuan%20Zhang%20and%20Bing%20Zhang%20and%20Bocheng%20Li%20and%20Yaning%20Wang%20and%20Bowen%20Wang&entry.1292438233=%20%20This%20study%20presents%20RSGaussian%2C%20an%20innovative%20novel%20view%20synthesis%20%28NVS%29%0Amethod%20for%20aerial%20remote%20sensing%20scenes%20that%20incorporate%20LiDAR%20point%20cloud%20as%0Aconstraints%20into%20the%203D%20Gaussian%20Splatting%20method%2C%20which%20ensures%20that%20Gaussians%0Agrow%20and%20split%20along%20geometric%20benchmarks%2C%20addressing%20the%20overgrowth%20and%0Afloaters%20issues%20occurs.%20Additionally%2C%20the%20approach%20introduces%20coordinate%0Atransformations%20with%20distortion%20parameters%20for%20camera%20models%20to%20achieve%0Apixel-level%20alignment%20between%20LiDAR%20point%20clouds%20and%202D%20images%2C%20facilitating%0Aheterogeneous%20data%20fusion%20and%20achieving%20the%20high-precision%20geo-alignment%0Arequired%20in%20aerial%20remote%20sensing.%20Depth%20and%20plane%20consistency%20losses%20are%0Aincorporated%20into%20the%20loss%20function%20to%20guide%20Gaussians%20towards%20real%20depth%20and%0Aplane%20representations%2C%20significantly%20improving%20depth%20estimation%20accuracy.%0AExperimental%20results%20indicate%20that%20our%20approach%20has%20achieved%20novel%20view%0Asynthesis%20that%20balances%20photo-realistic%20visual%20quality%20and%20high-precision%0Ageometric%20estimation%20under%20aerial%20remote%20sensing%20datasets.%20Finally%2C%20we%20have%0Aalso%20established%20and%20open-sourced%20a%20dense%20LiDAR%20point%20cloud%20dataset%20along%20with%0Aits%20corresponding%20aerial%20multi-view%20images%2C%20AIR-LONGYAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18380v1&entry.124074799=Read"},
{"title": "ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation", "author": "Hongjie Li and Hong-Xing Yu and Jiaman Li and Jiajun Wu", "abstract": "  Human-scene interaction (HSI) generation is crucial for applications in\nembodied AI, virtual reality, and robotics. While existing methods can\nsynthesize realistic human motions in 3D scenes and generate plausible\nhuman-object interactions, they heavily rely on datasets containing paired 3D\nscene and motion capture data, which are expensive and time-consuming to\ncollect across diverse environments and interactions. We present ZeroHSI, a\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\nintegrating video generation and neural human rendering. Our key insight is to\nleverage the rich motion priors learned by state-of-the-art video generation\nmodels, which have been trained on vast amounts of natural human movements and\ninteractions, and use differentiable rendering to reconstruct human-scene\ninteractions. ZeroHSI can synthesize realistic human motions in both static\nscenes and environments with dynamic objects, without requiring any\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\ntypes of various indoor and outdoor scenes with different interaction prompts,\ndemonstrating its ability to generate diverse and contextually appropriate\nhuman-scene interactions.\n", "link": "http://arxiv.org/abs/2412.18600v1", "date": "2024-12-24", "relevancy": 3.155, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6594}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6456}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroHSI%3A%20Zero-Shot%204D%20Human-Scene%20Interaction%20by%20Video%20Generation&body=Title%3A%20ZeroHSI%3A%20Zero-Shot%204D%20Human-Scene%20Interaction%20by%20Video%20Generation%0AAuthor%3A%20Hongjie%20Li%20and%20Hong-Xing%20Yu%20and%20Jiaman%20Li%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20Human-scene%20interaction%20%28HSI%29%20generation%20is%20crucial%20for%20applications%20in%0Aembodied%20AI%2C%20virtual%20reality%2C%20and%20robotics.%20While%20existing%20methods%20can%0Asynthesize%20realistic%20human%20motions%20in%203D%20scenes%20and%20generate%20plausible%0Ahuman-object%20interactions%2C%20they%20heavily%20rely%20on%20datasets%20containing%20paired%203D%0Ascene%20and%20motion%20capture%20data%2C%20which%20are%20expensive%20and%20time-consuming%20to%0Acollect%20across%20diverse%20environments%20and%20interactions.%20We%20present%20ZeroHSI%2C%20a%0Anovel%20approach%20that%20enables%20zero-shot%204D%20human-scene%20interaction%20synthesis%20by%0Aintegrating%20video%20generation%20and%20neural%20human%20rendering.%20Our%20key%20insight%20is%20to%0Aleverage%20the%20rich%20motion%20priors%20learned%20by%20state-of-the-art%20video%20generation%0Amodels%2C%20which%20have%20been%20trained%20on%20vast%20amounts%20of%20natural%20human%20movements%20and%0Ainteractions%2C%20and%20use%20differentiable%20rendering%20to%20reconstruct%20human-scene%0Ainteractions.%20ZeroHSI%20can%20synthesize%20realistic%20human%20motions%20in%20both%20static%0Ascenes%20and%20environments%20with%20dynamic%20objects%2C%20without%20requiring%20any%0Aground-truth%20motion%20data.%20We%20evaluate%20ZeroHSI%20on%20a%20curated%20dataset%20of%20different%0Atypes%20of%20various%20indoor%20and%20outdoor%20scenes%20with%20different%20interaction%20prompts%2C%0Ademonstrating%20its%20ability%20to%20generate%20diverse%20and%20contextually%20appropriate%0Ahuman-scene%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroHSI%253A%2520Zero-Shot%25204D%2520Human-Scene%2520Interaction%2520by%2520Video%2520Generation%26entry.906535625%3DHongjie%2520Li%2520and%2520Hong-Xing%2520Yu%2520and%2520Jiaman%2520Li%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520Human-scene%2520interaction%2520%2528HSI%2529%2520generation%2520is%2520crucial%2520for%2520applications%2520in%250Aembodied%2520AI%252C%2520virtual%2520reality%252C%2520and%2520robotics.%2520While%2520existing%2520methods%2520can%250Asynthesize%2520realistic%2520human%2520motions%2520in%25203D%2520scenes%2520and%2520generate%2520plausible%250Ahuman-object%2520interactions%252C%2520they%2520heavily%2520rely%2520on%2520datasets%2520containing%2520paired%25203D%250Ascene%2520and%2520motion%2520capture%2520data%252C%2520which%2520are%2520expensive%2520and%2520time-consuming%2520to%250Acollect%2520across%2520diverse%2520environments%2520and%2520interactions.%2520We%2520present%2520ZeroHSI%252C%2520a%250Anovel%2520approach%2520that%2520enables%2520zero-shot%25204D%2520human-scene%2520interaction%2520synthesis%2520by%250Aintegrating%2520video%2520generation%2520and%2520neural%2520human%2520rendering.%2520Our%2520key%2520insight%2520is%2520to%250Aleverage%2520the%2520rich%2520motion%2520priors%2520learned%2520by%2520state-of-the-art%2520video%2520generation%250Amodels%252C%2520which%2520have%2520been%2520trained%2520on%2520vast%2520amounts%2520of%2520natural%2520human%2520movements%2520and%250Ainteractions%252C%2520and%2520use%2520differentiable%2520rendering%2520to%2520reconstruct%2520human-scene%250Ainteractions.%2520ZeroHSI%2520can%2520synthesize%2520realistic%2520human%2520motions%2520in%2520both%2520static%250Ascenes%2520and%2520environments%2520with%2520dynamic%2520objects%252C%2520without%2520requiring%2520any%250Aground-truth%2520motion%2520data.%2520We%2520evaluate%2520ZeroHSI%2520on%2520a%2520curated%2520dataset%2520of%2520different%250Atypes%2520of%2520various%2520indoor%2520and%2520outdoor%2520scenes%2520with%2520different%2520interaction%2520prompts%252C%250Ademonstrating%2520its%2520ability%2520to%2520generate%2520diverse%2520and%2520contextually%2520appropriate%250Ahuman-scene%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroHSI%3A%20Zero-Shot%204D%20Human-Scene%20Interaction%20by%20Video%20Generation&entry.906535625=Hongjie%20Li%20and%20Hong-Xing%20Yu%20and%20Jiaman%20Li%20and%20Jiajun%20Wu&entry.1292438233=%20%20Human-scene%20interaction%20%28HSI%29%20generation%20is%20crucial%20for%20applications%20in%0Aembodied%20AI%2C%20virtual%20reality%2C%20and%20robotics.%20While%20existing%20methods%20can%0Asynthesize%20realistic%20human%20motions%20in%203D%20scenes%20and%20generate%20plausible%0Ahuman-object%20interactions%2C%20they%20heavily%20rely%20on%20datasets%20containing%20paired%203D%0Ascene%20and%20motion%20capture%20data%2C%20which%20are%20expensive%20and%20time-consuming%20to%0Acollect%20across%20diverse%20environments%20and%20interactions.%20We%20present%20ZeroHSI%2C%20a%0Anovel%20approach%20that%20enables%20zero-shot%204D%20human-scene%20interaction%20synthesis%20by%0Aintegrating%20video%20generation%20and%20neural%20human%20rendering.%20Our%20key%20insight%20is%20to%0Aleverage%20the%20rich%20motion%20priors%20learned%20by%20state-of-the-art%20video%20generation%0Amodels%2C%20which%20have%20been%20trained%20on%20vast%20amounts%20of%20natural%20human%20movements%20and%0Ainteractions%2C%20and%20use%20differentiable%20rendering%20to%20reconstruct%20human-scene%0Ainteractions.%20ZeroHSI%20can%20synthesize%20realistic%20human%20motions%20in%20both%20static%0Ascenes%20and%20environments%20with%20dynamic%20objects%2C%20without%20requiring%20any%0Aground-truth%20motion%20data.%20We%20evaluate%20ZeroHSI%20on%20a%20curated%20dataset%20of%20different%0Atypes%20of%20various%20indoor%20and%20outdoor%20scenes%20with%20different%20interaction%20prompts%2C%0Ademonstrating%20its%20ability%20to%20generate%20diverse%20and%20contextually%20appropriate%0Ahuman-scene%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18600v1&entry.124074799=Read"},
{"title": "Orient Anything: Learning Robust Object Orientation Estimation from\n  Rendering 3D Models", "author": "Zehan Wang and Ziang Zhang and Tianyu Pang and Chao Du and Hengshuang Zhao and Zhou Zhao", "abstract": "  Orientation is a key attribute of objects, crucial for understanding their\nspatial pose and arrangement in images. However, practical solutions for\naccurate orientation estimation from a single image remain underexplored. In\nthis work, we introduce Orient Anything, the first expert and foundational\nmodel designed to estimate object orientation in a single- and free-view image.\nDue to the scarcity of labeled data, we propose extracting knowledge from the\n3D world. By developing a pipeline to annotate the front face of 3D objects and\nrender images from random views, we collect 2M images with precise orientation\nannotations. To fully leverage the dataset, we design a robust training\nobjective that models the 3D orientation as probability distributions of three\nangles and predicts the object orientation by fitting these distributions.\nBesides, we employ several strategies to improve synthetic-to-real transfer.\nOur model achieves state-of-the-art orientation estimation accuracy in both\nrendered and real images and exhibits impressive zero-shot ability in various\nscenarios. More importantly, our model enhances many applications, such as\ncomprehension and generation of complex spatial concepts and 3D object pose\nadjustment.\n", "link": "http://arxiv.org/abs/2412.18605v1", "date": "2024-12-24", "relevancy": 2.9955, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6079}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5954}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orient%20Anything%3A%20Learning%20Robust%20Object%20Orientation%20Estimation%20from%0A%20%20Rendering%203D%20Models&body=Title%3A%20Orient%20Anything%3A%20Learning%20Robust%20Object%20Orientation%20Estimation%20from%0A%20%20Rendering%203D%20Models%0AAuthor%3A%20Zehan%20Wang%20and%20Ziang%20Zhang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Orientation%20is%20a%20key%20attribute%20of%20objects%2C%20crucial%20for%20understanding%20their%0Aspatial%20pose%20and%20arrangement%20in%20images.%20However%2C%20practical%20solutions%20for%0Aaccurate%20orientation%20estimation%20from%20a%20single%20image%20remain%20underexplored.%20In%0Athis%20work%2C%20we%20introduce%20Orient%20Anything%2C%20the%20first%20expert%20and%20foundational%0Amodel%20designed%20to%20estimate%20object%20orientation%20in%20a%20single-%20and%20free-view%20image.%0ADue%20to%20the%20scarcity%20of%20labeled%20data%2C%20we%20propose%20extracting%20knowledge%20from%20the%0A3D%20world.%20By%20developing%20a%20pipeline%20to%20annotate%20the%20front%20face%20of%203D%20objects%20and%0Arender%20images%20from%20random%20views%2C%20we%20collect%202M%20images%20with%20precise%20orientation%0Aannotations.%20To%20fully%20leverage%20the%20dataset%2C%20we%20design%20a%20robust%20training%0Aobjective%20that%20models%20the%203D%20orientation%20as%20probability%20distributions%20of%20three%0Aangles%20and%20predicts%20the%20object%20orientation%20by%20fitting%20these%20distributions.%0ABesides%2C%20we%20employ%20several%20strategies%20to%20improve%20synthetic-to-real%20transfer.%0AOur%20model%20achieves%20state-of-the-art%20orientation%20estimation%20accuracy%20in%20both%0Arendered%20and%20real%20images%20and%20exhibits%20impressive%20zero-shot%20ability%20in%20various%0Ascenarios.%20More%20importantly%2C%20our%20model%20enhances%20many%20applications%2C%20such%20as%0Acomprehension%20and%20generation%20of%20complex%20spatial%20concepts%20and%203D%20object%20pose%0Aadjustment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrient%2520Anything%253A%2520Learning%2520Robust%2520Object%2520Orientation%2520Estimation%2520from%250A%2520%2520Rendering%25203D%2520Models%26entry.906535625%3DZehan%2520Wang%2520and%2520Ziang%2520Zhang%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Hengshuang%2520Zhao%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Orientation%2520is%2520a%2520key%2520attribute%2520of%2520objects%252C%2520crucial%2520for%2520understanding%2520their%250Aspatial%2520pose%2520and%2520arrangement%2520in%2520images.%2520However%252C%2520practical%2520solutions%2520for%250Aaccurate%2520orientation%2520estimation%2520from%2520a%2520single%2520image%2520remain%2520underexplored.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520Orient%2520Anything%252C%2520the%2520first%2520expert%2520and%2520foundational%250Amodel%2520designed%2520to%2520estimate%2520object%2520orientation%2520in%2520a%2520single-%2520and%2520free-view%2520image.%250ADue%2520to%2520the%2520scarcity%2520of%2520labeled%2520data%252C%2520we%2520propose%2520extracting%2520knowledge%2520from%2520the%250A3D%2520world.%2520By%2520developing%2520a%2520pipeline%2520to%2520annotate%2520the%2520front%2520face%2520of%25203D%2520objects%2520and%250Arender%2520images%2520from%2520random%2520views%252C%2520we%2520collect%25202M%2520images%2520with%2520precise%2520orientation%250Aannotations.%2520To%2520fully%2520leverage%2520the%2520dataset%252C%2520we%2520design%2520a%2520robust%2520training%250Aobjective%2520that%2520models%2520the%25203D%2520orientation%2520as%2520probability%2520distributions%2520of%2520three%250Aangles%2520and%2520predicts%2520the%2520object%2520orientation%2520by%2520fitting%2520these%2520distributions.%250ABesides%252C%2520we%2520employ%2520several%2520strategies%2520to%2520improve%2520synthetic-to-real%2520transfer.%250AOur%2520model%2520achieves%2520state-of-the-art%2520orientation%2520estimation%2520accuracy%2520in%2520both%250Arendered%2520and%2520real%2520images%2520and%2520exhibits%2520impressive%2520zero-shot%2520ability%2520in%2520various%250Ascenarios.%2520More%2520importantly%252C%2520our%2520model%2520enhances%2520many%2520applications%252C%2520such%2520as%250Acomprehension%2520and%2520generation%2520of%2520complex%2520spatial%2520concepts%2520and%25203D%2520object%2520pose%250Aadjustment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orient%20Anything%3A%20Learning%20Robust%20Object%20Orientation%20Estimation%20from%0A%20%20Rendering%203D%20Models&entry.906535625=Zehan%20Wang%20and%20Ziang%20Zhang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao&entry.1292438233=%20%20Orientation%20is%20a%20key%20attribute%20of%20objects%2C%20crucial%20for%20understanding%20their%0Aspatial%20pose%20and%20arrangement%20in%20images.%20However%2C%20practical%20solutions%20for%0Aaccurate%20orientation%20estimation%20from%20a%20single%20image%20remain%20underexplored.%20In%0Athis%20work%2C%20we%20introduce%20Orient%20Anything%2C%20the%20first%20expert%20and%20foundational%0Amodel%20designed%20to%20estimate%20object%20orientation%20in%20a%20single-%20and%20free-view%20image.%0ADue%20to%20the%20scarcity%20of%20labeled%20data%2C%20we%20propose%20extracting%20knowledge%20from%20the%0A3D%20world.%20By%20developing%20a%20pipeline%20to%20annotate%20the%20front%20face%20of%203D%20objects%20and%0Arender%20images%20from%20random%20views%2C%20we%20collect%202M%20images%20with%20precise%20orientation%0Aannotations.%20To%20fully%20leverage%20the%20dataset%2C%20we%20design%20a%20robust%20training%0Aobjective%20that%20models%20the%203D%20orientation%20as%20probability%20distributions%20of%20three%0Aangles%20and%20predicts%20the%20object%20orientation%20by%20fitting%20these%20distributions.%0ABesides%2C%20we%20employ%20several%20strategies%20to%20improve%20synthetic-to-real%20transfer.%0AOur%20model%20achieves%20state-of-the-art%20orientation%20estimation%20accuracy%20in%20both%0Arendered%20and%20real%20images%20and%20exhibits%20impressive%20zero-shot%20ability%20in%20various%0Ascenarios.%20More%20importantly%2C%20our%20model%20enhances%20many%20applications%2C%20such%20as%0Acomprehension%20and%20generation%20of%20complex%20spatial%20concepts%20and%203D%20object%20pose%0Aadjustment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18605v1&entry.124074799=Read"},
{"title": "OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous\n  Driving", "author": "Lianqing Zheng and Long Yang and Qunshu Lin and Wenjin Ai and Minghao Liu and Shouyi Lu and Jianan Liu and Hongze Ren and Jingyue Mo and Xiaokai Bai and Jie Bai and Zhixiong Ma and Xichan Zhu", "abstract": "  The rapid advancement of deep learning has intensified the need for\ncomprehensive data for use by autonomous driving algorithms. High-quality\ndatasets are crucial for the development of effective data-driven autonomous\ndriving solutions. Next-generation autonomous driving datasets must be\nmultimodal, incorporating data from advanced sensors that feature extensive\ndata coverage, detailed annotations, and diverse scene representation. To\naddress this need, we present OmniHD-Scenes, a large-scale multimodal dataset\nthat provides comprehensive omnidirectional high-definition data. The\nOmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six\n4D imaging radar systems to achieve full environmental perception. The dataset\ncomprises 1501 clips, each approximately 30-s long, totaling more than 450K\nsynchronized frames and more than 5.85 million synchronized sensor data points.\nWe also propose a novel 4D annotation pipeline. To date, we have annotated 200\nclips with more than 514K precise 3D bounding boxes. These clips also include\nsemantic segmentation annotations for static scene elements. Additionally, we\nintroduce a novel automated pipeline for generation of the dense occupancy\nground truth, which effectively leverages information from non-key frames.\nAlongside the proposed dataset, we establish comprehensive evaluation metrics,\nbaseline models, and benchmarks for 3D detection and semantic occupancy\nprediction. These benchmarks utilize surround-view cameras and 4D imaging radar\nto explore cost-effective sensor solutions for autonomous driving applications.\nExtensive experiments demonstrate the effectiveness of our low-cost sensor\nconfiguration and its robustness under adverse conditions. Data will be\nreleased at https://www.2077ai.com/OmniHD-Scenes.\n", "link": "http://arxiv.org/abs/2412.10734v2", "date": "2024-12-24", "relevancy": 2.9887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniHD-Scenes%3A%20A%20Next-Generation%20Multimodal%20Dataset%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20OmniHD-Scenes%3A%20A%20Next-Generation%20Multimodal%20Dataset%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Lianqing%20Zheng%20and%20Long%20Yang%20and%20Qunshu%20Lin%20and%20Wenjin%20Ai%20and%20Minghao%20Liu%20and%20Shouyi%20Lu%20and%20Jianan%20Liu%20and%20Hongze%20Ren%20and%20Jingyue%20Mo%20and%20Xiaokai%20Bai%20and%20Jie%20Bai%20and%20Zhixiong%20Ma%20and%20Xichan%20Zhu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20deep%20learning%20has%20intensified%20the%20need%20for%0Acomprehensive%20data%20for%20use%20by%20autonomous%20driving%20algorithms.%20High-quality%0Adatasets%20are%20crucial%20for%20the%20development%20of%20effective%20data-driven%20autonomous%0Adriving%20solutions.%20Next-generation%20autonomous%20driving%20datasets%20must%20be%0Amultimodal%2C%20incorporating%20data%20from%20advanced%20sensors%20that%20feature%20extensive%0Adata%20coverage%2C%20detailed%20annotations%2C%20and%20diverse%20scene%20representation.%20To%0Aaddress%20this%20need%2C%20we%20present%20OmniHD-Scenes%2C%20a%20large-scale%20multimodal%20dataset%0Athat%20provides%20comprehensive%20omnidirectional%20high-definition%20data.%20The%0AOmniHD-Scenes%20dataset%20combines%20data%20from%20128-beam%20LiDAR%2C%20six%20cameras%2C%20and%20six%0A4D%20imaging%20radar%20systems%20to%20achieve%20full%20environmental%20perception.%20The%20dataset%0Acomprises%201501%20clips%2C%20each%20approximately%2030-s%20long%2C%20totaling%20more%20than%20450K%0Asynchronized%20frames%20and%20more%20than%205.85%20million%20synchronized%20sensor%20data%20points.%0AWe%20also%20propose%20a%20novel%204D%20annotation%20pipeline.%20To%20date%2C%20we%20have%20annotated%20200%0Aclips%20with%20more%20than%20514K%20precise%203D%20bounding%20boxes.%20These%20clips%20also%20include%0Asemantic%20segmentation%20annotations%20for%20static%20scene%20elements.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20automated%20pipeline%20for%20generation%20of%20the%20dense%20occupancy%0Aground%20truth%2C%20which%20effectively%20leverages%20information%20from%20non-key%20frames.%0AAlongside%20the%20proposed%20dataset%2C%20we%20establish%20comprehensive%20evaluation%20metrics%2C%0Abaseline%20models%2C%20and%20benchmarks%20for%203D%20detection%20and%20semantic%20occupancy%0Aprediction.%20These%20benchmarks%20utilize%20surround-view%20cameras%20and%204D%20imaging%20radar%0Ato%20explore%20cost-effective%20sensor%20solutions%20for%20autonomous%20driving%20applications.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20low-cost%20sensor%0Aconfiguration%20and%20its%20robustness%20under%20adverse%20conditions.%20Data%20will%20be%0Areleased%20at%20https%3A//www.2077ai.com/OmniHD-Scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniHD-Scenes%253A%2520A%2520Next-Generation%2520Multimodal%2520Dataset%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DLianqing%2520Zheng%2520and%2520Long%2520Yang%2520and%2520Qunshu%2520Lin%2520and%2520Wenjin%2520Ai%2520and%2520Minghao%2520Liu%2520and%2520Shouyi%2520Lu%2520and%2520Jianan%2520Liu%2520and%2520Hongze%2520Ren%2520and%2520Jingyue%2520Mo%2520and%2520Xiaokai%2520Bai%2520and%2520Jie%2520Bai%2520and%2520Zhixiong%2520Ma%2520and%2520Xichan%2520Zhu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520deep%2520learning%2520has%2520intensified%2520the%2520need%2520for%250Acomprehensive%2520data%2520for%2520use%2520by%2520autonomous%2520driving%2520algorithms.%2520High-quality%250Adatasets%2520are%2520crucial%2520for%2520the%2520development%2520of%2520effective%2520data-driven%2520autonomous%250Adriving%2520solutions.%2520Next-generation%2520autonomous%2520driving%2520datasets%2520must%2520be%250Amultimodal%252C%2520incorporating%2520data%2520from%2520advanced%2520sensors%2520that%2520feature%2520extensive%250Adata%2520coverage%252C%2520detailed%2520annotations%252C%2520and%2520diverse%2520scene%2520representation.%2520To%250Aaddress%2520this%2520need%252C%2520we%2520present%2520OmniHD-Scenes%252C%2520a%2520large-scale%2520multimodal%2520dataset%250Athat%2520provides%2520comprehensive%2520omnidirectional%2520high-definition%2520data.%2520The%250AOmniHD-Scenes%2520dataset%2520combines%2520data%2520from%2520128-beam%2520LiDAR%252C%2520six%2520cameras%252C%2520and%2520six%250A4D%2520imaging%2520radar%2520systems%2520to%2520achieve%2520full%2520environmental%2520perception.%2520The%2520dataset%250Acomprises%25201501%2520clips%252C%2520each%2520approximately%252030-s%2520long%252C%2520totaling%2520more%2520than%2520450K%250Asynchronized%2520frames%2520and%2520more%2520than%25205.85%2520million%2520synchronized%2520sensor%2520data%2520points.%250AWe%2520also%2520propose%2520a%2520novel%25204D%2520annotation%2520pipeline.%2520To%2520date%252C%2520we%2520have%2520annotated%2520200%250Aclips%2520with%2520more%2520than%2520514K%2520precise%25203D%2520bounding%2520boxes.%2520These%2520clips%2520also%2520include%250Asemantic%2520segmentation%2520annotations%2520for%2520static%2520scene%2520elements.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520novel%2520automated%2520pipeline%2520for%2520generation%2520of%2520the%2520dense%2520occupancy%250Aground%2520truth%252C%2520which%2520effectively%2520leverages%2520information%2520from%2520non-key%2520frames.%250AAlongside%2520the%2520proposed%2520dataset%252C%2520we%2520establish%2520comprehensive%2520evaluation%2520metrics%252C%250Abaseline%2520models%252C%2520and%2520benchmarks%2520for%25203D%2520detection%2520and%2520semantic%2520occupancy%250Aprediction.%2520These%2520benchmarks%2520utilize%2520surround-view%2520cameras%2520and%25204D%2520imaging%2520radar%250Ato%2520explore%2520cost-effective%2520sensor%2520solutions%2520for%2520autonomous%2520driving%2520applications.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520low-cost%2520sensor%250Aconfiguration%2520and%2520its%2520robustness%2520under%2520adverse%2520conditions.%2520Data%2520will%2520be%250Areleased%2520at%2520https%253A//www.2077ai.com/OmniHD-Scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniHD-Scenes%3A%20A%20Next-Generation%20Multimodal%20Dataset%20for%20Autonomous%0A%20%20Driving&entry.906535625=Lianqing%20Zheng%20and%20Long%20Yang%20and%20Qunshu%20Lin%20and%20Wenjin%20Ai%20and%20Minghao%20Liu%20and%20Shouyi%20Lu%20and%20Jianan%20Liu%20and%20Hongze%20Ren%20and%20Jingyue%20Mo%20and%20Xiaokai%20Bai%20and%20Jie%20Bai%20and%20Zhixiong%20Ma%20and%20Xichan%20Zhu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20deep%20learning%20has%20intensified%20the%20need%20for%0Acomprehensive%20data%20for%20use%20by%20autonomous%20driving%20algorithms.%20High-quality%0Adatasets%20are%20crucial%20for%20the%20development%20of%20effective%20data-driven%20autonomous%0Adriving%20solutions.%20Next-generation%20autonomous%20driving%20datasets%20must%20be%0Amultimodal%2C%20incorporating%20data%20from%20advanced%20sensors%20that%20feature%20extensive%0Adata%20coverage%2C%20detailed%20annotations%2C%20and%20diverse%20scene%20representation.%20To%0Aaddress%20this%20need%2C%20we%20present%20OmniHD-Scenes%2C%20a%20large-scale%20multimodal%20dataset%0Athat%20provides%20comprehensive%20omnidirectional%20high-definition%20data.%20The%0AOmniHD-Scenes%20dataset%20combines%20data%20from%20128-beam%20LiDAR%2C%20six%20cameras%2C%20and%20six%0A4D%20imaging%20radar%20systems%20to%20achieve%20full%20environmental%20perception.%20The%20dataset%0Acomprises%201501%20clips%2C%20each%20approximately%2030-s%20long%2C%20totaling%20more%20than%20450K%0Asynchronized%20frames%20and%20more%20than%205.85%20million%20synchronized%20sensor%20data%20points.%0AWe%20also%20propose%20a%20novel%204D%20annotation%20pipeline.%20To%20date%2C%20we%20have%20annotated%20200%0Aclips%20with%20more%20than%20514K%20precise%203D%20bounding%20boxes.%20These%20clips%20also%20include%0Asemantic%20segmentation%20annotations%20for%20static%20scene%20elements.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20automated%20pipeline%20for%20generation%20of%20the%20dense%20occupancy%0Aground%20truth%2C%20which%20effectively%20leverages%20information%20from%20non-key%20frames.%0AAlongside%20the%20proposed%20dataset%2C%20we%20establish%20comprehensive%20evaluation%20metrics%2C%0Abaseline%20models%2C%20and%20benchmarks%20for%203D%20detection%20and%20semantic%20occupancy%0Aprediction.%20These%20benchmarks%20utilize%20surround-view%20cameras%20and%204D%20imaging%20radar%0Ato%20explore%20cost-effective%20sensor%20solutions%20for%20autonomous%20driving%20applications.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20low-cost%20sensor%0Aconfiguration%20and%20its%20robustness%20under%20adverse%20conditions.%20Data%20will%20be%0Areleased%20at%20https%3A//www.2077ai.com/OmniHD-Scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10734v2&entry.124074799=Read"},
{"title": "CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With\n  Multimodal Information", "author": "Kaifan Zhang and Lihuo He and Xin Jiang and Wen Lu and Di Wang and Xinbo Gao", "abstract": "  Electroencephalogram (EEG) signals have attracted significant attention from\nresearchers due to their non-invasive nature and high temporal sensitivity in\ndecoding visual stimuli. However, most recent studies have focused solely on\nthe relationship between EEG and image data pairs, neglecting the valuable\n``beyond-image-modality\" information embedded in EEG signals. This results in\nthe loss of critical multimodal information in EEG. To address this limitation,\nwe propose CognitionCapturer, a unified framework that fully leverages\nmultimodal data to represent EEG signals. Specifically, CognitionCapturer\ntrains Modality Expert Encoders for each modality to extract cross-modal\ninformation from the EEG modality. Then, it introduces a diffusion prior to map\nthe EEG embedding space to the CLIP embedding space, followed by using a\npretrained generative model, the proposed framework can reconstruct visual\nstimuli with high semantic and structural fidelity. Notably, the framework does\nnot require any fine-tuning of the generative models and can be extended to\nincorporate more modalities. Through extensive experiments, we demonstrate that\nCognitionCapturer outperforms state-of-the-art methods both qualitatively and\nquantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.\n", "link": "http://arxiv.org/abs/2412.10489v2", "date": "2024-12-24", "relevancy": 2.9168, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CognitionCapturer%3A%20Decoding%20Visual%20Stimuli%20From%20Human%20EEG%20Signal%20With%0A%20%20Multimodal%20Information&body=Title%3A%20CognitionCapturer%3A%20Decoding%20Visual%20Stimuli%20From%20Human%20EEG%20Signal%20With%0A%20%20Multimodal%20Information%0AAuthor%3A%20Kaifan%20Zhang%20and%20Lihuo%20He%20and%20Xin%20Jiang%20and%20Wen%20Lu%20and%20Di%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Electroencephalogram%20%28EEG%29%20signals%20have%20attracted%20significant%20attention%20from%0Aresearchers%20due%20to%20their%20non-invasive%20nature%20and%20high%20temporal%20sensitivity%20in%0Adecoding%20visual%20stimuli.%20However%2C%20most%20recent%20studies%20have%20focused%20solely%20on%0Athe%20relationship%20between%20EEG%20and%20image%20data%20pairs%2C%20neglecting%20the%20valuable%0A%60%60beyond-image-modality%22%20information%20embedded%20in%20EEG%20signals.%20This%20results%20in%0Athe%20loss%20of%20critical%20multimodal%20information%20in%20EEG.%20To%20address%20this%20limitation%2C%0Awe%20propose%20CognitionCapturer%2C%20a%20unified%20framework%20that%20fully%20leverages%0Amultimodal%20data%20to%20represent%20EEG%20signals.%20Specifically%2C%20CognitionCapturer%0Atrains%20Modality%20Expert%20Encoders%20for%20each%20modality%20to%20extract%20cross-modal%0Ainformation%20from%20the%20EEG%20modality.%20Then%2C%20it%20introduces%20a%20diffusion%20prior%20to%20map%0Athe%20EEG%20embedding%20space%20to%20the%20CLIP%20embedding%20space%2C%20followed%20by%20using%20a%0Apretrained%20generative%20model%2C%20the%20proposed%20framework%20can%20reconstruct%20visual%0Astimuli%20with%20high%20semantic%20and%20structural%20fidelity.%20Notably%2C%20the%20framework%20does%0Anot%20require%20any%20fine-tuning%20of%20the%20generative%20models%20and%20can%20be%20extended%20to%0Aincorporate%20more%20modalities.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACognitionCapturer%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20Code%3A%20https%3A//github.com/XiaoZhangYES/CognitionCapturer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitionCapturer%253A%2520Decoding%2520Visual%2520Stimuli%2520From%2520Human%2520EEG%2520Signal%2520With%250A%2520%2520Multimodal%2520Information%26entry.906535625%3DKaifan%2520Zhang%2520and%2520Lihuo%2520He%2520and%2520Xin%2520Jiang%2520and%2520Wen%2520Lu%2520and%2520Di%2520Wang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Electroencephalogram%2520%2528EEG%2529%2520signals%2520have%2520attracted%2520significant%2520attention%2520from%250Aresearchers%2520due%2520to%2520their%2520non-invasive%2520nature%2520and%2520high%2520temporal%2520sensitivity%2520in%250Adecoding%2520visual%2520stimuli.%2520However%252C%2520most%2520recent%2520studies%2520have%2520focused%2520solely%2520on%250Athe%2520relationship%2520between%2520EEG%2520and%2520image%2520data%2520pairs%252C%2520neglecting%2520the%2520valuable%250A%2560%2560beyond-image-modality%2522%2520information%2520embedded%2520in%2520EEG%2520signals.%2520This%2520results%2520in%250Athe%2520loss%2520of%2520critical%2520multimodal%2520information%2520in%2520EEG.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520CognitionCapturer%252C%2520a%2520unified%2520framework%2520that%2520fully%2520leverages%250Amultimodal%2520data%2520to%2520represent%2520EEG%2520signals.%2520Specifically%252C%2520CognitionCapturer%250Atrains%2520Modality%2520Expert%2520Encoders%2520for%2520each%2520modality%2520to%2520extract%2520cross-modal%250Ainformation%2520from%2520the%2520EEG%2520modality.%2520Then%252C%2520it%2520introduces%2520a%2520diffusion%2520prior%2520to%2520map%250Athe%2520EEG%2520embedding%2520space%2520to%2520the%2520CLIP%2520embedding%2520space%252C%2520followed%2520by%2520using%2520a%250Apretrained%2520generative%2520model%252C%2520the%2520proposed%2520framework%2520can%2520reconstruct%2520visual%250Astimuli%2520with%2520high%2520semantic%2520and%2520structural%2520fidelity.%2520Notably%252C%2520the%2520framework%2520does%250Anot%2520require%2520any%2520fine-tuning%2520of%2520the%2520generative%2520models%2520and%2520can%2520be%2520extended%2520to%250Aincorporate%2520more%2520modalities.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%250ACognitionCapturer%2520outperforms%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%250Aquantitatively.%2520Code%253A%2520https%253A//github.com/XiaoZhangYES/CognitionCapturer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CognitionCapturer%3A%20Decoding%20Visual%20Stimuli%20From%20Human%20EEG%20Signal%20With%0A%20%20Multimodal%20Information&entry.906535625=Kaifan%20Zhang%20and%20Lihuo%20He%20and%20Xin%20Jiang%20and%20Wen%20Lu%20and%20Di%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Electroencephalogram%20%28EEG%29%20signals%20have%20attracted%20significant%20attention%20from%0Aresearchers%20due%20to%20their%20non-invasive%20nature%20and%20high%20temporal%20sensitivity%20in%0Adecoding%20visual%20stimuli.%20However%2C%20most%20recent%20studies%20have%20focused%20solely%20on%0Athe%20relationship%20between%20EEG%20and%20image%20data%20pairs%2C%20neglecting%20the%20valuable%0A%60%60beyond-image-modality%22%20information%20embedded%20in%20EEG%20signals.%20This%20results%20in%0Athe%20loss%20of%20critical%20multimodal%20information%20in%20EEG.%20To%20address%20this%20limitation%2C%0Awe%20propose%20CognitionCapturer%2C%20a%20unified%20framework%20that%20fully%20leverages%0Amultimodal%20data%20to%20represent%20EEG%20signals.%20Specifically%2C%20CognitionCapturer%0Atrains%20Modality%20Expert%20Encoders%20for%20each%20modality%20to%20extract%20cross-modal%0Ainformation%20from%20the%20EEG%20modality.%20Then%2C%20it%20introduces%20a%20diffusion%20prior%20to%20map%0Athe%20EEG%20embedding%20space%20to%20the%20CLIP%20embedding%20space%2C%20followed%20by%20using%20a%0Apretrained%20generative%20model%2C%20the%20proposed%20framework%20can%20reconstruct%20visual%0Astimuli%20with%20high%20semantic%20and%20structural%20fidelity.%20Notably%2C%20the%20framework%20does%0Anot%20require%20any%20fine-tuning%20of%20the%20generative%20models%20and%20can%20be%20extended%20to%0Aincorporate%20more%20modalities.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACognitionCapturer%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20Code%3A%20https%3A//github.com/XiaoZhangYES/CognitionCapturer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10489v2&entry.124074799=Read"},
{"title": "Extract Free Dense Misalignment from CLIP", "author": "JeongYeon Nam and Jinbae Im and Wonjae Kim and Taeho Kil", "abstract": "  Recent vision-language foundation models still frequently produce outputs\nmisaligned with their inputs, evidenced by object hallucination in captioning\nand prompt misalignment in the text-to-image generation model. Recent studies\nhave explored methods for identifying misaligned elements, aiming not only to\nenhance interpretability but also to improve model performance. However,\ncurrent approaches primarily rely on large foundation models in a zero-shot\nmanner or fine-tuned models with human annotations, which limits scalability\ndue to significant computational costs. This work proposes a novel approach,\ndubbed CLIP4DM, for detecting dense misalignments from pre-trained CLIP,\nspecifically focusing on pinpointing misaligned words between image and text.\nWe carefully revamp the gradient-based attribution computation method, enabling\nnegative gradient of individual text tokens to indicate misalignment. We also\npropose F-CLIPScore, which aggregates misaligned attributions with a global\nalignment score. We evaluate our method on various dense misalignment detection\nbenchmarks, covering various image and text domains and misalignment types. Our\nmethod demonstrates state-of-the-art performance among zero-shot models and\ncompetitive performance with fine-tuned models while maintaining superior\nefficiency. Our qualitative examples show that our method has a unique strength\nto detect entity-level objects, intangible objects, and attributes that can not\nbe easily detected for existing works. We conduct ablation studies and analyses\nto highlight the strengths and limitations of our approach. Our code is\npublicly available at https://github.com/naver-ai/CLIP4DM.\n", "link": "http://arxiv.org/abs/2412.18404v1", "date": "2024-12-24", "relevancy": 2.8748, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extract%20Free%20Dense%20Misalignment%20from%20CLIP&body=Title%3A%20Extract%20Free%20Dense%20Misalignment%20from%20CLIP%0AAuthor%3A%20JeongYeon%20Nam%20and%20Jinbae%20Im%20and%20Wonjae%20Kim%20and%20Taeho%20Kil%0AAbstract%3A%20%20%20Recent%20vision-language%20foundation%20models%20still%20frequently%20produce%20outputs%0Amisaligned%20with%20their%20inputs%2C%20evidenced%20by%20object%20hallucination%20in%20captioning%0Aand%20prompt%20misalignment%20in%20the%20text-to-image%20generation%20model.%20Recent%20studies%0Ahave%20explored%20methods%20for%20identifying%20misaligned%20elements%2C%20aiming%20not%20only%20to%0Aenhance%20interpretability%20but%20also%20to%20improve%20model%20performance.%20However%2C%0Acurrent%20approaches%20primarily%20rely%20on%20large%20foundation%20models%20in%20a%20zero-shot%0Amanner%20or%20fine-tuned%20models%20with%20human%20annotations%2C%20which%20limits%20scalability%0Adue%20to%20significant%20computational%20costs.%20This%20work%20proposes%20a%20novel%20approach%2C%0Adubbed%20CLIP4DM%2C%20for%20detecting%20dense%20misalignments%20from%20pre-trained%20CLIP%2C%0Aspecifically%20focusing%20on%20pinpointing%20misaligned%20words%20between%20image%20and%20text.%0AWe%20carefully%20revamp%20the%20gradient-based%20attribution%20computation%20method%2C%20enabling%0Anegative%20gradient%20of%20individual%20text%20tokens%20to%20indicate%20misalignment.%20We%20also%0Apropose%20F-CLIPScore%2C%20which%20aggregates%20misaligned%20attributions%20with%20a%20global%0Aalignment%20score.%20We%20evaluate%20our%20method%20on%20various%20dense%20misalignment%20detection%0Abenchmarks%2C%20covering%20various%20image%20and%20text%20domains%20and%20misalignment%20types.%20Our%0Amethod%20demonstrates%20state-of-the-art%20performance%20among%20zero-shot%20models%20and%0Acompetitive%20performance%20with%20fine-tuned%20models%20while%20maintaining%20superior%0Aefficiency.%20Our%20qualitative%20examples%20show%20that%20our%20method%20has%20a%20unique%20strength%0Ato%20detect%20entity-level%20objects%2C%20intangible%20objects%2C%20and%20attributes%20that%20can%20not%0Abe%20easily%20detected%20for%20existing%20works.%20We%20conduct%20ablation%20studies%20and%20analyses%0Ato%20highlight%20the%20strengths%20and%20limitations%20of%20our%20approach.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/naver-ai/CLIP4DM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtract%2520Free%2520Dense%2520Misalignment%2520from%2520CLIP%26entry.906535625%3DJeongYeon%2520Nam%2520and%2520Jinbae%2520Im%2520and%2520Wonjae%2520Kim%2520and%2520Taeho%2520Kil%26entry.1292438233%3D%2520%2520Recent%2520vision-language%2520foundation%2520models%2520still%2520frequently%2520produce%2520outputs%250Amisaligned%2520with%2520their%2520inputs%252C%2520evidenced%2520by%2520object%2520hallucination%2520in%2520captioning%250Aand%2520prompt%2520misalignment%2520in%2520the%2520text-to-image%2520generation%2520model.%2520Recent%2520studies%250Ahave%2520explored%2520methods%2520for%2520identifying%2520misaligned%2520elements%252C%2520aiming%2520not%2520only%2520to%250Aenhance%2520interpretability%2520but%2520also%2520to%2520improve%2520model%2520performance.%2520However%252C%250Acurrent%2520approaches%2520primarily%2520rely%2520on%2520large%2520foundation%2520models%2520in%2520a%2520zero-shot%250Amanner%2520or%2520fine-tuned%2520models%2520with%2520human%2520annotations%252C%2520which%2520limits%2520scalability%250Adue%2520to%2520significant%2520computational%2520costs.%2520This%2520work%2520proposes%2520a%2520novel%2520approach%252C%250Adubbed%2520CLIP4DM%252C%2520for%2520detecting%2520dense%2520misalignments%2520from%2520pre-trained%2520CLIP%252C%250Aspecifically%2520focusing%2520on%2520pinpointing%2520misaligned%2520words%2520between%2520image%2520and%2520text.%250AWe%2520carefully%2520revamp%2520the%2520gradient-based%2520attribution%2520computation%2520method%252C%2520enabling%250Anegative%2520gradient%2520of%2520individual%2520text%2520tokens%2520to%2520indicate%2520misalignment.%2520We%2520also%250Apropose%2520F-CLIPScore%252C%2520which%2520aggregates%2520misaligned%2520attributions%2520with%2520a%2520global%250Aalignment%2520score.%2520We%2520evaluate%2520our%2520method%2520on%2520various%2520dense%2520misalignment%2520detection%250Abenchmarks%252C%2520covering%2520various%2520image%2520and%2520text%2520domains%2520and%2520misalignment%2520types.%2520Our%250Amethod%2520demonstrates%2520state-of-the-art%2520performance%2520among%2520zero-shot%2520models%2520and%250Acompetitive%2520performance%2520with%2520fine-tuned%2520models%2520while%2520maintaining%2520superior%250Aefficiency.%2520Our%2520qualitative%2520examples%2520show%2520that%2520our%2520method%2520has%2520a%2520unique%2520strength%250Ato%2520detect%2520entity-level%2520objects%252C%2520intangible%2520objects%252C%2520and%2520attributes%2520that%2520can%2520not%250Abe%2520easily%2520detected%2520for%2520existing%2520works.%2520We%2520conduct%2520ablation%2520studies%2520and%2520analyses%250Ato%2520highlight%2520the%2520strengths%2520and%2520limitations%2520of%2520our%2520approach.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/naver-ai/CLIP4DM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extract%20Free%20Dense%20Misalignment%20from%20CLIP&entry.906535625=JeongYeon%20Nam%20and%20Jinbae%20Im%20and%20Wonjae%20Kim%20and%20Taeho%20Kil&entry.1292438233=%20%20Recent%20vision-language%20foundation%20models%20still%20frequently%20produce%20outputs%0Amisaligned%20with%20their%20inputs%2C%20evidenced%20by%20object%20hallucination%20in%20captioning%0Aand%20prompt%20misalignment%20in%20the%20text-to-image%20generation%20model.%20Recent%20studies%0Ahave%20explored%20methods%20for%20identifying%20misaligned%20elements%2C%20aiming%20not%20only%20to%0Aenhance%20interpretability%20but%20also%20to%20improve%20model%20performance.%20However%2C%0Acurrent%20approaches%20primarily%20rely%20on%20large%20foundation%20models%20in%20a%20zero-shot%0Amanner%20or%20fine-tuned%20models%20with%20human%20annotations%2C%20which%20limits%20scalability%0Adue%20to%20significant%20computational%20costs.%20This%20work%20proposes%20a%20novel%20approach%2C%0Adubbed%20CLIP4DM%2C%20for%20detecting%20dense%20misalignments%20from%20pre-trained%20CLIP%2C%0Aspecifically%20focusing%20on%20pinpointing%20misaligned%20words%20between%20image%20and%20text.%0AWe%20carefully%20revamp%20the%20gradient-based%20attribution%20computation%20method%2C%20enabling%0Anegative%20gradient%20of%20individual%20text%20tokens%20to%20indicate%20misalignment.%20We%20also%0Apropose%20F-CLIPScore%2C%20which%20aggregates%20misaligned%20attributions%20with%20a%20global%0Aalignment%20score.%20We%20evaluate%20our%20method%20on%20various%20dense%20misalignment%20detection%0Abenchmarks%2C%20covering%20various%20image%20and%20text%20domains%20and%20misalignment%20types.%20Our%0Amethod%20demonstrates%20state-of-the-art%20performance%20among%20zero-shot%20models%20and%0Acompetitive%20performance%20with%20fine-tuned%20models%20while%20maintaining%20superior%0Aefficiency.%20Our%20qualitative%20examples%20show%20that%20our%20method%20has%20a%20unique%20strength%0Ato%20detect%20entity-level%20objects%2C%20intangible%20objects%2C%20and%20attributes%20that%20can%20not%0Abe%20easily%20detected%20for%20existing%20works.%20We%20conduct%20ablation%20studies%20and%20analyses%0Ato%20highlight%20the%20strengths%20and%20limitations%20of%20our%20approach.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/naver-ai/CLIP4DM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18404v1&entry.124074799=Read"},
{"title": "The Key of Understanding Vision Tasks: Explanatory Instructions", "author": "Yang Shen and Xiu-Shen Wei and Yifan Sun and Yuxin Song and Tao Yuan and Jian Jin and Heyang Xu and Yazhou Yao and Errui Ding", "abstract": "  Computer Vision (CV) has yet to fully achieve the zero-shot task\ngeneralization observed in Natural Language Processing (NLP), despite following\nmany of the milestones established in NLP, such as large transformer models,\nextensive pre-training, and the auto-regression paradigm, among others. In this\npaper, we explore the idea that CV adopts discrete and terminological task\ndefinitions (\\eg, ``image segmentation''), which may be a key barrier to\nzero-shot task generalization. Our hypothesis is that without truly\nunderstanding previously-seen tasks--due to these terminological\ndefinitions--deep models struggle to generalize to novel tasks. To verify this,\nwe introduce Explanatory Instructions, which provide an intuitive way to define\nCV task objectives through detailed linguistic transformations from input\nimages to outputs. We create a large-scale dataset comprising 12 million\n``image input $\\to$ explanatory instruction $\\to$ output'' triplets, and train\nan auto-regressive-based vision-language model (AR-based VLM) that takes both\nimages and explanatory instructions as input. By learning to follow these\ninstructions, the AR-based VLM achieves instruction-level zero-shot\ncapabilities for previously-seen tasks and demonstrates strong zero-shot\ngeneralization for unseen CV tasks. Code and dataset will be openly available\non our GitHub repository.\n", "link": "http://arxiv.org/abs/2412.18525v1", "date": "2024-12-24", "relevancy": 2.8737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Key%20of%20Understanding%20Vision%20Tasks%3A%20Explanatory%20Instructions&body=Title%3A%20The%20Key%20of%20Understanding%20Vision%20Tasks%3A%20Explanatory%20Instructions%0AAuthor%3A%20Yang%20Shen%20and%20Xiu-Shen%20Wei%20and%20Yifan%20Sun%20and%20Yuxin%20Song%20and%20Tao%20Yuan%20and%20Jian%20Jin%20and%20Heyang%20Xu%20and%20Yazhou%20Yao%20and%20Errui%20Ding%0AAbstract%3A%20%20%20Computer%20Vision%20%28CV%29%20has%20yet%20to%20fully%20achieve%20the%20zero-shot%20task%0Ageneralization%20observed%20in%20Natural%20Language%20Processing%20%28NLP%29%2C%20despite%20following%0Amany%20of%20the%20milestones%20established%20in%20NLP%2C%20such%20as%20large%20transformer%20models%2C%0Aextensive%20pre-training%2C%20and%20the%20auto-regression%20paradigm%2C%20among%20others.%20In%20this%0Apaper%2C%20we%20explore%20the%20idea%20that%20CV%20adopts%20discrete%20and%20terminological%20task%0Adefinitions%20%28%5Ceg%2C%20%60%60image%20segmentation%27%27%29%2C%20which%20may%20be%20a%20key%20barrier%20to%0Azero-shot%20task%20generalization.%20Our%20hypothesis%20is%20that%20without%20truly%0Aunderstanding%20previously-seen%20tasks--due%20to%20these%20terminological%0Adefinitions--deep%20models%20struggle%20to%20generalize%20to%20novel%20tasks.%20To%20verify%20this%2C%0Awe%20introduce%20Explanatory%20Instructions%2C%20which%20provide%20an%20intuitive%20way%20to%20define%0ACV%20task%20objectives%20through%20detailed%20linguistic%20transformations%20from%20input%0Aimages%20to%20outputs.%20We%20create%20a%20large-scale%20dataset%20comprising%2012%20million%0A%60%60image%20input%20%24%5Cto%24%20explanatory%20instruction%20%24%5Cto%24%20output%27%27%20triplets%2C%20and%20train%0Aan%20auto-regressive-based%20vision-language%20model%20%28AR-based%20VLM%29%20that%20takes%20both%0Aimages%20and%20explanatory%20instructions%20as%20input.%20By%20learning%20to%20follow%20these%0Ainstructions%2C%20the%20AR-based%20VLM%20achieves%20instruction-level%20zero-shot%0Acapabilities%20for%20previously-seen%20tasks%20and%20demonstrates%20strong%20zero-shot%0Ageneralization%20for%20unseen%20CV%20tasks.%20Code%20and%20dataset%20will%20be%20openly%20available%0Aon%20our%20GitHub%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Key%2520of%2520Understanding%2520Vision%2520Tasks%253A%2520Explanatory%2520Instructions%26entry.906535625%3DYang%2520Shen%2520and%2520Xiu-Shen%2520Wei%2520and%2520Yifan%2520Sun%2520and%2520Yuxin%2520Song%2520and%2520Tao%2520Yuan%2520and%2520Jian%2520Jin%2520and%2520Heyang%2520Xu%2520and%2520Yazhou%2520Yao%2520and%2520Errui%2520Ding%26entry.1292438233%3D%2520%2520Computer%2520Vision%2520%2528CV%2529%2520has%2520yet%2520to%2520fully%2520achieve%2520the%2520zero-shot%2520task%250Ageneralization%2520observed%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%2520despite%2520following%250Amany%2520of%2520the%2520milestones%2520established%2520in%2520NLP%252C%2520such%2520as%2520large%2520transformer%2520models%252C%250Aextensive%2520pre-training%252C%2520and%2520the%2520auto-regression%2520paradigm%252C%2520among%2520others.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520the%2520idea%2520that%2520CV%2520adopts%2520discrete%2520and%2520terminological%2520task%250Adefinitions%2520%2528%255Ceg%252C%2520%2560%2560image%2520segmentation%2527%2527%2529%252C%2520which%2520may%2520be%2520a%2520key%2520barrier%2520to%250Azero-shot%2520task%2520generalization.%2520Our%2520hypothesis%2520is%2520that%2520without%2520truly%250Aunderstanding%2520previously-seen%2520tasks--due%2520to%2520these%2520terminological%250Adefinitions--deep%2520models%2520struggle%2520to%2520generalize%2520to%2520novel%2520tasks.%2520To%2520verify%2520this%252C%250Awe%2520introduce%2520Explanatory%2520Instructions%252C%2520which%2520provide%2520an%2520intuitive%2520way%2520to%2520define%250ACV%2520task%2520objectives%2520through%2520detailed%2520linguistic%2520transformations%2520from%2520input%250Aimages%2520to%2520outputs.%2520We%2520create%2520a%2520large-scale%2520dataset%2520comprising%252012%2520million%250A%2560%2560image%2520input%2520%2524%255Cto%2524%2520explanatory%2520instruction%2520%2524%255Cto%2524%2520output%2527%2527%2520triplets%252C%2520and%2520train%250Aan%2520auto-regressive-based%2520vision-language%2520model%2520%2528AR-based%2520VLM%2529%2520that%2520takes%2520both%250Aimages%2520and%2520explanatory%2520instructions%2520as%2520input.%2520By%2520learning%2520to%2520follow%2520these%250Ainstructions%252C%2520the%2520AR-based%2520VLM%2520achieves%2520instruction-level%2520zero-shot%250Acapabilities%2520for%2520previously-seen%2520tasks%2520and%2520demonstrates%2520strong%2520zero-shot%250Ageneralization%2520for%2520unseen%2520CV%2520tasks.%2520Code%2520and%2520dataset%2520will%2520be%2520openly%2520available%250Aon%2520our%2520GitHub%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Key%20of%20Understanding%20Vision%20Tasks%3A%20Explanatory%20Instructions&entry.906535625=Yang%20Shen%20and%20Xiu-Shen%20Wei%20and%20Yifan%20Sun%20and%20Yuxin%20Song%20and%20Tao%20Yuan%20and%20Jian%20Jin%20and%20Heyang%20Xu%20and%20Yazhou%20Yao%20and%20Errui%20Ding&entry.1292438233=%20%20Computer%20Vision%20%28CV%29%20has%20yet%20to%20fully%20achieve%20the%20zero-shot%20task%0Ageneralization%20observed%20in%20Natural%20Language%20Processing%20%28NLP%29%2C%20despite%20following%0Amany%20of%20the%20milestones%20established%20in%20NLP%2C%20such%20as%20large%20transformer%20models%2C%0Aextensive%20pre-training%2C%20and%20the%20auto-regression%20paradigm%2C%20among%20others.%20In%20this%0Apaper%2C%20we%20explore%20the%20idea%20that%20CV%20adopts%20discrete%20and%20terminological%20task%0Adefinitions%20%28%5Ceg%2C%20%60%60image%20segmentation%27%27%29%2C%20which%20may%20be%20a%20key%20barrier%20to%0Azero-shot%20task%20generalization.%20Our%20hypothesis%20is%20that%20without%20truly%0Aunderstanding%20previously-seen%20tasks--due%20to%20these%20terminological%0Adefinitions--deep%20models%20struggle%20to%20generalize%20to%20novel%20tasks.%20To%20verify%20this%2C%0Awe%20introduce%20Explanatory%20Instructions%2C%20which%20provide%20an%20intuitive%20way%20to%20define%0ACV%20task%20objectives%20through%20detailed%20linguistic%20transformations%20from%20input%0Aimages%20to%20outputs.%20We%20create%20a%20large-scale%20dataset%20comprising%2012%20million%0A%60%60image%20input%20%24%5Cto%24%20explanatory%20instruction%20%24%5Cto%24%20output%27%27%20triplets%2C%20and%20train%0Aan%20auto-regressive-based%20vision-language%20model%20%28AR-based%20VLM%29%20that%20takes%20both%0Aimages%20and%20explanatory%20instructions%20as%20input.%20By%20learning%20to%20follow%20these%0Ainstructions%2C%20the%20AR-based%20VLM%20achieves%20instruction-level%20zero-shot%0Acapabilities%20for%20previously-seen%20tasks%20and%20demonstrates%20strong%20zero-shot%0Ageneralization%20for%20unseen%20CV%20tasks.%20Code%20and%20dataset%20will%20be%20openly%20available%0Aon%20our%20GitHub%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18525v1&entry.124074799=Read"},
{"title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View\n  Diffusion Models", "author": "Minghao Chen and Roman Shapovalov and Iro Laina and Tom Monnier and Jianyuan Wang and David Novotny and Andrea Vedaldi", "abstract": "  Text- or image-to-3D generators and 3D scanners can now produce 3D assets\nwith high-quality shapes and textures. These assets typically consist of a\nsingle, fused representation, like an implicit neural field, a Gaussian\nmixture, or a mesh, without any useful structure. However, most applications\nand creative workflows require assets to be made of several meaningful parts\nthat can be manipulated independently. To address this gap, we introduce\nPartGen, a novel approach that generates 3D objects composed of meaningful\nparts starting from text, an image, or an unstructured 3D object. First, given\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\nmodel extracts a set of plausible and view-consistent part segmentations,\ndividing the object into parts. Then, a second multi-view diffusion model takes\neach part separately, fills in the occlusions, and uses those completed views\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\ncompletion process considers the context of the entire object to ensure that\nthe parts integrate cohesively. The generative completion model can make up for\nthe information missing due to occlusions; in extreme cases, it can hallucinate\nentirely invisible parts based on the input 3D asset. We evaluate our method on\ngenerated and real 3D assets and show that it outperforms segmentation and\npart-extraction baselines by a large margin. We also showcase downstream\napplications such as 3D part editing.\n", "link": "http://arxiv.org/abs/2412.18608v1", "date": "2024-12-24", "relevancy": 2.8698, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7331}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7331}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartGen%3A%20Part-level%203D%20Generation%20and%20Reconstruction%20with%20Multi-View%0A%20%20Diffusion%20Models&body=Title%3A%20PartGen%3A%20Part-level%203D%20Generation%20and%20Reconstruction%20with%20Multi-View%0A%20%20Diffusion%20Models%0AAuthor%3A%20Minghao%20Chen%20and%20Roman%20Shapovalov%20and%20Iro%20Laina%20and%20Tom%20Monnier%20and%20Jianyuan%20Wang%20and%20David%20Novotny%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20Text-%20or%20image-to-3D%20generators%20and%203D%20scanners%20can%20now%20produce%203D%20assets%0Awith%20high-quality%20shapes%20and%20textures.%20These%20assets%20typically%20consist%20of%20a%0Asingle%2C%20fused%20representation%2C%20like%20an%20implicit%20neural%20field%2C%20a%20Gaussian%0Amixture%2C%20or%20a%20mesh%2C%20without%20any%20useful%20structure.%20However%2C%20most%20applications%0Aand%20creative%20workflows%20require%20assets%20to%20be%20made%20of%20several%20meaningful%20parts%0Athat%20can%20be%20manipulated%20independently.%20To%20address%20this%20gap%2C%20we%20introduce%0APartGen%2C%20a%20novel%20approach%20that%20generates%203D%20objects%20composed%20of%20meaningful%0Aparts%20starting%20from%20text%2C%20an%20image%2C%20or%20an%20unstructured%203D%20object.%20First%2C%20given%0Amultiple%20views%20of%20a%203D%20object%2C%20generated%20or%20rendered%2C%20a%20multi-view%20diffusion%0Amodel%20extracts%20a%20set%20of%20plausible%20and%20view-consistent%20part%20segmentations%2C%0Adividing%20the%20object%20into%20parts.%20Then%2C%20a%20second%20multi-view%20diffusion%20model%20takes%0Aeach%20part%20separately%2C%20fills%20in%20the%20occlusions%2C%20and%20uses%20those%20completed%20views%0Afor%203D%20reconstruction%20by%20feeding%20them%20to%20a%203D%20reconstruction%20network.%20This%0Acompletion%20process%20considers%20the%20context%20of%20the%20entire%20object%20to%20ensure%20that%0Athe%20parts%20integrate%20cohesively.%20The%20generative%20completion%20model%20can%20make%20up%20for%0Athe%20information%20missing%20due%20to%20occlusions%3B%20in%20extreme%20cases%2C%20it%20can%20hallucinate%0Aentirely%20invisible%20parts%20based%20on%20the%20input%203D%20asset.%20We%20evaluate%20our%20method%20on%0Agenerated%20and%20real%203D%20assets%20and%20show%20that%20it%20outperforms%20segmentation%20and%0Apart-extraction%20baselines%20by%20a%20large%20margin.%20We%20also%20showcase%20downstream%0Aapplications%20such%20as%203D%20part%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartGen%253A%2520Part-level%25203D%2520Generation%2520and%2520Reconstruction%2520with%2520Multi-View%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DMinghao%2520Chen%2520and%2520Roman%2520Shapovalov%2520and%2520Iro%2520Laina%2520and%2520Tom%2520Monnier%2520and%2520Jianyuan%2520Wang%2520and%2520David%2520Novotny%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520Text-%2520or%2520image-to-3D%2520generators%2520and%25203D%2520scanners%2520can%2520now%2520produce%25203D%2520assets%250Awith%2520high-quality%2520shapes%2520and%2520textures.%2520These%2520assets%2520typically%2520consist%2520of%2520a%250Asingle%252C%2520fused%2520representation%252C%2520like%2520an%2520implicit%2520neural%2520field%252C%2520a%2520Gaussian%250Amixture%252C%2520or%2520a%2520mesh%252C%2520without%2520any%2520useful%2520structure.%2520However%252C%2520most%2520applications%250Aand%2520creative%2520workflows%2520require%2520assets%2520to%2520be%2520made%2520of%2520several%2520meaningful%2520parts%250Athat%2520can%2520be%2520manipulated%2520independently.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250APartGen%252C%2520a%2520novel%2520approach%2520that%2520generates%25203D%2520objects%2520composed%2520of%2520meaningful%250Aparts%2520starting%2520from%2520text%252C%2520an%2520image%252C%2520or%2520an%2520unstructured%25203D%2520object.%2520First%252C%2520given%250Amultiple%2520views%2520of%2520a%25203D%2520object%252C%2520generated%2520or%2520rendered%252C%2520a%2520multi-view%2520diffusion%250Amodel%2520extracts%2520a%2520set%2520of%2520plausible%2520and%2520view-consistent%2520part%2520segmentations%252C%250Adividing%2520the%2520object%2520into%2520parts.%2520Then%252C%2520a%2520second%2520multi-view%2520diffusion%2520model%2520takes%250Aeach%2520part%2520separately%252C%2520fills%2520in%2520the%2520occlusions%252C%2520and%2520uses%2520those%2520completed%2520views%250Afor%25203D%2520reconstruction%2520by%2520feeding%2520them%2520to%2520a%25203D%2520reconstruction%2520network.%2520This%250Acompletion%2520process%2520considers%2520the%2520context%2520of%2520the%2520entire%2520object%2520to%2520ensure%2520that%250Athe%2520parts%2520integrate%2520cohesively.%2520The%2520generative%2520completion%2520model%2520can%2520make%2520up%2520for%250Athe%2520information%2520missing%2520due%2520to%2520occlusions%253B%2520in%2520extreme%2520cases%252C%2520it%2520can%2520hallucinate%250Aentirely%2520invisible%2520parts%2520based%2520on%2520the%2520input%25203D%2520asset.%2520We%2520evaluate%2520our%2520method%2520on%250Agenerated%2520and%2520real%25203D%2520assets%2520and%2520show%2520that%2520it%2520outperforms%2520segmentation%2520and%250Apart-extraction%2520baselines%2520by%2520a%2520large%2520margin.%2520We%2520also%2520showcase%2520downstream%250Aapplications%2520such%2520as%25203D%2520part%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartGen%3A%20Part-level%203D%20Generation%20and%20Reconstruction%20with%20Multi-View%0A%20%20Diffusion%20Models&entry.906535625=Minghao%20Chen%20and%20Roman%20Shapovalov%20and%20Iro%20Laina%20and%20Tom%20Monnier%20and%20Jianyuan%20Wang%20and%20David%20Novotny%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20Text-%20or%20image-to-3D%20generators%20and%203D%20scanners%20can%20now%20produce%203D%20assets%0Awith%20high-quality%20shapes%20and%20textures.%20These%20assets%20typically%20consist%20of%20a%0Asingle%2C%20fused%20representation%2C%20like%20an%20implicit%20neural%20field%2C%20a%20Gaussian%0Amixture%2C%20or%20a%20mesh%2C%20without%20any%20useful%20structure.%20However%2C%20most%20applications%0Aand%20creative%20workflows%20require%20assets%20to%20be%20made%20of%20several%20meaningful%20parts%0Athat%20can%20be%20manipulated%20independently.%20To%20address%20this%20gap%2C%20we%20introduce%0APartGen%2C%20a%20novel%20approach%20that%20generates%203D%20objects%20composed%20of%20meaningful%0Aparts%20starting%20from%20text%2C%20an%20image%2C%20or%20an%20unstructured%203D%20object.%20First%2C%20given%0Amultiple%20views%20of%20a%203D%20object%2C%20generated%20or%20rendered%2C%20a%20multi-view%20diffusion%0Amodel%20extracts%20a%20set%20of%20plausible%20and%20view-consistent%20part%20segmentations%2C%0Adividing%20the%20object%20into%20parts.%20Then%2C%20a%20second%20multi-view%20diffusion%20model%20takes%0Aeach%20part%20separately%2C%20fills%20in%20the%20occlusions%2C%20and%20uses%20those%20completed%20views%0Afor%203D%20reconstruction%20by%20feeding%20them%20to%20a%203D%20reconstruction%20network.%20This%0Acompletion%20process%20considers%20the%20context%20of%20the%20entire%20object%20to%20ensure%20that%0Athe%20parts%20integrate%20cohesively.%20The%20generative%20completion%20model%20can%20make%20up%20for%0Athe%20information%20missing%20due%20to%20occlusions%3B%20in%20extreme%20cases%2C%20it%20can%20hallucinate%0Aentirely%20invisible%20parts%20based%20on%20the%20input%203D%20asset.%20We%20evaluate%20our%20method%20on%0Agenerated%20and%20real%203D%20assets%20and%20show%20that%20it%20outperforms%20segmentation%20and%0Apart-extraction%20baselines%20by%20a%20large%20margin.%20We%20also%20showcase%20downstream%0Aapplications%20such%20as%203D%20part%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18608v1&entry.124074799=Read"},
{"title": "Fashionability-Enhancing Outfit Image Editing with Conditional Diffusion\n  Models", "author": "Qice Qin and Yuki Hirakawa and Ryotaro Shimizu and Takuya Furusawa and Edgar Simo-Serra", "abstract": "  Image generation in the fashion domain has predominantly focused on\npreserving body characteristics or following input prompts, but little\nattention has been paid to improving the inherent fashionability of the output\nimages. This paper presents a novel diffusion model-based approach that\ngenerates fashion images with improved fashionability while maintaining control\nover key attributes. Key components of our method include: 1) fashionability\nenhancement, which ensures that the generated images are more fashionable than\nthe input; 2) preservation of body characteristics, encouraging the generated\nimages to maintain the original shape and proportions of the input; and 3)\nautomatic fashion optimization, which does not rely on manual input or external\nprompts. We also employ two methods to collect training data for guidance while\ngenerating and evaluating the images. In particular, we rate outfit images\nusing fashionability scores annotated by multiple fashion experts through\nOpenSkill-based and five critical aspect-based pairwise comparisons. These\nmethods provide complementary perspectives for assessing and improving the\nfashionability of the generated images. The experimental results show that our\napproach outperforms the baseline Fashion++ in generating images with superior\nfashionability, demonstrating its effectiveness in producing more stylish and\nappealing fashion images.\n", "link": "http://arxiv.org/abs/2412.18421v1", "date": "2024-12-24", "relevancy": 2.8062, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.8059}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7248}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fashionability-Enhancing%20Outfit%20Image%20Editing%20with%20Conditional%20Diffusion%0A%20%20Models&body=Title%3A%20Fashionability-Enhancing%20Outfit%20Image%20Editing%20with%20Conditional%20Diffusion%0A%20%20Models%0AAuthor%3A%20Qice%20Qin%20and%20Yuki%20Hirakawa%20and%20Ryotaro%20Shimizu%20and%20Takuya%20Furusawa%20and%20Edgar%20Simo-Serra%0AAbstract%3A%20%20%20Image%20generation%20in%20the%20fashion%20domain%20has%20predominantly%20focused%20on%0Apreserving%20body%20characteristics%20or%20following%20input%20prompts%2C%20but%20little%0Aattention%20has%20been%20paid%20to%20improving%20the%20inherent%20fashionability%20of%20the%20output%0Aimages.%20This%20paper%20presents%20a%20novel%20diffusion%20model-based%20approach%20that%0Agenerates%20fashion%20images%20with%20improved%20fashionability%20while%20maintaining%20control%0Aover%20key%20attributes.%20Key%20components%20of%20our%20method%20include%3A%201%29%20fashionability%0Aenhancement%2C%20which%20ensures%20that%20the%20generated%20images%20are%20more%20fashionable%20than%0Athe%20input%3B%202%29%20preservation%20of%20body%20characteristics%2C%20encouraging%20the%20generated%0Aimages%20to%20maintain%20the%20original%20shape%20and%20proportions%20of%20the%20input%3B%20and%203%29%0Aautomatic%20fashion%20optimization%2C%20which%20does%20not%20rely%20on%20manual%20input%20or%20external%0Aprompts.%20We%20also%20employ%20two%20methods%20to%20collect%20training%20data%20for%20guidance%20while%0Agenerating%20and%20evaluating%20the%20images.%20In%20particular%2C%20we%20rate%20outfit%20images%0Ausing%20fashionability%20scores%20annotated%20by%20multiple%20fashion%20experts%20through%0AOpenSkill-based%20and%20five%20critical%20aspect-based%20pairwise%20comparisons.%20These%0Amethods%20provide%20complementary%20perspectives%20for%20assessing%20and%20improving%20the%0Afashionability%20of%20the%20generated%20images.%20The%20experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20the%20baseline%20Fashion%2B%2B%20in%20generating%20images%20with%20superior%0Afashionability%2C%20demonstrating%20its%20effectiveness%20in%20producing%20more%20stylish%20and%0Aappealing%20fashion%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionability-Enhancing%2520Outfit%2520Image%2520Editing%2520with%2520Conditional%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DQice%2520Qin%2520and%2520Yuki%2520Hirakawa%2520and%2520Ryotaro%2520Shimizu%2520and%2520Takuya%2520Furusawa%2520and%2520Edgar%2520Simo-Serra%26entry.1292438233%3D%2520%2520Image%2520generation%2520in%2520the%2520fashion%2520domain%2520has%2520predominantly%2520focused%2520on%250Apreserving%2520body%2520characteristics%2520or%2520following%2520input%2520prompts%252C%2520but%2520little%250Aattention%2520has%2520been%2520paid%2520to%2520improving%2520the%2520inherent%2520fashionability%2520of%2520the%2520output%250Aimages.%2520This%2520paper%2520presents%2520a%2520novel%2520diffusion%2520model-based%2520approach%2520that%250Agenerates%2520fashion%2520images%2520with%2520improved%2520fashionability%2520while%2520maintaining%2520control%250Aover%2520key%2520attributes.%2520Key%2520components%2520of%2520our%2520method%2520include%253A%25201%2529%2520fashionability%250Aenhancement%252C%2520which%2520ensures%2520that%2520the%2520generated%2520images%2520are%2520more%2520fashionable%2520than%250Athe%2520input%253B%25202%2529%2520preservation%2520of%2520body%2520characteristics%252C%2520encouraging%2520the%2520generated%250Aimages%2520to%2520maintain%2520the%2520original%2520shape%2520and%2520proportions%2520of%2520the%2520input%253B%2520and%25203%2529%250Aautomatic%2520fashion%2520optimization%252C%2520which%2520does%2520not%2520rely%2520on%2520manual%2520input%2520or%2520external%250Aprompts.%2520We%2520also%2520employ%2520two%2520methods%2520to%2520collect%2520training%2520data%2520for%2520guidance%2520while%250Agenerating%2520and%2520evaluating%2520the%2520images.%2520In%2520particular%252C%2520we%2520rate%2520outfit%2520images%250Ausing%2520fashionability%2520scores%2520annotated%2520by%2520multiple%2520fashion%2520experts%2520through%250AOpenSkill-based%2520and%2520five%2520critical%2520aspect-based%2520pairwise%2520comparisons.%2520These%250Amethods%2520provide%2520complementary%2520perspectives%2520for%2520assessing%2520and%2520improving%2520the%250Afashionability%2520of%2520the%2520generated%2520images.%2520The%2520experimental%2520results%2520show%2520that%2520our%250Aapproach%2520outperforms%2520the%2520baseline%2520Fashion%252B%252B%2520in%2520generating%2520images%2520with%2520superior%250Afashionability%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520producing%2520more%2520stylish%2520and%250Aappealing%2520fashion%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fashionability-Enhancing%20Outfit%20Image%20Editing%20with%20Conditional%20Diffusion%0A%20%20Models&entry.906535625=Qice%20Qin%20and%20Yuki%20Hirakawa%20and%20Ryotaro%20Shimizu%20and%20Takuya%20Furusawa%20and%20Edgar%20Simo-Serra&entry.1292438233=%20%20Image%20generation%20in%20the%20fashion%20domain%20has%20predominantly%20focused%20on%0Apreserving%20body%20characteristics%20or%20following%20input%20prompts%2C%20but%20little%0Aattention%20has%20been%20paid%20to%20improving%20the%20inherent%20fashionability%20of%20the%20output%0Aimages.%20This%20paper%20presents%20a%20novel%20diffusion%20model-based%20approach%20that%0Agenerates%20fashion%20images%20with%20improved%20fashionability%20while%20maintaining%20control%0Aover%20key%20attributes.%20Key%20components%20of%20our%20method%20include%3A%201%29%20fashionability%0Aenhancement%2C%20which%20ensures%20that%20the%20generated%20images%20are%20more%20fashionable%20than%0Athe%20input%3B%202%29%20preservation%20of%20body%20characteristics%2C%20encouraging%20the%20generated%0Aimages%20to%20maintain%20the%20original%20shape%20and%20proportions%20of%20the%20input%3B%20and%203%29%0Aautomatic%20fashion%20optimization%2C%20which%20does%20not%20rely%20on%20manual%20input%20or%20external%0Aprompts.%20We%20also%20employ%20two%20methods%20to%20collect%20training%20data%20for%20guidance%20while%0Agenerating%20and%20evaluating%20the%20images.%20In%20particular%2C%20we%20rate%20outfit%20images%0Ausing%20fashionability%20scores%20annotated%20by%20multiple%20fashion%20experts%20through%0AOpenSkill-based%20and%20five%20critical%20aspect-based%20pairwise%20comparisons.%20These%0Amethods%20provide%20complementary%20perspectives%20for%20assessing%20and%20improving%20the%0Afashionability%20of%20the%20generated%20images.%20The%20experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20the%20baseline%20Fashion%2B%2B%20in%20generating%20images%20with%20superior%0Afashionability%2C%20demonstrating%20its%20effectiveness%20in%20producing%20more%20stylish%20and%0Aappealing%20fashion%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18421v1&entry.124074799=Read"},
{"title": "Data-Driven Self-Supervised Graph Representation Learning", "author": "Ahmed E. Samy and Zekarias T. Kefatoa and Sarunas Girdzijauskasa", "abstract": "  Self-supervised graph representation learning (SSGRL) is a representation\nlearning paradigm used to reduce or avoid manual labeling. An essential part of\nSSGRL is graph data augmentation. Existing methods usually rely on heuristics\ncommonly identified through trial and error and are effective only within some\napplication domains. Also, it is not clear why one heuristic is better than\nanother. Moreover, recent studies have argued against some techniques (e.g.,\ndropout: that can change the properties of molecular graphs or destroy relevant\nsignals for graph-based document classification tasks).\n  In this study, we propose a novel data-driven SSGRL approach that\nautomatically learns a suitable graph augmentation from the signal encoded in\nthe graph (i.e., the nodes' predictive feature and topological information). We\npropose two complementary approaches that produce learnable feature and\ntopological augmentations. The former learns multi-view augmentation of node\nfeatures, and the latter learns a high-order view of the topology. Moreover,\nthe augmentations are jointly learned with the representation. Our approach is\ngeneral that it can be applied to homogeneous and heterogeneous graphs. We\nperform extensive experiments on node classification (using nine homogeneous\nand heterogeneous datasets) and graph property prediction (using another eight\ndatasets). The results show that the proposed method matches or outperforms the\nSOTA SSGRL baselines and performs similarly to semi-supervised methods. The\nanonymised source code is available at https://github.com/AhmedESamy/dsgrl/\n", "link": "http://arxiv.org/abs/2412.18316v1", "date": "2024-12-24", "relevancy": 2.7974, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6067}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5578}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Self-Supervised%20Graph%20Representation%20Learning&body=Title%3A%20Data-Driven%20Self-Supervised%20Graph%20Representation%20Learning%0AAuthor%3A%20Ahmed%20E.%20Samy%20and%20Zekarias%20T.%20Kefatoa%20and%20Sarunas%20Girdzijauskasa%0AAbstract%3A%20%20%20Self-supervised%20graph%20representation%20learning%20%28SSGRL%29%20is%20a%20representation%0Alearning%20paradigm%20used%20to%20reduce%20or%20avoid%20manual%20labeling.%20An%20essential%20part%20of%0ASSGRL%20is%20graph%20data%20augmentation.%20Existing%20methods%20usually%20rely%20on%20heuristics%0Acommonly%20identified%20through%20trial%20and%20error%20and%20are%20effective%20only%20within%20some%0Aapplication%20domains.%20Also%2C%20it%20is%20not%20clear%20why%20one%20heuristic%20is%20better%20than%0Aanother.%20Moreover%2C%20recent%20studies%20have%20argued%20against%20some%20techniques%20%28e.g.%2C%0Adropout%3A%20that%20can%20change%20the%20properties%20of%20molecular%20graphs%20or%20destroy%20relevant%0Asignals%20for%20graph-based%20document%20classification%20tasks%29.%0A%20%20In%20this%20study%2C%20we%20propose%20a%20novel%20data-driven%20SSGRL%20approach%20that%0Aautomatically%20learns%20a%20suitable%20graph%20augmentation%20from%20the%20signal%20encoded%20in%0Athe%20graph%20%28i.e.%2C%20the%20nodes%27%20predictive%20feature%20and%20topological%20information%29.%20We%0Apropose%20two%20complementary%20approaches%20that%20produce%20learnable%20feature%20and%0Atopological%20augmentations.%20The%20former%20learns%20multi-view%20augmentation%20of%20node%0Afeatures%2C%20and%20the%20latter%20learns%20a%20high-order%20view%20of%20the%20topology.%20Moreover%2C%0Athe%20augmentations%20are%20jointly%20learned%20with%20the%20representation.%20Our%20approach%20is%0Ageneral%20that%20it%20can%20be%20applied%20to%20homogeneous%20and%20heterogeneous%20graphs.%20We%0Aperform%20extensive%20experiments%20on%20node%20classification%20%28using%20nine%20homogeneous%0Aand%20heterogeneous%20datasets%29%20and%20graph%20property%20prediction%20%28using%20another%20eight%0Adatasets%29.%20The%20results%20show%20that%20the%20proposed%20method%20matches%20or%20outperforms%20the%0ASOTA%20SSGRL%20baselines%20and%20performs%20similarly%20to%20semi-supervised%20methods.%20The%0Aanonymised%20source%20code%20is%20available%20at%20https%3A//github.com/AhmedESamy/dsgrl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Self-Supervised%2520Graph%2520Representation%2520Learning%26entry.906535625%3DAhmed%2520E.%2520Samy%2520and%2520Zekarias%2520T.%2520Kefatoa%2520and%2520Sarunas%2520Girdzijauskasa%26entry.1292438233%3D%2520%2520Self-supervised%2520graph%2520representation%2520learning%2520%2528SSGRL%2529%2520is%2520a%2520representation%250Alearning%2520paradigm%2520used%2520to%2520reduce%2520or%2520avoid%2520manual%2520labeling.%2520An%2520essential%2520part%2520of%250ASSGRL%2520is%2520graph%2520data%2520augmentation.%2520Existing%2520methods%2520usually%2520rely%2520on%2520heuristics%250Acommonly%2520identified%2520through%2520trial%2520and%2520error%2520and%2520are%2520effective%2520only%2520within%2520some%250Aapplication%2520domains.%2520Also%252C%2520it%2520is%2520not%2520clear%2520why%2520one%2520heuristic%2520is%2520better%2520than%250Aanother.%2520Moreover%252C%2520recent%2520studies%2520have%2520argued%2520against%2520some%2520techniques%2520%2528e.g.%252C%250Adropout%253A%2520that%2520can%2520change%2520the%2520properties%2520of%2520molecular%2520graphs%2520or%2520destroy%2520relevant%250Asignals%2520for%2520graph-based%2520document%2520classification%2520tasks%2529.%250A%2520%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520data-driven%2520SSGRL%2520approach%2520that%250Aautomatically%2520learns%2520a%2520suitable%2520graph%2520augmentation%2520from%2520the%2520signal%2520encoded%2520in%250Athe%2520graph%2520%2528i.e.%252C%2520the%2520nodes%2527%2520predictive%2520feature%2520and%2520topological%2520information%2529.%2520We%250Apropose%2520two%2520complementary%2520approaches%2520that%2520produce%2520learnable%2520feature%2520and%250Atopological%2520augmentations.%2520The%2520former%2520learns%2520multi-view%2520augmentation%2520of%2520node%250Afeatures%252C%2520and%2520the%2520latter%2520learns%2520a%2520high-order%2520view%2520of%2520the%2520topology.%2520Moreover%252C%250Athe%2520augmentations%2520are%2520jointly%2520learned%2520with%2520the%2520representation.%2520Our%2520approach%2520is%250Ageneral%2520that%2520it%2520can%2520be%2520applied%2520to%2520homogeneous%2520and%2520heterogeneous%2520graphs.%2520We%250Aperform%2520extensive%2520experiments%2520on%2520node%2520classification%2520%2528using%2520nine%2520homogeneous%250Aand%2520heterogeneous%2520datasets%2529%2520and%2520graph%2520property%2520prediction%2520%2528using%2520another%2520eight%250Adatasets%2529.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520method%2520matches%2520or%2520outperforms%2520the%250ASOTA%2520SSGRL%2520baselines%2520and%2520performs%2520similarly%2520to%2520semi-supervised%2520methods.%2520The%250Aanonymised%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/AhmedESamy/dsgrl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Self-Supervised%20Graph%20Representation%20Learning&entry.906535625=Ahmed%20E.%20Samy%20and%20Zekarias%20T.%20Kefatoa%20and%20Sarunas%20Girdzijauskasa&entry.1292438233=%20%20Self-supervised%20graph%20representation%20learning%20%28SSGRL%29%20is%20a%20representation%0Alearning%20paradigm%20used%20to%20reduce%20or%20avoid%20manual%20labeling.%20An%20essential%20part%20of%0ASSGRL%20is%20graph%20data%20augmentation.%20Existing%20methods%20usually%20rely%20on%20heuristics%0Acommonly%20identified%20through%20trial%20and%20error%20and%20are%20effective%20only%20within%20some%0Aapplication%20domains.%20Also%2C%20it%20is%20not%20clear%20why%20one%20heuristic%20is%20better%20than%0Aanother.%20Moreover%2C%20recent%20studies%20have%20argued%20against%20some%20techniques%20%28e.g.%2C%0Adropout%3A%20that%20can%20change%20the%20properties%20of%20molecular%20graphs%20or%20destroy%20relevant%0Asignals%20for%20graph-based%20document%20classification%20tasks%29.%0A%20%20In%20this%20study%2C%20we%20propose%20a%20novel%20data-driven%20SSGRL%20approach%20that%0Aautomatically%20learns%20a%20suitable%20graph%20augmentation%20from%20the%20signal%20encoded%20in%0Athe%20graph%20%28i.e.%2C%20the%20nodes%27%20predictive%20feature%20and%20topological%20information%29.%20We%0Apropose%20two%20complementary%20approaches%20that%20produce%20learnable%20feature%20and%0Atopological%20augmentations.%20The%20former%20learns%20multi-view%20augmentation%20of%20node%0Afeatures%2C%20and%20the%20latter%20learns%20a%20high-order%20view%20of%20the%20topology.%20Moreover%2C%0Athe%20augmentations%20are%20jointly%20learned%20with%20the%20representation.%20Our%20approach%20is%0Ageneral%20that%20it%20can%20be%20applied%20to%20homogeneous%20and%20heterogeneous%20graphs.%20We%0Aperform%20extensive%20experiments%20on%20node%20classification%20%28using%20nine%20homogeneous%0Aand%20heterogeneous%20datasets%29%20and%20graph%20property%20prediction%20%28using%20another%20eight%0Adatasets%29.%20The%20results%20show%20that%20the%20proposed%20method%20matches%20or%20outperforms%20the%0ASOTA%20SSGRL%20baselines%20and%20performs%20similarly%20to%20semi-supervised%20methods.%20The%0Aanonymised%20source%20code%20is%20available%20at%20https%3A//github.com/AhmedESamy/dsgrl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18316v1&entry.124074799=Read"},
{"title": "Computer Vision-Driven Gesture Recognition: Toward Natural and Intuitive\n  Human-Computer", "author": "Fenghua Shao and Tong Zhang and Shang Gao and Qi Sun and Liuqingqing Yang", "abstract": "  This study mainly explores the application of natural gesture recognition\nbased on computer vision in human-computer interaction, aiming to improve the\nfluency and naturalness of human-computer interaction through gesture\nrecognition technology. In the fields of virtual reality, augmented reality and\nsmart home, traditional input methods have gradually failed to meet the needs\nof users for interactive experience. As an intuitive and convenient interaction\nmethod, gestures have received more and more attention. This paper proposes a\ngesture recognition method based on a three-dimensional hand skeleton model. By\nsimulating the three-dimensional spatial distribution of hand joints, a\nsimplified hand skeleton structure is constructed. By connecting the palm and\neach finger joint, a dynamic and static gesture model of the hand is formed,\nwhich further improves the accuracy and efficiency of gesture recognition.\nExperimental results show that this method can effectively recognize various\ngestures and maintain high recognition accuracy and real-time response\ncapabilities in different environments. In addition, combined with multimodal\ntechnologies such as eye tracking, the intelligence level of the gesture\nrecognition system can be further improved, bringing a richer and more\nintuitive user experience. In the future, with the continuous development of\ncomputer vision, deep learning and multimodal interaction technology, natural\ninteraction based on gestures will play an important role in a wider range of\napplication scenarios and promote revolutionary progress in human-computer\ninteraction.\n", "link": "http://arxiv.org/abs/2412.18321v1", "date": "2024-12-24", "relevancy": 2.7968, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5987}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5452}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer%20Vision-Driven%20Gesture%20Recognition%3A%20Toward%20Natural%20and%20Intuitive%0A%20%20Human-Computer&body=Title%3A%20Computer%20Vision-Driven%20Gesture%20Recognition%3A%20Toward%20Natural%20and%20Intuitive%0A%20%20Human-Computer%0AAuthor%3A%20Fenghua%20Shao%20and%20Tong%20Zhang%20and%20Shang%20Gao%20and%20Qi%20Sun%20and%20Liuqingqing%20Yang%0AAbstract%3A%20%20%20This%20study%20mainly%20explores%20the%20application%20of%20natural%20gesture%20recognition%0Abased%20on%20computer%20vision%20in%20human-computer%20interaction%2C%20aiming%20to%20improve%20the%0Afluency%20and%20naturalness%20of%20human-computer%20interaction%20through%20gesture%0Arecognition%20technology.%20In%20the%20fields%20of%20virtual%20reality%2C%20augmented%20reality%20and%0Asmart%20home%2C%20traditional%20input%20methods%20have%20gradually%20failed%20to%20meet%20the%20needs%0Aof%20users%20for%20interactive%20experience.%20As%20an%20intuitive%20and%20convenient%20interaction%0Amethod%2C%20gestures%20have%20received%20more%20and%20more%20attention.%20This%20paper%20proposes%20a%0Agesture%20recognition%20method%20based%20on%20a%20three-dimensional%20hand%20skeleton%20model.%20By%0Asimulating%20the%20three-dimensional%20spatial%20distribution%20of%20hand%20joints%2C%20a%0Asimplified%20hand%20skeleton%20structure%20is%20constructed.%20By%20connecting%20the%20palm%20and%0Aeach%20finger%20joint%2C%20a%20dynamic%20and%20static%20gesture%20model%20of%20the%20hand%20is%20formed%2C%0Awhich%20further%20improves%20the%20accuracy%20and%20efficiency%20of%20gesture%20recognition.%0AExperimental%20results%20show%20that%20this%20method%20can%20effectively%20recognize%20various%0Agestures%20and%20maintain%20high%20recognition%20accuracy%20and%20real-time%20response%0Acapabilities%20in%20different%20environments.%20In%20addition%2C%20combined%20with%20multimodal%0Atechnologies%20such%20as%20eye%20tracking%2C%20the%20intelligence%20level%20of%20the%20gesture%0Arecognition%20system%20can%20be%20further%20improved%2C%20bringing%20a%20richer%20and%20more%0Aintuitive%20user%20experience.%20In%20the%20future%2C%20with%20the%20continuous%20development%20of%0Acomputer%20vision%2C%20deep%20learning%20and%20multimodal%20interaction%20technology%2C%20natural%0Ainteraction%20based%20on%20gestures%20will%20play%20an%20important%20role%20in%20a%20wider%20range%20of%0Aapplication%20scenarios%20and%20promote%20revolutionary%20progress%20in%20human-computer%0Ainteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer%2520Vision-Driven%2520Gesture%2520Recognition%253A%2520Toward%2520Natural%2520and%2520Intuitive%250A%2520%2520Human-Computer%26entry.906535625%3DFenghua%2520Shao%2520and%2520Tong%2520Zhang%2520and%2520Shang%2520Gao%2520and%2520Qi%2520Sun%2520and%2520Liuqingqing%2520Yang%26entry.1292438233%3D%2520%2520This%2520study%2520mainly%2520explores%2520the%2520application%2520of%2520natural%2520gesture%2520recognition%250Abased%2520on%2520computer%2520vision%2520in%2520human-computer%2520interaction%252C%2520aiming%2520to%2520improve%2520the%250Afluency%2520and%2520naturalness%2520of%2520human-computer%2520interaction%2520through%2520gesture%250Arecognition%2520technology.%2520In%2520the%2520fields%2520of%2520virtual%2520reality%252C%2520augmented%2520reality%2520and%250Asmart%2520home%252C%2520traditional%2520input%2520methods%2520have%2520gradually%2520failed%2520to%2520meet%2520the%2520needs%250Aof%2520users%2520for%2520interactive%2520experience.%2520As%2520an%2520intuitive%2520and%2520convenient%2520interaction%250Amethod%252C%2520gestures%2520have%2520received%2520more%2520and%2520more%2520attention.%2520This%2520paper%2520proposes%2520a%250Agesture%2520recognition%2520method%2520based%2520on%2520a%2520three-dimensional%2520hand%2520skeleton%2520model.%2520By%250Asimulating%2520the%2520three-dimensional%2520spatial%2520distribution%2520of%2520hand%2520joints%252C%2520a%250Asimplified%2520hand%2520skeleton%2520structure%2520is%2520constructed.%2520By%2520connecting%2520the%2520palm%2520and%250Aeach%2520finger%2520joint%252C%2520a%2520dynamic%2520and%2520static%2520gesture%2520model%2520of%2520the%2520hand%2520is%2520formed%252C%250Awhich%2520further%2520improves%2520the%2520accuracy%2520and%2520efficiency%2520of%2520gesture%2520recognition.%250AExperimental%2520results%2520show%2520that%2520this%2520method%2520can%2520effectively%2520recognize%2520various%250Agestures%2520and%2520maintain%2520high%2520recognition%2520accuracy%2520and%2520real-time%2520response%250Acapabilities%2520in%2520different%2520environments.%2520In%2520addition%252C%2520combined%2520with%2520multimodal%250Atechnologies%2520such%2520as%2520eye%2520tracking%252C%2520the%2520intelligence%2520level%2520of%2520the%2520gesture%250Arecognition%2520system%2520can%2520be%2520further%2520improved%252C%2520bringing%2520a%2520richer%2520and%2520more%250Aintuitive%2520user%2520experience.%2520In%2520the%2520future%252C%2520with%2520the%2520continuous%2520development%2520of%250Acomputer%2520vision%252C%2520deep%2520learning%2520and%2520multimodal%2520interaction%2520technology%252C%2520natural%250Ainteraction%2520based%2520on%2520gestures%2520will%2520play%2520an%2520important%2520role%2520in%2520a%2520wider%2520range%2520of%250Aapplication%2520scenarios%2520and%2520promote%2520revolutionary%2520progress%2520in%2520human-computer%250Ainteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20Vision-Driven%20Gesture%20Recognition%3A%20Toward%20Natural%20and%20Intuitive%0A%20%20Human-Computer&entry.906535625=Fenghua%20Shao%20and%20Tong%20Zhang%20and%20Shang%20Gao%20and%20Qi%20Sun%20and%20Liuqingqing%20Yang&entry.1292438233=%20%20This%20study%20mainly%20explores%20the%20application%20of%20natural%20gesture%20recognition%0Abased%20on%20computer%20vision%20in%20human-computer%20interaction%2C%20aiming%20to%20improve%20the%0Afluency%20and%20naturalness%20of%20human-computer%20interaction%20through%20gesture%0Arecognition%20technology.%20In%20the%20fields%20of%20virtual%20reality%2C%20augmented%20reality%20and%0Asmart%20home%2C%20traditional%20input%20methods%20have%20gradually%20failed%20to%20meet%20the%20needs%0Aof%20users%20for%20interactive%20experience.%20As%20an%20intuitive%20and%20convenient%20interaction%0Amethod%2C%20gestures%20have%20received%20more%20and%20more%20attention.%20This%20paper%20proposes%20a%0Agesture%20recognition%20method%20based%20on%20a%20three-dimensional%20hand%20skeleton%20model.%20By%0Asimulating%20the%20three-dimensional%20spatial%20distribution%20of%20hand%20joints%2C%20a%0Asimplified%20hand%20skeleton%20structure%20is%20constructed.%20By%20connecting%20the%20palm%20and%0Aeach%20finger%20joint%2C%20a%20dynamic%20and%20static%20gesture%20model%20of%20the%20hand%20is%20formed%2C%0Awhich%20further%20improves%20the%20accuracy%20and%20efficiency%20of%20gesture%20recognition.%0AExperimental%20results%20show%20that%20this%20method%20can%20effectively%20recognize%20various%0Agestures%20and%20maintain%20high%20recognition%20accuracy%20and%20real-time%20response%0Acapabilities%20in%20different%20environments.%20In%20addition%2C%20combined%20with%20multimodal%0Atechnologies%20such%20as%20eye%20tracking%2C%20the%20intelligence%20level%20of%20the%20gesture%0Arecognition%20system%20can%20be%20further%20improved%2C%20bringing%20a%20richer%20and%20more%0Aintuitive%20user%20experience.%20In%20the%20future%2C%20with%20the%20continuous%20development%20of%0Acomputer%20vision%2C%20deep%20learning%20and%20multimodal%20interaction%20technology%2C%20natural%0Ainteraction%20based%20on%20gestures%20will%20play%20an%20important%20role%20in%20a%20wider%20range%20of%0Aapplication%20scenarios%20and%20promote%20revolutionary%20progress%20in%20human-computer%0Ainteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18321v1&entry.124074799=Read"},
{"title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement", "author": "Yihang Luo and Shangchen Zhou and Yushi Lan and Xingang Pan and Chen Change Loy", "abstract": "  Despite advances in neural rendering, due to the scarcity of high-quality 3D\ndatasets and the inherent limitations of multi-view diffusion models, view\nsynthesis and 3D model generation are restricted to low resolutions with\nsuboptimal multi-view consistency. In this study, we present a novel 3D\nenhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent\ndiffusion model to enhance coarse 3D inputs while preserving multi-view\nconsistency. Our method includes a pose-aware encoder and a diffusion-based\ndenoiser to refine low-quality multi-view images, along with data augmentation\nand a multi-view attention module with epipolar aggregation to maintain\nconsistent, high-quality 3D outputs across views. Unlike existing video-based\napproaches, our model supports seamless multi-view enhancement with improved\ncoherence across diverse viewing angles. Extensive evaluations show that\n3DEnhancer significantly outperforms existing methods, boosting both multi-view\nenhancement and per-instance 3D optimization tasks.\n", "link": "http://arxiv.org/abs/2412.18565v1", "date": "2024-12-24", "relevancy": 2.766, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7027}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7027}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DEnhancer%3A%20Consistent%20Multi-View%20Diffusion%20for%203D%20Enhancement&body=Title%3A%203DEnhancer%3A%20Consistent%20Multi-View%20Diffusion%20for%203D%20Enhancement%0AAuthor%3A%20Yihang%20Luo%20and%20Shangchen%20Zhou%20and%20Yushi%20Lan%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Despite%20advances%20in%20neural%20rendering%2C%20due%20to%20the%20scarcity%20of%20high-quality%203D%0Adatasets%20and%20the%20inherent%20limitations%20of%20multi-view%20diffusion%20models%2C%20view%0Asynthesis%20and%203D%20model%20generation%20are%20restricted%20to%20low%20resolutions%20with%0Asuboptimal%20multi-view%20consistency.%20In%20this%20study%2C%20we%20present%20a%20novel%203D%0Aenhancement%20pipeline%2C%20dubbed%203DEnhancer%2C%20which%20employs%20a%20multi-view%20latent%0Adiffusion%20model%20to%20enhance%20coarse%203D%20inputs%20while%20preserving%20multi-view%0Aconsistency.%20Our%20method%20includes%20a%20pose-aware%20encoder%20and%20a%20diffusion-based%0Adenoiser%20to%20refine%20low-quality%20multi-view%20images%2C%20along%20with%20data%20augmentation%0Aand%20a%20multi-view%20attention%20module%20with%20epipolar%20aggregation%20to%20maintain%0Aconsistent%2C%20high-quality%203D%20outputs%20across%20views.%20Unlike%20existing%20video-based%0Aapproaches%2C%20our%20model%20supports%20seamless%20multi-view%20enhancement%20with%20improved%0Acoherence%20across%20diverse%20viewing%20angles.%20Extensive%20evaluations%20show%20that%0A3DEnhancer%20significantly%20outperforms%20existing%20methods%2C%20boosting%20both%20multi-view%0Aenhancement%20and%20per-instance%203D%20optimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DEnhancer%253A%2520Consistent%2520Multi-View%2520Diffusion%2520for%25203D%2520Enhancement%26entry.906535625%3DYihang%2520Luo%2520and%2520Shangchen%2520Zhou%2520and%2520Yushi%2520Lan%2520and%2520Xingang%2520Pan%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Despite%2520advances%2520in%2520neural%2520rendering%252C%2520due%2520to%2520the%2520scarcity%2520of%2520high-quality%25203D%250Adatasets%2520and%2520the%2520inherent%2520limitations%2520of%2520multi-view%2520diffusion%2520models%252C%2520view%250Asynthesis%2520and%25203D%2520model%2520generation%2520are%2520restricted%2520to%2520low%2520resolutions%2520with%250Asuboptimal%2520multi-view%2520consistency.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520novel%25203D%250Aenhancement%2520pipeline%252C%2520dubbed%25203DEnhancer%252C%2520which%2520employs%2520a%2520multi-view%2520latent%250Adiffusion%2520model%2520to%2520enhance%2520coarse%25203D%2520inputs%2520while%2520preserving%2520multi-view%250Aconsistency.%2520Our%2520method%2520includes%2520a%2520pose-aware%2520encoder%2520and%2520a%2520diffusion-based%250Adenoiser%2520to%2520refine%2520low-quality%2520multi-view%2520images%252C%2520along%2520with%2520data%2520augmentation%250Aand%2520a%2520multi-view%2520attention%2520module%2520with%2520epipolar%2520aggregation%2520to%2520maintain%250Aconsistent%252C%2520high-quality%25203D%2520outputs%2520across%2520views.%2520Unlike%2520existing%2520video-based%250Aapproaches%252C%2520our%2520model%2520supports%2520seamless%2520multi-view%2520enhancement%2520with%2520improved%250Acoherence%2520across%2520diverse%2520viewing%2520angles.%2520Extensive%2520evaluations%2520show%2520that%250A3DEnhancer%2520significantly%2520outperforms%2520existing%2520methods%252C%2520boosting%2520both%2520multi-view%250Aenhancement%2520and%2520per-instance%25203D%2520optimization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DEnhancer%3A%20Consistent%20Multi-View%20Diffusion%20for%203D%20Enhancement&entry.906535625=Yihang%20Luo%20and%20Shangchen%20Zhou%20and%20Yushi%20Lan%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Despite%20advances%20in%20neural%20rendering%2C%20due%20to%20the%20scarcity%20of%20high-quality%203D%0Adatasets%20and%20the%20inherent%20limitations%20of%20multi-view%20diffusion%20models%2C%20view%0Asynthesis%20and%203D%20model%20generation%20are%20restricted%20to%20low%20resolutions%20with%0Asuboptimal%20multi-view%20consistency.%20In%20this%20study%2C%20we%20present%20a%20novel%203D%0Aenhancement%20pipeline%2C%20dubbed%203DEnhancer%2C%20which%20employs%20a%20multi-view%20latent%0Adiffusion%20model%20to%20enhance%20coarse%203D%20inputs%20while%20preserving%20multi-view%0Aconsistency.%20Our%20method%20includes%20a%20pose-aware%20encoder%20and%20a%20diffusion-based%0Adenoiser%20to%20refine%20low-quality%20multi-view%20images%2C%20along%20with%20data%20augmentation%0Aand%20a%20multi-view%20attention%20module%20with%20epipolar%20aggregation%20to%20maintain%0Aconsistent%2C%20high-quality%203D%20outputs%20across%20views.%20Unlike%20existing%20video-based%0Aapproaches%2C%20our%20model%20supports%20seamless%20multi-view%20enhancement%20with%20improved%0Acoherence%20across%20diverse%20viewing%20angles.%20Extensive%20evaluations%20show%20that%0A3DEnhancer%20significantly%20outperforms%20existing%20methods%2C%20boosting%20both%20multi-view%0Aenhancement%20and%20per-instance%203D%20optimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18565v1&entry.124074799=Read"},
{"title": "Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders", "author": "Rui Chen and Jianfeng Zhang and Yixun Liang and Guan Luo and Weiyu Li and Jiarui Liu and Xiu Li and Xiaoxiao Long and Jiashi Feng and Ping Tan", "abstract": "  Recent 3D content generation pipelines commonly employ Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations for\ndiffusion-based generation. However, the widely adopted uniform point sampling\nstrategy in Shape VAE training often leads to a significant loss of geometric\ndetails, limiting the quality of shape reconstruction and downstream generation\ntasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction\nthrough our proposed sharp edge sampling strategy and a dual cross-attention\nmechanism. By identifying and prioritizing regions with high geometric\ncomplexity during training, our method significantly improves the preservation\nof fine-grained shape features. Such sampling strategy and the dual attention\nmechanism enable the VAE to focus on crucial geometric details that are\ntypically missed by uniform sampling approaches. To systematically evaluate VAE\nreconstruction quality, we additionally propose Dora-bench, a benchmark that\nquantifies shape complexity through the density of sharp edges, introducing a\nnew metric focused on reconstruction accuracy at these salient geometric\nfeatures. Extensive experiments on the Dora-bench demonstrate that Dora-VAE\nachieves comparable reconstruction quality to the state-of-the-art dense\nXCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs.\n> 10,000 codes). We will release our code and benchmark dataset to facilitate\nfuture research in 3D shape modeling.\n", "link": "http://arxiv.org/abs/2412.17808v2", "date": "2024-12-24", "relevancy": 2.7614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders&body=Title%3A%20Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders%0AAuthor%3A%20Rui%20Chen%20and%20Jianfeng%20Zhang%20and%20Yixun%20Liang%20and%20Guan%20Luo%20and%20Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Xiu%20Li%20and%20Xiaoxiao%20Long%20and%20Jiashi%20Feng%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Recent%203D%20content%20generation%20pipelines%20commonly%20employ%20Variational%0AAutoencoders%20%28VAEs%29%20to%20encode%20shapes%20into%20compact%20latent%20representations%20for%0Adiffusion-based%20generation.%20However%2C%20the%20widely%20adopted%20uniform%20point%20sampling%0Astrategy%20in%20Shape%20VAE%20training%20often%20leads%20to%20a%20significant%20loss%20of%20geometric%0Adetails%2C%20limiting%20the%20quality%20of%20shape%20reconstruction%20and%20downstream%20generation%0Atasks.%20We%20present%20Dora-VAE%2C%20a%20novel%20approach%20that%20enhances%20VAE%20reconstruction%0Athrough%20our%20proposed%20sharp%20edge%20sampling%20strategy%20and%20a%20dual%20cross-attention%0Amechanism.%20By%20identifying%20and%20prioritizing%20regions%20with%20high%20geometric%0Acomplexity%20during%20training%2C%20our%20method%20significantly%20improves%20the%20preservation%0Aof%20fine-grained%20shape%20features.%20Such%20sampling%20strategy%20and%20the%20dual%20attention%0Amechanism%20enable%20the%20VAE%20to%20focus%20on%20crucial%20geometric%20details%20that%20are%0Atypically%20missed%20by%20uniform%20sampling%20approaches.%20To%20systematically%20evaluate%20VAE%0Areconstruction%20quality%2C%20we%20additionally%20propose%20Dora-bench%2C%20a%20benchmark%20that%0Aquantifies%20shape%20complexity%20through%20the%20density%20of%20sharp%20edges%2C%20introducing%20a%0Anew%20metric%20focused%20on%20reconstruction%20accuracy%20at%20these%20salient%20geometric%0Afeatures.%20Extensive%20experiments%20on%20the%20Dora-bench%20demonstrate%20that%20Dora-VAE%0Aachieves%20comparable%20reconstruction%20quality%20to%20the%20state-of-the-art%20dense%0AXCube-VAE%20while%20requiring%20a%20latent%20space%20at%20least%208%24%5Ctimes%24%20smaller%20%281%2C280%20vs.%0A%3E%2010%2C000%20codes%29.%20We%20will%20release%20our%20code%20and%20benchmark%20dataset%20to%20facilitate%0Afuture%20research%20in%203D%20shape%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDora%253A%2520Sampling%2520and%2520Benchmarking%2520for%25203D%2520Shape%2520Variational%2520Auto-Encoders%26entry.906535625%3DRui%2520Chen%2520and%2520Jianfeng%2520Zhang%2520and%2520Yixun%2520Liang%2520and%2520Guan%2520Luo%2520and%2520Weiyu%2520Li%2520and%2520Jiarui%2520Liu%2520and%2520Xiu%2520Li%2520and%2520Xiaoxiao%2520Long%2520and%2520Jiashi%2520Feng%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Recent%25203D%2520content%2520generation%2520pipelines%2520commonly%2520employ%2520Variational%250AAutoencoders%2520%2528VAEs%2529%2520to%2520encode%2520shapes%2520into%2520compact%2520latent%2520representations%2520for%250Adiffusion-based%2520generation.%2520However%252C%2520the%2520widely%2520adopted%2520uniform%2520point%2520sampling%250Astrategy%2520in%2520Shape%2520VAE%2520training%2520often%2520leads%2520to%2520a%2520significant%2520loss%2520of%2520geometric%250Adetails%252C%2520limiting%2520the%2520quality%2520of%2520shape%2520reconstruction%2520and%2520downstream%2520generation%250Atasks.%2520We%2520present%2520Dora-VAE%252C%2520a%2520novel%2520approach%2520that%2520enhances%2520VAE%2520reconstruction%250Athrough%2520our%2520proposed%2520sharp%2520edge%2520sampling%2520strategy%2520and%2520a%2520dual%2520cross-attention%250Amechanism.%2520By%2520identifying%2520and%2520prioritizing%2520regions%2520with%2520high%2520geometric%250Acomplexity%2520during%2520training%252C%2520our%2520method%2520significantly%2520improves%2520the%2520preservation%250Aof%2520fine-grained%2520shape%2520features.%2520Such%2520sampling%2520strategy%2520and%2520the%2520dual%2520attention%250Amechanism%2520enable%2520the%2520VAE%2520to%2520focus%2520on%2520crucial%2520geometric%2520details%2520that%2520are%250Atypically%2520missed%2520by%2520uniform%2520sampling%2520approaches.%2520To%2520systematically%2520evaluate%2520VAE%250Areconstruction%2520quality%252C%2520we%2520additionally%2520propose%2520Dora-bench%252C%2520a%2520benchmark%2520that%250Aquantifies%2520shape%2520complexity%2520through%2520the%2520density%2520of%2520sharp%2520edges%252C%2520introducing%2520a%250Anew%2520metric%2520focused%2520on%2520reconstruction%2520accuracy%2520at%2520these%2520salient%2520geometric%250Afeatures.%2520Extensive%2520experiments%2520on%2520the%2520Dora-bench%2520demonstrate%2520that%2520Dora-VAE%250Aachieves%2520comparable%2520reconstruction%2520quality%2520to%2520the%2520state-of-the-art%2520dense%250AXCube-VAE%2520while%2520requiring%2520a%2520latent%2520space%2520at%2520least%25208%2524%255Ctimes%2524%2520smaller%2520%25281%252C280%2520vs.%250A%253E%252010%252C000%2520codes%2529.%2520We%2520will%2520release%2520our%2520code%2520and%2520benchmark%2520dataset%2520to%2520facilitate%250Afuture%2520research%2520in%25203D%2520shape%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders&entry.906535625=Rui%20Chen%20and%20Jianfeng%20Zhang%20and%20Yixun%20Liang%20and%20Guan%20Luo%20and%20Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Xiu%20Li%20and%20Xiaoxiao%20Long%20and%20Jiashi%20Feng%20and%20Ping%20Tan&entry.1292438233=%20%20Recent%203D%20content%20generation%20pipelines%20commonly%20employ%20Variational%0AAutoencoders%20%28VAEs%29%20to%20encode%20shapes%20into%20compact%20latent%20representations%20for%0Adiffusion-based%20generation.%20However%2C%20the%20widely%20adopted%20uniform%20point%20sampling%0Astrategy%20in%20Shape%20VAE%20training%20often%20leads%20to%20a%20significant%20loss%20of%20geometric%0Adetails%2C%20limiting%20the%20quality%20of%20shape%20reconstruction%20and%20downstream%20generation%0Atasks.%20We%20present%20Dora-VAE%2C%20a%20novel%20approach%20that%20enhances%20VAE%20reconstruction%0Athrough%20our%20proposed%20sharp%20edge%20sampling%20strategy%20and%20a%20dual%20cross-attention%0Amechanism.%20By%20identifying%20and%20prioritizing%20regions%20with%20high%20geometric%0Acomplexity%20during%20training%2C%20our%20method%20significantly%20improves%20the%20preservation%0Aof%20fine-grained%20shape%20features.%20Such%20sampling%20strategy%20and%20the%20dual%20attention%0Amechanism%20enable%20the%20VAE%20to%20focus%20on%20crucial%20geometric%20details%20that%20are%0Atypically%20missed%20by%20uniform%20sampling%20approaches.%20To%20systematically%20evaluate%20VAE%0Areconstruction%20quality%2C%20we%20additionally%20propose%20Dora-bench%2C%20a%20benchmark%20that%0Aquantifies%20shape%20complexity%20through%20the%20density%20of%20sharp%20edges%2C%20introducing%20a%0Anew%20metric%20focused%20on%20reconstruction%20accuracy%20at%20these%20salient%20geometric%0Afeatures.%20Extensive%20experiments%20on%20the%20Dora-bench%20demonstrate%20that%20Dora-VAE%0Aachieves%20comparable%20reconstruction%20quality%20to%20the%20state-of-the-art%20dense%0AXCube-VAE%20while%20requiring%20a%20latent%20space%20at%20least%208%24%5Ctimes%24%20smaller%20%281%2C280%20vs.%0A%3E%2010%2C000%20codes%29.%20We%20will%20release%20our%20code%20and%20benchmark%20dataset%20to%20facilitate%0Afuture%20research%20in%203D%20shape%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17808v2&entry.124074799=Read"},
{"title": "Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds", "author": "Hanfang Liang and Yizhuo Yang and Jinming Hu and Jianfei Yang and Fen Liu and Shenghai Yuan", "abstract": "  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n", "link": "http://arxiv.org/abs/2412.12716v3", "date": "2024-12-24", "relevancy": 2.7423, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20UAV%203D%20Trajectories%20Estimation%20with%20Sparse%20Point%20Clouds&body=Title%3A%20Unsupervised%20UAV%203D%20Trajectories%20Estimation%20with%20Sparse%20Point%20Clouds%0AAuthor%3A%20Hanfang%20Liang%20and%20Yizhuo%20Yang%20and%20Jinming%20Hu%20and%20Jianfei%20Yang%20and%20Fen%20Liu%20and%20Shenghai%20Yuan%0AAbstract%3A%20%20%20Compact%20UAV%20systems%2C%20while%20advancing%20delivery%20and%20surveillance%2C%20pose%0Asignificant%20security%20challenges%20due%20to%20their%20small%20size%2C%20which%20hinders%0Adetection%20by%20traditional%20methods.%20This%20paper%20presents%20a%20cost-effective%2C%0Aunsupervised%20UAV%20detection%20method%20using%20spatial-temporal%20sequence%20processing%20to%0Afuse%20multiple%20LiDAR%20scans%20for%20accurate%20UAV%20tracking%20in%20real-world%20scenarios.%0AOur%20approach%20segments%20point%20clouds%20into%20foreground%20and%20background%2C%20analyzes%0Aspatial-temporal%20data%2C%20and%20employs%20a%20scoring%20mechanism%20to%20enhance%20detection%0Aaccuracy.%20Tested%20on%20a%20public%20dataset%2C%20our%20solution%20placed%204th%20in%20the%20CVPR%202024%0AUG2%2B%20Challenge%2C%20demonstrating%20its%20practical%20effectiveness.%20We%20plan%20to%0Aopen-source%20all%20designs%2C%20code%2C%20and%20sample%20data%20for%20the%20research%20community%0Agithub.com/lianghanfang/UnLiDAR-UAV-Est.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12716v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520UAV%25203D%2520Trajectories%2520Estimation%2520with%2520Sparse%2520Point%2520Clouds%26entry.906535625%3DHanfang%2520Liang%2520and%2520Yizhuo%2520Yang%2520and%2520Jinming%2520Hu%2520and%2520Jianfei%2520Yang%2520and%2520Fen%2520Liu%2520and%2520Shenghai%2520Yuan%26entry.1292438233%3D%2520%2520Compact%2520UAV%2520systems%252C%2520while%2520advancing%2520delivery%2520and%2520surveillance%252C%2520pose%250Asignificant%2520security%2520challenges%2520due%2520to%2520their%2520small%2520size%252C%2520which%2520hinders%250Adetection%2520by%2520traditional%2520methods.%2520This%2520paper%2520presents%2520a%2520cost-effective%252C%250Aunsupervised%2520UAV%2520detection%2520method%2520using%2520spatial-temporal%2520sequence%2520processing%2520to%250Afuse%2520multiple%2520LiDAR%2520scans%2520for%2520accurate%2520UAV%2520tracking%2520in%2520real-world%2520scenarios.%250AOur%2520approach%2520segments%2520point%2520clouds%2520into%2520foreground%2520and%2520background%252C%2520analyzes%250Aspatial-temporal%2520data%252C%2520and%2520employs%2520a%2520scoring%2520mechanism%2520to%2520enhance%2520detection%250Aaccuracy.%2520Tested%2520on%2520a%2520public%2520dataset%252C%2520our%2520solution%2520placed%25204th%2520in%2520the%2520CVPR%25202024%250AUG2%252B%2520Challenge%252C%2520demonstrating%2520its%2520practical%2520effectiveness.%2520We%2520plan%2520to%250Aopen-source%2520all%2520designs%252C%2520code%252C%2520and%2520sample%2520data%2520for%2520the%2520research%2520community%250Agithub.com/lianghanfang/UnLiDAR-UAV-Est.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12716v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20UAV%203D%20Trajectories%20Estimation%20with%20Sparse%20Point%20Clouds&entry.906535625=Hanfang%20Liang%20and%20Yizhuo%20Yang%20and%20Jinming%20Hu%20and%20Jianfei%20Yang%20and%20Fen%20Liu%20and%20Shenghai%20Yuan&entry.1292438233=%20%20Compact%20UAV%20systems%2C%20while%20advancing%20delivery%20and%20surveillance%2C%20pose%0Asignificant%20security%20challenges%20due%20to%20their%20small%20size%2C%20which%20hinders%0Adetection%20by%20traditional%20methods.%20This%20paper%20presents%20a%20cost-effective%2C%0Aunsupervised%20UAV%20detection%20method%20using%20spatial-temporal%20sequence%20processing%20to%0Afuse%20multiple%20LiDAR%20scans%20for%20accurate%20UAV%20tracking%20in%20real-world%20scenarios.%0AOur%20approach%20segments%20point%20clouds%20into%20foreground%20and%20background%2C%20analyzes%0Aspatial-temporal%20data%2C%20and%20employs%20a%20scoring%20mechanism%20to%20enhance%20detection%0Aaccuracy.%20Tested%20on%20a%20public%20dataset%2C%20our%20solution%20placed%204th%20in%20the%20CVPR%202024%0AUG2%2B%20Challenge%2C%20demonstrating%20its%20practical%20effectiveness.%20We%20plan%20to%0Aopen-source%20all%20designs%2C%20code%2C%20and%20sample%20data%20for%20the%20research%20community%0Agithub.com/lianghanfang/UnLiDAR-UAV-Est.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12716v3&entry.124074799=Read"},
{"title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning", "author": "Julia Witte Zimmerman and Denis Hudon and Kathryn Cramer and Alejandro J. Ruiz and Calla Beauregard and Ashley Fehr and Mikaela Irene Fudolig and Bradford Demarest and Yoshi Meke Bird and Milo Z. Trujillo and Christopher M. Danforth and Peter Sheridan Dodds", "abstract": "  Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.\n", "link": "http://arxiv.org/abs/2412.10924v3", "date": "2024-12-24", "relevancy": 2.7129, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&body=Title%3A%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning%0AAuthor%3A%20Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds%0AAbstract%3A%20%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DH%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20motivates%0Alinguistically-informed%20interventions%20in%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokenization%20pretraining%20can%20be%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20meaningfully%0Ainsulated%20from%20the%20main%20system%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10924v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokens%252C%2520the%2520oft-overlooked%2520appetizer%253A%2520Large%2520language%2520models%252C%2520the%250A%2520%2520distributional%2520hypothesis%252C%2520and%2520meaning%26entry.906535625%3DJulia%2520Witte%2520Zimmerman%2520and%2520Denis%2520Hudon%2520and%2520Kathryn%2520Cramer%2520and%2520Alejandro%2520J.%2520Ruiz%2520and%2520Calla%2520Beauregard%2520and%2520Ashley%2520Fehr%2520and%2520Mikaela%2520Irene%2520Fudolig%2520and%2520Bradford%2520Demarest%2520and%2520Yoshi%2520Meke%2520Bird%2520and%2520Milo%2520Z.%2520Trujillo%2520and%2520Christopher%2520M.%2520Danforth%2520and%2520Peter%2520Sheridan%2520Dodds%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520necessary%2520component%2520within%2520the%2520current%2520architecture%2520of%2520many%250Alanguage%2520models%252C%2520including%2520the%2520transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%250Aof%2520Generative%2520AI%252C%2520yet%2520its%2520impact%2520on%2520the%2520model%2527s%2520cognition%2520is%2520often%2520overlooked.%250AWe%2520argue%2520that%2520LLMs%2520demonstrate%2520that%2520the%2520Distributional%2520Hypothesis%2520%2528DH%2529%2520is%250Asufficient%2520for%2520reasonably%2520human-like%2520language%2520performance%252C%2520and%2520that%2520the%250Aemergence%2520of%2520human-meaningful%2520linguistic%2520units%2520among%2520tokens%2520motivates%250Alinguistically-informed%2520interventions%2520in%2520existing%252C%2520linguistically-agnostic%250Atokenization%2520techniques%252C%2520particularly%2520with%2520respect%2520to%2520their%2520roles%2520as%2520%25281%2529%250Asemantic%2520primitives%2520and%2520as%2520%25282%2529%2520vehicles%2520for%2520conveying%2520salient%2520distributional%250Apatterns%2520from%2520human%2520language%2520to%2520the%2520model.%2520We%2520explore%2520tokenizations%2520from%2520a%2520BPE%250Atokenizer%253B%2520extant%2520model%2520vocabularies%2520obtained%2520from%2520Hugging%2520Face%2520and%2520tiktoken%253B%250Aand%2520the%2520information%2520in%2520exemplar%2520token%2520vectors%2520as%2520they%2520move%2520through%2520the%2520layers%250Aof%2520a%2520RoBERTa%2520%2528large%2529%2520model.%2520Besides%2520creating%2520sub-optimal%2520semantic%2520building%250Ablocks%2520and%2520obscuring%2520the%2520model%2527s%2520access%2520to%2520the%2520necessary%2520distributional%250Apatterns%252C%2520we%2520describe%2520how%2520tokenization%2520pretraining%2520can%2520be%2520a%2520backdoor%2520for%2520bias%250Aand%2520other%2520unwanted%2520content%252C%2520which%2520current%2520alignment%2520practices%2520may%2520not%250Aremediate.%2520Additionally%252C%2520we%2520relay%2520evidence%2520that%2520the%2520tokenization%2520algorithm%2527s%250Aobjective%2520function%2520impacts%2520the%2520LLM%2527s%2520cognition%252C%2520despite%2520being%2520meaningfully%250Ainsulated%2520from%2520the%2520main%2520system%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10924v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&entry.906535625=Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds&entry.1292438233=%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DH%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20motivates%0Alinguistically-informed%20interventions%20in%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokenization%20pretraining%20can%20be%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20meaningfully%0Ainsulated%20from%20the%20main%20system%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10924v3&entry.124074799=Read"},
{"title": "Addressing Spatial-Temporal Data Heterogeneity in Federated Continual\n  Learning via Tail Anchor", "author": "Hao Yu and Xin Yang and Le Zhang and Hanlin Gu and Tianrui Li and Lixin Fan and Qiang Yang", "abstract": "  Federated continual learning (FCL) allows each client to continually update\nits knowledge from task streams, enhancing the applicability of federated\nlearning in real-world scenarios. However, FCL needs to address not only\nspatial data heterogeneity between clients but also temporal data heterogeneity\nbetween tasks. In this paper, empirical experiments demonstrate that such\ninput-level heterogeneity significantly affects the model's internal parameters\nand outputs, leading to severe spatial-temporal catastrophic forgetting of\nlocal and previous knowledge. To this end, we propose Federated Tail Anchor\n(FedTA) to mix trainable Tail Anchor with the frozen output features to adjust\ntheir position in the feature space, thereby overcoming parameter-forgetting\nand output-forgetting. Moreover, three novel components are also included in\nFedTA: Input Enhancement for improving the performance of pre-trained models on\ndownstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous\nlocal knowledge on the server side; and Best Global Prototype Selection for\nfinding the best anchor point for each class in the feature space. Extensive\nexperiments demonstrate that FedTA not only outperforms existing FCL methods\nbut also effectively preserves the relative positions of features, remaining\nunaffected by spatial and temporal changes.\n", "link": "http://arxiv.org/abs/2412.18355v1", "date": "2024-12-24", "relevancy": 2.6845, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Spatial-Temporal%20Data%20Heterogeneity%20in%20Federated%20Continual%0A%20%20Learning%20via%20Tail%20Anchor&body=Title%3A%20Addressing%20Spatial-Temporal%20Data%20Heterogeneity%20in%20Federated%20Continual%0A%20%20Learning%20via%20Tail%20Anchor%0AAuthor%3A%20Hao%20Yu%20and%20Xin%20Yang%20and%20Le%20Zhang%20and%20Hanlin%20Gu%20and%20Tianrui%20Li%20and%20Lixin%20Fan%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20Federated%20continual%20learning%20%28FCL%29%20allows%20each%20client%20to%20continually%20update%0Aits%20knowledge%20from%20task%20streams%2C%20enhancing%20the%20applicability%20of%20federated%0Alearning%20in%20real-world%20scenarios.%20However%2C%20FCL%20needs%20to%20address%20not%20only%0Aspatial%20data%20heterogeneity%20between%20clients%20but%20also%20temporal%20data%20heterogeneity%0Abetween%20tasks.%20In%20this%20paper%2C%20empirical%20experiments%20demonstrate%20that%20such%0Ainput-level%20heterogeneity%20significantly%20affects%20the%20model%27s%20internal%20parameters%0Aand%20outputs%2C%20leading%20to%20severe%20spatial-temporal%20catastrophic%20forgetting%20of%0Alocal%20and%20previous%20knowledge.%20To%20this%20end%2C%20we%20propose%20Federated%20Tail%20Anchor%0A%28FedTA%29%20to%20mix%20trainable%20Tail%20Anchor%20with%20the%20frozen%20output%20features%20to%20adjust%0Atheir%20position%20in%20the%20feature%20space%2C%20thereby%20overcoming%20parameter-forgetting%0Aand%20output-forgetting.%20Moreover%2C%20three%20novel%20components%20are%20also%20included%20in%0AFedTA%3A%20Input%20Enhancement%20for%20improving%20the%20performance%20of%20pre-trained%20models%20on%0Adownstream%20tasks%3B%20Selective%20Input%20Knowledge%20Fusion%20for%20fusion%20of%20heterogeneous%0Alocal%20knowledge%20on%20the%20server%20side%3B%20and%20Best%20Global%20Prototype%20Selection%20for%0Afinding%20the%20best%20anchor%20point%20for%20each%20class%20in%20the%20feature%20space.%20Extensive%0Aexperiments%20demonstrate%20that%20FedTA%20not%20only%20outperforms%20existing%20FCL%20methods%0Abut%20also%20effectively%20preserves%20the%20relative%20positions%20of%20features%2C%20remaining%0Aunaffected%20by%20spatial%20and%20temporal%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Spatial-Temporal%2520Data%2520Heterogeneity%2520in%2520Federated%2520Continual%250A%2520%2520Learning%2520via%2520Tail%2520Anchor%26entry.906535625%3DHao%2520Yu%2520and%2520Xin%2520Yang%2520and%2520Le%2520Zhang%2520and%2520Hanlin%2520Gu%2520and%2520Tianrui%2520Li%2520and%2520Lixin%2520Fan%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520Federated%2520continual%2520learning%2520%2528FCL%2529%2520allows%2520each%2520client%2520to%2520continually%2520update%250Aits%2520knowledge%2520from%2520task%2520streams%252C%2520enhancing%2520the%2520applicability%2520of%2520federated%250Alearning%2520in%2520real-world%2520scenarios.%2520However%252C%2520FCL%2520needs%2520to%2520address%2520not%2520only%250Aspatial%2520data%2520heterogeneity%2520between%2520clients%2520but%2520also%2520temporal%2520data%2520heterogeneity%250Abetween%2520tasks.%2520In%2520this%2520paper%252C%2520empirical%2520experiments%2520demonstrate%2520that%2520such%250Ainput-level%2520heterogeneity%2520significantly%2520affects%2520the%2520model%2527s%2520internal%2520parameters%250Aand%2520outputs%252C%2520leading%2520to%2520severe%2520spatial-temporal%2520catastrophic%2520forgetting%2520of%250Alocal%2520and%2520previous%2520knowledge.%2520To%2520this%2520end%252C%2520we%2520propose%2520Federated%2520Tail%2520Anchor%250A%2528FedTA%2529%2520to%2520mix%2520trainable%2520Tail%2520Anchor%2520with%2520the%2520frozen%2520output%2520features%2520to%2520adjust%250Atheir%2520position%2520in%2520the%2520feature%2520space%252C%2520thereby%2520overcoming%2520parameter-forgetting%250Aand%2520output-forgetting.%2520Moreover%252C%2520three%2520novel%2520components%2520are%2520also%2520included%2520in%250AFedTA%253A%2520Input%2520Enhancement%2520for%2520improving%2520the%2520performance%2520of%2520pre-trained%2520models%2520on%250Adownstream%2520tasks%253B%2520Selective%2520Input%2520Knowledge%2520Fusion%2520for%2520fusion%2520of%2520heterogeneous%250Alocal%2520knowledge%2520on%2520the%2520server%2520side%253B%2520and%2520Best%2520Global%2520Prototype%2520Selection%2520for%250Afinding%2520the%2520best%2520anchor%2520point%2520for%2520each%2520class%2520in%2520the%2520feature%2520space.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520FedTA%2520not%2520only%2520outperforms%2520existing%2520FCL%2520methods%250Abut%2520also%2520effectively%2520preserves%2520the%2520relative%2520positions%2520of%2520features%252C%2520remaining%250Aunaffected%2520by%2520spatial%2520and%2520temporal%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Spatial-Temporal%20Data%20Heterogeneity%20in%20Federated%20Continual%0A%20%20Learning%20via%20Tail%20Anchor&entry.906535625=Hao%20Yu%20and%20Xin%20Yang%20and%20Le%20Zhang%20and%20Hanlin%20Gu%20and%20Tianrui%20Li%20and%20Lixin%20Fan%20and%20Qiang%20Yang&entry.1292438233=%20%20Federated%20continual%20learning%20%28FCL%29%20allows%20each%20client%20to%20continually%20update%0Aits%20knowledge%20from%20task%20streams%2C%20enhancing%20the%20applicability%20of%20federated%0Alearning%20in%20real-world%20scenarios.%20However%2C%20FCL%20needs%20to%20address%20not%20only%0Aspatial%20data%20heterogeneity%20between%20clients%20but%20also%20temporal%20data%20heterogeneity%0Abetween%20tasks.%20In%20this%20paper%2C%20empirical%20experiments%20demonstrate%20that%20such%0Ainput-level%20heterogeneity%20significantly%20affects%20the%20model%27s%20internal%20parameters%0Aand%20outputs%2C%20leading%20to%20severe%20spatial-temporal%20catastrophic%20forgetting%20of%0Alocal%20and%20previous%20knowledge.%20To%20this%20end%2C%20we%20propose%20Federated%20Tail%20Anchor%0A%28FedTA%29%20to%20mix%20trainable%20Tail%20Anchor%20with%20the%20frozen%20output%20features%20to%20adjust%0Atheir%20position%20in%20the%20feature%20space%2C%20thereby%20overcoming%20parameter-forgetting%0Aand%20output-forgetting.%20Moreover%2C%20three%20novel%20components%20are%20also%20included%20in%0AFedTA%3A%20Input%20Enhancement%20for%20improving%20the%20performance%20of%20pre-trained%20models%20on%0Adownstream%20tasks%3B%20Selective%20Input%20Knowledge%20Fusion%20for%20fusion%20of%20heterogeneous%0Alocal%20knowledge%20on%20the%20server%20side%3B%20and%20Best%20Global%20Prototype%20Selection%20for%0Afinding%20the%20best%20anchor%20point%20for%20each%20class%20in%20the%20feature%20space.%20Extensive%0Aexperiments%20demonstrate%20that%20FedTA%20not%20only%20outperforms%20existing%20FCL%20methods%0Abut%20also%20effectively%20preserves%20the%20relative%20positions%20of%20features%2C%20remaining%0Aunaffected%20by%20spatial%20and%20temporal%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18355v1&entry.124074799=Read"},
{"title": "Weak Scaling Capability in Token Space: An Observation from Large Vision\n  Language Model", "author": "Tenghui Li and Guoxu Zhou and Xuyang Zhao and Qibin Zhao", "abstract": "  The scaling capability has been widely validated with respect to the number\nof parameters and the size of training data. One important question that is\nunexplored is that does scaling capability also exists similarly with respect\nto the number of vision tokens? This study fills the gap by investigating the\nrelationship between the number of vision tokens and the performance of\nvision-language models. Our theoretical analysis and empirical evaluations\nreveal that the model exhibits weak scaling capabilities on the length \\(N_l\\),\nwith performance approximately \\(S(N_l) \\approx (c/N_l)^{\\alpha}\\), where \\(c,\n\\alpha\\) are hyperparameters. Interestingly, this scaling behavior remains\nlargely unaffected by the inclusion or exclusion of the user's question in the\ninput. Furthermore, fusing the user's question with the vision token can\nenhance model performance when the question is relevant to the task. To address\nthe computational challenges associated with large-scale vision tokens, we\npropose a novel architecture that efficiently reduces the token count while\nintegrating user question tokens into the representation. Our findings may\noffer insights for developing more efficient and effective vision-language\nmodels under specific task constraints.\n", "link": "http://arxiv.org/abs/2412.18387v1", "date": "2024-12-24", "relevancy": 2.6829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak%20Scaling%20Capability%20in%20Token%20Space%3A%20An%20Observation%20from%20Large%20Vision%0A%20%20Language%20Model&body=Title%3A%20Weak%20Scaling%20Capability%20in%20Token%20Space%3A%20An%20Observation%20from%20Large%20Vision%0A%20%20Language%20Model%0AAuthor%3A%20Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Qibin%20Zhao%0AAbstract%3A%20%20%20The%20scaling%20capability%20has%20been%20widely%20validated%20with%20respect%20to%20the%20number%0Aof%20parameters%20and%20the%20size%20of%20training%20data.%20One%20important%20question%20that%20is%0Aunexplored%20is%20that%20does%20scaling%20capability%20also%20exists%20similarly%20with%20respect%0Ato%20the%20number%20of%20vision%20tokens%3F%20This%20study%20fills%20the%20gap%20by%20investigating%20the%0Arelationship%20between%20the%20number%20of%20vision%20tokens%20and%20the%20performance%20of%0Avision-language%20models.%20Our%20theoretical%20analysis%20and%20empirical%20evaluations%0Areveal%20that%20the%20model%20exhibits%20weak%20scaling%20capabilities%20on%20the%20length%20%5C%28N_l%5C%29%2C%0Awith%20performance%20approximately%20%5C%28S%28N_l%29%20%5Capprox%20%28c/N_l%29%5E%7B%5Calpha%7D%5C%29%2C%20where%20%5C%28c%2C%0A%5Calpha%5C%29%20are%20hyperparameters.%20Interestingly%2C%20this%20scaling%20behavior%20remains%0Alargely%20unaffected%20by%20the%20inclusion%20or%20exclusion%20of%20the%20user%27s%20question%20in%20the%0Ainput.%20Furthermore%2C%20fusing%20the%20user%27s%20question%20with%20the%20vision%20token%20can%0Aenhance%20model%20performance%20when%20the%20question%20is%20relevant%20to%20the%20task.%20To%20address%0Athe%20computational%20challenges%20associated%20with%20large-scale%20vision%20tokens%2C%20we%0Apropose%20a%20novel%20architecture%20that%20efficiently%20reduces%20the%20token%20count%20while%0Aintegrating%20user%20question%20tokens%20into%20the%20representation.%20Our%20findings%20may%0Aoffer%20insights%20for%20developing%20more%20efficient%20and%20effective%20vision-language%0Amodels%20under%20specific%20task%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak%2520Scaling%2520Capability%2520in%2520Token%2520Space%253A%2520An%2520Observation%2520from%2520Large%2520Vision%250A%2520%2520Language%2520Model%26entry.906535625%3DTenghui%2520Li%2520and%2520Guoxu%2520Zhou%2520and%2520Xuyang%2520Zhao%2520and%2520Qibin%2520Zhao%26entry.1292438233%3D%2520%2520The%2520scaling%2520capability%2520has%2520been%2520widely%2520validated%2520with%2520respect%2520to%2520the%2520number%250Aof%2520parameters%2520and%2520the%2520size%2520of%2520training%2520data.%2520One%2520important%2520question%2520that%2520is%250Aunexplored%2520is%2520that%2520does%2520scaling%2520capability%2520also%2520exists%2520similarly%2520with%2520respect%250Ato%2520the%2520number%2520of%2520vision%2520tokens%253F%2520This%2520study%2520fills%2520the%2520gap%2520by%2520investigating%2520the%250Arelationship%2520between%2520the%2520number%2520of%2520vision%2520tokens%2520and%2520the%2520performance%2520of%250Avision-language%2520models.%2520Our%2520theoretical%2520analysis%2520and%2520empirical%2520evaluations%250Areveal%2520that%2520the%2520model%2520exhibits%2520weak%2520scaling%2520capabilities%2520on%2520the%2520length%2520%255C%2528N_l%255C%2529%252C%250Awith%2520performance%2520approximately%2520%255C%2528S%2528N_l%2529%2520%255Capprox%2520%2528c/N_l%2529%255E%257B%255Calpha%257D%255C%2529%252C%2520where%2520%255C%2528c%252C%250A%255Calpha%255C%2529%2520are%2520hyperparameters.%2520Interestingly%252C%2520this%2520scaling%2520behavior%2520remains%250Alargely%2520unaffected%2520by%2520the%2520inclusion%2520or%2520exclusion%2520of%2520the%2520user%2527s%2520question%2520in%2520the%250Ainput.%2520Furthermore%252C%2520fusing%2520the%2520user%2527s%2520question%2520with%2520the%2520vision%2520token%2520can%250Aenhance%2520model%2520performance%2520when%2520the%2520question%2520is%2520relevant%2520to%2520the%2520task.%2520To%2520address%250Athe%2520computational%2520challenges%2520associated%2520with%2520large-scale%2520vision%2520tokens%252C%2520we%250Apropose%2520a%2520novel%2520architecture%2520that%2520efficiently%2520reduces%2520the%2520token%2520count%2520while%250Aintegrating%2520user%2520question%2520tokens%2520into%2520the%2520representation.%2520Our%2520findings%2520may%250Aoffer%2520insights%2520for%2520developing%2520more%2520efficient%2520and%2520effective%2520vision-language%250Amodels%2520under%2520specific%2520task%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak%20Scaling%20Capability%20in%20Token%20Space%3A%20An%20Observation%20from%20Large%20Vision%0A%20%20Language%20Model&entry.906535625=Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Qibin%20Zhao&entry.1292438233=%20%20The%20scaling%20capability%20has%20been%20widely%20validated%20with%20respect%20to%20the%20number%0Aof%20parameters%20and%20the%20size%20of%20training%20data.%20One%20important%20question%20that%20is%0Aunexplored%20is%20that%20does%20scaling%20capability%20also%20exists%20similarly%20with%20respect%0Ato%20the%20number%20of%20vision%20tokens%3F%20This%20study%20fills%20the%20gap%20by%20investigating%20the%0Arelationship%20between%20the%20number%20of%20vision%20tokens%20and%20the%20performance%20of%0Avision-language%20models.%20Our%20theoretical%20analysis%20and%20empirical%20evaluations%0Areveal%20that%20the%20model%20exhibits%20weak%20scaling%20capabilities%20on%20the%20length%20%5C%28N_l%5C%29%2C%0Awith%20performance%20approximately%20%5C%28S%28N_l%29%20%5Capprox%20%28c/N_l%29%5E%7B%5Calpha%7D%5C%29%2C%20where%20%5C%28c%2C%0A%5Calpha%5C%29%20are%20hyperparameters.%20Interestingly%2C%20this%20scaling%20behavior%20remains%0Alargely%20unaffected%20by%20the%20inclusion%20or%20exclusion%20of%20the%20user%27s%20question%20in%20the%0Ainput.%20Furthermore%2C%20fusing%20the%20user%27s%20question%20with%20the%20vision%20token%20can%0Aenhance%20model%20performance%20when%20the%20question%20is%20relevant%20to%20the%20task.%20To%20address%0Athe%20computational%20challenges%20associated%20with%20large-scale%20vision%20tokens%2C%20we%0Apropose%20a%20novel%20architecture%20that%20efficiently%20reduces%20the%20token%20count%20while%0Aintegrating%20user%20question%20tokens%20into%20the%20representation.%20Our%20findings%20may%0Aoffer%20insights%20for%20developing%20more%20efficient%20and%20effective%20vision-language%0Amodels%20under%20specific%20task%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18387v1&entry.124074799=Read"},
{"title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding", "author": "Tatiana Zemskova and Dmitry Yudin", "abstract": "  A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM.\n", "link": "http://arxiv.org/abs/2412.18450v1", "date": "2024-12-24", "relevancy": 2.6755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6731}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGraphLLM%3A%20Combining%20Semantic%20Graphs%20and%20Large%20Language%20Models%20for%203D%0A%20%20Scene%20Understanding&body=Title%3A%203DGraphLLM%3A%20Combining%20Semantic%20Graphs%20and%20Large%20Language%20Models%20for%203D%0A%20%20Scene%20Understanding%0AAuthor%3A%20Tatiana%20Zemskova%20and%20Dmitry%20Yudin%0AAbstract%3A%20%20%20A%203D%20scene%20graph%20represents%20a%20compact%20scene%20model%2C%20storing%20information%20about%0Athe%20objects%20and%20the%20semantic%20relationships%20between%20them%2C%20making%20its%20use%0Apromising%20for%20robotic%20tasks.%20When%20interacting%20with%20a%20user%2C%20an%20embodied%0Aintelligent%20agent%20should%20be%20capable%20of%20responding%20to%20various%20queries%20about%20the%0Ascene%20formulated%20in%20natural%20language.%20Large%20Language%20Models%20%28LLMs%29%20are%0Abeneficial%20solutions%20for%20user-robot%20interaction%20due%20to%20their%20natural%20language%0Aunderstanding%20and%20reasoning%20abilities.%20Recent%20methods%20for%20creating%20learnable%0Arepresentations%20of%203D%20scenes%20have%20demonstrated%20the%20potential%20to%20improve%20the%0Aquality%20of%20LLMs%20responses%20by%20adapting%20to%20the%203D%20world.%20However%2C%20the%20existing%0Amethods%20do%20not%20explicitly%20utilize%20information%20about%20the%20semantic%20relationships%0Abetween%20objects%2C%20limiting%20themselves%20to%20information%20about%20their%20coordinates.%20In%0Athis%20work%2C%20we%20propose%20a%20method%203DGraphLLM%20for%20constructing%20a%20learnable%0Arepresentation%20of%20a%203D%20scene%20graph.%20The%20learnable%20representation%20is%20used%20as%0Ainput%20for%20LLMs%20to%20perform%203D%20vision-language%20tasks.%20In%20our%20experiments%20on%0Apopular%20ScanRefer%2C%20RIORefer%2C%20Multi3DRefer%2C%20ScanQA%2C%20Sqa3D%2C%20and%20Scan2cap%0Adatasets%2C%20we%20demonstrate%20the%20advantage%20of%20this%20approach%20over%20baseline%20methods%0Athat%20do%20not%20use%20information%20about%20the%20semantic%20relationships%20between%20objects.%0AThe%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CognitiveAISystems/3DGraphLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGraphLLM%253A%2520Combining%2520Semantic%2520Graphs%2520and%2520Large%2520Language%2520Models%2520for%25203D%250A%2520%2520Scene%2520Understanding%26entry.906535625%3DTatiana%2520Zemskova%2520and%2520Dmitry%2520Yudin%26entry.1292438233%3D%2520%2520A%25203D%2520scene%2520graph%2520represents%2520a%2520compact%2520scene%2520model%252C%2520storing%2520information%2520about%250Athe%2520objects%2520and%2520the%2520semantic%2520relationships%2520between%2520them%252C%2520making%2520its%2520use%250Apromising%2520for%2520robotic%2520tasks.%2520When%2520interacting%2520with%2520a%2520user%252C%2520an%2520embodied%250Aintelligent%2520agent%2520should%2520be%2520capable%2520of%2520responding%2520to%2520various%2520queries%2520about%2520the%250Ascene%2520formulated%2520in%2520natural%2520language.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%250Abeneficial%2520solutions%2520for%2520user-robot%2520interaction%2520due%2520to%2520their%2520natural%2520language%250Aunderstanding%2520and%2520reasoning%2520abilities.%2520Recent%2520methods%2520for%2520creating%2520learnable%250Arepresentations%2520of%25203D%2520scenes%2520have%2520demonstrated%2520the%2520potential%2520to%2520improve%2520the%250Aquality%2520of%2520LLMs%2520responses%2520by%2520adapting%2520to%2520the%25203D%2520world.%2520However%252C%2520the%2520existing%250Amethods%2520do%2520not%2520explicitly%2520utilize%2520information%2520about%2520the%2520semantic%2520relationships%250Abetween%2520objects%252C%2520limiting%2520themselves%2520to%2520information%2520about%2520their%2520coordinates.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520method%25203DGraphLLM%2520for%2520constructing%2520a%2520learnable%250Arepresentation%2520of%2520a%25203D%2520scene%2520graph.%2520The%2520learnable%2520representation%2520is%2520used%2520as%250Ainput%2520for%2520LLMs%2520to%2520perform%25203D%2520vision-language%2520tasks.%2520In%2520our%2520experiments%2520on%250Apopular%2520ScanRefer%252C%2520RIORefer%252C%2520Multi3DRefer%252C%2520ScanQA%252C%2520Sqa3D%252C%2520and%2520Scan2cap%250Adatasets%252C%2520we%2520demonstrate%2520the%2520advantage%2520of%2520this%2520approach%2520over%2520baseline%2520methods%250Athat%2520do%2520not%2520use%2520information%2520about%2520the%2520semantic%2520relationships%2520between%2520objects.%250AThe%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/CognitiveAISystems/3DGraphLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGraphLLM%3A%20Combining%20Semantic%20Graphs%20and%20Large%20Language%20Models%20for%203D%0A%20%20Scene%20Understanding&entry.906535625=Tatiana%20Zemskova%20and%20Dmitry%20Yudin&entry.1292438233=%20%20A%203D%20scene%20graph%20represents%20a%20compact%20scene%20model%2C%20storing%20information%20about%0Athe%20objects%20and%20the%20semantic%20relationships%20between%20them%2C%20making%20its%20use%0Apromising%20for%20robotic%20tasks.%20When%20interacting%20with%20a%20user%2C%20an%20embodied%0Aintelligent%20agent%20should%20be%20capable%20of%20responding%20to%20various%20queries%20about%20the%0Ascene%20formulated%20in%20natural%20language.%20Large%20Language%20Models%20%28LLMs%29%20are%0Abeneficial%20solutions%20for%20user-robot%20interaction%20due%20to%20their%20natural%20language%0Aunderstanding%20and%20reasoning%20abilities.%20Recent%20methods%20for%20creating%20learnable%0Arepresentations%20of%203D%20scenes%20have%20demonstrated%20the%20potential%20to%20improve%20the%0Aquality%20of%20LLMs%20responses%20by%20adapting%20to%20the%203D%20world.%20However%2C%20the%20existing%0Amethods%20do%20not%20explicitly%20utilize%20information%20about%20the%20semantic%20relationships%0Abetween%20objects%2C%20limiting%20themselves%20to%20information%20about%20their%20coordinates.%20In%0Athis%20work%2C%20we%20propose%20a%20method%203DGraphLLM%20for%20constructing%20a%20learnable%0Arepresentation%20of%20a%203D%20scene%20graph.%20The%20learnable%20representation%20is%20used%20as%0Ainput%20for%20LLMs%20to%20perform%203D%20vision-language%20tasks.%20In%20our%20experiments%20on%0Apopular%20ScanRefer%2C%20RIORefer%2C%20Multi3DRefer%2C%20ScanQA%2C%20Sqa3D%2C%20and%20Scan2cap%0Adatasets%2C%20we%20demonstrate%20the%20advantage%20of%20this%20approach%20over%20baseline%20methods%0Athat%20do%20not%20use%20information%20about%20the%20semantic%20relationships%20between%20objects.%0AThe%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CognitiveAISystems/3DGraphLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18450v1&entry.124074799=Read"},
{"title": "3D Shape Tokenization", "author": "Jen-Hao Rick Chang and Yuyang Wang and Miguel Angel Bautista Martin and Jiatao Gu and Josh Susskind and Oncel Tuzel", "abstract": "  We introduce Shape Tokens, a 3D representation that is continuous, compact,\nand easy to incorporate into machine learning models. Shape Tokens act as\nconditioning vectors that represent shape information in a 3D flow-matching\nmodel. The flow-matching model is trained to approximate probability density\nfunctions corresponding to delta functions concentrated on the surfaces of\nshapes in 3D. By attaching Shape Tokens to various machine learning models, we\ncan generate new shapes, convert images to 3D, align 3D shapes with text and\nimages, and render shapes directly at variable, user specified, resolution.\nMoreover, Shape Tokens enable a systematic analysis of geometric properties\nsuch as normal, density, and deformation field. Across all tasks and\nexperiments, utilizing Shape Tokens demonstrate strong performance compared to\nexisting baselines.\n", "link": "http://arxiv.org/abs/2412.15618v2", "date": "2024-12-24", "relevancy": 2.6727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5847}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5224}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Shape%20Tokenization&body=Title%3A%203D%20Shape%20Tokenization%0AAuthor%3A%20Jen-Hao%20Rick%20Chang%20and%20Yuyang%20Wang%20and%20Miguel%20Angel%20Bautista%20Martin%20and%20Jiatao%20Gu%20and%20Josh%20Susskind%20and%20Oncel%20Tuzel%0AAbstract%3A%20%20%20We%20introduce%20Shape%20Tokens%2C%20a%203D%20representation%20that%20is%20continuous%2C%20compact%2C%0Aand%20easy%20to%20incorporate%20into%20machine%20learning%20models.%20Shape%20Tokens%20act%20as%0Aconditioning%20vectors%20that%20represent%20shape%20information%20in%20a%203D%20flow-matching%0Amodel.%20The%20flow-matching%20model%20is%20trained%20to%20approximate%20probability%20density%0Afunctions%20corresponding%20to%20delta%20functions%20concentrated%20on%20the%20surfaces%20of%0Ashapes%20in%203D.%20By%20attaching%20Shape%20Tokens%20to%20various%20machine%20learning%20models%2C%20we%0Acan%20generate%20new%20shapes%2C%20convert%20images%20to%203D%2C%20align%203D%20shapes%20with%20text%20and%0Aimages%2C%20and%20render%20shapes%20directly%20at%20variable%2C%20user%20specified%2C%20resolution.%0AMoreover%2C%20Shape%20Tokens%20enable%20a%20systematic%20analysis%20of%20geometric%20properties%0Asuch%20as%20normal%2C%20density%2C%20and%20deformation%20field.%20Across%20all%20tasks%20and%0Aexperiments%2C%20utilizing%20Shape%20Tokens%20demonstrate%20strong%20performance%20compared%20to%0Aexisting%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Shape%2520Tokenization%26entry.906535625%3DJen-Hao%2520Rick%2520Chang%2520and%2520Yuyang%2520Wang%2520and%2520Miguel%2520Angel%2520Bautista%2520Martin%2520and%2520Jiatao%2520Gu%2520and%2520Josh%2520Susskind%2520and%2520Oncel%2520Tuzel%26entry.1292438233%3D%2520%2520We%2520introduce%2520Shape%2520Tokens%252C%2520a%25203D%2520representation%2520that%2520is%2520continuous%252C%2520compact%252C%250Aand%2520easy%2520to%2520incorporate%2520into%2520machine%2520learning%2520models.%2520Shape%2520Tokens%2520act%2520as%250Aconditioning%2520vectors%2520that%2520represent%2520shape%2520information%2520in%2520a%25203D%2520flow-matching%250Amodel.%2520The%2520flow-matching%2520model%2520is%2520trained%2520to%2520approximate%2520probability%2520density%250Afunctions%2520corresponding%2520to%2520delta%2520functions%2520concentrated%2520on%2520the%2520surfaces%2520of%250Ashapes%2520in%25203D.%2520By%2520attaching%2520Shape%2520Tokens%2520to%2520various%2520machine%2520learning%2520models%252C%2520we%250Acan%2520generate%2520new%2520shapes%252C%2520convert%2520images%2520to%25203D%252C%2520align%25203D%2520shapes%2520with%2520text%2520and%250Aimages%252C%2520and%2520render%2520shapes%2520directly%2520at%2520variable%252C%2520user%2520specified%252C%2520resolution.%250AMoreover%252C%2520Shape%2520Tokens%2520enable%2520a%2520systematic%2520analysis%2520of%2520geometric%2520properties%250Asuch%2520as%2520normal%252C%2520density%252C%2520and%2520deformation%2520field.%2520Across%2520all%2520tasks%2520and%250Aexperiments%252C%2520utilizing%2520Shape%2520Tokens%2520demonstrate%2520strong%2520performance%2520compared%2520to%250Aexisting%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Shape%20Tokenization&entry.906535625=Jen-Hao%20Rick%20Chang%20and%20Yuyang%20Wang%20and%20Miguel%20Angel%20Bautista%20Martin%20and%20Jiatao%20Gu%20and%20Josh%20Susskind%20and%20Oncel%20Tuzel&entry.1292438233=%20%20We%20introduce%20Shape%20Tokens%2C%20a%203D%20representation%20that%20is%20continuous%2C%20compact%2C%0Aand%20easy%20to%20incorporate%20into%20machine%20learning%20models.%20Shape%20Tokens%20act%20as%0Aconditioning%20vectors%20that%20represent%20shape%20information%20in%20a%203D%20flow-matching%0Amodel.%20The%20flow-matching%20model%20is%20trained%20to%20approximate%20probability%20density%0Afunctions%20corresponding%20to%20delta%20functions%20concentrated%20on%20the%20surfaces%20of%0Ashapes%20in%203D.%20By%20attaching%20Shape%20Tokens%20to%20various%20machine%20learning%20models%2C%20we%0Acan%20generate%20new%20shapes%2C%20convert%20images%20to%203D%2C%20align%203D%20shapes%20with%20text%20and%0Aimages%2C%20and%20render%20shapes%20directly%20at%20variable%2C%20user%20specified%2C%20resolution.%0AMoreover%2C%20Shape%20Tokens%20enable%20a%20systematic%20analysis%20of%20geometric%20properties%0Asuch%20as%20normal%2C%20density%2C%20and%20deformation%20field.%20Across%20all%20tasks%20and%0Aexperiments%2C%20utilizing%20Shape%20Tokens%20demonstrate%20strong%20performance%20compared%20to%0Aexisting%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15618v2&entry.124074799=Read"},
{"title": "Point-DeepONet: A Deep Operator Network Integrating PointNet for\n  Nonlinear Analysis of Non-Parametric 3D Geometries and Load Conditions", "author": "Jangseop Park and Namwoo Kang", "abstract": "  Nonlinear structural analyses in engineering often require extensive finite\nelement simulations, limiting their applicability in design optimization,\nuncertainty quantification, and real-time control. Conventional deep learning\nsurrogates, such as convolutional neural networks (CNNs), physics-informed\nneural networks (PINNs), and fourier neural operators (FNOs), face challenges\nwith complex non-parametric three-dimensional (3D) geometries, directionally\nvarying loads, and high-fidelity predictions on unstructured meshes. This work\npresents Point-DeepONet, an operator-learning-based surrogate that integrates\nPointNet into the DeepONet framework. By directly processing non-parametric\npoint clouds and incorporating signed distance functions (SDF) for geometric\ncontext, Point-DeepONet accurately predicts three-dimensional displacement and\nvon Mises stress fields without mesh parameterization or retraining. Trained\nusing only about 5,000 nodes (2.5% of the original 200,000-node mesh),\nPoint-DeepONet can still predict the entire mesh at high fidelity, achieving a\ncoefficient of determination reaching 0.987 for displacement and 0.923 for von\nMises stress under a horizontal load case. Compared to nonlinear finite element\nanalyses that require about 19.32 minutes per case, Point-DeepONet provides\npredictions in mere seconds-approximately 400 times faster-while maintaining\nexcellent scalability and accuracy with increasing dataset sizes. These\nfindings highlight the potential of Point-DeepONet to enable rapid,\nhigh-fidelity structural analyses, ultimately supporting more effective design\nexploration and informed decision-making in complex engineering workflows.\n", "link": "http://arxiv.org/abs/2412.18362v1", "date": "2024-12-24", "relevancy": 2.6715, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-DeepONet%3A%20A%20Deep%20Operator%20Network%20Integrating%20PointNet%20for%0A%20%20Nonlinear%20Analysis%20of%20Non-Parametric%203D%20Geometries%20and%20Load%20Conditions&body=Title%3A%20Point-DeepONet%3A%20A%20Deep%20Operator%20Network%20Integrating%20PointNet%20for%0A%20%20Nonlinear%20Analysis%20of%20Non-Parametric%203D%20Geometries%20and%20Load%20Conditions%0AAuthor%3A%20Jangseop%20Park%20and%20Namwoo%20Kang%0AAbstract%3A%20%20%20Nonlinear%20structural%20analyses%20in%20engineering%20often%20require%20extensive%20finite%0Aelement%20simulations%2C%20limiting%20their%20applicability%20in%20design%20optimization%2C%0Auncertainty%20quantification%2C%20and%20real-time%20control.%20Conventional%20deep%20learning%0Asurrogates%2C%20such%20as%20convolutional%20neural%20networks%20%28CNNs%29%2C%20physics-informed%0Aneural%20networks%20%28PINNs%29%2C%20and%20fourier%20neural%20operators%20%28FNOs%29%2C%20face%20challenges%0Awith%20complex%20non-parametric%20three-dimensional%20%283D%29%20geometries%2C%20directionally%0Avarying%20loads%2C%20and%20high-fidelity%20predictions%20on%20unstructured%20meshes.%20This%20work%0Apresents%20Point-DeepONet%2C%20an%20operator-learning-based%20surrogate%20that%20integrates%0APointNet%20into%20the%20DeepONet%20framework.%20By%20directly%20processing%20non-parametric%0Apoint%20clouds%20and%20incorporating%20signed%20distance%20functions%20%28SDF%29%20for%20geometric%0Acontext%2C%20Point-DeepONet%20accurately%20predicts%20three-dimensional%20displacement%20and%0Avon%20Mises%20stress%20fields%20without%20mesh%20parameterization%20or%20retraining.%20Trained%0Ausing%20only%20about%205%2C000%20nodes%20%282.5%25%20of%20the%20original%20200%2C000-node%20mesh%29%2C%0APoint-DeepONet%20can%20still%20predict%20the%20entire%20mesh%20at%20high%20fidelity%2C%20achieving%20a%0Acoefficient%20of%20determination%20reaching%200.987%20for%20displacement%20and%200.923%20for%20von%0AMises%20stress%20under%20a%20horizontal%20load%20case.%20Compared%20to%20nonlinear%20finite%20element%0Aanalyses%20that%20require%20about%2019.32%20minutes%20per%20case%2C%20Point-DeepONet%20provides%0Apredictions%20in%20mere%20seconds-approximately%20400%20times%20faster-while%20maintaining%0Aexcellent%20scalability%20and%20accuracy%20with%20increasing%20dataset%20sizes.%20These%0Afindings%20highlight%20the%20potential%20of%20Point-DeepONet%20to%20enable%20rapid%2C%0Ahigh-fidelity%20structural%20analyses%2C%20ultimately%20supporting%20more%20effective%20design%0Aexploration%20and%20informed%20decision-making%20in%20complex%20engineering%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-DeepONet%253A%2520A%2520Deep%2520Operator%2520Network%2520Integrating%2520PointNet%2520for%250A%2520%2520Nonlinear%2520Analysis%2520of%2520Non-Parametric%25203D%2520Geometries%2520and%2520Load%2520Conditions%26entry.906535625%3DJangseop%2520Park%2520and%2520Namwoo%2520Kang%26entry.1292438233%3D%2520%2520Nonlinear%2520structural%2520analyses%2520in%2520engineering%2520often%2520require%2520extensive%2520finite%250Aelement%2520simulations%252C%2520limiting%2520their%2520applicability%2520in%2520design%2520optimization%252C%250Auncertainty%2520quantification%252C%2520and%2520real-time%2520control.%2520Conventional%2520deep%2520learning%250Asurrogates%252C%2520such%2520as%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520physics-informed%250Aneural%2520networks%2520%2528PINNs%2529%252C%2520and%2520fourier%2520neural%2520operators%2520%2528FNOs%2529%252C%2520face%2520challenges%250Awith%2520complex%2520non-parametric%2520three-dimensional%2520%25283D%2529%2520geometries%252C%2520directionally%250Avarying%2520loads%252C%2520and%2520high-fidelity%2520predictions%2520on%2520unstructured%2520meshes.%2520This%2520work%250Apresents%2520Point-DeepONet%252C%2520an%2520operator-learning-based%2520surrogate%2520that%2520integrates%250APointNet%2520into%2520the%2520DeepONet%2520framework.%2520By%2520directly%2520processing%2520non-parametric%250Apoint%2520clouds%2520and%2520incorporating%2520signed%2520distance%2520functions%2520%2528SDF%2529%2520for%2520geometric%250Acontext%252C%2520Point-DeepONet%2520accurately%2520predicts%2520three-dimensional%2520displacement%2520and%250Avon%2520Mises%2520stress%2520fields%2520without%2520mesh%2520parameterization%2520or%2520retraining.%2520Trained%250Ausing%2520only%2520about%25205%252C000%2520nodes%2520%25282.5%2525%2520of%2520the%2520original%2520200%252C000-node%2520mesh%2529%252C%250APoint-DeepONet%2520can%2520still%2520predict%2520the%2520entire%2520mesh%2520at%2520high%2520fidelity%252C%2520achieving%2520a%250Acoefficient%2520of%2520determination%2520reaching%25200.987%2520for%2520displacement%2520and%25200.923%2520for%2520von%250AMises%2520stress%2520under%2520a%2520horizontal%2520load%2520case.%2520Compared%2520to%2520nonlinear%2520finite%2520element%250Aanalyses%2520that%2520require%2520about%252019.32%2520minutes%2520per%2520case%252C%2520Point-DeepONet%2520provides%250Apredictions%2520in%2520mere%2520seconds-approximately%2520400%2520times%2520faster-while%2520maintaining%250Aexcellent%2520scalability%2520and%2520accuracy%2520with%2520increasing%2520dataset%2520sizes.%2520These%250Afindings%2520highlight%2520the%2520potential%2520of%2520Point-DeepONet%2520to%2520enable%2520rapid%252C%250Ahigh-fidelity%2520structural%2520analyses%252C%2520ultimately%2520supporting%2520more%2520effective%2520design%250Aexploration%2520and%2520informed%2520decision-making%2520in%2520complex%2520engineering%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-DeepONet%3A%20A%20Deep%20Operator%20Network%20Integrating%20PointNet%20for%0A%20%20Nonlinear%20Analysis%20of%20Non-Parametric%203D%20Geometries%20and%20Load%20Conditions&entry.906535625=Jangseop%20Park%20and%20Namwoo%20Kang&entry.1292438233=%20%20Nonlinear%20structural%20analyses%20in%20engineering%20often%20require%20extensive%20finite%0Aelement%20simulations%2C%20limiting%20their%20applicability%20in%20design%20optimization%2C%0Auncertainty%20quantification%2C%20and%20real-time%20control.%20Conventional%20deep%20learning%0Asurrogates%2C%20such%20as%20convolutional%20neural%20networks%20%28CNNs%29%2C%20physics-informed%0Aneural%20networks%20%28PINNs%29%2C%20and%20fourier%20neural%20operators%20%28FNOs%29%2C%20face%20challenges%0Awith%20complex%20non-parametric%20three-dimensional%20%283D%29%20geometries%2C%20directionally%0Avarying%20loads%2C%20and%20high-fidelity%20predictions%20on%20unstructured%20meshes.%20This%20work%0Apresents%20Point-DeepONet%2C%20an%20operator-learning-based%20surrogate%20that%20integrates%0APointNet%20into%20the%20DeepONet%20framework.%20By%20directly%20processing%20non-parametric%0Apoint%20clouds%20and%20incorporating%20signed%20distance%20functions%20%28SDF%29%20for%20geometric%0Acontext%2C%20Point-DeepONet%20accurately%20predicts%20three-dimensional%20displacement%20and%0Avon%20Mises%20stress%20fields%20without%20mesh%20parameterization%20or%20retraining.%20Trained%0Ausing%20only%20about%205%2C000%20nodes%20%282.5%25%20of%20the%20original%20200%2C000-node%20mesh%29%2C%0APoint-DeepONet%20can%20still%20predict%20the%20entire%20mesh%20at%20high%20fidelity%2C%20achieving%20a%0Acoefficient%20of%20determination%20reaching%200.987%20for%20displacement%20and%200.923%20for%20von%0AMises%20stress%20under%20a%20horizontal%20load%20case.%20Compared%20to%20nonlinear%20finite%20element%0Aanalyses%20that%20require%20about%2019.32%20minutes%20per%20case%2C%20Point-DeepONet%20provides%0Apredictions%20in%20mere%20seconds-approximately%20400%20times%20faster-while%20maintaining%0Aexcellent%20scalability%20and%20accuracy%20with%20increasing%20dataset%20sizes.%20These%0Afindings%20highlight%20the%20potential%20of%20Point-DeepONet%20to%20enable%20rapid%2C%0Ahigh-fidelity%20structural%20analyses%2C%20ultimately%20supporting%20more%20effective%20design%0Aexploration%20and%20informed%20decision-making%20in%20complex%20engineering%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18362v1&entry.124074799=Read"},
{"title": "HAUR: Human Annotation Understanding and Recognition Through Text-Heavy\n  Images", "author": "Yuchen Yang and Haoran Yan and Yanhao Chen and Qingqiang Wu and Qingqi Hong", "abstract": "  Vision Question Answering (VQA) tasks use images to convey critical\ninformation to answer text-based questions, which is one of the most common\nforms of question answering in real-world scenarios. Numerous vision-text\nmodels exist today and have performed well on certain VQA tasks. However, these\nmodels exhibit significant limitations in understanding human annotations on\ntext-heavy images. To address this, we propose the Human Annotation\nUnderstanding and Recognition (HAUR) task. As part of this effort, we introduce\nthe Human Annotation Understanding and Recognition-5 (HAUR-5) dataset, which\nencompasses five common types of human annotations. Additionally, we developed\nand trained our model, OCR-Mix. Through comprehensive cross-model comparisons,\nour results demonstrate that OCR-Mix outperforms other models in this task. Our\ndataset and model will be released soon .\n", "link": "http://arxiv.org/abs/2412.18327v1", "date": "2024-12-24", "relevancy": 2.6172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAUR%3A%20Human%20Annotation%20Understanding%20and%20Recognition%20Through%20Text-Heavy%0A%20%20Images&body=Title%3A%20HAUR%3A%20Human%20Annotation%20Understanding%20and%20Recognition%20Through%20Text-Heavy%0A%20%20Images%0AAuthor%3A%20Yuchen%20Yang%20and%20Haoran%20Yan%20and%20Yanhao%20Chen%20and%20Qingqiang%20Wu%20and%20Qingqi%20Hong%0AAbstract%3A%20%20%20Vision%20Question%20Answering%20%28VQA%29%20tasks%20use%20images%20to%20convey%20critical%0Ainformation%20to%20answer%20text-based%20questions%2C%20which%20is%20one%20of%20the%20most%20common%0Aforms%20of%20question%20answering%20in%20real-world%20scenarios.%20Numerous%20vision-text%0Amodels%20exist%20today%20and%20have%20performed%20well%20on%20certain%20VQA%20tasks.%20However%2C%20these%0Amodels%20exhibit%20significant%20limitations%20in%20understanding%20human%20annotations%20on%0Atext-heavy%20images.%20To%20address%20this%2C%20we%20propose%20the%20Human%20Annotation%0AUnderstanding%20and%20Recognition%20%28HAUR%29%20task.%20As%20part%20of%20this%20effort%2C%20we%20introduce%0Athe%20Human%20Annotation%20Understanding%20and%20Recognition-5%20%28HAUR-5%29%20dataset%2C%20which%0Aencompasses%20five%20common%20types%20of%20human%20annotations.%20Additionally%2C%20we%20developed%0Aand%20trained%20our%20model%2C%20OCR-Mix.%20Through%20comprehensive%20cross-model%20comparisons%2C%0Aour%20results%20demonstrate%20that%20OCR-Mix%20outperforms%20other%20models%20in%20this%20task.%20Our%0Adataset%20and%20model%20will%20be%20released%20soon%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAUR%253A%2520Human%2520Annotation%2520Understanding%2520and%2520Recognition%2520Through%2520Text-Heavy%250A%2520%2520Images%26entry.906535625%3DYuchen%2520Yang%2520and%2520Haoran%2520Yan%2520and%2520Yanhao%2520Chen%2520and%2520Qingqiang%2520Wu%2520and%2520Qingqi%2520Hong%26entry.1292438233%3D%2520%2520Vision%2520Question%2520Answering%2520%2528VQA%2529%2520tasks%2520use%2520images%2520to%2520convey%2520critical%250Ainformation%2520to%2520answer%2520text-based%2520questions%252C%2520which%2520is%2520one%2520of%2520the%2520most%2520common%250Aforms%2520of%2520question%2520answering%2520in%2520real-world%2520scenarios.%2520Numerous%2520vision-text%250Amodels%2520exist%2520today%2520and%2520have%2520performed%2520well%2520on%2520certain%2520VQA%2520tasks.%2520However%252C%2520these%250Amodels%2520exhibit%2520significant%2520limitations%2520in%2520understanding%2520human%2520annotations%2520on%250Atext-heavy%2520images.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Human%2520Annotation%250AUnderstanding%2520and%2520Recognition%2520%2528HAUR%2529%2520task.%2520As%2520part%2520of%2520this%2520effort%252C%2520we%2520introduce%250Athe%2520Human%2520Annotation%2520Understanding%2520and%2520Recognition-5%2520%2528HAUR-5%2529%2520dataset%252C%2520which%250Aencompasses%2520five%2520common%2520types%2520of%2520human%2520annotations.%2520Additionally%252C%2520we%2520developed%250Aand%2520trained%2520our%2520model%252C%2520OCR-Mix.%2520Through%2520comprehensive%2520cross-model%2520comparisons%252C%250Aour%2520results%2520demonstrate%2520that%2520OCR-Mix%2520outperforms%2520other%2520models%2520in%2520this%2520task.%2520Our%250Adataset%2520and%2520model%2520will%2520be%2520released%2520soon%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAUR%3A%20Human%20Annotation%20Understanding%20and%20Recognition%20Through%20Text-Heavy%0A%20%20Images&entry.906535625=Yuchen%20Yang%20and%20Haoran%20Yan%20and%20Yanhao%20Chen%20and%20Qingqiang%20Wu%20and%20Qingqi%20Hong&entry.1292438233=%20%20Vision%20Question%20Answering%20%28VQA%29%20tasks%20use%20images%20to%20convey%20critical%0Ainformation%20to%20answer%20text-based%20questions%2C%20which%20is%20one%20of%20the%20most%20common%0Aforms%20of%20question%20answering%20in%20real-world%20scenarios.%20Numerous%20vision-text%0Amodels%20exist%20today%20and%20have%20performed%20well%20on%20certain%20VQA%20tasks.%20However%2C%20these%0Amodels%20exhibit%20significant%20limitations%20in%20understanding%20human%20annotations%20on%0Atext-heavy%20images.%20To%20address%20this%2C%20we%20propose%20the%20Human%20Annotation%0AUnderstanding%20and%20Recognition%20%28HAUR%29%20task.%20As%20part%20of%20this%20effort%2C%20we%20introduce%0Athe%20Human%20Annotation%20Understanding%20and%20Recognition-5%20%28HAUR-5%29%20dataset%2C%20which%0Aencompasses%20five%20common%20types%20of%20human%20annotations.%20Additionally%2C%20we%20developed%0Aand%20trained%20our%20model%2C%20OCR-Mix.%20Through%20comprehensive%20cross-model%20comparisons%2C%0Aour%20results%20demonstrate%20that%20OCR-Mix%20outperforms%20other%20models%20in%20this%20task.%20Our%0Adataset%20and%20model%20will%20be%20released%20soon%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18327v1&entry.124074799=Read"},
{"title": "Exploring Embedding Priors in Prompt-Tuning for Improved\n  Interpretability and Control", "author": "Sergey Sedov and Sumanth Bharadwaj Hachalli Karanam and Venu Gopal Kadamba", "abstract": "  Prompt-Tuning is an efficient method for adapting pre-trained language models\nto new tasks with minimal computational overhead by modifying prompt\nembeddings. In this work, we investigate how crucial the phenomenon of\nembedding collapse, frequently observed in Prompt-Tuning, is for the final\nperformance of the model. To address this question, we designed embedding\npriors and compared them with posteriors of the converged Soft and Deep\nPrompt-Tuning methods. Our findings suggest that priors strongly affect the\nposition of the tuned embeddings, and models can effectively work with\nembeddings from different parts of activation spaces, including completely new\nregions. As the final Prompt-Tuning capabilities are limited, we hypothesize\nthat controllable Prompt-Tuning posteriors may serve as a good starting point\nfor tasks such as chain-of-thought (COT) distillation. Our experiments also\nshow that generated trajectories are not localized in the activation space of\nthe models. However, there are distinct clusters of activations for distant\ntasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,\nQuestion-Answering and MLM) lie in the same cluster. These observations raise\nquestions about the importance of a single activation cluster for the\ngeneralization abilities of large language models.\n", "link": "http://arxiv.org/abs/2412.18582v1", "date": "2024-12-24", "relevancy": 2.5989, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Embedding%20Priors%20in%20Prompt-Tuning%20for%20Improved%0A%20%20Interpretability%20and%20Control&body=Title%3A%20Exploring%20Embedding%20Priors%20in%20Prompt-Tuning%20for%20Improved%0A%20%20Interpretability%20and%20Control%0AAuthor%3A%20Sergey%20Sedov%20and%20Sumanth%20Bharadwaj%20Hachalli%20Karanam%20and%20Venu%20Gopal%20Kadamba%0AAbstract%3A%20%20%20Prompt-Tuning%20is%20an%20efficient%20method%20for%20adapting%20pre-trained%20language%20models%0Ato%20new%20tasks%20with%20minimal%20computational%20overhead%20by%20modifying%20prompt%0Aembeddings.%20In%20this%20work%2C%20we%20investigate%20how%20crucial%20the%20phenomenon%20of%0Aembedding%20collapse%2C%20frequently%20observed%20in%20Prompt-Tuning%2C%20is%20for%20the%20final%0Aperformance%20of%20the%20model.%20To%20address%20this%20question%2C%20we%20designed%20embedding%0Apriors%20and%20compared%20them%20with%20posteriors%20of%20the%20converged%20Soft%20and%20Deep%0APrompt-Tuning%20methods.%20Our%20findings%20suggest%20that%20priors%20strongly%20affect%20the%0Aposition%20of%20the%20tuned%20embeddings%2C%20and%20models%20can%20effectively%20work%20with%0Aembeddings%20from%20different%20parts%20of%20activation%20spaces%2C%20including%20completely%20new%0Aregions.%20As%20the%20final%20Prompt-Tuning%20capabilities%20are%20limited%2C%20we%20hypothesize%0Athat%20controllable%20Prompt-Tuning%20posteriors%20may%20serve%20as%20a%20good%20starting%20point%0Afor%20tasks%20such%20as%20chain-of-thought%20%28COT%29%20distillation.%20Our%20experiments%20also%0Ashow%20that%20generated%20trajectories%20are%20not%20localized%20in%20the%20activation%20space%20of%0Athe%20models.%20However%2C%20there%20are%20distinct%20clusters%20of%20activations%20for%20distant%0Atasks%20%28e.g.%2C%20NLP%20and%20arithmetic%29%2C%20while%20activations%20between%20NLP%20tasks%20%28e.g.%2C%0AQuestion-Answering%20and%20MLM%29%20lie%20in%20the%20same%20cluster.%20These%20observations%20raise%0Aquestions%20about%20the%20importance%20of%20a%20single%20activation%20cluster%20for%20the%0Ageneralization%20abilities%20of%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Embedding%2520Priors%2520in%2520Prompt-Tuning%2520for%2520Improved%250A%2520%2520Interpretability%2520and%2520Control%26entry.906535625%3DSergey%2520Sedov%2520and%2520Sumanth%2520Bharadwaj%2520Hachalli%2520Karanam%2520and%2520Venu%2520Gopal%2520Kadamba%26entry.1292438233%3D%2520%2520Prompt-Tuning%2520is%2520an%2520efficient%2520method%2520for%2520adapting%2520pre-trained%2520language%2520models%250Ato%2520new%2520tasks%2520with%2520minimal%2520computational%2520overhead%2520by%2520modifying%2520prompt%250Aembeddings.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520crucial%2520the%2520phenomenon%2520of%250Aembedding%2520collapse%252C%2520frequently%2520observed%2520in%2520Prompt-Tuning%252C%2520is%2520for%2520the%2520final%250Aperformance%2520of%2520the%2520model.%2520To%2520address%2520this%2520question%252C%2520we%2520designed%2520embedding%250Apriors%2520and%2520compared%2520them%2520with%2520posteriors%2520of%2520the%2520converged%2520Soft%2520and%2520Deep%250APrompt-Tuning%2520methods.%2520Our%2520findings%2520suggest%2520that%2520priors%2520strongly%2520affect%2520the%250Aposition%2520of%2520the%2520tuned%2520embeddings%252C%2520and%2520models%2520can%2520effectively%2520work%2520with%250Aembeddings%2520from%2520different%2520parts%2520of%2520activation%2520spaces%252C%2520including%2520completely%2520new%250Aregions.%2520As%2520the%2520final%2520Prompt-Tuning%2520capabilities%2520are%2520limited%252C%2520we%2520hypothesize%250Athat%2520controllable%2520Prompt-Tuning%2520posteriors%2520may%2520serve%2520as%2520a%2520good%2520starting%2520point%250Afor%2520tasks%2520such%2520as%2520chain-of-thought%2520%2528COT%2529%2520distillation.%2520Our%2520experiments%2520also%250Ashow%2520that%2520generated%2520trajectories%2520are%2520not%2520localized%2520in%2520the%2520activation%2520space%2520of%250Athe%2520models.%2520However%252C%2520there%2520are%2520distinct%2520clusters%2520of%2520activations%2520for%2520distant%250Atasks%2520%2528e.g.%252C%2520NLP%2520and%2520arithmetic%2529%252C%2520while%2520activations%2520between%2520NLP%2520tasks%2520%2528e.g.%252C%250AQuestion-Answering%2520and%2520MLM%2529%2520lie%2520in%2520the%2520same%2520cluster.%2520These%2520observations%2520raise%250Aquestions%2520about%2520the%2520importance%2520of%2520a%2520single%2520activation%2520cluster%2520for%2520the%250Ageneralization%2520abilities%2520of%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Embedding%20Priors%20in%20Prompt-Tuning%20for%20Improved%0A%20%20Interpretability%20and%20Control&entry.906535625=Sergey%20Sedov%20and%20Sumanth%20Bharadwaj%20Hachalli%20Karanam%20and%20Venu%20Gopal%20Kadamba&entry.1292438233=%20%20Prompt-Tuning%20is%20an%20efficient%20method%20for%20adapting%20pre-trained%20language%20models%0Ato%20new%20tasks%20with%20minimal%20computational%20overhead%20by%20modifying%20prompt%0Aembeddings.%20In%20this%20work%2C%20we%20investigate%20how%20crucial%20the%20phenomenon%20of%0Aembedding%20collapse%2C%20frequently%20observed%20in%20Prompt-Tuning%2C%20is%20for%20the%20final%0Aperformance%20of%20the%20model.%20To%20address%20this%20question%2C%20we%20designed%20embedding%0Apriors%20and%20compared%20them%20with%20posteriors%20of%20the%20converged%20Soft%20and%20Deep%0APrompt-Tuning%20methods.%20Our%20findings%20suggest%20that%20priors%20strongly%20affect%20the%0Aposition%20of%20the%20tuned%20embeddings%2C%20and%20models%20can%20effectively%20work%20with%0Aembeddings%20from%20different%20parts%20of%20activation%20spaces%2C%20including%20completely%20new%0Aregions.%20As%20the%20final%20Prompt-Tuning%20capabilities%20are%20limited%2C%20we%20hypothesize%0Athat%20controllable%20Prompt-Tuning%20posteriors%20may%20serve%20as%20a%20good%20starting%20point%0Afor%20tasks%20such%20as%20chain-of-thought%20%28COT%29%20distillation.%20Our%20experiments%20also%0Ashow%20that%20generated%20trajectories%20are%20not%20localized%20in%20the%20activation%20space%20of%0Athe%20models.%20However%2C%20there%20are%20distinct%20clusters%20of%20activations%20for%20distant%0Atasks%20%28e.g.%2C%20NLP%20and%20arithmetic%29%2C%20while%20activations%20between%20NLP%20tasks%20%28e.g.%2C%0AQuestion-Answering%20and%20MLM%29%20lie%20in%20the%20same%20cluster.%20These%20observations%20raise%0Aquestions%20about%20the%20importance%20of%20a%20single%20activation%20cluster%20for%20the%0Ageneralization%20abilities%20of%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18582v1&entry.124074799=Read"},
{"title": "Structure Learning in Gaussian Graphical Models from Glauber Dynamics", "author": "Vignesh Tirukkonda and Anirudh Rayas and Gautam Dasarathy", "abstract": "  Gaussian graphical model selection is an important paradigm with numerous\napplications, including biological network modeling, financial network\nmodeling, and social network analysis. Traditional approaches assume access to\nindependent and identically distributed (i.i.d) samples, which is often\nimpractical in real-world scenarios. In this paper, we address Gaussian\ngraphical model selection under observations from a more realistic dependent\nstochastic process known as Glauber dynamics. Glauber dynamics, also called the\nGibbs sampler, is a Markov chain that sequentially updates the variables of the\nunderlying model based on the statistics of the remaining model. Such models,\naside from frequently being employed to generate samples from complex\nmultivariate distributions, naturally arise in various settings, such as\nopinion consensus in social networks and clearing/stock-price dynamics in\nfinancial networks.\n  In contrast to the extensive body of existing work, we present the first\nalgorithm for Gaussian graphical model selection when data are sampled\naccording to the Glauber dynamics. We provide theoretical guarantees on the\ncomputational and statistical complexity of the proposed algorithm's structure\nlearning performance. Additionally, we provide information-theoretic lower\nbounds on the statistical complexity and show that our algorithm is nearly\nminimax optimal for a broad class of problems.\n", "link": "http://arxiv.org/abs/2412.18594v1", "date": "2024-12-24", "relevancy": 2.5886, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5594}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4969}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics&body=Title%3A%20Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics%0AAuthor%3A%20Vignesh%20Tirukkonda%20and%20Anirudh%20Rayas%20and%20Gautam%20Dasarathy%0AAbstract%3A%20%20%20Gaussian%20graphical%20model%20selection%20is%20an%20important%20paradigm%20with%20numerous%0Aapplications%2C%20including%20biological%20network%20modeling%2C%20financial%20network%0Amodeling%2C%20and%20social%20network%20analysis.%20Traditional%20approaches%20assume%20access%20to%0Aindependent%20and%20identically%20distributed%20%28i.i.d%29%20samples%2C%20which%20is%20often%0Aimpractical%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20address%20Gaussian%0Agraphical%20model%20selection%20under%20observations%20from%20a%20more%20realistic%20dependent%0Astochastic%20process%20known%20as%20Glauber%20dynamics.%20Glauber%20dynamics%2C%20also%20called%20the%0AGibbs%20sampler%2C%20is%20a%20Markov%20chain%20that%20sequentially%20updates%20the%20variables%20of%20the%0Aunderlying%20model%20based%20on%20the%20statistics%20of%20the%20remaining%20model.%20Such%20models%2C%0Aaside%20from%20frequently%20being%20employed%20to%20generate%20samples%20from%20complex%0Amultivariate%20distributions%2C%20naturally%20arise%20in%20various%20settings%2C%20such%20as%0Aopinion%20consensus%20in%20social%20networks%20and%20clearing/stock-price%20dynamics%20in%0Afinancial%20networks.%0A%20%20In%20contrast%20to%20the%20extensive%20body%20of%20existing%20work%2C%20we%20present%20the%20first%0Aalgorithm%20for%20Gaussian%20graphical%20model%20selection%20when%20data%20are%20sampled%0Aaccording%20to%20the%20Glauber%20dynamics.%20We%20provide%20theoretical%20guarantees%20on%20the%0Acomputational%20and%20statistical%20complexity%20of%20the%20proposed%20algorithm%27s%20structure%0Alearning%20performance.%20Additionally%2C%20we%20provide%20information-theoretic%20lower%0Abounds%20on%20the%20statistical%20complexity%20and%20show%20that%20our%20algorithm%20is%20nearly%0Aminimax%20optimal%20for%20a%20broad%20class%20of%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Learning%2520in%2520Gaussian%2520Graphical%2520Models%2520from%2520Glauber%2520Dynamics%26entry.906535625%3DVignesh%2520Tirukkonda%2520and%2520Anirudh%2520Rayas%2520and%2520Gautam%2520Dasarathy%26entry.1292438233%3D%2520%2520Gaussian%2520graphical%2520model%2520selection%2520is%2520an%2520important%2520paradigm%2520with%2520numerous%250Aapplications%252C%2520including%2520biological%2520network%2520modeling%252C%2520financial%2520network%250Amodeling%252C%2520and%2520social%2520network%2520analysis.%2520Traditional%2520approaches%2520assume%2520access%2520to%250Aindependent%2520and%2520identically%2520distributed%2520%2528i.i.d%2529%2520samples%252C%2520which%2520is%2520often%250Aimpractical%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520address%2520Gaussian%250Agraphical%2520model%2520selection%2520under%2520observations%2520from%2520a%2520more%2520realistic%2520dependent%250Astochastic%2520process%2520known%2520as%2520Glauber%2520dynamics.%2520Glauber%2520dynamics%252C%2520also%2520called%2520the%250AGibbs%2520sampler%252C%2520is%2520a%2520Markov%2520chain%2520that%2520sequentially%2520updates%2520the%2520variables%2520of%2520the%250Aunderlying%2520model%2520based%2520on%2520the%2520statistics%2520of%2520the%2520remaining%2520model.%2520Such%2520models%252C%250Aaside%2520from%2520frequently%2520being%2520employed%2520to%2520generate%2520samples%2520from%2520complex%250Amultivariate%2520distributions%252C%2520naturally%2520arise%2520in%2520various%2520settings%252C%2520such%2520as%250Aopinion%2520consensus%2520in%2520social%2520networks%2520and%2520clearing/stock-price%2520dynamics%2520in%250Afinancial%2520networks.%250A%2520%2520In%2520contrast%2520to%2520the%2520extensive%2520body%2520of%2520existing%2520work%252C%2520we%2520present%2520the%2520first%250Aalgorithm%2520for%2520Gaussian%2520graphical%2520model%2520selection%2520when%2520data%2520are%2520sampled%250Aaccording%2520to%2520the%2520Glauber%2520dynamics.%2520We%2520provide%2520theoretical%2520guarantees%2520on%2520the%250Acomputational%2520and%2520statistical%2520complexity%2520of%2520the%2520proposed%2520algorithm%2527s%2520structure%250Alearning%2520performance.%2520Additionally%252C%2520we%2520provide%2520information-theoretic%2520lower%250Abounds%2520on%2520the%2520statistical%2520complexity%2520and%2520show%2520that%2520our%2520algorithm%2520is%2520nearly%250Aminimax%2520optimal%2520for%2520a%2520broad%2520class%2520of%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics&entry.906535625=Vignesh%20Tirukkonda%20and%20Anirudh%20Rayas%20and%20Gautam%20Dasarathy&entry.1292438233=%20%20Gaussian%20graphical%20model%20selection%20is%20an%20important%20paradigm%20with%20numerous%0Aapplications%2C%20including%20biological%20network%20modeling%2C%20financial%20network%0Amodeling%2C%20and%20social%20network%20analysis.%20Traditional%20approaches%20assume%20access%20to%0Aindependent%20and%20identically%20distributed%20%28i.i.d%29%20samples%2C%20which%20is%20often%0Aimpractical%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20address%20Gaussian%0Agraphical%20model%20selection%20under%20observations%20from%20a%20more%20realistic%20dependent%0Astochastic%20process%20known%20as%20Glauber%20dynamics.%20Glauber%20dynamics%2C%20also%20called%20the%0AGibbs%20sampler%2C%20is%20a%20Markov%20chain%20that%20sequentially%20updates%20the%20variables%20of%20the%0Aunderlying%20model%20based%20on%20the%20statistics%20of%20the%20remaining%20model.%20Such%20models%2C%0Aaside%20from%20frequently%20being%20employed%20to%20generate%20samples%20from%20complex%0Amultivariate%20distributions%2C%20naturally%20arise%20in%20various%20settings%2C%20such%20as%0Aopinion%20consensus%20in%20social%20networks%20and%20clearing/stock-price%20dynamics%20in%0Afinancial%20networks.%0A%20%20In%20contrast%20to%20the%20extensive%20body%20of%20existing%20work%2C%20we%20present%20the%20first%0Aalgorithm%20for%20Gaussian%20graphical%20model%20selection%20when%20data%20are%20sampled%0Aaccording%20to%20the%20Glauber%20dynamics.%20We%20provide%20theoretical%20guarantees%20on%20the%0Acomputational%20and%20statistical%20complexity%20of%20the%20proposed%20algorithm%27s%20structure%0Alearning%20performance.%20Additionally%2C%20we%20provide%20information-theoretic%20lower%0Abounds%20on%20the%20statistical%20complexity%20and%20show%20that%20our%20algorithm%20is%20nearly%0Aminimax%20optimal%20for%20a%20broad%20class%20of%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18594v1&entry.124074799=Read"},
{"title": "VORTEX: A Spatial Computing Framework for Optimized Drone Telemetry\n  Extraction from First-Person View Flight Data", "author": "James E. Gallagher and Edward J. Oughton", "abstract": "  This paper presents the Visual Optical Recognition Telemetry EXtraction\n(VORTEX) system for extracting and analyzing drone telemetry data from First\nPerson View (FPV) Uncrewed Aerial System (UAS) footage. VORTEX employs MMOCR, a\nPyTorch-based Optical Character Recognition (OCR) toolbox, to extract telemetry\nvariables from drone Heads Up Display (HUD) recordings, utilizing advanced\nimage preprocessing techniques, including CLAHE enhancement and adaptive\nthresholding. The study optimizes spatial accuracy and computational efficiency\nthrough systematic investigation of temporal sampling rates (1s, 5s, 10s, 15s,\n20s) and coordinate processing methods. Results demonstrate that the 5-second\nsampling rate, utilizing 4.07% of available frames, provides the optimal\nbalance with a point retention rate of 64% and mean speed accuracy within 4.2%\nof the 1-second baseline while reducing computational overhead by 80.5%.\nComparative analysis of coordinate processing methods reveals that while UTM\nZone 33N projection and Haversine calculations provide consistently similar\nresults (within 0.1% difference), raw WGS84 coordinates underestimate distances\nby 15-30% and speeds by 20-35%. Altitude measurements showed unexpected\nresilience to sampling rate variations, with only 2.1% variation across all\nintervals. This research is the first of its kind, providing quantitative\nbenchmarks for establishing a robust framework for drone telemetry extraction\nand analysis using open-source tools and spatial libraries.\n", "link": "http://arxiv.org/abs/2412.18505v1", "date": "2024-12-24", "relevancy": 2.5832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VORTEX%3A%20A%20Spatial%20Computing%20Framework%20for%20Optimized%20Drone%20Telemetry%0A%20%20Extraction%20from%20First-Person%20View%20Flight%20Data&body=Title%3A%20VORTEX%3A%20A%20Spatial%20Computing%20Framework%20for%20Optimized%20Drone%20Telemetry%0A%20%20Extraction%20from%20First-Person%20View%20Flight%20Data%0AAuthor%3A%20James%20E.%20Gallagher%20and%20Edward%20J.%20Oughton%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Visual%20Optical%20Recognition%20Telemetry%20EXtraction%0A%28VORTEX%29%20system%20for%20extracting%20and%20analyzing%20drone%20telemetry%20data%20from%20First%0APerson%20View%20%28FPV%29%20Uncrewed%20Aerial%20System%20%28UAS%29%20footage.%20VORTEX%20employs%20MMOCR%2C%20a%0APyTorch-based%20Optical%20Character%20Recognition%20%28OCR%29%20toolbox%2C%20to%20extract%20telemetry%0Avariables%20from%20drone%20Heads%20Up%20Display%20%28HUD%29%20recordings%2C%20utilizing%20advanced%0Aimage%20preprocessing%20techniques%2C%20including%20CLAHE%20enhancement%20and%20adaptive%0Athresholding.%20The%20study%20optimizes%20spatial%20accuracy%20and%20computational%20efficiency%0Athrough%20systematic%20investigation%20of%20temporal%20sampling%20rates%20%281s%2C%205s%2C%2010s%2C%2015s%2C%0A20s%29%20and%20coordinate%20processing%20methods.%20Results%20demonstrate%20that%20the%205-second%0Asampling%20rate%2C%20utilizing%204.07%25%20of%20available%20frames%2C%20provides%20the%20optimal%0Abalance%20with%20a%20point%20retention%20rate%20of%2064%25%20and%20mean%20speed%20accuracy%20within%204.2%25%0Aof%20the%201-second%20baseline%20while%20reducing%20computational%20overhead%20by%2080.5%25.%0AComparative%20analysis%20of%20coordinate%20processing%20methods%20reveals%20that%20while%20UTM%0AZone%2033N%20projection%20and%20Haversine%20calculations%20provide%20consistently%20similar%0Aresults%20%28within%200.1%25%20difference%29%2C%20raw%20WGS84%20coordinates%20underestimate%20distances%0Aby%2015-30%25%20and%20speeds%20by%2020-35%25.%20Altitude%20measurements%20showed%20unexpected%0Aresilience%20to%20sampling%20rate%20variations%2C%20with%20only%202.1%25%20variation%20across%20all%0Aintervals.%20This%20research%20is%20the%20first%20of%20its%20kind%2C%20providing%20quantitative%0Abenchmarks%20for%20establishing%20a%20robust%20framework%20for%20drone%20telemetry%20extraction%0Aand%20analysis%20using%20open-source%20tools%20and%20spatial%20libraries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVORTEX%253A%2520A%2520Spatial%2520Computing%2520Framework%2520for%2520Optimized%2520Drone%2520Telemetry%250A%2520%2520Extraction%2520from%2520First-Person%2520View%2520Flight%2520Data%26entry.906535625%3DJames%2520E.%2520Gallagher%2520and%2520Edward%2520J.%2520Oughton%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Visual%2520Optical%2520Recognition%2520Telemetry%2520EXtraction%250A%2528VORTEX%2529%2520system%2520for%2520extracting%2520and%2520analyzing%2520drone%2520telemetry%2520data%2520from%2520First%250APerson%2520View%2520%2528FPV%2529%2520Uncrewed%2520Aerial%2520System%2520%2528UAS%2529%2520footage.%2520VORTEX%2520employs%2520MMOCR%252C%2520a%250APyTorch-based%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520toolbox%252C%2520to%2520extract%2520telemetry%250Avariables%2520from%2520drone%2520Heads%2520Up%2520Display%2520%2528HUD%2529%2520recordings%252C%2520utilizing%2520advanced%250Aimage%2520preprocessing%2520techniques%252C%2520including%2520CLAHE%2520enhancement%2520and%2520adaptive%250Athresholding.%2520The%2520study%2520optimizes%2520spatial%2520accuracy%2520and%2520computational%2520efficiency%250Athrough%2520systematic%2520investigation%2520of%2520temporal%2520sampling%2520rates%2520%25281s%252C%25205s%252C%252010s%252C%252015s%252C%250A20s%2529%2520and%2520coordinate%2520processing%2520methods.%2520Results%2520demonstrate%2520that%2520the%25205-second%250Asampling%2520rate%252C%2520utilizing%25204.07%2525%2520of%2520available%2520frames%252C%2520provides%2520the%2520optimal%250Abalance%2520with%2520a%2520point%2520retention%2520rate%2520of%252064%2525%2520and%2520mean%2520speed%2520accuracy%2520within%25204.2%2525%250Aof%2520the%25201-second%2520baseline%2520while%2520reducing%2520computational%2520overhead%2520by%252080.5%2525.%250AComparative%2520analysis%2520of%2520coordinate%2520processing%2520methods%2520reveals%2520that%2520while%2520UTM%250AZone%252033N%2520projection%2520and%2520Haversine%2520calculations%2520provide%2520consistently%2520similar%250Aresults%2520%2528within%25200.1%2525%2520difference%2529%252C%2520raw%2520WGS84%2520coordinates%2520underestimate%2520distances%250Aby%252015-30%2525%2520and%2520speeds%2520by%252020-35%2525.%2520Altitude%2520measurements%2520showed%2520unexpected%250Aresilience%2520to%2520sampling%2520rate%2520variations%252C%2520with%2520only%25202.1%2525%2520variation%2520across%2520all%250Aintervals.%2520This%2520research%2520is%2520the%2520first%2520of%2520its%2520kind%252C%2520providing%2520quantitative%250Abenchmarks%2520for%2520establishing%2520a%2520robust%2520framework%2520for%2520drone%2520telemetry%2520extraction%250Aand%2520analysis%2520using%2520open-source%2520tools%2520and%2520spatial%2520libraries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VORTEX%3A%20A%20Spatial%20Computing%20Framework%20for%20Optimized%20Drone%20Telemetry%0A%20%20Extraction%20from%20First-Person%20View%20Flight%20Data&entry.906535625=James%20E.%20Gallagher%20and%20Edward%20J.%20Oughton&entry.1292438233=%20%20This%20paper%20presents%20the%20Visual%20Optical%20Recognition%20Telemetry%20EXtraction%0A%28VORTEX%29%20system%20for%20extracting%20and%20analyzing%20drone%20telemetry%20data%20from%20First%0APerson%20View%20%28FPV%29%20Uncrewed%20Aerial%20System%20%28UAS%29%20footage.%20VORTEX%20employs%20MMOCR%2C%20a%0APyTorch-based%20Optical%20Character%20Recognition%20%28OCR%29%20toolbox%2C%20to%20extract%20telemetry%0Avariables%20from%20drone%20Heads%20Up%20Display%20%28HUD%29%20recordings%2C%20utilizing%20advanced%0Aimage%20preprocessing%20techniques%2C%20including%20CLAHE%20enhancement%20and%20adaptive%0Athresholding.%20The%20study%20optimizes%20spatial%20accuracy%20and%20computational%20efficiency%0Athrough%20systematic%20investigation%20of%20temporal%20sampling%20rates%20%281s%2C%205s%2C%2010s%2C%2015s%2C%0A20s%29%20and%20coordinate%20processing%20methods.%20Results%20demonstrate%20that%20the%205-second%0Asampling%20rate%2C%20utilizing%204.07%25%20of%20available%20frames%2C%20provides%20the%20optimal%0Abalance%20with%20a%20point%20retention%20rate%20of%2064%25%20and%20mean%20speed%20accuracy%20within%204.2%25%0Aof%20the%201-second%20baseline%20while%20reducing%20computational%20overhead%20by%2080.5%25.%0AComparative%20analysis%20of%20coordinate%20processing%20methods%20reveals%20that%20while%20UTM%0AZone%2033N%20projection%20and%20Haversine%20calculations%20provide%20consistently%20similar%0Aresults%20%28within%200.1%25%20difference%29%2C%20raw%20WGS84%20coordinates%20underestimate%20distances%0Aby%2015-30%25%20and%20speeds%20by%2020-35%25.%20Altitude%20measurements%20showed%20unexpected%0Aresilience%20to%20sampling%20rate%20variations%2C%20with%20only%202.1%25%20variation%20across%20all%0Aintervals.%20This%20research%20is%20the%20first%20of%20its%20kind%2C%20providing%20quantitative%0Abenchmarks%20for%20establishing%20a%20robust%20framework%20for%20drone%20telemetry%20extraction%0Aand%20analysis%20using%20open-source%20tools%20and%20spatial%20libraries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18505v1&entry.124074799=Read"},
{"title": "Graph Structure Learning for Spatial-Temporal Imputation: Adapting to\n  Node and Feature Scales", "author": "Xinyu Yang and Yu Sun and Xinyang Chen and Ying Zhang and Xiaojie Yuan", "abstract": "  Spatial-temporal data collected across different geographic locations often\nsuffer from missing values, posing challenges to data analysis. Existing\nmethods primarily leverage fixed spatial graphs to impute missing values, which\nimplicitly assume that the spatial relationship is roughly the same for all\nfeatures across different locations. However, they may overlook the different\nspatial relationships of diverse features recorded by sensors in different\nlocations. To address this, we introduce the multi-scale Graph Structure\nLearning framework for spatial-temporal Imputation (GSLI) that dynamically\nadapts to the heterogeneous spatial correlations. Our framework encompasses\nnode-scale graph structure learning to cater to the distinct global spatial\ncorrelations of different features, and feature-scale graph structure learning\nto unveil common spatial correlation across features within all stations.\nIntegrated with prominence modeling, our framework emphasizes nodes and\nfeatures with greater significance in the imputation process. Furthermore, GSLI\nincorporates cross-feature and cross-temporal representation learning to\ncapture spatial-temporal dependencies. Evaluated on six real incomplete\nspatial-temporal datasets, GSLI showcases the improvement in data imputation.\n", "link": "http://arxiv.org/abs/2412.18535v1", "date": "2024-12-24", "relevancy": 2.5797, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Structure%20Learning%20for%20Spatial-Temporal%20Imputation%3A%20Adapting%20to%0A%20%20Node%20and%20Feature%20Scales&body=Title%3A%20Graph%20Structure%20Learning%20for%20Spatial-Temporal%20Imputation%3A%20Adapting%20to%0A%20%20Node%20and%20Feature%20Scales%0AAuthor%3A%20Xinyu%20Yang%20and%20Yu%20Sun%20and%20Xinyang%20Chen%20and%20Ying%20Zhang%20and%20Xiaojie%20Yuan%0AAbstract%3A%20%20%20Spatial-temporal%20data%20collected%20across%20different%20geographic%20locations%20often%0Asuffer%20from%20missing%20values%2C%20posing%20challenges%20to%20data%20analysis.%20Existing%0Amethods%20primarily%20leverage%20fixed%20spatial%20graphs%20to%20impute%20missing%20values%2C%20which%0Aimplicitly%20assume%20that%20the%20spatial%20relationship%20is%20roughly%20the%20same%20for%20all%0Afeatures%20across%20different%20locations.%20However%2C%20they%20may%20overlook%20the%20different%0Aspatial%20relationships%20of%20diverse%20features%20recorded%20by%20sensors%20in%20different%0Alocations.%20To%20address%20this%2C%20we%20introduce%20the%20multi-scale%20Graph%20Structure%0ALearning%20framework%20for%20spatial-temporal%20Imputation%20%28GSLI%29%20that%20dynamically%0Aadapts%20to%20the%20heterogeneous%20spatial%20correlations.%20Our%20framework%20encompasses%0Anode-scale%20graph%20structure%20learning%20to%20cater%20to%20the%20distinct%20global%20spatial%0Acorrelations%20of%20different%20features%2C%20and%20feature-scale%20graph%20structure%20learning%0Ato%20unveil%20common%20spatial%20correlation%20across%20features%20within%20all%20stations.%0AIntegrated%20with%20prominence%20modeling%2C%20our%20framework%20emphasizes%20nodes%20and%0Afeatures%20with%20greater%20significance%20in%20the%20imputation%20process.%20Furthermore%2C%20GSLI%0Aincorporates%20cross-feature%20and%20cross-temporal%20representation%20learning%20to%0Acapture%20spatial-temporal%20dependencies.%20Evaluated%20on%20six%20real%20incomplete%0Aspatial-temporal%20datasets%2C%20GSLI%20showcases%20the%20improvement%20in%20data%20imputation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Structure%2520Learning%2520for%2520Spatial-Temporal%2520Imputation%253A%2520Adapting%2520to%250A%2520%2520Node%2520and%2520Feature%2520Scales%26entry.906535625%3DXinyu%2520Yang%2520and%2520Yu%2520Sun%2520and%2520Xinyang%2520Chen%2520and%2520Ying%2520Zhang%2520and%2520Xiaojie%2520Yuan%26entry.1292438233%3D%2520%2520Spatial-temporal%2520data%2520collected%2520across%2520different%2520geographic%2520locations%2520often%250Asuffer%2520from%2520missing%2520values%252C%2520posing%2520challenges%2520to%2520data%2520analysis.%2520Existing%250Amethods%2520primarily%2520leverage%2520fixed%2520spatial%2520graphs%2520to%2520impute%2520missing%2520values%252C%2520which%250Aimplicitly%2520assume%2520that%2520the%2520spatial%2520relationship%2520is%2520roughly%2520the%2520same%2520for%2520all%250Afeatures%2520across%2520different%2520locations.%2520However%252C%2520they%2520may%2520overlook%2520the%2520different%250Aspatial%2520relationships%2520of%2520diverse%2520features%2520recorded%2520by%2520sensors%2520in%2520different%250Alocations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520multi-scale%2520Graph%2520Structure%250ALearning%2520framework%2520for%2520spatial-temporal%2520Imputation%2520%2528GSLI%2529%2520that%2520dynamically%250Aadapts%2520to%2520the%2520heterogeneous%2520spatial%2520correlations.%2520Our%2520framework%2520encompasses%250Anode-scale%2520graph%2520structure%2520learning%2520to%2520cater%2520to%2520the%2520distinct%2520global%2520spatial%250Acorrelations%2520of%2520different%2520features%252C%2520and%2520feature-scale%2520graph%2520structure%2520learning%250Ato%2520unveil%2520common%2520spatial%2520correlation%2520across%2520features%2520within%2520all%2520stations.%250AIntegrated%2520with%2520prominence%2520modeling%252C%2520our%2520framework%2520emphasizes%2520nodes%2520and%250Afeatures%2520with%2520greater%2520significance%2520in%2520the%2520imputation%2520process.%2520Furthermore%252C%2520GSLI%250Aincorporates%2520cross-feature%2520and%2520cross-temporal%2520representation%2520learning%2520to%250Acapture%2520spatial-temporal%2520dependencies.%2520Evaluated%2520on%2520six%2520real%2520incomplete%250Aspatial-temporal%2520datasets%252C%2520GSLI%2520showcases%2520the%2520improvement%2520in%2520data%2520imputation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Structure%20Learning%20for%20Spatial-Temporal%20Imputation%3A%20Adapting%20to%0A%20%20Node%20and%20Feature%20Scales&entry.906535625=Xinyu%20Yang%20and%20Yu%20Sun%20and%20Xinyang%20Chen%20and%20Ying%20Zhang%20and%20Xiaojie%20Yuan&entry.1292438233=%20%20Spatial-temporal%20data%20collected%20across%20different%20geographic%20locations%20often%0Asuffer%20from%20missing%20values%2C%20posing%20challenges%20to%20data%20analysis.%20Existing%0Amethods%20primarily%20leverage%20fixed%20spatial%20graphs%20to%20impute%20missing%20values%2C%20which%0Aimplicitly%20assume%20that%20the%20spatial%20relationship%20is%20roughly%20the%20same%20for%20all%0Afeatures%20across%20different%20locations.%20However%2C%20they%20may%20overlook%20the%20different%0Aspatial%20relationships%20of%20diverse%20features%20recorded%20by%20sensors%20in%20different%0Alocations.%20To%20address%20this%2C%20we%20introduce%20the%20multi-scale%20Graph%20Structure%0ALearning%20framework%20for%20spatial-temporal%20Imputation%20%28GSLI%29%20that%20dynamically%0Aadapts%20to%20the%20heterogeneous%20spatial%20correlations.%20Our%20framework%20encompasses%0Anode-scale%20graph%20structure%20learning%20to%20cater%20to%20the%20distinct%20global%20spatial%0Acorrelations%20of%20different%20features%2C%20and%20feature-scale%20graph%20structure%20learning%0Ato%20unveil%20common%20spatial%20correlation%20across%20features%20within%20all%20stations.%0AIntegrated%20with%20prominence%20modeling%2C%20our%20framework%20emphasizes%20nodes%20and%0Afeatures%20with%20greater%20significance%20in%20the%20imputation%20process.%20Furthermore%2C%20GSLI%0Aincorporates%20cross-feature%20and%20cross-temporal%20representation%20learning%20to%0Acapture%20spatial-temporal%20dependencies.%20Evaluated%20on%20six%20real%20incomplete%0Aspatial-temporal%20datasets%2C%20GSLI%20showcases%20the%20improvement%20in%20data%20imputation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18535v1&entry.124074799=Read"},
{"title": "Optimal-state Dynamics Estimation for Physics-based Human Motion Capture\n  from Videos", "author": "Cuong Le and Viktor Johansson and Manon Kok and Bastian Wandt", "abstract": "  Human motion capture from monocular videos has made significant progress in\nrecent years. However, modern approaches often produce temporal artifacts, e.g.\nin form of jittery motion and struggle to achieve smooth and physically\nplausible motions. Explicitly integrating physics, in form of internal forces\nand exterior torques, helps alleviating these artifacts. Current\nstate-of-the-art approaches make use of an automatic PD controller to predict\ntorques and reaction forces in order to re-simulate the input kinematics, i.e.\nthe joint angles of a predefined skeleton. However, due to imperfect physical\nmodels, these methods often require simplifying assumptions and extensive\npreprocessing of the input kinematics to achieve good performance. To this end,\nwe propose a novel method to selectively incorporate the physics models with\nthe kinematics observations in an online setting, inspired by a neural\nKalman-filtering approach. We develop a control loop as a meta-PD controller to\npredict internal joint torques and external reaction forces, followed by a\nphysics-based motion simulation. A recurrent neural network is introduced to\nrealize a Kalman filter that attentively balances the kinematics input and\nsimulated motion, resulting in an optimal-state dynamics prediction. We show\nthat this filtering step is crucial to provide an online supervision that helps\nbalancing the shortcoming of the respective input motions, thus being important\nfor not only capturing accurate global motion trajectories but also producing\nphysically plausible human poses. The proposed approach excels in the\nphysics-based human pose estimation task and demonstrates the physical\nplausibility of the predictive dynamics, compared to state of the art. The code\nis available on https://github.com/cuongle1206/OSDCap\n", "link": "http://arxiv.org/abs/2410.07795v3", "date": "2024-12-24", "relevancy": 2.539, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6519}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos&body=Title%3A%20Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos%0AAuthor%3A%20Cuong%20Le%20and%20Viktor%20Johansson%20and%20Manon%20Kok%20and%20Bastian%20Wandt%0AAbstract%3A%20%20%20Human%20motion%20capture%20from%20monocular%20videos%20has%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20modern%20approaches%20often%20produce%20temporal%20artifacts%2C%20e.g.%0Ain%20form%20of%20jittery%20motion%20and%20struggle%20to%20achieve%20smooth%20and%20physically%0Aplausible%20motions.%20Explicitly%20integrating%20physics%2C%20in%20form%20of%20internal%20forces%0Aand%20exterior%20torques%2C%20helps%20alleviating%20these%20artifacts.%20Current%0Astate-of-the-art%20approaches%20make%20use%20of%20an%20automatic%20PD%20controller%20to%20predict%0Atorques%20and%20reaction%20forces%20in%20order%20to%20re-simulate%20the%20input%20kinematics%2C%20i.e.%0Athe%20joint%20angles%20of%20a%20predefined%20skeleton.%20However%2C%20due%20to%20imperfect%20physical%0Amodels%2C%20these%20methods%20often%20require%20simplifying%20assumptions%20and%20extensive%0Apreprocessing%20of%20the%20input%20kinematics%20to%20achieve%20good%20performance.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20method%20to%20selectively%20incorporate%20the%20physics%20models%20with%0Athe%20kinematics%20observations%20in%20an%20online%20setting%2C%20inspired%20by%20a%20neural%0AKalman-filtering%20approach.%20We%20develop%20a%20control%20loop%20as%20a%20meta-PD%20controller%20to%0Apredict%20internal%20joint%20torques%20and%20external%20reaction%20forces%2C%20followed%20by%20a%0Aphysics-based%20motion%20simulation.%20A%20recurrent%20neural%20network%20is%20introduced%20to%0Arealize%20a%20Kalman%20filter%20that%20attentively%20balances%20the%20kinematics%20input%20and%0Asimulated%20motion%2C%20resulting%20in%20an%20optimal-state%20dynamics%20prediction.%20We%20show%0Athat%20this%20filtering%20step%20is%20crucial%20to%20provide%20an%20online%20supervision%20that%20helps%0Abalancing%20the%20shortcoming%20of%20the%20respective%20input%20motions%2C%20thus%20being%20important%0Afor%20not%20only%20capturing%20accurate%20global%20motion%20trajectories%20but%20also%20producing%0Aphysically%20plausible%20human%20poses.%20The%20proposed%20approach%20excels%20in%20the%0Aphysics-based%20human%20pose%20estimation%20task%20and%20demonstrates%20the%20physical%0Aplausibility%20of%20the%20predictive%20dynamics%2C%20compared%20to%20state%20of%20the%20art.%20The%20code%0Ais%20available%20on%20https%3A//github.com/cuongle1206/OSDCap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal-state%2520Dynamics%2520Estimation%2520for%2520Physics-based%2520Human%2520Motion%2520Capture%250A%2520%2520from%2520Videos%26entry.906535625%3DCuong%2520Le%2520and%2520Viktor%2520Johansson%2520and%2520Manon%2520Kok%2520and%2520Bastian%2520Wandt%26entry.1292438233%3D%2520%2520Human%2520motion%2520capture%2520from%2520monocular%2520videos%2520has%2520made%2520significant%2520progress%2520in%250Arecent%2520years.%2520However%252C%2520modern%2520approaches%2520often%2520produce%2520temporal%2520artifacts%252C%2520e.g.%250Ain%2520form%2520of%2520jittery%2520motion%2520and%2520struggle%2520to%2520achieve%2520smooth%2520and%2520physically%250Aplausible%2520motions.%2520Explicitly%2520integrating%2520physics%252C%2520in%2520form%2520of%2520internal%2520forces%250Aand%2520exterior%2520torques%252C%2520helps%2520alleviating%2520these%2520artifacts.%2520Current%250Astate-of-the-art%2520approaches%2520make%2520use%2520of%2520an%2520automatic%2520PD%2520controller%2520to%2520predict%250Atorques%2520and%2520reaction%2520forces%2520in%2520order%2520to%2520re-simulate%2520the%2520input%2520kinematics%252C%2520i.e.%250Athe%2520joint%2520angles%2520of%2520a%2520predefined%2520skeleton.%2520However%252C%2520due%2520to%2520imperfect%2520physical%250Amodels%252C%2520these%2520methods%2520often%2520require%2520simplifying%2520assumptions%2520and%2520extensive%250Apreprocessing%2520of%2520the%2520input%2520kinematics%2520to%2520achieve%2520good%2520performance.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520method%2520to%2520selectively%2520incorporate%2520the%2520physics%2520models%2520with%250Athe%2520kinematics%2520observations%2520in%2520an%2520online%2520setting%252C%2520inspired%2520by%2520a%2520neural%250AKalman-filtering%2520approach.%2520We%2520develop%2520a%2520control%2520loop%2520as%2520a%2520meta-PD%2520controller%2520to%250Apredict%2520internal%2520joint%2520torques%2520and%2520external%2520reaction%2520forces%252C%2520followed%2520by%2520a%250Aphysics-based%2520motion%2520simulation.%2520A%2520recurrent%2520neural%2520network%2520is%2520introduced%2520to%250Arealize%2520a%2520Kalman%2520filter%2520that%2520attentively%2520balances%2520the%2520kinematics%2520input%2520and%250Asimulated%2520motion%252C%2520resulting%2520in%2520an%2520optimal-state%2520dynamics%2520prediction.%2520We%2520show%250Athat%2520this%2520filtering%2520step%2520is%2520crucial%2520to%2520provide%2520an%2520online%2520supervision%2520that%2520helps%250Abalancing%2520the%2520shortcoming%2520of%2520the%2520respective%2520input%2520motions%252C%2520thus%2520being%2520important%250Afor%2520not%2520only%2520capturing%2520accurate%2520global%2520motion%2520trajectories%2520but%2520also%2520producing%250Aphysically%2520plausible%2520human%2520poses.%2520The%2520proposed%2520approach%2520excels%2520in%2520the%250Aphysics-based%2520human%2520pose%2520estimation%2520task%2520and%2520demonstrates%2520the%2520physical%250Aplausibility%2520of%2520the%2520predictive%2520dynamics%252C%2520compared%2520to%2520state%2520of%2520the%2520art.%2520The%2520code%250Ais%2520available%2520on%2520https%253A//github.com/cuongle1206/OSDCap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos&entry.906535625=Cuong%20Le%20and%20Viktor%20Johansson%20and%20Manon%20Kok%20and%20Bastian%20Wandt&entry.1292438233=%20%20Human%20motion%20capture%20from%20monocular%20videos%20has%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20modern%20approaches%20often%20produce%20temporal%20artifacts%2C%20e.g.%0Ain%20form%20of%20jittery%20motion%20and%20struggle%20to%20achieve%20smooth%20and%20physically%0Aplausible%20motions.%20Explicitly%20integrating%20physics%2C%20in%20form%20of%20internal%20forces%0Aand%20exterior%20torques%2C%20helps%20alleviating%20these%20artifacts.%20Current%0Astate-of-the-art%20approaches%20make%20use%20of%20an%20automatic%20PD%20controller%20to%20predict%0Atorques%20and%20reaction%20forces%20in%20order%20to%20re-simulate%20the%20input%20kinematics%2C%20i.e.%0Athe%20joint%20angles%20of%20a%20predefined%20skeleton.%20However%2C%20due%20to%20imperfect%20physical%0Amodels%2C%20these%20methods%20often%20require%20simplifying%20assumptions%20and%20extensive%0Apreprocessing%20of%20the%20input%20kinematics%20to%20achieve%20good%20performance.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20method%20to%20selectively%20incorporate%20the%20physics%20models%20with%0Athe%20kinematics%20observations%20in%20an%20online%20setting%2C%20inspired%20by%20a%20neural%0AKalman-filtering%20approach.%20We%20develop%20a%20control%20loop%20as%20a%20meta-PD%20controller%20to%0Apredict%20internal%20joint%20torques%20and%20external%20reaction%20forces%2C%20followed%20by%20a%0Aphysics-based%20motion%20simulation.%20A%20recurrent%20neural%20network%20is%20introduced%20to%0Arealize%20a%20Kalman%20filter%20that%20attentively%20balances%20the%20kinematics%20input%20and%0Asimulated%20motion%2C%20resulting%20in%20an%20optimal-state%20dynamics%20prediction.%20We%20show%0Athat%20this%20filtering%20step%20is%20crucial%20to%20provide%20an%20online%20supervision%20that%20helps%0Abalancing%20the%20shortcoming%20of%20the%20respective%20input%20motions%2C%20thus%20being%20important%0Afor%20not%20only%20capturing%20accurate%20global%20motion%20trajectories%20but%20also%20producing%0Aphysically%20plausible%20human%20poses.%20The%20proposed%20approach%20excels%20in%20the%0Aphysics-based%20human%20pose%20estimation%20task%20and%20demonstrates%20the%20physical%0Aplausibility%20of%20the%20predictive%20dynamics%2C%20compared%20to%20state%20of%20the%20art.%20The%20code%0Ais%20available%20on%20https%3A//github.com/cuongle1206/OSDCap%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07795v3&entry.124074799=Read"},
{"title": "Re-assessing ImageNet: How aligned is its single-label assumption with\n  its multi-label nature?", "author": "Esla Timothy Anzaku and Seyed Amir Mousavi and Arnout Van Messem and Wesley De Neve", "abstract": "  ImageNet, an influential dataset in computer vision, is traditionally\nevaluated using single-label classification, which assumes that an image can be\nadequately described by a single concept or label. However, this approach may\nnot fully capture the complex semantics within the images available in\nImageNet, potentially hindering the development of models that effectively\nlearn these intricacies. This study critically examines the prevalent\nsingle-label benchmarking approach and advocates for a shift to multi-label\nbenchmarking for ImageNet. This shift would enable a more comprehensive\nassessment of the capabilities of deep neural network (DNN) models. We analyze\nthe effectiveness of pre-trained state-of-the-art DNNs on ImageNet and one of\nits variants, ImageNetV2. Studies in the literature have reported unexpected\naccuracy drops of 11% to 14% on ImageNetV2. Our findings show that these\nreported declines are largely attributable to a characteristic of the dataset\nthat has not received sufficient attention -- the proportion of images with\nmultiple labels. Taking this characteristic into account, the results of our\nexperiments provide evidence that there is no substantial degradation in\neffectiveness on ImageNetV2. Furthermore, we acknowledge that ImageNet\npre-trained models exhibit some capability at capturing the multi-label nature\nof the dataset even though they were trained under the single-label assumption.\nConsequently, we propose a new evaluation approach to augment existing\napproaches that assess this capability. Our findings highlight the importance\nof considering the multi-label nature of the ImageNet dataset during\nbenchmarking. Failing to do so could lead to incorrect conclusions regarding\nthe effectiveness of DNNs and divert research efforts from addressing other\nsubstantial challenges related to the reliability and robustness of these\nmodels.\n", "link": "http://arxiv.org/abs/2412.18409v1", "date": "2024-12-24", "relevancy": 2.525, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-assessing%20ImageNet%3A%20How%20aligned%20is%20its%20single-label%20assumption%20with%0A%20%20its%20multi-label%20nature%3F&body=Title%3A%20Re-assessing%20ImageNet%3A%20How%20aligned%20is%20its%20single-label%20assumption%20with%0A%20%20its%20multi-label%20nature%3F%0AAuthor%3A%20Esla%20Timothy%20Anzaku%20and%20Seyed%20Amir%20Mousavi%20and%20Arnout%20Van%20Messem%20and%20Wesley%20De%20Neve%0AAbstract%3A%20%20%20ImageNet%2C%20an%20influential%20dataset%20in%20computer%20vision%2C%20is%20traditionally%0Aevaluated%20using%20single-label%20classification%2C%20which%20assumes%20that%20an%20image%20can%20be%0Aadequately%20described%20by%20a%20single%20concept%20or%20label.%20However%2C%20this%20approach%20may%0Anot%20fully%20capture%20the%20complex%20semantics%20within%20the%20images%20available%20in%0AImageNet%2C%20potentially%20hindering%20the%20development%20of%20models%20that%20effectively%0Alearn%20these%20intricacies.%20This%20study%20critically%20examines%20the%20prevalent%0Asingle-label%20benchmarking%20approach%20and%20advocates%20for%20a%20shift%20to%20multi-label%0Abenchmarking%20for%20ImageNet.%20This%20shift%20would%20enable%20a%20more%20comprehensive%0Aassessment%20of%20the%20capabilities%20of%20deep%20neural%20network%20%28DNN%29%20models.%20We%20analyze%0Athe%20effectiveness%20of%20pre-trained%20state-of-the-art%20DNNs%20on%20ImageNet%20and%20one%20of%0Aits%20variants%2C%20ImageNetV2.%20Studies%20in%20the%20literature%20have%20reported%20unexpected%0Aaccuracy%20drops%20of%2011%25%20to%2014%25%20on%20ImageNetV2.%20Our%20findings%20show%20that%20these%0Areported%20declines%20are%20largely%20attributable%20to%20a%20characteristic%20of%20the%20dataset%0Athat%20has%20not%20received%20sufficient%20attention%20--%20the%20proportion%20of%20images%20with%0Amultiple%20labels.%20Taking%20this%20characteristic%20into%20account%2C%20the%20results%20of%20our%0Aexperiments%20provide%20evidence%20that%20there%20is%20no%20substantial%20degradation%20in%0Aeffectiveness%20on%20ImageNetV2.%20Furthermore%2C%20we%20acknowledge%20that%20ImageNet%0Apre-trained%20models%20exhibit%20some%20capability%20at%20capturing%20the%20multi-label%20nature%0Aof%20the%20dataset%20even%20though%20they%20were%20trained%20under%20the%20single-label%20assumption.%0AConsequently%2C%20we%20propose%20a%20new%20evaluation%20approach%20to%20augment%20existing%0Aapproaches%20that%20assess%20this%20capability.%20Our%20findings%20highlight%20the%20importance%0Aof%20considering%20the%20multi-label%20nature%20of%20the%20ImageNet%20dataset%20during%0Abenchmarking.%20Failing%20to%20do%20so%20could%20lead%20to%20incorrect%20conclusions%20regarding%0Athe%20effectiveness%20of%20DNNs%20and%20divert%20research%20efforts%20from%20addressing%20other%0Asubstantial%20challenges%20related%20to%20the%20reliability%20and%20robustness%20of%20these%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-assessing%2520ImageNet%253A%2520How%2520aligned%2520is%2520its%2520single-label%2520assumption%2520with%250A%2520%2520its%2520multi-label%2520nature%253F%26entry.906535625%3DEsla%2520Timothy%2520Anzaku%2520and%2520Seyed%2520Amir%2520Mousavi%2520and%2520Arnout%2520Van%2520Messem%2520and%2520Wesley%2520De%2520Neve%26entry.1292438233%3D%2520%2520ImageNet%252C%2520an%2520influential%2520dataset%2520in%2520computer%2520vision%252C%2520is%2520traditionally%250Aevaluated%2520using%2520single-label%2520classification%252C%2520which%2520assumes%2520that%2520an%2520image%2520can%2520be%250Aadequately%2520described%2520by%2520a%2520single%2520concept%2520or%2520label.%2520However%252C%2520this%2520approach%2520may%250Anot%2520fully%2520capture%2520the%2520complex%2520semantics%2520within%2520the%2520images%2520available%2520in%250AImageNet%252C%2520potentially%2520hindering%2520the%2520development%2520of%2520models%2520that%2520effectively%250Alearn%2520these%2520intricacies.%2520This%2520study%2520critically%2520examines%2520the%2520prevalent%250Asingle-label%2520benchmarking%2520approach%2520and%2520advocates%2520for%2520a%2520shift%2520to%2520multi-label%250Abenchmarking%2520for%2520ImageNet.%2520This%2520shift%2520would%2520enable%2520a%2520more%2520comprehensive%250Aassessment%2520of%2520the%2520capabilities%2520of%2520deep%2520neural%2520network%2520%2528DNN%2529%2520models.%2520We%2520analyze%250Athe%2520effectiveness%2520of%2520pre-trained%2520state-of-the-art%2520DNNs%2520on%2520ImageNet%2520and%2520one%2520of%250Aits%2520variants%252C%2520ImageNetV2.%2520Studies%2520in%2520the%2520literature%2520have%2520reported%2520unexpected%250Aaccuracy%2520drops%2520of%252011%2525%2520to%252014%2525%2520on%2520ImageNetV2.%2520Our%2520findings%2520show%2520that%2520these%250Areported%2520declines%2520are%2520largely%2520attributable%2520to%2520a%2520characteristic%2520of%2520the%2520dataset%250Athat%2520has%2520not%2520received%2520sufficient%2520attention%2520--%2520the%2520proportion%2520of%2520images%2520with%250Amultiple%2520labels.%2520Taking%2520this%2520characteristic%2520into%2520account%252C%2520the%2520results%2520of%2520our%250Aexperiments%2520provide%2520evidence%2520that%2520there%2520is%2520no%2520substantial%2520degradation%2520in%250Aeffectiveness%2520on%2520ImageNetV2.%2520Furthermore%252C%2520we%2520acknowledge%2520that%2520ImageNet%250Apre-trained%2520models%2520exhibit%2520some%2520capability%2520at%2520capturing%2520the%2520multi-label%2520nature%250Aof%2520the%2520dataset%2520even%2520though%2520they%2520were%2520trained%2520under%2520the%2520single-label%2520assumption.%250AConsequently%252C%2520we%2520propose%2520a%2520new%2520evaluation%2520approach%2520to%2520augment%2520existing%250Aapproaches%2520that%2520assess%2520this%2520capability.%2520Our%2520findings%2520highlight%2520the%2520importance%250Aof%2520considering%2520the%2520multi-label%2520nature%2520of%2520the%2520ImageNet%2520dataset%2520during%250Abenchmarking.%2520Failing%2520to%2520do%2520so%2520could%2520lead%2520to%2520incorrect%2520conclusions%2520regarding%250Athe%2520effectiveness%2520of%2520DNNs%2520and%2520divert%2520research%2520efforts%2520from%2520addressing%2520other%250Asubstantial%2520challenges%2520related%2520to%2520the%2520reliability%2520and%2520robustness%2520of%2520these%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-assessing%20ImageNet%3A%20How%20aligned%20is%20its%20single-label%20assumption%20with%0A%20%20its%20multi-label%20nature%3F&entry.906535625=Esla%20Timothy%20Anzaku%20and%20Seyed%20Amir%20Mousavi%20and%20Arnout%20Van%20Messem%20and%20Wesley%20De%20Neve&entry.1292438233=%20%20ImageNet%2C%20an%20influential%20dataset%20in%20computer%20vision%2C%20is%20traditionally%0Aevaluated%20using%20single-label%20classification%2C%20which%20assumes%20that%20an%20image%20can%20be%0Aadequately%20described%20by%20a%20single%20concept%20or%20label.%20However%2C%20this%20approach%20may%0Anot%20fully%20capture%20the%20complex%20semantics%20within%20the%20images%20available%20in%0AImageNet%2C%20potentially%20hindering%20the%20development%20of%20models%20that%20effectively%0Alearn%20these%20intricacies.%20This%20study%20critically%20examines%20the%20prevalent%0Asingle-label%20benchmarking%20approach%20and%20advocates%20for%20a%20shift%20to%20multi-label%0Abenchmarking%20for%20ImageNet.%20This%20shift%20would%20enable%20a%20more%20comprehensive%0Aassessment%20of%20the%20capabilities%20of%20deep%20neural%20network%20%28DNN%29%20models.%20We%20analyze%0Athe%20effectiveness%20of%20pre-trained%20state-of-the-art%20DNNs%20on%20ImageNet%20and%20one%20of%0Aits%20variants%2C%20ImageNetV2.%20Studies%20in%20the%20literature%20have%20reported%20unexpected%0Aaccuracy%20drops%20of%2011%25%20to%2014%25%20on%20ImageNetV2.%20Our%20findings%20show%20that%20these%0Areported%20declines%20are%20largely%20attributable%20to%20a%20characteristic%20of%20the%20dataset%0Athat%20has%20not%20received%20sufficient%20attention%20--%20the%20proportion%20of%20images%20with%0Amultiple%20labels.%20Taking%20this%20characteristic%20into%20account%2C%20the%20results%20of%20our%0Aexperiments%20provide%20evidence%20that%20there%20is%20no%20substantial%20degradation%20in%0Aeffectiveness%20on%20ImageNetV2.%20Furthermore%2C%20we%20acknowledge%20that%20ImageNet%0Apre-trained%20models%20exhibit%20some%20capability%20at%20capturing%20the%20multi-label%20nature%0Aof%20the%20dataset%20even%20though%20they%20were%20trained%20under%20the%20single-label%20assumption.%0AConsequently%2C%20we%20propose%20a%20new%20evaluation%20approach%20to%20augment%20existing%0Aapproaches%20that%20assess%20this%20capability.%20Our%20findings%20highlight%20the%20importance%0Aof%20considering%20the%20multi-label%20nature%20of%20the%20ImageNet%20dataset%20during%0Abenchmarking.%20Failing%20to%20do%20so%20could%20lead%20to%20incorrect%20conclusions%20regarding%0Athe%20effectiveness%20of%20DNNs%20and%20divert%20research%20efforts%20from%20addressing%20other%0Asubstantial%20challenges%20related%20to%20the%20reliability%20and%20robustness%20of%20these%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18409v1&entry.124074799=Read"},
{"title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion\n  Transformer for Tuning-Free Multi-Prompt Longer Video Generation", "author": "Minghong Cai and Xiaodong Cun and Xiaoyu Li and Wenze Liu and Zhaoyang Zhang and Yong Zhang and Ying Shan and Xiangyu Yue", "abstract": "  Sora-like video generation models have achieved remarkable progress with a\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\nvideo generation models predominantly focus on single-prompt, struggling to\ngenerate coherent scenes with multiple sequential prompts that better reflect\nreal-world dynamic scenarios. While some pioneering works have explored\nmulti-prompt video generation, they face significant challenges including\nstrict training data requirements, weak prompt following, and unnatural\ntransitions. To address these problems, we propose DiTCtrl, a training-free\nmulti-prompt video generation method under MM-DiT architectures for the first\ntime. Our key idea is to take the multi-prompt video generation task as\ntemporal video editing with smooth transitions. To achieve this goal, we first\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\ndiffusion models, enabling mask-guided precise semantic control across\ndifferent prompts with attention sharing for multi-prompt video generation.\nBased on our careful design, the video generated by DiTCtrl achieves smooth\ntransitions and consistent object motion given multiple sequential prompts\nwithout additional training. Besides, we also present MPVBench, a new benchmark\nspecially designed for multi-prompt video generation to evaluate the\nperformance of multi-prompt generation. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance without additional training.\n", "link": "http://arxiv.org/abs/2412.18597v1", "date": "2024-12-24", "relevancy": 2.5035, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6542}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6251}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiTCtrl%3A%20Exploring%20Attention%20Control%20in%20Multi-Modal%20Diffusion%0A%20%20Transformer%20for%20Tuning-Free%20Multi-Prompt%20Longer%20Video%20Generation&body=Title%3A%20DiTCtrl%3A%20Exploring%20Attention%20Control%20in%20Multi-Modal%20Diffusion%0A%20%20Transformer%20for%20Tuning-Free%20Multi-Prompt%20Longer%20Video%20Generation%0AAuthor%3A%20Minghong%20Cai%20and%20Xiaodong%20Cun%20and%20Xiaoyu%20Li%20and%20Wenze%20Liu%20and%20Zhaoyang%20Zhang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Sora-like%20video%20generation%20models%20have%20achieved%20remarkable%20progress%20with%20a%0AMulti-Modal%20Diffusion%20Transformer%20MM-DiT%20architecture.%20However%2C%20the%20current%0Avideo%20generation%20models%20predominantly%20focus%20on%20single-prompt%2C%20struggling%20to%0Agenerate%20coherent%20scenes%20with%20multiple%20sequential%20prompts%20that%20better%20reflect%0Areal-world%20dynamic%20scenarios.%20While%20some%20pioneering%20works%20have%20explored%0Amulti-prompt%20video%20generation%2C%20they%20face%20significant%20challenges%20including%0Astrict%20training%20data%20requirements%2C%20weak%20prompt%20following%2C%20and%20unnatural%0Atransitions.%20To%20address%20these%20problems%2C%20we%20propose%20DiTCtrl%2C%20a%20training-free%0Amulti-prompt%20video%20generation%20method%20under%20MM-DiT%20architectures%20for%20the%20first%0Atime.%20Our%20key%20idea%20is%20to%20take%20the%20multi-prompt%20video%20generation%20task%20as%0Atemporal%20video%20editing%20with%20smooth%20transitions.%20To%20achieve%20this%20goal%2C%20we%20first%0Aanalyze%20MM-DiT%27s%20attention%20mechanism%2C%20finding%20that%20the%203D%20full%20attention%0Abehaves%20similarly%20to%20that%20of%20the%20cross/self-attention%20blocks%20in%20the%20UNet-like%0Adiffusion%20models%2C%20enabling%20mask-guided%20precise%20semantic%20control%20across%0Adifferent%20prompts%20with%20attention%20sharing%20for%20multi-prompt%20video%20generation.%0ABased%20on%20our%20careful%20design%2C%20the%20video%20generated%20by%20DiTCtrl%20achieves%20smooth%0Atransitions%20and%20consistent%20object%20motion%20given%20multiple%20sequential%20prompts%0Awithout%20additional%20training.%20Besides%2C%20we%20also%20present%20MPVBench%2C%20a%20new%20benchmark%0Aspecially%20designed%20for%20multi-prompt%20video%20generation%20to%20evaluate%20the%0Aperformance%20of%20multi-prompt%20generation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20without%20additional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiTCtrl%253A%2520Exploring%2520Attention%2520Control%2520in%2520Multi-Modal%2520Diffusion%250A%2520%2520Transformer%2520for%2520Tuning-Free%2520Multi-Prompt%2520Longer%2520Video%2520Generation%26entry.906535625%3DMinghong%2520Cai%2520and%2520Xiaodong%2520Cun%2520and%2520Xiaoyu%2520Li%2520and%2520Wenze%2520Liu%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yong%2520Zhang%2520and%2520Ying%2520Shan%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Sora-like%2520video%2520generation%2520models%2520have%2520achieved%2520remarkable%2520progress%2520with%2520a%250AMulti-Modal%2520Diffusion%2520Transformer%2520MM-DiT%2520architecture.%2520However%252C%2520the%2520current%250Avideo%2520generation%2520models%2520predominantly%2520focus%2520on%2520single-prompt%252C%2520struggling%2520to%250Agenerate%2520coherent%2520scenes%2520with%2520multiple%2520sequential%2520prompts%2520that%2520better%2520reflect%250Areal-world%2520dynamic%2520scenarios.%2520While%2520some%2520pioneering%2520works%2520have%2520explored%250Amulti-prompt%2520video%2520generation%252C%2520they%2520face%2520significant%2520challenges%2520including%250Astrict%2520training%2520data%2520requirements%252C%2520weak%2520prompt%2520following%252C%2520and%2520unnatural%250Atransitions.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520DiTCtrl%252C%2520a%2520training-free%250Amulti-prompt%2520video%2520generation%2520method%2520under%2520MM-DiT%2520architectures%2520for%2520the%2520first%250Atime.%2520Our%2520key%2520idea%2520is%2520to%2520take%2520the%2520multi-prompt%2520video%2520generation%2520task%2520as%250Atemporal%2520video%2520editing%2520with%2520smooth%2520transitions.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520first%250Aanalyze%2520MM-DiT%2527s%2520attention%2520mechanism%252C%2520finding%2520that%2520the%25203D%2520full%2520attention%250Abehaves%2520similarly%2520to%2520that%2520of%2520the%2520cross/self-attention%2520blocks%2520in%2520the%2520UNet-like%250Adiffusion%2520models%252C%2520enabling%2520mask-guided%2520precise%2520semantic%2520control%2520across%250Adifferent%2520prompts%2520with%2520attention%2520sharing%2520for%2520multi-prompt%2520video%2520generation.%250ABased%2520on%2520our%2520careful%2520design%252C%2520the%2520video%2520generated%2520by%2520DiTCtrl%2520achieves%2520smooth%250Atransitions%2520and%2520consistent%2520object%2520motion%2520given%2520multiple%2520sequential%2520prompts%250Awithout%2520additional%2520training.%2520Besides%252C%2520we%2520also%2520present%2520MPVBench%252C%2520a%2520new%2520benchmark%250Aspecially%2520designed%2520for%2520multi-prompt%2520video%2520generation%2520to%2520evaluate%2520the%250Aperformance%2520of%2520multi-prompt%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520without%2520additional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiTCtrl%3A%20Exploring%20Attention%20Control%20in%20Multi-Modal%20Diffusion%0A%20%20Transformer%20for%20Tuning-Free%20Multi-Prompt%20Longer%20Video%20Generation&entry.906535625=Minghong%20Cai%20and%20Xiaodong%20Cun%20and%20Xiaoyu%20Li%20and%20Wenze%20Liu%20and%20Zhaoyang%20Zhang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Sora-like%20video%20generation%20models%20have%20achieved%20remarkable%20progress%20with%20a%0AMulti-Modal%20Diffusion%20Transformer%20MM-DiT%20architecture.%20However%2C%20the%20current%0Avideo%20generation%20models%20predominantly%20focus%20on%20single-prompt%2C%20struggling%20to%0Agenerate%20coherent%20scenes%20with%20multiple%20sequential%20prompts%20that%20better%20reflect%0Areal-world%20dynamic%20scenarios.%20While%20some%20pioneering%20works%20have%20explored%0Amulti-prompt%20video%20generation%2C%20they%20face%20significant%20challenges%20including%0Astrict%20training%20data%20requirements%2C%20weak%20prompt%20following%2C%20and%20unnatural%0Atransitions.%20To%20address%20these%20problems%2C%20we%20propose%20DiTCtrl%2C%20a%20training-free%0Amulti-prompt%20video%20generation%20method%20under%20MM-DiT%20architectures%20for%20the%20first%0Atime.%20Our%20key%20idea%20is%20to%20take%20the%20multi-prompt%20video%20generation%20task%20as%0Atemporal%20video%20editing%20with%20smooth%20transitions.%20To%20achieve%20this%20goal%2C%20we%20first%0Aanalyze%20MM-DiT%27s%20attention%20mechanism%2C%20finding%20that%20the%203D%20full%20attention%0Abehaves%20similarly%20to%20that%20of%20the%20cross/self-attention%20blocks%20in%20the%20UNet-like%0Adiffusion%20models%2C%20enabling%20mask-guided%20precise%20semantic%20control%20across%0Adifferent%20prompts%20with%20attention%20sharing%20for%20multi-prompt%20video%20generation.%0ABased%20on%20our%20careful%20design%2C%20the%20video%20generated%20by%20DiTCtrl%20achieves%20smooth%0Atransitions%20and%20consistent%20object%20motion%20given%20multiple%20sequential%20prompts%0Awithout%20additional%20training.%20Besides%2C%20we%20also%20present%20MPVBench%2C%20a%20new%20benchmark%0Aspecially%20designed%20for%20multi-prompt%20video%20generation%20to%20evaluate%20the%0Aperformance%20of%20multi-prompt%20generation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20without%20additional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18597v1&entry.124074799=Read"},
{"title": "Hierarchical Classification Auxiliary Network for Time Series\n  Forecasting", "author": "Yanru Sun and Zongxia Xie and Dongyue Chen and Emadeldeen Eldele and Qinghua Hu", "abstract": "  Deep learning has significantly advanced time series forecasting through its\npowerful capacity to capture sequence relationships. However, training these\nmodels with the Mean Square Error (MSE) loss often results in over-smooth\npredictions, making it challenging to handle the complexity and learn\nhigh-entropy features from time series data with high variability and\nunpredictability. In this work, we introduce a novel approach by tokenizing\ntime series values to train forecasting models via cross-entropy loss, while\nconsidering the continuous nature of time series data. Specifically, we propose\na Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic\ncomponent that can be integrated with any forecasting model. HCAN is based on a\nHierarchy-Aware Attention module that integrates multi-granularity high-entropy\nfeatures at different hierarchy levels. At each level, we assign a class label\nfor timesteps to train an Uncertainty-Aware Classifier. This classifier\nmitigates the over-confidence in softmax loss via evidence theory. We also\nimplement a Hierarchical Consistency Loss to maintain prediction consistency\nacross hierarchy levels. Extensive experiments integrating HCAN with\nstate-of-the-art forecasting models demonstrate substantial improvements over\nbaselines on several real-world datasets.\n", "link": "http://arxiv.org/abs/2405.18975v2", "date": "2024-12-24", "relevancy": 2.4965, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4844}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Yanru%20Sun%20and%20Zongxia%20Xie%20and%20Dongyue%20Chen%20and%20Emadeldeen%20Eldele%20and%20Qinghua%20Hu%0AAbstract%3A%20%20%20Deep%20learning%20has%20significantly%20advanced%20time%20series%20forecasting%20through%20its%0Apowerful%20capacity%20to%20capture%20sequence%20relationships.%20However%2C%20training%20these%0Amodels%20with%20the%20Mean%20Square%20Error%20%28MSE%29%20loss%20often%20results%20in%20over-smooth%0Apredictions%2C%20making%20it%20challenging%20to%20handle%20the%20complexity%20and%20learn%0Ahigh-entropy%20features%20from%20time%20series%20data%20with%20high%20variability%20and%0Aunpredictability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20by%20tokenizing%0Atime%20series%20values%20to%20train%20forecasting%20models%20via%20cross-entropy%20loss%2C%20while%0Aconsidering%20the%20continuous%20nature%20of%20time%20series%20data.%20Specifically%2C%20we%20propose%0Aa%20Hierarchical%20Classification%20Auxiliary%20Network%2C%20HCAN%2C%20a%20general%20model-agnostic%0Acomponent%20that%20can%20be%20integrated%20with%20any%20forecasting%20model.%20HCAN%20is%20based%20on%20a%0AHierarchy-Aware%20Attention%20module%20that%20integrates%20multi-granularity%20high-entropy%0Afeatures%20at%20different%20hierarchy%20levels.%20At%20each%20level%2C%20we%20assign%20a%20class%20label%0Afor%20timesteps%20to%20train%20an%20Uncertainty-Aware%20Classifier.%20This%20classifier%0Amitigates%20the%20over-confidence%20in%20softmax%20loss%20via%20evidence%20theory.%20We%20also%0Aimplement%20a%20Hierarchical%20Consistency%20Loss%20to%20maintain%20prediction%20consistency%0Aacross%20hierarchy%20levels.%20Extensive%20experiments%20integrating%20HCAN%20with%0Astate-of-the-art%20forecasting%20models%20demonstrate%20substantial%20improvements%20over%0Abaselines%20on%20several%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Classification%2520Auxiliary%2520Network%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DYanru%2520Sun%2520and%2520Zongxia%2520Xie%2520and%2520Dongyue%2520Chen%2520and%2520Emadeldeen%2520Eldele%2520and%2520Qinghua%2520Hu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520significantly%2520advanced%2520time%2520series%2520forecasting%2520through%2520its%250Apowerful%2520capacity%2520to%2520capture%2520sequence%2520relationships.%2520However%252C%2520training%2520these%250Amodels%2520with%2520the%2520Mean%2520Square%2520Error%2520%2528MSE%2529%2520loss%2520often%2520results%2520in%2520over-smooth%250Apredictions%252C%2520making%2520it%2520challenging%2520to%2520handle%2520the%2520complexity%2520and%2520learn%250Ahigh-entropy%2520features%2520from%2520time%2520series%2520data%2520with%2520high%2520variability%2520and%250Aunpredictability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520by%2520tokenizing%250Atime%2520series%2520values%2520to%2520train%2520forecasting%2520models%2520via%2520cross-entropy%2520loss%252C%2520while%250Aconsidering%2520the%2520continuous%2520nature%2520of%2520time%2520series%2520data.%2520Specifically%252C%2520we%2520propose%250Aa%2520Hierarchical%2520Classification%2520Auxiliary%2520Network%252C%2520HCAN%252C%2520a%2520general%2520model-agnostic%250Acomponent%2520that%2520can%2520be%2520integrated%2520with%2520any%2520forecasting%2520model.%2520HCAN%2520is%2520based%2520on%2520a%250AHierarchy-Aware%2520Attention%2520module%2520that%2520integrates%2520multi-granularity%2520high-entropy%250Afeatures%2520at%2520different%2520hierarchy%2520levels.%2520At%2520each%2520level%252C%2520we%2520assign%2520a%2520class%2520label%250Afor%2520timesteps%2520to%2520train%2520an%2520Uncertainty-Aware%2520Classifier.%2520This%2520classifier%250Amitigates%2520the%2520over-confidence%2520in%2520softmax%2520loss%2520via%2520evidence%2520theory.%2520We%2520also%250Aimplement%2520a%2520Hierarchical%2520Consistency%2520Loss%2520to%2520maintain%2520prediction%2520consistency%250Aacross%2520hierarchy%2520levels.%2520Extensive%2520experiments%2520integrating%2520HCAN%2520with%250Astate-of-the-art%2520forecasting%2520models%2520demonstrate%2520substantial%2520improvements%2520over%250Abaselines%2520on%2520several%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Yanru%20Sun%20and%20Zongxia%20Xie%20and%20Dongyue%20Chen%20and%20Emadeldeen%20Eldele%20and%20Qinghua%20Hu&entry.1292438233=%20%20Deep%20learning%20has%20significantly%20advanced%20time%20series%20forecasting%20through%20its%0Apowerful%20capacity%20to%20capture%20sequence%20relationships.%20However%2C%20training%20these%0Amodels%20with%20the%20Mean%20Square%20Error%20%28MSE%29%20loss%20often%20results%20in%20over-smooth%0Apredictions%2C%20making%20it%20challenging%20to%20handle%20the%20complexity%20and%20learn%0Ahigh-entropy%20features%20from%20time%20series%20data%20with%20high%20variability%20and%0Aunpredictability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20by%20tokenizing%0Atime%20series%20values%20to%20train%20forecasting%20models%20via%20cross-entropy%20loss%2C%20while%0Aconsidering%20the%20continuous%20nature%20of%20time%20series%20data.%20Specifically%2C%20we%20propose%0Aa%20Hierarchical%20Classification%20Auxiliary%20Network%2C%20HCAN%2C%20a%20general%20model-agnostic%0Acomponent%20that%20can%20be%20integrated%20with%20any%20forecasting%20model.%20HCAN%20is%20based%20on%20a%0AHierarchy-Aware%20Attention%20module%20that%20integrates%20multi-granularity%20high-entropy%0Afeatures%20at%20different%20hierarchy%20levels.%20At%20each%20level%2C%20we%20assign%20a%20class%20label%0Afor%20timesteps%20to%20train%20an%20Uncertainty-Aware%20Classifier.%20This%20classifier%0Amitigates%20the%20over-confidence%20in%20softmax%20loss%20via%20evidence%20theory.%20We%20also%0Aimplement%20a%20Hierarchical%20Consistency%20Loss%20to%20maintain%20prediction%20consistency%0Aacross%20hierarchy%20levels.%20Extensive%20experiments%20integrating%20HCAN%20with%0Astate-of-the-art%20forecasting%20models%20demonstrate%20substantial%20improvements%20over%0Abaselines%20on%20several%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18975v2&entry.124074799=Read"},
{"title": "The Value of AI-Generated Metadata for UGC Platforms: Evidence from a\n  Large-scale Field Experiment", "author": "Xinyi Zhang and Chenshuo Sun and Renyu Zhang and Khim-Yong Goh", "abstract": "  AI-generated content (AIGC), such as advertisement copy, product\ndescriptions, and social media posts, is becoming ubiquitous in business\npractices. However, the value of AI-generated metadata, such as titles, remains\nunclear on user-generated content (UGC) platforms. To address this gap, we\nconducted a large-scale field experiment on a leading short-video platform in\nAsia to provide about 1 million users access to AI-generated titles for their\nuploaded videos. Our findings show that the provision of AI-generated titles\nsignificantly boosted content consumption, increasing valid watches by 1.6% and\nwatch duration by 0.9%. When producers adopted these titles, these increases\njumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely\nattributed to the use of this generative AI (GAI) tool increasing the\nlikelihood of videos having a title by 41.4%. The effect was more pronounced\nfor groups more affected by metadata sparsity. Mechanism analysis revealed that\nAI-generated metadata improved user-video matching accuracy in the platform's\nrecommender system. Interestingly, for a video for which the producer would\nhave posted a title anyway, adopting the AI-generated title decreased its\nviewership on average, implying that AI-generated titles may be of lower\nquality than human-generated ones. However, when producers chose to co-create\nwith GAI and significantly revised the AI-generated titles, the videos\noutperformed their counterparts with either fully AI-generated or\nhuman-generated titles, showcasing the benefits of human-AI co-creation. This\nstudy highlights the value of AI-generated metadata and human-AI metadata\nco-creation in enhancing user-content matching and content consumption for UGC\nplatforms.\n", "link": "http://arxiv.org/abs/2412.18337v1", "date": "2024-12-24", "relevancy": 2.4951, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5374}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5028}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Value%20of%20AI-Generated%20Metadata%20for%20UGC%20Platforms%3A%20Evidence%20from%20a%0A%20%20Large-scale%20Field%20Experiment&body=Title%3A%20The%20Value%20of%20AI-Generated%20Metadata%20for%20UGC%20Platforms%3A%20Evidence%20from%20a%0A%20%20Large-scale%20Field%20Experiment%0AAuthor%3A%20Xinyi%20Zhang%20and%20Chenshuo%20Sun%20and%20Renyu%20Zhang%20and%20Khim-Yong%20Goh%0AAbstract%3A%20%20%20AI-generated%20content%20%28AIGC%29%2C%20such%20as%20advertisement%20copy%2C%20product%0Adescriptions%2C%20and%20social%20media%20posts%2C%20is%20becoming%20ubiquitous%20in%20business%0Apractices.%20However%2C%20the%20value%20of%20AI-generated%20metadata%2C%20such%20as%20titles%2C%20remains%0Aunclear%20on%20user-generated%20content%20%28UGC%29%20platforms.%20To%20address%20this%20gap%2C%20we%0Aconducted%20a%20large-scale%20field%20experiment%20on%20a%20leading%20short-video%20platform%20in%0AAsia%20to%20provide%20about%201%20million%20users%20access%20to%20AI-generated%20titles%20for%20their%0Auploaded%20videos.%20Our%20findings%20show%20that%20the%20provision%20of%20AI-generated%20titles%0Asignificantly%20boosted%20content%20consumption%2C%20increasing%20valid%20watches%20by%201.6%25%20and%0Awatch%20duration%20by%200.9%25.%20When%20producers%20adopted%20these%20titles%2C%20these%20increases%0Ajumped%20to%207.1%25%20and%204.1%25%2C%20respectively.%20This%20viewership-boost%20effect%20was%20largely%0Aattributed%20to%20the%20use%20of%20this%20generative%20AI%20%28GAI%29%20tool%20increasing%20the%0Alikelihood%20of%20videos%20having%20a%20title%20by%2041.4%25.%20The%20effect%20was%20more%20pronounced%0Afor%20groups%20more%20affected%20by%20metadata%20sparsity.%20Mechanism%20analysis%20revealed%20that%0AAI-generated%20metadata%20improved%20user-video%20matching%20accuracy%20in%20the%20platform%27s%0Arecommender%20system.%20Interestingly%2C%20for%20a%20video%20for%20which%20the%20producer%20would%0Ahave%20posted%20a%20title%20anyway%2C%20adopting%20the%20AI-generated%20title%20decreased%20its%0Aviewership%20on%20average%2C%20implying%20that%20AI-generated%20titles%20may%20be%20of%20lower%0Aquality%20than%20human-generated%20ones.%20However%2C%20when%20producers%20chose%20to%20co-create%0Awith%20GAI%20and%20significantly%20revised%20the%20AI-generated%20titles%2C%20the%20videos%0Aoutperformed%20their%20counterparts%20with%20either%20fully%20AI-generated%20or%0Ahuman-generated%20titles%2C%20showcasing%20the%20benefits%20of%20human-AI%20co-creation.%20This%0Astudy%20highlights%20the%20value%20of%20AI-generated%20metadata%20and%20human-AI%20metadata%0Aco-creation%20in%20enhancing%20user-content%20matching%20and%20content%20consumption%20for%20UGC%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Value%2520of%2520AI-Generated%2520Metadata%2520for%2520UGC%2520Platforms%253A%2520Evidence%2520from%2520a%250A%2520%2520Large-scale%2520Field%2520Experiment%26entry.906535625%3DXinyi%2520Zhang%2520and%2520Chenshuo%2520Sun%2520and%2520Renyu%2520Zhang%2520and%2520Khim-Yong%2520Goh%26entry.1292438233%3D%2520%2520AI-generated%2520content%2520%2528AIGC%2529%252C%2520such%2520as%2520advertisement%2520copy%252C%2520product%250Adescriptions%252C%2520and%2520social%2520media%2520posts%252C%2520is%2520becoming%2520ubiquitous%2520in%2520business%250Apractices.%2520However%252C%2520the%2520value%2520of%2520AI-generated%2520metadata%252C%2520such%2520as%2520titles%252C%2520remains%250Aunclear%2520on%2520user-generated%2520content%2520%2528UGC%2529%2520platforms.%2520To%2520address%2520this%2520gap%252C%2520we%250Aconducted%2520a%2520large-scale%2520field%2520experiment%2520on%2520a%2520leading%2520short-video%2520platform%2520in%250AAsia%2520to%2520provide%2520about%25201%2520million%2520users%2520access%2520to%2520AI-generated%2520titles%2520for%2520their%250Auploaded%2520videos.%2520Our%2520findings%2520show%2520that%2520the%2520provision%2520of%2520AI-generated%2520titles%250Asignificantly%2520boosted%2520content%2520consumption%252C%2520increasing%2520valid%2520watches%2520by%25201.6%2525%2520and%250Awatch%2520duration%2520by%25200.9%2525.%2520When%2520producers%2520adopted%2520these%2520titles%252C%2520these%2520increases%250Ajumped%2520to%25207.1%2525%2520and%25204.1%2525%252C%2520respectively.%2520This%2520viewership-boost%2520effect%2520was%2520largely%250Aattributed%2520to%2520the%2520use%2520of%2520this%2520generative%2520AI%2520%2528GAI%2529%2520tool%2520increasing%2520the%250Alikelihood%2520of%2520videos%2520having%2520a%2520title%2520by%252041.4%2525.%2520The%2520effect%2520was%2520more%2520pronounced%250Afor%2520groups%2520more%2520affected%2520by%2520metadata%2520sparsity.%2520Mechanism%2520analysis%2520revealed%2520that%250AAI-generated%2520metadata%2520improved%2520user-video%2520matching%2520accuracy%2520in%2520the%2520platform%2527s%250Arecommender%2520system.%2520Interestingly%252C%2520for%2520a%2520video%2520for%2520which%2520the%2520producer%2520would%250Ahave%2520posted%2520a%2520title%2520anyway%252C%2520adopting%2520the%2520AI-generated%2520title%2520decreased%2520its%250Aviewership%2520on%2520average%252C%2520implying%2520that%2520AI-generated%2520titles%2520may%2520be%2520of%2520lower%250Aquality%2520than%2520human-generated%2520ones.%2520However%252C%2520when%2520producers%2520chose%2520to%2520co-create%250Awith%2520GAI%2520and%2520significantly%2520revised%2520the%2520AI-generated%2520titles%252C%2520the%2520videos%250Aoutperformed%2520their%2520counterparts%2520with%2520either%2520fully%2520AI-generated%2520or%250Ahuman-generated%2520titles%252C%2520showcasing%2520the%2520benefits%2520of%2520human-AI%2520co-creation.%2520This%250Astudy%2520highlights%2520the%2520value%2520of%2520AI-generated%2520metadata%2520and%2520human-AI%2520metadata%250Aco-creation%2520in%2520enhancing%2520user-content%2520matching%2520and%2520content%2520consumption%2520for%2520UGC%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Value%20of%20AI-Generated%20Metadata%20for%20UGC%20Platforms%3A%20Evidence%20from%20a%0A%20%20Large-scale%20Field%20Experiment&entry.906535625=Xinyi%20Zhang%20and%20Chenshuo%20Sun%20and%20Renyu%20Zhang%20and%20Khim-Yong%20Goh&entry.1292438233=%20%20AI-generated%20content%20%28AIGC%29%2C%20such%20as%20advertisement%20copy%2C%20product%0Adescriptions%2C%20and%20social%20media%20posts%2C%20is%20becoming%20ubiquitous%20in%20business%0Apractices.%20However%2C%20the%20value%20of%20AI-generated%20metadata%2C%20such%20as%20titles%2C%20remains%0Aunclear%20on%20user-generated%20content%20%28UGC%29%20platforms.%20To%20address%20this%20gap%2C%20we%0Aconducted%20a%20large-scale%20field%20experiment%20on%20a%20leading%20short-video%20platform%20in%0AAsia%20to%20provide%20about%201%20million%20users%20access%20to%20AI-generated%20titles%20for%20their%0Auploaded%20videos.%20Our%20findings%20show%20that%20the%20provision%20of%20AI-generated%20titles%0Asignificantly%20boosted%20content%20consumption%2C%20increasing%20valid%20watches%20by%201.6%25%20and%0Awatch%20duration%20by%200.9%25.%20When%20producers%20adopted%20these%20titles%2C%20these%20increases%0Ajumped%20to%207.1%25%20and%204.1%25%2C%20respectively.%20This%20viewership-boost%20effect%20was%20largely%0Aattributed%20to%20the%20use%20of%20this%20generative%20AI%20%28GAI%29%20tool%20increasing%20the%0Alikelihood%20of%20videos%20having%20a%20title%20by%2041.4%25.%20The%20effect%20was%20more%20pronounced%0Afor%20groups%20more%20affected%20by%20metadata%20sparsity.%20Mechanism%20analysis%20revealed%20that%0AAI-generated%20metadata%20improved%20user-video%20matching%20accuracy%20in%20the%20platform%27s%0Arecommender%20system.%20Interestingly%2C%20for%20a%20video%20for%20which%20the%20producer%20would%0Ahave%20posted%20a%20title%20anyway%2C%20adopting%20the%20AI-generated%20title%20decreased%20its%0Aviewership%20on%20average%2C%20implying%20that%20AI-generated%20titles%20may%20be%20of%20lower%0Aquality%20than%20human-generated%20ones.%20However%2C%20when%20producers%20chose%20to%20co-create%0Awith%20GAI%20and%20significantly%20revised%20the%20AI-generated%20titles%2C%20the%20videos%0Aoutperformed%20their%20counterparts%20with%20either%20fully%20AI-generated%20or%0Ahuman-generated%20titles%2C%20showcasing%20the%20benefits%20of%20human-AI%20co-creation.%20This%0Astudy%20highlights%20the%20value%20of%20AI-generated%20metadata%20and%20human-AI%20metadata%0Aco-creation%20in%20enhancing%20user-content%20matching%20and%20content%20consumption%20for%20UGC%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18337v1&entry.124074799=Read"},
{"title": "Refining CNN-based Heatmap Regression with Gradient-based Corner Points\n  for Electrode Localization", "author": "Lin Wu", "abstract": "  We propose a method for detecting the electrode positions in lithium-ion\nbatteries. The process begins by identifying the region of interest (ROI) in\nthe battery's X-ray image through corner point detection. A convolutional\nneural network is then used to regress the pole positions within this ROI.\nFinally, the regressed positions are optimized and corrected using corner point\npriors, significantly mitigating the loss of localization accuracy caused by\noperations such as feature map down-sampling and padding during network\ntraining. Our findings show that combining traditional pixel gradient analysis\nwith CNN-based heatmap regression for keypoint extraction enhances both\naccuracy and efficiency, resulting in significant performance improvements.\n", "link": "http://arxiv.org/abs/2412.17105v2", "date": "2024-12-24", "relevancy": 2.4807, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5403}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4811}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refining%20CNN-based%20Heatmap%20Regression%20with%20Gradient-based%20Corner%20Points%0A%20%20for%20Electrode%20Localization&body=Title%3A%20Refining%20CNN-based%20Heatmap%20Regression%20with%20Gradient-based%20Corner%20Points%0A%20%20for%20Electrode%20Localization%0AAuthor%3A%20Lin%20Wu%0AAbstract%3A%20%20%20We%20propose%20a%20method%20for%20detecting%20the%20electrode%20positions%20in%20lithium-ion%0Abatteries.%20The%20process%20begins%20by%20identifying%20the%20region%20of%20interest%20%28ROI%29%20in%0Athe%20battery%27s%20X-ray%20image%20through%20corner%20point%20detection.%20A%20convolutional%0Aneural%20network%20is%20then%20used%20to%20regress%20the%20pole%20positions%20within%20this%20ROI.%0AFinally%2C%20the%20regressed%20positions%20are%20optimized%20and%20corrected%20using%20corner%20point%0Apriors%2C%20significantly%20mitigating%20the%20loss%20of%20localization%20accuracy%20caused%20by%0Aoperations%20such%20as%20feature%20map%20down-sampling%20and%20padding%20during%20network%0Atraining.%20Our%20findings%20show%20that%20combining%20traditional%20pixel%20gradient%20analysis%0Awith%20CNN-based%20heatmap%20regression%20for%20keypoint%20extraction%20enhances%20both%0Aaccuracy%20and%20efficiency%2C%20resulting%20in%20significant%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefining%2520CNN-based%2520Heatmap%2520Regression%2520with%2520Gradient-based%2520Corner%2520Points%250A%2520%2520for%2520Electrode%2520Localization%26entry.906535625%3DLin%2520Wu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520for%2520detecting%2520the%2520electrode%2520positions%2520in%2520lithium-ion%250Abatteries.%2520The%2520process%2520begins%2520by%2520identifying%2520the%2520region%2520of%2520interest%2520%2528ROI%2529%2520in%250Athe%2520battery%2527s%2520X-ray%2520image%2520through%2520corner%2520point%2520detection.%2520A%2520convolutional%250Aneural%2520network%2520is%2520then%2520used%2520to%2520regress%2520the%2520pole%2520positions%2520within%2520this%2520ROI.%250AFinally%252C%2520the%2520regressed%2520positions%2520are%2520optimized%2520and%2520corrected%2520using%2520corner%2520point%250Apriors%252C%2520significantly%2520mitigating%2520the%2520loss%2520of%2520localization%2520accuracy%2520caused%2520by%250Aoperations%2520such%2520as%2520feature%2520map%2520down-sampling%2520and%2520padding%2520during%2520network%250Atraining.%2520Our%2520findings%2520show%2520that%2520combining%2520traditional%2520pixel%2520gradient%2520analysis%250Awith%2520CNN-based%2520heatmap%2520regression%2520for%2520keypoint%2520extraction%2520enhances%2520both%250Aaccuracy%2520and%2520efficiency%252C%2520resulting%2520in%2520significant%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20CNN-based%20Heatmap%20Regression%20with%20Gradient-based%20Corner%20Points%0A%20%20for%20Electrode%20Localization&entry.906535625=Lin%20Wu&entry.1292438233=%20%20We%20propose%20a%20method%20for%20detecting%20the%20electrode%20positions%20in%20lithium-ion%0Abatteries.%20The%20process%20begins%20by%20identifying%20the%20region%20of%20interest%20%28ROI%29%20in%0Athe%20battery%27s%20X-ray%20image%20through%20corner%20point%20detection.%20A%20convolutional%0Aneural%20network%20is%20then%20used%20to%20regress%20the%20pole%20positions%20within%20this%20ROI.%0AFinally%2C%20the%20regressed%20positions%20are%20optimized%20and%20corrected%20using%20corner%20point%0Apriors%2C%20significantly%20mitigating%20the%20loss%20of%20localization%20accuracy%20caused%20by%0Aoperations%20such%20as%20feature%20map%20down-sampling%20and%20padding%20during%20network%0Atraining.%20Our%20findings%20show%20that%20combining%20traditional%20pixel%20gradient%20analysis%0Awith%20CNN-based%20heatmap%20regression%20for%20keypoint%20extraction%20enhances%20both%0Aaccuracy%20and%20efficiency%2C%20resulting%20in%20significant%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17105v2&entry.124074799=Read"},
{"title": "FPPL: An Efficient and Non-IID Robust Federated Continual Learning\n  Framework", "author": "Yuchen He and Chuyun Shen and Xiangfeng Wang and Bo Jin", "abstract": "  Federated continual learning (FCL) aims to learn from sequential data stream\nin the decentralized federated learning setting, while simultaneously\nmitigating the catastrophic forgetting issue in classical continual learning.\nExisting FCL methods usually employ typical rehearsal mechanisms, which could\nresult in privacy violations or additional onerous storage and computational\nburdens. In this work, an efficient and non-IID robust federated continual\nlearning framework, called Federated Prototype-Augmented Prompt Learning\n(FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts\naugmented by prototypes without rehearsal. On the client side, a fusion\nfunction is employed to fully leverage the knowledge contained in task-specific\nprompts for alleviating catastrophic forgetting. Additionally, global\nprototypes aggregated from the server are used to obtain unified representation\nthrough contrastive learning, mitigating the impact of non-IID-derived data\nheterogeneity. On the server side, locally uploaded prototypes are utilized to\nperform debiasing on the classifier, further alleviating the performance\ndegradation caused by both non-IID and catastrophic forgetting. Empirical\nevaluations demonstrate the effectiveness of FPPL, achieving notable\nperformance with an efficient design while remaining robust to diverse non-IID\ndegrees. Code is available at: https://github.com/ycheoo/FPPL.\n", "link": "http://arxiv.org/abs/2411.01904v3", "date": "2024-12-24", "relevancy": 2.4784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FPPL%3A%20An%20Efficient%20and%20Non-IID%20Robust%20Federated%20Continual%20Learning%0A%20%20Framework&body=Title%3A%20FPPL%3A%20An%20Efficient%20and%20Non-IID%20Robust%20Federated%20Continual%20Learning%0A%20%20Framework%0AAuthor%3A%20Yuchen%20He%20and%20Chuyun%20Shen%20and%20Xiangfeng%20Wang%20and%20Bo%20Jin%0AAbstract%3A%20%20%20Federated%20continual%20learning%20%28FCL%29%20aims%20to%20learn%20from%20sequential%20data%20stream%0Ain%20the%20decentralized%20federated%20learning%20setting%2C%20while%20simultaneously%0Amitigating%20the%20catastrophic%20forgetting%20issue%20in%20classical%20continual%20learning.%0AExisting%20FCL%20methods%20usually%20employ%20typical%20rehearsal%20mechanisms%2C%20which%20could%0Aresult%20in%20privacy%20violations%20or%20additional%20onerous%20storage%20and%20computational%0Aburdens.%20In%20this%20work%2C%20an%20efficient%20and%20non-IID%20robust%20federated%20continual%0Alearning%20framework%2C%20called%20Federated%20Prototype-Augmented%20Prompt%20Learning%0A%28FPPL%29%2C%20is%20proposed.%20The%20FPPL%20can%20collaboratively%20learn%20lightweight%20prompts%0Aaugmented%20by%20prototypes%20without%20rehearsal.%20On%20the%20client%20side%2C%20a%20fusion%0Afunction%20is%20employed%20to%20fully%20leverage%20the%20knowledge%20contained%20in%20task-specific%0Aprompts%20for%20alleviating%20catastrophic%20forgetting.%20Additionally%2C%20global%0Aprototypes%20aggregated%20from%20the%20server%20are%20used%20to%20obtain%20unified%20representation%0Athrough%20contrastive%20learning%2C%20mitigating%20the%20impact%20of%20non-IID-derived%20data%0Aheterogeneity.%20On%20the%20server%20side%2C%20locally%20uploaded%20prototypes%20are%20utilized%20to%0Aperform%20debiasing%20on%20the%20classifier%2C%20further%20alleviating%20the%20performance%0Adegradation%20caused%20by%20both%20non-IID%20and%20catastrophic%20forgetting.%20Empirical%0Aevaluations%20demonstrate%20the%20effectiveness%20of%20FPPL%2C%20achieving%20notable%0Aperformance%20with%20an%20efficient%20design%20while%20remaining%20robust%20to%20diverse%20non-IID%0Adegrees.%20Code%20is%20available%20at%3A%20https%3A//github.com/ycheoo/FPPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01904v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFPPL%253A%2520An%2520Efficient%2520and%2520Non-IID%2520Robust%2520Federated%2520Continual%2520Learning%250A%2520%2520Framework%26entry.906535625%3DYuchen%2520He%2520and%2520Chuyun%2520Shen%2520and%2520Xiangfeng%2520Wang%2520and%2520Bo%2520Jin%26entry.1292438233%3D%2520%2520Federated%2520continual%2520learning%2520%2528FCL%2529%2520aims%2520to%2520learn%2520from%2520sequential%2520data%2520stream%250Ain%2520the%2520decentralized%2520federated%2520learning%2520setting%252C%2520while%2520simultaneously%250Amitigating%2520the%2520catastrophic%2520forgetting%2520issue%2520in%2520classical%2520continual%2520learning.%250AExisting%2520FCL%2520methods%2520usually%2520employ%2520typical%2520rehearsal%2520mechanisms%252C%2520which%2520could%250Aresult%2520in%2520privacy%2520violations%2520or%2520additional%2520onerous%2520storage%2520and%2520computational%250Aburdens.%2520In%2520this%2520work%252C%2520an%2520efficient%2520and%2520non-IID%2520robust%2520federated%2520continual%250Alearning%2520framework%252C%2520called%2520Federated%2520Prototype-Augmented%2520Prompt%2520Learning%250A%2528FPPL%2529%252C%2520is%2520proposed.%2520The%2520FPPL%2520can%2520collaboratively%2520learn%2520lightweight%2520prompts%250Aaugmented%2520by%2520prototypes%2520without%2520rehearsal.%2520On%2520the%2520client%2520side%252C%2520a%2520fusion%250Afunction%2520is%2520employed%2520to%2520fully%2520leverage%2520the%2520knowledge%2520contained%2520in%2520task-specific%250Aprompts%2520for%2520alleviating%2520catastrophic%2520forgetting.%2520Additionally%252C%2520global%250Aprototypes%2520aggregated%2520from%2520the%2520server%2520are%2520used%2520to%2520obtain%2520unified%2520representation%250Athrough%2520contrastive%2520learning%252C%2520mitigating%2520the%2520impact%2520of%2520non-IID-derived%2520data%250Aheterogeneity.%2520On%2520the%2520server%2520side%252C%2520locally%2520uploaded%2520prototypes%2520are%2520utilized%2520to%250Aperform%2520debiasing%2520on%2520the%2520classifier%252C%2520further%2520alleviating%2520the%2520performance%250Adegradation%2520caused%2520by%2520both%2520non-IID%2520and%2520catastrophic%2520forgetting.%2520Empirical%250Aevaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520FPPL%252C%2520achieving%2520notable%250Aperformance%2520with%2520an%2520efficient%2520design%2520while%2520remaining%2520robust%2520to%2520diverse%2520non-IID%250Adegrees.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/ycheoo/FPPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01904v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FPPL%3A%20An%20Efficient%20and%20Non-IID%20Robust%20Federated%20Continual%20Learning%0A%20%20Framework&entry.906535625=Yuchen%20He%20and%20Chuyun%20Shen%20and%20Xiangfeng%20Wang%20and%20Bo%20Jin&entry.1292438233=%20%20Federated%20continual%20learning%20%28FCL%29%20aims%20to%20learn%20from%20sequential%20data%20stream%0Ain%20the%20decentralized%20federated%20learning%20setting%2C%20while%20simultaneously%0Amitigating%20the%20catastrophic%20forgetting%20issue%20in%20classical%20continual%20learning.%0AExisting%20FCL%20methods%20usually%20employ%20typical%20rehearsal%20mechanisms%2C%20which%20could%0Aresult%20in%20privacy%20violations%20or%20additional%20onerous%20storage%20and%20computational%0Aburdens.%20In%20this%20work%2C%20an%20efficient%20and%20non-IID%20robust%20federated%20continual%0Alearning%20framework%2C%20called%20Federated%20Prototype-Augmented%20Prompt%20Learning%0A%28FPPL%29%2C%20is%20proposed.%20The%20FPPL%20can%20collaboratively%20learn%20lightweight%20prompts%0Aaugmented%20by%20prototypes%20without%20rehearsal.%20On%20the%20client%20side%2C%20a%20fusion%0Afunction%20is%20employed%20to%20fully%20leverage%20the%20knowledge%20contained%20in%20task-specific%0Aprompts%20for%20alleviating%20catastrophic%20forgetting.%20Additionally%2C%20global%0Aprototypes%20aggregated%20from%20the%20server%20are%20used%20to%20obtain%20unified%20representation%0Athrough%20contrastive%20learning%2C%20mitigating%20the%20impact%20of%20non-IID-derived%20data%0Aheterogeneity.%20On%20the%20server%20side%2C%20locally%20uploaded%20prototypes%20are%20utilized%20to%0Aperform%20debiasing%20on%20the%20classifier%2C%20further%20alleviating%20the%20performance%0Adegradation%20caused%20by%20both%20non-IID%20and%20catastrophic%20forgetting.%20Empirical%0Aevaluations%20demonstrate%20the%20effectiveness%20of%20FPPL%2C%20achieving%20notable%0Aperformance%20with%20an%20efficient%20design%20while%20remaining%20robust%20to%20diverse%20non-IID%0Adegrees.%20Code%20is%20available%20at%3A%20https%3A//github.com/ycheoo/FPPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01904v3&entry.124074799=Read"},
{"title": "TableRAG: Million-Token Table Understanding with Language Models", "author": "Si-An Chen and Lesly Miculicich and Julian Martin Eisenschlos and Zifeng Wang and Zilong Wang and Yanfei Chen and Yasuhisa Fujii and Hsuan-Tien Lin and Chen-Yu Lee and Tomas Pfister", "abstract": "  Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.\n", "link": "http://arxiv.org/abs/2410.04739v2", "date": "2024-12-24", "relevancy": 2.4603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TableRAG%3A%20Million-Token%20Table%20Understanding%20with%20Language%20Models&body=Title%3A%20TableRAG%3A%20Million-Token%20Table%20Understanding%20with%20Language%20Models%0AAuthor%3A%20Si-An%20Chen%20and%20Lesly%20Miculicich%20and%20Julian%20Martin%20Eisenschlos%20and%20Zifeng%20Wang%20and%20Zilong%20Wang%20and%20Yanfei%20Chen%20and%20Yasuhisa%20Fujii%20and%20Hsuan-Tien%20Lin%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister%0AAbstract%3A%20%20%20Recent%20advancements%20in%20language%20models%20%28LMs%29%20have%20notably%20enhanced%20their%0Aability%20to%20reason%20with%20tabular%20data%2C%20primarily%20through%20program-aided%20mechanisms%0Athat%20manipulate%20and%20analyze%20tables.%20However%2C%20these%20methods%20often%20require%20the%0Aentire%20table%20as%20input%2C%20leading%20to%20scalability%20challenges%20due%20to%20the%20positional%0Abias%20or%20context%20length%20constraints.%20In%20response%20to%20these%20challenges%2C%20we%0Aintroduce%20TableRAG%2C%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%0Aspecifically%20designed%20for%20LM-based%20table%20understanding.%20TableRAG%20leverages%0Aquery%20expansion%20combined%20with%20schema%20and%20cell%20retrieval%20to%20pinpoint%20crucial%0Ainformation%20before%20providing%20it%20to%20the%20LMs.%20This%20enables%20more%20efficient%20data%0Aencoding%20and%20precise%20retrieval%2C%20significantly%20reducing%20prompt%20lengths%20and%0Amitigating%20information%20loss.%20We%20have%20developed%20two%20new%20million-token%20benchmarks%0Afrom%20the%20Arcade%20and%20BIRD-SQL%20datasets%20to%20thoroughly%20evaluate%20TableRAG%27s%0Aeffectiveness%20at%20scale.%20Our%20results%20demonstrate%20that%20TableRAG%27s%20retrieval%0Adesign%20achieves%20the%20highest%20retrieval%20quality%2C%20leading%20to%20the%20new%0Astate-of-the-art%20performance%20on%20large-scale%20table%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTableRAG%253A%2520Million-Token%2520Table%2520Understanding%2520with%2520Language%2520Models%26entry.906535625%3DSi-An%2520Chen%2520and%2520Lesly%2520Miculicich%2520and%2520Julian%2520Martin%2520Eisenschlos%2520and%2520Zifeng%2520Wang%2520and%2520Zilong%2520Wang%2520and%2520Yanfei%2520Chen%2520and%2520Yasuhisa%2520Fujii%2520and%2520Hsuan-Tien%2520Lin%2520and%2520Chen-Yu%2520Lee%2520and%2520Tomas%2520Pfister%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520language%2520models%2520%2528LMs%2529%2520have%2520notably%2520enhanced%2520their%250Aability%2520to%2520reason%2520with%2520tabular%2520data%252C%2520primarily%2520through%2520program-aided%2520mechanisms%250Athat%2520manipulate%2520and%2520analyze%2520tables.%2520However%252C%2520these%2520methods%2520often%2520require%2520the%250Aentire%2520table%2520as%2520input%252C%2520leading%2520to%2520scalability%2520challenges%2520due%2520to%2520the%2520positional%250Abias%2520or%2520context%2520length%2520constraints.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%250Aintroduce%2520TableRAG%252C%2520a%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520framework%250Aspecifically%2520designed%2520for%2520LM-based%2520table%2520understanding.%2520TableRAG%2520leverages%250Aquery%2520expansion%2520combined%2520with%2520schema%2520and%2520cell%2520retrieval%2520to%2520pinpoint%2520crucial%250Ainformation%2520before%2520providing%2520it%2520to%2520the%2520LMs.%2520This%2520enables%2520more%2520efficient%2520data%250Aencoding%2520and%2520precise%2520retrieval%252C%2520significantly%2520reducing%2520prompt%2520lengths%2520and%250Amitigating%2520information%2520loss.%2520We%2520have%2520developed%2520two%2520new%2520million-token%2520benchmarks%250Afrom%2520the%2520Arcade%2520and%2520BIRD-SQL%2520datasets%2520to%2520thoroughly%2520evaluate%2520TableRAG%2527s%250Aeffectiveness%2520at%2520scale.%2520Our%2520results%2520demonstrate%2520that%2520TableRAG%2527s%2520retrieval%250Adesign%2520achieves%2520the%2520highest%2520retrieval%2520quality%252C%2520leading%2520to%2520the%2520new%250Astate-of-the-art%2520performance%2520on%2520large-scale%2520table%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TableRAG%3A%20Million-Token%20Table%20Understanding%20with%20Language%20Models&entry.906535625=Si-An%20Chen%20and%20Lesly%20Miculicich%20and%20Julian%20Martin%20Eisenschlos%20and%20Zifeng%20Wang%20and%20Zilong%20Wang%20and%20Yanfei%20Chen%20and%20Yasuhisa%20Fujii%20and%20Hsuan-Tien%20Lin%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister&entry.1292438233=%20%20Recent%20advancements%20in%20language%20models%20%28LMs%29%20have%20notably%20enhanced%20their%0Aability%20to%20reason%20with%20tabular%20data%2C%20primarily%20through%20program-aided%20mechanisms%0Athat%20manipulate%20and%20analyze%20tables.%20However%2C%20these%20methods%20often%20require%20the%0Aentire%20table%20as%20input%2C%20leading%20to%20scalability%20challenges%20due%20to%20the%20positional%0Abias%20or%20context%20length%20constraints.%20In%20response%20to%20these%20challenges%2C%20we%0Aintroduce%20TableRAG%2C%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%0Aspecifically%20designed%20for%20LM-based%20table%20understanding.%20TableRAG%20leverages%0Aquery%20expansion%20combined%20with%20schema%20and%20cell%20retrieval%20to%20pinpoint%20crucial%0Ainformation%20before%20providing%20it%20to%20the%20LMs.%20This%20enables%20more%20efficient%20data%0Aencoding%20and%20precise%20retrieval%2C%20significantly%20reducing%20prompt%20lengths%20and%0Amitigating%20information%20loss.%20We%20have%20developed%20two%20new%20million-token%20benchmarks%0Afrom%20the%20Arcade%20and%20BIRD-SQL%20datasets%20to%20thoroughly%20evaluate%20TableRAG%27s%0Aeffectiveness%20at%20scale.%20Our%20results%20demonstrate%20that%20TableRAG%27s%20retrieval%0Adesign%20achieves%20the%20highest%20retrieval%20quality%2C%20leading%20to%20the%20new%0Astate-of-the-art%20performance%20on%20large-scale%20table%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04739v2&entry.124074799=Read"},
{"title": "Gaussian entropic optimal transport: Schr\u00f6dinger bridges and the\n  Sinkhorn algorithm", "author": "O. Deniz Akyildiz and Pierre Del Moral and Joaqu\u00edn Miguez", "abstract": "  Entropic optimal transport problems are regularized versions of optimal\ntransport problems. These models play an increasingly important role in machine\nlearning and generative modelling. For finite spaces, these problems are\ncommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting\nprocedure). However, in more general settings the Sinkhorn iterations are based\non nonlinear conditional/conjugate transformations and exact finite-dimensional\nsolutions cannot be computed. This article presents a finite-dimensional\nrecursive formulation of the iterative proportional fitting procedure for\ngeneral Gaussian multivariate models. As expected, this recursive formulation\nis closely related to the celebrated Kalman filter and related Riccati matrix\ndifference equations, and it yields algorithms that can be implemented in\npractical settings without further approximations. We extend this filtering\nmethodology to develop a refined and self-contained convergence analysis of\nGaussian Sinkhorn algorithms, including closed form expressions of entropic\ntransport maps and Schr\\\"odinger bridges.\n", "link": "http://arxiv.org/abs/2412.18432v1", "date": "2024-12-24", "relevancy": 2.4431, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5094}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.485}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&body=Title%3A%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm%0AAuthor%3A%20O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez%0AAbstract%3A%20%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%20This%20article%20presents%20a%20finite-dimensional%0Arecursive%20formulation%20of%20the%20iterative%20proportional%20fitting%20procedure%20for%0Ageneral%20Gaussian%20multivariate%20models.%20As%20expected%2C%20this%20recursive%20formulation%0Ais%20closely%20related%20to%20the%20celebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%0Adifference%20equations%2C%20and%20it%20yields%20algorithms%20that%20can%20be%20implemented%20in%0Apractical%20settings%20without%20further%20approximations.%20We%20extend%20this%20filtering%0Amethodology%20to%20develop%20a%20refined%20and%20self-contained%20convergence%20analysis%20of%0AGaussian%20Sinkhorn%20algorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%0Atransport%20maps%20and%20Schr%5C%22odinger%20bridges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520entropic%2520optimal%2520transport%253A%2520Schr%25C3%25B6dinger%2520bridges%2520and%2520the%250A%2520%2520Sinkhorn%2520algorithm%26entry.906535625%3DO.%2520Deniz%2520Akyildiz%2520and%2520Pierre%2520Del%2520Moral%2520and%2520Joaqu%25C3%25ADn%2520Miguez%26entry.1292438233%3D%2520%2520Entropic%2520optimal%2520transport%2520problems%2520are%2520regularized%2520versions%2520of%2520optimal%250Atransport%2520problems.%2520These%2520models%2520play%2520an%2520increasingly%2520important%2520role%2520in%2520machine%250Alearning%2520and%2520generative%2520modelling.%2520For%2520finite%2520spaces%252C%2520these%2520problems%2520are%250Acommonly%2520solved%2520using%2520Sinkhorn%2520algorithm%2520%2528a.k.a.%2520iterative%2520proportional%2520fitting%250Aprocedure%2529.%2520However%252C%2520in%2520more%2520general%2520settings%2520the%2520Sinkhorn%2520iterations%2520are%2520based%250Aon%2520nonlinear%2520conditional/conjugate%2520transformations%2520and%2520exact%2520finite-dimensional%250Asolutions%2520cannot%2520be%2520computed.%2520This%2520article%2520presents%2520a%2520finite-dimensional%250Arecursive%2520formulation%2520of%2520the%2520iterative%2520proportional%2520fitting%2520procedure%2520for%250Ageneral%2520Gaussian%2520multivariate%2520models.%2520As%2520expected%252C%2520this%2520recursive%2520formulation%250Ais%2520closely%2520related%2520to%2520the%2520celebrated%2520Kalman%2520filter%2520and%2520related%2520Riccati%2520matrix%250Adifference%2520equations%252C%2520and%2520it%2520yields%2520algorithms%2520that%2520can%2520be%2520implemented%2520in%250Apractical%2520settings%2520without%2520further%2520approximations.%2520We%2520extend%2520this%2520filtering%250Amethodology%2520to%2520develop%2520a%2520refined%2520and%2520self-contained%2520convergence%2520analysis%2520of%250AGaussian%2520Sinkhorn%2520algorithms%252C%2520including%2520closed%2520form%2520expressions%2520of%2520entropic%250Atransport%2520maps%2520and%2520Schr%255C%2522odinger%2520bridges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&entry.906535625=O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez&entry.1292438233=%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%20This%20article%20presents%20a%20finite-dimensional%0Arecursive%20formulation%20of%20the%20iterative%20proportional%20fitting%20procedure%20for%0Ageneral%20Gaussian%20multivariate%20models.%20As%20expected%2C%20this%20recursive%20formulation%0Ais%20closely%20related%20to%20the%20celebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%0Adifference%20equations%2C%20and%20it%20yields%20algorithms%20that%20can%20be%20implemented%20in%0Apractical%20settings%20without%20further%20approximations.%20We%20extend%20this%20filtering%0Amethodology%20to%20develop%20a%20refined%20and%20self-contained%20convergence%20analysis%20of%0AGaussian%20Sinkhorn%20algorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%0Atransport%20maps%20and%20Schr%5C%22odinger%20bridges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18432v1&entry.124074799=Read"},
{"title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized\n  Attention", "author": "Siyuan Huang and Yunchong Song and Jiayue Zhou and Zhouhan Lin", "abstract": "  In the realm of graph learning, there is a category of methods that\nconceptualize graphs as hierarchical structures, utilizing node clustering to\ncapture broader structural information. While generally effective, these\nmethods often rely on a fixed graph coarsening routine, leading to overly\nhomogeneous cluster representations and loss of node-level information. In this\npaper, we envision the graph as a network of interconnected node sets without\ncompressing each cluster into a single embedding. To enable effective\ninformation transfer among these node sets, we propose the Node-to-Cluster\nAttention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple\nKernel Learning into the kernelized attention framework, effectively capturing\ninformation at both node and cluster levels. We then devise an efficient form\nfor N2C-Attn using the cluster-wise message-passing framework, achieving linear\ntime complexity. We further analyze how N2C-Attn combines bi-level feature maps\nof queries and keys, demonstrating its capability to merge dual-granularity\ninformation. The resulting architecture, Cluster-wise Graph Transformer\n(Cluster-GT), which uses node clusters as tokens and employs our proposed\nN2C-Attn module, shows superior performance on various graph-level tasks. Code\nis available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.\n", "link": "http://arxiv.org/abs/2410.06746v3", "date": "2024-12-24", "relevancy": 2.4368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&body=Title%3A%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention%0AAuthor%3A%20Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin%0AAbstract%3A%20%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06746v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster-wise%2520Graph%2520Transformer%2520with%2520Dual-granularity%2520Kernelized%250A%2520%2520Attention%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Yunchong%2520Song%2520and%2520Jiayue%2520Zhou%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520graph%2520learning%252C%2520there%2520is%2520a%2520category%2520of%2520methods%2520that%250Aconceptualize%2520graphs%2520as%2520hierarchical%2520structures%252C%2520utilizing%2520node%2520clustering%2520to%250Acapture%2520broader%2520structural%2520information.%2520While%2520generally%2520effective%252C%2520these%250Amethods%2520often%2520rely%2520on%2520a%2520fixed%2520graph%2520coarsening%2520routine%252C%2520leading%2520to%2520overly%250Ahomogeneous%2520cluster%2520representations%2520and%2520loss%2520of%2520node-level%2520information.%2520In%2520this%250Apaper%252C%2520we%2520envision%2520the%2520graph%2520as%2520a%2520network%2520of%2520interconnected%2520node%2520sets%2520without%250Acompressing%2520each%2520cluster%2520into%2520a%2520single%2520embedding.%2520To%2520enable%2520effective%250Ainformation%2520transfer%2520among%2520these%2520node%2520sets%252C%2520we%2520propose%2520the%2520Node-to-Cluster%250AAttention%2520%2528N2C-Attn%2529%2520mechanism.%2520N2C-Attn%2520incorporates%2520techniques%2520from%2520Multiple%250AKernel%2520Learning%2520into%2520the%2520kernelized%2520attention%2520framework%252C%2520effectively%2520capturing%250Ainformation%2520at%2520both%2520node%2520and%2520cluster%2520levels.%2520We%2520then%2520devise%2520an%2520efficient%2520form%250Afor%2520N2C-Attn%2520using%2520the%2520cluster-wise%2520message-passing%2520framework%252C%2520achieving%2520linear%250Atime%2520complexity.%2520We%2520further%2520analyze%2520how%2520N2C-Attn%2520combines%2520bi-level%2520feature%2520maps%250Aof%2520queries%2520and%2520keys%252C%2520demonstrating%2520its%2520capability%2520to%2520merge%2520dual-granularity%250Ainformation.%2520The%2520resulting%2520architecture%252C%2520Cluster-wise%2520Graph%2520Transformer%250A%2528Cluster-GT%2529%252C%2520which%2520uses%2520node%2520clusters%2520as%2520tokens%2520and%2520employs%2520our%2520proposed%250AN2C-Attn%2520module%252C%2520shows%2520superior%2520performance%2520on%2520various%2520graph-level%2520tasks.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06746v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&entry.906535625=Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin&entry.1292438233=%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06746v3&entry.124074799=Read"},
{"title": "Graph Neural Networks Are Evolutionary Algorithms", "author": "Kaichen Ouyang and Shengwei Fu", "abstract": "  In this paper, we reveal the intrinsic duality between graph neural networks\n(GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinct\nfields. Building on this insight, we propose Graph Neural Evolution (GNE), a\nnovel evolutionary algorithm that models individuals as nodes in a graph and\nleverages designed frequency-domain filters to balance global exploration and\nlocal exploitation. Through the use of these filters, GNE aggregates\nhigh-frequency (diversity-enhancing) and low-frequency (stability-promoting)\ninformation, transforming EAs into interpretable and tunable mechanisms in the\nfrequency domain. Extensive experiments on benchmark functions demonstrate that\nGNE consistently outperforms state-of-the-art algorithms such as GA, DE,\nCMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solution\nshifts, and noisy environments. Its robustness, adaptability, and superior\nconvergence highlight its practical and theoretical value. Beyond optimization,\nGNE establishes a conceptual and mathematical foundation linking EAs and GNNs,\noffering new perspectives for both fields. Its framework encourages the\ndevelopment of task-adaptive filters and hybrid approaches for EAs, while its\ninsights can inspire advances in GNNs, such as improved global information\npropagation and mitigation of oversmoothing. GNE's versatility extends to\nsolving challenges in machine learning, including hyperparameter tuning and\nneural architecture search, as well as real-world applications in engineering\nand operations research. By uniting the dynamics of EAs with the structural\ninsights of GNNs, this work provides a foundation for interdisciplinary\ninnovation, paving the way for scalable and interpretable solutions to complex\noptimization problems.\n", "link": "http://arxiv.org/abs/2412.17629v2", "date": "2024-12-24", "relevancy": 2.4175, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5428}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4573}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms&body=Title%3A%20Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms%0AAuthor%3A%20Kaichen%20Ouyang%20and%20Shengwei%20Fu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20reveal%20the%20intrinsic%20duality%20between%20graph%20neural%20networks%0A%28GNNs%29%20and%20evolutionary%20algorithms%20%28EAs%29%2C%20bridging%20two%20traditionally%20distinct%0Afields.%20Building%20on%20this%20insight%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20evolutionary%20algorithm%20that%20models%20individuals%20as%20nodes%20in%20a%20graph%20and%0Aleverages%20designed%20frequency-domain%20filters%20to%20balance%20global%20exploration%20and%0Alocal%20exploitation.%20Through%20the%20use%20of%20these%20filters%2C%20GNE%20aggregates%0Ahigh-frequency%20%28diversity-enhancing%29%20and%20low-frequency%20%28stability-promoting%29%0Ainformation%2C%20transforming%20EAs%20into%20interpretable%20and%20tunable%20mechanisms%20in%20the%0Afrequency%20domain.%20Extensive%20experiments%20on%20benchmark%20functions%20demonstrate%20that%0AGNE%20consistently%20outperforms%20state-of-the-art%20algorithms%20such%20as%20GA%2C%20DE%2C%0ACMA-ES%2C%20SDAES%2C%20and%20RL-SHADE%2C%20excelling%20in%20complex%20landscapes%2C%20optimal%20solution%0Ashifts%2C%20and%20noisy%20environments.%20Its%20robustness%2C%20adaptability%2C%20and%20superior%0Aconvergence%20highlight%20its%20practical%20and%20theoretical%20value.%20Beyond%20optimization%2C%0AGNE%20establishes%20a%20conceptual%20and%20mathematical%20foundation%20linking%20EAs%20and%20GNNs%2C%0Aoffering%20new%20perspectives%20for%20both%20fields.%20Its%20framework%20encourages%20the%0Adevelopment%20of%20task-adaptive%20filters%20and%20hybrid%20approaches%20for%20EAs%2C%20while%20its%0Ainsights%20can%20inspire%20advances%20in%20GNNs%2C%20such%20as%20improved%20global%20information%0Apropagation%20and%20mitigation%20of%20oversmoothing.%20GNE%27s%20versatility%20extends%20to%0Asolving%20challenges%20in%20machine%20learning%2C%20including%20hyperparameter%20tuning%20and%0Aneural%20architecture%20search%2C%20as%20well%20as%20real-world%20applications%20in%20engineering%0Aand%20operations%20research.%20By%20uniting%20the%20dynamics%20of%20EAs%20with%20the%20structural%0Ainsights%20of%20GNNs%2C%20this%20work%20provides%20a%20foundation%20for%20interdisciplinary%0Ainnovation%2C%20paving%20the%20way%20for%20scalable%20and%20interpretable%20solutions%20to%20complex%0Aoptimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520Are%2520Evolutionary%2520Algorithms%26entry.906535625%3DKaichen%2520Ouyang%2520and%2520Shengwei%2520Fu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520reveal%2520the%2520intrinsic%2520duality%2520between%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520and%2520evolutionary%2520algorithms%2520%2528EAs%2529%252C%2520bridging%2520two%2520traditionally%2520distinct%250Afields.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Graph%2520Neural%2520Evolution%2520%2528GNE%2529%252C%2520a%250Anovel%2520evolutionary%2520algorithm%2520that%2520models%2520individuals%2520as%2520nodes%2520in%2520a%2520graph%2520and%250Aleverages%2520designed%2520frequency-domain%2520filters%2520to%2520balance%2520global%2520exploration%2520and%250Alocal%2520exploitation.%2520Through%2520the%2520use%2520of%2520these%2520filters%252C%2520GNE%2520aggregates%250Ahigh-frequency%2520%2528diversity-enhancing%2529%2520and%2520low-frequency%2520%2528stability-promoting%2529%250Ainformation%252C%2520transforming%2520EAs%2520into%2520interpretable%2520and%2520tunable%2520mechanisms%2520in%2520the%250Afrequency%2520domain.%2520Extensive%2520experiments%2520on%2520benchmark%2520functions%2520demonstrate%2520that%250AGNE%2520consistently%2520outperforms%2520state-of-the-art%2520algorithms%2520such%2520as%2520GA%252C%2520DE%252C%250ACMA-ES%252C%2520SDAES%252C%2520and%2520RL-SHADE%252C%2520excelling%2520in%2520complex%2520landscapes%252C%2520optimal%2520solution%250Ashifts%252C%2520and%2520noisy%2520environments.%2520Its%2520robustness%252C%2520adaptability%252C%2520and%2520superior%250Aconvergence%2520highlight%2520its%2520practical%2520and%2520theoretical%2520value.%2520Beyond%2520optimization%252C%250AGNE%2520establishes%2520a%2520conceptual%2520and%2520mathematical%2520foundation%2520linking%2520EAs%2520and%2520GNNs%252C%250Aoffering%2520new%2520perspectives%2520for%2520both%2520fields.%2520Its%2520framework%2520encourages%2520the%250Adevelopment%2520of%2520task-adaptive%2520filters%2520and%2520hybrid%2520approaches%2520for%2520EAs%252C%2520while%2520its%250Ainsights%2520can%2520inspire%2520advances%2520in%2520GNNs%252C%2520such%2520as%2520improved%2520global%2520information%250Apropagation%2520and%2520mitigation%2520of%2520oversmoothing.%2520GNE%2527s%2520versatility%2520extends%2520to%250Asolving%2520challenges%2520in%2520machine%2520learning%252C%2520including%2520hyperparameter%2520tuning%2520and%250Aneural%2520architecture%2520search%252C%2520as%2520well%2520as%2520real-world%2520applications%2520in%2520engineering%250Aand%2520operations%2520research.%2520By%2520uniting%2520the%2520dynamics%2520of%2520EAs%2520with%2520the%2520structural%250Ainsights%2520of%2520GNNs%252C%2520this%2520work%2520provides%2520a%2520foundation%2520for%2520interdisciplinary%250Ainnovation%252C%2520paving%2520the%2520way%2520for%2520scalable%2520and%2520interpretable%2520solutions%2520to%2520complex%250Aoptimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms&entry.906535625=Kaichen%20Ouyang%20and%20Shengwei%20Fu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20reveal%20the%20intrinsic%20duality%20between%20graph%20neural%20networks%0A%28GNNs%29%20and%20evolutionary%20algorithms%20%28EAs%29%2C%20bridging%20two%20traditionally%20distinct%0Afields.%20Building%20on%20this%20insight%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20evolutionary%20algorithm%20that%20models%20individuals%20as%20nodes%20in%20a%20graph%20and%0Aleverages%20designed%20frequency-domain%20filters%20to%20balance%20global%20exploration%20and%0Alocal%20exploitation.%20Through%20the%20use%20of%20these%20filters%2C%20GNE%20aggregates%0Ahigh-frequency%20%28diversity-enhancing%29%20and%20low-frequency%20%28stability-promoting%29%0Ainformation%2C%20transforming%20EAs%20into%20interpretable%20and%20tunable%20mechanisms%20in%20the%0Afrequency%20domain.%20Extensive%20experiments%20on%20benchmark%20functions%20demonstrate%20that%0AGNE%20consistently%20outperforms%20state-of-the-art%20algorithms%20such%20as%20GA%2C%20DE%2C%0ACMA-ES%2C%20SDAES%2C%20and%20RL-SHADE%2C%20excelling%20in%20complex%20landscapes%2C%20optimal%20solution%0Ashifts%2C%20and%20noisy%20environments.%20Its%20robustness%2C%20adaptability%2C%20and%20superior%0Aconvergence%20highlight%20its%20practical%20and%20theoretical%20value.%20Beyond%20optimization%2C%0AGNE%20establishes%20a%20conceptual%20and%20mathematical%20foundation%20linking%20EAs%20and%20GNNs%2C%0Aoffering%20new%20perspectives%20for%20both%20fields.%20Its%20framework%20encourages%20the%0Adevelopment%20of%20task-adaptive%20filters%20and%20hybrid%20approaches%20for%20EAs%2C%20while%20its%0Ainsights%20can%20inspire%20advances%20in%20GNNs%2C%20such%20as%20improved%20global%20information%0Apropagation%20and%20mitigation%20of%20oversmoothing.%20GNE%27s%20versatility%20extends%20to%0Asolving%20challenges%20in%20machine%20learning%2C%20including%20hyperparameter%20tuning%20and%0Aneural%20architecture%20search%2C%20as%20well%20as%20real-world%20applications%20in%20engineering%0Aand%20operations%20research.%20By%20uniting%20the%20dynamics%20of%20EAs%20with%20the%20structural%0Ainsights%20of%20GNNs%2C%20this%20work%20provides%20a%20foundation%20for%20interdisciplinary%0Ainnovation%2C%20paving%20the%20way%20for%20scalable%20and%20interpretable%20solutions%20to%20complex%0Aoptimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17629v2&entry.124074799=Read"},
{"title": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback", "author": "Jacob Fein-Ashley", "abstract": "  Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures.\n", "link": "http://arxiv.org/abs/2412.17737v2", "date": "2024-12-24", "relevancy": 2.3916, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback&body=Title%3A%20Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback%0AAuthor%3A%20Jacob%20Fein-Ashley%0AAbstract%3A%20%20%20Deep%20neural%20networks%20typically%20rely%20on%20a%20single%20forward%20pass%20for%20inference%2C%0Awhich%20can%20limit%20their%20capacity%20to%20resolve%20ambiguous%20inputs.%20We%20introduce%0AContextual%20Backpropagation%20Loops%20%28CBLs%29%20as%20an%20iterative%20mechanism%20that%0Aincorporates%20top-down%20feedback%20to%20refine%20intermediate%20representations%2C%20thereby%0Aimproving%20accuracy%20and%20robustness.%20This%20repeated%20process%20mirrors%20how%20humans%0Acontinuously%20re-interpret%20sensory%20information%20in%20daily%20life-by%20checking%20and%0Are-checking%20our%20perceptions%20using%20contextual%20cues.%20Our%20results%20suggest%20that%0ACBLs%20can%20offer%20a%20straightforward%20yet%20powerful%20way%20to%20incorporate%20such%0Acontextual%20reasoning%20in%20modern%20deep%20learning%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Backpropagation%2520Loops%253A%2520Amplifying%2520Deep%2520Reasoning%2520with%250A%2520%2520Iterative%2520Top-Down%2520Feedback%26entry.906535625%3DJacob%2520Fein-Ashley%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520typically%2520rely%2520on%2520a%2520single%2520forward%2520pass%2520for%2520inference%252C%250Awhich%2520can%2520limit%2520their%2520capacity%2520to%2520resolve%2520ambiguous%2520inputs.%2520We%2520introduce%250AContextual%2520Backpropagation%2520Loops%2520%2528CBLs%2529%2520as%2520an%2520iterative%2520mechanism%2520that%250Aincorporates%2520top-down%2520feedback%2520to%2520refine%2520intermediate%2520representations%252C%2520thereby%250Aimproving%2520accuracy%2520and%2520robustness.%2520This%2520repeated%2520process%2520mirrors%2520how%2520humans%250Acontinuously%2520re-interpret%2520sensory%2520information%2520in%2520daily%2520life-by%2520checking%2520and%250Are-checking%2520our%2520perceptions%2520using%2520contextual%2520cues.%2520Our%2520results%2520suggest%2520that%250ACBLs%2520can%2520offer%2520a%2520straightforward%2520yet%2520powerful%2520way%2520to%2520incorporate%2520such%250Acontextual%2520reasoning%2520in%2520modern%2520deep%2520learning%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback&entry.906535625=Jacob%20Fein-Ashley&entry.1292438233=%20%20Deep%20neural%20networks%20typically%20rely%20on%20a%20single%20forward%20pass%20for%20inference%2C%0Awhich%20can%20limit%20their%20capacity%20to%20resolve%20ambiguous%20inputs.%20We%20introduce%0AContextual%20Backpropagation%20Loops%20%28CBLs%29%20as%20an%20iterative%20mechanism%20that%0Aincorporates%20top-down%20feedback%20to%20refine%20intermediate%20representations%2C%20thereby%0Aimproving%20accuracy%20and%20robustness.%20This%20repeated%20process%20mirrors%20how%20humans%0Acontinuously%20re-interpret%20sensory%20information%20in%20daily%20life-by%20checking%20and%0Are-checking%20our%20perceptions%20using%20contextual%20cues.%20Our%20results%20suggest%20that%0ACBLs%20can%20offer%20a%20straightforward%20yet%20powerful%20way%20to%20incorporate%20such%0Acontextual%20reasoning%20in%20modern%20deep%20learning%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17737v2&entry.124074799=Read"},
{"title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph\n  Link Prediction", "author": "Zhiqiang Liu and Mingyang Chen and Yin Hua and Zhuo Chen and Ziqi Liu and Lei Liang and Huajun Chen and Wen Zhang", "abstract": "  Beyond-triple fact representations including hyper-relational facts with\nauxiliary key-value pairs, temporal facts with additional timestamps, and\nnested facts implying relationships between facts, are gaining significant\nattention. However, existing link prediction models are usually designed for\none specific type of facts, making it difficult to generalize to other fact\nrepresentations. To overcome this limitation, we propose a Unified Hierarchical\nRepresentation learning framework (UniHR) for unified knowledge graph link\nprediction. It consists of a unified Hierarchical Data Representation (HiDR)\nmodule and a unified Hierarchical Structure Learning (HiSL) module as graph\nencoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested\nfactual KGs into triple-based representations. Then HiSL incorporates\nintra-fact and inter-fact message passing, focusing on enhancing the semantic\ninformation within individual facts and enriching the structural information\nbetween facts. Experimental results across 7 datasets from 3 types of KGs\ndemonstrate that our UniHR outperforms baselines designed for one specific kind\nof KG, indicating strong generalization capability of HiDR form and the\neffectiveness of HiSL module. Code and data are available at\nhttps://github.com/Lza12a/UniHR.\n", "link": "http://arxiv.org/abs/2411.07019v2", "date": "2024-12-24", "relevancy": 2.3795, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction&body=Title%3A%20UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction%0AAuthor%3A%20Zhiqiang%20Liu%20and%20Mingyang%20Chen%20and%20Yin%20Hua%20and%20Zhuo%20Chen%20and%20Ziqi%20Liu%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang%0AAbstract%3A%20%20%20Beyond-triple%20fact%20representations%20including%20hyper-relational%20facts%20with%0Aauxiliary%20key-value%20pairs%2C%20temporal%20facts%20with%20additional%20timestamps%2C%20and%0Anested%20facts%20implying%20relationships%20between%20facts%2C%20are%20gaining%20significant%0Aattention.%20However%2C%20existing%20link%20prediction%20models%20are%20usually%20designed%20for%0Aone%20specific%20type%20of%20facts%2C%20making%20it%20difficult%20to%20generalize%20to%20other%20fact%0Arepresentations.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Unified%20Hierarchical%0ARepresentation%20learning%20framework%20%28UniHR%29%20for%20unified%20knowledge%20graph%20link%0Aprediction.%20It%20consists%20of%20a%20unified%20Hierarchical%20Data%20Representation%20%28HiDR%29%0Amodule%20and%20a%20unified%20Hierarchical%20Structure%20Learning%20%28HiSL%29%20module%20as%20graph%0Aencoder.%20The%20HiDR%20module%20unifies%20hyper-relational%20KGs%2C%20temporal%20KGs%2C%20and%20nested%0Afactual%20KGs%20into%20triple-based%20representations.%20Then%20HiSL%20incorporates%0Aintra-fact%20and%20inter-fact%20message%20passing%2C%20focusing%20on%20enhancing%20the%20semantic%0Ainformation%20within%20individual%20facts%20and%20enriching%20the%20structural%20information%0Abetween%20facts.%20Experimental%20results%20across%207%20datasets%20from%203%20types%20of%20KGs%0Ademonstrate%20that%20our%20UniHR%20outperforms%20baselines%20designed%20for%20one%20specific%20kind%0Aof%20KG%2C%20indicating%20strong%20generalization%20capability%20of%20HiDR%20form%20and%20the%0Aeffectiveness%20of%20HiSL%20module.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/UniHR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniHR%253A%2520Hierarchical%2520Representation%2520Learning%2520for%2520Unified%2520Knowledge%2520Graph%250A%2520%2520Link%2520Prediction%26entry.906535625%3DZhiqiang%2520Liu%2520and%2520Mingyang%2520Chen%2520and%2520Yin%2520Hua%2520and%2520Zhuo%2520Chen%2520and%2520Ziqi%2520Liu%2520and%2520Lei%2520Liang%2520and%2520Huajun%2520Chen%2520and%2520Wen%2520Zhang%26entry.1292438233%3D%2520%2520Beyond-triple%2520fact%2520representations%2520including%2520hyper-relational%2520facts%2520with%250Aauxiliary%2520key-value%2520pairs%252C%2520temporal%2520facts%2520with%2520additional%2520timestamps%252C%2520and%250Anested%2520facts%2520implying%2520relationships%2520between%2520facts%252C%2520are%2520gaining%2520significant%250Aattention.%2520However%252C%2520existing%2520link%2520prediction%2520models%2520are%2520usually%2520designed%2520for%250Aone%2520specific%2520type%2520of%2520facts%252C%2520making%2520it%2520difficult%2520to%2520generalize%2520to%2520other%2520fact%250Arepresentations.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Unified%2520Hierarchical%250ARepresentation%2520learning%2520framework%2520%2528UniHR%2529%2520for%2520unified%2520knowledge%2520graph%2520link%250Aprediction.%2520It%2520consists%2520of%2520a%2520unified%2520Hierarchical%2520Data%2520Representation%2520%2528HiDR%2529%250Amodule%2520and%2520a%2520unified%2520Hierarchical%2520Structure%2520Learning%2520%2528HiSL%2529%2520module%2520as%2520graph%250Aencoder.%2520The%2520HiDR%2520module%2520unifies%2520hyper-relational%2520KGs%252C%2520temporal%2520KGs%252C%2520and%2520nested%250Afactual%2520KGs%2520into%2520triple-based%2520representations.%2520Then%2520HiSL%2520incorporates%250Aintra-fact%2520and%2520inter-fact%2520message%2520passing%252C%2520focusing%2520on%2520enhancing%2520the%2520semantic%250Ainformation%2520within%2520individual%2520facts%2520and%2520enriching%2520the%2520structural%2520information%250Abetween%2520facts.%2520Experimental%2520results%2520across%25207%2520datasets%2520from%25203%2520types%2520of%2520KGs%250Ademonstrate%2520that%2520our%2520UniHR%2520outperforms%2520baselines%2520designed%2520for%2520one%2520specific%2520kind%250Aof%2520KG%252C%2520indicating%2520strong%2520generalization%2520capability%2520of%2520HiDR%2520form%2520and%2520the%250Aeffectiveness%2520of%2520HiSL%2520module.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Lza12a/UniHR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction&entry.906535625=Zhiqiang%20Liu%20and%20Mingyang%20Chen%20and%20Yin%20Hua%20and%20Zhuo%20Chen%20and%20Ziqi%20Liu%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang&entry.1292438233=%20%20Beyond-triple%20fact%20representations%20including%20hyper-relational%20facts%20with%0Aauxiliary%20key-value%20pairs%2C%20temporal%20facts%20with%20additional%20timestamps%2C%20and%0Anested%20facts%20implying%20relationships%20between%20facts%2C%20are%20gaining%20significant%0Aattention.%20However%2C%20existing%20link%20prediction%20models%20are%20usually%20designed%20for%0Aone%20specific%20type%20of%20facts%2C%20making%20it%20difficult%20to%20generalize%20to%20other%20fact%0Arepresentations.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Unified%20Hierarchical%0ARepresentation%20learning%20framework%20%28UniHR%29%20for%20unified%20knowledge%20graph%20link%0Aprediction.%20It%20consists%20of%20a%20unified%20Hierarchical%20Data%20Representation%20%28HiDR%29%0Amodule%20and%20a%20unified%20Hierarchical%20Structure%20Learning%20%28HiSL%29%20module%20as%20graph%0Aencoder.%20The%20HiDR%20module%20unifies%20hyper-relational%20KGs%2C%20temporal%20KGs%2C%20and%20nested%0Afactual%20KGs%20into%20triple-based%20representations.%20Then%20HiSL%20incorporates%0Aintra-fact%20and%20inter-fact%20message%20passing%2C%20focusing%20on%20enhancing%20the%20semantic%0Ainformation%20within%20individual%20facts%20and%20enriching%20the%20structural%20information%0Abetween%20facts.%20Experimental%20results%20across%207%20datasets%20from%203%20types%20of%20KGs%0Ademonstrate%20that%20our%20UniHR%20outperforms%20baselines%20designed%20for%20one%20specific%20kind%0Aof%20KG%2C%20indicating%20strong%20generalization%20capability%20of%20HiDR%20form%20and%20the%0Aeffectiveness%20of%20HiSL%20module.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/UniHR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07019v2&entry.124074799=Read"},
{"title": "MR-COGraphs: Communication-efficient Multi-Robot Open-vocabulary Mapping\n  System via 3D Scene Graphs", "author": "Qiuyi Gu and Zhaocheng Ye and Jincheng Yu and Jiahao Tang and Tinghao Yi and Yuhan Dong and Jian Wang and Jinqiang Cui and Xinlei Chen and Yu Wang", "abstract": "  Collaborative perception in unknown environments is crucial for multi-robot\nsystems. With the emergence of foundation models, robots can now not only\nperceive geometric information but also achieve open-vocabulary scene\nunderstanding. However, existing map representations that support\nopen-vocabulary queries often involve large data volumes, which becomes a\nbottleneck for multi-robot transmission in communication-limited environments.\nTo address this challenge, we develop a method to construct a graph-structured\n3D representation called COGraph, where nodes represent objects with semantic\nfeatures and edges capture their spatial relationships. Before transmission, a\ndata-driven feature encoder is applied to compress the feature dimensions of\nthe COGraph. Upon receiving COGraphs from other robots, the semantic features\nof each node are recovered using a decoder. We also propose a feature-based\napproach for place recognition and translation estimation, enabling the merging\nof local COGraphs into a unified global map. We validate our framework using\nsimulation environments built on Isaac Sim and real-world datasets. The results\ndemonstrate that, compared to transmitting semantic point clouds and\n512-dimensional COGraphs, our framework can reduce the data volume by two\norders of magnitude, without compromising mapping and query performance. For\nmore details, please visit our website at\nhttps://github.com/efc-robot/MR-COGraphs.\n", "link": "http://arxiv.org/abs/2412.18381v1", "date": "2024-12-24", "relevancy": 2.3794, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR-COGraphs%3A%20Communication-efficient%20Multi-Robot%20Open-vocabulary%20Mapping%0A%20%20System%20via%203D%20Scene%20Graphs&body=Title%3A%20MR-COGraphs%3A%20Communication-efficient%20Multi-Robot%20Open-vocabulary%20Mapping%0A%20%20System%20via%203D%20Scene%20Graphs%0AAuthor%3A%20Qiuyi%20Gu%20and%20Zhaocheng%20Ye%20and%20Jincheng%20Yu%20and%20Jiahao%20Tang%20and%20Tinghao%20Yi%20and%20Yuhan%20Dong%20and%20Jian%20Wang%20and%20Jinqiang%20Cui%20and%20Xinlei%20Chen%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Collaborative%20perception%20in%20unknown%20environments%20is%20crucial%20for%20multi-robot%0Asystems.%20With%20the%20emergence%20of%20foundation%20models%2C%20robots%20can%20now%20not%20only%0Aperceive%20geometric%20information%20but%20also%20achieve%20open-vocabulary%20scene%0Aunderstanding.%20However%2C%20existing%20map%20representations%20that%20support%0Aopen-vocabulary%20queries%20often%20involve%20large%20data%20volumes%2C%20which%20becomes%20a%0Abottleneck%20for%20multi-robot%20transmission%20in%20communication-limited%20environments.%0ATo%20address%20this%20challenge%2C%20we%20develop%20a%20method%20to%20construct%20a%20graph-structured%0A3D%20representation%20called%20COGraph%2C%20where%20nodes%20represent%20objects%20with%20semantic%0Afeatures%20and%20edges%20capture%20their%20spatial%20relationships.%20Before%20transmission%2C%20a%0Adata-driven%20feature%20encoder%20is%20applied%20to%20compress%20the%20feature%20dimensions%20of%0Athe%20COGraph.%20Upon%20receiving%20COGraphs%20from%20other%20robots%2C%20the%20semantic%20features%0Aof%20each%20node%20are%20recovered%20using%20a%20decoder.%20We%20also%20propose%20a%20feature-based%0Aapproach%20for%20place%20recognition%20and%20translation%20estimation%2C%20enabling%20the%20merging%0Aof%20local%20COGraphs%20into%20a%20unified%20global%20map.%20We%20validate%20our%20framework%20using%0Asimulation%20environments%20built%20on%20Isaac%20Sim%20and%20real-world%20datasets.%20The%20results%0Ademonstrate%20that%2C%20compared%20to%20transmitting%20semantic%20point%20clouds%20and%0A512-dimensional%20COGraphs%2C%20our%20framework%20can%20reduce%20the%20data%20volume%20by%20two%0Aorders%20of%20magnitude%2C%20without%20compromising%20mapping%20and%20query%20performance.%20For%0Amore%20details%2C%20please%20visit%20our%20website%20at%0Ahttps%3A//github.com/efc-robot/MR-COGraphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR-COGraphs%253A%2520Communication-efficient%2520Multi-Robot%2520Open-vocabulary%2520Mapping%250A%2520%2520System%2520via%25203D%2520Scene%2520Graphs%26entry.906535625%3DQiuyi%2520Gu%2520and%2520Zhaocheng%2520Ye%2520and%2520Jincheng%2520Yu%2520and%2520Jiahao%2520Tang%2520and%2520Tinghao%2520Yi%2520and%2520Yuhan%2520Dong%2520and%2520Jian%2520Wang%2520and%2520Jinqiang%2520Cui%2520and%2520Xinlei%2520Chen%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520in%2520unknown%2520environments%2520is%2520crucial%2520for%2520multi-robot%250Asystems.%2520With%2520the%2520emergence%2520of%2520foundation%2520models%252C%2520robots%2520can%2520now%2520not%2520only%250Aperceive%2520geometric%2520information%2520but%2520also%2520achieve%2520open-vocabulary%2520scene%250Aunderstanding.%2520However%252C%2520existing%2520map%2520representations%2520that%2520support%250Aopen-vocabulary%2520queries%2520often%2520involve%2520large%2520data%2520volumes%252C%2520which%2520becomes%2520a%250Abottleneck%2520for%2520multi-robot%2520transmission%2520in%2520communication-limited%2520environments.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520develop%2520a%2520method%2520to%2520construct%2520a%2520graph-structured%250A3D%2520representation%2520called%2520COGraph%252C%2520where%2520nodes%2520represent%2520objects%2520with%2520semantic%250Afeatures%2520and%2520edges%2520capture%2520their%2520spatial%2520relationships.%2520Before%2520transmission%252C%2520a%250Adata-driven%2520feature%2520encoder%2520is%2520applied%2520to%2520compress%2520the%2520feature%2520dimensions%2520of%250Athe%2520COGraph.%2520Upon%2520receiving%2520COGraphs%2520from%2520other%2520robots%252C%2520the%2520semantic%2520features%250Aof%2520each%2520node%2520are%2520recovered%2520using%2520a%2520decoder.%2520We%2520also%2520propose%2520a%2520feature-based%250Aapproach%2520for%2520place%2520recognition%2520and%2520translation%2520estimation%252C%2520enabling%2520the%2520merging%250Aof%2520local%2520COGraphs%2520into%2520a%2520unified%2520global%2520map.%2520We%2520validate%2520our%2520framework%2520using%250Asimulation%2520environments%2520built%2520on%2520Isaac%2520Sim%2520and%2520real-world%2520datasets.%2520The%2520results%250Ademonstrate%2520that%252C%2520compared%2520to%2520transmitting%2520semantic%2520point%2520clouds%2520and%250A512-dimensional%2520COGraphs%252C%2520our%2520framework%2520can%2520reduce%2520the%2520data%2520volume%2520by%2520two%250Aorders%2520of%2520magnitude%252C%2520without%2520compromising%2520mapping%2520and%2520query%2520performance.%2520For%250Amore%2520details%252C%2520please%2520visit%2520our%2520website%2520at%250Ahttps%253A//github.com/efc-robot/MR-COGraphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR-COGraphs%3A%20Communication-efficient%20Multi-Robot%20Open-vocabulary%20Mapping%0A%20%20System%20via%203D%20Scene%20Graphs&entry.906535625=Qiuyi%20Gu%20and%20Zhaocheng%20Ye%20and%20Jincheng%20Yu%20and%20Jiahao%20Tang%20and%20Tinghao%20Yi%20and%20Yuhan%20Dong%20and%20Jian%20Wang%20and%20Jinqiang%20Cui%20and%20Xinlei%20Chen%20and%20Yu%20Wang&entry.1292438233=%20%20Collaborative%20perception%20in%20unknown%20environments%20is%20crucial%20for%20multi-robot%0Asystems.%20With%20the%20emergence%20of%20foundation%20models%2C%20robots%20can%20now%20not%20only%0Aperceive%20geometric%20information%20but%20also%20achieve%20open-vocabulary%20scene%0Aunderstanding.%20However%2C%20existing%20map%20representations%20that%20support%0Aopen-vocabulary%20queries%20often%20involve%20large%20data%20volumes%2C%20which%20becomes%20a%0Abottleneck%20for%20multi-robot%20transmission%20in%20communication-limited%20environments.%0ATo%20address%20this%20challenge%2C%20we%20develop%20a%20method%20to%20construct%20a%20graph-structured%0A3D%20representation%20called%20COGraph%2C%20where%20nodes%20represent%20objects%20with%20semantic%0Afeatures%20and%20edges%20capture%20their%20spatial%20relationships.%20Before%20transmission%2C%20a%0Adata-driven%20feature%20encoder%20is%20applied%20to%20compress%20the%20feature%20dimensions%20of%0Athe%20COGraph.%20Upon%20receiving%20COGraphs%20from%20other%20robots%2C%20the%20semantic%20features%0Aof%20each%20node%20are%20recovered%20using%20a%20decoder.%20We%20also%20propose%20a%20feature-based%0Aapproach%20for%20place%20recognition%20and%20translation%20estimation%2C%20enabling%20the%20merging%0Aof%20local%20COGraphs%20into%20a%20unified%20global%20map.%20We%20validate%20our%20framework%20using%0Asimulation%20environments%20built%20on%20Isaac%20Sim%20and%20real-world%20datasets.%20The%20results%0Ademonstrate%20that%2C%20compared%20to%20transmitting%20semantic%20point%20clouds%20and%0A512-dimensional%20COGraphs%2C%20our%20framework%20can%20reduce%20the%20data%20volume%20by%20two%0Aorders%20of%20magnitude%2C%20without%20compromising%20mapping%20and%20query%20performance.%20For%0Amore%20details%2C%20please%20visit%20our%20website%20at%0Ahttps%3A//github.com/efc-robot/MR-COGraphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18381v1&entry.124074799=Read"},
{"title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English", "author": "Avinash Anand and Kritarth Prasad and Chhavi Kirtani and Ashwin R Nair and Manvendra Kumar Nema and Raj Jaiswal and Rajiv Ratn Shah", "abstract": "  Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs.\n", "link": "http://arxiv.org/abs/2412.18415v1", "date": "2024-12-24", "relevancy": 2.3614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Mathematical%20Reasoning%3A%20Advancing%20Open-Source%20LLMs%20in%20Hindi%0A%20%20and%20English&body=Title%3A%20Multilingual%20Mathematical%20Reasoning%3A%20Advancing%20Open-Source%20LLMs%20in%20Hindi%0A%20%20and%20English%0AAuthor%3A%20Avinash%20Anand%20and%20Kritarth%20Prasad%20and%20Chhavi%20Kirtani%20and%20Ashwin%20R%20Nair%20and%20Manvendra%20Kumar%20Nema%20and%20Raj%20Jaiswal%20and%20Rajiv%20Ratn%20Shah%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20linguistic%20tasks%20but%20struggle%20with%0Amathematical%20reasoning%2C%20particularly%20in%20non%20English%20languages%20like%20Hindi.%20This%0Aresearch%20aims%20to%20enhance%20the%20mathematical%20reasoning%20skills%20of%20smaller%2C%20resource%0Aefficient%20open-source%20LLMs%20in%20both%20Hindi%20and%20English.%20We%20evaluate%20models%20like%0AOpenHathi%207B%2C%20LLaMA-2%207B%2C%20WizardMath%207B%2C%20Mistral%207B%2C%20LLeMMa%207B%2C%20MAmmoTH%207B%2C%0AGemini%20Pro%2C%20and%20GPT-4%20using%20zero-shot%2C%20few-shot%20chain-of-thought%20%28CoT%29%20methods%2C%0Aand%20supervised%20fine-tuning.%20Our%20approach%20incorporates%20curriculum%20learning%2C%0Aprogressively%20training%20models%20on%20increasingly%20difficult%20problems%2C%20a%20novel%0ADecomposition%20Strategy%20to%20simplify%20complex%20arithmetic%20operations%2C%20and%20a%0AStructured%20Solution%20Design%20that%20divides%20solutions%20into%20phases.%20Our%20experiments%0Aresult%20in%20notable%20performance%20enhancements.%20WizardMath%207B%20exceeds%20Gemini%27s%0Aaccuracy%20on%20English%20datasets%20by%20%2B6%25%20and%20matches%20Gemini%27s%20performance%20on%20Hindi%0Adatasets.%20Adopting%20a%20bilingual%20approach%20that%20combines%20English%20and%20Hindi%20samples%0Aachieves%20results%20comparable%20to%20individual%20language%20models%2C%20demonstrating%20the%0Acapability%20to%20learn%20mathematical%20reasoning%20in%20both%20languages.%20This%20research%0Ahighlights%20the%20potential%20for%20improving%20mathematical%20reasoning%20in%20open-source%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Mathematical%2520Reasoning%253A%2520Advancing%2520Open-Source%2520LLMs%2520in%2520Hindi%250A%2520%2520and%2520English%26entry.906535625%3DAvinash%2520Anand%2520and%2520Kritarth%2520Prasad%2520and%2520Chhavi%2520Kirtani%2520and%2520Ashwin%2520R%2520Nair%2520and%2520Manvendra%2520Kumar%2520Nema%2520and%2520Raj%2520Jaiswal%2520and%2520Rajiv%2520Ratn%2520Shah%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520linguistic%2520tasks%2520but%2520struggle%2520with%250Amathematical%2520reasoning%252C%2520particularly%2520in%2520non%2520English%2520languages%2520like%2520Hindi.%2520This%250Aresearch%2520aims%2520to%2520enhance%2520the%2520mathematical%2520reasoning%2520skills%2520of%2520smaller%252C%2520resource%250Aefficient%2520open-source%2520LLMs%2520in%2520both%2520Hindi%2520and%2520English.%2520We%2520evaluate%2520models%2520like%250AOpenHathi%25207B%252C%2520LLaMA-2%25207B%252C%2520WizardMath%25207B%252C%2520Mistral%25207B%252C%2520LLeMMa%25207B%252C%2520MAmmoTH%25207B%252C%250AGemini%2520Pro%252C%2520and%2520GPT-4%2520using%2520zero-shot%252C%2520few-shot%2520chain-of-thought%2520%2528CoT%2529%2520methods%252C%250Aand%2520supervised%2520fine-tuning.%2520Our%2520approach%2520incorporates%2520curriculum%2520learning%252C%250Aprogressively%2520training%2520models%2520on%2520increasingly%2520difficult%2520problems%252C%2520a%2520novel%250ADecomposition%2520Strategy%2520to%2520simplify%2520complex%2520arithmetic%2520operations%252C%2520and%2520a%250AStructured%2520Solution%2520Design%2520that%2520divides%2520solutions%2520into%2520phases.%2520Our%2520experiments%250Aresult%2520in%2520notable%2520performance%2520enhancements.%2520WizardMath%25207B%2520exceeds%2520Gemini%2527s%250Aaccuracy%2520on%2520English%2520datasets%2520by%2520%252B6%2525%2520and%2520matches%2520Gemini%2527s%2520performance%2520on%2520Hindi%250Adatasets.%2520Adopting%2520a%2520bilingual%2520approach%2520that%2520combines%2520English%2520and%2520Hindi%2520samples%250Aachieves%2520results%2520comparable%2520to%2520individual%2520language%2520models%252C%2520demonstrating%2520the%250Acapability%2520to%2520learn%2520mathematical%2520reasoning%2520in%2520both%2520languages.%2520This%2520research%250Ahighlights%2520the%2520potential%2520for%2520improving%2520mathematical%2520reasoning%2520in%2520open-source%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Mathematical%20Reasoning%3A%20Advancing%20Open-Source%20LLMs%20in%20Hindi%0A%20%20and%20English&entry.906535625=Avinash%20Anand%20and%20Kritarth%20Prasad%20and%20Chhavi%20Kirtani%20and%20Ashwin%20R%20Nair%20and%20Manvendra%20Kumar%20Nema%20and%20Raj%20Jaiswal%20and%20Rajiv%20Ratn%20Shah&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20linguistic%20tasks%20but%20struggle%20with%0Amathematical%20reasoning%2C%20particularly%20in%20non%20English%20languages%20like%20Hindi.%20This%0Aresearch%20aims%20to%20enhance%20the%20mathematical%20reasoning%20skills%20of%20smaller%2C%20resource%0Aefficient%20open-source%20LLMs%20in%20both%20Hindi%20and%20English.%20We%20evaluate%20models%20like%0AOpenHathi%207B%2C%20LLaMA-2%207B%2C%20WizardMath%207B%2C%20Mistral%207B%2C%20LLeMMa%207B%2C%20MAmmoTH%207B%2C%0AGemini%20Pro%2C%20and%20GPT-4%20using%20zero-shot%2C%20few-shot%20chain-of-thought%20%28CoT%29%20methods%2C%0Aand%20supervised%20fine-tuning.%20Our%20approach%20incorporates%20curriculum%20learning%2C%0Aprogressively%20training%20models%20on%20increasingly%20difficult%20problems%2C%20a%20novel%0ADecomposition%20Strategy%20to%20simplify%20complex%20arithmetic%20operations%2C%20and%20a%0AStructured%20Solution%20Design%20that%20divides%20solutions%20into%20phases.%20Our%20experiments%0Aresult%20in%20notable%20performance%20enhancements.%20WizardMath%207B%20exceeds%20Gemini%27s%0Aaccuracy%20on%20English%20datasets%20by%20%2B6%25%20and%20matches%20Gemini%27s%20performance%20on%20Hindi%0Adatasets.%20Adopting%20a%20bilingual%20approach%20that%20combines%20English%20and%20Hindi%20samples%0Aachieves%20results%20comparable%20to%20individual%20language%20models%2C%20demonstrating%20the%0Acapability%20to%20learn%20mathematical%20reasoning%20in%20both%20languages.%20This%20research%0Ahighlights%20the%20potential%20for%20improving%20mathematical%20reasoning%20in%20open-source%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18415v1&entry.124074799=Read"},
{"title": "Memory-Efficient Gradient Unrolling for Large-Scale Bi-level\n  Optimization", "author": "Qianli Shen and Yezhen Wang and Zhouhao Yang and Xiang Li and Haonan Wang and Yang Zhang and Jonathan Scarlett and Zhanxing Zhu and Kenji Kawaguchi", "abstract": "  Bi-level optimization (BO) has become a fundamental mathematical framework\nfor addressing hierarchical machine learning problems. As deep learning models\ncontinue to grow in size, the demand for scalable bi-level optimization\nsolutions has become increasingly critical. Traditional gradient-based bi-level\noptimization algorithms, due to their inherent characteristics, are ill-suited\nto meet the demands of large-scale applications. In this paper, we introduce\n$\\textbf{F}$orward $\\textbf{G}$radient $\\textbf{U}$nrolling with\n$\\textbf{F}$orward $\\textbf{F}$radient, abbreviated as\n$(\\textbf{FG})^2\\textbf{U}$, which achieves an unbiased stochastic\napproximation of the meta gradient for bi-level optimization.\n$(\\text{FG})^2\\text{U}$ circumvents the memory and approximation issues\nassociated with classical bi-level optimization approaches, and delivers\nsignificantly more accurate gradient estimates than existing large-scale\nbi-level optimization approaches. Additionally, $(\\text{FG})^2\\text{U}$ is\ninherently designed to support parallel computing, enabling it to effectively\nleverage large-scale distributed computing systems to achieve significant\ncomputational efficiency. In practice, $(\\text{FG})^2\\text{U}$ and other\nmethods can be strategically placed at different stages of the training process\nto achieve a more cost-effective two-phase paradigm. Further,\n$(\\text{FG})^2\\text{U}$ is easy to implement within popular deep learning\nframeworks, and can be conveniently adapted to address more challenging\nzeroth-order bi-level optimization scenarios. We provide a thorough convergence\nanalysis and a comprehensive practical discussion for $(\\text{FG})^2\\text{U}$,\ncomplemented by extensive empirical evaluations, showcasing its superior\nperformance in diverse large-scale bi-level optimization tasks. Code is\navailable at https://github.com/ShenQianli/FG2U.\n", "link": "http://arxiv.org/abs/2406.14095v2", "date": "2024-12-24", "relevancy": 2.3507, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4904}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4623}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%20Gradient%20Unrolling%20for%20Large-Scale%20Bi-level%0A%20%20Optimization&body=Title%3A%20Memory-Efficient%20Gradient%20Unrolling%20for%20Large-Scale%20Bi-level%0A%20%20Optimization%0AAuthor%3A%20Qianli%20Shen%20and%20Yezhen%20Wang%20and%20Zhouhao%20Yang%20and%20Xiang%20Li%20and%20Haonan%20Wang%20and%20Yang%20Zhang%20and%20Jonathan%20Scarlett%20and%20Zhanxing%20Zhu%20and%20Kenji%20Kawaguchi%0AAbstract%3A%20%20%20Bi-level%20optimization%20%28BO%29%20has%20become%20a%20fundamental%20mathematical%20framework%0Afor%20addressing%20hierarchical%20machine%20learning%20problems.%20As%20deep%20learning%20models%0Acontinue%20to%20grow%20in%20size%2C%20the%20demand%20for%20scalable%20bi-level%20optimization%0Asolutions%20has%20become%20increasingly%20critical.%20Traditional%20gradient-based%20bi-level%0Aoptimization%20algorithms%2C%20due%20to%20their%20inherent%20characteristics%2C%20are%20ill-suited%0Ato%20meet%20the%20demands%20of%20large-scale%20applications.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctextbf%7BF%7D%24orward%20%24%5Ctextbf%7BG%7D%24radient%20%24%5Ctextbf%7BU%7D%24nrolling%20with%0A%24%5Ctextbf%7BF%7D%24orward%20%24%5Ctextbf%7BF%7D%24radient%2C%20abbreviated%20as%0A%24%28%5Ctextbf%7BFG%7D%29%5E2%5Ctextbf%7BU%7D%24%2C%20which%20achieves%20an%20unbiased%20stochastic%0Aapproximation%20of%20the%20meta%20gradient%20for%20bi-level%20optimization.%0A%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20circumvents%20the%20memory%20and%20approximation%20issues%0Aassociated%20with%20classical%20bi-level%20optimization%20approaches%2C%20and%20delivers%0Asignificantly%20more%20accurate%20gradient%20estimates%20than%20existing%20large-scale%0Abi-level%20optimization%20approaches.%20Additionally%2C%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20is%0Ainherently%20designed%20to%20support%20parallel%20computing%2C%20enabling%20it%20to%20effectively%0Aleverage%20large-scale%20distributed%20computing%20systems%20to%20achieve%20significant%0Acomputational%20efficiency.%20In%20practice%2C%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20and%20other%0Amethods%20can%20be%20strategically%20placed%20at%20different%20stages%20of%20the%20training%20process%0Ato%20achieve%20a%20more%20cost-effective%20two-phase%20paradigm.%20Further%2C%0A%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20is%20easy%20to%20implement%20within%20popular%20deep%20learning%0Aframeworks%2C%20and%20can%20be%20conveniently%20adapted%20to%20address%20more%20challenging%0Azeroth-order%20bi-level%20optimization%20scenarios.%20We%20provide%20a%20thorough%20convergence%0Aanalysis%20and%20a%20comprehensive%20practical%20discussion%20for%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%2C%0Acomplemented%20by%20extensive%20empirical%20evaluations%2C%20showcasing%20its%20superior%0Aperformance%20in%20diverse%20large-scale%20bi-level%20optimization%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ShenQianli/FG2U.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%2520Gradient%2520Unrolling%2520for%2520Large-Scale%2520Bi-level%250A%2520%2520Optimization%26entry.906535625%3DQianli%2520Shen%2520and%2520Yezhen%2520Wang%2520and%2520Zhouhao%2520Yang%2520and%2520Xiang%2520Li%2520and%2520Haonan%2520Wang%2520and%2520Yang%2520Zhang%2520and%2520Jonathan%2520Scarlett%2520and%2520Zhanxing%2520Zhu%2520and%2520Kenji%2520Kawaguchi%26entry.1292438233%3D%2520%2520Bi-level%2520optimization%2520%2528BO%2529%2520has%2520become%2520a%2520fundamental%2520mathematical%2520framework%250Afor%2520addressing%2520hierarchical%2520machine%2520learning%2520problems.%2520As%2520deep%2520learning%2520models%250Acontinue%2520to%2520grow%2520in%2520size%252C%2520the%2520demand%2520for%2520scalable%2520bi-level%2520optimization%250Asolutions%2520has%2520become%2520increasingly%2520critical.%2520Traditional%2520gradient-based%2520bi-level%250Aoptimization%2520algorithms%252C%2520due%2520to%2520their%2520inherent%2520characteristics%252C%2520are%2520ill-suited%250Ato%2520meet%2520the%2520demands%2520of%2520large-scale%2520applications.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A%2524%255Ctextbf%257BF%257D%2524orward%2520%2524%255Ctextbf%257BG%257D%2524radient%2520%2524%255Ctextbf%257BU%257D%2524nrolling%2520with%250A%2524%255Ctextbf%257BF%257D%2524orward%2520%2524%255Ctextbf%257BF%257D%2524radient%252C%2520abbreviated%2520as%250A%2524%2528%255Ctextbf%257BFG%257D%2529%255E2%255Ctextbf%257BU%257D%2524%252C%2520which%2520achieves%2520an%2520unbiased%2520stochastic%250Aapproximation%2520of%2520the%2520meta%2520gradient%2520for%2520bi-level%2520optimization.%250A%2524%2528%255Ctext%257BFG%257D%2529%255E2%255Ctext%257BU%257D%2524%2520circumvents%2520the%2520memory%2520and%2520approximation%2520issues%250Aassociated%2520with%2520classical%2520bi-level%2520optimization%2520approaches%252C%2520and%2520delivers%250Asignificantly%2520more%2520accurate%2520gradient%2520estimates%2520than%2520existing%2520large-scale%250Abi-level%2520optimization%2520approaches.%2520Additionally%252C%2520%2524%2528%255Ctext%257BFG%257D%2529%255E2%255Ctext%257BU%257D%2524%2520is%250Ainherently%2520designed%2520to%2520support%2520parallel%2520computing%252C%2520enabling%2520it%2520to%2520effectively%250Aleverage%2520large-scale%2520distributed%2520computing%2520systems%2520to%2520achieve%2520significant%250Acomputational%2520efficiency.%2520In%2520practice%252C%2520%2524%2528%255Ctext%257BFG%257D%2529%255E2%255Ctext%257BU%257D%2524%2520and%2520other%250Amethods%2520can%2520be%2520strategically%2520placed%2520at%2520different%2520stages%2520of%2520the%2520training%2520process%250Ato%2520achieve%2520a%2520more%2520cost-effective%2520two-phase%2520paradigm.%2520Further%252C%250A%2524%2528%255Ctext%257BFG%257D%2529%255E2%255Ctext%257BU%257D%2524%2520is%2520easy%2520to%2520implement%2520within%2520popular%2520deep%2520learning%250Aframeworks%252C%2520and%2520can%2520be%2520conveniently%2520adapted%2520to%2520address%2520more%2520challenging%250Azeroth-order%2520bi-level%2520optimization%2520scenarios.%2520We%2520provide%2520a%2520thorough%2520convergence%250Aanalysis%2520and%2520a%2520comprehensive%2520practical%2520discussion%2520for%2520%2524%2528%255Ctext%257BFG%257D%2529%255E2%255Ctext%257BU%257D%2524%252C%250Acomplemented%2520by%2520extensive%2520empirical%2520evaluations%252C%2520showcasing%2520its%2520superior%250Aperformance%2520in%2520diverse%2520large-scale%2520bi-level%2520optimization%2520tasks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ShenQianli/FG2U.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%20Gradient%20Unrolling%20for%20Large-Scale%20Bi-level%0A%20%20Optimization&entry.906535625=Qianli%20Shen%20and%20Yezhen%20Wang%20and%20Zhouhao%20Yang%20and%20Xiang%20Li%20and%20Haonan%20Wang%20and%20Yang%20Zhang%20and%20Jonathan%20Scarlett%20and%20Zhanxing%20Zhu%20and%20Kenji%20Kawaguchi&entry.1292438233=%20%20Bi-level%20optimization%20%28BO%29%20has%20become%20a%20fundamental%20mathematical%20framework%0Afor%20addressing%20hierarchical%20machine%20learning%20problems.%20As%20deep%20learning%20models%0Acontinue%20to%20grow%20in%20size%2C%20the%20demand%20for%20scalable%20bi-level%20optimization%0Asolutions%20has%20become%20increasingly%20critical.%20Traditional%20gradient-based%20bi-level%0Aoptimization%20algorithms%2C%20due%20to%20their%20inherent%20characteristics%2C%20are%20ill-suited%0Ato%20meet%20the%20demands%20of%20large-scale%20applications.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctextbf%7BF%7D%24orward%20%24%5Ctextbf%7BG%7D%24radient%20%24%5Ctextbf%7BU%7D%24nrolling%20with%0A%24%5Ctextbf%7BF%7D%24orward%20%24%5Ctextbf%7BF%7D%24radient%2C%20abbreviated%20as%0A%24%28%5Ctextbf%7BFG%7D%29%5E2%5Ctextbf%7BU%7D%24%2C%20which%20achieves%20an%20unbiased%20stochastic%0Aapproximation%20of%20the%20meta%20gradient%20for%20bi-level%20optimization.%0A%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20circumvents%20the%20memory%20and%20approximation%20issues%0Aassociated%20with%20classical%20bi-level%20optimization%20approaches%2C%20and%20delivers%0Asignificantly%20more%20accurate%20gradient%20estimates%20than%20existing%20large-scale%0Abi-level%20optimization%20approaches.%20Additionally%2C%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20is%0Ainherently%20designed%20to%20support%20parallel%20computing%2C%20enabling%20it%20to%20effectively%0Aleverage%20large-scale%20distributed%20computing%20systems%20to%20achieve%20significant%0Acomputational%20efficiency.%20In%20practice%2C%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20and%20other%0Amethods%20can%20be%20strategically%20placed%20at%20different%20stages%20of%20the%20training%20process%0Ato%20achieve%20a%20more%20cost-effective%20two-phase%20paradigm.%20Further%2C%0A%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%20is%20easy%20to%20implement%20within%20popular%20deep%20learning%0Aframeworks%2C%20and%20can%20be%20conveniently%20adapted%20to%20address%20more%20challenging%0Azeroth-order%20bi-level%20optimization%20scenarios.%20We%20provide%20a%20thorough%20convergence%0Aanalysis%20and%20a%20comprehensive%20practical%20discussion%20for%20%24%28%5Ctext%7BFG%7D%29%5E2%5Ctext%7BU%7D%24%2C%0Acomplemented%20by%20extensive%20empirical%20evaluations%2C%20showcasing%20its%20superior%0Aperformance%20in%20diverse%20large-scale%20bi-level%20optimization%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ShenQianli/FG2U.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14095v2&entry.124074799=Read"},
{"title": "Prompted Contextual Vectors for Spear-Phishing Detection", "author": "Daniel Nahmias and Gal Engelberg and Dan Klein and Asaf Shabtai", "abstract": "  Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.\n", "link": "http://arxiv.org/abs/2402.08309v3", "date": "2024-12-24", "relevancy": 2.3295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompted%20Contextual%20Vectors%20for%20Spear-Phishing%20Detection&body=Title%3A%20Prompted%20Contextual%20Vectors%20for%20Spear-Phishing%20Detection%0AAuthor%3A%20Daniel%20Nahmias%20and%20Gal%20Engelberg%20and%20Dan%20Klein%20and%20Asaf%20Shabtai%0AAbstract%3A%20%20%20Spear-phishing%20attacks%20present%20a%20significant%20security%20challenge%2C%20with%20large%0Alanguage%20models%20%28LLMs%29%20escalating%20the%20threat%20by%20generating%20convincing%20emails%0Aand%20facilitating%20target%20reconnaissance.%20To%20address%20this%2C%20we%20propose%20a%20detection%0Aapproach%20based%20on%20a%20novel%20document%20vectorization%20method%20that%20utilizes%20an%0Aensemble%20of%20LLMs%20to%20create%20representation%20vectors.%20By%20prompting%20LLMs%20to%20reason%0Aand%20respond%20to%20human-crafted%20questions%2C%20we%20quantify%20the%20presence%20of%20common%0Apersuasion%20principles%20in%20the%20email%27s%20content%2C%20producing%20prompted%20contextual%0Adocument%20vectors%20for%20a%20downstream%20supervised%20machine%20learning%20model.%20We%0Aevaluate%20our%20method%20using%20a%20unique%20dataset%20generated%20by%20a%20proprietary%20system%0Athat%20automates%20target%20reconnaissance%20and%20spear-phishing%20email%20creation.%20Our%0Amethod%20achieves%20a%2091%5C%25%20F1%20score%20in%20identifying%20LLM-generated%20spear-phishing%0Aemails%2C%20with%20the%20training%20set%20comprising%20only%20traditional%20phishing%20and%20benign%0Aemails.%20Key%20contributions%20include%20a%20novel%20document%20vectorization%20method%0Autilizing%20LLM%20reasoning%2C%20a%20publicly%20available%20dataset%20of%20high-quality%0Aspear-phishing%20emails%2C%20and%20the%20demonstrated%20effectiveness%20of%20our%20method%20in%0Adetecting%20such%20emails.%20This%20methodology%20can%20be%20utilized%20for%20various%20document%0Aclassification%20tasks%2C%20particularly%20in%20adversarial%20problem%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08309v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompted%2520Contextual%2520Vectors%2520for%2520Spear-Phishing%2520Detection%26entry.906535625%3DDaniel%2520Nahmias%2520and%2520Gal%2520Engelberg%2520and%2520Dan%2520Klein%2520and%2520Asaf%2520Shabtai%26entry.1292438233%3D%2520%2520Spear-phishing%2520attacks%2520present%2520a%2520significant%2520security%2520challenge%252C%2520with%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520escalating%2520the%2520threat%2520by%2520generating%2520convincing%2520emails%250Aand%2520facilitating%2520target%2520reconnaissance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520detection%250Aapproach%2520based%2520on%2520a%2520novel%2520document%2520vectorization%2520method%2520that%2520utilizes%2520an%250Aensemble%2520of%2520LLMs%2520to%2520create%2520representation%2520vectors.%2520By%2520prompting%2520LLMs%2520to%2520reason%250Aand%2520respond%2520to%2520human-crafted%2520questions%252C%2520we%2520quantify%2520the%2520presence%2520of%2520common%250Apersuasion%2520principles%2520in%2520the%2520email%2527s%2520content%252C%2520producing%2520prompted%2520contextual%250Adocument%2520vectors%2520for%2520a%2520downstream%2520supervised%2520machine%2520learning%2520model.%2520We%250Aevaluate%2520our%2520method%2520using%2520a%2520unique%2520dataset%2520generated%2520by%2520a%2520proprietary%2520system%250Athat%2520automates%2520target%2520reconnaissance%2520and%2520spear-phishing%2520email%2520creation.%2520Our%250Amethod%2520achieves%2520a%252091%255C%2525%2520F1%2520score%2520in%2520identifying%2520LLM-generated%2520spear-phishing%250Aemails%252C%2520with%2520the%2520training%2520set%2520comprising%2520only%2520traditional%2520phishing%2520and%2520benign%250Aemails.%2520Key%2520contributions%2520include%2520a%2520novel%2520document%2520vectorization%2520method%250Autilizing%2520LLM%2520reasoning%252C%2520a%2520publicly%2520available%2520dataset%2520of%2520high-quality%250Aspear-phishing%2520emails%252C%2520and%2520the%2520demonstrated%2520effectiveness%2520of%2520our%2520method%2520in%250Adetecting%2520such%2520emails.%2520This%2520methodology%2520can%2520be%2520utilized%2520for%2520various%2520document%250Aclassification%2520tasks%252C%2520particularly%2520in%2520adversarial%2520problem%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08309v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompted%20Contextual%20Vectors%20for%20Spear-Phishing%20Detection&entry.906535625=Daniel%20Nahmias%20and%20Gal%20Engelberg%20and%20Dan%20Klein%20and%20Asaf%20Shabtai&entry.1292438233=%20%20Spear-phishing%20attacks%20present%20a%20significant%20security%20challenge%2C%20with%20large%0Alanguage%20models%20%28LLMs%29%20escalating%20the%20threat%20by%20generating%20convincing%20emails%0Aand%20facilitating%20target%20reconnaissance.%20To%20address%20this%2C%20we%20propose%20a%20detection%0Aapproach%20based%20on%20a%20novel%20document%20vectorization%20method%20that%20utilizes%20an%0Aensemble%20of%20LLMs%20to%20create%20representation%20vectors.%20By%20prompting%20LLMs%20to%20reason%0Aand%20respond%20to%20human-crafted%20questions%2C%20we%20quantify%20the%20presence%20of%20common%0Apersuasion%20principles%20in%20the%20email%27s%20content%2C%20producing%20prompted%20contextual%0Adocument%20vectors%20for%20a%20downstream%20supervised%20machine%20learning%20model.%20We%0Aevaluate%20our%20method%20using%20a%20unique%20dataset%20generated%20by%20a%20proprietary%20system%0Athat%20automates%20target%20reconnaissance%20and%20spear-phishing%20email%20creation.%20Our%0Amethod%20achieves%20a%2091%5C%25%20F1%20score%20in%20identifying%20LLM-generated%20spear-phishing%0Aemails%2C%20with%20the%20training%20set%20comprising%20only%20traditional%20phishing%20and%20benign%0Aemails.%20Key%20contributions%20include%20a%20novel%20document%20vectorization%20method%0Autilizing%20LLM%20reasoning%2C%20a%20publicly%20available%20dataset%20of%20high-quality%0Aspear-phishing%20emails%2C%20and%20the%20demonstrated%20effectiveness%20of%20our%20method%20in%0Adetecting%20such%20emails.%20This%20methodology%20can%20be%20utilized%20for%20various%20document%0Aclassification%20tasks%2C%20particularly%20in%20adversarial%20problem%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08309v3&entry.124074799=Read"},
{"title": "Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:\n  Diverse-Resolution Training Outperforms Interpolation", "author": "Anselm Krainovic and Stefan Ruschke and Reinhard Heckel", "abstract": "  Deep learning-based 3D imaging, in particular magnetic resonance imaging\n(MRI), is challenging because of limited availability of 3D training data.\nTherefore, 2D diffusion models trained on 2D slices are starting to be\nleveraged for 3D MRI reconstruction. However, as we show in this paper,\nexisting methods pertain to a fixed voxel size, and performance degrades when\nthe voxel size is varied, as it is often the case in clinical practice. In this\npaper, we propose and study several approaches for resolution-robust 3D MRI\nreconstruction with 2D diffusion priors. As a result of this investigation, we\nobtain a simple resolution-robust variational 3D reconstruction approach based\non diffusion-guided regularization of randomly sampled 2D slices. This method\nprovides competitive reconstruction quality compared to posterior sampling\nbaselines. Towards resolving the sensitivity to resolution-shifts, we\ninvestigate state-of-the-art model-based approaches including Gaussian\nsplatting, neural representations, and infinite-dimensional diffusion models,\nas well as a simple data-centric approach of training the diffusion model on\nseveral resolutions. Our experiments demonstrate that the model-based\napproaches fail to close the performance gap in 3D MRI. In contrast, the\ndata-centric approach of training the diffusion model on various resolutions\neffectively provides a resolution-robust method without compromising accuracy.\n", "link": "http://arxiv.org/abs/2412.18584v1", "date": "2024-12-24", "relevancy": 2.3212, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resolution-Robust%203D%20MRI%20Reconstruction%20with%202D%20Diffusion%20Priors%3A%0A%20%20Diverse-Resolution%20Training%20Outperforms%20Interpolation&body=Title%3A%20Resolution-Robust%203D%20MRI%20Reconstruction%20with%202D%20Diffusion%20Priors%3A%0A%20%20Diverse-Resolution%20Training%20Outperforms%20Interpolation%0AAuthor%3A%20Anselm%20Krainovic%20and%20Stefan%20Ruschke%20and%20Reinhard%20Heckel%0AAbstract%3A%20%20%20Deep%20learning-based%203D%20imaging%2C%20in%20particular%20magnetic%20resonance%20imaging%0A%28MRI%29%2C%20is%20challenging%20because%20of%20limited%20availability%20of%203D%20training%20data.%0ATherefore%2C%202D%20diffusion%20models%20trained%20on%202D%20slices%20are%20starting%20to%20be%0Aleveraged%20for%203D%20MRI%20reconstruction.%20However%2C%20as%20we%20show%20in%20this%20paper%2C%0Aexisting%20methods%20pertain%20to%20a%20fixed%20voxel%20size%2C%20and%20performance%20degrades%20when%0Athe%20voxel%20size%20is%20varied%2C%20as%20it%20is%20often%20the%20case%20in%20clinical%20practice.%20In%20this%0Apaper%2C%20we%20propose%20and%20study%20several%20approaches%20for%20resolution-robust%203D%20MRI%0Areconstruction%20with%202D%20diffusion%20priors.%20As%20a%20result%20of%20this%20investigation%2C%20we%0Aobtain%20a%20simple%20resolution-robust%20variational%203D%20reconstruction%20approach%20based%0Aon%20diffusion-guided%20regularization%20of%20randomly%20sampled%202D%20slices.%20This%20method%0Aprovides%20competitive%20reconstruction%20quality%20compared%20to%20posterior%20sampling%0Abaselines.%20Towards%20resolving%20the%20sensitivity%20to%20resolution-shifts%2C%20we%0Ainvestigate%20state-of-the-art%20model-based%20approaches%20including%20Gaussian%0Asplatting%2C%20neural%20representations%2C%20and%20infinite-dimensional%20diffusion%20models%2C%0Aas%20well%20as%20a%20simple%20data-centric%20approach%20of%20training%20the%20diffusion%20model%20on%0Aseveral%20resolutions.%20Our%20experiments%20demonstrate%20that%20the%20model-based%0Aapproaches%20fail%20to%20close%20the%20performance%20gap%20in%203D%20MRI.%20In%20contrast%2C%20the%0Adata-centric%20approach%20of%20training%20the%20diffusion%20model%20on%20various%20resolutions%0Aeffectively%20provides%20a%20resolution-robust%20method%20without%20compromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResolution-Robust%25203D%2520MRI%2520Reconstruction%2520with%25202D%2520Diffusion%2520Priors%253A%250A%2520%2520Diverse-Resolution%2520Training%2520Outperforms%2520Interpolation%26entry.906535625%3DAnselm%2520Krainovic%2520and%2520Stefan%2520Ruschke%2520and%2520Reinhard%2520Heckel%26entry.1292438233%3D%2520%2520Deep%2520learning-based%25203D%2520imaging%252C%2520in%2520particular%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%252C%2520is%2520challenging%2520because%2520of%2520limited%2520availability%2520of%25203D%2520training%2520data.%250ATherefore%252C%25202D%2520diffusion%2520models%2520trained%2520on%25202D%2520slices%2520are%2520starting%2520to%2520be%250Aleveraged%2520for%25203D%2520MRI%2520reconstruction.%2520However%252C%2520as%2520we%2520show%2520in%2520this%2520paper%252C%250Aexisting%2520methods%2520pertain%2520to%2520a%2520fixed%2520voxel%2520size%252C%2520and%2520performance%2520degrades%2520when%250Athe%2520voxel%2520size%2520is%2520varied%252C%2520as%2520it%2520is%2520often%2520the%2520case%2520in%2520clinical%2520practice.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520and%2520study%2520several%2520approaches%2520for%2520resolution-robust%25203D%2520MRI%250Areconstruction%2520with%25202D%2520diffusion%2520priors.%2520As%2520a%2520result%2520of%2520this%2520investigation%252C%2520we%250Aobtain%2520a%2520simple%2520resolution-robust%2520variational%25203D%2520reconstruction%2520approach%2520based%250Aon%2520diffusion-guided%2520regularization%2520of%2520randomly%2520sampled%25202D%2520slices.%2520This%2520method%250Aprovides%2520competitive%2520reconstruction%2520quality%2520compared%2520to%2520posterior%2520sampling%250Abaselines.%2520Towards%2520resolving%2520the%2520sensitivity%2520to%2520resolution-shifts%252C%2520we%250Ainvestigate%2520state-of-the-art%2520model-based%2520approaches%2520including%2520Gaussian%250Asplatting%252C%2520neural%2520representations%252C%2520and%2520infinite-dimensional%2520diffusion%2520models%252C%250Aas%2520well%2520as%2520a%2520simple%2520data-centric%2520approach%2520of%2520training%2520the%2520diffusion%2520model%2520on%250Aseveral%2520resolutions.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520model-based%250Aapproaches%2520fail%2520to%2520close%2520the%2520performance%2520gap%2520in%25203D%2520MRI.%2520In%2520contrast%252C%2520the%250Adata-centric%2520approach%2520of%2520training%2520the%2520diffusion%2520model%2520on%2520various%2520resolutions%250Aeffectively%2520provides%2520a%2520resolution-robust%2520method%2520without%2520compromising%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolution-Robust%203D%20MRI%20Reconstruction%20with%202D%20Diffusion%20Priors%3A%0A%20%20Diverse-Resolution%20Training%20Outperforms%20Interpolation&entry.906535625=Anselm%20Krainovic%20and%20Stefan%20Ruschke%20and%20Reinhard%20Heckel&entry.1292438233=%20%20Deep%20learning-based%203D%20imaging%2C%20in%20particular%20magnetic%20resonance%20imaging%0A%28MRI%29%2C%20is%20challenging%20because%20of%20limited%20availability%20of%203D%20training%20data.%0ATherefore%2C%202D%20diffusion%20models%20trained%20on%202D%20slices%20are%20starting%20to%20be%0Aleveraged%20for%203D%20MRI%20reconstruction.%20However%2C%20as%20we%20show%20in%20this%20paper%2C%0Aexisting%20methods%20pertain%20to%20a%20fixed%20voxel%20size%2C%20and%20performance%20degrades%20when%0Athe%20voxel%20size%20is%20varied%2C%20as%20it%20is%20often%20the%20case%20in%20clinical%20practice.%20In%20this%0Apaper%2C%20we%20propose%20and%20study%20several%20approaches%20for%20resolution-robust%203D%20MRI%0Areconstruction%20with%202D%20diffusion%20priors.%20As%20a%20result%20of%20this%20investigation%2C%20we%0Aobtain%20a%20simple%20resolution-robust%20variational%203D%20reconstruction%20approach%20based%0Aon%20diffusion-guided%20regularization%20of%20randomly%20sampled%202D%20slices.%20This%20method%0Aprovides%20competitive%20reconstruction%20quality%20compared%20to%20posterior%20sampling%0Abaselines.%20Towards%20resolving%20the%20sensitivity%20to%20resolution-shifts%2C%20we%0Ainvestigate%20state-of-the-art%20model-based%20approaches%20including%20Gaussian%0Asplatting%2C%20neural%20representations%2C%20and%20infinite-dimensional%20diffusion%20models%2C%0Aas%20well%20as%20a%20simple%20data-centric%20approach%20of%20training%20the%20diffusion%20model%20on%0Aseveral%20resolutions.%20Our%20experiments%20demonstrate%20that%20the%20model-based%0Aapproaches%20fail%20to%20close%20the%20performance%20gap%20in%203D%20MRI.%20In%20contrast%2C%20the%0Adata-centric%20approach%20of%20training%20the%20diffusion%20model%20on%20various%20resolutions%0Aeffectively%20provides%20a%20resolution-robust%20method%20without%20compromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18584v1&entry.124074799=Read"},
{"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free\n  Video-Language Models", "author": "Jinhui Yi and Syed Talal Wasim and Yanan Luo and Muzammal Naseer and Juergen Gall", "abstract": "  We present an efficient encoder-free approach for video-language\nunderstanding that achieves competitive performance while significantly\nreducing computational overhead. Current video-language models typically rely\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\nparameters), creating a substantial computational burden when processing\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\nBlock (STAB) that directly processes video inputs without requiring pre-trained\nencoders while using only 45M parameters for visual processing - at least a\n6.5$\\times$ reduction compared to traditional approaches. The STAB architecture\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\nefficient spatial downsampling through learned attention and separate\nmechanisms for modeling frame-level and video-level relationships. Our model\nachieves comparable or superior performance to encoder-based approaches for\nopen-ended video question answering on standard benchmarks. The fine-grained\nvideo question-answering evaluation demonstrates our model's effectiveness,\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\naspects like correctness and temporal understanding. Extensive ablation studies\nvalidate our architectural choices and demonstrate the effectiveness of our\nspatio-temporal modeling approach while achieving 3-4$\\times$ faster processing\nspeeds than previous methods. Code is available at\n\\url{https://github.com/jh-yi/Video-Panda}.\n", "link": "http://arxiv.org/abs/2412.18609v1", "date": "2024-12-24", "relevancy": 2.3175, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Panda%3A%20Parameter-efficient%20Alignment%20for%20Encoder-free%0A%20%20Video-Language%20Models&body=Title%3A%20Video-Panda%3A%20Parameter-efficient%20Alignment%20for%20Encoder-free%0A%20%20Video-Language%20Models%0AAuthor%3A%20Jinhui%20Yi%20and%20Syed%20Talal%20Wasim%20and%20Yanan%20Luo%20and%20Muzammal%20Naseer%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20We%20present%20an%20efficient%20encoder-free%20approach%20for%20video-language%0Aunderstanding%20that%20achieves%20competitive%20performance%20while%20significantly%0Areducing%20computational%20overhead.%20Current%20video-language%20models%20typically%20rely%0Aon%20heavyweight%20image%20encoders%20%28300M-1.1B%20parameters%29%20or%20video%20encoders%20%281B-1.4B%0Aparameters%29%2C%20creating%20a%20substantial%20computational%20burden%20when%20processing%0Amulti-frame%20videos.%20Our%20method%20introduces%20a%20novel%20Spatio-Temporal%20Alignment%0ABlock%20%28STAB%29%20that%20directly%20processes%20video%20inputs%20without%20requiring%20pre-trained%0Aencoders%20while%20using%20only%2045M%20parameters%20for%20visual%20processing%20-%20at%20least%20a%0A6.5%24%5Ctimes%24%20reduction%20compared%20to%20traditional%20approaches.%20The%20STAB%20architecture%0Acombines%20Local%20Spatio-Temporal%20Encoding%20for%20fine-grained%20feature%20extraction%2C%0Aefficient%20spatial%20downsampling%20through%20learned%20attention%20and%20separate%0Amechanisms%20for%20modeling%20frame-level%20and%20video-level%20relationships.%20Our%20model%0Aachieves%20comparable%20or%20superior%20performance%20to%20encoder-based%20approaches%20for%0Aopen-ended%20video%20question%20answering%20on%20standard%20benchmarks.%20The%20fine-grained%0Avideo%20question-answering%20evaluation%20demonstrates%20our%20model%27s%20effectiveness%2C%0Aoutperforming%20the%20encoder-based%20approaches%20Video-ChatGPT%20and%20Video-LLaVA%20in%20key%0Aaspects%20like%20correctness%20and%20temporal%20understanding.%20Extensive%20ablation%20studies%0Avalidate%20our%20architectural%20choices%20and%20demonstrate%20the%20effectiveness%20of%20our%0Aspatio-temporal%20modeling%20approach%20while%20achieving%203-4%24%5Ctimes%24%20faster%20processing%0Aspeeds%20than%20previous%20methods.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/jh-yi/Video-Panda%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Panda%253A%2520Parameter-efficient%2520Alignment%2520for%2520Encoder-free%250A%2520%2520Video-Language%2520Models%26entry.906535625%3DJinhui%2520Yi%2520and%2520Syed%2520Talal%2520Wasim%2520and%2520Yanan%2520Luo%2520and%2520Muzammal%2520Naseer%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520efficient%2520encoder-free%2520approach%2520for%2520video-language%250Aunderstanding%2520that%2520achieves%2520competitive%2520performance%2520while%2520significantly%250Areducing%2520computational%2520overhead.%2520Current%2520video-language%2520models%2520typically%2520rely%250Aon%2520heavyweight%2520image%2520encoders%2520%2528300M-1.1B%2520parameters%2529%2520or%2520video%2520encoders%2520%25281B-1.4B%250Aparameters%2529%252C%2520creating%2520a%2520substantial%2520computational%2520burden%2520when%2520processing%250Amulti-frame%2520videos.%2520Our%2520method%2520introduces%2520a%2520novel%2520Spatio-Temporal%2520Alignment%250ABlock%2520%2528STAB%2529%2520that%2520directly%2520processes%2520video%2520inputs%2520without%2520requiring%2520pre-trained%250Aencoders%2520while%2520using%2520only%252045M%2520parameters%2520for%2520visual%2520processing%2520-%2520at%2520least%2520a%250A6.5%2524%255Ctimes%2524%2520reduction%2520compared%2520to%2520traditional%2520approaches.%2520The%2520STAB%2520architecture%250Acombines%2520Local%2520Spatio-Temporal%2520Encoding%2520for%2520fine-grained%2520feature%2520extraction%252C%250Aefficient%2520spatial%2520downsampling%2520through%2520learned%2520attention%2520and%2520separate%250Amechanisms%2520for%2520modeling%2520frame-level%2520and%2520video-level%2520relationships.%2520Our%2520model%250Aachieves%2520comparable%2520or%2520superior%2520performance%2520to%2520encoder-based%2520approaches%2520for%250Aopen-ended%2520video%2520question%2520answering%2520on%2520standard%2520benchmarks.%2520The%2520fine-grained%250Avideo%2520question-answering%2520evaluation%2520demonstrates%2520our%2520model%2527s%2520effectiveness%252C%250Aoutperforming%2520the%2520encoder-based%2520approaches%2520Video-ChatGPT%2520and%2520Video-LLaVA%2520in%2520key%250Aaspects%2520like%2520correctness%2520and%2520temporal%2520understanding.%2520Extensive%2520ablation%2520studies%250Avalidate%2520our%2520architectural%2520choices%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aspatio-temporal%2520modeling%2520approach%2520while%2520achieving%25203-4%2524%255Ctimes%2524%2520faster%2520processing%250Aspeeds%2520than%2520previous%2520methods.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/jh-yi/Video-Panda%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Panda%3A%20Parameter-efficient%20Alignment%20for%20Encoder-free%0A%20%20Video-Language%20Models&entry.906535625=Jinhui%20Yi%20and%20Syed%20Talal%20Wasim%20and%20Yanan%20Luo%20and%20Muzammal%20Naseer%20and%20Juergen%20Gall&entry.1292438233=%20%20We%20present%20an%20efficient%20encoder-free%20approach%20for%20video-language%0Aunderstanding%20that%20achieves%20competitive%20performance%20while%20significantly%0Areducing%20computational%20overhead.%20Current%20video-language%20models%20typically%20rely%0Aon%20heavyweight%20image%20encoders%20%28300M-1.1B%20parameters%29%20or%20video%20encoders%20%281B-1.4B%0Aparameters%29%2C%20creating%20a%20substantial%20computational%20burden%20when%20processing%0Amulti-frame%20videos.%20Our%20method%20introduces%20a%20novel%20Spatio-Temporal%20Alignment%0ABlock%20%28STAB%29%20that%20directly%20processes%20video%20inputs%20without%20requiring%20pre-trained%0Aencoders%20while%20using%20only%2045M%20parameters%20for%20visual%20processing%20-%20at%20least%20a%0A6.5%24%5Ctimes%24%20reduction%20compared%20to%20traditional%20approaches.%20The%20STAB%20architecture%0Acombines%20Local%20Spatio-Temporal%20Encoding%20for%20fine-grained%20feature%20extraction%2C%0Aefficient%20spatial%20downsampling%20through%20learned%20attention%20and%20separate%0Amechanisms%20for%20modeling%20frame-level%20and%20video-level%20relationships.%20Our%20model%0Aachieves%20comparable%20or%20superior%20performance%20to%20encoder-based%20approaches%20for%0Aopen-ended%20video%20question%20answering%20on%20standard%20benchmarks.%20The%20fine-grained%0Avideo%20question-answering%20evaluation%20demonstrates%20our%20model%27s%20effectiveness%2C%0Aoutperforming%20the%20encoder-based%20approaches%20Video-ChatGPT%20and%20Video-LLaVA%20in%20key%0Aaspects%20like%20correctness%20and%20temporal%20understanding.%20Extensive%20ablation%20studies%0Avalidate%20our%20architectural%20choices%20and%20demonstrate%20the%20effectiveness%20of%20our%0Aspatio-temporal%20modeling%20approach%20while%20achieving%203-4%24%5Ctimes%24%20faster%20processing%0Aspeeds%20than%20previous%20methods.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/jh-yi/Video-Panda%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18609v1&entry.124074799=Read"},
{"title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark\n  Integrating Understanding, Reasoning, and Locating", "author": "Chao Deng and Jiale Yuan and Pi Bu and Peijie Wang and Zhong-Zhi Li and Jian Xu and Xiao-Hui Li and Yuan Gao and Jun Song and Bo Zheng and Cheng-Lin Liu", "abstract": "  Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field.\n", "link": "http://arxiv.org/abs/2412.18424v1", "date": "2024-12-24", "relevancy": 2.2959, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating&body=Title%3A%20LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating%0AAuthor%3A%20Chao%20Deng%20and%20Jiale%20Yuan%20and%20Pi%20Bu%20and%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Jian%20Xu%20and%20Xiao-Hui%20Li%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Large%20vision%20language%20models%20%28LVLMs%29%20have%20improved%20the%20document%20understanding%0Acapabilities%20remarkably%2C%20enabling%20the%20handling%20of%20complex%20document%20elements%2C%0Alonger%20contexts%2C%20and%20a%20wider%20range%20of%20tasks.%20However%2C%20existing%20document%0Aunderstanding%20benchmarks%20have%20been%20limited%20to%20handling%20only%20a%20small%20number%20of%0Apages%20and%20fail%20to%20provide%20a%20comprehensive%20analysis%20of%20layout%20elements%20locating.%0AIn%20this%20paper%2C%20we%20first%20define%20three%20primary%20task%20categories%3A%20Long%20Document%0AUnderstanding%2C%20numerical%20Reasoning%2C%20and%20cross-element%20Locating%2C%20and%20then%0Apropose%20a%20comprehensive%20benchmark%2C%20LongDocURL%2C%20integrating%20above%20three%20primary%0Atasks%20and%20comprising%2020%20sub-tasks%20categorized%20based%20on%20different%20primary%20tasks%0Aand%20answer%20evidences.%20Furthermore%2C%20we%20develop%20a%20semi-automated%20construction%0Apipeline%20and%20collect%202%2C325%20high-quality%20question-answering%20pairs%2C%20covering%20more%0Athan%2033%2C000%20pages%20of%20documents%2C%20significantly%20outperforming%20existing%0Abenchmarks.%20Subsequently%2C%20we%20conduct%20comprehensive%20evaluation%20experiments%20on%0Aboth%20open-source%20and%20closed-source%20models%20across%2026%20different%20configurations%2C%0Arevealing%20critical%20performance%20gaps%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongDocURL%253A%2520a%2520Comprehensive%2520Multimodal%2520Long%2520Document%2520Benchmark%250A%2520%2520Integrating%2520Understanding%252C%2520Reasoning%252C%2520and%2520Locating%26entry.906535625%3DChao%2520Deng%2520and%2520Jiale%2520Yuan%2520and%2520Pi%2520Bu%2520and%2520Peijie%2520Wang%2520and%2520Zhong-Zhi%2520Li%2520and%2520Jian%2520Xu%2520and%2520Xiao-Hui%2520Li%2520and%2520Yuan%2520Gao%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520have%2520improved%2520the%2520document%2520understanding%250Acapabilities%2520remarkably%252C%2520enabling%2520the%2520handling%2520of%2520complex%2520document%2520elements%252C%250Alonger%2520contexts%252C%2520and%2520a%2520wider%2520range%2520of%2520tasks.%2520However%252C%2520existing%2520document%250Aunderstanding%2520benchmarks%2520have%2520been%2520limited%2520to%2520handling%2520only%2520a%2520small%2520number%2520of%250Apages%2520and%2520fail%2520to%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520layout%2520elements%2520locating.%250AIn%2520this%2520paper%252C%2520we%2520first%2520define%2520three%2520primary%2520task%2520categories%253A%2520Long%2520Document%250AUnderstanding%252C%2520numerical%2520Reasoning%252C%2520and%2520cross-element%2520Locating%252C%2520and%2520then%250Apropose%2520a%2520comprehensive%2520benchmark%252C%2520LongDocURL%252C%2520integrating%2520above%2520three%2520primary%250Atasks%2520and%2520comprising%252020%2520sub-tasks%2520categorized%2520based%2520on%2520different%2520primary%2520tasks%250Aand%2520answer%2520evidences.%2520Furthermore%252C%2520we%2520develop%2520a%2520semi-automated%2520construction%250Apipeline%2520and%2520collect%25202%252C325%2520high-quality%2520question-answering%2520pairs%252C%2520covering%2520more%250Athan%252033%252C000%2520pages%2520of%2520documents%252C%2520significantly%2520outperforming%2520existing%250Abenchmarks.%2520Subsequently%252C%2520we%2520conduct%2520comprehensive%2520evaluation%2520experiments%2520on%250Aboth%2520open-source%2520and%2520closed-source%2520models%2520across%252026%2520different%2520configurations%252C%250Arevealing%2520critical%2520performance%2520gaps%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating&entry.906535625=Chao%20Deng%20and%20Jiale%20Yuan%20and%20Pi%20Bu%20and%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Jian%20Xu%20and%20Xiao-Hui%20Li%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Large%20vision%20language%20models%20%28LVLMs%29%20have%20improved%20the%20document%20understanding%0Acapabilities%20remarkably%2C%20enabling%20the%20handling%20of%20complex%20document%20elements%2C%0Alonger%20contexts%2C%20and%20a%20wider%20range%20of%20tasks.%20However%2C%20existing%20document%0Aunderstanding%20benchmarks%20have%20been%20limited%20to%20handling%20only%20a%20small%20number%20of%0Apages%20and%20fail%20to%20provide%20a%20comprehensive%20analysis%20of%20layout%20elements%20locating.%0AIn%20this%20paper%2C%20we%20first%20define%20three%20primary%20task%20categories%3A%20Long%20Document%0AUnderstanding%2C%20numerical%20Reasoning%2C%20and%20cross-element%20Locating%2C%20and%20then%0Apropose%20a%20comprehensive%20benchmark%2C%20LongDocURL%2C%20integrating%20above%20three%20primary%0Atasks%20and%20comprising%2020%20sub-tasks%20categorized%20based%20on%20different%20primary%20tasks%0Aand%20answer%20evidences.%20Furthermore%2C%20we%20develop%20a%20semi-automated%20construction%0Apipeline%20and%20collect%202%2C325%20high-quality%20question-answering%20pairs%2C%20covering%20more%0Athan%2033%2C000%20pages%20of%20documents%2C%20significantly%20outperforming%20existing%0Abenchmarks.%20Subsequently%2C%20we%20conduct%20comprehensive%20evaluation%20experiments%20on%0Aboth%20open-source%20and%20closed-source%20models%20across%2026%20different%20configurations%2C%0Arevealing%20critical%20performance%20gaps%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18424v1&entry.124074799=Read"},
{"title": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation", "author": "Dewu Zheng and Yanlin Wang and Ensheng Shi and Hongyu Zhang and Zibin Zheng", "abstract": "  Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.\n", "link": "http://arxiv.org/abs/2412.18573v1", "date": "2024-12-24", "relevancy": 2.2734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4628}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Well%20Do%20LLMs%20Generate%20Code%20for%20Different%20Application%20Domains%3F%0A%20%20Benchmark%20and%20Evaluation&body=Title%3A%20How%20Well%20Do%20LLMs%20Generate%20Code%20for%20Different%20Application%20Domains%3F%0A%20%20Benchmark%20and%20Evaluation%0AAuthor%3A%20Dewu%20Zheng%20and%20Yanlin%20Wang%20and%20Ensheng%20Shi%20and%20Hongyu%20Zhang%20and%20Zibin%20Zheng%0AAbstract%3A%20%20%20Recently%2C%20an%20increasing%20number%20of%20AI-driven%20programming%20assistants%20powered%20by%0Acode%20LLMs%20have%20been%20integrated%20into%20various%20real-world%20software%20development%0Aenvironments%2C%20significantly%20boosting%20developer%20productivity.%20However%2C%20existing%0Acode%20generation%20benchmarks%20primarily%20focus%20on%20general-purpose%20scenarios%2C%0Aleaving%20the%20code%20generation%20performance%20of%20LLMs%20for%20specific%20application%0Adomains%20largely%20unknown.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20benchmark%2C%0AMultiCodeBench%2C%20to%20fill%20this%20gap.%20MultiCodeBench%20comprises%202%2C400%20programming%0Atasks%2C%20covering%2012%20popular%20software%20development%20domains%20and%2015%20programming%0Alanguages.%20Specifically%2C%20we%20perform%20in-depth%20research%20to%20identify%20these%2012%0Aapplication%20domains.%20Given%20that%20each%20domain%20may%20involve%20multiple%20technical%0Aframeworks%2C%20and%20that%20different%20frameworks%20present%20distinct%20challenges%20in%20the%0Acoding%20process%2C%20we%20categorize%20the%20commonly%20used%20frameworks%20and%20platforms%20within%0Aeach%20domain.%20We%20then%20sample%20programming%20problems%20from%20GitHub%20repositories%0Arelated%20to%20these%20subdomains.%20To%20ensure%20the%20quality%20of%20the%20tasks%20and%20mitigate%0Adata%20leakage%20issues%2C%20we%20invite%20annotators%20to%20rewrite%20the%20docstrings%20for%20each%0Atask%20in%20MultiCodeBench.%20Additionally%2C%20we%20build%20a%20static%20analysis-based%0Adependency%20parsing%20tool%20to%20extract%20the%20dependencies%20in%20the%20ground%20truth%20for%0Aeach%20task%2C%20enabling%20deeper%20performance%20analysis.%20Through%20extensive%20experiments%0Aon%20MultiCodeBench%20with%20eleven%20representative%20mainstream%20LLMs%2C%20we%20reveal%20the%0Acode%20generation%20performance%20of%20the%20LLMs%20across%20different%20application%20domains%2C%0Aproviding%20practical%20insights%20for%20developers%20in%20downstream%20fields%20when%20selecting%0ALLMs.%20Furthermore%2C%20we%20analyze%20the%20reasons%20behind%20the%20models%27%20failures%20in%0Acompleting%20software%20application%20development%20tasks%2C%20offering%20guidance%20for%20model%0Adevelopers%20to%20enhance%20domain-specific%20code%20generation%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Well%2520Do%2520LLMs%2520Generate%2520Code%2520for%2520Different%2520Application%2520Domains%253F%250A%2520%2520Benchmark%2520and%2520Evaluation%26entry.906535625%3DDewu%2520Zheng%2520and%2520Yanlin%2520Wang%2520and%2520Ensheng%2520Shi%2520and%2520Hongyu%2520Zhang%2520and%2520Zibin%2520Zheng%26entry.1292438233%3D%2520%2520Recently%252C%2520an%2520increasing%2520number%2520of%2520AI-driven%2520programming%2520assistants%2520powered%2520by%250Acode%2520LLMs%2520have%2520been%2520integrated%2520into%2520various%2520real-world%2520software%2520development%250Aenvironments%252C%2520significantly%2520boosting%2520developer%2520productivity.%2520However%252C%2520existing%250Acode%2520generation%2520benchmarks%2520primarily%2520focus%2520on%2520general-purpose%2520scenarios%252C%250Aleaving%2520the%2520code%2520generation%2520performance%2520of%2520LLMs%2520for%2520specific%2520application%250Adomains%2520largely%2520unknown.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%250AMultiCodeBench%252C%2520to%2520fill%2520this%2520gap.%2520MultiCodeBench%2520comprises%25202%252C400%2520programming%250Atasks%252C%2520covering%252012%2520popular%2520software%2520development%2520domains%2520and%252015%2520programming%250Alanguages.%2520Specifically%252C%2520we%2520perform%2520in-depth%2520research%2520to%2520identify%2520these%252012%250Aapplication%2520domains.%2520Given%2520that%2520each%2520domain%2520may%2520involve%2520multiple%2520technical%250Aframeworks%252C%2520and%2520that%2520different%2520frameworks%2520present%2520distinct%2520challenges%2520in%2520the%250Acoding%2520process%252C%2520we%2520categorize%2520the%2520commonly%2520used%2520frameworks%2520and%2520platforms%2520within%250Aeach%2520domain.%2520We%2520then%2520sample%2520programming%2520problems%2520from%2520GitHub%2520repositories%250Arelated%2520to%2520these%2520subdomains.%2520To%2520ensure%2520the%2520quality%2520of%2520the%2520tasks%2520and%2520mitigate%250Adata%2520leakage%2520issues%252C%2520we%2520invite%2520annotators%2520to%2520rewrite%2520the%2520docstrings%2520for%2520each%250Atask%2520in%2520MultiCodeBench.%2520Additionally%252C%2520we%2520build%2520a%2520static%2520analysis-based%250Adependency%2520parsing%2520tool%2520to%2520extract%2520the%2520dependencies%2520in%2520the%2520ground%2520truth%2520for%250Aeach%2520task%252C%2520enabling%2520deeper%2520performance%2520analysis.%2520Through%2520extensive%2520experiments%250Aon%2520MultiCodeBench%2520with%2520eleven%2520representative%2520mainstream%2520LLMs%252C%2520we%2520reveal%2520the%250Acode%2520generation%2520performance%2520of%2520the%2520LLMs%2520across%2520different%2520application%2520domains%252C%250Aproviding%2520practical%2520insights%2520for%2520developers%2520in%2520downstream%2520fields%2520when%2520selecting%250ALLMs.%2520Furthermore%252C%2520we%2520analyze%2520the%2520reasons%2520behind%2520the%2520models%2527%2520failures%2520in%250Acompleting%2520software%2520application%2520development%2520tasks%252C%2520offering%2520guidance%2520for%2520model%250Adevelopers%2520to%2520enhance%2520domain-specific%2520code%2520generation%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Well%20Do%20LLMs%20Generate%20Code%20for%20Different%20Application%20Domains%3F%0A%20%20Benchmark%20and%20Evaluation&entry.906535625=Dewu%20Zheng%20and%20Yanlin%20Wang%20and%20Ensheng%20Shi%20and%20Hongyu%20Zhang%20and%20Zibin%20Zheng&entry.1292438233=%20%20Recently%2C%20an%20increasing%20number%20of%20AI-driven%20programming%20assistants%20powered%20by%0Acode%20LLMs%20have%20been%20integrated%20into%20various%20real-world%20software%20development%0Aenvironments%2C%20significantly%20boosting%20developer%20productivity.%20However%2C%20existing%0Acode%20generation%20benchmarks%20primarily%20focus%20on%20general-purpose%20scenarios%2C%0Aleaving%20the%20code%20generation%20performance%20of%20LLMs%20for%20specific%20application%0Adomains%20largely%20unknown.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20benchmark%2C%0AMultiCodeBench%2C%20to%20fill%20this%20gap.%20MultiCodeBench%20comprises%202%2C400%20programming%0Atasks%2C%20covering%2012%20popular%20software%20development%20domains%20and%2015%20programming%0Alanguages.%20Specifically%2C%20we%20perform%20in-depth%20research%20to%20identify%20these%2012%0Aapplication%20domains.%20Given%20that%20each%20domain%20may%20involve%20multiple%20technical%0Aframeworks%2C%20and%20that%20different%20frameworks%20present%20distinct%20challenges%20in%20the%0Acoding%20process%2C%20we%20categorize%20the%20commonly%20used%20frameworks%20and%20platforms%20within%0Aeach%20domain.%20We%20then%20sample%20programming%20problems%20from%20GitHub%20repositories%0Arelated%20to%20these%20subdomains.%20To%20ensure%20the%20quality%20of%20the%20tasks%20and%20mitigate%0Adata%20leakage%20issues%2C%20we%20invite%20annotators%20to%20rewrite%20the%20docstrings%20for%20each%0Atask%20in%20MultiCodeBench.%20Additionally%2C%20we%20build%20a%20static%20analysis-based%0Adependency%20parsing%20tool%20to%20extract%20the%20dependencies%20in%20the%20ground%20truth%20for%0Aeach%20task%2C%20enabling%20deeper%20performance%20analysis.%20Through%20extensive%20experiments%0Aon%20MultiCodeBench%20with%20eleven%20representative%20mainstream%20LLMs%2C%20we%20reveal%20the%0Acode%20generation%20performance%20of%20the%20LLMs%20across%20different%20application%20domains%2C%0Aproviding%20practical%20insights%20for%20developers%20in%20downstream%20fields%20when%20selecting%0ALLMs.%20Furthermore%2C%20we%20analyze%20the%20reasons%20behind%20the%20models%27%20failures%20in%0Acompleting%20software%20application%20development%20tasks%2C%20offering%20guidance%20for%20model%0Adevelopers%20to%20enhance%20domain-specific%20code%20generation%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18573v1&entry.124074799=Read"},
{"title": "An Empirical Analysis of Federated Learning Models Subject to\n  Label-Flipping Adversarial Attack", "author": "Kunal Bhatnagar and Sagana Chattanathan and Angela Dang and Bhargav Eranki and Ronnit Rana and Charan Sridhar and Siddharth Vedam and Angie Yao and Mark Stamp", "abstract": "  In this paper, we empirically analyze adversarial attacks on selected\nfederated learning models. The specific learning models considered are\nMultinominal Logistic Regression (MLR), Support Vector Classifier (SVC),\nMultilayer Perceptron (MLP), Convolution Neural Network (CNN), %Recurrent\nNeural Network (RNN), Random Forest, XGBoost, and Long Short-Term Memory\n(LSTM). For each model, we simulate label-flipping attacks, experimenting\nextensively with 10 federated clients and 100 federated clients. We vary the\npercentage of adversarial clients from 10% to 100% and, simultaneously, the\npercentage of labels flipped by each adversarial client is also varied from 10%\nto 100%. Among other results, we find that models differ in their inherent\nrobustness to the two vectors in our label-flipping attack, i.e., the\npercentage of adversarial clients, and the percentage of labels flipped by each\nadversarial client. We discuss the potential practical implications of our\nresults.\n", "link": "http://arxiv.org/abs/2412.18507v1", "date": "2024-12-24", "relevancy": 2.2733, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4382}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Analysis%20of%20Federated%20Learning%20Models%20Subject%20to%0A%20%20Label-Flipping%20Adversarial%20Attack&body=Title%3A%20An%20Empirical%20Analysis%20of%20Federated%20Learning%20Models%20Subject%20to%0A%20%20Label-Flipping%20Adversarial%20Attack%0AAuthor%3A%20Kunal%20Bhatnagar%20and%20Sagana%20Chattanathan%20and%20Angela%20Dang%20and%20Bhargav%20Eranki%20and%20Ronnit%20Rana%20and%20Charan%20Sridhar%20and%20Siddharth%20Vedam%20and%20Angie%20Yao%20and%20Mark%20Stamp%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20empirically%20analyze%20adversarial%20attacks%20on%20selected%0Afederated%20learning%20models.%20The%20specific%20learning%20models%20considered%20are%0AMultinominal%20Logistic%20Regression%20%28MLR%29%2C%20Support%20Vector%20Classifier%20%28SVC%29%2C%0AMultilayer%20Perceptron%20%28MLP%29%2C%20Convolution%20Neural%20Network%20%28CNN%29%2C%20%25Recurrent%0ANeural%20Network%20%28RNN%29%2C%20Random%20Forest%2C%20XGBoost%2C%20and%20Long%20Short-Term%20Memory%0A%28LSTM%29.%20For%20each%20model%2C%20we%20simulate%20label-flipping%20attacks%2C%20experimenting%0Aextensively%20with%2010%20federated%20clients%20and%20100%20federated%20clients.%20We%20vary%20the%0Apercentage%20of%20adversarial%20clients%20from%2010%25%20to%20100%25%20and%2C%20simultaneously%2C%20the%0Apercentage%20of%20labels%20flipped%20by%20each%20adversarial%20client%20is%20also%20varied%20from%2010%25%0Ato%20100%25.%20Among%20other%20results%2C%20we%20find%20that%20models%20differ%20in%20their%20inherent%0Arobustness%20to%20the%20two%20vectors%20in%20our%20label-flipping%20attack%2C%20i.e.%2C%20the%0Apercentage%20of%20adversarial%20clients%2C%20and%20the%20percentage%20of%20labels%20flipped%20by%20each%0Aadversarial%20client.%20We%20discuss%20the%20potential%20practical%20implications%20of%20our%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Analysis%2520of%2520Federated%2520Learning%2520Models%2520Subject%2520to%250A%2520%2520Label-Flipping%2520Adversarial%2520Attack%26entry.906535625%3DKunal%2520Bhatnagar%2520and%2520Sagana%2520Chattanathan%2520and%2520Angela%2520Dang%2520and%2520Bhargav%2520Eranki%2520and%2520Ronnit%2520Rana%2520and%2520Charan%2520Sridhar%2520and%2520Siddharth%2520Vedam%2520and%2520Angie%2520Yao%2520and%2520Mark%2520Stamp%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520empirically%2520analyze%2520adversarial%2520attacks%2520on%2520selected%250Afederated%2520learning%2520models.%2520The%2520specific%2520learning%2520models%2520considered%2520are%250AMultinominal%2520Logistic%2520Regression%2520%2528MLR%2529%252C%2520Support%2520Vector%2520Classifier%2520%2528SVC%2529%252C%250AMultilayer%2520Perceptron%2520%2528MLP%2529%252C%2520Convolution%2520Neural%2520Network%2520%2528CNN%2529%252C%2520%2525Recurrent%250ANeural%2520Network%2520%2528RNN%2529%252C%2520Random%2520Forest%252C%2520XGBoost%252C%2520and%2520Long%2520Short-Term%2520Memory%250A%2528LSTM%2529.%2520For%2520each%2520model%252C%2520we%2520simulate%2520label-flipping%2520attacks%252C%2520experimenting%250Aextensively%2520with%252010%2520federated%2520clients%2520and%2520100%2520federated%2520clients.%2520We%2520vary%2520the%250Apercentage%2520of%2520adversarial%2520clients%2520from%252010%2525%2520to%2520100%2525%2520and%252C%2520simultaneously%252C%2520the%250Apercentage%2520of%2520labels%2520flipped%2520by%2520each%2520adversarial%2520client%2520is%2520also%2520varied%2520from%252010%2525%250Ato%2520100%2525.%2520Among%2520other%2520results%252C%2520we%2520find%2520that%2520models%2520differ%2520in%2520their%2520inherent%250Arobustness%2520to%2520the%2520two%2520vectors%2520in%2520our%2520label-flipping%2520attack%252C%2520i.e.%252C%2520the%250Apercentage%2520of%2520adversarial%2520clients%252C%2520and%2520the%2520percentage%2520of%2520labels%2520flipped%2520by%2520each%250Aadversarial%2520client.%2520We%2520discuss%2520the%2520potential%2520practical%2520implications%2520of%2520our%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Analysis%20of%20Federated%20Learning%20Models%20Subject%20to%0A%20%20Label-Flipping%20Adversarial%20Attack&entry.906535625=Kunal%20Bhatnagar%20and%20Sagana%20Chattanathan%20and%20Angela%20Dang%20and%20Bhargav%20Eranki%20and%20Ronnit%20Rana%20and%20Charan%20Sridhar%20and%20Siddharth%20Vedam%20and%20Angie%20Yao%20and%20Mark%20Stamp&entry.1292438233=%20%20In%20this%20paper%2C%20we%20empirically%20analyze%20adversarial%20attacks%20on%20selected%0Afederated%20learning%20models.%20The%20specific%20learning%20models%20considered%20are%0AMultinominal%20Logistic%20Regression%20%28MLR%29%2C%20Support%20Vector%20Classifier%20%28SVC%29%2C%0AMultilayer%20Perceptron%20%28MLP%29%2C%20Convolution%20Neural%20Network%20%28CNN%29%2C%20%25Recurrent%0ANeural%20Network%20%28RNN%29%2C%20Random%20Forest%2C%20XGBoost%2C%20and%20Long%20Short-Term%20Memory%0A%28LSTM%29.%20For%20each%20model%2C%20we%20simulate%20label-flipping%20attacks%2C%20experimenting%0Aextensively%20with%2010%20federated%20clients%20and%20100%20federated%20clients.%20We%20vary%20the%0Apercentage%20of%20adversarial%20clients%20from%2010%25%20to%20100%25%20and%2C%20simultaneously%2C%20the%0Apercentage%20of%20labels%20flipped%20by%20each%20adversarial%20client%20is%20also%20varied%20from%2010%25%0Ato%20100%25.%20Among%20other%20results%2C%20we%20find%20that%20models%20differ%20in%20their%20inherent%0Arobustness%20to%20the%20two%20vectors%20in%20our%20label-flipping%20attack%2C%20i.e.%2C%20the%0Apercentage%20of%20adversarial%20clients%2C%20and%20the%20percentage%20of%20labels%20flipped%20by%20each%0Aadversarial%20client.%20We%20discuss%20the%20potential%20practical%20implications%20of%20our%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18507v1&entry.124074799=Read"},
{"title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment", "author": "Yibin Wang and Zhiyu Tan and Junyan Wang and Xiaomeng Yang and Cheng Jin and Hao Li", "abstract": "  Recent advancements in text-to-video (T2V) generative models have shown\nimpressive capabilities. However, these models are still inadequate in aligning\nsynthesized videos with human preferences (e.g., accurately reflecting text\ndescriptions), which is particularly difficult to address, as human preferences\nare inherently subjective and challenging to formalize as objective functions.\nTherefore, this paper proposes LiFT, a novel fine-tuning method leveraging\nhuman feedback for T2V model alignment. Specifically, we first construct a\nHuman Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k\nhuman annotations, each including a score and its corresponding rationale.\nBased on this, we train a reward model LiFT-Critic to learn reward function\neffectively, which serves as a proxy for human judgment, measuring the\nalignment between given videos and human expectations. Lastly, we leverage the\nlearned reward function to align the T2V model by maximizing the\nreward-weighted likelihood. As a case study, we apply our pipeline to\nCogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B\nacross all 16 metrics, highlighting the potential of human feedback in\nimproving the alignment and quality of synthesized videos.\n", "link": "http://arxiv.org/abs/2412.04814v2", "date": "2024-12-24", "relevancy": 2.2667, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6018}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiFT%3A%20Leveraging%20Human%20Feedback%20for%20Text-to-Video%20Model%20Alignment&body=Title%3A%20LiFT%3A%20Leveraging%20Human%20Feedback%20for%20Text-to-Video%20Model%20Alignment%0AAuthor%3A%20Yibin%20Wang%20and%20Zhiyu%20Tan%20and%20Junyan%20Wang%20and%20Xiaomeng%20Yang%20and%20Cheng%20Jin%20and%20Hao%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-video%20%28T2V%29%20generative%20models%20have%20shown%0Aimpressive%20capabilities.%20However%2C%20these%20models%20are%20still%20inadequate%20in%20aligning%0Asynthesized%20videos%20with%20human%20preferences%20%28e.g.%2C%20accurately%20reflecting%20text%0Adescriptions%29%2C%20which%20is%20particularly%20difficult%20to%20address%2C%20as%20human%20preferences%0Aare%20inherently%20subjective%20and%20challenging%20to%20formalize%20as%20objective%20functions.%0ATherefore%2C%20this%20paper%20proposes%20LiFT%2C%20a%20novel%20fine-tuning%20method%20leveraging%0Ahuman%20feedback%20for%20T2V%20model%20alignment.%20Specifically%2C%20we%20first%20construct%20a%0AHuman%20Rating%20Annotation%20dataset%2C%20LiFT-HRA%2C%20consisting%20of%20approximately%2010k%0Ahuman%20annotations%2C%20each%20including%20a%20score%20and%20its%20corresponding%20rationale.%0ABased%20on%20this%2C%20we%20train%20a%20reward%20model%20LiFT-Critic%20to%20learn%20reward%20function%0Aeffectively%2C%20which%20serves%20as%20a%20proxy%20for%20human%20judgment%2C%20measuring%20the%0Aalignment%20between%20given%20videos%20and%20human%20expectations.%20Lastly%2C%20we%20leverage%20the%0Alearned%20reward%20function%20to%20align%20the%20T2V%20model%20by%20maximizing%20the%0Areward-weighted%20likelihood.%20As%20a%20case%20study%2C%20we%20apply%20our%20pipeline%20to%0ACogVideoX-2B%2C%20showing%20that%20the%20fine-tuned%20model%20outperforms%20the%20CogVideoX-5B%0Aacross%20all%2016%20metrics%2C%20highlighting%20the%20potential%20of%20human%20feedback%20in%0Aimproving%20the%20alignment%20and%20quality%20of%20synthesized%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiFT%253A%2520Leveraging%2520Human%2520Feedback%2520for%2520Text-to-Video%2520Model%2520Alignment%26entry.906535625%3DYibin%2520Wang%2520and%2520Zhiyu%2520Tan%2520and%2520Junyan%2520Wang%2520and%2520Xiaomeng%2520Yang%2520and%2520Cheng%2520Jin%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-video%2520%2528T2V%2529%2520generative%2520models%2520have%2520shown%250Aimpressive%2520capabilities.%2520However%252C%2520these%2520models%2520are%2520still%2520inadequate%2520in%2520aligning%250Asynthesized%2520videos%2520with%2520human%2520preferences%2520%2528e.g.%252C%2520accurately%2520reflecting%2520text%250Adescriptions%2529%252C%2520which%2520is%2520particularly%2520difficult%2520to%2520address%252C%2520as%2520human%2520preferences%250Aare%2520inherently%2520subjective%2520and%2520challenging%2520to%2520formalize%2520as%2520objective%2520functions.%250ATherefore%252C%2520this%2520paper%2520proposes%2520LiFT%252C%2520a%2520novel%2520fine-tuning%2520method%2520leveraging%250Ahuman%2520feedback%2520for%2520T2V%2520model%2520alignment.%2520Specifically%252C%2520we%2520first%2520construct%2520a%250AHuman%2520Rating%2520Annotation%2520dataset%252C%2520LiFT-HRA%252C%2520consisting%2520of%2520approximately%252010k%250Ahuman%2520annotations%252C%2520each%2520including%2520a%2520score%2520and%2520its%2520corresponding%2520rationale.%250ABased%2520on%2520this%252C%2520we%2520train%2520a%2520reward%2520model%2520LiFT-Critic%2520to%2520learn%2520reward%2520function%250Aeffectively%252C%2520which%2520serves%2520as%2520a%2520proxy%2520for%2520human%2520judgment%252C%2520measuring%2520the%250Aalignment%2520between%2520given%2520videos%2520and%2520human%2520expectations.%2520Lastly%252C%2520we%2520leverage%2520the%250Alearned%2520reward%2520function%2520to%2520align%2520the%2520T2V%2520model%2520by%2520maximizing%2520the%250Areward-weighted%2520likelihood.%2520As%2520a%2520case%2520study%252C%2520we%2520apply%2520our%2520pipeline%2520to%250ACogVideoX-2B%252C%2520showing%2520that%2520the%2520fine-tuned%2520model%2520outperforms%2520the%2520CogVideoX-5B%250Aacross%2520all%252016%2520metrics%252C%2520highlighting%2520the%2520potential%2520of%2520human%2520feedback%2520in%250Aimproving%2520the%2520alignment%2520and%2520quality%2520of%2520synthesized%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiFT%3A%20Leveraging%20Human%20Feedback%20for%20Text-to-Video%20Model%20Alignment&entry.906535625=Yibin%20Wang%20and%20Zhiyu%20Tan%20and%20Junyan%20Wang%20and%20Xiaomeng%20Yang%20and%20Cheng%20Jin%20and%20Hao%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-video%20%28T2V%29%20generative%20models%20have%20shown%0Aimpressive%20capabilities.%20However%2C%20these%20models%20are%20still%20inadequate%20in%20aligning%0Asynthesized%20videos%20with%20human%20preferences%20%28e.g.%2C%20accurately%20reflecting%20text%0Adescriptions%29%2C%20which%20is%20particularly%20difficult%20to%20address%2C%20as%20human%20preferences%0Aare%20inherently%20subjective%20and%20challenging%20to%20formalize%20as%20objective%20functions.%0ATherefore%2C%20this%20paper%20proposes%20LiFT%2C%20a%20novel%20fine-tuning%20method%20leveraging%0Ahuman%20feedback%20for%20T2V%20model%20alignment.%20Specifically%2C%20we%20first%20construct%20a%0AHuman%20Rating%20Annotation%20dataset%2C%20LiFT-HRA%2C%20consisting%20of%20approximately%2010k%0Ahuman%20annotations%2C%20each%20including%20a%20score%20and%20its%20corresponding%20rationale.%0ABased%20on%20this%2C%20we%20train%20a%20reward%20model%20LiFT-Critic%20to%20learn%20reward%20function%0Aeffectively%2C%20which%20serves%20as%20a%20proxy%20for%20human%20judgment%2C%20measuring%20the%0Aalignment%20between%20given%20videos%20and%20human%20expectations.%20Lastly%2C%20we%20leverage%20the%0Alearned%20reward%20function%20to%20align%20the%20T2V%20model%20by%20maximizing%20the%0Areward-weighted%20likelihood.%20As%20a%20case%20study%2C%20we%20apply%20our%20pipeline%20to%0ACogVideoX-2B%2C%20showing%20that%20the%20fine-tuned%20model%20outperforms%20the%20CogVideoX-5B%0Aacross%20all%2016%20metrics%2C%20highlighting%20the%20potential%20of%20human%20feedback%20in%0Aimproving%20the%20alignment%20and%20quality%20of%20synthesized%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04814v2&entry.124074799=Read"},
{"title": "Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges", "author": "Meixia He and Peican Zhu and Keke Tang and Yangming Guo", "abstract": "  Recent studies have shown that Hypergraph Neural Networks (HGNNs) are\nvulnerable to adversarial attacks. Existing approaches focus on hypergraph\nmodification attacks guided by gradients, overlooking node spanning in the\nhypergraph and the group identity of hyperedges, thereby resulting in limited\nattack performance and detectable attacks. In this manuscript, we present a\nnovel framework, i.e., Hypergraph Attacks via Injecting Homogeneous Nodes into\nElite Hyperedges (IE-Attack), to tackle these challenges. Initially, utilizing\nthe node spanning in the hypergraph, we propose the elite hyperedges sampler to\nidentify hyperedges to be injected. Subsequently, a node generator utilizing\nKernel Density Estimation (KDE) is proposed to generate the homogeneous node\nwith the group identity of hyperedges. Finally, by injecting the homogeneous\nnode into elite hyperedges, IE-Attack improves the attack performance and\nenhances the imperceptibility of attacks. Extensive experiments are conducted\non five authentic datasets to validate the effectiveness of IE-Attack and the\ncorresponding superiority to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.18365v1", "date": "2024-12-24", "relevancy": 2.2624, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4288}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph%20Attacks%20via%20Injecting%20Homogeneous%20Nodes%20into%20Elite%20Hyperedges&body=Title%3A%20Hypergraph%20Attacks%20via%20Injecting%20Homogeneous%20Nodes%20into%20Elite%20Hyperedges%0AAuthor%3A%20Meixia%20He%20and%20Peican%20Zhu%20and%20Keke%20Tang%20and%20Yangming%20Guo%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20Hypergraph%20Neural%20Networks%20%28HGNNs%29%20are%0Avulnerable%20to%20adversarial%20attacks.%20Existing%20approaches%20focus%20on%20hypergraph%0Amodification%20attacks%20guided%20by%20gradients%2C%20overlooking%20node%20spanning%20in%20the%0Ahypergraph%20and%20the%20group%20identity%20of%20hyperedges%2C%20thereby%20resulting%20in%20limited%0Aattack%20performance%20and%20detectable%20attacks.%20In%20this%20manuscript%2C%20we%20present%20a%0Anovel%20framework%2C%20i.e.%2C%20Hypergraph%20Attacks%20via%20Injecting%20Homogeneous%20Nodes%20into%0AElite%20Hyperedges%20%28IE-Attack%29%2C%20to%20tackle%20these%20challenges.%20Initially%2C%20utilizing%0Athe%20node%20spanning%20in%20the%20hypergraph%2C%20we%20propose%20the%20elite%20hyperedges%20sampler%20to%0Aidentify%20hyperedges%20to%20be%20injected.%20Subsequently%2C%20a%20node%20generator%20utilizing%0AKernel%20Density%20Estimation%20%28KDE%29%20is%20proposed%20to%20generate%20the%20homogeneous%20node%0Awith%20the%20group%20identity%20of%20hyperedges.%20Finally%2C%20by%20injecting%20the%20homogeneous%0Anode%20into%20elite%20hyperedges%2C%20IE-Attack%20improves%20the%20attack%20performance%20and%0Aenhances%20the%20imperceptibility%20of%20attacks.%20Extensive%20experiments%20are%20conducted%0Aon%20five%20authentic%20datasets%20to%20validate%20the%20effectiveness%20of%20IE-Attack%20and%20the%0Acorresponding%20superiority%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph%2520Attacks%2520via%2520Injecting%2520Homogeneous%2520Nodes%2520into%2520Elite%2520Hyperedges%26entry.906535625%3DMeixia%2520He%2520and%2520Peican%2520Zhu%2520and%2520Keke%2520Tang%2520and%2520Yangming%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520Hypergraph%2520Neural%2520Networks%2520%2528HGNNs%2529%2520are%250Avulnerable%2520to%2520adversarial%2520attacks.%2520Existing%2520approaches%2520focus%2520on%2520hypergraph%250Amodification%2520attacks%2520guided%2520by%2520gradients%252C%2520overlooking%2520node%2520spanning%2520in%2520the%250Ahypergraph%2520and%2520the%2520group%2520identity%2520of%2520hyperedges%252C%2520thereby%2520resulting%2520in%2520limited%250Aattack%2520performance%2520and%2520detectable%2520attacks.%2520In%2520this%2520manuscript%252C%2520we%2520present%2520a%250Anovel%2520framework%252C%2520i.e.%252C%2520Hypergraph%2520Attacks%2520via%2520Injecting%2520Homogeneous%2520Nodes%2520into%250AElite%2520Hyperedges%2520%2528IE-Attack%2529%252C%2520to%2520tackle%2520these%2520challenges.%2520Initially%252C%2520utilizing%250Athe%2520node%2520spanning%2520in%2520the%2520hypergraph%252C%2520we%2520propose%2520the%2520elite%2520hyperedges%2520sampler%2520to%250Aidentify%2520hyperedges%2520to%2520be%2520injected.%2520Subsequently%252C%2520a%2520node%2520generator%2520utilizing%250AKernel%2520Density%2520Estimation%2520%2528KDE%2529%2520is%2520proposed%2520to%2520generate%2520the%2520homogeneous%2520node%250Awith%2520the%2520group%2520identity%2520of%2520hyperedges.%2520Finally%252C%2520by%2520injecting%2520the%2520homogeneous%250Anode%2520into%2520elite%2520hyperedges%252C%2520IE-Attack%2520improves%2520the%2520attack%2520performance%2520and%250Aenhances%2520the%2520imperceptibility%2520of%2520attacks.%2520Extensive%2520experiments%2520are%2520conducted%250Aon%2520five%2520authentic%2520datasets%2520to%2520validate%2520the%2520effectiveness%2520of%2520IE-Attack%2520and%2520the%250Acorresponding%2520superiority%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph%20Attacks%20via%20Injecting%20Homogeneous%20Nodes%20into%20Elite%20Hyperedges&entry.906535625=Meixia%20He%20and%20Peican%20Zhu%20and%20Keke%20Tang%20and%20Yangming%20Guo&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20Hypergraph%20Neural%20Networks%20%28HGNNs%29%20are%0Avulnerable%20to%20adversarial%20attacks.%20Existing%20approaches%20focus%20on%20hypergraph%0Amodification%20attacks%20guided%20by%20gradients%2C%20overlooking%20node%20spanning%20in%20the%0Ahypergraph%20and%20the%20group%20identity%20of%20hyperedges%2C%20thereby%20resulting%20in%20limited%0Aattack%20performance%20and%20detectable%20attacks.%20In%20this%20manuscript%2C%20we%20present%20a%0Anovel%20framework%2C%20i.e.%2C%20Hypergraph%20Attacks%20via%20Injecting%20Homogeneous%20Nodes%20into%0AElite%20Hyperedges%20%28IE-Attack%29%2C%20to%20tackle%20these%20challenges.%20Initially%2C%20utilizing%0Athe%20node%20spanning%20in%20the%20hypergraph%2C%20we%20propose%20the%20elite%20hyperedges%20sampler%20to%0Aidentify%20hyperedges%20to%20be%20injected.%20Subsequently%2C%20a%20node%20generator%20utilizing%0AKernel%20Density%20Estimation%20%28KDE%29%20is%20proposed%20to%20generate%20the%20homogeneous%20node%0Awith%20the%20group%20identity%20of%20hyperedges.%20Finally%2C%20by%20injecting%20the%20homogeneous%0Anode%20into%20elite%20hyperedges%2C%20IE-Attack%20improves%20the%20attack%20performance%20and%0Aenhances%20the%20imperceptibility%20of%20attacks.%20Extensive%20experiments%20are%20conducted%0Aon%20five%20authentic%20datasets%20to%20validate%20the%20effectiveness%20of%20IE-Attack%20and%20the%0Acorresponding%20superiority%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18365v1&entry.124074799=Read"},
{"title": "GCN-ABFT: Low-Cost Online Error Checking for Graph Convolutional\n  Networks", "author": "Christodoulos Peltekis and Giorgos Dimitrakopoulos", "abstract": "  Graph convolutional networks (GCNs) are popular for building machine-learning\napplication for graph-structured data. This widespread adoption led to the\ndevelopment of specialized GCN hardware accelerators. In this work, we address\na key architectural challenge for GCN accelerators: how to detect errors in GCN\ncomputations arising from random hardware faults with the least computation\ncost. Each GCN layer performs a graph convolution, mathematically equivalent to\nmultiplying three matrices, computed through two separate matrix\nmultiplications. Existing Algorithm-based Fault Tolerance(ABFT) techniques can\ncheck the results of individual matrix multiplications. However, for a GCN\nlayer, this check should be performed twice. To avoid this overhead, this work\nintroduces GCN-ABFT that directly calculates a checksum for the entire\nthree-matrix product within a single GCN layer, providing a cost-effective\napproach for error detection in GCN accelerators. Experimental results\ndemonstrate that GCN-ABFT reduces the number of operations needed for checksum\ncomputation by over 21% on average for representative GCN applications. These\nsavings are achieved without sacrificing fault-detection accuracy, as evidenced\nby the presented fault-injection analysis.\n", "link": "http://arxiv.org/abs/2412.18534v1", "date": "2024-12-24", "relevancy": 2.2535, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCN-ABFT%3A%20Low-Cost%20Online%20Error%20Checking%20for%20Graph%20Convolutional%0A%20%20Networks&body=Title%3A%20GCN-ABFT%3A%20Low-Cost%20Online%20Error%20Checking%20for%20Graph%20Convolutional%0A%20%20Networks%0AAuthor%3A%20Christodoulos%20Peltekis%20and%20Giorgos%20Dimitrakopoulos%0AAbstract%3A%20%20%20Graph%20convolutional%20networks%20%28GCNs%29%20are%20popular%20for%20building%20machine-learning%0Aapplication%20for%20graph-structured%20data.%20This%20widespread%20adoption%20led%20to%20the%0Adevelopment%20of%20specialized%20GCN%20hardware%20accelerators.%20In%20this%20work%2C%20we%20address%0Aa%20key%20architectural%20challenge%20for%20GCN%20accelerators%3A%20how%20to%20detect%20errors%20in%20GCN%0Acomputations%20arising%20from%20random%20hardware%20faults%20with%20the%20least%20computation%0Acost.%20Each%20GCN%20layer%20performs%20a%20graph%20convolution%2C%20mathematically%20equivalent%20to%0Amultiplying%20three%20matrices%2C%20computed%20through%20two%20separate%20matrix%0Amultiplications.%20Existing%20Algorithm-based%20Fault%20Tolerance%28ABFT%29%20techniques%20can%0Acheck%20the%20results%20of%20individual%20matrix%20multiplications.%20However%2C%20for%20a%20GCN%0Alayer%2C%20this%20check%20should%20be%20performed%20twice.%20To%20avoid%20this%20overhead%2C%20this%20work%0Aintroduces%20GCN-ABFT%20that%20directly%20calculates%20a%20checksum%20for%20the%20entire%0Athree-matrix%20product%20within%20a%20single%20GCN%20layer%2C%20providing%20a%20cost-effective%0Aapproach%20for%20error%20detection%20in%20GCN%20accelerators.%20Experimental%20results%0Ademonstrate%20that%20GCN-ABFT%20reduces%20the%20number%20of%20operations%20needed%20for%20checksum%0Acomputation%20by%20over%2021%25%20on%20average%20for%20representative%20GCN%20applications.%20These%0Asavings%20are%20achieved%20without%20sacrificing%20fault-detection%20accuracy%2C%20as%20evidenced%0Aby%20the%20presented%20fault-injection%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCN-ABFT%253A%2520Low-Cost%2520Online%2520Error%2520Checking%2520for%2520Graph%2520Convolutional%250A%2520%2520Networks%26entry.906535625%3DChristodoulos%2520Peltekis%2520and%2520Giorgos%2520Dimitrakopoulos%26entry.1292438233%3D%2520%2520Graph%2520convolutional%2520networks%2520%2528GCNs%2529%2520are%2520popular%2520for%2520building%2520machine-learning%250Aapplication%2520for%2520graph-structured%2520data.%2520This%2520widespread%2520adoption%2520led%2520to%2520the%250Adevelopment%2520of%2520specialized%2520GCN%2520hardware%2520accelerators.%2520In%2520this%2520work%252C%2520we%2520address%250Aa%2520key%2520architectural%2520challenge%2520for%2520GCN%2520accelerators%253A%2520how%2520to%2520detect%2520errors%2520in%2520GCN%250Acomputations%2520arising%2520from%2520random%2520hardware%2520faults%2520with%2520the%2520least%2520computation%250Acost.%2520Each%2520GCN%2520layer%2520performs%2520a%2520graph%2520convolution%252C%2520mathematically%2520equivalent%2520to%250Amultiplying%2520three%2520matrices%252C%2520computed%2520through%2520two%2520separate%2520matrix%250Amultiplications.%2520Existing%2520Algorithm-based%2520Fault%2520Tolerance%2528ABFT%2529%2520techniques%2520can%250Acheck%2520the%2520results%2520of%2520individual%2520matrix%2520multiplications.%2520However%252C%2520for%2520a%2520GCN%250Alayer%252C%2520this%2520check%2520should%2520be%2520performed%2520twice.%2520To%2520avoid%2520this%2520overhead%252C%2520this%2520work%250Aintroduces%2520GCN-ABFT%2520that%2520directly%2520calculates%2520a%2520checksum%2520for%2520the%2520entire%250Athree-matrix%2520product%2520within%2520a%2520single%2520GCN%2520layer%252C%2520providing%2520a%2520cost-effective%250Aapproach%2520for%2520error%2520detection%2520in%2520GCN%2520accelerators.%2520Experimental%2520results%250Ademonstrate%2520that%2520GCN-ABFT%2520reduces%2520the%2520number%2520of%2520operations%2520needed%2520for%2520checksum%250Acomputation%2520by%2520over%252021%2525%2520on%2520average%2520for%2520representative%2520GCN%2520applications.%2520These%250Asavings%2520are%2520achieved%2520without%2520sacrificing%2520fault-detection%2520accuracy%252C%2520as%2520evidenced%250Aby%2520the%2520presented%2520fault-injection%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCN-ABFT%3A%20Low-Cost%20Online%20Error%20Checking%20for%20Graph%20Convolutional%0A%20%20Networks&entry.906535625=Christodoulos%20Peltekis%20and%20Giorgos%20Dimitrakopoulos&entry.1292438233=%20%20Graph%20convolutional%20networks%20%28GCNs%29%20are%20popular%20for%20building%20machine-learning%0Aapplication%20for%20graph-structured%20data.%20This%20widespread%20adoption%20led%20to%20the%0Adevelopment%20of%20specialized%20GCN%20hardware%20accelerators.%20In%20this%20work%2C%20we%20address%0Aa%20key%20architectural%20challenge%20for%20GCN%20accelerators%3A%20how%20to%20detect%20errors%20in%20GCN%0Acomputations%20arising%20from%20random%20hardware%20faults%20with%20the%20least%20computation%0Acost.%20Each%20GCN%20layer%20performs%20a%20graph%20convolution%2C%20mathematically%20equivalent%20to%0Amultiplying%20three%20matrices%2C%20computed%20through%20two%20separate%20matrix%0Amultiplications.%20Existing%20Algorithm-based%20Fault%20Tolerance%28ABFT%29%20techniques%20can%0Acheck%20the%20results%20of%20individual%20matrix%20multiplications.%20However%2C%20for%20a%20GCN%0Alayer%2C%20this%20check%20should%20be%20performed%20twice.%20To%20avoid%20this%20overhead%2C%20this%20work%0Aintroduces%20GCN-ABFT%20that%20directly%20calculates%20a%20checksum%20for%20the%20entire%0Athree-matrix%20product%20within%20a%20single%20GCN%20layer%2C%20providing%20a%20cost-effective%0Aapproach%20for%20error%20detection%20in%20GCN%20accelerators.%20Experimental%20results%0Ademonstrate%20that%20GCN-ABFT%20reduces%20the%20number%20of%20operations%20needed%20for%20checksum%0Acomputation%20by%20over%2021%25%20on%20average%20for%20representative%20GCN%20applications.%20These%0Asavings%20are%20achieved%20without%20sacrificing%20fault-detection%20accuracy%2C%20as%20evidenced%0Aby%20the%20presented%20fault-injection%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18534v1&entry.124074799=Read"},
{"title": "DrivingGPT: Unifying Driving World Modeling and Planning with\n  Multi-modal Autoregressive Transformers", "author": "Yuntao Chen and Yuqi Wang and Zhaoxiang Zhang", "abstract": "  World model-based searching and planning are widely recognized as a promising\npath toward human-level physical intelligence. However, current driving world\nmodels primarily rely on video diffusion models, which specialize in visual\ngeneration but lack the flexibility to incorporate other modalities like\naction. In contrast, autoregressive transformers have demonstrated exceptional\ncapability in modeling multimodal data. Our work aims to unify both driving\nmodel simulation and trajectory planning into a single sequence modeling\nproblem. We introduce a multimodal driving language based on interleaved image\nand action tokens, and develop DrivingGPT to learn joint world modeling and\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\nstrong performance in both action-conditioned video generation and end-to-end\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\nbenchmarks.\n", "link": "http://arxiv.org/abs/2412.18607v1", "date": "2024-12-24", "relevancy": 2.2323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5769}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrivingGPT%3A%20Unifying%20Driving%20World%20Modeling%20and%20Planning%20with%0A%20%20Multi-modal%20Autoregressive%20Transformers&body=Title%3A%20DrivingGPT%3A%20Unifying%20Driving%20World%20Modeling%20and%20Planning%20with%0A%20%20Multi-modal%20Autoregressive%20Transformers%0AAuthor%3A%20Yuntao%20Chen%20and%20Yuqi%20Wang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20World%20model-based%20searching%20and%20planning%20are%20widely%20recognized%20as%20a%20promising%0Apath%20toward%20human-level%20physical%20intelligence.%20However%2C%20current%20driving%20world%0Amodels%20primarily%20rely%20on%20video%20diffusion%20models%2C%20which%20specialize%20in%20visual%0Ageneration%20but%20lack%20the%20flexibility%20to%20incorporate%20other%20modalities%20like%0Aaction.%20In%20contrast%2C%20autoregressive%20transformers%20have%20demonstrated%20exceptional%0Acapability%20in%20modeling%20multimodal%20data.%20Our%20work%20aims%20to%20unify%20both%20driving%0Amodel%20simulation%20and%20trajectory%20planning%20into%20a%20single%20sequence%20modeling%0Aproblem.%20We%20introduce%20a%20multimodal%20driving%20language%20based%20on%20interleaved%20image%0Aand%20action%20tokens%2C%20and%20develop%20DrivingGPT%20to%20learn%20joint%20world%20modeling%20and%0Aplanning%20through%20standard%20next-token%20prediction.%20Our%20DrivingGPT%20demonstrates%0Astrong%20performance%20in%20both%20action-conditioned%20video%20generation%20and%20end-to-end%0Aplanning%2C%20outperforming%20strong%20baselines%20on%20large-scale%20nuPlan%20and%20NAVSIM%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrivingGPT%253A%2520Unifying%2520Driving%2520World%2520Modeling%2520and%2520Planning%2520with%250A%2520%2520Multi-modal%2520Autoregressive%2520Transformers%26entry.906535625%3DYuntao%2520Chen%2520and%2520Yuqi%2520Wang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520World%2520model-based%2520searching%2520and%2520planning%2520are%2520widely%2520recognized%2520as%2520a%2520promising%250Apath%2520toward%2520human-level%2520physical%2520intelligence.%2520However%252C%2520current%2520driving%2520world%250Amodels%2520primarily%2520rely%2520on%2520video%2520diffusion%2520models%252C%2520which%2520specialize%2520in%2520visual%250Ageneration%2520but%2520lack%2520the%2520flexibility%2520to%2520incorporate%2520other%2520modalities%2520like%250Aaction.%2520In%2520contrast%252C%2520autoregressive%2520transformers%2520have%2520demonstrated%2520exceptional%250Acapability%2520in%2520modeling%2520multimodal%2520data.%2520Our%2520work%2520aims%2520to%2520unify%2520both%2520driving%250Amodel%2520simulation%2520and%2520trajectory%2520planning%2520into%2520a%2520single%2520sequence%2520modeling%250Aproblem.%2520We%2520introduce%2520a%2520multimodal%2520driving%2520language%2520based%2520on%2520interleaved%2520image%250Aand%2520action%2520tokens%252C%2520and%2520develop%2520DrivingGPT%2520to%2520learn%2520joint%2520world%2520modeling%2520and%250Aplanning%2520through%2520standard%2520next-token%2520prediction.%2520Our%2520DrivingGPT%2520demonstrates%250Astrong%2520performance%2520in%2520both%2520action-conditioned%2520video%2520generation%2520and%2520end-to-end%250Aplanning%252C%2520outperforming%2520strong%2520baselines%2520on%2520large-scale%2520nuPlan%2520and%2520NAVSIM%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrivingGPT%3A%20Unifying%20Driving%20World%20Modeling%20and%20Planning%20with%0A%20%20Multi-modal%20Autoregressive%20Transformers&entry.906535625=Yuntao%20Chen%20and%20Yuqi%20Wang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20World%20model-based%20searching%20and%20planning%20are%20widely%20recognized%20as%20a%20promising%0Apath%20toward%20human-level%20physical%20intelligence.%20However%2C%20current%20driving%20world%0Amodels%20primarily%20rely%20on%20video%20diffusion%20models%2C%20which%20specialize%20in%20visual%0Ageneration%20but%20lack%20the%20flexibility%20to%20incorporate%20other%20modalities%20like%0Aaction.%20In%20contrast%2C%20autoregressive%20transformers%20have%20demonstrated%20exceptional%0Acapability%20in%20modeling%20multimodal%20data.%20Our%20work%20aims%20to%20unify%20both%20driving%0Amodel%20simulation%20and%20trajectory%20planning%20into%20a%20single%20sequence%20modeling%0Aproblem.%20We%20introduce%20a%20multimodal%20driving%20language%20based%20on%20interleaved%20image%0Aand%20action%20tokens%2C%20and%20develop%20DrivingGPT%20to%20learn%20joint%20world%20modeling%20and%0Aplanning%20through%20standard%20next-token%20prediction.%20Our%20DrivingGPT%20demonstrates%0Astrong%20performance%20in%20both%20action-conditioned%20video%20generation%20and%20end-to-end%0Aplanning%2C%20outperforming%20strong%20baselines%20on%20large-scale%20nuPlan%20and%20NAVSIM%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18607v1&entry.124074799=Read"},
{"title": "FedGIG: Graph Inversion from Gradient in Federated Learning", "author": "Tianzhe Xiao and Yichen Li and Yining Qi and Haozhao Wang and Ruixuan Li", "abstract": "  Recent studies have shown that Federated learning (FL) is vulnerable to\nGradient Inversion Attacks (GIA), which can recover private training data from\nshared gradients. However, existing methods are designed for dense, continuous\ndata such as images or vectorized texts, and cannot be directly applied to\nsparse and discrete graph data. This paper first explores GIA's impact on\nFederated Graph Learning (FGL) and introduces Graph Inversion from Gradient in\nFederated Learning (FedGIG), a novel GIA method specifically designed for\ngraph-structured data. FedGIG includes the adjacency matrix constraining\nmodule, which ensures the sparsity and discreteness of the reconstructed graph\ndata, and the subgraph reconstruction module, which is designed to complete\nmissing common subgraph structures. Extensive experiments on molecular datasets\ndemonstrate FedGIG's superior accuracy over existing GIA techniques.\n", "link": "http://arxiv.org/abs/2412.18513v1", "date": "2024-12-24", "relevancy": 2.2304, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4679}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4354}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedGIG%3A%20Graph%20Inversion%20from%20Gradient%20in%20Federated%20Learning&body=Title%3A%20FedGIG%3A%20Graph%20Inversion%20from%20Gradient%20in%20Federated%20Learning%0AAuthor%3A%20Tianzhe%20Xiao%20and%20Yichen%20Li%20and%20Yining%20Qi%20and%20Haozhao%20Wang%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20Federated%20learning%20%28FL%29%20is%20vulnerable%20to%0AGradient%20Inversion%20Attacks%20%28GIA%29%2C%20which%20can%20recover%20private%20training%20data%20from%0Ashared%20gradients.%20However%2C%20existing%20methods%20are%20designed%20for%20dense%2C%20continuous%0Adata%20such%20as%20images%20or%20vectorized%20texts%2C%20and%20cannot%20be%20directly%20applied%20to%0Asparse%20and%20discrete%20graph%20data.%20This%20paper%20first%20explores%20GIA%27s%20impact%20on%0AFederated%20Graph%20Learning%20%28FGL%29%20and%20introduces%20Graph%20Inversion%20from%20Gradient%20in%0AFederated%20Learning%20%28FedGIG%29%2C%20a%20novel%20GIA%20method%20specifically%20designed%20for%0Agraph-structured%20data.%20FedGIG%20includes%20the%20adjacency%20matrix%20constraining%0Amodule%2C%20which%20ensures%20the%20sparsity%20and%20discreteness%20of%20the%20reconstructed%20graph%0Adata%2C%20and%20the%20subgraph%20reconstruction%20module%2C%20which%20is%20designed%20to%20complete%0Amissing%20common%20subgraph%20structures.%20Extensive%20experiments%20on%20molecular%20datasets%0Ademonstrate%20FedGIG%27s%20superior%20accuracy%20over%20existing%20GIA%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedGIG%253A%2520Graph%2520Inversion%2520from%2520Gradient%2520in%2520Federated%2520Learning%26entry.906535625%3DTianzhe%2520Xiao%2520and%2520Yichen%2520Li%2520and%2520Yining%2520Qi%2520and%2520Haozhao%2520Wang%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520Federated%2520learning%2520%2528FL%2529%2520is%2520vulnerable%2520to%250AGradient%2520Inversion%2520Attacks%2520%2528GIA%2529%252C%2520which%2520can%2520recover%2520private%2520training%2520data%2520from%250Ashared%2520gradients.%2520However%252C%2520existing%2520methods%2520are%2520designed%2520for%2520dense%252C%2520continuous%250Adata%2520such%2520as%2520images%2520or%2520vectorized%2520texts%252C%2520and%2520cannot%2520be%2520directly%2520applied%2520to%250Asparse%2520and%2520discrete%2520graph%2520data.%2520This%2520paper%2520first%2520explores%2520GIA%2527s%2520impact%2520on%250AFederated%2520Graph%2520Learning%2520%2528FGL%2529%2520and%2520introduces%2520Graph%2520Inversion%2520from%2520Gradient%2520in%250AFederated%2520Learning%2520%2528FedGIG%2529%252C%2520a%2520novel%2520GIA%2520method%2520specifically%2520designed%2520for%250Agraph-structured%2520data.%2520FedGIG%2520includes%2520the%2520adjacency%2520matrix%2520constraining%250Amodule%252C%2520which%2520ensures%2520the%2520sparsity%2520and%2520discreteness%2520of%2520the%2520reconstructed%2520graph%250Adata%252C%2520and%2520the%2520subgraph%2520reconstruction%2520module%252C%2520which%2520is%2520designed%2520to%2520complete%250Amissing%2520common%2520subgraph%2520structures.%2520Extensive%2520experiments%2520on%2520molecular%2520datasets%250Ademonstrate%2520FedGIG%2527s%2520superior%2520accuracy%2520over%2520existing%2520GIA%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedGIG%3A%20Graph%20Inversion%20from%20Gradient%20in%20Federated%20Learning&entry.906535625=Tianzhe%20Xiao%20and%20Yichen%20Li%20and%20Yining%20Qi%20and%20Haozhao%20Wang%20and%20Ruixuan%20Li&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20Federated%20learning%20%28FL%29%20is%20vulnerable%20to%0AGradient%20Inversion%20Attacks%20%28GIA%29%2C%20which%20can%20recover%20private%20training%20data%20from%0Ashared%20gradients.%20However%2C%20existing%20methods%20are%20designed%20for%20dense%2C%20continuous%0Adata%20such%20as%20images%20or%20vectorized%20texts%2C%20and%20cannot%20be%20directly%20applied%20to%0Asparse%20and%20discrete%20graph%20data.%20This%20paper%20first%20explores%20GIA%27s%20impact%20on%0AFederated%20Graph%20Learning%20%28FGL%29%20and%20introduces%20Graph%20Inversion%20from%20Gradient%20in%0AFederated%20Learning%20%28FedGIG%29%2C%20a%20novel%20GIA%20method%20specifically%20designed%20for%0Agraph-structured%20data.%20FedGIG%20includes%20the%20adjacency%20matrix%20constraining%0Amodule%2C%20which%20ensures%20the%20sparsity%20and%20discreteness%20of%20the%20reconstructed%20graph%0Adata%2C%20and%20the%20subgraph%20reconstruction%20module%2C%20which%20is%20designed%20to%20complete%0Amissing%20common%20subgraph%20structures.%20Extensive%20experiments%20on%20molecular%20datasets%0Ademonstrate%20FedGIG%27s%20superior%20accuracy%20over%20existing%20GIA%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18513v1&entry.124074799=Read"},
{"title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering", "author": "Zhongjian Hu and Peng Yang and Bing Li and Zhenqi Wang", "abstract": "  Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.\n", "link": "http://arxiv.org/abs/2412.18351v1", "date": "2024-12-24", "relevancy": 2.1909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agents%20Based%20on%20Large%20Language%20Models%20for%20Knowledge-based%20Visual%0A%20%20Question%20Answering&body=Title%3A%20Multi-Agents%20Based%20on%20Large%20Language%20Models%20for%20Knowledge-based%20Visual%0A%20%20Question%20Answering%0AAuthor%3A%20Zhongjian%20Hu%20and%20Peng%20Yang%20and%20Bing%20Li%20and%20Zhenqi%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20results%20in%0Aknowledge-based%20Visual%20Question%20Answering%20%28VQA%29.%20However%20existing%20methods%20still%0Ahave%20challenges%3A%20the%20inability%20to%20use%20external%20tools%20autonomously%2C%20and%20the%0Ainability%20to%20work%20in%20teams.%20Humans%20tend%20to%20know%20whether%20they%20need%20to%20use%0Aexternal%20tools%20when%20they%20encounter%20a%20new%20question%2C%20e.g.%2C%20they%20tend%20to%20be%20able%0Ato%20give%20a%20direct%20answer%20to%20a%20familiar%20question%2C%20whereas%20they%20tend%20to%20use%20tools%0Asuch%20as%20search%20engines%20when%20they%20encounter%20an%20unfamiliar%20question.%20In%20addition%2C%0Ahumans%20also%20tend%20to%20collaborate%20and%20discuss%20with%20others%20to%20get%20better%20answers.%0AInspired%20by%20this%2C%20we%20propose%20the%20multi-agent%20voting%20framework.%20We%20design%20three%0ALLM-based%20agents%20that%20simulate%20different%20levels%20of%20staff%20in%20a%20team%2C%20and%20assign%0Athe%20available%20tools%20according%20to%20the%20levels.%20Each%20agent%20provides%20the%0Acorresponding%20answer%2C%20and%20finally%20all%20the%20answers%20provided%20by%20the%20agents%20are%0Avoted%20to%20get%20the%20final%20answer.%20Experiments%20on%20OK-VQA%20and%20A-OKVQA%20show%20that%20our%0Aapproach%20outperforms%20other%20baselines%20by%202.2%20and%201.0%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agents%2520Based%2520on%2520Large%2520Language%2520Models%2520for%2520Knowledge-based%2520Visual%250A%2520%2520Question%2520Answering%26entry.906535625%3DZhongjian%2520Hu%2520and%2520Peng%2520Yang%2520and%2520Bing%2520Li%2520and%2520Zhenqi%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520results%2520in%250Aknowledge-based%2520Visual%2520Question%2520Answering%2520%2528VQA%2529.%2520However%2520existing%2520methods%2520still%250Ahave%2520challenges%253A%2520the%2520inability%2520to%2520use%2520external%2520tools%2520autonomously%252C%2520and%2520the%250Ainability%2520to%2520work%2520in%2520teams.%2520Humans%2520tend%2520to%2520know%2520whether%2520they%2520need%2520to%2520use%250Aexternal%2520tools%2520when%2520they%2520encounter%2520a%2520new%2520question%252C%2520e.g.%252C%2520they%2520tend%2520to%2520be%2520able%250Ato%2520give%2520a%2520direct%2520answer%2520to%2520a%2520familiar%2520question%252C%2520whereas%2520they%2520tend%2520to%2520use%2520tools%250Asuch%2520as%2520search%2520engines%2520when%2520they%2520encounter%2520an%2520unfamiliar%2520question.%2520In%2520addition%252C%250Ahumans%2520also%2520tend%2520to%2520collaborate%2520and%2520discuss%2520with%2520others%2520to%2520get%2520better%2520answers.%250AInspired%2520by%2520this%252C%2520we%2520propose%2520the%2520multi-agent%2520voting%2520framework.%2520We%2520design%2520three%250ALLM-based%2520agents%2520that%2520simulate%2520different%2520levels%2520of%2520staff%2520in%2520a%2520team%252C%2520and%2520assign%250Athe%2520available%2520tools%2520according%2520to%2520the%2520levels.%2520Each%2520agent%2520provides%2520the%250Acorresponding%2520answer%252C%2520and%2520finally%2520all%2520the%2520answers%2520provided%2520by%2520the%2520agents%2520are%250Avoted%2520to%2520get%2520the%2520final%2520answer.%2520Experiments%2520on%2520OK-VQA%2520and%2520A-OKVQA%2520show%2520that%2520our%250Aapproach%2520outperforms%2520other%2520baselines%2520by%25202.2%2520and%25201.0%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agents%20Based%20on%20Large%20Language%20Models%20for%20Knowledge-based%20Visual%0A%20%20Question%20Answering&entry.906535625=Zhongjian%20Hu%20and%20Peng%20Yang%20and%20Bing%20Li%20and%20Zhenqi%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20results%20in%0Aknowledge-based%20Visual%20Question%20Answering%20%28VQA%29.%20However%20existing%20methods%20still%0Ahave%20challenges%3A%20the%20inability%20to%20use%20external%20tools%20autonomously%2C%20and%20the%0Ainability%20to%20work%20in%20teams.%20Humans%20tend%20to%20know%20whether%20they%20need%20to%20use%0Aexternal%20tools%20when%20they%20encounter%20a%20new%20question%2C%20e.g.%2C%20they%20tend%20to%20be%20able%0Ato%20give%20a%20direct%20answer%20to%20a%20familiar%20question%2C%20whereas%20they%20tend%20to%20use%20tools%0Asuch%20as%20search%20engines%20when%20they%20encounter%20an%20unfamiliar%20question.%20In%20addition%2C%0Ahumans%20also%20tend%20to%20collaborate%20and%20discuss%20with%20others%20to%20get%20better%20answers.%0AInspired%20by%20this%2C%20we%20propose%20the%20multi-agent%20voting%20framework.%20We%20design%20three%0ALLM-based%20agents%20that%20simulate%20different%20levels%20of%20staff%20in%20a%20team%2C%20and%20assign%0Athe%20available%20tools%20according%20to%20the%20levels.%20Each%20agent%20provides%20the%0Acorresponding%20answer%2C%20and%20finally%20all%20the%20answers%20provided%20by%20the%20agents%20are%0Avoted%20to%20get%20the%20final%20answer.%20Experiments%20on%20OK-VQA%20and%20A-OKVQA%20show%20that%20our%0Aapproach%20outperforms%20other%20baselines%20by%202.2%20and%201.0%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18351v1&entry.124074799=Read"},
{"title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent", "author": "Farhad Nooralahzadeh and Yi Zhang and Jonathan Furst and Kurt Stockinger", "abstract": "  International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs.\n", "link": "http://arxiv.org/abs/2412.18428v1", "date": "2024-12-24", "relevancy": 2.188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Multi-Modal%20Data%20Exploration%20in%20Natural%20Language%20via%20LLM%0A%20%20Agent&body=Title%3A%20Explainable%20Multi-Modal%20Data%20Exploration%20in%20Natural%20Language%20via%20LLM%0A%20%20Agent%0AAuthor%3A%20Farhad%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Jonathan%20Furst%20and%20Kurt%20Stockinger%0AAbstract%3A%20%20%20International%20enterprises%2C%20organizations%2C%20or%20hospitals%20collect%20large%20amounts%0Aof%20multi-modal%20data%20stored%20in%20databases%2C%20text%20documents%2C%20images%2C%20and%20videos.%0AWhile%20there%20has%20been%20recent%20progress%20in%20the%20separate%20fields%20of%20multi-modal%20data%0Aexploration%20as%20well%20as%20in%20database%20systems%20that%20automatically%20translate%20natural%0Alanguage%20questions%20to%20database%20query%20languages%2C%20the%20research%20challenge%20of%0Aquerying%20database%20systems%20combined%20with%20other%20unstructured%20modalities%20such%20as%0Aimages%20in%20natural%20language%20is%20widely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20propose%20XMODE%20-%20a%20system%20that%20enables%20explainable%2C%0Amulti-modal%20data%20exploration%20in%20natural%20language.%20Our%20approach%20is%20based%20on%20the%0Afollowing%20research%20contributions%3A%20%281%29%20Our%20system%20is%20inspired%20by%20a%20real-world%0Ause%20case%20that%20enables%20users%20to%20explore%20multi-modal%20information%20systems.%20%282%29%0AXMODE%20leverages%20a%20LLM-based%20agentic%20AI%20framework%20to%20decompose%20a%20natural%0Alanguage%20question%20into%20subtasks%20such%20as%20text-to-SQL%20generation%20and%20image%0Aanalysis.%20%283%29%20Experimental%20results%20on%20multi-modal%20datasets%20over%20relational%20data%0Aand%20images%20demonstrate%20that%20our%20system%20outperforms%20state-of-the-art%20multi-modal%0Aexploration%20systems%2C%20excelling%20not%20only%20in%20accuracy%20but%20also%20in%20various%0Aperformance%20metrics%20such%20as%20query%20latency%2C%20API%20costs%2C%20planning%20efficiency%2C%20and%0Aexplanation%20quality%2C%20thanks%20to%20the%20more%20effective%20utilization%20of%20the%20reasoning%0Acapabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Multi-Modal%2520Data%2520Exploration%2520in%2520Natural%2520Language%2520via%2520LLM%250A%2520%2520Agent%26entry.906535625%3DFarhad%2520Nooralahzadeh%2520and%2520Yi%2520Zhang%2520and%2520Jonathan%2520Furst%2520and%2520Kurt%2520Stockinger%26entry.1292438233%3D%2520%2520International%2520enterprises%252C%2520organizations%252C%2520or%2520hospitals%2520collect%2520large%2520amounts%250Aof%2520multi-modal%2520data%2520stored%2520in%2520databases%252C%2520text%2520documents%252C%2520images%252C%2520and%2520videos.%250AWhile%2520there%2520has%2520been%2520recent%2520progress%2520in%2520the%2520separate%2520fields%2520of%2520multi-modal%2520data%250Aexploration%2520as%2520well%2520as%2520in%2520database%2520systems%2520that%2520automatically%2520translate%2520natural%250Alanguage%2520questions%2520to%2520database%2520query%2520languages%252C%2520the%2520research%2520challenge%2520of%250Aquerying%2520database%2520systems%2520combined%2520with%2520other%2520unstructured%2520modalities%2520such%2520as%250Aimages%2520in%2520natural%2520language%2520is%2520widely%2520unexplored.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520XMODE%2520-%2520a%2520system%2520that%2520enables%2520explainable%252C%250Amulti-modal%2520data%2520exploration%2520in%2520natural%2520language.%2520Our%2520approach%2520is%2520based%2520on%2520the%250Afollowing%2520research%2520contributions%253A%2520%25281%2529%2520Our%2520system%2520is%2520inspired%2520by%2520a%2520real-world%250Ause%2520case%2520that%2520enables%2520users%2520to%2520explore%2520multi-modal%2520information%2520systems.%2520%25282%2529%250AXMODE%2520leverages%2520a%2520LLM-based%2520agentic%2520AI%2520framework%2520to%2520decompose%2520a%2520natural%250Alanguage%2520question%2520into%2520subtasks%2520such%2520as%2520text-to-SQL%2520generation%2520and%2520image%250Aanalysis.%2520%25283%2529%2520Experimental%2520results%2520on%2520multi-modal%2520datasets%2520over%2520relational%2520data%250Aand%2520images%2520demonstrate%2520that%2520our%2520system%2520outperforms%2520state-of-the-art%2520multi-modal%250Aexploration%2520systems%252C%2520excelling%2520not%2520only%2520in%2520accuracy%2520but%2520also%2520in%2520various%250Aperformance%2520metrics%2520such%2520as%2520query%2520latency%252C%2520API%2520costs%252C%2520planning%2520efficiency%252C%2520and%250Aexplanation%2520quality%252C%2520thanks%2520to%2520the%2520more%2520effective%2520utilization%2520of%2520the%2520reasoning%250Acapabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Multi-Modal%20Data%20Exploration%20in%20Natural%20Language%20via%20LLM%0A%20%20Agent&entry.906535625=Farhad%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Jonathan%20Furst%20and%20Kurt%20Stockinger&entry.1292438233=%20%20International%20enterprises%2C%20organizations%2C%20or%20hospitals%20collect%20large%20amounts%0Aof%20multi-modal%20data%20stored%20in%20databases%2C%20text%20documents%2C%20images%2C%20and%20videos.%0AWhile%20there%20has%20been%20recent%20progress%20in%20the%20separate%20fields%20of%20multi-modal%20data%0Aexploration%20as%20well%20as%20in%20database%20systems%20that%20automatically%20translate%20natural%0Alanguage%20questions%20to%20database%20query%20languages%2C%20the%20research%20challenge%20of%0Aquerying%20database%20systems%20combined%20with%20other%20unstructured%20modalities%20such%20as%0Aimages%20in%20natural%20language%20is%20widely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20propose%20XMODE%20-%20a%20system%20that%20enables%20explainable%2C%0Amulti-modal%20data%20exploration%20in%20natural%20language.%20Our%20approach%20is%20based%20on%20the%0Afollowing%20research%20contributions%3A%20%281%29%20Our%20system%20is%20inspired%20by%20a%20real-world%0Ause%20case%20that%20enables%20users%20to%20explore%20multi-modal%20information%20systems.%20%282%29%0AXMODE%20leverages%20a%20LLM-based%20agentic%20AI%20framework%20to%20decompose%20a%20natural%0Alanguage%20question%20into%20subtasks%20such%20as%20text-to-SQL%20generation%20and%20image%0Aanalysis.%20%283%29%20Experimental%20results%20on%20multi-modal%20datasets%20over%20relational%20data%0Aand%20images%20demonstrate%20that%20our%20system%20outperforms%20state-of-the-art%20multi-modal%0Aexploration%20systems%2C%20excelling%20not%20only%20in%20accuracy%20but%20also%20in%20various%0Aperformance%20metrics%20such%20as%20query%20latency%2C%20API%20costs%2C%20planning%20efficiency%2C%20and%0Aexplanation%20quality%2C%20thanks%20to%20the%20more%20effective%20utilization%20of%20the%20reasoning%0Acapabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18428v1&entry.124074799=Read"},
{"title": "Clutter Resilient Occlusion Avoidance for Tightly-Coupled\n  Motion-Assisted Detection", "author": "Zhixuan Xie and Jianjun Chen and Guoliang Li and Shuai Wang and Kejiang Ye and Yonina C. Eldar and Chengzhong Xu", "abstract": "  Occlusion is a key factor leading to detection failures. This paper proposes\na motion-assisted detection (MAD) method that actively plans an executable\npath, for the robot to observe the target at a new viewpoint with potentially\nreduced occlusion. In contrast to existing MAD approaches that may fail in\ncluttered environments, the proposed framework is robust in such scenarios,\ntherefore termed clutter resilient occlusion avoidance (CROA). The crux to CROA\nis to minimize the occlusion probability under polyhedron-based collision\navoidance constraints via the convex-concave procedure and duality-based\nbilevel optimization. The system implementation supports lidar-based MAD with\nintertwined execution of learning-based detection and optimization-based\nplanning. Experiments show that CROA outperforms various MAD schemes under a\nsparse convolutional neural network detector, in terms of point density,\nocclusion ratio, and detection error, in a multi-lane urban driving scenario.\n", "link": "http://arxiv.org/abs/2412.18453v1", "date": "2024-12-24", "relevancy": 2.1827, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5569}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clutter%20Resilient%20Occlusion%20Avoidance%20for%20Tightly-Coupled%0A%20%20Motion-Assisted%20Detection&body=Title%3A%20Clutter%20Resilient%20Occlusion%20Avoidance%20for%20Tightly-Coupled%0A%20%20Motion-Assisted%20Detection%0AAuthor%3A%20Zhixuan%20Xie%20and%20Jianjun%20Chen%20and%20Guoliang%20Li%20and%20Shuai%20Wang%20and%20Kejiang%20Ye%20and%20Yonina%20C.%20Eldar%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20Occlusion%20is%20a%20key%20factor%20leading%20to%20detection%20failures.%20This%20paper%20proposes%0Aa%20motion-assisted%20detection%20%28MAD%29%20method%20that%20actively%20plans%20an%20executable%0Apath%2C%20for%20the%20robot%20to%20observe%20the%20target%20at%20a%20new%20viewpoint%20with%20potentially%0Areduced%20occlusion.%20In%20contrast%20to%20existing%20MAD%20approaches%20that%20may%20fail%20in%0Acluttered%20environments%2C%20the%20proposed%20framework%20is%20robust%20in%20such%20scenarios%2C%0Atherefore%20termed%20clutter%20resilient%20occlusion%20avoidance%20%28CROA%29.%20The%20crux%20to%20CROA%0Ais%20to%20minimize%20the%20occlusion%20probability%20under%20polyhedron-based%20collision%0Aavoidance%20constraints%20via%20the%20convex-concave%20procedure%20and%20duality-based%0Abilevel%20optimization.%20The%20system%20implementation%20supports%20lidar-based%20MAD%20with%0Aintertwined%20execution%20of%20learning-based%20detection%20and%20optimization-based%0Aplanning.%20Experiments%20show%20that%20CROA%20outperforms%20various%20MAD%20schemes%20under%20a%0Asparse%20convolutional%20neural%20network%20detector%2C%20in%20terms%20of%20point%20density%2C%0Aocclusion%20ratio%2C%20and%20detection%20error%2C%20in%20a%20multi-lane%20urban%20driving%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClutter%2520Resilient%2520Occlusion%2520Avoidance%2520for%2520Tightly-Coupled%250A%2520%2520Motion-Assisted%2520Detection%26entry.906535625%3DZhixuan%2520Xie%2520and%2520Jianjun%2520Chen%2520and%2520Guoliang%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Kejiang%2520Ye%2520and%2520Yonina%2520C.%2520Eldar%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520Occlusion%2520is%2520a%2520key%2520factor%2520leading%2520to%2520detection%2520failures.%2520This%2520paper%2520proposes%250Aa%2520motion-assisted%2520detection%2520%2528MAD%2529%2520method%2520that%2520actively%2520plans%2520an%2520executable%250Apath%252C%2520for%2520the%2520robot%2520to%2520observe%2520the%2520target%2520at%2520a%2520new%2520viewpoint%2520with%2520potentially%250Areduced%2520occlusion.%2520In%2520contrast%2520to%2520existing%2520MAD%2520approaches%2520that%2520may%2520fail%2520in%250Acluttered%2520environments%252C%2520the%2520proposed%2520framework%2520is%2520robust%2520in%2520such%2520scenarios%252C%250Atherefore%2520termed%2520clutter%2520resilient%2520occlusion%2520avoidance%2520%2528CROA%2529.%2520The%2520crux%2520to%2520CROA%250Ais%2520to%2520minimize%2520the%2520occlusion%2520probability%2520under%2520polyhedron-based%2520collision%250Aavoidance%2520constraints%2520via%2520the%2520convex-concave%2520procedure%2520and%2520duality-based%250Abilevel%2520optimization.%2520The%2520system%2520implementation%2520supports%2520lidar-based%2520MAD%2520with%250Aintertwined%2520execution%2520of%2520learning-based%2520detection%2520and%2520optimization-based%250Aplanning.%2520Experiments%2520show%2520that%2520CROA%2520outperforms%2520various%2520MAD%2520schemes%2520under%2520a%250Asparse%2520convolutional%2520neural%2520network%2520detector%252C%2520in%2520terms%2520of%2520point%2520density%252C%250Aocclusion%2520ratio%252C%2520and%2520detection%2520error%252C%2520in%2520a%2520multi-lane%2520urban%2520driving%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clutter%20Resilient%20Occlusion%20Avoidance%20for%20Tightly-Coupled%0A%20%20Motion-Assisted%20Detection&entry.906535625=Zhixuan%20Xie%20and%20Jianjun%20Chen%20and%20Guoliang%20Li%20and%20Shuai%20Wang%20and%20Kejiang%20Ye%20and%20Yonina%20C.%20Eldar%20and%20Chengzhong%20Xu&entry.1292438233=%20%20Occlusion%20is%20a%20key%20factor%20leading%20to%20detection%20failures.%20This%20paper%20proposes%0Aa%20motion-assisted%20detection%20%28MAD%29%20method%20that%20actively%20plans%20an%20executable%0Apath%2C%20for%20the%20robot%20to%20observe%20the%20target%20at%20a%20new%20viewpoint%20with%20potentially%0Areduced%20occlusion.%20In%20contrast%20to%20existing%20MAD%20approaches%20that%20may%20fail%20in%0Acluttered%20environments%2C%20the%20proposed%20framework%20is%20robust%20in%20such%20scenarios%2C%0Atherefore%20termed%20clutter%20resilient%20occlusion%20avoidance%20%28CROA%29.%20The%20crux%20to%20CROA%0Ais%20to%20minimize%20the%20occlusion%20probability%20under%20polyhedron-based%20collision%0Aavoidance%20constraints%20via%20the%20convex-concave%20procedure%20and%20duality-based%0Abilevel%20optimization.%20The%20system%20implementation%20supports%20lidar-based%20MAD%20with%0Aintertwined%20execution%20of%20learning-based%20detection%20and%20optimization-based%0Aplanning.%20Experiments%20show%20that%20CROA%20outperforms%20various%20MAD%20schemes%20under%20a%0Asparse%20convolutional%20neural%20network%20detector%2C%20in%20terms%20of%20point%20density%2C%0Aocclusion%20ratio%2C%20and%20detection%20error%2C%20in%20a%20multi-lane%20urban%20driving%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18453v1&entry.124074799=Read"},
{"title": "Guided Real Image Dehazing using YCbCr Color Space", "author": "Wenxuan Fang and Junkai Fan and Yu Zheng and Jiangwei Weng and Ying Tai and Jun Li", "abstract": "  Image dehazing, particularly with learning-based methods, has gained\nsignificant attention due to its importance in real-world applications.\nHowever, relying solely on the RGB color space often fall short, frequently\nleaving residual haze. This arises from two main issues: the difficulty in\nobtaining clear textural features from hazy RGB images and the complexity of\nacquiring real haze/clean image pairs outside controlled environments like\nsmoke-filled scenes. To address these issues, we first propose a novel\nStructure Guided Dehazing Network (SGDN) that leverages the superior structural\nproperties of YCbCr features over RGB. It comprises two key modules: Bi-Color\nGuidance Bridge (BGB) and Color Enhancement Module (CEM). BGB integrates a\nphase integration module and an interactive attention module, utilizing the\nrich texture features of the YCbCr space to guide the RGB space, thereby\nrecovering clearer features in both frequency and spatial domains. To maintain\ntonal consistency, CEM further enhances the color perception of RGB features by\naggregating YCbCr channel information. Furthermore, for effective supervised\nlearning, we introduce a Real-World Well-Aligned Haze (RW$^2$AH) dataset, which\nincludes a diverse range of scenes from various geographical regions and\nclimate conditions. Experimental results demonstrate that our method surpasses\nexisting state-of-the-art methods across multiple real-world smoke/haze\ndatasets. Code and Dataset:\n\\textcolor{blue}{\\url{https://github.com/fiwy0527/AAAI25_SGDN.}}\n", "link": "http://arxiv.org/abs/2412.17496v2", "date": "2024-12-24", "relevancy": 2.1728, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5581}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5496}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space&body=Title%3A%20Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space%0AAuthor%3A%20Wenxuan%20Fang%20and%20Junkai%20Fan%20and%20Yu%20Zheng%20and%20Jiangwei%20Weng%20and%20Ying%20Tai%20and%20Jun%20Li%0AAbstract%3A%20%20%20Image%20dehazing%2C%20particularly%20with%20learning-based%20methods%2C%20has%20gained%0Asignificant%20attention%20due%20to%20its%20importance%20in%20real-world%20applications.%0AHowever%2C%20relying%20solely%20on%20the%20RGB%20color%20space%20often%20fall%20short%2C%20frequently%0Aleaving%20residual%20haze.%20This%20arises%20from%20two%20main%20issues%3A%20the%20difficulty%20in%0Aobtaining%20clear%20textural%20features%20from%20hazy%20RGB%20images%20and%20the%20complexity%20of%0Aacquiring%20real%20haze/clean%20image%20pairs%20outside%20controlled%20environments%20like%0Asmoke-filled%20scenes.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%0AStructure%20Guided%20Dehazing%20Network%20%28SGDN%29%20that%20leverages%20the%20superior%20structural%0Aproperties%20of%20YCbCr%20features%20over%20RGB.%20It%20comprises%20two%20key%20modules%3A%20Bi-Color%0AGuidance%20Bridge%20%28BGB%29%20and%20Color%20Enhancement%20Module%20%28CEM%29.%20BGB%20integrates%20a%0Aphase%20integration%20module%20and%20an%20interactive%20attention%20module%2C%20utilizing%20the%0Arich%20texture%20features%20of%20the%20YCbCr%20space%20to%20guide%20the%20RGB%20space%2C%20thereby%0Arecovering%20clearer%20features%20in%20both%20frequency%20and%20spatial%20domains.%20To%20maintain%0Atonal%20consistency%2C%20CEM%20further%20enhances%20the%20color%20perception%20of%20RGB%20features%20by%0Aaggregating%20YCbCr%20channel%20information.%20Furthermore%2C%20for%20effective%20supervised%0Alearning%2C%20we%20introduce%20a%20Real-World%20Well-Aligned%20Haze%20%28RW%24%5E2%24AH%29%20dataset%2C%20which%0Aincludes%20a%20diverse%20range%20of%20scenes%20from%20various%20geographical%20regions%20and%0Aclimate%20conditions.%20Experimental%20results%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20state-of-the-art%20methods%20across%20multiple%20real-world%20smoke/haze%0Adatasets.%20Code%20and%20Dataset%3A%0A%5Ctextcolor%7Bblue%7D%7B%5Curl%7Bhttps%3A//github.com/fiwy0527/AAAI25_SGDN.%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Real%2520Image%2520Dehazing%2520using%2520YCbCr%2520Color%2520Space%26entry.906535625%3DWenxuan%2520Fang%2520and%2520Junkai%2520Fan%2520and%2520Yu%2520Zheng%2520and%2520Jiangwei%2520Weng%2520and%2520Ying%2520Tai%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Image%2520dehazing%252C%2520particularly%2520with%2520learning-based%2520methods%252C%2520has%2520gained%250Asignificant%2520attention%2520due%2520to%2520its%2520importance%2520in%2520real-world%2520applications.%250AHowever%252C%2520relying%2520solely%2520on%2520the%2520RGB%2520color%2520space%2520often%2520fall%2520short%252C%2520frequently%250Aleaving%2520residual%2520haze.%2520This%2520arises%2520from%2520two%2520main%2520issues%253A%2520the%2520difficulty%2520in%250Aobtaining%2520clear%2520textural%2520features%2520from%2520hazy%2520RGB%2520images%2520and%2520the%2520complexity%2520of%250Aacquiring%2520real%2520haze/clean%2520image%2520pairs%2520outside%2520controlled%2520environments%2520like%250Asmoke-filled%2520scenes.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520novel%250AStructure%2520Guided%2520Dehazing%2520Network%2520%2528SGDN%2529%2520that%2520leverages%2520the%2520superior%2520structural%250Aproperties%2520of%2520YCbCr%2520features%2520over%2520RGB.%2520It%2520comprises%2520two%2520key%2520modules%253A%2520Bi-Color%250AGuidance%2520Bridge%2520%2528BGB%2529%2520and%2520Color%2520Enhancement%2520Module%2520%2528CEM%2529.%2520BGB%2520integrates%2520a%250Aphase%2520integration%2520module%2520and%2520an%2520interactive%2520attention%2520module%252C%2520utilizing%2520the%250Arich%2520texture%2520features%2520of%2520the%2520YCbCr%2520space%2520to%2520guide%2520the%2520RGB%2520space%252C%2520thereby%250Arecovering%2520clearer%2520features%2520in%2520both%2520frequency%2520and%2520spatial%2520domains.%2520To%2520maintain%250Atonal%2520consistency%252C%2520CEM%2520further%2520enhances%2520the%2520color%2520perception%2520of%2520RGB%2520features%2520by%250Aaggregating%2520YCbCr%2520channel%2520information.%2520Furthermore%252C%2520for%2520effective%2520supervised%250Alearning%252C%2520we%2520introduce%2520a%2520Real-World%2520Well-Aligned%2520Haze%2520%2528RW%2524%255E2%2524AH%2529%2520dataset%252C%2520which%250Aincludes%2520a%2520diverse%2520range%2520of%2520scenes%2520from%2520various%2520geographical%2520regions%2520and%250Aclimate%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520surpasses%250Aexisting%2520state-of-the-art%2520methods%2520across%2520multiple%2520real-world%2520smoke/haze%250Adatasets.%2520Code%2520and%2520Dataset%253A%250A%255Ctextcolor%257Bblue%257D%257B%255Curl%257Bhttps%253A//github.com/fiwy0527/AAAI25_SGDN.%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space&entry.906535625=Wenxuan%20Fang%20and%20Junkai%20Fan%20and%20Yu%20Zheng%20and%20Jiangwei%20Weng%20and%20Ying%20Tai%20and%20Jun%20Li&entry.1292438233=%20%20Image%20dehazing%2C%20particularly%20with%20learning-based%20methods%2C%20has%20gained%0Asignificant%20attention%20due%20to%20its%20importance%20in%20real-world%20applications.%0AHowever%2C%20relying%20solely%20on%20the%20RGB%20color%20space%20often%20fall%20short%2C%20frequently%0Aleaving%20residual%20haze.%20This%20arises%20from%20two%20main%20issues%3A%20the%20difficulty%20in%0Aobtaining%20clear%20textural%20features%20from%20hazy%20RGB%20images%20and%20the%20complexity%20of%0Aacquiring%20real%20haze/clean%20image%20pairs%20outside%20controlled%20environments%20like%0Asmoke-filled%20scenes.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%0AStructure%20Guided%20Dehazing%20Network%20%28SGDN%29%20that%20leverages%20the%20superior%20structural%0Aproperties%20of%20YCbCr%20features%20over%20RGB.%20It%20comprises%20two%20key%20modules%3A%20Bi-Color%0AGuidance%20Bridge%20%28BGB%29%20and%20Color%20Enhancement%20Module%20%28CEM%29.%20BGB%20integrates%20a%0Aphase%20integration%20module%20and%20an%20interactive%20attention%20module%2C%20utilizing%20the%0Arich%20texture%20features%20of%20the%20YCbCr%20space%20to%20guide%20the%20RGB%20space%2C%20thereby%0Arecovering%20clearer%20features%20in%20both%20frequency%20and%20spatial%20domains.%20To%20maintain%0Atonal%20consistency%2C%20CEM%20further%20enhances%20the%20color%20perception%20of%20RGB%20features%20by%0Aaggregating%20YCbCr%20channel%20information.%20Furthermore%2C%20for%20effective%20supervised%0Alearning%2C%20we%20introduce%20a%20Real-World%20Well-Aligned%20Haze%20%28RW%24%5E2%24AH%29%20dataset%2C%20which%0Aincludes%20a%20diverse%20range%20of%20scenes%20from%20various%20geographical%20regions%20and%0Aclimate%20conditions.%20Experimental%20results%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20state-of-the-art%20methods%20across%20multiple%20real-world%20smoke/haze%0Adatasets.%20Code%20and%20Dataset%3A%0A%5Ctextcolor%7Bblue%7D%7B%5Curl%7Bhttps%3A//github.com/fiwy0527/AAAI25_SGDN.%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17496v2&entry.124074799=Read"},
{"title": "HTR-JAND: Handwritten Text Recognition with Joint Attention Network and\n  Knowledge Distillation", "author": "Mohammed Hamdan and Abderrahmane Rahiche and Mohamed Cheriet", "abstract": "  Despite significant advances in deep learning, current Handwritten Text\nRecognition (HTR) systems struggle with the inherent complexity of historical\ndocuments, including diverse writing styles, degraded text quality, and\ncomputational efficiency requirements across multiple languages and time\nperiods. This paper introduces HTR-JAND (HTR-JAND: Handwritten Text Recognition\nwith Joint Attention Network and Knowledge Distillation), an efficient HTR\nframework that combines advanced feature extraction with knowledge\ndistillation. Our architecture incorporates three key components: (1) a CNN\narchitecture integrating FullGatedConv2d layers with Squeeze-and-Excitation\nblocks for adaptive feature extraction, (2) a Combined Attention mechanism\nfusing Multi-Head Self-Attention with Proxima Attention for robust sequence\nmodeling, and (3) a Knowledge Distillation framework enabling efficient model\ncompression while preserving accuracy through curriculum-based training. The\nHTR-JAND framework implements a multi-stage training approach combining\ncurriculum learning, synthetic data generation, and multi-task learning for\ncross-dataset knowledge transfer. We enhance recognition accuracy through\ncontext-aware T5 post-processing, particularly effective for historical\ndocuments. Comprehensive evaluations demonstrate HTR-JAND's effectiveness,\nachieving state-of-the-art Character Error Rates (CER) of 1.23\\%, 1.02\\%, and\n2.02\\% on IAM, RIMES, and Bentham datasets respectively. Our Student model\nachieves a 48\\% parameter reduction (0.75M versus 1.5M parameters) while\nmaintaining competitive performance through efficient knowledge transfer.\nSource code and pre-trained models are available at\n\\href{https://github.com/DocumentRecognitionModels/HTR-JAND}{Github}.\n", "link": "http://arxiv.org/abs/2412.18524v1", "date": "2024-12-24", "relevancy": 2.1624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5417}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HTR-JAND%3A%20Handwritten%20Text%20Recognition%20with%20Joint%20Attention%20Network%20and%0A%20%20Knowledge%20Distillation&body=Title%3A%20HTR-JAND%3A%20Handwritten%20Text%20Recognition%20with%20Joint%20Attention%20Network%20and%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Mohammed%20Hamdan%20and%20Abderrahmane%20Rahiche%20and%20Mohamed%20Cheriet%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20deep%20learning%2C%20current%20Handwritten%20Text%0ARecognition%20%28HTR%29%20systems%20struggle%20with%20the%20inherent%20complexity%20of%20historical%0Adocuments%2C%20including%20diverse%20writing%20styles%2C%20degraded%20text%20quality%2C%20and%0Acomputational%20efficiency%20requirements%20across%20multiple%20languages%20and%20time%0Aperiods.%20This%20paper%20introduces%20HTR-JAND%20%28HTR-JAND%3A%20Handwritten%20Text%20Recognition%0Awith%20Joint%20Attention%20Network%20and%20Knowledge%20Distillation%29%2C%20an%20efficient%20HTR%0Aframework%20that%20combines%20advanced%20feature%20extraction%20with%20knowledge%0Adistillation.%20Our%20architecture%20incorporates%20three%20key%20components%3A%20%281%29%20a%20CNN%0Aarchitecture%20integrating%20FullGatedConv2d%20layers%20with%20Squeeze-and-Excitation%0Ablocks%20for%20adaptive%20feature%20extraction%2C%20%282%29%20a%20Combined%20Attention%20mechanism%0Afusing%20Multi-Head%20Self-Attention%20with%20Proxima%20Attention%20for%20robust%20sequence%0Amodeling%2C%20and%20%283%29%20a%20Knowledge%20Distillation%20framework%20enabling%20efficient%20model%0Acompression%20while%20preserving%20accuracy%20through%20curriculum-based%20training.%20The%0AHTR-JAND%20framework%20implements%20a%20multi-stage%20training%20approach%20combining%0Acurriculum%20learning%2C%20synthetic%20data%20generation%2C%20and%20multi-task%20learning%20for%0Across-dataset%20knowledge%20transfer.%20We%20enhance%20recognition%20accuracy%20through%0Acontext-aware%20T5%20post-processing%2C%20particularly%20effective%20for%20historical%0Adocuments.%20Comprehensive%20evaluations%20demonstrate%20HTR-JAND%27s%20effectiveness%2C%0Aachieving%20state-of-the-art%20Character%20Error%20Rates%20%28CER%29%20of%201.23%5C%25%2C%201.02%5C%25%2C%20and%0A2.02%5C%25%20on%20IAM%2C%20RIMES%2C%20and%20Bentham%20datasets%20respectively.%20Our%20Student%20model%0Aachieves%20a%2048%5C%25%20parameter%20reduction%20%280.75M%20versus%201.5M%20parameters%29%20while%0Amaintaining%20competitive%20performance%20through%20efficient%20knowledge%20transfer.%0ASource%20code%20and%20pre-trained%20models%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/DocumentRecognitionModels/HTR-JAND%7D%7BGithub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHTR-JAND%253A%2520Handwritten%2520Text%2520Recognition%2520with%2520Joint%2520Attention%2520Network%2520and%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DMohammed%2520Hamdan%2520and%2520Abderrahmane%2520Rahiche%2520and%2520Mohamed%2520Cheriet%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520deep%2520learning%252C%2520current%2520Handwritten%2520Text%250ARecognition%2520%2528HTR%2529%2520systems%2520struggle%2520with%2520the%2520inherent%2520complexity%2520of%2520historical%250Adocuments%252C%2520including%2520diverse%2520writing%2520styles%252C%2520degraded%2520text%2520quality%252C%2520and%250Acomputational%2520efficiency%2520requirements%2520across%2520multiple%2520languages%2520and%2520time%250Aperiods.%2520This%2520paper%2520introduces%2520HTR-JAND%2520%2528HTR-JAND%253A%2520Handwritten%2520Text%2520Recognition%250Awith%2520Joint%2520Attention%2520Network%2520and%2520Knowledge%2520Distillation%2529%252C%2520an%2520efficient%2520HTR%250Aframework%2520that%2520combines%2520advanced%2520feature%2520extraction%2520with%2520knowledge%250Adistillation.%2520Our%2520architecture%2520incorporates%2520three%2520key%2520components%253A%2520%25281%2529%2520a%2520CNN%250Aarchitecture%2520integrating%2520FullGatedConv2d%2520layers%2520with%2520Squeeze-and-Excitation%250Ablocks%2520for%2520adaptive%2520feature%2520extraction%252C%2520%25282%2529%2520a%2520Combined%2520Attention%2520mechanism%250Afusing%2520Multi-Head%2520Self-Attention%2520with%2520Proxima%2520Attention%2520for%2520robust%2520sequence%250Amodeling%252C%2520and%2520%25283%2529%2520a%2520Knowledge%2520Distillation%2520framework%2520enabling%2520efficient%2520model%250Acompression%2520while%2520preserving%2520accuracy%2520through%2520curriculum-based%2520training.%2520The%250AHTR-JAND%2520framework%2520implements%2520a%2520multi-stage%2520training%2520approach%2520combining%250Acurriculum%2520learning%252C%2520synthetic%2520data%2520generation%252C%2520and%2520multi-task%2520learning%2520for%250Across-dataset%2520knowledge%2520transfer.%2520We%2520enhance%2520recognition%2520accuracy%2520through%250Acontext-aware%2520T5%2520post-processing%252C%2520particularly%2520effective%2520for%2520historical%250Adocuments.%2520Comprehensive%2520evaluations%2520demonstrate%2520HTR-JAND%2527s%2520effectiveness%252C%250Aachieving%2520state-of-the-art%2520Character%2520Error%2520Rates%2520%2528CER%2529%2520of%25201.23%255C%2525%252C%25201.02%255C%2525%252C%2520and%250A2.02%255C%2525%2520on%2520IAM%252C%2520RIMES%252C%2520and%2520Bentham%2520datasets%2520respectively.%2520Our%2520Student%2520model%250Aachieves%2520a%252048%255C%2525%2520parameter%2520reduction%2520%25280.75M%2520versus%25201.5M%2520parameters%2529%2520while%250Amaintaining%2520competitive%2520performance%2520through%2520efficient%2520knowledge%2520transfer.%250ASource%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/DocumentRecognitionModels/HTR-JAND%257D%257BGithub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HTR-JAND%3A%20Handwritten%20Text%20Recognition%20with%20Joint%20Attention%20Network%20and%0A%20%20Knowledge%20Distillation&entry.906535625=Mohammed%20Hamdan%20and%20Abderrahmane%20Rahiche%20and%20Mohamed%20Cheriet&entry.1292438233=%20%20Despite%20significant%20advances%20in%20deep%20learning%2C%20current%20Handwritten%20Text%0ARecognition%20%28HTR%29%20systems%20struggle%20with%20the%20inherent%20complexity%20of%20historical%0Adocuments%2C%20including%20diverse%20writing%20styles%2C%20degraded%20text%20quality%2C%20and%0Acomputational%20efficiency%20requirements%20across%20multiple%20languages%20and%20time%0Aperiods.%20This%20paper%20introduces%20HTR-JAND%20%28HTR-JAND%3A%20Handwritten%20Text%20Recognition%0Awith%20Joint%20Attention%20Network%20and%20Knowledge%20Distillation%29%2C%20an%20efficient%20HTR%0Aframework%20that%20combines%20advanced%20feature%20extraction%20with%20knowledge%0Adistillation.%20Our%20architecture%20incorporates%20three%20key%20components%3A%20%281%29%20a%20CNN%0Aarchitecture%20integrating%20FullGatedConv2d%20layers%20with%20Squeeze-and-Excitation%0Ablocks%20for%20adaptive%20feature%20extraction%2C%20%282%29%20a%20Combined%20Attention%20mechanism%0Afusing%20Multi-Head%20Self-Attention%20with%20Proxima%20Attention%20for%20robust%20sequence%0Amodeling%2C%20and%20%283%29%20a%20Knowledge%20Distillation%20framework%20enabling%20efficient%20model%0Acompression%20while%20preserving%20accuracy%20through%20curriculum-based%20training.%20The%0AHTR-JAND%20framework%20implements%20a%20multi-stage%20training%20approach%20combining%0Acurriculum%20learning%2C%20synthetic%20data%20generation%2C%20and%20multi-task%20learning%20for%0Across-dataset%20knowledge%20transfer.%20We%20enhance%20recognition%20accuracy%20through%0Acontext-aware%20T5%20post-processing%2C%20particularly%20effective%20for%20historical%0Adocuments.%20Comprehensive%20evaluations%20demonstrate%20HTR-JAND%27s%20effectiveness%2C%0Aachieving%20state-of-the-art%20Character%20Error%20Rates%20%28CER%29%20of%201.23%5C%25%2C%201.02%5C%25%2C%20and%0A2.02%5C%25%20on%20IAM%2C%20RIMES%2C%20and%20Bentham%20datasets%20respectively.%20Our%20Student%20model%0Aachieves%20a%2048%5C%25%20parameter%20reduction%20%280.75M%20versus%201.5M%20parameters%29%20while%0Amaintaining%20competitive%20performance%20through%20efficient%20knowledge%20transfer.%0ASource%20code%20and%20pre-trained%20models%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/DocumentRecognitionModels/HTR-JAND%7D%7BGithub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18524v1&entry.124074799=Read"},
{"title": "Advancing Deformable Medical Image Registration with Multi-axis\n  Cross-covariance Attention", "author": "Mingyuan Meng and Michael Fulham and Lei Bi and Jinman Kim", "abstract": "  Deformable image registration is a fundamental requirement for medical image\nanalysis. Recently, transformers have been widely used in deep learning-based\nregistration methods for their ability to capture long-range dependency via\nself-attention (SA). However, the high computation and memory loads of SA\n(growing quadratically with the spatial resolution) hinder transformers from\nprocessing subtle textural information in high-resolution image features, e.g.,\nat the full and half image resolutions. This limits deformable registration as\nthe high-resolution textural information is crucial for finding precise\npixel-wise correspondence between subtle anatomical structures.\nCross-covariance Attention (XCA), as a \"transposed\" version of SA that operates\nacross feature channels, has complexity growing linearly with the spatial\nresolution, providing the feasibility of capturing long-range dependency among\nhigh-resolution image features. However, existing XCA-based transformers merely\ncapture coarse global long-range dependency, which are unsuitable for\ndeformable image registration relying primarily on fine-grained local\ncorrespondence. In this study, we propose to improve existing deep\nlearning-based registration methods by embedding a new XCA mechanism. To this\nend, we design an XCA-based transformer block optimized for deformable medical\nimage registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general\nnetwork block that can be embedded into various registration network\narchitectures. It can capture both global and local long-range dependency among\nhigh-resolution image features by applying regional and dilated XCA in parallel\nvia a multi-axis design. Extensive experiments on two well-benchmarked\ninter-/intra-patient registration tasks with seven public medical datasets\ndemonstrate that our MAXCA block enables state-of-the-art registration\nperformance.\n", "link": "http://arxiv.org/abs/2412.18545v1", "date": "2024-12-24", "relevancy": 2.1561, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5307}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Deformable%20Medical%20Image%20Registration%20with%20Multi-axis%0A%20%20Cross-covariance%20Attention&body=Title%3A%20Advancing%20Deformable%20Medical%20Image%20Registration%20with%20Multi-axis%0A%20%20Cross-covariance%20Attention%0AAuthor%3A%20Mingyuan%20Meng%20and%20Michael%20Fulham%20and%20Lei%20Bi%20and%20Jinman%20Kim%0AAbstract%3A%20%20%20Deformable%20image%20registration%20is%20a%20fundamental%20requirement%20for%20medical%20image%0Aanalysis.%20Recently%2C%20transformers%20have%20been%20widely%20used%20in%20deep%20learning-based%0Aregistration%20methods%20for%20their%20ability%20to%20capture%20long-range%20dependency%20via%0Aself-attention%20%28SA%29.%20However%2C%20the%20high%20computation%20and%20memory%20loads%20of%20SA%0A%28growing%20quadratically%20with%20the%20spatial%20resolution%29%20hinder%20transformers%20from%0Aprocessing%20subtle%20textural%20information%20in%20high-resolution%20image%20features%2C%20e.g.%2C%0Aat%20the%20full%20and%20half%20image%20resolutions.%20This%20limits%20deformable%20registration%20as%0Athe%20high-resolution%20textural%20information%20is%20crucial%20for%20finding%20precise%0Apixel-wise%20correspondence%20between%20subtle%20anatomical%20structures.%0ACross-covariance%20Attention%20%28XCA%29%2C%20as%20a%20%22transposed%22%20version%20of%20SA%20that%20operates%0Aacross%20feature%20channels%2C%20has%20complexity%20growing%20linearly%20with%20the%20spatial%0Aresolution%2C%20providing%20the%20feasibility%20of%20capturing%20long-range%20dependency%20among%0Ahigh-resolution%20image%20features.%20However%2C%20existing%20XCA-based%20transformers%20merely%0Acapture%20coarse%20global%20long-range%20dependency%2C%20which%20are%20unsuitable%20for%0Adeformable%20image%20registration%20relying%20primarily%20on%20fine-grained%20local%0Acorrespondence.%20In%20this%20study%2C%20we%20propose%20to%20improve%20existing%20deep%0Alearning-based%20registration%20methods%20by%20embedding%20a%20new%20XCA%20mechanism.%20To%20this%0Aend%2C%20we%20design%20an%20XCA-based%20transformer%20block%20optimized%20for%20deformable%20medical%0Aimage%20registration%2C%20named%20Multi-Axis%20XCA%20%28MAXCA%29.%20Our%20MAXCA%20serves%20as%20a%20general%0Anetwork%20block%20that%20can%20be%20embedded%20into%20various%20registration%20network%0Aarchitectures.%20It%20can%20capture%20both%20global%20and%20local%20long-range%20dependency%20among%0Ahigh-resolution%20image%20features%20by%20applying%20regional%20and%20dilated%20XCA%20in%20parallel%0Avia%20a%20multi-axis%20design.%20Extensive%20experiments%20on%20two%20well-benchmarked%0Ainter-/intra-patient%20registration%20tasks%20with%20seven%20public%20medical%20datasets%0Ademonstrate%20that%20our%20MAXCA%20block%20enables%20state-of-the-art%20registration%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Deformable%2520Medical%2520Image%2520Registration%2520with%2520Multi-axis%250A%2520%2520Cross-covariance%2520Attention%26entry.906535625%3DMingyuan%2520Meng%2520and%2520Michael%2520Fulham%2520and%2520Lei%2520Bi%2520and%2520Jinman%2520Kim%26entry.1292438233%3D%2520%2520Deformable%2520image%2520registration%2520is%2520a%2520fundamental%2520requirement%2520for%2520medical%2520image%250Aanalysis.%2520Recently%252C%2520transformers%2520have%2520been%2520widely%2520used%2520in%2520deep%2520learning-based%250Aregistration%2520methods%2520for%2520their%2520ability%2520to%2520capture%2520long-range%2520dependency%2520via%250Aself-attention%2520%2528SA%2529.%2520However%252C%2520the%2520high%2520computation%2520and%2520memory%2520loads%2520of%2520SA%250A%2528growing%2520quadratically%2520with%2520the%2520spatial%2520resolution%2529%2520hinder%2520transformers%2520from%250Aprocessing%2520subtle%2520textural%2520information%2520in%2520high-resolution%2520image%2520features%252C%2520e.g.%252C%250Aat%2520the%2520full%2520and%2520half%2520image%2520resolutions.%2520This%2520limits%2520deformable%2520registration%2520as%250Athe%2520high-resolution%2520textural%2520information%2520is%2520crucial%2520for%2520finding%2520precise%250Apixel-wise%2520correspondence%2520between%2520subtle%2520anatomical%2520structures.%250ACross-covariance%2520Attention%2520%2528XCA%2529%252C%2520as%2520a%2520%2522transposed%2522%2520version%2520of%2520SA%2520that%2520operates%250Aacross%2520feature%2520channels%252C%2520has%2520complexity%2520growing%2520linearly%2520with%2520the%2520spatial%250Aresolution%252C%2520providing%2520the%2520feasibility%2520of%2520capturing%2520long-range%2520dependency%2520among%250Ahigh-resolution%2520image%2520features.%2520However%252C%2520existing%2520XCA-based%2520transformers%2520merely%250Acapture%2520coarse%2520global%2520long-range%2520dependency%252C%2520which%2520are%2520unsuitable%2520for%250Adeformable%2520image%2520registration%2520relying%2520primarily%2520on%2520fine-grained%2520local%250Acorrespondence.%2520In%2520this%2520study%252C%2520we%2520propose%2520to%2520improve%2520existing%2520deep%250Alearning-based%2520registration%2520methods%2520by%2520embedding%2520a%2520new%2520XCA%2520mechanism.%2520To%2520this%250Aend%252C%2520we%2520design%2520an%2520XCA-based%2520transformer%2520block%2520optimized%2520for%2520deformable%2520medical%250Aimage%2520registration%252C%2520named%2520Multi-Axis%2520XCA%2520%2528MAXCA%2529.%2520Our%2520MAXCA%2520serves%2520as%2520a%2520general%250Anetwork%2520block%2520that%2520can%2520be%2520embedded%2520into%2520various%2520registration%2520network%250Aarchitectures.%2520It%2520can%2520capture%2520both%2520global%2520and%2520local%2520long-range%2520dependency%2520among%250Ahigh-resolution%2520image%2520features%2520by%2520applying%2520regional%2520and%2520dilated%2520XCA%2520in%2520parallel%250Avia%2520a%2520multi-axis%2520design.%2520Extensive%2520experiments%2520on%2520two%2520well-benchmarked%250Ainter-/intra-patient%2520registration%2520tasks%2520with%2520seven%2520public%2520medical%2520datasets%250Ademonstrate%2520that%2520our%2520MAXCA%2520block%2520enables%2520state-of-the-art%2520registration%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Deformable%20Medical%20Image%20Registration%20with%20Multi-axis%0A%20%20Cross-covariance%20Attention&entry.906535625=Mingyuan%20Meng%20and%20Michael%20Fulham%20and%20Lei%20Bi%20and%20Jinman%20Kim&entry.1292438233=%20%20Deformable%20image%20registration%20is%20a%20fundamental%20requirement%20for%20medical%20image%0Aanalysis.%20Recently%2C%20transformers%20have%20been%20widely%20used%20in%20deep%20learning-based%0Aregistration%20methods%20for%20their%20ability%20to%20capture%20long-range%20dependency%20via%0Aself-attention%20%28SA%29.%20However%2C%20the%20high%20computation%20and%20memory%20loads%20of%20SA%0A%28growing%20quadratically%20with%20the%20spatial%20resolution%29%20hinder%20transformers%20from%0Aprocessing%20subtle%20textural%20information%20in%20high-resolution%20image%20features%2C%20e.g.%2C%0Aat%20the%20full%20and%20half%20image%20resolutions.%20This%20limits%20deformable%20registration%20as%0Athe%20high-resolution%20textural%20information%20is%20crucial%20for%20finding%20precise%0Apixel-wise%20correspondence%20between%20subtle%20anatomical%20structures.%0ACross-covariance%20Attention%20%28XCA%29%2C%20as%20a%20%22transposed%22%20version%20of%20SA%20that%20operates%0Aacross%20feature%20channels%2C%20has%20complexity%20growing%20linearly%20with%20the%20spatial%0Aresolution%2C%20providing%20the%20feasibility%20of%20capturing%20long-range%20dependency%20among%0Ahigh-resolution%20image%20features.%20However%2C%20existing%20XCA-based%20transformers%20merely%0Acapture%20coarse%20global%20long-range%20dependency%2C%20which%20are%20unsuitable%20for%0Adeformable%20image%20registration%20relying%20primarily%20on%20fine-grained%20local%0Acorrespondence.%20In%20this%20study%2C%20we%20propose%20to%20improve%20existing%20deep%0Alearning-based%20registration%20methods%20by%20embedding%20a%20new%20XCA%20mechanism.%20To%20this%0Aend%2C%20we%20design%20an%20XCA-based%20transformer%20block%20optimized%20for%20deformable%20medical%0Aimage%20registration%2C%20named%20Multi-Axis%20XCA%20%28MAXCA%29.%20Our%20MAXCA%20serves%20as%20a%20general%0Anetwork%20block%20that%20can%20be%20embedded%20into%20various%20registration%20network%0Aarchitectures.%20It%20can%20capture%20both%20global%20and%20local%20long-range%20dependency%20among%0Ahigh-resolution%20image%20features%20by%20applying%20regional%20and%20dilated%20XCA%20in%20parallel%0Avia%20a%20multi-axis%20design.%20Extensive%20experiments%20on%20two%20well-benchmarked%0Ainter-/intra-patient%20registration%20tasks%20with%20seven%20public%20medical%20datasets%0Ademonstrate%20that%20our%20MAXCA%20block%20enables%20state-of-the-art%20registration%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18545v1&entry.124074799=Read"},
{"title": "Switch-a-View: Few-Shot View Selection Learned from Edited Videos", "author": "Sagnik Majumder and Tushar Nagarajan and Ziad Al-Halah and Kristen Grauman", "abstract": "  We introduce Switch-a-View, a model that learns to automatically select the\nviewpoint to display at each timepoint when creating a how-to video. The key\ninsight of our approach is how to train such a model from unlabeled--but\nhuman-edited--video samples. We pose a pretext task that pseudo-labels segments\nin the training videos for their primary viewpoint (egocentric or exocentric),\nand then discovers the patterns between those view-switch moments on the one\nhand and the visual and spoken content in the how-to video on the other hand.\nArmed with this predictor, our model then takes an unseen multi-view video as\ninput and orchestrates which viewpoint should be displayed when. We further\nintroduce a few-shot training setting that permits steering the model towards a\nnew data domain. We demonstrate our idea on a variety of real-world video from\nHowTo100M and Ego-Exo4D and rigorously validate its advantages.\n", "link": "http://arxiv.org/abs/2412.18386v1", "date": "2024-12-24", "relevancy": 2.1537, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6083}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Switch-a-View%3A%20Few-Shot%20View%20Selection%20Learned%20from%20Edited%20Videos&body=Title%3A%20Switch-a-View%3A%20Few-Shot%20View%20Selection%20Learned%20from%20Edited%20Videos%0AAuthor%3A%20Sagnik%20Majumder%20and%20Tushar%20Nagarajan%20and%20Ziad%20Al-Halah%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20We%20introduce%20Switch-a-View%2C%20a%20model%20that%20learns%20to%20automatically%20select%20the%0Aviewpoint%20to%20display%20at%20each%20timepoint%20when%20creating%20a%20how-to%20video.%20The%20key%0Ainsight%20of%20our%20approach%20is%20how%20to%20train%20such%20a%20model%20from%20unlabeled--but%0Ahuman-edited--video%20samples.%20We%20pose%20a%20pretext%20task%20that%20pseudo-labels%20segments%0Ain%20the%20training%20videos%20for%20their%20primary%20viewpoint%20%28egocentric%20or%20exocentric%29%2C%0Aand%20then%20discovers%20the%20patterns%20between%20those%20view-switch%20moments%20on%20the%20one%0Ahand%20and%20the%20visual%20and%20spoken%20content%20in%20the%20how-to%20video%20on%20the%20other%20hand.%0AArmed%20with%20this%20predictor%2C%20our%20model%20then%20takes%20an%20unseen%20multi-view%20video%20as%0Ainput%20and%20orchestrates%20which%20viewpoint%20should%20be%20displayed%20when.%20We%20further%0Aintroduce%20a%20few-shot%20training%20setting%20that%20permits%20steering%20the%20model%20towards%20a%0Anew%20data%20domain.%20We%20demonstrate%20our%20idea%20on%20a%20variety%20of%20real-world%20video%20from%0AHowTo100M%20and%20Ego-Exo4D%20and%20rigorously%20validate%20its%20advantages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwitch-a-View%253A%2520Few-Shot%2520View%2520Selection%2520Learned%2520from%2520Edited%2520Videos%26entry.906535625%3DSagnik%2520Majumder%2520and%2520Tushar%2520Nagarajan%2520and%2520Ziad%2520Al-Halah%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520We%2520introduce%2520Switch-a-View%252C%2520a%2520model%2520that%2520learns%2520to%2520automatically%2520select%2520the%250Aviewpoint%2520to%2520display%2520at%2520each%2520timepoint%2520when%2520creating%2520a%2520how-to%2520video.%2520The%2520key%250Ainsight%2520of%2520our%2520approach%2520is%2520how%2520to%2520train%2520such%2520a%2520model%2520from%2520unlabeled--but%250Ahuman-edited--video%2520samples.%2520We%2520pose%2520a%2520pretext%2520task%2520that%2520pseudo-labels%2520segments%250Ain%2520the%2520training%2520videos%2520for%2520their%2520primary%2520viewpoint%2520%2528egocentric%2520or%2520exocentric%2529%252C%250Aand%2520then%2520discovers%2520the%2520patterns%2520between%2520those%2520view-switch%2520moments%2520on%2520the%2520one%250Ahand%2520and%2520the%2520visual%2520and%2520spoken%2520content%2520in%2520the%2520how-to%2520video%2520on%2520the%2520other%2520hand.%250AArmed%2520with%2520this%2520predictor%252C%2520our%2520model%2520then%2520takes%2520an%2520unseen%2520multi-view%2520video%2520as%250Ainput%2520and%2520orchestrates%2520which%2520viewpoint%2520should%2520be%2520displayed%2520when.%2520We%2520further%250Aintroduce%2520a%2520few-shot%2520training%2520setting%2520that%2520permits%2520steering%2520the%2520model%2520towards%2520a%250Anew%2520data%2520domain.%2520We%2520demonstrate%2520our%2520idea%2520on%2520a%2520variety%2520of%2520real-world%2520video%2520from%250AHowTo100M%2520and%2520Ego-Exo4D%2520and%2520rigorously%2520validate%2520its%2520advantages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switch-a-View%3A%20Few-Shot%20View%20Selection%20Learned%20from%20Edited%20Videos&entry.906535625=Sagnik%20Majumder%20and%20Tushar%20Nagarajan%20and%20Ziad%20Al-Halah%20and%20Kristen%20Grauman&entry.1292438233=%20%20We%20introduce%20Switch-a-View%2C%20a%20model%20that%20learns%20to%20automatically%20select%20the%0Aviewpoint%20to%20display%20at%20each%20timepoint%20when%20creating%20a%20how-to%20video.%20The%20key%0Ainsight%20of%20our%20approach%20is%20how%20to%20train%20such%20a%20model%20from%20unlabeled--but%0Ahuman-edited--video%20samples.%20We%20pose%20a%20pretext%20task%20that%20pseudo-labels%20segments%0Ain%20the%20training%20videos%20for%20their%20primary%20viewpoint%20%28egocentric%20or%20exocentric%29%2C%0Aand%20then%20discovers%20the%20patterns%20between%20those%20view-switch%20moments%20on%20the%20one%0Ahand%20and%20the%20visual%20and%20spoken%20content%20in%20the%20how-to%20video%20on%20the%20other%20hand.%0AArmed%20with%20this%20predictor%2C%20our%20model%20then%20takes%20an%20unseen%20multi-view%20video%20as%0Ainput%20and%20orchestrates%20which%20viewpoint%20should%20be%20displayed%20when.%20We%20further%0Aintroduce%20a%20few-shot%20training%20setting%20that%20permits%20steering%20the%20model%20towards%20a%0Anew%20data%20domain.%20We%20demonstrate%20our%20idea%20on%20a%20variety%20of%20real-world%20video%20from%0AHowTo100M%20and%20Ego-Exo4D%20and%20rigorously%20validate%20its%20advantages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18386v1&entry.124074799=Read"},
{"title": "Underwater Image Restoration via Polymorphic Large Kernel CNNs", "author": "Xiaojiao Guo and Yihang Dong and Xuhang Chen and Weiwen Chen and Zimeng Li and FuChen Zheng and Chi-Man Pun", "abstract": "  Underwater Image Restoration (UIR) remains a challenging task in computer\nvision due to the complex degradation of images in underwater environments.\nWhile recent approaches have leveraged various deep learning techniques,\nincluding Transformers and complex, parameter-heavy models to achieve\nsignificant improvements in restoration effects, we demonstrate that pure CNN\narchitectures with lightweight parameters can achieve comparable results. In\nthis paper, we introduce UIR-PolyKernel, a novel method for underwater image\nrestoration that leverages Polymorphic Large Kernel CNNs. Our approach uniquely\ncombines large kernel convolutions of diverse sizes and shapes to effectively\ncapture long-range dependencies within underwater imagery. Additionally, we\nintroduce a Hybrid Domain Attention module that integrates frequency and\nspatial domain attention mechanisms to enhance feature importance. By\nleveraging the frequency domain, we can capture hidden features that may not be\nperceptible to humans but are crucial for identifying patterns in both\nunderwater and on-air images. This approach enhances the generalization and\nrobustness of our UIR model. Extensive experiments on benchmark datasets\ndemonstrate that UIR-PolyKernel achieves state-of-the-art performance in\nunderwater image restoration tasks, both quantitatively and qualitatively. Our\nresults show that well-designed pure CNN architectures can effectively compete\nwith more complex models, offering a balance between performance and\ncomputational efficiency. This work provides new insights into the potential of\nCNN-based approaches for challenging image restoration tasks in underwater\nenvironments. The code is available at\n\\href{https://github.com/CXH-Research/UIR-PolyKernel}{https://github.com/CXH-Research/UIR-PolyKernel}.\n", "link": "http://arxiv.org/abs/2412.18459v1", "date": "2024-12-24", "relevancy": 2.1513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5273}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Underwater%20Image%20Restoration%20via%20Polymorphic%20Large%20Kernel%20CNNs&body=Title%3A%20Underwater%20Image%20Restoration%20via%20Polymorphic%20Large%20Kernel%20CNNs%0AAuthor%3A%20Xiaojiao%20Guo%20and%20Yihang%20Dong%20and%20Xuhang%20Chen%20and%20Weiwen%20Chen%20and%20Zimeng%20Li%20and%20FuChen%20Zheng%20and%20Chi-Man%20Pun%0AAbstract%3A%20%20%20Underwater%20Image%20Restoration%20%28UIR%29%20remains%20a%20challenging%20task%20in%20computer%0Avision%20due%20to%20the%20complex%20degradation%20of%20images%20in%20underwater%20environments.%0AWhile%20recent%20approaches%20have%20leveraged%20various%20deep%20learning%20techniques%2C%0Aincluding%20Transformers%20and%20complex%2C%20parameter-heavy%20models%20to%20achieve%0Asignificant%20improvements%20in%20restoration%20effects%2C%20we%20demonstrate%20that%20pure%20CNN%0Aarchitectures%20with%20lightweight%20parameters%20can%20achieve%20comparable%20results.%20In%0Athis%20paper%2C%20we%20introduce%20UIR-PolyKernel%2C%20a%20novel%20method%20for%20underwater%20image%0Arestoration%20that%20leverages%20Polymorphic%20Large%20Kernel%20CNNs.%20Our%20approach%20uniquely%0Acombines%20large%20kernel%20convolutions%20of%20diverse%20sizes%20and%20shapes%20to%20effectively%0Acapture%20long-range%20dependencies%20within%20underwater%20imagery.%20Additionally%2C%20we%0Aintroduce%20a%20Hybrid%20Domain%20Attention%20module%20that%20integrates%20frequency%20and%0Aspatial%20domain%20attention%20mechanisms%20to%20enhance%20feature%20importance.%20By%0Aleveraging%20the%20frequency%20domain%2C%20we%20can%20capture%20hidden%20features%20that%20may%20not%20be%0Aperceptible%20to%20humans%20but%20are%20crucial%20for%20identifying%20patterns%20in%20both%0Aunderwater%20and%20on-air%20images.%20This%20approach%20enhances%20the%20generalization%20and%0Arobustness%20of%20our%20UIR%20model.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20that%20UIR-PolyKernel%20achieves%20state-of-the-art%20performance%20in%0Aunderwater%20image%20restoration%20tasks%2C%20both%20quantitatively%20and%20qualitatively.%20Our%0Aresults%20show%20that%20well-designed%20pure%20CNN%20architectures%20can%20effectively%20compete%0Awith%20more%20complex%20models%2C%20offering%20a%20balance%20between%20performance%20and%0Acomputational%20efficiency.%20This%20work%20provides%20new%20insights%20into%20the%20potential%20of%0ACNN-based%20approaches%20for%20challenging%20image%20restoration%20tasks%20in%20underwater%0Aenvironments.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CXH-Research/UIR-PolyKernel%7D%7Bhttps%3A//github.com/CXH-Research/UIR-PolyKernel%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderwater%2520Image%2520Restoration%2520via%2520Polymorphic%2520Large%2520Kernel%2520CNNs%26entry.906535625%3DXiaojiao%2520Guo%2520and%2520Yihang%2520Dong%2520and%2520Xuhang%2520Chen%2520and%2520Weiwen%2520Chen%2520and%2520Zimeng%2520Li%2520and%2520FuChen%2520Zheng%2520and%2520Chi-Man%2520Pun%26entry.1292438233%3D%2520%2520Underwater%2520Image%2520Restoration%2520%2528UIR%2529%2520remains%2520a%2520challenging%2520task%2520in%2520computer%250Avision%2520due%2520to%2520the%2520complex%2520degradation%2520of%2520images%2520in%2520underwater%2520environments.%250AWhile%2520recent%2520approaches%2520have%2520leveraged%2520various%2520deep%2520learning%2520techniques%252C%250Aincluding%2520Transformers%2520and%2520complex%252C%2520parameter-heavy%2520models%2520to%2520achieve%250Asignificant%2520improvements%2520in%2520restoration%2520effects%252C%2520we%2520demonstrate%2520that%2520pure%2520CNN%250Aarchitectures%2520with%2520lightweight%2520parameters%2520can%2520achieve%2520comparable%2520results.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520UIR-PolyKernel%252C%2520a%2520novel%2520method%2520for%2520underwater%2520image%250Arestoration%2520that%2520leverages%2520Polymorphic%2520Large%2520Kernel%2520CNNs.%2520Our%2520approach%2520uniquely%250Acombines%2520large%2520kernel%2520convolutions%2520of%2520diverse%2520sizes%2520and%2520shapes%2520to%2520effectively%250Acapture%2520long-range%2520dependencies%2520within%2520underwater%2520imagery.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520Hybrid%2520Domain%2520Attention%2520module%2520that%2520integrates%2520frequency%2520and%250Aspatial%2520domain%2520attention%2520mechanisms%2520to%2520enhance%2520feature%2520importance.%2520By%250Aleveraging%2520the%2520frequency%2520domain%252C%2520we%2520can%2520capture%2520hidden%2520features%2520that%2520may%2520not%2520be%250Aperceptible%2520to%2520humans%2520but%2520are%2520crucial%2520for%2520identifying%2520patterns%2520in%2520both%250Aunderwater%2520and%2520on-air%2520images.%2520This%2520approach%2520enhances%2520the%2520generalization%2520and%250Arobustness%2520of%2520our%2520UIR%2520model.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%250Ademonstrate%2520that%2520UIR-PolyKernel%2520achieves%2520state-of-the-art%2520performance%2520in%250Aunderwater%2520image%2520restoration%2520tasks%252C%2520both%2520quantitatively%2520and%2520qualitatively.%2520Our%250Aresults%2520show%2520that%2520well-designed%2520pure%2520CNN%2520architectures%2520can%2520effectively%2520compete%250Awith%2520more%2520complex%2520models%252C%2520offering%2520a%2520balance%2520between%2520performance%2520and%250Acomputational%2520efficiency.%2520This%2520work%2520provides%2520new%2520insights%2520into%2520the%2520potential%2520of%250ACNN-based%2520approaches%2520for%2520challenging%2520image%2520restoration%2520tasks%2520in%2520underwater%250Aenvironments.%2520The%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/CXH-Research/UIR-PolyKernel%257D%257Bhttps%253A//github.com/CXH-Research/UIR-PolyKernel%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underwater%20Image%20Restoration%20via%20Polymorphic%20Large%20Kernel%20CNNs&entry.906535625=Xiaojiao%20Guo%20and%20Yihang%20Dong%20and%20Xuhang%20Chen%20and%20Weiwen%20Chen%20and%20Zimeng%20Li%20and%20FuChen%20Zheng%20and%20Chi-Man%20Pun&entry.1292438233=%20%20Underwater%20Image%20Restoration%20%28UIR%29%20remains%20a%20challenging%20task%20in%20computer%0Avision%20due%20to%20the%20complex%20degradation%20of%20images%20in%20underwater%20environments.%0AWhile%20recent%20approaches%20have%20leveraged%20various%20deep%20learning%20techniques%2C%0Aincluding%20Transformers%20and%20complex%2C%20parameter-heavy%20models%20to%20achieve%0Asignificant%20improvements%20in%20restoration%20effects%2C%20we%20demonstrate%20that%20pure%20CNN%0Aarchitectures%20with%20lightweight%20parameters%20can%20achieve%20comparable%20results.%20In%0Athis%20paper%2C%20we%20introduce%20UIR-PolyKernel%2C%20a%20novel%20method%20for%20underwater%20image%0Arestoration%20that%20leverages%20Polymorphic%20Large%20Kernel%20CNNs.%20Our%20approach%20uniquely%0Acombines%20large%20kernel%20convolutions%20of%20diverse%20sizes%20and%20shapes%20to%20effectively%0Acapture%20long-range%20dependencies%20within%20underwater%20imagery.%20Additionally%2C%20we%0Aintroduce%20a%20Hybrid%20Domain%20Attention%20module%20that%20integrates%20frequency%20and%0Aspatial%20domain%20attention%20mechanisms%20to%20enhance%20feature%20importance.%20By%0Aleveraging%20the%20frequency%20domain%2C%20we%20can%20capture%20hidden%20features%20that%20may%20not%20be%0Aperceptible%20to%20humans%20but%20are%20crucial%20for%20identifying%20patterns%20in%20both%0Aunderwater%20and%20on-air%20images.%20This%20approach%20enhances%20the%20generalization%20and%0Arobustness%20of%20our%20UIR%20model.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20that%20UIR-PolyKernel%20achieves%20state-of-the-art%20performance%20in%0Aunderwater%20image%20restoration%20tasks%2C%20both%20quantitatively%20and%20qualitatively.%20Our%0Aresults%20show%20that%20well-designed%20pure%20CNN%20architectures%20can%20effectively%20compete%0Awith%20more%20complex%20models%2C%20offering%20a%20balance%20between%20performance%20and%0Acomputational%20efficiency.%20This%20work%20provides%20new%20insights%20into%20the%20potential%20of%0ACNN-based%20approaches%20for%20challenging%20image%20restoration%20tasks%20in%20underwater%0Aenvironments.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CXH-Research/UIR-PolyKernel%7D%7Bhttps%3A//github.com/CXH-Research/UIR-PolyKernel%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18459v1&entry.124074799=Read"},
{"title": "TPAoI: Ensuring Fresh Service Status at the Network Edge in\n  Compute-First Networking", "author": "Haosheng He and Jianpeng Qi and Chao Liu and Junyu Dong and Yanwei Yu", "abstract": "  In compute-first networking, maintaining fresh and accurate status\ninformation at the network edge is crucial for effective access to remote\nservices. This process typically involves three phases: Status updating, user\naccessing, and user requesting. However, current studies on status\neffectiveness, such as Age of Information at Query (QAoI), do not\ncomprehensively cover all these phases. Therefore, this paper introduces a\nnovel metric, TPAoI, aimed at optimizing update decisions by measuring the\nfreshness of service status. The stochastic nature of edge environments,\ncharacterized by unpredictable communication delays in updating, requesting,\nand user access times, poses a significant challenge when modeling. To address\nthis, we model the problem as a Markov Decision Process (MDP) and employ a\nDueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive\nexperiments demonstrate that the proposed TPAoI metric effectively minimizes\nAoI, ensuring timely and reliable service updates in dynamic edge environments.\nResults indicate that TPAoI reduces AoI by an average of 47\\% compared to QAoI\nmetrics and decreases update frequency by an average of 48\\% relative to\nconventional AoI metrics, showing significant improvement.\n", "link": "http://arxiv.org/abs/2412.18391v1", "date": "2024-12-24", "relevancy": 2.1506, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4236}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPAoI%3A%20Ensuring%20Fresh%20Service%20Status%20at%20the%20Network%20Edge%20in%0A%20%20Compute-First%20Networking&body=Title%3A%20TPAoI%3A%20Ensuring%20Fresh%20Service%20Status%20at%20the%20Network%20Edge%20in%0A%20%20Compute-First%20Networking%0AAuthor%3A%20Haosheng%20He%20and%20Jianpeng%20Qi%20and%20Chao%20Liu%20and%20Junyu%20Dong%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20In%20compute-first%20networking%2C%20maintaining%20fresh%20and%20accurate%20status%0Ainformation%20at%20the%20network%20edge%20is%20crucial%20for%20effective%20access%20to%20remote%0Aservices.%20This%20process%20typically%20involves%20three%20phases%3A%20Status%20updating%2C%20user%0Aaccessing%2C%20and%20user%20requesting.%20However%2C%20current%20studies%20on%20status%0Aeffectiveness%2C%20such%20as%20Age%20of%20Information%20at%20Query%20%28QAoI%29%2C%20do%20not%0Acomprehensively%20cover%20all%20these%20phases.%20Therefore%2C%20this%20paper%20introduces%20a%0Anovel%20metric%2C%20TPAoI%2C%20aimed%20at%20optimizing%20update%20decisions%20by%20measuring%20the%0Afreshness%20of%20service%20status.%20The%20stochastic%20nature%20of%20edge%20environments%2C%0Acharacterized%20by%20unpredictable%20communication%20delays%20in%20updating%2C%20requesting%2C%0Aand%20user%20access%20times%2C%20poses%20a%20significant%20challenge%20when%20modeling.%20To%20address%0Athis%2C%20we%20model%20the%20problem%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20employ%20a%0ADueling%20Double%20Deep%20Q-Network%20%28D3QN%29%20algorithm%20for%20optimization.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20TPAoI%20metric%20effectively%20minimizes%0AAoI%2C%20ensuring%20timely%20and%20reliable%20service%20updates%20in%20dynamic%20edge%20environments.%0AResults%20indicate%20that%20TPAoI%20reduces%20AoI%20by%20an%20average%20of%2047%5C%25%20compared%20to%20QAoI%0Ametrics%20and%20decreases%20update%20frequency%20by%20an%20average%20of%2048%5C%25%20relative%20to%0Aconventional%20AoI%20metrics%2C%20showing%20significant%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPAoI%253A%2520Ensuring%2520Fresh%2520Service%2520Status%2520at%2520the%2520Network%2520Edge%2520in%250A%2520%2520Compute-First%2520Networking%26entry.906535625%3DHaosheng%2520He%2520and%2520Jianpeng%2520Qi%2520and%2520Chao%2520Liu%2520and%2520Junyu%2520Dong%2520and%2520Yanwei%2520Yu%26entry.1292438233%3D%2520%2520In%2520compute-first%2520networking%252C%2520maintaining%2520fresh%2520and%2520accurate%2520status%250Ainformation%2520at%2520the%2520network%2520edge%2520is%2520crucial%2520for%2520effective%2520access%2520to%2520remote%250Aservices.%2520This%2520process%2520typically%2520involves%2520three%2520phases%253A%2520Status%2520updating%252C%2520user%250Aaccessing%252C%2520and%2520user%2520requesting.%2520However%252C%2520current%2520studies%2520on%2520status%250Aeffectiveness%252C%2520such%2520as%2520Age%2520of%2520Information%2520at%2520Query%2520%2528QAoI%2529%252C%2520do%2520not%250Acomprehensively%2520cover%2520all%2520these%2520phases.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anovel%2520metric%252C%2520TPAoI%252C%2520aimed%2520at%2520optimizing%2520update%2520decisions%2520by%2520measuring%2520the%250Afreshness%2520of%2520service%2520status.%2520The%2520stochastic%2520nature%2520of%2520edge%2520environments%252C%250Acharacterized%2520by%2520unpredictable%2520communication%2520delays%2520in%2520updating%252C%2520requesting%252C%250Aand%2520user%2520access%2520times%252C%2520poses%2520a%2520significant%2520challenge%2520when%2520modeling.%2520To%2520address%250Athis%252C%2520we%2520model%2520the%2520problem%2520as%2520a%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520and%2520employ%2520a%250ADueling%2520Double%2520Deep%2520Q-Network%2520%2528D3QN%2529%2520algorithm%2520for%2520optimization.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520TPAoI%2520metric%2520effectively%2520minimizes%250AAoI%252C%2520ensuring%2520timely%2520and%2520reliable%2520service%2520updates%2520in%2520dynamic%2520edge%2520environments.%250AResults%2520indicate%2520that%2520TPAoI%2520reduces%2520AoI%2520by%2520an%2520average%2520of%252047%255C%2525%2520compared%2520to%2520QAoI%250Ametrics%2520and%2520decreases%2520update%2520frequency%2520by%2520an%2520average%2520of%252048%255C%2525%2520relative%2520to%250Aconventional%2520AoI%2520metrics%252C%2520showing%2520significant%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPAoI%3A%20Ensuring%20Fresh%20Service%20Status%20at%20the%20Network%20Edge%20in%0A%20%20Compute-First%20Networking&entry.906535625=Haosheng%20He%20and%20Jianpeng%20Qi%20and%20Chao%20Liu%20and%20Junyu%20Dong%20and%20Yanwei%20Yu&entry.1292438233=%20%20In%20compute-first%20networking%2C%20maintaining%20fresh%20and%20accurate%20status%0Ainformation%20at%20the%20network%20edge%20is%20crucial%20for%20effective%20access%20to%20remote%0Aservices.%20This%20process%20typically%20involves%20three%20phases%3A%20Status%20updating%2C%20user%0Aaccessing%2C%20and%20user%20requesting.%20However%2C%20current%20studies%20on%20status%0Aeffectiveness%2C%20such%20as%20Age%20of%20Information%20at%20Query%20%28QAoI%29%2C%20do%20not%0Acomprehensively%20cover%20all%20these%20phases.%20Therefore%2C%20this%20paper%20introduces%20a%0Anovel%20metric%2C%20TPAoI%2C%20aimed%20at%20optimizing%20update%20decisions%20by%20measuring%20the%0Afreshness%20of%20service%20status.%20The%20stochastic%20nature%20of%20edge%20environments%2C%0Acharacterized%20by%20unpredictable%20communication%20delays%20in%20updating%2C%20requesting%2C%0Aand%20user%20access%20times%2C%20poses%20a%20significant%20challenge%20when%20modeling.%20To%20address%0Athis%2C%20we%20model%20the%20problem%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20employ%20a%0ADueling%20Double%20Deep%20Q-Network%20%28D3QN%29%20algorithm%20for%20optimization.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20TPAoI%20metric%20effectively%20minimizes%0AAoI%2C%20ensuring%20timely%20and%20reliable%20service%20updates%20in%20dynamic%20edge%20environments.%0AResults%20indicate%20that%20TPAoI%20reduces%20AoI%20by%20an%20average%20of%2047%5C%25%20compared%20to%20QAoI%0Ametrics%20and%20decreases%20update%20frequency%20by%20an%20average%20of%2048%5C%25%20relative%20to%0Aconventional%20AoI%20metrics%2C%20showing%20significant%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18391v1&entry.124074799=Read"},
{"title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality\n  Image Fusion", "author": "Yuchen Guo and Ruoxiang Xu and Rongcheng Li and Zhenghao Wu and Weifeng Su", "abstract": "  In extreme scenarios such as nighttime or low-visibility environments,\nachieving reliable perception is critical for applications like autonomous\ndriving, robotics, and surveillance. Multi-modality image fusion, particularly\nintegrating infrared imaging, offers a robust solution by combining\ncomplementary information from different modalities to enhance scene\nunderstanding and decision-making. However, current methods face significant\nlimitations: GAN-based approaches often produce blurry images that lack\nfine-grained details, while AE-based methods may introduce bias toward specific\nmodalities, leading to unnatural fusion results. To address these challenges,\nwe propose DAE-Fuse, a novel two-phase discriminative autoencoder framework\nthat generates sharp and natural fused images. Furthermore, We pioneer the\nextension of image fusion techniques from static images to the video domain\nwhile preserving temporal consistency across frames, thus advancing the\nperceptual capabilities required for autonomous navigation. Extensive\nexperiments on public datasets demonstrate that DAE-Fuse achieves\nstate-of-the-art performance on multiple benchmarks, with superior\ngeneralizability to tasks like medical image fusion.\n", "link": "http://arxiv.org/abs/2409.10080v2", "date": "2024-12-24", "relevancy": 2.149, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5487}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.53}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAE-Fuse%3A%20An%20Adaptive%20Discriminative%20Autoencoder%20for%20Multi-Modality%0A%20%20Image%20Fusion&body=Title%3A%20DAE-Fuse%3A%20An%20Adaptive%20Discriminative%20Autoencoder%20for%20Multi-Modality%0A%20%20Image%20Fusion%0AAuthor%3A%20Yuchen%20Guo%20and%20Ruoxiang%20Xu%20and%20Rongcheng%20Li%20and%20Zhenghao%20Wu%20and%20Weifeng%20Su%0AAbstract%3A%20%20%20In%20extreme%20scenarios%20such%20as%20nighttime%20or%20low-visibility%20environments%2C%0Aachieving%20reliable%20perception%20is%20critical%20for%20applications%20like%20autonomous%0Adriving%2C%20robotics%2C%20and%20surveillance.%20Multi-modality%20image%20fusion%2C%20particularly%0Aintegrating%20infrared%20imaging%2C%20offers%20a%20robust%20solution%20by%20combining%0Acomplementary%20information%20from%20different%20modalities%20to%20enhance%20scene%0Aunderstanding%20and%20decision-making.%20However%2C%20current%20methods%20face%20significant%0Alimitations%3A%20GAN-based%20approaches%20often%20produce%20blurry%20images%20that%20lack%0Afine-grained%20details%2C%20while%20AE-based%20methods%20may%20introduce%20bias%20toward%20specific%0Amodalities%2C%20leading%20to%20unnatural%20fusion%20results.%20To%20address%20these%20challenges%2C%0Awe%20propose%20DAE-Fuse%2C%20a%20novel%20two-phase%20discriminative%20autoencoder%20framework%0Athat%20generates%20sharp%20and%20natural%20fused%20images.%20Furthermore%2C%20We%20pioneer%20the%0Aextension%20of%20image%20fusion%20techniques%20from%20static%20images%20to%20the%20video%20domain%0Awhile%20preserving%20temporal%20consistency%20across%20frames%2C%20thus%20advancing%20the%0Aperceptual%20capabilities%20required%20for%20autonomous%20navigation.%20Extensive%0Aexperiments%20on%20public%20datasets%20demonstrate%20that%20DAE-Fuse%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20with%20superior%0Ageneralizability%20to%20tasks%20like%20medical%20image%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAE-Fuse%253A%2520An%2520Adaptive%2520Discriminative%2520Autoencoder%2520for%2520Multi-Modality%250A%2520%2520Image%2520Fusion%26entry.906535625%3DYuchen%2520Guo%2520and%2520Ruoxiang%2520Xu%2520and%2520Rongcheng%2520Li%2520and%2520Zhenghao%2520Wu%2520and%2520Weifeng%2520Su%26entry.1292438233%3D%2520%2520In%2520extreme%2520scenarios%2520such%2520as%2520nighttime%2520or%2520low-visibility%2520environments%252C%250Aachieving%2520reliable%2520perception%2520is%2520critical%2520for%2520applications%2520like%2520autonomous%250Adriving%252C%2520robotics%252C%2520and%2520surveillance.%2520Multi-modality%2520image%2520fusion%252C%2520particularly%250Aintegrating%2520infrared%2520imaging%252C%2520offers%2520a%2520robust%2520solution%2520by%2520combining%250Acomplementary%2520information%2520from%2520different%2520modalities%2520to%2520enhance%2520scene%250Aunderstanding%2520and%2520decision-making.%2520However%252C%2520current%2520methods%2520face%2520significant%250Alimitations%253A%2520GAN-based%2520approaches%2520often%2520produce%2520blurry%2520images%2520that%2520lack%250Afine-grained%2520details%252C%2520while%2520AE-based%2520methods%2520may%2520introduce%2520bias%2520toward%2520specific%250Amodalities%252C%2520leading%2520to%2520unnatural%2520fusion%2520results.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520DAE-Fuse%252C%2520a%2520novel%2520two-phase%2520discriminative%2520autoencoder%2520framework%250Athat%2520generates%2520sharp%2520and%2520natural%2520fused%2520images.%2520Furthermore%252C%2520We%2520pioneer%2520the%250Aextension%2520of%2520image%2520fusion%2520techniques%2520from%2520static%2520images%2520to%2520the%2520video%2520domain%250Awhile%2520preserving%2520temporal%2520consistency%2520across%2520frames%252C%2520thus%2520advancing%2520the%250Aperceptual%2520capabilities%2520required%2520for%2520autonomous%2520navigation.%2520Extensive%250Aexperiments%2520on%2520public%2520datasets%2520demonstrate%2520that%2520DAE-Fuse%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520with%2520superior%250Ageneralizability%2520to%2520tasks%2520like%2520medical%2520image%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAE-Fuse%3A%20An%20Adaptive%20Discriminative%20Autoencoder%20for%20Multi-Modality%0A%20%20Image%20Fusion&entry.906535625=Yuchen%20Guo%20and%20Ruoxiang%20Xu%20and%20Rongcheng%20Li%20and%20Zhenghao%20Wu%20and%20Weifeng%20Su&entry.1292438233=%20%20In%20extreme%20scenarios%20such%20as%20nighttime%20or%20low-visibility%20environments%2C%0Aachieving%20reliable%20perception%20is%20critical%20for%20applications%20like%20autonomous%0Adriving%2C%20robotics%2C%20and%20surveillance.%20Multi-modality%20image%20fusion%2C%20particularly%0Aintegrating%20infrared%20imaging%2C%20offers%20a%20robust%20solution%20by%20combining%0Acomplementary%20information%20from%20different%20modalities%20to%20enhance%20scene%0Aunderstanding%20and%20decision-making.%20However%2C%20current%20methods%20face%20significant%0Alimitations%3A%20GAN-based%20approaches%20often%20produce%20blurry%20images%20that%20lack%0Afine-grained%20details%2C%20while%20AE-based%20methods%20may%20introduce%20bias%20toward%20specific%0Amodalities%2C%20leading%20to%20unnatural%20fusion%20results.%20To%20address%20these%20challenges%2C%0Awe%20propose%20DAE-Fuse%2C%20a%20novel%20two-phase%20discriminative%20autoencoder%20framework%0Athat%20generates%20sharp%20and%20natural%20fused%20images.%20Furthermore%2C%20We%20pioneer%20the%0Aextension%20of%20image%20fusion%20techniques%20from%20static%20images%20to%20the%20video%20domain%0Awhile%20preserving%20temporal%20consistency%20across%20frames%2C%20thus%20advancing%20the%0Aperceptual%20capabilities%20required%20for%20autonomous%20navigation.%20Extensive%0Aexperiments%20on%20public%20datasets%20demonstrate%20that%20DAE-Fuse%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20with%20superior%0Ageneralizability%20to%20tasks%20like%20medical%20image%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10080v2&entry.124074799=Read"},
{"title": "StaR Maps: Unveiling Uncertainty in Geospatial Relations", "author": "Simon Kohaut and Benedict Flade and Julian Eggert and Devendra Singh Dhami and Kristian Kersting", "abstract": "  The growing complexity of intelligent transportation systems and their\napplications in public spaces has increased the demand for expressive and\nversatile knowledge representation. While various mapping efforts have achieved\nwidespread coverage, including detailed annotation of features with semantic\nlabels, it is essential to understand their inherent uncertainties, which are\ncommonly underrepresented by the respective geographic information systems.\nHence, it is critical to develop a representation that combines a statistical,\nprobabilistic perspective with the relational nature of geospatial data.\nFurther, such a representation should facilitate an honest view of the data's\naccuracy and provide an environment for high-level reasoning to obtain novel\ninsights from task-dependent queries. Our work addresses this gap in two ways.\nFirst, we present Statistical Relational Maps (StaR Maps) as a representation\nof uncertain, semantic map data. Second, we demonstrate efficient computation\nof StaR Maps to scale the approach to wide urban spaces. Through experiments on\nreal-world, crowd-sourced data, we underpin the application and utility of StaR\nMaps in terms of representing uncertain knowledge and reasoning for complex\ngeospatial information.\n", "link": "http://arxiv.org/abs/2412.18356v1", "date": "2024-12-24", "relevancy": 2.1424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5858}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StaR%20Maps%3A%20Unveiling%20Uncertainty%20in%20Geospatial%20Relations&body=Title%3A%20StaR%20Maps%3A%20Unveiling%20Uncertainty%20in%20Geospatial%20Relations%0AAuthor%3A%20Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Julian%20Eggert%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20The%20growing%20complexity%20of%20intelligent%20transportation%20systems%20and%20their%0Aapplications%20in%20public%20spaces%20has%20increased%20the%20demand%20for%20expressive%20and%0Aversatile%20knowledge%20representation.%20While%20various%20mapping%20efforts%20have%20achieved%0Awidespread%20coverage%2C%20including%20detailed%20annotation%20of%20features%20with%20semantic%0Alabels%2C%20it%20is%20essential%20to%20understand%20their%20inherent%20uncertainties%2C%20which%20are%0Acommonly%20underrepresented%20by%20the%20respective%20geographic%20information%20systems.%0AHence%2C%20it%20is%20critical%20to%20develop%20a%20representation%20that%20combines%20a%20statistical%2C%0Aprobabilistic%20perspective%20with%20the%20relational%20nature%20of%20geospatial%20data.%0AFurther%2C%20such%20a%20representation%20should%20facilitate%20an%20honest%20view%20of%20the%20data%27s%0Aaccuracy%20and%20provide%20an%20environment%20for%20high-level%20reasoning%20to%20obtain%20novel%0Ainsights%20from%20task-dependent%20queries.%20Our%20work%20addresses%20this%20gap%20in%20two%20ways.%0AFirst%2C%20we%20present%20Statistical%20Relational%20Maps%20%28StaR%20Maps%29%20as%20a%20representation%0Aof%20uncertain%2C%20semantic%20map%20data.%20Second%2C%20we%20demonstrate%20efficient%20computation%0Aof%20StaR%20Maps%20to%20scale%20the%20approach%20to%20wide%20urban%20spaces.%20Through%20experiments%20on%0Areal-world%2C%20crowd-sourced%20data%2C%20we%20underpin%20the%20application%20and%20utility%20of%20StaR%0AMaps%20in%20terms%20of%20representing%20uncertain%20knowledge%20and%20reasoning%20for%20complex%0Ageospatial%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStaR%2520Maps%253A%2520Unveiling%2520Uncertainty%2520in%2520Geospatial%2520Relations%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Benedict%2520Flade%2520and%2520Julian%2520Eggert%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520The%2520growing%2520complexity%2520of%2520intelligent%2520transportation%2520systems%2520and%2520their%250Aapplications%2520in%2520public%2520spaces%2520has%2520increased%2520the%2520demand%2520for%2520expressive%2520and%250Aversatile%2520knowledge%2520representation.%2520While%2520various%2520mapping%2520efforts%2520have%2520achieved%250Awidespread%2520coverage%252C%2520including%2520detailed%2520annotation%2520of%2520features%2520with%2520semantic%250Alabels%252C%2520it%2520is%2520essential%2520to%2520understand%2520their%2520inherent%2520uncertainties%252C%2520which%2520are%250Acommonly%2520underrepresented%2520by%2520the%2520respective%2520geographic%2520information%2520systems.%250AHence%252C%2520it%2520is%2520critical%2520to%2520develop%2520a%2520representation%2520that%2520combines%2520a%2520statistical%252C%250Aprobabilistic%2520perspective%2520with%2520the%2520relational%2520nature%2520of%2520geospatial%2520data.%250AFurther%252C%2520such%2520a%2520representation%2520should%2520facilitate%2520an%2520honest%2520view%2520of%2520the%2520data%2527s%250Aaccuracy%2520and%2520provide%2520an%2520environment%2520for%2520high-level%2520reasoning%2520to%2520obtain%2520novel%250Ainsights%2520from%2520task-dependent%2520queries.%2520Our%2520work%2520addresses%2520this%2520gap%2520in%2520two%2520ways.%250AFirst%252C%2520we%2520present%2520Statistical%2520Relational%2520Maps%2520%2528StaR%2520Maps%2529%2520as%2520a%2520representation%250Aof%2520uncertain%252C%2520semantic%2520map%2520data.%2520Second%252C%2520we%2520demonstrate%2520efficient%2520computation%250Aof%2520StaR%2520Maps%2520to%2520scale%2520the%2520approach%2520to%2520wide%2520urban%2520spaces.%2520Through%2520experiments%2520on%250Areal-world%252C%2520crowd-sourced%2520data%252C%2520we%2520underpin%2520the%2520application%2520and%2520utility%2520of%2520StaR%250AMaps%2520in%2520terms%2520of%2520representing%2520uncertain%2520knowledge%2520and%2520reasoning%2520for%2520complex%250Ageospatial%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StaR%20Maps%3A%20Unveiling%20Uncertainty%20in%20Geospatial%20Relations&entry.906535625=Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Julian%20Eggert%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting&entry.1292438233=%20%20The%20growing%20complexity%20of%20intelligent%20transportation%20systems%20and%20their%0Aapplications%20in%20public%20spaces%20has%20increased%20the%20demand%20for%20expressive%20and%0Aversatile%20knowledge%20representation.%20While%20various%20mapping%20efforts%20have%20achieved%0Awidespread%20coverage%2C%20including%20detailed%20annotation%20of%20features%20with%20semantic%0Alabels%2C%20it%20is%20essential%20to%20understand%20their%20inherent%20uncertainties%2C%20which%20are%0Acommonly%20underrepresented%20by%20the%20respective%20geographic%20information%20systems.%0AHence%2C%20it%20is%20critical%20to%20develop%20a%20representation%20that%20combines%20a%20statistical%2C%0Aprobabilistic%20perspective%20with%20the%20relational%20nature%20of%20geospatial%20data.%0AFurther%2C%20such%20a%20representation%20should%20facilitate%20an%20honest%20view%20of%20the%20data%27s%0Aaccuracy%20and%20provide%20an%20environment%20for%20high-level%20reasoning%20to%20obtain%20novel%0Ainsights%20from%20task-dependent%20queries.%20Our%20work%20addresses%20this%20gap%20in%20two%20ways.%0AFirst%2C%20we%20present%20Statistical%20Relational%20Maps%20%28StaR%20Maps%29%20as%20a%20representation%0Aof%20uncertain%2C%20semantic%20map%20data.%20Second%2C%20we%20demonstrate%20efficient%20computation%0Aof%20StaR%20Maps%20to%20scale%20the%20approach%20to%20wide%20urban%20spaces.%20Through%20experiments%20on%0Areal-world%2C%20crowd-sourced%20data%2C%20we%20underpin%20the%20application%20and%20utility%20of%20StaR%0AMaps%20in%20terms%20of%20representing%20uncertain%20knowledge%20and%20reasoning%20for%20complex%0Ageospatial%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18356v1&entry.124074799=Read"},
{"title": "FloNa: Floor Plan Guided Embodied Visual Navigation", "author": "Jiaxin Li and Weiqi Huang and Zan Wang and Wei Liang and Huijun Di and Feng Liu", "abstract": "  Humans naturally rely on floor plans to navigate in unfamiliar environments,\nas they are readily available, reliable, and provide rich geometrical guidance.\nHowever, existing visual navigation settings overlook this valuable prior\nknowledge, leading to limited efficiency and accuracy. To eliminate this gap,\nwe introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the\nfirst attempt to incorporate floor plan into embodied visual navigation. While\nthe floor plan offers significant advantages, two key challenges emerge: (1)\nhandling the spatial inconsistency between the floor plan and the actual scene\nlayout for collision-free navigation, and (2) aligning observed images with the\nfloor plan sketch despite their distinct modalities. To address these\nchallenges, we propose FloDiff, a novel diffusion policy framework\nincorporating a localization module to facilitate alignment between the current\nobservation and the floor plan. We further collect $20k$ navigation episodes\nacross $117$ scenes in the iGibson simulator to support the training and\nevaluation. Extensive experiments demonstrate the effectiveness and efficiency\nof our framework in unfamiliar scenes using floor plan knowledge. Project\nwebsite: https://gauleejx.github.io/flona/.\n", "link": "http://arxiv.org/abs/2412.18335v1", "date": "2024-12-24", "relevancy": 2.1216, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5487}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FloNa%3A%20Floor%20Plan%20Guided%20Embodied%20Visual%20Navigation&body=Title%3A%20FloNa%3A%20Floor%20Plan%20Guided%20Embodied%20Visual%20Navigation%0AAuthor%3A%20Jiaxin%20Li%20and%20Weiqi%20Huang%20and%20Zan%20Wang%20and%20Wei%20Liang%20and%20Huijun%20Di%20and%20Feng%20Liu%0AAbstract%3A%20%20%20Humans%20naturally%20rely%20on%20floor%20plans%20to%20navigate%20in%20unfamiliar%20environments%2C%0Aas%20they%20are%20readily%20available%2C%20reliable%2C%20and%20provide%20rich%20geometrical%20guidance.%0AHowever%2C%20existing%20visual%20navigation%20settings%20overlook%20this%20valuable%20prior%0Aknowledge%2C%20leading%20to%20limited%20efficiency%20and%20accuracy.%20To%20eliminate%20this%20gap%2C%0Awe%20introduce%20a%20novel%20navigation%20task%3A%20Floor%20Plan%20Visual%20Navigation%20%28FloNa%29%2C%20the%0Afirst%20attempt%20to%20incorporate%20floor%20plan%20into%20embodied%20visual%20navigation.%20While%0Athe%20floor%20plan%20offers%20significant%20advantages%2C%20two%20key%20challenges%20emerge%3A%20%281%29%0Ahandling%20the%20spatial%20inconsistency%20between%20the%20floor%20plan%20and%20the%20actual%20scene%0Alayout%20for%20collision-free%20navigation%2C%20and%20%282%29%20aligning%20observed%20images%20with%20the%0Afloor%20plan%20sketch%20despite%20their%20distinct%20modalities.%20To%20address%20these%0Achallenges%2C%20we%20propose%20FloDiff%2C%20a%20novel%20diffusion%20policy%20framework%0Aincorporating%20a%20localization%20module%20to%20facilitate%20alignment%20between%20the%20current%0Aobservation%20and%20the%20floor%20plan.%20We%20further%20collect%20%2420k%24%20navigation%20episodes%0Aacross%20%24117%24%20scenes%20in%20the%20iGibson%20simulator%20to%20support%20the%20training%20and%0Aevaluation.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20efficiency%0Aof%20our%20framework%20in%20unfamiliar%20scenes%20using%20floor%20plan%20knowledge.%20Project%0Awebsite%3A%20https%3A//gauleejx.github.io/flona/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloNa%253A%2520Floor%2520Plan%2520Guided%2520Embodied%2520Visual%2520Navigation%26entry.906535625%3DJiaxin%2520Li%2520and%2520Weiqi%2520Huang%2520and%2520Zan%2520Wang%2520and%2520Wei%2520Liang%2520and%2520Huijun%2520Di%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520Humans%2520naturally%2520rely%2520on%2520floor%2520plans%2520to%2520navigate%2520in%2520unfamiliar%2520environments%252C%250Aas%2520they%2520are%2520readily%2520available%252C%2520reliable%252C%2520and%2520provide%2520rich%2520geometrical%2520guidance.%250AHowever%252C%2520existing%2520visual%2520navigation%2520settings%2520overlook%2520this%2520valuable%2520prior%250Aknowledge%252C%2520leading%2520to%2520limited%2520efficiency%2520and%2520accuracy.%2520To%2520eliminate%2520this%2520gap%252C%250Awe%2520introduce%2520a%2520novel%2520navigation%2520task%253A%2520Floor%2520Plan%2520Visual%2520Navigation%2520%2528FloNa%2529%252C%2520the%250Afirst%2520attempt%2520to%2520incorporate%2520floor%2520plan%2520into%2520embodied%2520visual%2520navigation.%2520While%250Athe%2520floor%2520plan%2520offers%2520significant%2520advantages%252C%2520two%2520key%2520challenges%2520emerge%253A%2520%25281%2529%250Ahandling%2520the%2520spatial%2520inconsistency%2520between%2520the%2520floor%2520plan%2520and%2520the%2520actual%2520scene%250Alayout%2520for%2520collision-free%2520navigation%252C%2520and%2520%25282%2529%2520aligning%2520observed%2520images%2520with%2520the%250Afloor%2520plan%2520sketch%2520despite%2520their%2520distinct%2520modalities.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520FloDiff%252C%2520a%2520novel%2520diffusion%2520policy%2520framework%250Aincorporating%2520a%2520localization%2520module%2520to%2520facilitate%2520alignment%2520between%2520the%2520current%250Aobservation%2520and%2520the%2520floor%2520plan.%2520We%2520further%2520collect%2520%252420k%2524%2520navigation%2520episodes%250Aacross%2520%2524117%2524%2520scenes%2520in%2520the%2520iGibson%2520simulator%2520to%2520support%2520the%2520training%2520and%250Aevaluation.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%250Aof%2520our%2520framework%2520in%2520unfamiliar%2520scenes%2520using%2520floor%2520plan%2520knowledge.%2520Project%250Awebsite%253A%2520https%253A//gauleejx.github.io/flona/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FloNa%3A%20Floor%20Plan%20Guided%20Embodied%20Visual%20Navigation&entry.906535625=Jiaxin%20Li%20and%20Weiqi%20Huang%20and%20Zan%20Wang%20and%20Wei%20Liang%20and%20Huijun%20Di%20and%20Feng%20Liu&entry.1292438233=%20%20Humans%20naturally%20rely%20on%20floor%20plans%20to%20navigate%20in%20unfamiliar%20environments%2C%0Aas%20they%20are%20readily%20available%2C%20reliable%2C%20and%20provide%20rich%20geometrical%20guidance.%0AHowever%2C%20existing%20visual%20navigation%20settings%20overlook%20this%20valuable%20prior%0Aknowledge%2C%20leading%20to%20limited%20efficiency%20and%20accuracy.%20To%20eliminate%20this%20gap%2C%0Awe%20introduce%20a%20novel%20navigation%20task%3A%20Floor%20Plan%20Visual%20Navigation%20%28FloNa%29%2C%20the%0Afirst%20attempt%20to%20incorporate%20floor%20plan%20into%20embodied%20visual%20navigation.%20While%0Athe%20floor%20plan%20offers%20significant%20advantages%2C%20two%20key%20challenges%20emerge%3A%20%281%29%0Ahandling%20the%20spatial%20inconsistency%20between%20the%20floor%20plan%20and%20the%20actual%20scene%0Alayout%20for%20collision-free%20navigation%2C%20and%20%282%29%20aligning%20observed%20images%20with%20the%0Afloor%20plan%20sketch%20despite%20their%20distinct%20modalities.%20To%20address%20these%0Achallenges%2C%20we%20propose%20FloDiff%2C%20a%20novel%20diffusion%20policy%20framework%0Aincorporating%20a%20localization%20module%20to%20facilitate%20alignment%20between%20the%20current%0Aobservation%20and%20the%20floor%20plan.%20We%20further%20collect%20%2420k%24%20navigation%20episodes%0Aacross%20%24117%24%20scenes%20in%20the%20iGibson%20simulator%20to%20support%20the%20training%20and%0Aevaluation.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20efficiency%0Aof%20our%20framework%20in%20unfamiliar%20scenes%20using%20floor%20plan%20knowledge.%20Project%0Awebsite%3A%20https%3A//gauleejx.github.io/flona/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18335v1&entry.124074799=Read"},
{"title": "Joint Adaptive OFDM and Reinforcement Learning Design for Autonomous\n  Vehicles: Leveraging Age of Updates", "author": "Mamady Delamou and Ahmed Naeem and Huseyin Arslan and El Mehdi Amhoud", "abstract": "  Millimeter wave (mmWave)-based orthogonal frequency-division multiplexing\n(OFDM) stands out as a suitable alternative for high-resolution sensing and\nhigh-speed data transmission. To meet communication and sensing requirements,\nmany works propose a static configuration where the wave's hyperparameters such\nas the number of symbols in a frame and the number of frames in a communication\nslot are already predefined. However, two facts oblige us to redefine the\nproblem, (1) the environment is often dynamic and uncertain, and (2) mmWave is\nseverely impacted by wireless environments. A striking example where this\nchallenge is very prominent is autonomous vehicle (AV). Such a system leverages\nintegrated sensing and communication (ISAC) using mmWave to manage data\ntransmission and the dynamism of the environment. In this work, we consider an\nautonomous vehicle network where an AV utilizes its queue state information\n(QSI) and channel state information (CSI) in conjunction with reinforcement\nlearning techniques to manage communication and sensing. This enables the AV to\nachieve two primary objectives: establishing a stable communication link with\nother AVs and accurately estimating the velocities of surrounding objects with\nhigh resolution. The communication performance is therefore evaluated based on\nthe queue state, the effective data rate, and the discarded packets rate. In\ncontrast, the effectiveness of the sensing is assessed using the velocity\nresolution. In addition, we exploit adaptive OFDM techniques for dynamic\nmodulation, and we suggest a reward function that leverages the age of updates\nto handle the communication buffer and improve sensing. The system is validated\nusing advantage actor-critic (A2C) and proximal policy optimization (PPO).\nFurthermore, we compare our solution with the existing design and demonstrate\nits superior performance by computer simulations.\n", "link": "http://arxiv.org/abs/2412.18500v1", "date": "2024-12-24", "relevancy": 2.0754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.518}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Adaptive%20OFDM%20and%20Reinforcement%20Learning%20Design%20for%20Autonomous%0A%20%20Vehicles%3A%20Leveraging%20Age%20of%20Updates&body=Title%3A%20Joint%20Adaptive%20OFDM%20and%20Reinforcement%20Learning%20Design%20for%20Autonomous%0A%20%20Vehicles%3A%20Leveraging%20Age%20of%20Updates%0AAuthor%3A%20Mamady%20Delamou%20and%20Ahmed%20Naeem%20and%20Huseyin%20Arslan%20and%20El%20Mehdi%20Amhoud%0AAbstract%3A%20%20%20Millimeter%20wave%20%28mmWave%29-based%20orthogonal%20frequency-division%20multiplexing%0A%28OFDM%29%20stands%20out%20as%20a%20suitable%20alternative%20for%20high-resolution%20sensing%20and%0Ahigh-speed%20data%20transmission.%20To%20meet%20communication%20and%20sensing%20requirements%2C%0Amany%20works%20propose%20a%20static%20configuration%20where%20the%20wave%27s%20hyperparameters%20such%0Aas%20the%20number%20of%20symbols%20in%20a%20frame%20and%20the%20number%20of%20frames%20in%20a%20communication%0Aslot%20are%20already%20predefined.%20However%2C%20two%20facts%20oblige%20us%20to%20redefine%20the%0Aproblem%2C%20%281%29%20the%20environment%20is%20often%20dynamic%20and%20uncertain%2C%20and%20%282%29%20mmWave%20is%0Aseverely%20impacted%20by%20wireless%20environments.%20A%20striking%20example%20where%20this%0Achallenge%20is%20very%20prominent%20is%20autonomous%20vehicle%20%28AV%29.%20Such%20a%20system%20leverages%0Aintegrated%20sensing%20and%20communication%20%28ISAC%29%20using%20mmWave%20to%20manage%20data%0Atransmission%20and%20the%20dynamism%20of%20the%20environment.%20In%20this%20work%2C%20we%20consider%20an%0Aautonomous%20vehicle%20network%20where%20an%20AV%20utilizes%20its%20queue%20state%20information%0A%28QSI%29%20and%20channel%20state%20information%20%28CSI%29%20in%20conjunction%20with%20reinforcement%0Alearning%20techniques%20to%20manage%20communication%20and%20sensing.%20This%20enables%20the%20AV%20to%0Aachieve%20two%20primary%20objectives%3A%20establishing%20a%20stable%20communication%20link%20with%0Aother%20AVs%20and%20accurately%20estimating%20the%20velocities%20of%20surrounding%20objects%20with%0Ahigh%20resolution.%20The%20communication%20performance%20is%20therefore%20evaluated%20based%20on%0Athe%20queue%20state%2C%20the%20effective%20data%20rate%2C%20and%20the%20discarded%20packets%20rate.%20In%0Acontrast%2C%20the%20effectiveness%20of%20the%20sensing%20is%20assessed%20using%20the%20velocity%0Aresolution.%20In%20addition%2C%20we%20exploit%20adaptive%20OFDM%20techniques%20for%20dynamic%0Amodulation%2C%20and%20we%20suggest%20a%20reward%20function%20that%20leverages%20the%20age%20of%20updates%0Ato%20handle%20the%20communication%20buffer%20and%20improve%20sensing.%20The%20system%20is%20validated%0Ausing%20advantage%20actor-critic%20%28A2C%29%20and%20proximal%20policy%20optimization%20%28PPO%29.%0AFurthermore%2C%20we%20compare%20our%20solution%20with%20the%20existing%20design%20and%20demonstrate%0Aits%20superior%20performance%20by%20computer%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Adaptive%2520OFDM%2520and%2520Reinforcement%2520Learning%2520Design%2520for%2520Autonomous%250A%2520%2520Vehicles%253A%2520Leveraging%2520Age%2520of%2520Updates%26entry.906535625%3DMamady%2520Delamou%2520and%2520Ahmed%2520Naeem%2520and%2520Huseyin%2520Arslan%2520and%2520El%2520Mehdi%2520Amhoud%26entry.1292438233%3D%2520%2520Millimeter%2520wave%2520%2528mmWave%2529-based%2520orthogonal%2520frequency-division%2520multiplexing%250A%2528OFDM%2529%2520stands%2520out%2520as%2520a%2520suitable%2520alternative%2520for%2520high-resolution%2520sensing%2520and%250Ahigh-speed%2520data%2520transmission.%2520To%2520meet%2520communication%2520and%2520sensing%2520requirements%252C%250Amany%2520works%2520propose%2520a%2520static%2520configuration%2520where%2520the%2520wave%2527s%2520hyperparameters%2520such%250Aas%2520the%2520number%2520of%2520symbols%2520in%2520a%2520frame%2520and%2520the%2520number%2520of%2520frames%2520in%2520a%2520communication%250Aslot%2520are%2520already%2520predefined.%2520However%252C%2520two%2520facts%2520oblige%2520us%2520to%2520redefine%2520the%250Aproblem%252C%2520%25281%2529%2520the%2520environment%2520is%2520often%2520dynamic%2520and%2520uncertain%252C%2520and%2520%25282%2529%2520mmWave%2520is%250Aseverely%2520impacted%2520by%2520wireless%2520environments.%2520A%2520striking%2520example%2520where%2520this%250Achallenge%2520is%2520very%2520prominent%2520is%2520autonomous%2520vehicle%2520%2528AV%2529.%2520Such%2520a%2520system%2520leverages%250Aintegrated%2520sensing%2520and%2520communication%2520%2528ISAC%2529%2520using%2520mmWave%2520to%2520manage%2520data%250Atransmission%2520and%2520the%2520dynamism%2520of%2520the%2520environment.%2520In%2520this%2520work%252C%2520we%2520consider%2520an%250Aautonomous%2520vehicle%2520network%2520where%2520an%2520AV%2520utilizes%2520its%2520queue%2520state%2520information%250A%2528QSI%2529%2520and%2520channel%2520state%2520information%2520%2528CSI%2529%2520in%2520conjunction%2520with%2520reinforcement%250Alearning%2520techniques%2520to%2520manage%2520communication%2520and%2520sensing.%2520This%2520enables%2520the%2520AV%2520to%250Aachieve%2520two%2520primary%2520objectives%253A%2520establishing%2520a%2520stable%2520communication%2520link%2520with%250Aother%2520AVs%2520and%2520accurately%2520estimating%2520the%2520velocities%2520of%2520surrounding%2520objects%2520with%250Ahigh%2520resolution.%2520The%2520communication%2520performance%2520is%2520therefore%2520evaluated%2520based%2520on%250Athe%2520queue%2520state%252C%2520the%2520effective%2520data%2520rate%252C%2520and%2520the%2520discarded%2520packets%2520rate.%2520In%250Acontrast%252C%2520the%2520effectiveness%2520of%2520the%2520sensing%2520is%2520assessed%2520using%2520the%2520velocity%250Aresolution.%2520In%2520addition%252C%2520we%2520exploit%2520adaptive%2520OFDM%2520techniques%2520for%2520dynamic%250Amodulation%252C%2520and%2520we%2520suggest%2520a%2520reward%2520function%2520that%2520leverages%2520the%2520age%2520of%2520updates%250Ato%2520handle%2520the%2520communication%2520buffer%2520and%2520improve%2520sensing.%2520The%2520system%2520is%2520validated%250Ausing%2520advantage%2520actor-critic%2520%2528A2C%2529%2520and%2520proximal%2520policy%2520optimization%2520%2528PPO%2529.%250AFurthermore%252C%2520we%2520compare%2520our%2520solution%2520with%2520the%2520existing%2520design%2520and%2520demonstrate%250Aits%2520superior%2520performance%2520by%2520computer%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Adaptive%20OFDM%20and%20Reinforcement%20Learning%20Design%20for%20Autonomous%0A%20%20Vehicles%3A%20Leveraging%20Age%20of%20Updates&entry.906535625=Mamady%20Delamou%20and%20Ahmed%20Naeem%20and%20Huseyin%20Arslan%20and%20El%20Mehdi%20Amhoud&entry.1292438233=%20%20Millimeter%20wave%20%28mmWave%29-based%20orthogonal%20frequency-division%20multiplexing%0A%28OFDM%29%20stands%20out%20as%20a%20suitable%20alternative%20for%20high-resolution%20sensing%20and%0Ahigh-speed%20data%20transmission.%20To%20meet%20communication%20and%20sensing%20requirements%2C%0Amany%20works%20propose%20a%20static%20configuration%20where%20the%20wave%27s%20hyperparameters%20such%0Aas%20the%20number%20of%20symbols%20in%20a%20frame%20and%20the%20number%20of%20frames%20in%20a%20communication%0Aslot%20are%20already%20predefined.%20However%2C%20two%20facts%20oblige%20us%20to%20redefine%20the%0Aproblem%2C%20%281%29%20the%20environment%20is%20often%20dynamic%20and%20uncertain%2C%20and%20%282%29%20mmWave%20is%0Aseverely%20impacted%20by%20wireless%20environments.%20A%20striking%20example%20where%20this%0Achallenge%20is%20very%20prominent%20is%20autonomous%20vehicle%20%28AV%29.%20Such%20a%20system%20leverages%0Aintegrated%20sensing%20and%20communication%20%28ISAC%29%20using%20mmWave%20to%20manage%20data%0Atransmission%20and%20the%20dynamism%20of%20the%20environment.%20In%20this%20work%2C%20we%20consider%20an%0Aautonomous%20vehicle%20network%20where%20an%20AV%20utilizes%20its%20queue%20state%20information%0A%28QSI%29%20and%20channel%20state%20information%20%28CSI%29%20in%20conjunction%20with%20reinforcement%0Alearning%20techniques%20to%20manage%20communication%20and%20sensing.%20This%20enables%20the%20AV%20to%0Aachieve%20two%20primary%20objectives%3A%20establishing%20a%20stable%20communication%20link%20with%0Aother%20AVs%20and%20accurately%20estimating%20the%20velocities%20of%20surrounding%20objects%20with%0Ahigh%20resolution.%20The%20communication%20performance%20is%20therefore%20evaluated%20based%20on%0Athe%20queue%20state%2C%20the%20effective%20data%20rate%2C%20and%20the%20discarded%20packets%20rate.%20In%0Acontrast%2C%20the%20effectiveness%20of%20the%20sensing%20is%20assessed%20using%20the%20velocity%0Aresolution.%20In%20addition%2C%20we%20exploit%20adaptive%20OFDM%20techniques%20for%20dynamic%0Amodulation%2C%20and%20we%20suggest%20a%20reward%20function%20that%20leverages%20the%20age%20of%20updates%0Ato%20handle%20the%20communication%20buffer%20and%20improve%20sensing.%20The%20system%20is%20validated%0Ausing%20advantage%20actor-critic%20%28A2C%29%20and%20proximal%20policy%20optimization%20%28PPO%29.%0AFurthermore%2C%20we%20compare%20our%20solution%20with%20the%20existing%20design%20and%20demonstrate%0Aits%20superior%20performance%20by%20computer%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18500v1&entry.124074799=Read"},
{"title": "Improving robustness to corruptions with multiplicative weight\n  perturbations", "author": "Trung Trinh and Markus Heinonen and Luigi Acerbi and Samuel Kaski", "abstract": "  Deep neural networks (DNNs) excel on clean images but struggle with corrupted\nones. Incorporating specific corruptions into the data augmentation pipeline\ncan improve robustness to those corruptions but may harm performance on clean\nimages and other types of distortion. In this paper, we introduce an\nalternative approach that improves the robustness of DNNs to a wide range of\ncorruptions without compromising accuracy on clean images. We first demonstrate\nthat input perturbations can be mimicked by multiplicative perturbations in the\nweight space. Leveraging this, we propose Data Augmentation via Multiplicative\nPerturbation (DAMP), a training method that optimizes DNNs under random\nmultiplicative weight perturbations. We also examine the recently proposed\nAdaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs\nunder adversarial multiplicative weight perturbations. Experiments on image\nclassification datasets (CIFAR-10/100, TinyImageNet and ImageNet) and neural\nnetwork architectures (ResNet50, ViT-S/16, ViT-B/16) show that DAMP enhances\nmodel generalization performance in the presence of corruptions across\ndifferent settings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from\nscratch, reaching the top-1 error of 23.7% which is comparable to ResNet50\nwithout extensive data augmentations.\n", "link": "http://arxiv.org/abs/2406.16540v3", "date": "2024-12-24", "relevancy": 2.0649, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5218}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5206}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations&body=Title%3A%20Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations%0AAuthor%3A%20Trung%20Trinh%20and%20Markus%20Heinonen%20and%20Luigi%20Acerbi%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20excel%20on%20clean%20images%20but%20struggle%20with%20corrupted%0Aones.%20Incorporating%20specific%20corruptions%20into%20the%20data%20augmentation%20pipeline%0Acan%20improve%20robustness%20to%20those%20corruptions%20but%20may%20harm%20performance%20on%20clean%0Aimages%20and%20other%20types%20of%20distortion.%20In%20this%20paper%2C%20we%20introduce%20an%0Aalternative%20approach%20that%20improves%20the%20robustness%20of%20DNNs%20to%20a%20wide%20range%20of%0Acorruptions%20without%20compromising%20accuracy%20on%20clean%20images.%20We%20first%20demonstrate%0Athat%20input%20perturbations%20can%20be%20mimicked%20by%20multiplicative%20perturbations%20in%20the%0Aweight%20space.%20Leveraging%20this%2C%20we%20propose%20Data%20Augmentation%20via%20Multiplicative%0APerturbation%20%28DAMP%29%2C%20a%20training%20method%20that%20optimizes%20DNNs%20under%20random%0Amultiplicative%20weight%20perturbations.%20We%20also%20examine%20the%20recently%20proposed%0AAdaptive%20Sharpness-Aware%20Minimization%20%28ASAM%29%20and%20show%20that%20it%20optimizes%20DNNs%0Aunder%20adversarial%20multiplicative%20weight%20perturbations.%20Experiments%20on%20image%0Aclassification%20datasets%20%28CIFAR-10/100%2C%20TinyImageNet%20and%20ImageNet%29%20and%20neural%0Anetwork%20architectures%20%28ResNet50%2C%20ViT-S/16%2C%20ViT-B/16%29%20show%20that%20DAMP%20enhances%0Amodel%20generalization%20performance%20in%20the%20presence%20of%20corruptions%20across%0Adifferent%20settings.%20Notably%2C%20DAMP%20is%20able%20to%20train%20a%20ViT-S/16%20on%20ImageNet%20from%0Ascratch%2C%20reaching%20the%20top-1%20error%20of%2023.7%25%20which%20is%20comparable%20to%20ResNet50%0Awithout%20extensive%20data%20augmentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520robustness%2520to%2520corruptions%2520with%2520multiplicative%2520weight%250A%2520%2520perturbations%26entry.906535625%3DTrung%2520Trinh%2520and%2520Markus%2520Heinonen%2520and%2520Luigi%2520Acerbi%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520excel%2520on%2520clean%2520images%2520but%2520struggle%2520with%2520corrupted%250Aones.%2520Incorporating%2520specific%2520corruptions%2520into%2520the%2520data%2520augmentation%2520pipeline%250Acan%2520improve%2520robustness%2520to%2520those%2520corruptions%2520but%2520may%2520harm%2520performance%2520on%2520clean%250Aimages%2520and%2520other%2520types%2520of%2520distortion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%250Aalternative%2520approach%2520that%2520improves%2520the%2520robustness%2520of%2520DNNs%2520to%2520a%2520wide%2520range%2520of%250Acorruptions%2520without%2520compromising%2520accuracy%2520on%2520clean%2520images.%2520We%2520first%2520demonstrate%250Athat%2520input%2520perturbations%2520can%2520be%2520mimicked%2520by%2520multiplicative%2520perturbations%2520in%2520the%250Aweight%2520space.%2520Leveraging%2520this%252C%2520we%2520propose%2520Data%2520Augmentation%2520via%2520Multiplicative%250APerturbation%2520%2528DAMP%2529%252C%2520a%2520training%2520method%2520that%2520optimizes%2520DNNs%2520under%2520random%250Amultiplicative%2520weight%2520perturbations.%2520We%2520also%2520examine%2520the%2520recently%2520proposed%250AAdaptive%2520Sharpness-Aware%2520Minimization%2520%2528ASAM%2529%2520and%2520show%2520that%2520it%2520optimizes%2520DNNs%250Aunder%2520adversarial%2520multiplicative%2520weight%2520perturbations.%2520Experiments%2520on%2520image%250Aclassification%2520datasets%2520%2528CIFAR-10/100%252C%2520TinyImageNet%2520and%2520ImageNet%2529%2520and%2520neural%250Anetwork%2520architectures%2520%2528ResNet50%252C%2520ViT-S/16%252C%2520ViT-B/16%2529%2520show%2520that%2520DAMP%2520enhances%250Amodel%2520generalization%2520performance%2520in%2520the%2520presence%2520of%2520corruptions%2520across%250Adifferent%2520settings.%2520Notably%252C%2520DAMP%2520is%2520able%2520to%2520train%2520a%2520ViT-S/16%2520on%2520ImageNet%2520from%250Ascratch%252C%2520reaching%2520the%2520top-1%2520error%2520of%252023.7%2525%2520which%2520is%2520comparable%2520to%2520ResNet50%250Awithout%2520extensive%2520data%2520augmentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations&entry.906535625=Trung%20Trinh%20and%20Markus%20Heinonen%20and%20Luigi%20Acerbi%20and%20Samuel%20Kaski&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20excel%20on%20clean%20images%20but%20struggle%20with%20corrupted%0Aones.%20Incorporating%20specific%20corruptions%20into%20the%20data%20augmentation%20pipeline%0Acan%20improve%20robustness%20to%20those%20corruptions%20but%20may%20harm%20performance%20on%20clean%0Aimages%20and%20other%20types%20of%20distortion.%20In%20this%20paper%2C%20we%20introduce%20an%0Aalternative%20approach%20that%20improves%20the%20robustness%20of%20DNNs%20to%20a%20wide%20range%20of%0Acorruptions%20without%20compromising%20accuracy%20on%20clean%20images.%20We%20first%20demonstrate%0Athat%20input%20perturbations%20can%20be%20mimicked%20by%20multiplicative%20perturbations%20in%20the%0Aweight%20space.%20Leveraging%20this%2C%20we%20propose%20Data%20Augmentation%20via%20Multiplicative%0APerturbation%20%28DAMP%29%2C%20a%20training%20method%20that%20optimizes%20DNNs%20under%20random%0Amultiplicative%20weight%20perturbations.%20We%20also%20examine%20the%20recently%20proposed%0AAdaptive%20Sharpness-Aware%20Minimization%20%28ASAM%29%20and%20show%20that%20it%20optimizes%20DNNs%0Aunder%20adversarial%20multiplicative%20weight%20perturbations.%20Experiments%20on%20image%0Aclassification%20datasets%20%28CIFAR-10/100%2C%20TinyImageNet%20and%20ImageNet%29%20and%20neural%0Anetwork%20architectures%20%28ResNet50%2C%20ViT-S/16%2C%20ViT-B/16%29%20show%20that%20DAMP%20enhances%0Amodel%20generalization%20performance%20in%20the%20presence%20of%20corruptions%20across%0Adifferent%20settings.%20Notably%2C%20DAMP%20is%20able%20to%20train%20a%20ViT-S/16%20on%20ImageNet%20from%0Ascratch%2C%20reaching%20the%20top-1%20error%20of%2023.7%25%20which%20is%20comparable%20to%20ResNet50%0Awithout%20extensive%20data%20augmentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16540v3&entry.124074799=Read"},
{"title": "GeAR: Graph-enhanced Agent for Retrieval-augmented Generation", "author": "Zhili Shen and Chenxin Diao and Pavlos Vougiouklis and Pascual Merita and Shriram Piramanayagam and Damien Graux and Dandan Tu and Zeren Jiang and Ruofei Lai and Yang Ren and Jeff Z. Pan", "abstract": "  Retrieval-augmented generation systems rely on effective document retrieval\ncapabilities. By design, conventional sparse or dense retrievers face\nchallenges in multi-hop retrieval scenarios. In this paper, we present GeAR,\nwhich advances RAG performance through two key innovations: (i) graph\nexpansion, which enhances any conventional base retriever, such as BM25, and\n(ii) an agent framework that incorporates graph expansion. Our evaluation\ndemonstrates GeAR's superior retrieval performance on three multi-hop question\nanswering datasets. Additionally, our system achieves state-of-the-art results\nwith improvements exceeding 10% on the challenging MuSiQue dataset, while\nrequiring fewer tokens and iterations compared to other multi-step retrieval\nsystems.\n", "link": "http://arxiv.org/abs/2412.18431v1", "date": "2024-12-24", "relevancy": 2.0605, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5285}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5148}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeAR%3A%20Graph-enhanced%20Agent%20for%20Retrieval-augmented%20Generation&body=Title%3A%20GeAR%3A%20Graph-enhanced%20Agent%20for%20Retrieval-augmented%20Generation%0AAuthor%3A%20Zhili%20Shen%20and%20Chenxin%20Diao%20and%20Pavlos%20Vougiouklis%20and%20Pascual%20Merita%20and%20Shriram%20Piramanayagam%20and%20Damien%20Graux%20and%20Dandan%20Tu%20and%20Zeren%20Jiang%20and%20Ruofei%20Lai%20and%20Yang%20Ren%20and%20Jeff%20Z.%20Pan%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20systems%20rely%20on%20effective%20document%20retrieval%0Acapabilities.%20By%20design%2C%20conventional%20sparse%20or%20dense%20retrievers%20face%0Achallenges%20in%20multi-hop%20retrieval%20scenarios.%20In%20this%20paper%2C%20we%20present%20GeAR%2C%0Awhich%20advances%20RAG%20performance%20through%20two%20key%20innovations%3A%20%28i%29%20graph%0Aexpansion%2C%20which%20enhances%20any%20conventional%20base%20retriever%2C%20such%20as%20BM25%2C%20and%0A%28ii%29%20an%20agent%20framework%20that%20incorporates%20graph%20expansion.%20Our%20evaluation%0Ademonstrates%20GeAR%27s%20superior%20retrieval%20performance%20on%20three%20multi-hop%20question%0Aanswering%20datasets.%20Additionally%2C%20our%20system%20achieves%20state-of-the-art%20results%0Awith%20improvements%20exceeding%2010%25%20on%20the%20challenging%20MuSiQue%20dataset%2C%20while%0Arequiring%20fewer%20tokens%20and%20iterations%20compared%20to%20other%20multi-step%20retrieval%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeAR%253A%2520Graph-enhanced%2520Agent%2520for%2520Retrieval-augmented%2520Generation%26entry.906535625%3DZhili%2520Shen%2520and%2520Chenxin%2520Diao%2520and%2520Pavlos%2520Vougiouklis%2520and%2520Pascual%2520Merita%2520and%2520Shriram%2520Piramanayagam%2520and%2520Damien%2520Graux%2520and%2520Dandan%2520Tu%2520and%2520Zeren%2520Jiang%2520and%2520Ruofei%2520Lai%2520and%2520Yang%2520Ren%2520and%2520Jeff%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520systems%2520rely%2520on%2520effective%2520document%2520retrieval%250Acapabilities.%2520By%2520design%252C%2520conventional%2520sparse%2520or%2520dense%2520retrievers%2520face%250Achallenges%2520in%2520multi-hop%2520retrieval%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520GeAR%252C%250Awhich%2520advances%2520RAG%2520performance%2520through%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520graph%250Aexpansion%252C%2520which%2520enhances%2520any%2520conventional%2520base%2520retriever%252C%2520such%2520as%2520BM25%252C%2520and%250A%2528ii%2529%2520an%2520agent%2520framework%2520that%2520incorporates%2520graph%2520expansion.%2520Our%2520evaluation%250Ademonstrates%2520GeAR%2527s%2520superior%2520retrieval%2520performance%2520on%2520three%2520multi-hop%2520question%250Aanswering%2520datasets.%2520Additionally%252C%2520our%2520system%2520achieves%2520state-of-the-art%2520results%250Awith%2520improvements%2520exceeding%252010%2525%2520on%2520the%2520challenging%2520MuSiQue%2520dataset%252C%2520while%250Arequiring%2520fewer%2520tokens%2520and%2520iterations%2520compared%2520to%2520other%2520multi-step%2520retrieval%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeAR%3A%20Graph-enhanced%20Agent%20for%20Retrieval-augmented%20Generation&entry.906535625=Zhili%20Shen%20and%20Chenxin%20Diao%20and%20Pavlos%20Vougiouklis%20and%20Pascual%20Merita%20and%20Shriram%20Piramanayagam%20and%20Damien%20Graux%20and%20Dandan%20Tu%20and%20Zeren%20Jiang%20and%20Ruofei%20Lai%20and%20Yang%20Ren%20and%20Jeff%20Z.%20Pan&entry.1292438233=%20%20Retrieval-augmented%20generation%20systems%20rely%20on%20effective%20document%20retrieval%0Acapabilities.%20By%20design%2C%20conventional%20sparse%20or%20dense%20retrievers%20face%0Achallenges%20in%20multi-hop%20retrieval%20scenarios.%20In%20this%20paper%2C%20we%20present%20GeAR%2C%0Awhich%20advances%20RAG%20performance%20through%20two%20key%20innovations%3A%20%28i%29%20graph%0Aexpansion%2C%20which%20enhances%20any%20conventional%20base%20retriever%2C%20such%20as%20BM25%2C%20and%0A%28ii%29%20an%20agent%20framework%20that%20incorporates%20graph%20expansion.%20Our%20evaluation%0Ademonstrates%20GeAR%27s%20superior%20retrieval%20performance%20on%20three%20multi-hop%20question%0Aanswering%20datasets.%20Additionally%2C%20our%20system%20achieves%20state-of-the-art%20results%0Awith%20improvements%20exceeding%2010%25%20on%20the%20challenging%20MuSiQue%20dataset%2C%20while%0Arequiring%20fewer%20tokens%20and%20iterations%20compared%20to%20other%20multi-step%20retrieval%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18431v1&entry.124074799=Read"},
{"title": "FedVCK: Non-IID Robust and Communication-Efficient Federated Learning\n  via Valuable Condensed Knowledge for Medical Image Analysis", "author": "Guochen Yan and Luyuan Xie and Xinyi Gao and Wentao Zhang and Qingni Shen and Yuejian Fang and Zhonghai Wu", "abstract": "  Federated learning has become a promising solution for collaboration among\nmedical institutions. However, data owned by each institution would be highly\nheterogeneous and the distribution is always non-independent and identical\ndistribution (non-IID), resulting in client drift and unsatisfactory\nperformance. Despite existing federated learning methods attempting to solve\nthe non-IID problems, they still show marginal advantages but rely on frequent\ncommunication which would incur high costs and privacy concerns. In this paper,\nwe propose a novel federated learning method: \\textbf{Fed}erated learning via\n\\textbf{V}aluable \\textbf{C}ondensed \\textbf{K}nowledge (FedVCK). We enhance\nthe quality of condensed knowledge and select the most necessary knowledge\nguided by models, to tackle the non-IID problem within limited communication\nbudgets effectively. Specifically, on the client side, we condense the\nknowledge of each client into a small dataset and further enhance the\ncondensation procedure with latent distribution constraints, facilitating the\neffective capture of high-quality knowledge. During each round, we specifically\ntarget and condense knowledge that has not been assimilated by the current\nmodel, thereby preventing unnecessary repetition of homogeneous knowledge and\nminimizing the frequency of communications required. On the server side, we\npropose relational supervised contrastive learning to provide more supervision\nsignals to aid the global model updating. Comprehensive experiments across\nvarious medical tasks show that FedVCK can outperform state-of-the-art methods,\ndemonstrating that it's non-IID robust and communication-efficient.\n", "link": "http://arxiv.org/abs/2412.18557v1", "date": "2024-12-24", "relevancy": 2.0529, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5347}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5132}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedVCK%3A%20Non-IID%20Robust%20and%20Communication-Efficient%20Federated%20Learning%0A%20%20via%20Valuable%20Condensed%20Knowledge%20for%20Medical%20Image%20Analysis&body=Title%3A%20FedVCK%3A%20Non-IID%20Robust%20and%20Communication-Efficient%20Federated%20Learning%0A%20%20via%20Valuable%20Condensed%20Knowledge%20for%20Medical%20Image%20Analysis%0AAuthor%3A%20Guochen%20Yan%20and%20Luyuan%20Xie%20and%20Xinyi%20Gao%20and%20Wentao%20Zhang%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu%0AAbstract%3A%20%20%20Federated%20learning%20has%20become%20a%20promising%20solution%20for%20collaboration%20among%0Amedical%20institutions.%20However%2C%20data%20owned%20by%20each%20institution%20would%20be%20highly%0Aheterogeneous%20and%20the%20distribution%20is%20always%20non-independent%20and%20identical%0Adistribution%20%28non-IID%29%2C%20resulting%20in%20client%20drift%20and%20unsatisfactory%0Aperformance.%20Despite%20existing%20federated%20learning%20methods%20attempting%20to%20solve%0Athe%20non-IID%20problems%2C%20they%20still%20show%20marginal%20advantages%20but%20rely%20on%20frequent%0Acommunication%20which%20would%20incur%20high%20costs%20and%20privacy%20concerns.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20federated%20learning%20method%3A%20%5Ctextbf%7BFed%7Derated%20learning%20via%0A%5Ctextbf%7BV%7Daluable%20%5Ctextbf%7BC%7Dondensed%20%5Ctextbf%7BK%7Dnowledge%20%28FedVCK%29.%20We%20enhance%0Athe%20quality%20of%20condensed%20knowledge%20and%20select%20the%20most%20necessary%20knowledge%0Aguided%20by%20models%2C%20to%20tackle%20the%20non-IID%20problem%20within%20limited%20communication%0Abudgets%20effectively.%20Specifically%2C%20on%20the%20client%20side%2C%20we%20condense%20the%0Aknowledge%20of%20each%20client%20into%20a%20small%20dataset%20and%20further%20enhance%20the%0Acondensation%20procedure%20with%20latent%20distribution%20constraints%2C%20facilitating%20the%0Aeffective%20capture%20of%20high-quality%20knowledge.%20During%20each%20round%2C%20we%20specifically%0Atarget%20and%20condense%20knowledge%20that%20has%20not%20been%20assimilated%20by%20the%20current%0Amodel%2C%20thereby%20preventing%20unnecessary%20repetition%20of%20homogeneous%20knowledge%20and%0Aminimizing%20the%20frequency%20of%20communications%20required.%20On%20the%20server%20side%2C%20we%0Apropose%20relational%20supervised%20contrastive%20learning%20to%20provide%20more%20supervision%0Asignals%20to%20aid%20the%20global%20model%20updating.%20Comprehensive%20experiments%20across%0Avarious%20medical%20tasks%20show%20that%20FedVCK%20can%20outperform%20state-of-the-art%20methods%2C%0Ademonstrating%20that%20it%27s%20non-IID%20robust%20and%20communication-efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedVCK%253A%2520Non-IID%2520Robust%2520and%2520Communication-Efficient%2520Federated%2520Learning%250A%2520%2520via%2520Valuable%2520Condensed%2520Knowledge%2520for%2520Medical%2520Image%2520Analysis%26entry.906535625%3DGuochen%2520Yan%2520and%2520Luyuan%2520Xie%2520and%2520Xinyi%2520Gao%2520and%2520Wentao%2520Zhang%2520and%2520Qingni%2520Shen%2520and%2520Yuejian%2520Fang%2520and%2520Zhonghai%2520Wu%26entry.1292438233%3D%2520%2520Federated%2520learning%2520has%2520become%2520a%2520promising%2520solution%2520for%2520collaboration%2520among%250Amedical%2520institutions.%2520However%252C%2520data%2520owned%2520by%2520each%2520institution%2520would%2520be%2520highly%250Aheterogeneous%2520and%2520the%2520distribution%2520is%2520always%2520non-independent%2520and%2520identical%250Adistribution%2520%2528non-IID%2529%252C%2520resulting%2520in%2520client%2520drift%2520and%2520unsatisfactory%250Aperformance.%2520Despite%2520existing%2520federated%2520learning%2520methods%2520attempting%2520to%2520solve%250Athe%2520non-IID%2520problems%252C%2520they%2520still%2520show%2520marginal%2520advantages%2520but%2520rely%2520on%2520frequent%250Acommunication%2520which%2520would%2520incur%2520high%2520costs%2520and%2520privacy%2520concerns.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520federated%2520learning%2520method%253A%2520%255Ctextbf%257BFed%257Derated%2520learning%2520via%250A%255Ctextbf%257BV%257Daluable%2520%255Ctextbf%257BC%257Dondensed%2520%255Ctextbf%257BK%257Dnowledge%2520%2528FedVCK%2529.%2520We%2520enhance%250Athe%2520quality%2520of%2520condensed%2520knowledge%2520and%2520select%2520the%2520most%2520necessary%2520knowledge%250Aguided%2520by%2520models%252C%2520to%2520tackle%2520the%2520non-IID%2520problem%2520within%2520limited%2520communication%250Abudgets%2520effectively.%2520Specifically%252C%2520on%2520the%2520client%2520side%252C%2520we%2520condense%2520the%250Aknowledge%2520of%2520each%2520client%2520into%2520a%2520small%2520dataset%2520and%2520further%2520enhance%2520the%250Acondensation%2520procedure%2520with%2520latent%2520distribution%2520constraints%252C%2520facilitating%2520the%250Aeffective%2520capture%2520of%2520high-quality%2520knowledge.%2520During%2520each%2520round%252C%2520we%2520specifically%250Atarget%2520and%2520condense%2520knowledge%2520that%2520has%2520not%2520been%2520assimilated%2520by%2520the%2520current%250Amodel%252C%2520thereby%2520preventing%2520unnecessary%2520repetition%2520of%2520homogeneous%2520knowledge%2520and%250Aminimizing%2520the%2520frequency%2520of%2520communications%2520required.%2520On%2520the%2520server%2520side%252C%2520we%250Apropose%2520relational%2520supervised%2520contrastive%2520learning%2520to%2520provide%2520more%2520supervision%250Asignals%2520to%2520aid%2520the%2520global%2520model%2520updating.%2520Comprehensive%2520experiments%2520across%250Avarious%2520medical%2520tasks%2520show%2520that%2520FedVCK%2520can%2520outperform%2520state-of-the-art%2520methods%252C%250Ademonstrating%2520that%2520it%2527s%2520non-IID%2520robust%2520and%2520communication-efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedVCK%3A%20Non-IID%20Robust%20and%20Communication-Efficient%20Federated%20Learning%0A%20%20via%20Valuable%20Condensed%20Knowledge%20for%20Medical%20Image%20Analysis&entry.906535625=Guochen%20Yan%20and%20Luyuan%20Xie%20and%20Xinyi%20Gao%20and%20Wentao%20Zhang%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu&entry.1292438233=%20%20Federated%20learning%20has%20become%20a%20promising%20solution%20for%20collaboration%20among%0Amedical%20institutions.%20However%2C%20data%20owned%20by%20each%20institution%20would%20be%20highly%0Aheterogeneous%20and%20the%20distribution%20is%20always%20non-independent%20and%20identical%0Adistribution%20%28non-IID%29%2C%20resulting%20in%20client%20drift%20and%20unsatisfactory%0Aperformance.%20Despite%20existing%20federated%20learning%20methods%20attempting%20to%20solve%0Athe%20non-IID%20problems%2C%20they%20still%20show%20marginal%20advantages%20but%20rely%20on%20frequent%0Acommunication%20which%20would%20incur%20high%20costs%20and%20privacy%20concerns.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20federated%20learning%20method%3A%20%5Ctextbf%7BFed%7Derated%20learning%20via%0A%5Ctextbf%7BV%7Daluable%20%5Ctextbf%7BC%7Dondensed%20%5Ctextbf%7BK%7Dnowledge%20%28FedVCK%29.%20We%20enhance%0Athe%20quality%20of%20condensed%20knowledge%20and%20select%20the%20most%20necessary%20knowledge%0Aguided%20by%20models%2C%20to%20tackle%20the%20non-IID%20problem%20within%20limited%20communication%0Abudgets%20effectively.%20Specifically%2C%20on%20the%20client%20side%2C%20we%20condense%20the%0Aknowledge%20of%20each%20client%20into%20a%20small%20dataset%20and%20further%20enhance%20the%0Acondensation%20procedure%20with%20latent%20distribution%20constraints%2C%20facilitating%20the%0Aeffective%20capture%20of%20high-quality%20knowledge.%20During%20each%20round%2C%20we%20specifically%0Atarget%20and%20condense%20knowledge%20that%20has%20not%20been%20assimilated%20by%20the%20current%0Amodel%2C%20thereby%20preventing%20unnecessary%20repetition%20of%20homogeneous%20knowledge%20and%0Aminimizing%20the%20frequency%20of%20communications%20required.%20On%20the%20server%20side%2C%20we%0Apropose%20relational%20supervised%20contrastive%20learning%20to%20provide%20more%20supervision%0Asignals%20to%20aid%20the%20global%20model%20updating.%20Comprehensive%20experiments%20across%0Avarious%20medical%20tasks%20show%20that%20FedVCK%20can%20outperform%20state-of-the-art%20methods%2C%0Ademonstrating%20that%20it%27s%20non-IID%20robust%20and%20communication-efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18557v1&entry.124074799=Read"},
{"title": "GeFL: Model-Agnostic Federated Learning with Generative Models", "author": "Honggu Kang and Seohyeon Cha and Joonhyuk Kang", "abstract": "  Federated learning (FL) is a promising paradigm in distributed learning while\npreserving the privacy of users. However, the increasing size of recent models\nmakes it unaffordable for a few users to encompass the model. It leads the\nusers to adopt heterogeneous models based on their diverse computing\ncapabilities and network bandwidth. Correspondingly, FL with heterogeneous\nmodels should be addressed, given that FL typically involves training a single\nglobal model. In this paper, we propose Generative Model-Aided Federated\nLearning (GeFL), incorporating a generative model that aggregates global\nknowledge across users of heterogeneous models. Our experiments on various\nclassification tasks demonstrate notable performance improvements of GeFL\ncompared to baselines, as well as limitations in terms of privacy and\nscalability. To tackle these concerns, we introduce a novel framework, GeFL-F.\nIt trains target networks aided by feature-generative models. We empirically\ndemonstrate the consistent performance gains of GeFL-F, while demonstrating\nbetter privacy preservation and robustness to a large number of clients. Codes\nare available at [1].\n", "link": "http://arxiv.org/abs/2412.18460v1", "date": "2024-12-24", "relevancy": 2.0517, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5201}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5105}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeFL%3A%20Model-Agnostic%20Federated%20Learning%20with%20Generative%20Models&body=Title%3A%20GeFL%3A%20Model-Agnostic%20Federated%20Learning%20with%20Generative%20Models%0AAuthor%3A%20Honggu%20Kang%20and%20Seohyeon%20Cha%20and%20Joonhyuk%20Kang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20paradigm%20in%20distributed%20learning%20while%0Apreserving%20the%20privacy%20of%20users.%20However%2C%20the%20increasing%20size%20of%20recent%20models%0Amakes%20it%20unaffordable%20for%20a%20few%20users%20to%20encompass%20the%20model.%20It%20leads%20the%0Ausers%20to%20adopt%20heterogeneous%20models%20based%20on%20their%20diverse%20computing%0Acapabilities%20and%20network%20bandwidth.%20Correspondingly%2C%20FL%20with%20heterogeneous%0Amodels%20should%20be%20addressed%2C%20given%20that%20FL%20typically%20involves%20training%20a%20single%0Aglobal%20model.%20In%20this%20paper%2C%20we%20propose%20Generative%20Model-Aided%20Federated%0ALearning%20%28GeFL%29%2C%20incorporating%20a%20generative%20model%20that%20aggregates%20global%0Aknowledge%20across%20users%20of%20heterogeneous%20models.%20Our%20experiments%20on%20various%0Aclassification%20tasks%20demonstrate%20notable%20performance%20improvements%20of%20GeFL%0Acompared%20to%20baselines%2C%20as%20well%20as%20limitations%20in%20terms%20of%20privacy%20and%0Ascalability.%20To%20tackle%20these%20concerns%2C%20we%20introduce%20a%20novel%20framework%2C%20GeFL-F.%0AIt%20trains%20target%20networks%20aided%20by%20feature-generative%20models.%20We%20empirically%0Ademonstrate%20the%20consistent%20performance%20gains%20of%20GeFL-F%2C%20while%20demonstrating%0Abetter%20privacy%20preservation%20and%20robustness%20to%20a%20large%20number%20of%20clients.%20Codes%0Aare%20available%20at%20%5B1%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeFL%253A%2520Model-Agnostic%2520Federated%2520Learning%2520with%2520Generative%2520Models%26entry.906535625%3DHonggu%2520Kang%2520and%2520Seohyeon%2520Cha%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520promising%2520paradigm%2520in%2520distributed%2520learning%2520while%250Apreserving%2520the%2520privacy%2520of%2520users.%2520However%252C%2520the%2520increasing%2520size%2520of%2520recent%2520models%250Amakes%2520it%2520unaffordable%2520for%2520a%2520few%2520users%2520to%2520encompass%2520the%2520model.%2520It%2520leads%2520the%250Ausers%2520to%2520adopt%2520heterogeneous%2520models%2520based%2520on%2520their%2520diverse%2520computing%250Acapabilities%2520and%2520network%2520bandwidth.%2520Correspondingly%252C%2520FL%2520with%2520heterogeneous%250Amodels%2520should%2520be%2520addressed%252C%2520given%2520that%2520FL%2520typically%2520involves%2520training%2520a%2520single%250Aglobal%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Generative%2520Model-Aided%2520Federated%250ALearning%2520%2528GeFL%2529%252C%2520incorporating%2520a%2520generative%2520model%2520that%2520aggregates%2520global%250Aknowledge%2520across%2520users%2520of%2520heterogeneous%2520models.%2520Our%2520experiments%2520on%2520various%250Aclassification%2520tasks%2520demonstrate%2520notable%2520performance%2520improvements%2520of%2520GeFL%250Acompared%2520to%2520baselines%252C%2520as%2520well%2520as%2520limitations%2520in%2520terms%2520of%2520privacy%2520and%250Ascalability.%2520To%2520tackle%2520these%2520concerns%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%2520GeFL-F.%250AIt%2520trains%2520target%2520networks%2520aided%2520by%2520feature-generative%2520models.%2520We%2520empirically%250Ademonstrate%2520the%2520consistent%2520performance%2520gains%2520of%2520GeFL-F%252C%2520while%2520demonstrating%250Abetter%2520privacy%2520preservation%2520and%2520robustness%2520to%2520a%2520large%2520number%2520of%2520clients.%2520Codes%250Aare%2520available%2520at%2520%255B1%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeFL%3A%20Model-Agnostic%20Federated%20Learning%20with%20Generative%20Models&entry.906535625=Honggu%20Kang%20and%20Seohyeon%20Cha%20and%20Joonhyuk%20Kang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20paradigm%20in%20distributed%20learning%20while%0Apreserving%20the%20privacy%20of%20users.%20However%2C%20the%20increasing%20size%20of%20recent%20models%0Amakes%20it%20unaffordable%20for%20a%20few%20users%20to%20encompass%20the%20model.%20It%20leads%20the%0Ausers%20to%20adopt%20heterogeneous%20models%20based%20on%20their%20diverse%20computing%0Acapabilities%20and%20network%20bandwidth.%20Correspondingly%2C%20FL%20with%20heterogeneous%0Amodels%20should%20be%20addressed%2C%20given%20that%20FL%20typically%20involves%20training%20a%20single%0Aglobal%20model.%20In%20this%20paper%2C%20we%20propose%20Generative%20Model-Aided%20Federated%0ALearning%20%28GeFL%29%2C%20incorporating%20a%20generative%20model%20that%20aggregates%20global%0Aknowledge%20across%20users%20of%20heterogeneous%20models.%20Our%20experiments%20on%20various%0Aclassification%20tasks%20demonstrate%20notable%20performance%20improvements%20of%20GeFL%0Acompared%20to%20baselines%2C%20as%20well%20as%20limitations%20in%20terms%20of%20privacy%20and%0Ascalability.%20To%20tackle%20these%20concerns%2C%20we%20introduce%20a%20novel%20framework%2C%20GeFL-F.%0AIt%20trains%20target%20networks%20aided%20by%20feature-generative%20models.%20We%20empirically%0Ademonstrate%20the%20consistent%20performance%20gains%20of%20GeFL-F%2C%20while%20demonstrating%0Abetter%20privacy%20preservation%20and%20robustness%20to%20a%20large%20number%20of%20clients.%20Codes%0Aare%20available%20at%20%5B1%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18460v1&entry.124074799=Read"},
{"title": "SIGMA: Selective Gated Mamba for Sequential Recommendation", "author": "Ziwei Liu and Qidong Liu and Yejing Wang and Wanyu Wang and Pengyue Jia and Maolin Wang and Zitao Liu and Yi Chang and Xiangyu Zhao", "abstract": "  In various domains, Sequential Recommender Systems (SRS) have become\nessential due to their superior capability to discern intricate user\npreferences. Typically, SRS utilize transformer-based architectures to forecast\nthe subsequent item within a sequence. Nevertheless, the quadratic\ncomputational complexity inherent in these models often leads to\ninefficiencies, hindering the achievement of real-time recommendations. Mamba,\na recent advancement, has exhibited exceptional performance in time series\nprediction, significantly enhancing both efficiency and accuracy. However,\nintegrating Mamba directly into SRS poses several challenges. Its inherently\nunidirectional nature may constrain the model's capacity to capture the full\ncontext of user-item interactions, while its instability in state estimation\ncan compromise its ability to detect short-term patterns within interaction\nsequences.\n  To overcome these issues, we introduce a new framework named Selective Gated\nMamba (SIGMA) for Sequential Recommendation. This framework leverages a\nPartially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture\nspecifically tailored to improve contextual modeling. Additionally, an\ninput-sensitive Dense Selective Gate (DS Gate) is employed to optimize\ndirectional weights and enhance the processing of sequential information in\nPF-Mamba. For short sequence modeling, we have also developed a Feature Extract\nGRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results\nindicate that SIGMA outperforms current models on five real-world datasets. Our\nimplementation code is available at https://github.com/ziwliu-cityu/SIMGA to\nease reproducibility.\n", "link": "http://arxiv.org/abs/2408.11451v4", "date": "2024-12-24", "relevancy": 2.0413, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5143}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGMA%3A%20Selective%20Gated%20Mamba%20for%20Sequential%20Recommendation&body=Title%3A%20SIGMA%3A%20Selective%20Gated%20Mamba%20for%20Sequential%20Recommendation%0AAuthor%3A%20Ziwei%20Liu%20and%20Qidong%20Liu%20and%20Yejing%20Wang%20and%20Wanyu%20Wang%20and%20Pengyue%20Jia%20and%20Maolin%20Wang%20and%20Zitao%20Liu%20and%20Yi%20Chang%20and%20Xiangyu%20Zhao%0AAbstract%3A%20%20%20In%20various%20domains%2C%20Sequential%20Recommender%20Systems%20%28SRS%29%20have%20become%0Aessential%20due%20to%20their%20superior%20capability%20to%20discern%20intricate%20user%0Apreferences.%20Typically%2C%20SRS%20utilize%20transformer-based%20architectures%20to%20forecast%0Athe%20subsequent%20item%20within%20a%20sequence.%20Nevertheless%2C%20the%20quadratic%0Acomputational%20complexity%20inherent%20in%20these%20models%20often%20leads%20to%0Ainefficiencies%2C%20hindering%20the%20achievement%20of%20real-time%20recommendations.%20Mamba%2C%0Aa%20recent%20advancement%2C%20has%20exhibited%20exceptional%20performance%20in%20time%20series%0Aprediction%2C%20significantly%20enhancing%20both%20efficiency%20and%20accuracy.%20However%2C%0Aintegrating%20Mamba%20directly%20into%20SRS%20poses%20several%20challenges.%20Its%20inherently%0Aunidirectional%20nature%20may%20constrain%20the%20model%27s%20capacity%20to%20capture%20the%20full%0Acontext%20of%20user-item%20interactions%2C%20while%20its%20instability%20in%20state%20estimation%0Acan%20compromise%20its%20ability%20to%20detect%20short-term%20patterns%20within%20interaction%0Asequences.%0A%20%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20new%20framework%20named%20Selective%20Gated%0AMamba%20%28SIGMA%29%20for%20Sequential%20Recommendation.%20This%20framework%20leverages%20a%0APartially%20Flipped%20Mamba%20%28PF-Mamba%29%20to%20construct%20a%20bidirectional%20architecture%0Aspecifically%20tailored%20to%20improve%20contextual%20modeling.%20Additionally%2C%20an%0Ainput-sensitive%20Dense%20Selective%20Gate%20%28DS%20Gate%29%20is%20employed%20to%20optimize%0Adirectional%20weights%20and%20enhance%20the%20processing%20of%20sequential%20information%20in%0APF-Mamba.%20For%20short%20sequence%20modeling%2C%20we%20have%20also%20developed%20a%20Feature%20Extract%0AGRU%20%28FE-GRU%29%20to%20efficiently%20capture%20short-term%20dependencies.%20Empirical%20results%0Aindicate%20that%20SIGMA%20outperforms%20current%20models%20on%20five%20real-world%20datasets.%20Our%0Aimplementation%20code%20is%20available%20at%20https%3A//github.com/ziwliu-cityu/SIMGA%20to%0Aease%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11451v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGMA%253A%2520Selective%2520Gated%2520Mamba%2520for%2520Sequential%2520Recommendation%26entry.906535625%3DZiwei%2520Liu%2520and%2520Qidong%2520Liu%2520and%2520Yejing%2520Wang%2520and%2520Wanyu%2520Wang%2520and%2520Pengyue%2520Jia%2520and%2520Maolin%2520Wang%2520and%2520Zitao%2520Liu%2520and%2520Yi%2520Chang%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3D%2520%2520In%2520various%2520domains%252C%2520Sequential%2520Recommender%2520Systems%2520%2528SRS%2529%2520have%2520become%250Aessential%2520due%2520to%2520their%2520superior%2520capability%2520to%2520discern%2520intricate%2520user%250Apreferences.%2520Typically%252C%2520SRS%2520utilize%2520transformer-based%2520architectures%2520to%2520forecast%250Athe%2520subsequent%2520item%2520within%2520a%2520sequence.%2520Nevertheless%252C%2520the%2520quadratic%250Acomputational%2520complexity%2520inherent%2520in%2520these%2520models%2520often%2520leads%2520to%250Ainefficiencies%252C%2520hindering%2520the%2520achievement%2520of%2520real-time%2520recommendations.%2520Mamba%252C%250Aa%2520recent%2520advancement%252C%2520has%2520exhibited%2520exceptional%2520performance%2520in%2520time%2520series%250Aprediction%252C%2520significantly%2520enhancing%2520both%2520efficiency%2520and%2520accuracy.%2520However%252C%250Aintegrating%2520Mamba%2520directly%2520into%2520SRS%2520poses%2520several%2520challenges.%2520Its%2520inherently%250Aunidirectional%2520nature%2520may%2520constrain%2520the%2520model%2527s%2520capacity%2520to%2520capture%2520the%2520full%250Acontext%2520of%2520user-item%2520interactions%252C%2520while%2520its%2520instability%2520in%2520state%2520estimation%250Acan%2520compromise%2520its%2520ability%2520to%2520detect%2520short-term%2520patterns%2520within%2520interaction%250Asequences.%250A%2520%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520a%2520new%2520framework%2520named%2520Selective%2520Gated%250AMamba%2520%2528SIGMA%2529%2520for%2520Sequential%2520Recommendation.%2520This%2520framework%2520leverages%2520a%250APartially%2520Flipped%2520Mamba%2520%2528PF-Mamba%2529%2520to%2520construct%2520a%2520bidirectional%2520architecture%250Aspecifically%2520tailored%2520to%2520improve%2520contextual%2520modeling.%2520Additionally%252C%2520an%250Ainput-sensitive%2520Dense%2520Selective%2520Gate%2520%2528DS%2520Gate%2529%2520is%2520employed%2520to%2520optimize%250Adirectional%2520weights%2520and%2520enhance%2520the%2520processing%2520of%2520sequential%2520information%2520in%250APF-Mamba.%2520For%2520short%2520sequence%2520modeling%252C%2520we%2520have%2520also%2520developed%2520a%2520Feature%2520Extract%250AGRU%2520%2528FE-GRU%2529%2520to%2520efficiently%2520capture%2520short-term%2520dependencies.%2520Empirical%2520results%250Aindicate%2520that%2520SIGMA%2520outperforms%2520current%2520models%2520on%2520five%2520real-world%2520datasets.%2520Our%250Aimplementation%2520code%2520is%2520available%2520at%2520https%253A//github.com/ziwliu-cityu/SIMGA%2520to%250Aease%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11451v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMA%3A%20Selective%20Gated%20Mamba%20for%20Sequential%20Recommendation&entry.906535625=Ziwei%20Liu%20and%20Qidong%20Liu%20and%20Yejing%20Wang%20and%20Wanyu%20Wang%20and%20Pengyue%20Jia%20and%20Maolin%20Wang%20and%20Zitao%20Liu%20and%20Yi%20Chang%20and%20Xiangyu%20Zhao&entry.1292438233=%20%20In%20various%20domains%2C%20Sequential%20Recommender%20Systems%20%28SRS%29%20have%20become%0Aessential%20due%20to%20their%20superior%20capability%20to%20discern%20intricate%20user%0Apreferences.%20Typically%2C%20SRS%20utilize%20transformer-based%20architectures%20to%20forecast%0Athe%20subsequent%20item%20within%20a%20sequence.%20Nevertheless%2C%20the%20quadratic%0Acomputational%20complexity%20inherent%20in%20these%20models%20often%20leads%20to%0Ainefficiencies%2C%20hindering%20the%20achievement%20of%20real-time%20recommendations.%20Mamba%2C%0Aa%20recent%20advancement%2C%20has%20exhibited%20exceptional%20performance%20in%20time%20series%0Aprediction%2C%20significantly%20enhancing%20both%20efficiency%20and%20accuracy.%20However%2C%0Aintegrating%20Mamba%20directly%20into%20SRS%20poses%20several%20challenges.%20Its%20inherently%0Aunidirectional%20nature%20may%20constrain%20the%20model%27s%20capacity%20to%20capture%20the%20full%0Acontext%20of%20user-item%20interactions%2C%20while%20its%20instability%20in%20state%20estimation%0Acan%20compromise%20its%20ability%20to%20detect%20short-term%20patterns%20within%20interaction%0Asequences.%0A%20%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20new%20framework%20named%20Selective%20Gated%0AMamba%20%28SIGMA%29%20for%20Sequential%20Recommendation.%20This%20framework%20leverages%20a%0APartially%20Flipped%20Mamba%20%28PF-Mamba%29%20to%20construct%20a%20bidirectional%20architecture%0Aspecifically%20tailored%20to%20improve%20contextual%20modeling.%20Additionally%2C%20an%0Ainput-sensitive%20Dense%20Selective%20Gate%20%28DS%20Gate%29%20is%20employed%20to%20optimize%0Adirectional%20weights%20and%20enhance%20the%20processing%20of%20sequential%20information%20in%0APF-Mamba.%20For%20short%20sequence%20modeling%2C%20we%20have%20also%20developed%20a%20Feature%20Extract%0AGRU%20%28FE-GRU%29%20to%20efficiently%20capture%20short-term%20dependencies.%20Empirical%20results%0Aindicate%20that%20SIGMA%20outperforms%20current%20models%20on%20five%20real-world%20datasets.%20Our%0Aimplementation%20code%20is%20available%20at%20https%3A//github.com/ziwliu-cityu/SIMGA%20to%0Aease%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11451v4&entry.124074799=Read"},
{"title": "GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI\n  Testing Agent", "author": "Kangjia Zhao and Jiahui Song and Leigang Sha and Haozhan Shen and Zhi Chen and Tiancheng Zhao and Xiubo Liang and Jianwei Yin", "abstract": "  Nowadays, research on GUI agents is a hot topic in the AI community. However,\ncurrent research focuses on GUI task automation, limiting the scope of\napplications in various GUI scenarios. In this paper, we propose a formalized\nand comprehensive environment to evaluate the entire process of automated GUI\nTesting (GTArena), offering a fair, standardized environment for consistent\noperation of diverse multimodal large language models. We divide the testing\nprocess into three key subtasks: test intention generation, test task\nexecution, and GUI defect detection, and construct a benchmark dataset based on\nthese to conduct a comprehensive evaluation. It evaluates the performance of\ndifferent models using three data types: real mobile applications, mobile\napplications with artificially injected defects, and synthetic data, thoroughly\nassessing their capabilities in this relevant task. Additionally, we propose a\nmethod that helps researchers explore the correlation between the performance\nof multimodal language large models in specific scenarios and their general\ncapabilities in standard benchmark tests. Experimental results indicate that\neven the most advanced models struggle to perform well across all sub-tasks of\nautomated GUI Testing, highlighting a significant gap between the current\ncapabilities of Autonomous GUI Testing and its practical, real-world\napplicability. This gap provides guidance for the future direction of GUI Agent\ndevelopment. Our code is available at\nhttps://github.com/ZJU-ACES-ISE/ChatUITest.\n", "link": "http://arxiv.org/abs/2412.18426v1", "date": "2024-12-24", "relevancy": 2.0266, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5076}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI%20Testing%20Arena%3A%20A%20Unified%20Benchmark%20for%20Advancing%20Autonomous%20GUI%0A%20%20Testing%20Agent&body=Title%3A%20GUI%20Testing%20Arena%3A%20A%20Unified%20Benchmark%20for%20Advancing%20Autonomous%20GUI%0A%20%20Testing%20Agent%0AAuthor%3A%20Kangjia%20Zhao%20and%20Jiahui%20Song%20and%20Leigang%20Sha%20and%20Haozhan%20Shen%20and%20Zhi%20Chen%20and%20Tiancheng%20Zhao%20and%20Xiubo%20Liang%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20Nowadays%2C%20research%20on%20GUI%20agents%20is%20a%20hot%20topic%20in%20the%20AI%20community.%20However%2C%0Acurrent%20research%20focuses%20on%20GUI%20task%20automation%2C%20limiting%20the%20scope%20of%0Aapplications%20in%20various%20GUI%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20formalized%0Aand%20comprehensive%20environment%20to%20evaluate%20the%20entire%20process%20of%20automated%20GUI%0ATesting%20%28GTArena%29%2C%20offering%20a%20fair%2C%20standardized%20environment%20for%20consistent%0Aoperation%20of%20diverse%20multimodal%20large%20language%20models.%20We%20divide%20the%20testing%0Aprocess%20into%20three%20key%20subtasks%3A%20test%20intention%20generation%2C%20test%20task%0Aexecution%2C%20and%20GUI%20defect%20detection%2C%20and%20construct%20a%20benchmark%20dataset%20based%20on%0Athese%20to%20conduct%20a%20comprehensive%20evaluation.%20It%20evaluates%20the%20performance%20of%0Adifferent%20models%20using%20three%20data%20types%3A%20real%20mobile%20applications%2C%20mobile%0Aapplications%20with%20artificially%20injected%20defects%2C%20and%20synthetic%20data%2C%20thoroughly%0Aassessing%20their%20capabilities%20in%20this%20relevant%20task.%20Additionally%2C%20we%20propose%20a%0Amethod%20that%20helps%20researchers%20explore%20the%20correlation%20between%20the%20performance%0Aof%20multimodal%20language%20large%20models%20in%20specific%20scenarios%20and%20their%20general%0Acapabilities%20in%20standard%20benchmark%20tests.%20Experimental%20results%20indicate%20that%0Aeven%20the%20most%20advanced%20models%20struggle%20to%20perform%20well%20across%20all%20sub-tasks%20of%0Aautomated%20GUI%20Testing%2C%20highlighting%20a%20significant%20gap%20between%20the%20current%0Acapabilities%20of%20Autonomous%20GUI%20Testing%20and%20its%20practical%2C%20real-world%0Aapplicability.%20This%20gap%20provides%20guidance%20for%20the%20future%20direction%20of%20GUI%20Agent%0Adevelopment.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZJU-ACES-ISE/ChatUITest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI%2520Testing%2520Arena%253A%2520A%2520Unified%2520Benchmark%2520for%2520Advancing%2520Autonomous%2520GUI%250A%2520%2520Testing%2520Agent%26entry.906535625%3DKangjia%2520Zhao%2520and%2520Jiahui%2520Song%2520and%2520Leigang%2520Sha%2520and%2520Haozhan%2520Shen%2520and%2520Zhi%2520Chen%2520and%2520Tiancheng%2520Zhao%2520and%2520Xiubo%2520Liang%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520Nowadays%252C%2520research%2520on%2520GUI%2520agents%2520is%2520a%2520hot%2520topic%2520in%2520the%2520AI%2520community.%2520However%252C%250Acurrent%2520research%2520focuses%2520on%2520GUI%2520task%2520automation%252C%2520limiting%2520the%2520scope%2520of%250Aapplications%2520in%2520various%2520GUI%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520formalized%250Aand%2520comprehensive%2520environment%2520to%2520evaluate%2520the%2520entire%2520process%2520of%2520automated%2520GUI%250ATesting%2520%2528GTArena%2529%252C%2520offering%2520a%2520fair%252C%2520standardized%2520environment%2520for%2520consistent%250Aoperation%2520of%2520diverse%2520multimodal%2520large%2520language%2520models.%2520We%2520divide%2520the%2520testing%250Aprocess%2520into%2520three%2520key%2520subtasks%253A%2520test%2520intention%2520generation%252C%2520test%2520task%250Aexecution%252C%2520and%2520GUI%2520defect%2520detection%252C%2520and%2520construct%2520a%2520benchmark%2520dataset%2520based%2520on%250Athese%2520to%2520conduct%2520a%2520comprehensive%2520evaluation.%2520It%2520evaluates%2520the%2520performance%2520of%250Adifferent%2520models%2520using%2520three%2520data%2520types%253A%2520real%2520mobile%2520applications%252C%2520mobile%250Aapplications%2520with%2520artificially%2520injected%2520defects%252C%2520and%2520synthetic%2520data%252C%2520thoroughly%250Aassessing%2520their%2520capabilities%2520in%2520this%2520relevant%2520task.%2520Additionally%252C%2520we%2520propose%2520a%250Amethod%2520that%2520helps%2520researchers%2520explore%2520the%2520correlation%2520between%2520the%2520performance%250Aof%2520multimodal%2520language%2520large%2520models%2520in%2520specific%2520scenarios%2520and%2520their%2520general%250Acapabilities%2520in%2520standard%2520benchmark%2520tests.%2520Experimental%2520results%2520indicate%2520that%250Aeven%2520the%2520most%2520advanced%2520models%2520struggle%2520to%2520perform%2520well%2520across%2520all%2520sub-tasks%2520of%250Aautomated%2520GUI%2520Testing%252C%2520highlighting%2520a%2520significant%2520gap%2520between%2520the%2520current%250Acapabilities%2520of%2520Autonomous%2520GUI%2520Testing%2520and%2520its%2520practical%252C%2520real-world%250Aapplicability.%2520This%2520gap%2520provides%2520guidance%2520for%2520the%2520future%2520direction%2520of%2520GUI%2520Agent%250Adevelopment.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZJU-ACES-ISE/ChatUITest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI%20Testing%20Arena%3A%20A%20Unified%20Benchmark%20for%20Advancing%20Autonomous%20GUI%0A%20%20Testing%20Agent&entry.906535625=Kangjia%20Zhao%20and%20Jiahui%20Song%20and%20Leigang%20Sha%20and%20Haozhan%20Shen%20and%20Zhi%20Chen%20and%20Tiancheng%20Zhao%20and%20Xiubo%20Liang%20and%20Jianwei%20Yin&entry.1292438233=%20%20Nowadays%2C%20research%20on%20GUI%20agents%20is%20a%20hot%20topic%20in%20the%20AI%20community.%20However%2C%0Acurrent%20research%20focuses%20on%20GUI%20task%20automation%2C%20limiting%20the%20scope%20of%0Aapplications%20in%20various%20GUI%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20formalized%0Aand%20comprehensive%20environment%20to%20evaluate%20the%20entire%20process%20of%20automated%20GUI%0ATesting%20%28GTArena%29%2C%20offering%20a%20fair%2C%20standardized%20environment%20for%20consistent%0Aoperation%20of%20diverse%20multimodal%20large%20language%20models.%20We%20divide%20the%20testing%0Aprocess%20into%20three%20key%20subtasks%3A%20test%20intention%20generation%2C%20test%20task%0Aexecution%2C%20and%20GUI%20defect%20detection%2C%20and%20construct%20a%20benchmark%20dataset%20based%20on%0Athese%20to%20conduct%20a%20comprehensive%20evaluation.%20It%20evaluates%20the%20performance%20of%0Adifferent%20models%20using%20three%20data%20types%3A%20real%20mobile%20applications%2C%20mobile%0Aapplications%20with%20artificially%20injected%20defects%2C%20and%20synthetic%20data%2C%20thoroughly%0Aassessing%20their%20capabilities%20in%20this%20relevant%20task.%20Additionally%2C%20we%20propose%20a%0Amethod%20that%20helps%20researchers%20explore%20the%20correlation%20between%20the%20performance%0Aof%20multimodal%20language%20large%20models%20in%20specific%20scenarios%20and%20their%20general%0Acapabilities%20in%20standard%20benchmark%20tests.%20Experimental%20results%20indicate%20that%0Aeven%20the%20most%20advanced%20models%20struggle%20to%20perform%20well%20across%20all%20sub-tasks%20of%0Aautomated%20GUI%20Testing%2C%20highlighting%20a%20significant%20gap%20between%20the%20current%0Acapabilities%20of%20Autonomous%20GUI%20Testing%20and%20its%20practical%2C%20real-world%0Aapplicability.%20This%20gap%20provides%20guidance%20for%20the%20future%20direction%20of%20GUI%20Agent%0Adevelopment.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZJU-ACES-ISE/ChatUITest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18426v1&entry.124074799=Read"},
{"title": "Text-Driven Tumor Synthesis", "author": "Xinran Li and Yi Shuai and Chen Liu and Qi Chen and Qilong Wu and Pengfei Guo and Dong Yang and Can Zhao and Pedro R. A. S. Bassi and Daguang Xu and Kang Wang and Yang Yang and Alan Yuille and Zongwei Zhou", "abstract": "  Tumor synthesis can generate examples that AI often misses or over-detects,\nimproving AI performance by training on these challenging cases. However,\nexisting synthesis methods, which are typically unconditional -- generating\nimages from random variables -- or conditioned only by tumor shapes, lack\ncontrollability over specific tumor characteristics such as texture,\nheterogeneity, boundaries, and pathology type. As a result, the generated\ntumors may be overly similar or duplicates of existing training data, failing\nto effectively address AI's weaknesses. We propose a new text-driven tumor\nsynthesis approach, termed TextoMorph, that provides textual control over tumor\ncharacteristics. This is particularly beneficial for examples that confuse the\nAI the most, such as early tumor detection (increasing Sensitivity by +8.5%),\ntumor segmentation for precise radiotherapy (increasing DSC by +6.3%), and\nclassification between benign and malignant tumors (improving Sensitivity by\n+8.2%). By incorporating text mined from radiology reports into the synthesis\nprocess, we increase the variability and controllability of the synthetic\ntumors to target AI's failure cases more precisely. Moreover, TextoMorph uses\ncontrastive learning across different texts and CT scans, significantly\nreducing dependence on scarce image-report pairs (only 141 pairs used in this\nstudy) by leveraging a large corpus of 34,035 radiology reports. Finally, we\nhave developed rigorous tests to evaluate synthetic tumors, including\nText-Driven Visual Turing Test and Radiomics Pattern Analysis, showing that our\nsynthetic tumors is realistic and diverse in texture, heterogeneity,\nboundaries, and pathology.\n", "link": "http://arxiv.org/abs/2412.18589v1", "date": "2024-12-24", "relevancy": 2.0187, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5208}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Driven%20Tumor%20Synthesis&body=Title%3A%20Text-Driven%20Tumor%20Synthesis%0AAuthor%3A%20Xinran%20Li%20and%20Yi%20Shuai%20and%20Chen%20Liu%20and%20Qi%20Chen%20and%20Qilong%20Wu%20and%20Pengfei%20Guo%20and%20Dong%20Yang%20and%20Can%20Zhao%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Daguang%20Xu%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Tumor%20synthesis%20can%20generate%20examples%20that%20AI%20often%20misses%20or%20over-detects%2C%0Aimproving%20AI%20performance%20by%20training%20on%20these%20challenging%20cases.%20However%2C%0Aexisting%20synthesis%20methods%2C%20which%20are%20typically%20unconditional%20--%20generating%0Aimages%20from%20random%20variables%20--%20or%20conditioned%20only%20by%20tumor%20shapes%2C%20lack%0Acontrollability%20over%20specific%20tumor%20characteristics%20such%20as%20texture%2C%0Aheterogeneity%2C%20boundaries%2C%20and%20pathology%20type.%20As%20a%20result%2C%20the%20generated%0Atumors%20may%20be%20overly%20similar%20or%20duplicates%20of%20existing%20training%20data%2C%20failing%0Ato%20effectively%20address%20AI%27s%20weaknesses.%20We%20propose%20a%20new%20text-driven%20tumor%0Asynthesis%20approach%2C%20termed%20TextoMorph%2C%20that%20provides%20textual%20control%20over%20tumor%0Acharacteristics.%20This%20is%20particularly%20beneficial%20for%20examples%20that%20confuse%20the%0AAI%20the%20most%2C%20such%20as%20early%20tumor%20detection%20%28increasing%20Sensitivity%20by%20%2B8.5%25%29%2C%0Atumor%20segmentation%20for%20precise%20radiotherapy%20%28increasing%20DSC%20by%20%2B6.3%25%29%2C%20and%0Aclassification%20between%20benign%20and%20malignant%20tumors%20%28improving%20Sensitivity%20by%0A%2B8.2%25%29.%20By%20incorporating%20text%20mined%20from%20radiology%20reports%20into%20the%20synthesis%0Aprocess%2C%20we%20increase%20the%20variability%20and%20controllability%20of%20the%20synthetic%0Atumors%20to%20target%20AI%27s%20failure%20cases%20more%20precisely.%20Moreover%2C%20TextoMorph%20uses%0Acontrastive%20learning%20across%20different%20texts%20and%20CT%20scans%2C%20significantly%0Areducing%20dependence%20on%20scarce%20image-report%20pairs%20%28only%20141%20pairs%20used%20in%20this%0Astudy%29%20by%20leveraging%20a%20large%20corpus%20of%2034%2C035%20radiology%20reports.%20Finally%2C%20we%0Ahave%20developed%20rigorous%20tests%20to%20evaluate%20synthetic%20tumors%2C%20including%0AText-Driven%20Visual%20Turing%20Test%20and%20Radiomics%20Pattern%20Analysis%2C%20showing%20that%20our%0Asynthetic%20tumors%20is%20realistic%20and%20diverse%20in%20texture%2C%20heterogeneity%2C%0Aboundaries%2C%20and%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Driven%2520Tumor%2520Synthesis%26entry.906535625%3DXinran%2520Li%2520and%2520Yi%2520Shuai%2520and%2520Chen%2520Liu%2520and%2520Qi%2520Chen%2520and%2520Qilong%2520Wu%2520and%2520Pengfei%2520Guo%2520and%2520Dong%2520Yang%2520and%2520Can%2520Zhao%2520and%2520Pedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Daguang%2520Xu%2520and%2520Kang%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520Tumor%2520synthesis%2520can%2520generate%2520examples%2520that%2520AI%2520often%2520misses%2520or%2520over-detects%252C%250Aimproving%2520AI%2520performance%2520by%2520training%2520on%2520these%2520challenging%2520cases.%2520However%252C%250Aexisting%2520synthesis%2520methods%252C%2520which%2520are%2520typically%2520unconditional%2520--%2520generating%250Aimages%2520from%2520random%2520variables%2520--%2520or%2520conditioned%2520only%2520by%2520tumor%2520shapes%252C%2520lack%250Acontrollability%2520over%2520specific%2520tumor%2520characteristics%2520such%2520as%2520texture%252C%250Aheterogeneity%252C%2520boundaries%252C%2520and%2520pathology%2520type.%2520As%2520a%2520result%252C%2520the%2520generated%250Atumors%2520may%2520be%2520overly%2520similar%2520or%2520duplicates%2520of%2520existing%2520training%2520data%252C%2520failing%250Ato%2520effectively%2520address%2520AI%2527s%2520weaknesses.%2520We%2520propose%2520a%2520new%2520text-driven%2520tumor%250Asynthesis%2520approach%252C%2520termed%2520TextoMorph%252C%2520that%2520provides%2520textual%2520control%2520over%2520tumor%250Acharacteristics.%2520This%2520is%2520particularly%2520beneficial%2520for%2520examples%2520that%2520confuse%2520the%250AAI%2520the%2520most%252C%2520such%2520as%2520early%2520tumor%2520detection%2520%2528increasing%2520Sensitivity%2520by%2520%252B8.5%2525%2529%252C%250Atumor%2520segmentation%2520for%2520precise%2520radiotherapy%2520%2528increasing%2520DSC%2520by%2520%252B6.3%2525%2529%252C%2520and%250Aclassification%2520between%2520benign%2520and%2520malignant%2520tumors%2520%2528improving%2520Sensitivity%2520by%250A%252B8.2%2525%2529.%2520By%2520incorporating%2520text%2520mined%2520from%2520radiology%2520reports%2520into%2520the%2520synthesis%250Aprocess%252C%2520we%2520increase%2520the%2520variability%2520and%2520controllability%2520of%2520the%2520synthetic%250Atumors%2520to%2520target%2520AI%2527s%2520failure%2520cases%2520more%2520precisely.%2520Moreover%252C%2520TextoMorph%2520uses%250Acontrastive%2520learning%2520across%2520different%2520texts%2520and%2520CT%2520scans%252C%2520significantly%250Areducing%2520dependence%2520on%2520scarce%2520image-report%2520pairs%2520%2528only%2520141%2520pairs%2520used%2520in%2520this%250Astudy%2529%2520by%2520leveraging%2520a%2520large%2520corpus%2520of%252034%252C035%2520radiology%2520reports.%2520Finally%252C%2520we%250Ahave%2520developed%2520rigorous%2520tests%2520to%2520evaluate%2520synthetic%2520tumors%252C%2520including%250AText-Driven%2520Visual%2520Turing%2520Test%2520and%2520Radiomics%2520Pattern%2520Analysis%252C%2520showing%2520that%2520our%250Asynthetic%2520tumors%2520is%2520realistic%2520and%2520diverse%2520in%2520texture%252C%2520heterogeneity%252C%250Aboundaries%252C%2520and%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Driven%20Tumor%20Synthesis&entry.906535625=Xinran%20Li%20and%20Yi%20Shuai%20and%20Chen%20Liu%20and%20Qi%20Chen%20and%20Qilong%20Wu%20and%20Pengfei%20Guo%20and%20Dong%20Yang%20and%20Can%20Zhao%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Daguang%20Xu%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Tumor%20synthesis%20can%20generate%20examples%20that%20AI%20often%20misses%20or%20over-detects%2C%0Aimproving%20AI%20performance%20by%20training%20on%20these%20challenging%20cases.%20However%2C%0Aexisting%20synthesis%20methods%2C%20which%20are%20typically%20unconditional%20--%20generating%0Aimages%20from%20random%20variables%20--%20or%20conditioned%20only%20by%20tumor%20shapes%2C%20lack%0Acontrollability%20over%20specific%20tumor%20characteristics%20such%20as%20texture%2C%0Aheterogeneity%2C%20boundaries%2C%20and%20pathology%20type.%20As%20a%20result%2C%20the%20generated%0Atumors%20may%20be%20overly%20similar%20or%20duplicates%20of%20existing%20training%20data%2C%20failing%0Ato%20effectively%20address%20AI%27s%20weaknesses.%20We%20propose%20a%20new%20text-driven%20tumor%0Asynthesis%20approach%2C%20termed%20TextoMorph%2C%20that%20provides%20textual%20control%20over%20tumor%0Acharacteristics.%20This%20is%20particularly%20beneficial%20for%20examples%20that%20confuse%20the%0AAI%20the%20most%2C%20such%20as%20early%20tumor%20detection%20%28increasing%20Sensitivity%20by%20%2B8.5%25%29%2C%0Atumor%20segmentation%20for%20precise%20radiotherapy%20%28increasing%20DSC%20by%20%2B6.3%25%29%2C%20and%0Aclassification%20between%20benign%20and%20malignant%20tumors%20%28improving%20Sensitivity%20by%0A%2B8.2%25%29.%20By%20incorporating%20text%20mined%20from%20radiology%20reports%20into%20the%20synthesis%0Aprocess%2C%20we%20increase%20the%20variability%20and%20controllability%20of%20the%20synthetic%0Atumors%20to%20target%20AI%27s%20failure%20cases%20more%20precisely.%20Moreover%2C%20TextoMorph%20uses%0Acontrastive%20learning%20across%20different%20texts%20and%20CT%20scans%2C%20significantly%0Areducing%20dependence%20on%20scarce%20image-report%20pairs%20%28only%20141%20pairs%20used%20in%20this%0Astudy%29%20by%20leveraging%20a%20large%20corpus%20of%2034%2C035%20radiology%20reports.%20Finally%2C%20we%0Ahave%20developed%20rigorous%20tests%20to%20evaluate%20synthetic%20tumors%2C%20including%0AText-Driven%20Visual%20Turing%20Test%20and%20Radiomics%20Pattern%20Analysis%2C%20showing%20that%20our%0Asynthetic%20tumors%20is%20realistic%20and%20diverse%20in%20texture%2C%20heterogeneity%2C%0Aboundaries%2C%20and%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18589v1&entry.124074799=Read"},
{"title": "Physics-Based Dynamic Models Hybridisation Using Physics-Informed Neural\n  Networks", "author": "Branislava Lalic and Dinh Viet Cuong and Mina Petric and Vladimir Pavlovic and Ana Firanj Sremac and Mark Roantree", "abstract": "  Physics-based dynamic models (PBDMs) are simplified representations of\ncomplex dynamical systems. PBDMs take specific processes within a complex\nsystem and assign a fragment of variables and an accompanying set of parameters\nto depict the processes. As this often leads to suboptimal parameterisation of\nthe system, a key challenge requires refining the empirical parameters and\nvariables to reduce uncertainties while maintaining the model s explainability\nand enhancing its predictive accuracy. We demonstrate that a hybrid mosquito\npopulation dynamics model, which integrates a PBDM with Physics-Informed Neural\nNetworks (PINN), retains the explainability of the PBDM by incorporating the\nPINN-learned model parameters in place of its empirical counterparts.\nSpecifically, we address the limitations of traditional PBDMs by modelling the\nparameters of larva and pupa development rates using a PINN that encodes\ncomplex, learned interactions of air temperature, precipitation and humidity.\nOur results demonstrate improved mosquito population simulations including the\ndifficult-to-predict mosquito population peaks. This opens the possibility of\nhybridisation concept application on other complex systems based on PBDMs such\nas cancer growth to address the challenges posed by scarce and noisy data, and\nto numerical weather prediction and climate modelling to overcome the gap\nbetween physics-based and data-driven weather prediction models.\n", "link": "http://arxiv.org/abs/2412.07514v2", "date": "2024-12-24", "relevancy": 2.0186, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Based%20Dynamic%20Models%20Hybridisation%20Using%20Physics-Informed%20Neural%0A%20%20Networks&body=Title%3A%20Physics-Based%20Dynamic%20Models%20Hybridisation%20Using%20Physics-Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Branislava%20Lalic%20and%20Dinh%20Viet%20Cuong%20and%20Mina%20Petric%20and%20Vladimir%20Pavlovic%20and%20Ana%20Firanj%20Sremac%20and%20Mark%20Roantree%0AAbstract%3A%20%20%20Physics-based%20dynamic%20models%20%28PBDMs%29%20are%20simplified%20representations%20of%0Acomplex%20dynamical%20systems.%20PBDMs%20take%20specific%20processes%20within%20a%20complex%0Asystem%20and%20assign%20a%20fragment%20of%20variables%20and%20an%20accompanying%20set%20of%20parameters%0Ato%20depict%20the%20processes.%20As%20this%20often%20leads%20to%20suboptimal%20parameterisation%20of%0Athe%20system%2C%20a%20key%20challenge%20requires%20refining%20the%20empirical%20parameters%20and%0Avariables%20to%20reduce%20uncertainties%20while%20maintaining%20the%20model%20s%20explainability%0Aand%20enhancing%20its%20predictive%20accuracy.%20We%20demonstrate%20that%20a%20hybrid%20mosquito%0Apopulation%20dynamics%20model%2C%20which%20integrates%20a%20PBDM%20with%20Physics-Informed%20Neural%0ANetworks%20%28PINN%29%2C%20retains%20the%20explainability%20of%20the%20PBDM%20by%20incorporating%20the%0APINN-learned%20model%20parameters%20in%20place%20of%20its%20empirical%20counterparts.%0ASpecifically%2C%20we%20address%20the%20limitations%20of%20traditional%20PBDMs%20by%20modelling%20the%0Aparameters%20of%20larva%20and%20pupa%20development%20rates%20using%20a%20PINN%20that%20encodes%0Acomplex%2C%20learned%20interactions%20of%20air%20temperature%2C%20precipitation%20and%20humidity.%0AOur%20results%20demonstrate%20improved%20mosquito%20population%20simulations%20including%20the%0Adifficult-to-predict%20mosquito%20population%20peaks.%20This%20opens%20the%20possibility%20of%0Ahybridisation%20concept%20application%20on%20other%20complex%20systems%20based%20on%20PBDMs%20such%0Aas%20cancer%20growth%20to%20address%20the%20challenges%20posed%20by%20scarce%20and%20noisy%20data%2C%20and%0Ato%20numerical%20weather%20prediction%20and%20climate%20modelling%20to%20overcome%20the%20gap%0Abetween%20physics-based%20and%20data-driven%20weather%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Based%2520Dynamic%2520Models%2520Hybridisation%2520Using%2520Physics-Informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DBranislava%2520Lalic%2520and%2520Dinh%2520Viet%2520Cuong%2520and%2520Mina%2520Petric%2520and%2520Vladimir%2520Pavlovic%2520and%2520Ana%2520Firanj%2520Sremac%2520and%2520Mark%2520Roantree%26entry.1292438233%3D%2520%2520Physics-based%2520dynamic%2520models%2520%2528PBDMs%2529%2520are%2520simplified%2520representations%2520of%250Acomplex%2520dynamical%2520systems.%2520PBDMs%2520take%2520specific%2520processes%2520within%2520a%2520complex%250Asystem%2520and%2520assign%2520a%2520fragment%2520of%2520variables%2520and%2520an%2520accompanying%2520set%2520of%2520parameters%250Ato%2520depict%2520the%2520processes.%2520As%2520this%2520often%2520leads%2520to%2520suboptimal%2520parameterisation%2520of%250Athe%2520system%252C%2520a%2520key%2520challenge%2520requires%2520refining%2520the%2520empirical%2520parameters%2520and%250Avariables%2520to%2520reduce%2520uncertainties%2520while%2520maintaining%2520the%2520model%2520s%2520explainability%250Aand%2520enhancing%2520its%2520predictive%2520accuracy.%2520We%2520demonstrate%2520that%2520a%2520hybrid%2520mosquito%250Apopulation%2520dynamics%2520model%252C%2520which%2520integrates%2520a%2520PBDM%2520with%2520Physics-Informed%2520Neural%250ANetworks%2520%2528PINN%2529%252C%2520retains%2520the%2520explainability%2520of%2520the%2520PBDM%2520by%2520incorporating%2520the%250APINN-learned%2520model%2520parameters%2520in%2520place%2520of%2520its%2520empirical%2520counterparts.%250ASpecifically%252C%2520we%2520address%2520the%2520limitations%2520of%2520traditional%2520PBDMs%2520by%2520modelling%2520the%250Aparameters%2520of%2520larva%2520and%2520pupa%2520development%2520rates%2520using%2520a%2520PINN%2520that%2520encodes%250Acomplex%252C%2520learned%2520interactions%2520of%2520air%2520temperature%252C%2520precipitation%2520and%2520humidity.%250AOur%2520results%2520demonstrate%2520improved%2520mosquito%2520population%2520simulations%2520including%2520the%250Adifficult-to-predict%2520mosquito%2520population%2520peaks.%2520This%2520opens%2520the%2520possibility%2520of%250Ahybridisation%2520concept%2520application%2520on%2520other%2520complex%2520systems%2520based%2520on%2520PBDMs%2520such%250Aas%2520cancer%2520growth%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520scarce%2520and%2520noisy%2520data%252C%2520and%250Ato%2520numerical%2520weather%2520prediction%2520and%2520climate%2520modelling%2520to%2520overcome%2520the%2520gap%250Abetween%2520physics-based%2520and%2520data-driven%2520weather%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Based%20Dynamic%20Models%20Hybridisation%20Using%20Physics-Informed%20Neural%0A%20%20Networks&entry.906535625=Branislava%20Lalic%20and%20Dinh%20Viet%20Cuong%20and%20Mina%20Petric%20and%20Vladimir%20Pavlovic%20and%20Ana%20Firanj%20Sremac%20and%20Mark%20Roantree&entry.1292438233=%20%20Physics-based%20dynamic%20models%20%28PBDMs%29%20are%20simplified%20representations%20of%0Acomplex%20dynamical%20systems.%20PBDMs%20take%20specific%20processes%20within%20a%20complex%0Asystem%20and%20assign%20a%20fragment%20of%20variables%20and%20an%20accompanying%20set%20of%20parameters%0Ato%20depict%20the%20processes.%20As%20this%20often%20leads%20to%20suboptimal%20parameterisation%20of%0Athe%20system%2C%20a%20key%20challenge%20requires%20refining%20the%20empirical%20parameters%20and%0Avariables%20to%20reduce%20uncertainties%20while%20maintaining%20the%20model%20s%20explainability%0Aand%20enhancing%20its%20predictive%20accuracy.%20We%20demonstrate%20that%20a%20hybrid%20mosquito%0Apopulation%20dynamics%20model%2C%20which%20integrates%20a%20PBDM%20with%20Physics-Informed%20Neural%0ANetworks%20%28PINN%29%2C%20retains%20the%20explainability%20of%20the%20PBDM%20by%20incorporating%20the%0APINN-learned%20model%20parameters%20in%20place%20of%20its%20empirical%20counterparts.%0ASpecifically%2C%20we%20address%20the%20limitations%20of%20traditional%20PBDMs%20by%20modelling%20the%0Aparameters%20of%20larva%20and%20pupa%20development%20rates%20using%20a%20PINN%20that%20encodes%0Acomplex%2C%20learned%20interactions%20of%20air%20temperature%2C%20precipitation%20and%20humidity.%0AOur%20results%20demonstrate%20improved%20mosquito%20population%20simulations%20including%20the%0Adifficult-to-predict%20mosquito%20population%20peaks.%20This%20opens%20the%20possibility%20of%0Ahybridisation%20concept%20application%20on%20other%20complex%20systems%20based%20on%20PBDMs%20such%0Aas%20cancer%20growth%20to%20address%20the%20challenges%20posed%20by%20scarce%20and%20noisy%20data%2C%20and%0Ato%20numerical%20weather%20prediction%20and%20climate%20modelling%20to%20overcome%20the%20gap%0Abetween%20physics-based%20and%20data-driven%20weather%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07514v2&entry.124074799=Read"},
{"title": "The Constitutional Filter", "author": "Simon Kohaut and Felix Divo and Benedict Flade and Devendra Singh Dhami and Julian Eggert and Kristian Kersting", "abstract": "  Predictions in environments where a mix of legal policies, physical\nlimitations, and operational preferences impacts an agent's motion are\ninherently difficult. Since Neuro-Symbolic systems allow for differentiable\ninformation flow between deep learning and symbolic building blocks, they\npresent a promising avenue for expressing such high-level constraints. While\nprior work has demonstrated how to establish novel planning setups, e.g., in\nadvanced aerial mobility tasks, their application in prediction tasks has been\nunderdeveloped. We present the Constitutional Filter (CoFi), a novel filter\narchitecture leveraging a Neuro-Symbolic representation of an agent's rules,\ni.e., its constitution, to (i) improve filter accuracy, (ii) leverage expert\nknowledge, (iii) incorporate deep learning architectures, and (iv) account for\nuncertainties in the environments through probabilistic spatial relations. CoFi\nfollows a general, recursive Bayesian estimation setting, making it compatible\nwith a vast landscape of estimation techniques such as Particle Filters. To\nunderpin the advantages of CoFi, we validate its performance on real-world\nmarine data from the Automatic Identification System and official Electronic\nNavigational Charts.\n", "link": "http://arxiv.org/abs/2412.18347v1", "date": "2024-12-24", "relevancy": 2.0136, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Constitutional%20Filter&body=Title%3A%20The%20Constitutional%20Filter%0AAuthor%3A%20Simon%20Kohaut%20and%20Felix%20Divo%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Predictions%20in%20environments%20where%20a%20mix%20of%20legal%20policies%2C%20physical%0Alimitations%2C%20and%20operational%20preferences%20impacts%20an%20agent%27s%20motion%20are%0Ainherently%20difficult.%20Since%20Neuro-Symbolic%20systems%20allow%20for%20differentiable%0Ainformation%20flow%20between%20deep%20learning%20and%20symbolic%20building%20blocks%2C%20they%0Apresent%20a%20promising%20avenue%20for%20expressing%20such%20high-level%20constraints.%20While%0Aprior%20work%20has%20demonstrated%20how%20to%20establish%20novel%20planning%20setups%2C%20e.g.%2C%20in%0Aadvanced%20aerial%20mobility%20tasks%2C%20their%20application%20in%20prediction%20tasks%20has%20been%0Aunderdeveloped.%20We%20present%20the%20Constitutional%20Filter%20%28CoFi%29%2C%20a%20novel%20filter%0Aarchitecture%20leveraging%20a%20Neuro-Symbolic%20representation%20of%20an%20agent%27s%20rules%2C%0Ai.e.%2C%20its%20constitution%2C%20to%20%28i%29%20improve%20filter%20accuracy%2C%20%28ii%29%20leverage%20expert%0Aknowledge%2C%20%28iii%29%20incorporate%20deep%20learning%20architectures%2C%20and%20%28iv%29%20account%20for%0Auncertainties%20in%20the%20environments%20through%20probabilistic%20spatial%20relations.%20CoFi%0Afollows%20a%20general%2C%20recursive%20Bayesian%20estimation%20setting%2C%20making%20it%20compatible%0Awith%20a%20vast%20landscape%20of%20estimation%20techniques%20such%20as%20Particle%20Filters.%20To%0Aunderpin%20the%20advantages%20of%20CoFi%2C%20we%20validate%20its%20performance%20on%20real-world%0Amarine%20data%20from%20the%20Automatic%20Identification%20System%20and%20official%20Electronic%0ANavigational%20Charts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Constitutional%2520Filter%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Felix%2520Divo%2520and%2520Benedict%2520Flade%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Julian%2520Eggert%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Predictions%2520in%2520environments%2520where%2520a%2520mix%2520of%2520legal%2520policies%252C%2520physical%250Alimitations%252C%2520and%2520operational%2520preferences%2520impacts%2520an%2520agent%2527s%2520motion%2520are%250Ainherently%2520difficult.%2520Since%2520Neuro-Symbolic%2520systems%2520allow%2520for%2520differentiable%250Ainformation%2520flow%2520between%2520deep%2520learning%2520and%2520symbolic%2520building%2520blocks%252C%2520they%250Apresent%2520a%2520promising%2520avenue%2520for%2520expressing%2520such%2520high-level%2520constraints.%2520While%250Aprior%2520work%2520has%2520demonstrated%2520how%2520to%2520establish%2520novel%2520planning%2520setups%252C%2520e.g.%252C%2520in%250Aadvanced%2520aerial%2520mobility%2520tasks%252C%2520their%2520application%2520in%2520prediction%2520tasks%2520has%2520been%250Aunderdeveloped.%2520We%2520present%2520the%2520Constitutional%2520Filter%2520%2528CoFi%2529%252C%2520a%2520novel%2520filter%250Aarchitecture%2520leveraging%2520a%2520Neuro-Symbolic%2520representation%2520of%2520an%2520agent%2527s%2520rules%252C%250Ai.e.%252C%2520its%2520constitution%252C%2520to%2520%2528i%2529%2520improve%2520filter%2520accuracy%252C%2520%2528ii%2529%2520leverage%2520expert%250Aknowledge%252C%2520%2528iii%2529%2520incorporate%2520deep%2520learning%2520architectures%252C%2520and%2520%2528iv%2529%2520account%2520for%250Auncertainties%2520in%2520the%2520environments%2520through%2520probabilistic%2520spatial%2520relations.%2520CoFi%250Afollows%2520a%2520general%252C%2520recursive%2520Bayesian%2520estimation%2520setting%252C%2520making%2520it%2520compatible%250Awith%2520a%2520vast%2520landscape%2520of%2520estimation%2520techniques%2520such%2520as%2520Particle%2520Filters.%2520To%250Aunderpin%2520the%2520advantages%2520of%2520CoFi%252C%2520we%2520validate%2520its%2520performance%2520on%2520real-world%250Amarine%2520data%2520from%2520the%2520Automatic%2520Identification%2520System%2520and%2520official%2520Electronic%250ANavigational%2520Charts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Constitutional%20Filter&entry.906535625=Simon%20Kohaut%20and%20Felix%20Divo%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting&entry.1292438233=%20%20Predictions%20in%20environments%20where%20a%20mix%20of%20legal%20policies%2C%20physical%0Alimitations%2C%20and%20operational%20preferences%20impacts%20an%20agent%27s%20motion%20are%0Ainherently%20difficult.%20Since%20Neuro-Symbolic%20systems%20allow%20for%20differentiable%0Ainformation%20flow%20between%20deep%20learning%20and%20symbolic%20building%20blocks%2C%20they%0Apresent%20a%20promising%20avenue%20for%20expressing%20such%20high-level%20constraints.%20While%0Aprior%20work%20has%20demonstrated%20how%20to%20establish%20novel%20planning%20setups%2C%20e.g.%2C%20in%0Aadvanced%20aerial%20mobility%20tasks%2C%20their%20application%20in%20prediction%20tasks%20has%20been%0Aunderdeveloped.%20We%20present%20the%20Constitutional%20Filter%20%28CoFi%29%2C%20a%20novel%20filter%0Aarchitecture%20leveraging%20a%20Neuro-Symbolic%20representation%20of%20an%20agent%27s%20rules%2C%0Ai.e.%2C%20its%20constitution%2C%20to%20%28i%29%20improve%20filter%20accuracy%2C%20%28ii%29%20leverage%20expert%0Aknowledge%2C%20%28iii%29%20incorporate%20deep%20learning%20architectures%2C%20and%20%28iv%29%20account%20for%0Auncertainties%20in%20the%20environments%20through%20probabilistic%20spatial%20relations.%20CoFi%0Afollows%20a%20general%2C%20recursive%20Bayesian%20estimation%20setting%2C%20making%20it%20compatible%0Awith%20a%20vast%20landscape%20of%20estimation%20techniques%20such%20as%20Particle%20Filters.%20To%0Aunderpin%20the%20advantages%20of%20CoFi%2C%20we%20validate%20its%20performance%20on%20real-world%0Amarine%20data%20from%20the%20Automatic%20Identification%20System%20and%20official%20Electronic%0ANavigational%20Charts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18347v1&entry.124074799=Read"},
{"title": "Adversarial Attack Against Images Classification based on Generative\n  Adversarial Networks", "author": "Yahe Yang", "abstract": "  Adversarial attacks on image classification systems have always been an\nimportant problem in the field of machine learning, and generative adversarial\nnetworks (GANs), as popular models in the field of image generation, have been\nwidely used in various novel scenarios due to their powerful generative\ncapabilities. However, with the popularity of generative adversarial networks,\nthe misuse of fake image technology has raised a series of security problems,\nsuch as malicious tampering with other people's photos and videos, and invasion\nof personal privacy. Inspired by the generative adversarial networks, this work\nproposes a novel adversarial attack method, aiming to gain insight into the\nweaknesses of the image classification system and improve its anti-attack\nability. Specifically, the generative adversarial networks are used to generate\nadversarial samples with small perturbations but enough to affect the\ndecision-making of the classifier, and the adversarial samples are generated\nthrough the adversarial learning of the training generator and the classifier.\nFrom extensive experiment analysis, we evaluate the effectiveness of the method\non a classical image classification dataset, and the results show that our\nmodel successfully deceives a variety of advanced classifiers while maintaining\nthe naturalness of adversarial samples.\n", "link": "http://arxiv.org/abs/2412.16662v2", "date": "2024-12-24", "relevancy": 1.9914, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5149}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attack%20Against%20Images%20Classification%20based%20on%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20Adversarial%20Attack%20Against%20Images%20Classification%20based%20on%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Yahe%20Yang%0AAbstract%3A%20%20%20Adversarial%20attacks%20on%20image%20classification%20systems%20have%20always%20been%20an%0Aimportant%20problem%20in%20the%20field%20of%20machine%20learning%2C%20and%20generative%20adversarial%0Anetworks%20%28GANs%29%2C%20as%20popular%20models%20in%20the%20field%20of%20image%20generation%2C%20have%20been%0Awidely%20used%20in%20various%20novel%20scenarios%20due%20to%20their%20powerful%20generative%0Acapabilities.%20However%2C%20with%20the%20popularity%20of%20generative%20adversarial%20networks%2C%0Athe%20misuse%20of%20fake%20image%20technology%20has%20raised%20a%20series%20of%20security%20problems%2C%0Asuch%20as%20malicious%20tampering%20with%20other%20people%27s%20photos%20and%20videos%2C%20and%20invasion%0Aof%20personal%20privacy.%20Inspired%20by%20the%20generative%20adversarial%20networks%2C%20this%20work%0Aproposes%20a%20novel%20adversarial%20attack%20method%2C%20aiming%20to%20gain%20insight%20into%20the%0Aweaknesses%20of%20the%20image%20classification%20system%20and%20improve%20its%20anti-attack%0Aability.%20Specifically%2C%20the%20generative%20adversarial%20networks%20are%20used%20to%20generate%0Aadversarial%20samples%20with%20small%20perturbations%20but%20enough%20to%20affect%20the%0Adecision-making%20of%20the%20classifier%2C%20and%20the%20adversarial%20samples%20are%20generated%0Athrough%20the%20adversarial%20learning%20of%20the%20training%20generator%20and%20the%20classifier.%0AFrom%20extensive%20experiment%20analysis%2C%20we%20evaluate%20the%20effectiveness%20of%20the%20method%0Aon%20a%20classical%20image%20classification%20dataset%2C%20and%20the%20results%20show%20that%20our%0Amodel%20successfully%20deceives%20a%20variety%20of%20advanced%20classifiers%20while%20maintaining%0Athe%20naturalness%20of%20adversarial%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attack%2520Against%2520Images%2520Classification%2520based%2520on%2520Generative%250A%2520%2520Adversarial%2520Networks%26entry.906535625%3DYahe%2520Yang%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520on%2520image%2520classification%2520systems%2520have%2520always%2520been%2520an%250Aimportant%2520problem%2520in%2520the%2520field%2520of%2520machine%2520learning%252C%2520and%2520generative%2520adversarial%250Anetworks%2520%2528GANs%2529%252C%2520as%2520popular%2520models%2520in%2520the%2520field%2520of%2520image%2520generation%252C%2520have%2520been%250Awidely%2520used%2520in%2520various%2520novel%2520scenarios%2520due%2520to%2520their%2520powerful%2520generative%250Acapabilities.%2520However%252C%2520with%2520the%2520popularity%2520of%2520generative%2520adversarial%2520networks%252C%250Athe%2520misuse%2520of%2520fake%2520image%2520technology%2520has%2520raised%2520a%2520series%2520of%2520security%2520problems%252C%250Asuch%2520as%2520malicious%2520tampering%2520with%2520other%2520people%2527s%2520photos%2520and%2520videos%252C%2520and%2520invasion%250Aof%2520personal%2520privacy.%2520Inspired%2520by%2520the%2520generative%2520adversarial%2520networks%252C%2520this%2520work%250Aproposes%2520a%2520novel%2520adversarial%2520attack%2520method%252C%2520aiming%2520to%2520gain%2520insight%2520into%2520the%250Aweaknesses%2520of%2520the%2520image%2520classification%2520system%2520and%2520improve%2520its%2520anti-attack%250Aability.%2520Specifically%252C%2520the%2520generative%2520adversarial%2520networks%2520are%2520used%2520to%2520generate%250Aadversarial%2520samples%2520with%2520small%2520perturbations%2520but%2520enough%2520to%2520affect%2520the%250Adecision-making%2520of%2520the%2520classifier%252C%2520and%2520the%2520adversarial%2520samples%2520are%2520generated%250Athrough%2520the%2520adversarial%2520learning%2520of%2520the%2520training%2520generator%2520and%2520the%2520classifier.%250AFrom%2520extensive%2520experiment%2520analysis%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520method%250Aon%2520a%2520classical%2520image%2520classification%2520dataset%252C%2520and%2520the%2520results%2520show%2520that%2520our%250Amodel%2520successfully%2520deceives%2520a%2520variety%2520of%2520advanced%2520classifiers%2520while%2520maintaining%250Athe%2520naturalness%2520of%2520adversarial%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attack%20Against%20Images%20Classification%20based%20on%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Yahe%20Yang&entry.1292438233=%20%20Adversarial%20attacks%20on%20image%20classification%20systems%20have%20always%20been%20an%0Aimportant%20problem%20in%20the%20field%20of%20machine%20learning%2C%20and%20generative%20adversarial%0Anetworks%20%28GANs%29%2C%20as%20popular%20models%20in%20the%20field%20of%20image%20generation%2C%20have%20been%0Awidely%20used%20in%20various%20novel%20scenarios%20due%20to%20their%20powerful%20generative%0Acapabilities.%20However%2C%20with%20the%20popularity%20of%20generative%20adversarial%20networks%2C%0Athe%20misuse%20of%20fake%20image%20technology%20has%20raised%20a%20series%20of%20security%20problems%2C%0Asuch%20as%20malicious%20tampering%20with%20other%20people%27s%20photos%20and%20videos%2C%20and%20invasion%0Aof%20personal%20privacy.%20Inspired%20by%20the%20generative%20adversarial%20networks%2C%20this%20work%0Aproposes%20a%20novel%20adversarial%20attack%20method%2C%20aiming%20to%20gain%20insight%20into%20the%0Aweaknesses%20of%20the%20image%20classification%20system%20and%20improve%20its%20anti-attack%0Aability.%20Specifically%2C%20the%20generative%20adversarial%20networks%20are%20used%20to%20generate%0Aadversarial%20samples%20with%20small%20perturbations%20but%20enough%20to%20affect%20the%0Adecision-making%20of%20the%20classifier%2C%20and%20the%20adversarial%20samples%20are%20generated%0Athrough%20the%20adversarial%20learning%20of%20the%20training%20generator%20and%20the%20classifier.%0AFrom%20extensive%20experiment%20analysis%2C%20we%20evaluate%20the%20effectiveness%20of%20the%20method%0Aon%20a%20classical%20image%20classification%20dataset%2C%20and%20the%20results%20show%20that%20our%0Amodel%20successfully%20deceives%20a%20variety%20of%20advanced%20classifiers%20while%20maintaining%0Athe%20naturalness%20of%20adversarial%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16662v2&entry.124074799=Read"},
{"title": "DelGrad: Exact event-based gradients in spiking networks for training\n  delays and weights", "author": "Julian G\u00f6ltz and Jimmy Weber and Laura Kriener and Sebastian Billaudelle and Peter Lake and Johannes Schemmel and Melika Payvand and Mihai A. Petrovici", "abstract": "  Spiking neural networks (SNNs) inherently rely on the timing of signals for\nrepresenting and processing information. Incorporating trainable transmission\ndelays, alongside synaptic weights, is crucial for shaping these temporal\ndynamics. While recent methods have shown the benefits of training delays and\nweights in terms of accuracy and memory efficiency, they rely on discrete time,\napproximate gradients, and full access to internal variables like membrane\npotentials. This limits their precision, efficiency, and suitability for\nneuromorphic hardware due to increased memory requirements and I/O bandwidth\ndemands. To address these challenges, we propose DelGrad, an analytical,\nevent-based method to compute exact loss gradients for both synaptic weights\nand delays. The inclusion of delays in the training process emerges naturally\nwithin our proposed formalism, enriching the model's search space with a\ntemporal dimension. Moreover, DelGrad, grounded purely in spike timing,\neliminates the need to track additional variables such as membrane potentials.\nTo showcase this key advantage, we demonstrate the functionality and benefits\nof DelGrad on the BrainScaleS-2 neuromorphic platform, by training SNNs in a\nchip-in-the-loop fashion. For the first time, we experimentally demonstrate the\nmemory efficiency and accuracy benefits of adding delays to SNNs on noisy\nmixed-signal hardware. Additionally, these experiments also reveal the\npotential of delays for stabilizing networks against noise. DelGrad opens a new\nway for training SNNs with delays on neuromorphic hardware, which results in\nless number of required parameters, higher accuracy and ease of hardware\ntraining.\n", "link": "http://arxiv.org/abs/2404.19165v2", "date": "2024-12-24", "relevancy": 1.9867, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.513}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5066}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DelGrad%3A%20Exact%20event-based%20gradients%20in%20spiking%20networks%20for%20training%0A%20%20delays%20and%20weights&body=Title%3A%20DelGrad%3A%20Exact%20event-based%20gradients%20in%20spiking%20networks%20for%20training%0A%20%20delays%20and%20weights%0AAuthor%3A%20Julian%20G%C3%B6ltz%20and%20Jimmy%20Weber%20and%20Laura%20Kriener%20and%20Sebastian%20Billaudelle%20and%20Peter%20Lake%20and%20Johannes%20Schemmel%20and%20Melika%20Payvand%20and%20Mihai%20A.%20Petrovici%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20inherently%20rely%20on%20the%20timing%20of%20signals%20for%0Arepresenting%20and%20processing%20information.%20Incorporating%20trainable%20transmission%0Adelays%2C%20alongside%20synaptic%20weights%2C%20is%20crucial%20for%20shaping%20these%20temporal%0Adynamics.%20While%20recent%20methods%20have%20shown%20the%20benefits%20of%20training%20delays%20and%0Aweights%20in%20terms%20of%20accuracy%20and%20memory%20efficiency%2C%20they%20rely%20on%20discrete%20time%2C%0Aapproximate%20gradients%2C%20and%20full%20access%20to%20internal%20variables%20like%20membrane%0Apotentials.%20This%20limits%20their%20precision%2C%20efficiency%2C%20and%20suitability%20for%0Aneuromorphic%20hardware%20due%20to%20increased%20memory%20requirements%20and%20I/O%20bandwidth%0Ademands.%20To%20address%20these%20challenges%2C%20we%20propose%20DelGrad%2C%20an%20analytical%2C%0Aevent-based%20method%20to%20compute%20exact%20loss%20gradients%20for%20both%20synaptic%20weights%0Aand%20delays.%20The%20inclusion%20of%20delays%20in%20the%20training%20process%20emerges%20naturally%0Awithin%20our%20proposed%20formalism%2C%20enriching%20the%20model%27s%20search%20space%20with%20a%0Atemporal%20dimension.%20Moreover%2C%20DelGrad%2C%20grounded%20purely%20in%20spike%20timing%2C%0Aeliminates%20the%20need%20to%20track%20additional%20variables%20such%20as%20membrane%20potentials.%0ATo%20showcase%20this%20key%20advantage%2C%20we%20demonstrate%20the%20functionality%20and%20benefits%0Aof%20DelGrad%20on%20the%20BrainScaleS-2%20neuromorphic%20platform%2C%20by%20training%20SNNs%20in%20a%0Achip-in-the-loop%20fashion.%20For%20the%20first%20time%2C%20we%20experimentally%20demonstrate%20the%0Amemory%20efficiency%20and%20accuracy%20benefits%20of%20adding%20delays%20to%20SNNs%20on%20noisy%0Amixed-signal%20hardware.%20Additionally%2C%20these%20experiments%20also%20reveal%20the%0Apotential%20of%20delays%20for%20stabilizing%20networks%20against%20noise.%20DelGrad%20opens%20a%20new%0Away%20for%20training%20SNNs%20with%20delays%20on%20neuromorphic%20hardware%2C%20which%20results%20in%0Aless%20number%20of%20required%20parameters%2C%20higher%20accuracy%20and%20ease%20of%20hardware%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelGrad%253A%2520Exact%2520event-based%2520gradients%2520in%2520spiking%2520networks%2520for%2520training%250A%2520%2520delays%2520and%2520weights%26entry.906535625%3DJulian%2520G%25C3%25B6ltz%2520and%2520Jimmy%2520Weber%2520and%2520Laura%2520Kriener%2520and%2520Sebastian%2520Billaudelle%2520and%2520Peter%2520Lake%2520and%2520Johannes%2520Schemmel%2520and%2520Melika%2520Payvand%2520and%2520Mihai%2520A.%2520Petrovici%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520inherently%2520rely%2520on%2520the%2520timing%2520of%2520signals%2520for%250Arepresenting%2520and%2520processing%2520information.%2520Incorporating%2520trainable%2520transmission%250Adelays%252C%2520alongside%2520synaptic%2520weights%252C%2520is%2520crucial%2520for%2520shaping%2520these%2520temporal%250Adynamics.%2520While%2520recent%2520methods%2520have%2520shown%2520the%2520benefits%2520of%2520training%2520delays%2520and%250Aweights%2520in%2520terms%2520of%2520accuracy%2520and%2520memory%2520efficiency%252C%2520they%2520rely%2520on%2520discrete%2520time%252C%250Aapproximate%2520gradients%252C%2520and%2520full%2520access%2520to%2520internal%2520variables%2520like%2520membrane%250Apotentials.%2520This%2520limits%2520their%2520precision%252C%2520efficiency%252C%2520and%2520suitability%2520for%250Aneuromorphic%2520hardware%2520due%2520to%2520increased%2520memory%2520requirements%2520and%2520I/O%2520bandwidth%250Ademands.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DelGrad%252C%2520an%2520analytical%252C%250Aevent-based%2520method%2520to%2520compute%2520exact%2520loss%2520gradients%2520for%2520both%2520synaptic%2520weights%250Aand%2520delays.%2520The%2520inclusion%2520of%2520delays%2520in%2520the%2520training%2520process%2520emerges%2520naturally%250Awithin%2520our%2520proposed%2520formalism%252C%2520enriching%2520the%2520model%2527s%2520search%2520space%2520with%2520a%250Atemporal%2520dimension.%2520Moreover%252C%2520DelGrad%252C%2520grounded%2520purely%2520in%2520spike%2520timing%252C%250Aeliminates%2520the%2520need%2520to%2520track%2520additional%2520variables%2520such%2520as%2520membrane%2520potentials.%250ATo%2520showcase%2520this%2520key%2520advantage%252C%2520we%2520demonstrate%2520the%2520functionality%2520and%2520benefits%250Aof%2520DelGrad%2520on%2520the%2520BrainScaleS-2%2520neuromorphic%2520platform%252C%2520by%2520training%2520SNNs%2520in%2520a%250Achip-in-the-loop%2520fashion.%2520For%2520the%2520first%2520time%252C%2520we%2520experimentally%2520demonstrate%2520the%250Amemory%2520efficiency%2520and%2520accuracy%2520benefits%2520of%2520adding%2520delays%2520to%2520SNNs%2520on%2520noisy%250Amixed-signal%2520hardware.%2520Additionally%252C%2520these%2520experiments%2520also%2520reveal%2520the%250Apotential%2520of%2520delays%2520for%2520stabilizing%2520networks%2520against%2520noise.%2520DelGrad%2520opens%2520a%2520new%250Away%2520for%2520training%2520SNNs%2520with%2520delays%2520on%2520neuromorphic%2520hardware%252C%2520which%2520results%2520in%250Aless%2520number%2520of%2520required%2520parameters%252C%2520higher%2520accuracy%2520and%2520ease%2520of%2520hardware%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DelGrad%3A%20Exact%20event-based%20gradients%20in%20spiking%20networks%20for%20training%0A%20%20delays%20and%20weights&entry.906535625=Julian%20G%C3%B6ltz%20and%20Jimmy%20Weber%20and%20Laura%20Kriener%20and%20Sebastian%20Billaudelle%20and%20Peter%20Lake%20and%20Johannes%20Schemmel%20and%20Melika%20Payvand%20and%20Mihai%20A.%20Petrovici&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20inherently%20rely%20on%20the%20timing%20of%20signals%20for%0Arepresenting%20and%20processing%20information.%20Incorporating%20trainable%20transmission%0Adelays%2C%20alongside%20synaptic%20weights%2C%20is%20crucial%20for%20shaping%20these%20temporal%0Adynamics.%20While%20recent%20methods%20have%20shown%20the%20benefits%20of%20training%20delays%20and%0Aweights%20in%20terms%20of%20accuracy%20and%20memory%20efficiency%2C%20they%20rely%20on%20discrete%20time%2C%0Aapproximate%20gradients%2C%20and%20full%20access%20to%20internal%20variables%20like%20membrane%0Apotentials.%20This%20limits%20their%20precision%2C%20efficiency%2C%20and%20suitability%20for%0Aneuromorphic%20hardware%20due%20to%20increased%20memory%20requirements%20and%20I/O%20bandwidth%0Ademands.%20To%20address%20these%20challenges%2C%20we%20propose%20DelGrad%2C%20an%20analytical%2C%0Aevent-based%20method%20to%20compute%20exact%20loss%20gradients%20for%20both%20synaptic%20weights%0Aand%20delays.%20The%20inclusion%20of%20delays%20in%20the%20training%20process%20emerges%20naturally%0Awithin%20our%20proposed%20formalism%2C%20enriching%20the%20model%27s%20search%20space%20with%20a%0Atemporal%20dimension.%20Moreover%2C%20DelGrad%2C%20grounded%20purely%20in%20spike%20timing%2C%0Aeliminates%20the%20need%20to%20track%20additional%20variables%20such%20as%20membrane%20potentials.%0ATo%20showcase%20this%20key%20advantage%2C%20we%20demonstrate%20the%20functionality%20and%20benefits%0Aof%20DelGrad%20on%20the%20BrainScaleS-2%20neuromorphic%20platform%2C%20by%20training%20SNNs%20in%20a%0Achip-in-the-loop%20fashion.%20For%20the%20first%20time%2C%20we%20experimentally%20demonstrate%20the%0Amemory%20efficiency%20and%20accuracy%20benefits%20of%20adding%20delays%20to%20SNNs%20on%20noisy%0Amixed-signal%20hardware.%20Additionally%2C%20these%20experiments%20also%20reveal%20the%0Apotential%20of%20delays%20for%20stabilizing%20networks%20against%20noise.%20DelGrad%20opens%20a%20new%0Away%20for%20training%20SNNs%20with%20delays%20on%20neuromorphic%20hardware%2C%20which%20results%20in%0Aless%20number%20of%20required%20parameters%2C%20higher%20accuracy%20and%20ease%20of%20hardware%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19165v2&entry.124074799=Read"},
{"title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation", "author": "Qianren Mao and Yangyifei Luo and Jinlong Zhang and Hanwen Hao and Zhilong Cao and Xiaolong Wang and Xiao Guan and Zhenting Huang and Weifeng Jiang and Shuyu Guo and Zhentao Han and Qili Zhang and Siyuan Tao and Yujie Liu and Junnan Liu and Zhixing Tan and Jie Sun and Bo Li and Xudong Liu and Richong Zhang and Jianxin Li", "abstract": "  Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.\n", "link": "http://arxiv.org/abs/2412.15529v2", "date": "2024-12-24", "relevancy": 1.9811, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XRAG%3A%20eXamining%20the%20Core%20--%20Benchmarking%20Foundational%20Components%20in%0A%20%20Advanced%20Retrieval-Augmented%20Generation&body=Title%3A%20XRAG%3A%20eXamining%20the%20Core%20--%20Benchmarking%20Foundational%20Components%20in%0A%20%20Advanced%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Qianren%20Mao%20and%20Yangyifei%20Luo%20and%20Jinlong%20Zhang%20and%20Hanwen%20Hao%20and%20Zhilong%20Cao%20and%20Xiaolong%20Wang%20and%20Xiao%20Guan%20and%20Zhenting%20Huang%20and%20Weifeng%20Jiang%20and%20Shuyu%20Guo%20and%20Zhentao%20Han%20and%20Qili%20Zhang%20and%20Siyuan%20Tao%20and%20Yujie%20Liu%20and%20Junnan%20Liu%20and%20Zhixing%20Tan%20and%20Jie%20Sun%20and%20Bo%20Li%20and%20Xudong%20Liu%20and%20Richong%20Zhang%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20synergizes%20the%20retrieval%20of%20pertinent%0Adata%20with%20the%20generative%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20ensuring%0Athat%20the%20generated%20output%20is%20not%20only%20contextually%20relevant%20but%20also%20accurate%0Aand%20current.%20We%20introduce%20XRAG%2C%20an%20open-source%2C%20modular%20codebase%20that%0Afacilitates%20exhaustive%20evaluation%20of%20the%20performance%20of%20foundational%20components%0Aof%20advanced%20RAG%20modules.%20These%20components%20are%20systematically%20categorized%20into%0Afour%20core%20phases%3A%20pre-retrieval%2C%20retrieval%2C%20post-retrieval%2C%20and%20generation.%20We%0Asystematically%20analyse%20them%20across%20reconfigured%20datasets%2C%20providing%20a%0Acomprehensive%20benchmark%20for%20their%20effectiveness.%20As%20the%20complexity%20of%20RAG%0Asystems%20continues%20to%20escalate%2C%20we%20underscore%20the%20critical%20need%20to%20identify%0Apotential%20failure%20points%20in%20RAG%20systems.%20We%20formulate%20a%20suite%20of%20experimental%0Amethodologies%20and%20diagnostic%20testing%20protocols%20to%20dissect%20the%20failure%20points%0Ainherent%20in%20RAG%20engineering.%20Subsequently%2C%20we%20proffer%20bespoke%20solutions%20aimed%0Aat%20bolstering%20the%20overall%20performance%20of%20these%20modules.%20Our%20work%20thoroughly%0Aevaluates%20the%20performance%20of%20advanced%20core%20components%20in%20RAG%20systems%2C%20providing%0Ainsights%20into%20optimizations%20for%20prevalent%20failure%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXRAG%253A%2520eXamining%2520the%2520Core%2520--%2520Benchmarking%2520Foundational%2520Components%2520in%250A%2520%2520Advanced%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DQianren%2520Mao%2520and%2520Yangyifei%2520Luo%2520and%2520Jinlong%2520Zhang%2520and%2520Hanwen%2520Hao%2520and%2520Zhilong%2520Cao%2520and%2520Xiaolong%2520Wang%2520and%2520Xiao%2520Guan%2520and%2520Zhenting%2520Huang%2520and%2520Weifeng%2520Jiang%2520and%2520Shuyu%2520Guo%2520and%2520Zhentao%2520Han%2520and%2520Qili%2520Zhang%2520and%2520Siyuan%2520Tao%2520and%2520Yujie%2520Liu%2520and%2520Junnan%2520Liu%2520and%2520Zhixing%2520Tan%2520and%2520Jie%2520Sun%2520and%2520Bo%2520Li%2520and%2520Xudong%2520Liu%2520and%2520Richong%2520Zhang%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520synergizes%2520the%2520retrieval%2520of%2520pertinent%250Adata%2520with%2520the%2520generative%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520ensuring%250Athat%2520the%2520generated%2520output%2520is%2520not%2520only%2520contextually%2520relevant%2520but%2520also%2520accurate%250Aand%2520current.%2520We%2520introduce%2520XRAG%252C%2520an%2520open-source%252C%2520modular%2520codebase%2520that%250Afacilitates%2520exhaustive%2520evaluation%2520of%2520the%2520performance%2520of%2520foundational%2520components%250Aof%2520advanced%2520RAG%2520modules.%2520These%2520components%2520are%2520systematically%2520categorized%2520into%250Afour%2520core%2520phases%253A%2520pre-retrieval%252C%2520retrieval%252C%2520post-retrieval%252C%2520and%2520generation.%2520We%250Asystematically%2520analyse%2520them%2520across%2520reconfigured%2520datasets%252C%2520providing%2520a%250Acomprehensive%2520benchmark%2520for%2520their%2520effectiveness.%2520As%2520the%2520complexity%2520of%2520RAG%250Asystems%2520continues%2520to%2520escalate%252C%2520we%2520underscore%2520the%2520critical%2520need%2520to%2520identify%250Apotential%2520failure%2520points%2520in%2520RAG%2520systems.%2520We%2520formulate%2520a%2520suite%2520of%2520experimental%250Amethodologies%2520and%2520diagnostic%2520testing%2520protocols%2520to%2520dissect%2520the%2520failure%2520points%250Ainherent%2520in%2520RAG%2520engineering.%2520Subsequently%252C%2520we%2520proffer%2520bespoke%2520solutions%2520aimed%250Aat%2520bolstering%2520the%2520overall%2520performance%2520of%2520these%2520modules.%2520Our%2520work%2520thoroughly%250Aevaluates%2520the%2520performance%2520of%2520advanced%2520core%2520components%2520in%2520RAG%2520systems%252C%2520providing%250Ainsights%2520into%2520optimizations%2520for%2520prevalent%2520failure%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XRAG%3A%20eXamining%20the%20Core%20--%20Benchmarking%20Foundational%20Components%20in%0A%20%20Advanced%20Retrieval-Augmented%20Generation&entry.906535625=Qianren%20Mao%20and%20Yangyifei%20Luo%20and%20Jinlong%20Zhang%20and%20Hanwen%20Hao%20and%20Zhilong%20Cao%20and%20Xiaolong%20Wang%20and%20Xiao%20Guan%20and%20Zhenting%20Huang%20and%20Weifeng%20Jiang%20and%20Shuyu%20Guo%20and%20Zhentao%20Han%20and%20Qili%20Zhang%20and%20Siyuan%20Tao%20and%20Yujie%20Liu%20and%20Junnan%20Liu%20and%20Zhixing%20Tan%20and%20Jie%20Sun%20and%20Bo%20Li%20and%20Xudong%20Liu%20and%20Richong%20Zhang%20and%20Jianxin%20Li&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20synergizes%20the%20retrieval%20of%20pertinent%0Adata%20with%20the%20generative%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20ensuring%0Athat%20the%20generated%20output%20is%20not%20only%20contextually%20relevant%20but%20also%20accurate%0Aand%20current.%20We%20introduce%20XRAG%2C%20an%20open-source%2C%20modular%20codebase%20that%0Afacilitates%20exhaustive%20evaluation%20of%20the%20performance%20of%20foundational%20components%0Aof%20advanced%20RAG%20modules.%20These%20components%20are%20systematically%20categorized%20into%0Afour%20core%20phases%3A%20pre-retrieval%2C%20retrieval%2C%20post-retrieval%2C%20and%20generation.%20We%0Asystematically%20analyse%20them%20across%20reconfigured%20datasets%2C%20providing%20a%0Acomprehensive%20benchmark%20for%20their%20effectiveness.%20As%20the%20complexity%20of%20RAG%0Asystems%20continues%20to%20escalate%2C%20we%20underscore%20the%20critical%20need%20to%20identify%0Apotential%20failure%20points%20in%20RAG%20systems.%20We%20formulate%20a%20suite%20of%20experimental%0Amethodologies%20and%20diagnostic%20testing%20protocols%20to%20dissect%20the%20failure%20points%0Ainherent%20in%20RAG%20engineering.%20Subsequently%2C%20we%20proffer%20bespoke%20solutions%20aimed%0Aat%20bolstering%20the%20overall%20performance%20of%20these%20modules.%20Our%20work%20thoroughly%0Aevaluates%20the%20performance%20of%20advanced%20core%20components%20in%20RAG%20systems%2C%20providing%0Ainsights%20into%20optimizations%20for%20prevalent%20failure%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15529v2&entry.124074799=Read"},
{"title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing", "author": "Minghui Liu and Tahseen Rabbani and Tony O'Halloran and Ananth Sankaralingam and Mary-Anne Hartley and Brian Gravelle and Furong Huang and Cornelia Ferm\u00fcller and Yiannis Aloimonos", "abstract": "  Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.\n", "link": "http://arxiv.org/abs/2412.16187v2", "date": "2024-12-24", "relevancy": 1.9768, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5539}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HashEvict%3A%20A%20Pre-Attention%20KV%20Cache%20Eviction%20Strategy%20using%0A%20%20Locality-Sensitive%20Hashing&body=Title%3A%20HashEvict%3A%20A%20Pre-Attention%20KV%20Cache%20Eviction%20Strategy%20using%0A%20%20Locality-Sensitive%20Hashing%0AAuthor%3A%20Minghui%20Liu%20and%20Tahseen%20Rabbani%20and%20Tony%20O%27Halloran%20and%20Ananth%20Sankaralingam%20and%20Mary-Anne%20Hartley%20and%20Brian%20Gravelle%20and%20Furong%20Huang%20and%20Cornelia%20Ferm%C3%BCller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20use%20the%20key-value%20%28KV%29%20cache%0Ato%20significantly%20accelerate%20inference%20by%20storing%20the%20key%20and%20value%20embeddings%0Aof%20past%20tokens.%20However%2C%20this%20cache%20consumes%20significant%20GPU%20memory.%20In%20this%0Awork%2C%20we%20introduce%20HashEvict%2C%20an%20algorithm%20that%20uses%20locality-sensitive%20hashing%0A%28LSH%29%20to%20compress%20the%20KV%20cache.%20HashEvict%20quickly%20locates%20tokens%20in%20the%20cache%0Athat%20are%20cosine%20dissimilar%20to%20the%20current%20query%20token.%20This%20is%20achieved%20by%0Acomputing%20the%20Hamming%20distance%20between%20binarized%20Gaussian%20projections%20of%20the%0Acurrent%20token%20query%20and%20cached%20token%20keys%2C%20with%20a%20projection%20length%20much%0Asmaller%20than%20the%20embedding%20dimension.%20We%20maintain%20a%20lightweight%20binary%0Astructure%20in%20GPU%20memory%20to%20facilitate%20these%20calculations.%20Unlike%20existing%0Acompression%20strategies%20that%20compute%20attention%20to%20determine%20token%20retention%2C%0AHashEvict%20makes%20these%20decisions%20pre-attention%2C%20thereby%20reducing%20computational%0Acosts.%20Additionally%2C%20HashEvict%20is%20dynamic%20-%20at%20every%20decoding%20step%2C%20the%20key%20and%0Avalue%20of%20the%20current%20token%20replace%20the%20embeddings%20of%20a%20token%20expected%20to%0Aproduce%20the%20lowest%20attention%20score.%20We%20demonstrate%20that%20HashEvict%20can%20compress%0Athe%20KV%20cache%20by%2030%25-70%25%20while%20maintaining%20high%20performance%20across%20reasoning%2C%0Amultiple-choice%2C%20long-context%20retrieval%20and%20summarization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHashEvict%253A%2520A%2520Pre-Attention%2520KV%2520Cache%2520Eviction%2520Strategy%2520using%250A%2520%2520Locality-Sensitive%2520Hashing%26entry.906535625%3DMinghui%2520Liu%2520and%2520Tahseen%2520Rabbani%2520and%2520Tony%2520O%2527Halloran%2520and%2520Ananth%2520Sankaralingam%2520and%2520Mary-Anne%2520Hartley%2520and%2520Brian%2520Gravelle%2520and%2520Furong%2520Huang%2520and%2520Cornelia%2520Ferm%25C3%25BCller%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520use%2520the%2520key-value%2520%2528KV%2529%2520cache%250Ato%2520significantly%2520accelerate%2520inference%2520by%2520storing%2520the%2520key%2520and%2520value%2520embeddings%250Aof%2520past%2520tokens.%2520However%252C%2520this%2520cache%2520consumes%2520significant%2520GPU%2520memory.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520HashEvict%252C%2520an%2520algorithm%2520that%2520uses%2520locality-sensitive%2520hashing%250A%2528LSH%2529%2520to%2520compress%2520the%2520KV%2520cache.%2520HashEvict%2520quickly%2520locates%2520tokens%2520in%2520the%2520cache%250Athat%2520are%2520cosine%2520dissimilar%2520to%2520the%2520current%2520query%2520token.%2520This%2520is%2520achieved%2520by%250Acomputing%2520the%2520Hamming%2520distance%2520between%2520binarized%2520Gaussian%2520projections%2520of%2520the%250Acurrent%2520token%2520query%2520and%2520cached%2520token%2520keys%252C%2520with%2520a%2520projection%2520length%2520much%250Asmaller%2520than%2520the%2520embedding%2520dimension.%2520We%2520maintain%2520a%2520lightweight%2520binary%250Astructure%2520in%2520GPU%2520memory%2520to%2520facilitate%2520these%2520calculations.%2520Unlike%2520existing%250Acompression%2520strategies%2520that%2520compute%2520attention%2520to%2520determine%2520token%2520retention%252C%250AHashEvict%2520makes%2520these%2520decisions%2520pre-attention%252C%2520thereby%2520reducing%2520computational%250Acosts.%2520Additionally%252C%2520HashEvict%2520is%2520dynamic%2520-%2520at%2520every%2520decoding%2520step%252C%2520the%2520key%2520and%250Avalue%2520of%2520the%2520current%2520token%2520replace%2520the%2520embeddings%2520of%2520a%2520token%2520expected%2520to%250Aproduce%2520the%2520lowest%2520attention%2520score.%2520We%2520demonstrate%2520that%2520HashEvict%2520can%2520compress%250Athe%2520KV%2520cache%2520by%252030%2525-70%2525%2520while%2520maintaining%2520high%2520performance%2520across%2520reasoning%252C%250Amultiple-choice%252C%2520long-context%2520retrieval%2520and%2520summarization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HashEvict%3A%20A%20Pre-Attention%20KV%20Cache%20Eviction%20Strategy%20using%0A%20%20Locality-Sensitive%20Hashing&entry.906535625=Minghui%20Liu%20and%20Tahseen%20Rabbani%20and%20Tony%20O%27Halloran%20and%20Ananth%20Sankaralingam%20and%20Mary-Anne%20Hartley%20and%20Brian%20Gravelle%20and%20Furong%20Huang%20and%20Cornelia%20Ferm%C3%BCller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20use%20the%20key-value%20%28KV%29%20cache%0Ato%20significantly%20accelerate%20inference%20by%20storing%20the%20key%20and%20value%20embeddings%0Aof%20past%20tokens.%20However%2C%20this%20cache%20consumes%20significant%20GPU%20memory.%20In%20this%0Awork%2C%20we%20introduce%20HashEvict%2C%20an%20algorithm%20that%20uses%20locality-sensitive%20hashing%0A%28LSH%29%20to%20compress%20the%20KV%20cache.%20HashEvict%20quickly%20locates%20tokens%20in%20the%20cache%0Athat%20are%20cosine%20dissimilar%20to%20the%20current%20query%20token.%20This%20is%20achieved%20by%0Acomputing%20the%20Hamming%20distance%20between%20binarized%20Gaussian%20projections%20of%20the%0Acurrent%20token%20query%20and%20cached%20token%20keys%2C%20with%20a%20projection%20length%20much%0Asmaller%20than%20the%20embedding%20dimension.%20We%20maintain%20a%20lightweight%20binary%0Astructure%20in%20GPU%20memory%20to%20facilitate%20these%20calculations.%20Unlike%20existing%0Acompression%20strategies%20that%20compute%20attention%20to%20determine%20token%20retention%2C%0AHashEvict%20makes%20these%20decisions%20pre-attention%2C%20thereby%20reducing%20computational%0Acosts.%20Additionally%2C%20HashEvict%20is%20dynamic%20-%20at%20every%20decoding%20step%2C%20the%20key%20and%0Avalue%20of%20the%20current%20token%20replace%20the%20embeddings%20of%20a%20token%20expected%20to%0Aproduce%20the%20lowest%20attention%20score.%20We%20demonstrate%20that%20HashEvict%20can%20compress%0Athe%20KV%20cache%20by%2030%25-70%25%20while%20maintaining%20high%20performance%20across%20reasoning%2C%0Amultiple-choice%2C%20long-context%20retrieval%20and%20summarization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16187v2&entry.124074799=Read"},
{"title": "RA-RLHF: Provably Efficient Risk-Aware Reinforcement Learning Human\n  Feedback", "author": "Yujie Zhao and Jose Efraim Aguilar Escamill and Weyl Lu and Huazheng Wang", "abstract": "  Reinforcement Learning Human Feedback (RLHF) studies the problem where agents\nreceive only preferences over pairs of trajectories in each episode.\nTraditional approaches in this field have predominantly focused on the mean\nreward or utility criterion. However, in RLHF scenarios demanding heightened\nrisk awareness, such as in AI systems, healthcare, and agriculture, risk-aware\nmeasures are requisite. Traditional risk-aware objectives and algorithms are\nnot applicable in such one-episode-reward settings. To address this, we explore\nand prove the applicability of two risk-aware objectives to RLHF: nested and\nstatic quantile risk objectives. We also introduce Risk-Aware-RLHF (RA-RLHF),\nan algorithm designed to optimize both nested and static objectives.\nAdditionally, we provide a theoretical analysis of the regret upper bounds,\ndemonstrating that they are sublinear with respect to the number of episodes,\nand present empirical results to support our findings. Our code is available in\nhttps://github.com/aguilarjose11/pbrlNeurips.\n", "link": "http://arxiv.org/abs/2410.23569v2", "date": "2024-12-24", "relevancy": 1.9641, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RA-RLHF%3A%20Provably%20Efficient%20Risk-Aware%20Reinforcement%20Learning%20Human%0A%20%20Feedback&body=Title%3A%20RA-RLHF%3A%20Provably%20Efficient%20Risk-Aware%20Reinforcement%20Learning%20Human%0A%20%20Feedback%0AAuthor%3A%20Yujie%20Zhao%20and%20Jose%20Efraim%20Aguilar%20Escamill%20and%20Weyl%20Lu%20and%20Huazheng%20Wang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20Human%20Feedback%20%28RLHF%29%20studies%20the%20problem%20where%20agents%0Areceive%20only%20preferences%20over%20pairs%20of%20trajectories%20in%20each%20episode.%0ATraditional%20approaches%20in%20this%20field%20have%20predominantly%20focused%20on%20the%20mean%0Areward%20or%20utility%20criterion.%20However%2C%20in%20RLHF%20scenarios%20demanding%20heightened%0Arisk%20awareness%2C%20such%20as%20in%20AI%20systems%2C%20healthcare%2C%20and%20agriculture%2C%20risk-aware%0Ameasures%20are%20requisite.%20Traditional%20risk-aware%20objectives%20and%20algorithms%20are%0Anot%20applicable%20in%20such%20one-episode-reward%20settings.%20To%20address%20this%2C%20we%20explore%0Aand%20prove%20the%20applicability%20of%20two%20risk-aware%20objectives%20to%20RLHF%3A%20nested%20and%0Astatic%20quantile%20risk%20objectives.%20We%20also%20introduce%20Risk-Aware-RLHF%20%28RA-RLHF%29%2C%0Aan%20algorithm%20designed%20to%20optimize%20both%20nested%20and%20static%20objectives.%0AAdditionally%2C%20we%20provide%20a%20theoretical%20analysis%20of%20the%20regret%20upper%20bounds%2C%0Ademonstrating%20that%20they%20are%20sublinear%20with%20respect%20to%20the%20number%20of%20episodes%2C%0Aand%20present%20empirical%20results%20to%20support%20our%20findings.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/aguilarjose11/pbrlNeurips.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRA-RLHF%253A%2520Provably%2520Efficient%2520Risk-Aware%2520Reinforcement%2520Learning%2520Human%250A%2520%2520Feedback%26entry.906535625%3DYujie%2520Zhao%2520and%2520Jose%2520Efraim%2520Aguilar%2520Escamill%2520and%2520Weyl%2520Lu%2520and%2520Huazheng%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520Human%2520Feedback%2520%2528RLHF%2529%2520studies%2520the%2520problem%2520where%2520agents%250Areceive%2520only%2520preferences%2520over%2520pairs%2520of%2520trajectories%2520in%2520each%2520episode.%250ATraditional%2520approaches%2520in%2520this%2520field%2520have%2520predominantly%2520focused%2520on%2520the%2520mean%250Areward%2520or%2520utility%2520criterion.%2520However%252C%2520in%2520RLHF%2520scenarios%2520demanding%2520heightened%250Arisk%2520awareness%252C%2520such%2520as%2520in%2520AI%2520systems%252C%2520healthcare%252C%2520and%2520agriculture%252C%2520risk-aware%250Ameasures%2520are%2520requisite.%2520Traditional%2520risk-aware%2520objectives%2520and%2520algorithms%2520are%250Anot%2520applicable%2520in%2520such%2520one-episode-reward%2520settings.%2520To%2520address%2520this%252C%2520we%2520explore%250Aand%2520prove%2520the%2520applicability%2520of%2520two%2520risk-aware%2520objectives%2520to%2520RLHF%253A%2520nested%2520and%250Astatic%2520quantile%2520risk%2520objectives.%2520We%2520also%2520introduce%2520Risk-Aware-RLHF%2520%2528RA-RLHF%2529%252C%250Aan%2520algorithm%2520designed%2520to%2520optimize%2520both%2520nested%2520and%2520static%2520objectives.%250AAdditionally%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520of%2520the%2520regret%2520upper%2520bounds%252C%250Ademonstrating%2520that%2520they%2520are%2520sublinear%2520with%2520respect%2520to%2520the%2520number%2520of%2520episodes%252C%250Aand%2520present%2520empirical%2520results%2520to%2520support%2520our%2520findings.%2520Our%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/aguilarjose11/pbrlNeurips.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RA-RLHF%3A%20Provably%20Efficient%20Risk-Aware%20Reinforcement%20Learning%20Human%0A%20%20Feedback&entry.906535625=Yujie%20Zhao%20and%20Jose%20Efraim%20Aguilar%20Escamill%20and%20Weyl%20Lu%20and%20Huazheng%20Wang&entry.1292438233=%20%20Reinforcement%20Learning%20Human%20Feedback%20%28RLHF%29%20studies%20the%20problem%20where%20agents%0Areceive%20only%20preferences%20over%20pairs%20of%20trajectories%20in%20each%20episode.%0ATraditional%20approaches%20in%20this%20field%20have%20predominantly%20focused%20on%20the%20mean%0Areward%20or%20utility%20criterion.%20However%2C%20in%20RLHF%20scenarios%20demanding%20heightened%0Arisk%20awareness%2C%20such%20as%20in%20AI%20systems%2C%20healthcare%2C%20and%20agriculture%2C%20risk-aware%0Ameasures%20are%20requisite.%20Traditional%20risk-aware%20objectives%20and%20algorithms%20are%0Anot%20applicable%20in%20such%20one-episode-reward%20settings.%20To%20address%20this%2C%20we%20explore%0Aand%20prove%20the%20applicability%20of%20two%20risk-aware%20objectives%20to%20RLHF%3A%20nested%20and%0Astatic%20quantile%20risk%20objectives.%20We%20also%20introduce%20Risk-Aware-RLHF%20%28RA-RLHF%29%2C%0Aan%20algorithm%20designed%20to%20optimize%20both%20nested%20and%20static%20objectives.%0AAdditionally%2C%20we%20provide%20a%20theoretical%20analysis%20of%20the%20regret%20upper%20bounds%2C%0Ademonstrating%20that%20they%20are%20sublinear%20with%20respect%20to%20the%20number%20of%20episodes%2C%0Aand%20present%20empirical%20results%20to%20support%20our%20findings.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/aguilarjose11/pbrlNeurips.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23569v2&entry.124074799=Read"},
{"title": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning", "author": "Yixin Liu and Shiyuan Li and Yu Zheng and Qingfeng Chen and Chengqi Zhang and Shirui Pan", "abstract": "  Graph anomaly detection (GAD), which aims to identify abnormal nodes that\ndiffer from the majority within a graph, has garnered significant attention.\nHowever, current GAD methods necessitate training specific to each dataset,\nresulting in high training costs, substantial data requirements, and limited\ngeneralizability when being applied to new datasets and domains. To address\nthese limitations, this paper proposes ARC, a generalist GAD approach that\nenables a ``one-for-all'' GAD model to detect anomalies across various graph\ndatasets on-the-fly. Equipped with in-context learning, ARC can directly\nextract dataset-specific patterns from the target dataset using few-shot normal\nsamples at the inference stage, without the need for retraining or fine-tuning\non the target dataset. ARC comprises three components that are well-crafted for\ncapturing universal graph anomaly patterns: 1) smoothness-based feature\nAlignment module that unifies the features of different datasets into a common\nand anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns\nabnormality-related node embeddings; and 3) cross-attentive in-Context anomaly\nscoring module that predicts node abnormality by leveraging few-shot normal\nsamples. Extensive experiments on multiple benchmark datasets from various\ndomains demonstrate the superior anomaly detection performance, efficiency, and\ngeneralizability of ARC.\n", "link": "http://arxiv.org/abs/2405.16771v2", "date": "2024-12-24", "relevancy": 1.9578, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4962}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARC%3A%20A%20Generalist%20Graph%20Anomaly%20Detector%20with%20In-Context%20Learning&body=Title%3A%20ARC%3A%20A%20Generalist%20Graph%20Anomaly%20Detector%20with%20In-Context%20Learning%0AAuthor%3A%20Yixin%20Liu%20and%20Shiyuan%20Li%20and%20Yu%20Zheng%20and%20Qingfeng%20Chen%20and%20Chengqi%20Zhang%20and%20Shirui%20Pan%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%2C%20which%20aims%20to%20identify%20abnormal%20nodes%20that%0Adiffer%20from%20the%20majority%20within%20a%20graph%2C%20has%20garnered%20significant%20attention.%0AHowever%2C%20current%20GAD%20methods%20necessitate%20training%20specific%20to%20each%20dataset%2C%0Aresulting%20in%20high%20training%20costs%2C%20substantial%20data%20requirements%2C%20and%20limited%0Ageneralizability%20when%20being%20applied%20to%20new%20datasets%20and%20domains.%20To%20address%0Athese%20limitations%2C%20this%20paper%20proposes%20ARC%2C%20a%20generalist%20GAD%20approach%20that%0Aenables%20a%20%60%60one-for-all%27%27%20GAD%20model%20to%20detect%20anomalies%20across%20various%20graph%0Adatasets%20on-the-fly.%20Equipped%20with%20in-context%20learning%2C%20ARC%20can%20directly%0Aextract%20dataset-specific%20patterns%20from%20the%20target%20dataset%20using%20few-shot%20normal%0Asamples%20at%20the%20inference%20stage%2C%20without%20the%20need%20for%20retraining%20or%20fine-tuning%0Aon%20the%20target%20dataset.%20ARC%20comprises%20three%20components%20that%20are%20well-crafted%20for%0Acapturing%20universal%20graph%20anomaly%20patterns%3A%201%29%20smoothness-based%20feature%0AAlignment%20module%20that%20unifies%20the%20features%20of%20different%20datasets%20into%20a%20common%0Aand%20anomaly-sensitive%20space%3B%202%29%20ego-neighbor%20Residual%20graph%20encoder%20that%20learns%0Aabnormality-related%20node%20embeddings%3B%20and%203%29%20cross-attentive%20in-Context%20anomaly%0Ascoring%20module%20that%20predicts%20node%20abnormality%20by%20leveraging%20few-shot%20normal%0Asamples.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20from%20various%0Adomains%20demonstrate%20the%20superior%20anomaly%20detection%20performance%2C%20efficiency%2C%20and%0Ageneralizability%20of%20ARC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARC%253A%2520A%2520Generalist%2520Graph%2520Anomaly%2520Detector%2520with%2520In-Context%2520Learning%26entry.906535625%3DYixin%2520Liu%2520and%2520Shiyuan%2520Li%2520and%2520Yu%2520Zheng%2520and%2520Qingfeng%2520Chen%2520and%2520Chengqi%2520Zhang%2520and%2520Shirui%2520Pan%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%252C%2520which%2520aims%2520to%2520identify%2520abnormal%2520nodes%2520that%250Adiffer%2520from%2520the%2520majority%2520within%2520a%2520graph%252C%2520has%2520garnered%2520significant%2520attention.%250AHowever%252C%2520current%2520GAD%2520methods%2520necessitate%2520training%2520specific%2520to%2520each%2520dataset%252C%250Aresulting%2520in%2520high%2520training%2520costs%252C%2520substantial%2520data%2520requirements%252C%2520and%2520limited%250Ageneralizability%2520when%2520being%2520applied%2520to%2520new%2520datasets%2520and%2520domains.%2520To%2520address%250Athese%2520limitations%252C%2520this%2520paper%2520proposes%2520ARC%252C%2520a%2520generalist%2520GAD%2520approach%2520that%250Aenables%2520a%2520%2560%2560one-for-all%2527%2527%2520GAD%2520model%2520to%2520detect%2520anomalies%2520across%2520various%2520graph%250Adatasets%2520on-the-fly.%2520Equipped%2520with%2520in-context%2520learning%252C%2520ARC%2520can%2520directly%250Aextract%2520dataset-specific%2520patterns%2520from%2520the%2520target%2520dataset%2520using%2520few-shot%2520normal%250Asamples%2520at%2520the%2520inference%2520stage%252C%2520without%2520the%2520need%2520for%2520retraining%2520or%2520fine-tuning%250Aon%2520the%2520target%2520dataset.%2520ARC%2520comprises%2520three%2520components%2520that%2520are%2520well-crafted%2520for%250Acapturing%2520universal%2520graph%2520anomaly%2520patterns%253A%25201%2529%2520smoothness-based%2520feature%250AAlignment%2520module%2520that%2520unifies%2520the%2520features%2520of%2520different%2520datasets%2520into%2520a%2520common%250Aand%2520anomaly-sensitive%2520space%253B%25202%2529%2520ego-neighbor%2520Residual%2520graph%2520encoder%2520that%2520learns%250Aabnormality-related%2520node%2520embeddings%253B%2520and%25203%2529%2520cross-attentive%2520in-Context%2520anomaly%250Ascoring%2520module%2520that%2520predicts%2520node%2520abnormality%2520by%2520leveraging%2520few-shot%2520normal%250Asamples.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520from%2520various%250Adomains%2520demonstrate%2520the%2520superior%2520anomaly%2520detection%2520performance%252C%2520efficiency%252C%2520and%250Ageneralizability%2520of%2520ARC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARC%3A%20A%20Generalist%20Graph%20Anomaly%20Detector%20with%20In-Context%20Learning&entry.906535625=Yixin%20Liu%20and%20Shiyuan%20Li%20and%20Yu%20Zheng%20and%20Qingfeng%20Chen%20and%20Chengqi%20Zhang%20and%20Shirui%20Pan&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%2C%20which%20aims%20to%20identify%20abnormal%20nodes%20that%0Adiffer%20from%20the%20majority%20within%20a%20graph%2C%20has%20garnered%20significant%20attention.%0AHowever%2C%20current%20GAD%20methods%20necessitate%20training%20specific%20to%20each%20dataset%2C%0Aresulting%20in%20high%20training%20costs%2C%20substantial%20data%20requirements%2C%20and%20limited%0Ageneralizability%20when%20being%20applied%20to%20new%20datasets%20and%20domains.%20To%20address%0Athese%20limitations%2C%20this%20paper%20proposes%20ARC%2C%20a%20generalist%20GAD%20approach%20that%0Aenables%20a%20%60%60one-for-all%27%27%20GAD%20model%20to%20detect%20anomalies%20across%20various%20graph%0Adatasets%20on-the-fly.%20Equipped%20with%20in-context%20learning%2C%20ARC%20can%20directly%0Aextract%20dataset-specific%20patterns%20from%20the%20target%20dataset%20using%20few-shot%20normal%0Asamples%20at%20the%20inference%20stage%2C%20without%20the%20need%20for%20retraining%20or%20fine-tuning%0Aon%20the%20target%20dataset.%20ARC%20comprises%20three%20components%20that%20are%20well-crafted%20for%0Acapturing%20universal%20graph%20anomaly%20patterns%3A%201%29%20smoothness-based%20feature%0AAlignment%20module%20that%20unifies%20the%20features%20of%20different%20datasets%20into%20a%20common%0Aand%20anomaly-sensitive%20space%3B%202%29%20ego-neighbor%20Residual%20graph%20encoder%20that%20learns%0Aabnormality-related%20node%20embeddings%3B%20and%203%29%20cross-attentive%20in-Context%20anomaly%0Ascoring%20module%20that%20predicts%20node%20abnormality%20by%20leveraging%20few-shot%20normal%0Asamples.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20from%20various%0Adomains%20demonstrate%20the%20superior%20anomaly%20detection%20performance%2C%20efficiency%2C%20and%0Ageneralizability%20of%20ARC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16771v2&entry.124074799=Read"},
{"title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via\n  Collective Monte Carlo Tree Search", "author": "Huanjin Yao and Jiaxing Huang and Wenhao Wu and Jingyi Zhang and Yibo Wang and Shunyu Liu and Yingjie Wang and Yuxin Song and Haocheng Feng and Li Shen and Dacheng Tao", "abstract": "  In this work, we aim to develop an MLLM that understands and solves questions\nby learning to create each intermediate step of the reasoning involved till the\nfinal answer. To this end, we propose Collective Monte Carlo Tree Search\n(CoMCTS), a new learning-to-reason method for MLLMs, which introduces the\nconcept of collective learning into ``tree search'' for effective and efficient\nreasoning-path searching and learning. The core idea of CoMCTS is to leverage\ncollective knowledge from multiple models to collaboratively conjecture, search\nand identify effective reasoning paths toward correct answers via four\niterative operations including Expansion, Simulation and Error Positioning,\nBackpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a\nmultimodal dataset with a tree of rich, explicit and well-defined reasoning\nnodes for each question. With Mulberry-260k, we perform collective SFT to train\nour model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and\nReflection capabilities. Extensive experiments demonstrate the superiority of\nour proposed methods on various benchmarks. Code will be available at\nhttps://github.com/HJYao00/Mulberry\n", "link": "http://arxiv.org/abs/2412.18319v1", "date": "2024-12-24", "relevancy": 1.9571, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mulberry%3A%20Empowering%20MLLM%20with%20o1-like%20Reasoning%20and%20Reflection%20via%0A%20%20Collective%20Monte%20Carlo%20Tree%20Search&body=Title%3A%20Mulberry%3A%20Empowering%20MLLM%20with%20o1-like%20Reasoning%20and%20Reflection%20via%0A%20%20Collective%20Monte%20Carlo%20Tree%20Search%0AAuthor%3A%20Huanjin%20Yao%20and%20Jiaxing%20Huang%20and%20Wenhao%20Wu%20and%20Jingyi%20Zhang%20and%20Yibo%20Wang%20and%20Shunyu%20Liu%20and%20Yingjie%20Wang%20and%20Yuxin%20Song%20and%20Haocheng%20Feng%20and%20Li%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20aim%20to%20develop%20an%20MLLM%20that%20understands%20and%20solves%20questions%0Aby%20learning%20to%20create%20each%20intermediate%20step%20of%20the%20reasoning%20involved%20till%20the%0Afinal%20answer.%20To%20this%20end%2C%20we%20propose%20Collective%20Monte%20Carlo%20Tree%20Search%0A%28CoMCTS%29%2C%20a%20new%20learning-to-reason%20method%20for%20MLLMs%2C%20which%20introduces%20the%0Aconcept%20of%20collective%20learning%20into%20%60%60tree%20search%27%27%20for%20effective%20and%20efficient%0Areasoning-path%20searching%20and%20learning.%20The%20core%20idea%20of%20CoMCTS%20is%20to%20leverage%0Acollective%20knowledge%20from%20multiple%20models%20to%20collaboratively%20conjecture%2C%20search%0Aand%20identify%20effective%20reasoning%20paths%20toward%20correct%20answers%20via%20four%0Aiterative%20operations%20including%20Expansion%2C%20Simulation%20and%20Error%20Positioning%2C%0ABackpropagation%2C%20and%20Selection.%20Using%20CoMCTS%2C%20we%20construct%20Mulberry-260k%2C%20a%0Amultimodal%20dataset%20with%20a%20tree%20of%20rich%2C%20explicit%20and%20well-defined%20reasoning%0Anodes%20for%20each%20question.%20With%20Mulberry-260k%2C%20we%20perform%20collective%20SFT%20to%20train%0Aour%20model%2C%20Mulberry%2C%20a%20series%20of%20MLLMs%20with%20o1-like%20step-by-step%20Reasoning%20and%0AReflection%20capabilities.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%0Aour%20proposed%20methods%20on%20various%20benchmarks.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/HJYao00/Mulberry%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulberry%253A%2520Empowering%2520MLLM%2520with%2520o1-like%2520Reasoning%2520and%2520Reflection%2520via%250A%2520%2520Collective%2520Monte%2520Carlo%2520Tree%2520Search%26entry.906535625%3DHuanjin%2520Yao%2520and%2520Jiaxing%2520Huang%2520and%2520Wenhao%2520Wu%2520and%2520Jingyi%2520Zhang%2520and%2520Yibo%2520Wang%2520and%2520Shunyu%2520Liu%2520and%2520Yingjie%2520Wang%2520and%2520Yuxin%2520Song%2520and%2520Haocheng%2520Feng%2520and%2520Li%2520Shen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520develop%2520an%2520MLLM%2520that%2520understands%2520and%2520solves%2520questions%250Aby%2520learning%2520to%2520create%2520each%2520intermediate%2520step%2520of%2520the%2520reasoning%2520involved%2520till%2520the%250Afinal%2520answer.%2520To%2520this%2520end%252C%2520we%2520propose%2520Collective%2520Monte%2520Carlo%2520Tree%2520Search%250A%2528CoMCTS%2529%252C%2520a%2520new%2520learning-to-reason%2520method%2520for%2520MLLMs%252C%2520which%2520introduces%2520the%250Aconcept%2520of%2520collective%2520learning%2520into%2520%2560%2560tree%2520search%2527%2527%2520for%2520effective%2520and%2520efficient%250Areasoning-path%2520searching%2520and%2520learning.%2520The%2520core%2520idea%2520of%2520CoMCTS%2520is%2520to%2520leverage%250Acollective%2520knowledge%2520from%2520multiple%2520models%2520to%2520collaboratively%2520conjecture%252C%2520search%250Aand%2520identify%2520effective%2520reasoning%2520paths%2520toward%2520correct%2520answers%2520via%2520four%250Aiterative%2520operations%2520including%2520Expansion%252C%2520Simulation%2520and%2520Error%2520Positioning%252C%250ABackpropagation%252C%2520and%2520Selection.%2520Using%2520CoMCTS%252C%2520we%2520construct%2520Mulberry-260k%252C%2520a%250Amultimodal%2520dataset%2520with%2520a%2520tree%2520of%2520rich%252C%2520explicit%2520and%2520well-defined%2520reasoning%250Anodes%2520for%2520each%2520question.%2520With%2520Mulberry-260k%252C%2520we%2520perform%2520collective%2520SFT%2520to%2520train%250Aour%2520model%252C%2520Mulberry%252C%2520a%2520series%2520of%2520MLLMs%2520with%2520o1-like%2520step-by-step%2520Reasoning%2520and%250AReflection%2520capabilities.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%250Aour%2520proposed%2520methods%2520on%2520various%2520benchmarks.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/HJYao00/Mulberry%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mulberry%3A%20Empowering%20MLLM%20with%20o1-like%20Reasoning%20and%20Reflection%20via%0A%20%20Collective%20Monte%20Carlo%20Tree%20Search&entry.906535625=Huanjin%20Yao%20and%20Jiaxing%20Huang%20and%20Wenhao%20Wu%20and%20Jingyi%20Zhang%20and%20Yibo%20Wang%20and%20Shunyu%20Liu%20and%20Yingjie%20Wang%20and%20Yuxin%20Song%20and%20Haocheng%20Feng%20and%20Li%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20In%20this%20work%2C%20we%20aim%20to%20develop%20an%20MLLM%20that%20understands%20and%20solves%20questions%0Aby%20learning%20to%20create%20each%20intermediate%20step%20of%20the%20reasoning%20involved%20till%20the%0Afinal%20answer.%20To%20this%20end%2C%20we%20propose%20Collective%20Monte%20Carlo%20Tree%20Search%0A%28CoMCTS%29%2C%20a%20new%20learning-to-reason%20method%20for%20MLLMs%2C%20which%20introduces%20the%0Aconcept%20of%20collective%20learning%20into%20%60%60tree%20search%27%27%20for%20effective%20and%20efficient%0Areasoning-path%20searching%20and%20learning.%20The%20core%20idea%20of%20CoMCTS%20is%20to%20leverage%0Acollective%20knowledge%20from%20multiple%20models%20to%20collaboratively%20conjecture%2C%20search%0Aand%20identify%20effective%20reasoning%20paths%20toward%20correct%20answers%20via%20four%0Aiterative%20operations%20including%20Expansion%2C%20Simulation%20and%20Error%20Positioning%2C%0ABackpropagation%2C%20and%20Selection.%20Using%20CoMCTS%2C%20we%20construct%20Mulberry-260k%2C%20a%0Amultimodal%20dataset%20with%20a%20tree%20of%20rich%2C%20explicit%20and%20well-defined%20reasoning%0Anodes%20for%20each%20question.%20With%20Mulberry-260k%2C%20we%20perform%20collective%20SFT%20to%20train%0Aour%20model%2C%20Mulberry%2C%20a%20series%20of%20MLLMs%20with%20o1-like%20step-by-step%20Reasoning%20and%0AReflection%20capabilities.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%0Aour%20proposed%20methods%20on%20various%20benchmarks.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/HJYao00/Mulberry%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18319v1&entry.124074799=Read"},
{"title": "Multi-Agent Norm Perception and Induction in Distributed Healthcare", "author": "Chao Li and Olga Petruchik and Elizaveta Grishanina and Sergey Kovalchuk", "abstract": "  This paper presents a Multi-Agent Norm Perception and Induction Learning\nModel aimed at facilitating the integration of autonomous agent systems into\ndistributed healthcare environments through dynamic interaction processes. The\nnature of the medical norm system and its sharing channels necessitates\ndistinct approaches for Multi-Agent Systems to learn two types of norms.\nBuilding on this foundation, the model enables agents to simultaneously learn\ndescriptive norms, which capture collective tendencies, and prescriptive norms,\nwhich dictate ideal behaviors. Through parameterized mixed probability density\nmodels and practice-enhanced Markov games, the multi-agent system perceives\ndescriptive norms in dynamic interactions and captures emergent prescriptive\nnorms. We conducted experiments using a dataset from a neurological medical\ncenter spanning from 2016 to 2020.\n", "link": "http://arxiv.org/abs/2412.18454v1", "date": "2024-12-24", "relevancy": 1.9565, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4826}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Norm%20Perception%20and%20Induction%20in%20Distributed%20Healthcare&body=Title%3A%20Multi-Agent%20Norm%20Perception%20and%20Induction%20in%20Distributed%20Healthcare%0AAuthor%3A%20Chao%20Li%20and%20Olga%20Petruchik%20and%20Elizaveta%20Grishanina%20and%20Sergey%20Kovalchuk%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Multi-Agent%20Norm%20Perception%20and%20Induction%20Learning%0AModel%20aimed%20at%20facilitating%20the%20integration%20of%20autonomous%20agent%20systems%20into%0Adistributed%20healthcare%20environments%20through%20dynamic%20interaction%20processes.%20The%0Anature%20of%20the%20medical%20norm%20system%20and%20its%20sharing%20channels%20necessitates%0Adistinct%20approaches%20for%20Multi-Agent%20Systems%20to%20learn%20two%20types%20of%20norms.%0ABuilding%20on%20this%20foundation%2C%20the%20model%20enables%20agents%20to%20simultaneously%20learn%0Adescriptive%20norms%2C%20which%20capture%20collective%20tendencies%2C%20and%20prescriptive%20norms%2C%0Awhich%20dictate%20ideal%20behaviors.%20Through%20parameterized%20mixed%20probability%20density%0Amodels%20and%20practice-enhanced%20Markov%20games%2C%20the%20multi-agent%20system%20perceives%0Adescriptive%20norms%20in%20dynamic%20interactions%20and%20captures%20emergent%20prescriptive%0Anorms.%20We%20conducted%20experiments%20using%20a%20dataset%20from%20a%20neurological%20medical%0Acenter%20spanning%20from%202016%20to%202020.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Norm%2520Perception%2520and%2520Induction%2520in%2520Distributed%2520Healthcare%26entry.906535625%3DChao%2520Li%2520and%2520Olga%2520Petruchik%2520and%2520Elizaveta%2520Grishanina%2520and%2520Sergey%2520Kovalchuk%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Multi-Agent%2520Norm%2520Perception%2520and%2520Induction%2520Learning%250AModel%2520aimed%2520at%2520facilitating%2520the%2520integration%2520of%2520autonomous%2520agent%2520systems%2520into%250Adistributed%2520healthcare%2520environments%2520through%2520dynamic%2520interaction%2520processes.%2520The%250Anature%2520of%2520the%2520medical%2520norm%2520system%2520and%2520its%2520sharing%2520channels%2520necessitates%250Adistinct%2520approaches%2520for%2520Multi-Agent%2520Systems%2520to%2520learn%2520two%2520types%2520of%2520norms.%250ABuilding%2520on%2520this%2520foundation%252C%2520the%2520model%2520enables%2520agents%2520to%2520simultaneously%2520learn%250Adescriptive%2520norms%252C%2520which%2520capture%2520collective%2520tendencies%252C%2520and%2520prescriptive%2520norms%252C%250Awhich%2520dictate%2520ideal%2520behaviors.%2520Through%2520parameterized%2520mixed%2520probability%2520density%250Amodels%2520and%2520practice-enhanced%2520Markov%2520games%252C%2520the%2520multi-agent%2520system%2520perceives%250Adescriptive%2520norms%2520in%2520dynamic%2520interactions%2520and%2520captures%2520emergent%2520prescriptive%250Anorms.%2520We%2520conducted%2520experiments%2520using%2520a%2520dataset%2520from%2520a%2520neurological%2520medical%250Acenter%2520spanning%2520from%25202016%2520to%25202020.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Norm%20Perception%20and%20Induction%20in%20Distributed%20Healthcare&entry.906535625=Chao%20Li%20and%20Olga%20Petruchik%20and%20Elizaveta%20Grishanina%20and%20Sergey%20Kovalchuk&entry.1292438233=%20%20This%20paper%20presents%20a%20Multi-Agent%20Norm%20Perception%20and%20Induction%20Learning%0AModel%20aimed%20at%20facilitating%20the%20integration%20of%20autonomous%20agent%20systems%20into%0Adistributed%20healthcare%20environments%20through%20dynamic%20interaction%20processes.%20The%0Anature%20of%20the%20medical%20norm%20system%20and%20its%20sharing%20channels%20necessitates%0Adistinct%20approaches%20for%20Multi-Agent%20Systems%20to%20learn%20two%20types%20of%20norms.%0ABuilding%20on%20this%20foundation%2C%20the%20model%20enables%20agents%20to%20simultaneously%20learn%0Adescriptive%20norms%2C%20which%20capture%20collective%20tendencies%2C%20and%20prescriptive%20norms%2C%0Awhich%20dictate%20ideal%20behaviors.%20Through%20parameterized%20mixed%20probability%20density%0Amodels%20and%20practice-enhanced%20Markov%20games%2C%20the%20multi-agent%20system%20perceives%0Adescriptive%20norms%20in%20dynamic%20interactions%20and%20captures%20emergent%20prescriptive%0Anorms.%20We%20conducted%20experiments%20using%20a%20dataset%20from%20a%20neurological%20medical%0Acenter%20spanning%20from%202016%20to%202020.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18454v1&entry.124074799=Read"},
{"title": "ASP-based Multi-shot Reasoning via DLV2 with Incremental Grounding", "author": "Francesco Calimeri and Giovambattista Ianni and Francesco Pacenza and Simona Perri and Jessica Zangari", "abstract": "  DLV2 is an AI tool for Knowledge Representation and Reasoning which supports\nAnswer Set Programming (ASP) - a logic-based declarative formalism,\nsuccessfully used in both academic and industrial applications. Given a logic\nprogram modelling a computational problem, an execution of DLV2 produces the\nso-called answer sets that correspond one-to-one to the solutions to the\nproblem at hand. The computational process of DLV2 relies on the typical Ground\n& Solve approach where the grounding step transforms the input program into a\nnew, equivalent ground program, and the subsequent solving step applies\npropositional algorithms to search for the answer sets. Recently, emerging\napplications in contexts such as stream reasoning and event processing created\na demand for multi-shot reasoning: here, the system is expected to be reactive\nwhile repeatedly executed over rapidly changing data. In this work, we present\na new incremental reasoner obtained from the evolution of DLV2 towards iterated\nreasoning. Rather than restarting the computation from scratch, the system\nremains alive across repeated shots, and it incrementally handles the internal\ngrounding process. At each shot, the system reuses previous computations for\nbuilding and maintaining a large, more general ground program, from which a\nsmaller yet equivalent portion is determined and used for computing answer\nsets. Notably, the incremental process is performed in a completely transparent\nfashion for the user. We describe the system, its usage, its applicability and\nperformance in some practically relevant domains. Under consideration in Theory\nand Practice of Logic Programming (TPLP).\n", "link": "http://arxiv.org/abs/2412.17143v2", "date": "2024-12-24", "relevancy": 1.9541, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASP-based%20Multi-shot%20Reasoning%20via%20DLV2%20with%20Incremental%20Grounding&body=Title%3A%20ASP-based%20Multi-shot%20Reasoning%20via%20DLV2%20with%20Incremental%20Grounding%0AAuthor%3A%20Francesco%20Calimeri%20and%20Giovambattista%20Ianni%20and%20Francesco%20Pacenza%20and%20Simona%20Perri%20and%20Jessica%20Zangari%0AAbstract%3A%20%20%20DLV2%20is%20an%20AI%20tool%20for%20Knowledge%20Representation%20and%20Reasoning%20which%20supports%0AAnswer%20Set%20Programming%20%28ASP%29%20-%20a%20logic-based%20declarative%20formalism%2C%0Asuccessfully%20used%20in%20both%20academic%20and%20industrial%20applications.%20Given%20a%20logic%0Aprogram%20modelling%20a%20computational%20problem%2C%20an%20execution%20of%20DLV2%20produces%20the%0Aso-called%20answer%20sets%20that%20correspond%20one-to-one%20to%20the%20solutions%20to%20the%0Aproblem%20at%20hand.%20The%20computational%20process%20of%20DLV2%20relies%20on%20the%20typical%20Ground%0A%26%20Solve%20approach%20where%20the%20grounding%20step%20transforms%20the%20input%20program%20into%20a%0Anew%2C%20equivalent%20ground%20program%2C%20and%20the%20subsequent%20solving%20step%20applies%0Apropositional%20algorithms%20to%20search%20for%20the%20answer%20sets.%20Recently%2C%20emerging%0Aapplications%20in%20contexts%20such%20as%20stream%20reasoning%20and%20event%20processing%20created%0Aa%20demand%20for%20multi-shot%20reasoning%3A%20here%2C%20the%20system%20is%20expected%20to%20be%20reactive%0Awhile%20repeatedly%20executed%20over%20rapidly%20changing%20data.%20In%20this%20work%2C%20we%20present%0Aa%20new%20incremental%20reasoner%20obtained%20from%20the%20evolution%20of%20DLV2%20towards%20iterated%0Areasoning.%20Rather%20than%20restarting%20the%20computation%20from%20scratch%2C%20the%20system%0Aremains%20alive%20across%20repeated%20shots%2C%20and%20it%20incrementally%20handles%20the%20internal%0Agrounding%20process.%20At%20each%20shot%2C%20the%20system%20reuses%20previous%20computations%20for%0Abuilding%20and%20maintaining%20a%20large%2C%20more%20general%20ground%20program%2C%20from%20which%20a%0Asmaller%20yet%20equivalent%20portion%20is%20determined%20and%20used%20for%20computing%20answer%0Asets.%20Notably%2C%20the%20incremental%20process%20is%20performed%20in%20a%20completely%20transparent%0Afashion%20for%20the%20user.%20We%20describe%20the%20system%2C%20its%20usage%2C%20its%20applicability%20and%0Aperformance%20in%20some%20practically%20relevant%20domains.%20Under%20consideration%20in%20Theory%0Aand%20Practice%20of%20Logic%20Programming%20%28TPLP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASP-based%2520Multi-shot%2520Reasoning%2520via%2520DLV2%2520with%2520Incremental%2520Grounding%26entry.906535625%3DFrancesco%2520Calimeri%2520and%2520Giovambattista%2520Ianni%2520and%2520Francesco%2520Pacenza%2520and%2520Simona%2520Perri%2520and%2520Jessica%2520Zangari%26entry.1292438233%3D%2520%2520DLV2%2520is%2520an%2520AI%2520tool%2520for%2520Knowledge%2520Representation%2520and%2520Reasoning%2520which%2520supports%250AAnswer%2520Set%2520Programming%2520%2528ASP%2529%2520-%2520a%2520logic-based%2520declarative%2520formalism%252C%250Asuccessfully%2520used%2520in%2520both%2520academic%2520and%2520industrial%2520applications.%2520Given%2520a%2520logic%250Aprogram%2520modelling%2520a%2520computational%2520problem%252C%2520an%2520execution%2520of%2520DLV2%2520produces%2520the%250Aso-called%2520answer%2520sets%2520that%2520correspond%2520one-to-one%2520to%2520the%2520solutions%2520to%2520the%250Aproblem%2520at%2520hand.%2520The%2520computational%2520process%2520of%2520DLV2%2520relies%2520on%2520the%2520typical%2520Ground%250A%2526%2520Solve%2520approach%2520where%2520the%2520grounding%2520step%2520transforms%2520the%2520input%2520program%2520into%2520a%250Anew%252C%2520equivalent%2520ground%2520program%252C%2520and%2520the%2520subsequent%2520solving%2520step%2520applies%250Apropositional%2520algorithms%2520to%2520search%2520for%2520the%2520answer%2520sets.%2520Recently%252C%2520emerging%250Aapplications%2520in%2520contexts%2520such%2520as%2520stream%2520reasoning%2520and%2520event%2520processing%2520created%250Aa%2520demand%2520for%2520multi-shot%2520reasoning%253A%2520here%252C%2520the%2520system%2520is%2520expected%2520to%2520be%2520reactive%250Awhile%2520repeatedly%2520executed%2520over%2520rapidly%2520changing%2520data.%2520In%2520this%2520work%252C%2520we%2520present%250Aa%2520new%2520incremental%2520reasoner%2520obtained%2520from%2520the%2520evolution%2520of%2520DLV2%2520towards%2520iterated%250Areasoning.%2520Rather%2520than%2520restarting%2520the%2520computation%2520from%2520scratch%252C%2520the%2520system%250Aremains%2520alive%2520across%2520repeated%2520shots%252C%2520and%2520it%2520incrementally%2520handles%2520the%2520internal%250Agrounding%2520process.%2520At%2520each%2520shot%252C%2520the%2520system%2520reuses%2520previous%2520computations%2520for%250Abuilding%2520and%2520maintaining%2520a%2520large%252C%2520more%2520general%2520ground%2520program%252C%2520from%2520which%2520a%250Asmaller%2520yet%2520equivalent%2520portion%2520is%2520determined%2520and%2520used%2520for%2520computing%2520answer%250Asets.%2520Notably%252C%2520the%2520incremental%2520process%2520is%2520performed%2520in%2520a%2520completely%2520transparent%250Afashion%2520for%2520the%2520user.%2520We%2520describe%2520the%2520system%252C%2520its%2520usage%252C%2520its%2520applicability%2520and%250Aperformance%2520in%2520some%2520practically%2520relevant%2520domains.%2520Under%2520consideration%2520in%2520Theory%250Aand%2520Practice%2520of%2520Logic%2520Programming%2520%2528TPLP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASP-based%20Multi-shot%20Reasoning%20via%20DLV2%20with%20Incremental%20Grounding&entry.906535625=Francesco%20Calimeri%20and%20Giovambattista%20Ianni%20and%20Francesco%20Pacenza%20and%20Simona%20Perri%20and%20Jessica%20Zangari&entry.1292438233=%20%20DLV2%20is%20an%20AI%20tool%20for%20Knowledge%20Representation%20and%20Reasoning%20which%20supports%0AAnswer%20Set%20Programming%20%28ASP%29%20-%20a%20logic-based%20declarative%20formalism%2C%0Asuccessfully%20used%20in%20both%20academic%20and%20industrial%20applications.%20Given%20a%20logic%0Aprogram%20modelling%20a%20computational%20problem%2C%20an%20execution%20of%20DLV2%20produces%20the%0Aso-called%20answer%20sets%20that%20correspond%20one-to-one%20to%20the%20solutions%20to%20the%0Aproblem%20at%20hand.%20The%20computational%20process%20of%20DLV2%20relies%20on%20the%20typical%20Ground%0A%26%20Solve%20approach%20where%20the%20grounding%20step%20transforms%20the%20input%20program%20into%20a%0Anew%2C%20equivalent%20ground%20program%2C%20and%20the%20subsequent%20solving%20step%20applies%0Apropositional%20algorithms%20to%20search%20for%20the%20answer%20sets.%20Recently%2C%20emerging%0Aapplications%20in%20contexts%20such%20as%20stream%20reasoning%20and%20event%20processing%20created%0Aa%20demand%20for%20multi-shot%20reasoning%3A%20here%2C%20the%20system%20is%20expected%20to%20be%20reactive%0Awhile%20repeatedly%20executed%20over%20rapidly%20changing%20data.%20In%20this%20work%2C%20we%20present%0Aa%20new%20incremental%20reasoner%20obtained%20from%20the%20evolution%20of%20DLV2%20towards%20iterated%0Areasoning.%20Rather%20than%20restarting%20the%20computation%20from%20scratch%2C%20the%20system%0Aremains%20alive%20across%20repeated%20shots%2C%20and%20it%20incrementally%20handles%20the%20internal%0Agrounding%20process.%20At%20each%20shot%2C%20the%20system%20reuses%20previous%20computations%20for%0Abuilding%20and%20maintaining%20a%20large%2C%20more%20general%20ground%20program%2C%20from%20which%20a%0Asmaller%20yet%20equivalent%20portion%20is%20determined%20and%20used%20for%20computing%20answer%0Asets.%20Notably%2C%20the%20incremental%20process%20is%20performed%20in%20a%20completely%20transparent%0Afashion%20for%20the%20user.%20We%20describe%20the%20system%2C%20its%20usage%2C%20its%20applicability%20and%0Aperformance%20in%20some%20practically%20relevant%20domains.%20Under%20consideration%20in%20Theory%0Aand%20Practice%20of%20Logic%20Programming%20%28TPLP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17143v2&entry.124074799=Read"},
{"title": "Enhancing the Performance of Neural Networks Through Causal Discovery\n  and Integration of Domain Knowledge", "author": "Xiaoge Zhang and Xiao-Lin Wang and Fenglei Fan and Yiu-Ming Cheung and Indranil Bose", "abstract": "  In this paper, we develop a generic methodology to encode hierarchical\ncausality structure among observed variables into a neural network in order to\nimprove its predictive performance. The proposed methodology, called\ncausality-informed neural network (CINN), leverages three coherent steps to\nsystematically map the structural causal knowledge into the layer-to-layer\ndesign of neural network while strictly preserving the orientation of every\ncausal relationship. In the first step, CINN discovers causal relationships\nfrom observational data via directed acyclic graph (DAG) learning, where causal\ndiscovery is recast as a continuous optimization problem to avoid the\ncombinatorial nature. In the second step, the discovered hierarchical causality\nstructure among observed variables is systematically encoded into neural\nnetwork through a dedicated architecture and customized loss function. By\ncategorizing variables in the causal DAG as root, intermediate, and leaf nodes,\nthe hierarchical causal DAG is translated into CINN with a one-to-one\ncorrespondence between nodes in the causal DAG and units in the CINN while\nmaintaining the relative order among these nodes. Regarding the loss function,\nboth intermediate and leaf nodes in the DAG graph are treated as target outputs\nduring CINN training so as to drive co-learning of causal relationships among\ndifferent types of nodes. As multiple loss components emerge in CINN, we\nleverage the projection of conflicting gradients to mitigate gradient\ninterference among the multiple learning tasks. Computational experiments\nacross a broad spectrum of UCI data sets demonstrate substantial advantages of\nCINN in predictive performance over other state-of-the-art methods. In\naddition, an ablation study underscores the value of integrating structural and\nquantitative causal knowledge in enhancing the neural network's predictive\nperformance incrementally.\n", "link": "http://arxiv.org/abs/2311.17303v3", "date": "2024-12-24", "relevancy": 1.9443, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5259}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4659}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20the%20Performance%20of%20Neural%20Networks%20Through%20Causal%20Discovery%0A%20%20and%20Integration%20of%20Domain%20Knowledge&body=Title%3A%20Enhancing%20the%20Performance%20of%20Neural%20Networks%20Through%20Causal%20Discovery%0A%20%20and%20Integration%20of%20Domain%20Knowledge%0AAuthor%3A%20Xiaoge%20Zhang%20and%20Xiao-Lin%20Wang%20and%20Fenglei%20Fan%20and%20Yiu-Ming%20Cheung%20and%20Indranil%20Bose%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20generic%20methodology%20to%20encode%20hierarchical%0Acausality%20structure%20among%20observed%20variables%20into%20a%20neural%20network%20in%20order%20to%0Aimprove%20its%20predictive%20performance.%20The%20proposed%20methodology%2C%20called%0Acausality-informed%20neural%20network%20%28CINN%29%2C%20leverages%20three%20coherent%20steps%20to%0Asystematically%20map%20the%20structural%20causal%20knowledge%20into%20the%20layer-to-layer%0Adesign%20of%20neural%20network%20while%20strictly%20preserving%20the%20orientation%20of%20every%0Acausal%20relationship.%20In%20the%20first%20step%2C%20CINN%20discovers%20causal%20relationships%0Afrom%20observational%20data%20via%20directed%20acyclic%20graph%20%28DAG%29%20learning%2C%20where%20causal%0Adiscovery%20is%20recast%20as%20a%20continuous%20optimization%20problem%20to%20avoid%20the%0Acombinatorial%20nature.%20In%20the%20second%20step%2C%20the%20discovered%20hierarchical%20causality%0Astructure%20among%20observed%20variables%20is%20systematically%20encoded%20into%20neural%0Anetwork%20through%20a%20dedicated%20architecture%20and%20customized%20loss%20function.%20By%0Acategorizing%20variables%20in%20the%20causal%20DAG%20as%20root%2C%20intermediate%2C%20and%20leaf%20nodes%2C%0Athe%20hierarchical%20causal%20DAG%20is%20translated%20into%20CINN%20with%20a%20one-to-one%0Acorrespondence%20between%20nodes%20in%20the%20causal%20DAG%20and%20units%20in%20the%20CINN%20while%0Amaintaining%20the%20relative%20order%20among%20these%20nodes.%20Regarding%20the%20loss%20function%2C%0Aboth%20intermediate%20and%20leaf%20nodes%20in%20the%20DAG%20graph%20are%20treated%20as%20target%20outputs%0Aduring%20CINN%20training%20so%20as%20to%20drive%20co-learning%20of%20causal%20relationships%20among%0Adifferent%20types%20of%20nodes.%20As%20multiple%20loss%20components%20emerge%20in%20CINN%2C%20we%0Aleverage%20the%20projection%20of%20conflicting%20gradients%20to%20mitigate%20gradient%0Ainterference%20among%20the%20multiple%20learning%20tasks.%20Computational%20experiments%0Aacross%20a%20broad%20spectrum%20of%20UCI%20data%20sets%20demonstrate%20substantial%20advantages%20of%0ACINN%20in%20predictive%20performance%20over%20other%20state-of-the-art%20methods.%20In%0Aaddition%2C%20an%20ablation%20study%20underscores%20the%20value%20of%20integrating%20structural%20and%0Aquantitative%20causal%20knowledge%20in%20enhancing%20the%20neural%20network%27s%20predictive%0Aperformance%20incrementally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17303v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520the%2520Performance%2520of%2520Neural%2520Networks%2520Through%2520Causal%2520Discovery%250A%2520%2520and%2520Integration%2520of%2520Domain%2520Knowledge%26entry.906535625%3DXiaoge%2520Zhang%2520and%2520Xiao-Lin%2520Wang%2520and%2520Fenglei%2520Fan%2520and%2520Yiu-Ming%2520Cheung%2520and%2520Indranil%2520Bose%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520generic%2520methodology%2520to%2520encode%2520hierarchical%250Acausality%2520structure%2520among%2520observed%2520variables%2520into%2520a%2520neural%2520network%2520in%2520order%2520to%250Aimprove%2520its%2520predictive%2520performance.%2520The%2520proposed%2520methodology%252C%2520called%250Acausality-informed%2520neural%2520network%2520%2528CINN%2529%252C%2520leverages%2520three%2520coherent%2520steps%2520to%250Asystematically%2520map%2520the%2520structural%2520causal%2520knowledge%2520into%2520the%2520layer-to-layer%250Adesign%2520of%2520neural%2520network%2520while%2520strictly%2520preserving%2520the%2520orientation%2520of%2520every%250Acausal%2520relationship.%2520In%2520the%2520first%2520step%252C%2520CINN%2520discovers%2520causal%2520relationships%250Afrom%2520observational%2520data%2520via%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520learning%252C%2520where%2520causal%250Adiscovery%2520is%2520recast%2520as%2520a%2520continuous%2520optimization%2520problem%2520to%2520avoid%2520the%250Acombinatorial%2520nature.%2520In%2520the%2520second%2520step%252C%2520the%2520discovered%2520hierarchical%2520causality%250Astructure%2520among%2520observed%2520variables%2520is%2520systematically%2520encoded%2520into%2520neural%250Anetwork%2520through%2520a%2520dedicated%2520architecture%2520and%2520customized%2520loss%2520function.%2520By%250Acategorizing%2520variables%2520in%2520the%2520causal%2520DAG%2520as%2520root%252C%2520intermediate%252C%2520and%2520leaf%2520nodes%252C%250Athe%2520hierarchical%2520causal%2520DAG%2520is%2520translated%2520into%2520CINN%2520with%2520a%2520one-to-one%250Acorrespondence%2520between%2520nodes%2520in%2520the%2520causal%2520DAG%2520and%2520units%2520in%2520the%2520CINN%2520while%250Amaintaining%2520the%2520relative%2520order%2520among%2520these%2520nodes.%2520Regarding%2520the%2520loss%2520function%252C%250Aboth%2520intermediate%2520and%2520leaf%2520nodes%2520in%2520the%2520DAG%2520graph%2520are%2520treated%2520as%2520target%2520outputs%250Aduring%2520CINN%2520training%2520so%2520as%2520to%2520drive%2520co-learning%2520of%2520causal%2520relationships%2520among%250Adifferent%2520types%2520of%2520nodes.%2520As%2520multiple%2520loss%2520components%2520emerge%2520in%2520CINN%252C%2520we%250Aleverage%2520the%2520projection%2520of%2520conflicting%2520gradients%2520to%2520mitigate%2520gradient%250Ainterference%2520among%2520the%2520multiple%2520learning%2520tasks.%2520Computational%2520experiments%250Aacross%2520a%2520broad%2520spectrum%2520of%2520UCI%2520data%2520sets%2520demonstrate%2520substantial%2520advantages%2520of%250ACINN%2520in%2520predictive%2520performance%2520over%2520other%2520state-of-the-art%2520methods.%2520In%250Aaddition%252C%2520an%2520ablation%2520study%2520underscores%2520the%2520value%2520of%2520integrating%2520structural%2520and%250Aquantitative%2520causal%2520knowledge%2520in%2520enhancing%2520the%2520neural%2520network%2527s%2520predictive%250Aperformance%2520incrementally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17303v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Performance%20of%20Neural%20Networks%20Through%20Causal%20Discovery%0A%20%20and%20Integration%20of%20Domain%20Knowledge&entry.906535625=Xiaoge%20Zhang%20and%20Xiao-Lin%20Wang%20and%20Fenglei%20Fan%20and%20Yiu-Ming%20Cheung%20and%20Indranil%20Bose&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20generic%20methodology%20to%20encode%20hierarchical%0Acausality%20structure%20among%20observed%20variables%20into%20a%20neural%20network%20in%20order%20to%0Aimprove%20its%20predictive%20performance.%20The%20proposed%20methodology%2C%20called%0Acausality-informed%20neural%20network%20%28CINN%29%2C%20leverages%20three%20coherent%20steps%20to%0Asystematically%20map%20the%20structural%20causal%20knowledge%20into%20the%20layer-to-layer%0Adesign%20of%20neural%20network%20while%20strictly%20preserving%20the%20orientation%20of%20every%0Acausal%20relationship.%20In%20the%20first%20step%2C%20CINN%20discovers%20causal%20relationships%0Afrom%20observational%20data%20via%20directed%20acyclic%20graph%20%28DAG%29%20learning%2C%20where%20causal%0Adiscovery%20is%20recast%20as%20a%20continuous%20optimization%20problem%20to%20avoid%20the%0Acombinatorial%20nature.%20In%20the%20second%20step%2C%20the%20discovered%20hierarchical%20causality%0Astructure%20among%20observed%20variables%20is%20systematically%20encoded%20into%20neural%0Anetwork%20through%20a%20dedicated%20architecture%20and%20customized%20loss%20function.%20By%0Acategorizing%20variables%20in%20the%20causal%20DAG%20as%20root%2C%20intermediate%2C%20and%20leaf%20nodes%2C%0Athe%20hierarchical%20causal%20DAG%20is%20translated%20into%20CINN%20with%20a%20one-to-one%0Acorrespondence%20between%20nodes%20in%20the%20causal%20DAG%20and%20units%20in%20the%20CINN%20while%0Amaintaining%20the%20relative%20order%20among%20these%20nodes.%20Regarding%20the%20loss%20function%2C%0Aboth%20intermediate%20and%20leaf%20nodes%20in%20the%20DAG%20graph%20are%20treated%20as%20target%20outputs%0Aduring%20CINN%20training%20so%20as%20to%20drive%20co-learning%20of%20causal%20relationships%20among%0Adifferent%20types%20of%20nodes.%20As%20multiple%20loss%20components%20emerge%20in%20CINN%2C%20we%0Aleverage%20the%20projection%20of%20conflicting%20gradients%20to%20mitigate%20gradient%0Ainterference%20among%20the%20multiple%20learning%20tasks.%20Computational%20experiments%0Aacross%20a%20broad%20spectrum%20of%20UCI%20data%20sets%20demonstrate%20substantial%20advantages%20of%0ACINN%20in%20predictive%20performance%20over%20other%20state-of-the-art%20methods.%20In%0Aaddition%2C%20an%20ablation%20study%20underscores%20the%20value%20of%20integrating%20structural%20and%0Aquantitative%20causal%20knowledge%20in%20enhancing%20the%20neural%20network%27s%20predictive%0Aperformance%20incrementally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17303v3&entry.124074799=Read"},
{"title": "TrackGo: A Flexible and Efficient Method for Controllable Video\n  Generation", "author": "Haitao Zhou and Chuang Wang and Rui Nie and Jinlin Liu and Dongdong Yu and Qian Yu and Changhu Wang", "abstract": "  Recent years have seen substantial progress in diffusion-based controllable\nvideo generation. However, achieving precise control in complex scenarios,\nincluding fine-grained object parts, sophisticated motion trajectories, and\ncoherent background movement, remains a challenge. In this paper, we introduce\nTrackGo, a novel approach that leverages free-form masks and arrows for\nconditional video generation. This method offers users with a flexible and\nprecise mechanism for manipulating video content. We also propose the\nTrackAdapter for control implementation, an efficient and lightweight adapter\ndesigned to be seamlessly integrated into the temporal self-attention layers of\na pretrained video generation model. This design leverages our observation that\nthe attention map of these layers can accurately activate regions corresponding\nto motion in videos. Our experimental results demonstrate that our new\napproach, enhanced by the TrackAdapter, achieves state-of-the-art performance\non key metrics such as FVD, FID, and ObjMC scores.\n", "link": "http://arxiv.org/abs/2408.11475v2", "date": "2024-12-24", "relevancy": 1.9414, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.656}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6543}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackGo%3A%20A%20Flexible%20and%20Efficient%20Method%20for%20Controllable%20Video%0A%20%20Generation&body=Title%3A%20TrackGo%3A%20A%20Flexible%20and%20Efficient%20Method%20for%20Controllable%20Video%0A%20%20Generation%0AAuthor%3A%20Haitao%20Zhou%20and%20Chuang%20Wang%20and%20Rui%20Nie%20and%20Jinlin%20Liu%20and%20Dongdong%20Yu%20and%20Qian%20Yu%20and%20Changhu%20Wang%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20substantial%20progress%20in%20diffusion-based%20controllable%0Avideo%20generation.%20However%2C%20achieving%20precise%20control%20in%20complex%20scenarios%2C%0Aincluding%20fine-grained%20object%20parts%2C%20sophisticated%20motion%20trajectories%2C%20and%0Acoherent%20background%20movement%2C%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20introduce%0ATrackGo%2C%20a%20novel%20approach%20that%20leverages%20free-form%20masks%20and%20arrows%20for%0Aconditional%20video%20generation.%20This%20method%20offers%20users%20with%20a%20flexible%20and%0Aprecise%20mechanism%20for%20manipulating%20video%20content.%20We%20also%20propose%20the%0ATrackAdapter%20for%20control%20implementation%2C%20an%20efficient%20and%20lightweight%20adapter%0Adesigned%20to%20be%20seamlessly%20integrated%20into%20the%20temporal%20self-attention%20layers%20of%0Aa%20pretrained%20video%20generation%20model.%20This%20design%20leverages%20our%20observation%20that%0Athe%20attention%20map%20of%20these%20layers%20can%20accurately%20activate%20regions%20corresponding%0Ato%20motion%20in%20videos.%20Our%20experimental%20results%20demonstrate%20that%20our%20new%0Aapproach%2C%20enhanced%20by%20the%20TrackAdapter%2C%20achieves%20state-of-the-art%20performance%0Aon%20key%20metrics%20such%20as%20FVD%2C%20FID%2C%20and%20ObjMC%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackGo%253A%2520A%2520Flexible%2520and%2520Efficient%2520Method%2520for%2520Controllable%2520Video%250A%2520%2520Generation%26entry.906535625%3DHaitao%2520Zhou%2520and%2520Chuang%2520Wang%2520and%2520Rui%2520Nie%2520and%2520Jinlin%2520Liu%2520and%2520Dongdong%2520Yu%2520and%2520Qian%2520Yu%2520and%2520Changhu%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520substantial%2520progress%2520in%2520diffusion-based%2520controllable%250Avideo%2520generation.%2520However%252C%2520achieving%2520precise%2520control%2520in%2520complex%2520scenarios%252C%250Aincluding%2520fine-grained%2520object%2520parts%252C%2520sophisticated%2520motion%2520trajectories%252C%2520and%250Acoherent%2520background%2520movement%252C%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ATrackGo%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520free-form%2520masks%2520and%2520arrows%2520for%250Aconditional%2520video%2520generation.%2520This%2520method%2520offers%2520users%2520with%2520a%2520flexible%2520and%250Aprecise%2520mechanism%2520for%2520manipulating%2520video%2520content.%2520We%2520also%2520propose%2520the%250ATrackAdapter%2520for%2520control%2520implementation%252C%2520an%2520efficient%2520and%2520lightweight%2520adapter%250Adesigned%2520to%2520be%2520seamlessly%2520integrated%2520into%2520the%2520temporal%2520self-attention%2520layers%2520of%250Aa%2520pretrained%2520video%2520generation%2520model.%2520This%2520design%2520leverages%2520our%2520observation%2520that%250Athe%2520attention%2520map%2520of%2520these%2520layers%2520can%2520accurately%2520activate%2520regions%2520corresponding%250Ato%2520motion%2520in%2520videos.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520our%2520new%250Aapproach%252C%2520enhanced%2520by%2520the%2520TrackAdapter%252C%2520achieves%2520state-of-the-art%2520performance%250Aon%2520key%2520metrics%2520such%2520as%2520FVD%252C%2520FID%252C%2520and%2520ObjMC%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackGo%3A%20A%20Flexible%20and%20Efficient%20Method%20for%20Controllable%20Video%0A%20%20Generation&entry.906535625=Haitao%20Zhou%20and%20Chuang%20Wang%20and%20Rui%20Nie%20and%20Jinlin%20Liu%20and%20Dongdong%20Yu%20and%20Qian%20Yu%20and%20Changhu%20Wang&entry.1292438233=%20%20Recent%20years%20have%20seen%20substantial%20progress%20in%20diffusion-based%20controllable%0Avideo%20generation.%20However%2C%20achieving%20precise%20control%20in%20complex%20scenarios%2C%0Aincluding%20fine-grained%20object%20parts%2C%20sophisticated%20motion%20trajectories%2C%20and%0Acoherent%20background%20movement%2C%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20introduce%0ATrackGo%2C%20a%20novel%20approach%20that%20leverages%20free-form%20masks%20and%20arrows%20for%0Aconditional%20video%20generation.%20This%20method%20offers%20users%20with%20a%20flexible%20and%0Aprecise%20mechanism%20for%20manipulating%20video%20content.%20We%20also%20propose%20the%0ATrackAdapter%20for%20control%20implementation%2C%20an%20efficient%20and%20lightweight%20adapter%0Adesigned%20to%20be%20seamlessly%20integrated%20into%20the%20temporal%20self-attention%20layers%20of%0Aa%20pretrained%20video%20generation%20model.%20This%20design%20leverages%20our%20observation%20that%0Athe%20attention%20map%20of%20these%20layers%20can%20accurately%20activate%20regions%20corresponding%0Ato%20motion%20in%20videos.%20Our%20experimental%20results%20demonstrate%20that%20our%20new%0Aapproach%2C%20enhanced%20by%20the%20TrackAdapter%2C%20achieves%20state-of-the-art%20performance%0Aon%20key%20metrics%20such%20as%20FVD%2C%20FID%2C%20and%20ObjMC%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11475v2&entry.124074799=Read"},
{"title": "ReducedLUT: Table Decomposition with \"Don't Care\" Conditions", "author": "Oliver Cassidy and Marta Andronic and Samuel Coward and George A. Constantinides", "abstract": "  Lookup tables (LUTs) are frequently used to efficiently store arrays of\nprecomputed values for complex mathematical computations. When used in the\ncontext of neural networks, these functions exhibit a lack of recognizable\npatterns which presents an unusual challenge for conventional logic synthesis\ntechniques. Several approaches are known to break down a single large lookup\ntable into multiple smaller ones that can be recombined. Traditional methods,\nsuch as plain tabulation, piecewise linear approximation, and multipartite\ntable methods, often yield inefficient hardware solutions when applied to\nLUT-based NNs.\n  This paper introduces ReducedLUT, a novel method to reduce the footprint of\nthe LUTs by injecting don't cares into the compression process. This additional\nfreedom introduces more self-similarities which can be exploited using known\ndecomposition techniques. We then demonstrate a particular application to\nmachine learning; by replacing unobserved patterns within the training data of\nneural network models with don't cares, we enable greater compression with\nminimal model accuracy degradation. In practice, we achieve up to $1.63\\times$\nreduction in Physical LUT utilization, with a test accuracy drop of no more\nthan $0.01$ accuracy points.\n", "link": "http://arxiv.org/abs/2412.18579v1", "date": "2024-12-24", "relevancy": 1.9412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4958}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4843}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReducedLUT%3A%20Table%20Decomposition%20with%20%22Don%27t%20Care%22%20Conditions&body=Title%3A%20ReducedLUT%3A%20Table%20Decomposition%20with%20%22Don%27t%20Care%22%20Conditions%0AAuthor%3A%20Oliver%20Cassidy%20and%20Marta%20Andronic%20and%20Samuel%20Coward%20and%20George%20A.%20Constantinides%0AAbstract%3A%20%20%20Lookup%20tables%20%28LUTs%29%20are%20frequently%20used%20to%20efficiently%20store%20arrays%20of%0Aprecomputed%20values%20for%20complex%20mathematical%20computations.%20When%20used%20in%20the%0Acontext%20of%20neural%20networks%2C%20these%20functions%20exhibit%20a%20lack%20of%20recognizable%0Apatterns%20which%20presents%20an%20unusual%20challenge%20for%20conventional%20logic%20synthesis%0Atechniques.%20Several%20approaches%20are%20known%20to%20break%20down%20a%20single%20large%20lookup%0Atable%20into%20multiple%20smaller%20ones%20that%20can%20be%20recombined.%20Traditional%20methods%2C%0Asuch%20as%20plain%20tabulation%2C%20piecewise%20linear%20approximation%2C%20and%20multipartite%0Atable%20methods%2C%20often%20yield%20inefficient%20hardware%20solutions%20when%20applied%20to%0ALUT-based%20NNs.%0A%20%20This%20paper%20introduces%20ReducedLUT%2C%20a%20novel%20method%20to%20reduce%20the%20footprint%20of%0Athe%20LUTs%20by%20injecting%20don%27t%20cares%20into%20the%20compression%20process.%20This%20additional%0Afreedom%20introduces%20more%20self-similarities%20which%20can%20be%20exploited%20using%20known%0Adecomposition%20techniques.%20We%20then%20demonstrate%20a%20particular%20application%20to%0Amachine%20learning%3B%20by%20replacing%20unobserved%20patterns%20within%20the%20training%20data%20of%0Aneural%20network%20models%20with%20don%27t%20cares%2C%20we%20enable%20greater%20compression%20with%0Aminimal%20model%20accuracy%20degradation.%20In%20practice%2C%20we%20achieve%20up%20to%20%241.63%5Ctimes%24%0Areduction%20in%20Physical%20LUT%20utilization%2C%20with%20a%20test%20accuracy%20drop%20of%20no%20more%0Athan%20%240.01%24%20accuracy%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducedLUT%253A%2520Table%2520Decomposition%2520with%2520%2522Don%2527t%2520Care%2522%2520Conditions%26entry.906535625%3DOliver%2520Cassidy%2520and%2520Marta%2520Andronic%2520and%2520Samuel%2520Coward%2520and%2520George%2520A.%2520Constantinides%26entry.1292438233%3D%2520%2520Lookup%2520tables%2520%2528LUTs%2529%2520are%2520frequently%2520used%2520to%2520efficiently%2520store%2520arrays%2520of%250Aprecomputed%2520values%2520for%2520complex%2520mathematical%2520computations.%2520When%2520used%2520in%2520the%250Acontext%2520of%2520neural%2520networks%252C%2520these%2520functions%2520exhibit%2520a%2520lack%2520of%2520recognizable%250Apatterns%2520which%2520presents%2520an%2520unusual%2520challenge%2520for%2520conventional%2520logic%2520synthesis%250Atechniques.%2520Several%2520approaches%2520are%2520known%2520to%2520break%2520down%2520a%2520single%2520large%2520lookup%250Atable%2520into%2520multiple%2520smaller%2520ones%2520that%2520can%2520be%2520recombined.%2520Traditional%2520methods%252C%250Asuch%2520as%2520plain%2520tabulation%252C%2520piecewise%2520linear%2520approximation%252C%2520and%2520multipartite%250Atable%2520methods%252C%2520often%2520yield%2520inefficient%2520hardware%2520solutions%2520when%2520applied%2520to%250ALUT-based%2520NNs.%250A%2520%2520This%2520paper%2520introduces%2520ReducedLUT%252C%2520a%2520novel%2520method%2520to%2520reduce%2520the%2520footprint%2520of%250Athe%2520LUTs%2520by%2520injecting%2520don%2527t%2520cares%2520into%2520the%2520compression%2520process.%2520This%2520additional%250Afreedom%2520introduces%2520more%2520self-similarities%2520which%2520can%2520be%2520exploited%2520using%2520known%250Adecomposition%2520techniques.%2520We%2520then%2520demonstrate%2520a%2520particular%2520application%2520to%250Amachine%2520learning%253B%2520by%2520replacing%2520unobserved%2520patterns%2520within%2520the%2520training%2520data%2520of%250Aneural%2520network%2520models%2520with%2520don%2527t%2520cares%252C%2520we%2520enable%2520greater%2520compression%2520with%250Aminimal%2520model%2520accuracy%2520degradation.%2520In%2520practice%252C%2520we%2520achieve%2520up%2520to%2520%25241.63%255Ctimes%2524%250Areduction%2520in%2520Physical%2520LUT%2520utilization%252C%2520with%2520a%2520test%2520accuracy%2520drop%2520of%2520no%2520more%250Athan%2520%25240.01%2524%2520accuracy%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReducedLUT%3A%20Table%20Decomposition%20with%20%22Don%27t%20Care%22%20Conditions&entry.906535625=Oliver%20Cassidy%20and%20Marta%20Andronic%20and%20Samuel%20Coward%20and%20George%20A.%20Constantinides&entry.1292438233=%20%20Lookup%20tables%20%28LUTs%29%20are%20frequently%20used%20to%20efficiently%20store%20arrays%20of%0Aprecomputed%20values%20for%20complex%20mathematical%20computations.%20When%20used%20in%20the%0Acontext%20of%20neural%20networks%2C%20these%20functions%20exhibit%20a%20lack%20of%20recognizable%0Apatterns%20which%20presents%20an%20unusual%20challenge%20for%20conventional%20logic%20synthesis%0Atechniques.%20Several%20approaches%20are%20known%20to%20break%20down%20a%20single%20large%20lookup%0Atable%20into%20multiple%20smaller%20ones%20that%20can%20be%20recombined.%20Traditional%20methods%2C%0Asuch%20as%20plain%20tabulation%2C%20piecewise%20linear%20approximation%2C%20and%20multipartite%0Atable%20methods%2C%20often%20yield%20inefficient%20hardware%20solutions%20when%20applied%20to%0ALUT-based%20NNs.%0A%20%20This%20paper%20introduces%20ReducedLUT%2C%20a%20novel%20method%20to%20reduce%20the%20footprint%20of%0Athe%20LUTs%20by%20injecting%20don%27t%20cares%20into%20the%20compression%20process.%20This%20additional%0Afreedom%20introduces%20more%20self-similarities%20which%20can%20be%20exploited%20using%20known%0Adecomposition%20techniques.%20We%20then%20demonstrate%20a%20particular%20application%20to%0Amachine%20learning%3B%20by%20replacing%20unobserved%20patterns%20within%20the%20training%20data%20of%0Aneural%20network%20models%20with%20don%27t%20cares%2C%20we%20enable%20greater%20compression%20with%0Aminimal%20model%20accuracy%20degradation.%20In%20practice%2C%20we%20achieve%20up%20to%20%241.63%5Ctimes%24%0Areduction%20in%20Physical%20LUT%20utilization%2C%20with%20a%20test%20accuracy%20drop%20of%20no%20more%0Athan%20%240.01%24%20accuracy%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18579v1&entry.124074799=Read"},
{"title": "Exploring Facets of Language Generation in the Limit", "author": "Moses Charikar and Chirag Pabbaraju", "abstract": "  The recent work of Kleinberg & Mullainathan [KM24] provides a concrete model\nfor language generation in the limit: given a sequence of examples from an\nunknown target language, the goal is to generate new examples from the target\nlanguage such that no incorrect examples are generated beyond some point. In\nsharp contrast to strong negative results for the closely related problem of\nlanguage identification, they establish positive results for language\ngeneration in the limit for all countable collections of languages. Follow-up\nwork by Raman & Tewari [RT24] studies bounds on the number of distinct inputs\nrequired by an algorithm before correct language generation is achieved --\nnamely, whether this is a constant for all languages in the collection (uniform\ngeneration) or a language-dependent constant (non-uniform generation).\n  We show that every countable language collection has a generator which has\nthe stronger property of non-uniform generation in the limit. However, while\nthe generation algorithm of [KM24] can be implemented using membership queries,\nwe show that any algorithm cannot non-uniformly generate even for collections\nof just two languages, using only membership queries.\n  We also formalize the tension between validity and breadth in the generation\nalgorithm of [KM24] by introducing a definition of exhaustive generation, and\nshow a strong negative result for exhaustive generation. Our result shows that\na tradeoff between validity and breadth is inherent for generation in the\nlimit. We also provide a precise characterization of the language collections\nfor which exhaustive generation is possible. Finally, inspired by algorithms\nthat can choose to obtain feedback, we consider a model of uniform generation\nwith feedback, completely characterizing language collections for which such\nuniform generation with feedback is possible in terms of a complexity measure\nof the collection.\n", "link": "http://arxiv.org/abs/2411.15364v2", "date": "2024-12-24", "relevancy": 1.9408, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5076}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Facets%20of%20Language%20Generation%20in%20the%20Limit&body=Title%3A%20Exploring%20Facets%20of%20Language%20Generation%20in%20the%20Limit%0AAuthor%3A%20Moses%20Charikar%20and%20Chirag%20Pabbaraju%0AAbstract%3A%20%20%20The%20recent%20work%20of%20Kleinberg%20%26%20Mullainathan%20%5BKM24%5D%20provides%20a%20concrete%20model%0Afor%20language%20generation%20in%20the%20limit%3A%20given%20a%20sequence%20of%20examples%20from%20an%0Aunknown%20target%20language%2C%20the%20goal%20is%20to%20generate%20new%20examples%20from%20the%20target%0Alanguage%20such%20that%20no%20incorrect%20examples%20are%20generated%20beyond%20some%20point.%20In%0Asharp%20contrast%20to%20strong%20negative%20results%20for%20the%20closely%20related%20problem%20of%0Alanguage%20identification%2C%20they%20establish%20positive%20results%20for%20language%0Ageneration%20in%20the%20limit%20for%20all%20countable%20collections%20of%20languages.%20Follow-up%0Awork%20by%20Raman%20%26%20Tewari%20%5BRT24%5D%20studies%20bounds%20on%20the%20number%20of%20distinct%20inputs%0Arequired%20by%20an%20algorithm%20before%20correct%20language%20generation%20is%20achieved%20--%0Anamely%2C%20whether%20this%20is%20a%20constant%20for%20all%20languages%20in%20the%20collection%20%28uniform%0Ageneration%29%20or%20a%20language-dependent%20constant%20%28non-uniform%20generation%29.%0A%20%20We%20show%20that%20every%20countable%20language%20collection%20has%20a%20generator%20which%20has%0Athe%20stronger%20property%20of%20non-uniform%20generation%20in%20the%20limit.%20However%2C%20while%0Athe%20generation%20algorithm%20of%20%5BKM24%5D%20can%20be%20implemented%20using%20membership%20queries%2C%0Awe%20show%20that%20any%20algorithm%20cannot%20non-uniformly%20generate%20even%20for%20collections%0Aof%20just%20two%20languages%2C%20using%20only%20membership%20queries.%0A%20%20We%20also%20formalize%20the%20tension%20between%20validity%20and%20breadth%20in%20the%20generation%0Aalgorithm%20of%20%5BKM24%5D%20by%20introducing%20a%20definition%20of%20exhaustive%20generation%2C%20and%0Ashow%20a%20strong%20negative%20result%20for%20exhaustive%20generation.%20Our%20result%20shows%20that%0Aa%20tradeoff%20between%20validity%20and%20breadth%20is%20inherent%20for%20generation%20in%20the%0Alimit.%20We%20also%20provide%20a%20precise%20characterization%20of%20the%20language%20collections%0Afor%20which%20exhaustive%20generation%20is%20possible.%20Finally%2C%20inspired%20by%20algorithms%0Athat%20can%20choose%20to%20obtain%20feedback%2C%20we%20consider%20a%20model%20of%20uniform%20generation%0Awith%20feedback%2C%20completely%20characterizing%20language%20collections%20for%20which%20such%0Auniform%20generation%20with%20feedback%20is%20possible%20in%20terms%20of%20a%20complexity%20measure%0Aof%20the%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Facets%2520of%2520Language%2520Generation%2520in%2520the%2520Limit%26entry.906535625%3DMoses%2520Charikar%2520and%2520Chirag%2520Pabbaraju%26entry.1292438233%3D%2520%2520The%2520recent%2520work%2520of%2520Kleinberg%2520%2526%2520Mullainathan%2520%255BKM24%255D%2520provides%2520a%2520concrete%2520model%250Afor%2520language%2520generation%2520in%2520the%2520limit%253A%2520given%2520a%2520sequence%2520of%2520examples%2520from%2520an%250Aunknown%2520target%2520language%252C%2520the%2520goal%2520is%2520to%2520generate%2520new%2520examples%2520from%2520the%2520target%250Alanguage%2520such%2520that%2520no%2520incorrect%2520examples%2520are%2520generated%2520beyond%2520some%2520point.%2520In%250Asharp%2520contrast%2520to%2520strong%2520negative%2520results%2520for%2520the%2520closely%2520related%2520problem%2520of%250Alanguage%2520identification%252C%2520they%2520establish%2520positive%2520results%2520for%2520language%250Ageneration%2520in%2520the%2520limit%2520for%2520all%2520countable%2520collections%2520of%2520languages.%2520Follow-up%250Awork%2520by%2520Raman%2520%2526%2520Tewari%2520%255BRT24%255D%2520studies%2520bounds%2520on%2520the%2520number%2520of%2520distinct%2520inputs%250Arequired%2520by%2520an%2520algorithm%2520before%2520correct%2520language%2520generation%2520is%2520achieved%2520--%250Anamely%252C%2520whether%2520this%2520is%2520a%2520constant%2520for%2520all%2520languages%2520in%2520the%2520collection%2520%2528uniform%250Ageneration%2529%2520or%2520a%2520language-dependent%2520constant%2520%2528non-uniform%2520generation%2529.%250A%2520%2520We%2520show%2520that%2520every%2520countable%2520language%2520collection%2520has%2520a%2520generator%2520which%2520has%250Athe%2520stronger%2520property%2520of%2520non-uniform%2520generation%2520in%2520the%2520limit.%2520However%252C%2520while%250Athe%2520generation%2520algorithm%2520of%2520%255BKM24%255D%2520can%2520be%2520implemented%2520using%2520membership%2520queries%252C%250Awe%2520show%2520that%2520any%2520algorithm%2520cannot%2520non-uniformly%2520generate%2520even%2520for%2520collections%250Aof%2520just%2520two%2520languages%252C%2520using%2520only%2520membership%2520queries.%250A%2520%2520We%2520also%2520formalize%2520the%2520tension%2520between%2520validity%2520and%2520breadth%2520in%2520the%2520generation%250Aalgorithm%2520of%2520%255BKM24%255D%2520by%2520introducing%2520a%2520definition%2520of%2520exhaustive%2520generation%252C%2520and%250Ashow%2520a%2520strong%2520negative%2520result%2520for%2520exhaustive%2520generation.%2520Our%2520result%2520shows%2520that%250Aa%2520tradeoff%2520between%2520validity%2520and%2520breadth%2520is%2520inherent%2520for%2520generation%2520in%2520the%250Alimit.%2520We%2520also%2520provide%2520a%2520precise%2520characterization%2520of%2520the%2520language%2520collections%250Afor%2520which%2520exhaustive%2520generation%2520is%2520possible.%2520Finally%252C%2520inspired%2520by%2520algorithms%250Athat%2520can%2520choose%2520to%2520obtain%2520feedback%252C%2520we%2520consider%2520a%2520model%2520of%2520uniform%2520generation%250Awith%2520feedback%252C%2520completely%2520characterizing%2520language%2520collections%2520for%2520which%2520such%250Auniform%2520generation%2520with%2520feedback%2520is%2520possible%2520in%2520terms%2520of%2520a%2520complexity%2520measure%250Aof%2520the%2520collection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Facets%20of%20Language%20Generation%20in%20the%20Limit&entry.906535625=Moses%20Charikar%20and%20Chirag%20Pabbaraju&entry.1292438233=%20%20The%20recent%20work%20of%20Kleinberg%20%26%20Mullainathan%20%5BKM24%5D%20provides%20a%20concrete%20model%0Afor%20language%20generation%20in%20the%20limit%3A%20given%20a%20sequence%20of%20examples%20from%20an%0Aunknown%20target%20language%2C%20the%20goal%20is%20to%20generate%20new%20examples%20from%20the%20target%0Alanguage%20such%20that%20no%20incorrect%20examples%20are%20generated%20beyond%20some%20point.%20In%0Asharp%20contrast%20to%20strong%20negative%20results%20for%20the%20closely%20related%20problem%20of%0Alanguage%20identification%2C%20they%20establish%20positive%20results%20for%20language%0Ageneration%20in%20the%20limit%20for%20all%20countable%20collections%20of%20languages.%20Follow-up%0Awork%20by%20Raman%20%26%20Tewari%20%5BRT24%5D%20studies%20bounds%20on%20the%20number%20of%20distinct%20inputs%0Arequired%20by%20an%20algorithm%20before%20correct%20language%20generation%20is%20achieved%20--%0Anamely%2C%20whether%20this%20is%20a%20constant%20for%20all%20languages%20in%20the%20collection%20%28uniform%0Ageneration%29%20or%20a%20language-dependent%20constant%20%28non-uniform%20generation%29.%0A%20%20We%20show%20that%20every%20countable%20language%20collection%20has%20a%20generator%20which%20has%0Athe%20stronger%20property%20of%20non-uniform%20generation%20in%20the%20limit.%20However%2C%20while%0Athe%20generation%20algorithm%20of%20%5BKM24%5D%20can%20be%20implemented%20using%20membership%20queries%2C%0Awe%20show%20that%20any%20algorithm%20cannot%20non-uniformly%20generate%20even%20for%20collections%0Aof%20just%20two%20languages%2C%20using%20only%20membership%20queries.%0A%20%20We%20also%20formalize%20the%20tension%20between%20validity%20and%20breadth%20in%20the%20generation%0Aalgorithm%20of%20%5BKM24%5D%20by%20introducing%20a%20definition%20of%20exhaustive%20generation%2C%20and%0Ashow%20a%20strong%20negative%20result%20for%20exhaustive%20generation.%20Our%20result%20shows%20that%0Aa%20tradeoff%20between%20validity%20and%20breadth%20is%20inherent%20for%20generation%20in%20the%0Alimit.%20We%20also%20provide%20a%20precise%20characterization%20of%20the%20language%20collections%0Afor%20which%20exhaustive%20generation%20is%20possible.%20Finally%2C%20inspired%20by%20algorithms%0Athat%20can%20choose%20to%20obtain%20feedback%2C%20we%20consider%20a%20model%20of%20uniform%20generation%0Awith%20feedback%2C%20completely%20characterizing%20language%20collections%20for%20which%20such%0Auniform%20generation%20with%20feedback%20is%20possible%20in%20terms%20of%20a%20complexity%20measure%0Aof%20the%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15364v2&entry.124074799=Read"},
{"title": "Deep Adaptive Interest Network: Personalized Recommendation with\n  Context-Aware Learning", "author": "Shuaishuai Huang and Haowei Yang and You Yao and Xueting Lin and Yuming Tu", "abstract": "  In personalized recommendation systems, accurately capturing users' evolving\ninterests and combining them with contextual information is a critical research\narea. This paper proposes a novel model called the Deep Adaptive Interest\nNetwork (DAIN), which dynamically models users' interests while incorporating\ncontext-aware learning mechanisms to achieve precise and adaptive personalized\nrecommendations. DAIN leverages deep learning techniques to build an adaptive\ninterest network structure that can capture users' interest changes in\nreal-time while further optimizing recommendation results by integrating\ncontextual information. Experiments conducted on several public datasets\ndemonstrate that DAIN excels in both recommendation performance and\ncomputational efficiency. This research not only provides a new solution for\npersonalized recommendation systems but also offers fresh insights into the\napplication of context-aware learning in recommendation systems.\n", "link": "http://arxiv.org/abs/2409.02425v2", "date": "2024-12-24", "relevancy": 1.9404, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4676}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Adaptive%20Interest%20Network%3A%20Personalized%20Recommendation%20with%0A%20%20Context-Aware%20Learning&body=Title%3A%20Deep%20Adaptive%20Interest%20Network%3A%20Personalized%20Recommendation%20with%0A%20%20Context-Aware%20Learning%0AAuthor%3A%20Shuaishuai%20Huang%20and%20Haowei%20Yang%20and%20You%20Yao%20and%20Xueting%20Lin%20and%20Yuming%20Tu%0AAbstract%3A%20%20%20In%20personalized%20recommendation%20systems%2C%20accurately%20capturing%20users%27%20evolving%0Ainterests%20and%20combining%20them%20with%20contextual%20information%20is%20a%20critical%20research%0Aarea.%20This%20paper%20proposes%20a%20novel%20model%20called%20the%20Deep%20Adaptive%20Interest%0ANetwork%20%28DAIN%29%2C%20which%20dynamically%20models%20users%27%20interests%20while%20incorporating%0Acontext-aware%20learning%20mechanisms%20to%20achieve%20precise%20and%20adaptive%20personalized%0Arecommendations.%20DAIN%20leverages%20deep%20learning%20techniques%20to%20build%20an%20adaptive%0Ainterest%20network%20structure%20that%20can%20capture%20users%27%20interest%20changes%20in%0Areal-time%20while%20further%20optimizing%20recommendation%20results%20by%20integrating%0Acontextual%20information.%20Experiments%20conducted%20on%20several%20public%20datasets%0Ademonstrate%20that%20DAIN%20excels%20in%20both%20recommendation%20performance%20and%0Acomputational%20efficiency.%20This%20research%20not%20only%20provides%20a%20new%20solution%20for%0Apersonalized%20recommendation%20systems%20but%20also%20offers%20fresh%20insights%20into%20the%0Aapplication%20of%20context-aware%20learning%20in%20recommendation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Adaptive%2520Interest%2520Network%253A%2520Personalized%2520Recommendation%2520with%250A%2520%2520Context-Aware%2520Learning%26entry.906535625%3DShuaishuai%2520Huang%2520and%2520Haowei%2520Yang%2520and%2520You%2520Yao%2520and%2520Xueting%2520Lin%2520and%2520Yuming%2520Tu%26entry.1292438233%3D%2520%2520In%2520personalized%2520recommendation%2520systems%252C%2520accurately%2520capturing%2520users%2527%2520evolving%250Ainterests%2520and%2520combining%2520them%2520with%2520contextual%2520information%2520is%2520a%2520critical%2520research%250Aarea.%2520This%2520paper%2520proposes%2520a%2520novel%2520model%2520called%2520the%2520Deep%2520Adaptive%2520Interest%250ANetwork%2520%2528DAIN%2529%252C%2520which%2520dynamically%2520models%2520users%2527%2520interests%2520while%2520incorporating%250Acontext-aware%2520learning%2520mechanisms%2520to%2520achieve%2520precise%2520and%2520adaptive%2520personalized%250Arecommendations.%2520DAIN%2520leverages%2520deep%2520learning%2520techniques%2520to%2520build%2520an%2520adaptive%250Ainterest%2520network%2520structure%2520that%2520can%2520capture%2520users%2527%2520interest%2520changes%2520in%250Areal-time%2520while%2520further%2520optimizing%2520recommendation%2520results%2520by%2520integrating%250Acontextual%2520information.%2520Experiments%2520conducted%2520on%2520several%2520public%2520datasets%250Ademonstrate%2520that%2520DAIN%2520excels%2520in%2520both%2520recommendation%2520performance%2520and%250Acomputational%2520efficiency.%2520This%2520research%2520not%2520only%2520provides%2520a%2520new%2520solution%2520for%250Apersonalized%2520recommendation%2520systems%2520but%2520also%2520offers%2520fresh%2520insights%2520into%2520the%250Aapplication%2520of%2520context-aware%2520learning%2520in%2520recommendation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Adaptive%20Interest%20Network%3A%20Personalized%20Recommendation%20with%0A%20%20Context-Aware%20Learning&entry.906535625=Shuaishuai%20Huang%20and%20Haowei%20Yang%20and%20You%20Yao%20and%20Xueting%20Lin%20and%20Yuming%20Tu&entry.1292438233=%20%20In%20personalized%20recommendation%20systems%2C%20accurately%20capturing%20users%27%20evolving%0Ainterests%20and%20combining%20them%20with%20contextual%20information%20is%20a%20critical%20research%0Aarea.%20This%20paper%20proposes%20a%20novel%20model%20called%20the%20Deep%20Adaptive%20Interest%0ANetwork%20%28DAIN%29%2C%20which%20dynamically%20models%20users%27%20interests%20while%20incorporating%0Acontext-aware%20learning%20mechanisms%20to%20achieve%20precise%20and%20adaptive%20personalized%0Arecommendations.%20DAIN%20leverages%20deep%20learning%20techniques%20to%20build%20an%20adaptive%0Ainterest%20network%20structure%20that%20can%20capture%20users%27%20interest%20changes%20in%0Areal-time%20while%20further%20optimizing%20recommendation%20results%20by%20integrating%0Acontextual%20information.%20Experiments%20conducted%20on%20several%20public%20datasets%0Ademonstrate%20that%20DAIN%20excels%20in%20both%20recommendation%20performance%20and%0Acomputational%20efficiency.%20This%20research%20not%20only%20provides%20a%20new%20solution%20for%0Apersonalized%20recommendation%20systems%20but%20also%20offers%20fresh%20insights%20into%20the%0Aapplication%20of%20context-aware%20learning%20in%20recommendation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02425v2&entry.124074799=Read"},
{"title": "Predator Prey Scavenger Model using Holling's Functional Response of\n  Type III and Physics-Informed Deep Neural Networks", "author": "Aneesh Panchal and Kirti Beniwal and Vivek Kumar", "abstract": "  Nonlinear mathematical models introduce the relation between various physical\nand biological interactions present in nature. One of the most famous models is\nthe Lotka-Volterra model which defined the interaction between predator and\nprey species present in nature. However, predators, scavengers, and prey\npopulations coexist in a natural system where scavengers can additionally rely\non the dead bodies of predators present in the system. Keeping this in mind,\nthe formulation and simulation of the predator prey scavenger model is\nintroduced in this paper. For the predation response, respective prey species\nare assumed to have Holling's functional response of type III. The proposed\nmodel is tested for various simulations and is found to be showing satisfactory\nresults in different scenarios. After simulations, the American forest dataset\nis taken for parameter estimation which imitates the real-world case. For\nparameter estimation, a physics-informed deep neural network is used with the\nAdam backpropagation method which prevents the avalanche effect in trainable\nparameters updation. For neural networks, mean square error and\nphysics-informed informed error are considered. After the neural network, the\nhence-found parameters are fine-tuned using the\nBroyden-Fletcher-Goldfarb-Shanno algorithm. Finally, the hence-found parameters\nusing a natural dataset are tested for stability using Jacobian stability\nanalysis. Future research work includes minimization of error induced by\nparameters, bifurcation analysis, and sensitivity analysis of the parameters.\n", "link": "http://arxiv.org/abs/2412.18344v1", "date": "2024-12-24", "relevancy": 1.9389, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predator%20Prey%20Scavenger%20Model%20using%20Holling%27s%20Functional%20Response%20of%0A%20%20Type%20III%20and%20Physics-Informed%20Deep%20Neural%20Networks&body=Title%3A%20Predator%20Prey%20Scavenger%20Model%20using%20Holling%27s%20Functional%20Response%20of%0A%20%20Type%20III%20and%20Physics-Informed%20Deep%20Neural%20Networks%0AAuthor%3A%20Aneesh%20Panchal%20and%20Kirti%20Beniwal%20and%20Vivek%20Kumar%0AAbstract%3A%20%20%20Nonlinear%20mathematical%20models%20introduce%20the%20relation%20between%20various%20physical%0Aand%20biological%20interactions%20present%20in%20nature.%20One%20of%20the%20most%20famous%20models%20is%0Athe%20Lotka-Volterra%20model%20which%20defined%20the%20interaction%20between%20predator%20and%0Aprey%20species%20present%20in%20nature.%20However%2C%20predators%2C%20scavengers%2C%20and%20prey%0Apopulations%20coexist%20in%20a%20natural%20system%20where%20scavengers%20can%20additionally%20rely%0Aon%20the%20dead%20bodies%20of%20predators%20present%20in%20the%20system.%20Keeping%20this%20in%20mind%2C%0Athe%20formulation%20and%20simulation%20of%20the%20predator%20prey%20scavenger%20model%20is%0Aintroduced%20in%20this%20paper.%20For%20the%20predation%20response%2C%20respective%20prey%20species%0Aare%20assumed%20to%20have%20Holling%27s%20functional%20response%20of%20type%20III.%20The%20proposed%0Amodel%20is%20tested%20for%20various%20simulations%20and%20is%20found%20to%20be%20showing%20satisfactory%0Aresults%20in%20different%20scenarios.%20After%20simulations%2C%20the%20American%20forest%20dataset%0Ais%20taken%20for%20parameter%20estimation%20which%20imitates%20the%20real-world%20case.%20For%0Aparameter%20estimation%2C%20a%20physics-informed%20deep%20neural%20network%20is%20used%20with%20the%0AAdam%20backpropagation%20method%20which%20prevents%20the%20avalanche%20effect%20in%20trainable%0Aparameters%20updation.%20For%20neural%20networks%2C%20mean%20square%20error%20and%0Aphysics-informed%20informed%20error%20are%20considered.%20After%20the%20neural%20network%2C%20the%0Ahence-found%20parameters%20are%20fine-tuned%20using%20the%0ABroyden-Fletcher-Goldfarb-Shanno%20algorithm.%20Finally%2C%20the%20hence-found%20parameters%0Ausing%20a%20natural%20dataset%20are%20tested%20for%20stability%20using%20Jacobian%20stability%0Aanalysis.%20Future%20research%20work%20includes%20minimization%20of%20error%20induced%20by%0Aparameters%2C%20bifurcation%20analysis%2C%20and%20sensitivity%20analysis%20of%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredator%2520Prey%2520Scavenger%2520Model%2520using%2520Holling%2527s%2520Functional%2520Response%2520of%250A%2520%2520Type%2520III%2520and%2520Physics-Informed%2520Deep%2520Neural%2520Networks%26entry.906535625%3DAneesh%2520Panchal%2520and%2520Kirti%2520Beniwal%2520and%2520Vivek%2520Kumar%26entry.1292438233%3D%2520%2520Nonlinear%2520mathematical%2520models%2520introduce%2520the%2520relation%2520between%2520various%2520physical%250Aand%2520biological%2520interactions%2520present%2520in%2520nature.%2520One%2520of%2520the%2520most%2520famous%2520models%2520is%250Athe%2520Lotka-Volterra%2520model%2520which%2520defined%2520the%2520interaction%2520between%2520predator%2520and%250Aprey%2520species%2520present%2520in%2520nature.%2520However%252C%2520predators%252C%2520scavengers%252C%2520and%2520prey%250Apopulations%2520coexist%2520in%2520a%2520natural%2520system%2520where%2520scavengers%2520can%2520additionally%2520rely%250Aon%2520the%2520dead%2520bodies%2520of%2520predators%2520present%2520in%2520the%2520system.%2520Keeping%2520this%2520in%2520mind%252C%250Athe%2520formulation%2520and%2520simulation%2520of%2520the%2520predator%2520prey%2520scavenger%2520model%2520is%250Aintroduced%2520in%2520this%2520paper.%2520For%2520the%2520predation%2520response%252C%2520respective%2520prey%2520species%250Aare%2520assumed%2520to%2520have%2520Holling%2527s%2520functional%2520response%2520of%2520type%2520III.%2520The%2520proposed%250Amodel%2520is%2520tested%2520for%2520various%2520simulations%2520and%2520is%2520found%2520to%2520be%2520showing%2520satisfactory%250Aresults%2520in%2520different%2520scenarios.%2520After%2520simulations%252C%2520the%2520American%2520forest%2520dataset%250Ais%2520taken%2520for%2520parameter%2520estimation%2520which%2520imitates%2520the%2520real-world%2520case.%2520For%250Aparameter%2520estimation%252C%2520a%2520physics-informed%2520deep%2520neural%2520network%2520is%2520used%2520with%2520the%250AAdam%2520backpropagation%2520method%2520which%2520prevents%2520the%2520avalanche%2520effect%2520in%2520trainable%250Aparameters%2520updation.%2520For%2520neural%2520networks%252C%2520mean%2520square%2520error%2520and%250Aphysics-informed%2520informed%2520error%2520are%2520considered.%2520After%2520the%2520neural%2520network%252C%2520the%250Ahence-found%2520parameters%2520are%2520fine-tuned%2520using%2520the%250ABroyden-Fletcher-Goldfarb-Shanno%2520algorithm.%2520Finally%252C%2520the%2520hence-found%2520parameters%250Ausing%2520a%2520natural%2520dataset%2520are%2520tested%2520for%2520stability%2520using%2520Jacobian%2520stability%250Aanalysis.%2520Future%2520research%2520work%2520includes%2520minimization%2520of%2520error%2520induced%2520by%250Aparameters%252C%2520bifurcation%2520analysis%252C%2520and%2520sensitivity%2520analysis%2520of%2520the%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predator%20Prey%20Scavenger%20Model%20using%20Holling%27s%20Functional%20Response%20of%0A%20%20Type%20III%20and%20Physics-Informed%20Deep%20Neural%20Networks&entry.906535625=Aneesh%20Panchal%20and%20Kirti%20Beniwal%20and%20Vivek%20Kumar&entry.1292438233=%20%20Nonlinear%20mathematical%20models%20introduce%20the%20relation%20between%20various%20physical%0Aand%20biological%20interactions%20present%20in%20nature.%20One%20of%20the%20most%20famous%20models%20is%0Athe%20Lotka-Volterra%20model%20which%20defined%20the%20interaction%20between%20predator%20and%0Aprey%20species%20present%20in%20nature.%20However%2C%20predators%2C%20scavengers%2C%20and%20prey%0Apopulations%20coexist%20in%20a%20natural%20system%20where%20scavengers%20can%20additionally%20rely%0Aon%20the%20dead%20bodies%20of%20predators%20present%20in%20the%20system.%20Keeping%20this%20in%20mind%2C%0Athe%20formulation%20and%20simulation%20of%20the%20predator%20prey%20scavenger%20model%20is%0Aintroduced%20in%20this%20paper.%20For%20the%20predation%20response%2C%20respective%20prey%20species%0Aare%20assumed%20to%20have%20Holling%27s%20functional%20response%20of%20type%20III.%20The%20proposed%0Amodel%20is%20tested%20for%20various%20simulations%20and%20is%20found%20to%20be%20showing%20satisfactory%0Aresults%20in%20different%20scenarios.%20After%20simulations%2C%20the%20American%20forest%20dataset%0Ais%20taken%20for%20parameter%20estimation%20which%20imitates%20the%20real-world%20case.%20For%0Aparameter%20estimation%2C%20a%20physics-informed%20deep%20neural%20network%20is%20used%20with%20the%0AAdam%20backpropagation%20method%20which%20prevents%20the%20avalanche%20effect%20in%20trainable%0Aparameters%20updation.%20For%20neural%20networks%2C%20mean%20square%20error%20and%0Aphysics-informed%20informed%20error%20are%20considered.%20After%20the%20neural%20network%2C%20the%0Ahence-found%20parameters%20are%20fine-tuned%20using%20the%0ABroyden-Fletcher-Goldfarb-Shanno%20algorithm.%20Finally%2C%20the%20hence-found%20parameters%0Ausing%20a%20natural%20dataset%20are%20tested%20for%20stability%20using%20Jacobian%20stability%0Aanalysis.%20Future%20research%20work%20includes%20minimization%20of%20error%20induced%20by%0Aparameters%2C%20bifurcation%20analysis%2C%20and%20sensitivity%20analysis%20of%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18344v1&entry.124074799=Read"},
{"title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models", "author": "Shuaijie Shen and Chao Wang and Renzhuo Huang and Yan Zhong and Qinghai Guo and Zhichao Lu and Jianguo Zhang and Luziwei Leng", "abstract": "  Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.\n", "link": "http://arxiv.org/abs/2408.14909v2", "date": "2024-12-24", "relevancy": 1.9229, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikingSSMs%3A%20Learning%20Long%20Sequences%20with%20Sparse%20and%20Parallel%20Spiking%0A%20%20State%20Space%20Models&body=Title%3A%20SpikingSSMs%3A%20Learning%20Long%20Sequences%20with%20Sparse%20and%20Parallel%20Spiking%0A%20%20State%20Space%20Models%0AAuthor%3A%20Shuaijie%20Shen%20and%20Chao%20Wang%20and%20Renzhuo%20Huang%20and%20Yan%20Zhong%20and%20Qinghai%20Guo%20and%20Zhichao%20Lu%20and%20Jianguo%20Zhang%20and%20Luziwei%20Leng%0AAbstract%3A%20%20%20Known%20as%20low%20energy%20consumption%20networks%2C%20spiking%20neural%20networks%20%28SNNs%29%20have%0Agained%20a%20lot%20of%20attention%20within%20the%20past%20decades.%20While%20SNNs%20are%20increasing%0Acompetitive%20with%20artificial%20neural%20networks%20%28ANNs%29%20for%20vision%20tasks%2C%20they%20are%0Ararely%20used%20for%20long%20sequence%20tasks%2C%20despite%20their%20intrinsic%20temporal%20dynamics.%0AIn%20this%20work%2C%20we%20develop%20spiking%20state%20space%20models%20%28SpikingSSMs%29%20for%20long%0Asequence%20learning%20by%20leveraging%20on%20the%20sequence%20learning%20abilities%20of%20state%0Aspace%20models%20%28SSMs%29.%20Inspired%20by%20dendritic%20neuron%20structure%2C%20we%20hierarchically%0Aintegrate%20neuronal%20dynamics%20with%20the%20original%20SSM%20block%2C%20meanwhile%20realizing%0Asparse%20synaptic%20computation.%20Furthermore%2C%20to%20solve%20the%20conflict%20of%20event-driven%0Aneuronal%20dynamics%20with%20parallel%20computing%2C%20we%20propose%20a%20light-weight%20surrogate%0Adynamic%20network%20which%20accurately%20predicts%20the%20after-reset%20membrane%20potential%0Aand%20compatible%20to%20learnable%20thresholds%2C%20enabling%20orders%20of%20acceleration%20in%0Atraining%20speed%20compared%20with%20conventional%20iterative%20methods.%20On%20the%20long%20range%0Aarena%20benchmark%20task%2C%20SpikingSSM%20achieves%20competitive%20performance%20to%0Astate-of-the-art%20SSMs%20meanwhile%20realizing%20on%20average%2090%5C%25%20of%20network%20sparsity.%0AOn%20language%20modeling%2C%20our%20network%20significantly%20surpasses%20existing%20spiking%0Alarge%20language%20models%20%28spikingLLMs%29%20on%20the%20WikiText-103%20dataset%20with%20only%20a%0Athird%20of%20the%20model%20size%2C%20demonstrating%20its%20potential%20as%20backbone%20architecture%0Afor%20low%20computation%20cost%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikingSSMs%253A%2520Learning%2520Long%2520Sequences%2520with%2520Sparse%2520and%2520Parallel%2520Spiking%250A%2520%2520State%2520Space%2520Models%26entry.906535625%3DShuaijie%2520Shen%2520and%2520Chao%2520Wang%2520and%2520Renzhuo%2520Huang%2520and%2520Yan%2520Zhong%2520and%2520Qinghai%2520Guo%2520and%2520Zhichao%2520Lu%2520and%2520Jianguo%2520Zhang%2520and%2520Luziwei%2520Leng%26entry.1292438233%3D%2520%2520Known%2520as%2520low%2520energy%2520consumption%2520networks%252C%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520have%250Agained%2520a%2520lot%2520of%2520attention%2520within%2520the%2520past%2520decades.%2520While%2520SNNs%2520are%2520increasing%250Acompetitive%2520with%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%2520for%2520vision%2520tasks%252C%2520they%2520are%250Ararely%2520used%2520for%2520long%2520sequence%2520tasks%252C%2520despite%2520their%2520intrinsic%2520temporal%2520dynamics.%250AIn%2520this%2520work%252C%2520we%2520develop%2520spiking%2520state%2520space%2520models%2520%2528SpikingSSMs%2529%2520for%2520long%250Asequence%2520learning%2520by%2520leveraging%2520on%2520the%2520sequence%2520learning%2520abilities%2520of%2520state%250Aspace%2520models%2520%2528SSMs%2529.%2520Inspired%2520by%2520dendritic%2520neuron%2520structure%252C%2520we%2520hierarchically%250Aintegrate%2520neuronal%2520dynamics%2520with%2520the%2520original%2520SSM%2520block%252C%2520meanwhile%2520realizing%250Asparse%2520synaptic%2520computation.%2520Furthermore%252C%2520to%2520solve%2520the%2520conflict%2520of%2520event-driven%250Aneuronal%2520dynamics%2520with%2520parallel%2520computing%252C%2520we%2520propose%2520a%2520light-weight%2520surrogate%250Adynamic%2520network%2520which%2520accurately%2520predicts%2520the%2520after-reset%2520membrane%2520potential%250Aand%2520compatible%2520to%2520learnable%2520thresholds%252C%2520enabling%2520orders%2520of%2520acceleration%2520in%250Atraining%2520speed%2520compared%2520with%2520conventional%2520iterative%2520methods.%2520On%2520the%2520long%2520range%250Aarena%2520benchmark%2520task%252C%2520SpikingSSM%2520achieves%2520competitive%2520performance%2520to%250Astate-of-the-art%2520SSMs%2520meanwhile%2520realizing%2520on%2520average%252090%255C%2525%2520of%2520network%2520sparsity.%250AOn%2520language%2520modeling%252C%2520our%2520network%2520significantly%2520surpasses%2520existing%2520spiking%250Alarge%2520language%2520models%2520%2528spikingLLMs%2529%2520on%2520the%2520WikiText-103%2520dataset%2520with%2520only%2520a%250Athird%2520of%2520the%2520model%2520size%252C%2520demonstrating%2520its%2520potential%2520as%2520backbone%2520architecture%250Afor%2520low%2520computation%2520cost%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikingSSMs%3A%20Learning%20Long%20Sequences%20with%20Sparse%20and%20Parallel%20Spiking%0A%20%20State%20Space%20Models&entry.906535625=Shuaijie%20Shen%20and%20Chao%20Wang%20and%20Renzhuo%20Huang%20and%20Yan%20Zhong%20and%20Qinghai%20Guo%20and%20Zhichao%20Lu%20and%20Jianguo%20Zhang%20and%20Luziwei%20Leng&entry.1292438233=%20%20Known%20as%20low%20energy%20consumption%20networks%2C%20spiking%20neural%20networks%20%28SNNs%29%20have%0Agained%20a%20lot%20of%20attention%20within%20the%20past%20decades.%20While%20SNNs%20are%20increasing%0Acompetitive%20with%20artificial%20neural%20networks%20%28ANNs%29%20for%20vision%20tasks%2C%20they%20are%0Ararely%20used%20for%20long%20sequence%20tasks%2C%20despite%20their%20intrinsic%20temporal%20dynamics.%0AIn%20this%20work%2C%20we%20develop%20spiking%20state%20space%20models%20%28SpikingSSMs%29%20for%20long%0Asequence%20learning%20by%20leveraging%20on%20the%20sequence%20learning%20abilities%20of%20state%0Aspace%20models%20%28SSMs%29.%20Inspired%20by%20dendritic%20neuron%20structure%2C%20we%20hierarchically%0Aintegrate%20neuronal%20dynamics%20with%20the%20original%20SSM%20block%2C%20meanwhile%20realizing%0Asparse%20synaptic%20computation.%20Furthermore%2C%20to%20solve%20the%20conflict%20of%20event-driven%0Aneuronal%20dynamics%20with%20parallel%20computing%2C%20we%20propose%20a%20light-weight%20surrogate%0Adynamic%20network%20which%20accurately%20predicts%20the%20after-reset%20membrane%20potential%0Aand%20compatible%20to%20learnable%20thresholds%2C%20enabling%20orders%20of%20acceleration%20in%0Atraining%20speed%20compared%20with%20conventional%20iterative%20methods.%20On%20the%20long%20range%0Aarena%20benchmark%20task%2C%20SpikingSSM%20achieves%20competitive%20performance%20to%0Astate-of-the-art%20SSMs%20meanwhile%20realizing%20on%20average%2090%5C%25%20of%20network%20sparsity.%0AOn%20language%20modeling%2C%20our%20network%20significantly%20surpasses%20existing%20spiking%0Alarge%20language%20models%20%28spikingLLMs%29%20on%20the%20WikiText-103%20dataset%20with%20only%20a%0Athird%20of%20the%20model%20size%2C%20demonstrating%20its%20potential%20as%20backbone%20architecture%0Afor%20low%20computation%20cost%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14909v2&entry.124074799=Read"},
{"title": "Ultra-Low Complexity On-Orbit Compression for Remote Sensing Imagery via\n  Block Modulated Imaging", "author": "Zhibin Wang and Yanxin Cai and Jiayi Zhou and Yangming Zhang and Tianyu Li and Wei Li and Xun Liu and Guoqing Wang and Yang Yang", "abstract": "  The growing field of remote sensing faces a challenge: the ever-increasing\nsize and volume of imagery data are exceeding the storage and transmission\ncapabilities of satellite platforms. Efficient compression of remote sensing\nimagery is a critical solution to alleviate these burdens on satellites.\nHowever, existing compression methods are often too computationally expensive\nfor satellites. With the continued advancement of compressed sensing theory,\nsingle-pixel imaging emerges as a powerful tool that brings new possibilities\nfor on-orbit image compression. However, it still suffers from prolonged\nimaging times and the inability to perform high-resolution imaging, hindering\nits practical application. This paper advances the study of compressed sensing\nin remote sensing image compression, proposing Block Modulated Imaging (BMI).\nBy requiring only a single exposure, BMI significantly enhances imaging\nacquisition speeds. Additionally, BMI obviates the need for digital micromirror\ndevices and surpasses limitations in image resolution. Furthermore, we propose\na novel decoding network specifically designed to reconstruct images compressed\nunder the BMI framework. Leveraging the gated 3D convolutions and promoting\nefficient information flow across stages through a Two-Way Cross-Attention\nmodule, our decoding network exhibits demonstrably superior reconstruction\nperformance. Extensive experiments conducted on multiple renowned remote\nsensing datasets unequivocally demonstrate the efficacy of our proposed method.\nTo further validate its practical applicability, we developed and tested a\nprototype of the BMI-based camera, which has shown promising potential for\non-orbit image compression. The code is available at\nhttps://github.com/Johnathan218/BMNet.\n", "link": "http://arxiv.org/abs/2412.18417v1", "date": "2024-12-24", "relevancy": 1.5764, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5406}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-Low%20Complexity%20On-Orbit%20Compression%20for%20Remote%20Sensing%20Imagery%20via%0A%20%20Block%20Modulated%20Imaging&body=Title%3A%20Ultra-Low%20Complexity%20On-Orbit%20Compression%20for%20Remote%20Sensing%20Imagery%20via%0A%20%20Block%20Modulated%20Imaging%0AAuthor%3A%20Zhibin%20Wang%20and%20Yanxin%20Cai%20and%20Jiayi%20Zhou%20and%20Yangming%20Zhang%20and%20Tianyu%20Li%20and%20Wei%20Li%20and%20Xun%20Liu%20and%20Guoqing%20Wang%20and%20Yang%20Yang%0AAbstract%3A%20%20%20The%20growing%20field%20of%20remote%20sensing%20faces%20a%20challenge%3A%20the%20ever-increasing%0Asize%20and%20volume%20of%20imagery%20data%20are%20exceeding%20the%20storage%20and%20transmission%0Acapabilities%20of%20satellite%20platforms.%20Efficient%20compression%20of%20remote%20sensing%0Aimagery%20is%20a%20critical%20solution%20to%20alleviate%20these%20burdens%20on%20satellites.%0AHowever%2C%20existing%20compression%20methods%20are%20often%20too%20computationally%20expensive%0Afor%20satellites.%20With%20the%20continued%20advancement%20of%20compressed%20sensing%20theory%2C%0Asingle-pixel%20imaging%20emerges%20as%20a%20powerful%20tool%20that%20brings%20new%20possibilities%0Afor%20on-orbit%20image%20compression.%20However%2C%20it%20still%20suffers%20from%20prolonged%0Aimaging%20times%20and%20the%20inability%20to%20perform%20high-resolution%20imaging%2C%20hindering%0Aits%20practical%20application.%20This%20paper%20advances%20the%20study%20of%20compressed%20sensing%0Ain%20remote%20sensing%20image%20compression%2C%20proposing%20Block%20Modulated%20Imaging%20%28BMI%29.%0ABy%20requiring%20only%20a%20single%20exposure%2C%20BMI%20significantly%20enhances%20imaging%0Aacquisition%20speeds.%20Additionally%2C%20BMI%20obviates%20the%20need%20for%20digital%20micromirror%0Adevices%20and%20surpasses%20limitations%20in%20image%20resolution.%20Furthermore%2C%20we%20propose%0Aa%20novel%20decoding%20network%20specifically%20designed%20to%20reconstruct%20images%20compressed%0Aunder%20the%20BMI%20framework.%20Leveraging%20the%20gated%203D%20convolutions%20and%20promoting%0Aefficient%20information%20flow%20across%20stages%20through%20a%20Two-Way%20Cross-Attention%0Amodule%2C%20our%20decoding%20network%20exhibits%20demonstrably%20superior%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20multiple%20renowned%20remote%0Asensing%20datasets%20unequivocally%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method.%0ATo%20further%20validate%20its%20practical%20applicability%2C%20we%20developed%20and%20tested%20a%0Aprototype%20of%20the%20BMI-based%20camera%2C%20which%20has%20shown%20promising%20potential%20for%0Aon-orbit%20image%20compression.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Johnathan218/BMNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-Low%2520Complexity%2520On-Orbit%2520Compression%2520for%2520Remote%2520Sensing%2520Imagery%2520via%250A%2520%2520Block%2520Modulated%2520Imaging%26entry.906535625%3DZhibin%2520Wang%2520and%2520Yanxin%2520Cai%2520and%2520Jiayi%2520Zhou%2520and%2520Yangming%2520Zhang%2520and%2520Tianyu%2520Li%2520and%2520Wei%2520Li%2520and%2520Xun%2520Liu%2520and%2520Guoqing%2520Wang%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520The%2520growing%2520field%2520of%2520remote%2520sensing%2520faces%2520a%2520challenge%253A%2520the%2520ever-increasing%250Asize%2520and%2520volume%2520of%2520imagery%2520data%2520are%2520exceeding%2520the%2520storage%2520and%2520transmission%250Acapabilities%2520of%2520satellite%2520platforms.%2520Efficient%2520compression%2520of%2520remote%2520sensing%250Aimagery%2520is%2520a%2520critical%2520solution%2520to%2520alleviate%2520these%2520burdens%2520on%2520satellites.%250AHowever%252C%2520existing%2520compression%2520methods%2520are%2520often%2520too%2520computationally%2520expensive%250Afor%2520satellites.%2520With%2520the%2520continued%2520advancement%2520of%2520compressed%2520sensing%2520theory%252C%250Asingle-pixel%2520imaging%2520emerges%2520as%2520a%2520powerful%2520tool%2520that%2520brings%2520new%2520possibilities%250Afor%2520on-orbit%2520image%2520compression.%2520However%252C%2520it%2520still%2520suffers%2520from%2520prolonged%250Aimaging%2520times%2520and%2520the%2520inability%2520to%2520perform%2520high-resolution%2520imaging%252C%2520hindering%250Aits%2520practical%2520application.%2520This%2520paper%2520advances%2520the%2520study%2520of%2520compressed%2520sensing%250Ain%2520remote%2520sensing%2520image%2520compression%252C%2520proposing%2520Block%2520Modulated%2520Imaging%2520%2528BMI%2529.%250ABy%2520requiring%2520only%2520a%2520single%2520exposure%252C%2520BMI%2520significantly%2520enhances%2520imaging%250Aacquisition%2520speeds.%2520Additionally%252C%2520BMI%2520obviates%2520the%2520need%2520for%2520digital%2520micromirror%250Adevices%2520and%2520surpasses%2520limitations%2520in%2520image%2520resolution.%2520Furthermore%252C%2520we%2520propose%250Aa%2520novel%2520decoding%2520network%2520specifically%2520designed%2520to%2520reconstruct%2520images%2520compressed%250Aunder%2520the%2520BMI%2520framework.%2520Leveraging%2520the%2520gated%25203D%2520convolutions%2520and%2520promoting%250Aefficient%2520information%2520flow%2520across%2520stages%2520through%2520a%2520Two-Way%2520Cross-Attention%250Amodule%252C%2520our%2520decoding%2520network%2520exhibits%2520demonstrably%2520superior%2520reconstruction%250Aperformance.%2520Extensive%2520experiments%2520conducted%2520on%2520multiple%2520renowned%2520remote%250Asensing%2520datasets%2520unequivocally%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520method.%250ATo%2520further%2520validate%2520its%2520practical%2520applicability%252C%2520we%2520developed%2520and%2520tested%2520a%250Aprototype%2520of%2520the%2520BMI-based%2520camera%252C%2520which%2520has%2520shown%2520promising%2520potential%2520for%250Aon-orbit%2520image%2520compression.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Johnathan218/BMNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-Low%20Complexity%20On-Orbit%20Compression%20for%20Remote%20Sensing%20Imagery%20via%0A%20%20Block%20Modulated%20Imaging&entry.906535625=Zhibin%20Wang%20and%20Yanxin%20Cai%20and%20Jiayi%20Zhou%20and%20Yangming%20Zhang%20and%20Tianyu%20Li%20and%20Wei%20Li%20and%20Xun%20Liu%20and%20Guoqing%20Wang%20and%20Yang%20Yang&entry.1292438233=%20%20The%20growing%20field%20of%20remote%20sensing%20faces%20a%20challenge%3A%20the%20ever-increasing%0Asize%20and%20volume%20of%20imagery%20data%20are%20exceeding%20the%20storage%20and%20transmission%0Acapabilities%20of%20satellite%20platforms.%20Efficient%20compression%20of%20remote%20sensing%0Aimagery%20is%20a%20critical%20solution%20to%20alleviate%20these%20burdens%20on%20satellites.%0AHowever%2C%20existing%20compression%20methods%20are%20often%20too%20computationally%20expensive%0Afor%20satellites.%20With%20the%20continued%20advancement%20of%20compressed%20sensing%20theory%2C%0Asingle-pixel%20imaging%20emerges%20as%20a%20powerful%20tool%20that%20brings%20new%20possibilities%0Afor%20on-orbit%20image%20compression.%20However%2C%20it%20still%20suffers%20from%20prolonged%0Aimaging%20times%20and%20the%20inability%20to%20perform%20high-resolution%20imaging%2C%20hindering%0Aits%20practical%20application.%20This%20paper%20advances%20the%20study%20of%20compressed%20sensing%0Ain%20remote%20sensing%20image%20compression%2C%20proposing%20Block%20Modulated%20Imaging%20%28BMI%29.%0ABy%20requiring%20only%20a%20single%20exposure%2C%20BMI%20significantly%20enhances%20imaging%0Aacquisition%20speeds.%20Additionally%2C%20BMI%20obviates%20the%20need%20for%20digital%20micromirror%0Adevices%20and%20surpasses%20limitations%20in%20image%20resolution.%20Furthermore%2C%20we%20propose%0Aa%20novel%20decoding%20network%20specifically%20designed%20to%20reconstruct%20images%20compressed%0Aunder%20the%20BMI%20framework.%20Leveraging%20the%20gated%203D%20convolutions%20and%20promoting%0Aefficient%20information%20flow%20across%20stages%20through%20a%20Two-Way%20Cross-Attention%0Amodule%2C%20our%20decoding%20network%20exhibits%20demonstrably%20superior%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20multiple%20renowned%20remote%0Asensing%20datasets%20unequivocally%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method.%0ATo%20further%20validate%20its%20practical%20applicability%2C%20we%20developed%20and%20tested%20a%0Aprototype%20of%20the%20BMI-based%20camera%2C%20which%20has%20shown%20promising%20potential%20for%0Aon-orbit%20image%20compression.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Johnathan218/BMNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18417v1&entry.124074799=Read"},
{"title": "MixMAS: A Framework for Sampling-Based Mixer Architecture Search for\n  Multimodal Fusion and Learning", "author": "Abdelmadjid Chergui and Grigor Bezirganyan and Sana Sellami and Laure Berti-\u00c9quille and S\u00e9bastien Fournier", "abstract": "  Choosing a suitable deep learning architecture for multimodal data fusion is\na challenging task, as it requires the effective integration and processing of\ndiverse data types, each with distinct structures and characteristics. In this\npaper, we introduce MixMAS, a novel framework for sampling-based mixer\narchitecture search tailored to multimodal learning. Our approach automatically\nselects the optimal MLP-based architecture for a given multimodal machine\nlearning (MML) task. Specifically, MixMAS utilizes a sampling-based\nmicro-benchmarking strategy to explore various combinations of\nmodality-specific encoders, fusion functions, and fusion networks,\nsystematically identifying the architecture that best meets the task's\nperformance metrics.\n", "link": "http://arxiv.org/abs/2412.18437v1", "date": "2024-12-24", "relevancy": 1.537, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MixMAS%3A%20A%20Framework%20for%20Sampling-Based%20Mixer%20Architecture%20Search%20for%0A%20%20Multimodal%20Fusion%20and%20Learning&body=Title%3A%20MixMAS%3A%20A%20Framework%20for%20Sampling-Based%20Mixer%20Architecture%20Search%20for%0A%20%20Multimodal%20Fusion%20and%20Learning%0AAuthor%3A%20Abdelmadjid%20Chergui%20and%20Grigor%20Bezirganyan%20and%20Sana%20Sellami%20and%20Laure%20Berti-%C3%89quille%20and%20S%C3%A9bastien%20Fournier%0AAbstract%3A%20%20%20Choosing%20a%20suitable%20deep%20learning%20architecture%20for%20multimodal%20data%20fusion%20is%0Aa%20challenging%20task%2C%20as%20it%20requires%20the%20effective%20integration%20and%20processing%20of%0Adiverse%20data%20types%2C%20each%20with%20distinct%20structures%20and%20characteristics.%20In%20this%0Apaper%2C%20we%20introduce%20MixMAS%2C%20a%20novel%20framework%20for%20sampling-based%20mixer%0Aarchitecture%20search%20tailored%20to%20multimodal%20learning.%20Our%20approach%20automatically%0Aselects%20the%20optimal%20MLP-based%20architecture%20for%20a%20given%20multimodal%20machine%0Alearning%20%28MML%29%20task.%20Specifically%2C%20MixMAS%20utilizes%20a%20sampling-based%0Amicro-benchmarking%20strategy%20to%20explore%20various%20combinations%20of%0Amodality-specific%20encoders%2C%20fusion%20functions%2C%20and%20fusion%20networks%2C%0Asystematically%20identifying%20the%20architecture%20that%20best%20meets%20the%20task%27s%0Aperformance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixMAS%253A%2520A%2520Framework%2520for%2520Sampling-Based%2520Mixer%2520Architecture%2520Search%2520for%250A%2520%2520Multimodal%2520Fusion%2520and%2520Learning%26entry.906535625%3DAbdelmadjid%2520Chergui%2520and%2520Grigor%2520Bezirganyan%2520and%2520Sana%2520Sellami%2520and%2520Laure%2520Berti-%25C3%2589quille%2520and%2520S%25C3%25A9bastien%2520Fournier%26entry.1292438233%3D%2520%2520Choosing%2520a%2520suitable%2520deep%2520learning%2520architecture%2520for%2520multimodal%2520data%2520fusion%2520is%250Aa%2520challenging%2520task%252C%2520as%2520it%2520requires%2520the%2520effective%2520integration%2520and%2520processing%2520of%250Adiverse%2520data%2520types%252C%2520each%2520with%2520distinct%2520structures%2520and%2520characteristics.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520MixMAS%252C%2520a%2520novel%2520framework%2520for%2520sampling-based%2520mixer%250Aarchitecture%2520search%2520tailored%2520to%2520multimodal%2520learning.%2520Our%2520approach%2520automatically%250Aselects%2520the%2520optimal%2520MLP-based%2520architecture%2520for%2520a%2520given%2520multimodal%2520machine%250Alearning%2520%2528MML%2529%2520task.%2520Specifically%252C%2520MixMAS%2520utilizes%2520a%2520sampling-based%250Amicro-benchmarking%2520strategy%2520to%2520explore%2520various%2520combinations%2520of%250Amodality-specific%2520encoders%252C%2520fusion%2520functions%252C%2520and%2520fusion%2520networks%252C%250Asystematically%2520identifying%2520the%2520architecture%2520that%2520best%2520meets%2520the%2520task%2527s%250Aperformance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MixMAS%3A%20A%20Framework%20for%20Sampling-Based%20Mixer%20Architecture%20Search%20for%0A%20%20Multimodal%20Fusion%20and%20Learning&entry.906535625=Abdelmadjid%20Chergui%20and%20Grigor%20Bezirganyan%20and%20Sana%20Sellami%20and%20Laure%20Berti-%C3%89quille%20and%20S%C3%A9bastien%20Fournier&entry.1292438233=%20%20Choosing%20a%20suitable%20deep%20learning%20architecture%20for%20multimodal%20data%20fusion%20is%0Aa%20challenging%20task%2C%20as%20it%20requires%20the%20effective%20integration%20and%20processing%20of%0Adiverse%20data%20types%2C%20each%20with%20distinct%20structures%20and%20characteristics.%20In%20this%0Apaper%2C%20we%20introduce%20MixMAS%2C%20a%20novel%20framework%20for%20sampling-based%20mixer%0Aarchitecture%20search%20tailored%20to%20multimodal%20learning.%20Our%20approach%20automatically%0Aselects%20the%20optimal%20MLP-based%20architecture%20for%20a%20given%20multimodal%20machine%0Alearning%20%28MML%29%20task.%20Specifically%2C%20MixMAS%20utilizes%20a%20sampling-based%0Amicro-benchmarking%20strategy%20to%20explore%20various%20combinations%20of%0Amodality-specific%20encoders%2C%20fusion%20functions%2C%20and%20fusion%20networks%2C%0Asystematically%20identifying%20the%20architecture%20that%20best%20meets%20the%20task%27s%0Aperformance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18437v1&entry.124074799=Read"},
{"title": "Subsampling, aligning, and averaging to find circular coordinates in\n  recurrent time series", "author": "Andrew J. Blumberg and Mathieu Carri\u00e8re and Jun Hou Fung and Michael A. Mandell", "abstract": "  We introduce a new algorithm for finding robust circular coordinates on data\nthat is expected to exhibit recurrence, such as that which appears in neuronal\nrecordings of C. elegans. Techniques exist to create circular coordinates on a\nsimplicial complex from a dimension 1 cohomology class, and these can be\napplied to the Rips complex of a dataset when it has a prominent class in its\ndimension 1 cohomology. However, it is known this approach is extremely\nsensitive to uneven sampling density.\n  Our algorithm comes with a new method to correct for uneven sampling density,\nadapting our prior work on averaging coordinates in manifold learning. We use\nrejection sampling to correct for inhomogeneous sampling and then apply\nProcrustes matching to align and average the subsamples. In addition to\nproviding a more robust coordinate than other approaches, this subsampling and\naveraging approach has better efficiency.\n  We validate our technique on both synthetic data sets and neuronal activity\nrecordings. Our results reveal a topological model of neuronal trajectories for\nC. elegans that is constructed from loops in which different regions of the\nbrain state space can be mapped to specific and interpretable macroscopic\nbehaviors in the worm.\n", "link": "http://arxiv.org/abs/2412.18515v1", "date": "2024-12-24", "relevancy": 1.9018, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4904}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subsampling%2C%20aligning%2C%20and%20averaging%20to%20find%20circular%20coordinates%20in%0A%20%20recurrent%20time%20series&body=Title%3A%20Subsampling%2C%20aligning%2C%20and%20averaging%20to%20find%20circular%20coordinates%20in%0A%20%20recurrent%20time%20series%0AAuthor%3A%20Andrew%20J.%20Blumberg%20and%20Mathieu%20Carri%C3%A8re%20and%20Jun%20Hou%20Fung%20and%20Michael%20A.%20Mandell%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20algorithm%20for%20finding%20robust%20circular%20coordinates%20on%20data%0Athat%20is%20expected%20to%20exhibit%20recurrence%2C%20such%20as%20that%20which%20appears%20in%20neuronal%0Arecordings%20of%20C.%20elegans.%20Techniques%20exist%20to%20create%20circular%20coordinates%20on%20a%0Asimplicial%20complex%20from%20a%20dimension%201%20cohomology%20class%2C%20and%20these%20can%20be%0Aapplied%20to%20the%20Rips%20complex%20of%20a%20dataset%20when%20it%20has%20a%20prominent%20class%20in%20its%0Adimension%201%20cohomology.%20However%2C%20it%20is%20known%20this%20approach%20is%20extremely%0Asensitive%20to%20uneven%20sampling%20density.%0A%20%20Our%20algorithm%20comes%20with%20a%20new%20method%20to%20correct%20for%20uneven%20sampling%20density%2C%0Aadapting%20our%20prior%20work%20on%20averaging%20coordinates%20in%20manifold%20learning.%20We%20use%0Arejection%20sampling%20to%20correct%20for%20inhomogeneous%20sampling%20and%20then%20apply%0AProcrustes%20matching%20to%20align%20and%20average%20the%20subsamples.%20In%20addition%20to%0Aproviding%20a%20more%20robust%20coordinate%20than%20other%20approaches%2C%20this%20subsampling%20and%0Aaveraging%20approach%20has%20better%20efficiency.%0A%20%20We%20validate%20our%20technique%20on%20both%20synthetic%20data%20sets%20and%20neuronal%20activity%0Arecordings.%20Our%20results%20reveal%20a%20topological%20model%20of%20neuronal%20trajectories%20for%0AC.%20elegans%20that%20is%20constructed%20from%20loops%20in%20which%20different%20regions%20of%20the%0Abrain%20state%20space%20can%20be%20mapped%20to%20specific%20and%20interpretable%20macroscopic%0Abehaviors%20in%20the%20worm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubsampling%252C%2520aligning%252C%2520and%2520averaging%2520to%2520find%2520circular%2520coordinates%2520in%250A%2520%2520recurrent%2520time%2520series%26entry.906535625%3DAndrew%2520J.%2520Blumberg%2520and%2520Mathieu%2520Carri%25C3%25A8re%2520and%2520Jun%2520Hou%2520Fung%2520and%2520Michael%2520A.%2520Mandell%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520algorithm%2520for%2520finding%2520robust%2520circular%2520coordinates%2520on%2520data%250Athat%2520is%2520expected%2520to%2520exhibit%2520recurrence%252C%2520such%2520as%2520that%2520which%2520appears%2520in%2520neuronal%250Arecordings%2520of%2520C.%2520elegans.%2520Techniques%2520exist%2520to%2520create%2520circular%2520coordinates%2520on%2520a%250Asimplicial%2520complex%2520from%2520a%2520dimension%25201%2520cohomology%2520class%252C%2520and%2520these%2520can%2520be%250Aapplied%2520to%2520the%2520Rips%2520complex%2520of%2520a%2520dataset%2520when%2520it%2520has%2520a%2520prominent%2520class%2520in%2520its%250Adimension%25201%2520cohomology.%2520However%252C%2520it%2520is%2520known%2520this%2520approach%2520is%2520extremely%250Asensitive%2520to%2520uneven%2520sampling%2520density.%250A%2520%2520Our%2520algorithm%2520comes%2520with%2520a%2520new%2520method%2520to%2520correct%2520for%2520uneven%2520sampling%2520density%252C%250Aadapting%2520our%2520prior%2520work%2520on%2520averaging%2520coordinates%2520in%2520manifold%2520learning.%2520We%2520use%250Arejection%2520sampling%2520to%2520correct%2520for%2520inhomogeneous%2520sampling%2520and%2520then%2520apply%250AProcrustes%2520matching%2520to%2520align%2520and%2520average%2520the%2520subsamples.%2520In%2520addition%2520to%250Aproviding%2520a%2520more%2520robust%2520coordinate%2520than%2520other%2520approaches%252C%2520this%2520subsampling%2520and%250Aaveraging%2520approach%2520has%2520better%2520efficiency.%250A%2520%2520We%2520validate%2520our%2520technique%2520on%2520both%2520synthetic%2520data%2520sets%2520and%2520neuronal%2520activity%250Arecordings.%2520Our%2520results%2520reveal%2520a%2520topological%2520model%2520of%2520neuronal%2520trajectories%2520for%250AC.%2520elegans%2520that%2520is%2520constructed%2520from%2520loops%2520in%2520which%2520different%2520regions%2520of%2520the%250Abrain%2520state%2520space%2520can%2520be%2520mapped%2520to%2520specific%2520and%2520interpretable%2520macroscopic%250Abehaviors%2520in%2520the%2520worm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subsampling%2C%20aligning%2C%20and%20averaging%20to%20find%20circular%20coordinates%20in%0A%20%20recurrent%20time%20series&entry.906535625=Andrew%20J.%20Blumberg%20and%20Mathieu%20Carri%C3%A8re%20and%20Jun%20Hou%20Fung%20and%20Michael%20A.%20Mandell&entry.1292438233=%20%20We%20introduce%20a%20new%20algorithm%20for%20finding%20robust%20circular%20coordinates%20on%20data%0Athat%20is%20expected%20to%20exhibit%20recurrence%2C%20such%20as%20that%20which%20appears%20in%20neuronal%0Arecordings%20of%20C.%20elegans.%20Techniques%20exist%20to%20create%20circular%20coordinates%20on%20a%0Asimplicial%20complex%20from%20a%20dimension%201%20cohomology%20class%2C%20and%20these%20can%20be%0Aapplied%20to%20the%20Rips%20complex%20of%20a%20dataset%20when%20it%20has%20a%20prominent%20class%20in%20its%0Adimension%201%20cohomology.%20However%2C%20it%20is%20known%20this%20approach%20is%20extremely%0Asensitive%20to%20uneven%20sampling%20density.%0A%20%20Our%20algorithm%20comes%20with%20a%20new%20method%20to%20correct%20for%20uneven%20sampling%20density%2C%0Aadapting%20our%20prior%20work%20on%20averaging%20coordinates%20in%20manifold%20learning.%20We%20use%0Arejection%20sampling%20to%20correct%20for%20inhomogeneous%20sampling%20and%20then%20apply%0AProcrustes%20matching%20to%20align%20and%20average%20the%20subsamples.%20In%20addition%20to%0Aproviding%20a%20more%20robust%20coordinate%20than%20other%20approaches%2C%20this%20subsampling%20and%0Aaveraging%20approach%20has%20better%20efficiency.%0A%20%20We%20validate%20our%20technique%20on%20both%20synthetic%20data%20sets%20and%20neuronal%20activity%0Arecordings.%20Our%20results%20reveal%20a%20topological%20model%20of%20neuronal%20trajectories%20for%0AC.%20elegans%20that%20is%20constructed%20from%20loops%20in%20which%20different%20regions%20of%20the%0Abrain%20state%20space%20can%20be%20mapped%20to%20specific%20and%20interpretable%20macroscopic%0Abehaviors%20in%20the%20worm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18515v1&entry.124074799=Read"},
{"title": "Explaining in Diffusion: Explaining a Classifier Through Hierarchical\n  Semantics with Text-to-Image Diffusion Models", "author": "Tahira Kazimi and Ritika Allada and Pinar Yanardag", "abstract": "  Classifiers are important components in many computer vision tasks, serving\nas the foundational backbone of a wide variety of models employed across\ndiverse applications. However, understanding the decision-making process of\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\nthat leverages the capabilities of text-to-image diffusion models to explain\nclassifier decisions. Unlike traditional GAN-based explainability models, which\nare limited to simple, single-concept analyses and typically require training a\nnew model for each classifier, our approach can explain classifiers that focus\non single concepts (such as faces or animals) as well as those that handle\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\nmodels to create a hierarchical list of semantics, allowing users to identify\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\nproviding a hierarchical tool that delivers a more detailed and fine-grained\nunderstanding of classifier decisions.\n", "link": "http://arxiv.org/abs/2412.18604v1", "date": "2024-12-24", "relevancy": 1.6496, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5718}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5558}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20in%20Diffusion%3A%20Explaining%20a%20Classifier%20Through%20Hierarchical%0A%20%20Semantics%20with%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Explaining%20in%20Diffusion%3A%20Explaining%20a%20Classifier%20Through%20Hierarchical%0A%20%20Semantics%20with%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Tahira%20Kazimi%20and%20Ritika%20Allada%20and%20Pinar%20Yanardag%0AAbstract%3A%20%20%20Classifiers%20are%20important%20components%20in%20many%20computer%20vision%20tasks%2C%20serving%0Aas%20the%20foundational%20backbone%20of%20a%20wide%20variety%20of%20models%20employed%20across%0Adiverse%20applications.%20However%2C%20understanding%20the%20decision-making%20process%20of%0Aclassifiers%20remains%20a%20significant%20challenge.%20We%20propose%20DiffEx%2C%20a%20novel%20method%0Athat%20leverages%20the%20capabilities%20of%20text-to-image%20diffusion%20models%20to%20explain%0Aclassifier%20decisions.%20Unlike%20traditional%20GAN-based%20explainability%20models%2C%20which%0Aare%20limited%20to%20simple%2C%20single-concept%20analyses%20and%20typically%20require%20training%20a%0Anew%20model%20for%20each%20classifier%2C%20our%20approach%20can%20explain%20classifiers%20that%20focus%0Aon%20single%20concepts%20%28such%20as%20faces%20or%20animals%29%20as%20well%20as%20those%20that%20handle%0Acomplex%20scenes%20involving%20multiple%20concepts.%20DiffEx%20employs%20vision-language%0Amodels%20to%20create%20a%20hierarchical%20list%20of%20semantics%2C%20allowing%20users%20to%20identify%0Anot%20only%20the%20overarching%20semantic%20influences%20on%20classifiers%20%28e.g.%2C%20the%20%27beard%27%0Asemantic%20in%20a%20facial%20classifier%29%20but%20also%20their%20sub-types%2C%20such%20as%20%27goatee%27%20or%0A%27Balbo%27%20beard.%20Our%20experiments%20demonstrate%20that%20DiffEx%20is%20able%20to%20cover%20a%0Asignificantly%20broader%20spectrum%20of%20semantics%20compared%20to%20its%20GAN%20counterparts%2C%0Aproviding%20a%20hierarchical%20tool%20that%20delivers%20a%20more%20detailed%20and%20fine-grained%0Aunderstanding%20of%20classifier%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520in%2520Diffusion%253A%2520Explaining%2520a%2520Classifier%2520Through%2520Hierarchical%250A%2520%2520Semantics%2520with%2520Text-to-Image%2520Diffusion%2520Models%26entry.906535625%3DTahira%2520Kazimi%2520and%2520Ritika%2520Allada%2520and%2520Pinar%2520Yanardag%26entry.1292438233%3D%2520%2520Classifiers%2520are%2520important%2520components%2520in%2520many%2520computer%2520vision%2520tasks%252C%2520serving%250Aas%2520the%2520foundational%2520backbone%2520of%2520a%2520wide%2520variety%2520of%2520models%2520employed%2520across%250Adiverse%2520applications.%2520However%252C%2520understanding%2520the%2520decision-making%2520process%2520of%250Aclassifiers%2520remains%2520a%2520significant%2520challenge.%2520We%2520propose%2520DiffEx%252C%2520a%2520novel%2520method%250Athat%2520leverages%2520the%2520capabilities%2520of%2520text-to-image%2520diffusion%2520models%2520to%2520explain%250Aclassifier%2520decisions.%2520Unlike%2520traditional%2520GAN-based%2520explainability%2520models%252C%2520which%250Aare%2520limited%2520to%2520simple%252C%2520single-concept%2520analyses%2520and%2520typically%2520require%2520training%2520a%250Anew%2520model%2520for%2520each%2520classifier%252C%2520our%2520approach%2520can%2520explain%2520classifiers%2520that%2520focus%250Aon%2520single%2520concepts%2520%2528such%2520as%2520faces%2520or%2520animals%2529%2520as%2520well%2520as%2520those%2520that%2520handle%250Acomplex%2520scenes%2520involving%2520multiple%2520concepts.%2520DiffEx%2520employs%2520vision-language%250Amodels%2520to%2520create%2520a%2520hierarchical%2520list%2520of%2520semantics%252C%2520allowing%2520users%2520to%2520identify%250Anot%2520only%2520the%2520overarching%2520semantic%2520influences%2520on%2520classifiers%2520%2528e.g.%252C%2520the%2520%2527beard%2527%250Asemantic%2520in%2520a%2520facial%2520classifier%2529%2520but%2520also%2520their%2520sub-types%252C%2520such%2520as%2520%2527goatee%2527%2520or%250A%2527Balbo%2527%2520beard.%2520Our%2520experiments%2520demonstrate%2520that%2520DiffEx%2520is%2520able%2520to%2520cover%2520a%250Asignificantly%2520broader%2520spectrum%2520of%2520semantics%2520compared%2520to%2520its%2520GAN%2520counterparts%252C%250Aproviding%2520a%2520hierarchical%2520tool%2520that%2520delivers%2520a%2520more%2520detailed%2520and%2520fine-grained%250Aunderstanding%2520of%2520classifier%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20in%20Diffusion%3A%20Explaining%20a%20Classifier%20Through%20Hierarchical%0A%20%20Semantics%20with%20Text-to-Image%20Diffusion%20Models&entry.906535625=Tahira%20Kazimi%20and%20Ritika%20Allada%20and%20Pinar%20Yanardag&entry.1292438233=%20%20Classifiers%20are%20important%20components%20in%20many%20computer%20vision%20tasks%2C%20serving%0Aas%20the%20foundational%20backbone%20of%20a%20wide%20variety%20of%20models%20employed%20across%0Adiverse%20applications.%20However%2C%20understanding%20the%20decision-making%20process%20of%0Aclassifiers%20remains%20a%20significant%20challenge.%20We%20propose%20DiffEx%2C%20a%20novel%20method%0Athat%20leverages%20the%20capabilities%20of%20text-to-image%20diffusion%20models%20to%20explain%0Aclassifier%20decisions.%20Unlike%20traditional%20GAN-based%20explainability%20models%2C%20which%0Aare%20limited%20to%20simple%2C%20single-concept%20analyses%20and%20typically%20require%20training%20a%0Anew%20model%20for%20each%20classifier%2C%20our%20approach%20can%20explain%20classifiers%20that%20focus%0Aon%20single%20concepts%20%28such%20as%20faces%20or%20animals%29%20as%20well%20as%20those%20that%20handle%0Acomplex%20scenes%20involving%20multiple%20concepts.%20DiffEx%20employs%20vision-language%0Amodels%20to%20create%20a%20hierarchical%20list%20of%20semantics%2C%20allowing%20users%20to%20identify%0Anot%20only%20the%20overarching%20semantic%20influences%20on%20classifiers%20%28e.g.%2C%20the%20%27beard%27%0Asemantic%20in%20a%20facial%20classifier%29%20but%20also%20their%20sub-types%2C%20such%20as%20%27goatee%27%20or%0A%27Balbo%27%20beard.%20Our%20experiments%20demonstrate%20that%20DiffEx%20is%20able%20to%20cover%20a%0Asignificantly%20broader%20spectrum%20of%20semantics%20compared%20to%20its%20GAN%20counterparts%2C%0Aproviding%20a%20hierarchical%20tool%20that%20delivers%20a%20more%20detailed%20and%20fine-grained%0Aunderstanding%20of%20classifier%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18604v1&entry.124074799=Read"},
{"title": "Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in\n  Open-Set Domain Generalization", "author": "Kunyu Peng and Di Wen and Sarfraz M. Saquib and Yufan Chen and Junwei Zheng and David Schneider and Kailun Yang and Jiamin Wu and Alina Roitberg and Rainer Stiefelhagen", "abstract": "  Open-Set Domain Generalization (OSDG) is a challenging task requiring models\nto accurately predict familiar categories while minimizing confidence for\nunknown categories to effectively reject them in unseen domains. While the OSDG\nfield has seen considerable advancements, the impact of label noise--a common\nissue in real-world datasets--has been largely overlooked. Label noise can\nmislead model optimization, thereby exacerbating the challenges of open-set\nrecognition in novel domains. In this study, we take the first step towards\naddressing Open-Set Domain Generalization under Noisy Labels (OSDG-NL) by\nconstructing dedicated benchmarks derived from widely used OSDG datasets,\nincluding PACS and DigitsDG. We evaluate baseline approaches by integrating\ntechniques from both label denoising and OSDG methodologies, highlighting the\nlimitations of existing strategies in handling label noise effectively. To\naddress these limitations, we propose HyProMeta, a novel framework that\nintegrates hyperbolic category prototypes for label noise-aware meta-learning\nalongside a learnable new-category agnostic prompt designed to enhance\ngeneralization to unseen classes. Our extensive experiments demonstrate the\nsuperior performance of HyProMeta compared to state-of-the-art methods across\nthe newly established benchmarks. The source code of this work is released at\nhttps://github.com/KPeng9510/HyProMeta.\n", "link": "http://arxiv.org/abs/2412.18342v1", "date": "2024-12-24", "relevancy": 1.6049, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5509}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Label%20Noise%20using%20Prompt-Based%20Hyperbolic%20Meta-Learning%20in%0A%20%20Open-Set%20Domain%20Generalization&body=Title%3A%20Mitigating%20Label%20Noise%20using%20Prompt-Based%20Hyperbolic%20Meta-Learning%20in%0A%20%20Open-Set%20Domain%20Generalization%0AAuthor%3A%20Kunyu%20Peng%20and%20Di%20Wen%20and%20Sarfraz%20M.%20Saquib%20and%20Yufan%20Chen%20and%20Junwei%20Zheng%20and%20David%20Schneider%20and%20Kailun%20Yang%20and%20Jiamin%20Wu%20and%20Alina%20Roitberg%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Open-Set%20Domain%20Generalization%20%28OSDG%29%20is%20a%20challenging%20task%20requiring%20models%0Ato%20accurately%20predict%20familiar%20categories%20while%20minimizing%20confidence%20for%0Aunknown%20categories%20to%20effectively%20reject%20them%20in%20unseen%20domains.%20While%20the%20OSDG%0Afield%20has%20seen%20considerable%20advancements%2C%20the%20impact%20of%20label%20noise--a%20common%0Aissue%20in%20real-world%20datasets--has%20been%20largely%20overlooked.%20Label%20noise%20can%0Amislead%20model%20optimization%2C%20thereby%20exacerbating%20the%20challenges%20of%20open-set%0Arecognition%20in%20novel%20domains.%20In%20this%20study%2C%20we%20take%20the%20first%20step%20towards%0Aaddressing%20Open-Set%20Domain%20Generalization%20under%20Noisy%20Labels%20%28OSDG-NL%29%20by%0Aconstructing%20dedicated%20benchmarks%20derived%20from%20widely%20used%20OSDG%20datasets%2C%0Aincluding%20PACS%20and%20DigitsDG.%20We%20evaluate%20baseline%20approaches%20by%20integrating%0Atechniques%20from%20both%20label%20denoising%20and%20OSDG%20methodologies%2C%20highlighting%20the%0Alimitations%20of%20existing%20strategies%20in%20handling%20label%20noise%20effectively.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20HyProMeta%2C%20a%20novel%20framework%20that%0Aintegrates%20hyperbolic%20category%20prototypes%20for%20label%20noise-aware%20meta-learning%0Aalongside%20a%20learnable%20new-category%20agnostic%20prompt%20designed%20to%20enhance%0Ageneralization%20to%20unseen%20classes.%20Our%20extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20HyProMeta%20compared%20to%20state-of-the-art%20methods%20across%0Athe%20newly%20established%20benchmarks.%20The%20source%20code%20of%20this%20work%20is%20released%20at%0Ahttps%3A//github.com/KPeng9510/HyProMeta.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Label%2520Noise%2520using%2520Prompt-Based%2520Hyperbolic%2520Meta-Learning%2520in%250A%2520%2520Open-Set%2520Domain%2520Generalization%26entry.906535625%3DKunyu%2520Peng%2520and%2520Di%2520Wen%2520and%2520Sarfraz%2520M.%2520Saquib%2520and%2520Yufan%2520Chen%2520and%2520Junwei%2520Zheng%2520and%2520David%2520Schneider%2520and%2520Kailun%2520Yang%2520and%2520Jiamin%2520Wu%2520and%2520Alina%2520Roitberg%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Open-Set%2520Domain%2520Generalization%2520%2528OSDG%2529%2520is%2520a%2520challenging%2520task%2520requiring%2520models%250Ato%2520accurately%2520predict%2520familiar%2520categories%2520while%2520minimizing%2520confidence%2520for%250Aunknown%2520categories%2520to%2520effectively%2520reject%2520them%2520in%2520unseen%2520domains.%2520While%2520the%2520OSDG%250Afield%2520has%2520seen%2520considerable%2520advancements%252C%2520the%2520impact%2520of%2520label%2520noise--a%2520common%250Aissue%2520in%2520real-world%2520datasets--has%2520been%2520largely%2520overlooked.%2520Label%2520noise%2520can%250Amislead%2520model%2520optimization%252C%2520thereby%2520exacerbating%2520the%2520challenges%2520of%2520open-set%250Arecognition%2520in%2520novel%2520domains.%2520In%2520this%2520study%252C%2520we%2520take%2520the%2520first%2520step%2520towards%250Aaddressing%2520Open-Set%2520Domain%2520Generalization%2520under%2520Noisy%2520Labels%2520%2528OSDG-NL%2529%2520by%250Aconstructing%2520dedicated%2520benchmarks%2520derived%2520from%2520widely%2520used%2520OSDG%2520datasets%252C%250Aincluding%2520PACS%2520and%2520DigitsDG.%2520We%2520evaluate%2520baseline%2520approaches%2520by%2520integrating%250Atechniques%2520from%2520both%2520label%2520denoising%2520and%2520OSDG%2520methodologies%252C%2520highlighting%2520the%250Alimitations%2520of%2520existing%2520strategies%2520in%2520handling%2520label%2520noise%2520effectively.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520HyProMeta%252C%2520a%2520novel%2520framework%2520that%250Aintegrates%2520hyperbolic%2520category%2520prototypes%2520for%2520label%2520noise-aware%2520meta-learning%250Aalongside%2520a%2520learnable%2520new-category%2520agnostic%2520prompt%2520designed%2520to%2520enhance%250Ageneralization%2520to%2520unseen%2520classes.%2520Our%2520extensive%2520experiments%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520HyProMeta%2520compared%2520to%2520state-of-the-art%2520methods%2520across%250Athe%2520newly%2520established%2520benchmarks.%2520The%2520source%2520code%2520of%2520this%2520work%2520is%2520released%2520at%250Ahttps%253A//github.com/KPeng9510/HyProMeta.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Label%20Noise%20using%20Prompt-Based%20Hyperbolic%20Meta-Learning%20in%0A%20%20Open-Set%20Domain%20Generalization&entry.906535625=Kunyu%20Peng%20and%20Di%20Wen%20and%20Sarfraz%20M.%20Saquib%20and%20Yufan%20Chen%20and%20Junwei%20Zheng%20and%20David%20Schneider%20and%20Kailun%20Yang%20and%20Jiamin%20Wu%20and%20Alina%20Roitberg%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Open-Set%20Domain%20Generalization%20%28OSDG%29%20is%20a%20challenging%20task%20requiring%20models%0Ato%20accurately%20predict%20familiar%20categories%20while%20minimizing%20confidence%20for%0Aunknown%20categories%20to%20effectively%20reject%20them%20in%20unseen%20domains.%20While%20the%20OSDG%0Afield%20has%20seen%20considerable%20advancements%2C%20the%20impact%20of%20label%20noise--a%20common%0Aissue%20in%20real-world%20datasets--has%20been%20largely%20overlooked.%20Label%20noise%20can%0Amislead%20model%20optimization%2C%20thereby%20exacerbating%20the%20challenges%20of%20open-set%0Arecognition%20in%20novel%20domains.%20In%20this%20study%2C%20we%20take%20the%20first%20step%20towards%0Aaddressing%20Open-Set%20Domain%20Generalization%20under%20Noisy%20Labels%20%28OSDG-NL%29%20by%0Aconstructing%20dedicated%20benchmarks%20derived%20from%20widely%20used%20OSDG%20datasets%2C%0Aincluding%20PACS%20and%20DigitsDG.%20We%20evaluate%20baseline%20approaches%20by%20integrating%0Atechniques%20from%20both%20label%20denoising%20and%20OSDG%20methodologies%2C%20highlighting%20the%0Alimitations%20of%20existing%20strategies%20in%20handling%20label%20noise%20effectively.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20HyProMeta%2C%20a%20novel%20framework%20that%0Aintegrates%20hyperbolic%20category%20prototypes%20for%20label%20noise-aware%20meta-learning%0Aalongside%20a%20learnable%20new-category%20agnostic%20prompt%20designed%20to%20enhance%0Ageneralization%20to%20unseen%20classes.%20Our%20extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20HyProMeta%20compared%20to%20state-of-the-art%20methods%20across%0Athe%20newly%20established%20benchmarks.%20The%20source%20code%20of%20this%20work%20is%20released%20at%0Ahttps%3A//github.com/KPeng9510/HyProMeta.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18342v1&entry.124074799=Read"},
{"title": "SoK: On the Offensive Potential of AI", "author": "Saskia Laura Schr\u00f6er and Giovanni Apruzzese and Soheil Human and Pavel Laskov and Hyrum S. Anderson and Edward W. N. Bernroider and Aurore Fass and Ben Nassi and Vera Rimmer and Fabio Roli and Samer Salam and Ashley Shen and Ali Sunyaev and Tim Wadwha-Brown and Isabel Wagner and Gang Wang", "abstract": "  Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laymen -- all of which being valuable sources of information\non offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome.\n", "link": "http://arxiv.org/abs/2412.18442v1", "date": "2024-12-24", "relevancy": 0.7983, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4264}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.396}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20On%20the%20Offensive%20Potential%20of%20AI&body=Title%3A%20SoK%3A%20On%20the%20Offensive%20Potential%20of%20AI%0AAuthor%3A%20Saskia%20Laura%20Schr%C3%B6er%20and%20Giovanni%20Apruzzese%20and%20Soheil%20Human%20and%20Pavel%20Laskov%20and%20Hyrum%20S.%20Anderson%20and%20Edward%20W.%20N.%20Bernroider%20and%20Aurore%20Fass%20and%20Ben%20Nassi%20and%20Vera%20Rimmer%20and%20Fabio%20Roli%20and%20Samer%20Salam%20and%20Ashley%20Shen%20and%20Ali%20Sunyaev%20and%20Tim%20Wadwha-Brown%20and%20Isabel%20Wagner%20and%20Gang%20Wang%0AAbstract%3A%20%20%20Our%20society%20increasingly%20benefits%20from%20Artificial%20Intelligence%20%28AI%29.%0AUnfortunately%2C%20more%20and%20more%20evidence%20shows%20that%20AI%20is%20also%20used%20for%20offensive%0Apurposes.%20Prior%20works%20have%20revealed%20various%20examples%20of%20use%20cases%20in%20which%20the%0Adeployment%20of%20AI%20can%20lead%20to%20violation%20of%20security%20and%20privacy%20objectives.%20No%0Aextant%20work%2C%20however%2C%20has%20been%20able%20to%20draw%20a%20holistic%20picture%20of%20the%20offensive%0Apotential%20of%20AI.%20In%20this%20SoK%20paper%20we%20seek%20to%20lay%20the%20ground%20for%20a%20systematic%0Aanalysis%20of%20the%20heterogeneous%20capabilities%20of%20offensive%20AI.%20In%20particular%20we%0A%28i%29%20account%20for%20AI%20risks%20to%20both%20humans%20and%20systems%20while%20%28ii%29%20consolidating%0Aand%20distilling%20knowledge%20from%20academic%20literature%2C%20expert%20opinions%2C%20industrial%0Avenues%2C%20as%20well%20as%20laymen%20--%20all%20of%20which%20being%20valuable%20sources%20of%20information%0Aon%20offensive%20AI.%0A%20%20To%20enable%20alignment%20of%20such%20diverse%20sources%20of%20knowledge%2C%20we%20devise%20a%20common%0Aset%20of%20criteria%20reflecting%20essential%20technological%20factors%20related%20to%20offensive%0AAI.%20With%20the%20help%20of%20such%20criteria%2C%20we%20systematically%20analyze%3A%2095%20research%0Apapers%3B%2038%20InfoSec%20briefings%20%28from%2C%20e.g.%2C%20BlackHat%29%3B%20the%20responses%20of%20a%20user%0Astudy%20%28N%3D549%29%20entailing%20individuals%20with%20diverse%20backgrounds%20and%20expertise%3B%20and%0Athe%20opinion%20of%2012%20experts.%20Our%20contributions%20not%20only%20reveal%20concerning%20ways%0A%28some%20of%20which%20overlooked%20by%20prior%20work%29%20in%20which%20AI%20can%20be%20offensively%20used%0Atoday%2C%20but%20also%20represent%20a%20foothold%20to%20address%20this%20threat%20in%20the%20years%20to%0Acome.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520On%2520the%2520Offensive%2520Potential%2520of%2520AI%26entry.906535625%3DSaskia%2520Laura%2520Schr%25C3%25B6er%2520and%2520Giovanni%2520Apruzzese%2520and%2520Soheil%2520Human%2520and%2520Pavel%2520Laskov%2520and%2520Hyrum%2520S.%2520Anderson%2520and%2520Edward%2520W.%2520N.%2520Bernroider%2520and%2520Aurore%2520Fass%2520and%2520Ben%2520Nassi%2520and%2520Vera%2520Rimmer%2520and%2520Fabio%2520Roli%2520and%2520Samer%2520Salam%2520and%2520Ashley%2520Shen%2520and%2520Ali%2520Sunyaev%2520and%2520Tim%2520Wadwha-Brown%2520and%2520Isabel%2520Wagner%2520and%2520Gang%2520Wang%26entry.1292438233%3D%2520%2520Our%2520society%2520increasingly%2520benefits%2520from%2520Artificial%2520Intelligence%2520%2528AI%2529.%250AUnfortunately%252C%2520more%2520and%2520more%2520evidence%2520shows%2520that%2520AI%2520is%2520also%2520used%2520for%2520offensive%250Apurposes.%2520Prior%2520works%2520have%2520revealed%2520various%2520examples%2520of%2520use%2520cases%2520in%2520which%2520the%250Adeployment%2520of%2520AI%2520can%2520lead%2520to%2520violation%2520of%2520security%2520and%2520privacy%2520objectives.%2520No%250Aextant%2520work%252C%2520however%252C%2520has%2520been%2520able%2520to%2520draw%2520a%2520holistic%2520picture%2520of%2520the%2520offensive%250Apotential%2520of%2520AI.%2520In%2520this%2520SoK%2520paper%2520we%2520seek%2520to%2520lay%2520the%2520ground%2520for%2520a%2520systematic%250Aanalysis%2520of%2520the%2520heterogeneous%2520capabilities%2520of%2520offensive%2520AI.%2520In%2520particular%2520we%250A%2528i%2529%2520account%2520for%2520AI%2520risks%2520to%2520both%2520humans%2520and%2520systems%2520while%2520%2528ii%2529%2520consolidating%250Aand%2520distilling%2520knowledge%2520from%2520academic%2520literature%252C%2520expert%2520opinions%252C%2520industrial%250Avenues%252C%2520as%2520well%2520as%2520laymen%2520--%2520all%2520of%2520which%2520being%2520valuable%2520sources%2520of%2520information%250Aon%2520offensive%2520AI.%250A%2520%2520To%2520enable%2520alignment%2520of%2520such%2520diverse%2520sources%2520of%2520knowledge%252C%2520we%2520devise%2520a%2520common%250Aset%2520of%2520criteria%2520reflecting%2520essential%2520technological%2520factors%2520related%2520to%2520offensive%250AAI.%2520With%2520the%2520help%2520of%2520such%2520criteria%252C%2520we%2520systematically%2520analyze%253A%252095%2520research%250Apapers%253B%252038%2520InfoSec%2520briefings%2520%2528from%252C%2520e.g.%252C%2520BlackHat%2529%253B%2520the%2520responses%2520of%2520a%2520user%250Astudy%2520%2528N%253D549%2529%2520entailing%2520individuals%2520with%2520diverse%2520backgrounds%2520and%2520expertise%253B%2520and%250Athe%2520opinion%2520of%252012%2520experts.%2520Our%2520contributions%2520not%2520only%2520reveal%2520concerning%2520ways%250A%2528some%2520of%2520which%2520overlooked%2520by%2520prior%2520work%2529%2520in%2520which%2520AI%2520can%2520be%2520offensively%2520used%250Atoday%252C%2520but%2520also%2520represent%2520a%2520foothold%2520to%2520address%2520this%2520threat%2520in%2520the%2520years%2520to%250Acome.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20On%20the%20Offensive%20Potential%20of%20AI&entry.906535625=Saskia%20Laura%20Schr%C3%B6er%20and%20Giovanni%20Apruzzese%20and%20Soheil%20Human%20and%20Pavel%20Laskov%20and%20Hyrum%20S.%20Anderson%20and%20Edward%20W.%20N.%20Bernroider%20and%20Aurore%20Fass%20and%20Ben%20Nassi%20and%20Vera%20Rimmer%20and%20Fabio%20Roli%20and%20Samer%20Salam%20and%20Ashley%20Shen%20and%20Ali%20Sunyaev%20and%20Tim%20Wadwha-Brown%20and%20Isabel%20Wagner%20and%20Gang%20Wang&entry.1292438233=%20%20Our%20society%20increasingly%20benefits%20from%20Artificial%20Intelligence%20%28AI%29.%0AUnfortunately%2C%20more%20and%20more%20evidence%20shows%20that%20AI%20is%20also%20used%20for%20offensive%0Apurposes.%20Prior%20works%20have%20revealed%20various%20examples%20of%20use%20cases%20in%20which%20the%0Adeployment%20of%20AI%20can%20lead%20to%20violation%20of%20security%20and%20privacy%20objectives.%20No%0Aextant%20work%2C%20however%2C%20has%20been%20able%20to%20draw%20a%20holistic%20picture%20of%20the%20offensive%0Apotential%20of%20AI.%20In%20this%20SoK%20paper%20we%20seek%20to%20lay%20the%20ground%20for%20a%20systematic%0Aanalysis%20of%20the%20heterogeneous%20capabilities%20of%20offensive%20AI.%20In%20particular%20we%0A%28i%29%20account%20for%20AI%20risks%20to%20both%20humans%20and%20systems%20while%20%28ii%29%20consolidating%0Aand%20distilling%20knowledge%20from%20academic%20literature%2C%20expert%20opinions%2C%20industrial%0Avenues%2C%20as%20well%20as%20laymen%20--%20all%20of%20which%20being%20valuable%20sources%20of%20information%0Aon%20offensive%20AI.%0A%20%20To%20enable%20alignment%20of%20such%20diverse%20sources%20of%20knowledge%2C%20we%20devise%20a%20common%0Aset%20of%20criteria%20reflecting%20essential%20technological%20factors%20related%20to%20offensive%0AAI.%20With%20the%20help%20of%20such%20criteria%2C%20we%20systematically%20analyze%3A%2095%20research%0Apapers%3B%2038%20InfoSec%20briefings%20%28from%2C%20e.g.%2C%20BlackHat%29%3B%20the%20responses%20of%20a%20user%0Astudy%20%28N%3D549%29%20entailing%20individuals%20with%20diverse%20backgrounds%20and%20expertise%3B%20and%0Athe%20opinion%20of%2012%20experts.%20Our%20contributions%20not%20only%20reveal%20concerning%20ways%0A%28some%20of%20which%20overlooked%20by%20prior%20work%29%20in%20which%20AI%20can%20be%20offensively%20used%0Atoday%2C%20but%20also%20represent%20a%20foothold%20to%20address%20this%20threat%20in%20the%20years%20to%0Acome.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18442v1&entry.124074799=Read"},
{"title": "A Divide-Align-Conquer Strategy for Program Synthesis", "author": "Jonas Witt and Sebastijan Duman\u010di\u0107 and Tias Guns and Claus-Christian Carbon", "abstract": "  A major bottleneck in search-based program synthesis is the exponentially\ngrowing search space which makes learning large programs intractable. Humans\nmitigate this problem by leveraging the compositional nature of the real world:\nIn structured domains, a logical specification can often be decomposed into\nsmaller, complementary solution programs. We show that compositional\nsegmentation can be applied in the programming by examples setting to divide\nthe search for large programs across multiple smaller program synthesis\nproblems. For each example, we search for a decomposition into smaller units\nwhich maximizes the reconstruction accuracy in the output under a latent task\nprogram. A structural alignment of the constituent parts in the input and\noutput leads to pairwise correspondences used to guide the program synthesis\nsearch. In order to align the input/output structures, we make use of the\nStructure-Mapping Theory (SMT), a formal model of human analogical reasoning\nwhich originated in the cognitive sciences. We show that decomposition-driven\nprogram synthesis with structural alignment outperforms Inductive Logic\nProgramming (ILP) baselines on string transformation tasks even with minimal\nknowledge priors. Unlike existing methods, the predictive accuracy of our agent\nmonotonically increases for additional examples and achieves an average time\ncomplexity of $\\mathcal{O}(m)$ in the number $m$ of partial programs for highly\nstructured domains such as strings. We extend this method to the complex\nsetting of visual reasoning in the Abstraction and Reasoning Corpus (ARC) for\nwhich ILP methods were previously infeasible.\n", "link": "http://arxiv.org/abs/2301.03094v2", "date": "2024-12-24", "relevancy": 1.4474, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Divide-Align-Conquer%20Strategy%20for%20Program%20Synthesis&body=Title%3A%20A%20Divide-Align-Conquer%20Strategy%20for%20Program%20Synthesis%0AAuthor%3A%20Jonas%20Witt%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Tias%20Guns%20and%20Claus-Christian%20Carbon%0AAbstract%3A%20%20%20A%20major%20bottleneck%20in%20search-based%20program%20synthesis%20is%20the%20exponentially%0Agrowing%20search%20space%20which%20makes%20learning%20large%20programs%20intractable.%20Humans%0Amitigate%20this%20problem%20by%20leveraging%20the%20compositional%20nature%20of%20the%20real%20world%3A%0AIn%20structured%20domains%2C%20a%20logical%20specification%20can%20often%20be%20decomposed%20into%0Asmaller%2C%20complementary%20solution%20programs.%20We%20show%20that%20compositional%0Asegmentation%20can%20be%20applied%20in%20the%20programming%20by%20examples%20setting%20to%20divide%0Athe%20search%20for%20large%20programs%20across%20multiple%20smaller%20program%20synthesis%0Aproblems.%20For%20each%20example%2C%20we%20search%20for%20a%20decomposition%20into%20smaller%20units%0Awhich%20maximizes%20the%20reconstruction%20accuracy%20in%20the%20output%20under%20a%20latent%20task%0Aprogram.%20A%20structural%20alignment%20of%20the%20constituent%20parts%20in%20the%20input%20and%0Aoutput%20leads%20to%20pairwise%20correspondences%20used%20to%20guide%20the%20program%20synthesis%0Asearch.%20In%20order%20to%20align%20the%20input/output%20structures%2C%20we%20make%20use%20of%20the%0AStructure-Mapping%20Theory%20%28SMT%29%2C%20a%20formal%20model%20of%20human%20analogical%20reasoning%0Awhich%20originated%20in%20the%20cognitive%20sciences.%20We%20show%20that%20decomposition-driven%0Aprogram%20synthesis%20with%20structural%20alignment%20outperforms%20Inductive%20Logic%0AProgramming%20%28ILP%29%20baselines%20on%20string%20transformation%20tasks%20even%20with%20minimal%0Aknowledge%20priors.%20Unlike%20existing%20methods%2C%20the%20predictive%20accuracy%20of%20our%20agent%0Amonotonically%20increases%20for%20additional%20examples%20and%20achieves%20an%20average%20time%0Acomplexity%20of%20%24%5Cmathcal%7BO%7D%28m%29%24%20in%20the%20number%20%24m%24%20of%20partial%20programs%20for%20highly%0Astructured%20domains%20such%20as%20strings.%20We%20extend%20this%20method%20to%20the%20complex%0Asetting%20of%20visual%20reasoning%20in%20the%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20for%0Awhich%20ILP%20methods%20were%20previously%20infeasible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.03094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Divide-Align-Conquer%2520Strategy%2520for%2520Program%2520Synthesis%26entry.906535625%3DJonas%2520Witt%2520and%2520Sebastijan%2520Duman%25C4%258Di%25C4%2587%2520and%2520Tias%2520Guns%2520and%2520Claus-Christian%2520Carbon%26entry.1292438233%3D%2520%2520A%2520major%2520bottleneck%2520in%2520search-based%2520program%2520synthesis%2520is%2520the%2520exponentially%250Agrowing%2520search%2520space%2520which%2520makes%2520learning%2520large%2520programs%2520intractable.%2520Humans%250Amitigate%2520this%2520problem%2520by%2520leveraging%2520the%2520compositional%2520nature%2520of%2520the%2520real%2520world%253A%250AIn%2520structured%2520domains%252C%2520a%2520logical%2520specification%2520can%2520often%2520be%2520decomposed%2520into%250Asmaller%252C%2520complementary%2520solution%2520programs.%2520We%2520show%2520that%2520compositional%250Asegmentation%2520can%2520be%2520applied%2520in%2520the%2520programming%2520by%2520examples%2520setting%2520to%2520divide%250Athe%2520search%2520for%2520large%2520programs%2520across%2520multiple%2520smaller%2520program%2520synthesis%250Aproblems.%2520For%2520each%2520example%252C%2520we%2520search%2520for%2520a%2520decomposition%2520into%2520smaller%2520units%250Awhich%2520maximizes%2520the%2520reconstruction%2520accuracy%2520in%2520the%2520output%2520under%2520a%2520latent%2520task%250Aprogram.%2520A%2520structural%2520alignment%2520of%2520the%2520constituent%2520parts%2520in%2520the%2520input%2520and%250Aoutput%2520leads%2520to%2520pairwise%2520correspondences%2520used%2520to%2520guide%2520the%2520program%2520synthesis%250Asearch.%2520In%2520order%2520to%2520align%2520the%2520input/output%2520structures%252C%2520we%2520make%2520use%2520of%2520the%250AStructure-Mapping%2520Theory%2520%2528SMT%2529%252C%2520a%2520formal%2520model%2520of%2520human%2520analogical%2520reasoning%250Awhich%2520originated%2520in%2520the%2520cognitive%2520sciences.%2520We%2520show%2520that%2520decomposition-driven%250Aprogram%2520synthesis%2520with%2520structural%2520alignment%2520outperforms%2520Inductive%2520Logic%250AProgramming%2520%2528ILP%2529%2520baselines%2520on%2520string%2520transformation%2520tasks%2520even%2520with%2520minimal%250Aknowledge%2520priors.%2520Unlike%2520existing%2520methods%252C%2520the%2520predictive%2520accuracy%2520of%2520our%2520agent%250Amonotonically%2520increases%2520for%2520additional%2520examples%2520and%2520achieves%2520an%2520average%2520time%250Acomplexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528m%2529%2524%2520in%2520the%2520number%2520%2524m%2524%2520of%2520partial%2520programs%2520for%2520highly%250Astructured%2520domains%2520such%2520as%2520strings.%2520We%2520extend%2520this%2520method%2520to%2520the%2520complex%250Asetting%2520of%2520visual%2520reasoning%2520in%2520the%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520%2528ARC%2529%2520for%250Awhich%2520ILP%2520methods%2520were%2520previously%2520infeasible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.03094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Divide-Align-Conquer%20Strategy%20for%20Program%20Synthesis&entry.906535625=Jonas%20Witt%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Tias%20Guns%20and%20Claus-Christian%20Carbon&entry.1292438233=%20%20A%20major%20bottleneck%20in%20search-based%20program%20synthesis%20is%20the%20exponentially%0Agrowing%20search%20space%20which%20makes%20learning%20large%20programs%20intractable.%20Humans%0Amitigate%20this%20problem%20by%20leveraging%20the%20compositional%20nature%20of%20the%20real%20world%3A%0AIn%20structured%20domains%2C%20a%20logical%20specification%20can%20often%20be%20decomposed%20into%0Asmaller%2C%20complementary%20solution%20programs.%20We%20show%20that%20compositional%0Asegmentation%20can%20be%20applied%20in%20the%20programming%20by%20examples%20setting%20to%20divide%0Athe%20search%20for%20large%20programs%20across%20multiple%20smaller%20program%20synthesis%0Aproblems.%20For%20each%20example%2C%20we%20search%20for%20a%20decomposition%20into%20smaller%20units%0Awhich%20maximizes%20the%20reconstruction%20accuracy%20in%20the%20output%20under%20a%20latent%20task%0Aprogram.%20A%20structural%20alignment%20of%20the%20constituent%20parts%20in%20the%20input%20and%0Aoutput%20leads%20to%20pairwise%20correspondences%20used%20to%20guide%20the%20program%20synthesis%0Asearch.%20In%20order%20to%20align%20the%20input/output%20structures%2C%20we%20make%20use%20of%20the%0AStructure-Mapping%20Theory%20%28SMT%29%2C%20a%20formal%20model%20of%20human%20analogical%20reasoning%0Awhich%20originated%20in%20the%20cognitive%20sciences.%20We%20show%20that%20decomposition-driven%0Aprogram%20synthesis%20with%20structural%20alignment%20outperforms%20Inductive%20Logic%0AProgramming%20%28ILP%29%20baselines%20on%20string%20transformation%20tasks%20even%20with%20minimal%0Aknowledge%20priors.%20Unlike%20existing%20methods%2C%20the%20predictive%20accuracy%20of%20our%20agent%0Amonotonically%20increases%20for%20additional%20examples%20and%20achieves%20an%20average%20time%0Acomplexity%20of%20%24%5Cmathcal%7BO%7D%28m%29%24%20in%20the%20number%20%24m%24%20of%20partial%20programs%20for%20highly%0Astructured%20domains%20such%20as%20strings.%20We%20extend%20this%20method%20to%20the%20complex%0Asetting%20of%20visual%20reasoning%20in%20the%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20for%0Awhich%20ILP%20methods%20were%20previously%20infeasible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.03094v2&entry.124074799=Read"},
{"title": "Token-Budget-Aware LLM Reasoning", "author": "Tingxu Han and Chunrong Fang and Shiyu Zhao and Shiqing Ma and Zhenyu Chen and Zhenting Wang", "abstract": "  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n", "link": "http://arxiv.org/abs/2412.18547v1", "date": "2024-12-24", "relevancy": 1.8079, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-Budget-Aware%20LLM%20Reasoning&body=Title%3A%20Token-Budget-Aware%20LLM%20Reasoning%0AAuthor%3A%20Tingxu%20Han%20and%20Chunrong%20Fang%20and%20Shiyu%20Zhao%20and%20Shiqing%20Ma%20and%20Zhenyu%20Chen%20and%20Zhenting%20Wang%0AAbstract%3A%20%20%20Reasoning%20is%20critical%20for%20large%20language%20models%20%28LLMs%29%20to%20excel%20in%20a%20wide%0Arange%20of%20tasks.%20While%20methods%20like%20Chain-of-Thought%20%28CoT%29%20reasoning%20enhance%20LLM%0Aperformance%20by%20decomposing%20problems%20into%20intermediate%20steps%2C%20they%20also%20incur%0Asignificant%20overhead%20in%20token%20usage%2C%20leading%20to%20increased%20costs.%20We%20find%20that%0Athe%20reasoning%20process%20of%20current%20LLMs%20is%20unnecessarily%20lengthy%20and%20it%20can%20be%0Acompressed%20by%20including%20a%20reasonable%20token%20budget%20in%20the%20prompt%2C%20but%20the%20choice%0Aof%20token%20budget%20plays%20a%20crucial%20role%20in%20the%20actual%20compression%20effectiveness.%0AWe%20then%20propose%20a%20token-budget-aware%20LLM%20reasoning%20framework%2C%20which%20dynamically%0Aestimates%20token%20budgets%20for%20different%20problems%20based%20on%20reasoning%20complexity%0Aand%20uses%20the%20estimated%20token%20budgets%20to%20guide%20the%20reasoning%20process.%0AExperiments%20show%20that%20our%20method%20effectively%20reduces%20token%20costs%20in%20CoT%0Areasoning%20with%20only%20a%20slight%20performance%20reduction%2C%20offering%20a%20practical%0Asolution%20to%20balance%20efficiency%20and%20accuracy%20in%20LLM%20reasoning.%20Code%3A%0Ahttps%3A//github.com/GeniusHTX/TALE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-Budget-Aware%2520LLM%2520Reasoning%26entry.906535625%3DTingxu%2520Han%2520and%2520Chunrong%2520Fang%2520and%2520Shiyu%2520Zhao%2520and%2520Shiqing%2520Ma%2520and%2520Zhenyu%2520Chen%2520and%2520Zhenting%2520Wang%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520critical%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520excel%2520in%2520a%2520wide%250Arange%2520of%2520tasks.%2520While%2520methods%2520like%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520enhance%2520LLM%250Aperformance%2520by%2520decomposing%2520problems%2520into%2520intermediate%2520steps%252C%2520they%2520also%2520incur%250Asignificant%2520overhead%2520in%2520token%2520usage%252C%2520leading%2520to%2520increased%2520costs.%2520We%2520find%2520that%250Athe%2520reasoning%2520process%2520of%2520current%2520LLMs%2520is%2520unnecessarily%2520lengthy%2520and%2520it%2520can%2520be%250Acompressed%2520by%2520including%2520a%2520reasonable%2520token%2520budget%2520in%2520the%2520prompt%252C%2520but%2520the%2520choice%250Aof%2520token%2520budget%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520actual%2520compression%2520effectiveness.%250AWe%2520then%2520propose%2520a%2520token-budget-aware%2520LLM%2520reasoning%2520framework%252C%2520which%2520dynamically%250Aestimates%2520token%2520budgets%2520for%2520different%2520problems%2520based%2520on%2520reasoning%2520complexity%250Aand%2520uses%2520the%2520estimated%2520token%2520budgets%2520to%2520guide%2520the%2520reasoning%2520process.%250AExperiments%2520show%2520that%2520our%2520method%2520effectively%2520reduces%2520token%2520costs%2520in%2520CoT%250Areasoning%2520with%2520only%2520a%2520slight%2520performance%2520reduction%252C%2520offering%2520a%2520practical%250Asolution%2520to%2520balance%2520efficiency%2520and%2520accuracy%2520in%2520LLM%2520reasoning.%2520Code%253A%250Ahttps%253A//github.com/GeniusHTX/TALE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-Budget-Aware%20LLM%20Reasoning&entry.906535625=Tingxu%20Han%20and%20Chunrong%20Fang%20and%20Shiyu%20Zhao%20and%20Shiqing%20Ma%20and%20Zhenyu%20Chen%20and%20Zhenting%20Wang&entry.1292438233=%20%20Reasoning%20is%20critical%20for%20large%20language%20models%20%28LLMs%29%20to%20excel%20in%20a%20wide%0Arange%20of%20tasks.%20While%20methods%20like%20Chain-of-Thought%20%28CoT%29%20reasoning%20enhance%20LLM%0Aperformance%20by%20decomposing%20problems%20into%20intermediate%20steps%2C%20they%20also%20incur%0Asignificant%20overhead%20in%20token%20usage%2C%20leading%20to%20increased%20costs.%20We%20find%20that%0Athe%20reasoning%20process%20of%20current%20LLMs%20is%20unnecessarily%20lengthy%20and%20it%20can%20be%0Acompressed%20by%20including%20a%20reasonable%20token%20budget%20in%20the%20prompt%2C%20but%20the%20choice%0Aof%20token%20budget%20plays%20a%20crucial%20role%20in%20the%20actual%20compression%20effectiveness.%0AWe%20then%20propose%20a%20token-budget-aware%20LLM%20reasoning%20framework%2C%20which%20dynamically%0Aestimates%20token%20budgets%20for%20different%20problems%20based%20on%20reasoning%20complexity%0Aand%20uses%20the%20estimated%20token%20budgets%20to%20guide%20the%20reasoning%20process.%0AExperiments%20show%20that%20our%20method%20effectively%20reduces%20token%20costs%20in%20CoT%0Areasoning%20with%20only%20a%20slight%20performance%20reduction%2C%20offering%20a%20practical%0Asolution%20to%20balance%20efficiency%20and%20accuracy%20in%20LLM%20reasoning.%20Code%3A%0Ahttps%3A//github.com/GeniusHTX/TALE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18547v1&entry.124074799=Read"},
{"title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory", "author": "Junyeong Park and Junmo Cho and Sungjin Ahn", "abstract": "  Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.\n", "link": "http://arxiv.org/abs/2411.06736v3", "date": "2024-12-24", "relevancy": 1.6313, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6004}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5326}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MrSteve%3A%20Instruction-Following%20Agents%20in%20Minecraft%20with%20What-Where-When%0A%20%20Memory&body=Title%3A%20MrSteve%3A%20Instruction-Following%20Agents%20in%20Minecraft%20with%20What-Where-When%0A%20%20Memory%0AAuthor%3A%20Junyeong%20Park%20and%20Junmo%20Cho%20and%20Sungjin%20Ahn%0AAbstract%3A%20%20%20Significant%20advances%20have%20been%20made%20in%20developing%20general-purpose%20embodied%20AI%0Ain%20environments%20like%20Minecraft%20through%20the%20adoption%20of%20LLM-augmented%0Ahierarchical%20approaches.%20While%20these%20approaches%2C%20which%20combine%20high-level%0Aplanners%20with%20low-level%20controllers%2C%20show%20promise%2C%20low-level%20controllers%0Afrequently%20become%20performance%20bottlenecks%20due%20to%20repeated%20failures.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20primary%20cause%20of%20failure%20in%20many%20low-level%20controllers%0Ais%20the%20absence%20of%20an%20episodic%20memory%20system.%20To%20address%20this%2C%20we%20introduce%0AMrSteve%20%28Memory%20Recall%20Steve-1%29%2C%20a%20novel%20low-level%20controller%20equipped%20with%0APlace%20Event%20Memory%20%28PEM%29%2C%20a%20form%20of%20episodic%20memory%20that%20captures%20what%2C%20where%2C%0Aand%20when%20information%20from%20episodes.%20This%20directly%20addresses%20the%20main%20limitation%0Aof%20the%20popular%20low-level%20controller%2C%20Steve-1.%20Unlike%20previous%20models%20that%20rely%0Aon%20short-term%20memory%2C%20PEM%20organizes%20spatial%20and%20event-based%20data%2C%20enabling%0Aefficient%20recall%20and%20navigation%20in%20long-horizon%20tasks.%20Additionally%2C%20we%20propose%0Aan%20Exploration%20Strategy%20and%20a%20Memory-Augmented%20Task%20Solving%20Framework%2C%20allowing%0Aagents%20to%20alternate%20between%20exploration%20and%20task-solving%20based%20on%20recalled%0Aevents.%20Our%20approach%20significantly%20improves%20task-solving%20and%20exploration%0Aefficiency%20compared%20to%20existing%20methods.%20We%20will%20release%20our%20code%20and%20demos%20on%0Athe%20project%20page%3A%20https%3A//sites.google.com/view/mr-steve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06736v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMrSteve%253A%2520Instruction-Following%2520Agents%2520in%2520Minecraft%2520with%2520What-Where-When%250A%2520%2520Memory%26entry.906535625%3DJunyeong%2520Park%2520and%2520Junmo%2520Cho%2520and%2520Sungjin%2520Ahn%26entry.1292438233%3D%2520%2520Significant%2520advances%2520have%2520been%2520made%2520in%2520developing%2520general-purpose%2520embodied%2520AI%250Ain%2520environments%2520like%2520Minecraft%2520through%2520the%2520adoption%2520of%2520LLM-augmented%250Ahierarchical%2520approaches.%2520While%2520these%2520approaches%252C%2520which%2520combine%2520high-level%250Aplanners%2520with%2520low-level%2520controllers%252C%2520show%2520promise%252C%2520low-level%2520controllers%250Afrequently%2520become%2520performance%2520bottlenecks%2520due%2520to%2520repeated%2520failures.%2520In%2520this%250Apaper%252C%2520we%2520argue%2520that%2520the%2520primary%2520cause%2520of%2520failure%2520in%2520many%2520low-level%2520controllers%250Ais%2520the%2520absence%2520of%2520an%2520episodic%2520memory%2520system.%2520To%2520address%2520this%252C%2520we%2520introduce%250AMrSteve%2520%2528Memory%2520Recall%2520Steve-1%2529%252C%2520a%2520novel%2520low-level%2520controller%2520equipped%2520with%250APlace%2520Event%2520Memory%2520%2528PEM%2529%252C%2520a%2520form%2520of%2520episodic%2520memory%2520that%2520captures%2520what%252C%2520where%252C%250Aand%2520when%2520information%2520from%2520episodes.%2520This%2520directly%2520addresses%2520the%2520main%2520limitation%250Aof%2520the%2520popular%2520low-level%2520controller%252C%2520Steve-1.%2520Unlike%2520previous%2520models%2520that%2520rely%250Aon%2520short-term%2520memory%252C%2520PEM%2520organizes%2520spatial%2520and%2520event-based%2520data%252C%2520enabling%250Aefficient%2520recall%2520and%2520navigation%2520in%2520long-horizon%2520tasks.%2520Additionally%252C%2520we%2520propose%250Aan%2520Exploration%2520Strategy%2520and%2520a%2520Memory-Augmented%2520Task%2520Solving%2520Framework%252C%2520allowing%250Aagents%2520to%2520alternate%2520between%2520exploration%2520and%2520task-solving%2520based%2520on%2520recalled%250Aevents.%2520Our%2520approach%2520significantly%2520improves%2520task-solving%2520and%2520exploration%250Aefficiency%2520compared%2520to%2520existing%2520methods.%2520We%2520will%2520release%2520our%2520code%2520and%2520demos%2520on%250Athe%2520project%2520page%253A%2520https%253A//sites.google.com/view/mr-steve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06736v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MrSteve%3A%20Instruction-Following%20Agents%20in%20Minecraft%20with%20What-Where-When%0A%20%20Memory&entry.906535625=Junyeong%20Park%20and%20Junmo%20Cho%20and%20Sungjin%20Ahn&entry.1292438233=%20%20Significant%20advances%20have%20been%20made%20in%20developing%20general-purpose%20embodied%20AI%0Ain%20environments%20like%20Minecraft%20through%20the%20adoption%20of%20LLM-augmented%0Ahierarchical%20approaches.%20While%20these%20approaches%2C%20which%20combine%20high-level%0Aplanners%20with%20low-level%20controllers%2C%20show%20promise%2C%20low-level%20controllers%0Afrequently%20become%20performance%20bottlenecks%20due%20to%20repeated%20failures.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20primary%20cause%20of%20failure%20in%20many%20low-level%20controllers%0Ais%20the%20absence%20of%20an%20episodic%20memory%20system.%20To%20address%20this%2C%20we%20introduce%0AMrSteve%20%28Memory%20Recall%20Steve-1%29%2C%20a%20novel%20low-level%20controller%20equipped%20with%0APlace%20Event%20Memory%20%28PEM%29%2C%20a%20form%20of%20episodic%20memory%20that%20captures%20what%2C%20where%2C%0Aand%20when%20information%20from%20episodes.%20This%20directly%20addresses%20the%20main%20limitation%0Aof%20the%20popular%20low-level%20controller%2C%20Steve-1.%20Unlike%20previous%20models%20that%20rely%0Aon%20short-term%20memory%2C%20PEM%20organizes%20spatial%20and%20event-based%20data%2C%20enabling%0Aefficient%20recall%20and%20navigation%20in%20long-horizon%20tasks.%20Additionally%2C%20we%20propose%0Aan%20Exploration%20Strategy%20and%20a%20Memory-Augmented%20Task%20Solving%20Framework%2C%20allowing%0Aagents%20to%20alternate%20between%20exploration%20and%20task-solving%20based%20on%20recalled%0Aevents.%20Our%20approach%20significantly%20improves%20task-solving%20and%20exploration%0Aefficiency%20compared%20to%20existing%20methods.%20We%20will%20release%20our%20code%20and%20demos%20on%0Athe%20project%20page%3A%20https%3A//sites.google.com/view/mr-steve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06736v3&entry.124074799=Read"},
{"title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token\n  Prediction", "author": "Wu Xiaoping and Hu Jie and Wei Xiaoming", "abstract": "  Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community.\n", "link": "http://arxiv.org/abs/2412.18390v1", "date": "2024-12-24", "relevancy": 1.9008, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6362}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RDPM%3A%20Solve%20Diffusion%20Probabilistic%20Models%20via%20Recurrent%20Token%0A%20%20Prediction&body=Title%3A%20RDPM%3A%20Solve%20Diffusion%20Probabilistic%20Models%20via%20Recurrent%20Token%0A%20%20Prediction%0AAuthor%3A%20Wu%20Xiaoping%20and%20Hu%20Jie%20and%20Wei%20Xiaoming%0AAbstract%3A%20%20%20Diffusion%20Probabilistic%20Models%20%28DPMs%29%20have%20emerged%20as%20the%20de%20facto%20approach%0Afor%20high-fidelity%20image%20synthesis%2C%20operating%20diffusion%20processes%20on%20continuous%0AVAE%20latent%2C%20which%20significantly%20differ%20from%20the%20text%20generation%20methods%0Aemployed%20by%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Agenerative%20framework%2C%20the%20Recurrent%20Diffusion%20Probabilistic%20Model%20%28RDPM%29%2C%20which%0Aenhances%20the%20diffusion%20process%20through%20a%20recurrent%20token%20prediction%20mechanism%2C%0Athereby%20pioneering%20the%20field%20of%20Discrete%20Diffusion.%20By%20progressively%0Aintroducing%20Gaussian%20noise%20into%20the%20latent%20representations%20of%20images%20and%0Aencoding%20them%20into%20vector-quantized%20tokens%20in%20a%20recurrent%20manner%2C%20RDPM%0Afacilitates%20a%20unique%20diffusion%20process%20on%20discrete-value%20domains.%20This%20process%0Aiteratively%20predicts%20the%20token%20codes%20for%20subsequent%20timesteps%2C%20transforming%20the%0Ainitial%20standard%20Gaussian%20noise%20into%20the%20source%20data%20distribution%2C%20aligning%0Awith%20GPT-style%20models%20in%20terms%20of%20the%20loss%20function.%20RDPM%20demonstrates%20superior%0Aperformance%20while%20benefiting%20from%20the%20speed%20advantage%20of%20requiring%20only%20a%20few%0Ainference%20steps.%20This%20model%20not%20only%20leverages%20the%20diffusion%20process%20to%20ensure%0Ahigh-quality%20generation%20but%20also%20converts%20continuous%20signals%20into%20a%20series%20of%0Ahigh-fidelity%20discrete%20tokens%2C%20thereby%20maintaining%20a%20unified%20optimization%0Astrategy%20with%20other%20discrete%20tokens%2C%20such%20as%20text.%20We%20anticipate%20that%20this%20work%0Awill%20contribute%20to%20the%20development%20of%20a%20unified%20model%20for%20multimodal%0Ageneration%2C%20specifically%20by%20integrating%20continuous%20signal%20domains%20such%20as%0Aimages%2C%20videos%2C%20and%20audio%20with%20text.%20We%20will%20release%20the%20code%20and%20model%20weights%0Ato%20the%20open-source%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRDPM%253A%2520Solve%2520Diffusion%2520Probabilistic%2520Models%2520via%2520Recurrent%2520Token%250A%2520%2520Prediction%26entry.906535625%3DWu%2520Xiaoping%2520and%2520Hu%2520Jie%2520and%2520Wei%2520Xiaoming%26entry.1292438233%3D%2520%2520Diffusion%2520Probabilistic%2520Models%2520%2528DPMs%2529%2520have%2520emerged%2520as%2520the%2520de%2520facto%2520approach%250Afor%2520high-fidelity%2520image%2520synthesis%252C%2520operating%2520diffusion%2520processes%2520on%2520continuous%250AVAE%2520latent%252C%2520which%2520significantly%2520differ%2520from%2520the%2520text%2520generation%2520methods%250Aemployed%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Agenerative%2520framework%252C%2520the%2520Recurrent%2520Diffusion%2520Probabilistic%2520Model%2520%2528RDPM%2529%252C%2520which%250Aenhances%2520the%2520diffusion%2520process%2520through%2520a%2520recurrent%2520token%2520prediction%2520mechanism%252C%250Athereby%2520pioneering%2520the%2520field%2520of%2520Discrete%2520Diffusion.%2520By%2520progressively%250Aintroducing%2520Gaussian%2520noise%2520into%2520the%2520latent%2520representations%2520of%2520images%2520and%250Aencoding%2520them%2520into%2520vector-quantized%2520tokens%2520in%2520a%2520recurrent%2520manner%252C%2520RDPM%250Afacilitates%2520a%2520unique%2520diffusion%2520process%2520on%2520discrete-value%2520domains.%2520This%2520process%250Aiteratively%2520predicts%2520the%2520token%2520codes%2520for%2520subsequent%2520timesteps%252C%2520transforming%2520the%250Ainitial%2520standard%2520Gaussian%2520noise%2520into%2520the%2520source%2520data%2520distribution%252C%2520aligning%250Awith%2520GPT-style%2520models%2520in%2520terms%2520of%2520the%2520loss%2520function.%2520RDPM%2520demonstrates%2520superior%250Aperformance%2520while%2520benefiting%2520from%2520the%2520speed%2520advantage%2520of%2520requiring%2520only%2520a%2520few%250Ainference%2520steps.%2520This%2520model%2520not%2520only%2520leverages%2520the%2520diffusion%2520process%2520to%2520ensure%250Ahigh-quality%2520generation%2520but%2520also%2520converts%2520continuous%2520signals%2520into%2520a%2520series%2520of%250Ahigh-fidelity%2520discrete%2520tokens%252C%2520thereby%2520maintaining%2520a%2520unified%2520optimization%250Astrategy%2520with%2520other%2520discrete%2520tokens%252C%2520such%2520as%2520text.%2520We%2520anticipate%2520that%2520this%2520work%250Awill%2520contribute%2520to%2520the%2520development%2520of%2520a%2520unified%2520model%2520for%2520multimodal%250Ageneration%252C%2520specifically%2520by%2520integrating%2520continuous%2520signal%2520domains%2520such%2520as%250Aimages%252C%2520videos%252C%2520and%2520audio%2520with%2520text.%2520We%2520will%2520release%2520the%2520code%2520and%2520model%2520weights%250Ato%2520the%2520open-source%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RDPM%3A%20Solve%20Diffusion%20Probabilistic%20Models%20via%20Recurrent%20Token%0A%20%20Prediction&entry.906535625=Wu%20Xiaoping%20and%20Hu%20Jie%20and%20Wei%20Xiaoming&entry.1292438233=%20%20Diffusion%20Probabilistic%20Models%20%28DPMs%29%20have%20emerged%20as%20the%20de%20facto%20approach%0Afor%20high-fidelity%20image%20synthesis%2C%20operating%20diffusion%20processes%20on%20continuous%0AVAE%20latent%2C%20which%20significantly%20differ%20from%20the%20text%20generation%20methods%0Aemployed%20by%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Agenerative%20framework%2C%20the%20Recurrent%20Diffusion%20Probabilistic%20Model%20%28RDPM%29%2C%20which%0Aenhances%20the%20diffusion%20process%20through%20a%20recurrent%20token%20prediction%20mechanism%2C%0Athereby%20pioneering%20the%20field%20of%20Discrete%20Diffusion.%20By%20progressively%0Aintroducing%20Gaussian%20noise%20into%20the%20latent%20representations%20of%20images%20and%0Aencoding%20them%20into%20vector-quantized%20tokens%20in%20a%20recurrent%20manner%2C%20RDPM%0Afacilitates%20a%20unique%20diffusion%20process%20on%20discrete-value%20domains.%20This%20process%0Aiteratively%20predicts%20the%20token%20codes%20for%20subsequent%20timesteps%2C%20transforming%20the%0Ainitial%20standard%20Gaussian%20noise%20into%20the%20source%20data%20distribution%2C%20aligning%0Awith%20GPT-style%20models%20in%20terms%20of%20the%20loss%20function.%20RDPM%20demonstrates%20superior%0Aperformance%20while%20benefiting%20from%20the%20speed%20advantage%20of%20requiring%20only%20a%20few%0Ainference%20steps.%20This%20model%20not%20only%20leverages%20the%20diffusion%20process%20to%20ensure%0Ahigh-quality%20generation%20but%20also%20converts%20continuous%20signals%20into%20a%20series%20of%0Ahigh-fidelity%20discrete%20tokens%2C%20thereby%20maintaining%20a%20unified%20optimization%0Astrategy%20with%20other%20discrete%20tokens%2C%20such%20as%20text.%20We%20anticipate%20that%20this%20work%0Awill%20contribute%20to%20the%20development%20of%20a%20unified%20model%20for%20multimodal%0Ageneration%2C%20specifically%20by%20integrating%20continuous%20signal%20domains%20such%20as%0Aimages%2C%20videos%2C%20and%20audio%20with%20text.%20We%20will%20release%20the%20code%20and%20model%20weights%0Ato%20the%20open-source%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18390v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


