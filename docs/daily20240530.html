<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240529.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NPGA: Neural Parametric Gaussian Avatars", "author": "Simon Giebenhain and Tobias Kirschstein and Martin R\u00fcnz and Lourdes Agapito and Matthias Nie\u00dfner", "abstract": "  The creation of high-fidelity, digital versions of human heads is an\nimportant stepping stone in the process of further integrating virtual\ncomponents into our everyday lives. Constructing such avatars is a challenging\nresearch problem, due to a high demand for photo-realism and real-time\nrendering performance. In this work, we propose Neural Parametric Gaussian\nAvatars (NPGA), a data-driven approach to create high-fidelity, controllable\navatars from multi-view video recordings. We build our method around 3D\nGaussian Splatting for its highly efficient rendering and to inherit the\ntopological flexibility of point clouds. In contrast to previous work, we\ncondition our avatars' dynamics on the rich expression space of neural\nparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we\ndistill the backward deformation field of our underlying NPHM into forward\ndeformations which are compatible with rasterization-based rendering. All\nremaining fine-scale, expression-dependent details are learned from the\nmulti-view videos. To increase the representational capacity of our avatars, we\naugment the canonical Gaussian point cloud using per-primitive latent features\nwhich govern its dynamic behavior. To regularize this increased dynamic\nexpressivity, we propose Laplacian terms on the latent features and predicted\ndynamics. We evaluate our method on the public NeRSemble dataset, demonstrating\nthat NPGA significantly outperforms the previous state-of-the-art avatars on\nthe self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate\nanimation capabilities from real-world monocular videos.\n", "link": "http://arxiv.org/abs/2405.19331v1", "date": "2024-05-29", "relevancy": 3.6523, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7677}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars&body=Title%3A%20NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars%0AAuthor%3A%20Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20The%20creation%20of%20high-fidelity%2C%20digital%20versions%20of%20human%20heads%20is%20an%0Aimportant%20stepping%20stone%20in%20the%20process%20of%20further%20integrating%20virtual%0Acomponents%20into%20our%20everyday%20lives.%20Constructing%20such%20avatars%20is%20a%20challenging%0Aresearch%20problem%2C%20due%20to%20a%20high%20demand%20for%20photo-realism%20and%20real-time%0Arendering%20performance.%20In%20this%20work%2C%20we%20propose%20Neural%20Parametric%20Gaussian%0AAvatars%20%28NPGA%29%2C%20a%20data-driven%20approach%20to%20create%20high-fidelity%2C%20controllable%0Aavatars%20from%20multi-view%20video%20recordings.%20We%20build%20our%20method%20around%203D%0AGaussian%20Splatting%20for%20its%20highly%20efficient%20rendering%20and%20to%20inherit%20the%0Atopological%20flexibility%20of%20point%20clouds.%20In%20contrast%20to%20previous%20work%2C%20we%0Acondition%20our%20avatars%27%20dynamics%20on%20the%20rich%20expression%20space%20of%20neural%0Aparametric%20head%20models%20%28NPHM%29%2C%20instead%20of%20mesh-based%203DMMs.%20To%20this%20end%2C%20we%0Adistill%20the%20backward%20deformation%20field%20of%20our%20underlying%20NPHM%20into%20forward%0Adeformations%20which%20are%20compatible%20with%20rasterization-based%20rendering.%20All%0Aremaining%20fine-scale%2C%20expression-dependent%20details%20are%20learned%20from%20the%0Amulti-view%20videos.%20To%20increase%20the%20representational%20capacity%20of%20our%20avatars%2C%20we%0Aaugment%20the%20canonical%20Gaussian%20point%20cloud%20using%20per-primitive%20latent%20features%0Awhich%20govern%20its%20dynamic%20behavior.%20To%20regularize%20this%20increased%20dynamic%0Aexpressivity%2C%20we%20propose%20Laplacian%20terms%20on%20the%20latent%20features%20and%20predicted%0Adynamics.%20We%20evaluate%20our%20method%20on%20the%20public%20NeRSemble%20dataset%2C%20demonstrating%0Athat%20NPGA%20significantly%20outperforms%20the%20previous%20state-of-the-art%20avatars%20on%0Athe%20self-reenactment%20task%20by%202.6%20PSNR.%20Furthermore%2C%20we%20demonstrate%20accurate%0Aanimation%20capabilities%20from%20real-world%20monocular%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNPGA%253A%2520Neural%2520Parametric%2520Gaussian%2520Avatars%26entry.906535625%3DSimon%2520Giebenhain%2520and%2520Tobias%2520Kirschstein%2520and%2520Martin%2520R%25C3%25BCnz%2520and%2520Lourdes%2520Agapito%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520The%2520creation%2520of%2520high-fidelity%252C%2520digital%2520versions%2520of%2520human%2520heads%2520is%2520an%250Aimportant%2520stepping%2520stone%2520in%2520the%2520process%2520of%2520further%2520integrating%2520virtual%250Acomponents%2520into%2520our%2520everyday%2520lives.%2520Constructing%2520such%2520avatars%2520is%2520a%2520challenging%250Aresearch%2520problem%252C%2520due%2520to%2520a%2520high%2520demand%2520for%2520photo-realism%2520and%2520real-time%250Arendering%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520Neural%2520Parametric%2520Gaussian%250AAvatars%2520%2528NPGA%2529%252C%2520a%2520data-driven%2520approach%2520to%2520create%2520high-fidelity%252C%2520controllable%250Aavatars%2520from%2520multi-view%2520video%2520recordings.%2520We%2520build%2520our%2520method%2520around%25203D%250AGaussian%2520Splatting%2520for%2520its%2520highly%2520efficient%2520rendering%2520and%2520to%2520inherit%2520the%250Atopological%2520flexibility%2520of%2520point%2520clouds.%2520In%2520contrast%2520to%2520previous%2520work%252C%2520we%250Acondition%2520our%2520avatars%2527%2520dynamics%2520on%2520the%2520rich%2520expression%2520space%2520of%2520neural%250Aparametric%2520head%2520models%2520%2528NPHM%2529%252C%2520instead%2520of%2520mesh-based%25203DMMs.%2520To%2520this%2520end%252C%2520we%250Adistill%2520the%2520backward%2520deformation%2520field%2520of%2520our%2520underlying%2520NPHM%2520into%2520forward%250Adeformations%2520which%2520are%2520compatible%2520with%2520rasterization-based%2520rendering.%2520All%250Aremaining%2520fine-scale%252C%2520expression-dependent%2520details%2520are%2520learned%2520from%2520the%250Amulti-view%2520videos.%2520To%2520increase%2520the%2520representational%2520capacity%2520of%2520our%2520avatars%252C%2520we%250Aaugment%2520the%2520canonical%2520Gaussian%2520point%2520cloud%2520using%2520per-primitive%2520latent%2520features%250Awhich%2520govern%2520its%2520dynamic%2520behavior.%2520To%2520regularize%2520this%2520increased%2520dynamic%250Aexpressivity%252C%2520we%2520propose%2520Laplacian%2520terms%2520on%2520the%2520latent%2520features%2520and%2520predicted%250Adynamics.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520public%2520NeRSemble%2520dataset%252C%2520demonstrating%250Athat%2520NPGA%2520significantly%2520outperforms%2520the%2520previous%2520state-of-the-art%2520avatars%2520on%250Athe%2520self-reenactment%2520task%2520by%25202.6%2520PSNR.%2520Furthermore%252C%2520we%2520demonstrate%2520accurate%250Aanimation%2520capabilities%2520from%2520real-world%2520monocular%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars&entry.906535625=Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20The%20creation%20of%20high-fidelity%2C%20digital%20versions%20of%20human%20heads%20is%20an%0Aimportant%20stepping%20stone%20in%20the%20process%20of%20further%20integrating%20virtual%0Acomponents%20into%20our%20everyday%20lives.%20Constructing%20such%20avatars%20is%20a%20challenging%0Aresearch%20problem%2C%20due%20to%20a%20high%20demand%20for%20photo-realism%20and%20real-time%0Arendering%20performance.%20In%20this%20work%2C%20we%20propose%20Neural%20Parametric%20Gaussian%0AAvatars%20%28NPGA%29%2C%20a%20data-driven%20approach%20to%20create%20high-fidelity%2C%20controllable%0Aavatars%20from%20multi-view%20video%20recordings.%20We%20build%20our%20method%20around%203D%0AGaussian%20Splatting%20for%20its%20highly%20efficient%20rendering%20and%20to%20inherit%20the%0Atopological%20flexibility%20of%20point%20clouds.%20In%20contrast%20to%20previous%20work%2C%20we%0Acondition%20our%20avatars%27%20dynamics%20on%20the%20rich%20expression%20space%20of%20neural%0Aparametric%20head%20models%20%28NPHM%29%2C%20instead%20of%20mesh-based%203DMMs.%20To%20this%20end%2C%20we%0Adistill%20the%20backward%20deformation%20field%20of%20our%20underlying%20NPHM%20into%20forward%0Adeformations%20which%20are%20compatible%20with%20rasterization-based%20rendering.%20All%0Aremaining%20fine-scale%2C%20expression-dependent%20details%20are%20learned%20from%20the%0Amulti-view%20videos.%20To%20increase%20the%20representational%20capacity%20of%20our%20avatars%2C%20we%0Aaugment%20the%20canonical%20Gaussian%20point%20cloud%20using%20per-primitive%20latent%20features%0Awhich%20govern%20its%20dynamic%20behavior.%20To%20regularize%20this%20increased%20dynamic%0Aexpressivity%2C%20we%20propose%20Laplacian%20terms%20on%20the%20latent%20features%20and%20predicted%0Adynamics.%20We%20evaluate%20our%20method%20on%20the%20public%20NeRSemble%20dataset%2C%20demonstrating%0Athat%20NPGA%20significantly%20outperforms%20the%20previous%20state-of-the-art%20avatars%20on%0Athe%20self-reenactment%20task%20by%202.6%20PSNR.%20Furthermore%2C%20we%20demonstrate%20accurate%0Aanimation%20capabilities%20from%20real-world%20monocular%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19331v1&entry.124074799=Read"},
{"title": "$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation", "author": "Weitian Zhang and Yichao Yan and Yunhui Liu and Xingdong Sheng and Xiaokang Yang", "abstract": "  This paper aims to introduce 3D Gaussian for efficient, expressive, and\neditable digital avatar generation. This task faces two major challenges: (1)\nThe unstructured nature of 3D Gaussian makes it incompatible with current\ngeneration pipelines; (2) the expressive animation of 3D Gaussian in a\ngenerative setting that involves training with multiple subjects remains\nunexplored. In this paper, we propose a novel avatar generation method named\n$E^3$Gen, to effectively address these challenges. First, we propose a novel\ngenerative UV features plane representation that encodes unstructured 3D\nGaussian onto a structured 2D UV space defined by the SMPL-X parametric model.\nThis novel representation not only preserves the representation ability of the\noriginal 3D Gaussian but also introduces a shared structure among subjects to\nenable generative learning of the diffusion model. To tackle the second\nchallenge, we propose a part-aware deformation module to achieve robust and\naccurate full-body expressive pose control. Extensive experiments demonstrate\nthat our method achieves superior performance in avatar generation and enables\nexpressive full-body pose control and editing.\n", "link": "http://arxiv.org/abs/2405.19203v1", "date": "2024-05-29", "relevancy": 3.3817, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.723}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.723}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation&body=Title%3A%20%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation%0AAuthor%3A%20Weitian%20Zhang%20and%20Yichao%20Yan%20and%20Yunhui%20Liu%20and%20Xingdong%20Sheng%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20introduce%203D%20Gaussian%20for%20efficient%2C%20expressive%2C%20and%0Aeditable%20digital%20avatar%20generation.%20This%20task%20faces%20two%20major%20challenges%3A%20%281%29%0AThe%20unstructured%20nature%20of%203D%20Gaussian%20makes%20it%20incompatible%20with%20current%0Ageneration%20pipelines%3B%20%282%29%20the%20expressive%20animation%20of%203D%20Gaussian%20in%20a%0Agenerative%20setting%20that%20involves%20training%20with%20multiple%20subjects%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20avatar%20generation%20method%20named%0A%24E%5E3%24Gen%2C%20to%20effectively%20address%20these%20challenges.%20First%2C%20we%20propose%20a%20novel%0Agenerative%20UV%20features%20plane%20representation%20that%20encodes%20unstructured%203D%0AGaussian%20onto%20a%20structured%202D%20UV%20space%20defined%20by%20the%20SMPL-X%20parametric%20model.%0AThis%20novel%20representation%20not%20only%20preserves%20the%20representation%20ability%20of%20the%0Aoriginal%203D%20Gaussian%20but%20also%20introduces%20a%20shared%20structure%20among%20subjects%20to%0Aenable%20generative%20learning%20of%20the%20diffusion%20model.%20To%20tackle%20the%20second%0Achallenge%2C%20we%20propose%20a%20part-aware%20deformation%20module%20to%20achieve%20robust%20and%0Aaccurate%20full-body%20expressive%20pose%20control.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20superior%20performance%20in%20avatar%20generation%20and%20enables%0Aexpressive%20full-body%20pose%20control%20and%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524E%255E%257B3%257D%2524Gen%253A%2520Efficient%252C%2520Expressive%2520and%2520Editable%2520Avatars%2520Generation%26entry.906535625%3DWeitian%2520Zhang%2520and%2520Yichao%2520Yan%2520and%2520Yunhui%2520Liu%2520and%2520Xingdong%2520Sheng%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520introduce%25203D%2520Gaussian%2520for%2520efficient%252C%2520expressive%252C%2520and%250Aeditable%2520digital%2520avatar%2520generation.%2520This%2520task%2520faces%2520two%2520major%2520challenges%253A%2520%25281%2529%250AThe%2520unstructured%2520nature%2520of%25203D%2520Gaussian%2520makes%2520it%2520incompatible%2520with%2520current%250Ageneration%2520pipelines%253B%2520%25282%2529%2520the%2520expressive%2520animation%2520of%25203D%2520Gaussian%2520in%2520a%250Agenerative%2520setting%2520that%2520involves%2520training%2520with%2520multiple%2520subjects%2520remains%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520avatar%2520generation%2520method%2520named%250A%2524E%255E3%2524Gen%252C%2520to%2520effectively%2520address%2520these%2520challenges.%2520First%252C%2520we%2520propose%2520a%2520novel%250Agenerative%2520UV%2520features%2520plane%2520representation%2520that%2520encodes%2520unstructured%25203D%250AGaussian%2520onto%2520a%2520structured%25202D%2520UV%2520space%2520defined%2520by%2520the%2520SMPL-X%2520parametric%2520model.%250AThis%2520novel%2520representation%2520not%2520only%2520preserves%2520the%2520representation%2520ability%2520of%2520the%250Aoriginal%25203D%2520Gaussian%2520but%2520also%2520introduces%2520a%2520shared%2520structure%2520among%2520subjects%2520to%250Aenable%2520generative%2520learning%2520of%2520the%2520diffusion%2520model.%2520To%2520tackle%2520the%2520second%250Achallenge%252C%2520we%2520propose%2520a%2520part-aware%2520deformation%2520module%2520to%2520achieve%2520robust%2520and%250Aaccurate%2520full-body%2520expressive%2520pose%2520control.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520superior%2520performance%2520in%2520avatar%2520generation%2520and%2520enables%250Aexpressive%2520full-body%2520pose%2520control%2520and%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation&entry.906535625=Weitian%20Zhang%20and%20Yichao%20Yan%20and%20Yunhui%20Liu%20and%20Xingdong%20Sheng%20and%20Xiaokang%20Yang&entry.1292438233=%20%20This%20paper%20aims%20to%20introduce%203D%20Gaussian%20for%20efficient%2C%20expressive%2C%20and%0Aeditable%20digital%20avatar%20generation.%20This%20task%20faces%20two%20major%20challenges%3A%20%281%29%0AThe%20unstructured%20nature%20of%203D%20Gaussian%20makes%20it%20incompatible%20with%20current%0Ageneration%20pipelines%3B%20%282%29%20the%20expressive%20animation%20of%203D%20Gaussian%20in%20a%0Agenerative%20setting%20that%20involves%20training%20with%20multiple%20subjects%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20avatar%20generation%20method%20named%0A%24E%5E3%24Gen%2C%20to%20effectively%20address%20these%20challenges.%20First%2C%20we%20propose%20a%20novel%0Agenerative%20UV%20features%20plane%20representation%20that%20encodes%20unstructured%203D%0AGaussian%20onto%20a%20structured%202D%20UV%20space%20defined%20by%20the%20SMPL-X%20parametric%20model.%0AThis%20novel%20representation%20not%20only%20preserves%20the%20representation%20ability%20of%20the%0Aoriginal%203D%20Gaussian%20but%20also%20introduces%20a%20shared%20structure%20among%20subjects%20to%0Aenable%20generative%20learning%20of%20the%20diffusion%20model.%20To%20tackle%20the%20second%0Achallenge%2C%20we%20propose%20a%20part-aware%20deformation%20module%20to%20achieve%20robust%20and%0Aaccurate%20full-body%20expressive%20pose%20control.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20superior%20performance%20in%20avatar%20generation%20and%20enables%0Aexpressive%20full-body%20pose%20control%20and%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19203v1&entry.124074799=Read"},
{"title": "DGD: Dynamic 3D Gaussians Distillation", "author": "Isaac Labe and Noam Issachar and Itai Lang and Sagie Benaim", "abstract": "  We tackle the task of learning dynamic 3D semantic radiance fields given a\nsingle monocular video as input. Our learned semantic radiance field captures\nper-point semantics as well as color and geometric properties for a dynamic 3D\nscene, enabling the generation of novel views and their corresponding\nsemantics. This enables the segmentation and tracking of a diverse set of 3D\nsemantic entities, specified using a simple and intuitive interface that\nincludes a user click or a text prompt. To this end, we present DGD, a unified\n3D representation for both the appearance and semantics of a dynamic 3D scene,\nbuilding upon the recently proposed dynamic 3D Gaussians representation. Our\nrepresentation is optimized over time with both color and semantic information.\nKey to our method is the joint optimization of the appearance and semantic\nattributes, which jointly affect the geometric properties of the scene. We\nevaluate our approach in its ability to enable dense semantic 3D object\ntracking and demonstrate high-quality results that are fast to render, for a\ndiverse set of scenes. Our project webpage is available on\nhttps://isaaclabe.github.io/DGD-Website/\n", "link": "http://arxiv.org/abs/2405.19321v1", "date": "2024-05-29", "relevancy": 3.3126, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7217}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6608}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGD%3A%20Dynamic%203D%20Gaussians%20Distillation&body=Title%3A%20DGD%3A%20Dynamic%203D%20Gaussians%20Distillation%0AAuthor%3A%20Isaac%20Labe%20and%20Noam%20Issachar%20and%20Itai%20Lang%20and%20Sagie%20Benaim%0AAbstract%3A%20%20%20We%20tackle%20the%20task%20of%20learning%20dynamic%203D%20semantic%20radiance%20fields%20given%20a%0Asingle%20monocular%20video%20as%20input.%20Our%20learned%20semantic%20radiance%20field%20captures%0Aper-point%20semantics%20as%20well%20as%20color%20and%20geometric%20properties%20for%20a%20dynamic%203D%0Ascene%2C%20enabling%20the%20generation%20of%20novel%20views%20and%20their%20corresponding%0Asemantics.%20This%20enables%20the%20segmentation%20and%20tracking%20of%20a%20diverse%20set%20of%203D%0Asemantic%20entities%2C%20specified%20using%20a%20simple%20and%20intuitive%20interface%20that%0Aincludes%20a%20user%20click%20or%20a%20text%20prompt.%20To%20this%20end%2C%20we%20present%20DGD%2C%20a%20unified%0A3D%20representation%20for%20both%20the%20appearance%20and%20semantics%20of%20a%20dynamic%203D%20scene%2C%0Abuilding%20upon%20the%20recently%20proposed%20dynamic%203D%20Gaussians%20representation.%20Our%0Arepresentation%20is%20optimized%20over%20time%20with%20both%20color%20and%20semantic%20information.%0AKey%20to%20our%20method%20is%20the%20joint%20optimization%20of%20the%20appearance%20and%20semantic%0Aattributes%2C%20which%20jointly%20affect%20the%20geometric%20properties%20of%20the%20scene.%20We%0Aevaluate%20our%20approach%20in%20its%20ability%20to%20enable%20dense%20semantic%203D%20object%0Atracking%20and%20demonstrate%20high-quality%20results%20that%20are%20fast%20to%20render%2C%20for%20a%0Adiverse%20set%20of%20scenes.%20Our%20project%20webpage%20is%20available%20on%0Ahttps%3A//isaaclabe.github.io/DGD-Website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGD%253A%2520Dynamic%25203D%2520Gaussians%2520Distillation%26entry.906535625%3DIsaac%2520Labe%2520and%2520Noam%2520Issachar%2520and%2520Itai%2520Lang%2520and%2520Sagie%2520Benaim%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520task%2520of%2520learning%2520dynamic%25203D%2520semantic%2520radiance%2520fields%2520given%2520a%250Asingle%2520monocular%2520video%2520as%2520input.%2520Our%2520learned%2520semantic%2520radiance%2520field%2520captures%250Aper-point%2520semantics%2520as%2520well%2520as%2520color%2520and%2520geometric%2520properties%2520for%2520a%2520dynamic%25203D%250Ascene%252C%2520enabling%2520the%2520generation%2520of%2520novel%2520views%2520and%2520their%2520corresponding%250Asemantics.%2520This%2520enables%2520the%2520segmentation%2520and%2520tracking%2520of%2520a%2520diverse%2520set%2520of%25203D%250Asemantic%2520entities%252C%2520specified%2520using%2520a%2520simple%2520and%2520intuitive%2520interface%2520that%250Aincludes%2520a%2520user%2520click%2520or%2520a%2520text%2520prompt.%2520To%2520this%2520end%252C%2520we%2520present%2520DGD%252C%2520a%2520unified%250A3D%2520representation%2520for%2520both%2520the%2520appearance%2520and%2520semantics%2520of%2520a%2520dynamic%25203D%2520scene%252C%250Abuilding%2520upon%2520the%2520recently%2520proposed%2520dynamic%25203D%2520Gaussians%2520representation.%2520Our%250Arepresentation%2520is%2520optimized%2520over%2520time%2520with%2520both%2520color%2520and%2520semantic%2520information.%250AKey%2520to%2520our%2520method%2520is%2520the%2520joint%2520optimization%2520of%2520the%2520appearance%2520and%2520semantic%250Aattributes%252C%2520which%2520jointly%2520affect%2520the%2520geometric%2520properties%2520of%2520the%2520scene.%2520We%250Aevaluate%2520our%2520approach%2520in%2520its%2520ability%2520to%2520enable%2520dense%2520semantic%25203D%2520object%250Atracking%2520and%2520demonstrate%2520high-quality%2520results%2520that%2520are%2520fast%2520to%2520render%252C%2520for%2520a%250Adiverse%2520set%2520of%2520scenes.%2520Our%2520project%2520webpage%2520is%2520available%2520on%250Ahttps%253A//isaaclabe.github.io/DGD-Website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGD%3A%20Dynamic%203D%20Gaussians%20Distillation&entry.906535625=Isaac%20Labe%20and%20Noam%20Issachar%20and%20Itai%20Lang%20and%20Sagie%20Benaim&entry.1292438233=%20%20We%20tackle%20the%20task%20of%20learning%20dynamic%203D%20semantic%20radiance%20fields%20given%20a%0Asingle%20monocular%20video%20as%20input.%20Our%20learned%20semantic%20radiance%20field%20captures%0Aper-point%20semantics%20as%20well%20as%20color%20and%20geometric%20properties%20for%20a%20dynamic%203D%0Ascene%2C%20enabling%20the%20generation%20of%20novel%20views%20and%20their%20corresponding%0Asemantics.%20This%20enables%20the%20segmentation%20and%20tracking%20of%20a%20diverse%20set%20of%203D%0Asemantic%20entities%2C%20specified%20using%20a%20simple%20and%20intuitive%20interface%20that%0Aincludes%20a%20user%20click%20or%20a%20text%20prompt.%20To%20this%20end%2C%20we%20present%20DGD%2C%20a%20unified%0A3D%20representation%20for%20both%20the%20appearance%20and%20semantics%20of%20a%20dynamic%203D%20scene%2C%0Abuilding%20upon%20the%20recently%20proposed%20dynamic%203D%20Gaussians%20representation.%20Our%0Arepresentation%20is%20optimized%20over%20time%20with%20both%20color%20and%20semantic%20information.%0AKey%20to%20our%20method%20is%20the%20joint%20optimization%20of%20the%20appearance%20and%20semantic%0Aattributes%2C%20which%20jointly%20affect%20the%20geometric%20properties%20of%20the%20scene.%20We%0Aevaluate%20our%20approach%20in%20its%20ability%20to%20enable%20dense%20semantic%203D%20object%0Atracking%20and%20demonstrate%20high-quality%20results%20that%20are%20fast%20to%20render%2C%20for%20a%0Adiverse%20set%20of%20scenes.%20Our%20project%20webpage%20is%20available%20on%0Ahttps%3A//isaaclabe.github.io/DGD-Website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19321v1&entry.124074799=Read"},
{"title": "On Exploring PDE Modeling for Point Cloud Video Representation Learning", "author": "Zhuoxu Huang and Zhenkun Fan and Tao Xu and Jungong Han", "abstract": "  Point cloud video representation learning is challenging due to complex\nstructures and unordered spatial arrangement. Traditional methods struggle with\nframe-to-frame correlations and point-wise correspondence tracking. Recently,\npartial differential equations (PDE) have provided a new perspective in\nuniformly solving spatial-temporal data information within certain constraints.\nWhile tracking tangible point correspondence remains challenging, we propose to\nformalize point cloud video representation learning as a PDE-solving problem.\nInspired by fluid analysis, where PDEs are used to solve the deformation of\nspatial shape over time, we employ PDE to solve the variations of spatial\npoints affected by temporal information. By modeling spatial-temporal\ncorrelations, we aim to regularize spatial variations with temporal features,\nthereby enhancing representation learning in point cloud videos. We introduce\nMotion PointNet composed of a PointNet-like encoder and a PDE-solving module.\nInitially, we construct a lightweight yet effective encoder to model an initial\nstate of the spatial variations. Subsequently, we develop our PDE-solving\nmodule in a parameterized latent space, tailored to address the spatio-temporal\ncorrelations inherent in point cloud video. The process of solving PDE is\nguided and refined by a contrastive learning structure, which is pivotal in\nreshaping the feature distribution, thereby optimizing the feature\nrepresentation within point cloud video data. Remarkably, our Motion PointNet\nachieves an impressive accuracy of 97.52% on the MSRAction-3D dataset,\nsurpassing the current state-of-the-art in all aspects while consuming minimal\nresources (only 0.72M parameters and 0.82G FLOPs).\n", "link": "http://arxiv.org/abs/2404.04720v2", "date": "2024-05-29", "relevancy": 3.0045, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6184}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6084}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Exploring%20PDE%20Modeling%20for%20Point%20Cloud%20Video%20Representation%20Learning&body=Title%3A%20On%20Exploring%20PDE%20Modeling%20for%20Point%20Cloud%20Video%20Representation%20Learning%0AAuthor%3A%20Zhuoxu%20Huang%20and%20Zhenkun%20Fan%20and%20Tao%20Xu%20and%20Jungong%20Han%0AAbstract%3A%20%20%20Point%20cloud%20video%20representation%20learning%20is%20challenging%20due%20to%20complex%0Astructures%20and%20unordered%20spatial%20arrangement.%20Traditional%20methods%20struggle%20with%0Aframe-to-frame%20correlations%20and%20point-wise%20correspondence%20tracking.%20Recently%2C%0Apartial%20differential%20equations%20%28PDE%29%20have%20provided%20a%20new%20perspective%20in%0Auniformly%20solving%20spatial-temporal%20data%20information%20within%20certain%20constraints.%0AWhile%20tracking%20tangible%20point%20correspondence%20remains%20challenging%2C%20we%20propose%20to%0Aformalize%20point%20cloud%20video%20representation%20learning%20as%20a%20PDE-solving%20problem.%0AInspired%20by%20fluid%20analysis%2C%20where%20PDEs%20are%20used%20to%20solve%20the%20deformation%20of%0Aspatial%20shape%20over%20time%2C%20we%20employ%20PDE%20to%20solve%20the%20variations%20of%20spatial%0Apoints%20affected%20by%20temporal%20information.%20By%20modeling%20spatial-temporal%0Acorrelations%2C%20we%20aim%20to%20regularize%20spatial%20variations%20with%20temporal%20features%2C%0Athereby%20enhancing%20representation%20learning%20in%20point%20cloud%20videos.%20We%20introduce%0AMotion%20PointNet%20composed%20of%20a%20PointNet-like%20encoder%20and%20a%20PDE-solving%20module.%0AInitially%2C%20we%20construct%20a%20lightweight%20yet%20effective%20encoder%20to%20model%20an%20initial%0Astate%20of%20the%20spatial%20variations.%20Subsequently%2C%20we%20develop%20our%20PDE-solving%0Amodule%20in%20a%20parameterized%20latent%20space%2C%20tailored%20to%20address%20the%20spatio-temporal%0Acorrelations%20inherent%20in%20point%20cloud%20video.%20The%20process%20of%20solving%20PDE%20is%0Aguided%20and%20refined%20by%20a%20contrastive%20learning%20structure%2C%20which%20is%20pivotal%20in%0Areshaping%20the%20feature%20distribution%2C%20thereby%20optimizing%20the%20feature%0Arepresentation%20within%20point%20cloud%20video%20data.%20Remarkably%2C%20our%20Motion%20PointNet%0Aachieves%20an%20impressive%20accuracy%20of%2097.52%25%20on%20the%20MSRAction-3D%20dataset%2C%0Asurpassing%20the%20current%20state-of-the-art%20in%20all%20aspects%20while%20consuming%20minimal%0Aresources%20%28only%200.72M%20parameters%20and%200.82G%20FLOPs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Exploring%2520PDE%2520Modeling%2520for%2520Point%2520Cloud%2520Video%2520Representation%2520Learning%26entry.906535625%3DZhuoxu%2520Huang%2520and%2520Zhenkun%2520Fan%2520and%2520Tao%2520Xu%2520and%2520Jungong%2520Han%26entry.1292438233%3D%2520%2520Point%2520cloud%2520video%2520representation%2520learning%2520is%2520challenging%2520due%2520to%2520complex%250Astructures%2520and%2520unordered%2520spatial%2520arrangement.%2520Traditional%2520methods%2520struggle%2520with%250Aframe-to-frame%2520correlations%2520and%2520point-wise%2520correspondence%2520tracking.%2520Recently%252C%250Apartial%2520differential%2520equations%2520%2528PDE%2529%2520have%2520provided%2520a%2520new%2520perspective%2520in%250Auniformly%2520solving%2520spatial-temporal%2520data%2520information%2520within%2520certain%2520constraints.%250AWhile%2520tracking%2520tangible%2520point%2520correspondence%2520remains%2520challenging%252C%2520we%2520propose%2520to%250Aformalize%2520point%2520cloud%2520video%2520representation%2520learning%2520as%2520a%2520PDE-solving%2520problem.%250AInspired%2520by%2520fluid%2520analysis%252C%2520where%2520PDEs%2520are%2520used%2520to%2520solve%2520the%2520deformation%2520of%250Aspatial%2520shape%2520over%2520time%252C%2520we%2520employ%2520PDE%2520to%2520solve%2520the%2520variations%2520of%2520spatial%250Apoints%2520affected%2520by%2520temporal%2520information.%2520By%2520modeling%2520spatial-temporal%250Acorrelations%252C%2520we%2520aim%2520to%2520regularize%2520spatial%2520variations%2520with%2520temporal%2520features%252C%250Athereby%2520enhancing%2520representation%2520learning%2520in%2520point%2520cloud%2520videos.%2520We%2520introduce%250AMotion%2520PointNet%2520composed%2520of%2520a%2520PointNet-like%2520encoder%2520and%2520a%2520PDE-solving%2520module.%250AInitially%252C%2520we%2520construct%2520a%2520lightweight%2520yet%2520effective%2520encoder%2520to%2520model%2520an%2520initial%250Astate%2520of%2520the%2520spatial%2520variations.%2520Subsequently%252C%2520we%2520develop%2520our%2520PDE-solving%250Amodule%2520in%2520a%2520parameterized%2520latent%2520space%252C%2520tailored%2520to%2520address%2520the%2520spatio-temporal%250Acorrelations%2520inherent%2520in%2520point%2520cloud%2520video.%2520The%2520process%2520of%2520solving%2520PDE%2520is%250Aguided%2520and%2520refined%2520by%2520a%2520contrastive%2520learning%2520structure%252C%2520which%2520is%2520pivotal%2520in%250Areshaping%2520the%2520feature%2520distribution%252C%2520thereby%2520optimizing%2520the%2520feature%250Arepresentation%2520within%2520point%2520cloud%2520video%2520data.%2520Remarkably%252C%2520our%2520Motion%2520PointNet%250Aachieves%2520an%2520impressive%2520accuracy%2520of%252097.52%2525%2520on%2520the%2520MSRAction-3D%2520dataset%252C%250Asurpassing%2520the%2520current%2520state-of-the-art%2520in%2520all%2520aspects%2520while%2520consuming%2520minimal%250Aresources%2520%2528only%25200.72M%2520parameters%2520and%25200.82G%2520FLOPs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Exploring%20PDE%20Modeling%20for%20Point%20Cloud%20Video%20Representation%20Learning&entry.906535625=Zhuoxu%20Huang%20and%20Zhenkun%20Fan%20and%20Tao%20Xu%20and%20Jungong%20Han&entry.1292438233=%20%20Point%20cloud%20video%20representation%20learning%20is%20challenging%20due%20to%20complex%0Astructures%20and%20unordered%20spatial%20arrangement.%20Traditional%20methods%20struggle%20with%0Aframe-to-frame%20correlations%20and%20point-wise%20correspondence%20tracking.%20Recently%2C%0Apartial%20differential%20equations%20%28PDE%29%20have%20provided%20a%20new%20perspective%20in%0Auniformly%20solving%20spatial-temporal%20data%20information%20within%20certain%20constraints.%0AWhile%20tracking%20tangible%20point%20correspondence%20remains%20challenging%2C%20we%20propose%20to%0Aformalize%20point%20cloud%20video%20representation%20learning%20as%20a%20PDE-solving%20problem.%0AInspired%20by%20fluid%20analysis%2C%20where%20PDEs%20are%20used%20to%20solve%20the%20deformation%20of%0Aspatial%20shape%20over%20time%2C%20we%20employ%20PDE%20to%20solve%20the%20variations%20of%20spatial%0Apoints%20affected%20by%20temporal%20information.%20By%20modeling%20spatial-temporal%0Acorrelations%2C%20we%20aim%20to%20regularize%20spatial%20variations%20with%20temporal%20features%2C%0Athereby%20enhancing%20representation%20learning%20in%20point%20cloud%20videos.%20We%20introduce%0AMotion%20PointNet%20composed%20of%20a%20PointNet-like%20encoder%20and%20a%20PDE-solving%20module.%0AInitially%2C%20we%20construct%20a%20lightweight%20yet%20effective%20encoder%20to%20model%20an%20initial%0Astate%20of%20the%20spatial%20variations.%20Subsequently%2C%20we%20develop%20our%20PDE-solving%0Amodule%20in%20a%20parameterized%20latent%20space%2C%20tailored%20to%20address%20the%20spatio-temporal%0Acorrelations%20inherent%20in%20point%20cloud%20video.%20The%20process%20of%20solving%20PDE%20is%0Aguided%20and%20refined%20by%20a%20contrastive%20learning%20structure%2C%20which%20is%20pivotal%20in%0Areshaping%20the%20feature%20distribution%2C%20thereby%20optimizing%20the%20feature%0Arepresentation%20within%20point%20cloud%20video%20data.%20Remarkably%2C%20our%20Motion%20PointNet%0Aachieves%20an%20impressive%20accuracy%20of%2097.52%25%20on%20the%20MSRAction-3D%20dataset%2C%0Asurpassing%20the%20current%20state-of-the-art%20in%20all%20aspects%20while%20consuming%20minimal%0Aresources%20%28only%200.72M%20parameters%20and%200.82G%20FLOPs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04720v2&entry.124074799=Read"},
{"title": "Patch-enhanced Mask Encoder Prompt Image Generation", "author": "Shusong Xu and Peiye Liu", "abstract": "  Artificial Intelligence Generated Content(AIGC), known for its superior\nvisual results, represents a promising mitigation method for high-cost\nadvertising applications. Numerous approaches have been developed to manipulate\ngenerated content under different conditions. However, a crucial limitation\nlies in the accurate description of products in advertising applications.\nApplying previous methods directly may lead to considerable distortion and\ndeformation of advertised products, primarily due to oversimplified content\ncontrol conditions. Hence, in this work, we propose a patch-enhanced mask\nencoder approach to ensure accurate product descriptions while preserving\ndiverse backgrounds. Our approach consists of three components Patch Flexible\nVisibility, Mask Encoder Prompt Adapter and an image Foundation Model. Patch\nFlexible Visibility is used for generating a more reasonable background image.\nMask Encoder Prompt Adapter enables region-controlled fusion. We also conduct\nan analysis of the structure and operational mechanisms of the Generation\nModule. Experimental results show our method can achieve the highest visual\nresults and FID scores compared with other methods.\n", "link": "http://arxiv.org/abs/2405.19085v1", "date": "2024-05-29", "relevancy": 2.9722, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.618}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.604}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-enhanced%20Mask%20Encoder%20Prompt%20Image%20Generation&body=Title%3A%20Patch-enhanced%20Mask%20Encoder%20Prompt%20Image%20Generation%0AAuthor%3A%20Shusong%20Xu%20and%20Peiye%20Liu%0AAbstract%3A%20%20%20Artificial%20Intelligence%20Generated%20Content%28AIGC%29%2C%20known%20for%20its%20superior%0Avisual%20results%2C%20represents%20a%20promising%20mitigation%20method%20for%20high-cost%0Aadvertising%20applications.%20Numerous%20approaches%20have%20been%20developed%20to%20manipulate%0Agenerated%20content%20under%20different%20conditions.%20However%2C%20a%20crucial%20limitation%0Alies%20in%20the%20accurate%20description%20of%20products%20in%20advertising%20applications.%0AApplying%20previous%20methods%20directly%20may%20lead%20to%20considerable%20distortion%20and%0Adeformation%20of%20advertised%20products%2C%20primarily%20due%20to%20oversimplified%20content%0Acontrol%20conditions.%20Hence%2C%20in%20this%20work%2C%20we%20propose%20a%20patch-enhanced%20mask%0Aencoder%20approach%20to%20ensure%20accurate%20product%20descriptions%20while%20preserving%0Adiverse%20backgrounds.%20Our%20approach%20consists%20of%20three%20components%20Patch%20Flexible%0AVisibility%2C%20Mask%20Encoder%20Prompt%20Adapter%20and%20an%20image%20Foundation%20Model.%20Patch%0AFlexible%20Visibility%20is%20used%20for%20generating%20a%20more%20reasonable%20background%20image.%0AMask%20Encoder%20Prompt%20Adapter%20enables%20region-controlled%20fusion.%20We%20also%20conduct%0Aan%20analysis%20of%20the%20structure%20and%20operational%20mechanisms%20of%20the%20Generation%0AModule.%20Experimental%20results%20show%20our%20method%20can%20achieve%20the%20highest%20visual%0Aresults%20and%20FID%20scores%20compared%20with%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-enhanced%2520Mask%2520Encoder%2520Prompt%2520Image%2520Generation%26entry.906535625%3DShusong%2520Xu%2520and%2520Peiye%2520Liu%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520Generated%2520Content%2528AIGC%2529%252C%2520known%2520for%2520its%2520superior%250Avisual%2520results%252C%2520represents%2520a%2520promising%2520mitigation%2520method%2520for%2520high-cost%250Aadvertising%2520applications.%2520Numerous%2520approaches%2520have%2520been%2520developed%2520to%2520manipulate%250Agenerated%2520content%2520under%2520different%2520conditions.%2520However%252C%2520a%2520crucial%2520limitation%250Alies%2520in%2520the%2520accurate%2520description%2520of%2520products%2520in%2520advertising%2520applications.%250AApplying%2520previous%2520methods%2520directly%2520may%2520lead%2520to%2520considerable%2520distortion%2520and%250Adeformation%2520of%2520advertised%2520products%252C%2520primarily%2520due%2520to%2520oversimplified%2520content%250Acontrol%2520conditions.%2520Hence%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520patch-enhanced%2520mask%250Aencoder%2520approach%2520to%2520ensure%2520accurate%2520product%2520descriptions%2520while%2520preserving%250Adiverse%2520backgrounds.%2520Our%2520approach%2520consists%2520of%2520three%2520components%2520Patch%2520Flexible%250AVisibility%252C%2520Mask%2520Encoder%2520Prompt%2520Adapter%2520and%2520an%2520image%2520Foundation%2520Model.%2520Patch%250AFlexible%2520Visibility%2520is%2520used%2520for%2520generating%2520a%2520more%2520reasonable%2520background%2520image.%250AMask%2520Encoder%2520Prompt%2520Adapter%2520enables%2520region-controlled%2520fusion.%2520We%2520also%2520conduct%250Aan%2520analysis%2520of%2520the%2520structure%2520and%2520operational%2520mechanisms%2520of%2520the%2520Generation%250AModule.%2520Experimental%2520results%2520show%2520our%2520method%2520can%2520achieve%2520the%2520highest%2520visual%250Aresults%2520and%2520FID%2520scores%2520compared%2520with%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-enhanced%20Mask%20Encoder%20Prompt%20Image%20Generation&entry.906535625=Shusong%20Xu%20and%20Peiye%20Liu&entry.1292438233=%20%20Artificial%20Intelligence%20Generated%20Content%28AIGC%29%2C%20known%20for%20its%20superior%0Avisual%20results%2C%20represents%20a%20promising%20mitigation%20method%20for%20high-cost%0Aadvertising%20applications.%20Numerous%20approaches%20have%20been%20developed%20to%20manipulate%0Agenerated%20content%20under%20different%20conditions.%20However%2C%20a%20crucial%20limitation%0Alies%20in%20the%20accurate%20description%20of%20products%20in%20advertising%20applications.%0AApplying%20previous%20methods%20directly%20may%20lead%20to%20considerable%20distortion%20and%0Adeformation%20of%20advertised%20products%2C%20primarily%20due%20to%20oversimplified%20content%0Acontrol%20conditions.%20Hence%2C%20in%20this%20work%2C%20we%20propose%20a%20patch-enhanced%20mask%0Aencoder%20approach%20to%20ensure%20accurate%20product%20descriptions%20while%20preserving%0Adiverse%20backgrounds.%20Our%20approach%20consists%20of%20three%20components%20Patch%20Flexible%0AVisibility%2C%20Mask%20Encoder%20Prompt%20Adapter%20and%20an%20image%20Foundation%20Model.%20Patch%0AFlexible%20Visibility%20is%20used%20for%20generating%20a%20more%20reasonable%20background%20image.%0AMask%20Encoder%20Prompt%20Adapter%20enables%20region-controlled%20fusion.%20We%20also%20conduct%0Aan%20analysis%20of%20the%20structure%20and%20operational%20mechanisms%20of%20the%20Generation%0AModule.%20Experimental%20results%20show%20our%20method%20can%20achieve%20the%20highest%20visual%0Aresults%20and%20FID%20scores%20compared%20with%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19085v1&entry.124074799=Read"},
{"title": "PointNetPGAP-SLC: A 3D LiDAR-based Place Recognition Approach with\n  Segment-level Consistency Training for Mobile Robots in Horticulture", "author": "T. Barros and L. Garrote and P. Conde and M. J. Coombes and C. Liu and C. Premebida and U. J. Nunes", "abstract": "  This paper addresses robotic place recognition in horticultural environments\nusing 3D-LiDAR technology and deep learning. Three main contributions are\nproposed: (i) a novel model called PointNetPGAP, which combines a global\naverage pooling aggregator and a pairwise feature interaction aggregator; (ii)\na Segment-Level Consistency (SLC) model, used only during training, with the\ngoal of augmenting the contrastive loss with a context-specific training signal\nto enhance descriptors; and (iii) a novel dataset named HORTO-3DLM featuring\nsequences from orchards and strawberry plantations. The experimental\nevaluation, conducted on the new HORTO-3DLM dataset, compares PointNetPGAP at\nthe sequence- and segment-level with state-of-the-art (SOTA) models, including\nOverlapTransformer, PointNetVLAD, and LOGG3D. Additionally, all models were\ntrained and evaluated using the SLC. Empirical results obtained through a\ncross-validation evaluation protocol demonstrate the superiority of\nPointNetPGAP compared to existing SOTA models. PointNetPGAP emerges as the best\nmodel in retrieving the top-1 candidate, outperforming PointNetVLAD (the\nsecond-best model). Moreover, when comparing the impact of training with the\nSLC model, performance increased on four out of the five evaluated models,\nindicating that adding a context-specific signal to the contrastive loss leads\nto improved descriptors.\n", "link": "http://arxiv.org/abs/2405.19038v1", "date": "2024-05-29", "relevancy": 2.9498, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5919}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointNetPGAP-SLC%3A%20A%203D%20LiDAR-based%20Place%20Recognition%20Approach%20with%0A%20%20Segment-level%20Consistency%20Training%20for%20Mobile%20Robots%20in%20Horticulture&body=Title%3A%20PointNetPGAP-SLC%3A%20A%203D%20LiDAR-based%20Place%20Recognition%20Approach%20with%0A%20%20Segment-level%20Consistency%20Training%20for%20Mobile%20Robots%20in%20Horticulture%0AAuthor%3A%20T.%20Barros%20and%20L.%20Garrote%20and%20P.%20Conde%20and%20M.%20J.%20Coombes%20and%20C.%20Liu%20and%20C.%20Premebida%20and%20U.%20J.%20Nunes%0AAbstract%3A%20%20%20This%20paper%20addresses%20robotic%20place%20recognition%20in%20horticultural%20environments%0Ausing%203D-LiDAR%20technology%20and%20deep%20learning.%20Three%20main%20contributions%20are%0Aproposed%3A%20%28i%29%20a%20novel%20model%20called%20PointNetPGAP%2C%20which%20combines%20a%20global%0Aaverage%20pooling%20aggregator%20and%20a%20pairwise%20feature%20interaction%20aggregator%3B%20%28ii%29%0Aa%20Segment-Level%20Consistency%20%28SLC%29%20model%2C%20used%20only%20during%20training%2C%20with%20the%0Agoal%20of%20augmenting%20the%20contrastive%20loss%20with%20a%20context-specific%20training%20signal%0Ato%20enhance%20descriptors%3B%20and%20%28iii%29%20a%20novel%20dataset%20named%20HORTO-3DLM%20featuring%0Asequences%20from%20orchards%20and%20strawberry%20plantations.%20The%20experimental%0Aevaluation%2C%20conducted%20on%20the%20new%20HORTO-3DLM%20dataset%2C%20compares%20PointNetPGAP%20at%0Athe%20sequence-%20and%20segment-level%20with%20state-of-the-art%20%28SOTA%29%20models%2C%20including%0AOverlapTransformer%2C%20PointNetVLAD%2C%20and%20LOGG3D.%20Additionally%2C%20all%20models%20were%0Atrained%20and%20evaluated%20using%20the%20SLC.%20Empirical%20results%20obtained%20through%20a%0Across-validation%20evaluation%20protocol%20demonstrate%20the%20superiority%20of%0APointNetPGAP%20compared%20to%20existing%20SOTA%20models.%20PointNetPGAP%20emerges%20as%20the%20best%0Amodel%20in%20retrieving%20the%20top-1%20candidate%2C%20outperforming%20PointNetVLAD%20%28the%0Asecond-best%20model%29.%20Moreover%2C%20when%20comparing%20the%20impact%20of%20training%20with%20the%0ASLC%20model%2C%20performance%20increased%20on%20four%20out%20of%20the%20five%20evaluated%20models%2C%0Aindicating%20that%20adding%20a%20context-specific%20signal%20to%20the%20contrastive%20loss%20leads%0Ato%20improved%20descriptors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointNetPGAP-SLC%253A%2520A%25203D%2520LiDAR-based%2520Place%2520Recognition%2520Approach%2520with%250A%2520%2520Segment-level%2520Consistency%2520Training%2520for%2520Mobile%2520Robots%2520in%2520Horticulture%26entry.906535625%3DT.%2520Barros%2520and%2520L.%2520Garrote%2520and%2520P.%2520Conde%2520and%2520M.%2520J.%2520Coombes%2520and%2520C.%2520Liu%2520and%2520C.%2520Premebida%2520and%2520U.%2520J.%2520Nunes%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520robotic%2520place%2520recognition%2520in%2520horticultural%2520environments%250Ausing%25203D-LiDAR%2520technology%2520and%2520deep%2520learning.%2520Three%2520main%2520contributions%2520are%250Aproposed%253A%2520%2528i%2529%2520a%2520novel%2520model%2520called%2520PointNetPGAP%252C%2520which%2520combines%2520a%2520global%250Aaverage%2520pooling%2520aggregator%2520and%2520a%2520pairwise%2520feature%2520interaction%2520aggregator%253B%2520%2528ii%2529%250Aa%2520Segment-Level%2520Consistency%2520%2528SLC%2529%2520model%252C%2520used%2520only%2520during%2520training%252C%2520with%2520the%250Agoal%2520of%2520augmenting%2520the%2520contrastive%2520loss%2520with%2520a%2520context-specific%2520training%2520signal%250Ato%2520enhance%2520descriptors%253B%2520and%2520%2528iii%2529%2520a%2520novel%2520dataset%2520named%2520HORTO-3DLM%2520featuring%250Asequences%2520from%2520orchards%2520and%2520strawberry%2520plantations.%2520The%2520experimental%250Aevaluation%252C%2520conducted%2520on%2520the%2520new%2520HORTO-3DLM%2520dataset%252C%2520compares%2520PointNetPGAP%2520at%250Athe%2520sequence-%2520and%2520segment-level%2520with%2520state-of-the-art%2520%2528SOTA%2529%2520models%252C%2520including%250AOverlapTransformer%252C%2520PointNetVLAD%252C%2520and%2520LOGG3D.%2520Additionally%252C%2520all%2520models%2520were%250Atrained%2520and%2520evaluated%2520using%2520the%2520SLC.%2520Empirical%2520results%2520obtained%2520through%2520a%250Across-validation%2520evaluation%2520protocol%2520demonstrate%2520the%2520superiority%2520of%250APointNetPGAP%2520compared%2520to%2520existing%2520SOTA%2520models.%2520PointNetPGAP%2520emerges%2520as%2520the%2520best%250Amodel%2520in%2520retrieving%2520the%2520top-1%2520candidate%252C%2520outperforming%2520PointNetVLAD%2520%2528the%250Asecond-best%2520model%2529.%2520Moreover%252C%2520when%2520comparing%2520the%2520impact%2520of%2520training%2520with%2520the%250ASLC%2520model%252C%2520performance%2520increased%2520on%2520four%2520out%2520of%2520the%2520five%2520evaluated%2520models%252C%250Aindicating%2520that%2520adding%2520a%2520context-specific%2520signal%2520to%2520the%2520contrastive%2520loss%2520leads%250Ato%2520improved%2520descriptors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointNetPGAP-SLC%3A%20A%203D%20LiDAR-based%20Place%20Recognition%20Approach%20with%0A%20%20Segment-level%20Consistency%20Training%20for%20Mobile%20Robots%20in%20Horticulture&entry.906535625=T.%20Barros%20and%20L.%20Garrote%20and%20P.%20Conde%20and%20M.%20J.%20Coombes%20and%20C.%20Liu%20and%20C.%20Premebida%20and%20U.%20J.%20Nunes&entry.1292438233=%20%20This%20paper%20addresses%20robotic%20place%20recognition%20in%20horticultural%20environments%0Ausing%203D-LiDAR%20technology%20and%20deep%20learning.%20Three%20main%20contributions%20are%0Aproposed%3A%20%28i%29%20a%20novel%20model%20called%20PointNetPGAP%2C%20which%20combines%20a%20global%0Aaverage%20pooling%20aggregator%20and%20a%20pairwise%20feature%20interaction%20aggregator%3B%20%28ii%29%0Aa%20Segment-Level%20Consistency%20%28SLC%29%20model%2C%20used%20only%20during%20training%2C%20with%20the%0Agoal%20of%20augmenting%20the%20contrastive%20loss%20with%20a%20context-specific%20training%20signal%0Ato%20enhance%20descriptors%3B%20and%20%28iii%29%20a%20novel%20dataset%20named%20HORTO-3DLM%20featuring%0Asequences%20from%20orchards%20and%20strawberry%20plantations.%20The%20experimental%0Aevaluation%2C%20conducted%20on%20the%20new%20HORTO-3DLM%20dataset%2C%20compares%20PointNetPGAP%20at%0Athe%20sequence-%20and%20segment-level%20with%20state-of-the-art%20%28SOTA%29%20models%2C%20including%0AOverlapTransformer%2C%20PointNetVLAD%2C%20and%20LOGG3D.%20Additionally%2C%20all%20models%20were%0Atrained%20and%20evaluated%20using%20the%20SLC.%20Empirical%20results%20obtained%20through%20a%0Across-validation%20evaluation%20protocol%20demonstrate%20the%20superiority%20of%0APointNetPGAP%20compared%20to%20existing%20SOTA%20models.%20PointNetPGAP%20emerges%20as%20the%20best%0Amodel%20in%20retrieving%20the%20top-1%20candidate%2C%20outperforming%20PointNetVLAD%20%28the%0Asecond-best%20model%29.%20Moreover%2C%20when%20comparing%20the%20impact%20of%20training%20with%20the%0ASLC%20model%2C%20performance%20increased%20on%20four%20out%20of%20the%20five%20evaluated%20models%2C%0Aindicating%20that%20adding%20a%20context-specific%20signal%20to%20the%20contrastive%20loss%20leads%0Ato%20improved%20descriptors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19038v1&entry.124074799=Read"},
{"title": "Textureless Deformable Surface Reconstruction with Invisible Markers", "author": "Xinyuan Li and Yu Guo and Yubei Tu and Yu Ji and Yanchen Liu and Jinwei Ye and Changxi Zheng", "abstract": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "link": "http://arxiv.org/abs/2308.13678v2", "date": "2024-05-29", "relevancy": 2.8385, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5737}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textureless%20Deformable%20Surface%20Reconstruction%20with%20Invisible%20Markers&body=Title%3A%20Textureless%20Deformable%20Surface%20Reconstruction%20with%20Invisible%20Markers%0AAuthor%3A%20Xinyuan%20Li%20and%20Yu%20Guo%20and%20Yubei%20Tu%20and%20Yu%20Ji%20and%20Yanchen%20Liu%20and%20Jinwei%20Ye%20and%20Changxi%20Zheng%0AAbstract%3A%20%20%20Reconstructing%20and%20tracking%20deformable%20surface%20with%20little%20or%20no%20texture%20has%0Aposed%20long-standing%20challenges.%20Fundamentally%2C%20the%20challenges%20stem%20from%0Atextureless%20surfaces%20lacking%20features%20for%20establishing%20cross-image%0Acorrespondences.%20In%20this%20work%2C%20we%20present%20a%20novel%20type%20of%20markers%20to%0Aproactively%20enrich%20the%20object%27s%20surface%20features%2C%20and%20thereby%20ease%20the%203D%0Asurface%20reconstruction%20and%20correspondence%20tracking.%20Our%20markers%20are%20made%20of%0Afluorescent%20dyes%2C%20visible%20only%20under%20the%20ultraviolet%20%28UV%29%20light%20and%20invisible%0Aunder%20regular%20lighting%20condition.%20Leveraging%20the%20markers%2C%20we%20design%20a%0Amulti-camera%20system%20that%20captures%20surface%20deformation%20under%20the%20UV%20light%20and%0Athe%20visible%20light%20in%20a%20time%20multiplexing%20fashion.%20Under%20the%20UV%20light%2C%20markers%0Aon%20the%20object%20emerge%20to%20enrich%20its%20surface%20texture%2C%20allowing%20high-quality%203D%0Ashape%20reconstruction%20and%20tracking.%20Under%20the%20visible%20light%2C%20markers%20become%0Ainvisible%2C%20allowing%20us%20to%20capture%20the%20object%27s%20original%20untouched%20appearance.%0AWe%20perform%20experiments%20on%20various%20challenging%20scenes%2C%20including%20hand%20gestures%2C%0Afacial%20expressions%2C%20waving%20cloth%2C%20and%20hand-object%20interaction.%20In%20all%20these%0Acases%2C%20we%20demonstrate%20that%20our%20system%20is%20able%20to%20produce%20robust%2C%20high-quality%0A3D%20reconstruction%20and%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextureless%2520Deformable%2520Surface%2520Reconstruction%2520with%2520Invisible%2520Markers%26entry.906535625%3DXinyuan%2520Li%2520and%2520Yu%2520Guo%2520and%2520Yubei%2520Tu%2520and%2520Yu%2520Ji%2520and%2520Yanchen%2520Liu%2520and%2520Jinwei%2520Ye%2520and%2520Changxi%2520Zheng%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520tracking%2520deformable%2520surface%2520with%2520little%2520or%2520no%2520texture%2520has%250Aposed%2520long-standing%2520challenges.%2520Fundamentally%252C%2520the%2520challenges%2520stem%2520from%250Atextureless%2520surfaces%2520lacking%2520features%2520for%2520establishing%2520cross-image%250Acorrespondences.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520type%2520of%2520markers%2520to%250Aproactively%2520enrich%2520the%2520object%2527s%2520surface%2520features%252C%2520and%2520thereby%2520ease%2520the%25203D%250Asurface%2520reconstruction%2520and%2520correspondence%2520tracking.%2520Our%2520markers%2520are%2520made%2520of%250Afluorescent%2520dyes%252C%2520visible%2520only%2520under%2520the%2520ultraviolet%2520%2528UV%2529%2520light%2520and%2520invisible%250Aunder%2520regular%2520lighting%2520condition.%2520Leveraging%2520the%2520markers%252C%2520we%2520design%2520a%250Amulti-camera%2520system%2520that%2520captures%2520surface%2520deformation%2520under%2520the%2520UV%2520light%2520and%250Athe%2520visible%2520light%2520in%2520a%2520time%2520multiplexing%2520fashion.%2520Under%2520the%2520UV%2520light%252C%2520markers%250Aon%2520the%2520object%2520emerge%2520to%2520enrich%2520its%2520surface%2520texture%252C%2520allowing%2520high-quality%25203D%250Ashape%2520reconstruction%2520and%2520tracking.%2520Under%2520the%2520visible%2520light%252C%2520markers%2520become%250Ainvisible%252C%2520allowing%2520us%2520to%2520capture%2520the%2520object%2527s%2520original%2520untouched%2520appearance.%250AWe%2520perform%2520experiments%2520on%2520various%2520challenging%2520scenes%252C%2520including%2520hand%2520gestures%252C%250Afacial%2520expressions%252C%2520waving%2520cloth%252C%2520and%2520hand-object%2520interaction.%2520In%2520all%2520these%250Acases%252C%2520we%2520demonstrate%2520that%2520our%2520system%2520is%2520able%2520to%2520produce%2520robust%252C%2520high-quality%250A3D%2520reconstruction%2520and%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textureless%20Deformable%20Surface%20Reconstruction%20with%20Invisible%20Markers&entry.906535625=Xinyuan%20Li%20and%20Yu%20Guo%20and%20Yubei%20Tu%20and%20Yu%20Ji%20and%20Yanchen%20Liu%20and%20Jinwei%20Ye%20and%20Changxi%20Zheng&entry.1292438233=%20%20Reconstructing%20and%20tracking%20deformable%20surface%20with%20little%20or%20no%20texture%20has%0Aposed%20long-standing%20challenges.%20Fundamentally%2C%20the%20challenges%20stem%20from%0Atextureless%20surfaces%20lacking%20features%20for%20establishing%20cross-image%0Acorrespondences.%20In%20this%20work%2C%20we%20present%20a%20novel%20type%20of%20markers%20to%0Aproactively%20enrich%20the%20object%27s%20surface%20features%2C%20and%20thereby%20ease%20the%203D%0Asurface%20reconstruction%20and%20correspondence%20tracking.%20Our%20markers%20are%20made%20of%0Afluorescent%20dyes%2C%20visible%20only%20under%20the%20ultraviolet%20%28UV%29%20light%20and%20invisible%0Aunder%20regular%20lighting%20condition.%20Leveraging%20the%20markers%2C%20we%20design%20a%0Amulti-camera%20system%20that%20captures%20surface%20deformation%20under%20the%20UV%20light%20and%0Athe%20visible%20light%20in%20a%20time%20multiplexing%20fashion.%20Under%20the%20UV%20light%2C%20markers%0Aon%20the%20object%20emerge%20to%20enrich%20its%20surface%20texture%2C%20allowing%20high-quality%203D%0Ashape%20reconstruction%20and%20tracking.%20Under%20the%20visible%20light%2C%20markers%20become%0Ainvisible%2C%20allowing%20us%20to%20capture%20the%20object%27s%20original%20untouched%20appearance.%0AWe%20perform%20experiments%20on%20various%20challenging%20scenes%2C%20including%20hand%20gestures%2C%0Afacial%20expressions%2C%20waving%20cloth%2C%20and%20hand-object%20interaction.%20In%20all%20these%0Acases%2C%20we%20demonstrate%20that%20our%20system%20is%20able%20to%20produce%20robust%2C%20high-quality%0A3D%20reconstruction%20and%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13678v2&entry.124074799=Read"},
{"title": "Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise\n  Attention", "author": "Peng Li and Yuan Liu and Xiaoxiao Long and Feihu Zhang and Cheng Lin and Mengfei Li and Xingqun Qi and Shanghang Zhang and Wenhan Luo and Ping Tan and Wenping Wang and Qifeng Liu and Yike Guo", "abstract": "  In this paper, we introduce Era3D, a novel multiview diffusion method that\ngenerates high-resolution multiview images from a single-view image. Despite\nsignificant advancements in multiview generation, existing methods still suffer\nfrom camera prior mismatch, inefficacy, and low resolution, resulting in\npoor-quality multiview images. Specifically, these methods assume that the\ninput images should comply with a predefined camera type, e.g. a perspective\ncamera with a fixed focal length, leading to distorted shapes when the\nassumption fails. Moreover, the full-image or dense multiview attention they\nemploy leads to an exponential explosion of computational complexity as image\nresolution increases, resulting in prohibitively expensive training costs. To\nbridge the gap between assumption and reality, Era3D first proposes a\ndiffusion-based camera prediction module to estimate the focal length and\nelevation of the input image, which allows our method to generate images\nwithout shape distortions. Furthermore, a simple but efficient attention layer,\nnamed row-wise attention, is used to enforce epipolar priors in the multiview\ndiffusion, facilitating efficient cross-view information fusion. Consequently,\ncompared with state-of-the-art methods, Era3D generates high-quality multiview\nimages with up to a 512*512 resolution while reducing computation complexity by\n12x times. Comprehensive experiments demonstrate that Era3D can reconstruct\nhigh-quality and detailed 3D meshes from diverse single-view input images,\nsignificantly outperforming baseline multiview diffusion methods. Project page:\nhttps://penghtyx.github.io/Era3D/.\n", "link": "http://arxiv.org/abs/2405.11616v2", "date": "2024-05-29", "relevancy": 2.8258, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.729}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention&body=Title%3A%20Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention%0AAuthor%3A%20Peng%20Li%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Feihu%20Zhang%20and%20Cheng%20Lin%20and%20Mengfei%20Li%20and%20Xingqun%20Qi%20and%20Shanghang%20Zhang%20and%20Wenhan%20Luo%20and%20Ping%20Tan%20and%20Wenping%20Wang%20and%20Qifeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Era3D%2C%20a%20novel%20multiview%20diffusion%20method%20that%0Agenerates%20high-resolution%20multiview%20images%20from%20a%20single-view%20image.%20Despite%0Asignificant%20advancements%20in%20multiview%20generation%2C%20existing%20methods%20still%20suffer%0Afrom%20camera%20prior%20mismatch%2C%20inefficacy%2C%20and%20low%20resolution%2C%20resulting%20in%0Apoor-quality%20multiview%20images.%20Specifically%2C%20these%20methods%20assume%20that%20the%0Ainput%20images%20should%20comply%20with%20a%20predefined%20camera%20type%2C%20e.g.%20a%20perspective%0Acamera%20with%20a%20fixed%20focal%20length%2C%20leading%20to%20distorted%20shapes%20when%20the%0Aassumption%20fails.%20Moreover%2C%20the%20full-image%20or%20dense%20multiview%20attention%20they%0Aemploy%20leads%20to%20an%20exponential%20explosion%20of%20computational%20complexity%20as%20image%0Aresolution%20increases%2C%20resulting%20in%20prohibitively%20expensive%20training%20costs.%20To%0Abridge%20the%20gap%20between%20assumption%20and%20reality%2C%20Era3D%20first%20proposes%20a%0Adiffusion-based%20camera%20prediction%20module%20to%20estimate%20the%20focal%20length%20and%0Aelevation%20of%20the%20input%20image%2C%20which%20allows%20our%20method%20to%20generate%20images%0Awithout%20shape%20distortions.%20Furthermore%2C%20a%20simple%20but%20efficient%20attention%20layer%2C%0Anamed%20row-wise%20attention%2C%20is%20used%20to%20enforce%20epipolar%20priors%20in%20the%20multiview%0Adiffusion%2C%20facilitating%20efficient%20cross-view%20information%20fusion.%20Consequently%2C%0Acompared%20with%20state-of-the-art%20methods%2C%20Era3D%20generates%20high-quality%20multiview%0Aimages%20with%20up%20to%20a%20512%2A512%20resolution%20while%20reducing%20computation%20complexity%20by%0A12x%20times.%20Comprehensive%20experiments%20demonstrate%20that%20Era3D%20can%20reconstruct%0Ahigh-quality%20and%20detailed%203D%20meshes%20from%20diverse%20single-view%20input%20images%2C%0Asignificantly%20outperforming%20baseline%20multiview%20diffusion%20methods.%20Project%20page%3A%0Ahttps%3A//penghtyx.github.io/Era3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEra3D%253A%2520High-Resolution%2520Multiview%2520Diffusion%2520using%2520Efficient%2520Row-wise%250A%2520%2520Attention%26entry.906535625%3DPeng%2520Li%2520and%2520Yuan%2520Liu%2520and%2520Xiaoxiao%2520Long%2520and%2520Feihu%2520Zhang%2520and%2520Cheng%2520Lin%2520and%2520Mengfei%2520Li%2520and%2520Xingqun%2520Qi%2520and%2520Shanghang%2520Zhang%2520and%2520Wenhan%2520Luo%2520and%2520Ping%2520Tan%2520and%2520Wenping%2520Wang%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Era3D%252C%2520a%2520novel%2520multiview%2520diffusion%2520method%2520that%250Agenerates%2520high-resolution%2520multiview%2520images%2520from%2520a%2520single-view%2520image.%2520Despite%250Asignificant%2520advancements%2520in%2520multiview%2520generation%252C%2520existing%2520methods%2520still%2520suffer%250Afrom%2520camera%2520prior%2520mismatch%252C%2520inefficacy%252C%2520and%2520low%2520resolution%252C%2520resulting%2520in%250Apoor-quality%2520multiview%2520images.%2520Specifically%252C%2520these%2520methods%2520assume%2520that%2520the%250Ainput%2520images%2520should%2520comply%2520with%2520a%2520predefined%2520camera%2520type%252C%2520e.g.%2520a%2520perspective%250Acamera%2520with%2520a%2520fixed%2520focal%2520length%252C%2520leading%2520to%2520distorted%2520shapes%2520when%2520the%250Aassumption%2520fails.%2520Moreover%252C%2520the%2520full-image%2520or%2520dense%2520multiview%2520attention%2520they%250Aemploy%2520leads%2520to%2520an%2520exponential%2520explosion%2520of%2520computational%2520complexity%2520as%2520image%250Aresolution%2520increases%252C%2520resulting%2520in%2520prohibitively%2520expensive%2520training%2520costs.%2520To%250Abridge%2520the%2520gap%2520between%2520assumption%2520and%2520reality%252C%2520Era3D%2520first%2520proposes%2520a%250Adiffusion-based%2520camera%2520prediction%2520module%2520to%2520estimate%2520the%2520focal%2520length%2520and%250Aelevation%2520of%2520the%2520input%2520image%252C%2520which%2520allows%2520our%2520method%2520to%2520generate%2520images%250Awithout%2520shape%2520distortions.%2520Furthermore%252C%2520a%2520simple%2520but%2520efficient%2520attention%2520layer%252C%250Anamed%2520row-wise%2520attention%252C%2520is%2520used%2520to%2520enforce%2520epipolar%2520priors%2520in%2520the%2520multiview%250Adiffusion%252C%2520facilitating%2520efficient%2520cross-view%2520information%2520fusion.%2520Consequently%252C%250Acompared%2520with%2520state-of-the-art%2520methods%252C%2520Era3D%2520generates%2520high-quality%2520multiview%250Aimages%2520with%2520up%2520to%2520a%2520512%252A512%2520resolution%2520while%2520reducing%2520computation%2520complexity%2520by%250A12x%2520times.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520Era3D%2520can%2520reconstruct%250Ahigh-quality%2520and%2520detailed%25203D%2520meshes%2520from%2520diverse%2520single-view%2520input%2520images%252C%250Asignificantly%2520outperforming%2520baseline%2520multiview%2520diffusion%2520methods.%2520Project%2520page%253A%250Ahttps%253A//penghtyx.github.io/Era3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention&entry.906535625=Peng%20Li%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Feihu%20Zhang%20and%20Cheng%20Lin%20and%20Mengfei%20Li%20and%20Xingqun%20Qi%20and%20Shanghang%20Zhang%20and%20Wenhan%20Luo%20and%20Ping%20Tan%20and%20Wenping%20Wang%20and%20Qifeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Era3D%2C%20a%20novel%20multiview%20diffusion%20method%20that%0Agenerates%20high-resolution%20multiview%20images%20from%20a%20single-view%20image.%20Despite%0Asignificant%20advancements%20in%20multiview%20generation%2C%20existing%20methods%20still%20suffer%0Afrom%20camera%20prior%20mismatch%2C%20inefficacy%2C%20and%20low%20resolution%2C%20resulting%20in%0Apoor-quality%20multiview%20images.%20Specifically%2C%20these%20methods%20assume%20that%20the%0Ainput%20images%20should%20comply%20with%20a%20predefined%20camera%20type%2C%20e.g.%20a%20perspective%0Acamera%20with%20a%20fixed%20focal%20length%2C%20leading%20to%20distorted%20shapes%20when%20the%0Aassumption%20fails.%20Moreover%2C%20the%20full-image%20or%20dense%20multiview%20attention%20they%0Aemploy%20leads%20to%20an%20exponential%20explosion%20of%20computational%20complexity%20as%20image%0Aresolution%20increases%2C%20resulting%20in%20prohibitively%20expensive%20training%20costs.%20To%0Abridge%20the%20gap%20between%20assumption%20and%20reality%2C%20Era3D%20first%20proposes%20a%0Adiffusion-based%20camera%20prediction%20module%20to%20estimate%20the%20focal%20length%20and%0Aelevation%20of%20the%20input%20image%2C%20which%20allows%20our%20method%20to%20generate%20images%0Awithout%20shape%20distortions.%20Furthermore%2C%20a%20simple%20but%20efficient%20attention%20layer%2C%0Anamed%20row-wise%20attention%2C%20is%20used%20to%20enforce%20epipolar%20priors%20in%20the%20multiview%0Adiffusion%2C%20facilitating%20efficient%20cross-view%20information%20fusion.%20Consequently%2C%0Acompared%20with%20state-of-the-art%20methods%2C%20Era3D%20generates%20high-quality%20multiview%0Aimages%20with%20up%20to%20a%20512%2A512%20resolution%20while%20reducing%20computation%20complexity%20by%0A12x%20times.%20Comprehensive%20experiments%20demonstrate%20that%20Era3D%20can%20reconstruct%0Ahigh-quality%20and%20detailed%203D%20meshes%20from%20diverse%20single-view%20input%20images%2C%0Asignificantly%20outperforming%20baseline%20multiview%20diffusion%20methods.%20Project%20page%3A%0Ahttps%3A//penghtyx.github.io/Era3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11616v2&entry.124074799=Read"},
{"title": "Multimodal Unsupervised Domain Generalization by Retrieving Across the\n  Modality Gap", "author": "Christopher Liao and Christian So and Theodoros Tsiligkaridis and Brian Kulis", "abstract": "  Domain generalization (DG) is an important problem that learns a model which\ngeneralizes to unseen test domains leveraging one or more source domains, under\nthe assumption of shared label spaces. However, most DG methods assume access\nto abundant source data in the target label space, a requirement that proves\noverly stringent for numerous real-world applications, where acquiring the same\nlabel space as the target task is prohibitively expensive. For this setting, we\ntackle the multimodal version of the unsupervised domain generalization (MUDG)\nproblem, which uses a large task-agnostic unlabeled source dataset during\nfinetuning. Our framework does not explicitly assume any relationship between\nthe source dataset and target task. Instead, it relies only on the premise that\nthe source dataset can be accurately and efficiently searched in a joint\nvision-language space. We make three contributions in the MUDG setting.\nFirstly, we show theoretically that cross-modal approximate nearest neighbor\nsearch suffers from low recall due to the large distance between text queries\nand the image centroids used for coarse quantization. Accordingly, we propose\npaired k-means, a simple clustering algorithm that improves nearest neighbor\nrecall by storing centroids in query space instead of image space. Secondly, we\npropose an adaptive text augmentation scheme for target labels designed to\nimprove zero-shot accuracy and diversify retrieved image data. Lastly, we\npresent two simple but effective components to further improve downstream\ntarget accuracy. We compare against state-of-the-art name-only transfer,\nsource-free DG and zero-shot (ZS) methods on their respective benchmarks and\nshow consistent improvement in accuracy on 20 diverse datasets. Code is\navailable: https://github.com/Chris210634/mudg\n", "link": "http://arxiv.org/abs/2402.04416v2", "date": "2024-05-29", "relevancy": 2.8044, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Unsupervised%20Domain%20Generalization%20by%20Retrieving%20Across%20the%0A%20%20Modality%20Gap&body=Title%3A%20Multimodal%20Unsupervised%20Domain%20Generalization%20by%20Retrieving%20Across%20the%0A%20%20Modality%20Gap%0AAuthor%3A%20Christopher%20Liao%20and%20Christian%20So%20and%20Theodoros%20Tsiligkaridis%20and%20Brian%20Kulis%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20is%20an%20important%20problem%20that%20learns%20a%20model%20which%0Ageneralizes%20to%20unseen%20test%20domains%20leveraging%20one%20or%20more%20source%20domains%2C%20under%0Athe%20assumption%20of%20shared%20label%20spaces.%20However%2C%20most%20DG%20methods%20assume%20access%0Ato%20abundant%20source%20data%20in%20the%20target%20label%20space%2C%20a%20requirement%20that%20proves%0Aoverly%20stringent%20for%20numerous%20real-world%20applications%2C%20where%20acquiring%20the%20same%0Alabel%20space%20as%20the%20target%20task%20is%20prohibitively%20expensive.%20For%20this%20setting%2C%20we%0Atackle%20the%20multimodal%20version%20of%20the%20unsupervised%20domain%20generalization%20%28MUDG%29%0Aproblem%2C%20which%20uses%20a%20large%20task-agnostic%20unlabeled%20source%20dataset%20during%0Afinetuning.%20Our%20framework%20does%20not%20explicitly%20assume%20any%20relationship%20between%0Athe%20source%20dataset%20and%20target%20task.%20Instead%2C%20it%20relies%20only%20on%20the%20premise%20that%0Athe%20source%20dataset%20can%20be%20accurately%20and%20efficiently%20searched%20in%20a%20joint%0Avision-language%20space.%20We%20make%20three%20contributions%20in%20the%20MUDG%20setting.%0AFirstly%2C%20we%20show%20theoretically%20that%20cross-modal%20approximate%20nearest%20neighbor%0Asearch%20suffers%20from%20low%20recall%20due%20to%20the%20large%20distance%20between%20text%20queries%0Aand%20the%20image%20centroids%20used%20for%20coarse%20quantization.%20Accordingly%2C%20we%20propose%0Apaired%20k-means%2C%20a%20simple%20clustering%20algorithm%20that%20improves%20nearest%20neighbor%0Arecall%20by%20storing%20centroids%20in%20query%20space%20instead%20of%20image%20space.%20Secondly%2C%20we%0Apropose%20an%20adaptive%20text%20augmentation%20scheme%20for%20target%20labels%20designed%20to%0Aimprove%20zero-shot%20accuracy%20and%20diversify%20retrieved%20image%20data.%20Lastly%2C%20we%0Apresent%20two%20simple%20but%20effective%20components%20to%20further%20improve%20downstream%0Atarget%20accuracy.%20We%20compare%20against%20state-of-the-art%20name-only%20transfer%2C%0Asource-free%20DG%20and%20zero-shot%20%28ZS%29%20methods%20on%20their%20respective%20benchmarks%20and%0Ashow%20consistent%20improvement%20in%20accuracy%20on%2020%20diverse%20datasets.%20Code%20is%0Aavailable%3A%20https%3A//github.com/Chris210634/mudg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Unsupervised%2520Domain%2520Generalization%2520by%2520Retrieving%2520Across%2520the%250A%2520%2520Modality%2520Gap%26entry.906535625%3DChristopher%2520Liao%2520and%2520Christian%2520So%2520and%2520Theodoros%2520Tsiligkaridis%2520and%2520Brian%2520Kulis%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520%2528DG%2529%2520is%2520an%2520important%2520problem%2520that%2520learns%2520a%2520model%2520which%250Ageneralizes%2520to%2520unseen%2520test%2520domains%2520leveraging%2520one%2520or%2520more%2520source%2520domains%252C%2520under%250Athe%2520assumption%2520of%2520shared%2520label%2520spaces.%2520However%252C%2520most%2520DG%2520methods%2520assume%2520access%250Ato%2520abundant%2520source%2520data%2520in%2520the%2520target%2520label%2520space%252C%2520a%2520requirement%2520that%2520proves%250Aoverly%2520stringent%2520for%2520numerous%2520real-world%2520applications%252C%2520where%2520acquiring%2520the%2520same%250Alabel%2520space%2520as%2520the%2520target%2520task%2520is%2520prohibitively%2520expensive.%2520For%2520this%2520setting%252C%2520we%250Atackle%2520the%2520multimodal%2520version%2520of%2520the%2520unsupervised%2520domain%2520generalization%2520%2528MUDG%2529%250Aproblem%252C%2520which%2520uses%2520a%2520large%2520task-agnostic%2520unlabeled%2520source%2520dataset%2520during%250Afinetuning.%2520Our%2520framework%2520does%2520not%2520explicitly%2520assume%2520any%2520relationship%2520between%250Athe%2520source%2520dataset%2520and%2520target%2520task.%2520Instead%252C%2520it%2520relies%2520only%2520on%2520the%2520premise%2520that%250Athe%2520source%2520dataset%2520can%2520be%2520accurately%2520and%2520efficiently%2520searched%2520in%2520a%2520joint%250Avision-language%2520space.%2520We%2520make%2520three%2520contributions%2520in%2520the%2520MUDG%2520setting.%250AFirstly%252C%2520we%2520show%2520theoretically%2520that%2520cross-modal%2520approximate%2520nearest%2520neighbor%250Asearch%2520suffers%2520from%2520low%2520recall%2520due%2520to%2520the%2520large%2520distance%2520between%2520text%2520queries%250Aand%2520the%2520image%2520centroids%2520used%2520for%2520coarse%2520quantization.%2520Accordingly%252C%2520we%2520propose%250Apaired%2520k-means%252C%2520a%2520simple%2520clustering%2520algorithm%2520that%2520improves%2520nearest%2520neighbor%250Arecall%2520by%2520storing%2520centroids%2520in%2520query%2520space%2520instead%2520of%2520image%2520space.%2520Secondly%252C%2520we%250Apropose%2520an%2520adaptive%2520text%2520augmentation%2520scheme%2520for%2520target%2520labels%2520designed%2520to%250Aimprove%2520zero-shot%2520accuracy%2520and%2520diversify%2520retrieved%2520image%2520data.%2520Lastly%252C%2520we%250Apresent%2520two%2520simple%2520but%2520effective%2520components%2520to%2520further%2520improve%2520downstream%250Atarget%2520accuracy.%2520We%2520compare%2520against%2520state-of-the-art%2520name-only%2520transfer%252C%250Asource-free%2520DG%2520and%2520zero-shot%2520%2528ZS%2529%2520methods%2520on%2520their%2520respective%2520benchmarks%2520and%250Ashow%2520consistent%2520improvement%2520in%2520accuracy%2520on%252020%2520diverse%2520datasets.%2520Code%2520is%250Aavailable%253A%2520https%253A//github.com/Chris210634/mudg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Unsupervised%20Domain%20Generalization%20by%20Retrieving%20Across%20the%0A%20%20Modality%20Gap&entry.906535625=Christopher%20Liao%20and%20Christian%20So%20and%20Theodoros%20Tsiligkaridis%20and%20Brian%20Kulis&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20is%20an%20important%20problem%20that%20learns%20a%20model%20which%0Ageneralizes%20to%20unseen%20test%20domains%20leveraging%20one%20or%20more%20source%20domains%2C%20under%0Athe%20assumption%20of%20shared%20label%20spaces.%20However%2C%20most%20DG%20methods%20assume%20access%0Ato%20abundant%20source%20data%20in%20the%20target%20label%20space%2C%20a%20requirement%20that%20proves%0Aoverly%20stringent%20for%20numerous%20real-world%20applications%2C%20where%20acquiring%20the%20same%0Alabel%20space%20as%20the%20target%20task%20is%20prohibitively%20expensive.%20For%20this%20setting%2C%20we%0Atackle%20the%20multimodal%20version%20of%20the%20unsupervised%20domain%20generalization%20%28MUDG%29%0Aproblem%2C%20which%20uses%20a%20large%20task-agnostic%20unlabeled%20source%20dataset%20during%0Afinetuning.%20Our%20framework%20does%20not%20explicitly%20assume%20any%20relationship%20between%0Athe%20source%20dataset%20and%20target%20task.%20Instead%2C%20it%20relies%20only%20on%20the%20premise%20that%0Athe%20source%20dataset%20can%20be%20accurately%20and%20efficiently%20searched%20in%20a%20joint%0Avision-language%20space.%20We%20make%20three%20contributions%20in%20the%20MUDG%20setting.%0AFirstly%2C%20we%20show%20theoretically%20that%20cross-modal%20approximate%20nearest%20neighbor%0Asearch%20suffers%20from%20low%20recall%20due%20to%20the%20large%20distance%20between%20text%20queries%0Aand%20the%20image%20centroids%20used%20for%20coarse%20quantization.%20Accordingly%2C%20we%20propose%0Apaired%20k-means%2C%20a%20simple%20clustering%20algorithm%20that%20improves%20nearest%20neighbor%0Arecall%20by%20storing%20centroids%20in%20query%20space%20instead%20of%20image%20space.%20Secondly%2C%20we%0Apropose%20an%20adaptive%20text%20augmentation%20scheme%20for%20target%20labels%20designed%20to%0Aimprove%20zero-shot%20accuracy%20and%20diversify%20retrieved%20image%20data.%20Lastly%2C%20we%0Apresent%20two%20simple%20but%20effective%20components%20to%20further%20improve%20downstream%0Atarget%20accuracy.%20We%20compare%20against%20state-of-the-art%20name-only%20transfer%2C%0Asource-free%20DG%20and%20zero-shot%20%28ZS%29%20methods%20on%20their%20respective%20benchmarks%20and%0Ashow%20consistent%20improvement%20in%20accuracy%20on%2020%20diverse%20datasets.%20Code%20is%0Aavailable%3A%20https%3A//github.com/Chris210634/mudg%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04416v2&entry.124074799=Read"},
{"title": "Neural Isometries: Taming Transformations for Equivariant ML", "author": "Thomas W. Mitchel and Michael Taylor and Vincent Sitzmann", "abstract": "  Real-world geometry and 3D vision tasks are replete with challenging\nsymmetries that defy tractable analytical expression. In this paper, we\nintroduce Neural Isometries, an autoencoder framework which learns to map the\nobservation space to a general-purpose latent space wherein encodings are\nrelated by isometries whenever their corresponding observations are\ngeometrically related in world space. Specifically, we regularize the latent\nspace such that maps between encodings preserve a learned inner product and\ncommute with a learned functional operator, in the same manner as rigid-body\ntransformations commute with the Laplacian. This approach forms an effective\nbackbone for self-supervised representation learning, and we demonstrate that a\nsimple off-the-shelf equivariant network operating in the pre-trained latent\nspace can achieve results on par with meticulously-engineered, handcrafted\nnetworks designed to handle complex, nonlinear symmetries. Furthermore,\nisometric maps capture information about the respective transformations in\nworld space, and we show that this allows us to regress camera poses directly\nfrom the coefficients of the maps between encodings of adjacent views of a\nscene.\n", "link": "http://arxiv.org/abs/2405.19296v1", "date": "2024-05-29", "relevancy": 2.7933, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5802}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5493}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Isometries%3A%20Taming%20Transformations%20for%20Equivariant%20ML&body=Title%3A%20Neural%20Isometries%3A%20Taming%20Transformations%20for%20Equivariant%20ML%0AAuthor%3A%20Thomas%20W.%20Mitchel%20and%20Michael%20Taylor%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20Real-world%20geometry%20and%203D%20vision%20tasks%20are%20replete%20with%20challenging%0Asymmetries%20that%20defy%20tractable%20analytical%20expression.%20In%20this%20paper%2C%20we%0Aintroduce%20Neural%20Isometries%2C%20an%20autoencoder%20framework%20which%20learns%20to%20map%20the%0Aobservation%20space%20to%20a%20general-purpose%20latent%20space%20wherein%20encodings%20are%0Arelated%20by%20isometries%20whenever%20their%20corresponding%20observations%20are%0Ageometrically%20related%20in%20world%20space.%20Specifically%2C%20we%20regularize%20the%20latent%0Aspace%20such%20that%20maps%20between%20encodings%20preserve%20a%20learned%20inner%20product%20and%0Acommute%20with%20a%20learned%20functional%20operator%2C%20in%20the%20same%20manner%20as%20rigid-body%0Atransformations%20commute%20with%20the%20Laplacian.%20This%20approach%20forms%20an%20effective%0Abackbone%20for%20self-supervised%20representation%20learning%2C%20and%20we%20demonstrate%20that%20a%0Asimple%20off-the-shelf%20equivariant%20network%20operating%20in%20the%20pre-trained%20latent%0Aspace%20can%20achieve%20results%20on%20par%20with%20meticulously-engineered%2C%20handcrafted%0Anetworks%20designed%20to%20handle%20complex%2C%20nonlinear%20symmetries.%20Furthermore%2C%0Aisometric%20maps%20capture%20information%20about%20the%20respective%20transformations%20in%0Aworld%20space%2C%20and%20we%20show%20that%20this%20allows%20us%20to%20regress%20camera%20poses%20directly%0Afrom%20the%20coefficients%20of%20the%20maps%20between%20encodings%20of%20adjacent%20views%20of%20a%0Ascene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Isometries%253A%2520Taming%2520Transformations%2520for%2520Equivariant%2520ML%26entry.906535625%3DThomas%2520W.%2520Mitchel%2520and%2520Michael%2520Taylor%2520and%2520Vincent%2520Sitzmann%26entry.1292438233%3D%2520%2520Real-world%2520geometry%2520and%25203D%2520vision%2520tasks%2520are%2520replete%2520with%2520challenging%250Asymmetries%2520that%2520defy%2520tractable%2520analytical%2520expression.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Neural%2520Isometries%252C%2520an%2520autoencoder%2520framework%2520which%2520learns%2520to%2520map%2520the%250Aobservation%2520space%2520to%2520a%2520general-purpose%2520latent%2520space%2520wherein%2520encodings%2520are%250Arelated%2520by%2520isometries%2520whenever%2520their%2520corresponding%2520observations%2520are%250Ageometrically%2520related%2520in%2520world%2520space.%2520Specifically%252C%2520we%2520regularize%2520the%2520latent%250Aspace%2520such%2520that%2520maps%2520between%2520encodings%2520preserve%2520a%2520learned%2520inner%2520product%2520and%250Acommute%2520with%2520a%2520learned%2520functional%2520operator%252C%2520in%2520the%2520same%2520manner%2520as%2520rigid-body%250Atransformations%2520commute%2520with%2520the%2520Laplacian.%2520This%2520approach%2520forms%2520an%2520effective%250Abackbone%2520for%2520self-supervised%2520representation%2520learning%252C%2520and%2520we%2520demonstrate%2520that%2520a%250Asimple%2520off-the-shelf%2520equivariant%2520network%2520operating%2520in%2520the%2520pre-trained%2520latent%250Aspace%2520can%2520achieve%2520results%2520on%2520par%2520with%2520meticulously-engineered%252C%2520handcrafted%250Anetworks%2520designed%2520to%2520handle%2520complex%252C%2520nonlinear%2520symmetries.%2520Furthermore%252C%250Aisometric%2520maps%2520capture%2520information%2520about%2520the%2520respective%2520transformations%2520in%250Aworld%2520space%252C%2520and%2520we%2520show%2520that%2520this%2520allows%2520us%2520to%2520regress%2520camera%2520poses%2520directly%250Afrom%2520the%2520coefficients%2520of%2520the%2520maps%2520between%2520encodings%2520of%2520adjacent%2520views%2520of%2520a%250Ascene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Isometries%3A%20Taming%20Transformations%20for%20Equivariant%20ML&entry.906535625=Thomas%20W.%20Mitchel%20and%20Michael%20Taylor%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20Real-world%20geometry%20and%203D%20vision%20tasks%20are%20replete%20with%20challenging%0Asymmetries%20that%20defy%20tractable%20analytical%20expression.%20In%20this%20paper%2C%20we%0Aintroduce%20Neural%20Isometries%2C%20an%20autoencoder%20framework%20which%20learns%20to%20map%20the%0Aobservation%20space%20to%20a%20general-purpose%20latent%20space%20wherein%20encodings%20are%0Arelated%20by%20isometries%20whenever%20their%20corresponding%20observations%20are%0Ageometrically%20related%20in%20world%20space.%20Specifically%2C%20we%20regularize%20the%20latent%0Aspace%20such%20that%20maps%20between%20encodings%20preserve%20a%20learned%20inner%20product%20and%0Acommute%20with%20a%20learned%20functional%20operator%2C%20in%20the%20same%20manner%20as%20rigid-body%0Atransformations%20commute%20with%20the%20Laplacian.%20This%20approach%20forms%20an%20effective%0Abackbone%20for%20self-supervised%20representation%20learning%2C%20and%20we%20demonstrate%20that%20a%0Asimple%20off-the-shelf%20equivariant%20network%20operating%20in%20the%20pre-trained%20latent%0Aspace%20can%20achieve%20results%20on%20par%20with%20meticulously-engineered%2C%20handcrafted%0Anetworks%20designed%20to%20handle%20complex%2C%20nonlinear%20symmetries.%20Furthermore%2C%0Aisometric%20maps%20capture%20information%20about%20the%20respective%20transformations%20in%0Aworld%20space%2C%20and%20we%20show%20that%20this%20allows%20us%20to%20regress%20camera%20poses%20directly%0Afrom%20the%20coefficients%20of%20the%20maps%20between%20encodings%20of%20adjacent%20views%20of%20a%0Ascene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19296v1&entry.124074799=Read"},
{"title": "Multi-Modal Generative Embedding Model", "author": "Feipeng Ma and Hongwei Xue and Guangting Wang and Yizhou Zhou and Fengyun Rao and Shilin Yan and Yueyi Zhang and Siying Wu and Mike Zheng Shou and Xiaoyan Sun", "abstract": "  Most multi-modal tasks can be formulated into problems of either generation\nor embedding. Existing models usually tackle these two types of problems by\ndecoupling language modules into a text decoder for generation, and a text\nencoder for embedding. To explore the minimalism of multi-modal paradigms, we\nattempt to achieve only one model per modality in this work. We propose a\nMulti-Modal Generative Embedding Model (MM-GEM), whereby the generative and\nembedding objectives are encapsulated in one Large Language Model. We also\npropose a PoolAggregator to boost efficiency and enable the ability of\nfine-grained embedding and generation. A surprising finding is that these two\nobjectives do not significantly conflict with each other. For example, MM-GEM\ninstantiated from ViT-Large and TinyLlama shows competitive performance on\nbenchmarks for multimodal embedding models such as cross-modal retrieval and\nzero-shot classification, while has good ability of image captioning.\nAdditionally, MM-GEM can seamlessly execute region-level image caption\ngeneration and retrieval tasks. Besides, the advanced text model in MM-GEM\nbrings over 5% improvement in Recall@1 for long text and image retrieval.\n", "link": "http://arxiv.org/abs/2405.19333v1", "date": "2024-05-29", "relevancy": 2.7586, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5607}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Generative%20Embedding%20Model&body=Title%3A%20Multi-Modal%20Generative%20Embedding%20Model%0AAuthor%3A%20Feipeng%20Ma%20and%20Hongwei%20Xue%20and%20Guangting%20Wang%20and%20Yizhou%20Zhou%20and%20Fengyun%20Rao%20and%20Shilin%20Yan%20and%20Yueyi%20Zhang%20and%20Siying%20Wu%20and%20Mike%20Zheng%20Shou%20and%20Xiaoyan%20Sun%0AAbstract%3A%20%20%20Most%20multi-modal%20tasks%20can%20be%20formulated%20into%20problems%20of%20either%20generation%0Aor%20embedding.%20Existing%20models%20usually%20tackle%20these%20two%20types%20of%20problems%20by%0Adecoupling%20language%20modules%20into%20a%20text%20decoder%20for%20generation%2C%20and%20a%20text%0Aencoder%20for%20embedding.%20To%20explore%20the%20minimalism%20of%20multi-modal%20paradigms%2C%20we%0Aattempt%20to%20achieve%20only%20one%20model%20per%20modality%20in%20this%20work.%20We%20propose%20a%0AMulti-Modal%20Generative%20Embedding%20Model%20%28MM-GEM%29%2C%20whereby%20the%20generative%20and%0Aembedding%20objectives%20are%20encapsulated%20in%20one%20Large%20Language%20Model.%20We%20also%0Apropose%20a%20PoolAggregator%20to%20boost%20efficiency%20and%20enable%20the%20ability%20of%0Afine-grained%20embedding%20and%20generation.%20A%20surprising%20finding%20is%20that%20these%20two%0Aobjectives%20do%20not%20significantly%20conflict%20with%20each%20other.%20For%20example%2C%20MM-GEM%0Ainstantiated%20from%20ViT-Large%20and%20TinyLlama%20shows%20competitive%20performance%20on%0Abenchmarks%20for%20multimodal%20embedding%20models%20such%20as%20cross-modal%20retrieval%20and%0Azero-shot%20classification%2C%20while%20has%20good%20ability%20of%20image%20captioning.%0AAdditionally%2C%20MM-GEM%20can%20seamlessly%20execute%20region-level%20image%20caption%0Ageneration%20and%20retrieval%20tasks.%20Besides%2C%20the%20advanced%20text%20model%20in%20MM-GEM%0Abrings%20over%205%25%20improvement%20in%20Recall%401%20for%20long%20text%20and%20image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Generative%2520Embedding%2520Model%26entry.906535625%3DFeipeng%2520Ma%2520and%2520Hongwei%2520Xue%2520and%2520Guangting%2520Wang%2520and%2520Yizhou%2520Zhou%2520and%2520Fengyun%2520Rao%2520and%2520Shilin%2520Yan%2520and%2520Yueyi%2520Zhang%2520and%2520Siying%2520Wu%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3D%2520%2520Most%2520multi-modal%2520tasks%2520can%2520be%2520formulated%2520into%2520problems%2520of%2520either%2520generation%250Aor%2520embedding.%2520Existing%2520models%2520usually%2520tackle%2520these%2520two%2520types%2520of%2520problems%2520by%250Adecoupling%2520language%2520modules%2520into%2520a%2520text%2520decoder%2520for%2520generation%252C%2520and%2520a%2520text%250Aencoder%2520for%2520embedding.%2520To%2520explore%2520the%2520minimalism%2520of%2520multi-modal%2520paradigms%252C%2520we%250Aattempt%2520to%2520achieve%2520only%2520one%2520model%2520per%2520modality%2520in%2520this%2520work.%2520We%2520propose%2520a%250AMulti-Modal%2520Generative%2520Embedding%2520Model%2520%2528MM-GEM%2529%252C%2520whereby%2520the%2520generative%2520and%250Aembedding%2520objectives%2520are%2520encapsulated%2520in%2520one%2520Large%2520Language%2520Model.%2520We%2520also%250Apropose%2520a%2520PoolAggregator%2520to%2520boost%2520efficiency%2520and%2520enable%2520the%2520ability%2520of%250Afine-grained%2520embedding%2520and%2520generation.%2520A%2520surprising%2520finding%2520is%2520that%2520these%2520two%250Aobjectives%2520do%2520not%2520significantly%2520conflict%2520with%2520each%2520other.%2520For%2520example%252C%2520MM-GEM%250Ainstantiated%2520from%2520ViT-Large%2520and%2520TinyLlama%2520shows%2520competitive%2520performance%2520on%250Abenchmarks%2520for%2520multimodal%2520embedding%2520models%2520such%2520as%2520cross-modal%2520retrieval%2520and%250Azero-shot%2520classification%252C%2520while%2520has%2520good%2520ability%2520of%2520image%2520captioning.%250AAdditionally%252C%2520MM-GEM%2520can%2520seamlessly%2520execute%2520region-level%2520image%2520caption%250Ageneration%2520and%2520retrieval%2520tasks.%2520Besides%252C%2520the%2520advanced%2520text%2520model%2520in%2520MM-GEM%250Abrings%2520over%25205%2525%2520improvement%2520in%2520Recall%25401%2520for%2520long%2520text%2520and%2520image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Generative%20Embedding%20Model&entry.906535625=Feipeng%20Ma%20and%20Hongwei%20Xue%20and%20Guangting%20Wang%20and%20Yizhou%20Zhou%20and%20Fengyun%20Rao%20and%20Shilin%20Yan%20and%20Yueyi%20Zhang%20and%20Siying%20Wu%20and%20Mike%20Zheng%20Shou%20and%20Xiaoyan%20Sun&entry.1292438233=%20%20Most%20multi-modal%20tasks%20can%20be%20formulated%20into%20problems%20of%20either%20generation%0Aor%20embedding.%20Existing%20models%20usually%20tackle%20these%20two%20types%20of%20problems%20by%0Adecoupling%20language%20modules%20into%20a%20text%20decoder%20for%20generation%2C%20and%20a%20text%0Aencoder%20for%20embedding.%20To%20explore%20the%20minimalism%20of%20multi-modal%20paradigms%2C%20we%0Aattempt%20to%20achieve%20only%20one%20model%20per%20modality%20in%20this%20work.%20We%20propose%20a%0AMulti-Modal%20Generative%20Embedding%20Model%20%28MM-GEM%29%2C%20whereby%20the%20generative%20and%0Aembedding%20objectives%20are%20encapsulated%20in%20one%20Large%20Language%20Model.%20We%20also%0Apropose%20a%20PoolAggregator%20to%20boost%20efficiency%20and%20enable%20the%20ability%20of%0Afine-grained%20embedding%20and%20generation.%20A%20surprising%20finding%20is%20that%20these%20two%0Aobjectives%20do%20not%20significantly%20conflict%20with%20each%20other.%20For%20example%2C%20MM-GEM%0Ainstantiated%20from%20ViT-Large%20and%20TinyLlama%20shows%20competitive%20performance%20on%0Abenchmarks%20for%20multimodal%20embedding%20models%20such%20as%20cross-modal%20retrieval%20and%0Azero-shot%20classification%2C%20while%20has%20good%20ability%20of%20image%20captioning.%0AAdditionally%2C%20MM-GEM%20can%20seamlessly%20execute%20region-level%20image%20caption%0Ageneration%20and%20retrieval%20tasks.%20Besides%2C%20the%20advanced%20text%20model%20in%20MM-GEM%0Abrings%20over%205%25%20improvement%20in%20Recall%401%20for%20long%20text%20and%20image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19333v1&entry.124074799=Read"},
{"title": "Exploring AI-based Anonymization of Industrial Image and Video Data in\n  the Context of Feature Preservation", "author": "Sabrina Cynthia Triess and Timo Leitritz and Christian Jauch", "abstract": "  With rising technologies, the protection of privacy-sensitive information is\nbecoming increasingly important. In industry and production facilities, image\nor video recordings are beneficial for documentation, tracing production errors\nor coordinating workflows. Individuals in images or videos need to be\nanonymized. However, the anonymized data should be reusable for further\napplications. In this work, we apply the Deep Learning-based full-body\nanonymization framework DeepPrivacy2, which generates artificial identities, to\nindustrial image and video data. We compare its performance with conventional\nanonymization techniques. Therefore, we consider the quality of identity\ngeneration, temporal consistency, and the applicability of pose estimation and\naction recognition.\n", "link": "http://arxiv.org/abs/2405.19173v1", "date": "2024-05-29", "relevancy": 2.7427, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5684}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.54}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20AI-based%20Anonymization%20of%20Industrial%20Image%20and%20Video%20Data%20in%0A%20%20the%20Context%20of%20Feature%20Preservation&body=Title%3A%20Exploring%20AI-based%20Anonymization%20of%20Industrial%20Image%20and%20Video%20Data%20in%0A%20%20the%20Context%20of%20Feature%20Preservation%0AAuthor%3A%20Sabrina%20Cynthia%20Triess%20and%20Timo%20Leitritz%20and%20Christian%20Jauch%0AAbstract%3A%20%20%20With%20rising%20technologies%2C%20the%20protection%20of%20privacy-sensitive%20information%20is%0Abecoming%20increasingly%20important.%20In%20industry%20and%20production%20facilities%2C%20image%0Aor%20video%20recordings%20are%20beneficial%20for%20documentation%2C%20tracing%20production%20errors%0Aor%20coordinating%20workflows.%20Individuals%20in%20images%20or%20videos%20need%20to%20be%0Aanonymized.%20However%2C%20the%20anonymized%20data%20should%20be%20reusable%20for%20further%0Aapplications.%20In%20this%20work%2C%20we%20apply%20the%20Deep%20Learning-based%20full-body%0Aanonymization%20framework%20DeepPrivacy2%2C%20which%20generates%20artificial%20identities%2C%20to%0Aindustrial%20image%20and%20video%20data.%20We%20compare%20its%20performance%20with%20conventional%0Aanonymization%20techniques.%20Therefore%2C%20we%20consider%20the%20quality%20of%20identity%0Ageneration%2C%20temporal%20consistency%2C%20and%20the%20applicability%20of%20pose%20estimation%20and%0Aaction%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520AI-based%2520Anonymization%2520of%2520Industrial%2520Image%2520and%2520Video%2520Data%2520in%250A%2520%2520the%2520Context%2520of%2520Feature%2520Preservation%26entry.906535625%3DSabrina%2520Cynthia%2520Triess%2520and%2520Timo%2520Leitritz%2520and%2520Christian%2520Jauch%26entry.1292438233%3D%2520%2520With%2520rising%2520technologies%252C%2520the%2520protection%2520of%2520privacy-sensitive%2520information%2520is%250Abecoming%2520increasingly%2520important.%2520In%2520industry%2520and%2520production%2520facilities%252C%2520image%250Aor%2520video%2520recordings%2520are%2520beneficial%2520for%2520documentation%252C%2520tracing%2520production%2520errors%250Aor%2520coordinating%2520workflows.%2520Individuals%2520in%2520images%2520or%2520videos%2520need%2520to%2520be%250Aanonymized.%2520However%252C%2520the%2520anonymized%2520data%2520should%2520be%2520reusable%2520for%2520further%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520apply%2520the%2520Deep%2520Learning-based%2520full-body%250Aanonymization%2520framework%2520DeepPrivacy2%252C%2520which%2520generates%2520artificial%2520identities%252C%2520to%250Aindustrial%2520image%2520and%2520video%2520data.%2520We%2520compare%2520its%2520performance%2520with%2520conventional%250Aanonymization%2520techniques.%2520Therefore%252C%2520we%2520consider%2520the%2520quality%2520of%2520identity%250Ageneration%252C%2520temporal%2520consistency%252C%2520and%2520the%2520applicability%2520of%2520pose%2520estimation%2520and%250Aaction%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20AI-based%20Anonymization%20of%20Industrial%20Image%20and%20Video%20Data%20in%0A%20%20the%20Context%20of%20Feature%20Preservation&entry.906535625=Sabrina%20Cynthia%20Triess%20and%20Timo%20Leitritz%20and%20Christian%20Jauch&entry.1292438233=%20%20With%20rising%20technologies%2C%20the%20protection%20of%20privacy-sensitive%20information%20is%0Abecoming%20increasingly%20important.%20In%20industry%20and%20production%20facilities%2C%20image%0Aor%20video%20recordings%20are%20beneficial%20for%20documentation%2C%20tracing%20production%20errors%0Aor%20coordinating%20workflows.%20Individuals%20in%20images%20or%20videos%20need%20to%20be%0Aanonymized.%20However%2C%20the%20anonymized%20data%20should%20be%20reusable%20for%20further%0Aapplications.%20In%20this%20work%2C%20we%20apply%20the%20Deep%20Learning-based%20full-body%0Aanonymization%20framework%20DeepPrivacy2%2C%20which%20generates%20artificial%20identities%2C%20to%0Aindustrial%20image%20and%20video%20data.%20We%20compare%20its%20performance%20with%20conventional%0Aanonymization%20techniques.%20Therefore%2C%20we%20consider%20the%20quality%20of%20identity%0Ageneration%2C%20temporal%20consistency%2C%20and%20the%20applicability%20of%20pose%20estimation%20and%0Aaction%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19173v1&entry.124074799=Read"},
{"title": "PointMamba: A Simple State Space Model for Point Cloud Analysis", "author": "Dingkang Liang and Xin Zhou and Wei Xu and Xingkui Zhu and Zhikang Zou and Xiaoqing Ye and Xiao Tan and Xiang Bai", "abstract": "  Transformers have become one of the foundational architectures in point cloud\nanalysis tasks due to their excellent global modeling ability. However, the\nattention mechanism has quadratic complexity, making the design of a linear\ncomplexity method with global modeling appealing. In this paper, we propose\nPointMamba, transferring the success of Mamba, a recent representative state\nspace model (SSM), from NLP to point cloud analysis tasks. Unlike traditional\nTransformers, PointMamba employs a linear complexity algorithm, presenting\nglobal modeling capacity while significantly reducing computational costs.\nSpecifically, our method leverages space-filling curves for effective point\ntokenization and adopts an extremely simple, non-hierarchical Mamba encoder as\nthe backbone. Comprehensive evaluations demonstrate that PointMamba achieves\nsuperior performance across multiple datasets while significantly reducing GPU\nmemory usage and FLOPs. This work underscores the potential of SSMs in 3D\nvision-related tasks and presents a simple yet effective Mamba-based baseline\nfor future research. The code is available at\nhttps://github.com/LMD0311/PointMamba.\n", "link": "http://arxiv.org/abs/2402.10739v4", "date": "2024-05-29", "relevancy": 2.7344, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5522}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5459}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointMamba%3A%20A%20Simple%20State%20Space%20Model%20for%20Point%20Cloud%20Analysis&body=Title%3A%20PointMamba%3A%20A%20Simple%20State%20Space%20Model%20for%20Point%20Cloud%20Analysis%0AAuthor%3A%20Dingkang%20Liang%20and%20Xin%20Zhou%20and%20Wei%20Xu%20and%20Xingkui%20Zhu%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Xiao%20Tan%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Transformers%20have%20become%20one%20of%20the%20foundational%20architectures%20in%20point%20cloud%0Aanalysis%20tasks%20due%20to%20their%20excellent%20global%20modeling%20ability.%20However%2C%20the%0Aattention%20mechanism%20has%20quadratic%20complexity%2C%20making%20the%20design%20of%20a%20linear%0Acomplexity%20method%20with%20global%20modeling%20appealing.%20In%20this%20paper%2C%20we%20propose%0APointMamba%2C%20transferring%20the%20success%20of%20Mamba%2C%20a%20recent%20representative%20state%0Aspace%20model%20%28SSM%29%2C%20from%20NLP%20to%20point%20cloud%20analysis%20tasks.%20Unlike%20traditional%0ATransformers%2C%20PointMamba%20employs%20a%20linear%20complexity%20algorithm%2C%20presenting%0Aglobal%20modeling%20capacity%20while%20significantly%20reducing%20computational%20costs.%0ASpecifically%2C%20our%20method%20leverages%20space-filling%20curves%20for%20effective%20point%0Atokenization%20and%20adopts%20an%20extremely%20simple%2C%20non-hierarchical%20Mamba%20encoder%20as%0Athe%20backbone.%20Comprehensive%20evaluations%20demonstrate%20that%20PointMamba%20achieves%0Asuperior%20performance%20across%20multiple%20datasets%20while%20significantly%20reducing%20GPU%0Amemory%20usage%20and%20FLOPs.%20This%20work%20underscores%20the%20potential%20of%20SSMs%20in%203D%0Avision-related%20tasks%20and%20presents%20a%20simple%20yet%20effective%20Mamba-based%20baseline%0Afor%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LMD0311/PointMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10739v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointMamba%253A%2520A%2520Simple%2520State%2520Space%2520Model%2520for%2520Point%2520Cloud%2520Analysis%26entry.906535625%3DDingkang%2520Liang%2520and%2520Xin%2520Zhou%2520and%2520Wei%2520Xu%2520and%2520Xingkui%2520Zhu%2520and%2520Zhikang%2520Zou%2520and%2520Xiaoqing%2520Ye%2520and%2520Xiao%2520Tan%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Transformers%2520have%2520become%2520one%2520of%2520the%2520foundational%2520architectures%2520in%2520point%2520cloud%250Aanalysis%2520tasks%2520due%2520to%2520their%2520excellent%2520global%2520modeling%2520ability.%2520However%252C%2520the%250Aattention%2520mechanism%2520has%2520quadratic%2520complexity%252C%2520making%2520the%2520design%2520of%2520a%2520linear%250Acomplexity%2520method%2520with%2520global%2520modeling%2520appealing.%2520In%2520this%2520paper%252C%2520we%2520propose%250APointMamba%252C%2520transferring%2520the%2520success%2520of%2520Mamba%252C%2520a%2520recent%2520representative%2520state%250Aspace%2520model%2520%2528SSM%2529%252C%2520from%2520NLP%2520to%2520point%2520cloud%2520analysis%2520tasks.%2520Unlike%2520traditional%250ATransformers%252C%2520PointMamba%2520employs%2520a%2520linear%2520complexity%2520algorithm%252C%2520presenting%250Aglobal%2520modeling%2520capacity%2520while%2520significantly%2520reducing%2520computational%2520costs.%250ASpecifically%252C%2520our%2520method%2520leverages%2520space-filling%2520curves%2520for%2520effective%2520point%250Atokenization%2520and%2520adopts%2520an%2520extremely%2520simple%252C%2520non-hierarchical%2520Mamba%2520encoder%2520as%250Athe%2520backbone.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520PointMamba%2520achieves%250Asuperior%2520performance%2520across%2520multiple%2520datasets%2520while%2520significantly%2520reducing%2520GPU%250Amemory%2520usage%2520and%2520FLOPs.%2520This%2520work%2520underscores%2520the%2520potential%2520of%2520SSMs%2520in%25203D%250Avision-related%2520tasks%2520and%2520presents%2520a%2520simple%2520yet%2520effective%2520Mamba-based%2520baseline%250Afor%2520future%2520research.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LMD0311/PointMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10739v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointMamba%3A%20A%20Simple%20State%20Space%20Model%20for%20Point%20Cloud%20Analysis&entry.906535625=Dingkang%20Liang%20and%20Xin%20Zhou%20and%20Wei%20Xu%20and%20Xingkui%20Zhu%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Xiao%20Tan%20and%20Xiang%20Bai&entry.1292438233=%20%20Transformers%20have%20become%20one%20of%20the%20foundational%20architectures%20in%20point%20cloud%0Aanalysis%20tasks%20due%20to%20their%20excellent%20global%20modeling%20ability.%20However%2C%20the%0Aattention%20mechanism%20has%20quadratic%20complexity%2C%20making%20the%20design%20of%20a%20linear%0Acomplexity%20method%20with%20global%20modeling%20appealing.%20In%20this%20paper%2C%20we%20propose%0APointMamba%2C%20transferring%20the%20success%20of%20Mamba%2C%20a%20recent%20representative%20state%0Aspace%20model%20%28SSM%29%2C%20from%20NLP%20to%20point%20cloud%20analysis%20tasks.%20Unlike%20traditional%0ATransformers%2C%20PointMamba%20employs%20a%20linear%20complexity%20algorithm%2C%20presenting%0Aglobal%20modeling%20capacity%20while%20significantly%20reducing%20computational%20costs.%0ASpecifically%2C%20our%20method%20leverages%20space-filling%20curves%20for%20effective%20point%0Atokenization%20and%20adopts%20an%20extremely%20simple%2C%20non-hierarchical%20Mamba%20encoder%20as%0Athe%20backbone.%20Comprehensive%20evaluations%20demonstrate%20that%20PointMamba%20achieves%0Asuperior%20performance%20across%20multiple%20datasets%20while%20significantly%20reducing%20GPU%0Amemory%20usage%20and%20FLOPs.%20This%20work%20underscores%20the%20potential%20of%20SSMs%20in%203D%0Avision-related%20tasks%20and%20presents%20a%20simple%20yet%20effective%20Mamba-based%20baseline%0Afor%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LMD0311/PointMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10739v4&entry.124074799=Read"},
{"title": "Resurrecting Old Classes with New Data for Exemplar-Free Continual\n  Learning", "author": "Dipam Goswami and Albin Soutif--Cormerais and Yuyang Liu and Sandesh Kamath and Bart\u0142omiej Twardowski and Joost van de Weijer", "abstract": "  Continual learning methods are known to suffer from catastrophic forgetting,\na phenomenon that is particularly hard to counter for methods that do not store\nexemplars of previous tasks. Therefore, to reduce potential drift in the\nfeature extractor, existing exemplar-free methods are typically evaluated in\nsettings where the first task is significantly larger than subsequent tasks.\nTheir performance drops drastically in more challenging settings starting with\na smaller first task. To address this problem of feature drift estimation for\nexemplar-free methods, we propose to adversarially perturb the current samples\nsuch that their embeddings are close to the old class prototypes in the old\nmodel embedding space. We then estimate the drift in the embedding space from\nthe old to the new model using the perturbed images and compensate the\nprototypes accordingly. We exploit the fact that adversarial samples are\ntransferable from the old to the new feature space in a continual learning\nsetting. The generation of these images is simple and computationally cheap. We\ndemonstrate in our experiments that the proposed approach better tracks the\nmovement of prototypes in embedding space and outperforms existing methods on\nseveral standard continual learning benchmarks as well as on fine-grained\ndatasets. Code is available at https://github.com/dipamgoswami/ADC.\n", "link": "http://arxiv.org/abs/2405.19074v1", "date": "2024-05-29", "relevancy": 2.6968, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resurrecting%20Old%20Classes%20with%20New%20Data%20for%20Exemplar-Free%20Continual%0A%20%20Learning&body=Title%3A%20Resurrecting%20Old%20Classes%20with%20New%20Data%20for%20Exemplar-Free%20Continual%0A%20%20Learning%0AAuthor%3A%20Dipam%20Goswami%20and%20Albin%20Soutif--Cormerais%20and%20Yuyang%20Liu%20and%20Sandesh%20Kamath%20and%20Bart%C5%82omiej%20Twardowski%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20Continual%20learning%20methods%20are%20known%20to%20suffer%20from%20catastrophic%20forgetting%2C%0Aa%20phenomenon%20that%20is%20particularly%20hard%20to%20counter%20for%20methods%20that%20do%20not%20store%0Aexemplars%20of%20previous%20tasks.%20Therefore%2C%20to%20reduce%20potential%20drift%20in%20the%0Afeature%20extractor%2C%20existing%20exemplar-free%20methods%20are%20typically%20evaluated%20in%0Asettings%20where%20the%20first%20task%20is%20significantly%20larger%20than%20subsequent%20tasks.%0ATheir%20performance%20drops%20drastically%20in%20more%20challenging%20settings%20starting%20with%0Aa%20smaller%20first%20task.%20To%20address%20this%20problem%20of%20feature%20drift%20estimation%20for%0Aexemplar-free%20methods%2C%20we%20propose%20to%20adversarially%20perturb%20the%20current%20samples%0Asuch%20that%20their%20embeddings%20are%20close%20to%20the%20old%20class%20prototypes%20in%20the%20old%0Amodel%20embedding%20space.%20We%20then%20estimate%20the%20drift%20in%20the%20embedding%20space%20from%0Athe%20old%20to%20the%20new%20model%20using%20the%20perturbed%20images%20and%20compensate%20the%0Aprototypes%20accordingly.%20We%20exploit%20the%20fact%20that%20adversarial%20samples%20are%0Atransferable%20from%20the%20old%20to%20the%20new%20feature%20space%20in%20a%20continual%20learning%0Asetting.%20The%20generation%20of%20these%20images%20is%20simple%20and%20computationally%20cheap.%20We%0Ademonstrate%20in%20our%20experiments%20that%20the%20proposed%20approach%20better%20tracks%20the%0Amovement%20of%20prototypes%20in%20embedding%20space%20and%20outperforms%20existing%20methods%20on%0Aseveral%20standard%20continual%20learning%20benchmarks%20as%20well%20as%20on%20fine-grained%0Adatasets.%20Code%20is%20available%20at%20https%3A//github.com/dipamgoswami/ADC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResurrecting%2520Old%2520Classes%2520with%2520New%2520Data%2520for%2520Exemplar-Free%2520Continual%250A%2520%2520Learning%26entry.906535625%3DDipam%2520Goswami%2520and%2520Albin%2520Soutif--Cormerais%2520and%2520Yuyang%2520Liu%2520and%2520Sandesh%2520Kamath%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520Continual%2520learning%2520methods%2520are%2520known%2520to%2520suffer%2520from%2520catastrophic%2520forgetting%252C%250Aa%2520phenomenon%2520that%2520is%2520particularly%2520hard%2520to%2520counter%2520for%2520methods%2520that%2520do%2520not%2520store%250Aexemplars%2520of%2520previous%2520tasks.%2520Therefore%252C%2520to%2520reduce%2520potential%2520drift%2520in%2520the%250Afeature%2520extractor%252C%2520existing%2520exemplar-free%2520methods%2520are%2520typically%2520evaluated%2520in%250Asettings%2520where%2520the%2520first%2520task%2520is%2520significantly%2520larger%2520than%2520subsequent%2520tasks.%250ATheir%2520performance%2520drops%2520drastically%2520in%2520more%2520challenging%2520settings%2520starting%2520with%250Aa%2520smaller%2520first%2520task.%2520To%2520address%2520this%2520problem%2520of%2520feature%2520drift%2520estimation%2520for%250Aexemplar-free%2520methods%252C%2520we%2520propose%2520to%2520adversarially%2520perturb%2520the%2520current%2520samples%250Asuch%2520that%2520their%2520embeddings%2520are%2520close%2520to%2520the%2520old%2520class%2520prototypes%2520in%2520the%2520old%250Amodel%2520embedding%2520space.%2520We%2520then%2520estimate%2520the%2520drift%2520in%2520the%2520embedding%2520space%2520from%250Athe%2520old%2520to%2520the%2520new%2520model%2520using%2520the%2520perturbed%2520images%2520and%2520compensate%2520the%250Aprototypes%2520accordingly.%2520We%2520exploit%2520the%2520fact%2520that%2520adversarial%2520samples%2520are%250Atransferable%2520from%2520the%2520old%2520to%2520the%2520new%2520feature%2520space%2520in%2520a%2520continual%2520learning%250Asetting.%2520The%2520generation%2520of%2520these%2520images%2520is%2520simple%2520and%2520computationally%2520cheap.%2520We%250Ademonstrate%2520in%2520our%2520experiments%2520that%2520the%2520proposed%2520approach%2520better%2520tracks%2520the%250Amovement%2520of%2520prototypes%2520in%2520embedding%2520space%2520and%2520outperforms%2520existing%2520methods%2520on%250Aseveral%2520standard%2520continual%2520learning%2520benchmarks%2520as%2520well%2520as%2520on%2520fine-grained%250Adatasets.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/dipamgoswami/ADC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resurrecting%20Old%20Classes%20with%20New%20Data%20for%20Exemplar-Free%20Continual%0A%20%20Learning&entry.906535625=Dipam%20Goswami%20and%20Albin%20Soutif--Cormerais%20and%20Yuyang%20Liu%20and%20Sandesh%20Kamath%20and%20Bart%C5%82omiej%20Twardowski%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20Continual%20learning%20methods%20are%20known%20to%20suffer%20from%20catastrophic%20forgetting%2C%0Aa%20phenomenon%20that%20is%20particularly%20hard%20to%20counter%20for%20methods%20that%20do%20not%20store%0Aexemplars%20of%20previous%20tasks.%20Therefore%2C%20to%20reduce%20potential%20drift%20in%20the%0Afeature%20extractor%2C%20existing%20exemplar-free%20methods%20are%20typically%20evaluated%20in%0Asettings%20where%20the%20first%20task%20is%20significantly%20larger%20than%20subsequent%20tasks.%0ATheir%20performance%20drops%20drastically%20in%20more%20challenging%20settings%20starting%20with%0Aa%20smaller%20first%20task.%20To%20address%20this%20problem%20of%20feature%20drift%20estimation%20for%0Aexemplar-free%20methods%2C%20we%20propose%20to%20adversarially%20perturb%20the%20current%20samples%0Asuch%20that%20their%20embeddings%20are%20close%20to%20the%20old%20class%20prototypes%20in%20the%20old%0Amodel%20embedding%20space.%20We%20then%20estimate%20the%20drift%20in%20the%20embedding%20space%20from%0Athe%20old%20to%20the%20new%20model%20using%20the%20perturbed%20images%20and%20compensate%20the%0Aprototypes%20accordingly.%20We%20exploit%20the%20fact%20that%20adversarial%20samples%20are%0Atransferable%20from%20the%20old%20to%20the%20new%20feature%20space%20in%20a%20continual%20learning%0Asetting.%20The%20generation%20of%20these%20images%20is%20simple%20and%20computationally%20cheap.%20We%0Ademonstrate%20in%20our%20experiments%20that%20the%20proposed%20approach%20better%20tracks%20the%0Amovement%20of%20prototypes%20in%20embedding%20space%20and%20outperforms%20existing%20methods%20on%0Aseveral%20standard%20continual%20learning%20benchmarks%20as%20well%20as%20on%20fine-grained%0Adatasets.%20Code%20is%20available%20at%20https%3A//github.com/dipamgoswami/ADC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19074v1&entry.124074799=Read"},
{"title": "X-VILA: Cross-Modality Alignment for Large Language Model", "author": "Hanrong Ye and De-An Huang and Yao Lu and Zhiding Yu and Wei Ping and Andrew Tao and Jan Kautz and Song Han and Dan Xu and Pavlo Molchanov and Hongxu Yin", "abstract": "  We introduce X-VILA, an omni-modality model designed to extend the\ncapabilities of large language models (LLMs) by incorporating image, video, and\naudio modalities. By aligning modality-specific encoders with LLM inputs and\ndiffusion decoders with LLM outputs, X-VILA achieves cross-modality\nunderstanding, reasoning, and generation. To facilitate this cross-modality\nalignment, we curate an effective interleaved any-to-any modality\ninstruction-following dataset. Furthermore, we identify a significant problem\nwith the current cross-modality alignment method, which results in visual\ninformation loss. To address the issue, we propose a visual alignment mechanism\nwith a visual embedding highway module. We then introduce a resource-efficient\nrecipe for training X-VILA, that exhibits proficiency in any-to-any modality\nconversation, surpassing previous approaches by large margins. X-VILA also\nshowcases emergent properties across modalities even in the absence of similar\ntraining data. The project will be made open-source.\n", "link": "http://arxiv.org/abs/2405.19335v1", "date": "2024-05-29", "relevancy": 2.6927, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5892}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5198}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-VILA%3A%20Cross-Modality%20Alignment%20for%20Large%20Language%20Model&body=Title%3A%20X-VILA%3A%20Cross-Modality%20Alignment%20for%20Large%20Language%20Model%0AAuthor%3A%20Hanrong%20Ye%20and%20De-An%20Huang%20and%20Yao%20Lu%20and%20Zhiding%20Yu%20and%20Wei%20Ping%20and%20Andrew%20Tao%20and%20Jan%20Kautz%20and%20Song%20Han%20and%20Dan%20Xu%20and%20Pavlo%20Molchanov%20and%20Hongxu%20Yin%0AAbstract%3A%20%20%20We%20introduce%20X-VILA%2C%20an%20omni-modality%20model%20designed%20to%20extend%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20incorporating%20image%2C%20video%2C%20and%0Aaudio%20modalities.%20By%20aligning%20modality-specific%20encoders%20with%20LLM%20inputs%20and%0Adiffusion%20decoders%20with%20LLM%20outputs%2C%20X-VILA%20achieves%20cross-modality%0Aunderstanding%2C%20reasoning%2C%20and%20generation.%20To%20facilitate%20this%20cross-modality%0Aalignment%2C%20we%20curate%20an%20effective%20interleaved%20any-to-any%20modality%0Ainstruction-following%20dataset.%20Furthermore%2C%20we%20identify%20a%20significant%20problem%0Awith%20the%20current%20cross-modality%20alignment%20method%2C%20which%20results%20in%20visual%0Ainformation%20loss.%20To%20address%20the%20issue%2C%20we%20propose%20a%20visual%20alignment%20mechanism%0Awith%20a%20visual%20embedding%20highway%20module.%20We%20then%20introduce%20a%20resource-efficient%0Arecipe%20for%20training%20X-VILA%2C%20that%20exhibits%20proficiency%20in%20any-to-any%20modality%0Aconversation%2C%20surpassing%20previous%20approaches%20by%20large%20margins.%20X-VILA%20also%0Ashowcases%20emergent%20properties%20across%20modalities%20even%20in%20the%20absence%20of%20similar%0Atraining%20data.%20The%20project%20will%20be%20made%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-VILA%253A%2520Cross-Modality%2520Alignment%2520for%2520Large%2520Language%2520Model%26entry.906535625%3DHanrong%2520Ye%2520and%2520De-An%2520Huang%2520and%2520Yao%2520Lu%2520and%2520Zhiding%2520Yu%2520and%2520Wei%2520Ping%2520and%2520Andrew%2520Tao%2520and%2520Jan%2520Kautz%2520and%2520Song%2520Han%2520and%2520Dan%2520Xu%2520and%2520Pavlo%2520Molchanov%2520and%2520Hongxu%2520Yin%26entry.1292438233%3D%2520%2520We%2520introduce%2520X-VILA%252C%2520an%2520omni-modality%2520model%2520designed%2520to%2520extend%2520the%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520incorporating%2520image%252C%2520video%252C%2520and%250Aaudio%2520modalities.%2520By%2520aligning%2520modality-specific%2520encoders%2520with%2520LLM%2520inputs%2520and%250Adiffusion%2520decoders%2520with%2520LLM%2520outputs%252C%2520X-VILA%2520achieves%2520cross-modality%250Aunderstanding%252C%2520reasoning%252C%2520and%2520generation.%2520To%2520facilitate%2520this%2520cross-modality%250Aalignment%252C%2520we%2520curate%2520an%2520effective%2520interleaved%2520any-to-any%2520modality%250Ainstruction-following%2520dataset.%2520Furthermore%252C%2520we%2520identify%2520a%2520significant%2520problem%250Awith%2520the%2520current%2520cross-modality%2520alignment%2520method%252C%2520which%2520results%2520in%2520visual%250Ainformation%2520loss.%2520To%2520address%2520the%2520issue%252C%2520we%2520propose%2520a%2520visual%2520alignment%2520mechanism%250Awith%2520a%2520visual%2520embedding%2520highway%2520module.%2520We%2520then%2520introduce%2520a%2520resource-efficient%250Arecipe%2520for%2520training%2520X-VILA%252C%2520that%2520exhibits%2520proficiency%2520in%2520any-to-any%2520modality%250Aconversation%252C%2520surpassing%2520previous%2520approaches%2520by%2520large%2520margins.%2520X-VILA%2520also%250Ashowcases%2520emergent%2520properties%2520across%2520modalities%2520even%2520in%2520the%2520absence%2520of%2520similar%250Atraining%2520data.%2520The%2520project%2520will%2520be%2520made%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-VILA%3A%20Cross-Modality%20Alignment%20for%20Large%20Language%20Model&entry.906535625=Hanrong%20Ye%20and%20De-An%20Huang%20and%20Yao%20Lu%20and%20Zhiding%20Yu%20and%20Wei%20Ping%20and%20Andrew%20Tao%20and%20Jan%20Kautz%20and%20Song%20Han%20and%20Dan%20Xu%20and%20Pavlo%20Molchanov%20and%20Hongxu%20Yin&entry.1292438233=%20%20We%20introduce%20X-VILA%2C%20an%20omni-modality%20model%20designed%20to%20extend%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20incorporating%20image%2C%20video%2C%20and%0Aaudio%20modalities.%20By%20aligning%20modality-specific%20encoders%20with%20LLM%20inputs%20and%0Adiffusion%20decoders%20with%20LLM%20outputs%2C%20X-VILA%20achieves%20cross-modality%0Aunderstanding%2C%20reasoning%2C%20and%20generation.%20To%20facilitate%20this%20cross-modality%0Aalignment%2C%20we%20curate%20an%20effective%20interleaved%20any-to-any%20modality%0Ainstruction-following%20dataset.%20Furthermore%2C%20we%20identify%20a%20significant%20problem%0Awith%20the%20current%20cross-modality%20alignment%20method%2C%20which%20results%20in%20visual%0Ainformation%20loss.%20To%20address%20the%20issue%2C%20we%20propose%20a%20visual%20alignment%20mechanism%0Awith%20a%20visual%20embedding%20highway%20module.%20We%20then%20introduce%20a%20resource-efficient%0Arecipe%20for%20training%20X-VILA%2C%20that%20exhibits%20proficiency%20in%20any-to-any%20modality%0Aconversation%2C%20surpassing%20previous%20approaches%20by%20large%20margins.%20X-VILA%20also%0Ashowcases%20emergent%20properties%20across%20modalities%20even%20in%20the%20absence%20of%20similar%0Atraining%20data.%20The%20project%20will%20be%20made%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19335v1&entry.124074799=Read"},
{"title": "Towards Global Glacier Mapping with Deep Learning and Open Earth\n  Observation Data", "author": "Konstantin A. Maslov and Claudio Persello and Thomas Schellenberger and Alfred Stein", "abstract": "  Accurate global glacier mapping is critical for understanding climate change\nimpacts. Despite its importance, automated glacier mapping at a global scale\nremains largely unexplored. Here we address this gap and propose\nGlacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep\nlearning model, and five strategies for multitemporal global-scale glacier\nmapping using open satellite imagery. Assessing the spatial, temporal and\ncross-sensor generalisation shows that our best strategy achieves intersection\nover union >0.85 on previously unobserved images in most cases, which drops to\n>0.75 for debris-rich areas such as High-Mountain Asia and increases to >0.90\nfor regions dominated by clean ice. A comparative validation against human\nexpert uncertainties in terms of area and distance deviations underscores\nGlaViTU performance, approaching or matching expert-level delineation. Adding\nsynthetic aperture radar data, namely, backscatter and interferometric\ncoherence, increases the accuracy in all regions where available. The\ncalibrated confidence for glacier extents is reported making the predictions\nmore reliable and interpretable. We also release a benchmark dataset that\ncovers 9% of glaciers worldwide. Our results support efforts towards automated\nmultitemporal and global glacier mapping.\n", "link": "http://arxiv.org/abs/2401.15113v2", "date": "2024-05-29", "relevancy": 2.6922, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.571}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Global%20Glacier%20Mapping%20with%20Deep%20Learning%20and%20Open%20Earth%0A%20%20Observation%20Data&body=Title%3A%20Towards%20Global%20Glacier%20Mapping%20with%20Deep%20Learning%20and%20Open%20Earth%0A%20%20Observation%20Data%0AAuthor%3A%20Konstantin%20A.%20Maslov%20and%20Claudio%20Persello%20and%20Thomas%20Schellenberger%20and%20Alfred%20Stein%0AAbstract%3A%20%20%20Accurate%20global%20glacier%20mapping%20is%20critical%20for%20understanding%20climate%20change%0Aimpacts.%20Despite%20its%20importance%2C%20automated%20glacier%20mapping%20at%20a%20global%20scale%0Aremains%20largely%20unexplored.%20Here%20we%20address%20this%20gap%20and%20propose%0AGlacier-VisionTransformer-U-Net%20%28GlaViTU%29%2C%20a%20convolutional-transformer%20deep%0Alearning%20model%2C%20and%20five%20strategies%20for%20multitemporal%20global-scale%20glacier%0Amapping%20using%20open%20satellite%20imagery.%20Assessing%20the%20spatial%2C%20temporal%20and%0Across-sensor%20generalisation%20shows%20that%20our%20best%20strategy%20achieves%20intersection%0Aover%20union%20%3E0.85%20on%20previously%20unobserved%20images%20in%20most%20cases%2C%20which%20drops%20to%0A%3E0.75%20for%20debris-rich%20areas%20such%20as%20High-Mountain%20Asia%20and%20increases%20to%20%3E0.90%0Afor%20regions%20dominated%20by%20clean%20ice.%20A%20comparative%20validation%20against%20human%0Aexpert%20uncertainties%20in%20terms%20of%20area%20and%20distance%20deviations%20underscores%0AGlaViTU%20performance%2C%20approaching%20or%20matching%20expert-level%20delineation.%20Adding%0Asynthetic%20aperture%20radar%20data%2C%20namely%2C%20backscatter%20and%20interferometric%0Acoherence%2C%20increases%20the%20accuracy%20in%20all%20regions%20where%20available.%20The%0Acalibrated%20confidence%20for%20glacier%20extents%20is%20reported%20making%20the%20predictions%0Amore%20reliable%20and%20interpretable.%20We%20also%20release%20a%20benchmark%20dataset%20that%0Acovers%209%25%20of%20glaciers%20worldwide.%20Our%20results%20support%20efforts%20towards%20automated%0Amultitemporal%20and%20global%20glacier%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Global%2520Glacier%2520Mapping%2520with%2520Deep%2520Learning%2520and%2520Open%2520Earth%250A%2520%2520Observation%2520Data%26entry.906535625%3DKonstantin%2520A.%2520Maslov%2520and%2520Claudio%2520Persello%2520and%2520Thomas%2520Schellenberger%2520and%2520Alfred%2520Stein%26entry.1292438233%3D%2520%2520Accurate%2520global%2520glacier%2520mapping%2520is%2520critical%2520for%2520understanding%2520climate%2520change%250Aimpacts.%2520Despite%2520its%2520importance%252C%2520automated%2520glacier%2520mapping%2520at%2520a%2520global%2520scale%250Aremains%2520largely%2520unexplored.%2520Here%2520we%2520address%2520this%2520gap%2520and%2520propose%250AGlacier-VisionTransformer-U-Net%2520%2528GlaViTU%2529%252C%2520a%2520convolutional-transformer%2520deep%250Alearning%2520model%252C%2520and%2520five%2520strategies%2520for%2520multitemporal%2520global-scale%2520glacier%250Amapping%2520using%2520open%2520satellite%2520imagery.%2520Assessing%2520the%2520spatial%252C%2520temporal%2520and%250Across-sensor%2520generalisation%2520shows%2520that%2520our%2520best%2520strategy%2520achieves%2520intersection%250Aover%2520union%2520%253E0.85%2520on%2520previously%2520unobserved%2520images%2520in%2520most%2520cases%252C%2520which%2520drops%2520to%250A%253E0.75%2520for%2520debris-rich%2520areas%2520such%2520as%2520High-Mountain%2520Asia%2520and%2520increases%2520to%2520%253E0.90%250Afor%2520regions%2520dominated%2520by%2520clean%2520ice.%2520A%2520comparative%2520validation%2520against%2520human%250Aexpert%2520uncertainties%2520in%2520terms%2520of%2520area%2520and%2520distance%2520deviations%2520underscores%250AGlaViTU%2520performance%252C%2520approaching%2520or%2520matching%2520expert-level%2520delineation.%2520Adding%250Asynthetic%2520aperture%2520radar%2520data%252C%2520namely%252C%2520backscatter%2520and%2520interferometric%250Acoherence%252C%2520increases%2520the%2520accuracy%2520in%2520all%2520regions%2520where%2520available.%2520The%250Acalibrated%2520confidence%2520for%2520glacier%2520extents%2520is%2520reported%2520making%2520the%2520predictions%250Amore%2520reliable%2520and%2520interpretable.%2520We%2520also%2520release%2520a%2520benchmark%2520dataset%2520that%250Acovers%25209%2525%2520of%2520glaciers%2520worldwide.%2520Our%2520results%2520support%2520efforts%2520towards%2520automated%250Amultitemporal%2520and%2520global%2520glacier%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Global%20Glacier%20Mapping%20with%20Deep%20Learning%20and%20Open%20Earth%0A%20%20Observation%20Data&entry.906535625=Konstantin%20A.%20Maslov%20and%20Claudio%20Persello%20and%20Thomas%20Schellenberger%20and%20Alfred%20Stein&entry.1292438233=%20%20Accurate%20global%20glacier%20mapping%20is%20critical%20for%20understanding%20climate%20change%0Aimpacts.%20Despite%20its%20importance%2C%20automated%20glacier%20mapping%20at%20a%20global%20scale%0Aremains%20largely%20unexplored.%20Here%20we%20address%20this%20gap%20and%20propose%0AGlacier-VisionTransformer-U-Net%20%28GlaViTU%29%2C%20a%20convolutional-transformer%20deep%0Alearning%20model%2C%20and%20five%20strategies%20for%20multitemporal%20global-scale%20glacier%0Amapping%20using%20open%20satellite%20imagery.%20Assessing%20the%20spatial%2C%20temporal%20and%0Across-sensor%20generalisation%20shows%20that%20our%20best%20strategy%20achieves%20intersection%0Aover%20union%20%3E0.85%20on%20previously%20unobserved%20images%20in%20most%20cases%2C%20which%20drops%20to%0A%3E0.75%20for%20debris-rich%20areas%20such%20as%20High-Mountain%20Asia%20and%20increases%20to%20%3E0.90%0Afor%20regions%20dominated%20by%20clean%20ice.%20A%20comparative%20validation%20against%20human%0Aexpert%20uncertainties%20in%20terms%20of%20area%20and%20distance%20deviations%20underscores%0AGlaViTU%20performance%2C%20approaching%20or%20matching%20expert-level%20delineation.%20Adding%0Asynthetic%20aperture%20radar%20data%2C%20namely%2C%20backscatter%20and%20interferometric%0Acoherence%2C%20increases%20the%20accuracy%20in%20all%20regions%20where%20available.%20The%0Acalibrated%20confidence%20for%20glacier%20extents%20is%20reported%20making%20the%20predictions%0Amore%20reliable%20and%20interpretable.%20We%20also%20release%20a%20benchmark%20dataset%20that%0Acovers%209%25%20of%20glaciers%20worldwide.%20Our%20results%20support%20efforts%20towards%20automated%0Amultitemporal%20and%20global%20glacier%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15113v2&entry.124074799=Read"},
{"title": "Self-Pro: Self-Prompt and Tuning Framework for Graph Neural Networks", "author": "Chenghua Gong and Xiang Li and Jianxiang Yu and Cheng Yao and Jiaqi Tan and Chengcheng Yu", "abstract": "  Graphs have become an important modeling tool for Web applications, and graph\nneural networks (GNNs) have achieved great success in graph representation\nlearning. However, their performance heavily relies on a large amount of\nsupervision. Recently, ``pre-train, fine-tune'' has become the paradigm to\naddress the issues of label dependency and poor generalization. However, the\npre-training strategies vary for graphs with homophily and heterophily, and the\nobjectives for various downstream tasks also differ. This leads to a gap\nbetween pretexts and downstream tasks, resulting in ``negative transfer'' and\npoor performance. Inspired by prompt learning in natural language processing,\nmany studies turn to bridge the gap and fully leverage the pre-trained model.\nHowever, existing methods for graph prompting are tailored to homophily,\nneglecting inherent heterophily on graphs. Meanwhile, most of them rely on\nrandomly initialized prompts, which negatively impact on the stability.\nTherefore, we propose Self-Prompt, a prompting framework for graphs based on\nthe model and data itself. We first introduce asymmetric graph contrastive\nlearning as pretext to address heterophily and align the objectives of pretext\nand downstream tasks. Then we reuse the component from pre-training as the self\nadapter and introduce self-prompts based on graph itself for task adaptation.\nFinally, we conduct extensive experiments on 11 benchmark datasets to\ndemonstrate its superiority. We provide our codes at\n\\url{https://github.com/gongchenghua/Self-Pro}.\n", "link": "http://arxiv.org/abs/2310.10362v2", "date": "2024-05-29", "relevancy": 2.6898, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5444}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Pro%3A%20Self-Prompt%20and%20Tuning%20Framework%20for%20Graph%20Neural%20Networks&body=Title%3A%20Self-Pro%3A%20Self-Prompt%20and%20Tuning%20Framework%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Chenghua%20Gong%20and%20Xiang%20Li%20and%20Jianxiang%20Yu%20and%20Cheng%20Yao%20and%20Jiaqi%20Tan%20and%20Chengcheng%20Yu%0AAbstract%3A%20%20%20Graphs%20have%20become%20an%20important%20modeling%20tool%20for%20Web%20applications%2C%20and%20graph%0Aneural%20networks%20%28GNNs%29%20have%20achieved%20great%20success%20in%20graph%20representation%0Alearning.%20However%2C%20their%20performance%20heavily%20relies%20on%20a%20large%20amount%20of%0Asupervision.%20Recently%2C%20%60%60pre-train%2C%20fine-tune%27%27%20has%20become%20the%20paradigm%20to%0Aaddress%20the%20issues%20of%20label%20dependency%20and%20poor%20generalization.%20However%2C%20the%0Apre-training%20strategies%20vary%20for%20graphs%20with%20homophily%20and%20heterophily%2C%20and%20the%0Aobjectives%20for%20various%20downstream%20tasks%20also%20differ.%20This%20leads%20to%20a%20gap%0Abetween%20pretexts%20and%20downstream%20tasks%2C%20resulting%20in%20%60%60negative%20transfer%27%27%20and%0Apoor%20performance.%20Inspired%20by%20prompt%20learning%20in%20natural%20language%20processing%2C%0Amany%20studies%20turn%20to%20bridge%20the%20gap%20and%20fully%20leverage%20the%20pre-trained%20model.%0AHowever%2C%20existing%20methods%20for%20graph%20prompting%20are%20tailored%20to%20homophily%2C%0Aneglecting%20inherent%20heterophily%20on%20graphs.%20Meanwhile%2C%20most%20of%20them%20rely%20on%0Arandomly%20initialized%20prompts%2C%20which%20negatively%20impact%20on%20the%20stability.%0ATherefore%2C%20we%20propose%20Self-Prompt%2C%20a%20prompting%20framework%20for%20graphs%20based%20on%0Athe%20model%20and%20data%20itself.%20We%20first%20introduce%20asymmetric%20graph%20contrastive%0Alearning%20as%20pretext%20to%20address%20heterophily%20and%20align%20the%20objectives%20of%20pretext%0Aand%20downstream%20tasks.%20Then%20we%20reuse%20the%20component%20from%20pre-training%20as%20the%20self%0Aadapter%20and%20introduce%20self-prompts%20based%20on%20graph%20itself%20for%20task%20adaptation.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20on%2011%20benchmark%20datasets%20to%0Ademonstrate%20its%20superiority.%20We%20provide%20our%20codes%20at%0A%5Curl%7Bhttps%3A//github.com/gongchenghua/Self-Pro%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Pro%253A%2520Self-Prompt%2520and%2520Tuning%2520Framework%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DChenghua%2520Gong%2520and%2520Xiang%2520Li%2520and%2520Jianxiang%2520Yu%2520and%2520Cheng%2520Yao%2520and%2520Jiaqi%2520Tan%2520and%2520Chengcheng%2520Yu%26entry.1292438233%3D%2520%2520Graphs%2520have%2520become%2520an%2520important%2520modeling%2520tool%2520for%2520Web%2520applications%252C%2520and%2520graph%250Aneural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520great%2520success%2520in%2520graph%2520representation%250Alearning.%2520However%252C%2520their%2520performance%2520heavily%2520relies%2520on%2520a%2520large%2520amount%2520of%250Asupervision.%2520Recently%252C%2520%2560%2560pre-train%252C%2520fine-tune%2527%2527%2520has%2520become%2520the%2520paradigm%2520to%250Aaddress%2520the%2520issues%2520of%2520label%2520dependency%2520and%2520poor%2520generalization.%2520However%252C%2520the%250Apre-training%2520strategies%2520vary%2520for%2520graphs%2520with%2520homophily%2520and%2520heterophily%252C%2520and%2520the%250Aobjectives%2520for%2520various%2520downstream%2520tasks%2520also%2520differ.%2520This%2520leads%2520to%2520a%2520gap%250Abetween%2520pretexts%2520and%2520downstream%2520tasks%252C%2520resulting%2520in%2520%2560%2560negative%2520transfer%2527%2527%2520and%250Apoor%2520performance.%2520Inspired%2520by%2520prompt%2520learning%2520in%2520natural%2520language%2520processing%252C%250Amany%2520studies%2520turn%2520to%2520bridge%2520the%2520gap%2520and%2520fully%2520leverage%2520the%2520pre-trained%2520model.%250AHowever%252C%2520existing%2520methods%2520for%2520graph%2520prompting%2520are%2520tailored%2520to%2520homophily%252C%250Aneglecting%2520inherent%2520heterophily%2520on%2520graphs.%2520Meanwhile%252C%2520most%2520of%2520them%2520rely%2520on%250Arandomly%2520initialized%2520prompts%252C%2520which%2520negatively%2520impact%2520on%2520the%2520stability.%250ATherefore%252C%2520we%2520propose%2520Self-Prompt%252C%2520a%2520prompting%2520framework%2520for%2520graphs%2520based%2520on%250Athe%2520model%2520and%2520data%2520itself.%2520We%2520first%2520introduce%2520asymmetric%2520graph%2520contrastive%250Alearning%2520as%2520pretext%2520to%2520address%2520heterophily%2520and%2520align%2520the%2520objectives%2520of%2520pretext%250Aand%2520downstream%2520tasks.%2520Then%2520we%2520reuse%2520the%2520component%2520from%2520pre-training%2520as%2520the%2520self%250Aadapter%2520and%2520introduce%2520self-prompts%2520based%2520on%2520graph%2520itself%2520for%2520task%2520adaptation.%250AFinally%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%252011%2520benchmark%2520datasets%2520to%250Ademonstrate%2520its%2520superiority.%2520We%2520provide%2520our%2520codes%2520at%250A%255Curl%257Bhttps%253A//github.com/gongchenghua/Self-Pro%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Pro%3A%20Self-Prompt%20and%20Tuning%20Framework%20for%20Graph%20Neural%20Networks&entry.906535625=Chenghua%20Gong%20and%20Xiang%20Li%20and%20Jianxiang%20Yu%20and%20Cheng%20Yao%20and%20Jiaqi%20Tan%20and%20Chengcheng%20Yu&entry.1292438233=%20%20Graphs%20have%20become%20an%20important%20modeling%20tool%20for%20Web%20applications%2C%20and%20graph%0Aneural%20networks%20%28GNNs%29%20have%20achieved%20great%20success%20in%20graph%20representation%0Alearning.%20However%2C%20their%20performance%20heavily%20relies%20on%20a%20large%20amount%20of%0Asupervision.%20Recently%2C%20%60%60pre-train%2C%20fine-tune%27%27%20has%20become%20the%20paradigm%20to%0Aaddress%20the%20issues%20of%20label%20dependency%20and%20poor%20generalization.%20However%2C%20the%0Apre-training%20strategies%20vary%20for%20graphs%20with%20homophily%20and%20heterophily%2C%20and%20the%0Aobjectives%20for%20various%20downstream%20tasks%20also%20differ.%20This%20leads%20to%20a%20gap%0Abetween%20pretexts%20and%20downstream%20tasks%2C%20resulting%20in%20%60%60negative%20transfer%27%27%20and%0Apoor%20performance.%20Inspired%20by%20prompt%20learning%20in%20natural%20language%20processing%2C%0Amany%20studies%20turn%20to%20bridge%20the%20gap%20and%20fully%20leverage%20the%20pre-trained%20model.%0AHowever%2C%20existing%20methods%20for%20graph%20prompting%20are%20tailored%20to%20homophily%2C%0Aneglecting%20inherent%20heterophily%20on%20graphs.%20Meanwhile%2C%20most%20of%20them%20rely%20on%0Arandomly%20initialized%20prompts%2C%20which%20negatively%20impact%20on%20the%20stability.%0ATherefore%2C%20we%20propose%20Self-Prompt%2C%20a%20prompting%20framework%20for%20graphs%20based%20on%0Athe%20model%20and%20data%20itself.%20We%20first%20introduce%20asymmetric%20graph%20contrastive%0Alearning%20as%20pretext%20to%20address%20heterophily%20and%20align%20the%20objectives%20of%20pretext%0Aand%20downstream%20tasks.%20Then%20we%20reuse%20the%20component%20from%20pre-training%20as%20the%20self%0Aadapter%20and%20introduce%20self-prompts%20based%20on%20graph%20itself%20for%20task%20adaptation.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20on%2011%20benchmark%20datasets%20to%0Ademonstrate%20its%20superiority.%20We%20provide%20our%20codes%20at%0A%5Curl%7Bhttps%3A//github.com/gongchenghua/Self-Pro%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10362v2&entry.124074799=Read"},
{"title": "ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval\n  from Linguistically Complex Descriptions", "author": "Honglin Lin and Siyu Li and Guoshun Nan and Chaoyue Tang and Xueting Wang and Jingxin Xu and Rong Yankai and Zhili Zhou and Yutong Gao and Qimei Cui and Xiaofeng Tao", "abstract": "  Image retrieval from contextual descriptions (IRCD) aims to identify an image\nwithin a set of minimally contrastive candidates based on linguistically\ncomplex text. Despite the success of VLMs, they still significantly lag behind\nhuman performance in IRCD. The main challenges lie in aligning key contextual\ncues in two modalities, where these subtle cues are concealed in tiny areas of\nmultiple contrastive images and within the complex linguistics of textual\ndescriptions. This motivates us to propose ContextBLIP, a simple yet effective\nmethod that relies on a doubly contextual alignment scheme for challenging\nIRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching\nloss, and a text-guided masking loss. The adapter learns to capture\nfine-grained visual cues. The two losses enable iterative supervision for the\nadapter, gradually highlighting the focal patches of a single image to the key\ntextual cues. We term such a way as intra-contextual alignment. 2) Then,\nContextBLIP further employs an inter-context encoder to learn dependencies\namong candidates, facilitating alignment between the text to multiple images.\nWe term this step as inter-contextual alignment. Consequently, the nuanced cues\nconcealed in each modality can be effectively aligned. Experiments on two\nbenchmarks show the superiority of our method. We observe that ContextBLIP can\nyield comparable results with GPT-4V, despite involving about 7,500 times fewer\nparameters.\n", "link": "http://arxiv.org/abs/2405.19226v1", "date": "2024-05-29", "relevancy": 2.6574, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5182}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextBLIP%3A%20Doubly%20Contextual%20Alignment%20for%20Contrastive%20Image%20Retrieval%0A%20%20from%20Linguistically%20Complex%20Descriptions&body=Title%3A%20ContextBLIP%3A%20Doubly%20Contextual%20Alignment%20for%20Contrastive%20Image%20Retrieval%0A%20%20from%20Linguistically%20Complex%20Descriptions%0AAuthor%3A%20Honglin%20Lin%20and%20Siyu%20Li%20and%20Guoshun%20Nan%20and%20Chaoyue%20Tang%20and%20Xueting%20Wang%20and%20Jingxin%20Xu%20and%20Rong%20Yankai%20and%20Zhili%20Zhou%20and%20Yutong%20Gao%20and%20Qimei%20Cui%20and%20Xiaofeng%20Tao%0AAbstract%3A%20%20%20Image%20retrieval%20from%20contextual%20descriptions%20%28IRCD%29%20aims%20to%20identify%20an%20image%0Awithin%20a%20set%20of%20minimally%20contrastive%20candidates%20based%20on%20linguistically%0Acomplex%20text.%20Despite%20the%20success%20of%20VLMs%2C%20they%20still%20significantly%20lag%20behind%0Ahuman%20performance%20in%20IRCD.%20The%20main%20challenges%20lie%20in%20aligning%20key%20contextual%0Acues%20in%20two%20modalities%2C%20where%20these%20subtle%20cues%20are%20concealed%20in%20tiny%20areas%20of%0Amultiple%20contrastive%20images%20and%20within%20the%20complex%20linguistics%20of%20textual%0Adescriptions.%20This%20motivates%20us%20to%20propose%20ContextBLIP%2C%20a%20simple%20yet%20effective%0Amethod%20that%20relies%20on%20a%20doubly%20contextual%20alignment%20scheme%20for%20challenging%0AIRCD.%20Specifically%2C%201%29%20our%20model%20comprises%20a%20multi-scale%20adapter%2C%20a%20matching%0Aloss%2C%20and%20a%20text-guided%20masking%20loss.%20The%20adapter%20learns%20to%20capture%0Afine-grained%20visual%20cues.%20The%20two%20losses%20enable%20iterative%20supervision%20for%20the%0Aadapter%2C%20gradually%20highlighting%20the%20focal%20patches%20of%20a%20single%20image%20to%20the%20key%0Atextual%20cues.%20We%20term%20such%20a%20way%20as%20intra-contextual%20alignment.%202%29%20Then%2C%0AContextBLIP%20further%20employs%20an%20inter-context%20encoder%20to%20learn%20dependencies%0Aamong%20candidates%2C%20facilitating%20alignment%20between%20the%20text%20to%20multiple%20images.%0AWe%20term%20this%20step%20as%20inter-contextual%20alignment.%20Consequently%2C%20the%20nuanced%20cues%0Aconcealed%20in%20each%20modality%20can%20be%20effectively%20aligned.%20Experiments%20on%20two%0Abenchmarks%20show%20the%20superiority%20of%20our%20method.%20We%20observe%20that%20ContextBLIP%20can%0Ayield%20comparable%20results%20with%20GPT-4V%2C%20despite%20involving%20about%207%2C500%20times%20fewer%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextBLIP%253A%2520Doubly%2520Contextual%2520Alignment%2520for%2520Contrastive%2520Image%2520Retrieval%250A%2520%2520from%2520Linguistically%2520Complex%2520Descriptions%26entry.906535625%3DHonglin%2520Lin%2520and%2520Siyu%2520Li%2520and%2520Guoshun%2520Nan%2520and%2520Chaoyue%2520Tang%2520and%2520Xueting%2520Wang%2520and%2520Jingxin%2520Xu%2520and%2520Rong%2520Yankai%2520and%2520Zhili%2520Zhou%2520and%2520Yutong%2520Gao%2520and%2520Qimei%2520Cui%2520and%2520Xiaofeng%2520Tao%26entry.1292438233%3D%2520%2520Image%2520retrieval%2520from%2520contextual%2520descriptions%2520%2528IRCD%2529%2520aims%2520to%2520identify%2520an%2520image%250Awithin%2520a%2520set%2520of%2520minimally%2520contrastive%2520candidates%2520based%2520on%2520linguistically%250Acomplex%2520text.%2520Despite%2520the%2520success%2520of%2520VLMs%252C%2520they%2520still%2520significantly%2520lag%2520behind%250Ahuman%2520performance%2520in%2520IRCD.%2520The%2520main%2520challenges%2520lie%2520in%2520aligning%2520key%2520contextual%250Acues%2520in%2520two%2520modalities%252C%2520where%2520these%2520subtle%2520cues%2520are%2520concealed%2520in%2520tiny%2520areas%2520of%250Amultiple%2520contrastive%2520images%2520and%2520within%2520the%2520complex%2520linguistics%2520of%2520textual%250Adescriptions.%2520This%2520motivates%2520us%2520to%2520propose%2520ContextBLIP%252C%2520a%2520simple%2520yet%2520effective%250Amethod%2520that%2520relies%2520on%2520a%2520doubly%2520contextual%2520alignment%2520scheme%2520for%2520challenging%250AIRCD.%2520Specifically%252C%25201%2529%2520our%2520model%2520comprises%2520a%2520multi-scale%2520adapter%252C%2520a%2520matching%250Aloss%252C%2520and%2520a%2520text-guided%2520masking%2520loss.%2520The%2520adapter%2520learns%2520to%2520capture%250Afine-grained%2520visual%2520cues.%2520The%2520two%2520losses%2520enable%2520iterative%2520supervision%2520for%2520the%250Aadapter%252C%2520gradually%2520highlighting%2520the%2520focal%2520patches%2520of%2520a%2520single%2520image%2520to%2520the%2520key%250Atextual%2520cues.%2520We%2520term%2520such%2520a%2520way%2520as%2520intra-contextual%2520alignment.%25202%2529%2520Then%252C%250AContextBLIP%2520further%2520employs%2520an%2520inter-context%2520encoder%2520to%2520learn%2520dependencies%250Aamong%2520candidates%252C%2520facilitating%2520alignment%2520between%2520the%2520text%2520to%2520multiple%2520images.%250AWe%2520term%2520this%2520step%2520as%2520inter-contextual%2520alignment.%2520Consequently%252C%2520the%2520nuanced%2520cues%250Aconcealed%2520in%2520each%2520modality%2520can%2520be%2520effectively%2520aligned.%2520Experiments%2520on%2520two%250Abenchmarks%2520show%2520the%2520superiority%2520of%2520our%2520method.%2520We%2520observe%2520that%2520ContextBLIP%2520can%250Ayield%2520comparable%2520results%2520with%2520GPT-4V%252C%2520despite%2520involving%2520about%25207%252C500%2520times%2520fewer%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextBLIP%3A%20Doubly%20Contextual%20Alignment%20for%20Contrastive%20Image%20Retrieval%0A%20%20from%20Linguistically%20Complex%20Descriptions&entry.906535625=Honglin%20Lin%20and%20Siyu%20Li%20and%20Guoshun%20Nan%20and%20Chaoyue%20Tang%20and%20Xueting%20Wang%20and%20Jingxin%20Xu%20and%20Rong%20Yankai%20and%20Zhili%20Zhou%20and%20Yutong%20Gao%20and%20Qimei%20Cui%20and%20Xiaofeng%20Tao&entry.1292438233=%20%20Image%20retrieval%20from%20contextual%20descriptions%20%28IRCD%29%20aims%20to%20identify%20an%20image%0Awithin%20a%20set%20of%20minimally%20contrastive%20candidates%20based%20on%20linguistically%0Acomplex%20text.%20Despite%20the%20success%20of%20VLMs%2C%20they%20still%20significantly%20lag%20behind%0Ahuman%20performance%20in%20IRCD.%20The%20main%20challenges%20lie%20in%20aligning%20key%20contextual%0Acues%20in%20two%20modalities%2C%20where%20these%20subtle%20cues%20are%20concealed%20in%20tiny%20areas%20of%0Amultiple%20contrastive%20images%20and%20within%20the%20complex%20linguistics%20of%20textual%0Adescriptions.%20This%20motivates%20us%20to%20propose%20ContextBLIP%2C%20a%20simple%20yet%20effective%0Amethod%20that%20relies%20on%20a%20doubly%20contextual%20alignment%20scheme%20for%20challenging%0AIRCD.%20Specifically%2C%201%29%20our%20model%20comprises%20a%20multi-scale%20adapter%2C%20a%20matching%0Aloss%2C%20and%20a%20text-guided%20masking%20loss.%20The%20adapter%20learns%20to%20capture%0Afine-grained%20visual%20cues.%20The%20two%20losses%20enable%20iterative%20supervision%20for%20the%0Aadapter%2C%20gradually%20highlighting%20the%20focal%20patches%20of%20a%20single%20image%20to%20the%20key%0Atextual%20cues.%20We%20term%20such%20a%20way%20as%20intra-contextual%20alignment.%202%29%20Then%2C%0AContextBLIP%20further%20employs%20an%20inter-context%20encoder%20to%20learn%20dependencies%0Aamong%20candidates%2C%20facilitating%20alignment%20between%20the%20text%20to%20multiple%20images.%0AWe%20term%20this%20step%20as%20inter-contextual%20alignment.%20Consequently%2C%20the%20nuanced%20cues%0Aconcealed%20in%20each%20modality%20can%20be%20effectively%20aligned.%20Experiments%20on%20two%0Abenchmarks%20show%20the%20superiority%20of%20our%20method.%20We%20observe%20that%20ContextBLIP%20can%0Ayield%20comparable%20results%20with%20GPT-4V%2C%20despite%20involving%20about%207%2C500%20times%20fewer%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19226v1&entry.124074799=Read"},
{"title": "SIG: Efficient Self-Interpretable Graph Neural Network for\n  Continuous-time Dynamic Graphs", "author": "Lanting Fang and Yulian Yang and Kai Wang and Shanshan Feng and Kaiyu Feng and Jie Gui and Shuliang Wang and Yew-Soon Ong", "abstract": "  While dynamic graph neural networks have shown promise in various\napplications, explaining their predictions on continuous-time dynamic graphs\n(CTDGs) is difficult. This paper investigates a new research task:\nself-interpretable GNNs for CTDGs. We aim to predict future links within the\ndynamic graph while simultaneously providing causal explanations for these\npredictions. There are two key challenges: (1) capturing the underlying\nstructural and temporal information that remains consistent across both\nindependent and identically distributed (IID) and out-of-distribution (OOD)\ndata, and (2) efficiently generating high-quality link prediction results and\nexplanations. To tackle these challenges, we propose a novel causal inference\nmodel, namely the Independent and Confounded Causal Model (ICCM). ICCM is then\nintegrated into a deep learning architecture that considers both effectiveness\nand efficiency. Extensive experiments demonstrate that our proposed model\nsignificantly outperforms existing methods across link prediction accuracy,\nexplanation quality, and robustness to shortcut features. Our code and datasets\nare anonymously released at https://github.com/2024SIG/SIG.\n", "link": "http://arxiv.org/abs/2405.19062v1", "date": "2024-05-29", "relevancy": 2.6287, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5271}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIG%3A%20Efficient%20Self-Interpretable%20Graph%20Neural%20Network%20for%0A%20%20Continuous-time%20Dynamic%20Graphs&body=Title%3A%20SIG%3A%20Efficient%20Self-Interpretable%20Graph%20Neural%20Network%20for%0A%20%20Continuous-time%20Dynamic%20Graphs%0AAuthor%3A%20Lanting%20Fang%20and%20Yulian%20Yang%20and%20Kai%20Wang%20and%20Shanshan%20Feng%20and%20Kaiyu%20Feng%20and%20Jie%20Gui%20and%20Shuliang%20Wang%20and%20Yew-Soon%20Ong%0AAbstract%3A%20%20%20While%20dynamic%20graph%20neural%20networks%20have%20shown%20promise%20in%20various%0Aapplications%2C%20explaining%20their%20predictions%20on%20continuous-time%20dynamic%20graphs%0A%28CTDGs%29%20is%20difficult.%20This%20paper%20investigates%20a%20new%20research%20task%3A%0Aself-interpretable%20GNNs%20for%20CTDGs.%20We%20aim%20to%20predict%20future%20links%20within%20the%0Adynamic%20graph%20while%20simultaneously%20providing%20causal%20explanations%20for%20these%0Apredictions.%20There%20are%20two%20key%20challenges%3A%20%281%29%20capturing%20the%20underlying%0Astructural%20and%20temporal%20information%20that%20remains%20consistent%20across%20both%0Aindependent%20and%20identically%20distributed%20%28IID%29%20and%20out-of-distribution%20%28OOD%29%0Adata%2C%20and%20%282%29%20efficiently%20generating%20high-quality%20link%20prediction%20results%20and%0Aexplanations.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20novel%20causal%20inference%0Amodel%2C%20namely%20the%20Independent%20and%20Confounded%20Causal%20Model%20%28ICCM%29.%20ICCM%20is%20then%0Aintegrated%20into%20a%20deep%20learning%20architecture%20that%20considers%20both%20effectiveness%0Aand%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20model%0Asignificantly%20outperforms%20existing%20methods%20across%20link%20prediction%20accuracy%2C%0Aexplanation%20quality%2C%20and%20robustness%20to%20shortcut%20features.%20Our%20code%20and%20datasets%0Aare%20anonymously%20released%20at%20https%3A//github.com/2024SIG/SIG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIG%253A%2520Efficient%2520Self-Interpretable%2520Graph%2520Neural%2520Network%2520for%250A%2520%2520Continuous-time%2520Dynamic%2520Graphs%26entry.906535625%3DLanting%2520Fang%2520and%2520Yulian%2520Yang%2520and%2520Kai%2520Wang%2520and%2520Shanshan%2520Feng%2520and%2520Kaiyu%2520Feng%2520and%2520Jie%2520Gui%2520and%2520Shuliang%2520Wang%2520and%2520Yew-Soon%2520Ong%26entry.1292438233%3D%2520%2520While%2520dynamic%2520graph%2520neural%2520networks%2520have%2520shown%2520promise%2520in%2520various%250Aapplications%252C%2520explaining%2520their%2520predictions%2520on%2520continuous-time%2520dynamic%2520graphs%250A%2528CTDGs%2529%2520is%2520difficult.%2520This%2520paper%2520investigates%2520a%2520new%2520research%2520task%253A%250Aself-interpretable%2520GNNs%2520for%2520CTDGs.%2520We%2520aim%2520to%2520predict%2520future%2520links%2520within%2520the%250Adynamic%2520graph%2520while%2520simultaneously%2520providing%2520causal%2520explanations%2520for%2520these%250Apredictions.%2520There%2520are%2520two%2520key%2520challenges%253A%2520%25281%2529%2520capturing%2520the%2520underlying%250Astructural%2520and%2520temporal%2520information%2520that%2520remains%2520consistent%2520across%2520both%250Aindependent%2520and%2520identically%2520distributed%2520%2528IID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%250Adata%252C%2520and%2520%25282%2529%2520efficiently%2520generating%2520high-quality%2520link%2520prediction%2520results%2520and%250Aexplanations.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520causal%2520inference%250Amodel%252C%2520namely%2520the%2520Independent%2520and%2520Confounded%2520Causal%2520Model%2520%2528ICCM%2529.%2520ICCM%2520is%2520then%250Aintegrated%2520into%2520a%2520deep%2520learning%2520architecture%2520that%2520considers%2520both%2520effectiveness%250Aand%2520efficiency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520model%250Asignificantly%2520outperforms%2520existing%2520methods%2520across%2520link%2520prediction%2520accuracy%252C%250Aexplanation%2520quality%252C%2520and%2520robustness%2520to%2520shortcut%2520features.%2520Our%2520code%2520and%2520datasets%250Aare%2520anonymously%2520released%2520at%2520https%253A//github.com/2024SIG/SIG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIG%3A%20Efficient%20Self-Interpretable%20Graph%20Neural%20Network%20for%0A%20%20Continuous-time%20Dynamic%20Graphs&entry.906535625=Lanting%20Fang%20and%20Yulian%20Yang%20and%20Kai%20Wang%20and%20Shanshan%20Feng%20and%20Kaiyu%20Feng%20and%20Jie%20Gui%20and%20Shuliang%20Wang%20and%20Yew-Soon%20Ong&entry.1292438233=%20%20While%20dynamic%20graph%20neural%20networks%20have%20shown%20promise%20in%20various%0Aapplications%2C%20explaining%20their%20predictions%20on%20continuous-time%20dynamic%20graphs%0A%28CTDGs%29%20is%20difficult.%20This%20paper%20investigates%20a%20new%20research%20task%3A%0Aself-interpretable%20GNNs%20for%20CTDGs.%20We%20aim%20to%20predict%20future%20links%20within%20the%0Adynamic%20graph%20while%20simultaneously%20providing%20causal%20explanations%20for%20these%0Apredictions.%20There%20are%20two%20key%20challenges%3A%20%281%29%20capturing%20the%20underlying%0Astructural%20and%20temporal%20information%20that%20remains%20consistent%20across%20both%0Aindependent%20and%20identically%20distributed%20%28IID%29%20and%20out-of-distribution%20%28OOD%29%0Adata%2C%20and%20%282%29%20efficiently%20generating%20high-quality%20link%20prediction%20results%20and%0Aexplanations.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20novel%20causal%20inference%0Amodel%2C%20namely%20the%20Independent%20and%20Confounded%20Causal%20Model%20%28ICCM%29.%20ICCM%20is%20then%0Aintegrated%20into%20a%20deep%20learning%20architecture%20that%20considers%20both%20effectiveness%0Aand%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20model%0Asignificantly%20outperforms%20existing%20methods%20across%20link%20prediction%20accuracy%2C%0Aexplanation%20quality%2C%20and%20robustness%20to%20shortcut%20features.%20Our%20code%20and%20datasets%0Aare%20anonymously%20released%20at%20https%3A//github.com/2024SIG/SIG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19062v1&entry.124074799=Read"},
{"title": "Incremental Object Detection with CLIP", "author": "Ziyue Huang and Yupeng He and Qingjie Liu and Yunhong Wang", "abstract": "  In contrast to the incremental classification task, the incremental detection\ntask is characterized by the presence of data ambiguity, as an image may have\ndifferently labeled bounding boxes across multiple continuous learning stages.\nThis phenomenon often impairs the model's ability to effectively learn new\nclasses. However, existing research has paid less attention to the forward\ncompatibility of the model, which limits its suitability for incremental\nlearning. To overcome this obstacle, we propose leveraging a visual-language\nmodel such as CLIP to generate text feature embeddings for different class\nsets, which enhances the feature space globally. We then employ super-classes\nto replace the unavailable novel classes in the early learning stage to\nsimulate the incremental scenario. Finally, we utilize the CLIP image encoder\nto accurately identify potential objects. We incorporate the finely recognized\ndetection boxes as pseudo-annotations into the training process, thereby\nfurther improving the detection performance. We evaluate our approach on\nvarious incremental learning settings using the PASCAL VOC 2007 dataset, and\nour approach outperforms state-of-the-art methods, particularly for recognizing\nthe new classes.\n", "link": "http://arxiv.org/abs/2310.08815v2", "date": "2024-05-29", "relevancy": 2.6274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5159}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Object%20Detection%20with%20CLIP&body=Title%3A%20Incremental%20Object%20Detection%20with%20CLIP%0AAuthor%3A%20Ziyue%20Huang%20and%20Yupeng%20He%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20In%20contrast%20to%20the%20incremental%20classification%20task%2C%20the%20incremental%20detection%0Atask%20is%20characterized%20by%20the%20presence%20of%20data%20ambiguity%2C%20as%20an%20image%20may%20have%0Adifferently%20labeled%20bounding%20boxes%20across%20multiple%20continuous%20learning%20stages.%0AThis%20phenomenon%20often%20impairs%20the%20model%27s%20ability%20to%20effectively%20learn%20new%0Aclasses.%20However%2C%20existing%20research%20has%20paid%20less%20attention%20to%20the%20forward%0Acompatibility%20of%20the%20model%2C%20which%20limits%20its%20suitability%20for%20incremental%0Alearning.%20To%20overcome%20this%20obstacle%2C%20we%20propose%20leveraging%20a%20visual-language%0Amodel%20such%20as%20CLIP%20to%20generate%20text%20feature%20embeddings%20for%20different%20class%0Asets%2C%20which%20enhances%20the%20feature%20space%20globally.%20We%20then%20employ%20super-classes%0Ato%20replace%20the%20unavailable%20novel%20classes%20in%20the%20early%20learning%20stage%20to%0Asimulate%20the%20incremental%20scenario.%20Finally%2C%20we%20utilize%20the%20CLIP%20image%20encoder%0Ato%20accurately%20identify%20potential%20objects.%20We%20incorporate%20the%20finely%20recognized%0Adetection%20boxes%20as%20pseudo-annotations%20into%20the%20training%20process%2C%20thereby%0Afurther%20improving%20the%20detection%20performance.%20We%20evaluate%20our%20approach%20on%0Avarious%20incremental%20learning%20settings%20using%20the%20PASCAL%20VOC%202007%20dataset%2C%20and%0Aour%20approach%20outperforms%20state-of-the-art%20methods%2C%20particularly%20for%20recognizing%0Athe%20new%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Object%2520Detection%2520with%2520CLIP%26entry.906535625%3DZiyue%2520Huang%2520and%2520Yupeng%2520He%2520and%2520Qingjie%2520Liu%2520and%2520Yunhong%2520Wang%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520the%2520incremental%2520classification%2520task%252C%2520the%2520incremental%2520detection%250Atask%2520is%2520characterized%2520by%2520the%2520presence%2520of%2520data%2520ambiguity%252C%2520as%2520an%2520image%2520may%2520have%250Adifferently%2520labeled%2520bounding%2520boxes%2520across%2520multiple%2520continuous%2520learning%2520stages.%250AThis%2520phenomenon%2520often%2520impairs%2520the%2520model%2527s%2520ability%2520to%2520effectively%2520learn%2520new%250Aclasses.%2520However%252C%2520existing%2520research%2520has%2520paid%2520less%2520attention%2520to%2520the%2520forward%250Acompatibility%2520of%2520the%2520model%252C%2520which%2520limits%2520its%2520suitability%2520for%2520incremental%250Alearning.%2520To%2520overcome%2520this%2520obstacle%252C%2520we%2520propose%2520leveraging%2520a%2520visual-language%250Amodel%2520such%2520as%2520CLIP%2520to%2520generate%2520text%2520feature%2520embeddings%2520for%2520different%2520class%250Asets%252C%2520which%2520enhances%2520the%2520feature%2520space%2520globally.%2520We%2520then%2520employ%2520super-classes%250Ato%2520replace%2520the%2520unavailable%2520novel%2520classes%2520in%2520the%2520early%2520learning%2520stage%2520to%250Asimulate%2520the%2520incremental%2520scenario.%2520Finally%252C%2520we%2520utilize%2520the%2520CLIP%2520image%2520encoder%250Ato%2520accurately%2520identify%2520potential%2520objects.%2520We%2520incorporate%2520the%2520finely%2520recognized%250Adetection%2520boxes%2520as%2520pseudo-annotations%2520into%2520the%2520training%2520process%252C%2520thereby%250Afurther%2520improving%2520the%2520detection%2520performance.%2520We%2520evaluate%2520our%2520approach%2520on%250Avarious%2520incremental%2520learning%2520settings%2520using%2520the%2520PASCAL%2520VOC%25202007%2520dataset%252C%2520and%250Aour%2520approach%2520outperforms%2520state-of-the-art%2520methods%252C%2520particularly%2520for%2520recognizing%250Athe%2520new%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Object%20Detection%20with%20CLIP&entry.906535625=Ziyue%20Huang%20and%20Yupeng%20He%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang&entry.1292438233=%20%20In%20contrast%20to%20the%20incremental%20classification%20task%2C%20the%20incremental%20detection%0Atask%20is%20characterized%20by%20the%20presence%20of%20data%20ambiguity%2C%20as%20an%20image%20may%20have%0Adifferently%20labeled%20bounding%20boxes%20across%20multiple%20continuous%20learning%20stages.%0AThis%20phenomenon%20often%20impairs%20the%20model%27s%20ability%20to%20effectively%20learn%20new%0Aclasses.%20However%2C%20existing%20research%20has%20paid%20less%20attention%20to%20the%20forward%0Acompatibility%20of%20the%20model%2C%20which%20limits%20its%20suitability%20for%20incremental%0Alearning.%20To%20overcome%20this%20obstacle%2C%20we%20propose%20leveraging%20a%20visual-language%0Amodel%20such%20as%20CLIP%20to%20generate%20text%20feature%20embeddings%20for%20different%20class%0Asets%2C%20which%20enhances%20the%20feature%20space%20globally.%20We%20then%20employ%20super-classes%0Ato%20replace%20the%20unavailable%20novel%20classes%20in%20the%20early%20learning%20stage%20to%0Asimulate%20the%20incremental%20scenario.%20Finally%2C%20we%20utilize%20the%20CLIP%20image%20encoder%0Ato%20accurately%20identify%20potential%20objects.%20We%20incorporate%20the%20finely%20recognized%0Adetection%20boxes%20as%20pseudo-annotations%20into%20the%20training%20process%2C%20thereby%0Afurther%20improving%20the%20detection%20performance.%20We%20evaluate%20our%20approach%20on%0Avarious%20incremental%20learning%20settings%20using%20the%20PASCAL%20VOC%202007%20dataset%2C%20and%0Aour%20approach%20outperforms%20state-of-the-art%20methods%2C%20particularly%20for%20recognizing%0Athe%20new%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08815v2&entry.124074799=Read"},
{"title": "Federated Learning under Partially Class-Disjoint Data via Manifold\n  Reshaping", "author": "Ziqing Fan and Jiangchao Yao and Ruipeng Zhang and Lingjuan Lyu and Ya Zhang and Yanfeng Wang", "abstract": "  Statistical heterogeneity severely limits the performance of federated\nlearning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn,\nto alleviate this problem. Despite effectiveness, their considered scenario\ngenerally requires samples from almost all classes during the local training of\neach client, although some covariate shifts may exist among clients. In fact,\nthe natural case of partially class-disjoint data (PCDD), where each client\ncontributes a few classes (instead of all classes) of samples, is practical yet\nunderexplored. Specifically, the unique collapse and invasion characteristics\nof PCDD can induce the biased optimization direction in local training, which\nprevents the efficiency of federated learning. To address this dilemma, we\npropose a manifold reshaping approach called FedMR to calibrate the feature\nspace of local training. Our FedMR adds two interplaying losses to the vanilla\nfederated learning: one is intra-class loss to decorrelate feature dimensions\nfor anti-collapse; and the other one is inter-class loss to guarantee the\nproper margin among categories in the feature expansion. We conduct extensive\nexperiments on a range of datasets to demonstrate that our FedMR achieves much\nhigher accuracy and better communication efficiency. Source code is available\nat: https://github.com/MediaBrain-SJTU/FedMR.git.\n", "link": "http://arxiv.org/abs/2405.18983v1", "date": "2024-05-29", "relevancy": 2.5829, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping&body=Title%3A%20Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping%0AAuthor%3A%20Ziqing%20Fan%20and%20Jiangchao%20Yao%20and%20Ruipeng%20Zhang%20and%20Lingjuan%20Lyu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Statistical%20heterogeneity%20severely%20limits%20the%20performance%20of%20federated%0Alearning%20%28FL%29%2C%20motivating%20several%20explorations%20e.g.%2C%20FedProx%2C%20MOON%20and%20FedDyn%2C%0Ato%20alleviate%20this%20problem.%20Despite%20effectiveness%2C%20their%20considered%20scenario%0Agenerally%20requires%20samples%20from%20almost%20all%20classes%20during%20the%20local%20training%20of%0Aeach%20client%2C%20although%20some%20covariate%20shifts%20may%20exist%20among%20clients.%20In%20fact%2C%0Athe%20natural%20case%20of%20partially%20class-disjoint%20data%20%28PCDD%29%2C%20where%20each%20client%0Acontributes%20a%20few%20classes%20%28instead%20of%20all%20classes%29%20of%20samples%2C%20is%20practical%20yet%0Aunderexplored.%20Specifically%2C%20the%20unique%20collapse%20and%20invasion%20characteristics%0Aof%20PCDD%20can%20induce%20the%20biased%20optimization%20direction%20in%20local%20training%2C%20which%0Aprevents%20the%20efficiency%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%0Apropose%20a%20manifold%20reshaping%20approach%20called%20FedMR%20to%20calibrate%20the%20feature%0Aspace%20of%20local%20training.%20Our%20FedMR%20adds%20two%20interplaying%20losses%20to%20the%20vanilla%0Afederated%20learning%3A%20one%20is%20intra-class%20loss%20to%20decorrelate%20feature%20dimensions%0Afor%20anti-collapse%3B%20and%20the%20other%20one%20is%20inter-class%20loss%20to%20guarantee%20the%0Aproper%20margin%20among%20categories%20in%20the%20feature%20expansion.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20range%20of%20datasets%20to%20demonstrate%20that%20our%20FedMR%20achieves%20much%0Ahigher%20accuracy%20and%20better%20communication%20efficiency.%20Source%20code%20is%20available%0Aat%3A%20https%3A//github.com/MediaBrain-SJTU/FedMR.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520under%2520Partially%2520Class-Disjoint%2520Data%2520via%2520Manifold%250A%2520%2520Reshaping%26entry.906535625%3DZiqing%2520Fan%2520and%2520Jiangchao%2520Yao%2520and%2520Ruipeng%2520Zhang%2520and%2520Lingjuan%2520Lyu%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Statistical%2520heterogeneity%2520severely%2520limits%2520the%2520performance%2520of%2520federated%250Alearning%2520%2528FL%2529%252C%2520motivating%2520several%2520explorations%2520e.g.%252C%2520FedProx%252C%2520MOON%2520and%2520FedDyn%252C%250Ato%2520alleviate%2520this%2520problem.%2520Despite%2520effectiveness%252C%2520their%2520considered%2520scenario%250Agenerally%2520requires%2520samples%2520from%2520almost%2520all%2520classes%2520during%2520the%2520local%2520training%2520of%250Aeach%2520client%252C%2520although%2520some%2520covariate%2520shifts%2520may%2520exist%2520among%2520clients.%2520In%2520fact%252C%250Athe%2520natural%2520case%2520of%2520partially%2520class-disjoint%2520data%2520%2528PCDD%2529%252C%2520where%2520each%2520client%250Acontributes%2520a%2520few%2520classes%2520%2528instead%2520of%2520all%2520classes%2529%2520of%2520samples%252C%2520is%2520practical%2520yet%250Aunderexplored.%2520Specifically%252C%2520the%2520unique%2520collapse%2520and%2520invasion%2520characteristics%250Aof%2520PCDD%2520can%2520induce%2520the%2520biased%2520optimization%2520direction%2520in%2520local%2520training%252C%2520which%250Aprevents%2520the%2520efficiency%2520of%2520federated%2520learning.%2520To%2520address%2520this%2520dilemma%252C%2520we%250Apropose%2520a%2520manifold%2520reshaping%2520approach%2520called%2520FedMR%2520to%2520calibrate%2520the%2520feature%250Aspace%2520of%2520local%2520training.%2520Our%2520FedMR%2520adds%2520two%2520interplaying%2520losses%2520to%2520the%2520vanilla%250Afederated%2520learning%253A%2520one%2520is%2520intra-class%2520loss%2520to%2520decorrelate%2520feature%2520dimensions%250Afor%2520anti-collapse%253B%2520and%2520the%2520other%2520one%2520is%2520inter-class%2520loss%2520to%2520guarantee%2520the%250Aproper%2520margin%2520among%2520categories%2520in%2520the%2520feature%2520expansion.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520a%2520range%2520of%2520datasets%2520to%2520demonstrate%2520that%2520our%2520FedMR%2520achieves%2520much%250Ahigher%2520accuracy%2520and%2520better%2520communication%2520efficiency.%2520Source%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/MediaBrain-SJTU/FedMR.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping&entry.906535625=Ziqing%20Fan%20and%20Jiangchao%20Yao%20and%20Ruipeng%20Zhang%20and%20Lingjuan%20Lyu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Statistical%20heterogeneity%20severely%20limits%20the%20performance%20of%20federated%0Alearning%20%28FL%29%2C%20motivating%20several%20explorations%20e.g.%2C%20FedProx%2C%20MOON%20and%20FedDyn%2C%0Ato%20alleviate%20this%20problem.%20Despite%20effectiveness%2C%20their%20considered%20scenario%0Agenerally%20requires%20samples%20from%20almost%20all%20classes%20during%20the%20local%20training%20of%0Aeach%20client%2C%20although%20some%20covariate%20shifts%20may%20exist%20among%20clients.%20In%20fact%2C%0Athe%20natural%20case%20of%20partially%20class-disjoint%20data%20%28PCDD%29%2C%20where%20each%20client%0Acontributes%20a%20few%20classes%20%28instead%20of%20all%20classes%29%20of%20samples%2C%20is%20practical%20yet%0Aunderexplored.%20Specifically%2C%20the%20unique%20collapse%20and%20invasion%20characteristics%0Aof%20PCDD%20can%20induce%20the%20biased%20optimization%20direction%20in%20local%20training%2C%20which%0Aprevents%20the%20efficiency%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%0Apropose%20a%20manifold%20reshaping%20approach%20called%20FedMR%20to%20calibrate%20the%20feature%0Aspace%20of%20local%20training.%20Our%20FedMR%20adds%20two%20interplaying%20losses%20to%20the%20vanilla%0Afederated%20learning%3A%20one%20is%20intra-class%20loss%20to%20decorrelate%20feature%20dimensions%0Afor%20anti-collapse%3B%20and%20the%20other%20one%20is%20inter-class%20loss%20to%20guarantee%20the%0Aproper%20margin%20among%20categories%20in%20the%20feature%20expansion.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20range%20of%20datasets%20to%20demonstrate%20that%20our%20FedMR%20achieves%20much%0Ahigher%20accuracy%20and%20better%20communication%20efficiency.%20Source%20code%20is%20available%0Aat%3A%20https%3A//github.com/MediaBrain-SJTU/FedMR.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18983v1&entry.124074799=Read"},
{"title": "Hierarchical Classification Auxiliary Network for Time Series\n  Forecasting", "author": "Yanru Sun and Zongxia Xie and Dongyue Chen and Emadeldeen Eldele and Qinghua Hu", "abstract": "  Deep learning has significantly advanced time series forecasting through its\npowerful capacity to capture sequence relationships. However, training these\nmodels with the Mean Square Error (MSE) loss often results in over-smooth\npredictions, making it challenging to handle the complexity and learn\nhigh-entropy features from time series data with high variability and\nunpredictability. In this work, we introduce a novel approach by tokenizing\ntime series values to train forecasting models via cross-entropy loss, while\nconsidering the continuous nature of time series data. Specifically, we propose\nHierarchical Classification Auxiliary Network, HCAN, a general model-agnostic\ncomponent that can be integrated with any forecasting model. HCAN is based on a\nHierarchy-Aware Attention module that integrates multi-granularity high-entropy\nfeatures at different hierarchy levels. At each level, we assign a class label\nfor timesteps to train an Uncertainty-Aware Classifier. This classifier\nmitigates the over-confidence in softmax loss via evidence theory. We also\nimplement a Hierarchical Consistency Loss to maintain prediction consistency\nacross hierarchy levels. Extensive experiments integrating HCAN with\nstate-of-the-art forecasting models demonstrate substantial improvements over\nbaselines on several real-world datasets. Code is available\nat:https://github.com/syrGitHub/HCAN.\n", "link": "http://arxiv.org/abs/2405.18975v1", "date": "2024-05-29", "relevancy": 2.5255, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5335}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Yanru%20Sun%20and%20Zongxia%20Xie%20and%20Dongyue%20Chen%20and%20Emadeldeen%20Eldele%20and%20Qinghua%20Hu%0AAbstract%3A%20%20%20Deep%20learning%20has%20significantly%20advanced%20time%20series%20forecasting%20through%20its%0Apowerful%20capacity%20to%20capture%20sequence%20relationships.%20However%2C%20training%20these%0Amodels%20with%20the%20Mean%20Square%20Error%20%28MSE%29%20loss%20often%20results%20in%20over-smooth%0Apredictions%2C%20making%20it%20challenging%20to%20handle%20the%20complexity%20and%20learn%0Ahigh-entropy%20features%20from%20time%20series%20data%20with%20high%20variability%20and%0Aunpredictability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20by%20tokenizing%0Atime%20series%20values%20to%20train%20forecasting%20models%20via%20cross-entropy%20loss%2C%20while%0Aconsidering%20the%20continuous%20nature%20of%20time%20series%20data.%20Specifically%2C%20we%20propose%0AHierarchical%20Classification%20Auxiliary%20Network%2C%20HCAN%2C%20a%20general%20model-agnostic%0Acomponent%20that%20can%20be%20integrated%20with%20any%20forecasting%20model.%20HCAN%20is%20based%20on%20a%0AHierarchy-Aware%20Attention%20module%20that%20integrates%20multi-granularity%20high-entropy%0Afeatures%20at%20different%20hierarchy%20levels.%20At%20each%20level%2C%20we%20assign%20a%20class%20label%0Afor%20timesteps%20to%20train%20an%20Uncertainty-Aware%20Classifier.%20This%20classifier%0Amitigates%20the%20over-confidence%20in%20softmax%20loss%20via%20evidence%20theory.%20We%20also%0Aimplement%20a%20Hierarchical%20Consistency%20Loss%20to%20maintain%20prediction%20consistency%0Aacross%20hierarchy%20levels.%20Extensive%20experiments%20integrating%20HCAN%20with%0Astate-of-the-art%20forecasting%20models%20demonstrate%20substantial%20improvements%20over%0Abaselines%20on%20several%20real-world%20datasets.%20Code%20is%20available%0Aat%3Ahttps%3A//github.com/syrGitHub/HCAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Classification%2520Auxiliary%2520Network%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DYanru%2520Sun%2520and%2520Zongxia%2520Xie%2520and%2520Dongyue%2520Chen%2520and%2520Emadeldeen%2520Eldele%2520and%2520Qinghua%2520Hu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520significantly%2520advanced%2520time%2520series%2520forecasting%2520through%2520its%250Apowerful%2520capacity%2520to%2520capture%2520sequence%2520relationships.%2520However%252C%2520training%2520these%250Amodels%2520with%2520the%2520Mean%2520Square%2520Error%2520%2528MSE%2529%2520loss%2520often%2520results%2520in%2520over-smooth%250Apredictions%252C%2520making%2520it%2520challenging%2520to%2520handle%2520the%2520complexity%2520and%2520learn%250Ahigh-entropy%2520features%2520from%2520time%2520series%2520data%2520with%2520high%2520variability%2520and%250Aunpredictability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520by%2520tokenizing%250Atime%2520series%2520values%2520to%2520train%2520forecasting%2520models%2520via%2520cross-entropy%2520loss%252C%2520while%250Aconsidering%2520the%2520continuous%2520nature%2520of%2520time%2520series%2520data.%2520Specifically%252C%2520we%2520propose%250AHierarchical%2520Classification%2520Auxiliary%2520Network%252C%2520HCAN%252C%2520a%2520general%2520model-agnostic%250Acomponent%2520that%2520can%2520be%2520integrated%2520with%2520any%2520forecasting%2520model.%2520HCAN%2520is%2520based%2520on%2520a%250AHierarchy-Aware%2520Attention%2520module%2520that%2520integrates%2520multi-granularity%2520high-entropy%250Afeatures%2520at%2520different%2520hierarchy%2520levels.%2520At%2520each%2520level%252C%2520we%2520assign%2520a%2520class%2520label%250Afor%2520timesteps%2520to%2520train%2520an%2520Uncertainty-Aware%2520Classifier.%2520This%2520classifier%250Amitigates%2520the%2520over-confidence%2520in%2520softmax%2520loss%2520via%2520evidence%2520theory.%2520We%2520also%250Aimplement%2520a%2520Hierarchical%2520Consistency%2520Loss%2520to%2520maintain%2520prediction%2520consistency%250Aacross%2520hierarchy%2520levels.%2520Extensive%2520experiments%2520integrating%2520HCAN%2520with%250Astate-of-the-art%2520forecasting%2520models%2520demonstrate%2520substantial%2520improvements%2520over%250Abaselines%2520on%2520several%2520real-world%2520datasets.%2520Code%2520is%2520available%250Aat%253Ahttps%253A//github.com/syrGitHub/HCAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Classification%20Auxiliary%20Network%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Yanru%20Sun%20and%20Zongxia%20Xie%20and%20Dongyue%20Chen%20and%20Emadeldeen%20Eldele%20and%20Qinghua%20Hu&entry.1292438233=%20%20Deep%20learning%20has%20significantly%20advanced%20time%20series%20forecasting%20through%20its%0Apowerful%20capacity%20to%20capture%20sequence%20relationships.%20However%2C%20training%20these%0Amodels%20with%20the%20Mean%20Square%20Error%20%28MSE%29%20loss%20often%20results%20in%20over-smooth%0Apredictions%2C%20making%20it%20challenging%20to%20handle%20the%20complexity%20and%20learn%0Ahigh-entropy%20features%20from%20time%20series%20data%20with%20high%20variability%20and%0Aunpredictability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20by%20tokenizing%0Atime%20series%20values%20to%20train%20forecasting%20models%20via%20cross-entropy%20loss%2C%20while%0Aconsidering%20the%20continuous%20nature%20of%20time%20series%20data.%20Specifically%2C%20we%20propose%0AHierarchical%20Classification%20Auxiliary%20Network%2C%20HCAN%2C%20a%20general%20model-agnostic%0Acomponent%20that%20can%20be%20integrated%20with%20any%20forecasting%20model.%20HCAN%20is%20based%20on%20a%0AHierarchy-Aware%20Attention%20module%20that%20integrates%20multi-granularity%20high-entropy%0Afeatures%20at%20different%20hierarchy%20levels.%20At%20each%20level%2C%20we%20assign%20a%20class%20label%0Afor%20timesteps%20to%20train%20an%20Uncertainty-Aware%20Classifier.%20This%20classifier%0Amitigates%20the%20over-confidence%20in%20softmax%20loss%20via%20evidence%20theory.%20We%20also%0Aimplement%20a%20Hierarchical%20Consistency%20Loss%20to%20maintain%20prediction%20consistency%0Aacross%20hierarchy%20levels.%20Extensive%20experiments%20integrating%20HCAN%20with%0Astate-of-the-art%20forecasting%20models%20demonstrate%20substantial%20improvements%20over%0Abaselines%20on%20several%20real-world%20datasets.%20Code%20is%20available%0Aat%3Ahttps%3A//github.com/syrGitHub/HCAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18975v1&entry.124074799=Read"},
{"title": "InstructVid2Vid: Controllable Video Editing with Natural Language\n  Instructions", "author": "Bosheng Qin and Juncheng Li and Siliang Tang and Tat-Seng Chua and Yueting Zhuang", "abstract": "  We introduce InstructVid2Vid, an end-to-end diffusion-based methodology for\nvideo editing guided by human language instructions. Our approach empowers\nvideo manipulation guided by natural language directives, eliminating the need\nfor per-example fine-tuning or inversion. The proposed InstructVid2Vid model\nmodifies a pretrained image generation model, Stable Diffusion, to generate a\ntime-dependent sequence of video frames. By harnessing the collective\nintelligence of disparate models, we engineer a training dataset rich in\nvideo-instruction triplets, which is a more cost-efficient alternative to\ncollecting data in real-world scenarios. To enhance the coherence between\nsuccessive frames within the generated videos, we propose the Inter-Frames\nConsistency Loss and incorporate it during the training process. With\nmultimodal classifier-free guidance during the inference stage, the generated\nvideos is able to resonate with both the input video and the accompanying\ninstructions. Experimental results demonstrate that InstructVid2Vid is capable\nof generating high-quality, temporally coherent videos and performing diverse\nedits, including attribute editing, background changes, and style transfer.\nThese results underscore the versatility and effectiveness of our proposed\nmethod.\n", "link": "http://arxiv.org/abs/2305.12328v2", "date": "2024-05-29", "relevancy": 2.5093, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6603}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructVid2Vid%3A%20Controllable%20Video%20Editing%20with%20Natural%20Language%0A%20%20Instructions&body=Title%3A%20InstructVid2Vid%3A%20Controllable%20Video%20Editing%20with%20Natural%20Language%0A%20%20Instructions%0AAuthor%3A%20Bosheng%20Qin%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Tat-Seng%20Chua%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20We%20introduce%20InstructVid2Vid%2C%20an%20end-to-end%20diffusion-based%20methodology%20for%0Avideo%20editing%20guided%20by%20human%20language%20instructions.%20Our%20approach%20empowers%0Avideo%20manipulation%20guided%20by%20natural%20language%20directives%2C%20eliminating%20the%20need%0Afor%20per-example%20fine-tuning%20or%20inversion.%20The%20proposed%20InstructVid2Vid%20model%0Amodifies%20a%20pretrained%20image%20generation%20model%2C%20Stable%20Diffusion%2C%20to%20generate%20a%0Atime-dependent%20sequence%20of%20video%20frames.%20By%20harnessing%20the%20collective%0Aintelligence%20of%20disparate%20models%2C%20we%20engineer%20a%20training%20dataset%20rich%20in%0Avideo-instruction%20triplets%2C%20which%20is%20a%20more%20cost-efficient%20alternative%20to%0Acollecting%20data%20in%20real-world%20scenarios.%20To%20enhance%20the%20coherence%20between%0Asuccessive%20frames%20within%20the%20generated%20videos%2C%20we%20propose%20the%20Inter-Frames%0AConsistency%20Loss%20and%20incorporate%20it%20during%20the%20training%20process.%20With%0Amultimodal%20classifier-free%20guidance%20during%20the%20inference%20stage%2C%20the%20generated%0Avideos%20is%20able%20to%20resonate%20with%20both%20the%20input%20video%20and%20the%20accompanying%0Ainstructions.%20Experimental%20results%20demonstrate%20that%20InstructVid2Vid%20is%20capable%0Aof%20generating%20high-quality%2C%20temporally%20coherent%20videos%20and%20performing%20diverse%0Aedits%2C%20including%20attribute%20editing%2C%20background%20changes%2C%20and%20style%20transfer.%0AThese%20results%20underscore%20the%20versatility%20and%20effectiveness%20of%20our%20proposed%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructVid2Vid%253A%2520Controllable%2520Video%2520Editing%2520with%2520Natural%2520Language%250A%2520%2520Instructions%26entry.906535625%3DBosheng%2520Qin%2520and%2520Juncheng%2520Li%2520and%2520Siliang%2520Tang%2520and%2520Tat-Seng%2520Chua%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InstructVid2Vid%252C%2520an%2520end-to-end%2520diffusion-based%2520methodology%2520for%250Avideo%2520editing%2520guided%2520by%2520human%2520language%2520instructions.%2520Our%2520approach%2520empowers%250Avideo%2520manipulation%2520guided%2520by%2520natural%2520language%2520directives%252C%2520eliminating%2520the%2520need%250Afor%2520per-example%2520fine-tuning%2520or%2520inversion.%2520The%2520proposed%2520InstructVid2Vid%2520model%250Amodifies%2520a%2520pretrained%2520image%2520generation%2520model%252C%2520Stable%2520Diffusion%252C%2520to%2520generate%2520a%250Atime-dependent%2520sequence%2520of%2520video%2520frames.%2520By%2520harnessing%2520the%2520collective%250Aintelligence%2520of%2520disparate%2520models%252C%2520we%2520engineer%2520a%2520training%2520dataset%2520rich%2520in%250Avideo-instruction%2520triplets%252C%2520which%2520is%2520a%2520more%2520cost-efficient%2520alternative%2520to%250Acollecting%2520data%2520in%2520real-world%2520scenarios.%2520To%2520enhance%2520the%2520coherence%2520between%250Asuccessive%2520frames%2520within%2520the%2520generated%2520videos%252C%2520we%2520propose%2520the%2520Inter-Frames%250AConsistency%2520Loss%2520and%2520incorporate%2520it%2520during%2520the%2520training%2520process.%2520With%250Amultimodal%2520classifier-free%2520guidance%2520during%2520the%2520inference%2520stage%252C%2520the%2520generated%250Avideos%2520is%2520able%2520to%2520resonate%2520with%2520both%2520the%2520input%2520video%2520and%2520the%2520accompanying%250Ainstructions.%2520Experimental%2520results%2520demonstrate%2520that%2520InstructVid2Vid%2520is%2520capable%250Aof%2520generating%2520high-quality%252C%2520temporally%2520coherent%2520videos%2520and%2520performing%2520diverse%250Aedits%252C%2520including%2520attribute%2520editing%252C%2520background%2520changes%252C%2520and%2520style%2520transfer.%250AThese%2520results%2520underscore%2520the%2520versatility%2520and%2520effectiveness%2520of%2520our%2520proposed%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructVid2Vid%3A%20Controllable%20Video%20Editing%20with%20Natural%20Language%0A%20%20Instructions&entry.906535625=Bosheng%20Qin%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Tat-Seng%20Chua%20and%20Yueting%20Zhuang&entry.1292438233=%20%20We%20introduce%20InstructVid2Vid%2C%20an%20end-to-end%20diffusion-based%20methodology%20for%0Avideo%20editing%20guided%20by%20human%20language%20instructions.%20Our%20approach%20empowers%0Avideo%20manipulation%20guided%20by%20natural%20language%20directives%2C%20eliminating%20the%20need%0Afor%20per-example%20fine-tuning%20or%20inversion.%20The%20proposed%20InstructVid2Vid%20model%0Amodifies%20a%20pretrained%20image%20generation%20model%2C%20Stable%20Diffusion%2C%20to%20generate%20a%0Atime-dependent%20sequence%20of%20video%20frames.%20By%20harnessing%20the%20collective%0Aintelligence%20of%20disparate%20models%2C%20we%20engineer%20a%20training%20dataset%20rich%20in%0Avideo-instruction%20triplets%2C%20which%20is%20a%20more%20cost-efficient%20alternative%20to%0Acollecting%20data%20in%20real-world%20scenarios.%20To%20enhance%20the%20coherence%20between%0Asuccessive%20frames%20within%20the%20generated%20videos%2C%20we%20propose%20the%20Inter-Frames%0AConsistency%20Loss%20and%20incorporate%20it%20during%20the%20training%20process.%20With%0Amultimodal%20classifier-free%20guidance%20during%20the%20inference%20stage%2C%20the%20generated%0Avideos%20is%20able%20to%20resonate%20with%20both%20the%20input%20video%20and%20the%20accompanying%0Ainstructions.%20Experimental%20results%20demonstrate%20that%20InstructVid2Vid%20is%20capable%0Aof%20generating%20high-quality%2C%20temporally%20coherent%20videos%20and%20performing%20diverse%0Aedits%2C%20including%20attribute%20editing%2C%20background%20changes%2C%20and%20style%20transfer.%0AThese%20results%20underscore%20the%20versatility%20and%20effectiveness%20of%20our%20proposed%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12328v2&entry.124074799=Read"},
{"title": "CiliaGraph: Enabling Expression-enhanced Hyper-Dimensional Computation\n  in Ultra-Lightweight and One-Shot Graph Classification on Edge", "author": "Yuxi Han and Jihe Wang and Danghui Wang", "abstract": "  Graph Neural Networks (GNNs) are computationally demanding and inefficient\nwhen applied to graph classification tasks in resource-constrained edge\nscenarios due to their inherent process, involving multiple rounds of forward\nand backward propagation. As a lightweight alternative, Hyper-Dimensional\nComputing (HDC), which leverages high-dimensional vectors for data encoding and\nprocessing, offers a more efficient solution by addressing computational\nbottleneck. However, current HDC methods primarily focus on static graphs and\nneglect to effectively capture node attributes and structural information,\nwhich leads to poor accuracy. In this work, we propose CiliaGraph, an enhanced\nexpressive yet ultra-lightweight HDC model for graph classification. This model\nintroduces a novel node encoding strategy that preserves relative distance\nisomorphism for accurate node connection representation. In addition, node\ndistances are utilized as edge weights for information aggregation, and the\nencoded node attributes and structural information are concatenated to obtain a\ncomprehensive graph representation. Furthermore, we explore the relationship\nbetween orthogonality and dimensionality to reduce the dimensions, thereby\nfurther enhancing computational efficiency. Compared to the SOTA GNNs,\nextensive experiments show that CiliaGraph reduces memory usage and accelerates\ntraining speed by an average of 292 times(up to 2341 times) and 103 times(up to\n313 times) respectively while maintaining comparable accuracy.\n", "link": "http://arxiv.org/abs/2405.19033v1", "date": "2024-05-29", "relevancy": 2.5054, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CiliaGraph%3A%20Enabling%20Expression-enhanced%20Hyper-Dimensional%20Computation%0A%20%20in%20Ultra-Lightweight%20and%20One-Shot%20Graph%20Classification%20on%20Edge&body=Title%3A%20CiliaGraph%3A%20Enabling%20Expression-enhanced%20Hyper-Dimensional%20Computation%0A%20%20in%20Ultra-Lightweight%20and%20One-Shot%20Graph%20Classification%20on%20Edge%0AAuthor%3A%20Yuxi%20Han%20and%20Jihe%20Wang%20and%20Danghui%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20computationally%20demanding%20and%20inefficient%0Awhen%20applied%20to%20graph%20classification%20tasks%20in%20resource-constrained%20edge%0Ascenarios%20due%20to%20their%20inherent%20process%2C%20involving%20multiple%20rounds%20of%20forward%0Aand%20backward%20propagation.%20As%20a%20lightweight%20alternative%2C%20Hyper-Dimensional%0AComputing%20%28HDC%29%2C%20which%20leverages%20high-dimensional%20vectors%20for%20data%20encoding%20and%0Aprocessing%2C%20offers%20a%20more%20efficient%20solution%20by%20addressing%20computational%0Abottleneck.%20However%2C%20current%20HDC%20methods%20primarily%20focus%20on%20static%20graphs%20and%0Aneglect%20to%20effectively%20capture%20node%20attributes%20and%20structural%20information%2C%0Awhich%20leads%20to%20poor%20accuracy.%20In%20this%20work%2C%20we%20propose%20CiliaGraph%2C%20an%20enhanced%0Aexpressive%20yet%20ultra-lightweight%20HDC%20model%20for%20graph%20classification.%20This%20model%0Aintroduces%20a%20novel%20node%20encoding%20strategy%20that%20preserves%20relative%20distance%0Aisomorphism%20for%20accurate%20node%20connection%20representation.%20In%20addition%2C%20node%0Adistances%20are%20utilized%20as%20edge%20weights%20for%20information%20aggregation%2C%20and%20the%0Aencoded%20node%20attributes%20and%20structural%20information%20are%20concatenated%20to%20obtain%20a%0Acomprehensive%20graph%20representation.%20Furthermore%2C%20we%20explore%20the%20relationship%0Abetween%20orthogonality%20and%20dimensionality%20to%20reduce%20the%20dimensions%2C%20thereby%0Afurther%20enhancing%20computational%20efficiency.%20Compared%20to%20the%20SOTA%20GNNs%2C%0Aextensive%20experiments%20show%20that%20CiliaGraph%20reduces%20memory%20usage%20and%20accelerates%0Atraining%20speed%20by%20an%20average%20of%20292%20times%28up%20to%202341%20times%29%20and%20103%20times%28up%20to%0A313%20times%29%20respectively%20while%20maintaining%20comparable%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCiliaGraph%253A%2520Enabling%2520Expression-enhanced%2520Hyper-Dimensional%2520Computation%250A%2520%2520in%2520Ultra-Lightweight%2520and%2520One-Shot%2520Graph%2520Classification%2520on%2520Edge%26entry.906535625%3DYuxi%2520Han%2520and%2520Jihe%2520Wang%2520and%2520Danghui%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520computationally%2520demanding%2520and%2520inefficient%250Awhen%2520applied%2520to%2520graph%2520classification%2520tasks%2520in%2520resource-constrained%2520edge%250Ascenarios%2520due%2520to%2520their%2520inherent%2520process%252C%2520involving%2520multiple%2520rounds%2520of%2520forward%250Aand%2520backward%2520propagation.%2520As%2520a%2520lightweight%2520alternative%252C%2520Hyper-Dimensional%250AComputing%2520%2528HDC%2529%252C%2520which%2520leverages%2520high-dimensional%2520vectors%2520for%2520data%2520encoding%2520and%250Aprocessing%252C%2520offers%2520a%2520more%2520efficient%2520solution%2520by%2520addressing%2520computational%250Abottleneck.%2520However%252C%2520current%2520HDC%2520methods%2520primarily%2520focus%2520on%2520static%2520graphs%2520and%250Aneglect%2520to%2520effectively%2520capture%2520node%2520attributes%2520and%2520structural%2520information%252C%250Awhich%2520leads%2520to%2520poor%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520propose%2520CiliaGraph%252C%2520an%2520enhanced%250Aexpressive%2520yet%2520ultra-lightweight%2520HDC%2520model%2520for%2520graph%2520classification.%2520This%2520model%250Aintroduces%2520a%2520novel%2520node%2520encoding%2520strategy%2520that%2520preserves%2520relative%2520distance%250Aisomorphism%2520for%2520accurate%2520node%2520connection%2520representation.%2520In%2520addition%252C%2520node%250Adistances%2520are%2520utilized%2520as%2520edge%2520weights%2520for%2520information%2520aggregation%252C%2520and%2520the%250Aencoded%2520node%2520attributes%2520and%2520structural%2520information%2520are%2520concatenated%2520to%2520obtain%2520a%250Acomprehensive%2520graph%2520representation.%2520Furthermore%252C%2520we%2520explore%2520the%2520relationship%250Abetween%2520orthogonality%2520and%2520dimensionality%2520to%2520reduce%2520the%2520dimensions%252C%2520thereby%250Afurther%2520enhancing%2520computational%2520efficiency.%2520Compared%2520to%2520the%2520SOTA%2520GNNs%252C%250Aextensive%2520experiments%2520show%2520that%2520CiliaGraph%2520reduces%2520memory%2520usage%2520and%2520accelerates%250Atraining%2520speed%2520by%2520an%2520average%2520of%2520292%2520times%2528up%2520to%25202341%2520times%2529%2520and%2520103%2520times%2528up%2520to%250A313%2520times%2529%2520respectively%2520while%2520maintaining%2520comparable%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CiliaGraph%3A%20Enabling%20Expression-enhanced%20Hyper-Dimensional%20Computation%0A%20%20in%20Ultra-Lightweight%20and%20One-Shot%20Graph%20Classification%20on%20Edge&entry.906535625=Yuxi%20Han%20and%20Jihe%20Wang%20and%20Danghui%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20computationally%20demanding%20and%20inefficient%0Awhen%20applied%20to%20graph%20classification%20tasks%20in%20resource-constrained%20edge%0Ascenarios%20due%20to%20their%20inherent%20process%2C%20involving%20multiple%20rounds%20of%20forward%0Aand%20backward%20propagation.%20As%20a%20lightweight%20alternative%2C%20Hyper-Dimensional%0AComputing%20%28HDC%29%2C%20which%20leverages%20high-dimensional%20vectors%20for%20data%20encoding%20and%0Aprocessing%2C%20offers%20a%20more%20efficient%20solution%20by%20addressing%20computational%0Abottleneck.%20However%2C%20current%20HDC%20methods%20primarily%20focus%20on%20static%20graphs%20and%0Aneglect%20to%20effectively%20capture%20node%20attributes%20and%20structural%20information%2C%0Awhich%20leads%20to%20poor%20accuracy.%20In%20this%20work%2C%20we%20propose%20CiliaGraph%2C%20an%20enhanced%0Aexpressive%20yet%20ultra-lightweight%20HDC%20model%20for%20graph%20classification.%20This%20model%0Aintroduces%20a%20novel%20node%20encoding%20strategy%20that%20preserves%20relative%20distance%0Aisomorphism%20for%20accurate%20node%20connection%20representation.%20In%20addition%2C%20node%0Adistances%20are%20utilized%20as%20edge%20weights%20for%20information%20aggregation%2C%20and%20the%0Aencoded%20node%20attributes%20and%20structural%20information%20are%20concatenated%20to%20obtain%20a%0Acomprehensive%20graph%20representation.%20Furthermore%2C%20we%20explore%20the%20relationship%0Abetween%20orthogonality%20and%20dimensionality%20to%20reduce%20the%20dimensions%2C%20thereby%0Afurther%20enhancing%20computational%20efficiency.%20Compared%20to%20the%20SOTA%20GNNs%2C%0Aextensive%20experiments%20show%20that%20CiliaGraph%20reduces%20memory%20usage%20and%20accelerates%0Atraining%20speed%20by%20an%20average%20of%20292%20times%28up%20to%202341%20times%29%20and%20103%20times%28up%20to%0A313%20times%29%20respectively%20while%20maintaining%20comparable%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19033v1&entry.124074799=Read"},
{"title": "Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance\n  ML Robustness", "author": "Mohamed elShehaby and Aditya Kotha and Ashraf Matrawy", "abstract": "  Adversarial training enhances the robustness of Machine Learning (ML) models\nagainst adversarial attacks. However, obtaining labeled training and\nadversarial training data in network/cybersecurity domains is challenging and\ncostly. Therefore, this letter introduces Adaptive Continuous Adversarial\nTraining (ACAT), a method that integrates adversarial training samples into the\nmodel during continuous learning sessions using real-world detected adversarial\ndata. Experimental results with a SPAM detection dataset demonstrate that ACAT\nreduces the time required for adversarial sample detection compared to\ntraditional processes. Moreover, the accuracy of the under-attack ML-based SPAM\nfilter increased from 69% to over 88% after just three retraining sessions.\n", "link": "http://arxiv.org/abs/2403.10461v2", "date": "2024-05-29", "relevancy": 2.4928, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Adaptive%20Continuous%20Adversarial%20Training%20%28ACAT%29%20to%20Enhance%0A%20%20ML%20Robustness&body=Title%3A%20Introducing%20Adaptive%20Continuous%20Adversarial%20Training%20%28ACAT%29%20to%20Enhance%0A%20%20ML%20Robustness%0AAuthor%3A%20Mohamed%20elShehaby%20and%20Aditya%20Kotha%20and%20Ashraf%20Matrawy%0AAbstract%3A%20%20%20Adversarial%20training%20enhances%20the%20robustness%20of%20Machine%20Learning%20%28ML%29%20models%0Aagainst%20adversarial%20attacks.%20However%2C%20obtaining%20labeled%20training%20and%0Aadversarial%20training%20data%20in%20network/cybersecurity%20domains%20is%20challenging%20and%0Acostly.%20Therefore%2C%20this%20letter%20introduces%20Adaptive%20Continuous%20Adversarial%0ATraining%20%28ACAT%29%2C%20a%20method%20that%20integrates%20adversarial%20training%20samples%20into%20the%0Amodel%20during%20continuous%20learning%20sessions%20using%20real-world%20detected%20adversarial%0Adata.%20Experimental%20results%20with%20a%20SPAM%20detection%20dataset%20demonstrate%20that%20ACAT%0Areduces%20the%20time%20required%20for%20adversarial%20sample%20detection%20compared%20to%0Atraditional%20processes.%20Moreover%2C%20the%20accuracy%20of%20the%20under-attack%20ML-based%20SPAM%0Afilter%20increased%20from%2069%25%20to%20over%2088%25%20after%20just%20three%20retraining%20sessions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Adaptive%2520Continuous%2520Adversarial%2520Training%2520%2528ACAT%2529%2520to%2520Enhance%250A%2520%2520ML%2520Robustness%26entry.906535625%3DMohamed%2520elShehaby%2520and%2520Aditya%2520Kotha%2520and%2520Ashraf%2520Matrawy%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520enhances%2520the%2520robustness%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520models%250Aagainst%2520adversarial%2520attacks.%2520However%252C%2520obtaining%2520labeled%2520training%2520and%250Aadversarial%2520training%2520data%2520in%2520network/cybersecurity%2520domains%2520is%2520challenging%2520and%250Acostly.%2520Therefore%252C%2520this%2520letter%2520introduces%2520Adaptive%2520Continuous%2520Adversarial%250ATraining%2520%2528ACAT%2529%252C%2520a%2520method%2520that%2520integrates%2520adversarial%2520training%2520samples%2520into%2520the%250Amodel%2520during%2520continuous%2520learning%2520sessions%2520using%2520real-world%2520detected%2520adversarial%250Adata.%2520Experimental%2520results%2520with%2520a%2520SPAM%2520detection%2520dataset%2520demonstrate%2520that%2520ACAT%250Areduces%2520the%2520time%2520required%2520for%2520adversarial%2520sample%2520detection%2520compared%2520to%250Atraditional%2520processes.%2520Moreover%252C%2520the%2520accuracy%2520of%2520the%2520under-attack%2520ML-based%2520SPAM%250Afilter%2520increased%2520from%252069%2525%2520to%2520over%252088%2525%2520after%2520just%2520three%2520retraining%2520sessions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Adaptive%20Continuous%20Adversarial%20Training%20%28ACAT%29%20to%20Enhance%0A%20%20ML%20Robustness&entry.906535625=Mohamed%20elShehaby%20and%20Aditya%20Kotha%20and%20Ashraf%20Matrawy&entry.1292438233=%20%20Adversarial%20training%20enhances%20the%20robustness%20of%20Machine%20Learning%20%28ML%29%20models%0Aagainst%20adversarial%20attacks.%20However%2C%20obtaining%20labeled%20training%20and%0Aadversarial%20training%20data%20in%20network/cybersecurity%20domains%20is%20challenging%20and%0Acostly.%20Therefore%2C%20this%20letter%20introduces%20Adaptive%20Continuous%20Adversarial%0ATraining%20%28ACAT%29%2C%20a%20method%20that%20integrates%20adversarial%20training%20samples%20into%20the%0Amodel%20during%20continuous%20learning%20sessions%20using%20real-world%20detected%20adversarial%0Adata.%20Experimental%20results%20with%20a%20SPAM%20detection%20dataset%20demonstrate%20that%20ACAT%0Areduces%20the%20time%20required%20for%20adversarial%20sample%20detection%20compared%20to%0Atraditional%20processes.%20Moreover%2C%20the%20accuracy%20of%20the%20under-attack%20ML-based%20SPAM%0Afilter%20increased%20from%2069%25%20to%20over%2088%25%20after%20just%20three%20retraining%20sessions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10461v2&entry.124074799=Read"},
{"title": "Continual Contrastive Spoken Language Understanding", "author": "Umberto Cappellazzo and Enrico Fini and Muqiao Yang and Daniele Falavigna and Alessio Brutti and Bhiksha Raj", "abstract": "  Recently, neural networks have shown impressive progress across diverse\nfields, with speech processing being no exception. However, recent\nbreakthroughs in this area require extensive offline training using large\ndatasets and tremendous computing resources. Unfortunately, these models\nstruggle to retain their previously acquired knowledge when learning new tasks\ncontinually, and retraining from scratch is almost always impractical. In this\npaper, we investigate the problem of learning sequence-to-sequence models for\nspoken language understanding in a class-incremental learning (CIL) setting and\nwe propose COCONUT, a CIL method that relies on the combination of experience\nreplay and contrastive learning. Through a modified version of the standard\nsupervised contrastive loss applied only to the rehearsal samples, COCONUT\npreserves the learned representations by pulling closer samples from the same\nclass and pushing away the others. Moreover, we leverage a multimodal\ncontrastive loss that helps the model learn more discriminative representations\nof the new data by aligning audio and text features. We also investigate\ndifferent contrastive designs to combine the strengths of the contrastive loss\nwith teacher-student architectures used for distillation. Experiments on two\nestablished SLU datasets reveal the effectiveness of our proposed approach and\nsignificant improvements over the baselines. We also show that COCONUT can be\ncombined with methods that operate on the decoder side of the model, resulting\nin further metrics improvements.\n", "link": "http://arxiv.org/abs/2310.02699v2", "date": "2024-05-29", "relevancy": 2.4854, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Contrastive%20Spoken%20Language%20Understanding&body=Title%3A%20Continual%20Contrastive%20Spoken%20Language%20Understanding%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Enrico%20Fini%20and%20Muqiao%20Yang%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Recently%2C%20neural%20networks%20have%20shown%20impressive%20progress%20across%20diverse%0Afields%2C%20with%20speech%20processing%20being%20no%20exception.%20However%2C%20recent%0Abreakthroughs%20in%20this%20area%20require%20extensive%20offline%20training%20using%20large%0Adatasets%20and%20tremendous%20computing%20resources.%20Unfortunately%2C%20these%20models%0Astruggle%20to%20retain%20their%20previously%20acquired%20knowledge%20when%20learning%20new%20tasks%0Acontinually%2C%20and%20retraining%20from%20scratch%20is%20almost%20always%20impractical.%20In%20this%0Apaper%2C%20we%20investigate%20the%20problem%20of%20learning%20sequence-to-sequence%20models%20for%0Aspoken%20language%20understanding%20in%20a%20class-incremental%20learning%20%28CIL%29%20setting%20and%0Awe%20propose%20COCONUT%2C%20a%20CIL%20method%20that%20relies%20on%20the%20combination%20of%20experience%0Areplay%20and%20contrastive%20learning.%20Through%20a%20modified%20version%20of%20the%20standard%0Asupervised%20contrastive%20loss%20applied%20only%20to%20the%20rehearsal%20samples%2C%20COCONUT%0Apreserves%20the%20learned%20representations%20by%20pulling%20closer%20samples%20from%20the%20same%0Aclass%20and%20pushing%20away%20the%20others.%20Moreover%2C%20we%20leverage%20a%20multimodal%0Acontrastive%20loss%20that%20helps%20the%20model%20learn%20more%20discriminative%20representations%0Aof%20the%20new%20data%20by%20aligning%20audio%20and%20text%20features.%20We%20also%20investigate%0Adifferent%20contrastive%20designs%20to%20combine%20the%20strengths%20of%20the%20contrastive%20loss%0Awith%20teacher-student%20architectures%20used%20for%20distillation.%20Experiments%20on%20two%0Aestablished%20SLU%20datasets%20reveal%20the%20effectiveness%20of%20our%20proposed%20approach%20and%0Asignificant%20improvements%20over%20the%20baselines.%20We%20also%20show%20that%20COCONUT%20can%20be%0Acombined%20with%20methods%20that%20operate%20on%20the%20decoder%20side%20of%20the%20model%2C%20resulting%0Ain%20further%20metrics%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Contrastive%2520Spoken%2520Language%2520Understanding%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Enrico%2520Fini%2520and%2520Muqiao%2520Yang%2520and%2520Daniele%2520Falavigna%2520and%2520Alessio%2520Brutti%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3D%2520%2520Recently%252C%2520neural%2520networks%2520have%2520shown%2520impressive%2520progress%2520across%2520diverse%250Afields%252C%2520with%2520speech%2520processing%2520being%2520no%2520exception.%2520However%252C%2520recent%250Abreakthroughs%2520in%2520this%2520area%2520require%2520extensive%2520offline%2520training%2520using%2520large%250Adatasets%2520and%2520tremendous%2520computing%2520resources.%2520Unfortunately%252C%2520these%2520models%250Astruggle%2520to%2520retain%2520their%2520previously%2520acquired%2520knowledge%2520when%2520learning%2520new%2520tasks%250Acontinually%252C%2520and%2520retraining%2520from%2520scratch%2520is%2520almost%2520always%2520impractical.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520problem%2520of%2520learning%2520sequence-to-sequence%2520models%2520for%250Aspoken%2520language%2520understanding%2520in%2520a%2520class-incremental%2520learning%2520%2528CIL%2529%2520setting%2520and%250Awe%2520propose%2520COCONUT%252C%2520a%2520CIL%2520method%2520that%2520relies%2520on%2520the%2520combination%2520of%2520experience%250Areplay%2520and%2520contrastive%2520learning.%2520Through%2520a%2520modified%2520version%2520of%2520the%2520standard%250Asupervised%2520contrastive%2520loss%2520applied%2520only%2520to%2520the%2520rehearsal%2520samples%252C%2520COCONUT%250Apreserves%2520the%2520learned%2520representations%2520by%2520pulling%2520closer%2520samples%2520from%2520the%2520same%250Aclass%2520and%2520pushing%2520away%2520the%2520others.%2520Moreover%252C%2520we%2520leverage%2520a%2520multimodal%250Acontrastive%2520loss%2520that%2520helps%2520the%2520model%2520learn%2520more%2520discriminative%2520representations%250Aof%2520the%2520new%2520data%2520by%2520aligning%2520audio%2520and%2520text%2520features.%2520We%2520also%2520investigate%250Adifferent%2520contrastive%2520designs%2520to%2520combine%2520the%2520strengths%2520of%2520the%2520contrastive%2520loss%250Awith%2520teacher-student%2520architectures%2520used%2520for%2520distillation.%2520Experiments%2520on%2520two%250Aestablished%2520SLU%2520datasets%2520reveal%2520the%2520effectiveness%2520of%2520our%2520proposed%2520approach%2520and%250Asignificant%2520improvements%2520over%2520the%2520baselines.%2520We%2520also%2520show%2520that%2520COCONUT%2520can%2520be%250Acombined%2520with%2520methods%2520that%2520operate%2520on%2520the%2520decoder%2520side%2520of%2520the%2520model%252C%2520resulting%250Ain%2520further%2520metrics%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Contrastive%20Spoken%20Language%20Understanding&entry.906535625=Umberto%20Cappellazzo%20and%20Enrico%20Fini%20and%20Muqiao%20Yang%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Recently%2C%20neural%20networks%20have%20shown%20impressive%20progress%20across%20diverse%0Afields%2C%20with%20speech%20processing%20being%20no%20exception.%20However%2C%20recent%0Abreakthroughs%20in%20this%20area%20require%20extensive%20offline%20training%20using%20large%0Adatasets%20and%20tremendous%20computing%20resources.%20Unfortunately%2C%20these%20models%0Astruggle%20to%20retain%20their%20previously%20acquired%20knowledge%20when%20learning%20new%20tasks%0Acontinually%2C%20and%20retraining%20from%20scratch%20is%20almost%20always%20impractical.%20In%20this%0Apaper%2C%20we%20investigate%20the%20problem%20of%20learning%20sequence-to-sequence%20models%20for%0Aspoken%20language%20understanding%20in%20a%20class-incremental%20learning%20%28CIL%29%20setting%20and%0Awe%20propose%20COCONUT%2C%20a%20CIL%20method%20that%20relies%20on%20the%20combination%20of%20experience%0Areplay%20and%20contrastive%20learning.%20Through%20a%20modified%20version%20of%20the%20standard%0Asupervised%20contrastive%20loss%20applied%20only%20to%20the%20rehearsal%20samples%2C%20COCONUT%0Apreserves%20the%20learned%20representations%20by%20pulling%20closer%20samples%20from%20the%20same%0Aclass%20and%20pushing%20away%20the%20others.%20Moreover%2C%20we%20leverage%20a%20multimodal%0Acontrastive%20loss%20that%20helps%20the%20model%20learn%20more%20discriminative%20representations%0Aof%20the%20new%20data%20by%20aligning%20audio%20and%20text%20features.%20We%20also%20investigate%0Adifferent%20contrastive%20designs%20to%20combine%20the%20strengths%20of%20the%20contrastive%20loss%0Awith%20teacher-student%20architectures%20used%20for%20distillation.%20Experiments%20on%20two%0Aestablished%20SLU%20datasets%20reveal%20the%20effectiveness%20of%20our%20proposed%20approach%20and%0Asignificant%20improvements%20over%20the%20baselines.%20We%20also%20show%20that%20COCONUT%20can%20be%0Acombined%20with%20methods%20that%20operate%20on%20the%20decoder%20side%20of%20the%20model%2C%20resulting%0Ain%20further%20metrics%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02699v2&entry.124074799=Read"},
{"title": "EasyAnimate: A High-Performance Long Video Generation Method based on\n  Transformer Architecture", "author": "Jiaqi Xu and Xinyi Zou and Kunzhe Huang and Yunkuo Chen and Bo Liu and MengLi Cheng and Xing Shi and Jun Huang", "abstract": "  This paper presents EasyAnimate, an advanced method for video generation that\nleverages the power of transformer architecture for high-performance outcomes.\nWe have expanded the DiT framework originally designed for 2D image synthesis\nto accommodate the complexities of 3D video generation by incorporating a\nmotion module block. It is used to capture temporal dynamics, thereby ensuring\nthe production of consistent frames and seamless motion transitions. The motion\nmodule can be adapted to various DiT baseline methods to generate video with\ndifferent styles. It can also generate videos with different frame rates and\nresolutions during both training and inference phases, suitable for both images\nand videos. Moreover, we introduce slice VAE, a novel approach to condense the\ntemporal axis, facilitating the generation of long duration videos. Currently,\nEasyAnimate exhibits the proficiency to generate videos with 144 frames. We\nprovide a holistic ecosystem for video production based on DiT, encompassing\naspects such as data pre-processing, VAE training, DiT models training (both\nthe baseline model and LoRA model), and end-to-end video inference. Code is\navailable at: https://github.com/aigc-apps/EasyAnimate. We are continuously\nworking to enhance the performance of our method.\n", "link": "http://arxiv.org/abs/2405.18991v1", "date": "2024-05-29", "relevancy": 2.4703, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6185}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6172}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture&body=Title%3A%20EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture%0AAuthor%3A%20Jiaqi%20Xu%20and%20Xinyi%20Zou%20and%20Kunzhe%20Huang%20and%20Yunkuo%20Chen%20and%20Bo%20Liu%20and%20MengLi%20Cheng%20and%20Xing%20Shi%20and%20Jun%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20EasyAnimate%2C%20an%20advanced%20method%20for%20video%20generation%20that%0Aleverages%20the%20power%20of%20transformer%20architecture%20for%20high-performance%20outcomes.%0AWe%20have%20expanded%20the%20DiT%20framework%20originally%20designed%20for%202D%20image%20synthesis%0Ato%20accommodate%20the%20complexities%20of%203D%20video%20generation%20by%20incorporating%20a%0Amotion%20module%20block.%20It%20is%20used%20to%20capture%20temporal%20dynamics%2C%20thereby%20ensuring%0Athe%20production%20of%20consistent%20frames%20and%20seamless%20motion%20transitions.%20The%20motion%0Amodule%20can%20be%20adapted%20to%20various%20DiT%20baseline%20methods%20to%20generate%20video%20with%0Adifferent%20styles.%20It%20can%20also%20generate%20videos%20with%20different%20frame%20rates%20and%0Aresolutions%20during%20both%20training%20and%20inference%20phases%2C%20suitable%20for%20both%20images%0Aand%20videos.%20Moreover%2C%20we%20introduce%20slice%20VAE%2C%20a%20novel%20approach%20to%20condense%20the%0Atemporal%20axis%2C%20facilitating%20the%20generation%20of%20long%20duration%20videos.%20Currently%2C%0AEasyAnimate%20exhibits%20the%20proficiency%20to%20generate%20videos%20with%20144%20frames.%20We%0Aprovide%20a%20holistic%20ecosystem%20for%20video%20production%20based%20on%20DiT%2C%20encompassing%0Aaspects%20such%20as%20data%20pre-processing%2C%20VAE%20training%2C%20DiT%20models%20training%20%28both%0Athe%20baseline%20model%20and%20LoRA%20model%29%2C%20and%20end-to-end%20video%20inference.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/aigc-apps/EasyAnimate.%20We%20are%20continuously%0Aworking%20to%20enhance%20the%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyAnimate%253A%2520A%2520High-Performance%2520Long%2520Video%2520Generation%2520Method%2520based%2520on%250A%2520%2520Transformer%2520Architecture%26entry.906535625%3DJiaqi%2520Xu%2520and%2520Xinyi%2520Zou%2520and%2520Kunzhe%2520Huang%2520and%2520Yunkuo%2520Chen%2520and%2520Bo%2520Liu%2520and%2520MengLi%2520Cheng%2520and%2520Xing%2520Shi%2520and%2520Jun%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520EasyAnimate%252C%2520an%2520advanced%2520method%2520for%2520video%2520generation%2520that%250Aleverages%2520the%2520power%2520of%2520transformer%2520architecture%2520for%2520high-performance%2520outcomes.%250AWe%2520have%2520expanded%2520the%2520DiT%2520framework%2520originally%2520designed%2520for%25202D%2520image%2520synthesis%250Ato%2520accommodate%2520the%2520complexities%2520of%25203D%2520video%2520generation%2520by%2520incorporating%2520a%250Amotion%2520module%2520block.%2520It%2520is%2520used%2520to%2520capture%2520temporal%2520dynamics%252C%2520thereby%2520ensuring%250Athe%2520production%2520of%2520consistent%2520frames%2520and%2520seamless%2520motion%2520transitions.%2520The%2520motion%250Amodule%2520can%2520be%2520adapted%2520to%2520various%2520DiT%2520baseline%2520methods%2520to%2520generate%2520video%2520with%250Adifferent%2520styles.%2520It%2520can%2520also%2520generate%2520videos%2520with%2520different%2520frame%2520rates%2520and%250Aresolutions%2520during%2520both%2520training%2520and%2520inference%2520phases%252C%2520suitable%2520for%2520both%2520images%250Aand%2520videos.%2520Moreover%252C%2520we%2520introduce%2520slice%2520VAE%252C%2520a%2520novel%2520approach%2520to%2520condense%2520the%250Atemporal%2520axis%252C%2520facilitating%2520the%2520generation%2520of%2520long%2520duration%2520videos.%2520Currently%252C%250AEasyAnimate%2520exhibits%2520the%2520proficiency%2520to%2520generate%2520videos%2520with%2520144%2520frames.%2520We%250Aprovide%2520a%2520holistic%2520ecosystem%2520for%2520video%2520production%2520based%2520on%2520DiT%252C%2520encompassing%250Aaspects%2520such%2520as%2520data%2520pre-processing%252C%2520VAE%2520training%252C%2520DiT%2520models%2520training%2520%2528both%250Athe%2520baseline%2520model%2520and%2520LoRA%2520model%2529%252C%2520and%2520end-to-end%2520video%2520inference.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/aigc-apps/EasyAnimate.%2520We%2520are%2520continuously%250Aworking%2520to%2520enhance%2520the%2520performance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture&entry.906535625=Jiaqi%20Xu%20and%20Xinyi%20Zou%20and%20Kunzhe%20Huang%20and%20Yunkuo%20Chen%20and%20Bo%20Liu%20and%20MengLi%20Cheng%20and%20Xing%20Shi%20and%20Jun%20Huang&entry.1292438233=%20%20This%20paper%20presents%20EasyAnimate%2C%20an%20advanced%20method%20for%20video%20generation%20that%0Aleverages%20the%20power%20of%20transformer%20architecture%20for%20high-performance%20outcomes.%0AWe%20have%20expanded%20the%20DiT%20framework%20originally%20designed%20for%202D%20image%20synthesis%0Ato%20accommodate%20the%20complexities%20of%203D%20video%20generation%20by%20incorporating%20a%0Amotion%20module%20block.%20It%20is%20used%20to%20capture%20temporal%20dynamics%2C%20thereby%20ensuring%0Athe%20production%20of%20consistent%20frames%20and%20seamless%20motion%20transitions.%20The%20motion%0Amodule%20can%20be%20adapted%20to%20various%20DiT%20baseline%20methods%20to%20generate%20video%20with%0Adifferent%20styles.%20It%20can%20also%20generate%20videos%20with%20different%20frame%20rates%20and%0Aresolutions%20during%20both%20training%20and%20inference%20phases%2C%20suitable%20for%20both%20images%0Aand%20videos.%20Moreover%2C%20we%20introduce%20slice%20VAE%2C%20a%20novel%20approach%20to%20condense%20the%0Atemporal%20axis%2C%20facilitating%20the%20generation%20of%20long%20duration%20videos.%20Currently%2C%0AEasyAnimate%20exhibits%20the%20proficiency%20to%20generate%20videos%20with%20144%20frames.%20We%0Aprovide%20a%20holistic%20ecosystem%20for%20video%20production%20based%20on%20DiT%2C%20encompassing%0Aaspects%20such%20as%20data%20pre-processing%2C%20VAE%20training%2C%20DiT%20models%20training%20%28both%0Athe%20baseline%20model%20and%20LoRA%20model%29%2C%20and%20end-to-end%20video%20inference.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/aigc-apps/EasyAnimate.%20We%20are%20continuously%0Aworking%20to%20enhance%20the%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18991v1&entry.124074799=Read"},
{"title": "Efficient Error Certification for Physics-Informed Neural Networks", "author": "Francisco Eiras and Adel Bibi and Rudy Bunel and Krishnamurthy Dj Dvijotham and Philip Torr and M. Pawan Kumar", "abstract": "  Recent work provides promising evidence that Physics-Informed Neural Networks\n(PINN) can efficiently solve partial differential equations (PDE). However,\nprevious works have failed to provide guarantees on the worst-case residual\nerror of a PINN across the spatio-temporal domain - a measure akin to the\ntolerance of numerical solvers - focusing instead on point-wise comparisons\nbetween their solution and the ones obtained by a solver on a set of inputs. In\nreal-world applications, one cannot consider tests on a finite set of points to\nbe sufficient grounds for deployment, as the performance could be substantially\nworse on a different set. To alleviate this issue, we establish guaranteed\nerror-based conditions for PINNs over their continuous applicability domain. To\nverify the extent to which they hold, we introduce $\\partial$-CROWN: a general,\nefficient and scalable post-training framework to bound PINN residual errors.\nWe demonstrate its effectiveness in obtaining tight certificates by applying it\nto two classically studied PINNs - Burgers' and Schr\\\"odinger's equations -,\nand two more challenging ones with real-world applications - the Allan-Cahn and\nDiffusion-Sorption equations.\n", "link": "http://arxiv.org/abs/2305.10157v2", "date": "2024-05-29", "relevancy": 2.4462, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4863}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Error%20Certification%20for%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Efficient%20Error%20Certification%20for%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Francisco%20Eiras%20and%20Adel%20Bibi%20and%20Rudy%20Bunel%20and%20Krishnamurthy%20Dj%20Dvijotham%20and%20Philip%20Torr%20and%20M.%20Pawan%20Kumar%0AAbstract%3A%20%20%20Recent%20work%20provides%20promising%20evidence%20that%20Physics-Informed%20Neural%20Networks%0A%28PINN%29%20can%20efficiently%20solve%20partial%20differential%20equations%20%28PDE%29.%20However%2C%0Aprevious%20works%20have%20failed%20to%20provide%20guarantees%20on%20the%20worst-case%20residual%0Aerror%20of%20a%20PINN%20across%20the%20spatio-temporal%20domain%20-%20a%20measure%20akin%20to%20the%0Atolerance%20of%20numerical%20solvers%20-%20focusing%20instead%20on%20point-wise%20comparisons%0Abetween%20their%20solution%20and%20the%20ones%20obtained%20by%20a%20solver%20on%20a%20set%20of%20inputs.%20In%0Areal-world%20applications%2C%20one%20cannot%20consider%20tests%20on%20a%20finite%20set%20of%20points%20to%0Abe%20sufficient%20grounds%20for%20deployment%2C%20as%20the%20performance%20could%20be%20substantially%0Aworse%20on%20a%20different%20set.%20To%20alleviate%20this%20issue%2C%20we%20establish%20guaranteed%0Aerror-based%20conditions%20for%20PINNs%20over%20their%20continuous%20applicability%20domain.%20To%0Averify%20the%20extent%20to%20which%20they%20hold%2C%20we%20introduce%20%24%5Cpartial%24-CROWN%3A%20a%20general%2C%0Aefficient%20and%20scalable%20post-training%20framework%20to%20bound%20PINN%20residual%20errors.%0AWe%20demonstrate%20its%20effectiveness%20in%20obtaining%20tight%20certificates%20by%20applying%20it%0Ato%20two%20classically%20studied%20PINNs%20-%20Burgers%27%20and%20Schr%5C%22odinger%27s%20equations%20-%2C%0Aand%20two%20more%20challenging%20ones%20with%20real-world%20applications%20-%20the%20Allan-Cahn%20and%0ADiffusion-Sorption%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Error%2520Certification%2520for%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DFrancisco%2520Eiras%2520and%2520Adel%2520Bibi%2520and%2520Rudy%2520Bunel%2520and%2520Krishnamurthy%2520Dj%2520Dvijotham%2520and%2520Philip%2520Torr%2520and%2520M.%2520Pawan%2520Kumar%26entry.1292438233%3D%2520%2520Recent%2520work%2520provides%2520promising%2520evidence%2520that%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINN%2529%2520can%2520efficiently%2520solve%2520partial%2520differential%2520equations%2520%2528PDE%2529.%2520However%252C%250Aprevious%2520works%2520have%2520failed%2520to%2520provide%2520guarantees%2520on%2520the%2520worst-case%2520residual%250Aerror%2520of%2520a%2520PINN%2520across%2520the%2520spatio-temporal%2520domain%2520-%2520a%2520measure%2520akin%2520to%2520the%250Atolerance%2520of%2520numerical%2520solvers%2520-%2520focusing%2520instead%2520on%2520point-wise%2520comparisons%250Abetween%2520their%2520solution%2520and%2520the%2520ones%2520obtained%2520by%2520a%2520solver%2520on%2520a%2520set%2520of%2520inputs.%2520In%250Areal-world%2520applications%252C%2520one%2520cannot%2520consider%2520tests%2520on%2520a%2520finite%2520set%2520of%2520points%2520to%250Abe%2520sufficient%2520grounds%2520for%2520deployment%252C%2520as%2520the%2520performance%2520could%2520be%2520substantially%250Aworse%2520on%2520a%2520different%2520set.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520establish%2520guaranteed%250Aerror-based%2520conditions%2520for%2520PINNs%2520over%2520their%2520continuous%2520applicability%2520domain.%2520To%250Averify%2520the%2520extent%2520to%2520which%2520they%2520hold%252C%2520we%2520introduce%2520%2524%255Cpartial%2524-CROWN%253A%2520a%2520general%252C%250Aefficient%2520and%2520scalable%2520post-training%2520framework%2520to%2520bound%2520PINN%2520residual%2520errors.%250AWe%2520demonstrate%2520its%2520effectiveness%2520in%2520obtaining%2520tight%2520certificates%2520by%2520applying%2520it%250Ato%2520two%2520classically%2520studied%2520PINNs%2520-%2520Burgers%2527%2520and%2520Schr%255C%2522odinger%2527s%2520equations%2520-%252C%250Aand%2520two%2520more%2520challenging%2520ones%2520with%2520real-world%2520applications%2520-%2520the%2520Allan-Cahn%2520and%250ADiffusion-Sorption%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Error%20Certification%20for%20Physics-Informed%20Neural%20Networks&entry.906535625=Francisco%20Eiras%20and%20Adel%20Bibi%20and%20Rudy%20Bunel%20and%20Krishnamurthy%20Dj%20Dvijotham%20and%20Philip%20Torr%20and%20M.%20Pawan%20Kumar&entry.1292438233=%20%20Recent%20work%20provides%20promising%20evidence%20that%20Physics-Informed%20Neural%20Networks%0A%28PINN%29%20can%20efficiently%20solve%20partial%20differential%20equations%20%28PDE%29.%20However%2C%0Aprevious%20works%20have%20failed%20to%20provide%20guarantees%20on%20the%20worst-case%20residual%0Aerror%20of%20a%20PINN%20across%20the%20spatio-temporal%20domain%20-%20a%20measure%20akin%20to%20the%0Atolerance%20of%20numerical%20solvers%20-%20focusing%20instead%20on%20point-wise%20comparisons%0Abetween%20their%20solution%20and%20the%20ones%20obtained%20by%20a%20solver%20on%20a%20set%20of%20inputs.%20In%0Areal-world%20applications%2C%20one%20cannot%20consider%20tests%20on%20a%20finite%20set%20of%20points%20to%0Abe%20sufficient%20grounds%20for%20deployment%2C%20as%20the%20performance%20could%20be%20substantially%0Aworse%20on%20a%20different%20set.%20To%20alleviate%20this%20issue%2C%20we%20establish%20guaranteed%0Aerror-based%20conditions%20for%20PINNs%20over%20their%20continuous%20applicability%20domain.%20To%0Averify%20the%20extent%20to%20which%20they%20hold%2C%20we%20introduce%20%24%5Cpartial%24-CROWN%3A%20a%20general%2C%0Aefficient%20and%20scalable%20post-training%20framework%20to%20bound%20PINN%20residual%20errors.%0AWe%20demonstrate%20its%20effectiveness%20in%20obtaining%20tight%20certificates%20by%20applying%20it%0Ato%20two%20classically%20studied%20PINNs%20-%20Burgers%27%20and%20Schr%5C%22odinger%27s%20equations%20-%2C%0Aand%20two%20more%20challenging%20ones%20with%20real-world%20applications%20-%20the%20Allan-Cahn%20and%0ADiffusion-Sorption%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10157v2&entry.124074799=Read"},
{"title": "Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo\n  Algorithms", "author": "Tim Tsz-Kit Lau and Han Liu and Thomas Pock", "abstract": "  We study the problem of approximate sampling from non-log-concave\ndistributions, e.g., Gaussian mixtures, which is often challenging even in low\ndimensions due to their multimodality. We focus on performing this task via\nMarkov chain Monte Carlo (MCMC) methods derived from discretizations of the\noverdamped Langevin diffusions, which are commonly known as Langevin Monte\nCarlo algorithms. Furthermore, we are also interested in two nonsmooth cases\nfor which a large class of proximal MCMC methods have been developed: (i) a\nnonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a\nLaplacian mixture distribution. Such nonsmooth and non-log-concave sampling\ntasks arise from a wide range of applications to Bayesian inference and imaging\ninverse problems such as image deconvolution. We perform numerical simulations\nto compare the performance of most commonly used Langevin Monte Carlo\nalgorithms.\n", "link": "http://arxiv.org/abs/2305.15988v2", "date": "2024-05-29", "relevancy": 2.4444, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5054}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Log-Concave%20and%20Nonsmooth%20Sampling%20via%20Langevin%20Monte%20Carlo%0A%20%20Algorithms&body=Title%3A%20Non-Log-Concave%20and%20Nonsmooth%20Sampling%20via%20Langevin%20Monte%20Carlo%0A%20%20Algorithms%0AAuthor%3A%20Tim%20Tsz-Kit%20Lau%20and%20Han%20Liu%20and%20Thomas%20Pock%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20approximate%20sampling%20from%20non-log-concave%0Adistributions%2C%20e.g.%2C%20Gaussian%20mixtures%2C%20which%20is%20often%20challenging%20even%20in%20low%0Adimensions%20due%20to%20their%20multimodality.%20We%20focus%20on%20performing%20this%20task%20via%0AMarkov%20chain%20Monte%20Carlo%20%28MCMC%29%20methods%20derived%20from%20discretizations%20of%20the%0Aoverdamped%20Langevin%20diffusions%2C%20which%20are%20commonly%20known%20as%20Langevin%20Monte%0ACarlo%20algorithms.%20Furthermore%2C%20we%20are%20also%20interested%20in%20two%20nonsmooth%20cases%0Afor%20which%20a%20large%20class%20of%20proximal%20MCMC%20methods%20have%20been%20developed%3A%20%28i%29%20a%0Anonsmooth%20prior%20is%20considered%20with%20a%20Gaussian%20mixture%20likelihood%3B%20%28ii%29%20a%0ALaplacian%20mixture%20distribution.%20Such%20nonsmooth%20and%20non-log-concave%20sampling%0Atasks%20arise%20from%20a%20wide%20range%20of%20applications%20to%20Bayesian%20inference%20and%20imaging%0Ainverse%20problems%20such%20as%20image%20deconvolution.%20We%20perform%20numerical%20simulations%0Ato%20compare%20the%20performance%20of%20most%20commonly%20used%20Langevin%20Monte%20Carlo%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Log-Concave%2520and%2520Nonsmooth%2520Sampling%2520via%2520Langevin%2520Monte%2520Carlo%250A%2520%2520Algorithms%26entry.906535625%3DTim%2520Tsz-Kit%2520Lau%2520and%2520Han%2520Liu%2520and%2520Thomas%2520Pock%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520approximate%2520sampling%2520from%2520non-log-concave%250Adistributions%252C%2520e.g.%252C%2520Gaussian%2520mixtures%252C%2520which%2520is%2520often%2520challenging%2520even%2520in%2520low%250Adimensions%2520due%2520to%2520their%2520multimodality.%2520We%2520focus%2520on%2520performing%2520this%2520task%2520via%250AMarkov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520methods%2520derived%2520from%2520discretizations%2520of%2520the%250Aoverdamped%2520Langevin%2520diffusions%252C%2520which%2520are%2520commonly%2520known%2520as%2520Langevin%2520Monte%250ACarlo%2520algorithms.%2520Furthermore%252C%2520we%2520are%2520also%2520interested%2520in%2520two%2520nonsmooth%2520cases%250Afor%2520which%2520a%2520large%2520class%2520of%2520proximal%2520MCMC%2520methods%2520have%2520been%2520developed%253A%2520%2528i%2529%2520a%250Anonsmooth%2520prior%2520is%2520considered%2520with%2520a%2520Gaussian%2520mixture%2520likelihood%253B%2520%2528ii%2529%2520a%250ALaplacian%2520mixture%2520distribution.%2520Such%2520nonsmooth%2520and%2520non-log-concave%2520sampling%250Atasks%2520arise%2520from%2520a%2520wide%2520range%2520of%2520applications%2520to%2520Bayesian%2520inference%2520and%2520imaging%250Ainverse%2520problems%2520such%2520as%2520image%2520deconvolution.%2520We%2520perform%2520numerical%2520simulations%250Ato%2520compare%2520the%2520performance%2520of%2520most%2520commonly%2520used%2520Langevin%2520Monte%2520Carlo%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Log-Concave%20and%20Nonsmooth%20Sampling%20via%20Langevin%20Monte%20Carlo%0A%20%20Algorithms&entry.906535625=Tim%20Tsz-Kit%20Lau%20and%20Han%20Liu%20and%20Thomas%20Pock&entry.1292438233=%20%20We%20study%20the%20problem%20of%20approximate%20sampling%20from%20non-log-concave%0Adistributions%2C%20e.g.%2C%20Gaussian%20mixtures%2C%20which%20is%20often%20challenging%20even%20in%20low%0Adimensions%20due%20to%20their%20multimodality.%20We%20focus%20on%20performing%20this%20task%20via%0AMarkov%20chain%20Monte%20Carlo%20%28MCMC%29%20methods%20derived%20from%20discretizations%20of%20the%0Aoverdamped%20Langevin%20diffusions%2C%20which%20are%20commonly%20known%20as%20Langevin%20Monte%0ACarlo%20algorithms.%20Furthermore%2C%20we%20are%20also%20interested%20in%20two%20nonsmooth%20cases%0Afor%20which%20a%20large%20class%20of%20proximal%20MCMC%20methods%20have%20been%20developed%3A%20%28i%29%20a%0Anonsmooth%20prior%20is%20considered%20with%20a%20Gaussian%20mixture%20likelihood%3B%20%28ii%29%20a%0ALaplacian%20mixture%20distribution.%20Such%20nonsmooth%20and%20non-log-concave%20sampling%0Atasks%20arise%20from%20a%20wide%20range%20of%20applications%20to%20Bayesian%20inference%20and%20imaging%0Ainverse%20problems%20such%20as%20image%20deconvolution.%20We%20perform%20numerical%20simulations%0Ato%20compare%20the%20performance%20of%20most%20commonly%20used%20Langevin%20Monte%20Carlo%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15988v2&entry.124074799=Read"},
{"title": "The ethical situation of DALL-E 2", "author": "Eduard Hogea and Josem Rocafortf", "abstract": "  A hot topic of Artificial Intelligence right now is image generation from\nprompts. DALL-E 2 is one of the biggest names in this domain, as it allows\npeople to create images from simple text inputs, to even more complicated ones.\nThe company that made this possible, OpenAI, has assured everyone that visited\ntheir website that their mission is to ensure that artificial general\nintelligence benefits all humanity. A noble idea in our opinion, that also\nstood as the motive behind us choosing this subject. This paper analyzes the\nethical implications of an AI image generative system, with an emphasis on how\nsociety is responding to it, how it probably will and how it should if all the\nright measures are taken.\n", "link": "http://arxiv.org/abs/2405.19176v1", "date": "2024-05-29", "relevancy": 2.4366, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4916}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4906}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20ethical%20situation%20of%20DALL-E%202&body=Title%3A%20The%20ethical%20situation%20of%20DALL-E%202%0AAuthor%3A%20Eduard%20Hogea%20and%20Josem%20Rocafortf%0AAbstract%3A%20%20%20A%20hot%20topic%20of%20Artificial%20Intelligence%20right%20now%20is%20image%20generation%20from%0Aprompts.%20DALL-E%202%20is%20one%20of%20the%20biggest%20names%20in%20this%20domain%2C%20as%20it%20allows%0Apeople%20to%20create%20images%20from%20simple%20text%20inputs%2C%20to%20even%20more%20complicated%20ones.%0AThe%20company%20that%20made%20this%20possible%2C%20OpenAI%2C%20has%20assured%20everyone%20that%20visited%0Atheir%20website%20that%20their%20mission%20is%20to%20ensure%20that%20artificial%20general%0Aintelligence%20benefits%20all%20humanity.%20A%20noble%20idea%20in%20our%20opinion%2C%20that%20also%0Astood%20as%20the%20motive%20behind%20us%20choosing%20this%20subject.%20This%20paper%20analyzes%20the%0Aethical%20implications%20of%20an%20AI%20image%20generative%20system%2C%20with%20an%20emphasis%20on%20how%0Asociety%20is%20responding%20to%20it%2C%20how%20it%20probably%20will%20and%20how%20it%20should%20if%20all%20the%0Aright%20measures%20are%20taken.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520ethical%2520situation%2520of%2520DALL-E%25202%26entry.906535625%3DEduard%2520Hogea%2520and%2520Josem%2520Rocafortf%26entry.1292438233%3D%2520%2520A%2520hot%2520topic%2520of%2520Artificial%2520Intelligence%2520right%2520now%2520is%2520image%2520generation%2520from%250Aprompts.%2520DALL-E%25202%2520is%2520one%2520of%2520the%2520biggest%2520names%2520in%2520this%2520domain%252C%2520as%2520it%2520allows%250Apeople%2520to%2520create%2520images%2520from%2520simple%2520text%2520inputs%252C%2520to%2520even%2520more%2520complicated%2520ones.%250AThe%2520company%2520that%2520made%2520this%2520possible%252C%2520OpenAI%252C%2520has%2520assured%2520everyone%2520that%2520visited%250Atheir%2520website%2520that%2520their%2520mission%2520is%2520to%2520ensure%2520that%2520artificial%2520general%250Aintelligence%2520benefits%2520all%2520humanity.%2520A%2520noble%2520idea%2520in%2520our%2520opinion%252C%2520that%2520also%250Astood%2520as%2520the%2520motive%2520behind%2520us%2520choosing%2520this%2520subject.%2520This%2520paper%2520analyzes%2520the%250Aethical%2520implications%2520of%2520an%2520AI%2520image%2520generative%2520system%252C%2520with%2520an%2520emphasis%2520on%2520how%250Asociety%2520is%2520responding%2520to%2520it%252C%2520how%2520it%2520probably%2520will%2520and%2520how%2520it%2520should%2520if%2520all%2520the%250Aright%2520measures%2520are%2520taken.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20ethical%20situation%20of%20DALL-E%202&entry.906535625=Eduard%20Hogea%20and%20Josem%20Rocafortf&entry.1292438233=%20%20A%20hot%20topic%20of%20Artificial%20Intelligence%20right%20now%20is%20image%20generation%20from%0Aprompts.%20DALL-E%202%20is%20one%20of%20the%20biggest%20names%20in%20this%20domain%2C%20as%20it%20allows%0Apeople%20to%20create%20images%20from%20simple%20text%20inputs%2C%20to%20even%20more%20complicated%20ones.%0AThe%20company%20that%20made%20this%20possible%2C%20OpenAI%2C%20has%20assured%20everyone%20that%20visited%0Atheir%20website%20that%20their%20mission%20is%20to%20ensure%20that%20artificial%20general%0Aintelligence%20benefits%20all%20humanity.%20A%20noble%20idea%20in%20our%20opinion%2C%20that%20also%0Astood%20as%20the%20motive%20behind%20us%20choosing%20this%20subject.%20This%20paper%20analyzes%20the%0Aethical%20implications%20of%20an%20AI%20image%20generative%20system%2C%20with%20an%20emphasis%20on%20how%0Asociety%20is%20responding%20to%20it%2C%20how%20it%20probably%20will%20and%20how%20it%20should%20if%20all%20the%0Aright%20measures%20are%20taken.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19176v1&entry.124074799=Read"},
{"title": "Grasp as You Say: Language-guided Dexterous Grasp Generation", "author": "Yi-Lin Wei and Jian-Jian Jiang and Chengyi Xing and Xiantuo Tan and Xiao-Ming Wu and Hao Li and Mark Cutkosky and Wei-Shi Zheng", "abstract": "  This paper explores a novel task \"\"Dexterous Grasp as You Say\"\" (DexGYS),\nenabling robots to perform dexterous grasping based on human commands expressed\nin natural language. However, the development of this field is hindered by the\nlack of datasets with natural human guidance; thus, we propose a\nlanguage-guided dexterous grasp dataset, named DexGYSNet, offering high-quality\ndexterous grasp annotations along with flexible and fine-grained human language\nguidance. Our dataset construction is cost-efficient, with the carefully-design\nhand-object interaction retargeting strategy, and the LLM-assisted language\nguidance annotation system. Equipped with this dataset, we introduce the\nDexGYSGrasp framework for generating dexterous grasps based on human language\ninstructions, with the capability of producing grasps that are intent-aligned,\nhigh quality and diversity. To achieve this capability, our framework\ndecomposes the complex learning process into two manageable progressive\nobjectives and introduce two components to realize them. The first component\nlearns the grasp distribution focusing on intention alignment and generation\ndiversity. And the second component refines the grasp quality while maintaining\nintention consistency. Extensive experiments are conducted on DexGYSNet and\nreal world environment for validation.\n", "link": "http://arxiv.org/abs/2405.19291v1", "date": "2024-05-29", "relevancy": 2.4345, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7151}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5722}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grasp%20as%20You%20Say%3A%20Language-guided%20Dexterous%20Grasp%20Generation&body=Title%3A%20Grasp%20as%20You%20Say%3A%20Language-guided%20Dexterous%20Grasp%20Generation%0AAuthor%3A%20Yi-Lin%20Wei%20and%20Jian-Jian%20Jiang%20and%20Chengyi%20Xing%20and%20Xiantuo%20Tan%20and%20Xiao-Ming%20Wu%20and%20Hao%20Li%20and%20Mark%20Cutkosky%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20This%20paper%20explores%20a%20novel%20task%20%22%22Dexterous%20Grasp%20as%20You%20Say%22%22%20%28DexGYS%29%2C%0Aenabling%20robots%20to%20perform%20dexterous%20grasping%20based%20on%20human%20commands%20expressed%0Ain%20natural%20language.%20However%2C%20the%20development%20of%20this%20field%20is%20hindered%20by%20the%0Alack%20of%20datasets%20with%20natural%20human%20guidance%3B%20thus%2C%20we%20propose%20a%0Alanguage-guided%20dexterous%20grasp%20dataset%2C%20named%20DexGYSNet%2C%20offering%20high-quality%0Adexterous%20grasp%20annotations%20along%20with%20flexible%20and%20fine-grained%20human%20language%0Aguidance.%20Our%20dataset%20construction%20is%20cost-efficient%2C%20with%20the%20carefully-design%0Ahand-object%20interaction%20retargeting%20strategy%2C%20and%20the%20LLM-assisted%20language%0Aguidance%20annotation%20system.%20Equipped%20with%20this%20dataset%2C%20we%20introduce%20the%0ADexGYSGrasp%20framework%20for%20generating%20dexterous%20grasps%20based%20on%20human%20language%0Ainstructions%2C%20with%20the%20capability%20of%20producing%20grasps%20that%20are%20intent-aligned%2C%0Ahigh%20quality%20and%20diversity.%20To%20achieve%20this%20capability%2C%20our%20framework%0Adecomposes%20the%20complex%20learning%20process%20into%20two%20manageable%20progressive%0Aobjectives%20and%20introduce%20two%20components%20to%20realize%20them.%20The%20first%20component%0Alearns%20the%20grasp%20distribution%20focusing%20on%20intention%20alignment%20and%20generation%0Adiversity.%20And%20the%20second%20component%20refines%20the%20grasp%20quality%20while%20maintaining%0Aintention%20consistency.%20Extensive%20experiments%20are%20conducted%20on%20DexGYSNet%20and%0Areal%20world%20environment%20for%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrasp%2520as%2520You%2520Say%253A%2520Language-guided%2520Dexterous%2520Grasp%2520Generation%26entry.906535625%3DYi-Lin%2520Wei%2520and%2520Jian-Jian%2520Jiang%2520and%2520Chengyi%2520Xing%2520and%2520Xiantuo%2520Tan%2520and%2520Xiao-Ming%2520Wu%2520and%2520Hao%2520Li%2520and%2520Mark%2520Cutkosky%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520a%2520novel%2520task%2520%2522%2522Dexterous%2520Grasp%2520as%2520You%2520Say%2522%2522%2520%2528DexGYS%2529%252C%250Aenabling%2520robots%2520to%2520perform%2520dexterous%2520grasping%2520based%2520on%2520human%2520commands%2520expressed%250Ain%2520natural%2520language.%2520However%252C%2520the%2520development%2520of%2520this%2520field%2520is%2520hindered%2520by%2520the%250Alack%2520of%2520datasets%2520with%2520natural%2520human%2520guidance%253B%2520thus%252C%2520we%2520propose%2520a%250Alanguage-guided%2520dexterous%2520grasp%2520dataset%252C%2520named%2520DexGYSNet%252C%2520offering%2520high-quality%250Adexterous%2520grasp%2520annotations%2520along%2520with%2520flexible%2520and%2520fine-grained%2520human%2520language%250Aguidance.%2520Our%2520dataset%2520construction%2520is%2520cost-efficient%252C%2520with%2520the%2520carefully-design%250Ahand-object%2520interaction%2520retargeting%2520strategy%252C%2520and%2520the%2520LLM-assisted%2520language%250Aguidance%2520annotation%2520system.%2520Equipped%2520with%2520this%2520dataset%252C%2520we%2520introduce%2520the%250ADexGYSGrasp%2520framework%2520for%2520generating%2520dexterous%2520grasps%2520based%2520on%2520human%2520language%250Ainstructions%252C%2520with%2520the%2520capability%2520of%2520producing%2520grasps%2520that%2520are%2520intent-aligned%252C%250Ahigh%2520quality%2520and%2520diversity.%2520To%2520achieve%2520this%2520capability%252C%2520our%2520framework%250Adecomposes%2520the%2520complex%2520learning%2520process%2520into%2520two%2520manageable%2520progressive%250Aobjectives%2520and%2520introduce%2520two%2520components%2520to%2520realize%2520them.%2520The%2520first%2520component%250Alearns%2520the%2520grasp%2520distribution%2520focusing%2520on%2520intention%2520alignment%2520and%2520generation%250Adiversity.%2520And%2520the%2520second%2520component%2520refines%2520the%2520grasp%2520quality%2520while%2520maintaining%250Aintention%2520consistency.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520DexGYSNet%2520and%250Areal%2520world%2520environment%2520for%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%20as%20You%20Say%3A%20Language-guided%20Dexterous%20Grasp%20Generation&entry.906535625=Yi-Lin%20Wei%20and%20Jian-Jian%20Jiang%20and%20Chengyi%20Xing%20and%20Xiantuo%20Tan%20and%20Xiao-Ming%20Wu%20and%20Hao%20Li%20and%20Mark%20Cutkosky%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20This%20paper%20explores%20a%20novel%20task%20%22%22Dexterous%20Grasp%20as%20You%20Say%22%22%20%28DexGYS%29%2C%0Aenabling%20robots%20to%20perform%20dexterous%20grasping%20based%20on%20human%20commands%20expressed%0Ain%20natural%20language.%20However%2C%20the%20development%20of%20this%20field%20is%20hindered%20by%20the%0Alack%20of%20datasets%20with%20natural%20human%20guidance%3B%20thus%2C%20we%20propose%20a%0Alanguage-guided%20dexterous%20grasp%20dataset%2C%20named%20DexGYSNet%2C%20offering%20high-quality%0Adexterous%20grasp%20annotations%20along%20with%20flexible%20and%20fine-grained%20human%20language%0Aguidance.%20Our%20dataset%20construction%20is%20cost-efficient%2C%20with%20the%20carefully-design%0Ahand-object%20interaction%20retargeting%20strategy%2C%20and%20the%20LLM-assisted%20language%0Aguidance%20annotation%20system.%20Equipped%20with%20this%20dataset%2C%20we%20introduce%20the%0ADexGYSGrasp%20framework%20for%20generating%20dexterous%20grasps%20based%20on%20human%20language%0Ainstructions%2C%20with%20the%20capability%20of%20producing%20grasps%20that%20are%20intent-aligned%2C%0Ahigh%20quality%20and%20diversity.%20To%20achieve%20this%20capability%2C%20our%20framework%0Adecomposes%20the%20complex%20learning%20process%20into%20two%20manageable%20progressive%0Aobjectives%20and%20introduce%20two%20components%20to%20realize%20them.%20The%20first%20component%0Alearns%20the%20grasp%20distribution%20focusing%20on%20intention%20alignment%20and%20generation%0Adiversity.%20And%20the%20second%20component%20refines%20the%20grasp%20quality%20while%20maintaining%0Aintention%20consistency.%20Extensive%20experiments%20are%20conducted%20on%20DexGYSNet%20and%0Areal%20world%20environment%20for%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19291v1&entry.124074799=Read"},
{"title": "Generating In-Distribution Proxy Graphs for Explaining Graph Neural\n  Networks", "author": "Zhuomin Chen and Jiaxing Zhang and Jingchao Ni and Xiaoting Li and Yuchen Bian and Md Mezbahul Islam and Ananda Mohan Mondal and Hua Wei and Dongsheng Luo", "abstract": "  Graph Neural Networks (GNNs) have become a building block in graph data\nprocessing, with wide applications in critical domains. The growing needs to\ndeploy GNNs in high-stakes applications necessitate explainability for users in\nthe decision-making processes. A popular paradigm for the explainability of\nGNNs is to identify explainable subgraphs by comparing their labels with the\nones of original graphs. This task is challenging due to the substantial\ndistributional shift from the original graphs in the training set to the set of\nexplainable subgraphs, which prevents accurate prediction of labels with the\nsubgraphs. To address it, in this paper, we propose a novel method that\ngenerates proxy graphs for explainable subgraphs that are in the distribution\nof training data. We introduce a parametric method that employs graph\ngenerators to produce proxy graphs. A new training objective based on\ninformation theory is designed to ensure that proxy graphs not only adhere to\nthe distribution of training data but also preserve explanatory factors. Such\ngenerated proxy graphs can be reliably used to approximate the predictions of\nthe labels of explainable subgraphs. Empirical evaluations across various\ndatasets demonstrate our method achieves more accurate explanations for GNNs.\n", "link": "http://arxiv.org/abs/2402.02036v2", "date": "2024-05-29", "relevancy": 2.4154, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4773}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20In-Distribution%20Proxy%20Graphs%20for%20Explaining%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Generating%20In-Distribution%20Proxy%20Graphs%20for%20Explaining%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Zhuomin%20Chen%20and%20Jiaxing%20Zhang%20and%20Jingchao%20Ni%20and%20Xiaoting%20Li%20and%20Yuchen%20Bian%20and%20Md%20Mezbahul%20Islam%20and%20Ananda%20Mohan%20Mondal%20and%20Hua%20Wei%20and%20Dongsheng%20Luo%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20a%20building%20block%20in%20graph%20data%0Aprocessing%2C%20with%20wide%20applications%20in%20critical%20domains.%20The%20growing%20needs%20to%0Adeploy%20GNNs%20in%20high-stakes%20applications%20necessitate%20explainability%20for%20users%20in%0Athe%20decision-making%20processes.%20A%20popular%20paradigm%20for%20the%20explainability%20of%0AGNNs%20is%20to%20identify%20explainable%20subgraphs%20by%20comparing%20their%20labels%20with%20the%0Aones%20of%20original%20graphs.%20This%20task%20is%20challenging%20due%20to%20the%20substantial%0Adistributional%20shift%20from%20the%20original%20graphs%20in%20the%20training%20set%20to%20the%20set%20of%0Aexplainable%20subgraphs%2C%20which%20prevents%20accurate%20prediction%20of%20labels%20with%20the%0Asubgraphs.%20To%20address%20it%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20method%20that%0Agenerates%20proxy%20graphs%20for%20explainable%20subgraphs%20that%20are%20in%20the%20distribution%0Aof%20training%20data.%20We%20introduce%20a%20parametric%20method%20that%20employs%20graph%0Agenerators%20to%20produce%20proxy%20graphs.%20A%20new%20training%20objective%20based%20on%0Ainformation%20theory%20is%20designed%20to%20ensure%20that%20proxy%20graphs%20not%20only%20adhere%20to%0Athe%20distribution%20of%20training%20data%20but%20also%20preserve%20explanatory%20factors.%20Such%0Agenerated%20proxy%20graphs%20can%20be%20reliably%20used%20to%20approximate%20the%20predictions%20of%0Athe%20labels%20of%20explainable%20subgraphs.%20Empirical%20evaluations%20across%20various%0Adatasets%20demonstrate%20our%20method%20achieves%20more%20accurate%20explanations%20for%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520In-Distribution%2520Proxy%2520Graphs%2520for%2520Explaining%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DZhuomin%2520Chen%2520and%2520Jiaxing%2520Zhang%2520and%2520Jingchao%2520Ni%2520and%2520Xiaoting%2520Li%2520and%2520Yuchen%2520Bian%2520and%2520Md%2520Mezbahul%2520Islam%2520and%2520Ananda%2520Mohan%2520Mondal%2520and%2520Hua%2520Wei%2520and%2520Dongsheng%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520building%2520block%2520in%2520graph%2520data%250Aprocessing%252C%2520with%2520wide%2520applications%2520in%2520critical%2520domains.%2520The%2520growing%2520needs%2520to%250Adeploy%2520GNNs%2520in%2520high-stakes%2520applications%2520necessitate%2520explainability%2520for%2520users%2520in%250Athe%2520decision-making%2520processes.%2520A%2520popular%2520paradigm%2520for%2520the%2520explainability%2520of%250AGNNs%2520is%2520to%2520identify%2520explainable%2520subgraphs%2520by%2520comparing%2520their%2520labels%2520with%2520the%250Aones%2520of%2520original%2520graphs.%2520This%2520task%2520is%2520challenging%2520due%2520to%2520the%2520substantial%250Adistributional%2520shift%2520from%2520the%2520original%2520graphs%2520in%2520the%2520training%2520set%2520to%2520the%2520set%2520of%250Aexplainable%2520subgraphs%252C%2520which%2520prevents%2520accurate%2520prediction%2520of%2520labels%2520with%2520the%250Asubgraphs.%2520To%2520address%2520it%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%250Agenerates%2520proxy%2520graphs%2520for%2520explainable%2520subgraphs%2520that%2520are%2520in%2520the%2520distribution%250Aof%2520training%2520data.%2520We%2520introduce%2520a%2520parametric%2520method%2520that%2520employs%2520graph%250Agenerators%2520to%2520produce%2520proxy%2520graphs.%2520A%2520new%2520training%2520objective%2520based%2520on%250Ainformation%2520theory%2520is%2520designed%2520to%2520ensure%2520that%2520proxy%2520graphs%2520not%2520only%2520adhere%2520to%250Athe%2520distribution%2520of%2520training%2520data%2520but%2520also%2520preserve%2520explanatory%2520factors.%2520Such%250Agenerated%2520proxy%2520graphs%2520can%2520be%2520reliably%2520used%2520to%2520approximate%2520the%2520predictions%2520of%250Athe%2520labels%2520of%2520explainable%2520subgraphs.%2520Empirical%2520evaluations%2520across%2520various%250Adatasets%2520demonstrate%2520our%2520method%2520achieves%2520more%2520accurate%2520explanations%2520for%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20In-Distribution%20Proxy%20Graphs%20for%20Explaining%20Graph%20Neural%0A%20%20Networks&entry.906535625=Zhuomin%20Chen%20and%20Jiaxing%20Zhang%20and%20Jingchao%20Ni%20and%20Xiaoting%20Li%20and%20Yuchen%20Bian%20and%20Md%20Mezbahul%20Islam%20and%20Ananda%20Mohan%20Mondal%20and%20Hua%20Wei%20and%20Dongsheng%20Luo&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20a%20building%20block%20in%20graph%20data%0Aprocessing%2C%20with%20wide%20applications%20in%20critical%20domains.%20The%20growing%20needs%20to%0Adeploy%20GNNs%20in%20high-stakes%20applications%20necessitate%20explainability%20for%20users%20in%0Athe%20decision-making%20processes.%20A%20popular%20paradigm%20for%20the%20explainability%20of%0AGNNs%20is%20to%20identify%20explainable%20subgraphs%20by%20comparing%20their%20labels%20with%20the%0Aones%20of%20original%20graphs.%20This%20task%20is%20challenging%20due%20to%20the%20substantial%0Adistributional%20shift%20from%20the%20original%20graphs%20in%20the%20training%20set%20to%20the%20set%20of%0Aexplainable%20subgraphs%2C%20which%20prevents%20accurate%20prediction%20of%20labels%20with%20the%0Asubgraphs.%20To%20address%20it%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20method%20that%0Agenerates%20proxy%20graphs%20for%20explainable%20subgraphs%20that%20are%20in%20the%20distribution%0Aof%20training%20data.%20We%20introduce%20a%20parametric%20method%20that%20employs%20graph%0Agenerators%20to%20produce%20proxy%20graphs.%20A%20new%20training%20objective%20based%20on%0Ainformation%20theory%20is%20designed%20to%20ensure%20that%20proxy%20graphs%20not%20only%20adhere%20to%0Athe%20distribution%20of%20training%20data%20but%20also%20preserve%20explanatory%20factors.%20Such%0Agenerated%20proxy%20graphs%20can%20be%20reliably%20used%20to%20approximate%20the%20predictions%20of%0Athe%20labels%20of%20explainable%20subgraphs.%20Empirical%20evaluations%20across%20various%0Adatasets%20demonstrate%20our%20method%20achieves%20more%20accurate%20explanations%20for%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02036v2&entry.124074799=Read"},
{"title": "Getting More Juice Out of the SFT Data: Reward Learning from Human\n  Demonstration Improves SFT for LLM Alignment", "author": "Jiaxiang Li and Siliang Zeng and Hoi-To Wai and Chenliang Li and Alfredo Garcia and Mingyi Hong", "abstract": "  Aligning human preference and value is an important requirement for\ncontemporary foundation models. State-of-the-art techniques such as\nReinforcement Learning from Human Feedback (RLHF) often consist of two stages:\n1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from\nhuman demonstration data; 2) Preference learning, where preference data is used\nto learn a reward model, which is in turn used by a reinforcement learning (RL)\nstep to fine-tune the model. Such reward model serves as a proxy to human\npreference, and it is critical to guide the RL step towards improving the model\nquality. In this work, we argue that the SFT stage significantly benefits from\nlearning a reward model as well. Instead of using the human demonstration data\ndirectly via supervised learning, we propose to leverage an Inverse\nReinforcement Learning (IRL) technique to (explicitly or implicitly) build an\nreward model, while learning the policy model. This approach leads to new SFT\nalgorithms that are not only efficient to implement, but also promote the\nability to distinguish between the preferred and non-preferred continuations.\nMoreover, we identify a connection between the proposed IRL based approach, and\ncertain self-play approach proposed recently, and showed that self-play is a\nspecial case of modeling a reward-learning agent. Theoretically, we show that\nthe proposed algorithms converge to the stationary solutions of the IRL\nproblem. Empirically, we align 1B and 7B models using proposed methods and\nevaluate them on a reward benchmark model and the HuggingFace Open LLM\nLeaderboard. The proposed methods show significant performance improvement over\nexisting SFT approaches. Our results indicate that it is beneficial to\nexplicitly or implicitly leverage reward learning throughout the entire\nalignment process.\n", "link": "http://arxiv.org/abs/2405.17888v2", "date": "2024-05-29", "relevancy": 2.4126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Getting%20More%20Juice%20Out%20of%20the%20SFT%20Data%3A%20Reward%20Learning%20from%20Human%0A%20%20Demonstration%20Improves%20SFT%20for%20LLM%20Alignment&body=Title%3A%20Getting%20More%20Juice%20Out%20of%20the%20SFT%20Data%3A%20Reward%20Learning%20from%20Human%0A%20%20Demonstration%20Improves%20SFT%20for%20LLM%20Alignment%0AAuthor%3A%20Jiaxiang%20Li%20and%20Siliang%20Zeng%20and%20Hoi-To%20Wai%20and%20Chenliang%20Li%20and%20Alfredo%20Garcia%20and%20Mingyi%20Hong%0AAbstract%3A%20%20%20Aligning%20human%20preference%20and%20value%20is%20an%20important%20requirement%20for%0Acontemporary%20foundation%20models.%20State-of-the-art%20techniques%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20often%20consist%20of%20two%20stages%3A%0A1%29%20supervised%20fine-tuning%20%28SFT%29%2C%20where%20the%20model%20is%20fine-tuned%20by%20learning%20from%0Ahuman%20demonstration%20data%3B%202%29%20Preference%20learning%2C%20where%20preference%20data%20is%20used%0Ato%20learn%20a%20reward%20model%2C%20which%20is%20in%20turn%20used%20by%20a%20reinforcement%20learning%20%28RL%29%0Astep%20to%20fine-tune%20the%20model.%20Such%20reward%20model%20serves%20as%20a%20proxy%20to%20human%0Apreference%2C%20and%20it%20is%20critical%20to%20guide%20the%20RL%20step%20towards%20improving%20the%20model%0Aquality.%20In%20this%20work%2C%20we%20argue%20that%20the%20SFT%20stage%20significantly%20benefits%20from%0Alearning%20a%20reward%20model%20as%20well.%20Instead%20of%20using%20the%20human%20demonstration%20data%0Adirectly%20via%20supervised%20learning%2C%20we%20propose%20to%20leverage%20an%20Inverse%0AReinforcement%20Learning%20%28IRL%29%20technique%20to%20%28explicitly%20or%20implicitly%29%20build%20an%0Areward%20model%2C%20while%20learning%20the%20policy%20model.%20This%20approach%20leads%20to%20new%20SFT%0Aalgorithms%20that%20are%20not%20only%20efficient%20to%20implement%2C%20but%20also%20promote%20the%0Aability%20to%20distinguish%20between%20the%20preferred%20and%20non-preferred%20continuations.%0AMoreover%2C%20we%20identify%20a%20connection%20between%20the%20proposed%20IRL%20based%20approach%2C%20and%0Acertain%20self-play%20approach%20proposed%20recently%2C%20and%20showed%20that%20self-play%20is%20a%0Aspecial%20case%20of%20modeling%20a%20reward-learning%20agent.%20Theoretically%2C%20we%20show%20that%0Athe%20proposed%20algorithms%20converge%20to%20the%20stationary%20solutions%20of%20the%20IRL%0Aproblem.%20Empirically%2C%20we%20align%201B%20and%207B%20models%20using%20proposed%20methods%20and%0Aevaluate%20them%20on%20a%20reward%20benchmark%20model%20and%20the%20HuggingFace%20Open%20LLM%0ALeaderboard.%20The%20proposed%20methods%20show%20significant%20performance%20improvement%20over%0Aexisting%20SFT%20approaches.%20Our%20results%20indicate%20that%20it%20is%20beneficial%20to%0Aexplicitly%20or%20implicitly%20leverage%20reward%20learning%20throughout%20the%20entire%0Aalignment%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGetting%2520More%2520Juice%2520Out%2520of%2520the%2520SFT%2520Data%253A%2520Reward%2520Learning%2520from%2520Human%250A%2520%2520Demonstration%2520Improves%2520SFT%2520for%2520LLM%2520Alignment%26entry.906535625%3DJiaxiang%2520Li%2520and%2520Siliang%2520Zeng%2520and%2520Hoi-To%2520Wai%2520and%2520Chenliang%2520Li%2520and%2520Alfredo%2520Garcia%2520and%2520Mingyi%2520Hong%26entry.1292438233%3D%2520%2520Aligning%2520human%2520preference%2520and%2520value%2520is%2520an%2520important%2520requirement%2520for%250Acontemporary%2520foundation%2520models.%2520State-of-the-art%2520techniques%2520such%2520as%250AReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520often%2520consist%2520of%2520two%2520stages%253A%250A1%2529%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520where%2520the%2520model%2520is%2520fine-tuned%2520by%2520learning%2520from%250Ahuman%2520demonstration%2520data%253B%25202%2529%2520Preference%2520learning%252C%2520where%2520preference%2520data%2520is%2520used%250Ato%2520learn%2520a%2520reward%2520model%252C%2520which%2520is%2520in%2520turn%2520used%2520by%2520a%2520reinforcement%2520learning%2520%2528RL%2529%250Astep%2520to%2520fine-tune%2520the%2520model.%2520Such%2520reward%2520model%2520serves%2520as%2520a%2520proxy%2520to%2520human%250Apreference%252C%2520and%2520it%2520is%2520critical%2520to%2520guide%2520the%2520RL%2520step%2520towards%2520improving%2520the%2520model%250Aquality.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520the%2520SFT%2520stage%2520significantly%2520benefits%2520from%250Alearning%2520a%2520reward%2520model%2520as%2520well.%2520Instead%2520of%2520using%2520the%2520human%2520demonstration%2520data%250Adirectly%2520via%2520supervised%2520learning%252C%2520we%2520propose%2520to%2520leverage%2520an%2520Inverse%250AReinforcement%2520Learning%2520%2528IRL%2529%2520technique%2520to%2520%2528explicitly%2520or%2520implicitly%2529%2520build%2520an%250Areward%2520model%252C%2520while%2520learning%2520the%2520policy%2520model.%2520This%2520approach%2520leads%2520to%2520new%2520SFT%250Aalgorithms%2520that%2520are%2520not%2520only%2520efficient%2520to%2520implement%252C%2520but%2520also%2520promote%2520the%250Aability%2520to%2520distinguish%2520between%2520the%2520preferred%2520and%2520non-preferred%2520continuations.%250AMoreover%252C%2520we%2520identify%2520a%2520connection%2520between%2520the%2520proposed%2520IRL%2520based%2520approach%252C%2520and%250Acertain%2520self-play%2520approach%2520proposed%2520recently%252C%2520and%2520showed%2520that%2520self-play%2520is%2520a%250Aspecial%2520case%2520of%2520modeling%2520a%2520reward-learning%2520agent.%2520Theoretically%252C%2520we%2520show%2520that%250Athe%2520proposed%2520algorithms%2520converge%2520to%2520the%2520stationary%2520solutions%2520of%2520the%2520IRL%250Aproblem.%2520Empirically%252C%2520we%2520align%25201B%2520and%25207B%2520models%2520using%2520proposed%2520methods%2520and%250Aevaluate%2520them%2520on%2520a%2520reward%2520benchmark%2520model%2520and%2520the%2520HuggingFace%2520Open%2520LLM%250ALeaderboard.%2520The%2520proposed%2520methods%2520show%2520significant%2520performance%2520improvement%2520over%250Aexisting%2520SFT%2520approaches.%2520Our%2520results%2520indicate%2520that%2520it%2520is%2520beneficial%2520to%250Aexplicitly%2520or%2520implicitly%2520leverage%2520reward%2520learning%2520throughout%2520the%2520entire%250Aalignment%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Getting%20More%20Juice%20Out%20of%20the%20SFT%20Data%3A%20Reward%20Learning%20from%20Human%0A%20%20Demonstration%20Improves%20SFT%20for%20LLM%20Alignment&entry.906535625=Jiaxiang%20Li%20and%20Siliang%20Zeng%20and%20Hoi-To%20Wai%20and%20Chenliang%20Li%20and%20Alfredo%20Garcia%20and%20Mingyi%20Hong&entry.1292438233=%20%20Aligning%20human%20preference%20and%20value%20is%20an%20important%20requirement%20for%0Acontemporary%20foundation%20models.%20State-of-the-art%20techniques%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20often%20consist%20of%20two%20stages%3A%0A1%29%20supervised%20fine-tuning%20%28SFT%29%2C%20where%20the%20model%20is%20fine-tuned%20by%20learning%20from%0Ahuman%20demonstration%20data%3B%202%29%20Preference%20learning%2C%20where%20preference%20data%20is%20used%0Ato%20learn%20a%20reward%20model%2C%20which%20is%20in%20turn%20used%20by%20a%20reinforcement%20learning%20%28RL%29%0Astep%20to%20fine-tune%20the%20model.%20Such%20reward%20model%20serves%20as%20a%20proxy%20to%20human%0Apreference%2C%20and%20it%20is%20critical%20to%20guide%20the%20RL%20step%20towards%20improving%20the%20model%0Aquality.%20In%20this%20work%2C%20we%20argue%20that%20the%20SFT%20stage%20significantly%20benefits%20from%0Alearning%20a%20reward%20model%20as%20well.%20Instead%20of%20using%20the%20human%20demonstration%20data%0Adirectly%20via%20supervised%20learning%2C%20we%20propose%20to%20leverage%20an%20Inverse%0AReinforcement%20Learning%20%28IRL%29%20technique%20to%20%28explicitly%20or%20implicitly%29%20build%20an%0Areward%20model%2C%20while%20learning%20the%20policy%20model.%20This%20approach%20leads%20to%20new%20SFT%0Aalgorithms%20that%20are%20not%20only%20efficient%20to%20implement%2C%20but%20also%20promote%20the%0Aability%20to%20distinguish%20between%20the%20preferred%20and%20non-preferred%20continuations.%0AMoreover%2C%20we%20identify%20a%20connection%20between%20the%20proposed%20IRL%20based%20approach%2C%20and%0Acertain%20self-play%20approach%20proposed%20recently%2C%20and%20showed%20that%20self-play%20is%20a%0Aspecial%20case%20of%20modeling%20a%20reward-learning%20agent.%20Theoretically%2C%20we%20show%20that%0Athe%20proposed%20algorithms%20converge%20to%20the%20stationary%20solutions%20of%20the%20IRL%0Aproblem.%20Empirically%2C%20we%20align%201B%20and%207B%20models%20using%20proposed%20methods%20and%0Aevaluate%20them%20on%20a%20reward%20benchmark%20model%20and%20the%20HuggingFace%20Open%20LLM%0ALeaderboard.%20The%20proposed%20methods%20show%20significant%20performance%20improvement%20over%0Aexisting%20SFT%20approaches.%20Our%20results%20indicate%20that%20it%20is%20beneficial%20to%0Aexplicitly%20or%20implicitly%20leverage%20reward%20learning%20throughout%20the%20entire%0Aalignment%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17888v2&entry.124074799=Read"},
{"title": "HetCAN: A Heterogeneous Graph Cascade Attention Network with Dual-Level\n  Awareness", "author": "Zeyuan Zhao and Qingqing Ge and Anfeng Cheng and Yiding Liu and Xiang Li and Shuaiqiang Wang", "abstract": "  Heterogeneous graph neural networks(HGNNs) have recently shown impressive\ncapability in modeling heterogeneous graphs that are ubiquitous in real-world\napplications. Most existing methods for heterogeneous graphs mainly learn node\nembeddings by stacking multiple convolutional or attentional layers, which can\nbe considered as capturing the high-order information from node-level aspect.\nHowever, different types of nodes in heterogeneous graphs have diverse\nfeatures, it is also necessary to capture interactions among node features,\nnamely the high-order information from feature-level aspect. In addition, most\nmethods first align node features by mapping them into one same low-dimensional\nspace, while they may lose some type information of nodes in this way. To\naddress these problems, in this paper, we propose a novel Heterogeneous graph\nCascade Attention Network (HetCAN) composed of multiple cascade blocks. Each\ncascade block includes two components, the type-aware encoder and the\ndimension-aware encoder. Specifically, the type-aware encoder compensates for\nthe loss of node type information and aims to make full use of graph\nheterogeneity. The dimension-aware encoder is able to learn the feature-level\nhigh-order information by capturing the interactions among node features. With\nthe assistance of these components, HetCAN can comprehensively encode\ninformation of node features, graph heterogeneity and graph structure in node\nembeddings. Extensive experiments demonstrate the superiority of HetCAN over\nadvanced competitors and also exhibit its efficiency and robustness.\n", "link": "http://arxiv.org/abs/2311.03275v2", "date": "2024-05-29", "relevancy": 2.3947, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4649}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HetCAN%3A%20A%20Heterogeneous%20Graph%20Cascade%20Attention%20Network%20with%20Dual-Level%0A%20%20Awareness&body=Title%3A%20HetCAN%3A%20A%20Heterogeneous%20Graph%20Cascade%20Attention%20Network%20with%20Dual-Level%0A%20%20Awareness%0AAuthor%3A%20Zeyuan%20Zhao%20and%20Qingqing%20Ge%20and%20Anfeng%20Cheng%20and%20Yiding%20Liu%20and%20Xiang%20Li%20and%20Shuaiqiang%20Wang%0AAbstract%3A%20%20%20Heterogeneous%20graph%20neural%20networks%28HGNNs%29%20have%20recently%20shown%20impressive%0Acapability%20in%20modeling%20heterogeneous%20graphs%20that%20are%20ubiquitous%20in%20real-world%0Aapplications.%20Most%20existing%20methods%20for%20heterogeneous%20graphs%20mainly%20learn%20node%0Aembeddings%20by%20stacking%20multiple%20convolutional%20or%20attentional%20layers%2C%20which%20can%0Abe%20considered%20as%20capturing%20the%20high-order%20information%20from%20node-level%20aspect.%0AHowever%2C%20different%20types%20of%20nodes%20in%20heterogeneous%20graphs%20have%20diverse%0Afeatures%2C%20it%20is%20also%20necessary%20to%20capture%20interactions%20among%20node%20features%2C%0Anamely%20the%20high-order%20information%20from%20feature-level%20aspect.%20In%20addition%2C%20most%0Amethods%20first%20align%20node%20features%20by%20mapping%20them%20into%20one%20same%20low-dimensional%0Aspace%2C%20while%20they%20may%20lose%20some%20type%20information%20of%20nodes%20in%20this%20way.%20To%0Aaddress%20these%20problems%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20Heterogeneous%20graph%0ACascade%20Attention%20Network%20%28HetCAN%29%20composed%20of%20multiple%20cascade%20blocks.%20Each%0Acascade%20block%20includes%20two%20components%2C%20the%20type-aware%20encoder%20and%20the%0Adimension-aware%20encoder.%20Specifically%2C%20the%20type-aware%20encoder%20compensates%20for%0Athe%20loss%20of%20node%20type%20information%20and%20aims%20to%20make%20full%20use%20of%20graph%0Aheterogeneity.%20The%20dimension-aware%20encoder%20is%20able%20to%20learn%20the%20feature-level%0Ahigh-order%20information%20by%20capturing%20the%20interactions%20among%20node%20features.%20With%0Athe%20assistance%20of%20these%20components%2C%20HetCAN%20can%20comprehensively%20encode%0Ainformation%20of%20node%20features%2C%20graph%20heterogeneity%20and%20graph%20structure%20in%20node%0Aembeddings.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20HetCAN%20over%0Aadvanced%20competitors%20and%20also%20exhibit%20its%20efficiency%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHetCAN%253A%2520A%2520Heterogeneous%2520Graph%2520Cascade%2520Attention%2520Network%2520with%2520Dual-Level%250A%2520%2520Awareness%26entry.906535625%3DZeyuan%2520Zhao%2520and%2520Qingqing%2520Ge%2520and%2520Anfeng%2520Cheng%2520and%2520Yiding%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Shuaiqiang%2520Wang%26entry.1292438233%3D%2520%2520Heterogeneous%2520graph%2520neural%2520networks%2528HGNNs%2529%2520have%2520recently%2520shown%2520impressive%250Acapability%2520in%2520modeling%2520heterogeneous%2520graphs%2520that%2520are%2520ubiquitous%2520in%2520real-world%250Aapplications.%2520Most%2520existing%2520methods%2520for%2520heterogeneous%2520graphs%2520mainly%2520learn%2520node%250Aembeddings%2520by%2520stacking%2520multiple%2520convolutional%2520or%2520attentional%2520layers%252C%2520which%2520can%250Abe%2520considered%2520as%2520capturing%2520the%2520high-order%2520information%2520from%2520node-level%2520aspect.%250AHowever%252C%2520different%2520types%2520of%2520nodes%2520in%2520heterogeneous%2520graphs%2520have%2520diverse%250Afeatures%252C%2520it%2520is%2520also%2520necessary%2520to%2520capture%2520interactions%2520among%2520node%2520features%252C%250Anamely%2520the%2520high-order%2520information%2520from%2520feature-level%2520aspect.%2520In%2520addition%252C%2520most%250Amethods%2520first%2520align%2520node%2520features%2520by%2520mapping%2520them%2520into%2520one%2520same%2520low-dimensional%250Aspace%252C%2520while%2520they%2520may%2520lose%2520some%2520type%2520information%2520of%2520nodes%2520in%2520this%2520way.%2520To%250Aaddress%2520these%2520problems%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Heterogeneous%2520graph%250ACascade%2520Attention%2520Network%2520%2528HetCAN%2529%2520composed%2520of%2520multiple%2520cascade%2520blocks.%2520Each%250Acascade%2520block%2520includes%2520two%2520components%252C%2520the%2520type-aware%2520encoder%2520and%2520the%250Adimension-aware%2520encoder.%2520Specifically%252C%2520the%2520type-aware%2520encoder%2520compensates%2520for%250Athe%2520loss%2520of%2520node%2520type%2520information%2520and%2520aims%2520to%2520make%2520full%2520use%2520of%2520graph%250Aheterogeneity.%2520The%2520dimension-aware%2520encoder%2520is%2520able%2520to%2520learn%2520the%2520feature-level%250Ahigh-order%2520information%2520by%2520capturing%2520the%2520interactions%2520among%2520node%2520features.%2520With%250Athe%2520assistance%2520of%2520these%2520components%252C%2520HetCAN%2520can%2520comprehensively%2520encode%250Ainformation%2520of%2520node%2520features%252C%2520graph%2520heterogeneity%2520and%2520graph%2520structure%2520in%2520node%250Aembeddings.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520HetCAN%2520over%250Aadvanced%2520competitors%2520and%2520also%2520exhibit%2520its%2520efficiency%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HetCAN%3A%20A%20Heterogeneous%20Graph%20Cascade%20Attention%20Network%20with%20Dual-Level%0A%20%20Awareness&entry.906535625=Zeyuan%20Zhao%20and%20Qingqing%20Ge%20and%20Anfeng%20Cheng%20and%20Yiding%20Liu%20and%20Xiang%20Li%20and%20Shuaiqiang%20Wang&entry.1292438233=%20%20Heterogeneous%20graph%20neural%20networks%28HGNNs%29%20have%20recently%20shown%20impressive%0Acapability%20in%20modeling%20heterogeneous%20graphs%20that%20are%20ubiquitous%20in%20real-world%0Aapplications.%20Most%20existing%20methods%20for%20heterogeneous%20graphs%20mainly%20learn%20node%0Aembeddings%20by%20stacking%20multiple%20convolutional%20or%20attentional%20layers%2C%20which%20can%0Abe%20considered%20as%20capturing%20the%20high-order%20information%20from%20node-level%20aspect.%0AHowever%2C%20different%20types%20of%20nodes%20in%20heterogeneous%20graphs%20have%20diverse%0Afeatures%2C%20it%20is%20also%20necessary%20to%20capture%20interactions%20among%20node%20features%2C%0Anamely%20the%20high-order%20information%20from%20feature-level%20aspect.%20In%20addition%2C%20most%0Amethods%20first%20align%20node%20features%20by%20mapping%20them%20into%20one%20same%20low-dimensional%0Aspace%2C%20while%20they%20may%20lose%20some%20type%20information%20of%20nodes%20in%20this%20way.%20To%0Aaddress%20these%20problems%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20Heterogeneous%20graph%0ACascade%20Attention%20Network%20%28HetCAN%29%20composed%20of%20multiple%20cascade%20blocks.%20Each%0Acascade%20block%20includes%20two%20components%2C%20the%20type-aware%20encoder%20and%20the%0Adimension-aware%20encoder.%20Specifically%2C%20the%20type-aware%20encoder%20compensates%20for%0Athe%20loss%20of%20node%20type%20information%20and%20aims%20to%20make%20full%20use%20of%20graph%0Aheterogeneity.%20The%20dimension-aware%20encoder%20is%20able%20to%20learn%20the%20feature-level%0Ahigh-order%20information%20by%20capturing%20the%20interactions%20among%20node%20features.%20With%0Athe%20assistance%20of%20these%20components%2C%20HetCAN%20can%20comprehensively%20encode%0Ainformation%20of%20node%20features%2C%20graph%20heterogeneity%20and%20graph%20structure%20in%20node%0Aembeddings.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20HetCAN%20over%0Aadvanced%20competitors%20and%20also%20exhibit%20its%20efficiency%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03275v2&entry.124074799=Read"},
{"title": "DGRC: An Effective Fine-tuning Framework for Distractor Generation in\n  Chinese Multi-choice Reading Comprehension", "author": "Runfeng Lin and Dacheng Xu and Huijiang Wang and Zebiao Chen and Yating Wang and Shouqiang Liu", "abstract": "  When evaluating a learner's knowledge proficiency, the multiple-choice\nquestion is an efficient and widely used format in standardized tests.\nNevertheless, generating these questions, particularly plausible distractors\n(incorrect options), poses a considerable challenge. Generally, the distractor\ngeneration can be classified into cloze-style distractor generation (CDG) and\nnatural questions distractor generation (NQDG). In contrast to the CDG,\nutilizing pre-trained language models (PLMs) for NQDG presents three primary\nchallenges: (1) PLMs are typically trained to generate ``correct'' content,\nlike answers, while rarely trained to generate ``plausible\" content, like\ndistractors; (2) PLMs often struggle to produce content that aligns well with\nspecific knowledge and the style of exams; (3) NQDG necessitates the model to\nproduce longer, context-sensitive, and question-relevant distractors. In this\nstudy, we introduce a fine-tuning framework named DGRC for NQDG in Chinese\nmulti-choice reading comprehension from authentic examinations. DGRC comprises\nthree major components: hard chain-of-thought, multi-task learning, and\ngeneration mask patterns. The experiment results demonstrate that DGRC\nsignificantly enhances generation performance, achieving a more than 2.5-fold\nimprovement in BLEU scores.\n", "link": "http://arxiv.org/abs/2405.19139v1", "date": "2024-05-29", "relevancy": 2.3939, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4891}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4794}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGRC%3A%20An%20Effective%20Fine-tuning%20Framework%20for%20Distractor%20Generation%20in%0A%20%20Chinese%20Multi-choice%20Reading%20Comprehension&body=Title%3A%20DGRC%3A%20An%20Effective%20Fine-tuning%20Framework%20for%20Distractor%20Generation%20in%0A%20%20Chinese%20Multi-choice%20Reading%20Comprehension%0AAuthor%3A%20Runfeng%20Lin%20and%20Dacheng%20Xu%20and%20Huijiang%20Wang%20and%20Zebiao%20Chen%20and%20Yating%20Wang%20and%20Shouqiang%20Liu%0AAbstract%3A%20%20%20When%20evaluating%20a%20learner%27s%20knowledge%20proficiency%2C%20the%20multiple-choice%0Aquestion%20is%20an%20efficient%20and%20widely%20used%20format%20in%20standardized%20tests.%0ANevertheless%2C%20generating%20these%20questions%2C%20particularly%20plausible%20distractors%0A%28incorrect%20options%29%2C%20poses%20a%20considerable%20challenge.%20Generally%2C%20the%20distractor%0Ageneration%20can%20be%20classified%20into%20cloze-style%20distractor%20generation%20%28CDG%29%20and%0Anatural%20questions%20distractor%20generation%20%28NQDG%29.%20In%20contrast%20to%20the%20CDG%2C%0Autilizing%20pre-trained%20language%20models%20%28PLMs%29%20for%20NQDG%20presents%20three%20primary%0Achallenges%3A%20%281%29%20PLMs%20are%20typically%20trained%20to%20generate%20%60%60correct%27%27%20content%2C%0Alike%20answers%2C%20while%20rarely%20trained%20to%20generate%20%60%60plausible%22%20content%2C%20like%0Adistractors%3B%20%282%29%20PLMs%20often%20struggle%20to%20produce%20content%20that%20aligns%20well%20with%0Aspecific%20knowledge%20and%20the%20style%20of%20exams%3B%20%283%29%20NQDG%20necessitates%20the%20model%20to%0Aproduce%20longer%2C%20context-sensitive%2C%20and%20question-relevant%20distractors.%20In%20this%0Astudy%2C%20we%20introduce%20a%20fine-tuning%20framework%20named%20DGRC%20for%20NQDG%20in%20Chinese%0Amulti-choice%20reading%20comprehension%20from%20authentic%20examinations.%20DGRC%20comprises%0Athree%20major%20components%3A%20hard%20chain-of-thought%2C%20multi-task%20learning%2C%20and%0Ageneration%20mask%20patterns.%20The%20experiment%20results%20demonstrate%20that%20DGRC%0Asignificantly%20enhances%20generation%20performance%2C%20achieving%20a%20more%20than%202.5-fold%0Aimprovement%20in%20BLEU%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGRC%253A%2520An%2520Effective%2520Fine-tuning%2520Framework%2520for%2520Distractor%2520Generation%2520in%250A%2520%2520Chinese%2520Multi-choice%2520Reading%2520Comprehension%26entry.906535625%3DRunfeng%2520Lin%2520and%2520Dacheng%2520Xu%2520and%2520Huijiang%2520Wang%2520and%2520Zebiao%2520Chen%2520and%2520Yating%2520Wang%2520and%2520Shouqiang%2520Liu%26entry.1292438233%3D%2520%2520When%2520evaluating%2520a%2520learner%2527s%2520knowledge%2520proficiency%252C%2520the%2520multiple-choice%250Aquestion%2520is%2520an%2520efficient%2520and%2520widely%2520used%2520format%2520in%2520standardized%2520tests.%250ANevertheless%252C%2520generating%2520these%2520questions%252C%2520particularly%2520plausible%2520distractors%250A%2528incorrect%2520options%2529%252C%2520poses%2520a%2520considerable%2520challenge.%2520Generally%252C%2520the%2520distractor%250Ageneration%2520can%2520be%2520classified%2520into%2520cloze-style%2520distractor%2520generation%2520%2528CDG%2529%2520and%250Anatural%2520questions%2520distractor%2520generation%2520%2528NQDG%2529.%2520In%2520contrast%2520to%2520the%2520CDG%252C%250Autilizing%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520for%2520NQDG%2520presents%2520three%2520primary%250Achallenges%253A%2520%25281%2529%2520PLMs%2520are%2520typically%2520trained%2520to%2520generate%2520%2560%2560correct%2527%2527%2520content%252C%250Alike%2520answers%252C%2520while%2520rarely%2520trained%2520to%2520generate%2520%2560%2560plausible%2522%2520content%252C%2520like%250Adistractors%253B%2520%25282%2529%2520PLMs%2520often%2520struggle%2520to%2520produce%2520content%2520that%2520aligns%2520well%2520with%250Aspecific%2520knowledge%2520and%2520the%2520style%2520of%2520exams%253B%2520%25283%2529%2520NQDG%2520necessitates%2520the%2520model%2520to%250Aproduce%2520longer%252C%2520context-sensitive%252C%2520and%2520question-relevant%2520distractors.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520a%2520fine-tuning%2520framework%2520named%2520DGRC%2520for%2520NQDG%2520in%2520Chinese%250Amulti-choice%2520reading%2520comprehension%2520from%2520authentic%2520examinations.%2520DGRC%2520comprises%250Athree%2520major%2520components%253A%2520hard%2520chain-of-thought%252C%2520multi-task%2520learning%252C%2520and%250Ageneration%2520mask%2520patterns.%2520The%2520experiment%2520results%2520demonstrate%2520that%2520DGRC%250Asignificantly%2520enhances%2520generation%2520performance%252C%2520achieving%2520a%2520more%2520than%25202.5-fold%250Aimprovement%2520in%2520BLEU%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGRC%3A%20An%20Effective%20Fine-tuning%20Framework%20for%20Distractor%20Generation%20in%0A%20%20Chinese%20Multi-choice%20Reading%20Comprehension&entry.906535625=Runfeng%20Lin%20and%20Dacheng%20Xu%20and%20Huijiang%20Wang%20and%20Zebiao%20Chen%20and%20Yating%20Wang%20and%20Shouqiang%20Liu&entry.1292438233=%20%20When%20evaluating%20a%20learner%27s%20knowledge%20proficiency%2C%20the%20multiple-choice%0Aquestion%20is%20an%20efficient%20and%20widely%20used%20format%20in%20standardized%20tests.%0ANevertheless%2C%20generating%20these%20questions%2C%20particularly%20plausible%20distractors%0A%28incorrect%20options%29%2C%20poses%20a%20considerable%20challenge.%20Generally%2C%20the%20distractor%0Ageneration%20can%20be%20classified%20into%20cloze-style%20distractor%20generation%20%28CDG%29%20and%0Anatural%20questions%20distractor%20generation%20%28NQDG%29.%20In%20contrast%20to%20the%20CDG%2C%0Autilizing%20pre-trained%20language%20models%20%28PLMs%29%20for%20NQDG%20presents%20three%20primary%0Achallenges%3A%20%281%29%20PLMs%20are%20typically%20trained%20to%20generate%20%60%60correct%27%27%20content%2C%0Alike%20answers%2C%20while%20rarely%20trained%20to%20generate%20%60%60plausible%22%20content%2C%20like%0Adistractors%3B%20%282%29%20PLMs%20often%20struggle%20to%20produce%20content%20that%20aligns%20well%20with%0Aspecific%20knowledge%20and%20the%20style%20of%20exams%3B%20%283%29%20NQDG%20necessitates%20the%20model%20to%0Aproduce%20longer%2C%20context-sensitive%2C%20and%20question-relevant%20distractors.%20In%20this%0Astudy%2C%20we%20introduce%20a%20fine-tuning%20framework%20named%20DGRC%20for%20NQDG%20in%20Chinese%0Amulti-choice%20reading%20comprehension%20from%20authentic%20examinations.%20DGRC%20comprises%0Athree%20major%20components%3A%20hard%20chain-of-thought%2C%20multi-task%20learning%2C%20and%0Ageneration%20mask%20patterns.%20The%20experiment%20results%20demonstrate%20that%20DGRC%0Asignificantly%20enhances%20generation%20performance%2C%20achieving%20a%20more%20than%202.5-fold%0Aimprovement%20in%20BLEU%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19139v1&entry.124074799=Read"},
{"title": "Benchmarking General Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) capabilities are becoming increasingly appealing\nfor building general intelligence due to their sample efficiency and\nindependence from artificial optimization skills. To enhance generalization,\nbiological neural systems primarily inherit learning capabilities and\nsubsequently refine their memory, acquiring diverse skills and knowledge\nthrough extensive lifelong experiences. This process gives rise to the concept\nof general-purpose in-context learning (GPICL). Compared to standard ICL, GPICL\naddresses a broader range of tasks, extends learning horizons, and starts at a\nlower zero-shot baseline. We introduce two lightweight but insightful\nbenchmarks specifically crafted to train and evaluate GPICL functionalities.\nEach benchmark includes a vast number of tasks characterized by significant\ntask variance and minimal transferable knowledge among tasks, facilitating\nlifelong in-context learning through continuous generation and interaction.\nThese features pose significant challenges for models that rely on context or\ninteractions to improve their proficiency, including language models, decision\nmodels, and world models. Our experiments reveal that parameter scale alone may\nnot be crucial for ICL or GPICL, suggesting alternative approaches such as\nincreasing the scale of contexts and memory states.\n", "link": "http://arxiv.org/abs/2405.17234v2", "date": "2024-05-29", "relevancy": 2.3779, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4593}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General%20Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General%20Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20capabilities%20are%20becoming%20increasingly%20appealing%0Afor%20building%20general%20intelligence%20due%20to%20their%20sample%20efficiency%20and%0Aindependence%20from%20artificial%20optimization%20skills.%20To%20enhance%20generalization%2C%0Abiological%20neural%20systems%20primarily%20inherit%20learning%20capabilities%20and%0Asubsequently%20refine%20their%20memory%2C%20acquiring%20diverse%20skills%20and%20knowledge%0Athrough%20extensive%20lifelong%20experiences.%20This%20process%20gives%20rise%20to%20the%20concept%0Aof%20general-purpose%20in-context%20learning%20%28GPICL%29.%20Compared%20to%20standard%20ICL%2C%20GPICL%0Aaddresses%20a%20broader%20range%20of%20tasks%2C%20extends%20learning%20horizons%2C%20and%20starts%20at%20a%0Alower%20zero-shot%20baseline.%20We%20introduce%20two%20lightweight%20but%20insightful%0Abenchmarks%20specifically%20crafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%0AEach%20benchmark%20includes%20a%20vast%20number%20of%20tasks%20characterized%20by%20significant%0Atask%20variance%20and%20minimal%20transferable%20knowledge%20among%20tasks%2C%20facilitating%0Alifelong%20in-context%20learning%20through%20continuous%20generation%20and%20interaction.%0AThese%20features%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%20context%20or%0Ainteractions%20to%20improve%20their%20proficiency%2C%20including%20language%20models%2C%20decision%0Amodels%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20parameter%20scale%20alone%20may%0Anot%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%0Aincreasing%20the%20scale%20of%20contexts%20and%20memory%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General%2520Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520capabilities%2520are%2520becoming%2520increasingly%2520appealing%250Afor%2520building%2520general%2520intelligence%2520due%2520to%2520their%2520sample%2520efficiency%2520and%250Aindependence%2520from%2520artificial%2520optimization%2520skills.%2520To%2520enhance%2520generalization%252C%250Abiological%2520neural%2520systems%2520primarily%2520inherit%2520learning%2520capabilities%2520and%250Asubsequently%2520refine%2520their%2520memory%252C%2520acquiring%2520diverse%2520skills%2520and%2520knowledge%250Athrough%2520extensive%2520lifelong%2520experiences.%2520This%2520process%2520gives%2520rise%2520to%2520the%2520concept%250Aof%2520general-purpose%2520in-context%2520learning%2520%2528GPICL%2529.%2520Compared%2520to%2520standard%2520ICL%252C%2520GPICL%250Aaddresses%2520a%2520broader%2520range%2520of%2520tasks%252C%2520extends%2520learning%2520horizons%252C%2520and%2520starts%2520at%2520a%250Alower%2520zero-shot%2520baseline.%2520We%2520introduce%2520two%2520lightweight%2520but%2520insightful%250Abenchmarks%2520specifically%2520crafted%2520to%2520train%2520and%2520evaluate%2520GPICL%2520functionalities.%250AEach%2520benchmark%2520includes%2520a%2520vast%2520number%2520of%2520tasks%2520characterized%2520by%2520significant%250Atask%2520variance%2520and%2520minimal%2520transferable%2520knowledge%2520among%2520tasks%252C%2520facilitating%250Alifelong%2520in-context%2520learning%2520through%2520continuous%2520generation%2520and%2520interaction.%250AThese%2520features%2520pose%2520significant%2520challenges%2520for%2520models%2520that%2520rely%2520on%2520context%2520or%250Ainteractions%2520to%2520improve%2520their%2520proficiency%252C%2520including%2520language%2520models%252C%2520decision%250Amodels%252C%2520and%2520world%2520models.%2520Our%2520experiments%2520reveal%2520that%2520parameter%2520scale%2520alone%2520may%250Anot%2520be%2520crucial%2520for%2520ICL%2520or%2520GPICL%252C%2520suggesting%2520alternative%2520approaches%2520such%2520as%250Aincreasing%2520the%2520scale%2520of%2520contexts%2520and%2520memory%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General%20Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20capabilities%20are%20becoming%20increasingly%20appealing%0Afor%20building%20general%20intelligence%20due%20to%20their%20sample%20efficiency%20and%0Aindependence%20from%20artificial%20optimization%20skills.%20To%20enhance%20generalization%2C%0Abiological%20neural%20systems%20primarily%20inherit%20learning%20capabilities%20and%0Asubsequently%20refine%20their%20memory%2C%20acquiring%20diverse%20skills%20and%20knowledge%0Athrough%20extensive%20lifelong%20experiences.%20This%20process%20gives%20rise%20to%20the%20concept%0Aof%20general-purpose%20in-context%20learning%20%28GPICL%29.%20Compared%20to%20standard%20ICL%2C%20GPICL%0Aaddresses%20a%20broader%20range%20of%20tasks%2C%20extends%20learning%20horizons%2C%20and%20starts%20at%20a%0Alower%20zero-shot%20baseline.%20We%20introduce%20two%20lightweight%20but%20insightful%0Abenchmarks%20specifically%20crafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%0AEach%20benchmark%20includes%20a%20vast%20number%20of%20tasks%20characterized%20by%20significant%0Atask%20variance%20and%20minimal%20transferable%20knowledge%20among%20tasks%2C%20facilitating%0Alifelong%20in-context%20learning%20through%20continuous%20generation%20and%20interaction.%0AThese%20features%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%20context%20or%0Ainteractions%20to%20improve%20their%20proficiency%2C%20including%20language%20models%2C%20decision%0Amodels%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20parameter%20scale%20alone%20may%0Anot%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%0Aincreasing%20the%20scale%20of%20contexts%20and%20memory%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v2&entry.124074799=Read"},
{"title": "A Good Foundation is Worth Many Labels: Label-Efficient Panoptic\n  Segmentation", "author": "Niclas V\u00f6disch and K\u00fcrsat Petek and Markus K\u00e4ppeler and Abhinav Valada and Wolfram Burgard", "abstract": "  A key challenge for the widespread application of learning-based models for\nrobotic perception is to significantly reduce the required amount of annotated\ntraining data while achieving accurate predictions. This is essential not only\nto decrease operating costs but also to speed up deployment time. In this work,\nwe address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by\nexploiting the groundwork paved by visual foundation models. We leverage\ndescriptive image features from such a model to train two lightweight network\nheads for semantic segmentation and object boundary detection, using very few\nannotated training samples. We then merge their predictions via a novel fusion\nmodule that yields panoptic maps based on normalized cut. To further enhance\nthe performance, we utilize self-training on unlabeled images selected by a\nfeature-driven similarity scheme. We underline the relevance of our approach by\nemploying PASTEL to important robot perception use cases from autonomous\ndriving and agricultural robotics. In extensive experiments, we demonstrate\nthat PASTEL significantly outperforms previous methods for label-efficient\nsegmentation even when using fewer annotations. The code of our work is\npublicly available at http://pastel.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2405.19035v1", "date": "2024-05-29", "relevancy": 2.3582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation&body=Title%3A%20A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation%0AAuthor%3A%20Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20Markus%20K%C3%A4ppeler%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20A%20key%20challenge%20for%20the%20widespread%20application%20of%20learning-based%20models%20for%0Arobotic%20perception%20is%20to%20significantly%20reduce%20the%20required%20amount%20of%20annotated%0Atraining%20data%20while%20achieving%20accurate%20predictions.%20This%20is%20essential%20not%20only%0Ato%20decrease%20operating%20costs%20but%20also%20to%20speed%20up%20deployment%20time.%20In%20this%20work%2C%0Awe%20address%20this%20challenge%20for%20PAnoptic%20SegmenTation%20with%20fEw%20Labels%20%28PASTEL%29%20by%0Aexploiting%20the%20groundwork%20paved%20by%20visual%20foundation%20models.%20We%20leverage%0Adescriptive%20image%20features%20from%20such%20a%20model%20to%20train%20two%20lightweight%20network%0Aheads%20for%20semantic%20segmentation%20and%20object%20boundary%20detection%2C%20using%20very%20few%0Aannotated%20training%20samples.%20We%20then%20merge%20their%20predictions%20via%20a%20novel%20fusion%0Amodule%20that%20yields%20panoptic%20maps%20based%20on%20normalized%20cut.%20To%20further%20enhance%0Athe%20performance%2C%20we%20utilize%20self-training%20on%20unlabeled%20images%20selected%20by%20a%0Afeature-driven%20similarity%20scheme.%20We%20underline%20the%20relevance%20of%20our%20approach%20by%0Aemploying%20PASTEL%20to%20important%20robot%20perception%20use%20cases%20from%20autonomous%0Adriving%20and%20agricultural%20robotics.%20In%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20PASTEL%20significantly%20outperforms%20previous%20methods%20for%20label-efficient%0Asegmentation%20even%20when%20using%20fewer%20annotations.%20The%20code%20of%20our%20work%20is%0Apublicly%20available%20at%20http%3A//pastel.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Good%2520Foundation%2520is%2520Worth%2520Many%2520Labels%253A%2520Label-Efficient%2520Panoptic%250A%2520%2520Segmentation%26entry.906535625%3DNiclas%2520V%25C3%25B6disch%2520and%2520K%25C3%25BCrsat%2520Petek%2520and%2520Markus%2520K%25C3%25A4ppeler%2520and%2520Abhinav%2520Valada%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520for%2520the%2520widespread%2520application%2520of%2520learning-based%2520models%2520for%250Arobotic%2520perception%2520is%2520to%2520significantly%2520reduce%2520the%2520required%2520amount%2520of%2520annotated%250Atraining%2520data%2520while%2520achieving%2520accurate%2520predictions.%2520This%2520is%2520essential%2520not%2520only%250Ato%2520decrease%2520operating%2520costs%2520but%2520also%2520to%2520speed%2520up%2520deployment%2520time.%2520In%2520this%2520work%252C%250Awe%2520address%2520this%2520challenge%2520for%2520PAnoptic%2520SegmenTation%2520with%2520fEw%2520Labels%2520%2528PASTEL%2529%2520by%250Aexploiting%2520the%2520groundwork%2520paved%2520by%2520visual%2520foundation%2520models.%2520We%2520leverage%250Adescriptive%2520image%2520features%2520from%2520such%2520a%2520model%2520to%2520train%2520two%2520lightweight%2520network%250Aheads%2520for%2520semantic%2520segmentation%2520and%2520object%2520boundary%2520detection%252C%2520using%2520very%2520few%250Aannotated%2520training%2520samples.%2520We%2520then%2520merge%2520their%2520predictions%2520via%2520a%2520novel%2520fusion%250Amodule%2520that%2520yields%2520panoptic%2520maps%2520based%2520on%2520normalized%2520cut.%2520To%2520further%2520enhance%250Athe%2520performance%252C%2520we%2520utilize%2520self-training%2520on%2520unlabeled%2520images%2520selected%2520by%2520a%250Afeature-driven%2520similarity%2520scheme.%2520We%2520underline%2520the%2520relevance%2520of%2520our%2520approach%2520by%250Aemploying%2520PASTEL%2520to%2520important%2520robot%2520perception%2520use%2520cases%2520from%2520autonomous%250Adriving%2520and%2520agricultural%2520robotics.%2520In%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520PASTEL%2520significantly%2520outperforms%2520previous%2520methods%2520for%2520label-efficient%250Asegmentation%2520even%2520when%2520using%2520fewer%2520annotations.%2520The%2520code%2520of%2520our%2520work%2520is%250Apublicly%2520available%2520at%2520http%253A//pastel.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation&entry.906535625=Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20Markus%20K%C3%A4ppeler%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20A%20key%20challenge%20for%20the%20widespread%20application%20of%20learning-based%20models%20for%0Arobotic%20perception%20is%20to%20significantly%20reduce%20the%20required%20amount%20of%20annotated%0Atraining%20data%20while%20achieving%20accurate%20predictions.%20This%20is%20essential%20not%20only%0Ato%20decrease%20operating%20costs%20but%20also%20to%20speed%20up%20deployment%20time.%20In%20this%20work%2C%0Awe%20address%20this%20challenge%20for%20PAnoptic%20SegmenTation%20with%20fEw%20Labels%20%28PASTEL%29%20by%0Aexploiting%20the%20groundwork%20paved%20by%20visual%20foundation%20models.%20We%20leverage%0Adescriptive%20image%20features%20from%20such%20a%20model%20to%20train%20two%20lightweight%20network%0Aheads%20for%20semantic%20segmentation%20and%20object%20boundary%20detection%2C%20using%20very%20few%0Aannotated%20training%20samples.%20We%20then%20merge%20their%20predictions%20via%20a%20novel%20fusion%0Amodule%20that%20yields%20panoptic%20maps%20based%20on%20normalized%20cut.%20To%20further%20enhance%0Athe%20performance%2C%20we%20utilize%20self-training%20on%20unlabeled%20images%20selected%20by%20a%0Afeature-driven%20similarity%20scheme.%20We%20underline%20the%20relevance%20of%20our%20approach%20by%0Aemploying%20PASTEL%20to%20important%20robot%20perception%20use%20cases%20from%20autonomous%0Adriving%20and%20agricultural%20robotics.%20In%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20PASTEL%20significantly%20outperforms%20previous%20methods%20for%20label-efficient%0Asegmentation%20even%20when%20using%20fewer%20annotations.%20The%20code%20of%20our%20work%20is%0Apublicly%20available%20at%20http%3A//pastel.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19035v1&entry.124074799=Read"},
{"title": "Programmable Motion Generation for Open-Set Motion Control Tasks", "author": "Hanchao Liu and Xiaohang Zhan and Shaoli Huang and Tai-Jiang Mu and Ying Shan", "abstract": "  Character animation in real-world scenarios necessitates a variety of\nconstraints, such as trajectories, key-frames, interactions, etc. Existing\nmethodologies typically treat single or a finite set of these constraint(s) as\nseparate control tasks. They are often specialized, and the tasks they address\nare rarely extendable or customizable. We categorize these as solutions to the\nclose-set motion control problem. In response to the complexity of practical\nmotion control, we propose and attempt to solve the open-set motion control\nproblem. This problem is characterized by an open and fully customizable set of\nmotion control tasks. To address this, we introduce a new paradigm,\nprogrammable motion generation. In this paradigm, any given motion control task\nis broken down into a combination of atomic constraints. These constraints are\nthen programmed into an error function that quantifies the degree to which a\nmotion sequence adheres to them. We utilize a pre-trained motion generation\nmodel and optimize its latent code to minimize the error function of the\ngenerated motion. Consequently, the generated motion not only inherits the\nprior of the generative model but also satisfies the required constraints.\nExperiments show that we can generate high-quality motions when addressing a\nwide range of unseen tasks. These tasks encompass motion control by motion\ndynamics, geometric constraints, physical laws, interactions with scenes,\nobjects or the character own body parts, etc. All of these are achieved in a\nunified approach, without the need for ad-hoc paired training data collection\nor specialized network designs. During the programming of novel tasks, we\nobserved the emergence of new skills beyond those of the prior model. With the\nassistance of large language models, we also achieved automatic programming. We\nhope that this work will pave the way for the motion control of general AI\nagents.\n", "link": "http://arxiv.org/abs/2405.19283v1", "date": "2024-05-29", "relevancy": 2.3463, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5962}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5813}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programmable%20Motion%20Generation%20for%20Open-Set%20Motion%20Control%20Tasks&body=Title%3A%20Programmable%20Motion%20Generation%20for%20Open-Set%20Motion%20Control%20Tasks%0AAuthor%3A%20Hanchao%20Liu%20and%20Xiaohang%20Zhan%20and%20Shaoli%20Huang%20and%20Tai-Jiang%20Mu%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Character%20animation%20in%20real-world%20scenarios%20necessitates%20a%20variety%20of%0Aconstraints%2C%20such%20as%20trajectories%2C%20key-frames%2C%20interactions%2C%20etc.%20Existing%0Amethodologies%20typically%20treat%20single%20or%20a%20finite%20set%20of%20these%20constraint%28s%29%20as%0Aseparate%20control%20tasks.%20They%20are%20often%20specialized%2C%20and%20the%20tasks%20they%20address%0Aare%20rarely%20extendable%20or%20customizable.%20We%20categorize%20these%20as%20solutions%20to%20the%0Aclose-set%20motion%20control%20problem.%20In%20response%20to%20the%20complexity%20of%20practical%0Amotion%20control%2C%20we%20propose%20and%20attempt%20to%20solve%20the%20open-set%20motion%20control%0Aproblem.%20This%20problem%20is%20characterized%20by%20an%20open%20and%20fully%20customizable%20set%20of%0Amotion%20control%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%20new%20paradigm%2C%0Aprogrammable%20motion%20generation.%20In%20this%20paradigm%2C%20any%20given%20motion%20control%20task%0Ais%20broken%20down%20into%20a%20combination%20of%20atomic%20constraints.%20These%20constraints%20are%0Athen%20programmed%20into%20an%20error%20function%20that%20quantifies%20the%20degree%20to%20which%20a%0Amotion%20sequence%20adheres%20to%20them.%20We%20utilize%20a%20pre-trained%20motion%20generation%0Amodel%20and%20optimize%20its%20latent%20code%20to%20minimize%20the%20error%20function%20of%20the%0Agenerated%20motion.%20Consequently%2C%20the%20generated%20motion%20not%20only%20inherits%20the%0Aprior%20of%20the%20generative%20model%20but%20also%20satisfies%20the%20required%20constraints.%0AExperiments%20show%20that%20we%20can%20generate%20high-quality%20motions%20when%20addressing%20a%0Awide%20range%20of%20unseen%20tasks.%20These%20tasks%20encompass%20motion%20control%20by%20motion%0Adynamics%2C%20geometric%20constraints%2C%20physical%20laws%2C%20interactions%20with%20scenes%2C%0Aobjects%20or%20the%20character%20own%20body%20parts%2C%20etc.%20All%20of%20these%20are%20achieved%20in%20a%0Aunified%20approach%2C%20without%20the%20need%20for%20ad-hoc%20paired%20training%20data%20collection%0Aor%20specialized%20network%20designs.%20During%20the%20programming%20of%20novel%20tasks%2C%20we%0Aobserved%20the%20emergence%20of%20new%20skills%20beyond%20those%20of%20the%20prior%20model.%20With%20the%0Aassistance%20of%20large%20language%20models%2C%20we%20also%20achieved%20automatic%20programming.%20We%0Ahope%20that%20this%20work%20will%20pave%20the%20way%20for%20the%20motion%20control%20of%20general%20AI%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgrammable%2520Motion%2520Generation%2520for%2520Open-Set%2520Motion%2520Control%2520Tasks%26entry.906535625%3DHanchao%2520Liu%2520and%2520Xiaohang%2520Zhan%2520and%2520Shaoli%2520Huang%2520and%2520Tai-Jiang%2520Mu%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Character%2520animation%2520in%2520real-world%2520scenarios%2520necessitates%2520a%2520variety%2520of%250Aconstraints%252C%2520such%2520as%2520trajectories%252C%2520key-frames%252C%2520interactions%252C%2520etc.%2520Existing%250Amethodologies%2520typically%2520treat%2520single%2520or%2520a%2520finite%2520set%2520of%2520these%2520constraint%2528s%2529%2520as%250Aseparate%2520control%2520tasks.%2520They%2520are%2520often%2520specialized%252C%2520and%2520the%2520tasks%2520they%2520address%250Aare%2520rarely%2520extendable%2520or%2520customizable.%2520We%2520categorize%2520these%2520as%2520solutions%2520to%2520the%250Aclose-set%2520motion%2520control%2520problem.%2520In%2520response%2520to%2520the%2520complexity%2520of%2520practical%250Amotion%2520control%252C%2520we%2520propose%2520and%2520attempt%2520to%2520solve%2520the%2520open-set%2520motion%2520control%250Aproblem.%2520This%2520problem%2520is%2520characterized%2520by%2520an%2520open%2520and%2520fully%2520customizable%2520set%2520of%250Amotion%2520control%2520tasks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%2520paradigm%252C%250Aprogrammable%2520motion%2520generation.%2520In%2520this%2520paradigm%252C%2520any%2520given%2520motion%2520control%2520task%250Ais%2520broken%2520down%2520into%2520a%2520combination%2520of%2520atomic%2520constraints.%2520These%2520constraints%2520are%250Athen%2520programmed%2520into%2520an%2520error%2520function%2520that%2520quantifies%2520the%2520degree%2520to%2520which%2520a%250Amotion%2520sequence%2520adheres%2520to%2520them.%2520We%2520utilize%2520a%2520pre-trained%2520motion%2520generation%250Amodel%2520and%2520optimize%2520its%2520latent%2520code%2520to%2520minimize%2520the%2520error%2520function%2520of%2520the%250Agenerated%2520motion.%2520Consequently%252C%2520the%2520generated%2520motion%2520not%2520only%2520inherits%2520the%250Aprior%2520of%2520the%2520generative%2520model%2520but%2520also%2520satisfies%2520the%2520required%2520constraints.%250AExperiments%2520show%2520that%2520we%2520can%2520generate%2520high-quality%2520motions%2520when%2520addressing%2520a%250Awide%2520range%2520of%2520unseen%2520tasks.%2520These%2520tasks%2520encompass%2520motion%2520control%2520by%2520motion%250Adynamics%252C%2520geometric%2520constraints%252C%2520physical%2520laws%252C%2520interactions%2520with%2520scenes%252C%250Aobjects%2520or%2520the%2520character%2520own%2520body%2520parts%252C%2520etc.%2520All%2520of%2520these%2520are%2520achieved%2520in%2520a%250Aunified%2520approach%252C%2520without%2520the%2520need%2520for%2520ad-hoc%2520paired%2520training%2520data%2520collection%250Aor%2520specialized%2520network%2520designs.%2520During%2520the%2520programming%2520of%2520novel%2520tasks%252C%2520we%250Aobserved%2520the%2520emergence%2520of%2520new%2520skills%2520beyond%2520those%2520of%2520the%2520prior%2520model.%2520With%2520the%250Aassistance%2520of%2520large%2520language%2520models%252C%2520we%2520also%2520achieved%2520automatic%2520programming.%2520We%250Ahope%2520that%2520this%2520work%2520will%2520pave%2520the%2520way%2520for%2520the%2520motion%2520control%2520of%2520general%2520AI%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programmable%20Motion%20Generation%20for%20Open-Set%20Motion%20Control%20Tasks&entry.906535625=Hanchao%20Liu%20and%20Xiaohang%20Zhan%20and%20Shaoli%20Huang%20and%20Tai-Jiang%20Mu%20and%20Ying%20Shan&entry.1292438233=%20%20Character%20animation%20in%20real-world%20scenarios%20necessitates%20a%20variety%20of%0Aconstraints%2C%20such%20as%20trajectories%2C%20key-frames%2C%20interactions%2C%20etc.%20Existing%0Amethodologies%20typically%20treat%20single%20or%20a%20finite%20set%20of%20these%20constraint%28s%29%20as%0Aseparate%20control%20tasks.%20They%20are%20often%20specialized%2C%20and%20the%20tasks%20they%20address%0Aare%20rarely%20extendable%20or%20customizable.%20We%20categorize%20these%20as%20solutions%20to%20the%0Aclose-set%20motion%20control%20problem.%20In%20response%20to%20the%20complexity%20of%20practical%0Amotion%20control%2C%20we%20propose%20and%20attempt%20to%20solve%20the%20open-set%20motion%20control%0Aproblem.%20This%20problem%20is%20characterized%20by%20an%20open%20and%20fully%20customizable%20set%20of%0Amotion%20control%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%20new%20paradigm%2C%0Aprogrammable%20motion%20generation.%20In%20this%20paradigm%2C%20any%20given%20motion%20control%20task%0Ais%20broken%20down%20into%20a%20combination%20of%20atomic%20constraints.%20These%20constraints%20are%0Athen%20programmed%20into%20an%20error%20function%20that%20quantifies%20the%20degree%20to%20which%20a%0Amotion%20sequence%20adheres%20to%20them.%20We%20utilize%20a%20pre-trained%20motion%20generation%0Amodel%20and%20optimize%20its%20latent%20code%20to%20minimize%20the%20error%20function%20of%20the%0Agenerated%20motion.%20Consequently%2C%20the%20generated%20motion%20not%20only%20inherits%20the%0Aprior%20of%20the%20generative%20model%20but%20also%20satisfies%20the%20required%20constraints.%0AExperiments%20show%20that%20we%20can%20generate%20high-quality%20motions%20when%20addressing%20a%0Awide%20range%20of%20unseen%20tasks.%20These%20tasks%20encompass%20motion%20control%20by%20motion%0Adynamics%2C%20geometric%20constraints%2C%20physical%20laws%2C%20interactions%20with%20scenes%2C%0Aobjects%20or%20the%20character%20own%20body%20parts%2C%20etc.%20All%20of%20these%20are%20achieved%20in%20a%0Aunified%20approach%2C%20without%20the%20need%20for%20ad-hoc%20paired%20training%20data%20collection%0Aor%20specialized%20network%20designs.%20During%20the%20programming%20of%20novel%20tasks%2C%20we%0Aobserved%20the%20emergence%20of%20new%20skills%20beyond%20those%20of%20the%20prior%20model.%20With%20the%0Aassistance%20of%20large%20language%20models%2C%20we%20also%20achieved%20automatic%20programming.%20We%0Ahope%20that%20this%20work%20will%20pave%20the%20way%20for%20the%20motion%20control%20of%20general%20AI%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19283v1&entry.124074799=Read"},
{"title": "A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered\n  by Semantic Communication", "author": "Runze Cheng and Yao Sun and Dusit Niyato and Lan Zhang and Lei Zhang and Muhammad Ali Imran", "abstract": "  Generative AI applications have been recently catering to a vast user base by\ncreating diverse and high-quality AI-generated content (AIGC). With the\nproliferation of mobile devices and rapid growth of mobile traffic, providing\nubiquitous access to high-quality AIGC services via wireless communication\nnetworks is becoming the future direction. However, it is challenging to\nprovide qualified AIGC services in wireless networks with unstable channels,\nlimited bandwidth resources, and unevenly distributed computational resources.\nTo tackle these challenges, we propose a semantic communication\n(SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where\nonly semantic information of the content rather than all the binary bits should\nbe generated and transmitted by using SemCom. Specifically, SemAIGC integrates\ndiffusion models within the semantic encoder and decoder to design a\nworkload-adjustable transceiver thereby allowing adjustment of computational\nresource utilization in edge and local. In addition, a Resource-aware wOrk lOad\nTrade-off (ROOT) scheme is devised to intelligently make workload adaptation\ndecisions for the transceiver, thus efficiently generating, transmitting, and\nfine-tuning content as per dynamic wireless channel conditions and service\nrequirements. Simulations verify the superiority of our proposed SemAIGC\nframework in terms of latency and content quality compared to conventional\napproaches.\n", "link": "http://arxiv.org/abs/2310.17705v2", "date": "2024-05-29", "relevancy": 2.3313, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4874}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4627}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Wireless%20AI-Generated%20Content%20%28AIGC%29%20Provisioning%20Framework%20Empowered%0A%20%20by%20Semantic%20Communication&body=Title%3A%20A%20Wireless%20AI-Generated%20Content%20%28AIGC%29%20Provisioning%20Framework%20Empowered%0A%20%20by%20Semantic%20Communication%0AAuthor%3A%20Runze%20Cheng%20and%20Yao%20Sun%20and%20Dusit%20Niyato%20and%20Lan%20Zhang%20and%20Lei%20Zhang%20and%20Muhammad%20Ali%20Imran%0AAbstract%3A%20%20%20Generative%20AI%20applications%20have%20been%20recently%20catering%20to%20a%20vast%20user%20base%20by%0Acreating%20diverse%20and%20high-quality%20AI-generated%20content%20%28AIGC%29.%20With%20the%0Aproliferation%20of%20mobile%20devices%20and%20rapid%20growth%20of%20mobile%20traffic%2C%20providing%0Aubiquitous%20access%20to%20high-quality%20AIGC%20services%20via%20wireless%20communication%0Anetworks%20is%20becoming%20the%20future%20direction.%20However%2C%20it%20is%20challenging%20to%0Aprovide%20qualified%20AIGC%20services%20in%20wireless%20networks%20with%20unstable%20channels%2C%0Alimited%20bandwidth%20resources%2C%20and%20unevenly%20distributed%20computational%20resources.%0ATo%20tackle%20these%20challenges%2C%20we%20propose%20a%20semantic%20communication%0A%28SemCom%29-empowered%20AIGC%20%28SemAIGC%29%20generation%20and%20transmission%20framework%2C%20where%0Aonly%20semantic%20information%20of%20the%20content%20rather%20than%20all%20the%20binary%20bits%20should%0Abe%20generated%20and%20transmitted%20by%20using%20SemCom.%20Specifically%2C%20SemAIGC%20integrates%0Adiffusion%20models%20within%20the%20semantic%20encoder%20and%20decoder%20to%20design%20a%0Aworkload-adjustable%20transceiver%20thereby%20allowing%20adjustment%20of%20computational%0Aresource%20utilization%20in%20edge%20and%20local.%20In%20addition%2C%20a%20Resource-aware%20wOrk%20lOad%0ATrade-off%20%28ROOT%29%20scheme%20is%20devised%20to%20intelligently%20make%20workload%20adaptation%0Adecisions%20for%20the%20transceiver%2C%20thus%20efficiently%20generating%2C%20transmitting%2C%20and%0Afine-tuning%20content%20as%20per%20dynamic%20wireless%20channel%20conditions%20and%20service%0Arequirements.%20Simulations%20verify%20the%20superiority%20of%20our%20proposed%20SemAIGC%0Aframework%20in%20terms%20of%20latency%20and%20content%20quality%20compared%20to%20conventional%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Wireless%2520AI-Generated%2520Content%2520%2528AIGC%2529%2520Provisioning%2520Framework%2520Empowered%250A%2520%2520by%2520Semantic%2520Communication%26entry.906535625%3DRunze%2520Cheng%2520and%2520Yao%2520Sun%2520and%2520Dusit%2520Niyato%2520and%2520Lan%2520Zhang%2520and%2520Lei%2520Zhang%2520and%2520Muhammad%2520Ali%2520Imran%26entry.1292438233%3D%2520%2520Generative%2520AI%2520applications%2520have%2520been%2520recently%2520catering%2520to%2520a%2520vast%2520user%2520base%2520by%250Acreating%2520diverse%2520and%2520high-quality%2520AI-generated%2520content%2520%2528AIGC%2529.%2520With%2520the%250Aproliferation%2520of%2520mobile%2520devices%2520and%2520rapid%2520growth%2520of%2520mobile%2520traffic%252C%2520providing%250Aubiquitous%2520access%2520to%2520high-quality%2520AIGC%2520services%2520via%2520wireless%2520communication%250Anetworks%2520is%2520becoming%2520the%2520future%2520direction.%2520However%252C%2520it%2520is%2520challenging%2520to%250Aprovide%2520qualified%2520AIGC%2520services%2520in%2520wireless%2520networks%2520with%2520unstable%2520channels%252C%250Alimited%2520bandwidth%2520resources%252C%2520and%2520unevenly%2520distributed%2520computational%2520resources.%250ATo%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520semantic%2520communication%250A%2528SemCom%2529-empowered%2520AIGC%2520%2528SemAIGC%2529%2520generation%2520and%2520transmission%2520framework%252C%2520where%250Aonly%2520semantic%2520information%2520of%2520the%2520content%2520rather%2520than%2520all%2520the%2520binary%2520bits%2520should%250Abe%2520generated%2520and%2520transmitted%2520by%2520using%2520SemCom.%2520Specifically%252C%2520SemAIGC%2520integrates%250Adiffusion%2520models%2520within%2520the%2520semantic%2520encoder%2520and%2520decoder%2520to%2520design%2520a%250Aworkload-adjustable%2520transceiver%2520thereby%2520allowing%2520adjustment%2520of%2520computational%250Aresource%2520utilization%2520in%2520edge%2520and%2520local.%2520In%2520addition%252C%2520a%2520Resource-aware%2520wOrk%2520lOad%250ATrade-off%2520%2528ROOT%2529%2520scheme%2520is%2520devised%2520to%2520intelligently%2520make%2520workload%2520adaptation%250Adecisions%2520for%2520the%2520transceiver%252C%2520thus%2520efficiently%2520generating%252C%2520transmitting%252C%2520and%250Afine-tuning%2520content%2520as%2520per%2520dynamic%2520wireless%2520channel%2520conditions%2520and%2520service%250Arequirements.%2520Simulations%2520verify%2520the%2520superiority%2520of%2520our%2520proposed%2520SemAIGC%250Aframework%2520in%2520terms%2520of%2520latency%2520and%2520content%2520quality%2520compared%2520to%2520conventional%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Wireless%20AI-Generated%20Content%20%28AIGC%29%20Provisioning%20Framework%20Empowered%0A%20%20by%20Semantic%20Communication&entry.906535625=Runze%20Cheng%20and%20Yao%20Sun%20and%20Dusit%20Niyato%20and%20Lan%20Zhang%20and%20Lei%20Zhang%20and%20Muhammad%20Ali%20Imran&entry.1292438233=%20%20Generative%20AI%20applications%20have%20been%20recently%20catering%20to%20a%20vast%20user%20base%20by%0Acreating%20diverse%20and%20high-quality%20AI-generated%20content%20%28AIGC%29.%20With%20the%0Aproliferation%20of%20mobile%20devices%20and%20rapid%20growth%20of%20mobile%20traffic%2C%20providing%0Aubiquitous%20access%20to%20high-quality%20AIGC%20services%20via%20wireless%20communication%0Anetworks%20is%20becoming%20the%20future%20direction.%20However%2C%20it%20is%20challenging%20to%0Aprovide%20qualified%20AIGC%20services%20in%20wireless%20networks%20with%20unstable%20channels%2C%0Alimited%20bandwidth%20resources%2C%20and%20unevenly%20distributed%20computational%20resources.%0ATo%20tackle%20these%20challenges%2C%20we%20propose%20a%20semantic%20communication%0A%28SemCom%29-empowered%20AIGC%20%28SemAIGC%29%20generation%20and%20transmission%20framework%2C%20where%0Aonly%20semantic%20information%20of%20the%20content%20rather%20than%20all%20the%20binary%20bits%20should%0Abe%20generated%20and%20transmitted%20by%20using%20SemCom.%20Specifically%2C%20SemAIGC%20integrates%0Adiffusion%20models%20within%20the%20semantic%20encoder%20and%20decoder%20to%20design%20a%0Aworkload-adjustable%20transceiver%20thereby%20allowing%20adjustment%20of%20computational%0Aresource%20utilization%20in%20edge%20and%20local.%20In%20addition%2C%20a%20Resource-aware%20wOrk%20lOad%0ATrade-off%20%28ROOT%29%20scheme%20is%20devised%20to%20intelligently%20make%20workload%20adaptation%0Adecisions%20for%20the%20transceiver%2C%20thus%20efficiently%20generating%2C%20transmitting%2C%20and%0Afine-tuning%20content%20as%20per%20dynamic%20wireless%20channel%20conditions%20and%20service%0Arequirements.%20Simulations%20verify%20the%20superiority%20of%20our%20proposed%20SemAIGC%0Aframework%20in%20terms%20of%20latency%20and%20content%20quality%20compared%20to%20conventional%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17705v2&entry.124074799=Read"},
{"title": "Contrastive-Adversarial and Diffusion: Exploring pre-training and\n  fine-tuning strategies for sulcal identification", "author": "Michail Mamalakis and H\u00e9lo\u00efse de Vareilles and Shun-Chin Jim Wu and Ingrid Agartz and Lynn Egeland M\u00f8rch-Johnsen and Jane Garrison and Jon Simons and Pietro Lio and John Suckling and Graham Murray", "abstract": "  In the last decade, computer vision has witnessed the establishment of\nvarious training and learning approaches. Techniques like adversarial learning,\ncontrastive learning, diffusion denoising learning, and ordinary reconstruction\nlearning have become standard, representing state-of-the-art methods\nextensively employed for fully training or pre-training networks across various\nvision tasks. The exploration of fine-tuning approaches has emerged as a\ncurrent focal point, addressing the need for efficient model tuning with\nreduced GPU memory usage and time costs while enhancing overall performance, as\nexemplified by methodologies like low-rank adaptation (LoRA). Key questions\narise: which pre-training technique yields optimal results - adversarial,\ncontrastive, reconstruction, or diffusion denoising? How does the performance\nof these approaches vary as the complexity of fine-tuning is adjusted? This\nstudy aims to elucidate the advantages of pre-training techniques and\nfine-tuning strategies to enhance the learning process of neural networks in\nindependent identical distribution (IID) cohorts. We underscore the\nsignificance of fine-tuning by examining various cases, including full tuning,\ndecoder tuning, top-level tuning, and fine-tuning of linear parameters using\nLoRA. Systematic summaries of model performance and efficiency are presented,\nleveraging metrics such as accuracy, time cost, and memory efficiency. To\nempirically demonstrate our findings, we focus on a multi-task\nsegmentation-classification challenge involving the paracingulate sulcus (PCS)\nusing different 3D Convolutional Neural Network (CNN) architectures by using\nthe TOP-OSLO cohort comprising 596 subjects.\n", "link": "http://arxiv.org/abs/2405.19204v1", "date": "2024-05-29", "relevancy": 2.2915, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive-Adversarial%20and%20Diffusion%3A%20Exploring%20pre-training%20and%0A%20%20fine-tuning%20strategies%20for%20sulcal%20identification&body=Title%3A%20Contrastive-Adversarial%20and%20Diffusion%3A%20Exploring%20pre-training%20and%0A%20%20fine-tuning%20strategies%20for%20sulcal%20identification%0AAuthor%3A%20Michail%20Mamalakis%20and%20H%C3%A9lo%C3%AFse%20de%20Vareilles%20and%20Shun-Chin%20Jim%20Wu%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray%0AAbstract%3A%20%20%20In%20the%20last%20decade%2C%20computer%20vision%20has%20witnessed%20the%20establishment%20of%0Avarious%20training%20and%20learning%20approaches.%20Techniques%20like%20adversarial%20learning%2C%0Acontrastive%20learning%2C%20diffusion%20denoising%20learning%2C%20and%20ordinary%20reconstruction%0Alearning%20have%20become%20standard%2C%20representing%20state-of-the-art%20methods%0Aextensively%20employed%20for%20fully%20training%20or%20pre-training%20networks%20across%20various%0Avision%20tasks.%20The%20exploration%20of%20fine-tuning%20approaches%20has%20emerged%20as%20a%0Acurrent%20focal%20point%2C%20addressing%20the%20need%20for%20efficient%20model%20tuning%20with%0Areduced%20GPU%20memory%20usage%20and%20time%20costs%20while%20enhancing%20overall%20performance%2C%20as%0Aexemplified%20by%20methodologies%20like%20low-rank%20adaptation%20%28LoRA%29.%20Key%20questions%0Aarise%3A%20which%20pre-training%20technique%20yields%20optimal%20results%20-%20adversarial%2C%0Acontrastive%2C%20reconstruction%2C%20or%20diffusion%20denoising%3F%20How%20does%20the%20performance%0Aof%20these%20approaches%20vary%20as%20the%20complexity%20of%20fine-tuning%20is%20adjusted%3F%20This%0Astudy%20aims%20to%20elucidate%20the%20advantages%20of%20pre-training%20techniques%20and%0Afine-tuning%20strategies%20to%20enhance%20the%20learning%20process%20of%20neural%20networks%20in%0Aindependent%20identical%20distribution%20%28IID%29%20cohorts.%20We%20underscore%20the%0Asignificance%20of%20fine-tuning%20by%20examining%20various%20cases%2C%20including%20full%20tuning%2C%0Adecoder%20tuning%2C%20top-level%20tuning%2C%20and%20fine-tuning%20of%20linear%20parameters%20using%0ALoRA.%20Systematic%20summaries%20of%20model%20performance%20and%20efficiency%20are%20presented%2C%0Aleveraging%20metrics%20such%20as%20accuracy%2C%20time%20cost%2C%20and%20memory%20efficiency.%20To%0Aempirically%20demonstrate%20our%20findings%2C%20we%20focus%20on%20a%20multi-task%0Asegmentation-classification%20challenge%20involving%20the%20paracingulate%20sulcus%20%28PCS%29%0Ausing%20different%203D%20Convolutional%20Neural%20Network%20%28CNN%29%20architectures%20by%20using%0Athe%20TOP-OSLO%20cohort%20comprising%20596%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive-Adversarial%2520and%2520Diffusion%253A%2520Exploring%2520pre-training%2520and%250A%2520%2520fine-tuning%2520strategies%2520for%2520sulcal%2520identification%26entry.906535625%3DMichail%2520Mamalakis%2520and%2520H%25C3%25A9lo%25C3%25AFse%2520de%2520Vareilles%2520and%2520Shun-Chin%2520Jim%2520Wu%2520and%2520Ingrid%2520Agartz%2520and%2520Lynn%2520Egeland%2520M%25C3%25B8rch-Johnsen%2520and%2520Jane%2520Garrison%2520and%2520Jon%2520Simons%2520and%2520Pietro%2520Lio%2520and%2520John%2520Suckling%2520and%2520Graham%2520Murray%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520decade%252C%2520computer%2520vision%2520has%2520witnessed%2520the%2520establishment%2520of%250Avarious%2520training%2520and%2520learning%2520approaches.%2520Techniques%2520like%2520adversarial%2520learning%252C%250Acontrastive%2520learning%252C%2520diffusion%2520denoising%2520learning%252C%2520and%2520ordinary%2520reconstruction%250Alearning%2520have%2520become%2520standard%252C%2520representing%2520state-of-the-art%2520methods%250Aextensively%2520employed%2520for%2520fully%2520training%2520or%2520pre-training%2520networks%2520across%2520various%250Avision%2520tasks.%2520The%2520exploration%2520of%2520fine-tuning%2520approaches%2520has%2520emerged%2520as%2520a%250Acurrent%2520focal%2520point%252C%2520addressing%2520the%2520need%2520for%2520efficient%2520model%2520tuning%2520with%250Areduced%2520GPU%2520memory%2520usage%2520and%2520time%2520costs%2520while%2520enhancing%2520overall%2520performance%252C%2520as%250Aexemplified%2520by%2520methodologies%2520like%2520low-rank%2520adaptation%2520%2528LoRA%2529.%2520Key%2520questions%250Aarise%253A%2520which%2520pre-training%2520technique%2520yields%2520optimal%2520results%2520-%2520adversarial%252C%250Acontrastive%252C%2520reconstruction%252C%2520or%2520diffusion%2520denoising%253F%2520How%2520does%2520the%2520performance%250Aof%2520these%2520approaches%2520vary%2520as%2520the%2520complexity%2520of%2520fine-tuning%2520is%2520adjusted%253F%2520This%250Astudy%2520aims%2520to%2520elucidate%2520the%2520advantages%2520of%2520pre-training%2520techniques%2520and%250Afine-tuning%2520strategies%2520to%2520enhance%2520the%2520learning%2520process%2520of%2520neural%2520networks%2520in%250Aindependent%2520identical%2520distribution%2520%2528IID%2529%2520cohorts.%2520We%2520underscore%2520the%250Asignificance%2520of%2520fine-tuning%2520by%2520examining%2520various%2520cases%252C%2520including%2520full%2520tuning%252C%250Adecoder%2520tuning%252C%2520top-level%2520tuning%252C%2520and%2520fine-tuning%2520of%2520linear%2520parameters%2520using%250ALoRA.%2520Systematic%2520summaries%2520of%2520model%2520performance%2520and%2520efficiency%2520are%2520presented%252C%250Aleveraging%2520metrics%2520such%2520as%2520accuracy%252C%2520time%2520cost%252C%2520and%2520memory%2520efficiency.%2520To%250Aempirically%2520demonstrate%2520our%2520findings%252C%2520we%2520focus%2520on%2520a%2520multi-task%250Asegmentation-classification%2520challenge%2520involving%2520the%2520paracingulate%2520sulcus%2520%2528PCS%2529%250Ausing%2520different%25203D%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520architectures%2520by%2520using%250Athe%2520TOP-OSLO%2520cohort%2520comprising%2520596%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive-Adversarial%20and%20Diffusion%3A%20Exploring%20pre-training%20and%0A%20%20fine-tuning%20strategies%20for%20sulcal%20identification&entry.906535625=Michail%20Mamalakis%20and%20H%C3%A9lo%C3%AFse%20de%20Vareilles%20and%20Shun-Chin%20Jim%20Wu%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray&entry.1292438233=%20%20In%20the%20last%20decade%2C%20computer%20vision%20has%20witnessed%20the%20establishment%20of%0Avarious%20training%20and%20learning%20approaches.%20Techniques%20like%20adversarial%20learning%2C%0Acontrastive%20learning%2C%20diffusion%20denoising%20learning%2C%20and%20ordinary%20reconstruction%0Alearning%20have%20become%20standard%2C%20representing%20state-of-the-art%20methods%0Aextensively%20employed%20for%20fully%20training%20or%20pre-training%20networks%20across%20various%0Avision%20tasks.%20The%20exploration%20of%20fine-tuning%20approaches%20has%20emerged%20as%20a%0Acurrent%20focal%20point%2C%20addressing%20the%20need%20for%20efficient%20model%20tuning%20with%0Areduced%20GPU%20memory%20usage%20and%20time%20costs%20while%20enhancing%20overall%20performance%2C%20as%0Aexemplified%20by%20methodologies%20like%20low-rank%20adaptation%20%28LoRA%29.%20Key%20questions%0Aarise%3A%20which%20pre-training%20technique%20yields%20optimal%20results%20-%20adversarial%2C%0Acontrastive%2C%20reconstruction%2C%20or%20diffusion%20denoising%3F%20How%20does%20the%20performance%0Aof%20these%20approaches%20vary%20as%20the%20complexity%20of%20fine-tuning%20is%20adjusted%3F%20This%0Astudy%20aims%20to%20elucidate%20the%20advantages%20of%20pre-training%20techniques%20and%0Afine-tuning%20strategies%20to%20enhance%20the%20learning%20process%20of%20neural%20networks%20in%0Aindependent%20identical%20distribution%20%28IID%29%20cohorts.%20We%20underscore%20the%0Asignificance%20of%20fine-tuning%20by%20examining%20various%20cases%2C%20including%20full%20tuning%2C%0Adecoder%20tuning%2C%20top-level%20tuning%2C%20and%20fine-tuning%20of%20linear%20parameters%20using%0ALoRA.%20Systematic%20summaries%20of%20model%20performance%20and%20efficiency%20are%20presented%2C%0Aleveraging%20metrics%20such%20as%20accuracy%2C%20time%20cost%2C%20and%20memory%20efficiency.%20To%0Aempirically%20demonstrate%20our%20findings%2C%20we%20focus%20on%20a%20multi-task%0Asegmentation-classification%20challenge%20involving%20the%20paracingulate%20sulcus%20%28PCS%29%0Ausing%20different%203D%20Convolutional%20Neural%20Network%20%28CNN%29%20architectures%20by%20using%0Athe%20TOP-OSLO%20cohort%20comprising%20596%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19204v1&entry.124074799=Read"},
{"title": "Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot\n  Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language\n  Models", "author": "Tianrun Chen and Chunan Yu and Jing Li and Jianqi Zhang and Lanyun Zhu and Deyi Ji and Yong Zhang and Ying Zang and Zejian Li and Lingyun Sun", "abstract": "  In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation\nfor parts searching and localization for objects, which is a new paradigm to 3D\nsegmentation that transcends limitations for previous category-specific 3D\nsemantic segmentation, 3D instance segmentation, and open-vocabulary 3D\nsegmentation. We design a simple baseline method, Reasoning3D, with the\ncapability to understand and execute complex commands for (fine-grained)\nsegmenting specific parts for 3D meshes with contextual awareness and reasoned\nanswers for interactive segmentation. Specifically, Reasoning3D leverages an\noff-the-shelf pre-trained 2D segmentation network, powered by Large Language\nModels (LLMs), to interpret user input queries in a zero-shot manner. Previous\nresearch have shown that extensive pre-training endows foundation models with\nprior world knowledge, enabling them to comprehend complex commands, a\ncapability we can harness to \"segment anything\" in 3D with limited 3D datasets\n(source efficient). Experimentation reveals that our approach is generalizable\nand can effectively localize and highlight parts of 3D objects (in 3D mesh)\nbased on implicit textual queries, including these articulated 3d objects and\nreal-world scanned data. Our method can also generate natural language\nexplanations corresponding to these 3D models and the decomposition. Moreover,\nour training-free approach allows rapid deployment and serves as a viable\nuniversal baseline for future research of part-level 3d (semantic) object\nunderstanding in various fields including robotics, object manipulation, part\nassembly, autonomous driving applications, augment reality and virtual reality\n(AR/VR), and medical applications. The code, the model weight, the deployment\nguide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/\n", "link": "http://arxiv.org/abs/2405.19326v1", "date": "2024-05-29", "relevancy": 2.2911, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5757}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5747}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning3D%20--%20Grounding%20and%20Reasoning%20in%203D%3A%20Fine-Grained%20Zero-Shot%0A%20%20Open-Vocabulary%203D%20Reasoning%20Part%20Segmentation%20via%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Reasoning3D%20--%20Grounding%20and%20Reasoning%20in%203D%3A%20Fine-Grained%20Zero-Shot%0A%20%20Open-Vocabulary%203D%20Reasoning%20Part%20Segmentation%20via%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Tianrun%20Chen%20and%20Chunan%20Yu%20and%20Jing%20Li%20and%20Jianqi%20Zhang%20and%20Lanyun%20Zhu%20and%20Deyi%20Ji%20and%20Yong%20Zhang%20and%20Ying%20Zang%20and%20Zejian%20Li%20and%20Lingyun%20Sun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20Zero-Shot%203D%20Reasoning%20Segmentation%0Afor%20parts%20searching%20and%20localization%20for%20objects%2C%20which%20is%20a%20new%20paradigm%20to%203D%0Asegmentation%20that%20transcends%20limitations%20for%20previous%20category-specific%203D%0Asemantic%20segmentation%2C%203D%20instance%20segmentation%2C%20and%20open-vocabulary%203D%0Asegmentation.%20We%20design%20a%20simple%20baseline%20method%2C%20Reasoning3D%2C%20with%20the%0Acapability%20to%20understand%20and%20execute%20complex%20commands%20for%20%28fine-grained%29%0Asegmenting%20specific%20parts%20for%203D%20meshes%20with%20contextual%20awareness%20and%20reasoned%0Aanswers%20for%20interactive%20segmentation.%20Specifically%2C%20Reasoning3D%20leverages%20an%0Aoff-the-shelf%20pre-trained%202D%20segmentation%20network%2C%20powered%20by%20Large%20Language%0AModels%20%28LLMs%29%2C%20to%20interpret%20user%20input%20queries%20in%20a%20zero-shot%20manner.%20Previous%0Aresearch%20have%20shown%20that%20extensive%20pre-training%20endows%20foundation%20models%20with%0Aprior%20world%20knowledge%2C%20enabling%20them%20to%20comprehend%20complex%20commands%2C%20a%0Acapability%20we%20can%20harness%20to%20%22segment%20anything%22%20in%203D%20with%20limited%203D%20datasets%0A%28source%20efficient%29.%20Experimentation%20reveals%20that%20our%20approach%20is%20generalizable%0Aand%20can%20effectively%20localize%20and%20highlight%20parts%20of%203D%20objects%20%28in%203D%20mesh%29%0Abased%20on%20implicit%20textual%20queries%2C%20including%20these%20articulated%203d%20objects%20and%0Areal-world%20scanned%20data.%20Our%20method%20can%20also%20generate%20natural%20language%0Aexplanations%20corresponding%20to%20these%203D%20models%20and%20the%20decomposition.%20Moreover%2C%0Aour%20training-free%20approach%20allows%20rapid%20deployment%20and%20serves%20as%20a%20viable%0Auniversal%20baseline%20for%20future%20research%20of%20part-level%203d%20%28semantic%29%20object%0Aunderstanding%20in%20various%20fields%20including%20robotics%2C%20object%20manipulation%2C%20part%0Aassembly%2C%20autonomous%20driving%20applications%2C%20augment%20reality%20and%20virtual%20reality%0A%28AR/VR%29%2C%20and%20medical%20applications.%20The%20code%2C%20the%20model%20weight%2C%20the%20deployment%0Aguide%2C%20and%20the%20evaluation%20protocol%20are%3A%20http%3A//tianrun-chen.github.io/Reason3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning3D%2520--%2520Grounding%2520and%2520Reasoning%2520in%25203D%253A%2520Fine-Grained%2520Zero-Shot%250A%2520%2520Open-Vocabulary%25203D%2520Reasoning%2520Part%2520Segmentation%2520via%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DTianrun%2520Chen%2520and%2520Chunan%2520Yu%2520and%2520Jing%2520Li%2520and%2520Jianqi%2520Zhang%2520and%2520Lanyun%2520Zhu%2520and%2520Deyi%2520Ji%2520and%2520Yong%2520Zhang%2520and%2520Ying%2520Zang%2520and%2520Zejian%2520Li%2520and%2520Lingyun%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520task%253A%2520Zero-Shot%25203D%2520Reasoning%2520Segmentation%250Afor%2520parts%2520searching%2520and%2520localization%2520for%2520objects%252C%2520which%2520is%2520a%2520new%2520paradigm%2520to%25203D%250Asegmentation%2520that%2520transcends%2520limitations%2520for%2520previous%2520category-specific%25203D%250Asemantic%2520segmentation%252C%25203D%2520instance%2520segmentation%252C%2520and%2520open-vocabulary%25203D%250Asegmentation.%2520We%2520design%2520a%2520simple%2520baseline%2520method%252C%2520Reasoning3D%252C%2520with%2520the%250Acapability%2520to%2520understand%2520and%2520execute%2520complex%2520commands%2520for%2520%2528fine-grained%2529%250Asegmenting%2520specific%2520parts%2520for%25203D%2520meshes%2520with%2520contextual%2520awareness%2520and%2520reasoned%250Aanswers%2520for%2520interactive%2520segmentation.%2520Specifically%252C%2520Reasoning3D%2520leverages%2520an%250Aoff-the-shelf%2520pre-trained%25202D%2520segmentation%2520network%252C%2520powered%2520by%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520to%2520interpret%2520user%2520input%2520queries%2520in%2520a%2520zero-shot%2520manner.%2520Previous%250Aresearch%2520have%2520shown%2520that%2520extensive%2520pre-training%2520endows%2520foundation%2520models%2520with%250Aprior%2520world%2520knowledge%252C%2520enabling%2520them%2520to%2520comprehend%2520complex%2520commands%252C%2520a%250Acapability%2520we%2520can%2520harness%2520to%2520%2522segment%2520anything%2522%2520in%25203D%2520with%2520limited%25203D%2520datasets%250A%2528source%2520efficient%2529.%2520Experimentation%2520reveals%2520that%2520our%2520approach%2520is%2520generalizable%250Aand%2520can%2520effectively%2520localize%2520and%2520highlight%2520parts%2520of%25203D%2520objects%2520%2528in%25203D%2520mesh%2529%250Abased%2520on%2520implicit%2520textual%2520queries%252C%2520including%2520these%2520articulated%25203d%2520objects%2520and%250Areal-world%2520scanned%2520data.%2520Our%2520method%2520can%2520also%2520generate%2520natural%2520language%250Aexplanations%2520corresponding%2520to%2520these%25203D%2520models%2520and%2520the%2520decomposition.%2520Moreover%252C%250Aour%2520training-free%2520approach%2520allows%2520rapid%2520deployment%2520and%2520serves%2520as%2520a%2520viable%250Auniversal%2520baseline%2520for%2520future%2520research%2520of%2520part-level%25203d%2520%2528semantic%2529%2520object%250Aunderstanding%2520in%2520various%2520fields%2520including%2520robotics%252C%2520object%2520manipulation%252C%2520part%250Aassembly%252C%2520autonomous%2520driving%2520applications%252C%2520augment%2520reality%2520and%2520virtual%2520reality%250A%2528AR/VR%2529%252C%2520and%2520medical%2520applications.%2520The%2520code%252C%2520the%2520model%2520weight%252C%2520the%2520deployment%250Aguide%252C%2520and%2520the%2520evaluation%2520protocol%2520are%253A%2520http%253A//tianrun-chen.github.io/Reason3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning3D%20--%20Grounding%20and%20Reasoning%20in%203D%3A%20Fine-Grained%20Zero-Shot%0A%20%20Open-Vocabulary%203D%20Reasoning%20Part%20Segmentation%20via%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Tianrun%20Chen%20and%20Chunan%20Yu%20and%20Jing%20Li%20and%20Jianqi%20Zhang%20and%20Lanyun%20Zhu%20and%20Deyi%20Ji%20and%20Yong%20Zhang%20and%20Ying%20Zang%20and%20Zejian%20Li%20and%20Lingyun%20Sun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20Zero-Shot%203D%20Reasoning%20Segmentation%0Afor%20parts%20searching%20and%20localization%20for%20objects%2C%20which%20is%20a%20new%20paradigm%20to%203D%0Asegmentation%20that%20transcends%20limitations%20for%20previous%20category-specific%203D%0Asemantic%20segmentation%2C%203D%20instance%20segmentation%2C%20and%20open-vocabulary%203D%0Asegmentation.%20We%20design%20a%20simple%20baseline%20method%2C%20Reasoning3D%2C%20with%20the%0Acapability%20to%20understand%20and%20execute%20complex%20commands%20for%20%28fine-grained%29%0Asegmenting%20specific%20parts%20for%203D%20meshes%20with%20contextual%20awareness%20and%20reasoned%0Aanswers%20for%20interactive%20segmentation.%20Specifically%2C%20Reasoning3D%20leverages%20an%0Aoff-the-shelf%20pre-trained%202D%20segmentation%20network%2C%20powered%20by%20Large%20Language%0AModels%20%28LLMs%29%2C%20to%20interpret%20user%20input%20queries%20in%20a%20zero-shot%20manner.%20Previous%0Aresearch%20have%20shown%20that%20extensive%20pre-training%20endows%20foundation%20models%20with%0Aprior%20world%20knowledge%2C%20enabling%20them%20to%20comprehend%20complex%20commands%2C%20a%0Acapability%20we%20can%20harness%20to%20%22segment%20anything%22%20in%203D%20with%20limited%203D%20datasets%0A%28source%20efficient%29.%20Experimentation%20reveals%20that%20our%20approach%20is%20generalizable%0Aand%20can%20effectively%20localize%20and%20highlight%20parts%20of%203D%20objects%20%28in%203D%20mesh%29%0Abased%20on%20implicit%20textual%20queries%2C%20including%20these%20articulated%203d%20objects%20and%0Areal-world%20scanned%20data.%20Our%20method%20can%20also%20generate%20natural%20language%0Aexplanations%20corresponding%20to%20these%203D%20models%20and%20the%20decomposition.%20Moreover%2C%0Aour%20training-free%20approach%20allows%20rapid%20deployment%20and%20serves%20as%20a%20viable%0Auniversal%20baseline%20for%20future%20research%20of%20part-level%203d%20%28semantic%29%20object%0Aunderstanding%20in%20various%20fields%20including%20robotics%2C%20object%20manipulation%2C%20part%0Aassembly%2C%20autonomous%20driving%20applications%2C%20augment%20reality%20and%20virtual%20reality%0A%28AR/VR%29%2C%20and%20medical%20applications.%20The%20code%2C%20the%20model%20weight%2C%20the%20deployment%0Aguide%2C%20and%20the%20evaluation%20protocol%20are%3A%20http%3A//tianrun-chen.github.io/Reason3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19326v1&entry.124074799=Read"},
{"title": "3D Neural Edge Reconstruction", "author": "Lei Li and Songyou Peng and Zehao Yu and Shaohui Liu and R\u00e9mi Pautrat and Xiaochuan Yin and Marc Pollefeys", "abstract": "  Real-world objects and environments are predominantly composed of edge\nfeatures, including straight lines and curves. Such edges are crucial elements\nfor various applications, such as CAD modeling, surface meshing, lane mapping,\netc. However, existing traditional methods only prioritize lines over curves\nfor simplicity in geometric modeling. To this end, we introduce EMAP, a new\nmethod for learning 3D edge representations with a focus on both lines and\ncurves. Our method implicitly encodes 3D edge distance and direction in\nUnsigned Distance Functions (UDF) from multi-view edge maps. On top of this\nneural representation, we propose an edge extraction algorithm that robustly\nabstracts parametric 3D edges from the inferred edge points and their\ndirections. Comprehensive evaluations demonstrate that our method achieves\nbetter 3D edge reconstruction on multiple challenging datasets. We further show\nthat our learned UDF field enhances neural surface reconstruction by capturing\nmore details.\n", "link": "http://arxiv.org/abs/2405.19295v1", "date": "2024-05-29", "relevancy": 2.2635, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5792}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5715}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Neural%20Edge%20Reconstruction&body=Title%3A%203D%20Neural%20Edge%20Reconstruction%0AAuthor%3A%20Lei%20Li%20and%20Songyou%20Peng%20and%20Zehao%20Yu%20and%20Shaohui%20Liu%20and%20R%C3%A9mi%20Pautrat%20and%20Xiaochuan%20Yin%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Real-world%20objects%20and%20environments%20are%20predominantly%20composed%20of%20edge%0Afeatures%2C%20including%20straight%20lines%20and%20curves.%20Such%20edges%20are%20crucial%20elements%0Afor%20various%20applications%2C%20such%20as%20CAD%20modeling%2C%20surface%20meshing%2C%20lane%20mapping%2C%0Aetc.%20However%2C%20existing%20traditional%20methods%20only%20prioritize%20lines%20over%20curves%0Afor%20simplicity%20in%20geometric%20modeling.%20To%20this%20end%2C%20we%20introduce%20EMAP%2C%20a%20new%0Amethod%20for%20learning%203D%20edge%20representations%20with%20a%20focus%20on%20both%20lines%20and%0Acurves.%20Our%20method%20implicitly%20encodes%203D%20edge%20distance%20and%20direction%20in%0AUnsigned%20Distance%20Functions%20%28UDF%29%20from%20multi-view%20edge%20maps.%20On%20top%20of%20this%0Aneural%20representation%2C%20we%20propose%20an%20edge%20extraction%20algorithm%20that%20robustly%0Aabstracts%20parametric%203D%20edges%20from%20the%20inferred%20edge%20points%20and%20their%0Adirections.%20Comprehensive%20evaluations%20demonstrate%20that%20our%20method%20achieves%0Abetter%203D%20edge%20reconstruction%20on%20multiple%20challenging%20datasets.%20We%20further%20show%0Athat%20our%20learned%20UDF%20field%20enhances%20neural%20surface%20reconstruction%20by%20capturing%0Amore%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Neural%2520Edge%2520Reconstruction%26entry.906535625%3DLei%2520Li%2520and%2520Songyou%2520Peng%2520and%2520Zehao%2520Yu%2520and%2520Shaohui%2520Liu%2520and%2520R%25C3%25A9mi%2520Pautrat%2520and%2520Xiaochuan%2520Yin%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Real-world%2520objects%2520and%2520environments%2520are%2520predominantly%2520composed%2520of%2520edge%250Afeatures%252C%2520including%2520straight%2520lines%2520and%2520curves.%2520Such%2520edges%2520are%2520crucial%2520elements%250Afor%2520various%2520applications%252C%2520such%2520as%2520CAD%2520modeling%252C%2520surface%2520meshing%252C%2520lane%2520mapping%252C%250Aetc.%2520However%252C%2520existing%2520traditional%2520methods%2520only%2520prioritize%2520lines%2520over%2520curves%250Afor%2520simplicity%2520in%2520geometric%2520modeling.%2520To%2520this%2520end%252C%2520we%2520introduce%2520EMAP%252C%2520a%2520new%250Amethod%2520for%2520learning%25203D%2520edge%2520representations%2520with%2520a%2520focus%2520on%2520both%2520lines%2520and%250Acurves.%2520Our%2520method%2520implicitly%2520encodes%25203D%2520edge%2520distance%2520and%2520direction%2520in%250AUnsigned%2520Distance%2520Functions%2520%2528UDF%2529%2520from%2520multi-view%2520edge%2520maps.%2520On%2520top%2520of%2520this%250Aneural%2520representation%252C%2520we%2520propose%2520an%2520edge%2520extraction%2520algorithm%2520that%2520robustly%250Aabstracts%2520parametric%25203D%2520edges%2520from%2520the%2520inferred%2520edge%2520points%2520and%2520their%250Adirections.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520our%2520method%2520achieves%250Abetter%25203D%2520edge%2520reconstruction%2520on%2520multiple%2520challenging%2520datasets.%2520We%2520further%2520show%250Athat%2520our%2520learned%2520UDF%2520field%2520enhances%2520neural%2520surface%2520reconstruction%2520by%2520capturing%250Amore%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Neural%20Edge%20Reconstruction&entry.906535625=Lei%20Li%20and%20Songyou%20Peng%20and%20Zehao%20Yu%20and%20Shaohui%20Liu%20and%20R%C3%A9mi%20Pautrat%20and%20Xiaochuan%20Yin%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Real-world%20objects%20and%20environments%20are%20predominantly%20composed%20of%20edge%0Afeatures%2C%20including%20straight%20lines%20and%20curves.%20Such%20edges%20are%20crucial%20elements%0Afor%20various%20applications%2C%20such%20as%20CAD%20modeling%2C%20surface%20meshing%2C%20lane%20mapping%2C%0Aetc.%20However%2C%20existing%20traditional%20methods%20only%20prioritize%20lines%20over%20curves%0Afor%20simplicity%20in%20geometric%20modeling.%20To%20this%20end%2C%20we%20introduce%20EMAP%2C%20a%20new%0Amethod%20for%20learning%203D%20edge%20representations%20with%20a%20focus%20on%20both%20lines%20and%0Acurves.%20Our%20method%20implicitly%20encodes%203D%20edge%20distance%20and%20direction%20in%0AUnsigned%20Distance%20Functions%20%28UDF%29%20from%20multi-view%20edge%20maps.%20On%20top%20of%20this%0Aneural%20representation%2C%20we%20propose%20an%20edge%20extraction%20algorithm%20that%20robustly%0Aabstracts%20parametric%203D%20edges%20from%20the%20inferred%20edge%20points%20and%20their%0Adirections.%20Comprehensive%20evaluations%20demonstrate%20that%20our%20method%20achieves%0Abetter%203D%20edge%20reconstruction%20on%20multiple%20challenging%20datasets.%20We%20further%20show%0Athat%20our%20learned%20UDF%20field%20enhances%20neural%20surface%20reconstruction%20by%20capturing%0Amore%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19295v1&entry.124074799=Read"},
{"title": "RGB-T Object Detection via Group Shuffled Multi-receptive Attention and\n  Multi-modal Supervision", "author": "Jinzhong Wang and Xuetao Tian and Shun Dai and Tao Zhuo and Haorui Zeng and Hongjuan Liu and Jiaqi Liu and Xiuwei Zhang and Yanning Zhang", "abstract": "  Multispectral object detection, utilizing both visible (RGB) and thermal\ninfrared (T) modals, has garnered significant attention for its robust\nperformance across diverse weather and lighting conditions. However,\neffectively exploiting the complementarity between RGB-T modals while\nmaintaining efficiency remains a critical challenge. In this paper, a very\nsimple Group Shuffled Multi-receptive Attention (GSMA) module is proposed to\nextract and combine multi-scale RGB and thermal features. Then, the extracted\nmulti-modal features are directly integrated with a multi-level path\naggregation neck, which significantly improves the fusion effect and\nefficiency. Meanwhile, multi-modal object detection often adopts union\nannotations for both modals. This kind of supervision is not sufficient and\nunfair, since objects observed in one modal may not be seen in the other modal.\nTo solve this issue, Multi-modal Supervision (MS) is proposed to sufficiently\nsupervise RGB-T object detection. Comprehensive experiments on two challenging\nbenchmarks, KAIST and DroneVehicle, demonstrate the proposed model achieves the\nstate-of-the-art accuracy while maintaining competitive efficiency.\n", "link": "http://arxiv.org/abs/2405.18955v1", "date": "2024-05-29", "relevancy": 2.2527, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5601}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-T%20Object%20Detection%20via%20Group%20Shuffled%20Multi-receptive%20Attention%20and%0A%20%20Multi-modal%20Supervision&body=Title%3A%20RGB-T%20Object%20Detection%20via%20Group%20Shuffled%20Multi-receptive%20Attention%20and%0A%20%20Multi-modal%20Supervision%0AAuthor%3A%20Jinzhong%20Wang%20and%20Xuetao%20Tian%20and%20Shun%20Dai%20and%20Tao%20Zhuo%20and%20Haorui%20Zeng%20and%20Hongjuan%20Liu%20and%20Jiaqi%20Liu%20and%20Xiuwei%20Zhang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Multispectral%20object%20detection%2C%20utilizing%20both%20visible%20%28RGB%29%20and%20thermal%0Ainfrared%20%28T%29%20modals%2C%20has%20garnered%20significant%20attention%20for%20its%20robust%0Aperformance%20across%20diverse%20weather%20and%20lighting%20conditions.%20However%2C%0Aeffectively%20exploiting%20the%20complementarity%20between%20RGB-T%20modals%20while%0Amaintaining%20efficiency%20remains%20a%20critical%20challenge.%20In%20this%20paper%2C%20a%20very%0Asimple%20Group%20Shuffled%20Multi-receptive%20Attention%20%28GSMA%29%20module%20is%20proposed%20to%0Aextract%20and%20combine%20multi-scale%20RGB%20and%20thermal%20features.%20Then%2C%20the%20extracted%0Amulti-modal%20features%20are%20directly%20integrated%20with%20a%20multi-level%20path%0Aaggregation%20neck%2C%20which%20significantly%20improves%20the%20fusion%20effect%20and%0Aefficiency.%20Meanwhile%2C%20multi-modal%20object%20detection%20often%20adopts%20union%0Aannotations%20for%20both%20modals.%20This%20kind%20of%20supervision%20is%20not%20sufficient%20and%0Aunfair%2C%20since%20objects%20observed%20in%20one%20modal%20may%20not%20be%20seen%20in%20the%20other%20modal.%0ATo%20solve%20this%20issue%2C%20Multi-modal%20Supervision%20%28MS%29%20is%20proposed%20to%20sufficiently%0Asupervise%20RGB-T%20object%20detection.%20Comprehensive%20experiments%20on%20two%20challenging%0Abenchmarks%2C%20KAIST%20and%20DroneVehicle%2C%20demonstrate%20the%20proposed%20model%20achieves%20the%0Astate-of-the-art%20accuracy%20while%20maintaining%20competitive%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-T%2520Object%2520Detection%2520via%2520Group%2520Shuffled%2520Multi-receptive%2520Attention%2520and%250A%2520%2520Multi-modal%2520Supervision%26entry.906535625%3DJinzhong%2520Wang%2520and%2520Xuetao%2520Tian%2520and%2520Shun%2520Dai%2520and%2520Tao%2520Zhuo%2520and%2520Haorui%2520Zeng%2520and%2520Hongjuan%2520Liu%2520and%2520Jiaqi%2520Liu%2520and%2520Xiuwei%2520Zhang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520Multispectral%2520object%2520detection%252C%2520utilizing%2520both%2520visible%2520%2528RGB%2529%2520and%2520thermal%250Ainfrared%2520%2528T%2529%2520modals%252C%2520has%2520garnered%2520significant%2520attention%2520for%2520its%2520robust%250Aperformance%2520across%2520diverse%2520weather%2520and%2520lighting%2520conditions.%2520However%252C%250Aeffectively%2520exploiting%2520the%2520complementarity%2520between%2520RGB-T%2520modals%2520while%250Amaintaining%2520efficiency%2520remains%2520a%2520critical%2520challenge.%2520In%2520this%2520paper%252C%2520a%2520very%250Asimple%2520Group%2520Shuffled%2520Multi-receptive%2520Attention%2520%2528GSMA%2529%2520module%2520is%2520proposed%2520to%250Aextract%2520and%2520combine%2520multi-scale%2520RGB%2520and%2520thermal%2520features.%2520Then%252C%2520the%2520extracted%250Amulti-modal%2520features%2520are%2520directly%2520integrated%2520with%2520a%2520multi-level%2520path%250Aaggregation%2520neck%252C%2520which%2520significantly%2520improves%2520the%2520fusion%2520effect%2520and%250Aefficiency.%2520Meanwhile%252C%2520multi-modal%2520object%2520detection%2520often%2520adopts%2520union%250Aannotations%2520for%2520both%2520modals.%2520This%2520kind%2520of%2520supervision%2520is%2520not%2520sufficient%2520and%250Aunfair%252C%2520since%2520objects%2520observed%2520in%2520one%2520modal%2520may%2520not%2520be%2520seen%2520in%2520the%2520other%2520modal.%250ATo%2520solve%2520this%2520issue%252C%2520Multi-modal%2520Supervision%2520%2528MS%2529%2520is%2520proposed%2520to%2520sufficiently%250Asupervise%2520RGB-T%2520object%2520detection.%2520Comprehensive%2520experiments%2520on%2520two%2520challenging%250Abenchmarks%252C%2520KAIST%2520and%2520DroneVehicle%252C%2520demonstrate%2520the%2520proposed%2520model%2520achieves%2520the%250Astate-of-the-art%2520accuracy%2520while%2520maintaining%2520competitive%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-T%20Object%20Detection%20via%20Group%20Shuffled%20Multi-receptive%20Attention%20and%0A%20%20Multi-modal%20Supervision&entry.906535625=Jinzhong%20Wang%20and%20Xuetao%20Tian%20and%20Shun%20Dai%20and%20Tao%20Zhuo%20and%20Haorui%20Zeng%20and%20Hongjuan%20Liu%20and%20Jiaqi%20Liu%20and%20Xiuwei%20Zhang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Multispectral%20object%20detection%2C%20utilizing%20both%20visible%20%28RGB%29%20and%20thermal%0Ainfrared%20%28T%29%20modals%2C%20has%20garnered%20significant%20attention%20for%20its%20robust%0Aperformance%20across%20diverse%20weather%20and%20lighting%20conditions.%20However%2C%0Aeffectively%20exploiting%20the%20complementarity%20between%20RGB-T%20modals%20while%0Amaintaining%20efficiency%20remains%20a%20critical%20challenge.%20In%20this%20paper%2C%20a%20very%0Asimple%20Group%20Shuffled%20Multi-receptive%20Attention%20%28GSMA%29%20module%20is%20proposed%20to%0Aextract%20and%20combine%20multi-scale%20RGB%20and%20thermal%20features.%20Then%2C%20the%20extracted%0Amulti-modal%20features%20are%20directly%20integrated%20with%20a%20multi-level%20path%0Aaggregation%20neck%2C%20which%20significantly%20improves%20the%20fusion%20effect%20and%0Aefficiency.%20Meanwhile%2C%20multi-modal%20object%20detection%20often%20adopts%20union%0Aannotations%20for%20both%20modals.%20This%20kind%20of%20supervision%20is%20not%20sufficient%20and%0Aunfair%2C%20since%20objects%20observed%20in%20one%20modal%20may%20not%20be%20seen%20in%20the%20other%20modal.%0ATo%20solve%20this%20issue%2C%20Multi-modal%20Supervision%20%28MS%29%20is%20proposed%20to%20sufficiently%0Asupervise%20RGB-T%20object%20detection.%20Comprehensive%20experiments%20on%20two%20challenging%0Abenchmarks%2C%20KAIST%20and%20DroneVehicle%2C%20demonstrate%20the%20proposed%20model%20achieves%20the%0Astate-of-the-art%20accuracy%20while%20maintaining%20competitive%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18955v1&entry.124074799=Read"},
{"title": "Physics-Based Rigid Body Object Tracking and Friction Filtering From\n  RGB-D Videos", "author": "Rama Krishna Kandukuri and Michael Strecke and Joerg Stueckler", "abstract": "  Physics-based understanding of object interactions from sensory observations\nis an essential capability in augmented reality and robotics. It enables to\ncapture the properties of a scene for simulation and control. In this paper, we\npropose a novel approach for real-to-sim which tracks rigid objects in 3D from\nRGB-D images and infers physical properties of the objects. We use a\ndifferentiable physics simulation as state-transition model in an Extended\nKalman Filter which can model contact and friction for arbitrary mesh-based\nshapes and in this way estimate physically plausible trajectories. We\ndemonstrate that our approach can filter position, orientation, velocities, and\nconcurrently can estimate the coefficient of friction of the objects. We\nanalyze our approach on various sliding scenarios in synthetic image sequences\nof single objects and colliding objects. We also demonstrate and evaluate our\napproach on a real-world dataset. We make our novel benchmark datasets publicly\navailable to foster future research in this novel problem setting and\ncomparison with our method.\n", "link": "http://arxiv.org/abs/2309.15703v2", "date": "2024-05-29", "relevancy": 2.2524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6491}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5706}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos&body=Title%3A%20Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos%0AAuthor%3A%20Rama%20Krishna%20Kandukuri%20and%20Michael%20Strecke%20and%20Joerg%20Stueckler%0AAbstract%3A%20%20%20Physics-based%20understanding%20of%20object%20interactions%20from%20sensory%20observations%0Ais%20an%20essential%20capability%20in%20augmented%20reality%20and%20robotics.%20It%20enables%20to%0Acapture%20the%20properties%20of%20a%20scene%20for%20simulation%20and%20control.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20for%20real-to-sim%20which%20tracks%20rigid%20objects%20in%203D%20from%0ARGB-D%20images%20and%20infers%20physical%20properties%20of%20the%20objects.%20We%20use%20a%0Adifferentiable%20physics%20simulation%20as%20state-transition%20model%20in%20an%20Extended%0AKalman%20Filter%20which%20can%20model%20contact%20and%20friction%20for%20arbitrary%20mesh-based%0Ashapes%20and%20in%20this%20way%20estimate%20physically%20plausible%20trajectories.%20We%0Ademonstrate%20that%20our%20approach%20can%20filter%20position%2C%20orientation%2C%20velocities%2C%20and%0Aconcurrently%20can%20estimate%20the%20coefficient%20of%20friction%20of%20the%20objects.%20We%0Aanalyze%20our%20approach%20on%20various%20sliding%20scenarios%20in%20synthetic%20image%20sequences%0Aof%20single%20objects%20and%20colliding%20objects.%20We%20also%20demonstrate%20and%20evaluate%20our%0Aapproach%20on%20a%20real-world%20dataset.%20We%20make%20our%20novel%20benchmark%20datasets%20publicly%0Aavailable%20to%20foster%20future%20research%20in%20this%20novel%20problem%20setting%20and%0Acomparison%20with%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Based%2520Rigid%2520Body%2520Object%2520Tracking%2520and%2520Friction%2520Filtering%2520From%250A%2520%2520RGB-D%2520Videos%26entry.906535625%3DRama%2520Krishna%2520Kandukuri%2520and%2520Michael%2520Strecke%2520and%2520Joerg%2520Stueckler%26entry.1292438233%3D%2520%2520Physics-based%2520understanding%2520of%2520object%2520interactions%2520from%2520sensory%2520observations%250Ais%2520an%2520essential%2520capability%2520in%2520augmented%2520reality%2520and%2520robotics.%2520It%2520enables%2520to%250Acapture%2520the%2520properties%2520of%2520a%2520scene%2520for%2520simulation%2520and%2520control.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520for%2520real-to-sim%2520which%2520tracks%2520rigid%2520objects%2520in%25203D%2520from%250ARGB-D%2520images%2520and%2520infers%2520physical%2520properties%2520of%2520the%2520objects.%2520We%2520use%2520a%250Adifferentiable%2520physics%2520simulation%2520as%2520state-transition%2520model%2520in%2520an%2520Extended%250AKalman%2520Filter%2520which%2520can%2520model%2520contact%2520and%2520friction%2520for%2520arbitrary%2520mesh-based%250Ashapes%2520and%2520in%2520this%2520way%2520estimate%2520physically%2520plausible%2520trajectories.%2520We%250Ademonstrate%2520that%2520our%2520approach%2520can%2520filter%2520position%252C%2520orientation%252C%2520velocities%252C%2520and%250Aconcurrently%2520can%2520estimate%2520the%2520coefficient%2520of%2520friction%2520of%2520the%2520objects.%2520We%250Aanalyze%2520our%2520approach%2520on%2520various%2520sliding%2520scenarios%2520in%2520synthetic%2520image%2520sequences%250Aof%2520single%2520objects%2520and%2520colliding%2520objects.%2520We%2520also%2520demonstrate%2520and%2520evaluate%2520our%250Aapproach%2520on%2520a%2520real-world%2520dataset.%2520We%2520make%2520our%2520novel%2520benchmark%2520datasets%2520publicly%250Aavailable%2520to%2520foster%2520future%2520research%2520in%2520this%2520novel%2520problem%2520setting%2520and%250Acomparison%2520with%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos&entry.906535625=Rama%20Krishna%20Kandukuri%20and%20Michael%20Strecke%20and%20Joerg%20Stueckler&entry.1292438233=%20%20Physics-based%20understanding%20of%20object%20interactions%20from%20sensory%20observations%0Ais%20an%20essential%20capability%20in%20augmented%20reality%20and%20robotics.%20It%20enables%20to%0Acapture%20the%20properties%20of%20a%20scene%20for%20simulation%20and%20control.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20for%20real-to-sim%20which%20tracks%20rigid%20objects%20in%203D%20from%0ARGB-D%20images%20and%20infers%20physical%20properties%20of%20the%20objects.%20We%20use%20a%0Adifferentiable%20physics%20simulation%20as%20state-transition%20model%20in%20an%20Extended%0AKalman%20Filter%20which%20can%20model%20contact%20and%20friction%20for%20arbitrary%20mesh-based%0Ashapes%20and%20in%20this%20way%20estimate%20physically%20plausible%20trajectories.%20We%0Ademonstrate%20that%20our%20approach%20can%20filter%20position%2C%20orientation%2C%20velocities%2C%20and%0Aconcurrently%20can%20estimate%20the%20coefficient%20of%20friction%20of%20the%20objects.%20We%0Aanalyze%20our%20approach%20on%20various%20sliding%20scenarios%20in%20synthetic%20image%20sequences%0Aof%20single%20objects%20and%20colliding%20objects.%20We%20also%20demonstrate%20and%20evaluate%20our%0Aapproach%20on%20a%20real-world%20dataset.%20We%20make%20our%20novel%20benchmark%20datasets%20publicly%0Aavailable%20to%20foster%20future%20research%20in%20this%20novel%20problem%20setting%20and%0Acomparison%20with%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15703v2&entry.124074799=Read"},
{"title": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer", "author": "Zengqun Zhao and Yu Cao and Shaogang Gong and Ioannis Patras", "abstract": "  Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ntraining. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at\n\\url{https://github.com/zengqunzhao/Exp-CLIP}.\n", "link": "http://arxiv.org/abs/2405.19100v1", "date": "2024-05-29", "relevancy": 2.2452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.592}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5397}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer&body=Title%3A%20Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer%0AAuthor%3A%20Zengqun%20Zhao%20and%20Yu%20Cao%20and%20Shaogang%20Gong%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20Current%20facial%20expression%20recognition%20%28FER%29%20models%20are%20often%20designed%20in%20a%0Asupervised%20learning%20manner%20thus%20are%20constrained%20by%20the%20lack%20of%20large-scale%0Afacial%20expression%20images%20with%20high-quality%20annotations.%20Consequently%2C%20these%0Amodels%20often%20fail%20to%20generalize%20well%2C%20performing%20poorly%20on%20unseen%20images%20in%0Atraining.%20Vision-language-based%20zero-shot%20models%20demonstrate%20a%20promising%0Apotential%20for%20addressing%20such%20challenges.%20However%2C%20these%20models%20lack%0Atask-specific%20knowledge%20therefore%20are%20not%20optimized%20for%20the%20nuances%20of%0Arecognizing%20facial%20expressions.%20To%20bridge%20this%20gap%2C%20this%20work%20proposes%20a%20novel%0Amethod%2C%20Exp-CLIP%2C%20to%20enhance%20zero-shot%20FER%20by%20transferring%20the%20task%20knowledge%0Afrom%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20based%20on%20the%20pre-trained%0Avision-language%20encoders%2C%20we%20incorporate%20a%20projection%20head%20designed%20to%20map%20the%0Ainitial%20joint%20vision-language%20space%20into%20a%20space%20that%20captures%20representations%0Aof%20facial%20actions.%20To%20train%20this%20projection%20head%20for%20subsequent%20zero-shot%0Apredictions%2C%20we%20propose%20to%20align%20the%20projected%20visual%20representations%20with%0Atask-specific%20semantic%20meanings%20derived%20from%20the%20LLM%20encoder%2C%20and%20the%20text%0Ainstruction-based%20strategy%20is%20employed%20to%20customize%20the%20LLM%20knowledge.%20Given%0Aunlabelled%20facial%20data%20and%20efficient%20training%20of%20the%20projection%20head%2C%20Exp-CLIP%0Aachieves%20superior%20zero-shot%20results%20to%20the%20CLIP%20models%20and%20several%20other%20large%0Avision-language%20models%20%28LVLMs%29%20on%20seven%20in-the-wild%20FER%20datasets.%20The%20code%20and%0Apre-trained%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zengqunzhao/Exp-CLIP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Zero-Shot%2520Facial%2520Expression%2520Recognition%2520by%2520LLM%2520Knowledge%250A%2520%2520Transfer%26entry.906535625%3DZengqun%2520Zhao%2520and%2520Yu%2520Cao%2520and%2520Shaogang%2520Gong%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520Current%2520facial%2520expression%2520recognition%2520%2528FER%2529%2520models%2520are%2520often%2520designed%2520in%2520a%250Asupervised%2520learning%2520manner%2520thus%2520are%2520constrained%2520by%2520the%2520lack%2520of%2520large-scale%250Afacial%2520expression%2520images%2520with%2520high-quality%2520annotations.%2520Consequently%252C%2520these%250Amodels%2520often%2520fail%2520to%2520generalize%2520well%252C%2520performing%2520poorly%2520on%2520unseen%2520images%2520in%250Atraining.%2520Vision-language-based%2520zero-shot%2520models%2520demonstrate%2520a%2520promising%250Apotential%2520for%2520addressing%2520such%2520challenges.%2520However%252C%2520these%2520models%2520lack%250Atask-specific%2520knowledge%2520therefore%2520are%2520not%2520optimized%2520for%2520the%2520nuances%2520of%250Arecognizing%2520facial%2520expressions.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520work%2520proposes%2520a%2520novel%250Amethod%252C%2520Exp-CLIP%252C%2520to%2520enhance%2520zero-shot%2520FER%2520by%2520transferring%2520the%2520task%2520knowledge%250Afrom%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520based%2520on%2520the%2520pre-trained%250Avision-language%2520encoders%252C%2520we%2520incorporate%2520a%2520projection%2520head%2520designed%2520to%2520map%2520the%250Ainitial%2520joint%2520vision-language%2520space%2520into%2520a%2520space%2520that%2520captures%2520representations%250Aof%2520facial%2520actions.%2520To%2520train%2520this%2520projection%2520head%2520for%2520subsequent%2520zero-shot%250Apredictions%252C%2520we%2520propose%2520to%2520align%2520the%2520projected%2520visual%2520representations%2520with%250Atask-specific%2520semantic%2520meanings%2520derived%2520from%2520the%2520LLM%2520encoder%252C%2520and%2520the%2520text%250Ainstruction-based%2520strategy%2520is%2520employed%2520to%2520customize%2520the%2520LLM%2520knowledge.%2520Given%250Aunlabelled%2520facial%2520data%2520and%2520efficient%2520training%2520of%2520the%2520projection%2520head%252C%2520Exp-CLIP%250Aachieves%2520superior%2520zero-shot%2520results%2520to%2520the%2520CLIP%2520models%2520and%2520several%2520other%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%2520on%2520seven%2520in-the-wild%2520FER%2520datasets.%2520The%2520code%2520and%250Apre-trained%2520models%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/zengqunzhao/Exp-CLIP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer&entry.906535625=Zengqun%20Zhao%20and%20Yu%20Cao%20and%20Shaogang%20Gong%20and%20Ioannis%20Patras&entry.1292438233=%20%20Current%20facial%20expression%20recognition%20%28FER%29%20models%20are%20often%20designed%20in%20a%0Asupervised%20learning%20manner%20thus%20are%20constrained%20by%20the%20lack%20of%20large-scale%0Afacial%20expression%20images%20with%20high-quality%20annotations.%20Consequently%2C%20these%0Amodels%20often%20fail%20to%20generalize%20well%2C%20performing%20poorly%20on%20unseen%20images%20in%0Atraining.%20Vision-language-based%20zero-shot%20models%20demonstrate%20a%20promising%0Apotential%20for%20addressing%20such%20challenges.%20However%2C%20these%20models%20lack%0Atask-specific%20knowledge%20therefore%20are%20not%20optimized%20for%20the%20nuances%20of%0Arecognizing%20facial%20expressions.%20To%20bridge%20this%20gap%2C%20this%20work%20proposes%20a%20novel%0Amethod%2C%20Exp-CLIP%2C%20to%20enhance%20zero-shot%20FER%20by%20transferring%20the%20task%20knowledge%0Afrom%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20based%20on%20the%20pre-trained%0Avision-language%20encoders%2C%20we%20incorporate%20a%20projection%20head%20designed%20to%20map%20the%0Ainitial%20joint%20vision-language%20space%20into%20a%20space%20that%20captures%20representations%0Aof%20facial%20actions.%20To%20train%20this%20projection%20head%20for%20subsequent%20zero-shot%0Apredictions%2C%20we%20propose%20to%20align%20the%20projected%20visual%20representations%20with%0Atask-specific%20semantic%20meanings%20derived%20from%20the%20LLM%20encoder%2C%20and%20the%20text%0Ainstruction-based%20strategy%20is%20employed%20to%20customize%20the%20LLM%20knowledge.%20Given%0Aunlabelled%20facial%20data%20and%20efficient%20training%20of%20the%20projection%20head%2C%20Exp-CLIP%0Aachieves%20superior%20zero-shot%20results%20to%20the%20CLIP%20models%20and%20several%20other%20large%0Avision-language%20models%20%28LVLMs%29%20on%20seven%20in-the-wild%20FER%20datasets.%20The%20code%20and%0Apre-trained%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zengqunzhao/Exp-CLIP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19100v1&entry.124074799=Read"},
{"title": "UniIF: Unified Molecule Inverse Folding", "author": "Zhangyang Gao and Jue Wang and Cheng Tan and Lirong Wu and Yufei Huang and Siyuan Li and Zhirui Ye and Stan Z. Li", "abstract": "  Molecule inverse folding has been a long-standing challenge in chemistry and\nbiology, with the potential to revolutionize drug discovery and material\nscience. Despite specified models have been proposed for different small- or\nmacro-molecules, few have attempted to unify the learning process, resulting in\nredundant efforts. Complementary to recent advancements in molecular structure\nprediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified\nmodel UniIF for the inverse folding of all molecules. We do such unification in\ntwo levels: 1) Data-Level: We propose a unified block graph data form for all\nmolecules, including the local frame building and geometric feature\ninitialization. 2) Model-Level: We introduce a geometric block attention\nnetwork, comprising a geometric interaction, interactive attention and virtual\nlong-term dependency modules, to capture the 3D interactions of all molecules.\nThrough comprehensive evaluations across various tasks such as protein design,\nRNA design, and material design, we demonstrate that our proposed method\nsurpasses state-of-the-art methods on all tasks. UniIF offers a versatile and\neffective solution for general molecule inverse folding.\n", "link": "http://arxiv.org/abs/2405.18968v1", "date": "2024-05-29", "relevancy": 2.2448, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4434}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniIF%3A%20Unified%20Molecule%20Inverse%20Folding&body=Title%3A%20UniIF%3A%20Unified%20Molecule%20Inverse%20Folding%0AAuthor%3A%20Zhangyang%20Gao%20and%20Jue%20Wang%20and%20Cheng%20Tan%20and%20Lirong%20Wu%20and%20Yufei%20Huang%20and%20Siyuan%20Li%20and%20Zhirui%20Ye%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20Molecule%20inverse%20folding%20has%20been%20a%20long-standing%20challenge%20in%20chemistry%20and%0Abiology%2C%20with%20the%20potential%20to%20revolutionize%20drug%20discovery%20and%20material%0Ascience.%20Despite%20specified%20models%20have%20been%20proposed%20for%20different%20small-%20or%0Amacro-molecules%2C%20few%20have%20attempted%20to%20unify%20the%20learning%20process%2C%20resulting%20in%0Aredundant%20efforts.%20Complementary%20to%20recent%20advancements%20in%20molecular%20structure%0Aprediction%2C%20such%20as%20RoseTTAFold%20All-Atom%20and%20AlphaFold3%2C%20we%20propose%20the%20unified%0Amodel%20UniIF%20for%20the%20inverse%20folding%20of%20all%20molecules.%20We%20do%20such%20unification%20in%0Atwo%20levels%3A%201%29%20Data-Level%3A%20We%20propose%20a%20unified%20block%20graph%20data%20form%20for%20all%0Amolecules%2C%20including%20the%20local%20frame%20building%20and%20geometric%20feature%0Ainitialization.%202%29%20Model-Level%3A%20We%20introduce%20a%20geometric%20block%20attention%0Anetwork%2C%20comprising%20a%20geometric%20interaction%2C%20interactive%20attention%20and%20virtual%0Along-term%20dependency%20modules%2C%20to%20capture%20the%203D%20interactions%20of%20all%20molecules.%0AThrough%20comprehensive%20evaluations%20across%20various%20tasks%20such%20as%20protein%20design%2C%0ARNA%20design%2C%20and%20material%20design%2C%20we%20demonstrate%20that%20our%20proposed%20method%0Asurpasses%20state-of-the-art%20methods%20on%20all%20tasks.%20UniIF%20offers%20a%20versatile%20and%0Aeffective%20solution%20for%20general%20molecule%20inverse%20folding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniIF%253A%2520Unified%2520Molecule%2520Inverse%2520Folding%26entry.906535625%3DZhangyang%2520Gao%2520and%2520Jue%2520Wang%2520and%2520Cheng%2520Tan%2520and%2520Lirong%2520Wu%2520and%2520Yufei%2520Huang%2520and%2520Siyuan%2520Li%2520and%2520Zhirui%2520Ye%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520Molecule%2520inverse%2520folding%2520has%2520been%2520a%2520long-standing%2520challenge%2520in%2520chemistry%2520and%250Abiology%252C%2520with%2520the%2520potential%2520to%2520revolutionize%2520drug%2520discovery%2520and%2520material%250Ascience.%2520Despite%2520specified%2520models%2520have%2520been%2520proposed%2520for%2520different%2520small-%2520or%250Amacro-molecules%252C%2520few%2520have%2520attempted%2520to%2520unify%2520the%2520learning%2520process%252C%2520resulting%2520in%250Aredundant%2520efforts.%2520Complementary%2520to%2520recent%2520advancements%2520in%2520molecular%2520structure%250Aprediction%252C%2520such%2520as%2520RoseTTAFold%2520All-Atom%2520and%2520AlphaFold3%252C%2520we%2520propose%2520the%2520unified%250Amodel%2520UniIF%2520for%2520the%2520inverse%2520folding%2520of%2520all%2520molecules.%2520We%2520do%2520such%2520unification%2520in%250Atwo%2520levels%253A%25201%2529%2520Data-Level%253A%2520We%2520propose%2520a%2520unified%2520block%2520graph%2520data%2520form%2520for%2520all%250Amolecules%252C%2520including%2520the%2520local%2520frame%2520building%2520and%2520geometric%2520feature%250Ainitialization.%25202%2529%2520Model-Level%253A%2520We%2520introduce%2520a%2520geometric%2520block%2520attention%250Anetwork%252C%2520comprising%2520a%2520geometric%2520interaction%252C%2520interactive%2520attention%2520and%2520virtual%250Along-term%2520dependency%2520modules%252C%2520to%2520capture%2520the%25203D%2520interactions%2520of%2520all%2520molecules.%250AThrough%2520comprehensive%2520evaluations%2520across%2520various%2520tasks%2520such%2520as%2520protein%2520design%252C%250ARNA%2520design%252C%2520and%2520material%2520design%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%2520method%250Asurpasses%2520state-of-the-art%2520methods%2520on%2520all%2520tasks.%2520UniIF%2520offers%2520a%2520versatile%2520and%250Aeffective%2520solution%2520for%2520general%2520molecule%2520inverse%2520folding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniIF%3A%20Unified%20Molecule%20Inverse%20Folding&entry.906535625=Zhangyang%20Gao%20and%20Jue%20Wang%20and%20Cheng%20Tan%20and%20Lirong%20Wu%20and%20Yufei%20Huang%20and%20Siyuan%20Li%20and%20Zhirui%20Ye%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20Molecule%20inverse%20folding%20has%20been%20a%20long-standing%20challenge%20in%20chemistry%20and%0Abiology%2C%20with%20the%20potential%20to%20revolutionize%20drug%20discovery%20and%20material%0Ascience.%20Despite%20specified%20models%20have%20been%20proposed%20for%20different%20small-%20or%0Amacro-molecules%2C%20few%20have%20attempted%20to%20unify%20the%20learning%20process%2C%20resulting%20in%0Aredundant%20efforts.%20Complementary%20to%20recent%20advancements%20in%20molecular%20structure%0Aprediction%2C%20such%20as%20RoseTTAFold%20All-Atom%20and%20AlphaFold3%2C%20we%20propose%20the%20unified%0Amodel%20UniIF%20for%20the%20inverse%20folding%20of%20all%20molecules.%20We%20do%20such%20unification%20in%0Atwo%20levels%3A%201%29%20Data-Level%3A%20We%20propose%20a%20unified%20block%20graph%20data%20form%20for%20all%0Amolecules%2C%20including%20the%20local%20frame%20building%20and%20geometric%20feature%0Ainitialization.%202%29%20Model-Level%3A%20We%20introduce%20a%20geometric%20block%20attention%0Anetwork%2C%20comprising%20a%20geometric%20interaction%2C%20interactive%20attention%20and%20virtual%0Along-term%20dependency%20modules%2C%20to%20capture%20the%203D%20interactions%20of%20all%20molecules.%0AThrough%20comprehensive%20evaluations%20across%20various%20tasks%20such%20as%20protein%20design%2C%0ARNA%20design%2C%20and%20material%20design%2C%20we%20demonstrate%20that%20our%20proposed%20method%0Asurpasses%20state-of-the-art%20methods%20on%20all%20tasks.%20UniIF%20offers%20a%20versatile%20and%0Aeffective%20solution%20for%20general%20molecule%20inverse%20folding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18968v1&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Model with Unmasked Token Alignment", "author": "Jihao Liu and Jinliang Zheng and Boxiao Liu and Yu Liu and Hongsheng Li", "abstract": "  Contrastive pre-training on image-text pairs, exemplified by CLIP, becomes a\nstandard technique for learning multi-modal visual-language representations.\nAlthough CLIP has demonstrated remarkable performance, training it from scratch\non noisy web-scale datasets is computationally demanding. On the other hand,\nmask-then-predict pre-training approaches, like Masked Image Modeling (MIM),\noffer efficient self-supervised learning for single-modal representations. This\npaper introduces Unmasked Token Alignment (UTA), a method that leverages\nexisting CLIP models to further enhance its vision-language representations.\nUTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the\ncorresponding image tokens from a frozen CLIP vision encoder, which\nautomatically aligns the ViT model with the CLIP text encoder. The pre-trained\nViT can be directly applied for zero-shot evaluation even without training on\nimage-text pairs. Compared to MIM approaches, UTA does not suffer from\ntraining-finetuning inconsistency and is much more training-efficient by\navoiding using the extra [MASK] tokens. Extensive experimental results\ndemonstrate that UTA can enhance CLIP models and outperform existing MIM\nmethods on various uni- and multi-modal benchmarks. Code and models are\navailable at https://github.com/jihaonew/UTA.\n", "link": "http://arxiv.org/abs/2405.19009v1", "date": "2024-05-29", "relevancy": 2.239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment&body=Title%3A%20Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment%0AAuthor%3A%20Jihao%20Liu%20and%20Jinliang%20Zheng%20and%20Boxiao%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Contrastive%20pre-training%20on%20image-text%20pairs%2C%20exemplified%20by%20CLIP%2C%20becomes%20a%0Astandard%20technique%20for%20learning%20multi-modal%20visual-language%20representations.%0AAlthough%20CLIP%20has%20demonstrated%20remarkable%20performance%2C%20training%20it%20from%20scratch%0Aon%20noisy%20web-scale%20datasets%20is%20computationally%20demanding.%20On%20the%20other%20hand%2C%0Amask-then-predict%20pre-training%20approaches%2C%20like%20Masked%20Image%20Modeling%20%28MIM%29%2C%0Aoffer%20efficient%20self-supervised%20learning%20for%20single-modal%20representations.%20This%0Apaper%20introduces%20Unmasked%20Token%20Alignment%20%28UTA%29%2C%20a%20method%20that%20leverages%0Aexisting%20CLIP%20models%20to%20further%20enhance%20its%20vision-language%20representations.%0AUTA%20trains%20a%20Vision%20Transformer%20%28ViT%29%20by%20aligning%20unmasked%20visual%20tokens%20to%20the%0Acorresponding%20image%20tokens%20from%20a%20frozen%20CLIP%20vision%20encoder%2C%20which%0Aautomatically%20aligns%20the%20ViT%20model%20with%20the%20CLIP%20text%20encoder.%20The%20pre-trained%0AViT%20can%20be%20directly%20applied%20for%20zero-shot%20evaluation%20even%20without%20training%20on%0Aimage-text%20pairs.%20Compared%20to%20MIM%20approaches%2C%20UTA%20does%20not%20suffer%20from%0Atraining-finetuning%20inconsistency%20and%20is%20much%20more%20training-efficient%20by%0Aavoiding%20using%20the%20extra%20%5BMASK%5D%20tokens.%20Extensive%20experimental%20results%0Ademonstrate%20that%20UTA%20can%20enhance%20CLIP%20models%20and%20outperform%20existing%20MIM%0Amethods%20on%20various%20uni-%20and%20multi-modal%20benchmarks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/jihaonew/UTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Model%2520with%2520Unmasked%2520Token%2520Alignment%26entry.906535625%3DJihao%2520Liu%2520and%2520Jinliang%2520Zheng%2520and%2520Boxiao%2520Liu%2520and%2520Yu%2520Liu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Contrastive%2520pre-training%2520on%2520image-text%2520pairs%252C%2520exemplified%2520by%2520CLIP%252C%2520becomes%2520a%250Astandard%2520technique%2520for%2520learning%2520multi-modal%2520visual-language%2520representations.%250AAlthough%2520CLIP%2520has%2520demonstrated%2520remarkable%2520performance%252C%2520training%2520it%2520from%2520scratch%250Aon%2520noisy%2520web-scale%2520datasets%2520is%2520computationally%2520demanding.%2520On%2520the%2520other%2520hand%252C%250Amask-then-predict%2520pre-training%2520approaches%252C%2520like%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%252C%250Aoffer%2520efficient%2520self-supervised%2520learning%2520for%2520single-modal%2520representations.%2520This%250Apaper%2520introduces%2520Unmasked%2520Token%2520Alignment%2520%2528UTA%2529%252C%2520a%2520method%2520that%2520leverages%250Aexisting%2520CLIP%2520models%2520to%2520further%2520enhance%2520its%2520vision-language%2520representations.%250AUTA%2520trains%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%2520by%2520aligning%2520unmasked%2520visual%2520tokens%2520to%2520the%250Acorresponding%2520image%2520tokens%2520from%2520a%2520frozen%2520CLIP%2520vision%2520encoder%252C%2520which%250Aautomatically%2520aligns%2520the%2520ViT%2520model%2520with%2520the%2520CLIP%2520text%2520encoder.%2520The%2520pre-trained%250AViT%2520can%2520be%2520directly%2520applied%2520for%2520zero-shot%2520evaluation%2520even%2520without%2520training%2520on%250Aimage-text%2520pairs.%2520Compared%2520to%2520MIM%2520approaches%252C%2520UTA%2520does%2520not%2520suffer%2520from%250Atraining-finetuning%2520inconsistency%2520and%2520is%2520much%2520more%2520training-efficient%2520by%250Aavoiding%2520using%2520the%2520extra%2520%255BMASK%255D%2520tokens.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520UTA%2520can%2520enhance%2520CLIP%2520models%2520and%2520outperform%2520existing%2520MIM%250Amethods%2520on%2520various%2520uni-%2520and%2520multi-modal%2520benchmarks.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/jihaonew/UTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment&entry.906535625=Jihao%20Liu%20and%20Jinliang%20Zheng%20and%20Boxiao%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20Contrastive%20pre-training%20on%20image-text%20pairs%2C%20exemplified%20by%20CLIP%2C%20becomes%20a%0Astandard%20technique%20for%20learning%20multi-modal%20visual-language%20representations.%0AAlthough%20CLIP%20has%20demonstrated%20remarkable%20performance%2C%20training%20it%20from%20scratch%0Aon%20noisy%20web-scale%20datasets%20is%20computationally%20demanding.%20On%20the%20other%20hand%2C%0Amask-then-predict%20pre-training%20approaches%2C%20like%20Masked%20Image%20Modeling%20%28MIM%29%2C%0Aoffer%20efficient%20self-supervised%20learning%20for%20single-modal%20representations.%20This%0Apaper%20introduces%20Unmasked%20Token%20Alignment%20%28UTA%29%2C%20a%20method%20that%20leverages%0Aexisting%20CLIP%20models%20to%20further%20enhance%20its%20vision-language%20representations.%0AUTA%20trains%20a%20Vision%20Transformer%20%28ViT%29%20by%20aligning%20unmasked%20visual%20tokens%20to%20the%0Acorresponding%20image%20tokens%20from%20a%20frozen%20CLIP%20vision%20encoder%2C%20which%0Aautomatically%20aligns%20the%20ViT%20model%20with%20the%20CLIP%20text%20encoder.%20The%20pre-trained%0AViT%20can%20be%20directly%20applied%20for%20zero-shot%20evaluation%20even%20without%20training%20on%0Aimage-text%20pairs.%20Compared%20to%20MIM%20approaches%2C%20UTA%20does%20not%20suffer%20from%0Atraining-finetuning%20inconsistency%20and%20is%20much%20more%20training-efficient%20by%0Aavoiding%20using%20the%20extra%20%5BMASK%5D%20tokens.%20Extensive%20experimental%20results%0Ademonstrate%20that%20UTA%20can%20enhance%20CLIP%20models%20and%20outperform%20existing%20MIM%0Amethods%20on%20various%20uni-%20and%20multi-modal%20benchmarks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/jihaonew/UTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19009v1&entry.124074799=Read"},
{"title": "M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion\n  Comprehension and Generation", "author": "Mingshuang Luo and Ruibing Hou and Hong Chang and Zimo Liu and Yaowei Wang and Shiguang Shan", "abstract": "  This paper presents M$^3$GPT, an advanced $\\textbf{M}$ultimodal,\n$\\textbf{M}$ultitask framework for $\\textbf{M}$otion comprehension and\ngeneration. M$^3$GPT operates on three fundamental principles. The first\nfocuses on creating a unified representation space for various motion-relevant\nmodalities. We employ discrete vector quantization for multimodal control and\ngeneration signals, such as text, music and motion/dance, enabling seamless\nintegration into a large language model (LLM) with a single vocabulary. The\nsecond involves modeling model generation directly in the raw motion space.\nThis strategy circumvents the information loss associated with discrete\ntokenizer, resulting in more detailed and comprehensive model generation.\nThird, M$^3$GPT learns to model the connections and synergies among various\nmotion-relevant tasks. Text, the most familiar and well-understood modality for\nLLMs, is utilized as a bridge to establish connections between different motion\ntasks, facilitating mutual reinforcement. To our knowledge, M$^3$GPT is the\nfirst model capable of comprehending and generating motions based on multiple\nsignals. Extensive experiments highlight M$^3$GPT's superior performance across\nvarious motion-relevant tasks and its powerful zero-shot generalization\ncapabilities for extremely challenging tasks.\n", "link": "http://arxiv.org/abs/2405.16273v3", "date": "2024-05-29", "relevancy": 2.2353, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E3%24GPT%3A%20An%20Advanced%20Multimodal%2C%20Multitask%20Framework%20for%20Motion%0A%20%20Comprehension%20and%20Generation&body=Title%3A%20M%24%5E3%24GPT%3A%20An%20Advanced%20Multimodal%2C%20Multitask%20Framework%20for%20Motion%0A%20%20Comprehension%20and%20Generation%0AAuthor%3A%20Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Yaowei%20Wang%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20This%20paper%20presents%20M%24%5E3%24GPT%2C%20an%20advanced%20%24%5Ctextbf%7BM%7D%24ultimodal%2C%0A%24%5Ctextbf%7BM%7D%24ultitask%20framework%20for%20%24%5Ctextbf%7BM%7D%24otion%20comprehension%20and%0Ageneration.%20M%24%5E3%24GPT%20operates%20on%20three%20fundamental%20principles.%20The%20first%0Afocuses%20on%20creating%20a%20unified%20representation%20space%20for%20various%20motion-relevant%0Amodalities.%20We%20employ%20discrete%20vector%20quantization%20for%20multimodal%20control%20and%0Ageneration%20signals%2C%20such%20as%20text%2C%20music%20and%20motion/dance%2C%20enabling%20seamless%0Aintegration%20into%20a%20large%20language%20model%20%28LLM%29%20with%20a%20single%20vocabulary.%20The%0Asecond%20involves%20modeling%20model%20generation%20directly%20in%20the%20raw%20motion%20space.%0AThis%20strategy%20circumvents%20the%20information%20loss%20associated%20with%20discrete%0Atokenizer%2C%20resulting%20in%20more%20detailed%20and%20comprehensive%20model%20generation.%0AThird%2C%20M%24%5E3%24GPT%20learns%20to%20model%20the%20connections%20and%20synergies%20among%20various%0Amotion-relevant%20tasks.%20Text%2C%20the%20most%20familiar%20and%20well-understood%20modality%20for%0ALLMs%2C%20is%20utilized%20as%20a%20bridge%20to%20establish%20connections%20between%20different%20motion%0Atasks%2C%20facilitating%20mutual%20reinforcement.%20To%20our%20knowledge%2C%20M%24%5E3%24GPT%20is%20the%0Afirst%20model%20capable%20of%20comprehending%20and%20generating%20motions%20based%20on%20multiple%0Asignals.%20Extensive%20experiments%20highlight%20M%24%5E3%24GPT%27s%20superior%20performance%20across%0Avarious%20motion-relevant%20tasks%20and%20its%20powerful%20zero-shot%20generalization%0Acapabilities%20for%20extremely%20challenging%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16273v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E3%2524GPT%253A%2520An%2520Advanced%2520Multimodal%252C%2520Multitask%2520Framework%2520for%2520Motion%250A%2520%2520Comprehension%2520and%2520Generation%26entry.906535625%3DMingshuang%2520Luo%2520and%2520Ruibing%2520Hou%2520and%2520Hong%2520Chang%2520and%2520Zimo%2520Liu%2520and%2520Yaowei%2520Wang%2520and%2520Shiguang%2520Shan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520M%2524%255E3%2524GPT%252C%2520an%2520advanced%2520%2524%255Ctextbf%257BM%257D%2524ultimodal%252C%250A%2524%255Ctextbf%257BM%257D%2524ultitask%2520framework%2520for%2520%2524%255Ctextbf%257BM%257D%2524otion%2520comprehension%2520and%250Ageneration.%2520M%2524%255E3%2524GPT%2520operates%2520on%2520three%2520fundamental%2520principles.%2520The%2520first%250Afocuses%2520on%2520creating%2520a%2520unified%2520representation%2520space%2520for%2520various%2520motion-relevant%250Amodalities.%2520We%2520employ%2520discrete%2520vector%2520quantization%2520for%2520multimodal%2520control%2520and%250Ageneration%2520signals%252C%2520such%2520as%2520text%252C%2520music%2520and%2520motion/dance%252C%2520enabling%2520seamless%250Aintegration%2520into%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520with%2520a%2520single%2520vocabulary.%2520The%250Asecond%2520involves%2520modeling%2520model%2520generation%2520directly%2520in%2520the%2520raw%2520motion%2520space.%250AThis%2520strategy%2520circumvents%2520the%2520information%2520loss%2520associated%2520with%2520discrete%250Atokenizer%252C%2520resulting%2520in%2520more%2520detailed%2520and%2520comprehensive%2520model%2520generation.%250AThird%252C%2520M%2524%255E3%2524GPT%2520learns%2520to%2520model%2520the%2520connections%2520and%2520synergies%2520among%2520various%250Amotion-relevant%2520tasks.%2520Text%252C%2520the%2520most%2520familiar%2520and%2520well-understood%2520modality%2520for%250ALLMs%252C%2520is%2520utilized%2520as%2520a%2520bridge%2520to%2520establish%2520connections%2520between%2520different%2520motion%250Atasks%252C%2520facilitating%2520mutual%2520reinforcement.%2520To%2520our%2520knowledge%252C%2520M%2524%255E3%2524GPT%2520is%2520the%250Afirst%2520model%2520capable%2520of%2520comprehending%2520and%2520generating%2520motions%2520based%2520on%2520multiple%250Asignals.%2520Extensive%2520experiments%2520highlight%2520M%2524%255E3%2524GPT%2527s%2520superior%2520performance%2520across%250Avarious%2520motion-relevant%2520tasks%2520and%2520its%2520powerful%2520zero-shot%2520generalization%250Acapabilities%2520for%2520extremely%2520challenging%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16273v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E3%24GPT%3A%20An%20Advanced%20Multimodal%2C%20Multitask%20Framework%20for%20Motion%0A%20%20Comprehension%20and%20Generation&entry.906535625=Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Yaowei%20Wang%20and%20Shiguang%20Shan&entry.1292438233=%20%20This%20paper%20presents%20M%24%5E3%24GPT%2C%20an%20advanced%20%24%5Ctextbf%7BM%7D%24ultimodal%2C%0A%24%5Ctextbf%7BM%7D%24ultitask%20framework%20for%20%24%5Ctextbf%7BM%7D%24otion%20comprehension%20and%0Ageneration.%20M%24%5E3%24GPT%20operates%20on%20three%20fundamental%20principles.%20The%20first%0Afocuses%20on%20creating%20a%20unified%20representation%20space%20for%20various%20motion-relevant%0Amodalities.%20We%20employ%20discrete%20vector%20quantization%20for%20multimodal%20control%20and%0Ageneration%20signals%2C%20such%20as%20text%2C%20music%20and%20motion/dance%2C%20enabling%20seamless%0Aintegration%20into%20a%20large%20language%20model%20%28LLM%29%20with%20a%20single%20vocabulary.%20The%0Asecond%20involves%20modeling%20model%20generation%20directly%20in%20the%20raw%20motion%20space.%0AThis%20strategy%20circumvents%20the%20information%20loss%20associated%20with%20discrete%0Atokenizer%2C%20resulting%20in%20more%20detailed%20and%20comprehensive%20model%20generation.%0AThird%2C%20M%24%5E3%24GPT%20learns%20to%20model%20the%20connections%20and%20synergies%20among%20various%0Amotion-relevant%20tasks.%20Text%2C%20the%20most%20familiar%20and%20well-understood%20modality%20for%0ALLMs%2C%20is%20utilized%20as%20a%20bridge%20to%20establish%20connections%20between%20different%20motion%0Atasks%2C%20facilitating%20mutual%20reinforcement.%20To%20our%20knowledge%2C%20M%24%5E3%24GPT%20is%20the%0Afirst%20model%20capable%20of%20comprehending%20and%20generating%20motions%20based%20on%20multiple%0Asignals.%20Extensive%20experiments%20highlight%20M%24%5E3%24GPT%27s%20superior%20performance%20across%0Avarious%20motion-relevant%20tasks%20and%20its%20powerful%20zero-shot%20generalization%0Acapabilities%20for%20extremely%20challenging%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16273v3&entry.124074799=Read"},
{"title": "On the Influence of Smoothness Constraints in Computed Tomography Motion\n  Compensation", "author": "Mareike Thies and Fabian Wagner and Noah Maul and Siyuan Mei and Mingxuan Gu and Laura Pfaff and Nastassia Vysotskaya and Haijun Yu and Andreas Maier", "abstract": "  Computed tomography (CT) relies on precise patient immobilization during\nimage acquisition. Nevertheless, motion artifacts in the reconstructed images\ncan persist. Motion compensation methods aim to correct such artifacts\npost-acquisition, often incorporating temporal smoothness constraints on the\nestimated motion patterns. This study analyzes the influence of a spline-based\nmotion model within an existing rigid motion compensation algorithm for\ncone-beam CT on the recoverable motion frequencies. Results demonstrate that\nthe choice of motion model crucially influences recoverable frequencies. The\noptimization-based motion compensation algorithm is able to accurately fit the\nspline nodes for frequencies almost up to the node-dependent theoretical limit\naccording to the Nyquist-Shannon theorem. Notably, a higher node count does not\ncompromise reconstruction performance for slow motion patterns, but can extend\nthe range of recoverable high frequencies for the investigated algorithm.\nEventually, the optimal motion model is dependent on the imaged anatomy,\nclinical use case, and scanning protocol and should be tailored carefully to\nthe expected motion frequency spectrum to ensure accurate motion compensation.\n", "link": "http://arxiv.org/abs/2405.19079v1", "date": "2024-05-29", "relevancy": 2.2236, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4638}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Influence%20of%20Smoothness%20Constraints%20in%20Computed%20Tomography%20Motion%0A%20%20Compensation&body=Title%3A%20On%20the%20Influence%20of%20Smoothness%20Constraints%20in%20Computed%20Tomography%20Motion%0A%20%20Compensation%0AAuthor%3A%20Mareike%20Thies%20and%20Fabian%20Wagner%20and%20Noah%20Maul%20and%20Siyuan%20Mei%20and%20Mingxuan%20Gu%20and%20Laura%20Pfaff%20and%20Nastassia%20Vysotskaya%20and%20Haijun%20Yu%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20relies%20on%20precise%20patient%20immobilization%20during%0Aimage%20acquisition.%20Nevertheless%2C%20motion%20artifacts%20in%20the%20reconstructed%20images%0Acan%20persist.%20Motion%20compensation%20methods%20aim%20to%20correct%20such%20artifacts%0Apost-acquisition%2C%20often%20incorporating%20temporal%20smoothness%20constraints%20on%20the%0Aestimated%20motion%20patterns.%20This%20study%20analyzes%20the%20influence%20of%20a%20spline-based%0Amotion%20model%20within%20an%20existing%20rigid%20motion%20compensation%20algorithm%20for%0Acone-beam%20CT%20on%20the%20recoverable%20motion%20frequencies.%20Results%20demonstrate%20that%0Athe%20choice%20of%20motion%20model%20crucially%20influences%20recoverable%20frequencies.%20The%0Aoptimization-based%20motion%20compensation%20algorithm%20is%20able%20to%20accurately%20fit%20the%0Aspline%20nodes%20for%20frequencies%20almost%20up%20to%20the%20node-dependent%20theoretical%20limit%0Aaccording%20to%20the%20Nyquist-Shannon%20theorem.%20Notably%2C%20a%20higher%20node%20count%20does%20not%0Acompromise%20reconstruction%20performance%20for%20slow%20motion%20patterns%2C%20but%20can%20extend%0Athe%20range%20of%20recoverable%20high%20frequencies%20for%20the%20investigated%20algorithm.%0AEventually%2C%20the%20optimal%20motion%20model%20is%20dependent%20on%20the%20imaged%20anatomy%2C%0Aclinical%20use%20case%2C%20and%20scanning%20protocol%20and%20should%20be%20tailored%20carefully%20to%0Athe%20expected%20motion%20frequency%20spectrum%20to%20ensure%20accurate%20motion%20compensation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Influence%2520of%2520Smoothness%2520Constraints%2520in%2520Computed%2520Tomography%2520Motion%250A%2520%2520Compensation%26entry.906535625%3DMareike%2520Thies%2520and%2520Fabian%2520Wagner%2520and%2520Noah%2520Maul%2520and%2520Siyuan%2520Mei%2520and%2520Mingxuan%2520Gu%2520and%2520Laura%2520Pfaff%2520and%2520Nastassia%2520Vysotskaya%2520and%2520Haijun%2520Yu%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520relies%2520on%2520precise%2520patient%2520immobilization%2520during%250Aimage%2520acquisition.%2520Nevertheless%252C%2520motion%2520artifacts%2520in%2520the%2520reconstructed%2520images%250Acan%2520persist.%2520Motion%2520compensation%2520methods%2520aim%2520to%2520correct%2520such%2520artifacts%250Apost-acquisition%252C%2520often%2520incorporating%2520temporal%2520smoothness%2520constraints%2520on%2520the%250Aestimated%2520motion%2520patterns.%2520This%2520study%2520analyzes%2520the%2520influence%2520of%2520a%2520spline-based%250Amotion%2520model%2520within%2520an%2520existing%2520rigid%2520motion%2520compensation%2520algorithm%2520for%250Acone-beam%2520CT%2520on%2520the%2520recoverable%2520motion%2520frequencies.%2520Results%2520demonstrate%2520that%250Athe%2520choice%2520of%2520motion%2520model%2520crucially%2520influences%2520recoverable%2520frequencies.%2520The%250Aoptimization-based%2520motion%2520compensation%2520algorithm%2520is%2520able%2520to%2520accurately%2520fit%2520the%250Aspline%2520nodes%2520for%2520frequencies%2520almost%2520up%2520to%2520the%2520node-dependent%2520theoretical%2520limit%250Aaccording%2520to%2520the%2520Nyquist-Shannon%2520theorem.%2520Notably%252C%2520a%2520higher%2520node%2520count%2520does%2520not%250Acompromise%2520reconstruction%2520performance%2520for%2520slow%2520motion%2520patterns%252C%2520but%2520can%2520extend%250Athe%2520range%2520of%2520recoverable%2520high%2520frequencies%2520for%2520the%2520investigated%2520algorithm.%250AEventually%252C%2520the%2520optimal%2520motion%2520model%2520is%2520dependent%2520on%2520the%2520imaged%2520anatomy%252C%250Aclinical%2520use%2520case%252C%2520and%2520scanning%2520protocol%2520and%2520should%2520be%2520tailored%2520carefully%2520to%250Athe%2520expected%2520motion%2520frequency%2520spectrum%2520to%2520ensure%2520accurate%2520motion%2520compensation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Influence%20of%20Smoothness%20Constraints%20in%20Computed%20Tomography%20Motion%0A%20%20Compensation&entry.906535625=Mareike%20Thies%20and%20Fabian%20Wagner%20and%20Noah%20Maul%20and%20Siyuan%20Mei%20and%20Mingxuan%20Gu%20and%20Laura%20Pfaff%20and%20Nastassia%20Vysotskaya%20and%20Haijun%20Yu%20and%20Andreas%20Maier&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20relies%20on%20precise%20patient%20immobilization%20during%0Aimage%20acquisition.%20Nevertheless%2C%20motion%20artifacts%20in%20the%20reconstructed%20images%0Acan%20persist.%20Motion%20compensation%20methods%20aim%20to%20correct%20such%20artifacts%0Apost-acquisition%2C%20often%20incorporating%20temporal%20smoothness%20constraints%20on%20the%0Aestimated%20motion%20patterns.%20This%20study%20analyzes%20the%20influence%20of%20a%20spline-based%0Amotion%20model%20within%20an%20existing%20rigid%20motion%20compensation%20algorithm%20for%0Acone-beam%20CT%20on%20the%20recoverable%20motion%20frequencies.%20Results%20demonstrate%20that%0Athe%20choice%20of%20motion%20model%20crucially%20influences%20recoverable%20frequencies.%20The%0Aoptimization-based%20motion%20compensation%20algorithm%20is%20able%20to%20accurately%20fit%20the%0Aspline%20nodes%20for%20frequencies%20almost%20up%20to%20the%20node-dependent%20theoretical%20limit%0Aaccording%20to%20the%20Nyquist-Shannon%20theorem.%20Notably%2C%20a%20higher%20node%20count%20does%20not%0Acompromise%20reconstruction%20performance%20for%20slow%20motion%20patterns%2C%20but%20can%20extend%0Athe%20range%20of%20recoverable%20high%20frequencies%20for%20the%20investigated%20algorithm.%0AEventually%2C%20the%20optimal%20motion%20model%20is%20dependent%20on%20the%20imaged%20anatomy%2C%0Aclinical%20use%20case%2C%20and%20scanning%20protocol%20and%20should%20be%20tailored%20carefully%20to%0Athe%20expected%20motion%20frequency%20spectrum%20to%20ensure%20accurate%20motion%20compensation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19079v1&entry.124074799=Read"},
{"title": "Comparative Study of Neighbor-based Methods for Local Outlier Detection", "author": "Zhuang Qi and Junlin Zhang and Xiaming Chen and Xin Qi", "abstract": "  The neighbor-based method has become a powerful tool to handle the outlier\ndetection problem, which aims to infer the abnormal degree of the sample based\non the compactness of the sample and its neighbors. However, the existing\nmethods commonly focus on designing different processes to locate outliers in\nthe dataset, while the contributions of different types neighbors to outlier\ndetection has not been well discussed. To this end, this paper studies the\nneighbor in the existing outlier detection algorithms and a taxonomy is\nintroduced, which uses the three-level components of information, neighbor and\nmethodology to define hybrid methods. This taxonomy can serve as a paradigm\nwhere a novel neighbor-based outlier detection method can be proposed by\ncombining different components in this taxonomy. A large number of comparative\nexperiments were conducted on synthetic and real-world datasets in terms of\nperformance comparison and case study, and the results show that reverse\nK-nearest neighbor based methods achieve promising performance and dynamic\nselection method is suitable for working in high-dimensional space. Notably, it\nis verified that rationally selecting components from this taxonomy may create\nan algorithms superior to existing methods.\n", "link": "http://arxiv.org/abs/2405.19247v1", "date": "2024-05-29", "relevancy": 2.2233, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4306}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Study%20of%20Neighbor-based%20Methods%20for%20Local%20Outlier%20Detection&body=Title%3A%20Comparative%20Study%20of%20Neighbor-based%20Methods%20for%20Local%20Outlier%20Detection%0AAuthor%3A%20Zhuang%20Qi%20and%20Junlin%20Zhang%20and%20Xiaming%20Chen%20and%20Xin%20Qi%0AAbstract%3A%20%20%20The%20neighbor-based%20method%20has%20become%20a%20powerful%20tool%20to%20handle%20the%20outlier%0Adetection%20problem%2C%20which%20aims%20to%20infer%20the%20abnormal%20degree%20of%20the%20sample%20based%0Aon%20the%20compactness%20of%20the%20sample%20and%20its%20neighbors.%20However%2C%20the%20existing%0Amethods%20commonly%20focus%20on%20designing%20different%20processes%20to%20locate%20outliers%20in%0Athe%20dataset%2C%20while%20the%20contributions%20of%20different%20types%20neighbors%20to%20outlier%0Adetection%20has%20not%20been%20well%20discussed.%20To%20this%20end%2C%20this%20paper%20studies%20the%0Aneighbor%20in%20the%20existing%20outlier%20detection%20algorithms%20and%20a%20taxonomy%20is%0Aintroduced%2C%20which%20uses%20the%20three-level%20components%20of%20information%2C%20neighbor%20and%0Amethodology%20to%20define%20hybrid%20methods.%20This%20taxonomy%20can%20serve%20as%20a%20paradigm%0Awhere%20a%20novel%20neighbor-based%20outlier%20detection%20method%20can%20be%20proposed%20by%0Acombining%20different%20components%20in%20this%20taxonomy.%20A%20large%20number%20of%20comparative%0Aexperiments%20were%20conducted%20on%20synthetic%20and%20real-world%20datasets%20in%20terms%20of%0Aperformance%20comparison%20and%20case%20study%2C%20and%20the%20results%20show%20that%20reverse%0AK-nearest%20neighbor%20based%20methods%20achieve%20promising%20performance%20and%20dynamic%0Aselection%20method%20is%20suitable%20for%20working%20in%20high-dimensional%20space.%20Notably%2C%20it%0Ais%20verified%20that%20rationally%20selecting%20components%20from%20this%20taxonomy%20may%20create%0Aan%20algorithms%20superior%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Study%2520of%2520Neighbor-based%2520Methods%2520for%2520Local%2520Outlier%2520Detection%26entry.906535625%3DZhuang%2520Qi%2520and%2520Junlin%2520Zhang%2520and%2520Xiaming%2520Chen%2520and%2520Xin%2520Qi%26entry.1292438233%3D%2520%2520The%2520neighbor-based%2520method%2520has%2520become%2520a%2520powerful%2520tool%2520to%2520handle%2520the%2520outlier%250Adetection%2520problem%252C%2520which%2520aims%2520to%2520infer%2520the%2520abnormal%2520degree%2520of%2520the%2520sample%2520based%250Aon%2520the%2520compactness%2520of%2520the%2520sample%2520and%2520its%2520neighbors.%2520However%252C%2520the%2520existing%250Amethods%2520commonly%2520focus%2520on%2520designing%2520different%2520processes%2520to%2520locate%2520outliers%2520in%250Athe%2520dataset%252C%2520while%2520the%2520contributions%2520of%2520different%2520types%2520neighbors%2520to%2520outlier%250Adetection%2520has%2520not%2520been%2520well%2520discussed.%2520To%2520this%2520end%252C%2520this%2520paper%2520studies%2520the%250Aneighbor%2520in%2520the%2520existing%2520outlier%2520detection%2520algorithms%2520and%2520a%2520taxonomy%2520is%250Aintroduced%252C%2520which%2520uses%2520the%2520three-level%2520components%2520of%2520information%252C%2520neighbor%2520and%250Amethodology%2520to%2520define%2520hybrid%2520methods.%2520This%2520taxonomy%2520can%2520serve%2520as%2520a%2520paradigm%250Awhere%2520a%2520novel%2520neighbor-based%2520outlier%2520detection%2520method%2520can%2520be%2520proposed%2520by%250Acombining%2520different%2520components%2520in%2520this%2520taxonomy.%2520A%2520large%2520number%2520of%2520comparative%250Aexperiments%2520were%2520conducted%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520in%2520terms%2520of%250Aperformance%2520comparison%2520and%2520case%2520study%252C%2520and%2520the%2520results%2520show%2520that%2520reverse%250AK-nearest%2520neighbor%2520based%2520methods%2520achieve%2520promising%2520performance%2520and%2520dynamic%250Aselection%2520method%2520is%2520suitable%2520for%2520working%2520in%2520high-dimensional%2520space.%2520Notably%252C%2520it%250Ais%2520verified%2520that%2520rationally%2520selecting%2520components%2520from%2520this%2520taxonomy%2520may%2520create%250Aan%2520algorithms%2520superior%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Study%20of%20Neighbor-based%20Methods%20for%20Local%20Outlier%20Detection&entry.906535625=Zhuang%20Qi%20and%20Junlin%20Zhang%20and%20Xiaming%20Chen%20and%20Xin%20Qi&entry.1292438233=%20%20The%20neighbor-based%20method%20has%20become%20a%20powerful%20tool%20to%20handle%20the%20outlier%0Adetection%20problem%2C%20which%20aims%20to%20infer%20the%20abnormal%20degree%20of%20the%20sample%20based%0Aon%20the%20compactness%20of%20the%20sample%20and%20its%20neighbors.%20However%2C%20the%20existing%0Amethods%20commonly%20focus%20on%20designing%20different%20processes%20to%20locate%20outliers%20in%0Athe%20dataset%2C%20while%20the%20contributions%20of%20different%20types%20neighbors%20to%20outlier%0Adetection%20has%20not%20been%20well%20discussed.%20To%20this%20end%2C%20this%20paper%20studies%20the%0Aneighbor%20in%20the%20existing%20outlier%20detection%20algorithms%20and%20a%20taxonomy%20is%0Aintroduced%2C%20which%20uses%20the%20three-level%20components%20of%20information%2C%20neighbor%20and%0Amethodology%20to%20define%20hybrid%20methods.%20This%20taxonomy%20can%20serve%20as%20a%20paradigm%0Awhere%20a%20novel%20neighbor-based%20outlier%20detection%20method%20can%20be%20proposed%20by%0Acombining%20different%20components%20in%20this%20taxonomy.%20A%20large%20number%20of%20comparative%0Aexperiments%20were%20conducted%20on%20synthetic%20and%20real-world%20datasets%20in%20terms%20of%0Aperformance%20comparison%20and%20case%20study%2C%20and%20the%20results%20show%20that%20reverse%0AK-nearest%20neighbor%20based%20methods%20achieve%20promising%20performance%20and%20dynamic%0Aselection%20method%20is%20suitable%20for%20working%20in%20high-dimensional%20space.%20Notably%2C%20it%0Ais%20verified%20that%20rationally%20selecting%20components%20from%20this%20taxonomy%20may%20create%0Aan%20algorithms%20superior%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19247v1&entry.124074799=Read"},
{"title": "Going beyond compositional generalization, DDPMs can produce zero-shot\n  interpolation", "author": "Justin Deschenaux and Igor Krawczuk and Grigorios Chrysos and Volkan Cevher", "abstract": "  Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable\ncapabilities in image generation, with studies suggesting that they can\ngeneralize by composing latent factors learned from the training data. In this\nwork, we go further and study DDPMs trained on strictly separate subsets of the\ndata distribution with large gaps on the support of the latent factors. We show\nthat such a model can effectively generate images in the unexplored,\nintermediate regions of the distribution. For instance, when trained on clearly\nsmiling and non-smiling faces, we demonstrate a sampling procedure which can\ngenerate slightly smiling faces without reference images (zero-shot\ninterpolation). We replicate these findings for other attributes as well as\nother datasets.\n$\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our\ncode is available on GitHub.}}$\n", "link": "http://arxiv.org/abs/2405.19201v1", "date": "2024-05-29", "relevancy": 2.1947, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5865}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Going%20beyond%20compositional%20generalization%2C%20DDPMs%20can%20produce%20zero-shot%0A%20%20interpolation&body=Title%3A%20Going%20beyond%20compositional%20generalization%2C%20DDPMs%20can%20produce%20zero-shot%0A%20%20interpolation%0AAuthor%3A%20Justin%20Deschenaux%20and%20Igor%20Krawczuk%20and%20Grigorios%20Chrysos%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20exhibit%20remarkable%0Acapabilities%20in%20image%20generation%2C%20with%20studies%20suggesting%20that%20they%20can%0Ageneralize%20by%20composing%20latent%20factors%20learned%20from%20the%20training%20data.%20In%20this%0Awork%2C%20we%20go%20further%20and%20study%20DDPMs%20trained%20on%20strictly%20separate%20subsets%20of%20the%0Adata%20distribution%20with%20large%20gaps%20on%20the%20support%20of%20the%20latent%20factors.%20We%20show%0Athat%20such%20a%20model%20can%20effectively%20generate%20images%20in%20the%20unexplored%2C%0Aintermediate%20regions%20of%20the%20distribution.%20For%20instance%2C%20when%20trained%20on%20clearly%0Asmiling%20and%20non-smiling%20faces%2C%20we%20demonstrate%20a%20sampling%20procedure%20which%20can%0Agenerate%20slightly%20smiling%20faces%20without%20reference%20images%20%28zero-shot%0Ainterpolation%29.%20We%20replicate%20these%20findings%20for%20other%20attributes%20as%20well%20as%0Aother%20datasets.%0A%24%5Chref%7Bhttps%3A//github.com/jdeschena/ddpm-zero-shot-interpolation%7D%7B%5Ctext%7BOur%0Acode%20is%20available%20on%20GitHub.%7D%7D%24%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoing%2520beyond%2520compositional%2520generalization%252C%2520DDPMs%2520can%2520produce%2520zero-shot%250A%2520%2520interpolation%26entry.906535625%3DJustin%2520Deschenaux%2520and%2520Igor%2520Krawczuk%2520and%2520Grigorios%2520Chrysos%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520exhibit%2520remarkable%250Acapabilities%2520in%2520image%2520generation%252C%2520with%2520studies%2520suggesting%2520that%2520they%2520can%250Ageneralize%2520by%2520composing%2520latent%2520factors%2520learned%2520from%2520the%2520training%2520data.%2520In%2520this%250Awork%252C%2520we%2520go%2520further%2520and%2520study%2520DDPMs%2520trained%2520on%2520strictly%2520separate%2520subsets%2520of%2520the%250Adata%2520distribution%2520with%2520large%2520gaps%2520on%2520the%2520support%2520of%2520the%2520latent%2520factors.%2520We%2520show%250Athat%2520such%2520a%2520model%2520can%2520effectively%2520generate%2520images%2520in%2520the%2520unexplored%252C%250Aintermediate%2520regions%2520of%2520the%2520distribution.%2520For%2520instance%252C%2520when%2520trained%2520on%2520clearly%250Asmiling%2520and%2520non-smiling%2520faces%252C%2520we%2520demonstrate%2520a%2520sampling%2520procedure%2520which%2520can%250Agenerate%2520slightly%2520smiling%2520faces%2520without%2520reference%2520images%2520%2528zero-shot%250Ainterpolation%2529.%2520We%2520replicate%2520these%2520findings%2520for%2520other%2520attributes%2520as%2520well%2520as%250Aother%2520datasets.%250A%2524%255Chref%257Bhttps%253A//github.com/jdeschena/ddpm-zero-shot-interpolation%257D%257B%255Ctext%257BOur%250Acode%2520is%2520available%2520on%2520GitHub.%257D%257D%2524%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Going%20beyond%20compositional%20generalization%2C%20DDPMs%20can%20produce%20zero-shot%0A%20%20interpolation&entry.906535625=Justin%20Deschenaux%20and%20Igor%20Krawczuk%20and%20Grigorios%20Chrysos%20and%20Volkan%20Cevher&entry.1292438233=%20%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20exhibit%20remarkable%0Acapabilities%20in%20image%20generation%2C%20with%20studies%20suggesting%20that%20they%20can%0Ageneralize%20by%20composing%20latent%20factors%20learned%20from%20the%20training%20data.%20In%20this%0Awork%2C%20we%20go%20further%20and%20study%20DDPMs%20trained%20on%20strictly%20separate%20subsets%20of%20the%0Adata%20distribution%20with%20large%20gaps%20on%20the%20support%20of%20the%20latent%20factors.%20We%20show%0Athat%20such%20a%20model%20can%20effectively%20generate%20images%20in%20the%20unexplored%2C%0Aintermediate%20regions%20of%20the%20distribution.%20For%20instance%2C%20when%20trained%20on%20clearly%0Asmiling%20and%20non-smiling%20faces%2C%20we%20demonstrate%20a%20sampling%20procedure%20which%20can%0Agenerate%20slightly%20smiling%20faces%20without%20reference%20images%20%28zero-shot%0Ainterpolation%29.%20We%20replicate%20these%20findings%20for%20other%20attributes%20as%20well%20as%0Aother%20datasets.%0A%24%5Chref%7Bhttps%3A//github.com/jdeschena/ddpm-zero-shot-interpolation%7D%7B%5Ctext%7BOur%0Acode%20is%20available%20on%20GitHub.%7D%7D%24%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19201v1&entry.124074799=Read"},
{"title": "Benchmarking and Improving Detail Image Caption", "author": "Hongyuan Dong and Jiawen Li and Bohong Wu and Jiacong Wang and Yuan Zhang and Haoyuan Guo", "abstract": "  Image captioning has long been regarded as a fundamental task in visual\nunderstanding. Recently, however, few large vision-language model (LVLM)\nresearch discusses model's image captioning performance because of the outdated\nshort-caption benchmarks and unreliable evaluation metrics. In this work, we\npropose to benchmark detail image caption task by curating high-quality\nevaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro. We\nalso design a more reliable caption evaluation metric called CAPTURE (CAPtion\nevaluation by exTracting and coUpling coRE information). CAPTURE extracts\nvisual elements, e.g., objects, attributes and relations from captions, and\nthen matches these elements through three stages, achieving the highest\nconsistency with expert judgements over other rule-based or model-based caption\nmetrics. The proposed benchmark and metric provide reliable evaluation for\nLVLM's detailed image captioning ability. Guided by this evaluation, we further\nexplore to unleash LVLM's detail caption capabilities by synthesizing\nhigh-quality data through a five-stage data construction pipeline. Our pipeline\nonly uses a given LVLM itself and other open-source tools, without any human or\nGPT-4V annotation in the loop. Experiments show that the proposed data\nconstruction strategy significantly improves model-generated detail caption\ndata quality for LVLMs with leading performance, and the data quality can be\nfurther improved in a self-looping paradigm. All code and dataset will be\npublicly available at https://github.com/foundation-multimodal-models/CAPTURE.\n", "link": "http://arxiv.org/abs/2405.19092v1", "date": "2024-05-29", "relevancy": 2.194, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5739}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20and%20Improving%20Detail%20Image%20Caption&body=Title%3A%20Benchmarking%20and%20Improving%20Detail%20Image%20Caption%0AAuthor%3A%20Hongyuan%20Dong%20and%20Jiawen%20Li%20and%20Bohong%20Wu%20and%20Jiacong%20Wang%20and%20Yuan%20Zhang%20and%20Haoyuan%20Guo%0AAbstract%3A%20%20%20Image%20captioning%20has%20long%20been%20regarded%20as%20a%20fundamental%20task%20in%20visual%0Aunderstanding.%20Recently%2C%20however%2C%20few%20large%20vision-language%20model%20%28LVLM%29%0Aresearch%20discusses%20model%27s%20image%20captioning%20performance%20because%20of%20the%20outdated%0Ashort-caption%20benchmarks%20and%20unreliable%20evaluation%20metrics.%20In%20this%20work%2C%20we%0Apropose%20to%20benchmark%20detail%20image%20caption%20task%20by%20curating%20high-quality%0Aevaluation%20datasets%20annotated%20by%20human%20experts%2C%20GPT-4V%20and%20Gemini-1.5-Pro.%20We%0Aalso%20design%20a%20more%20reliable%20caption%20evaluation%20metric%20called%20CAPTURE%20%28CAPtion%0Aevaluation%20by%20exTracting%20and%20coUpling%20coRE%20information%29.%20CAPTURE%20extracts%0Avisual%20elements%2C%20e.g.%2C%20objects%2C%20attributes%20and%20relations%20from%20captions%2C%20and%0Athen%20matches%20these%20elements%20through%20three%20stages%2C%20achieving%20the%20highest%0Aconsistency%20with%20expert%20judgements%20over%20other%20rule-based%20or%20model-based%20caption%0Ametrics.%20The%20proposed%20benchmark%20and%20metric%20provide%20reliable%20evaluation%20for%0ALVLM%27s%20detailed%20image%20captioning%20ability.%20Guided%20by%20this%20evaluation%2C%20we%20further%0Aexplore%20to%20unleash%20LVLM%27s%20detail%20caption%20capabilities%20by%20synthesizing%0Ahigh-quality%20data%20through%20a%20five-stage%20data%20construction%20pipeline.%20Our%20pipeline%0Aonly%20uses%20a%20given%20LVLM%20itself%20and%20other%20open-source%20tools%2C%20without%20any%20human%20or%0AGPT-4V%20annotation%20in%20the%20loop.%20Experiments%20show%20that%20the%20proposed%20data%0Aconstruction%20strategy%20significantly%20improves%20model-generated%20detail%20caption%0Adata%20quality%20for%20LVLMs%20with%20leading%20performance%2C%20and%20the%20data%20quality%20can%20be%0Afurther%20improved%20in%20a%20self-looping%20paradigm.%20All%20code%20and%20dataset%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/foundation-multimodal-models/CAPTURE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520and%2520Improving%2520Detail%2520Image%2520Caption%26entry.906535625%3DHongyuan%2520Dong%2520and%2520Jiawen%2520Li%2520and%2520Bohong%2520Wu%2520and%2520Jiacong%2520Wang%2520and%2520Yuan%2520Zhang%2520and%2520Haoyuan%2520Guo%26entry.1292438233%3D%2520%2520Image%2520captioning%2520has%2520long%2520been%2520regarded%2520as%2520a%2520fundamental%2520task%2520in%2520visual%250Aunderstanding.%2520Recently%252C%2520however%252C%2520few%2520large%2520vision-language%2520model%2520%2528LVLM%2529%250Aresearch%2520discusses%2520model%2527s%2520image%2520captioning%2520performance%2520because%2520of%2520the%2520outdated%250Ashort-caption%2520benchmarks%2520and%2520unreliable%2520evaluation%2520metrics.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520benchmark%2520detail%2520image%2520caption%2520task%2520by%2520curating%2520high-quality%250Aevaluation%2520datasets%2520annotated%2520by%2520human%2520experts%252C%2520GPT-4V%2520and%2520Gemini-1.5-Pro.%2520We%250Aalso%2520design%2520a%2520more%2520reliable%2520caption%2520evaluation%2520metric%2520called%2520CAPTURE%2520%2528CAPtion%250Aevaluation%2520by%2520exTracting%2520and%2520coUpling%2520coRE%2520information%2529.%2520CAPTURE%2520extracts%250Avisual%2520elements%252C%2520e.g.%252C%2520objects%252C%2520attributes%2520and%2520relations%2520from%2520captions%252C%2520and%250Athen%2520matches%2520these%2520elements%2520through%2520three%2520stages%252C%2520achieving%2520the%2520highest%250Aconsistency%2520with%2520expert%2520judgements%2520over%2520other%2520rule-based%2520or%2520model-based%2520caption%250Ametrics.%2520The%2520proposed%2520benchmark%2520and%2520metric%2520provide%2520reliable%2520evaluation%2520for%250ALVLM%2527s%2520detailed%2520image%2520captioning%2520ability.%2520Guided%2520by%2520this%2520evaluation%252C%2520we%2520further%250Aexplore%2520to%2520unleash%2520LVLM%2527s%2520detail%2520caption%2520capabilities%2520by%2520synthesizing%250Ahigh-quality%2520data%2520through%2520a%2520five-stage%2520data%2520construction%2520pipeline.%2520Our%2520pipeline%250Aonly%2520uses%2520a%2520given%2520LVLM%2520itself%2520and%2520other%2520open-source%2520tools%252C%2520without%2520any%2520human%2520or%250AGPT-4V%2520annotation%2520in%2520the%2520loop.%2520Experiments%2520show%2520that%2520the%2520proposed%2520data%250Aconstruction%2520strategy%2520significantly%2520improves%2520model-generated%2520detail%2520caption%250Adata%2520quality%2520for%2520LVLMs%2520with%2520leading%2520performance%252C%2520and%2520the%2520data%2520quality%2520can%2520be%250Afurther%2520improved%2520in%2520a%2520self-looping%2520paradigm.%2520All%2520code%2520and%2520dataset%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/foundation-multimodal-models/CAPTURE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20and%20Improving%20Detail%20Image%20Caption&entry.906535625=Hongyuan%20Dong%20and%20Jiawen%20Li%20and%20Bohong%20Wu%20and%20Jiacong%20Wang%20and%20Yuan%20Zhang%20and%20Haoyuan%20Guo&entry.1292438233=%20%20Image%20captioning%20has%20long%20been%20regarded%20as%20a%20fundamental%20task%20in%20visual%0Aunderstanding.%20Recently%2C%20however%2C%20few%20large%20vision-language%20model%20%28LVLM%29%0Aresearch%20discusses%20model%27s%20image%20captioning%20performance%20because%20of%20the%20outdated%0Ashort-caption%20benchmarks%20and%20unreliable%20evaluation%20metrics.%20In%20this%20work%2C%20we%0Apropose%20to%20benchmark%20detail%20image%20caption%20task%20by%20curating%20high-quality%0Aevaluation%20datasets%20annotated%20by%20human%20experts%2C%20GPT-4V%20and%20Gemini-1.5-Pro.%20We%0Aalso%20design%20a%20more%20reliable%20caption%20evaluation%20metric%20called%20CAPTURE%20%28CAPtion%0Aevaluation%20by%20exTracting%20and%20coUpling%20coRE%20information%29.%20CAPTURE%20extracts%0Avisual%20elements%2C%20e.g.%2C%20objects%2C%20attributes%20and%20relations%20from%20captions%2C%20and%0Athen%20matches%20these%20elements%20through%20three%20stages%2C%20achieving%20the%20highest%0Aconsistency%20with%20expert%20judgements%20over%20other%20rule-based%20or%20model-based%20caption%0Ametrics.%20The%20proposed%20benchmark%20and%20metric%20provide%20reliable%20evaluation%20for%0ALVLM%27s%20detailed%20image%20captioning%20ability.%20Guided%20by%20this%20evaluation%2C%20we%20further%0Aexplore%20to%20unleash%20LVLM%27s%20detail%20caption%20capabilities%20by%20synthesizing%0Ahigh-quality%20data%20through%20a%20five-stage%20data%20construction%20pipeline.%20Our%20pipeline%0Aonly%20uses%20a%20given%20LVLM%20itself%20and%20other%20open-source%20tools%2C%20without%20any%20human%20or%0AGPT-4V%20annotation%20in%20the%20loop.%20Experiments%20show%20that%20the%20proposed%20data%0Aconstruction%20strategy%20significantly%20improves%20model-generated%20detail%20caption%0Adata%20quality%20for%20LVLMs%20with%20leading%20performance%2C%20and%20the%20data%20quality%20can%20be%0Afurther%20improved%20in%20a%20self-looping%20paradigm.%20All%20code%20and%20dataset%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/foundation-multimodal-models/CAPTURE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19092v1&entry.124074799=Read"},
{"title": "Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal\n  Best Arm Identification", "author": "Masahiro Kato", "abstract": "  This study investigates a local asymptotic minimax optimal strategy for\nfixed-budget best arm identification (BAI). We propose the Adaptive Generalized\nNeyman Allocation (AGNA) strategy and show that its worst-case upper bound of\nthe probability of misidentifying the best arm aligns with the worst-case lower\nbound under the small-gap regime, where the gap between the expected outcomes\nof the best and suboptimal arms is small. Our strategy corresponds to a\ngeneralization of the Neyman allocation for two-armed bandits (Neyman, 1934;\nKaufmann et al., 2016) and a refinement of existing strategies such as the ones\nproposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama\net al. (2022), which proposes a minimax rate-optimal strategy, our proposed\nstrategy has a tighter upper bound that exactly matches the lower bound,\nincluding the constant terms, by restricting the class of distributions to the\nclass of small-gap distributions. Our result contributes to the longstanding\nopen issue about the existence of asymptotically optimal strategies in\nfixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.\n", "link": "http://arxiv.org/abs/2405.19317v1", "date": "2024-05-29", "relevancy": 2.1822, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4592}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4269}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Generalized%20Neyman%20Allocation%3A%20Local%20Asymptotic%20Minimax%20Optimal%0A%20%20Best%20Arm%20Identification&body=Title%3A%20Adaptive%20Generalized%20Neyman%20Allocation%3A%20Local%20Asymptotic%20Minimax%20Optimal%0A%20%20Best%20Arm%20Identification%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20This%20study%20investigates%20a%20local%20asymptotic%20minimax%20optimal%20strategy%20for%0Afixed-budget%20best%20arm%20identification%20%28BAI%29.%20We%20propose%20the%20Adaptive%20Generalized%0ANeyman%20Allocation%20%28AGNA%29%20strategy%20and%20show%20that%20its%20worst-case%20upper%20bound%20of%0Athe%20probability%20of%20misidentifying%20the%20best%20arm%20aligns%20with%20the%20worst-case%20lower%0Abound%20under%20the%20small-gap%20regime%2C%20where%20the%20gap%20between%20the%20expected%20outcomes%0Aof%20the%20best%20and%20suboptimal%20arms%20is%20small.%20Our%20strategy%20corresponds%20to%20a%0Ageneralization%20of%20the%20Neyman%20allocation%20for%20two-armed%20bandits%20%28Neyman%2C%201934%3B%0AKaufmann%20et%20al.%2C%202016%29%20and%20a%20refinement%20of%20existing%20strategies%20such%20as%20the%20ones%0Aproposed%20by%20Glynn%20%26%20Juneja%20%282004%29%20and%20Shin%20et%20al.%20%282018%29.%20Compared%20to%20Komiyama%0Aet%20al.%20%282022%29%2C%20which%20proposes%20a%20minimax%20rate-optimal%20strategy%2C%20our%20proposed%0Astrategy%20has%20a%20tighter%20upper%20bound%20that%20exactly%20matches%20the%20lower%20bound%2C%0Aincluding%20the%20constant%20terms%2C%20by%20restricting%20the%20class%20of%20distributions%20to%20the%0Aclass%20of%20small-gap%20distributions.%20Our%20result%20contributes%20to%20the%20longstanding%0Aopen%20issue%20about%20the%20existence%20of%20asymptotically%20optimal%20strategies%20in%0Afixed-budget%20BAI%2C%20by%20presenting%20the%20local%20asymptotic%20minimax%20optimal%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Generalized%2520Neyman%2520Allocation%253A%2520Local%2520Asymptotic%2520Minimax%2520Optimal%250A%2520%2520Best%2520Arm%2520Identification%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520a%2520local%2520asymptotic%2520minimax%2520optimal%2520strategy%2520for%250Afixed-budget%2520best%2520arm%2520identification%2520%2528BAI%2529.%2520We%2520propose%2520the%2520Adaptive%2520Generalized%250ANeyman%2520Allocation%2520%2528AGNA%2529%2520strategy%2520and%2520show%2520that%2520its%2520worst-case%2520upper%2520bound%2520of%250Athe%2520probability%2520of%2520misidentifying%2520the%2520best%2520arm%2520aligns%2520with%2520the%2520worst-case%2520lower%250Abound%2520under%2520the%2520small-gap%2520regime%252C%2520where%2520the%2520gap%2520between%2520the%2520expected%2520outcomes%250Aof%2520the%2520best%2520and%2520suboptimal%2520arms%2520is%2520small.%2520Our%2520strategy%2520corresponds%2520to%2520a%250Ageneralization%2520of%2520the%2520Neyman%2520allocation%2520for%2520two-armed%2520bandits%2520%2528Neyman%252C%25201934%253B%250AKaufmann%2520et%2520al.%252C%25202016%2529%2520and%2520a%2520refinement%2520of%2520existing%2520strategies%2520such%2520as%2520the%2520ones%250Aproposed%2520by%2520Glynn%2520%2526%2520Juneja%2520%25282004%2529%2520and%2520Shin%2520et%2520al.%2520%25282018%2529.%2520Compared%2520to%2520Komiyama%250Aet%2520al.%2520%25282022%2529%252C%2520which%2520proposes%2520a%2520minimax%2520rate-optimal%2520strategy%252C%2520our%2520proposed%250Astrategy%2520has%2520a%2520tighter%2520upper%2520bound%2520that%2520exactly%2520matches%2520the%2520lower%2520bound%252C%250Aincluding%2520the%2520constant%2520terms%252C%2520by%2520restricting%2520the%2520class%2520of%2520distributions%2520to%2520the%250Aclass%2520of%2520small-gap%2520distributions.%2520Our%2520result%2520contributes%2520to%2520the%2520longstanding%250Aopen%2520issue%2520about%2520the%2520existence%2520of%2520asymptotically%2520optimal%2520strategies%2520in%250Afixed-budget%2520BAI%252C%2520by%2520presenting%2520the%2520local%2520asymptotic%2520minimax%2520optimal%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Generalized%20Neyman%20Allocation%3A%20Local%20Asymptotic%20Minimax%20Optimal%0A%20%20Best%20Arm%20Identification&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20This%20study%20investigates%20a%20local%20asymptotic%20minimax%20optimal%20strategy%20for%0Afixed-budget%20best%20arm%20identification%20%28BAI%29.%20We%20propose%20the%20Adaptive%20Generalized%0ANeyman%20Allocation%20%28AGNA%29%20strategy%20and%20show%20that%20its%20worst-case%20upper%20bound%20of%0Athe%20probability%20of%20misidentifying%20the%20best%20arm%20aligns%20with%20the%20worst-case%20lower%0Abound%20under%20the%20small-gap%20regime%2C%20where%20the%20gap%20between%20the%20expected%20outcomes%0Aof%20the%20best%20and%20suboptimal%20arms%20is%20small.%20Our%20strategy%20corresponds%20to%20a%0Ageneralization%20of%20the%20Neyman%20allocation%20for%20two-armed%20bandits%20%28Neyman%2C%201934%3B%0AKaufmann%20et%20al.%2C%202016%29%20and%20a%20refinement%20of%20existing%20strategies%20such%20as%20the%20ones%0Aproposed%20by%20Glynn%20%26%20Juneja%20%282004%29%20and%20Shin%20et%20al.%20%282018%29.%20Compared%20to%20Komiyama%0Aet%20al.%20%282022%29%2C%20which%20proposes%20a%20minimax%20rate-optimal%20strategy%2C%20our%20proposed%0Astrategy%20has%20a%20tighter%20upper%20bound%20that%20exactly%20matches%20the%20lower%20bound%2C%0Aincluding%20the%20constant%20terms%2C%20by%20restricting%20the%20class%20of%20distributions%20to%20the%0Aclass%20of%20small-gap%20distributions.%20Our%20result%20contributes%20to%20the%20longstanding%0Aopen%20issue%20about%20the%20existence%20of%20asymptotically%20optimal%20strategies%20in%0Afixed-budget%20BAI%2C%20by%20presenting%20the%20local%20asymptotic%20minimax%20optimal%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19317v1&entry.124074799=Read"},
{"title": "Verifiably Robust Conformal Prediction", "author": "Linus Jeary and Tom Kuipers and Mehran Hosseini and Nicola Paoletti", "abstract": "  Conformal Prediction (CP) is a popular uncertainty quantification method that\nprovides distribution-free, statistically valid prediction sets, assuming that\ntraining and test data are exchangeable. In such a case, CP's prediction sets\nare guaranteed to cover the (unknown) true test output with a user-specified\nprobability. Nevertheless, this guarantee is violated when the data is\nsubjected to adversarial attacks, which often result in a significant loss of\ncoverage. Recently, several approaches have been put forward to recover CP\nguarantees in this setting. These approaches leverage variations of randomised\nsmoothing to produce conservative sets which account for the effect of the\nadversarial perturbations. They are, however, limited in that they only support\n$\\ell^2$-bounded perturbations and classification tasks. This paper introduces\n\\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that\nleverages recent neural network verification methods to recover coverage\nguarantees under adversarial attacks. Our VRCP method is the first to support\nperturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and\n$\\ell^\\infty$, as well as regression tasks. We evaluate and compare our\napproach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet)\nand regression tasks for deep reinforcement learning environments. In every\ncase, VRCP achieves above nominal coverage and yields significantly more\nefficient and informative prediction regions than the SotA.\n", "link": "http://arxiv.org/abs/2405.18942v1", "date": "2024-05-29", "relevancy": 2.1649, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5508}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5457}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifiably%20Robust%20Conformal%20Prediction&body=Title%3A%20Verifiably%20Robust%20Conformal%20Prediction%0AAuthor%3A%20Linus%20Jeary%20and%20Tom%20Kuipers%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20uncertainty%20quantification%20method%20that%0Aprovides%20distribution-free%2C%20statistically%20valid%20prediction%20sets%2C%20assuming%20that%0Atraining%20and%20test%20data%20are%20exchangeable.%20In%20such%20a%20case%2C%20CP%27s%20prediction%20sets%0Aare%20guaranteed%20to%20cover%20the%20%28unknown%29%20true%20test%20output%20with%20a%20user-specified%0Aprobability.%20Nevertheless%2C%20this%20guarantee%20is%20violated%20when%20the%20data%20is%0Asubjected%20to%20adversarial%20attacks%2C%20which%20often%20result%20in%20a%20significant%20loss%20of%0Acoverage.%20Recently%2C%20several%20approaches%20have%20been%20put%20forward%20to%20recover%20CP%0Aguarantees%20in%20this%20setting.%20These%20approaches%20leverage%20variations%20of%20randomised%0Asmoothing%20to%20produce%20conservative%20sets%20which%20account%20for%20the%20effect%20of%20the%0Aadversarial%20perturbations.%20They%20are%2C%20however%2C%20limited%20in%20that%20they%20only%20support%0A%24%5Cell%5E2%24-bounded%20perturbations%20and%20classification%20tasks.%20This%20paper%20introduces%0A%5Cemph%7BVRCP%20%28Verifiably%20Robust%20Conformal%20Prediction%29%7D%2C%20a%20new%20framework%20that%0Aleverages%20recent%20neural%20network%20verification%20methods%20to%20recover%20coverage%0Aguarantees%20under%20adversarial%20attacks.%20Our%20VRCP%20method%20is%20the%20first%20to%20support%0Aperturbations%20bounded%20by%20arbitrary%20norms%20including%20%24%5Cell%5E1%24%2C%20%24%5Cell%5E2%24%2C%20and%0A%24%5Cell%5E%5Cinfty%24%2C%20as%20well%20as%20regression%20tasks.%20We%20evaluate%20and%20compare%20our%0Aapproach%20on%20image%20classification%20tasks%20%28CIFAR10%2C%20CIFAR100%2C%20and%20TinyImageNet%29%0Aand%20regression%20tasks%20for%20deep%20reinforcement%20learning%20environments.%20In%20every%0Acase%2C%20VRCP%20achieves%20above%20nominal%20coverage%20and%20yields%20significantly%20more%0Aefficient%20and%20informative%20prediction%20regions%20than%20the%20SotA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifiably%2520Robust%2520Conformal%2520Prediction%26entry.906535625%3DLinus%2520Jeary%2520and%2520Tom%2520Kuipers%2520and%2520Mehran%2520Hosseini%2520and%2520Nicola%2520Paoletti%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520popular%2520uncertainty%2520quantification%2520method%2520that%250Aprovides%2520distribution-free%252C%2520statistically%2520valid%2520prediction%2520sets%252C%2520assuming%2520that%250Atraining%2520and%2520test%2520data%2520are%2520exchangeable.%2520In%2520such%2520a%2520case%252C%2520CP%2527s%2520prediction%2520sets%250Aare%2520guaranteed%2520to%2520cover%2520the%2520%2528unknown%2529%2520true%2520test%2520output%2520with%2520a%2520user-specified%250Aprobability.%2520Nevertheless%252C%2520this%2520guarantee%2520is%2520violated%2520when%2520the%2520data%2520is%250Asubjected%2520to%2520adversarial%2520attacks%252C%2520which%2520often%2520result%2520in%2520a%2520significant%2520loss%2520of%250Acoverage.%2520Recently%252C%2520several%2520approaches%2520have%2520been%2520put%2520forward%2520to%2520recover%2520CP%250Aguarantees%2520in%2520this%2520setting.%2520These%2520approaches%2520leverage%2520variations%2520of%2520randomised%250Asmoothing%2520to%2520produce%2520conservative%2520sets%2520which%2520account%2520for%2520the%2520effect%2520of%2520the%250Aadversarial%2520perturbations.%2520They%2520are%252C%2520however%252C%2520limited%2520in%2520that%2520they%2520only%2520support%250A%2524%255Cell%255E2%2524-bounded%2520perturbations%2520and%2520classification%2520tasks.%2520This%2520paper%2520introduces%250A%255Cemph%257BVRCP%2520%2528Verifiably%2520Robust%2520Conformal%2520Prediction%2529%257D%252C%2520a%2520new%2520framework%2520that%250Aleverages%2520recent%2520neural%2520network%2520verification%2520methods%2520to%2520recover%2520coverage%250Aguarantees%2520under%2520adversarial%2520attacks.%2520Our%2520VRCP%2520method%2520is%2520the%2520first%2520to%2520support%250Aperturbations%2520bounded%2520by%2520arbitrary%2520norms%2520including%2520%2524%255Cell%255E1%2524%252C%2520%2524%255Cell%255E2%2524%252C%2520and%250A%2524%255Cell%255E%255Cinfty%2524%252C%2520as%2520well%2520as%2520regression%2520tasks.%2520We%2520evaluate%2520and%2520compare%2520our%250Aapproach%2520on%2520image%2520classification%2520tasks%2520%2528CIFAR10%252C%2520CIFAR100%252C%2520and%2520TinyImageNet%2529%250Aand%2520regression%2520tasks%2520for%2520deep%2520reinforcement%2520learning%2520environments.%2520In%2520every%250Acase%252C%2520VRCP%2520achieves%2520above%2520nominal%2520coverage%2520and%2520yields%2520significantly%2520more%250Aefficient%2520and%2520informative%2520prediction%2520regions%2520than%2520the%2520SotA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifiably%20Robust%20Conformal%20Prediction&entry.906535625=Linus%20Jeary%20and%20Tom%20Kuipers%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20uncertainty%20quantification%20method%20that%0Aprovides%20distribution-free%2C%20statistically%20valid%20prediction%20sets%2C%20assuming%20that%0Atraining%20and%20test%20data%20are%20exchangeable.%20In%20such%20a%20case%2C%20CP%27s%20prediction%20sets%0Aare%20guaranteed%20to%20cover%20the%20%28unknown%29%20true%20test%20output%20with%20a%20user-specified%0Aprobability.%20Nevertheless%2C%20this%20guarantee%20is%20violated%20when%20the%20data%20is%0Asubjected%20to%20adversarial%20attacks%2C%20which%20often%20result%20in%20a%20significant%20loss%20of%0Acoverage.%20Recently%2C%20several%20approaches%20have%20been%20put%20forward%20to%20recover%20CP%0Aguarantees%20in%20this%20setting.%20These%20approaches%20leverage%20variations%20of%20randomised%0Asmoothing%20to%20produce%20conservative%20sets%20which%20account%20for%20the%20effect%20of%20the%0Aadversarial%20perturbations.%20They%20are%2C%20however%2C%20limited%20in%20that%20they%20only%20support%0A%24%5Cell%5E2%24-bounded%20perturbations%20and%20classification%20tasks.%20This%20paper%20introduces%0A%5Cemph%7BVRCP%20%28Verifiably%20Robust%20Conformal%20Prediction%29%7D%2C%20a%20new%20framework%20that%0Aleverages%20recent%20neural%20network%20verification%20methods%20to%20recover%20coverage%0Aguarantees%20under%20adversarial%20attacks.%20Our%20VRCP%20method%20is%20the%20first%20to%20support%0Aperturbations%20bounded%20by%20arbitrary%20norms%20including%20%24%5Cell%5E1%24%2C%20%24%5Cell%5E2%24%2C%20and%0A%24%5Cell%5E%5Cinfty%24%2C%20as%20well%20as%20regression%20tasks.%20We%20evaluate%20and%20compare%20our%0Aapproach%20on%20image%20classification%20tasks%20%28CIFAR10%2C%20CIFAR100%2C%20and%20TinyImageNet%29%0Aand%20regression%20tasks%20for%20deep%20reinforcement%20learning%20environments.%20In%20every%0Acase%2C%20VRCP%20achieves%20above%20nominal%20coverage%20and%20yields%20significantly%20more%0Aefficient%20and%20informative%20prediction%20regions%20than%20the%20SotA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18942v1&entry.124074799=Read"},
{"title": "Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning", "author": "Yongqi Dong and Xingmin Lu and Ruohan Li and Wei Song and Bart van Arem and Haneen Farah", "abstract": "  The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.\n", "link": "http://arxiv.org/abs/2312.04398v2", "date": "2024-05-29", "relevancy": 2.1638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning&body=Title%3A%20Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning%0AAuthor%3A%20Yongqi%20Dong%20and%20Xingmin%20Lu%20and%20Ruohan%20Li%20and%20Wei%20Song%20and%20Bart%20van%20Arem%20and%20Haneen%20Farah%0AAbstract%3A%20%20%20The%20burgeoning%20navigation%20services%20using%20digital%20maps%20provide%20great%0Aconvenience%20to%20drivers.%20Nevertheless%2C%20the%20presence%20of%20anomalies%20in%20lane%0Arendering%20map%20images%20occasionally%20introduces%20potential%20hazards%2C%20as%20such%0Aanomalies%20can%20be%20misleading%20to%20human%20drivers%20and%20consequently%20contribute%20to%0Aunsafe%20driving%20conditions.%20In%20response%20to%20this%20concern%20and%20to%20accurately%20and%0Aeffectively%20detect%20the%20anomalies%2C%20this%20paper%20transforms%20lane%20rendering%20image%0Aanomaly%20detection%20into%20a%20classification%20problem%20and%20proposes%20a%20four-phase%0Apipeline%20consisting%20of%20data%20pre-processing%2C%20self-supervised%20pre-training%20with%0Athe%20masked%20image%20modeling%20%28MiM%29%20method%2C%20customized%20fine-tuning%20using%0Across-entropy%20based%20loss%20with%20label%20smoothing%2C%20and%20post-processing%20to%20tackle%20it%0Aleveraging%20state-of-the-art%20deep%20learning%20techniques%2C%20especially%20those%0Ainvolving%20Transformer%20models.%20Various%20experiments%20verify%20the%20effectiveness%20of%0Athe%20proposed%20pipeline.%20Results%20indicate%20that%20the%20proposed%20pipeline%20exhibits%0Asuperior%20performance%20in%20lane%20rendering%20image%20anomaly%20detection%2C%20and%20notably%2C%0Athe%20self-supervised%20pre-training%20with%20MiM%20can%20greatly%20enhance%20the%20detection%0Aaccuracy%20while%20significantly%20reducing%20the%20total%20training%20time.%20For%20instance%2C%0Aemploying%20the%20Swin%20Transformer%20with%20Uniform%20Masking%20as%20self-supervised%0Apretraining%20%28Swin-Trans-UM%29%20yielded%20a%20heightened%20accuracy%20at%2094.77%25%20and%20an%0Aimproved%20Area%20Under%20The%20Curve%20%28AUC%29%20score%20of%200.9743%20compared%20with%20the%20pure%20Swin%0ATransformer%20without%20pre-training%20%28Swin-Trans%29%20with%20an%20accuracy%20of%2094.01%25%20and%20an%0AAUC%20of%200.9498.%20The%20fine-tuning%20epochs%20were%20dramatically%20reduced%20to%2041%20from%20the%0Aoriginal%20280.%20In%20conclusion%2C%20the%20proposed%20pipeline%2C%20with%20its%20incorporation%20of%0Aself-supervised%20pre-training%20using%20MiM%20and%20other%20advanced%20deep%20learning%0Atechniques%2C%20emerges%20as%20a%20robust%20solution%20for%20enhancing%20the%20accuracy%20and%0Aefficiency%20of%20lane%20rendering%20image%20anomaly%20detection%20in%20digital%20navigation%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Anomaly%2520Detection%2520for%2520Lane%2520Rendering%2520Using%2520Transformer%2520with%250A%2520%2520Self-Supervised%2520Pre-Training%2520and%2520Customized%2520Fine-Tuning%26entry.906535625%3DYongqi%2520Dong%2520and%2520Xingmin%2520Lu%2520and%2520Ruohan%2520Li%2520and%2520Wei%2520Song%2520and%2520Bart%2520van%2520Arem%2520and%2520Haneen%2520Farah%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520navigation%2520services%2520using%2520digital%2520maps%2520provide%2520great%250Aconvenience%2520to%2520drivers.%2520Nevertheless%252C%2520the%2520presence%2520of%2520anomalies%2520in%2520lane%250Arendering%2520map%2520images%2520occasionally%2520introduces%2520potential%2520hazards%252C%2520as%2520such%250Aanomalies%2520can%2520be%2520misleading%2520to%2520human%2520drivers%2520and%2520consequently%2520contribute%2520to%250Aunsafe%2520driving%2520conditions.%2520In%2520response%2520to%2520this%2520concern%2520and%2520to%2520accurately%2520and%250Aeffectively%2520detect%2520the%2520anomalies%252C%2520this%2520paper%2520transforms%2520lane%2520rendering%2520image%250Aanomaly%2520detection%2520into%2520a%2520classification%2520problem%2520and%2520proposes%2520a%2520four-phase%250Apipeline%2520consisting%2520of%2520data%2520pre-processing%252C%2520self-supervised%2520pre-training%2520with%250Athe%2520masked%2520image%2520modeling%2520%2528MiM%2529%2520method%252C%2520customized%2520fine-tuning%2520using%250Across-entropy%2520based%2520loss%2520with%2520label%2520smoothing%252C%2520and%2520post-processing%2520to%2520tackle%2520it%250Aleveraging%2520state-of-the-art%2520deep%2520learning%2520techniques%252C%2520especially%2520those%250Ainvolving%2520Transformer%2520models.%2520Various%2520experiments%2520verify%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520pipeline.%2520Results%2520indicate%2520that%2520the%2520proposed%2520pipeline%2520exhibits%250Asuperior%2520performance%2520in%2520lane%2520rendering%2520image%2520anomaly%2520detection%252C%2520and%2520notably%252C%250Athe%2520self-supervised%2520pre-training%2520with%2520MiM%2520can%2520greatly%2520enhance%2520the%2520detection%250Aaccuracy%2520while%2520significantly%2520reducing%2520the%2520total%2520training%2520time.%2520For%2520instance%252C%250Aemploying%2520the%2520Swin%2520Transformer%2520with%2520Uniform%2520Masking%2520as%2520self-supervised%250Apretraining%2520%2528Swin-Trans-UM%2529%2520yielded%2520a%2520heightened%2520accuracy%2520at%252094.77%2525%2520and%2520an%250Aimproved%2520Area%2520Under%2520The%2520Curve%2520%2528AUC%2529%2520score%2520of%25200.9743%2520compared%2520with%2520the%2520pure%2520Swin%250ATransformer%2520without%2520pre-training%2520%2528Swin-Trans%2529%2520with%2520an%2520accuracy%2520of%252094.01%2525%2520and%2520an%250AAUC%2520of%25200.9498.%2520The%2520fine-tuning%2520epochs%2520were%2520dramatically%2520reduced%2520to%252041%2520from%2520the%250Aoriginal%2520280.%2520In%2520conclusion%252C%2520the%2520proposed%2520pipeline%252C%2520with%2520its%2520incorporation%2520of%250Aself-supervised%2520pre-training%2520using%2520MiM%2520and%2520other%2520advanced%2520deep%2520learning%250Atechniques%252C%2520emerges%2520as%2520a%2520robust%2520solution%2520for%2520enhancing%2520the%2520accuracy%2520and%250Aefficiency%2520of%2520lane%2520rendering%2520image%2520anomaly%2520detection%2520in%2520digital%2520navigation%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning&entry.906535625=Yongqi%20Dong%20and%20Xingmin%20Lu%20and%20Ruohan%20Li%20and%20Wei%20Song%20and%20Bart%20van%20Arem%20and%20Haneen%20Farah&entry.1292438233=%20%20The%20burgeoning%20navigation%20services%20using%20digital%20maps%20provide%20great%0Aconvenience%20to%20drivers.%20Nevertheless%2C%20the%20presence%20of%20anomalies%20in%20lane%0Arendering%20map%20images%20occasionally%20introduces%20potential%20hazards%2C%20as%20such%0Aanomalies%20can%20be%20misleading%20to%20human%20drivers%20and%20consequently%20contribute%20to%0Aunsafe%20driving%20conditions.%20In%20response%20to%20this%20concern%20and%20to%20accurately%20and%0Aeffectively%20detect%20the%20anomalies%2C%20this%20paper%20transforms%20lane%20rendering%20image%0Aanomaly%20detection%20into%20a%20classification%20problem%20and%20proposes%20a%20four-phase%0Apipeline%20consisting%20of%20data%20pre-processing%2C%20self-supervised%20pre-training%20with%0Athe%20masked%20image%20modeling%20%28MiM%29%20method%2C%20customized%20fine-tuning%20using%0Across-entropy%20based%20loss%20with%20label%20smoothing%2C%20and%20post-processing%20to%20tackle%20it%0Aleveraging%20state-of-the-art%20deep%20learning%20techniques%2C%20especially%20those%0Ainvolving%20Transformer%20models.%20Various%20experiments%20verify%20the%20effectiveness%20of%0Athe%20proposed%20pipeline.%20Results%20indicate%20that%20the%20proposed%20pipeline%20exhibits%0Asuperior%20performance%20in%20lane%20rendering%20image%20anomaly%20detection%2C%20and%20notably%2C%0Athe%20self-supervised%20pre-training%20with%20MiM%20can%20greatly%20enhance%20the%20detection%0Aaccuracy%20while%20significantly%20reducing%20the%20total%20training%20time.%20For%20instance%2C%0Aemploying%20the%20Swin%20Transformer%20with%20Uniform%20Masking%20as%20self-supervised%0Apretraining%20%28Swin-Trans-UM%29%20yielded%20a%20heightened%20accuracy%20at%2094.77%25%20and%20an%0Aimproved%20Area%20Under%20The%20Curve%20%28AUC%29%20score%20of%200.9743%20compared%20with%20the%20pure%20Swin%0ATransformer%20without%20pre-training%20%28Swin-Trans%29%20with%20an%20accuracy%20of%2094.01%25%20and%20an%0AAUC%20of%200.9498.%20The%20fine-tuning%20epochs%20were%20dramatically%20reduced%20to%2041%20from%20the%0Aoriginal%20280.%20In%20conclusion%2C%20the%20proposed%20pipeline%2C%20with%20its%20incorporation%20of%0Aself-supervised%20pre-training%20using%20MiM%20and%20other%20advanced%20deep%20learning%0Atechniques%2C%20emerges%20as%20a%20robust%20solution%20for%20enhancing%20the%20accuracy%20and%0Aefficiency%20of%20lane%20rendering%20image%20anomaly%20detection%20in%20digital%20navigation%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04398v2&entry.124074799=Read"},
{"title": "LOGO: Video Text Spotting with Language Collaboration and Glyph\n  Perception Model", "author": "Hongen Liu and Yi Liu and Di Sun and Jiahao Wang and Gang Pan", "abstract": "  Video text spotting aims to simultaneously localize, recognize and track text\ninstances in videos. To address the limited recognition capability of\nend-to-end methods, tracking the zero-shot results of state-of-the-art image\ntext spotters directly can achieve impressive performance. However, owing to\nthe domain gap between different datasets, these methods usually obtain limited\ntracking trajectories on extreme dataset. Fine-tuning transformer-based text\nspotters on specific datasets could yield performance enhancements, albeit at\nthe expense of considerable training resources. In this paper, we propose a\nLanguage Collaboration and Glyph Perception Model, termed LOGO to enhance the\nperformance of conventional text spotters through the integration of a synergy\nmodule. To achieve this goal, a language synergy classifier (LSC) is designed\nto explicitly discern text instances from background noise in the recognition\nstage. Specially, the language synergy classifier can output text content or\nbackground code based on the legibility of text regions, thus computing\nlanguage scores. Subsequently, fusion scores are computed by taking the average\nof detection scores and language scores, and are utilized to re-score the\ndetection results before tracking. By the re-scoring mechanism, the proposed\nLSC facilitates the detection of low-resolution text instances while filtering\nout text-like regions. Besides, the glyph supervision and visual position\nmixture module are proposed to enhance the recognition accuracy of noisy text\nregions, and acquire more discriminative tracking features, respectively.\nExtensive experiments on public benchmarks validate the effectiveness of the\nproposed method.\n", "link": "http://arxiv.org/abs/2405.19194v1", "date": "2024-05-29", "relevancy": 2.1573, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5759}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOGO%3A%20Video%20Text%20Spotting%20with%20Language%20Collaboration%20and%20Glyph%0A%20%20Perception%20Model&body=Title%3A%20LOGO%3A%20Video%20Text%20Spotting%20with%20Language%20Collaboration%20and%20Glyph%0A%20%20Perception%20Model%0AAuthor%3A%20Hongen%20Liu%20and%20Yi%20Liu%20and%20Di%20Sun%20and%20Jiahao%20Wang%20and%20Gang%20Pan%0AAbstract%3A%20%20%20Video%20text%20spotting%20aims%20to%20simultaneously%20localize%2C%20recognize%20and%20track%20text%0Ainstances%20in%20videos.%20To%20address%20the%20limited%20recognition%20capability%20of%0Aend-to-end%20methods%2C%20tracking%20the%20zero-shot%20results%20of%20state-of-the-art%20image%0Atext%20spotters%20directly%20can%20achieve%20impressive%20performance.%20However%2C%20owing%20to%0Athe%20domain%20gap%20between%20different%20datasets%2C%20these%20methods%20usually%20obtain%20limited%0Atracking%20trajectories%20on%20extreme%20dataset.%20Fine-tuning%20transformer-based%20text%0Aspotters%20on%20specific%20datasets%20could%20yield%20performance%20enhancements%2C%20albeit%20at%0Athe%20expense%20of%20considerable%20training%20resources.%20In%20this%20paper%2C%20we%20propose%20a%0ALanguage%20Collaboration%20and%20Glyph%20Perception%20Model%2C%20termed%20LOGO%20to%20enhance%20the%0Aperformance%20of%20conventional%20text%20spotters%20through%20the%20integration%20of%20a%20synergy%0Amodule.%20To%20achieve%20this%20goal%2C%20a%20language%20synergy%20classifier%20%28LSC%29%20is%20designed%0Ato%20explicitly%20discern%20text%20instances%20from%20background%20noise%20in%20the%20recognition%0Astage.%20Specially%2C%20the%20language%20synergy%20classifier%20can%20output%20text%20content%20or%0Abackground%20code%20based%20on%20the%20legibility%20of%20text%20regions%2C%20thus%20computing%0Alanguage%20scores.%20Subsequently%2C%20fusion%20scores%20are%20computed%20by%20taking%20the%20average%0Aof%20detection%20scores%20and%20language%20scores%2C%20and%20are%20utilized%20to%20re-score%20the%0Adetection%20results%20before%20tracking.%20By%20the%20re-scoring%20mechanism%2C%20the%20proposed%0ALSC%20facilitates%20the%20detection%20of%20low-resolution%20text%20instances%20while%20filtering%0Aout%20text-like%20regions.%20Besides%2C%20the%20glyph%20supervision%20and%20visual%20position%0Amixture%20module%20are%20proposed%20to%20enhance%20the%20recognition%20accuracy%20of%20noisy%20text%0Aregions%2C%20and%20acquire%20more%20discriminative%20tracking%20features%2C%20respectively.%0AExtensive%20experiments%20on%20public%20benchmarks%20validate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOGO%253A%2520Video%2520Text%2520Spotting%2520with%2520Language%2520Collaboration%2520and%2520Glyph%250A%2520%2520Perception%2520Model%26entry.906535625%3DHongen%2520Liu%2520and%2520Yi%2520Liu%2520and%2520Di%2520Sun%2520and%2520Jiahao%2520Wang%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520Video%2520text%2520spotting%2520aims%2520to%2520simultaneously%2520localize%252C%2520recognize%2520and%2520track%2520text%250Ainstances%2520in%2520videos.%2520To%2520address%2520the%2520limited%2520recognition%2520capability%2520of%250Aend-to-end%2520methods%252C%2520tracking%2520the%2520zero-shot%2520results%2520of%2520state-of-the-art%2520image%250Atext%2520spotters%2520directly%2520can%2520achieve%2520impressive%2520performance.%2520However%252C%2520owing%2520to%250Athe%2520domain%2520gap%2520between%2520different%2520datasets%252C%2520these%2520methods%2520usually%2520obtain%2520limited%250Atracking%2520trajectories%2520on%2520extreme%2520dataset.%2520Fine-tuning%2520transformer-based%2520text%250Aspotters%2520on%2520specific%2520datasets%2520could%2520yield%2520performance%2520enhancements%252C%2520albeit%2520at%250Athe%2520expense%2520of%2520considerable%2520training%2520resources.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250ALanguage%2520Collaboration%2520and%2520Glyph%2520Perception%2520Model%252C%2520termed%2520LOGO%2520to%2520enhance%2520the%250Aperformance%2520of%2520conventional%2520text%2520spotters%2520through%2520the%2520integration%2520of%2520a%2520synergy%250Amodule.%2520To%2520achieve%2520this%2520goal%252C%2520a%2520language%2520synergy%2520classifier%2520%2528LSC%2529%2520is%2520designed%250Ato%2520explicitly%2520discern%2520text%2520instances%2520from%2520background%2520noise%2520in%2520the%2520recognition%250Astage.%2520Specially%252C%2520the%2520language%2520synergy%2520classifier%2520can%2520output%2520text%2520content%2520or%250Abackground%2520code%2520based%2520on%2520the%2520legibility%2520of%2520text%2520regions%252C%2520thus%2520computing%250Alanguage%2520scores.%2520Subsequently%252C%2520fusion%2520scores%2520are%2520computed%2520by%2520taking%2520the%2520average%250Aof%2520detection%2520scores%2520and%2520language%2520scores%252C%2520and%2520are%2520utilized%2520to%2520re-score%2520the%250Adetection%2520results%2520before%2520tracking.%2520By%2520the%2520re-scoring%2520mechanism%252C%2520the%2520proposed%250ALSC%2520facilitates%2520the%2520detection%2520of%2520low-resolution%2520text%2520instances%2520while%2520filtering%250Aout%2520text-like%2520regions.%2520Besides%252C%2520the%2520glyph%2520supervision%2520and%2520visual%2520position%250Amixture%2520module%2520are%2520proposed%2520to%2520enhance%2520the%2520recognition%2520accuracy%2520of%2520noisy%2520text%250Aregions%252C%2520and%2520acquire%2520more%2520discriminative%2520tracking%2520features%252C%2520respectively.%250AExtensive%2520experiments%2520on%2520public%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOGO%3A%20Video%20Text%20Spotting%20with%20Language%20Collaboration%20and%20Glyph%0A%20%20Perception%20Model&entry.906535625=Hongen%20Liu%20and%20Yi%20Liu%20and%20Di%20Sun%20and%20Jiahao%20Wang%20and%20Gang%20Pan&entry.1292438233=%20%20Video%20text%20spotting%20aims%20to%20simultaneously%20localize%2C%20recognize%20and%20track%20text%0Ainstances%20in%20videos.%20To%20address%20the%20limited%20recognition%20capability%20of%0Aend-to-end%20methods%2C%20tracking%20the%20zero-shot%20results%20of%20state-of-the-art%20image%0Atext%20spotters%20directly%20can%20achieve%20impressive%20performance.%20However%2C%20owing%20to%0Athe%20domain%20gap%20between%20different%20datasets%2C%20these%20methods%20usually%20obtain%20limited%0Atracking%20trajectories%20on%20extreme%20dataset.%20Fine-tuning%20transformer-based%20text%0Aspotters%20on%20specific%20datasets%20could%20yield%20performance%20enhancements%2C%20albeit%20at%0Athe%20expense%20of%20considerable%20training%20resources.%20In%20this%20paper%2C%20we%20propose%20a%0ALanguage%20Collaboration%20and%20Glyph%20Perception%20Model%2C%20termed%20LOGO%20to%20enhance%20the%0Aperformance%20of%20conventional%20text%20spotters%20through%20the%20integration%20of%20a%20synergy%0Amodule.%20To%20achieve%20this%20goal%2C%20a%20language%20synergy%20classifier%20%28LSC%29%20is%20designed%0Ato%20explicitly%20discern%20text%20instances%20from%20background%20noise%20in%20the%20recognition%0Astage.%20Specially%2C%20the%20language%20synergy%20classifier%20can%20output%20text%20content%20or%0Abackground%20code%20based%20on%20the%20legibility%20of%20text%20regions%2C%20thus%20computing%0Alanguage%20scores.%20Subsequently%2C%20fusion%20scores%20are%20computed%20by%20taking%20the%20average%0Aof%20detection%20scores%20and%20language%20scores%2C%20and%20are%20utilized%20to%20re-score%20the%0Adetection%20results%20before%20tracking.%20By%20the%20re-scoring%20mechanism%2C%20the%20proposed%0ALSC%20facilitates%20the%20detection%20of%20low-resolution%20text%20instances%20while%20filtering%0Aout%20text-like%20regions.%20Besides%2C%20the%20glyph%20supervision%20and%20visual%20position%0Amixture%20module%20are%20proposed%20to%20enhance%20the%20recognition%20accuracy%20of%20noisy%20text%0Aregions%2C%20and%20acquire%20more%20discriminative%20tracking%20features%2C%20respectively.%0AExtensive%20experiments%20on%20public%20benchmarks%20validate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19194v1&entry.124074799=Read"},
{"title": "Continuously Optimizing Radar Placement with Model Predictive Path\n  Integrals", "author": "Michael Potter and Shuo Tang and Paul Ghanem and Milica Stojanovic and Pau Closas and Murat Akcakaya and Ben Wright and Marius Necsoiu and Deniz Erdogmus and Michael Everett and Talees Imbiriba", "abstract": "  Continuously optimizing sensor placement is essential for precise target\nlocalization in various military and civilian applications. While information\ntheory has shown promise in optimizing sensor placement, many studies\noversimplify sensor measurement models or neglect dynamic constraints of mobile\nsensors. To address these challenges, we employ a range measurement model that\nincorporates radar parameters and radar-target distance, coupled with Model\nPredictive Path Integral (MPPI) control to manage complex environmental\nobstacles and dynamic constraints. We compare the proposed approach against\nstationary radars or simplified range measurement models based on the root mean\nsquared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the\ntargets' state. Additionally, we visualize the evolving geometry of radars and\ntargets over time, highlighting areas of highest measurement information gain,\ndemonstrating the strengths of the approach. The proposed strategy outperforms\nstationary radars and simplified range measurement models in target\nlocalization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction\nin the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl\n(MC) trials across all time steps.\n  Code will be made publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2405.18999v1", "date": "2024-05-29", "relevancy": 2.1424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5629}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuously%20Optimizing%20Radar%20Placement%20with%20Model%20Predictive%20Path%0A%20%20Integrals&body=Title%3A%20Continuously%20Optimizing%20Radar%20Placement%20with%20Model%20Predictive%20Path%0A%20%20Integrals%0AAuthor%3A%20Michael%20Potter%20and%20Shuo%20Tang%20and%20Paul%20Ghanem%20and%20Milica%20Stojanovic%20and%20Pau%20Closas%20and%20Murat%20Akcakaya%20and%20Ben%20Wright%20and%20Marius%20Necsoiu%20and%20Deniz%20Erdogmus%20and%20Michael%20Everett%20and%20Talees%20Imbiriba%0AAbstract%3A%20%20%20Continuously%20optimizing%20sensor%20placement%20is%20essential%20for%20precise%20target%0Alocalization%20in%20various%20military%20and%20civilian%20applications.%20While%20information%0Atheory%20has%20shown%20promise%20in%20optimizing%20sensor%20placement%2C%20many%20studies%0Aoversimplify%20sensor%20measurement%20models%20or%20neglect%20dynamic%20constraints%20of%20mobile%0Asensors.%20To%20address%20these%20challenges%2C%20we%20employ%20a%20range%20measurement%20model%20that%0Aincorporates%20radar%20parameters%20and%20radar-target%20distance%2C%20coupled%20with%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20control%20to%20manage%20complex%20environmental%0Aobstacles%20and%20dynamic%20constraints.%20We%20compare%20the%20proposed%20approach%20against%0Astationary%20radars%20or%20simplified%20range%20measurement%20models%20based%20on%20the%20root%20mean%0Asquared%20error%20%28RMSE%29%20of%20the%20Cubature%20Kalman%20Filter%20%28CKF%29%20estimator%20for%20the%0Atargets%27%20state.%20Additionally%2C%20we%20visualize%20the%20evolving%20geometry%20of%20radars%20and%0Atargets%20over%20time%2C%20highlighting%20areas%20of%20highest%20measurement%20information%20gain%2C%0Ademonstrating%20the%20strengths%20of%20the%20approach.%20The%20proposed%20strategy%20outperforms%0Astationary%20radars%20and%20simplified%20range%20measurement%20models%20in%20target%0Alocalization%2C%20achieving%20a%2038-74%25%20reduction%20in%20mean%20RMSE%20and%20a%2033-79%25%20reduction%0Ain%20the%20upper%20tail%20of%20the%2090%25%20Highest%20Density%20Interval%20%28HDI%29%20over%20500%20Monte%20Carl%0A%28MC%29%20trials%20across%20all%20time%20steps.%0A%20%20Code%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuously%2520Optimizing%2520Radar%2520Placement%2520with%2520Model%2520Predictive%2520Path%250A%2520%2520Integrals%26entry.906535625%3DMichael%2520Potter%2520and%2520Shuo%2520Tang%2520and%2520Paul%2520Ghanem%2520and%2520Milica%2520Stojanovic%2520and%2520Pau%2520Closas%2520and%2520Murat%2520Akcakaya%2520and%2520Ben%2520Wright%2520and%2520Marius%2520Necsoiu%2520and%2520Deniz%2520Erdogmus%2520and%2520Michael%2520Everett%2520and%2520Talees%2520Imbiriba%26entry.1292438233%3D%2520%2520Continuously%2520optimizing%2520sensor%2520placement%2520is%2520essential%2520for%2520precise%2520target%250Alocalization%2520in%2520various%2520military%2520and%2520civilian%2520applications.%2520While%2520information%250Atheory%2520has%2520shown%2520promise%2520in%2520optimizing%2520sensor%2520placement%252C%2520many%2520studies%250Aoversimplify%2520sensor%2520measurement%2520models%2520or%2520neglect%2520dynamic%2520constraints%2520of%2520mobile%250Asensors.%2520To%2520address%2520these%2520challenges%252C%2520we%2520employ%2520a%2520range%2520measurement%2520model%2520that%250Aincorporates%2520radar%2520parameters%2520and%2520radar-target%2520distance%252C%2520coupled%2520with%2520Model%250APredictive%2520Path%2520Integral%2520%2528MPPI%2529%2520control%2520to%2520manage%2520complex%2520environmental%250Aobstacles%2520and%2520dynamic%2520constraints.%2520We%2520compare%2520the%2520proposed%2520approach%2520against%250Astationary%2520radars%2520or%2520simplified%2520range%2520measurement%2520models%2520based%2520on%2520the%2520root%2520mean%250Asquared%2520error%2520%2528RMSE%2529%2520of%2520the%2520Cubature%2520Kalman%2520Filter%2520%2528CKF%2529%2520estimator%2520for%2520the%250Atargets%2527%2520state.%2520Additionally%252C%2520we%2520visualize%2520the%2520evolving%2520geometry%2520of%2520radars%2520and%250Atargets%2520over%2520time%252C%2520highlighting%2520areas%2520of%2520highest%2520measurement%2520information%2520gain%252C%250Ademonstrating%2520the%2520strengths%2520of%2520the%2520approach.%2520The%2520proposed%2520strategy%2520outperforms%250Astationary%2520radars%2520and%2520simplified%2520range%2520measurement%2520models%2520in%2520target%250Alocalization%252C%2520achieving%2520a%252038-74%2525%2520reduction%2520in%2520mean%2520RMSE%2520and%2520a%252033-79%2525%2520reduction%250Ain%2520the%2520upper%2520tail%2520of%2520the%252090%2525%2520Highest%2520Density%2520Interval%2520%2528HDI%2529%2520over%2520500%2520Monte%2520Carl%250A%2528MC%2529%2520trials%2520across%2520all%2520time%2520steps.%250A%2520%2520Code%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuously%20Optimizing%20Radar%20Placement%20with%20Model%20Predictive%20Path%0A%20%20Integrals&entry.906535625=Michael%20Potter%20and%20Shuo%20Tang%20and%20Paul%20Ghanem%20and%20Milica%20Stojanovic%20and%20Pau%20Closas%20and%20Murat%20Akcakaya%20and%20Ben%20Wright%20and%20Marius%20Necsoiu%20and%20Deniz%20Erdogmus%20and%20Michael%20Everett%20and%20Talees%20Imbiriba&entry.1292438233=%20%20Continuously%20optimizing%20sensor%20placement%20is%20essential%20for%20precise%20target%0Alocalization%20in%20various%20military%20and%20civilian%20applications.%20While%20information%0Atheory%20has%20shown%20promise%20in%20optimizing%20sensor%20placement%2C%20many%20studies%0Aoversimplify%20sensor%20measurement%20models%20or%20neglect%20dynamic%20constraints%20of%20mobile%0Asensors.%20To%20address%20these%20challenges%2C%20we%20employ%20a%20range%20measurement%20model%20that%0Aincorporates%20radar%20parameters%20and%20radar-target%20distance%2C%20coupled%20with%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20control%20to%20manage%20complex%20environmental%0Aobstacles%20and%20dynamic%20constraints.%20We%20compare%20the%20proposed%20approach%20against%0Astationary%20radars%20or%20simplified%20range%20measurement%20models%20based%20on%20the%20root%20mean%0Asquared%20error%20%28RMSE%29%20of%20the%20Cubature%20Kalman%20Filter%20%28CKF%29%20estimator%20for%20the%0Atargets%27%20state.%20Additionally%2C%20we%20visualize%20the%20evolving%20geometry%20of%20radars%20and%0Atargets%20over%20time%2C%20highlighting%20areas%20of%20highest%20measurement%20information%20gain%2C%0Ademonstrating%20the%20strengths%20of%20the%20approach.%20The%20proposed%20strategy%20outperforms%0Astationary%20radars%20and%20simplified%20range%20measurement%20models%20in%20target%0Alocalization%2C%20achieving%20a%2038-74%25%20reduction%20in%20mean%20RMSE%20and%20a%2033-79%25%20reduction%0Ain%20the%20upper%20tail%20of%20the%2090%25%20Highest%20Density%20Interval%20%28HDI%29%20over%20500%20Monte%20Carl%0A%28MC%29%20trials%20across%20all%20time%20steps.%0A%20%20Code%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18999v1&entry.124074799=Read"},
{"title": "Valid Conformal Prediction for Dynamic GNNs", "author": "Ed Davis and Ian Gallagher and Daniel John Lawson and Patrick Rubin-Delanchy", "abstract": "  Graph neural networks (GNNs) are powerful black-box models which have shown\nimpressive empirical performance. However, without any form of uncertainty\nquantification, it can be difficult to trust such models in high-risk\nscenarios. Conformal prediction aims to address this problem, however, an\nassumption of exchangeability is required for its validity which has limited\nits applicability to static graphs and transductive regimes. We propose to use\nunfolding, which allows any existing static GNN to output a dynamic graph\nembedding with exchangeability properties. Using this, we extend the validity\nof conformal prediction to dynamic GNNs in both transductive and semi-inductive\nregimes. We provide a theoretical guarantee of valid conformal prediction in\nthese cases and demonstrate the empirical validity, as well as the performance\ngains, of unfolded GNNs against standard GNN architectures on both simulated\nand real datasets.\n", "link": "http://arxiv.org/abs/2405.19230v1", "date": "2024-05-29", "relevancy": 2.1403, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5261}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valid%20Conformal%20Prediction%20for%20Dynamic%20GNNs&body=Title%3A%20Valid%20Conformal%20Prediction%20for%20Dynamic%20GNNs%0AAuthor%3A%20Ed%20Davis%20and%20Ian%20Gallagher%20and%20Daniel%20John%20Lawson%20and%20Patrick%20Rubin-Delanchy%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20black-box%20models%20which%20have%20shown%0Aimpressive%20empirical%20performance.%20However%2C%20without%20any%20form%20of%20uncertainty%0Aquantification%2C%20it%20can%20be%20difficult%20to%20trust%20such%20models%20in%20high-risk%0Ascenarios.%20Conformal%20prediction%20aims%20to%20address%20this%20problem%2C%20however%2C%20an%0Aassumption%20of%20exchangeability%20is%20required%20for%20its%20validity%20which%20has%20limited%0Aits%20applicability%20to%20static%20graphs%20and%20transductive%20regimes.%20We%20propose%20to%20use%0Aunfolding%2C%20which%20allows%20any%20existing%20static%20GNN%20to%20output%20a%20dynamic%20graph%0Aembedding%20with%20exchangeability%20properties.%20Using%20this%2C%20we%20extend%20the%20validity%0Aof%20conformal%20prediction%20to%20dynamic%20GNNs%20in%20both%20transductive%20and%20semi-inductive%0Aregimes.%20We%20provide%20a%20theoretical%20guarantee%20of%20valid%20conformal%20prediction%20in%0Athese%20cases%20and%20demonstrate%20the%20empirical%20validity%2C%20as%20well%20as%20the%20performance%0Agains%2C%20of%20unfolded%20GNNs%20against%20standard%20GNN%20architectures%20on%20both%20simulated%0Aand%20real%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValid%2520Conformal%2520Prediction%2520for%2520Dynamic%2520GNNs%26entry.906535625%3DEd%2520Davis%2520and%2520Ian%2520Gallagher%2520and%2520Daniel%2520John%2520Lawson%2520and%2520Patrick%2520Rubin-Delanchy%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520powerful%2520black-box%2520models%2520which%2520have%2520shown%250Aimpressive%2520empirical%2520performance.%2520However%252C%2520without%2520any%2520form%2520of%2520uncertainty%250Aquantification%252C%2520it%2520can%2520be%2520difficult%2520to%2520trust%2520such%2520models%2520in%2520high-risk%250Ascenarios.%2520Conformal%2520prediction%2520aims%2520to%2520address%2520this%2520problem%252C%2520however%252C%2520an%250Aassumption%2520of%2520exchangeability%2520is%2520required%2520for%2520its%2520validity%2520which%2520has%2520limited%250Aits%2520applicability%2520to%2520static%2520graphs%2520and%2520transductive%2520regimes.%2520We%2520propose%2520to%2520use%250Aunfolding%252C%2520which%2520allows%2520any%2520existing%2520static%2520GNN%2520to%2520output%2520a%2520dynamic%2520graph%250Aembedding%2520with%2520exchangeability%2520properties.%2520Using%2520this%252C%2520we%2520extend%2520the%2520validity%250Aof%2520conformal%2520prediction%2520to%2520dynamic%2520GNNs%2520in%2520both%2520transductive%2520and%2520semi-inductive%250Aregimes.%2520We%2520provide%2520a%2520theoretical%2520guarantee%2520of%2520valid%2520conformal%2520prediction%2520in%250Athese%2520cases%2520and%2520demonstrate%2520the%2520empirical%2520validity%252C%2520as%2520well%2520as%2520the%2520performance%250Agains%252C%2520of%2520unfolded%2520GNNs%2520against%2520standard%2520GNN%2520architectures%2520on%2520both%2520simulated%250Aand%2520real%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valid%20Conformal%20Prediction%20for%20Dynamic%20GNNs&entry.906535625=Ed%20Davis%20and%20Ian%20Gallagher%20and%20Daniel%20John%20Lawson%20and%20Patrick%20Rubin-Delanchy&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20black-box%20models%20which%20have%20shown%0Aimpressive%20empirical%20performance.%20However%2C%20without%20any%20form%20of%20uncertainty%0Aquantification%2C%20it%20can%20be%20difficult%20to%20trust%20such%20models%20in%20high-risk%0Ascenarios.%20Conformal%20prediction%20aims%20to%20address%20this%20problem%2C%20however%2C%20an%0Aassumption%20of%20exchangeability%20is%20required%20for%20its%20validity%20which%20has%20limited%0Aits%20applicability%20to%20static%20graphs%20and%20transductive%20regimes.%20We%20propose%20to%20use%0Aunfolding%2C%20which%20allows%20any%20existing%20static%20GNN%20to%20output%20a%20dynamic%20graph%0Aembedding%20with%20exchangeability%20properties.%20Using%20this%2C%20we%20extend%20the%20validity%0Aof%20conformal%20prediction%20to%20dynamic%20GNNs%20in%20both%20transductive%20and%20semi-inductive%0Aregimes.%20We%20provide%20a%20theoretical%20guarantee%20of%20valid%20conformal%20prediction%20in%0Athese%20cases%20and%20demonstrate%20the%20empirical%20validity%2C%20as%20well%20as%20the%20performance%0Agains%2C%20of%20unfolded%20GNNs%20against%20standard%20GNN%20architectures%20on%20both%20simulated%0Aand%20real%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19230v1&entry.124074799=Read"},
{"title": "FedMAP: Unlocking Potential in Personalized Federated Learning through\n  Bi-Level MAP Optimization", "author": "Fan Zhang and Carlos Esteve-Yag\u00fce and S\u00f6ren Dittmer and Carola-Bibiane Sch\u00f6nlieb and Michael Roberts", "abstract": "  Federated Learning (FL) enables collaborative training of machine learning\nmodels on decentralized data while preserving data privacy. However, data\nacross clients often differs significantly due to class imbalance, feature\ndistribution skew, sample size imbalance, and other phenomena. Leveraging\ninformation from these not identically distributed (non-IID) datasets poses\nsubstantial challenges. FL methods based on a single global model cannot\neffectively capture the variations in client data and underperform in non-IID\nsettings. Consequently, Personalized FL (PFL) approaches that adapt to each\nclient's data distribution but leverage other clients' data are essential but\ncurrently underexplored. We propose a novel Bayesian PFL framework using\nbi-level optimization to tackle the data heterogeneity challenges. Our proposed\nframework utilizes the global model as a prior distribution within a Maximum A\nPosteriori (MAP) estimation of personalized client models. This approach\nfacilitates PFL by integrating shared knowledge from the prior, thereby\nenhancing local model performance, generalization ability, and communication\nefficiency. We extensively evaluated our bi-level optimization approach on\nreal-world and synthetic datasets, demonstrating significant improvements in\nmodel accuracy compared to existing methods while reducing communication\noverhead. This study contributes to PFL by establishing a solid theoretical\nfoundation for the proposed method and offering a robust, ready-to-use\nframework that effectively addresses the challenges posed by non-IID data in\nFL.\n", "link": "http://arxiv.org/abs/2405.19000v1", "date": "2024-05-29", "relevancy": 2.1403, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMAP%3A%20Unlocking%20Potential%20in%20Personalized%20Federated%20Learning%20through%0A%20%20Bi-Level%20MAP%20Optimization&body=Title%3A%20FedMAP%3A%20Unlocking%20Potential%20in%20Personalized%20Federated%20Learning%20through%0A%20%20Bi-Level%20MAP%20Optimization%0AAuthor%3A%20Fan%20Zhang%20and%20Carlos%20Esteve-Yag%C3%BCe%20and%20S%C3%B6ren%20Dittmer%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Michael%20Roberts%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20machine%20learning%0Amodels%20on%20decentralized%20data%20while%20preserving%20data%20privacy.%20However%2C%20data%0Aacross%20clients%20often%20differs%20significantly%20due%20to%20class%20imbalance%2C%20feature%0Adistribution%20skew%2C%20sample%20size%20imbalance%2C%20and%20other%20phenomena.%20Leveraging%0Ainformation%20from%20these%20not%20identically%20distributed%20%28non-IID%29%20datasets%20poses%0Asubstantial%20challenges.%20FL%20methods%20based%20on%20a%20single%20global%20model%20cannot%0Aeffectively%20capture%20the%20variations%20in%20client%20data%20and%20underperform%20in%20non-IID%0Asettings.%20Consequently%2C%20Personalized%20FL%20%28PFL%29%20approaches%20that%20adapt%20to%20each%0Aclient%27s%20data%20distribution%20but%20leverage%20other%20clients%27%20data%20are%20essential%20but%0Acurrently%20underexplored.%20We%20propose%20a%20novel%20Bayesian%20PFL%20framework%20using%0Abi-level%20optimization%20to%20tackle%20the%20data%20heterogeneity%20challenges.%20Our%20proposed%0Aframework%20utilizes%20the%20global%20model%20as%20a%20prior%20distribution%20within%20a%20Maximum%20A%0APosteriori%20%28MAP%29%20estimation%20of%20personalized%20client%20models.%20This%20approach%0Afacilitates%20PFL%20by%20integrating%20shared%20knowledge%20from%20the%20prior%2C%20thereby%0Aenhancing%20local%20model%20performance%2C%20generalization%20ability%2C%20and%20communication%0Aefficiency.%20We%20extensively%20evaluated%20our%20bi-level%20optimization%20approach%20on%0Areal-world%20and%20synthetic%20datasets%2C%20demonstrating%20significant%20improvements%20in%0Amodel%20accuracy%20compared%20to%20existing%20methods%20while%20reducing%20communication%0Aoverhead.%20This%20study%20contributes%20to%20PFL%20by%20establishing%20a%20solid%20theoretical%0Afoundation%20for%20the%20proposed%20method%20and%20offering%20a%20robust%2C%20ready-to-use%0Aframework%20that%20effectively%20addresses%20the%20challenges%20posed%20by%20non-IID%20data%20in%0AFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMAP%253A%2520Unlocking%2520Potential%2520in%2520Personalized%2520Federated%2520Learning%2520through%250A%2520%2520Bi-Level%2520MAP%2520Optimization%26entry.906535625%3DFan%2520Zhang%2520and%2520Carlos%2520Esteve-Yag%25C3%25BCe%2520and%2520S%25C3%25B6ren%2520Dittmer%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Michael%2520Roberts%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520of%2520machine%2520learning%250Amodels%2520on%2520decentralized%2520data%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520data%250Aacross%2520clients%2520often%2520differs%2520significantly%2520due%2520to%2520class%2520imbalance%252C%2520feature%250Adistribution%2520skew%252C%2520sample%2520size%2520imbalance%252C%2520and%2520other%2520phenomena.%2520Leveraging%250Ainformation%2520from%2520these%2520not%2520identically%2520distributed%2520%2528non-IID%2529%2520datasets%2520poses%250Asubstantial%2520challenges.%2520FL%2520methods%2520based%2520on%2520a%2520single%2520global%2520model%2520cannot%250Aeffectively%2520capture%2520the%2520variations%2520in%2520client%2520data%2520and%2520underperform%2520in%2520non-IID%250Asettings.%2520Consequently%252C%2520Personalized%2520FL%2520%2528PFL%2529%2520approaches%2520that%2520adapt%2520to%2520each%250Aclient%2527s%2520data%2520distribution%2520but%2520leverage%2520other%2520clients%2527%2520data%2520are%2520essential%2520but%250Acurrently%2520underexplored.%2520We%2520propose%2520a%2520novel%2520Bayesian%2520PFL%2520framework%2520using%250Abi-level%2520optimization%2520to%2520tackle%2520the%2520data%2520heterogeneity%2520challenges.%2520Our%2520proposed%250Aframework%2520utilizes%2520the%2520global%2520model%2520as%2520a%2520prior%2520distribution%2520within%2520a%2520Maximum%2520A%250APosteriori%2520%2528MAP%2529%2520estimation%2520of%2520personalized%2520client%2520models.%2520This%2520approach%250Afacilitates%2520PFL%2520by%2520integrating%2520shared%2520knowledge%2520from%2520the%2520prior%252C%2520thereby%250Aenhancing%2520local%2520model%2520performance%252C%2520generalization%2520ability%252C%2520and%2520communication%250Aefficiency.%2520We%2520extensively%2520evaluated%2520our%2520bi-level%2520optimization%2520approach%2520on%250Areal-world%2520and%2520synthetic%2520datasets%252C%2520demonstrating%2520significant%2520improvements%2520in%250Amodel%2520accuracy%2520compared%2520to%2520existing%2520methods%2520while%2520reducing%2520communication%250Aoverhead.%2520This%2520study%2520contributes%2520to%2520PFL%2520by%2520establishing%2520a%2520solid%2520theoretical%250Afoundation%2520for%2520the%2520proposed%2520method%2520and%2520offering%2520a%2520robust%252C%2520ready-to-use%250Aframework%2520that%2520effectively%2520addresses%2520the%2520challenges%2520posed%2520by%2520non-IID%2520data%2520in%250AFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMAP%3A%20Unlocking%20Potential%20in%20Personalized%20Federated%20Learning%20through%0A%20%20Bi-Level%20MAP%20Optimization&entry.906535625=Fan%20Zhang%20and%20Carlos%20Esteve-Yag%C3%BCe%20and%20S%C3%B6ren%20Dittmer%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Michael%20Roberts&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20machine%20learning%0Amodels%20on%20decentralized%20data%20while%20preserving%20data%20privacy.%20However%2C%20data%0Aacross%20clients%20often%20differs%20significantly%20due%20to%20class%20imbalance%2C%20feature%0Adistribution%20skew%2C%20sample%20size%20imbalance%2C%20and%20other%20phenomena.%20Leveraging%0Ainformation%20from%20these%20not%20identically%20distributed%20%28non-IID%29%20datasets%20poses%0Asubstantial%20challenges.%20FL%20methods%20based%20on%20a%20single%20global%20model%20cannot%0Aeffectively%20capture%20the%20variations%20in%20client%20data%20and%20underperform%20in%20non-IID%0Asettings.%20Consequently%2C%20Personalized%20FL%20%28PFL%29%20approaches%20that%20adapt%20to%20each%0Aclient%27s%20data%20distribution%20but%20leverage%20other%20clients%27%20data%20are%20essential%20but%0Acurrently%20underexplored.%20We%20propose%20a%20novel%20Bayesian%20PFL%20framework%20using%0Abi-level%20optimization%20to%20tackle%20the%20data%20heterogeneity%20challenges.%20Our%20proposed%0Aframework%20utilizes%20the%20global%20model%20as%20a%20prior%20distribution%20within%20a%20Maximum%20A%0APosteriori%20%28MAP%29%20estimation%20of%20personalized%20client%20models.%20This%20approach%0Afacilitates%20PFL%20by%20integrating%20shared%20knowledge%20from%20the%20prior%2C%20thereby%0Aenhancing%20local%20model%20performance%2C%20generalization%20ability%2C%20and%20communication%0Aefficiency.%20We%20extensively%20evaluated%20our%20bi-level%20optimization%20approach%20on%0Areal-world%20and%20synthetic%20datasets%2C%20demonstrating%20significant%20improvements%20in%0Amodel%20accuracy%20compared%20to%20existing%20methods%20while%20reducing%20communication%0Aoverhead.%20This%20study%20contributes%20to%20PFL%20by%20establishing%20a%20solid%20theoretical%0Afoundation%20for%20the%20proposed%20method%20and%20offering%20a%20robust%2C%20ready-to-use%0Aframework%20that%20effectively%20addresses%20the%20challenges%20posed%20by%20non-IID%20data%20in%0AFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19000v1&entry.124074799=Read"},
{"title": "High-Performance Hybrid Algorithm for Minimum Sum-of-Squares Clustering\n  of Infinitely Tall Data", "author": "Ravil Mussabayev and Rustam Mussabayev", "abstract": "  This paper introduces a novel formulation of the clustering problem, namely\nthe Minimum Sum-of-Squares Clustering of Infinitely Tall Data (MSSC-ITD), and\npresents HPClust, an innovative set of hybrid parallel approaches for its\neffective solution. By utilizing modern high-performance computing techniques,\nHPClust enhances key clustering metrics: effectiveness, computational\nefficiency, and scalability. In contrast to vanilla data parallelism, which\nonly accelerates processing time through the MapReduce framework, our approach\nunlocks superior performance by leveraging the multi-strategy\ncompetitive-cooperative parallelism and intricate properties of the objective\nfunction landscape. Unlike other available algorithms that struggle to scale,\nour algorithm is inherently parallel in nature, improving solution quality\nthrough increased scalability and parallelism, and outperforming even advanced\nalgorithms designed for small and medium-sized datasets. Our evaluation of\nHPClust, featuring four parallel strategies, demonstrates its superiority over\ntraditional and cutting-edge methods by offering better performance in the key\nmetrics. These results also show that parallel processing not only enhances the\nclustering efficiency, but the accuracy as well. Additionally, we explore the\nbalance between computational efficiency and clustering quality, providing\ninsights into optimal parallel strategies based on dataset specifics and\nresource availability. This research advances our understanding of parallelism\nin clustering algorithms, demonstrating that a judicious hybridization of\nadvanced parallel approaches yields optimal results for MSSC-ITD. Experiments\non synthetic data further confirm HPClust's exceptional scalability and\nrobustness to noise.\n", "link": "http://arxiv.org/abs/2311.04517v3", "date": "2024-05-29", "relevancy": 2.1396, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4296}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4283}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&body=Title%3A%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data%0AAuthor%3A%20Ravil%20Mussabayev%20and%20Rustam%20Mussabayev%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04517v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Performance%2520Hybrid%2520Algorithm%2520for%2520Minimum%2520Sum-of-Squares%2520Clustering%250A%2520%2520of%2520Infinitely%2520Tall%2520Data%26entry.906535625%3DRavil%2520Mussabayev%2520and%2520Rustam%2520Mussabayev%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520formulation%2520of%2520the%2520clustering%2520problem%252C%2520namely%250Athe%2520Minimum%2520Sum-of-Squares%2520Clustering%2520of%2520Infinitely%2520Tall%2520Data%2520%2528MSSC-ITD%2529%252C%2520and%250Apresents%2520HPClust%252C%2520an%2520innovative%2520set%2520of%2520hybrid%2520parallel%2520approaches%2520for%2520its%250Aeffective%2520solution.%2520By%2520utilizing%2520modern%2520high-performance%2520computing%2520techniques%252C%250AHPClust%2520enhances%2520key%2520clustering%2520metrics%253A%2520effectiveness%252C%2520computational%250Aefficiency%252C%2520and%2520scalability.%2520In%2520contrast%2520to%2520vanilla%2520data%2520parallelism%252C%2520which%250Aonly%2520accelerates%2520processing%2520time%2520through%2520the%2520MapReduce%2520framework%252C%2520our%2520approach%250Aunlocks%2520superior%2520performance%2520by%2520leveraging%2520the%2520multi-strategy%250Acompetitive-cooperative%2520parallelism%2520and%2520intricate%2520properties%2520of%2520the%2520objective%250Afunction%2520landscape.%2520Unlike%2520other%2520available%2520algorithms%2520that%2520struggle%2520to%2520scale%252C%250Aour%2520algorithm%2520is%2520inherently%2520parallel%2520in%2520nature%252C%2520improving%2520solution%2520quality%250Athrough%2520increased%2520scalability%2520and%2520parallelism%252C%2520and%2520outperforming%2520even%2520advanced%250Aalgorithms%2520designed%2520for%2520small%2520and%2520medium-sized%2520datasets.%2520Our%2520evaluation%2520of%250AHPClust%252C%2520featuring%2520four%2520parallel%2520strategies%252C%2520demonstrates%2520its%2520superiority%2520over%250Atraditional%2520and%2520cutting-edge%2520methods%2520by%2520offering%2520better%2520performance%2520in%2520the%2520key%250Ametrics.%2520These%2520results%2520also%2520show%2520that%2520parallel%2520processing%2520not%2520only%2520enhances%2520the%250Aclustering%2520efficiency%252C%2520but%2520the%2520accuracy%2520as%2520well.%2520Additionally%252C%2520we%2520explore%2520the%250Abalance%2520between%2520computational%2520efficiency%2520and%2520clustering%2520quality%252C%2520providing%250Ainsights%2520into%2520optimal%2520parallel%2520strategies%2520based%2520on%2520dataset%2520specifics%2520and%250Aresource%2520availability.%2520This%2520research%2520advances%2520our%2520understanding%2520of%2520parallelism%250Ain%2520clustering%2520algorithms%252C%2520demonstrating%2520that%2520a%2520judicious%2520hybridization%2520of%250Aadvanced%2520parallel%2520approaches%2520yields%2520optimal%2520results%2520for%2520MSSC-ITD.%2520Experiments%250Aon%2520synthetic%2520data%2520further%2520confirm%2520HPClust%2527s%2520exceptional%2520scalability%2520and%250Arobustness%2520to%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04517v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&entry.906535625=Ravil%20Mussabayev%20and%20Rustam%20Mussabayev&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04517v3&entry.124074799=Read"},
{"title": "Masked Autoencoders are PDE Learners", "author": "Anthony Zhou and Amir Barati Farimani", "abstract": "  Neural solvers for partial differential equations (PDEs) have great potential\nto generate fast and accurate physics solutions, yet their practicality is\ncurrently limited by their generalizability. PDEs evolve over broad scales and\nexhibit diverse behaviors; predicting these phenomena will require learning\nrepresentations across a wide variety of inputs which may encompass different\ncoefficients, boundary conditions, resolutions, or even equations. As a step\ntowards generalizable PDE modeling, we adapt masked pretraining for physics\nproblems. Through self-supervised learning across PDEs, masked autoencoders can\nconsolidate heterogeneous physics to learn meaningful latent representations\nand perform latent PDE arithmetic in this space. Furthermore, we demonstrate\nthat masked pretraining can improve PDE coefficient regression and the\nclassification of PDE features. Lastly, conditioning neural solvers on learned\nlatent representations can improve time-stepping and super-resolution\nperformance across a variety of coefficients, discretizations, or boundary\nconditions, as well as on unseen PDEs. We hope that masked pretraining can\nemerge as a unifying method across large, unlabeled, and heterogeneous datasets\nto learn latent physics at scale.\n", "link": "http://arxiv.org/abs/2403.17728v2", "date": "2024-05-29", "relevancy": 2.139, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Autoencoders%20are%20PDE%20Learners&body=Title%3A%20Masked%20Autoencoders%20are%20PDE%20Learners%0AAuthor%3A%20Anthony%20Zhou%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Neural%20solvers%20for%20partial%20differential%20equations%20%28PDEs%29%20have%20great%20potential%0Ato%20generate%20fast%20and%20accurate%20physics%20solutions%2C%20yet%20their%20practicality%20is%0Acurrently%20limited%20by%20their%20generalizability.%20PDEs%20evolve%20over%20broad%20scales%20and%0Aexhibit%20diverse%20behaviors%3B%20predicting%20these%20phenomena%20will%20require%20learning%0Arepresentations%20across%20a%20wide%20variety%20of%20inputs%20which%20may%20encompass%20different%0Acoefficients%2C%20boundary%20conditions%2C%20resolutions%2C%20or%20even%20equations.%20As%20a%20step%0Atowards%20generalizable%20PDE%20modeling%2C%20we%20adapt%20masked%20pretraining%20for%20physics%0Aproblems.%20Through%20self-supervised%20learning%20across%20PDEs%2C%20masked%20autoencoders%20can%0Aconsolidate%20heterogeneous%20physics%20to%20learn%20meaningful%20latent%20representations%0Aand%20perform%20latent%20PDE%20arithmetic%20in%20this%20space.%20Furthermore%2C%20we%20demonstrate%0Athat%20masked%20pretraining%20can%20improve%20PDE%20coefficient%20regression%20and%20the%0Aclassification%20of%20PDE%20features.%20Lastly%2C%20conditioning%20neural%20solvers%20on%20learned%0Alatent%20representations%20can%20improve%20time-stepping%20and%20super-resolution%0Aperformance%20across%20a%20variety%20of%20coefficients%2C%20discretizations%2C%20or%20boundary%0Aconditions%2C%20as%20well%20as%20on%20unseen%20PDEs.%20We%20hope%20that%20masked%20pretraining%20can%0Aemerge%20as%20a%20unifying%20method%20across%20large%2C%20unlabeled%2C%20and%20heterogeneous%20datasets%0Ato%20learn%20latent%20physics%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Autoencoders%2520are%2520PDE%2520Learners%26entry.906535625%3DAnthony%2520Zhou%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Neural%2520solvers%2520for%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520have%2520great%2520potential%250Ato%2520generate%2520fast%2520and%2520accurate%2520physics%2520solutions%252C%2520yet%2520their%2520practicality%2520is%250Acurrently%2520limited%2520by%2520their%2520generalizability.%2520PDEs%2520evolve%2520over%2520broad%2520scales%2520and%250Aexhibit%2520diverse%2520behaviors%253B%2520predicting%2520these%2520phenomena%2520will%2520require%2520learning%250Arepresentations%2520across%2520a%2520wide%2520variety%2520of%2520inputs%2520which%2520may%2520encompass%2520different%250Acoefficients%252C%2520boundary%2520conditions%252C%2520resolutions%252C%2520or%2520even%2520equations.%2520As%2520a%2520step%250Atowards%2520generalizable%2520PDE%2520modeling%252C%2520we%2520adapt%2520masked%2520pretraining%2520for%2520physics%250Aproblems.%2520Through%2520self-supervised%2520learning%2520across%2520PDEs%252C%2520masked%2520autoencoders%2520can%250Aconsolidate%2520heterogeneous%2520physics%2520to%2520learn%2520meaningful%2520latent%2520representations%250Aand%2520perform%2520latent%2520PDE%2520arithmetic%2520in%2520this%2520space.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520masked%2520pretraining%2520can%2520improve%2520PDE%2520coefficient%2520regression%2520and%2520the%250Aclassification%2520of%2520PDE%2520features.%2520Lastly%252C%2520conditioning%2520neural%2520solvers%2520on%2520learned%250Alatent%2520representations%2520can%2520improve%2520time-stepping%2520and%2520super-resolution%250Aperformance%2520across%2520a%2520variety%2520of%2520coefficients%252C%2520discretizations%252C%2520or%2520boundary%250Aconditions%252C%2520as%2520well%2520as%2520on%2520unseen%2520PDEs.%2520We%2520hope%2520that%2520masked%2520pretraining%2520can%250Aemerge%2520as%2520a%2520unifying%2520method%2520across%2520large%252C%2520unlabeled%252C%2520and%2520heterogeneous%2520datasets%250Ato%2520learn%2520latent%2520physics%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Autoencoders%20are%20PDE%20Learners&entry.906535625=Anthony%20Zhou%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Neural%20solvers%20for%20partial%20differential%20equations%20%28PDEs%29%20have%20great%20potential%0Ato%20generate%20fast%20and%20accurate%20physics%20solutions%2C%20yet%20their%20practicality%20is%0Acurrently%20limited%20by%20their%20generalizability.%20PDEs%20evolve%20over%20broad%20scales%20and%0Aexhibit%20diverse%20behaviors%3B%20predicting%20these%20phenomena%20will%20require%20learning%0Arepresentations%20across%20a%20wide%20variety%20of%20inputs%20which%20may%20encompass%20different%0Acoefficients%2C%20boundary%20conditions%2C%20resolutions%2C%20or%20even%20equations.%20As%20a%20step%0Atowards%20generalizable%20PDE%20modeling%2C%20we%20adapt%20masked%20pretraining%20for%20physics%0Aproblems.%20Through%20self-supervised%20learning%20across%20PDEs%2C%20masked%20autoencoders%20can%0Aconsolidate%20heterogeneous%20physics%20to%20learn%20meaningful%20latent%20representations%0Aand%20perform%20latent%20PDE%20arithmetic%20in%20this%20space.%20Furthermore%2C%20we%20demonstrate%0Athat%20masked%20pretraining%20can%20improve%20PDE%20coefficient%20regression%20and%20the%0Aclassification%20of%20PDE%20features.%20Lastly%2C%20conditioning%20neural%20solvers%20on%20learned%0Alatent%20representations%20can%20improve%20time-stepping%20and%20super-resolution%0Aperformance%20across%20a%20variety%20of%20coefficients%2C%20discretizations%2C%20or%20boundary%0Aconditions%2C%20as%20well%20as%20on%20unseen%20PDEs.%20We%20hope%20that%20masked%20pretraining%20can%0Aemerge%20as%20a%20unifying%20method%20across%20large%2C%20unlabeled%2C%20and%20heterogeneous%20datasets%0Ato%20learn%20latent%20physics%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17728v2&entry.124074799=Read"},
{"title": "Auto-selected Knowledge Adapters for Lifelong Person Re-identification", "author": "Xuelin Qian and Ruiqi Wu and Gong Cheng and Junwei Han", "abstract": "  Lifelong Person Re-Identification (LReID) extends traditional ReID by\nrequiring systems to continually learn from non-overlapping datasets across\ndifferent times and locations, adapting to new identities while preserving\nknowledge of previous ones. Existing approaches, either rehearsal-free or\nrehearsal-based, still suffer from the problem of catastrophic forgetting since\nthey try to cram diverse knowledge into one fixed model. To overcome this\nlimitation, we introduce a novel framework AdalReID, that adopts knowledge\nadapters and a parameter-free auto-selection mechanism for lifelong learning.\nConcretely, we incrementally build distinct adapters to learn domain-specific\nknowledge at each step, which can effectively learn and preserve knowledge\nacross different datasets. Meanwhile, the proposed auto-selection strategy\nadaptively calculates the knowledge similarity between the input set and the\nadapters. On the one hand, the appropriate adapters are selected for the inputs\nto process ReID, and on the other hand, the knowledge interaction and fusion\nbetween adapters are enhanced to improve the generalization ability of the\nmodel. Extensive experiments are conducted to demonstrate the superiority of\nour AdalReID, which significantly outperforms SOTAs by about 10$\\sim$20\\% mAP\non both seen and unseen domains.\n", "link": "http://arxiv.org/abs/2405.19005v1", "date": "2024-05-29", "relevancy": 2.1375, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5272}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-selected%20Knowledge%20Adapters%20for%20Lifelong%20Person%20Re-identification&body=Title%3A%20Auto-selected%20Knowledge%20Adapters%20for%20Lifelong%20Person%20Re-identification%0AAuthor%3A%20Xuelin%20Qian%20and%20Ruiqi%20Wu%20and%20Gong%20Cheng%20and%20Junwei%20Han%0AAbstract%3A%20%20%20Lifelong%20Person%20Re-Identification%20%28LReID%29%20extends%20traditional%20ReID%20by%0Arequiring%20systems%20to%20continually%20learn%20from%20non-overlapping%20datasets%20across%0Adifferent%20times%20and%20locations%2C%20adapting%20to%20new%20identities%20while%20preserving%0Aknowledge%20of%20previous%20ones.%20Existing%20approaches%2C%20either%20rehearsal-free%20or%0Arehearsal-based%2C%20still%20suffer%20from%20the%20problem%20of%20catastrophic%20forgetting%20since%0Athey%20try%20to%20cram%20diverse%20knowledge%20into%20one%20fixed%20model.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20a%20novel%20framework%20AdalReID%2C%20that%20adopts%20knowledge%0Aadapters%20and%20a%20parameter-free%20auto-selection%20mechanism%20for%20lifelong%20learning.%0AConcretely%2C%20we%20incrementally%20build%20distinct%20adapters%20to%20learn%20domain-specific%0Aknowledge%20at%20each%20step%2C%20which%20can%20effectively%20learn%20and%20preserve%20knowledge%0Aacross%20different%20datasets.%20Meanwhile%2C%20the%20proposed%20auto-selection%20strategy%0Aadaptively%20calculates%20the%20knowledge%20similarity%20between%20the%20input%20set%20and%20the%0Aadapters.%20On%20the%20one%20hand%2C%20the%20appropriate%20adapters%20are%20selected%20for%20the%20inputs%0Ato%20process%20ReID%2C%20and%20on%20the%20other%20hand%2C%20the%20knowledge%20interaction%20and%20fusion%0Abetween%20adapters%20are%20enhanced%20to%20improve%20the%20generalization%20ability%20of%20the%0Amodel.%20Extensive%20experiments%20are%20conducted%20to%20demonstrate%20the%20superiority%20of%0Aour%20AdalReID%2C%20which%20significantly%20outperforms%20SOTAs%20by%20about%2010%24%5Csim%2420%5C%25%20mAP%0Aon%20both%20seen%20and%20unseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-selected%2520Knowledge%2520Adapters%2520for%2520Lifelong%2520Person%2520Re-identification%26entry.906535625%3DXuelin%2520Qian%2520and%2520Ruiqi%2520Wu%2520and%2520Gong%2520Cheng%2520and%2520Junwei%2520Han%26entry.1292438233%3D%2520%2520Lifelong%2520Person%2520Re-Identification%2520%2528LReID%2529%2520extends%2520traditional%2520ReID%2520by%250Arequiring%2520systems%2520to%2520continually%2520learn%2520from%2520non-overlapping%2520datasets%2520across%250Adifferent%2520times%2520and%2520locations%252C%2520adapting%2520to%2520new%2520identities%2520while%2520preserving%250Aknowledge%2520of%2520previous%2520ones.%2520Existing%2520approaches%252C%2520either%2520rehearsal-free%2520or%250Arehearsal-based%252C%2520still%2520suffer%2520from%2520the%2520problem%2520of%2520catastrophic%2520forgetting%2520since%250Athey%2520try%2520to%2520cram%2520diverse%2520knowledge%2520into%2520one%2520fixed%2520model.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520AdalReID%252C%2520that%2520adopts%2520knowledge%250Aadapters%2520and%2520a%2520parameter-free%2520auto-selection%2520mechanism%2520for%2520lifelong%2520learning.%250AConcretely%252C%2520we%2520incrementally%2520build%2520distinct%2520adapters%2520to%2520learn%2520domain-specific%250Aknowledge%2520at%2520each%2520step%252C%2520which%2520can%2520effectively%2520learn%2520and%2520preserve%2520knowledge%250Aacross%2520different%2520datasets.%2520Meanwhile%252C%2520the%2520proposed%2520auto-selection%2520strategy%250Aadaptively%2520calculates%2520the%2520knowledge%2520similarity%2520between%2520the%2520input%2520set%2520and%2520the%250Aadapters.%2520On%2520the%2520one%2520hand%252C%2520the%2520appropriate%2520adapters%2520are%2520selected%2520for%2520the%2520inputs%250Ato%2520process%2520ReID%252C%2520and%2520on%2520the%2520other%2520hand%252C%2520the%2520knowledge%2520interaction%2520and%2520fusion%250Abetween%2520adapters%2520are%2520enhanced%2520to%2520improve%2520the%2520generalization%2520ability%2520of%2520the%250Amodel.%2520Extensive%2520experiments%2520are%2520conducted%2520to%2520demonstrate%2520the%2520superiority%2520of%250Aour%2520AdalReID%252C%2520which%2520significantly%2520outperforms%2520SOTAs%2520by%2520about%252010%2524%255Csim%252420%255C%2525%2520mAP%250Aon%2520both%2520seen%2520and%2520unseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-selected%20Knowledge%20Adapters%20for%20Lifelong%20Person%20Re-identification&entry.906535625=Xuelin%20Qian%20and%20Ruiqi%20Wu%20and%20Gong%20Cheng%20and%20Junwei%20Han&entry.1292438233=%20%20Lifelong%20Person%20Re-Identification%20%28LReID%29%20extends%20traditional%20ReID%20by%0Arequiring%20systems%20to%20continually%20learn%20from%20non-overlapping%20datasets%20across%0Adifferent%20times%20and%20locations%2C%20adapting%20to%20new%20identities%20while%20preserving%0Aknowledge%20of%20previous%20ones.%20Existing%20approaches%2C%20either%20rehearsal-free%20or%0Arehearsal-based%2C%20still%20suffer%20from%20the%20problem%20of%20catastrophic%20forgetting%20since%0Athey%20try%20to%20cram%20diverse%20knowledge%20into%20one%20fixed%20model.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20a%20novel%20framework%20AdalReID%2C%20that%20adopts%20knowledge%0Aadapters%20and%20a%20parameter-free%20auto-selection%20mechanism%20for%20lifelong%20learning.%0AConcretely%2C%20we%20incrementally%20build%20distinct%20adapters%20to%20learn%20domain-specific%0Aknowledge%20at%20each%20step%2C%20which%20can%20effectively%20learn%20and%20preserve%20knowledge%0Aacross%20different%20datasets.%20Meanwhile%2C%20the%20proposed%20auto-selection%20strategy%0Aadaptively%20calculates%20the%20knowledge%20similarity%20between%20the%20input%20set%20and%20the%0Aadapters.%20On%20the%20one%20hand%2C%20the%20appropriate%20adapters%20are%20selected%20for%20the%20inputs%0Ato%20process%20ReID%2C%20and%20on%20the%20other%20hand%2C%20the%20knowledge%20interaction%20and%20fusion%0Abetween%20adapters%20are%20enhanced%20to%20improve%20the%20generalization%20ability%20of%20the%0Amodel.%20Extensive%20experiments%20are%20conducted%20to%20demonstrate%20the%20superiority%20of%0Aour%20AdalReID%2C%20which%20significantly%20outperforms%20SOTAs%20by%20about%2010%24%5Csim%2420%5C%25%20mAP%0Aon%20both%20seen%20and%20unseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19005v1&entry.124074799=Read"},
{"title": "Real-Time Environment Condition Classification for Autonomous Vehicles", "author": "Marco Introvigne and Andrea Ramazzina and Stefanie Walz and Dominik Scheuble and Mario Bijelic", "abstract": "  Current autonomous driving technologies are being rolled out in geo-fenced\nareas with well-defined operation conditions such as time of operation, area,\nweather conditions and road conditions. In this way, challenging conditions as\nadverse weather, slippery road or densely-populated city centers can be\nexcluded. In order to lift the geo-fenced restriction and allow a more dynamic\navailability of autonomous driving functions, it is necessary for the vehicle\nto autonomously perform an environment condition assessment in real time to\nidentify when the system cannot operate safely and either stop operation or\nrequire the resting passenger to take control. In particular, adverse-weather\nchallenges are a fundamental limitation as sensor performance degenerates\nquickly, prohibiting the use of sensors such as cameras to locate and monitor\nroad signs, pedestrians or other vehicles. To address this issue, we train a\ndeep learning model to identify outdoor weather and dangerous road conditions,\nenabling a quick reaction to new situations and environments. We achieve this\nby introducing an improved taxonomy and label hierarchy for a state-of-the-art\nadverse-weather dataset, relabelling it with a novel semi-automated labeling\npipeline. Using the novel proposed dataset and hierarchy, we train RECNet, a\ndeep learning model for the classification of environment conditions from a\nsingle RGB frame. We outperform baseline models by relative 16% in F1- Score,\nwhile maintaining a real-time capable performance of 20 Hz.\n", "link": "http://arxiv.org/abs/2405.19305v1", "date": "2024-05-29", "relevancy": 2.1345, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5332}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Environment%20Condition%20Classification%20for%20Autonomous%20Vehicles&body=Title%3A%20Real-Time%20Environment%20Condition%20Classification%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Marco%20Introvigne%20and%20Andrea%20Ramazzina%20and%20Stefanie%20Walz%20and%20Dominik%20Scheuble%20and%20Mario%20Bijelic%0AAbstract%3A%20%20%20Current%20autonomous%20driving%20technologies%20are%20being%20rolled%20out%20in%20geo-fenced%0Aareas%20with%20well-defined%20operation%20conditions%20such%20as%20time%20of%20operation%2C%20area%2C%0Aweather%20conditions%20and%20road%20conditions.%20In%20this%20way%2C%20challenging%20conditions%20as%0Aadverse%20weather%2C%20slippery%20road%20or%20densely-populated%20city%20centers%20can%20be%0Aexcluded.%20In%20order%20to%20lift%20the%20geo-fenced%20restriction%20and%20allow%20a%20more%20dynamic%0Aavailability%20of%20autonomous%20driving%20functions%2C%20it%20is%20necessary%20for%20the%20vehicle%0Ato%20autonomously%20perform%20an%20environment%20condition%20assessment%20in%20real%20time%20to%0Aidentify%20when%20the%20system%20cannot%20operate%20safely%20and%20either%20stop%20operation%20or%0Arequire%20the%20resting%20passenger%20to%20take%20control.%20In%20particular%2C%20adverse-weather%0Achallenges%20are%20a%20fundamental%20limitation%20as%20sensor%20performance%20degenerates%0Aquickly%2C%20prohibiting%20the%20use%20of%20sensors%20such%20as%20cameras%20to%20locate%20and%20monitor%0Aroad%20signs%2C%20pedestrians%20or%20other%20vehicles.%20To%20address%20this%20issue%2C%20we%20train%20a%0Adeep%20learning%20model%20to%20identify%20outdoor%20weather%20and%20dangerous%20road%20conditions%2C%0Aenabling%20a%20quick%20reaction%20to%20new%20situations%20and%20environments.%20We%20achieve%20this%0Aby%20introducing%20an%20improved%20taxonomy%20and%20label%20hierarchy%20for%20a%20state-of-the-art%0Aadverse-weather%20dataset%2C%20relabelling%20it%20with%20a%20novel%20semi-automated%20labeling%0Apipeline.%20Using%20the%20novel%20proposed%20dataset%20and%20hierarchy%2C%20we%20train%20RECNet%2C%20a%0Adeep%20learning%20model%20for%20the%20classification%20of%20environment%20conditions%20from%20a%0Asingle%20RGB%20frame.%20We%20outperform%20baseline%20models%20by%20relative%2016%25%20in%20F1-%20Score%2C%0Awhile%20maintaining%20a%20real-time%20capable%20performance%20of%2020%20Hz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Environment%2520Condition%2520Classification%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DMarco%2520Introvigne%2520and%2520Andrea%2520Ramazzina%2520and%2520Stefanie%2520Walz%2520and%2520Dominik%2520Scheuble%2520and%2520Mario%2520Bijelic%26entry.1292438233%3D%2520%2520Current%2520autonomous%2520driving%2520technologies%2520are%2520being%2520rolled%2520out%2520in%2520geo-fenced%250Aareas%2520with%2520well-defined%2520operation%2520conditions%2520such%2520as%2520time%2520of%2520operation%252C%2520area%252C%250Aweather%2520conditions%2520and%2520road%2520conditions.%2520In%2520this%2520way%252C%2520challenging%2520conditions%2520as%250Aadverse%2520weather%252C%2520slippery%2520road%2520or%2520densely-populated%2520city%2520centers%2520can%2520be%250Aexcluded.%2520In%2520order%2520to%2520lift%2520the%2520geo-fenced%2520restriction%2520and%2520allow%2520a%2520more%2520dynamic%250Aavailability%2520of%2520autonomous%2520driving%2520functions%252C%2520it%2520is%2520necessary%2520for%2520the%2520vehicle%250Ato%2520autonomously%2520perform%2520an%2520environment%2520condition%2520assessment%2520in%2520real%2520time%2520to%250Aidentify%2520when%2520the%2520system%2520cannot%2520operate%2520safely%2520and%2520either%2520stop%2520operation%2520or%250Arequire%2520the%2520resting%2520passenger%2520to%2520take%2520control.%2520In%2520particular%252C%2520adverse-weather%250Achallenges%2520are%2520a%2520fundamental%2520limitation%2520as%2520sensor%2520performance%2520degenerates%250Aquickly%252C%2520prohibiting%2520the%2520use%2520of%2520sensors%2520such%2520as%2520cameras%2520to%2520locate%2520and%2520monitor%250Aroad%2520signs%252C%2520pedestrians%2520or%2520other%2520vehicles.%2520To%2520address%2520this%2520issue%252C%2520we%2520train%2520a%250Adeep%2520learning%2520model%2520to%2520identify%2520outdoor%2520weather%2520and%2520dangerous%2520road%2520conditions%252C%250Aenabling%2520a%2520quick%2520reaction%2520to%2520new%2520situations%2520and%2520environments.%2520We%2520achieve%2520this%250Aby%2520introducing%2520an%2520improved%2520taxonomy%2520and%2520label%2520hierarchy%2520for%2520a%2520state-of-the-art%250Aadverse-weather%2520dataset%252C%2520relabelling%2520it%2520with%2520a%2520novel%2520semi-automated%2520labeling%250Apipeline.%2520Using%2520the%2520novel%2520proposed%2520dataset%2520and%2520hierarchy%252C%2520we%2520train%2520RECNet%252C%2520a%250Adeep%2520learning%2520model%2520for%2520the%2520classification%2520of%2520environment%2520conditions%2520from%2520a%250Asingle%2520RGB%2520frame.%2520We%2520outperform%2520baseline%2520models%2520by%2520relative%252016%2525%2520in%2520F1-%2520Score%252C%250Awhile%2520maintaining%2520a%2520real-time%2520capable%2520performance%2520of%252020%2520Hz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Environment%20Condition%20Classification%20for%20Autonomous%20Vehicles&entry.906535625=Marco%20Introvigne%20and%20Andrea%20Ramazzina%20and%20Stefanie%20Walz%20and%20Dominik%20Scheuble%20and%20Mario%20Bijelic&entry.1292438233=%20%20Current%20autonomous%20driving%20technologies%20are%20being%20rolled%20out%20in%20geo-fenced%0Aareas%20with%20well-defined%20operation%20conditions%20such%20as%20time%20of%20operation%2C%20area%2C%0Aweather%20conditions%20and%20road%20conditions.%20In%20this%20way%2C%20challenging%20conditions%20as%0Aadverse%20weather%2C%20slippery%20road%20or%20densely-populated%20city%20centers%20can%20be%0Aexcluded.%20In%20order%20to%20lift%20the%20geo-fenced%20restriction%20and%20allow%20a%20more%20dynamic%0Aavailability%20of%20autonomous%20driving%20functions%2C%20it%20is%20necessary%20for%20the%20vehicle%0Ato%20autonomously%20perform%20an%20environment%20condition%20assessment%20in%20real%20time%20to%0Aidentify%20when%20the%20system%20cannot%20operate%20safely%20and%20either%20stop%20operation%20or%0Arequire%20the%20resting%20passenger%20to%20take%20control.%20In%20particular%2C%20adverse-weather%0Achallenges%20are%20a%20fundamental%20limitation%20as%20sensor%20performance%20degenerates%0Aquickly%2C%20prohibiting%20the%20use%20of%20sensors%20such%20as%20cameras%20to%20locate%20and%20monitor%0Aroad%20signs%2C%20pedestrians%20or%20other%20vehicles.%20To%20address%20this%20issue%2C%20we%20train%20a%0Adeep%20learning%20model%20to%20identify%20outdoor%20weather%20and%20dangerous%20road%20conditions%2C%0Aenabling%20a%20quick%20reaction%20to%20new%20situations%20and%20environments.%20We%20achieve%20this%0Aby%20introducing%20an%20improved%20taxonomy%20and%20label%20hierarchy%20for%20a%20state-of-the-art%0Aadverse-weather%20dataset%2C%20relabelling%20it%20with%20a%20novel%20semi-automated%20labeling%0Apipeline.%20Using%20the%20novel%20proposed%20dataset%20and%20hierarchy%2C%20we%20train%20RECNet%2C%20a%0Adeep%20learning%20model%20for%20the%20classification%20of%20environment%20conditions%20from%20a%0Asingle%20RGB%20frame.%20We%20outperform%20baseline%20models%20by%20relative%2016%25%20in%20F1-%20Score%2C%0Awhile%20maintaining%20a%20real-time%20capable%20performance%20of%2020%20Hz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19305v1&entry.124074799=Read"},
{"title": "CaLa: Complementary Association Learning for Augmenting Composed Image\n  Retrieval", "author": "Xintong Jiang and Yaxiong Wang and Mengjian Li and Yujiao Wu and Bingwen Hu and Xueming Qian", "abstract": "  Composed Image Retrieval (CIR) involves searching for target images based on\nan image-text pair query. While current methods treat this as a query-target\nmatching problem, we argue that CIR triplets contain additional associations\nbeyond this primary relation. In our paper, we identify two new relations\nwithin triplets, treating each triplet as a graph node. Firstly, we introduce\nthe concept of text-bridged image alignment, where the query text serves as a\nbridge between the query image and the target image. We propose a hinge-based\ncross-attention mechanism to incorporate this relation into network learning.\nSecondly, we explore complementary text reasoning, considering CIR as a form of\ncross-modal retrieval where two images compose to reason about complementary\ntext. To integrate these perspectives effectively, we design a twin\nattention-based compositor. By combining these complementary associations with\nthe explicit query pair-target image relation, we establish a comprehensive set\nof constraints for CIR. Our framework, CaLa (Complementary Association Learning\nfor Augmenting Composed Image Retrieval), leverages these insights. We evaluate\nCaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating\nits superiority in composed image retrieval.\n", "link": "http://arxiv.org/abs/2405.19149v1", "date": "2024-05-29", "relevancy": 2.1263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5309}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval&body=Title%3A%20CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval%0AAuthor%3A%20Xintong%20Jiang%20and%20Yaxiong%20Wang%20and%20Mengjian%20Li%20and%20Yujiao%20Wu%20and%20Bingwen%20Hu%20and%20Xueming%20Qian%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20involves%20searching%20for%20target%20images%20based%20on%0Aan%20image-text%20pair%20query.%20While%20current%20methods%20treat%20this%20as%20a%20query-target%0Amatching%20problem%2C%20we%20argue%20that%20CIR%20triplets%20contain%20additional%20associations%0Abeyond%20this%20primary%20relation.%20In%20our%20paper%2C%20we%20identify%20two%20new%20relations%0Awithin%20triplets%2C%20treating%20each%20triplet%20as%20a%20graph%20node.%20Firstly%2C%20we%20introduce%0Athe%20concept%20of%20text-bridged%20image%20alignment%2C%20where%20the%20query%20text%20serves%20as%20a%0Abridge%20between%20the%20query%20image%20and%20the%20target%20image.%20We%20propose%20a%20hinge-based%0Across-attention%20mechanism%20to%20incorporate%20this%20relation%20into%20network%20learning.%0ASecondly%2C%20we%20explore%20complementary%20text%20reasoning%2C%20considering%20CIR%20as%20a%20form%20of%0Across-modal%20retrieval%20where%20two%20images%20compose%20to%20reason%20about%20complementary%0Atext.%20To%20integrate%20these%20perspectives%20effectively%2C%20we%20design%20a%20twin%0Aattention-based%20compositor.%20By%20combining%20these%20complementary%20associations%20with%0Athe%20explicit%20query%20pair-target%20image%20relation%2C%20we%20establish%20a%20comprehensive%20set%0Aof%20constraints%20for%20CIR.%20Our%20framework%2C%20CaLa%20%28Complementary%20Association%20Learning%0Afor%20Augmenting%20Composed%20Image%20Retrieval%29%2C%20leverages%20these%20insights.%20We%20evaluate%0ACaLa%20on%20CIRR%20and%20FashionIQ%20benchmarks%20with%20multiple%20backbones%2C%20demonstrating%0Aits%20superiority%20in%20composed%20image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaLa%253A%2520Complementary%2520Association%2520Learning%2520for%2520Augmenting%2520Composed%2520Image%250A%2520%2520Retrieval%26entry.906535625%3DXintong%2520Jiang%2520and%2520Yaxiong%2520Wang%2520and%2520Mengjian%2520Li%2520and%2520Yujiao%2520Wu%2520and%2520Bingwen%2520Hu%2520and%2520Xueming%2520Qian%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520involves%2520searching%2520for%2520target%2520images%2520based%2520on%250Aan%2520image-text%2520pair%2520query.%2520While%2520current%2520methods%2520treat%2520this%2520as%2520a%2520query-target%250Amatching%2520problem%252C%2520we%2520argue%2520that%2520CIR%2520triplets%2520contain%2520additional%2520associations%250Abeyond%2520this%2520primary%2520relation.%2520In%2520our%2520paper%252C%2520we%2520identify%2520two%2520new%2520relations%250Awithin%2520triplets%252C%2520treating%2520each%2520triplet%2520as%2520a%2520graph%2520node.%2520Firstly%252C%2520we%2520introduce%250Athe%2520concept%2520of%2520text-bridged%2520image%2520alignment%252C%2520where%2520the%2520query%2520text%2520serves%2520as%2520a%250Abridge%2520between%2520the%2520query%2520image%2520and%2520the%2520target%2520image.%2520We%2520propose%2520a%2520hinge-based%250Across-attention%2520mechanism%2520to%2520incorporate%2520this%2520relation%2520into%2520network%2520learning.%250ASecondly%252C%2520we%2520explore%2520complementary%2520text%2520reasoning%252C%2520considering%2520CIR%2520as%2520a%2520form%2520of%250Across-modal%2520retrieval%2520where%2520two%2520images%2520compose%2520to%2520reason%2520about%2520complementary%250Atext.%2520To%2520integrate%2520these%2520perspectives%2520effectively%252C%2520we%2520design%2520a%2520twin%250Aattention-based%2520compositor.%2520By%2520combining%2520these%2520complementary%2520associations%2520with%250Athe%2520explicit%2520query%2520pair-target%2520image%2520relation%252C%2520we%2520establish%2520a%2520comprehensive%2520set%250Aof%2520constraints%2520for%2520CIR.%2520Our%2520framework%252C%2520CaLa%2520%2528Complementary%2520Association%2520Learning%250Afor%2520Augmenting%2520Composed%2520Image%2520Retrieval%2529%252C%2520leverages%2520these%2520insights.%2520We%2520evaluate%250ACaLa%2520on%2520CIRR%2520and%2520FashionIQ%2520benchmarks%2520with%2520multiple%2520backbones%252C%2520demonstrating%250Aits%2520superiority%2520in%2520composed%2520image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval&entry.906535625=Xintong%20Jiang%20and%20Yaxiong%20Wang%20and%20Mengjian%20Li%20and%20Yujiao%20Wu%20and%20Bingwen%20Hu%20and%20Xueming%20Qian&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20involves%20searching%20for%20target%20images%20based%20on%0Aan%20image-text%20pair%20query.%20While%20current%20methods%20treat%20this%20as%20a%20query-target%0Amatching%20problem%2C%20we%20argue%20that%20CIR%20triplets%20contain%20additional%20associations%0Abeyond%20this%20primary%20relation.%20In%20our%20paper%2C%20we%20identify%20two%20new%20relations%0Awithin%20triplets%2C%20treating%20each%20triplet%20as%20a%20graph%20node.%20Firstly%2C%20we%20introduce%0Athe%20concept%20of%20text-bridged%20image%20alignment%2C%20where%20the%20query%20text%20serves%20as%20a%0Abridge%20between%20the%20query%20image%20and%20the%20target%20image.%20We%20propose%20a%20hinge-based%0Across-attention%20mechanism%20to%20incorporate%20this%20relation%20into%20network%20learning.%0ASecondly%2C%20we%20explore%20complementary%20text%20reasoning%2C%20considering%20CIR%20as%20a%20form%20of%0Across-modal%20retrieval%20where%20two%20images%20compose%20to%20reason%20about%20complementary%0Atext.%20To%20integrate%20these%20perspectives%20effectively%2C%20we%20design%20a%20twin%0Aattention-based%20compositor.%20By%20combining%20these%20complementary%20associations%20with%0Athe%20explicit%20query%20pair-target%20image%20relation%2C%20we%20establish%20a%20comprehensive%20set%0Aof%20constraints%20for%20CIR.%20Our%20framework%2C%20CaLa%20%28Complementary%20Association%20Learning%0Afor%20Augmenting%20Composed%20Image%20Retrieval%29%2C%20leverages%20these%20insights.%20We%20evaluate%0ACaLa%20on%20CIRR%20and%20FashionIQ%20benchmarks%20with%20multiple%20backbones%2C%20demonstrating%0Aits%20superiority%20in%20composed%20image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19149v1&entry.124074799=Read"},
{"title": "Spatio-Spectral Graph Neural Networks", "author": "Simon Geisler and Arthur Kosmala and Daniel Herbst and Stephan G\u00fcnnemann", "abstract": "  Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for\nlearning on graph-structured data. However, key limitations of l-step MPGNNs\nare that their \"receptive field\" is typically limited to the l-hop neighborhood\nof a node and that information exchange between distant nodes is limited by\nover-squashing. Motivated by these limitations, we propose Spatio-Spectral\nGraph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural\nNetworks (GNNs) that synergistically combines spatially and spectrally\nparametrized graph filters. Parameterizing filters partially in the frequency\ndomain enables global yet efficient information propagation. We show that\nS$^2$GNNs vanquish over-squashing and yield strictly tighter\napproximation-theoretic error bounds than MPGNNs. Further, rethinking graph\nconvolutions at a fundamental level unlocks new design spaces. For example,\nS$^2$GNNs allow for free positional encodings that make them strictly more\nexpressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain\ngeneral-purpose S$^2$GNNs, we propose spectrally parametrized filters for\ndirected graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and\ngraph rewirings, e.g., on the peptide long-range benchmark tasks, and are\ncompetitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs\nscale to millions of nodes.\n", "link": "http://arxiv.org/abs/2405.19121v1", "date": "2024-05-29", "relevancy": 2.1259, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5263}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Spectral%20Graph%20Neural%20Networks&body=Title%3A%20Spatio-Spectral%20Graph%20Neural%20Networks%0AAuthor%3A%20Simon%20Geisler%20and%20Arthur%20Kosmala%20and%20Daniel%20Herbst%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Spatial%20Message%20Passing%20Graph%20Neural%20Networks%20%28MPGNNs%29%20are%20widely%20used%20for%0Alearning%20on%20graph-structured%20data.%20However%2C%20key%20limitations%20of%20l-step%20MPGNNs%0Aare%20that%20their%20%22receptive%20field%22%20is%20typically%20limited%20to%20the%20l-hop%20neighborhood%0Aof%20a%20node%20and%20that%20information%20exchange%20between%20distant%20nodes%20is%20limited%20by%0Aover-squashing.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20Spatio-Spectral%0AGraph%20Neural%20Networks%20%28S%24%5E2%24GNNs%29%20--%20a%20new%20modeling%20paradigm%20for%20Graph%20Neural%0ANetworks%20%28GNNs%29%20that%20synergistically%20combines%20spatially%20and%20spectrally%0Aparametrized%20graph%20filters.%20Parameterizing%20filters%20partially%20in%20the%20frequency%0Adomain%20enables%20global%20yet%20efficient%20information%20propagation.%20We%20show%20that%0AS%24%5E2%24GNNs%20vanquish%20over-squashing%20and%20yield%20strictly%20tighter%0Aapproximation-theoretic%20error%20bounds%20than%20MPGNNs.%20Further%2C%20rethinking%20graph%0Aconvolutions%20at%20a%20fundamental%20level%20unlocks%20new%20design%20spaces.%20For%20example%2C%0AS%24%5E2%24GNNs%20allow%20for%20free%20positional%20encodings%20that%20make%20them%20strictly%20more%0Aexpressive%20than%20the%201-Weisfeiler-Lehman%20%28WL%29%20test.%20Moreover%2C%20to%20obtain%0Ageneral-purpose%20S%24%5E2%24GNNs%2C%20we%20propose%20spectrally%20parametrized%20filters%20for%0Adirected%20graphs.%20S%24%5E2%24GNNs%20outperform%20spatial%20MPGNNs%2C%20graph%20transformers%2C%20and%0Agraph%20rewirings%2C%20e.g.%2C%20on%20the%20peptide%20long-range%20benchmark%20tasks%2C%20and%20are%0Acompetitive%20with%20state-of-the-art%20sequence%20modeling.%20On%20a%2040%20GB%20GPU%2C%20S%24%5E2%24GNNs%0Ascale%20to%20millions%20of%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Spectral%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSimon%2520Geisler%2520and%2520Arthur%2520Kosmala%2520and%2520Daniel%2520Herbst%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Spatial%2520Message%2520Passing%2520Graph%2520Neural%2520Networks%2520%2528MPGNNs%2529%2520are%2520widely%2520used%2520for%250Alearning%2520on%2520graph-structured%2520data.%2520However%252C%2520key%2520limitations%2520of%2520l-step%2520MPGNNs%250Aare%2520that%2520their%2520%2522receptive%2520field%2522%2520is%2520typically%2520limited%2520to%2520the%2520l-hop%2520neighborhood%250Aof%2520a%2520node%2520and%2520that%2520information%2520exchange%2520between%2520distant%2520nodes%2520is%2520limited%2520by%250Aover-squashing.%2520Motivated%2520by%2520these%2520limitations%252C%2520we%2520propose%2520Spatio-Spectral%250AGraph%2520Neural%2520Networks%2520%2528S%2524%255E2%2524GNNs%2529%2520--%2520a%2520new%2520modeling%2520paradigm%2520for%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520that%2520synergistically%2520combines%2520spatially%2520and%2520spectrally%250Aparametrized%2520graph%2520filters.%2520Parameterizing%2520filters%2520partially%2520in%2520the%2520frequency%250Adomain%2520enables%2520global%2520yet%2520efficient%2520information%2520propagation.%2520We%2520show%2520that%250AS%2524%255E2%2524GNNs%2520vanquish%2520over-squashing%2520and%2520yield%2520strictly%2520tighter%250Aapproximation-theoretic%2520error%2520bounds%2520than%2520MPGNNs.%2520Further%252C%2520rethinking%2520graph%250Aconvolutions%2520at%2520a%2520fundamental%2520level%2520unlocks%2520new%2520design%2520spaces.%2520For%2520example%252C%250AS%2524%255E2%2524GNNs%2520allow%2520for%2520free%2520positional%2520encodings%2520that%2520make%2520them%2520strictly%2520more%250Aexpressive%2520than%2520the%25201-Weisfeiler-Lehman%2520%2528WL%2529%2520test.%2520Moreover%252C%2520to%2520obtain%250Ageneral-purpose%2520S%2524%255E2%2524GNNs%252C%2520we%2520propose%2520spectrally%2520parametrized%2520filters%2520for%250Adirected%2520graphs.%2520S%2524%255E2%2524GNNs%2520outperform%2520spatial%2520MPGNNs%252C%2520graph%2520transformers%252C%2520and%250Agraph%2520rewirings%252C%2520e.g.%252C%2520on%2520the%2520peptide%2520long-range%2520benchmark%2520tasks%252C%2520and%2520are%250Acompetitive%2520with%2520state-of-the-art%2520sequence%2520modeling.%2520On%2520a%252040%2520GB%2520GPU%252C%2520S%2524%255E2%2524GNNs%250Ascale%2520to%2520millions%2520of%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Spectral%20Graph%20Neural%20Networks&entry.906535625=Simon%20Geisler%20and%20Arthur%20Kosmala%20and%20Daniel%20Herbst%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Spatial%20Message%20Passing%20Graph%20Neural%20Networks%20%28MPGNNs%29%20are%20widely%20used%20for%0Alearning%20on%20graph-structured%20data.%20However%2C%20key%20limitations%20of%20l-step%20MPGNNs%0Aare%20that%20their%20%22receptive%20field%22%20is%20typically%20limited%20to%20the%20l-hop%20neighborhood%0Aof%20a%20node%20and%20that%20information%20exchange%20between%20distant%20nodes%20is%20limited%20by%0Aover-squashing.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20Spatio-Spectral%0AGraph%20Neural%20Networks%20%28S%24%5E2%24GNNs%29%20--%20a%20new%20modeling%20paradigm%20for%20Graph%20Neural%0ANetworks%20%28GNNs%29%20that%20synergistically%20combines%20spatially%20and%20spectrally%0Aparametrized%20graph%20filters.%20Parameterizing%20filters%20partially%20in%20the%20frequency%0Adomain%20enables%20global%20yet%20efficient%20information%20propagation.%20We%20show%20that%0AS%24%5E2%24GNNs%20vanquish%20over-squashing%20and%20yield%20strictly%20tighter%0Aapproximation-theoretic%20error%20bounds%20than%20MPGNNs.%20Further%2C%20rethinking%20graph%0Aconvolutions%20at%20a%20fundamental%20level%20unlocks%20new%20design%20spaces.%20For%20example%2C%0AS%24%5E2%24GNNs%20allow%20for%20free%20positional%20encodings%20that%20make%20them%20strictly%20more%0Aexpressive%20than%20the%201-Weisfeiler-Lehman%20%28WL%29%20test.%20Moreover%2C%20to%20obtain%0Ageneral-purpose%20S%24%5E2%24GNNs%2C%20we%20propose%20spectrally%20parametrized%20filters%20for%0Adirected%20graphs.%20S%24%5E2%24GNNs%20outperform%20spatial%20MPGNNs%2C%20graph%20transformers%2C%20and%0Agraph%20rewirings%2C%20e.g.%2C%20on%20the%20peptide%20long-range%20benchmark%20tasks%2C%20and%20are%0Acompetitive%20with%20state-of-the-art%20sequence%20modeling.%20On%20a%2040%20GB%20GPU%2C%20S%24%5E2%24GNNs%0Ascale%20to%20millions%20of%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19121v1&entry.124074799=Read"},
{"title": "Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language\n  Models via Instruction Tuning", "author": "Yixiao Zhang and Yukara Ikemiya and Woosung Choi and Naoki Murata and Marco A. Mart\u00ednez-Ram\u00edrez and Liwei Lin and Gus Xia and Wei-Hsiang Liao and Yuki Mitsufuji and Simon Dixon", "abstract": "  Recent advances in text-to-music editing, which employ text queries to modify\nmusic (e.g.\\ by changing its style or adjusting instrumental components),\npresent unique challenges and opportunities for AI-assisted music creation.\nPrevious approaches in this domain have been constrained by the necessity to\ntrain specific editing models from scratch, which is both resource-intensive\nand inefficient; other research uses large language models to predict edited\nmusic, resulting in imprecise audio reconstruction. To Combine the strengths\nand address these limitations, we introduce Instruct-MusicGen, a novel approach\nthat finetunes a pretrained MusicGen model to efficiently follow editing\ninstructions such as adding, removing, or separating stems. Our approach\ninvolves a modification of the original MusicGen architecture by incorporating\na text fusion module and an audio fusion module, which allow the model to\nprocess instruction texts and audio inputs concurrently and yield the desired\nedited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters\nto the original MusicGen model and only trains for 5K steps, yet it achieves\nsuperior performance across all tasks compared to existing baselines, and\ndemonstrates performance comparable to the models trained for specific tasks.\nThis advancement not only enhances the efficiency of text-to-music editing but\nalso broadens the applicability of music language models in dynamic music\nproduction environments.\n", "link": "http://arxiv.org/abs/2405.18386v2", "date": "2024-05-29", "relevancy": 2.1205, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5147}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning&body=Title%3A%20Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning%0AAuthor%3A%20Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Woosung%20Choi%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Liwei%20Lin%20and%20Gus%20Xia%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-music%20editing%2C%20which%20employ%20text%20queries%20to%20modify%0Amusic%20%28e.g.%5C%20by%20changing%20its%20style%20or%20adjusting%20instrumental%20components%29%2C%0Apresent%20unique%20challenges%20and%20opportunities%20for%20AI-assisted%20music%20creation.%0APrevious%20approaches%20in%20this%20domain%20have%20been%20constrained%20by%20the%20necessity%20to%0Atrain%20specific%20editing%20models%20from%20scratch%2C%20which%20is%20both%20resource-intensive%0Aand%20inefficient%3B%20other%20research%20uses%20large%20language%20models%20to%20predict%20edited%0Amusic%2C%20resulting%20in%20imprecise%20audio%20reconstruction.%20To%20Combine%20the%20strengths%0Aand%20address%20these%20limitations%2C%20we%20introduce%20Instruct-MusicGen%2C%20a%20novel%20approach%0Athat%20finetunes%20a%20pretrained%20MusicGen%20model%20to%20efficiently%20follow%20editing%0Ainstructions%20such%20as%20adding%2C%20removing%2C%20or%20separating%20stems.%20Our%20approach%0Ainvolves%20a%20modification%20of%20the%20original%20MusicGen%20architecture%20by%20incorporating%0Aa%20text%20fusion%20module%20and%20an%20audio%20fusion%20module%2C%20which%20allow%20the%20model%20to%0Aprocess%20instruction%20texts%20and%20audio%20inputs%20concurrently%20and%20yield%20the%20desired%0Aedited%20music.%20Remarkably%2C%20Instruct-MusicGen%20only%20introduces%208%25%20new%20parameters%0Ato%20the%20original%20MusicGen%20model%20and%20only%20trains%20for%205K%20steps%2C%20yet%20it%20achieves%0Asuperior%20performance%20across%20all%20tasks%20compared%20to%20existing%20baselines%2C%20and%0Ademonstrates%20performance%20comparable%20to%20the%20models%20trained%20for%20specific%20tasks.%0AThis%20advancement%20not%20only%20enhances%20the%20efficiency%20of%20text-to-music%20editing%20but%0Aalso%20broadens%20the%20applicability%20of%20music%20language%20models%20in%20dynamic%20music%0Aproduction%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruct-MusicGen%253A%2520Unlocking%2520Text-to-Music%2520Editing%2520for%2520Music%2520Language%250A%2520%2520Models%2520via%2520Instruction%2520Tuning%26entry.906535625%3DYixiao%2520Zhang%2520and%2520Yukara%2520Ikemiya%2520and%2520Woosung%2520Choi%2520and%2520Naoki%2520Murata%2520and%2520Marco%2520A.%2520Mart%25C3%25ADnez-Ram%25C3%25ADrez%2520and%2520Liwei%2520Lin%2520and%2520Gus%2520Xia%2520and%2520Wei-Hsiang%2520Liao%2520and%2520Yuki%2520Mitsufuji%2520and%2520Simon%2520Dixon%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-music%2520editing%252C%2520which%2520employ%2520text%2520queries%2520to%2520modify%250Amusic%2520%2528e.g.%255C%2520by%2520changing%2520its%2520style%2520or%2520adjusting%2520instrumental%2520components%2529%252C%250Apresent%2520unique%2520challenges%2520and%2520opportunities%2520for%2520AI-assisted%2520music%2520creation.%250APrevious%2520approaches%2520in%2520this%2520domain%2520have%2520been%2520constrained%2520by%2520the%2520necessity%2520to%250Atrain%2520specific%2520editing%2520models%2520from%2520scratch%252C%2520which%2520is%2520both%2520resource-intensive%250Aand%2520inefficient%253B%2520other%2520research%2520uses%2520large%2520language%2520models%2520to%2520predict%2520edited%250Amusic%252C%2520resulting%2520in%2520imprecise%2520audio%2520reconstruction.%2520To%2520Combine%2520the%2520strengths%250Aand%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Instruct-MusicGen%252C%2520a%2520novel%2520approach%250Athat%2520finetunes%2520a%2520pretrained%2520MusicGen%2520model%2520to%2520efficiently%2520follow%2520editing%250Ainstructions%2520such%2520as%2520adding%252C%2520removing%252C%2520or%2520separating%2520stems.%2520Our%2520approach%250Ainvolves%2520a%2520modification%2520of%2520the%2520original%2520MusicGen%2520architecture%2520by%2520incorporating%250Aa%2520text%2520fusion%2520module%2520and%2520an%2520audio%2520fusion%2520module%252C%2520which%2520allow%2520the%2520model%2520to%250Aprocess%2520instruction%2520texts%2520and%2520audio%2520inputs%2520concurrently%2520and%2520yield%2520the%2520desired%250Aedited%2520music.%2520Remarkably%252C%2520Instruct-MusicGen%2520only%2520introduces%25208%2525%2520new%2520parameters%250Ato%2520the%2520original%2520MusicGen%2520model%2520and%2520only%2520trains%2520for%25205K%2520steps%252C%2520yet%2520it%2520achieves%250Asuperior%2520performance%2520across%2520all%2520tasks%2520compared%2520to%2520existing%2520baselines%252C%2520and%250Ademonstrates%2520performance%2520comparable%2520to%2520the%2520models%2520trained%2520for%2520specific%2520tasks.%250AThis%2520advancement%2520not%2520only%2520enhances%2520the%2520efficiency%2520of%2520text-to-music%2520editing%2520but%250Aalso%2520broadens%2520the%2520applicability%2520of%2520music%2520language%2520models%2520in%2520dynamic%2520music%250Aproduction%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning&entry.906535625=Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Woosung%20Choi%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Liwei%20Lin%20and%20Gus%20Xia%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon&entry.1292438233=%20%20Recent%20advances%20in%20text-to-music%20editing%2C%20which%20employ%20text%20queries%20to%20modify%0Amusic%20%28e.g.%5C%20by%20changing%20its%20style%20or%20adjusting%20instrumental%20components%29%2C%0Apresent%20unique%20challenges%20and%20opportunities%20for%20AI-assisted%20music%20creation.%0APrevious%20approaches%20in%20this%20domain%20have%20been%20constrained%20by%20the%20necessity%20to%0Atrain%20specific%20editing%20models%20from%20scratch%2C%20which%20is%20both%20resource-intensive%0Aand%20inefficient%3B%20other%20research%20uses%20large%20language%20models%20to%20predict%20edited%0Amusic%2C%20resulting%20in%20imprecise%20audio%20reconstruction.%20To%20Combine%20the%20strengths%0Aand%20address%20these%20limitations%2C%20we%20introduce%20Instruct-MusicGen%2C%20a%20novel%20approach%0Athat%20finetunes%20a%20pretrained%20MusicGen%20model%20to%20efficiently%20follow%20editing%0Ainstructions%20such%20as%20adding%2C%20removing%2C%20or%20separating%20stems.%20Our%20approach%0Ainvolves%20a%20modification%20of%20the%20original%20MusicGen%20architecture%20by%20incorporating%0Aa%20text%20fusion%20module%20and%20an%20audio%20fusion%20module%2C%20which%20allow%20the%20model%20to%0Aprocess%20instruction%20texts%20and%20audio%20inputs%20concurrently%20and%20yield%20the%20desired%0Aedited%20music.%20Remarkably%2C%20Instruct-MusicGen%20only%20introduces%208%25%20new%20parameters%0Ato%20the%20original%20MusicGen%20model%20and%20only%20trains%20for%205K%20steps%2C%20yet%20it%20achieves%0Asuperior%20performance%20across%20all%20tasks%20compared%20to%20existing%20baselines%2C%20and%0Ademonstrates%20performance%20comparable%20to%20the%20models%20trained%20for%20specific%20tasks.%0AThis%20advancement%20not%20only%20enhances%20the%20efficiency%20of%20text-to-music%20editing%20but%0Aalso%20broadens%20the%20applicability%20of%20music%20language%20models%20in%20dynamic%20music%0Aproduction%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18386v2&entry.124074799=Read"},
{"title": "FaultFormer: Pretraining Transformers for Adaptable Bearing Fault\n  Classification", "author": "Anthony Zhou and Amir Barati Farimani", "abstract": "  The growth of global consumption has motivated important applications of deep\nlearning to smart manufacturing and machine health monitoring. In particular,\nanalyzing vibration data offers great potential to extract meaningful insights\ninto predictive maintenance by the detection of bearing faults. Deep learning\ncan be a powerful method to predict these mechanical failures; however, they\nlack generalizability to new tasks or datasets and require expensive, labeled\nmechanical data. We address this by presenting a novel self-supervised\npretraining and fine-tuning framework based on transformer models. In\nparticular, we investigate different tokenization and data augmentation\nstrategies to reach state-of-the-art accuracies using transformer models.\nFurthermore, we demonstrate self-supervised masked pretraining for vibration\nsignals and its application to low-data regimes, task adaptation, and dataset\nadaptation. Pretraining is able to improve performance on scarce, unseen\ntraining samples, as well as when fine-tuning on fault classes outside of the\npretraining distribution. Furthermore, pretrained transformers are shown to be\nable to generalize to a different dataset in a few-shot manner. This introduces\na new paradigm where models can be pretrained on unlabeled data from different\nbearings, faults, and machinery and quickly deployed to new, data-scarce\napplications to suit specific manufacturing needs.\n", "link": "http://arxiv.org/abs/2312.02380v3", "date": "2024-05-29", "relevancy": 2.1157, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaultFormer%3A%20Pretraining%20Transformers%20for%20Adaptable%20Bearing%20Fault%0A%20%20Classification&body=Title%3A%20FaultFormer%3A%20Pretraining%20Transformers%20for%20Adaptable%20Bearing%20Fault%0A%20%20Classification%0AAuthor%3A%20Anthony%20Zhou%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20The%20growth%20of%20global%20consumption%20has%20motivated%20important%20applications%20of%20deep%0Alearning%20to%20smart%20manufacturing%20and%20machine%20health%20monitoring.%20In%20particular%2C%0Aanalyzing%20vibration%20data%20offers%20great%20potential%20to%20extract%20meaningful%20insights%0Ainto%20predictive%20maintenance%20by%20the%20detection%20of%20bearing%20faults.%20Deep%20learning%0Acan%20be%20a%20powerful%20method%20to%20predict%20these%20mechanical%20failures%3B%20however%2C%20they%0Alack%20generalizability%20to%20new%20tasks%20or%20datasets%20and%20require%20expensive%2C%20labeled%0Amechanical%20data.%20We%20address%20this%20by%20presenting%20a%20novel%20self-supervised%0Apretraining%20and%20fine-tuning%20framework%20based%20on%20transformer%20models.%20In%0Aparticular%2C%20we%20investigate%20different%20tokenization%20and%20data%20augmentation%0Astrategies%20to%20reach%20state-of-the-art%20accuracies%20using%20transformer%20models.%0AFurthermore%2C%20we%20demonstrate%20self-supervised%20masked%20pretraining%20for%20vibration%0Asignals%20and%20its%20application%20to%20low-data%20regimes%2C%20task%20adaptation%2C%20and%20dataset%0Aadaptation.%20Pretraining%20is%20able%20to%20improve%20performance%20on%20scarce%2C%20unseen%0Atraining%20samples%2C%20as%20well%20as%20when%20fine-tuning%20on%20fault%20classes%20outside%20of%20the%0Apretraining%20distribution.%20Furthermore%2C%20pretrained%20transformers%20are%20shown%20to%20be%0Aable%20to%20generalize%20to%20a%20different%20dataset%20in%20a%20few-shot%20manner.%20This%20introduces%0Aa%20new%20paradigm%20where%20models%20can%20be%20pretrained%20on%20unlabeled%20data%20from%20different%0Abearings%2C%20faults%2C%20and%20machinery%20and%20quickly%20deployed%20to%20new%2C%20data-scarce%0Aapplications%20to%20suit%20specific%20manufacturing%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaultFormer%253A%2520Pretraining%2520Transformers%2520for%2520Adaptable%2520Bearing%2520Fault%250A%2520%2520Classification%26entry.906535625%3DAnthony%2520Zhou%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520The%2520growth%2520of%2520global%2520consumption%2520has%2520motivated%2520important%2520applications%2520of%2520deep%250Alearning%2520to%2520smart%2520manufacturing%2520and%2520machine%2520health%2520monitoring.%2520In%2520particular%252C%250Aanalyzing%2520vibration%2520data%2520offers%2520great%2520potential%2520to%2520extract%2520meaningful%2520insights%250Ainto%2520predictive%2520maintenance%2520by%2520the%2520detection%2520of%2520bearing%2520faults.%2520Deep%2520learning%250Acan%2520be%2520a%2520powerful%2520method%2520to%2520predict%2520these%2520mechanical%2520failures%253B%2520however%252C%2520they%250Alack%2520generalizability%2520to%2520new%2520tasks%2520or%2520datasets%2520and%2520require%2520expensive%252C%2520labeled%250Amechanical%2520data.%2520We%2520address%2520this%2520by%2520presenting%2520a%2520novel%2520self-supervised%250Apretraining%2520and%2520fine-tuning%2520framework%2520based%2520on%2520transformer%2520models.%2520In%250Aparticular%252C%2520we%2520investigate%2520different%2520tokenization%2520and%2520data%2520augmentation%250Astrategies%2520to%2520reach%2520state-of-the-art%2520accuracies%2520using%2520transformer%2520models.%250AFurthermore%252C%2520we%2520demonstrate%2520self-supervised%2520masked%2520pretraining%2520for%2520vibration%250Asignals%2520and%2520its%2520application%2520to%2520low-data%2520regimes%252C%2520task%2520adaptation%252C%2520and%2520dataset%250Aadaptation.%2520Pretraining%2520is%2520able%2520to%2520improve%2520performance%2520on%2520scarce%252C%2520unseen%250Atraining%2520samples%252C%2520as%2520well%2520as%2520when%2520fine-tuning%2520on%2520fault%2520classes%2520outside%2520of%2520the%250Apretraining%2520distribution.%2520Furthermore%252C%2520pretrained%2520transformers%2520are%2520shown%2520to%2520be%250Aable%2520to%2520generalize%2520to%2520a%2520different%2520dataset%2520in%2520a%2520few-shot%2520manner.%2520This%2520introduces%250Aa%2520new%2520paradigm%2520where%2520models%2520can%2520be%2520pretrained%2520on%2520unlabeled%2520data%2520from%2520different%250Abearings%252C%2520faults%252C%2520and%2520machinery%2520and%2520quickly%2520deployed%2520to%2520new%252C%2520data-scarce%250Aapplications%2520to%2520suit%2520specific%2520manufacturing%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaultFormer%3A%20Pretraining%20Transformers%20for%20Adaptable%20Bearing%20Fault%0A%20%20Classification&entry.906535625=Anthony%20Zhou%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20The%20growth%20of%20global%20consumption%20has%20motivated%20important%20applications%20of%20deep%0Alearning%20to%20smart%20manufacturing%20and%20machine%20health%20monitoring.%20In%20particular%2C%0Aanalyzing%20vibration%20data%20offers%20great%20potential%20to%20extract%20meaningful%20insights%0Ainto%20predictive%20maintenance%20by%20the%20detection%20of%20bearing%20faults.%20Deep%20learning%0Acan%20be%20a%20powerful%20method%20to%20predict%20these%20mechanical%20failures%3B%20however%2C%20they%0Alack%20generalizability%20to%20new%20tasks%20or%20datasets%20and%20require%20expensive%2C%20labeled%0Amechanical%20data.%20We%20address%20this%20by%20presenting%20a%20novel%20self-supervised%0Apretraining%20and%20fine-tuning%20framework%20based%20on%20transformer%20models.%20In%0Aparticular%2C%20we%20investigate%20different%20tokenization%20and%20data%20augmentation%0Astrategies%20to%20reach%20state-of-the-art%20accuracies%20using%20transformer%20models.%0AFurthermore%2C%20we%20demonstrate%20self-supervised%20masked%20pretraining%20for%20vibration%0Asignals%20and%20its%20application%20to%20low-data%20regimes%2C%20task%20adaptation%2C%20and%20dataset%0Aadaptation.%20Pretraining%20is%20able%20to%20improve%20performance%20on%20scarce%2C%20unseen%0Atraining%20samples%2C%20as%20well%20as%20when%20fine-tuning%20on%20fault%20classes%20outside%20of%20the%0Apretraining%20distribution.%20Furthermore%2C%20pretrained%20transformers%20are%20shown%20to%20be%0Aable%20to%20generalize%20to%20a%20different%20dataset%20in%20a%20few-shot%20manner.%20This%20introduces%0Aa%20new%20paradigm%20where%20models%20can%20be%20pretrained%20on%20unlabeled%20data%20from%20different%0Abearings%2C%20faults%2C%20and%20machinery%20and%20quickly%20deployed%20to%20new%2C%20data-scarce%0Aapplications%20to%20suit%20specific%20manufacturing%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02380v3&entry.124074799=Read"},
{"title": "Channel-Selective Normalization for Label-Shift Robust Test-Time\n  Adaptation", "author": "Pedro Vianna and Muawiz Chaudhary and Paria Mehrbod and An Tang and Guy Cloutier and Guy Wolf and Michael Eickenberg and Eugene Belilovsky", "abstract": "  Deep neural networks have useful applications in many different tasks,\nhowever their performance can be severely affected by changes in the data\ndistribution. For example, in the biomedical field, their performance can be\naffected by changes in the data (different machines, populations) between\ntraining and test datasets. To ensure robustness and generalization to\nreal-world scenarios, test-time adaptation has been recently studied as an\napproach to adjust models to a new data distribution during inference.\nTest-time batch normalization is a simple and popular method that achieved\ncompelling performance on domain shift benchmarks. It is implemented by\nrecalculating batch normalization statistics on test batches. Prior work has\nfocused on analysis with test data that has the same label distribution as the\ntraining data. However, in many practical applications this technique is\nvulnerable to label distribution shifts, sometimes producing catastrophic\nfailure. This presents a risk in applying test time adaptation methods in\ndeployment. We propose to tackle this challenge by only selectively adapting\nchannels in a deep network, minimizing drastic adaptation that is sensitive to\nlabel shifts. Our selection scheme is based on two principles that we\nempirically motivate: (1) later layers of networks are more sensitive to label\nshift (2) individual features can be sensitive to specific classes. We apply\nthe proposed technique to three classification tasks, including CIFAR10-C,\nImagenet-C, and diagnosis of fatty liver, where we explore both covariate and\nlabel distribution shifts. We find that our method allows to bring the benefits\nof TTA while significantly reducing the risk of failure common in other\nmethods, while being robust to choice in hyperparameters.\n", "link": "http://arxiv.org/abs/2402.04958v2", "date": "2024-05-29", "relevancy": 2.112, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5478}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5183}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel-Selective%20Normalization%20for%20Label-Shift%20Robust%20Test-Time%0A%20%20Adaptation&body=Title%3A%20Channel-Selective%20Normalization%20for%20Label-Shift%20Robust%20Test-Time%0A%20%20Adaptation%0AAuthor%3A%20Pedro%20Vianna%20and%20Muawiz%20Chaudhary%20and%20Paria%20Mehrbod%20and%20An%20Tang%20and%20Guy%20Cloutier%20and%20Guy%20Wolf%20and%20Michael%20Eickenberg%20and%20Eugene%20Belilovsky%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20useful%20applications%20in%20many%20different%20tasks%2C%0Ahowever%20their%20performance%20can%20be%20severely%20affected%20by%20changes%20in%20the%20data%0Adistribution.%20For%20example%2C%20in%20the%20biomedical%20field%2C%20their%20performance%20can%20be%0Aaffected%20by%20changes%20in%20the%20data%20%28different%20machines%2C%20populations%29%20between%0Atraining%20and%20test%20datasets.%20To%20ensure%20robustness%20and%20generalization%20to%0Areal-world%20scenarios%2C%20test-time%20adaptation%20has%20been%20recently%20studied%20as%20an%0Aapproach%20to%20adjust%20models%20to%20a%20new%20data%20distribution%20during%20inference.%0ATest-time%20batch%20normalization%20is%20a%20simple%20and%20popular%20method%20that%20achieved%0Acompelling%20performance%20on%20domain%20shift%20benchmarks.%20It%20is%20implemented%20by%0Arecalculating%20batch%20normalization%20statistics%20on%20test%20batches.%20Prior%20work%20has%0Afocused%20on%20analysis%20with%20test%20data%20that%20has%20the%20same%20label%20distribution%20as%20the%0Atraining%20data.%20However%2C%20in%20many%20practical%20applications%20this%20technique%20is%0Avulnerable%20to%20label%20distribution%20shifts%2C%20sometimes%20producing%20catastrophic%0Afailure.%20This%20presents%20a%20risk%20in%20applying%20test%20time%20adaptation%20methods%20in%0Adeployment.%20We%20propose%20to%20tackle%20this%20challenge%20by%20only%20selectively%20adapting%0Achannels%20in%20a%20deep%20network%2C%20minimizing%20drastic%20adaptation%20that%20is%20sensitive%20to%0Alabel%20shifts.%20Our%20selection%20scheme%20is%20based%20on%20two%20principles%20that%20we%0Aempirically%20motivate%3A%20%281%29%20later%20layers%20of%20networks%20are%20more%20sensitive%20to%20label%0Ashift%20%282%29%20individual%20features%20can%20be%20sensitive%20to%20specific%20classes.%20We%20apply%0Athe%20proposed%20technique%20to%20three%20classification%20tasks%2C%20including%20CIFAR10-C%2C%0AImagenet-C%2C%20and%20diagnosis%20of%20fatty%20liver%2C%20where%20we%20explore%20both%20covariate%20and%0Alabel%20distribution%20shifts.%20We%20find%20that%20our%20method%20allows%20to%20bring%20the%20benefits%0Aof%20TTA%20while%20significantly%20reducing%20the%20risk%20of%20failure%20common%20in%20other%0Amethods%2C%20while%20being%20robust%20to%20choice%20in%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel-Selective%2520Normalization%2520for%2520Label-Shift%2520Robust%2520Test-Time%250A%2520%2520Adaptation%26entry.906535625%3DPedro%2520Vianna%2520and%2520Muawiz%2520Chaudhary%2520and%2520Paria%2520Mehrbod%2520and%2520An%2520Tang%2520and%2520Guy%2520Cloutier%2520and%2520Guy%2520Wolf%2520and%2520Michael%2520Eickenberg%2520and%2520Eugene%2520Belilovsky%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520useful%2520applications%2520in%2520many%2520different%2520tasks%252C%250Ahowever%2520their%2520performance%2520can%2520be%2520severely%2520affected%2520by%2520changes%2520in%2520the%2520data%250Adistribution.%2520For%2520example%252C%2520in%2520the%2520biomedical%2520field%252C%2520their%2520performance%2520can%2520be%250Aaffected%2520by%2520changes%2520in%2520the%2520data%2520%2528different%2520machines%252C%2520populations%2529%2520between%250Atraining%2520and%2520test%2520datasets.%2520To%2520ensure%2520robustness%2520and%2520generalization%2520to%250Areal-world%2520scenarios%252C%2520test-time%2520adaptation%2520has%2520been%2520recently%2520studied%2520as%2520an%250Aapproach%2520to%2520adjust%2520models%2520to%2520a%2520new%2520data%2520distribution%2520during%2520inference.%250ATest-time%2520batch%2520normalization%2520is%2520a%2520simple%2520and%2520popular%2520method%2520that%2520achieved%250Acompelling%2520performance%2520on%2520domain%2520shift%2520benchmarks.%2520It%2520is%2520implemented%2520by%250Arecalculating%2520batch%2520normalization%2520statistics%2520on%2520test%2520batches.%2520Prior%2520work%2520has%250Afocused%2520on%2520analysis%2520with%2520test%2520data%2520that%2520has%2520the%2520same%2520label%2520distribution%2520as%2520the%250Atraining%2520data.%2520However%252C%2520in%2520many%2520practical%2520applications%2520this%2520technique%2520is%250Avulnerable%2520to%2520label%2520distribution%2520shifts%252C%2520sometimes%2520producing%2520catastrophic%250Afailure.%2520This%2520presents%2520a%2520risk%2520in%2520applying%2520test%2520time%2520adaptation%2520methods%2520in%250Adeployment.%2520We%2520propose%2520to%2520tackle%2520this%2520challenge%2520by%2520only%2520selectively%2520adapting%250Achannels%2520in%2520a%2520deep%2520network%252C%2520minimizing%2520drastic%2520adaptation%2520that%2520is%2520sensitive%2520to%250Alabel%2520shifts.%2520Our%2520selection%2520scheme%2520is%2520based%2520on%2520two%2520principles%2520that%2520we%250Aempirically%2520motivate%253A%2520%25281%2529%2520later%2520layers%2520of%2520networks%2520are%2520more%2520sensitive%2520to%2520label%250Ashift%2520%25282%2529%2520individual%2520features%2520can%2520be%2520sensitive%2520to%2520specific%2520classes.%2520We%2520apply%250Athe%2520proposed%2520technique%2520to%2520three%2520classification%2520tasks%252C%2520including%2520CIFAR10-C%252C%250AImagenet-C%252C%2520and%2520diagnosis%2520of%2520fatty%2520liver%252C%2520where%2520we%2520explore%2520both%2520covariate%2520and%250Alabel%2520distribution%2520shifts.%2520We%2520find%2520that%2520our%2520method%2520allows%2520to%2520bring%2520the%2520benefits%250Aof%2520TTA%2520while%2520significantly%2520reducing%2520the%2520risk%2520of%2520failure%2520common%2520in%2520other%250Amethods%252C%2520while%2520being%2520robust%2520to%2520choice%2520in%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel-Selective%20Normalization%20for%20Label-Shift%20Robust%20Test-Time%0A%20%20Adaptation&entry.906535625=Pedro%20Vianna%20and%20Muawiz%20Chaudhary%20and%20Paria%20Mehrbod%20and%20An%20Tang%20and%20Guy%20Cloutier%20and%20Guy%20Wolf%20and%20Michael%20Eickenberg%20and%20Eugene%20Belilovsky&entry.1292438233=%20%20Deep%20neural%20networks%20have%20useful%20applications%20in%20many%20different%20tasks%2C%0Ahowever%20their%20performance%20can%20be%20severely%20affected%20by%20changes%20in%20the%20data%0Adistribution.%20For%20example%2C%20in%20the%20biomedical%20field%2C%20their%20performance%20can%20be%0Aaffected%20by%20changes%20in%20the%20data%20%28different%20machines%2C%20populations%29%20between%0Atraining%20and%20test%20datasets.%20To%20ensure%20robustness%20and%20generalization%20to%0Areal-world%20scenarios%2C%20test-time%20adaptation%20has%20been%20recently%20studied%20as%20an%0Aapproach%20to%20adjust%20models%20to%20a%20new%20data%20distribution%20during%20inference.%0ATest-time%20batch%20normalization%20is%20a%20simple%20and%20popular%20method%20that%20achieved%0Acompelling%20performance%20on%20domain%20shift%20benchmarks.%20It%20is%20implemented%20by%0Arecalculating%20batch%20normalization%20statistics%20on%20test%20batches.%20Prior%20work%20has%0Afocused%20on%20analysis%20with%20test%20data%20that%20has%20the%20same%20label%20distribution%20as%20the%0Atraining%20data.%20However%2C%20in%20many%20practical%20applications%20this%20technique%20is%0Avulnerable%20to%20label%20distribution%20shifts%2C%20sometimes%20producing%20catastrophic%0Afailure.%20This%20presents%20a%20risk%20in%20applying%20test%20time%20adaptation%20methods%20in%0Adeployment.%20We%20propose%20to%20tackle%20this%20challenge%20by%20only%20selectively%20adapting%0Achannels%20in%20a%20deep%20network%2C%20minimizing%20drastic%20adaptation%20that%20is%20sensitive%20to%0Alabel%20shifts.%20Our%20selection%20scheme%20is%20based%20on%20two%20principles%20that%20we%0Aempirically%20motivate%3A%20%281%29%20later%20layers%20of%20networks%20are%20more%20sensitive%20to%20label%0Ashift%20%282%29%20individual%20features%20can%20be%20sensitive%20to%20specific%20classes.%20We%20apply%0Athe%20proposed%20technique%20to%20three%20classification%20tasks%2C%20including%20CIFAR10-C%2C%0AImagenet-C%2C%20and%20diagnosis%20of%20fatty%20liver%2C%20where%20we%20explore%20both%20covariate%20and%0Alabel%20distribution%20shifts.%20We%20find%20that%20our%20method%20allows%20to%20bring%20the%20benefits%0Aof%20TTA%20while%20significantly%20reducing%20the%20risk%20of%20failure%20common%20in%20other%0Amethods%2C%20while%20being%20robust%20to%20choice%20in%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04958v2&entry.124074799=Read"},
{"title": "Federated Learning with Bilateral Curation for Partially Class-Disjoint\n  Data", "author": "Ziqing Fan and Ruipeng Zhang and Jiangchao Yao and Bo Han and Ya Zhang and Yanfeng Wang", "abstract": "  Partially class-disjoint data (PCDD), a common yet under-explored data\nformation where each client contributes a part of classes (instead of all\nclasses) of samples, severely challenges the performance of federated\nalgorithms. Without full classes, the local objective will contradict the\nglobal objective, yielding the angle collapse problem for locally missing\nclasses and the space waste problem for locally existing classes. As far as we\nknow, none of the existing methods can intrinsically mitigate PCDD challenges\nto achieve holistic improvement in the bilateral views (both global view and\nlocal view) of federated learning. To address this dilemma, we are inspired by\nthe strong generalization of simplex Equiangular Tight Frame~(ETF) on the\nimbalanced data, and propose a novel approach called FedGELA where the\nclassifier is globally fixed as a simplex ETF while locally adapted to the\npersonal distributions. Globally, FedGELA provides fair and equal\ndiscrimination for all classes and avoids inaccurate updates of the classifier,\nwhile locally it utilizes the space of locally missing classes for locally\nexisting classes. We conduct extensive experiments on a range of datasets to\ndemonstrate that our FedGELA achieves promising performance~(averaged\nimprovement of 3.9% to FedAvg and 1.5% to best baselines) and provide both\nlocal and global convergence guarantees. Source code is available\nat:https://github.com/MediaBrain-SJTU/FedGELA.git.\n", "link": "http://arxiv.org/abs/2405.18972v1", "date": "2024-05-29", "relevancy": 2.111, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5367}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20with%20Bilateral%20Curation%20for%20Partially%20Class-Disjoint%0A%20%20Data&body=Title%3A%20Federated%20Learning%20with%20Bilateral%20Curation%20for%20Partially%20Class-Disjoint%0A%20%20Data%0AAuthor%3A%20Ziqing%20Fan%20and%20Ruipeng%20Zhang%20and%20Jiangchao%20Yao%20and%20Bo%20Han%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Partially%20class-disjoint%20data%20%28PCDD%29%2C%20a%20common%20yet%20under-explored%20data%0Aformation%20where%20each%20client%20contributes%20a%20part%20of%20classes%20%28instead%20of%20all%0Aclasses%29%20of%20samples%2C%20severely%20challenges%20the%20performance%20of%20federated%0Aalgorithms.%20Without%20full%20classes%2C%20the%20local%20objective%20will%20contradict%20the%0Aglobal%20objective%2C%20yielding%20the%20angle%20collapse%20problem%20for%20locally%20missing%0Aclasses%20and%20the%20space%20waste%20problem%20for%20locally%20existing%20classes.%20As%20far%20as%20we%0Aknow%2C%20none%20of%20the%20existing%20methods%20can%20intrinsically%20mitigate%20PCDD%20challenges%0Ato%20achieve%20holistic%20improvement%20in%20the%20bilateral%20views%20%28both%20global%20view%20and%0Alocal%20view%29%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%20are%20inspired%20by%0Athe%20strong%20generalization%20of%20simplex%20Equiangular%20Tight%20Frame~%28ETF%29%20on%20the%0Aimbalanced%20data%2C%20and%20propose%20a%20novel%20approach%20called%20FedGELA%20where%20the%0Aclassifier%20is%20globally%20fixed%20as%20a%20simplex%20ETF%20while%20locally%20adapted%20to%20the%0Apersonal%20distributions.%20Globally%2C%20FedGELA%20provides%20fair%20and%20equal%0Adiscrimination%20for%20all%20classes%20and%20avoids%20inaccurate%20updates%20of%20the%20classifier%2C%0Awhile%20locally%20it%20utilizes%20the%20space%20of%20locally%20missing%20classes%20for%20locally%0Aexisting%20classes.%20We%20conduct%20extensive%20experiments%20on%20a%20range%20of%20datasets%20to%0Ademonstrate%20that%20our%20FedGELA%20achieves%20promising%20performance~%28averaged%0Aimprovement%20of%203.9%25%20to%20FedAvg%20and%201.5%25%20to%20best%20baselines%29%20and%20provide%20both%0Alocal%20and%20global%20convergence%20guarantees.%20Source%20code%20is%20available%0Aat%3Ahttps%3A//github.com/MediaBrain-SJTU/FedGELA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520with%2520Bilateral%2520Curation%2520for%2520Partially%2520Class-Disjoint%250A%2520%2520Data%26entry.906535625%3DZiqing%2520Fan%2520and%2520Ruipeng%2520Zhang%2520and%2520Jiangchao%2520Yao%2520and%2520Bo%2520Han%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Partially%2520class-disjoint%2520data%2520%2528PCDD%2529%252C%2520a%2520common%2520yet%2520under-explored%2520data%250Aformation%2520where%2520each%2520client%2520contributes%2520a%2520part%2520of%2520classes%2520%2528instead%2520of%2520all%250Aclasses%2529%2520of%2520samples%252C%2520severely%2520challenges%2520the%2520performance%2520of%2520federated%250Aalgorithms.%2520Without%2520full%2520classes%252C%2520the%2520local%2520objective%2520will%2520contradict%2520the%250Aglobal%2520objective%252C%2520yielding%2520the%2520angle%2520collapse%2520problem%2520for%2520locally%2520missing%250Aclasses%2520and%2520the%2520space%2520waste%2520problem%2520for%2520locally%2520existing%2520classes.%2520As%2520far%2520as%2520we%250Aknow%252C%2520none%2520of%2520the%2520existing%2520methods%2520can%2520intrinsically%2520mitigate%2520PCDD%2520challenges%250Ato%2520achieve%2520holistic%2520improvement%2520in%2520the%2520bilateral%2520views%2520%2528both%2520global%2520view%2520and%250Alocal%2520view%2529%2520of%2520federated%2520learning.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520are%2520inspired%2520by%250Athe%2520strong%2520generalization%2520of%2520simplex%2520Equiangular%2520Tight%2520Frame~%2528ETF%2529%2520on%2520the%250Aimbalanced%2520data%252C%2520and%2520propose%2520a%2520novel%2520approach%2520called%2520FedGELA%2520where%2520the%250Aclassifier%2520is%2520globally%2520fixed%2520as%2520a%2520simplex%2520ETF%2520while%2520locally%2520adapted%2520to%2520the%250Apersonal%2520distributions.%2520Globally%252C%2520FedGELA%2520provides%2520fair%2520and%2520equal%250Adiscrimination%2520for%2520all%2520classes%2520and%2520avoids%2520inaccurate%2520updates%2520of%2520the%2520classifier%252C%250Awhile%2520locally%2520it%2520utilizes%2520the%2520space%2520of%2520locally%2520missing%2520classes%2520for%2520locally%250Aexisting%2520classes.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520a%2520range%2520of%2520datasets%2520to%250Ademonstrate%2520that%2520our%2520FedGELA%2520achieves%2520promising%2520performance~%2528averaged%250Aimprovement%2520of%25203.9%2525%2520to%2520FedAvg%2520and%25201.5%2525%2520to%2520best%2520baselines%2529%2520and%2520provide%2520both%250Alocal%2520and%2520global%2520convergence%2520guarantees.%2520Source%2520code%2520is%2520available%250Aat%253Ahttps%253A//github.com/MediaBrain-SJTU/FedGELA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20with%20Bilateral%20Curation%20for%20Partially%20Class-Disjoint%0A%20%20Data&entry.906535625=Ziqing%20Fan%20and%20Ruipeng%20Zhang%20and%20Jiangchao%20Yao%20and%20Bo%20Han%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Partially%20class-disjoint%20data%20%28PCDD%29%2C%20a%20common%20yet%20under-explored%20data%0Aformation%20where%20each%20client%20contributes%20a%20part%20of%20classes%20%28instead%20of%20all%0Aclasses%29%20of%20samples%2C%20severely%20challenges%20the%20performance%20of%20federated%0Aalgorithms.%20Without%20full%20classes%2C%20the%20local%20objective%20will%20contradict%20the%0Aglobal%20objective%2C%20yielding%20the%20angle%20collapse%20problem%20for%20locally%20missing%0Aclasses%20and%20the%20space%20waste%20problem%20for%20locally%20existing%20classes.%20As%20far%20as%20we%0Aknow%2C%20none%20of%20the%20existing%20methods%20can%20intrinsically%20mitigate%20PCDD%20challenges%0Ato%20achieve%20holistic%20improvement%20in%20the%20bilateral%20views%20%28both%20global%20view%20and%0Alocal%20view%29%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%20are%20inspired%20by%0Athe%20strong%20generalization%20of%20simplex%20Equiangular%20Tight%20Frame~%28ETF%29%20on%20the%0Aimbalanced%20data%2C%20and%20propose%20a%20novel%20approach%20called%20FedGELA%20where%20the%0Aclassifier%20is%20globally%20fixed%20as%20a%20simplex%20ETF%20while%20locally%20adapted%20to%20the%0Apersonal%20distributions.%20Globally%2C%20FedGELA%20provides%20fair%20and%20equal%0Adiscrimination%20for%20all%20classes%20and%20avoids%20inaccurate%20updates%20of%20the%20classifier%2C%0Awhile%20locally%20it%20utilizes%20the%20space%20of%20locally%20missing%20classes%20for%20locally%0Aexisting%20classes.%20We%20conduct%20extensive%20experiments%20on%20a%20range%20of%20datasets%20to%0Ademonstrate%20that%20our%20FedGELA%20achieves%20promising%20performance~%28averaged%0Aimprovement%20of%203.9%25%20to%20FedAvg%20and%201.5%25%20to%20best%20baselines%29%20and%20provide%20both%0Alocal%20and%20global%20convergence%20guarantees.%20Source%20code%20is%20available%0Aat%3Ahttps%3A//github.com/MediaBrain-SJTU/FedGELA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18972v1&entry.124074799=Read"},
{"title": "HawkVision: Low-Latency Modeless Edge AI Serving", "author": "ChonLam Lao and Jiaqi Gao and Ganesh Ananthanarayanan and Aditya Akella and Minlan Yu", "abstract": "  The trend of modeless ML inference is increasingly growing in popularity as\nit hides the complexity of model inference from users and caters to diverse\nuser and application accuracy requirements. Previous work mostly focuses on\nmodeless inference in data centers. To provide low-latency inference, in this\npaper, we promote modeless inference at the edge. The edge environment\nintroduces additional challenges related to low power consumption, limited\ndevice memory, and volatile network environments.\n  To address these challenges, we propose HawkVision, which provides\nlow-latency modeless serving of vision DNNs. HawkVision leverages a two-layer\nedge-DC architecture that employs confidence scaling to reduce the number of\nmodel options while meeting diverse accuracy requirements. It also supports\nlossy inference under volatile network environments. Our experimental results\nshow that HawkVision outperforms current serving systems by up to 1.6X in P99\nlatency for providing modeless service. Our FPGA prototype demonstrates similar\nperformance at certain accuracy levels with up to a 3.34X reduction in power\nconsumption.\n", "link": "http://arxiv.org/abs/2405.19213v1", "date": "2024-05-29", "relevancy": 2.1069, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5927}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5163}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HawkVision%3A%20Low-Latency%20Modeless%20Edge%20AI%20Serving&body=Title%3A%20HawkVision%3A%20Low-Latency%20Modeless%20Edge%20AI%20Serving%0AAuthor%3A%20ChonLam%20Lao%20and%20Jiaqi%20Gao%20and%20Ganesh%20Ananthanarayanan%20and%20Aditya%20Akella%20and%20Minlan%20Yu%0AAbstract%3A%20%20%20The%20trend%20of%20modeless%20ML%20inference%20is%20increasingly%20growing%20in%20popularity%20as%0Ait%20hides%20the%20complexity%20of%20model%20inference%20from%20users%20and%20caters%20to%20diverse%0Auser%20and%20application%20accuracy%20requirements.%20Previous%20work%20mostly%20focuses%20on%0Amodeless%20inference%20in%20data%20centers.%20To%20provide%20low-latency%20inference%2C%20in%20this%0Apaper%2C%20we%20promote%20modeless%20inference%20at%20the%20edge.%20The%20edge%20environment%0Aintroduces%20additional%20challenges%20related%20to%20low%20power%20consumption%2C%20limited%0Adevice%20memory%2C%20and%20volatile%20network%20environments.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20HawkVision%2C%20which%20provides%0Alow-latency%20modeless%20serving%20of%20vision%20DNNs.%20HawkVision%20leverages%20a%20two-layer%0Aedge-DC%20architecture%20that%20employs%20confidence%20scaling%20to%20reduce%20the%20number%20of%0Amodel%20options%20while%20meeting%20diverse%20accuracy%20requirements.%20It%20also%20supports%0Alossy%20inference%20under%20volatile%20network%20environments.%20Our%20experimental%20results%0Ashow%20that%20HawkVision%20outperforms%20current%20serving%20systems%20by%20up%20to%201.6X%20in%20P99%0Alatency%20for%20providing%20modeless%20service.%20Our%20FPGA%20prototype%20demonstrates%20similar%0Aperformance%20at%20certain%20accuracy%20levels%20with%20up%20to%20a%203.34X%20reduction%20in%20power%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHawkVision%253A%2520Low-Latency%2520Modeless%2520Edge%2520AI%2520Serving%26entry.906535625%3DChonLam%2520Lao%2520and%2520Jiaqi%2520Gao%2520and%2520Ganesh%2520Ananthanarayanan%2520and%2520Aditya%2520Akella%2520and%2520Minlan%2520Yu%26entry.1292438233%3D%2520%2520The%2520trend%2520of%2520modeless%2520ML%2520inference%2520is%2520increasingly%2520growing%2520in%2520popularity%2520as%250Ait%2520hides%2520the%2520complexity%2520of%2520model%2520inference%2520from%2520users%2520and%2520caters%2520to%2520diverse%250Auser%2520and%2520application%2520accuracy%2520requirements.%2520Previous%2520work%2520mostly%2520focuses%2520on%250Amodeless%2520inference%2520in%2520data%2520centers.%2520To%2520provide%2520low-latency%2520inference%252C%2520in%2520this%250Apaper%252C%2520we%2520promote%2520modeless%2520inference%2520at%2520the%2520edge.%2520The%2520edge%2520environment%250Aintroduces%2520additional%2520challenges%2520related%2520to%2520low%2520power%2520consumption%252C%2520limited%250Adevice%2520memory%252C%2520and%2520volatile%2520network%2520environments.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520HawkVision%252C%2520which%2520provides%250Alow-latency%2520modeless%2520serving%2520of%2520vision%2520DNNs.%2520HawkVision%2520leverages%2520a%2520two-layer%250Aedge-DC%2520architecture%2520that%2520employs%2520confidence%2520scaling%2520to%2520reduce%2520the%2520number%2520of%250Amodel%2520options%2520while%2520meeting%2520diverse%2520accuracy%2520requirements.%2520It%2520also%2520supports%250Alossy%2520inference%2520under%2520volatile%2520network%2520environments.%2520Our%2520experimental%2520results%250Ashow%2520that%2520HawkVision%2520outperforms%2520current%2520serving%2520systems%2520by%2520up%2520to%25201.6X%2520in%2520P99%250Alatency%2520for%2520providing%2520modeless%2520service.%2520Our%2520FPGA%2520prototype%2520demonstrates%2520similar%250Aperformance%2520at%2520certain%2520accuracy%2520levels%2520with%2520up%2520to%2520a%25203.34X%2520reduction%2520in%2520power%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HawkVision%3A%20Low-Latency%20Modeless%20Edge%20AI%20Serving&entry.906535625=ChonLam%20Lao%20and%20Jiaqi%20Gao%20and%20Ganesh%20Ananthanarayanan%20and%20Aditya%20Akella%20and%20Minlan%20Yu&entry.1292438233=%20%20The%20trend%20of%20modeless%20ML%20inference%20is%20increasingly%20growing%20in%20popularity%20as%0Ait%20hides%20the%20complexity%20of%20model%20inference%20from%20users%20and%20caters%20to%20diverse%0Auser%20and%20application%20accuracy%20requirements.%20Previous%20work%20mostly%20focuses%20on%0Amodeless%20inference%20in%20data%20centers.%20To%20provide%20low-latency%20inference%2C%20in%20this%0Apaper%2C%20we%20promote%20modeless%20inference%20at%20the%20edge.%20The%20edge%20environment%0Aintroduces%20additional%20challenges%20related%20to%20low%20power%20consumption%2C%20limited%0Adevice%20memory%2C%20and%20volatile%20network%20environments.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20HawkVision%2C%20which%20provides%0Alow-latency%20modeless%20serving%20of%20vision%20DNNs.%20HawkVision%20leverages%20a%20two-layer%0Aedge-DC%20architecture%20that%20employs%20confidence%20scaling%20to%20reduce%20the%20number%20of%0Amodel%20options%20while%20meeting%20diverse%20accuracy%20requirements.%20It%20also%20supports%0Alossy%20inference%20under%20volatile%20network%20environments.%20Our%20experimental%20results%0Ashow%20that%20HawkVision%20outperforms%20current%20serving%20systems%20by%20up%20to%201.6X%20in%20P99%0Alatency%20for%20providing%20modeless%20service.%20Our%20FPGA%20prototype%20demonstrates%20similar%0Aperformance%20at%20certain%20accuracy%20levels%20with%20up%20to%20a%203.34X%20reduction%20in%20power%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19213v1&entry.124074799=Read"},
{"title": "Physics-Aware Neural Implicit Solvers for multiscale, parametric PDEs\n  with applications in heterogeneous media", "author": "Matthaios Chatzopoulos and Phaedon-Stelios Koutsourelakis", "abstract": "  We propose Physics-Aware Neural Implicit Solvers (PANIS), a novel,\ndata-driven framework for learning surrogates for parametrized Partial\nDifferential Equations (PDEs). It consists of a probabilistic, learning\nobjective in which weighted residuals are used to probe the PDE and provide a\nsource of {\\em virtual} data i.e. the actual PDE never needs to be solved. This\nis combined with a physics-aware implicit solver that consists of a much\ncoarser, discretized version of the original PDE, which provides the requisite\ninformation bottleneck for high-dimensional problems and enables generalization\nin out-of-distribution settings (e.g. different boundary conditions). We\ndemonstrate its capability in the context of random heterogeneous materials\nwhere the input parameters represent the material microstructure. We extend the\nframework to multiscale problems and show that a surrogate can be learned for\nthe effective (homogenized) solution without ever solving the reference\nproblem. We further demonstrate how the proposed framework can accommodate and\ngeneralize several existing learning objectives and architectures while\nyielding probabilistic surrogates that can quantify predictive uncertainty.\n", "link": "http://arxiv.org/abs/2405.19019v1", "date": "2024-05-29", "relevancy": 2.1054, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5383}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Aware%20Neural%20Implicit%20Solvers%20for%20multiscale%2C%20parametric%20PDEs%0A%20%20with%20applications%20in%20heterogeneous%20media&body=Title%3A%20Physics-Aware%20Neural%20Implicit%20Solvers%20for%20multiscale%2C%20parametric%20PDEs%0A%20%20with%20applications%20in%20heterogeneous%20media%0AAuthor%3A%20Matthaios%20Chatzopoulos%20and%20Phaedon-Stelios%20Koutsourelakis%0AAbstract%3A%20%20%20We%20propose%20Physics-Aware%20Neural%20Implicit%20Solvers%20%28PANIS%29%2C%20a%20novel%2C%0Adata-driven%20framework%20for%20learning%20surrogates%20for%20parametrized%20Partial%0ADifferential%20Equations%20%28PDEs%29.%20It%20consists%20of%20a%20probabilistic%2C%20learning%0Aobjective%20in%20which%20weighted%20residuals%20are%20used%20to%20probe%20the%20PDE%20and%20provide%20a%0Asource%20of%20%7B%5Cem%20virtual%7D%20data%20i.e.%20the%20actual%20PDE%20never%20needs%20to%20be%20solved.%20This%0Ais%20combined%20with%20a%20physics-aware%20implicit%20solver%20that%20consists%20of%20a%20much%0Acoarser%2C%20discretized%20version%20of%20the%20original%20PDE%2C%20which%20provides%20the%20requisite%0Ainformation%20bottleneck%20for%20high-dimensional%20problems%20and%20enables%20generalization%0Ain%20out-of-distribution%20settings%20%28e.g.%20different%20boundary%20conditions%29.%20We%0Ademonstrate%20its%20capability%20in%20the%20context%20of%20random%20heterogeneous%20materials%0Awhere%20the%20input%20parameters%20represent%20the%20material%20microstructure.%20We%20extend%20the%0Aframework%20to%20multiscale%20problems%20and%20show%20that%20a%20surrogate%20can%20be%20learned%20for%0Athe%20effective%20%28homogenized%29%20solution%20without%20ever%20solving%20the%20reference%0Aproblem.%20We%20further%20demonstrate%20how%20the%20proposed%20framework%20can%20accommodate%20and%0Ageneralize%20several%20existing%20learning%20objectives%20and%20architectures%20while%0Ayielding%20probabilistic%20surrogates%20that%20can%20quantify%20predictive%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Aware%2520Neural%2520Implicit%2520Solvers%2520for%2520multiscale%252C%2520parametric%2520PDEs%250A%2520%2520with%2520applications%2520in%2520heterogeneous%2520media%26entry.906535625%3DMatthaios%2520Chatzopoulos%2520and%2520Phaedon-Stelios%2520Koutsourelakis%26entry.1292438233%3D%2520%2520We%2520propose%2520Physics-Aware%2520Neural%2520Implicit%2520Solvers%2520%2528PANIS%2529%252C%2520a%2520novel%252C%250Adata-driven%2520framework%2520for%2520learning%2520surrogates%2520for%2520parametrized%2520Partial%250ADifferential%2520Equations%2520%2528PDEs%2529.%2520It%2520consists%2520of%2520a%2520probabilistic%252C%2520learning%250Aobjective%2520in%2520which%2520weighted%2520residuals%2520are%2520used%2520to%2520probe%2520the%2520PDE%2520and%2520provide%2520a%250Asource%2520of%2520%257B%255Cem%2520virtual%257D%2520data%2520i.e.%2520the%2520actual%2520PDE%2520never%2520needs%2520to%2520be%2520solved.%2520This%250Ais%2520combined%2520with%2520a%2520physics-aware%2520implicit%2520solver%2520that%2520consists%2520of%2520a%2520much%250Acoarser%252C%2520discretized%2520version%2520of%2520the%2520original%2520PDE%252C%2520which%2520provides%2520the%2520requisite%250Ainformation%2520bottleneck%2520for%2520high-dimensional%2520problems%2520and%2520enables%2520generalization%250Ain%2520out-of-distribution%2520settings%2520%2528e.g.%2520different%2520boundary%2520conditions%2529.%2520We%250Ademonstrate%2520its%2520capability%2520in%2520the%2520context%2520of%2520random%2520heterogeneous%2520materials%250Awhere%2520the%2520input%2520parameters%2520represent%2520the%2520material%2520microstructure.%2520We%2520extend%2520the%250Aframework%2520to%2520multiscale%2520problems%2520and%2520show%2520that%2520a%2520surrogate%2520can%2520be%2520learned%2520for%250Athe%2520effective%2520%2528homogenized%2529%2520solution%2520without%2520ever%2520solving%2520the%2520reference%250Aproblem.%2520We%2520further%2520demonstrate%2520how%2520the%2520proposed%2520framework%2520can%2520accommodate%2520and%250Ageneralize%2520several%2520existing%2520learning%2520objectives%2520and%2520architectures%2520while%250Ayielding%2520probabilistic%2520surrogates%2520that%2520can%2520quantify%2520predictive%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Aware%20Neural%20Implicit%20Solvers%20for%20multiscale%2C%20parametric%20PDEs%0A%20%20with%20applications%20in%20heterogeneous%20media&entry.906535625=Matthaios%20Chatzopoulos%20and%20Phaedon-Stelios%20Koutsourelakis&entry.1292438233=%20%20We%20propose%20Physics-Aware%20Neural%20Implicit%20Solvers%20%28PANIS%29%2C%20a%20novel%2C%0Adata-driven%20framework%20for%20learning%20surrogates%20for%20parametrized%20Partial%0ADifferential%20Equations%20%28PDEs%29.%20It%20consists%20of%20a%20probabilistic%2C%20learning%0Aobjective%20in%20which%20weighted%20residuals%20are%20used%20to%20probe%20the%20PDE%20and%20provide%20a%0Asource%20of%20%7B%5Cem%20virtual%7D%20data%20i.e.%20the%20actual%20PDE%20never%20needs%20to%20be%20solved.%20This%0Ais%20combined%20with%20a%20physics-aware%20implicit%20solver%20that%20consists%20of%20a%20much%0Acoarser%2C%20discretized%20version%20of%20the%20original%20PDE%2C%20which%20provides%20the%20requisite%0Ainformation%20bottleneck%20for%20high-dimensional%20problems%20and%20enables%20generalization%0Ain%20out-of-distribution%20settings%20%28e.g.%20different%20boundary%20conditions%29.%20We%0Ademonstrate%20its%20capability%20in%20the%20context%20of%20random%20heterogeneous%20materials%0Awhere%20the%20input%20parameters%20represent%20the%20material%20microstructure.%20We%20extend%20the%0Aframework%20to%20multiscale%20problems%20and%20show%20that%20a%20surrogate%20can%20be%20learned%20for%0Athe%20effective%20%28homogenized%29%20solution%20without%20ever%20solving%20the%20reference%0Aproblem.%20We%20further%20demonstrate%20how%20the%20proposed%20framework%20can%20accommodate%20and%0Ageneralize%20several%20existing%20learning%20objectives%20and%20architectures%20while%0Ayielding%20probabilistic%20surrogates%20that%20can%20quantify%20predictive%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19019v1&entry.124074799=Read"},
{"title": "SDPRLayers: Certifiable Backpropagation Through Polynomial Optimization\n  Problems in Robotics", "author": "Connor Holmes and Frederike D\u00fcmbgen and Timothy D. Barfoot", "abstract": "  Differentiable optimization is a powerful new paradigm capable of reconciling\nmodel-based and learning-based approaches in robotics. However, the majority of\nrobotics optimization problems are non-convex and current differentiable\noptimization techniques are therefore prone to convergence to local minima.\nWhen this occurs, the gradients provided by these existing solvers can be\nwildly inaccurate and will ultimately corrupt the training process. On the\nother hand, any non-convex robotics problems can be framed as polynomial\noptimization problems and, in turn, admit convex relaxations that can be used\nto recover a global solution via so-called certifiably correct methods. We\npresent SDPRLayers, an approach that leverages these methods as well as\nstate-of-the-art convex implicit differentiation techniques to provide\ncertifiably correct gradients throughout the training process. We introduce\nthis approach and showcase theoretical results that provide conditions under\nwhich correctness of the gradients is guaranteed. We demonstrate our approach\non two simple-but-demonstrative simulated examples, which expose the potential\npitfalls of existing, state-of-the-art, differentiable optimization methods. We\napply our method in a real-world application: we train a deep neural network to\ndetect image keypoints for robot localization in challenging lighting\nconditions. An open-source, PyTorch implementation of SDPRLayers will be made\navailable upon paper acceptance.\n", "link": "http://arxiv.org/abs/2405.19309v1", "date": "2024-05-29", "relevancy": 2.104, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5724}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5191}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDPRLayers%3A%20Certifiable%20Backpropagation%20Through%20Polynomial%20Optimization%0A%20%20Problems%20in%20Robotics&body=Title%3A%20SDPRLayers%3A%20Certifiable%20Backpropagation%20Through%20Polynomial%20Optimization%0A%20%20Problems%20in%20Robotics%0AAuthor%3A%20Connor%20Holmes%20and%20Frederike%20D%C3%BCmbgen%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Differentiable%20optimization%20is%20a%20powerful%20new%20paradigm%20capable%20of%20reconciling%0Amodel-based%20and%20learning-based%20approaches%20in%20robotics.%20However%2C%20the%20majority%20of%0Arobotics%20optimization%20problems%20are%20non-convex%20and%20current%20differentiable%0Aoptimization%20techniques%20are%20therefore%20prone%20to%20convergence%20to%20local%20minima.%0AWhen%20this%20occurs%2C%20the%20gradients%20provided%20by%20these%20existing%20solvers%20can%20be%0Awildly%20inaccurate%20and%20will%20ultimately%20corrupt%20the%20training%20process.%20On%20the%0Aother%20hand%2C%20any%20non-convex%20robotics%20problems%20can%20be%20framed%20as%20polynomial%0Aoptimization%20problems%20and%2C%20in%20turn%2C%20admit%20convex%20relaxations%20that%20can%20be%20used%0Ato%20recover%20a%20global%20solution%20via%20so-called%20certifiably%20correct%20methods.%20We%0Apresent%20SDPRLayers%2C%20an%20approach%20that%20leverages%20these%20methods%20as%20well%20as%0Astate-of-the-art%20convex%20implicit%20differentiation%20techniques%20to%20provide%0Acertifiably%20correct%20gradients%20throughout%20the%20training%20process.%20We%20introduce%0Athis%20approach%20and%20showcase%20theoretical%20results%20that%20provide%20conditions%20under%0Awhich%20correctness%20of%20the%20gradients%20is%20guaranteed.%20We%20demonstrate%20our%20approach%0Aon%20two%20simple-but-demonstrative%20simulated%20examples%2C%20which%20expose%20the%20potential%0Apitfalls%20of%20existing%2C%20state-of-the-art%2C%20differentiable%20optimization%20methods.%20We%0Aapply%20our%20method%20in%20a%20real-world%20application%3A%20we%20train%20a%20deep%20neural%20network%20to%0Adetect%20image%20keypoints%20for%20robot%20localization%20in%20challenging%20lighting%0Aconditions.%20An%20open-source%2C%20PyTorch%20implementation%20of%20SDPRLayers%20will%20be%20made%0Aavailable%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDPRLayers%253A%2520Certifiable%2520Backpropagation%2520Through%2520Polynomial%2520Optimization%250A%2520%2520Problems%2520in%2520Robotics%26entry.906535625%3DConnor%2520Holmes%2520and%2520Frederike%2520D%25C3%25BCmbgen%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Differentiable%2520optimization%2520is%2520a%2520powerful%2520new%2520paradigm%2520capable%2520of%2520reconciling%250Amodel-based%2520and%2520learning-based%2520approaches%2520in%2520robotics.%2520However%252C%2520the%2520majority%2520of%250Arobotics%2520optimization%2520problems%2520are%2520non-convex%2520and%2520current%2520differentiable%250Aoptimization%2520techniques%2520are%2520therefore%2520prone%2520to%2520convergence%2520to%2520local%2520minima.%250AWhen%2520this%2520occurs%252C%2520the%2520gradients%2520provided%2520by%2520these%2520existing%2520solvers%2520can%2520be%250Awildly%2520inaccurate%2520and%2520will%2520ultimately%2520corrupt%2520the%2520training%2520process.%2520On%2520the%250Aother%2520hand%252C%2520any%2520non-convex%2520robotics%2520problems%2520can%2520be%2520framed%2520as%2520polynomial%250Aoptimization%2520problems%2520and%252C%2520in%2520turn%252C%2520admit%2520convex%2520relaxations%2520that%2520can%2520be%2520used%250Ato%2520recover%2520a%2520global%2520solution%2520via%2520so-called%2520certifiably%2520correct%2520methods.%2520We%250Apresent%2520SDPRLayers%252C%2520an%2520approach%2520that%2520leverages%2520these%2520methods%2520as%2520well%2520as%250Astate-of-the-art%2520convex%2520implicit%2520differentiation%2520techniques%2520to%2520provide%250Acertifiably%2520correct%2520gradients%2520throughout%2520the%2520training%2520process.%2520We%2520introduce%250Athis%2520approach%2520and%2520showcase%2520theoretical%2520results%2520that%2520provide%2520conditions%2520under%250Awhich%2520correctness%2520of%2520the%2520gradients%2520is%2520guaranteed.%2520We%2520demonstrate%2520our%2520approach%250Aon%2520two%2520simple-but-demonstrative%2520simulated%2520examples%252C%2520which%2520expose%2520the%2520potential%250Apitfalls%2520of%2520existing%252C%2520state-of-the-art%252C%2520differentiable%2520optimization%2520methods.%2520We%250Aapply%2520our%2520method%2520in%2520a%2520real-world%2520application%253A%2520we%2520train%2520a%2520deep%2520neural%2520network%2520to%250Adetect%2520image%2520keypoints%2520for%2520robot%2520localization%2520in%2520challenging%2520lighting%250Aconditions.%2520An%2520open-source%252C%2520PyTorch%2520implementation%2520of%2520SDPRLayers%2520will%2520be%2520made%250Aavailable%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDPRLayers%3A%20Certifiable%20Backpropagation%20Through%20Polynomial%20Optimization%0A%20%20Problems%20in%20Robotics&entry.906535625=Connor%20Holmes%20and%20Frederike%20D%C3%BCmbgen%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Differentiable%20optimization%20is%20a%20powerful%20new%20paradigm%20capable%20of%20reconciling%0Amodel-based%20and%20learning-based%20approaches%20in%20robotics.%20However%2C%20the%20majority%20of%0Arobotics%20optimization%20problems%20are%20non-convex%20and%20current%20differentiable%0Aoptimization%20techniques%20are%20therefore%20prone%20to%20convergence%20to%20local%20minima.%0AWhen%20this%20occurs%2C%20the%20gradients%20provided%20by%20these%20existing%20solvers%20can%20be%0Awildly%20inaccurate%20and%20will%20ultimately%20corrupt%20the%20training%20process.%20On%20the%0Aother%20hand%2C%20any%20non-convex%20robotics%20problems%20can%20be%20framed%20as%20polynomial%0Aoptimization%20problems%20and%2C%20in%20turn%2C%20admit%20convex%20relaxations%20that%20can%20be%20used%0Ato%20recover%20a%20global%20solution%20via%20so-called%20certifiably%20correct%20methods.%20We%0Apresent%20SDPRLayers%2C%20an%20approach%20that%20leverages%20these%20methods%20as%20well%20as%0Astate-of-the-art%20convex%20implicit%20differentiation%20techniques%20to%20provide%0Acertifiably%20correct%20gradients%20throughout%20the%20training%20process.%20We%20introduce%0Athis%20approach%20and%20showcase%20theoretical%20results%20that%20provide%20conditions%20under%0Awhich%20correctness%20of%20the%20gradients%20is%20guaranteed.%20We%20demonstrate%20our%20approach%0Aon%20two%20simple-but-demonstrative%20simulated%20examples%2C%20which%20expose%20the%20potential%0Apitfalls%20of%20existing%2C%20state-of-the-art%2C%20differentiable%20optimization%20methods.%20We%0Aapply%20our%20method%20in%20a%20real-world%20application%3A%20we%20train%20a%20deep%20neural%20network%20to%0Adetect%20image%20keypoints%20for%20robot%20localization%20in%20challenging%20lighting%0Aconditions.%20An%20open-source%2C%20PyTorch%20implementation%20of%20SDPRLayers%20will%20be%20made%0Aavailable%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19309v1&entry.124074799=Read"},
{"title": "LLMs Meet Multimodal Generation and Editing: A Survey", "author": "Yingqing He and Zhaoyang Liu and Jingye Chen and Zeyue Tian and Hongyu Liu and Xiaowei Chi and Runtao Liu and Ruibin Yuan and Yazhou Xing and Wenhai Wang and Jifeng Dai and Yong Zhang and Wei Xue and Qifeng Liu and Yike Guo and Qifeng Chen", "abstract": "  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on understanding. This\nsurvey elaborates on multimodal generation across different domains, including\nimage, video, 3D, and audio, where we highlight the notable advancements with\nmilestone works in these fields. Specifically, we exhaustively investigate the\nkey technical components behind methods and multimodal datasets utilized in\nthese studies. Moreover, we dig into tool-augmented multimodal agents that can\nuse existing generative models for human-computer interaction. Lastly, we also\ncomprehensively discuss the advancement in AI safety and investigate emerging\napplications as well as future prospects. Our work provides a systematic and\ninsightful overview of multimodal generation, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n", "link": "http://arxiv.org/abs/2405.19334v1", "date": "2024-05-29", "relevancy": 2.0998, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5354}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5277}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Meet%20Multimodal%20Generation%20and%20Editing%3A%20A%20Survey&body=Title%3A%20LLMs%20Meet%20Multimodal%20Generation%20and%20Editing%3A%20A%20Survey%0AAuthor%3A%20Yingqing%20He%20and%20Zhaoyang%20Liu%20and%20Jingye%20Chen%20and%20Zeyue%20Tian%20and%20Hongyu%20Liu%20and%20Xiaowei%20Chi%20and%20Runtao%20Liu%20and%20Ruibin%20Yuan%20and%20Yazhou%20Xing%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Yong%20Zhang%20and%20Wei%20Xue%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20With%20the%20recent%20advancement%20in%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20a%0Agrowing%20interest%20in%20combining%20LLMs%20with%20multimodal%20learning.%20Previous%20surveys%0Aof%20multimodal%20large%20language%20models%20%28MLLMs%29%20mainly%20focus%20on%20understanding.%20This%0Asurvey%20elaborates%20on%20multimodal%20generation%20across%20different%20domains%2C%20including%0Aimage%2C%20video%2C%203D%2C%20and%20audio%2C%20where%20we%20highlight%20the%20notable%20advancements%20with%0Amilestone%20works%20in%20these%20fields.%20Specifically%2C%20we%20exhaustively%20investigate%20the%0Akey%20technical%20components%20behind%20methods%20and%20multimodal%20datasets%20utilized%20in%0Athese%20studies.%20Moreover%2C%20we%20dig%20into%20tool-augmented%20multimodal%20agents%20that%20can%0Ause%20existing%20generative%20models%20for%20human-computer%20interaction.%20Lastly%2C%20we%20also%0Acomprehensively%20discuss%20the%20advancement%20in%20AI%20safety%20and%20investigate%20emerging%0Aapplications%20as%20well%20as%20future%20prospects.%20Our%20work%20provides%20a%20systematic%20and%0Ainsightful%20overview%20of%20multimodal%20generation%2C%20which%20is%20expected%20to%20advance%20the%0Adevelopment%20of%20Artificial%20Intelligence%20for%20Generative%20Content%20%28AIGC%29%20and%20world%0Amodels.%20A%20curated%20list%20of%20all%20related%20papers%20can%20be%20found%20at%0Ahttps%3A//github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Meet%2520Multimodal%2520Generation%2520and%2520Editing%253A%2520A%2520Survey%26entry.906535625%3DYingqing%2520He%2520and%2520Zhaoyang%2520Liu%2520and%2520Jingye%2520Chen%2520and%2520Zeyue%2520Tian%2520and%2520Hongyu%2520Liu%2520and%2520Xiaowei%2520Chi%2520and%2520Runtao%2520Liu%2520and%2520Ruibin%2520Yuan%2520and%2520Yazhou%2520Xing%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Yong%2520Zhang%2520and%2520Wei%2520Xue%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520recent%2520advancement%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%2520is%2520a%250Agrowing%2520interest%2520in%2520combining%2520LLMs%2520with%2520multimodal%2520learning.%2520Previous%2520surveys%250Aof%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520mainly%2520focus%2520on%2520understanding.%2520This%250Asurvey%2520elaborates%2520on%2520multimodal%2520generation%2520across%2520different%2520domains%252C%2520including%250Aimage%252C%2520video%252C%25203D%252C%2520and%2520audio%252C%2520where%2520we%2520highlight%2520the%2520notable%2520advancements%2520with%250Amilestone%2520works%2520in%2520these%2520fields.%2520Specifically%252C%2520we%2520exhaustively%2520investigate%2520the%250Akey%2520technical%2520components%2520behind%2520methods%2520and%2520multimodal%2520datasets%2520utilized%2520in%250Athese%2520studies.%2520Moreover%252C%2520we%2520dig%2520into%2520tool-augmented%2520multimodal%2520agents%2520that%2520can%250Ause%2520existing%2520generative%2520models%2520for%2520human-computer%2520interaction.%2520Lastly%252C%2520we%2520also%250Acomprehensively%2520discuss%2520the%2520advancement%2520in%2520AI%2520safety%2520and%2520investigate%2520emerging%250Aapplications%2520as%2520well%2520as%2520future%2520prospects.%2520Our%2520work%2520provides%2520a%2520systematic%2520and%250Ainsightful%2520overview%2520of%2520multimodal%2520generation%252C%2520which%2520is%2520expected%2520to%2520advance%2520the%250Adevelopment%2520of%2520Artificial%2520Intelligence%2520for%2520Generative%2520Content%2520%2528AIGC%2529%2520and%2520world%250Amodels.%2520A%2520curated%2520list%2520of%2520all%2520related%2520papers%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Meet%20Multimodal%20Generation%20and%20Editing%3A%20A%20Survey&entry.906535625=Yingqing%20He%20and%20Zhaoyang%20Liu%20and%20Jingye%20Chen%20and%20Zeyue%20Tian%20and%20Hongyu%20Liu%20and%20Xiaowei%20Chi%20and%20Runtao%20Liu%20and%20Ruibin%20Yuan%20and%20Yazhou%20Xing%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Yong%20Zhang%20and%20Wei%20Xue%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Qifeng%20Chen&entry.1292438233=%20%20With%20the%20recent%20advancement%20in%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20a%0Agrowing%20interest%20in%20combining%20LLMs%20with%20multimodal%20learning.%20Previous%20surveys%0Aof%20multimodal%20large%20language%20models%20%28MLLMs%29%20mainly%20focus%20on%20understanding.%20This%0Asurvey%20elaborates%20on%20multimodal%20generation%20across%20different%20domains%2C%20including%0Aimage%2C%20video%2C%203D%2C%20and%20audio%2C%20where%20we%20highlight%20the%20notable%20advancements%20with%0Amilestone%20works%20in%20these%20fields.%20Specifically%2C%20we%20exhaustively%20investigate%20the%0Akey%20technical%20components%20behind%20methods%20and%20multimodal%20datasets%20utilized%20in%0Athese%20studies.%20Moreover%2C%20we%20dig%20into%20tool-augmented%20multimodal%20agents%20that%20can%0Ause%20existing%20generative%20models%20for%20human-computer%20interaction.%20Lastly%2C%20we%20also%0Acomprehensively%20discuss%20the%20advancement%20in%20AI%20safety%20and%20investigate%20emerging%0Aapplications%20as%20well%20as%20future%20prospects.%20Our%20work%20provides%20a%20systematic%20and%0Ainsightful%20overview%20of%20multimodal%20generation%2C%20which%20is%20expected%20to%20advance%20the%0Adevelopment%20of%20Artificial%20Intelligence%20for%20Generative%20Content%20%28AIGC%29%20and%20world%0Amodels.%20A%20curated%20list%20of%20all%20related%20papers%20can%20be%20found%20at%0Ahttps%3A//github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19334v1&entry.124074799=Read"},
{"title": "Matrix Manifold Neural Networks++", "author": "Xuan Son Nguyen and Shuo Yang and Aymeric Histace", "abstract": "  Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing\ninterest in various applied areas. For instance, DNNs on spherical and\nhyperbolic manifolds have been designed to solve a wide range of computer\nvision and nature language processing tasks. One of the key factors that\ncontribute to the success of these networks is that spherical and hyperbolic\nmanifolds have the rich algebraic structures of gyrogroups and gyrovector\nspaces. This enables principled and effective generalizations of the most\nsuccessful DNNs to these manifolds. Recently, some works have shown that many\nconcepts in the theory of gyrogroups and gyrovector spaces can also be\ngeneralized to matrix manifolds such as Symmetric Positive Definite (SPD) and\nGrassmann manifolds. As a result, some building blocks for SPD and Grassmann\nneural networks, e.g., isometric models and multinomial logistic regression\n(MLR) can be derived in a way that is fully analogous to their spherical and\nhyperbolic counterparts. Building upon these works, we design fully-connected\n(FC) and convolutional layers for SPD neural networks. We also develop MLR on\nSymmetric Positive Semi-definite (SPSD) manifolds, and propose a method for\nperforming backpropagation with the Grassmann logarithmic map in the projector\nperspective. We demonstrate the effectiveness of the proposed approach in the\nhuman action recognition and node classification tasks.\n", "link": "http://arxiv.org/abs/2405.19206v1", "date": "2024-05-29", "relevancy": 2.0971, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5588}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix%20Manifold%20Neural%20Networks%2B%2B&body=Title%3A%20Matrix%20Manifold%20Neural%20Networks%2B%2B%0AAuthor%3A%20Xuan%20Son%20Nguyen%20and%20Shuo%20Yang%20and%20Aymeric%20Histace%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20on%20Riemannian%20manifolds%20have%20garnered%20increasing%0Ainterest%20in%20various%20applied%20areas.%20For%20instance%2C%20DNNs%20on%20spherical%20and%0Ahyperbolic%20manifolds%20have%20been%20designed%20to%20solve%20a%20wide%20range%20of%20computer%0Avision%20and%20nature%20language%20processing%20tasks.%20One%20of%20the%20key%20factors%20that%0Acontribute%20to%20the%20success%20of%20these%20networks%20is%20that%20spherical%20and%20hyperbolic%0Amanifolds%20have%20the%20rich%20algebraic%20structures%20of%20gyrogroups%20and%20gyrovector%0Aspaces.%20This%20enables%20principled%20and%20effective%20generalizations%20of%20the%20most%0Asuccessful%20DNNs%20to%20these%20manifolds.%20Recently%2C%20some%20works%20have%20shown%20that%20many%0Aconcepts%20in%20the%20theory%20of%20gyrogroups%20and%20gyrovector%20spaces%20can%20also%20be%0Ageneralized%20to%20matrix%20manifolds%20such%20as%20Symmetric%20Positive%20Definite%20%28SPD%29%20and%0AGrassmann%20manifolds.%20As%20a%20result%2C%20some%20building%20blocks%20for%20SPD%20and%20Grassmann%0Aneural%20networks%2C%20e.g.%2C%20isometric%20models%20and%20multinomial%20logistic%20regression%0A%28MLR%29%20can%20be%20derived%20in%20a%20way%20that%20is%20fully%20analogous%20to%20their%20spherical%20and%0Ahyperbolic%20counterparts.%20Building%20upon%20these%20works%2C%20we%20design%20fully-connected%0A%28FC%29%20and%20convolutional%20layers%20for%20SPD%20neural%20networks.%20We%20also%20develop%20MLR%20on%0ASymmetric%20Positive%20Semi-definite%20%28SPSD%29%20manifolds%2C%20and%20propose%20a%20method%20for%0Aperforming%20backpropagation%20with%20the%20Grassmann%20logarithmic%20map%20in%20the%20projector%0Aperspective.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20the%0Ahuman%20action%20recognition%20and%20node%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix%2520Manifold%2520Neural%2520Networks%252B%252B%26entry.906535625%3DXuan%2520Son%2520Nguyen%2520and%2520Shuo%2520Yang%2520and%2520Aymeric%2520Histace%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520on%2520Riemannian%2520manifolds%2520have%2520garnered%2520increasing%250Ainterest%2520in%2520various%2520applied%2520areas.%2520For%2520instance%252C%2520DNNs%2520on%2520spherical%2520and%250Ahyperbolic%2520manifolds%2520have%2520been%2520designed%2520to%2520solve%2520a%2520wide%2520range%2520of%2520computer%250Avision%2520and%2520nature%2520language%2520processing%2520tasks.%2520One%2520of%2520the%2520key%2520factors%2520that%250Acontribute%2520to%2520the%2520success%2520of%2520these%2520networks%2520is%2520that%2520spherical%2520and%2520hyperbolic%250Amanifolds%2520have%2520the%2520rich%2520algebraic%2520structures%2520of%2520gyrogroups%2520and%2520gyrovector%250Aspaces.%2520This%2520enables%2520principled%2520and%2520effective%2520generalizations%2520of%2520the%2520most%250Asuccessful%2520DNNs%2520to%2520these%2520manifolds.%2520Recently%252C%2520some%2520works%2520have%2520shown%2520that%2520many%250Aconcepts%2520in%2520the%2520theory%2520of%2520gyrogroups%2520and%2520gyrovector%2520spaces%2520can%2520also%2520be%250Ageneralized%2520to%2520matrix%2520manifolds%2520such%2520as%2520Symmetric%2520Positive%2520Definite%2520%2528SPD%2529%2520and%250AGrassmann%2520manifolds.%2520As%2520a%2520result%252C%2520some%2520building%2520blocks%2520for%2520SPD%2520and%2520Grassmann%250Aneural%2520networks%252C%2520e.g.%252C%2520isometric%2520models%2520and%2520multinomial%2520logistic%2520regression%250A%2528MLR%2529%2520can%2520be%2520derived%2520in%2520a%2520way%2520that%2520is%2520fully%2520analogous%2520to%2520their%2520spherical%2520and%250Ahyperbolic%2520counterparts.%2520Building%2520upon%2520these%2520works%252C%2520we%2520design%2520fully-connected%250A%2528FC%2529%2520and%2520convolutional%2520layers%2520for%2520SPD%2520neural%2520networks.%2520We%2520also%2520develop%2520MLR%2520on%250ASymmetric%2520Positive%2520Semi-definite%2520%2528SPSD%2529%2520manifolds%252C%2520and%2520propose%2520a%2520method%2520for%250Aperforming%2520backpropagation%2520with%2520the%2520Grassmann%2520logarithmic%2520map%2520in%2520the%2520projector%250Aperspective.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520in%2520the%250Ahuman%2520action%2520recognition%2520and%2520node%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix%20Manifold%20Neural%20Networks%2B%2B&entry.906535625=Xuan%20Son%20Nguyen%20and%20Shuo%20Yang%20and%20Aymeric%20Histace&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20on%20Riemannian%20manifolds%20have%20garnered%20increasing%0Ainterest%20in%20various%20applied%20areas.%20For%20instance%2C%20DNNs%20on%20spherical%20and%0Ahyperbolic%20manifolds%20have%20been%20designed%20to%20solve%20a%20wide%20range%20of%20computer%0Avision%20and%20nature%20language%20processing%20tasks.%20One%20of%20the%20key%20factors%20that%0Acontribute%20to%20the%20success%20of%20these%20networks%20is%20that%20spherical%20and%20hyperbolic%0Amanifolds%20have%20the%20rich%20algebraic%20structures%20of%20gyrogroups%20and%20gyrovector%0Aspaces.%20This%20enables%20principled%20and%20effective%20generalizations%20of%20the%20most%0Asuccessful%20DNNs%20to%20these%20manifolds.%20Recently%2C%20some%20works%20have%20shown%20that%20many%0Aconcepts%20in%20the%20theory%20of%20gyrogroups%20and%20gyrovector%20spaces%20can%20also%20be%0Ageneralized%20to%20matrix%20manifolds%20such%20as%20Symmetric%20Positive%20Definite%20%28SPD%29%20and%0AGrassmann%20manifolds.%20As%20a%20result%2C%20some%20building%20blocks%20for%20SPD%20and%20Grassmann%0Aneural%20networks%2C%20e.g.%2C%20isometric%20models%20and%20multinomial%20logistic%20regression%0A%28MLR%29%20can%20be%20derived%20in%20a%20way%20that%20is%20fully%20analogous%20to%20their%20spherical%20and%0Ahyperbolic%20counterparts.%20Building%20upon%20these%20works%2C%20we%20design%20fully-connected%0A%28FC%29%20and%20convolutional%20layers%20for%20SPD%20neural%20networks.%20We%20also%20develop%20MLR%20on%0ASymmetric%20Positive%20Semi-definite%20%28SPSD%29%20manifolds%2C%20and%20propose%20a%20method%20for%0Aperforming%20backpropagation%20with%20the%20Grassmann%20logarithmic%20map%20in%20the%20projector%0Aperspective.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20the%0Ahuman%20action%20recognition%20and%20node%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19206v1&entry.124074799=Read"},
{"title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos", "author": "Ziyang Wang and Shoubin Yu and Elias Stengel-Eskin and Jaehong Yoon and Feng Cheng and Gedas Bertasius and Mohit Bansal", "abstract": "  Video-language understanding tasks have focused on short video clips, often\nstruggling with long-form video understanding tasks. Recently, many long\nvideo-language understanding approaches have leveraged the reasoning\ncapabilities of Large Language Models (LLMs) to perform long video QA,\ntransforming videos into densely sampled frame captions, and asking LLMs to\nrespond to text queries over captions. However, the frames used for captioning\nare often redundant and contain irrelevant information, making dense sampling\ninefficient, and ignoring the fact that video QA requires varying levels of\ngranularity, with some video segments being highly relevant to the question\n(needing more fine-grained detail) while others being less relevant. Thus,\nthese LLM-based approaches are prone to missing information and operate on\nlarge numbers of irrelevant captions, lowering both performance and efficiency.\nTo address these issues, we introduce VideoTree, a query-adaptive and\nhierarchical framework for long-video understanding with LLMs. VideoTree\ndynamically extracts query-related information from a video and builds a\ntree-based representation for LLM reasoning. First, VideoTree adaptively\nselects frames for captioning by iteratively clustering frames based on their\nvisual features and scoring clusters using their relevance to the query.\nSecond, it organizes visual clusters into a query-adaptive and hierarchical\ntree structure; the tree encodes varying levels of granularity, with higher\nresolution on relevant segments. Finally, VideoTree produces an answer by\ntraversing the tree's keyframes and passing their captions to an LLM answerer.\nOur method improves both reasoning accuracy and efficiency compared to existing\nmethods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines\non the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while\nreducing inference time by 40%.\n", "link": "http://arxiv.org/abs/2405.19209v1", "date": "2024-05-29", "relevancy": 2.0957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5167}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoTree%3A%20Adaptive%20Tree-based%20Video%20Representation%20for%20LLM%20Reasoning%20on%0A%20%20Long%20Videos&body=Title%3A%20VideoTree%3A%20Adaptive%20Tree-based%20Video%20Representation%20for%20LLM%20Reasoning%20on%0A%20%20Long%20Videos%0AAuthor%3A%20Ziyang%20Wang%20and%20Shoubin%20Yu%20and%20Elias%20Stengel-Eskin%20and%20Jaehong%20Yoon%20and%20Feng%20Cheng%20and%20Gedas%20Bertasius%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Video-language%20understanding%20tasks%20have%20focused%20on%20short%20video%20clips%2C%20often%0Astruggling%20with%20long-form%20video%20understanding%20tasks.%20Recently%2C%20many%20long%0Avideo-language%20understanding%20approaches%20have%20leveraged%20the%20reasoning%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%20long%20video%20QA%2C%0Atransforming%20videos%20into%20densely%20sampled%20frame%20captions%2C%20and%20asking%20LLMs%20to%0Arespond%20to%20text%20queries%20over%20captions.%20However%2C%20the%20frames%20used%20for%20captioning%0Aare%20often%20redundant%20and%20contain%20irrelevant%20information%2C%20making%20dense%20sampling%0Ainefficient%2C%20and%20ignoring%20the%20fact%20that%20video%20QA%20requires%20varying%20levels%20of%0Agranularity%2C%20with%20some%20video%20segments%20being%20highly%20relevant%20to%20the%20question%0A%28needing%20more%20fine-grained%20detail%29%20while%20others%20being%20less%20relevant.%20Thus%2C%0Athese%20LLM-based%20approaches%20are%20prone%20to%20missing%20information%20and%20operate%20on%0Alarge%20numbers%20of%20irrelevant%20captions%2C%20lowering%20both%20performance%20and%20efficiency.%0ATo%20address%20these%20issues%2C%20we%20introduce%20VideoTree%2C%20a%20query-adaptive%20and%0Ahierarchical%20framework%20for%20long-video%20understanding%20with%20LLMs.%20VideoTree%0Adynamically%20extracts%20query-related%20information%20from%20a%20video%20and%20builds%20a%0Atree-based%20representation%20for%20LLM%20reasoning.%20First%2C%20VideoTree%20adaptively%0Aselects%20frames%20for%20captioning%20by%20iteratively%20clustering%20frames%20based%20on%20their%0Avisual%20features%20and%20scoring%20clusters%20using%20their%20relevance%20to%20the%20query.%0ASecond%2C%20it%20organizes%20visual%20clusters%20into%20a%20query-adaptive%20and%20hierarchical%0Atree%20structure%3B%20the%20tree%20encodes%20varying%20levels%20of%20granularity%2C%20with%20higher%0Aresolution%20on%20relevant%20segments.%20Finally%2C%20VideoTree%20produces%20an%20answer%20by%0Atraversing%20the%20tree%27s%20keyframes%20and%20passing%20their%20captions%20to%20an%20LLM%20answerer.%0AOur%20method%20improves%20both%20reasoning%20accuracy%20and%20efficiency%20compared%20to%20existing%0Amethods%3A%20VideoTree%20achieves%20a%207.0%25%2C%202.2%25%2C%20and%202.7%25%20accuracy%20gain%20over%20baselines%0Aon%20the%20EgoSchema%2C%20NExT-QA%2C%20and%20IntentQA%20benchmarks%2C%20respectively%2C%20while%0Areducing%20inference%20time%20by%2040%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoTree%253A%2520Adaptive%2520Tree-based%2520Video%2520Representation%2520for%2520LLM%2520Reasoning%2520on%250A%2520%2520Long%2520Videos%26entry.906535625%3DZiyang%2520Wang%2520and%2520Shoubin%2520Yu%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Jaehong%2520Yoon%2520and%2520Feng%2520Cheng%2520and%2520Gedas%2520Bertasius%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Video-language%2520understanding%2520tasks%2520have%2520focused%2520on%2520short%2520video%2520clips%252C%2520often%250Astruggling%2520with%2520long-form%2520video%2520understanding%2520tasks.%2520Recently%252C%2520many%2520long%250Avideo-language%2520understanding%2520approaches%2520have%2520leveraged%2520the%2520reasoning%250Acapabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520perform%2520long%2520video%2520QA%252C%250Atransforming%2520videos%2520into%2520densely%2520sampled%2520frame%2520captions%252C%2520and%2520asking%2520LLMs%2520to%250Arespond%2520to%2520text%2520queries%2520over%2520captions.%2520However%252C%2520the%2520frames%2520used%2520for%2520captioning%250Aare%2520often%2520redundant%2520and%2520contain%2520irrelevant%2520information%252C%2520making%2520dense%2520sampling%250Ainefficient%252C%2520and%2520ignoring%2520the%2520fact%2520that%2520video%2520QA%2520requires%2520varying%2520levels%2520of%250Agranularity%252C%2520with%2520some%2520video%2520segments%2520being%2520highly%2520relevant%2520to%2520the%2520question%250A%2528needing%2520more%2520fine-grained%2520detail%2529%2520while%2520others%2520being%2520less%2520relevant.%2520Thus%252C%250Athese%2520LLM-based%2520approaches%2520are%2520prone%2520to%2520missing%2520information%2520and%2520operate%2520on%250Alarge%2520numbers%2520of%2520irrelevant%2520captions%252C%2520lowering%2520both%2520performance%2520and%2520efficiency.%250ATo%2520address%2520these%2520issues%252C%2520we%2520introduce%2520VideoTree%252C%2520a%2520query-adaptive%2520and%250Ahierarchical%2520framework%2520for%2520long-video%2520understanding%2520with%2520LLMs.%2520VideoTree%250Adynamically%2520extracts%2520query-related%2520information%2520from%2520a%2520video%2520and%2520builds%2520a%250Atree-based%2520representation%2520for%2520LLM%2520reasoning.%2520First%252C%2520VideoTree%2520adaptively%250Aselects%2520frames%2520for%2520captioning%2520by%2520iteratively%2520clustering%2520frames%2520based%2520on%2520their%250Avisual%2520features%2520and%2520scoring%2520clusters%2520using%2520their%2520relevance%2520to%2520the%2520query.%250ASecond%252C%2520it%2520organizes%2520visual%2520clusters%2520into%2520a%2520query-adaptive%2520and%2520hierarchical%250Atree%2520structure%253B%2520the%2520tree%2520encodes%2520varying%2520levels%2520of%2520granularity%252C%2520with%2520higher%250Aresolution%2520on%2520relevant%2520segments.%2520Finally%252C%2520VideoTree%2520produces%2520an%2520answer%2520by%250Atraversing%2520the%2520tree%2527s%2520keyframes%2520and%2520passing%2520their%2520captions%2520to%2520an%2520LLM%2520answerer.%250AOur%2520method%2520improves%2520both%2520reasoning%2520accuracy%2520and%2520efficiency%2520compared%2520to%2520existing%250Amethods%253A%2520VideoTree%2520achieves%2520a%25207.0%2525%252C%25202.2%2525%252C%2520and%25202.7%2525%2520accuracy%2520gain%2520over%2520baselines%250Aon%2520the%2520EgoSchema%252C%2520NExT-QA%252C%2520and%2520IntentQA%2520benchmarks%252C%2520respectively%252C%2520while%250Areducing%2520inference%2520time%2520by%252040%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoTree%3A%20Adaptive%20Tree-based%20Video%20Representation%20for%20LLM%20Reasoning%20on%0A%20%20Long%20Videos&entry.906535625=Ziyang%20Wang%20and%20Shoubin%20Yu%20and%20Elias%20Stengel-Eskin%20and%20Jaehong%20Yoon%20and%20Feng%20Cheng%20and%20Gedas%20Bertasius%20and%20Mohit%20Bansal&entry.1292438233=%20%20Video-language%20understanding%20tasks%20have%20focused%20on%20short%20video%20clips%2C%20often%0Astruggling%20with%20long-form%20video%20understanding%20tasks.%20Recently%2C%20many%20long%0Avideo-language%20understanding%20approaches%20have%20leveraged%20the%20reasoning%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%20long%20video%20QA%2C%0Atransforming%20videos%20into%20densely%20sampled%20frame%20captions%2C%20and%20asking%20LLMs%20to%0Arespond%20to%20text%20queries%20over%20captions.%20However%2C%20the%20frames%20used%20for%20captioning%0Aare%20often%20redundant%20and%20contain%20irrelevant%20information%2C%20making%20dense%20sampling%0Ainefficient%2C%20and%20ignoring%20the%20fact%20that%20video%20QA%20requires%20varying%20levels%20of%0Agranularity%2C%20with%20some%20video%20segments%20being%20highly%20relevant%20to%20the%20question%0A%28needing%20more%20fine-grained%20detail%29%20while%20others%20being%20less%20relevant.%20Thus%2C%0Athese%20LLM-based%20approaches%20are%20prone%20to%20missing%20information%20and%20operate%20on%0Alarge%20numbers%20of%20irrelevant%20captions%2C%20lowering%20both%20performance%20and%20efficiency.%0ATo%20address%20these%20issues%2C%20we%20introduce%20VideoTree%2C%20a%20query-adaptive%20and%0Ahierarchical%20framework%20for%20long-video%20understanding%20with%20LLMs.%20VideoTree%0Adynamically%20extracts%20query-related%20information%20from%20a%20video%20and%20builds%20a%0Atree-based%20representation%20for%20LLM%20reasoning.%20First%2C%20VideoTree%20adaptively%0Aselects%20frames%20for%20captioning%20by%20iteratively%20clustering%20frames%20based%20on%20their%0Avisual%20features%20and%20scoring%20clusters%20using%20their%20relevance%20to%20the%20query.%0ASecond%2C%20it%20organizes%20visual%20clusters%20into%20a%20query-adaptive%20and%20hierarchical%0Atree%20structure%3B%20the%20tree%20encodes%20varying%20levels%20of%20granularity%2C%20with%20higher%0Aresolution%20on%20relevant%20segments.%20Finally%2C%20VideoTree%20produces%20an%20answer%20by%0Atraversing%20the%20tree%27s%20keyframes%20and%20passing%20their%20captions%20to%20an%20LLM%20answerer.%0AOur%20method%20improves%20both%20reasoning%20accuracy%20and%20efficiency%20compared%20to%20existing%0Amethods%3A%20VideoTree%20achieves%20a%207.0%25%2C%202.2%25%2C%20and%202.7%25%20accuracy%20gain%20over%20baselines%0Aon%20the%20EgoSchema%2C%20NExT-QA%2C%20and%20IntentQA%20benchmarks%2C%20respectively%2C%20while%0Areducing%20inference%20time%20by%2040%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19209v1&entry.124074799=Read"},
{"title": "Matryoshka Query Transformer for Large Vision-Language Models", "author": "Wenbo Hu and Zi-Yi Dou and Liunian Harold Li and Amita Kamath and Nanyun Peng and Kai-Wei Chang", "abstract": "  Large Vision-Language Models (LVLMs) typically encode an image into a fixed\nnumber of visual tokens (e.g., 576) and process these tokens with a language\nmodel. Despite their strong performance, LVLMs face challenges in adapting to\nvarying computational constraints. This raises the question: can we achieve\nflexibility in the number of visual tokens to suit different tasks and\ncomputational resources? We answer this with an emphatic yes. Inspired by\nMatryoshka Representation Learning, we introduce the Matryoshka Query\nTransformer (MQT), capable of encoding an image into m visual tokens during\ninference, where m can be any number up to a predefined maximum. This is\nachieved by employing a query transformer with M latent query tokens to\ncompress the visual embeddings. During each training step, we randomly select m\n<= M latent query tokens and train the model using only these first m tokens,\ndiscarding the rest. Combining MQT with LLaVA, we train a single model once,\nand flexibly and drastically reduce the number of inference-time visual tokens\nwhile maintaining similar or better performance compared to training\nindependent models for each number of tokens. Our model, MQT-LLAVA, matches\nLLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens\ninstead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only\nsacrifices the performance by 2.4 points on MMBench. On certain tasks such as\nScienceQA and MMMU, we can even go down to only 2 visual tokens with\nperformance drops of just 3% and 6% each. Our exploration of the trade-off\nbetween the accuracy and computational cost brought about by the number of\nvisual tokens facilitates future research to achieve the best of both worlds.\n", "link": "http://arxiv.org/abs/2405.19315v1", "date": "2024-05-29", "relevancy": 2.0954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matryoshka%20Query%20Transformer%20for%20Large%20Vision-Language%20Models&body=Title%3A%20Matryoshka%20Query%20Transformer%20for%20Large%20Vision-Language%20Models%0AAuthor%3A%20Wenbo%20Hu%20and%20Zi-Yi%20Dou%20and%20Liunian%20Harold%20Li%20and%20Amita%20Kamath%20and%20Nanyun%20Peng%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20typically%20encode%20an%20image%20into%20a%20fixed%0Anumber%20of%20visual%20tokens%20%28e.g.%2C%20576%29%20and%20process%20these%20tokens%20with%20a%20language%0Amodel.%20Despite%20their%20strong%20performance%2C%20LVLMs%20face%20challenges%20in%20adapting%20to%0Avarying%20computational%20constraints.%20This%20raises%20the%20question%3A%20can%20we%20achieve%0Aflexibility%20in%20the%20number%20of%20visual%20tokens%20to%20suit%20different%20tasks%20and%0Acomputational%20resources%3F%20We%20answer%20this%20with%20an%20emphatic%20yes.%20Inspired%20by%0AMatryoshka%20Representation%20Learning%2C%20we%20introduce%20the%20Matryoshka%20Query%0ATransformer%20%28MQT%29%2C%20capable%20of%20encoding%20an%20image%20into%20m%20visual%20tokens%20during%0Ainference%2C%20where%20m%20can%20be%20any%20number%20up%20to%20a%20predefined%20maximum.%20This%20is%0Aachieved%20by%20employing%20a%20query%20transformer%20with%20M%20latent%20query%20tokens%20to%0Acompress%20the%20visual%20embeddings.%20During%20each%20training%20step%2C%20we%20randomly%20select%20m%0A%3C%3D%20M%20latent%20query%20tokens%20and%20train%20the%20model%20using%20only%20these%20first%20m%20tokens%2C%0Adiscarding%20the%20rest.%20Combining%20MQT%20with%20LLaVA%2C%20we%20train%20a%20single%20model%20once%2C%0Aand%20flexibly%20and%20drastically%20reduce%20the%20number%20of%20inference-time%20visual%20tokens%0Awhile%20maintaining%20similar%20or%20better%20performance%20compared%20to%20training%0Aindependent%20models%20for%20each%20number%20of%20tokens.%20Our%20model%2C%20MQT-LLAVA%2C%20matches%0ALLaVA-1.5%20performance%20across%2011%20benchmarks%20using%20a%20maximum%20of%20256%20tokens%0Ainstead%20of%20LLaVA%27s%20fixed%20576.%20Reducing%20to%2016%20tokens%20%288x%20less%20TFLOPs%29%20only%0Asacrifices%20the%20performance%20by%202.4%20points%20on%20MMBench.%20On%20certain%20tasks%20such%20as%0AScienceQA%20and%20MMMU%2C%20we%20can%20even%20go%20down%20to%20only%202%20visual%20tokens%20with%0Aperformance%20drops%20of%20just%203%25%20and%206%25%20each.%20Our%20exploration%20of%20the%20trade-off%0Abetween%20the%20accuracy%20and%20computational%20cost%20brought%20about%20by%20the%20number%20of%0Avisual%20tokens%20facilitates%20future%20research%20to%20achieve%20the%20best%20of%20both%20worlds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatryoshka%2520Query%2520Transformer%2520for%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DWenbo%2520Hu%2520and%2520Zi-Yi%2520Dou%2520and%2520Liunian%2520Harold%2520Li%2520and%2520Amita%2520Kamath%2520and%2520Nanyun%2520Peng%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520typically%2520encode%2520an%2520image%2520into%2520a%2520fixed%250Anumber%2520of%2520visual%2520tokens%2520%2528e.g.%252C%2520576%2529%2520and%2520process%2520these%2520tokens%2520with%2520a%2520language%250Amodel.%2520Despite%2520their%2520strong%2520performance%252C%2520LVLMs%2520face%2520challenges%2520in%2520adapting%2520to%250Avarying%2520computational%2520constraints.%2520This%2520raises%2520the%2520question%253A%2520can%2520we%2520achieve%250Aflexibility%2520in%2520the%2520number%2520of%2520visual%2520tokens%2520to%2520suit%2520different%2520tasks%2520and%250Acomputational%2520resources%253F%2520We%2520answer%2520this%2520with%2520an%2520emphatic%2520yes.%2520Inspired%2520by%250AMatryoshka%2520Representation%2520Learning%252C%2520we%2520introduce%2520the%2520Matryoshka%2520Query%250ATransformer%2520%2528MQT%2529%252C%2520capable%2520of%2520encoding%2520an%2520image%2520into%2520m%2520visual%2520tokens%2520during%250Ainference%252C%2520where%2520m%2520can%2520be%2520any%2520number%2520up%2520to%2520a%2520predefined%2520maximum.%2520This%2520is%250Aachieved%2520by%2520employing%2520a%2520query%2520transformer%2520with%2520M%2520latent%2520query%2520tokens%2520to%250Acompress%2520the%2520visual%2520embeddings.%2520During%2520each%2520training%2520step%252C%2520we%2520randomly%2520select%2520m%250A%253C%253D%2520M%2520latent%2520query%2520tokens%2520and%2520train%2520the%2520model%2520using%2520only%2520these%2520first%2520m%2520tokens%252C%250Adiscarding%2520the%2520rest.%2520Combining%2520MQT%2520with%2520LLaVA%252C%2520we%2520train%2520a%2520single%2520model%2520once%252C%250Aand%2520flexibly%2520and%2520drastically%2520reduce%2520the%2520number%2520of%2520inference-time%2520visual%2520tokens%250Awhile%2520maintaining%2520similar%2520or%2520better%2520performance%2520compared%2520to%2520training%250Aindependent%2520models%2520for%2520each%2520number%2520of%2520tokens.%2520Our%2520model%252C%2520MQT-LLAVA%252C%2520matches%250ALLaVA-1.5%2520performance%2520across%252011%2520benchmarks%2520using%2520a%2520maximum%2520of%2520256%2520tokens%250Ainstead%2520of%2520LLaVA%2527s%2520fixed%2520576.%2520Reducing%2520to%252016%2520tokens%2520%25288x%2520less%2520TFLOPs%2529%2520only%250Asacrifices%2520the%2520performance%2520by%25202.4%2520points%2520on%2520MMBench.%2520On%2520certain%2520tasks%2520such%2520as%250AScienceQA%2520and%2520MMMU%252C%2520we%2520can%2520even%2520go%2520down%2520to%2520only%25202%2520visual%2520tokens%2520with%250Aperformance%2520drops%2520of%2520just%25203%2525%2520and%25206%2525%2520each.%2520Our%2520exploration%2520of%2520the%2520trade-off%250Abetween%2520the%2520accuracy%2520and%2520computational%2520cost%2520brought%2520about%2520by%2520the%2520number%2520of%250Avisual%2520tokens%2520facilitates%2520future%2520research%2520to%2520achieve%2520the%2520best%2520of%2520both%2520worlds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matryoshka%20Query%20Transformer%20for%20Large%20Vision-Language%20Models&entry.906535625=Wenbo%20Hu%20and%20Zi-Yi%20Dou%20and%20Liunian%20Harold%20Li%20and%20Amita%20Kamath%20and%20Nanyun%20Peng%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20typically%20encode%20an%20image%20into%20a%20fixed%0Anumber%20of%20visual%20tokens%20%28e.g.%2C%20576%29%20and%20process%20these%20tokens%20with%20a%20language%0Amodel.%20Despite%20their%20strong%20performance%2C%20LVLMs%20face%20challenges%20in%20adapting%20to%0Avarying%20computational%20constraints.%20This%20raises%20the%20question%3A%20can%20we%20achieve%0Aflexibility%20in%20the%20number%20of%20visual%20tokens%20to%20suit%20different%20tasks%20and%0Acomputational%20resources%3F%20We%20answer%20this%20with%20an%20emphatic%20yes.%20Inspired%20by%0AMatryoshka%20Representation%20Learning%2C%20we%20introduce%20the%20Matryoshka%20Query%0ATransformer%20%28MQT%29%2C%20capable%20of%20encoding%20an%20image%20into%20m%20visual%20tokens%20during%0Ainference%2C%20where%20m%20can%20be%20any%20number%20up%20to%20a%20predefined%20maximum.%20This%20is%0Aachieved%20by%20employing%20a%20query%20transformer%20with%20M%20latent%20query%20tokens%20to%0Acompress%20the%20visual%20embeddings.%20During%20each%20training%20step%2C%20we%20randomly%20select%20m%0A%3C%3D%20M%20latent%20query%20tokens%20and%20train%20the%20model%20using%20only%20these%20first%20m%20tokens%2C%0Adiscarding%20the%20rest.%20Combining%20MQT%20with%20LLaVA%2C%20we%20train%20a%20single%20model%20once%2C%0Aand%20flexibly%20and%20drastically%20reduce%20the%20number%20of%20inference-time%20visual%20tokens%0Awhile%20maintaining%20similar%20or%20better%20performance%20compared%20to%20training%0Aindependent%20models%20for%20each%20number%20of%20tokens.%20Our%20model%2C%20MQT-LLAVA%2C%20matches%0ALLaVA-1.5%20performance%20across%2011%20benchmarks%20using%20a%20maximum%20of%20256%20tokens%0Ainstead%20of%20LLaVA%27s%20fixed%20576.%20Reducing%20to%2016%20tokens%20%288x%20less%20TFLOPs%29%20only%0Asacrifices%20the%20performance%20by%202.4%20points%20on%20MMBench.%20On%20certain%20tasks%20such%20as%0AScienceQA%20and%20MMMU%2C%20we%20can%20even%20go%20down%20to%20only%202%20visual%20tokens%20with%0Aperformance%20drops%20of%20just%203%25%20and%206%25%20each.%20Our%20exploration%20of%20the%20trade-off%0Abetween%20the%20accuracy%20and%20computational%20cost%20brought%20about%20by%20the%20number%20of%0Avisual%20tokens%20facilitates%20future%20research%20to%20achieve%20the%20best%20of%20both%20worlds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19315v1&entry.124074799=Read"},
{"title": "Improving Neural Additive Models with Bayesian Principles", "author": "Kouroche Bouchiat and Alexander Immer and Hugo Y\u00e8che and Gunnar R\u00e4tsch and Vincent Fortuin", "abstract": "  Neural additive models (NAMs) enhance the transparency of deep neural\nnetworks by handling input features in separate additive sub-networks. However,\nthey lack inherent mechanisms that provide calibrated uncertainties and enable\nselection of relevant features and interactions. Approaching NAMs from a\nBayesian perspective, we augment them in three primary ways, namely by a)\nproviding credible intervals for the individual additive sub-networks; b)\nestimating the marginal likelihood to perform an implicit selection of features\nvia an empirical Bayes procedure; and c) facilitating the ranking of feature\npairs as candidates for second-order interaction in fine-tuned models. In\nparticular, we develop Laplace-approximated NAMs (LA-NAMs), which show improved\nempirical performance on tabular datasets and challenging real-world medical\ntasks.\n", "link": "http://arxiv.org/abs/2305.16905v4", "date": "2024-05-29", "relevancy": 2.0812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Neural%20Additive%20Models%20with%20Bayesian%20Principles&body=Title%3A%20Improving%20Neural%20Additive%20Models%20with%20Bayesian%20Principles%0AAuthor%3A%20Kouroche%20Bouchiat%20and%20Alexander%20Immer%20and%20Hugo%20Y%C3%A8che%20and%20Gunnar%20R%C3%A4tsch%20and%20Vincent%20Fortuin%0AAbstract%3A%20%20%20Neural%20additive%20models%20%28NAMs%29%20enhance%20the%20transparency%20of%20deep%20neural%0Anetworks%20by%20handling%20input%20features%20in%20separate%20additive%20sub-networks.%20However%2C%0Athey%20lack%20inherent%20mechanisms%20that%20provide%20calibrated%20uncertainties%20and%20enable%0Aselection%20of%20relevant%20features%20and%20interactions.%20Approaching%20NAMs%20from%20a%0ABayesian%20perspective%2C%20we%20augment%20them%20in%20three%20primary%20ways%2C%20namely%20by%20a%29%0Aproviding%20credible%20intervals%20for%20the%20individual%20additive%20sub-networks%3B%20b%29%0Aestimating%20the%20marginal%20likelihood%20to%20perform%20an%20implicit%20selection%20of%20features%0Avia%20an%20empirical%20Bayes%20procedure%3B%20and%20c%29%20facilitating%20the%20ranking%20of%20feature%0Apairs%20as%20candidates%20for%20second-order%20interaction%20in%20fine-tuned%20models.%20In%0Aparticular%2C%20we%20develop%20Laplace-approximated%20NAMs%20%28LA-NAMs%29%2C%20which%20show%20improved%0Aempirical%20performance%20on%20tabular%20datasets%20and%20challenging%20real-world%20medical%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16905v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Neural%2520Additive%2520Models%2520with%2520Bayesian%2520Principles%26entry.906535625%3DKouroche%2520Bouchiat%2520and%2520Alexander%2520Immer%2520and%2520Hugo%2520Y%25C3%25A8che%2520and%2520Gunnar%2520R%25C3%25A4tsch%2520and%2520Vincent%2520Fortuin%26entry.1292438233%3D%2520%2520Neural%2520additive%2520models%2520%2528NAMs%2529%2520enhance%2520the%2520transparency%2520of%2520deep%2520neural%250Anetworks%2520by%2520handling%2520input%2520features%2520in%2520separate%2520additive%2520sub-networks.%2520However%252C%250Athey%2520lack%2520inherent%2520mechanisms%2520that%2520provide%2520calibrated%2520uncertainties%2520and%2520enable%250Aselection%2520of%2520relevant%2520features%2520and%2520interactions.%2520Approaching%2520NAMs%2520from%2520a%250ABayesian%2520perspective%252C%2520we%2520augment%2520them%2520in%2520three%2520primary%2520ways%252C%2520namely%2520by%2520a%2529%250Aproviding%2520credible%2520intervals%2520for%2520the%2520individual%2520additive%2520sub-networks%253B%2520b%2529%250Aestimating%2520the%2520marginal%2520likelihood%2520to%2520perform%2520an%2520implicit%2520selection%2520of%2520features%250Avia%2520an%2520empirical%2520Bayes%2520procedure%253B%2520and%2520c%2529%2520facilitating%2520the%2520ranking%2520of%2520feature%250Apairs%2520as%2520candidates%2520for%2520second-order%2520interaction%2520in%2520fine-tuned%2520models.%2520In%250Aparticular%252C%2520we%2520develop%2520Laplace-approximated%2520NAMs%2520%2528LA-NAMs%2529%252C%2520which%2520show%2520improved%250Aempirical%2520performance%2520on%2520tabular%2520datasets%2520and%2520challenging%2520real-world%2520medical%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16905v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Neural%20Additive%20Models%20with%20Bayesian%20Principles&entry.906535625=Kouroche%20Bouchiat%20and%20Alexander%20Immer%20and%20Hugo%20Y%C3%A8che%20and%20Gunnar%20R%C3%A4tsch%20and%20Vincent%20Fortuin&entry.1292438233=%20%20Neural%20additive%20models%20%28NAMs%29%20enhance%20the%20transparency%20of%20deep%20neural%0Anetworks%20by%20handling%20input%20features%20in%20separate%20additive%20sub-networks.%20However%2C%0Athey%20lack%20inherent%20mechanisms%20that%20provide%20calibrated%20uncertainties%20and%20enable%0Aselection%20of%20relevant%20features%20and%20interactions.%20Approaching%20NAMs%20from%20a%0ABayesian%20perspective%2C%20we%20augment%20them%20in%20three%20primary%20ways%2C%20namely%20by%20a%29%0Aproviding%20credible%20intervals%20for%20the%20individual%20additive%20sub-networks%3B%20b%29%0Aestimating%20the%20marginal%20likelihood%20to%20perform%20an%20implicit%20selection%20of%20features%0Avia%20an%20empirical%20Bayes%20procedure%3B%20and%20c%29%20facilitating%20the%20ranking%20of%20feature%0Apairs%20as%20candidates%20for%20second-order%20interaction%20in%20fine-tuned%20models.%20In%0Aparticular%2C%20we%20develop%20Laplace-approximated%20NAMs%20%28LA-NAMs%29%2C%20which%20show%20improved%0Aempirical%20performance%20on%20tabular%20datasets%20and%20challenging%20real-world%20medical%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16905v4&entry.124074799=Read"},
{"title": "Eigen Is All You Need: Efficient Lidar-Inertial Continuous-Time Odometry\n  with Internal Association", "author": "Thien-Minh Nguyen and Xinhang Xu and Tongxing Jin and Yizhuo Yang and Jianping Li and Shenghai Yuan and Lihua Xie", "abstract": "  In this paper, we propose a continuous-time lidar-inertial odometry (CT-LIO)\nsystem named SLICT2, which promotes two main insights. One, contrary to\nconventional wisdom, CT-LIO algorithm can be optimized by linear solvers in\nonly a few iterations, which is more efficient than commonly used nonlinear\nsolvers. Two, CT-LIO benefits more from the correct association than the number\nof iterations. Based on these ideas, we implement our method with a customized\nsolver where the feature association process is performed immediately after\neach incremental step, and the solution can converge within a few iterations.\nOur implementation can achieve real-time performance with a high density of\ncontrol points while yielding competitive performance in highly dynamical\nmotion scenarios. We demonstrate the advantages of our method by comparing with\nother existing state-of-the-art CT-LIO methods. The source code will be\nreleased for the benefit of the community.\n", "link": "http://arxiv.org/abs/2402.02337v2", "date": "2024-05-29", "relevancy": 2.0773, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5212}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eigen%20Is%20All%20You%20Need%3A%20Efficient%20Lidar-Inertial%20Continuous-Time%20Odometry%0A%20%20with%20Internal%20Association&body=Title%3A%20Eigen%20Is%20All%20You%20Need%3A%20Efficient%20Lidar-Inertial%20Continuous-Time%20Odometry%0A%20%20with%20Internal%20Association%0AAuthor%3A%20Thien-Minh%20Nguyen%20and%20Xinhang%20Xu%20and%20Tongxing%20Jin%20and%20Yizhuo%20Yang%20and%20Jianping%20Li%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20continuous-time%20lidar-inertial%20odometry%20%28CT-LIO%29%0Asystem%20named%20SLICT2%2C%20which%20promotes%20two%20main%20insights.%20One%2C%20contrary%20to%0Aconventional%20wisdom%2C%20CT-LIO%20algorithm%20can%20be%20optimized%20by%20linear%20solvers%20in%0Aonly%20a%20few%20iterations%2C%20which%20is%20more%20efficient%20than%20commonly%20used%20nonlinear%0Asolvers.%20Two%2C%20CT-LIO%20benefits%20more%20from%20the%20correct%20association%20than%20the%20number%0Aof%20iterations.%20Based%20on%20these%20ideas%2C%20we%20implement%20our%20method%20with%20a%20customized%0Asolver%20where%20the%20feature%20association%20process%20is%20performed%20immediately%20after%0Aeach%20incremental%20step%2C%20and%20the%20solution%20can%20converge%20within%20a%20few%20iterations.%0AOur%20implementation%20can%20achieve%20real-time%20performance%20with%20a%20high%20density%20of%0Acontrol%20points%20while%20yielding%20competitive%20performance%20in%20highly%20dynamical%0Amotion%20scenarios.%20We%20demonstrate%20the%20advantages%20of%20our%20method%20by%20comparing%20with%0Aother%20existing%20state-of-the-art%20CT-LIO%20methods.%20The%20source%20code%20will%20be%0Areleased%20for%20the%20benefit%20of%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02337v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigen%2520Is%2520All%2520You%2520Need%253A%2520Efficient%2520Lidar-Inertial%2520Continuous-Time%2520Odometry%250A%2520%2520with%2520Internal%2520Association%26entry.906535625%3DThien-Minh%2520Nguyen%2520and%2520Xinhang%2520Xu%2520and%2520Tongxing%2520Jin%2520and%2520Yizhuo%2520Yang%2520and%2520Jianping%2520Li%2520and%2520Shenghai%2520Yuan%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520continuous-time%2520lidar-inertial%2520odometry%2520%2528CT-LIO%2529%250Asystem%2520named%2520SLICT2%252C%2520which%2520promotes%2520two%2520main%2520insights.%2520One%252C%2520contrary%2520to%250Aconventional%2520wisdom%252C%2520CT-LIO%2520algorithm%2520can%2520be%2520optimized%2520by%2520linear%2520solvers%2520in%250Aonly%2520a%2520few%2520iterations%252C%2520which%2520is%2520more%2520efficient%2520than%2520commonly%2520used%2520nonlinear%250Asolvers.%2520Two%252C%2520CT-LIO%2520benefits%2520more%2520from%2520the%2520correct%2520association%2520than%2520the%2520number%250Aof%2520iterations.%2520Based%2520on%2520these%2520ideas%252C%2520we%2520implement%2520our%2520method%2520with%2520a%2520customized%250Asolver%2520where%2520the%2520feature%2520association%2520process%2520is%2520performed%2520immediately%2520after%250Aeach%2520incremental%2520step%252C%2520and%2520the%2520solution%2520can%2520converge%2520within%2520a%2520few%2520iterations.%250AOur%2520implementation%2520can%2520achieve%2520real-time%2520performance%2520with%2520a%2520high%2520density%2520of%250Acontrol%2520points%2520while%2520yielding%2520competitive%2520performance%2520in%2520highly%2520dynamical%250Amotion%2520scenarios.%2520We%2520demonstrate%2520the%2520advantages%2520of%2520our%2520method%2520by%2520comparing%2520with%250Aother%2520existing%2520state-of-the-art%2520CT-LIO%2520methods.%2520The%2520source%2520code%2520will%2520be%250Areleased%2520for%2520the%2520benefit%2520of%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02337v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eigen%20Is%20All%20You%20Need%3A%20Efficient%20Lidar-Inertial%20Continuous-Time%20Odometry%0A%20%20with%20Internal%20Association&entry.906535625=Thien-Minh%20Nguyen%20and%20Xinhang%20Xu%20and%20Tongxing%20Jin%20and%20Yizhuo%20Yang%20and%20Jianping%20Li%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20continuous-time%20lidar-inertial%20odometry%20%28CT-LIO%29%0Asystem%20named%20SLICT2%2C%20which%20promotes%20two%20main%20insights.%20One%2C%20contrary%20to%0Aconventional%20wisdom%2C%20CT-LIO%20algorithm%20can%20be%20optimized%20by%20linear%20solvers%20in%0Aonly%20a%20few%20iterations%2C%20which%20is%20more%20efficient%20than%20commonly%20used%20nonlinear%0Asolvers.%20Two%2C%20CT-LIO%20benefits%20more%20from%20the%20correct%20association%20than%20the%20number%0Aof%20iterations.%20Based%20on%20these%20ideas%2C%20we%20implement%20our%20method%20with%20a%20customized%0Asolver%20where%20the%20feature%20association%20process%20is%20performed%20immediately%20after%0Aeach%20incremental%20step%2C%20and%20the%20solution%20can%20converge%20within%20a%20few%20iterations.%0AOur%20implementation%20can%20achieve%20real-time%20performance%20with%20a%20high%20density%20of%0Acontrol%20points%20while%20yielding%20competitive%20performance%20in%20highly%20dynamical%0Amotion%20scenarios.%20We%20demonstrate%20the%20advantages%20of%20our%20method%20by%20comparing%20with%0Aother%20existing%20state-of-the-art%20CT-LIO%20methods.%20The%20source%20code%20will%20be%0Areleased%20for%20the%20benefit%20of%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02337v2&entry.124074799=Read"},
{"title": "Position: Foundation Agents as the Paradigm Shift for Decision Making", "author": "Xiaoqian Liu and Xingzhou Lou and Jianbin Jiao and Junge Zhang", "abstract": "  Decision making demands intricate interplay between perception, memory, and\nreasoning to discern optimal policies. Conventional approaches to decision\nmaking face challenges related to low sample efficiency and poor\ngeneralization. In contrast, foundation models in language and vision have\nshowcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\nconstruction of foundation agents as a transformative shift in the learning\nparadigm of agents. This proposal is underpinned by the formulation of\nfoundation agents with their fundamental characteristics and challenges\nmotivated by the success of large language models (LLMs). Moreover, we specify\nthe roadmap of foundation agents from large interactive data collection or\ngeneration, to self-supervised pretraining and adaptation, and knowledge and\nvalue alignment with LLMs. Lastly, we pinpoint critical research questions\nderived from the formulation and delineate trends for foundation agents\nsupported by real-world use cases, addressing both technical and theoretical\naspects to propel the field towards a more comprehensive and impactful future.\n", "link": "http://arxiv.org/abs/2405.17009v3", "date": "2024-05-29", "relevancy": 2.0699, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5218}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5217}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&body=Title%3A%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making%0AAuthor%3A%20Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang%0AAbstract%3A%20%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20have%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20their%20fundamental%20characteristics%20and%20challenges%0Amotivated%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%0Athe%20roadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Foundation%2520Agents%2520as%2520the%2520Paradigm%2520Shift%2520for%2520Decision%2520Making%26entry.906535625%3DXiaoqian%2520Liu%2520and%2520Xingzhou%2520Lou%2520and%2520Jianbin%2520Jiao%2520and%2520Junge%2520Zhang%26entry.1292438233%3D%2520%2520Decision%2520making%2520demands%2520intricate%2520interplay%2520between%2520perception%252C%2520memory%252C%2520and%250Areasoning%2520to%2520discern%2520optimal%2520policies.%2520Conventional%2520approaches%2520to%2520decision%250Amaking%2520face%2520challenges%2520related%2520to%2520low%2520sample%2520efficiency%2520and%2520poor%250Ageneralization.%2520In%2520contrast%252C%2520foundation%2520models%2520in%2520language%2520and%2520vision%2520have%250Ashowcased%2520rapid%2520adaptation%2520to%2520diverse%2520new%2520tasks.%2520Therefore%252C%2520we%2520advocate%2520for%2520the%250Aconstruction%2520of%2520foundation%2520agents%2520as%2520a%2520transformative%2520shift%2520in%2520the%2520learning%250Aparadigm%2520of%2520agents.%2520This%2520proposal%2520is%2520underpinned%2520by%2520the%2520formulation%2520of%250Afoundation%2520agents%2520with%2520their%2520fundamental%2520characteristics%2520and%2520challenges%250Amotivated%2520by%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Moreover%252C%2520we%2520specify%250Athe%2520roadmap%2520of%2520foundation%2520agents%2520from%2520large%2520interactive%2520data%2520collection%2520or%250Ageneration%252C%2520to%2520self-supervised%2520pretraining%2520and%2520adaptation%252C%2520and%2520knowledge%2520and%250Avalue%2520alignment%2520with%2520LLMs.%2520Lastly%252C%2520we%2520pinpoint%2520critical%2520research%2520questions%250Aderived%2520from%2520the%2520formulation%2520and%2520delineate%2520trends%2520for%2520foundation%2520agents%250Asupported%2520by%2520real-world%2520use%2520cases%252C%2520addressing%2520both%2520technical%2520and%2520theoretical%250Aaspects%2520to%2520propel%2520the%2520field%2520towards%2520a%2520more%2520comprehensive%2520and%2520impactful%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&entry.906535625=Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang&entry.1292438233=%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20have%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20their%20fundamental%20characteristics%20and%20challenges%0Amotivated%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%0Athe%20roadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17009v3&entry.124074799=Read"},
{"title": "Weak Generative Sampler to Efficiently Sample Invariant Distribution of\n  Stochastic Differential Equation", "author": "Zhiqiang Cai and Yu Cao and Yuanfei Huang and Xiang Zhou", "abstract": "  Sampling invariant distributions from an Ito diffusion process presents a\nsignificant challenge in stochastic simulation. Traditional numerical solvers\nfor stochastic differential equations require both a fine step size and a\nlengthy simulation period, resulting in both biased and correlated samples.\nCurrent deep learning-based method solves the stationary Fokker--Planck\nequation to determine the invariant probability density function in form of\ndeep neural networks, but they generally do not directly address the problem of\nsampling from the computed density function. In this work, we introduce a\nframework that employs a weak generative sampler (WGS) to directly generate\nindependent and identically distributed (iid) samples induced by a\ntransformation map derived from the stationary Fokker--Planck equation. Our\nproposed loss function is based on the weak form of the Fokker--Planck\nequation, integrating normalizing flows to characterize the invariant\ndistribution and facilitate sample generation from the base distribution. Our\nrandomized test function circumvents the need for mini-max optimization in the\ntraditional weak formulation. Distinct from conventional generative models, our\nmethod neither necessitates the computationally intensive calculation of the\nJacobian determinant nor the invertibility of the transformation map. A crucial\ncomponent of our framework is the adaptively chosen family of test functions in\nthe form of Gaussian kernel functions with centres selected from the generated\ndata samples. Experimental results on several benchmark examples demonstrate\nthe effectiveness of our method, which offers both low computational costs and\nexcellent capability in exploring multiple metastable states.\n", "link": "http://arxiv.org/abs/2405.19256v1", "date": "2024-05-29", "relevancy": 2.0628, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5197}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5148}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak%20Generative%20Sampler%20to%20Efficiently%20Sample%20Invariant%20Distribution%20of%0A%20%20Stochastic%20Differential%20Equation&body=Title%3A%20Weak%20Generative%20Sampler%20to%20Efficiently%20Sample%20Invariant%20Distribution%20of%0A%20%20Stochastic%20Differential%20Equation%0AAuthor%3A%20Zhiqiang%20Cai%20and%20Yu%20Cao%20and%20Yuanfei%20Huang%20and%20Xiang%20Zhou%0AAbstract%3A%20%20%20Sampling%20invariant%20distributions%20from%20an%20Ito%20diffusion%20process%20presents%20a%0Asignificant%20challenge%20in%20stochastic%20simulation.%20Traditional%20numerical%20solvers%0Afor%20stochastic%20differential%20equations%20require%20both%20a%20fine%20step%20size%20and%20a%0Alengthy%20simulation%20period%2C%20resulting%20in%20both%20biased%20and%20correlated%20samples.%0ACurrent%20deep%20learning-based%20method%20solves%20the%20stationary%20Fokker--Planck%0Aequation%20to%20determine%20the%20invariant%20probability%20density%20function%20in%20form%20of%0Adeep%20neural%20networks%2C%20but%20they%20generally%20do%20not%20directly%20address%20the%20problem%20of%0Asampling%20from%20the%20computed%20density%20function.%20In%20this%20work%2C%20we%20introduce%20a%0Aframework%20that%20employs%20a%20weak%20generative%20sampler%20%28WGS%29%20to%20directly%20generate%0Aindependent%20and%20identically%20distributed%20%28iid%29%20samples%20induced%20by%20a%0Atransformation%20map%20derived%20from%20the%20stationary%20Fokker--Planck%20equation.%20Our%0Aproposed%20loss%20function%20is%20based%20on%20the%20weak%20form%20of%20the%20Fokker--Planck%0Aequation%2C%20integrating%20normalizing%20flows%20to%20characterize%20the%20invariant%0Adistribution%20and%20facilitate%20sample%20generation%20from%20the%20base%20distribution.%20Our%0Arandomized%20test%20function%20circumvents%20the%20need%20for%20mini-max%20optimization%20in%20the%0Atraditional%20weak%20formulation.%20Distinct%20from%20conventional%20generative%20models%2C%20our%0Amethod%20neither%20necessitates%20the%20computationally%20intensive%20calculation%20of%20the%0AJacobian%20determinant%20nor%20the%20invertibility%20of%20the%20transformation%20map.%20A%20crucial%0Acomponent%20of%20our%20framework%20is%20the%20adaptively%20chosen%20family%20of%20test%20functions%20in%0Athe%20form%20of%20Gaussian%20kernel%20functions%20with%20centres%20selected%20from%20the%20generated%0Adata%20samples.%20Experimental%20results%20on%20several%20benchmark%20examples%20demonstrate%0Athe%20effectiveness%20of%20our%20method%2C%20which%20offers%20both%20low%20computational%20costs%20and%0Aexcellent%20capability%20in%20exploring%20multiple%20metastable%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak%2520Generative%2520Sampler%2520to%2520Efficiently%2520Sample%2520Invariant%2520Distribution%2520of%250A%2520%2520Stochastic%2520Differential%2520Equation%26entry.906535625%3DZhiqiang%2520Cai%2520and%2520Yu%2520Cao%2520and%2520Yuanfei%2520Huang%2520and%2520Xiang%2520Zhou%26entry.1292438233%3D%2520%2520Sampling%2520invariant%2520distributions%2520from%2520an%2520Ito%2520diffusion%2520process%2520presents%2520a%250Asignificant%2520challenge%2520in%2520stochastic%2520simulation.%2520Traditional%2520numerical%2520solvers%250Afor%2520stochastic%2520differential%2520equations%2520require%2520both%2520a%2520fine%2520step%2520size%2520and%2520a%250Alengthy%2520simulation%2520period%252C%2520resulting%2520in%2520both%2520biased%2520and%2520correlated%2520samples.%250ACurrent%2520deep%2520learning-based%2520method%2520solves%2520the%2520stationary%2520Fokker--Planck%250Aequation%2520to%2520determine%2520the%2520invariant%2520probability%2520density%2520function%2520in%2520form%2520of%250Adeep%2520neural%2520networks%252C%2520but%2520they%2520generally%2520do%2520not%2520directly%2520address%2520the%2520problem%2520of%250Asampling%2520from%2520the%2520computed%2520density%2520function.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Aframework%2520that%2520employs%2520a%2520weak%2520generative%2520sampler%2520%2528WGS%2529%2520to%2520directly%2520generate%250Aindependent%2520and%2520identically%2520distributed%2520%2528iid%2529%2520samples%2520induced%2520by%2520a%250Atransformation%2520map%2520derived%2520from%2520the%2520stationary%2520Fokker--Planck%2520equation.%2520Our%250Aproposed%2520loss%2520function%2520is%2520based%2520on%2520the%2520weak%2520form%2520of%2520the%2520Fokker--Planck%250Aequation%252C%2520integrating%2520normalizing%2520flows%2520to%2520characterize%2520the%2520invariant%250Adistribution%2520and%2520facilitate%2520sample%2520generation%2520from%2520the%2520base%2520distribution.%2520Our%250Arandomized%2520test%2520function%2520circumvents%2520the%2520need%2520for%2520mini-max%2520optimization%2520in%2520the%250Atraditional%2520weak%2520formulation.%2520Distinct%2520from%2520conventional%2520generative%2520models%252C%2520our%250Amethod%2520neither%2520necessitates%2520the%2520computationally%2520intensive%2520calculation%2520of%2520the%250AJacobian%2520determinant%2520nor%2520the%2520invertibility%2520of%2520the%2520transformation%2520map.%2520A%2520crucial%250Acomponent%2520of%2520our%2520framework%2520is%2520the%2520adaptively%2520chosen%2520family%2520of%2520test%2520functions%2520in%250Athe%2520form%2520of%2520Gaussian%2520kernel%2520functions%2520with%2520centres%2520selected%2520from%2520the%2520generated%250Adata%2520samples.%2520Experimental%2520results%2520on%2520several%2520benchmark%2520examples%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520method%252C%2520which%2520offers%2520both%2520low%2520computational%2520costs%2520and%250Aexcellent%2520capability%2520in%2520exploring%2520multiple%2520metastable%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak%20Generative%20Sampler%20to%20Efficiently%20Sample%20Invariant%20Distribution%20of%0A%20%20Stochastic%20Differential%20Equation&entry.906535625=Zhiqiang%20Cai%20and%20Yu%20Cao%20and%20Yuanfei%20Huang%20and%20Xiang%20Zhou&entry.1292438233=%20%20Sampling%20invariant%20distributions%20from%20an%20Ito%20diffusion%20process%20presents%20a%0Asignificant%20challenge%20in%20stochastic%20simulation.%20Traditional%20numerical%20solvers%0Afor%20stochastic%20differential%20equations%20require%20both%20a%20fine%20step%20size%20and%20a%0Alengthy%20simulation%20period%2C%20resulting%20in%20both%20biased%20and%20correlated%20samples.%0ACurrent%20deep%20learning-based%20method%20solves%20the%20stationary%20Fokker--Planck%0Aequation%20to%20determine%20the%20invariant%20probability%20density%20function%20in%20form%20of%0Adeep%20neural%20networks%2C%20but%20they%20generally%20do%20not%20directly%20address%20the%20problem%20of%0Asampling%20from%20the%20computed%20density%20function.%20In%20this%20work%2C%20we%20introduce%20a%0Aframework%20that%20employs%20a%20weak%20generative%20sampler%20%28WGS%29%20to%20directly%20generate%0Aindependent%20and%20identically%20distributed%20%28iid%29%20samples%20induced%20by%20a%0Atransformation%20map%20derived%20from%20the%20stationary%20Fokker--Planck%20equation.%20Our%0Aproposed%20loss%20function%20is%20based%20on%20the%20weak%20form%20of%20the%20Fokker--Planck%0Aequation%2C%20integrating%20normalizing%20flows%20to%20characterize%20the%20invariant%0Adistribution%20and%20facilitate%20sample%20generation%20from%20the%20base%20distribution.%20Our%0Arandomized%20test%20function%20circumvents%20the%20need%20for%20mini-max%20optimization%20in%20the%0Atraditional%20weak%20formulation.%20Distinct%20from%20conventional%20generative%20models%2C%20our%0Amethod%20neither%20necessitates%20the%20computationally%20intensive%20calculation%20of%20the%0AJacobian%20determinant%20nor%20the%20invertibility%20of%20the%20transformation%20map.%20A%20crucial%0Acomponent%20of%20our%20framework%20is%20the%20adaptively%20chosen%20family%20of%20test%20functions%20in%0Athe%20form%20of%20Gaussian%20kernel%20functions%20with%20centres%20selected%20from%20the%20generated%0Adata%20samples.%20Experimental%20results%20on%20several%20benchmark%20examples%20demonstrate%0Athe%20effectiveness%20of%20our%20method%2C%20which%20offers%20both%20low%20computational%20costs%20and%0Aexcellent%20capability%20in%20exploring%20multiple%20metastable%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19256v1&entry.124074799=Read"},
{"title": "Track Anything Rapter(TAR)", "author": "Tharun V. Puthanveettil and Fnu Obaid ur Rahman", "abstract": "  Object tracking is a fundamental task in computer vision with broad practical\napplications across various domains, including traffic monitoring, robotics,\nand autonomous vehicle tracking. In this project, we aim to develop a\nsophisticated aerial vehicle system known as Track Anything Rapter (TAR),\ndesigned to detect, segment, and track objects of interest based on\nuser-provided multimodal queries, such as text, images, and clicks. TAR\nutilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate\nthe relative pose of the queried object. The tracking problem is approached as\na Visual Servoing task, enabling the UAV to consistently focus on the object\nthrough advanced motion planning and control algorithms. We showcase how the\nintegration of these foundational models with a custom high-level control\nalgorithm results in a highly stable and precise tracking system deployed on a\ncustom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking\nalgorithm's performance, we compare it against Vicon-based ground truth.\nAdditionally, we evaluate the reliability of the foundational models in aiding\ntracking in scenarios involving occlusions. Finally, we test and validate the\nmodel's ability to work seamlessly with multiple modalities, such as click,\nbounding box, and image templates.\n", "link": "http://arxiv.org/abs/2405.11655v2", "date": "2024-05-29", "relevancy": 2.0593, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Track%20Anything%20Rapter%28TAR%29&body=Title%3A%20Track%20Anything%20Rapter%28TAR%29%0AAuthor%3A%20Tharun%20V.%20Puthanveettil%20and%20Fnu%20Obaid%20ur%20Rahman%0AAbstract%3A%20%20%20Object%20tracking%20is%20a%20fundamental%20task%20in%20computer%20vision%20with%20broad%20practical%0Aapplications%20across%20various%20domains%2C%20including%20traffic%20monitoring%2C%20robotics%2C%0Aand%20autonomous%20vehicle%20tracking.%20In%20this%20project%2C%20we%20aim%20to%20develop%20a%0Asophisticated%20aerial%20vehicle%20system%20known%20as%20Track%20Anything%20Rapter%20%28TAR%29%2C%0Adesigned%20to%20detect%2C%20segment%2C%20and%20track%20objects%20of%20interest%20based%20on%0Auser-provided%20multimodal%20queries%2C%20such%20as%20text%2C%20images%2C%20and%20clicks.%20TAR%0Autilizes%20cutting-edge%20pre-trained%20models%20like%20DINO%2C%20CLIP%2C%20and%20SAM%20to%20estimate%0Athe%20relative%20pose%20of%20the%20queried%20object.%20The%20tracking%20problem%20is%20approached%20as%0Aa%20Visual%20Servoing%20task%2C%20enabling%20the%20UAV%20to%20consistently%20focus%20on%20the%20object%0Athrough%20advanced%20motion%20planning%20and%20control%20algorithms.%20We%20showcase%20how%20the%0Aintegration%20of%20these%20foundational%20models%20with%20a%20custom%20high-level%20control%0Aalgorithm%20results%20in%20a%20highly%20stable%20and%20precise%20tracking%20system%20deployed%20on%20a%0Acustom-built%20PX4%20Autopilot-enabled%20Voxl2%20M500%20drone.%20To%20validate%20the%20tracking%0Aalgorithm%27s%20performance%2C%20we%20compare%20it%20against%20Vicon-based%20ground%20truth.%0AAdditionally%2C%20we%20evaluate%20the%20reliability%20of%20the%20foundational%20models%20in%20aiding%0Atracking%20in%20scenarios%20involving%20occlusions.%20Finally%2C%20we%20test%20and%20validate%20the%0Amodel%27s%20ability%20to%20work%20seamlessly%20with%20multiple%20modalities%2C%20such%20as%20click%2C%0Abounding%20box%2C%20and%20image%20templates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrack%2520Anything%2520Rapter%2528TAR%2529%26entry.906535625%3DTharun%2520V.%2520Puthanveettil%2520and%2520Fnu%2520Obaid%2520ur%2520Rahman%26entry.1292438233%3D%2520%2520Object%2520tracking%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%2520with%2520broad%2520practical%250Aapplications%2520across%2520various%2520domains%252C%2520including%2520traffic%2520monitoring%252C%2520robotics%252C%250Aand%2520autonomous%2520vehicle%2520tracking.%2520In%2520this%2520project%252C%2520we%2520aim%2520to%2520develop%2520a%250Asophisticated%2520aerial%2520vehicle%2520system%2520known%2520as%2520Track%2520Anything%2520Rapter%2520%2528TAR%2529%252C%250Adesigned%2520to%2520detect%252C%2520segment%252C%2520and%2520track%2520objects%2520of%2520interest%2520based%2520on%250Auser-provided%2520multimodal%2520queries%252C%2520such%2520as%2520text%252C%2520images%252C%2520and%2520clicks.%2520TAR%250Autilizes%2520cutting-edge%2520pre-trained%2520models%2520like%2520DINO%252C%2520CLIP%252C%2520and%2520SAM%2520to%2520estimate%250Athe%2520relative%2520pose%2520of%2520the%2520queried%2520object.%2520The%2520tracking%2520problem%2520is%2520approached%2520as%250Aa%2520Visual%2520Servoing%2520task%252C%2520enabling%2520the%2520UAV%2520to%2520consistently%2520focus%2520on%2520the%2520object%250Athrough%2520advanced%2520motion%2520planning%2520and%2520control%2520algorithms.%2520We%2520showcase%2520how%2520the%250Aintegration%2520of%2520these%2520foundational%2520models%2520with%2520a%2520custom%2520high-level%2520control%250Aalgorithm%2520results%2520in%2520a%2520highly%2520stable%2520and%2520precise%2520tracking%2520system%2520deployed%2520on%2520a%250Acustom-built%2520PX4%2520Autopilot-enabled%2520Voxl2%2520M500%2520drone.%2520To%2520validate%2520the%2520tracking%250Aalgorithm%2527s%2520performance%252C%2520we%2520compare%2520it%2520against%2520Vicon-based%2520ground%2520truth.%250AAdditionally%252C%2520we%2520evaluate%2520the%2520reliability%2520of%2520the%2520foundational%2520models%2520in%2520aiding%250Atracking%2520in%2520scenarios%2520involving%2520occlusions.%2520Finally%252C%2520we%2520test%2520and%2520validate%2520the%250Amodel%2527s%2520ability%2520to%2520work%2520seamlessly%2520with%2520multiple%2520modalities%252C%2520such%2520as%2520click%252C%250Abounding%2520box%252C%2520and%2520image%2520templates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Track%20Anything%20Rapter%28TAR%29&entry.906535625=Tharun%20V.%20Puthanveettil%20and%20Fnu%20Obaid%20ur%20Rahman&entry.1292438233=%20%20Object%20tracking%20is%20a%20fundamental%20task%20in%20computer%20vision%20with%20broad%20practical%0Aapplications%20across%20various%20domains%2C%20including%20traffic%20monitoring%2C%20robotics%2C%0Aand%20autonomous%20vehicle%20tracking.%20In%20this%20project%2C%20we%20aim%20to%20develop%20a%0Asophisticated%20aerial%20vehicle%20system%20known%20as%20Track%20Anything%20Rapter%20%28TAR%29%2C%0Adesigned%20to%20detect%2C%20segment%2C%20and%20track%20objects%20of%20interest%20based%20on%0Auser-provided%20multimodal%20queries%2C%20such%20as%20text%2C%20images%2C%20and%20clicks.%20TAR%0Autilizes%20cutting-edge%20pre-trained%20models%20like%20DINO%2C%20CLIP%2C%20and%20SAM%20to%20estimate%0Athe%20relative%20pose%20of%20the%20queried%20object.%20The%20tracking%20problem%20is%20approached%20as%0Aa%20Visual%20Servoing%20task%2C%20enabling%20the%20UAV%20to%20consistently%20focus%20on%20the%20object%0Athrough%20advanced%20motion%20planning%20and%20control%20algorithms.%20We%20showcase%20how%20the%0Aintegration%20of%20these%20foundational%20models%20with%20a%20custom%20high-level%20control%0Aalgorithm%20results%20in%20a%20highly%20stable%20and%20precise%20tracking%20system%20deployed%20on%20a%0Acustom-built%20PX4%20Autopilot-enabled%20Voxl2%20M500%20drone.%20To%20validate%20the%20tracking%0Aalgorithm%27s%20performance%2C%20we%20compare%20it%20against%20Vicon-based%20ground%20truth.%0AAdditionally%2C%20we%20evaluate%20the%20reliability%20of%20the%20foundational%20models%20in%20aiding%0Atracking%20in%20scenarios%20involving%20occlusions.%20Finally%2C%20we%20test%20and%20validate%20the%0Amodel%27s%20ability%20to%20work%20seamlessly%20with%20multiple%20modalities%2C%20such%20as%20click%2C%0Abounding%20box%2C%20and%20image%20templates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11655v2&entry.124074799=Read"},
{"title": "Gradient Guided Hypotheses: A unified solution to enable machine\n  learning models on scarce and noisy data regimes", "author": "Paulo Neves and Joerg K. Wegner and Philippe Schwaller", "abstract": "  Ensuring high-quality data is paramount for maximizing the performance of\nmachine learning models and business intelligence systems. However, challenges\nin data quality, including noise in data capture, missing records, limited data\nproduction, and confounding variables, significantly constrain the potential\nperformance of these systems. In this study, we propose an\narchitecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to\naddress these challenges. GGH analyses gradients from hypotheses as a proxy of\ndistinct and possibly contradictory patterns in the data. This framework\nentails an additional step in machine learning training, where gradients can be\nincluded or excluded from backpropagation. In this manner, missing and noisy\ndata are addressed through a unified solution that perceives both challenges as\nfacets of the same overarching issue: the propagation of erroneous information.\nExperimental validation of GGH is conducted using real-world open-source\ndatasets, where records with missing rates of up to 98.5% are simulated.\nComparative analysis with state-of-the-art imputation methods demonstrates a\nsubstantial improvement in model performance achieved by GGH. Specifically in\nvery high scarcity regimes, GGH was found to be the only viable solution.\nAdditionally, GGH's noise detection capabilities are showcased by introducing\nsimulated noise into the datasets and observing enhanced model performance\nafter filtering out the noisy data. This study presents GGH as a promising\nsolution for improving data quality and model performance in various\napplications.\n", "link": "http://arxiv.org/abs/2405.19210v1", "date": "2024-05-29", "relevancy": 2.0581, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5385}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Guided%20Hypotheses%3A%20A%20unified%20solution%20to%20enable%20machine%0A%20%20learning%20models%20on%20scarce%20and%20noisy%20data%20regimes&body=Title%3A%20Gradient%20Guided%20Hypotheses%3A%20A%20unified%20solution%20to%20enable%20machine%0A%20%20learning%20models%20on%20scarce%20and%20noisy%20data%20regimes%0AAuthor%3A%20Paulo%20Neves%20and%20Joerg%20K.%20Wegner%20and%20Philippe%20Schwaller%0AAbstract%3A%20%20%20Ensuring%20high-quality%20data%20is%20paramount%20for%20maximizing%20the%20performance%20of%0Amachine%20learning%20models%20and%20business%20intelligence%20systems.%20However%2C%20challenges%0Ain%20data%20quality%2C%20including%20noise%20in%20data%20capture%2C%20missing%20records%2C%20limited%20data%0Aproduction%2C%20and%20confounding%20variables%2C%20significantly%20constrain%20the%20potential%0Aperformance%20of%20these%20systems.%20In%20this%20study%2C%20we%20propose%20an%0Aarchitecture-agnostic%20algorithm%2C%20Gradient%20Guided%20Hypotheses%20%28GGH%29%2C%20designed%20to%0Aaddress%20these%20challenges.%20GGH%20analyses%20gradients%20from%20hypotheses%20as%20a%20proxy%20of%0Adistinct%20and%20possibly%20contradictory%20patterns%20in%20the%20data.%20This%20framework%0Aentails%20an%20additional%20step%20in%20machine%20learning%20training%2C%20where%20gradients%20can%20be%0Aincluded%20or%20excluded%20from%20backpropagation.%20In%20this%20manner%2C%20missing%20and%20noisy%0Adata%20are%20addressed%20through%20a%20unified%20solution%20that%20perceives%20both%20challenges%20as%0Afacets%20of%20the%20same%20overarching%20issue%3A%20the%20propagation%20of%20erroneous%20information.%0AExperimental%20validation%20of%20GGH%20is%20conducted%20using%20real-world%20open-source%0Adatasets%2C%20where%20records%20with%20missing%20rates%20of%20up%20to%2098.5%25%20are%20simulated.%0AComparative%20analysis%20with%20state-of-the-art%20imputation%20methods%20demonstrates%20a%0Asubstantial%20improvement%20in%20model%20performance%20achieved%20by%20GGH.%20Specifically%20in%0Avery%20high%20scarcity%20regimes%2C%20GGH%20was%20found%20to%20be%20the%20only%20viable%20solution.%0AAdditionally%2C%20GGH%27s%20noise%20detection%20capabilities%20are%20showcased%20by%20introducing%0Asimulated%20noise%20into%20the%20datasets%20and%20observing%20enhanced%20model%20performance%0Aafter%20filtering%20out%20the%20noisy%20data.%20This%20study%20presents%20GGH%20as%20a%20promising%0Asolution%20for%20improving%20data%20quality%20and%20model%20performance%20in%20various%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Guided%2520Hypotheses%253A%2520A%2520unified%2520solution%2520to%2520enable%2520machine%250A%2520%2520learning%2520models%2520on%2520scarce%2520and%2520noisy%2520data%2520regimes%26entry.906535625%3DPaulo%2520Neves%2520and%2520Joerg%2520K.%2520Wegner%2520and%2520Philippe%2520Schwaller%26entry.1292438233%3D%2520%2520Ensuring%2520high-quality%2520data%2520is%2520paramount%2520for%2520maximizing%2520the%2520performance%2520of%250Amachine%2520learning%2520models%2520and%2520business%2520intelligence%2520systems.%2520However%252C%2520challenges%250Ain%2520data%2520quality%252C%2520including%2520noise%2520in%2520data%2520capture%252C%2520missing%2520records%252C%2520limited%2520data%250Aproduction%252C%2520and%2520confounding%2520variables%252C%2520significantly%2520constrain%2520the%2520potential%250Aperformance%2520of%2520these%2520systems.%2520In%2520this%2520study%252C%2520we%2520propose%2520an%250Aarchitecture-agnostic%2520algorithm%252C%2520Gradient%2520Guided%2520Hypotheses%2520%2528GGH%2529%252C%2520designed%2520to%250Aaddress%2520these%2520challenges.%2520GGH%2520analyses%2520gradients%2520from%2520hypotheses%2520as%2520a%2520proxy%2520of%250Adistinct%2520and%2520possibly%2520contradictory%2520patterns%2520in%2520the%2520data.%2520This%2520framework%250Aentails%2520an%2520additional%2520step%2520in%2520machine%2520learning%2520training%252C%2520where%2520gradients%2520can%2520be%250Aincluded%2520or%2520excluded%2520from%2520backpropagation.%2520In%2520this%2520manner%252C%2520missing%2520and%2520noisy%250Adata%2520are%2520addressed%2520through%2520a%2520unified%2520solution%2520that%2520perceives%2520both%2520challenges%2520as%250Afacets%2520of%2520the%2520same%2520overarching%2520issue%253A%2520the%2520propagation%2520of%2520erroneous%2520information.%250AExperimental%2520validation%2520of%2520GGH%2520is%2520conducted%2520using%2520real-world%2520open-source%250Adatasets%252C%2520where%2520records%2520with%2520missing%2520rates%2520of%2520up%2520to%252098.5%2525%2520are%2520simulated.%250AComparative%2520analysis%2520with%2520state-of-the-art%2520imputation%2520methods%2520demonstrates%2520a%250Asubstantial%2520improvement%2520in%2520model%2520performance%2520achieved%2520by%2520GGH.%2520Specifically%2520in%250Avery%2520high%2520scarcity%2520regimes%252C%2520GGH%2520was%2520found%2520to%2520be%2520the%2520only%2520viable%2520solution.%250AAdditionally%252C%2520GGH%2527s%2520noise%2520detection%2520capabilities%2520are%2520showcased%2520by%2520introducing%250Asimulated%2520noise%2520into%2520the%2520datasets%2520and%2520observing%2520enhanced%2520model%2520performance%250Aafter%2520filtering%2520out%2520the%2520noisy%2520data.%2520This%2520study%2520presents%2520GGH%2520as%2520a%2520promising%250Asolution%2520for%2520improving%2520data%2520quality%2520and%2520model%2520performance%2520in%2520various%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Guided%20Hypotheses%3A%20A%20unified%20solution%20to%20enable%20machine%0A%20%20learning%20models%20on%20scarce%20and%20noisy%20data%20regimes&entry.906535625=Paulo%20Neves%20and%20Joerg%20K.%20Wegner%20and%20Philippe%20Schwaller&entry.1292438233=%20%20Ensuring%20high-quality%20data%20is%20paramount%20for%20maximizing%20the%20performance%20of%0Amachine%20learning%20models%20and%20business%20intelligence%20systems.%20However%2C%20challenges%0Ain%20data%20quality%2C%20including%20noise%20in%20data%20capture%2C%20missing%20records%2C%20limited%20data%0Aproduction%2C%20and%20confounding%20variables%2C%20significantly%20constrain%20the%20potential%0Aperformance%20of%20these%20systems.%20In%20this%20study%2C%20we%20propose%20an%0Aarchitecture-agnostic%20algorithm%2C%20Gradient%20Guided%20Hypotheses%20%28GGH%29%2C%20designed%20to%0Aaddress%20these%20challenges.%20GGH%20analyses%20gradients%20from%20hypotheses%20as%20a%20proxy%20of%0Adistinct%20and%20possibly%20contradictory%20patterns%20in%20the%20data.%20This%20framework%0Aentails%20an%20additional%20step%20in%20machine%20learning%20training%2C%20where%20gradients%20can%20be%0Aincluded%20or%20excluded%20from%20backpropagation.%20In%20this%20manner%2C%20missing%20and%20noisy%0Adata%20are%20addressed%20through%20a%20unified%20solution%20that%20perceives%20both%20challenges%20as%0Afacets%20of%20the%20same%20overarching%20issue%3A%20the%20propagation%20of%20erroneous%20information.%0AExperimental%20validation%20of%20GGH%20is%20conducted%20using%20real-world%20open-source%0Adatasets%2C%20where%20records%20with%20missing%20rates%20of%20up%20to%2098.5%25%20are%20simulated.%0AComparative%20analysis%20with%20state-of-the-art%20imputation%20methods%20demonstrates%20a%0Asubstantial%20improvement%20in%20model%20performance%20achieved%20by%20GGH.%20Specifically%20in%0Avery%20high%20scarcity%20regimes%2C%20GGH%20was%20found%20to%20be%20the%20only%20viable%20solution.%0AAdditionally%2C%20GGH%27s%20noise%20detection%20capabilities%20are%20showcased%20by%20introducing%0Asimulated%20noise%20into%20the%20datasets%20and%20observing%20enhanced%20model%20performance%0Aafter%20filtering%20out%20the%20noisy%20data.%20This%20study%20presents%20GGH%20as%20a%20promising%0Asolution%20for%20improving%20data%20quality%20and%20model%20performance%20in%20various%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19210v1&entry.124074799=Read"},
{"title": "Does learning the right latent variables necessarily improve in-context\n  learning?", "author": "Sarthak Mittal and Eric Elmoznino and Leo Gagnon and Sangnie Bhardwaj and Dhanya Sridhar and Guillaume Lajoie", "abstract": "  Large autoregressive models like Transformers can solve tasks through\nin-context learning (ICL) without learning new weights, suggesting avenues for\nefficiently solving new tasks. For many tasks, e.g., linear regression, the\ndata factorizes: examples are independent given a task latent that generates\nthe data, e.g., linear coefficients. While an optimal predictor leverages this\nfactorization by inferring task latents, it is unclear if Transformers\nimplicitly do so or if they instead exploit heuristics and statistical\nshortcuts enabled by attention layers. Both scenarios have inspired active\nongoing work. In this paper, we systematically investigate the effect of\nexplicitly inferring task latents. We minimally modify the Transformer\narchitecture with a bottleneck designed to prevent shortcuts in favor of more\nstructured solutions, and then compare performance against standard\nTransformers across various ICL tasks. Contrary to intuition and some recent\nworks, we find little discernible difference between the two; biasing towards\ntask-relevant latent variables does not lead to better out-of-distribution\nperformance, in general. Curiously, we find that while the bottleneck\neffectively learns to extract latent task variables from context, downstream\nprocessing struggles to utilize them for robust prediction. Our study\nhighlights the intrinsic limitations of Transformers in achieving structured\nICL solutions that generalize, and shows that while inferring the right latents\naids interpretability, it is not sufficient to alleviate this problem.\n", "link": "http://arxiv.org/abs/2405.19162v1", "date": "2024-05-29", "relevancy": 2.0544, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20learning%20the%20right%20latent%20variables%20necessarily%20improve%20in-context%0A%20%20learning%3F&body=Title%3A%20Does%20learning%20the%20right%20latent%20variables%20necessarily%20improve%20in-context%0A%20%20learning%3F%0AAuthor%3A%20Sarthak%20Mittal%20and%20Eric%20Elmoznino%20and%20Leo%20Gagnon%20and%20Sangnie%20Bhardwaj%20and%20Dhanya%20Sridhar%20and%20Guillaume%20Lajoie%0AAbstract%3A%20%20%20Large%20autoregressive%20models%20like%20Transformers%20can%20solve%20tasks%20through%0Ain-context%20learning%20%28ICL%29%20without%20learning%20new%20weights%2C%20suggesting%20avenues%20for%0Aefficiently%20solving%20new%20tasks.%20For%20many%20tasks%2C%20e.g.%2C%20linear%20regression%2C%20the%0Adata%20factorizes%3A%20examples%20are%20independent%20given%20a%20task%20latent%20that%20generates%0Athe%20data%2C%20e.g.%2C%20linear%20coefficients.%20While%20an%20optimal%20predictor%20leverages%20this%0Afactorization%20by%20inferring%20task%20latents%2C%20it%20is%20unclear%20if%20Transformers%0Aimplicitly%20do%20so%20or%20if%20they%20instead%20exploit%20heuristics%20and%20statistical%0Ashortcuts%20enabled%20by%20attention%20layers.%20Both%20scenarios%20have%20inspired%20active%0Aongoing%20work.%20In%20this%20paper%2C%20we%20systematically%20investigate%20the%20effect%20of%0Aexplicitly%20inferring%20task%20latents.%20We%20minimally%20modify%20the%20Transformer%0Aarchitecture%20with%20a%20bottleneck%20designed%20to%20prevent%20shortcuts%20in%20favor%20of%20more%0Astructured%20solutions%2C%20and%20then%20compare%20performance%20against%20standard%0ATransformers%20across%20various%20ICL%20tasks.%20Contrary%20to%20intuition%20and%20some%20recent%0Aworks%2C%20we%20find%20little%20discernible%20difference%20between%20the%20two%3B%20biasing%20towards%0Atask-relevant%20latent%20variables%20does%20not%20lead%20to%20better%20out-of-distribution%0Aperformance%2C%20in%20general.%20Curiously%2C%20we%20find%20that%20while%20the%20bottleneck%0Aeffectively%20learns%20to%20extract%20latent%20task%20variables%20from%20context%2C%20downstream%0Aprocessing%20struggles%20to%20utilize%20them%20for%20robust%20prediction.%20Our%20study%0Ahighlights%20the%20intrinsic%20limitations%20of%20Transformers%20in%20achieving%20structured%0AICL%20solutions%20that%20generalize%2C%20and%20shows%20that%20while%20inferring%20the%20right%20latents%0Aaids%20interpretability%2C%20it%20is%20not%20sufficient%20to%20alleviate%20this%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520learning%2520the%2520right%2520latent%2520variables%2520necessarily%2520improve%2520in-context%250A%2520%2520learning%253F%26entry.906535625%3DSarthak%2520Mittal%2520and%2520Eric%2520Elmoznino%2520and%2520Leo%2520Gagnon%2520and%2520Sangnie%2520Bhardwaj%2520and%2520Dhanya%2520Sridhar%2520and%2520Guillaume%2520Lajoie%26entry.1292438233%3D%2520%2520Large%2520autoregressive%2520models%2520like%2520Transformers%2520can%2520solve%2520tasks%2520through%250Ain-context%2520learning%2520%2528ICL%2529%2520without%2520learning%2520new%2520weights%252C%2520suggesting%2520avenues%2520for%250Aefficiently%2520solving%2520new%2520tasks.%2520For%2520many%2520tasks%252C%2520e.g.%252C%2520linear%2520regression%252C%2520the%250Adata%2520factorizes%253A%2520examples%2520are%2520independent%2520given%2520a%2520task%2520latent%2520that%2520generates%250Athe%2520data%252C%2520e.g.%252C%2520linear%2520coefficients.%2520While%2520an%2520optimal%2520predictor%2520leverages%2520this%250Afactorization%2520by%2520inferring%2520task%2520latents%252C%2520it%2520is%2520unclear%2520if%2520Transformers%250Aimplicitly%2520do%2520so%2520or%2520if%2520they%2520instead%2520exploit%2520heuristics%2520and%2520statistical%250Ashortcuts%2520enabled%2520by%2520attention%2520layers.%2520Both%2520scenarios%2520have%2520inspired%2520active%250Aongoing%2520work.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520the%2520effect%2520of%250Aexplicitly%2520inferring%2520task%2520latents.%2520We%2520minimally%2520modify%2520the%2520Transformer%250Aarchitecture%2520with%2520a%2520bottleneck%2520designed%2520to%2520prevent%2520shortcuts%2520in%2520favor%2520of%2520more%250Astructured%2520solutions%252C%2520and%2520then%2520compare%2520performance%2520against%2520standard%250ATransformers%2520across%2520various%2520ICL%2520tasks.%2520Contrary%2520to%2520intuition%2520and%2520some%2520recent%250Aworks%252C%2520we%2520find%2520little%2520discernible%2520difference%2520between%2520the%2520two%253B%2520biasing%2520towards%250Atask-relevant%2520latent%2520variables%2520does%2520not%2520lead%2520to%2520better%2520out-of-distribution%250Aperformance%252C%2520in%2520general.%2520Curiously%252C%2520we%2520find%2520that%2520while%2520the%2520bottleneck%250Aeffectively%2520learns%2520to%2520extract%2520latent%2520task%2520variables%2520from%2520context%252C%2520downstream%250Aprocessing%2520struggles%2520to%2520utilize%2520them%2520for%2520robust%2520prediction.%2520Our%2520study%250Ahighlights%2520the%2520intrinsic%2520limitations%2520of%2520Transformers%2520in%2520achieving%2520structured%250AICL%2520solutions%2520that%2520generalize%252C%2520and%2520shows%2520that%2520while%2520inferring%2520the%2520right%2520latents%250Aaids%2520interpretability%252C%2520it%2520is%2520not%2520sufficient%2520to%2520alleviate%2520this%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20learning%20the%20right%20latent%20variables%20necessarily%20improve%20in-context%0A%20%20learning%3F&entry.906535625=Sarthak%20Mittal%20and%20Eric%20Elmoznino%20and%20Leo%20Gagnon%20and%20Sangnie%20Bhardwaj%20and%20Dhanya%20Sridhar%20and%20Guillaume%20Lajoie&entry.1292438233=%20%20Large%20autoregressive%20models%20like%20Transformers%20can%20solve%20tasks%20through%0Ain-context%20learning%20%28ICL%29%20without%20learning%20new%20weights%2C%20suggesting%20avenues%20for%0Aefficiently%20solving%20new%20tasks.%20For%20many%20tasks%2C%20e.g.%2C%20linear%20regression%2C%20the%0Adata%20factorizes%3A%20examples%20are%20independent%20given%20a%20task%20latent%20that%20generates%0Athe%20data%2C%20e.g.%2C%20linear%20coefficients.%20While%20an%20optimal%20predictor%20leverages%20this%0Afactorization%20by%20inferring%20task%20latents%2C%20it%20is%20unclear%20if%20Transformers%0Aimplicitly%20do%20so%20or%20if%20they%20instead%20exploit%20heuristics%20and%20statistical%0Ashortcuts%20enabled%20by%20attention%20layers.%20Both%20scenarios%20have%20inspired%20active%0Aongoing%20work.%20In%20this%20paper%2C%20we%20systematically%20investigate%20the%20effect%20of%0Aexplicitly%20inferring%20task%20latents.%20We%20minimally%20modify%20the%20Transformer%0Aarchitecture%20with%20a%20bottleneck%20designed%20to%20prevent%20shortcuts%20in%20favor%20of%20more%0Astructured%20solutions%2C%20and%20then%20compare%20performance%20against%20standard%0ATransformers%20across%20various%20ICL%20tasks.%20Contrary%20to%20intuition%20and%20some%20recent%0Aworks%2C%20we%20find%20little%20discernible%20difference%20between%20the%20two%3B%20biasing%20towards%0Atask-relevant%20latent%20variables%20does%20not%20lead%20to%20better%20out-of-distribution%0Aperformance%2C%20in%20general.%20Curiously%2C%20we%20find%20that%20while%20the%20bottleneck%0Aeffectively%20learns%20to%20extract%20latent%20task%20variables%20from%20context%2C%20downstream%0Aprocessing%20struggles%20to%20utilize%20them%20for%20robust%20prediction.%20Our%20study%0Ahighlights%20the%20intrinsic%20limitations%20of%20Transformers%20in%20achieving%20structured%0AICL%20solutions%20that%20generalize%2C%20and%20shows%20that%20while%20inferring%20the%20right%20latents%0Aaids%20interpretability%2C%20it%20is%20not%20sufficient%20to%20alleviate%20this%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19162v1&entry.124074799=Read"},
{"title": "Vocos: Closing the gap between time-domain and Fourier-based neural\n  vocoders for high-quality audio synthesis", "author": "Hubert Siuzdak", "abstract": "  Recent advancements in neural vocoding are predominantly driven by Generative\nAdversarial Networks (GANs) operating in the time-domain. While effective, this\napproach neglects the inductive bias offered by time-frequency representations,\nresulting in reduntant and computionally-intensive upsampling operations.\nFourier-based time-frequency representation is an appealing alternative,\naligning more accurately with human auditory perception, and benefitting from\nwell-established fast algorithms for its computation. Nevertheless, direct\nreconstruction of complex-valued spectrograms has been historically\nproblematic, primarily due to phase recovery issues. This study seeks to close\nthis gap by presenting Vocos, a new model that directly generates Fourier\nspectral coefficients. Vocos not only matches the state-of-the-art in audio\nquality, as demonstrated in our evaluations, but it also substantially improves\ncomputational efficiency, achieving an order of magnitude increase in speed\ncompared to prevailing time-domain neural vocoding approaches. The source code\nand model weights have been open-sourced at https://github.com/gemelo-ai/vocos.\n", "link": "http://arxiv.org/abs/2306.00814v3", "date": "2024-05-29", "relevancy": 2.052, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5408}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5067}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vocos%3A%20Closing%20the%20gap%20between%20time-domain%20and%20Fourier-based%20neural%0A%20%20vocoders%20for%20high-quality%20audio%20synthesis&body=Title%3A%20Vocos%3A%20Closing%20the%20gap%20between%20time-domain%20and%20Fourier-based%20neural%0A%20%20vocoders%20for%20high-quality%20audio%20synthesis%0AAuthor%3A%20Hubert%20Siuzdak%0AAbstract%3A%20%20%20Recent%20advancements%20in%20neural%20vocoding%20are%20predominantly%20driven%20by%20Generative%0AAdversarial%20Networks%20%28GANs%29%20operating%20in%20the%20time-domain.%20While%20effective%2C%20this%0Aapproach%20neglects%20the%20inductive%20bias%20offered%20by%20time-frequency%20representations%2C%0Aresulting%20in%20reduntant%20and%20computionally-intensive%20upsampling%20operations.%0AFourier-based%20time-frequency%20representation%20is%20an%20appealing%20alternative%2C%0Aaligning%20more%20accurately%20with%20human%20auditory%20perception%2C%20and%20benefitting%20from%0Awell-established%20fast%20algorithms%20for%20its%20computation.%20Nevertheless%2C%20direct%0Areconstruction%20of%20complex-valued%20spectrograms%20has%20been%20historically%0Aproblematic%2C%20primarily%20due%20to%20phase%20recovery%20issues.%20This%20study%20seeks%20to%20close%0Athis%20gap%20by%20presenting%20Vocos%2C%20a%20new%20model%20that%20directly%20generates%20Fourier%0Aspectral%20coefficients.%20Vocos%20not%20only%20matches%20the%20state-of-the-art%20in%20audio%0Aquality%2C%20as%20demonstrated%20in%20our%20evaluations%2C%20but%20it%20also%20substantially%20improves%0Acomputational%20efficiency%2C%20achieving%20an%20order%20of%20magnitude%20increase%20in%20speed%0Acompared%20to%20prevailing%20time-domain%20neural%20vocoding%20approaches.%20The%20source%20code%0Aand%20model%20weights%20have%20been%20open-sourced%20at%20https%3A//github.com/gemelo-ai/vocos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00814v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVocos%253A%2520Closing%2520the%2520gap%2520between%2520time-domain%2520and%2520Fourier-based%2520neural%250A%2520%2520vocoders%2520for%2520high-quality%2520audio%2520synthesis%26entry.906535625%3DHubert%2520Siuzdak%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520neural%2520vocoding%2520are%2520predominantly%2520driven%2520by%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%2520operating%2520in%2520the%2520time-domain.%2520While%2520effective%252C%2520this%250Aapproach%2520neglects%2520the%2520inductive%2520bias%2520offered%2520by%2520time-frequency%2520representations%252C%250Aresulting%2520in%2520reduntant%2520and%2520computionally-intensive%2520upsampling%2520operations.%250AFourier-based%2520time-frequency%2520representation%2520is%2520an%2520appealing%2520alternative%252C%250Aaligning%2520more%2520accurately%2520with%2520human%2520auditory%2520perception%252C%2520and%2520benefitting%2520from%250Awell-established%2520fast%2520algorithms%2520for%2520its%2520computation.%2520Nevertheless%252C%2520direct%250Areconstruction%2520of%2520complex-valued%2520spectrograms%2520has%2520been%2520historically%250Aproblematic%252C%2520primarily%2520due%2520to%2520phase%2520recovery%2520issues.%2520This%2520study%2520seeks%2520to%2520close%250Athis%2520gap%2520by%2520presenting%2520Vocos%252C%2520a%2520new%2520model%2520that%2520directly%2520generates%2520Fourier%250Aspectral%2520coefficients.%2520Vocos%2520not%2520only%2520matches%2520the%2520state-of-the-art%2520in%2520audio%250Aquality%252C%2520as%2520demonstrated%2520in%2520our%2520evaluations%252C%2520but%2520it%2520also%2520substantially%2520improves%250Acomputational%2520efficiency%252C%2520achieving%2520an%2520order%2520of%2520magnitude%2520increase%2520in%2520speed%250Acompared%2520to%2520prevailing%2520time-domain%2520neural%2520vocoding%2520approaches.%2520The%2520source%2520code%250Aand%2520model%2520weights%2520have%2520been%2520open-sourced%2520at%2520https%253A//github.com/gemelo-ai/vocos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00814v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vocos%3A%20Closing%20the%20gap%20between%20time-domain%20and%20Fourier-based%20neural%0A%20%20vocoders%20for%20high-quality%20audio%20synthesis&entry.906535625=Hubert%20Siuzdak&entry.1292438233=%20%20Recent%20advancements%20in%20neural%20vocoding%20are%20predominantly%20driven%20by%20Generative%0AAdversarial%20Networks%20%28GANs%29%20operating%20in%20the%20time-domain.%20While%20effective%2C%20this%0Aapproach%20neglects%20the%20inductive%20bias%20offered%20by%20time-frequency%20representations%2C%0Aresulting%20in%20reduntant%20and%20computionally-intensive%20upsampling%20operations.%0AFourier-based%20time-frequency%20representation%20is%20an%20appealing%20alternative%2C%0Aaligning%20more%20accurately%20with%20human%20auditory%20perception%2C%20and%20benefitting%20from%0Awell-established%20fast%20algorithms%20for%20its%20computation.%20Nevertheless%2C%20direct%0Areconstruction%20of%20complex-valued%20spectrograms%20has%20been%20historically%0Aproblematic%2C%20primarily%20due%20to%20phase%20recovery%20issues.%20This%20study%20seeks%20to%20close%0Athis%20gap%20by%20presenting%20Vocos%2C%20a%20new%20model%20that%20directly%20generates%20Fourier%0Aspectral%20coefficients.%20Vocos%20not%20only%20matches%20the%20state-of-the-art%20in%20audio%0Aquality%2C%20as%20demonstrated%20in%20our%20evaluations%2C%20but%20it%20also%20substantially%20improves%0Acomputational%20efficiency%2C%20achieving%20an%20order%20of%20magnitude%20increase%20in%20speed%0Acompared%20to%20prevailing%20time-domain%20neural%20vocoding%20approaches.%20The%20source%20code%0Aand%20model%20weights%20have%20been%20open-sourced%20at%20https%3A//github.com/gemelo-ai/vocos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00814v3&entry.124074799=Read"},
{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "author": "Parker Ewen and Hao Chen and Yuzhen Chen and Anran Li and Anup Bagali and Gitesh Gunjal and Ram Vasudevan", "abstract": "  Robots must be able to understand their surroundings to perform complex tasks\nin challenging environments and many of these complex tasks require estimates\nof physical properties such as friction or weight. Estimating such properties\nusing learning is challenging due to the large amounts of labelled data\nrequired for training and the difficulty of updating these learned models\nonline at run time. To overcome these challenges, this paper introduces a\nnovel, multi-modal approach for representing semantic predictions and physical\nproperty estimates jointly in a probabilistic manner. By using conjugate pairs,\nthe proposed method enables closed-form Bayesian updates given visual and\ntactile measurements without requiring additional training data. The efficacy\nof the proposed algorithm is demonstrated through several hardware experiments.\nIn particular, this paper illustrates that by conditioning semantic\nclassifications on physical properties, the proposed method quantitatively\noutperforms state-of-the-art semantic classification methods that rely on\nvision alone. To further illustrate its utility, the proposed method is used in\nseveral applications including to represent affordance-based properties\nprobabilistically and a challenging terrain traversal task using a legged\nrobot. In the latter task, the proposed method represents the coefficient of\nfriction of the terrain probabilistically, which enables the use of an on-line\nrisk-aware planner that switches the legged robot from a dynamic gait to a\nstatic, stable gait when the expected value of the coefficient of friction\nfalls below a given threshold. Videos of these case studies as well as the\nopen-source C++ and ROS interface can be found at\nhttps://roahmlab.github.io/multimodal_mapping/.\n", "link": "http://arxiv.org/abs/2402.05872v4", "date": "2024-05-29", "relevancy": 2.0498, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 1.0}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6032}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction&body=Title%3A%20You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction%0AAuthor%3A%20Parker%20Ewen%20and%20Hao%20Chen%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Anup%20Bagali%20and%20Gitesh%20Gunjal%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Robots%20must%20be%20able%20to%20understand%20their%20surroundings%20to%20perform%20complex%20tasks%0Ain%20challenging%20environments%20and%20many%20of%20these%20complex%20tasks%20require%20estimates%0Aof%20physical%20properties%20such%20as%20friction%20or%20weight.%20Estimating%20such%20properties%0Ausing%20learning%20is%20challenging%20due%20to%20the%20large%20amounts%20of%20labelled%20data%0Arequired%20for%20training%20and%20the%20difficulty%20of%20updating%20these%20learned%20models%0Aonline%20at%20run%20time.%20To%20overcome%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%2C%20multi-modal%20approach%20for%20representing%20semantic%20predictions%20and%20physical%0Aproperty%20estimates%20jointly%20in%20a%20probabilistic%20manner.%20By%20using%20conjugate%20pairs%2C%0Athe%20proposed%20method%20enables%20closed-form%20Bayesian%20updates%20given%20visual%20and%0Atactile%20measurements%20without%20requiring%20additional%20training%20data.%20The%20efficacy%0Aof%20the%20proposed%20algorithm%20is%20demonstrated%20through%20several%20hardware%20experiments.%0AIn%20particular%2C%20this%20paper%20illustrates%20that%20by%20conditioning%20semantic%0Aclassifications%20on%20physical%20properties%2C%20the%20proposed%20method%20quantitatively%0Aoutperforms%20state-of-the-art%20semantic%20classification%20methods%20that%20rely%20on%0Avision%20alone.%20To%20further%20illustrate%20its%20utility%2C%20the%20proposed%20method%20is%20used%20in%0Aseveral%20applications%20including%20to%20represent%20affordance-based%20properties%0Aprobabilistically%20and%20a%20challenging%20terrain%20traversal%20task%20using%20a%20legged%0Arobot.%20In%20the%20latter%20task%2C%20the%20proposed%20method%20represents%20the%20coefficient%20of%0Afriction%20of%20the%20terrain%20probabilistically%2C%20which%20enables%20the%20use%20of%20an%20on-line%0Arisk-aware%20planner%20that%20switches%20the%20legged%20robot%20from%20a%20dynamic%20gait%20to%20a%0Astatic%2C%20stable%20gait%20when%20the%20expected%20value%20of%20the%20coefficient%20of%20friction%0Afalls%20below%20a%20given%20threshold.%20Videos%20of%20these%20case%20studies%20as%20well%20as%20the%0Aopen-source%20C%2B%2B%20and%20ROS%20interface%20can%20be%20found%20at%0Ahttps%3A//roahmlab.github.io/multimodal_mapping/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05872v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2527ve%2520Got%2520to%2520Feel%2520It%2520To%2520Believe%2520It%253A%2520Multi-Modal%2520Bayesian%2520Inference%2520for%250A%2520%2520Semantic%2520and%2520Property%2520Prediction%26entry.906535625%3DParker%2520Ewen%2520and%2520Hao%2520Chen%2520and%2520Yuzhen%2520Chen%2520and%2520Anran%2520Li%2520and%2520Anup%2520Bagali%2520and%2520Gitesh%2520Gunjal%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Robots%2520must%2520be%2520able%2520to%2520understand%2520their%2520surroundings%2520to%2520perform%2520complex%2520tasks%250Ain%2520challenging%2520environments%2520and%2520many%2520of%2520these%2520complex%2520tasks%2520require%2520estimates%250Aof%2520physical%2520properties%2520such%2520as%2520friction%2520or%2520weight.%2520Estimating%2520such%2520properties%250Ausing%2520learning%2520is%2520challenging%2520due%2520to%2520the%2520large%2520amounts%2520of%2520labelled%2520data%250Arequired%2520for%2520training%2520and%2520the%2520difficulty%2520of%2520updating%2520these%2520learned%2520models%250Aonline%2520at%2520run%2520time.%2520To%2520overcome%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520a%250Anovel%252C%2520multi-modal%2520approach%2520for%2520representing%2520semantic%2520predictions%2520and%2520physical%250Aproperty%2520estimates%2520jointly%2520in%2520a%2520probabilistic%2520manner.%2520By%2520using%2520conjugate%2520pairs%252C%250Athe%2520proposed%2520method%2520enables%2520closed-form%2520Bayesian%2520updates%2520given%2520visual%2520and%250Atactile%2520measurements%2520without%2520requiring%2520additional%2520training%2520data.%2520The%2520efficacy%250Aof%2520the%2520proposed%2520algorithm%2520is%2520demonstrated%2520through%2520several%2520hardware%2520experiments.%250AIn%2520particular%252C%2520this%2520paper%2520illustrates%2520that%2520by%2520conditioning%2520semantic%250Aclassifications%2520on%2520physical%2520properties%252C%2520the%2520proposed%2520method%2520quantitatively%250Aoutperforms%2520state-of-the-art%2520semantic%2520classification%2520methods%2520that%2520rely%2520on%250Avision%2520alone.%2520To%2520further%2520illustrate%2520its%2520utility%252C%2520the%2520proposed%2520method%2520is%2520used%2520in%250Aseveral%2520applications%2520including%2520to%2520represent%2520affordance-based%2520properties%250Aprobabilistically%2520and%2520a%2520challenging%2520terrain%2520traversal%2520task%2520using%2520a%2520legged%250Arobot.%2520In%2520the%2520latter%2520task%252C%2520the%2520proposed%2520method%2520represents%2520the%2520coefficient%2520of%250Afriction%2520of%2520the%2520terrain%2520probabilistically%252C%2520which%2520enables%2520the%2520use%2520of%2520an%2520on-line%250Arisk-aware%2520planner%2520that%2520switches%2520the%2520legged%2520robot%2520from%2520a%2520dynamic%2520gait%2520to%2520a%250Astatic%252C%2520stable%2520gait%2520when%2520the%2520expected%2520value%2520of%2520the%2520coefficient%2520of%2520friction%250Afalls%2520below%2520a%2520given%2520threshold.%2520Videos%2520of%2520these%2520case%2520studies%2520as%2520well%2520as%2520the%250Aopen-source%2520C%252B%252B%2520and%2520ROS%2520interface%2520can%2520be%2520found%2520at%250Ahttps%253A//roahmlab.github.io/multimodal_mapping/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05872v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction&entry.906535625=Parker%20Ewen%20and%20Hao%20Chen%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Anup%20Bagali%20and%20Gitesh%20Gunjal%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Robots%20must%20be%20able%20to%20understand%20their%20surroundings%20to%20perform%20complex%20tasks%0Ain%20challenging%20environments%20and%20many%20of%20these%20complex%20tasks%20require%20estimates%0Aof%20physical%20properties%20such%20as%20friction%20or%20weight.%20Estimating%20such%20properties%0Ausing%20learning%20is%20challenging%20due%20to%20the%20large%20amounts%20of%20labelled%20data%0Arequired%20for%20training%20and%20the%20difficulty%20of%20updating%20these%20learned%20models%0Aonline%20at%20run%20time.%20To%20overcome%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%2C%20multi-modal%20approach%20for%20representing%20semantic%20predictions%20and%20physical%0Aproperty%20estimates%20jointly%20in%20a%20probabilistic%20manner.%20By%20using%20conjugate%20pairs%2C%0Athe%20proposed%20method%20enables%20closed-form%20Bayesian%20updates%20given%20visual%20and%0Atactile%20measurements%20without%20requiring%20additional%20training%20data.%20The%20efficacy%0Aof%20the%20proposed%20algorithm%20is%20demonstrated%20through%20several%20hardware%20experiments.%0AIn%20particular%2C%20this%20paper%20illustrates%20that%20by%20conditioning%20semantic%0Aclassifications%20on%20physical%20properties%2C%20the%20proposed%20method%20quantitatively%0Aoutperforms%20state-of-the-art%20semantic%20classification%20methods%20that%20rely%20on%0Avision%20alone.%20To%20further%20illustrate%20its%20utility%2C%20the%20proposed%20method%20is%20used%20in%0Aseveral%20applications%20including%20to%20represent%20affordance-based%20properties%0Aprobabilistically%20and%20a%20challenging%20terrain%20traversal%20task%20using%20a%20legged%0Arobot.%20In%20the%20latter%20task%2C%20the%20proposed%20method%20represents%20the%20coefficient%20of%0Afriction%20of%20the%20terrain%20probabilistically%2C%20which%20enables%20the%20use%20of%20an%20on-line%0Arisk-aware%20planner%20that%20switches%20the%20legged%20robot%20from%20a%20dynamic%20gait%20to%20a%0Astatic%2C%20stable%20gait%20when%20the%20expected%20value%20of%20the%20coefficient%20of%20friction%0Afalls%20below%20a%20given%20threshold.%20Videos%20of%20these%20case%20studies%20as%20well%20as%20the%0Aopen-source%20C%2B%2B%20and%20ROS%20interface%20can%20be%20found%20at%0Ahttps%3A//roahmlab.github.io/multimodal_mapping/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05872v4&entry.124074799=Read"},
{"title": "Assessing the Efficacy of Deep Learning Approaches for Facial Expression\n  Recognition in Individuals with Intellectual Disabilities", "author": "F. Xavier Gaya-Morey and Silvia Ramis and Jose M. Buades-Rubio and Cristina Manresa-Yee", "abstract": "  Facial expression recognition has gained significance as a means of imparting\nsocial robots with the capacity to discern the emotional states of users. The\nuse of social robotics includes a variety of settings, including homes, nursing\nhomes or daycare centers, serving to a wide range of users. Remarkable\nperformance has been achieved by deep learning approaches, however, its direct\nuse for recognizing facial expressions in individuals with intellectual\ndisabilities has not been yet studied in the literature, to the best of our\nknowledge. To address this objective, we train a set of 12 convolutional neural\nnetworks in different approaches, including an ensemble of datasets without\nindividuals with intellectual disabilities and a dataset featuring such\nindividuals. Our examination of the outcomes, both the performance and the\nimportant image regions for the models, reveals significant distinctions in\nfacial expressions between individuals with and without intellectual\ndisabilities, as well as among individuals with intellectual disabilities.\nRemarkably, our findings show the need of facial expression recognition within\nthis population through tailored user-specific training methodologies, which\nenable the models to effectively address the unique expressions of each user.\n", "link": "http://arxiv.org/abs/2401.11877v2", "date": "2024-05-29", "relevancy": 2.0451, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5213}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5053}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Efficacy%20of%20Deep%20Learning%20Approaches%20for%20Facial%20Expression%0A%20%20Recognition%20in%20Individuals%20with%20Intellectual%20Disabilities&body=Title%3A%20Assessing%20the%20Efficacy%20of%20Deep%20Learning%20Approaches%20for%20Facial%20Expression%0A%20%20Recognition%20in%20Individuals%20with%20Intellectual%20Disabilities%0AAuthor%3A%20F.%20Xavier%20Gaya-Morey%20and%20Silvia%20Ramis%20and%20Jose%20M.%20Buades-Rubio%20and%20Cristina%20Manresa-Yee%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20has%20gained%20significance%20as%20a%20means%20of%20imparting%0Asocial%20robots%20with%20the%20capacity%20to%20discern%20the%20emotional%20states%20of%20users.%20The%0Ause%20of%20social%20robotics%20includes%20a%20variety%20of%20settings%2C%20including%20homes%2C%20nursing%0Ahomes%20or%20daycare%20centers%2C%20serving%20to%20a%20wide%20range%20of%20users.%20Remarkable%0Aperformance%20has%20been%20achieved%20by%20deep%20learning%20approaches%2C%20however%2C%20its%20direct%0Ause%20for%20recognizing%20facial%20expressions%20in%20individuals%20with%20intellectual%0Adisabilities%20has%20not%20been%20yet%20studied%20in%20the%20literature%2C%20to%20the%20best%20of%20our%0Aknowledge.%20To%20address%20this%20objective%2C%20we%20train%20a%20set%20of%2012%20convolutional%20neural%0Anetworks%20in%20different%20approaches%2C%20including%20an%20ensemble%20of%20datasets%20without%0Aindividuals%20with%20intellectual%20disabilities%20and%20a%20dataset%20featuring%20such%0Aindividuals.%20Our%20examination%20of%20the%20outcomes%2C%20both%20the%20performance%20and%20the%0Aimportant%20image%20regions%20for%20the%20models%2C%20reveals%20significant%20distinctions%20in%0Afacial%20expressions%20between%20individuals%20with%20and%20without%20intellectual%0Adisabilities%2C%20as%20well%20as%20among%20individuals%20with%20intellectual%20disabilities.%0ARemarkably%2C%20our%20findings%20show%20the%20need%20of%20facial%20expression%20recognition%20within%0Athis%20population%20through%20tailored%20user-specific%20training%20methodologies%2C%20which%0Aenable%20the%20models%20to%20effectively%20address%20the%20unique%20expressions%20of%20each%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Efficacy%2520of%2520Deep%2520Learning%2520Approaches%2520for%2520Facial%2520Expression%250A%2520%2520Recognition%2520in%2520Individuals%2520with%2520Intellectual%2520Disabilities%26entry.906535625%3DF.%2520Xavier%2520Gaya-Morey%2520and%2520Silvia%2520Ramis%2520and%2520Jose%2520M.%2520Buades-Rubio%2520and%2520Cristina%2520Manresa-Yee%26entry.1292438233%3D%2520%2520Facial%2520expression%2520recognition%2520has%2520gained%2520significance%2520as%2520a%2520means%2520of%2520imparting%250Asocial%2520robots%2520with%2520the%2520capacity%2520to%2520discern%2520the%2520emotional%2520states%2520of%2520users.%2520The%250Ause%2520of%2520social%2520robotics%2520includes%2520a%2520variety%2520of%2520settings%252C%2520including%2520homes%252C%2520nursing%250Ahomes%2520or%2520daycare%2520centers%252C%2520serving%2520to%2520a%2520wide%2520range%2520of%2520users.%2520Remarkable%250Aperformance%2520has%2520been%2520achieved%2520by%2520deep%2520learning%2520approaches%252C%2520however%252C%2520its%2520direct%250Ause%2520for%2520recognizing%2520facial%2520expressions%2520in%2520individuals%2520with%2520intellectual%250Adisabilities%2520has%2520not%2520been%2520yet%2520studied%2520in%2520the%2520literature%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge.%2520To%2520address%2520this%2520objective%252C%2520we%2520train%2520a%2520set%2520of%252012%2520convolutional%2520neural%250Anetworks%2520in%2520different%2520approaches%252C%2520including%2520an%2520ensemble%2520of%2520datasets%2520without%250Aindividuals%2520with%2520intellectual%2520disabilities%2520and%2520a%2520dataset%2520featuring%2520such%250Aindividuals.%2520Our%2520examination%2520of%2520the%2520outcomes%252C%2520both%2520the%2520performance%2520and%2520the%250Aimportant%2520image%2520regions%2520for%2520the%2520models%252C%2520reveals%2520significant%2520distinctions%2520in%250Afacial%2520expressions%2520between%2520individuals%2520with%2520and%2520without%2520intellectual%250Adisabilities%252C%2520as%2520well%2520as%2520among%2520individuals%2520with%2520intellectual%2520disabilities.%250ARemarkably%252C%2520our%2520findings%2520show%2520the%2520need%2520of%2520facial%2520expression%2520recognition%2520within%250Athis%2520population%2520through%2520tailored%2520user-specific%2520training%2520methodologies%252C%2520which%250Aenable%2520the%2520models%2520to%2520effectively%2520address%2520the%2520unique%2520expressions%2520of%2520each%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Efficacy%20of%20Deep%20Learning%20Approaches%20for%20Facial%20Expression%0A%20%20Recognition%20in%20Individuals%20with%20Intellectual%20Disabilities&entry.906535625=F.%20Xavier%20Gaya-Morey%20and%20Silvia%20Ramis%20and%20Jose%20M.%20Buades-Rubio%20and%20Cristina%20Manresa-Yee&entry.1292438233=%20%20Facial%20expression%20recognition%20has%20gained%20significance%20as%20a%20means%20of%20imparting%0Asocial%20robots%20with%20the%20capacity%20to%20discern%20the%20emotional%20states%20of%20users.%20The%0Ause%20of%20social%20robotics%20includes%20a%20variety%20of%20settings%2C%20including%20homes%2C%20nursing%0Ahomes%20or%20daycare%20centers%2C%20serving%20to%20a%20wide%20range%20of%20users.%20Remarkable%0Aperformance%20has%20been%20achieved%20by%20deep%20learning%20approaches%2C%20however%2C%20its%20direct%0Ause%20for%20recognizing%20facial%20expressions%20in%20individuals%20with%20intellectual%0Adisabilities%20has%20not%20been%20yet%20studied%20in%20the%20literature%2C%20to%20the%20best%20of%20our%0Aknowledge.%20To%20address%20this%20objective%2C%20we%20train%20a%20set%20of%2012%20convolutional%20neural%0Anetworks%20in%20different%20approaches%2C%20including%20an%20ensemble%20of%20datasets%20without%0Aindividuals%20with%20intellectual%20disabilities%20and%20a%20dataset%20featuring%20such%0Aindividuals.%20Our%20examination%20of%20the%20outcomes%2C%20both%20the%20performance%20and%20the%0Aimportant%20image%20regions%20for%20the%20models%2C%20reveals%20significant%20distinctions%20in%0Afacial%20expressions%20between%20individuals%20with%20and%20without%20intellectual%0Adisabilities%2C%20as%20well%20as%20among%20individuals%20with%20intellectual%20disabilities.%0ARemarkably%2C%20our%20findings%20show%20the%20need%20of%20facial%20expression%20recognition%20within%0Athis%20population%20through%20tailored%20user-specific%20training%20methodologies%2C%20which%0Aenable%20the%20models%20to%20effectively%20address%20the%20unique%20expressions%20of%20each%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11877v2&entry.124074799=Read"},
{"title": "LLMs as Bridges: Reformulating Grounded Multimodal Named Entity\n  Recognition", "author": "Jinyuan Li and Han Li and Di Sun and Jiahao Wang and Wenkun Zhang and Zan Wang and Gang Pan", "abstract": "  Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nmodule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.\n", "link": "http://arxiv.org/abs/2402.09989v4", "date": "2024-05-29", "relevancy": 2.0382, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Bridges%3A%20Reformulating%20Grounded%20Multimodal%20Named%20Entity%0A%20%20Recognition&body=Title%3A%20LLMs%20as%20Bridges%3A%20Reformulating%20Grounded%20Multimodal%20Named%20Entity%0A%20%20Recognition%0AAuthor%3A%20Jinyuan%20Li%20and%20Han%20Li%20and%20Di%20Sun%20and%20Jiahao%20Wang%20and%20Wenkun%20Zhang%20and%20Zan%20Wang%20and%20Gang%20Pan%0AAbstract%3A%20%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20is%20a%20nascent%20multimodal%0Atask%20that%20aims%20to%20identify%20named%20entities%2C%20entity%20types%20and%20their%20corresponding%0Avisual%20regions.%20GMNER%20task%20exhibits%20two%20challenging%20properties%3A%201%29%20The%20weak%0Acorrelation%20between%20image-text%20pairs%20in%20social%20media%20results%20in%20a%20significant%0Aportion%20of%20named%20entities%20being%20ungroundable.%202%29%20There%20exists%20a%20distinction%0Abetween%20coarse-grained%20referring%20expressions%20commonly%20used%20in%20similar%20tasks%0A%28e.g.%2C%20phrase%20localization%2C%20referring%20expression%20comprehension%29%20and%0Afine-grained%20named%20entities.%20In%20this%20paper%2C%20we%20propose%20RiVEG%2C%20a%20unified%0Aframework%20that%20reformulates%20GMNER%20into%20a%20joint%20MNER-VE-VG%20task%20by%20leveraging%0Alarge%20language%20models%20%28LLMs%29%20as%20a%20connecting%20bridge.%20This%20reformulation%20brings%0Atwo%20benefits%3A%201%29%20It%20maintains%20the%20optimal%20MNER%20performance%20and%20eliminates%20the%0Aneed%20for%20employing%20object%20detection%20methods%20to%20pre-extract%20regional%20features%2C%0Athereby%20naturally%20addressing%20two%20major%20limitations%20of%20existing%20GMNER%20methods.%0A2%29%20The%20introduction%20of%20entity%20expansion%20expression%20and%20Visual%20Entailment%20%28VE%29%0Amodule%20unifies%20Visual%20Grounding%20%28VG%29%20and%20Entity%20Grounding%20%28EG%29.%20It%20enables%0ARiVEG%20to%20effortlessly%20inherit%20the%20Visual%20Entailment%20and%20Visual%20Grounding%0Acapabilities%20of%20any%20current%20or%20prospective%20multimodal%20pretraining%20models.%0AExtensive%20experiments%20demonstrate%20that%20RiVEG%20outperforms%20state-of-the-art%0Amethods%20on%20the%20existing%20GMNER%20dataset%20and%20achieves%20absolute%20leads%20of%2010.65%25%2C%0A6.21%25%2C%20and%208.83%25%20in%20all%20three%20subtasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09989v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Bridges%253A%2520Reformulating%2520Grounded%2520Multimodal%2520Named%2520Entity%250A%2520%2520Recognition%26entry.906535625%3DJinyuan%2520Li%2520and%2520Han%2520Li%2520and%2520Di%2520Sun%2520and%2520Jiahao%2520Wang%2520and%2520Wenkun%2520Zhang%2520and%2520Zan%2520Wang%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520Grounded%2520Multimodal%2520Named%2520Entity%2520Recognition%2520%2528GMNER%2529%2520is%2520a%2520nascent%2520multimodal%250Atask%2520that%2520aims%2520to%2520identify%2520named%2520entities%252C%2520entity%2520types%2520and%2520their%2520corresponding%250Avisual%2520regions.%2520GMNER%2520task%2520exhibits%2520two%2520challenging%2520properties%253A%25201%2529%2520The%2520weak%250Acorrelation%2520between%2520image-text%2520pairs%2520in%2520social%2520media%2520results%2520in%2520a%2520significant%250Aportion%2520of%2520named%2520entities%2520being%2520ungroundable.%25202%2529%2520There%2520exists%2520a%2520distinction%250Abetween%2520coarse-grained%2520referring%2520expressions%2520commonly%2520used%2520in%2520similar%2520tasks%250A%2528e.g.%252C%2520phrase%2520localization%252C%2520referring%2520expression%2520comprehension%2529%2520and%250Afine-grained%2520named%2520entities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RiVEG%252C%2520a%2520unified%250Aframework%2520that%2520reformulates%2520GMNER%2520into%2520a%2520joint%2520MNER-VE-VG%2520task%2520by%2520leveraging%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520as%2520a%2520connecting%2520bridge.%2520This%2520reformulation%2520brings%250Atwo%2520benefits%253A%25201%2529%2520It%2520maintains%2520the%2520optimal%2520MNER%2520performance%2520and%2520eliminates%2520the%250Aneed%2520for%2520employing%2520object%2520detection%2520methods%2520to%2520pre-extract%2520regional%2520features%252C%250Athereby%2520naturally%2520addressing%2520two%2520major%2520limitations%2520of%2520existing%2520GMNER%2520methods.%250A2%2529%2520The%2520introduction%2520of%2520entity%2520expansion%2520expression%2520and%2520Visual%2520Entailment%2520%2528VE%2529%250Amodule%2520unifies%2520Visual%2520Grounding%2520%2528VG%2529%2520and%2520Entity%2520Grounding%2520%2528EG%2529.%2520It%2520enables%250ARiVEG%2520to%2520effortlessly%2520inherit%2520the%2520Visual%2520Entailment%2520and%2520Visual%2520Grounding%250Acapabilities%2520of%2520any%2520current%2520or%2520prospective%2520multimodal%2520pretraining%2520models.%250AExtensive%2520experiments%2520demonstrate%2520that%2520RiVEG%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520the%2520existing%2520GMNER%2520dataset%2520and%2520achieves%2520absolute%2520leads%2520of%252010.65%2525%252C%250A6.21%2525%252C%2520and%25208.83%2525%2520in%2520all%2520three%2520subtasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09989v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Bridges%3A%20Reformulating%20Grounded%20Multimodal%20Named%20Entity%0A%20%20Recognition&entry.906535625=Jinyuan%20Li%20and%20Han%20Li%20and%20Di%20Sun%20and%20Jiahao%20Wang%20and%20Wenkun%20Zhang%20and%20Zan%20Wang%20and%20Gang%20Pan&entry.1292438233=%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20is%20a%20nascent%20multimodal%0Atask%20that%20aims%20to%20identify%20named%20entities%2C%20entity%20types%20and%20their%20corresponding%0Avisual%20regions.%20GMNER%20task%20exhibits%20two%20challenging%20properties%3A%201%29%20The%20weak%0Acorrelation%20between%20image-text%20pairs%20in%20social%20media%20results%20in%20a%20significant%0Aportion%20of%20named%20entities%20being%20ungroundable.%202%29%20There%20exists%20a%20distinction%0Abetween%20coarse-grained%20referring%20expressions%20commonly%20used%20in%20similar%20tasks%0A%28e.g.%2C%20phrase%20localization%2C%20referring%20expression%20comprehension%29%20and%0Afine-grained%20named%20entities.%20In%20this%20paper%2C%20we%20propose%20RiVEG%2C%20a%20unified%0Aframework%20that%20reformulates%20GMNER%20into%20a%20joint%20MNER-VE-VG%20task%20by%20leveraging%0Alarge%20language%20models%20%28LLMs%29%20as%20a%20connecting%20bridge.%20This%20reformulation%20brings%0Atwo%20benefits%3A%201%29%20It%20maintains%20the%20optimal%20MNER%20performance%20and%20eliminates%20the%0Aneed%20for%20employing%20object%20detection%20methods%20to%20pre-extract%20regional%20features%2C%0Athereby%20naturally%20addressing%20two%20major%20limitations%20of%20existing%20GMNER%20methods.%0A2%29%20The%20introduction%20of%20entity%20expansion%20expression%20and%20Visual%20Entailment%20%28VE%29%0Amodule%20unifies%20Visual%20Grounding%20%28VG%29%20and%20Entity%20Grounding%20%28EG%29.%20It%20enables%0ARiVEG%20to%20effortlessly%20inherit%20the%20Visual%20Entailment%20and%20Visual%20Grounding%0Acapabilities%20of%20any%20current%20or%20prospective%20multimodal%20pretraining%20models.%0AExtensive%20experiments%20demonstrate%20that%20RiVEG%20outperforms%20state-of-the-art%0Amethods%20on%20the%20existing%20GMNER%20dataset%20and%20achieves%20absolute%20leads%20of%2010.65%25%2C%0A6.21%25%2C%20and%208.83%25%20in%20all%20three%20subtasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09989v4&entry.124074799=Read"},
{"title": "Learning Topological Representations with Bidirectional Graph Attention\n  Network for Solving Job Shop Scheduling Problem", "author": "Cong Zhang and Zhiguang Cao and Yaoxin Wu and Wen Song and Jing Sun", "abstract": "  Existing learning-based methods for solving job shop scheduling problems\n(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and\nneglect the rich and meaningful topological structures of disjunctive graphs\n(DGs). This paper proposes the topology-aware bidirectional graph attention\nnetwork (TBGAT), a novel GNN architecture based on the attention mechanism, to\nembed the DG for solving JSSP in a local search framework. Specifically, TBGAT\nembeds the DG from a forward and a backward view, respectively, where the\nmessages are propagated by following the different topologies of the views and\naggregated via graph attention. Then, we propose a novel operator based on the\nmessage-passing mechanism to calculate the forward and backward topological\nsorts of the DG, which are the features for characterizing the topological\nstructures and exploited by our model. In addition, we theoretically and\nexperimentally show that TBGAT has linear computational complexity to the\nnumber of jobs and machines, respectively, strengthening our method's practical\nvalue. Besides, extensive experiments on five synthetic datasets and seven\nclassic benchmarks show that TBGAT achieves new SOTA results by outperforming a\nwide range of neural methods by a large margin. All the code and data are\npublicly available online at https://github.com/zcaicaros/TBGAT.\n", "link": "http://arxiv.org/abs/2402.17606v2", "date": "2024-05-29", "relevancy": 2.0346, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4987}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Topological%20Representations%20with%20Bidirectional%20Graph%20Attention%0A%20%20Network%20for%20Solving%20Job%20Shop%20Scheduling%20Problem&body=Title%3A%20Learning%20Topological%20Representations%20with%20Bidirectional%20Graph%20Attention%0A%20%20Network%20for%20Solving%20Job%20Shop%20Scheduling%20Problem%0AAuthor%3A%20Cong%20Zhang%20and%20Zhiguang%20Cao%20and%20Yaoxin%20Wu%20and%20Wen%20Song%20and%20Jing%20Sun%0AAbstract%3A%20%20%20Existing%20learning-based%20methods%20for%20solving%20job%20shop%20scheduling%20problems%0A%28JSSP%29%20usually%20use%20off-the-shelf%20GNN%20models%20tailored%20to%20undirected%20graphs%20and%0Aneglect%20the%20rich%20and%20meaningful%20topological%20structures%20of%20disjunctive%20graphs%0A%28DGs%29.%20This%20paper%20proposes%20the%20topology-aware%20bidirectional%20graph%20attention%0Anetwork%20%28TBGAT%29%2C%20a%20novel%20GNN%20architecture%20based%20on%20the%20attention%20mechanism%2C%20to%0Aembed%20the%20DG%20for%20solving%20JSSP%20in%20a%20local%20search%20framework.%20Specifically%2C%20TBGAT%0Aembeds%20the%20DG%20from%20a%20forward%20and%20a%20backward%20view%2C%20respectively%2C%20where%20the%0Amessages%20are%20propagated%20by%20following%20the%20different%20topologies%20of%20the%20views%20and%0Aaggregated%20via%20graph%20attention.%20Then%2C%20we%20propose%20a%20novel%20operator%20based%20on%20the%0Amessage-passing%20mechanism%20to%20calculate%20the%20forward%20and%20backward%20topological%0Asorts%20of%20the%20DG%2C%20which%20are%20the%20features%20for%20characterizing%20the%20topological%0Astructures%20and%20exploited%20by%20our%20model.%20In%20addition%2C%20we%20theoretically%20and%0Aexperimentally%20show%20that%20TBGAT%20has%20linear%20computational%20complexity%20to%20the%0Anumber%20of%20jobs%20and%20machines%2C%20respectively%2C%20strengthening%20our%20method%27s%20practical%0Avalue.%20Besides%2C%20extensive%20experiments%20on%20five%20synthetic%20datasets%20and%20seven%0Aclassic%20benchmarks%20show%20that%20TBGAT%20achieves%20new%20SOTA%20results%20by%20outperforming%20a%0Awide%20range%20of%20neural%20methods%20by%20a%20large%20margin.%20All%20the%20code%20and%20data%20are%0Apublicly%20available%20online%20at%20https%3A//github.com/zcaicaros/TBGAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Topological%2520Representations%2520with%2520Bidirectional%2520Graph%2520Attention%250A%2520%2520Network%2520for%2520Solving%2520Job%2520Shop%2520Scheduling%2520Problem%26entry.906535625%3DCong%2520Zhang%2520and%2520Zhiguang%2520Cao%2520and%2520Yaoxin%2520Wu%2520and%2520Wen%2520Song%2520and%2520Jing%2520Sun%26entry.1292438233%3D%2520%2520Existing%2520learning-based%2520methods%2520for%2520solving%2520job%2520shop%2520scheduling%2520problems%250A%2528JSSP%2529%2520usually%2520use%2520off-the-shelf%2520GNN%2520models%2520tailored%2520to%2520undirected%2520graphs%2520and%250Aneglect%2520the%2520rich%2520and%2520meaningful%2520topological%2520structures%2520of%2520disjunctive%2520graphs%250A%2528DGs%2529.%2520This%2520paper%2520proposes%2520the%2520topology-aware%2520bidirectional%2520graph%2520attention%250Anetwork%2520%2528TBGAT%2529%252C%2520a%2520novel%2520GNN%2520architecture%2520based%2520on%2520the%2520attention%2520mechanism%252C%2520to%250Aembed%2520the%2520DG%2520for%2520solving%2520JSSP%2520in%2520a%2520local%2520search%2520framework.%2520Specifically%252C%2520TBGAT%250Aembeds%2520the%2520DG%2520from%2520a%2520forward%2520and%2520a%2520backward%2520view%252C%2520respectively%252C%2520where%2520the%250Amessages%2520are%2520propagated%2520by%2520following%2520the%2520different%2520topologies%2520of%2520the%2520views%2520and%250Aaggregated%2520via%2520graph%2520attention.%2520Then%252C%2520we%2520propose%2520a%2520novel%2520operator%2520based%2520on%2520the%250Amessage-passing%2520mechanism%2520to%2520calculate%2520the%2520forward%2520and%2520backward%2520topological%250Asorts%2520of%2520the%2520DG%252C%2520which%2520are%2520the%2520features%2520for%2520characterizing%2520the%2520topological%250Astructures%2520and%2520exploited%2520by%2520our%2520model.%2520In%2520addition%252C%2520we%2520theoretically%2520and%250Aexperimentally%2520show%2520that%2520TBGAT%2520has%2520linear%2520computational%2520complexity%2520to%2520the%250Anumber%2520of%2520jobs%2520and%2520machines%252C%2520respectively%252C%2520strengthening%2520our%2520method%2527s%2520practical%250Avalue.%2520Besides%252C%2520extensive%2520experiments%2520on%2520five%2520synthetic%2520datasets%2520and%2520seven%250Aclassic%2520benchmarks%2520show%2520that%2520TBGAT%2520achieves%2520new%2520SOTA%2520results%2520by%2520outperforming%2520a%250Awide%2520range%2520of%2520neural%2520methods%2520by%2520a%2520large%2520margin.%2520All%2520the%2520code%2520and%2520data%2520are%250Apublicly%2520available%2520online%2520at%2520https%253A//github.com/zcaicaros/TBGAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Topological%20Representations%20with%20Bidirectional%20Graph%20Attention%0A%20%20Network%20for%20Solving%20Job%20Shop%20Scheduling%20Problem&entry.906535625=Cong%20Zhang%20and%20Zhiguang%20Cao%20and%20Yaoxin%20Wu%20and%20Wen%20Song%20and%20Jing%20Sun&entry.1292438233=%20%20Existing%20learning-based%20methods%20for%20solving%20job%20shop%20scheduling%20problems%0A%28JSSP%29%20usually%20use%20off-the-shelf%20GNN%20models%20tailored%20to%20undirected%20graphs%20and%0Aneglect%20the%20rich%20and%20meaningful%20topological%20structures%20of%20disjunctive%20graphs%0A%28DGs%29.%20This%20paper%20proposes%20the%20topology-aware%20bidirectional%20graph%20attention%0Anetwork%20%28TBGAT%29%2C%20a%20novel%20GNN%20architecture%20based%20on%20the%20attention%20mechanism%2C%20to%0Aembed%20the%20DG%20for%20solving%20JSSP%20in%20a%20local%20search%20framework.%20Specifically%2C%20TBGAT%0Aembeds%20the%20DG%20from%20a%20forward%20and%20a%20backward%20view%2C%20respectively%2C%20where%20the%0Amessages%20are%20propagated%20by%20following%20the%20different%20topologies%20of%20the%20views%20and%0Aaggregated%20via%20graph%20attention.%20Then%2C%20we%20propose%20a%20novel%20operator%20based%20on%20the%0Amessage-passing%20mechanism%20to%20calculate%20the%20forward%20and%20backward%20topological%0Asorts%20of%20the%20DG%2C%20which%20are%20the%20features%20for%20characterizing%20the%20topological%0Astructures%20and%20exploited%20by%20our%20model.%20In%20addition%2C%20we%20theoretically%20and%0Aexperimentally%20show%20that%20TBGAT%20has%20linear%20computational%20complexity%20to%20the%0Anumber%20of%20jobs%20and%20machines%2C%20respectively%2C%20strengthening%20our%20method%27s%20practical%0Avalue.%20Besides%2C%20extensive%20experiments%20on%20five%20synthetic%20datasets%20and%20seven%0Aclassic%20benchmarks%20show%20that%20TBGAT%20achieves%20new%20SOTA%20results%20by%20outperforming%20a%0Awide%20range%20of%20neural%20methods%20by%20a%20large%20margin.%20All%20the%20code%20and%20data%20are%0Apublicly%20available%20online%20at%20https%3A//github.com/zcaicaros/TBGAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17606v2&entry.124074799=Read"},
{"title": "Implicit Neural Image Field for Biological Microscopy Image Compression", "author": "Gaole Dai and Cheng-Ching Tseng and Qingpo Wuwu and Rongyu Zhang and Shaokang Wang and Ming Lu and Tiejun Huang and Yu Zhou and Ali Ata Tuz and Matthias Gunzer and Jianxu Chen and Shanghang Zhang", "abstract": "  The rapid pace of innovation in biological microscopy imaging has led to\nlarge images, putting pressure on data storage and impeding efficient sharing,\nmanagement, and visualization. This necessitates the development of efficient\ncompression solutions. Traditional CODEC methods struggle to adapt to the\ndiverse bioimaging data and often suffer from sub-optimal compression. In this\nstudy, we propose an adaptive compression workflow based on Implicit Neural\nRepresentation (INR). This approach permits application-specific compression\nobjectives, capable of compressing images of any shape and arbitrary pixel-wise\ndecompression. We demonstrated on a wide range of microscopy images from real\napplications that our workflow not only achieved high, controllable compression\nratios (e.g., 512x) but also preserved detailed information critical for\ndownstream analysis.\n", "link": "http://arxiv.org/abs/2405.19012v1", "date": "2024-05-29", "relevancy": 2.032, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5292}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5147}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Neural%20Image%20Field%20for%20Biological%20Microscopy%20Image%20Compression&body=Title%3A%20Implicit%20Neural%20Image%20Field%20for%20Biological%20Microscopy%20Image%20Compression%0AAuthor%3A%20Gaole%20Dai%20and%20Cheng-Ching%20Tseng%20and%20Qingpo%20Wuwu%20and%20Rongyu%20Zhang%20and%20Shaokang%20Wang%20and%20Ming%20Lu%20and%20Tiejun%20Huang%20and%20Yu%20Zhou%20and%20Ali%20Ata%20Tuz%20and%20Matthias%20Gunzer%20and%20Jianxu%20Chen%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20The%20rapid%20pace%20of%20innovation%20in%20biological%20microscopy%20imaging%20has%20led%20to%0Alarge%20images%2C%20putting%20pressure%20on%20data%20storage%20and%20impeding%20efficient%20sharing%2C%0Amanagement%2C%20and%20visualization.%20This%20necessitates%20the%20development%20of%20efficient%0Acompression%20solutions.%20Traditional%20CODEC%20methods%20struggle%20to%20adapt%20to%20the%0Adiverse%20bioimaging%20data%20and%20often%20suffer%20from%20sub-optimal%20compression.%20In%20this%0Astudy%2C%20we%20propose%20an%20adaptive%20compression%20workflow%20based%20on%20Implicit%20Neural%0ARepresentation%20%28INR%29.%20This%20approach%20permits%20application-specific%20compression%0Aobjectives%2C%20capable%20of%20compressing%20images%20of%20any%20shape%20and%20arbitrary%20pixel-wise%0Adecompression.%20We%20demonstrated%20on%20a%20wide%20range%20of%20microscopy%20images%20from%20real%0Aapplications%20that%20our%20workflow%20not%20only%20achieved%20high%2C%20controllable%20compression%0Aratios%20%28e.g.%2C%20512x%29%20but%20also%20preserved%20detailed%20information%20critical%20for%0Adownstream%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Neural%2520Image%2520Field%2520for%2520Biological%2520Microscopy%2520Image%2520Compression%26entry.906535625%3DGaole%2520Dai%2520and%2520Cheng-Ching%2520Tseng%2520and%2520Qingpo%2520Wuwu%2520and%2520Rongyu%2520Zhang%2520and%2520Shaokang%2520Wang%2520and%2520Ming%2520Lu%2520and%2520Tiejun%2520Huang%2520and%2520Yu%2520Zhou%2520and%2520Ali%2520Ata%2520Tuz%2520and%2520Matthias%2520Gunzer%2520and%2520Jianxu%2520Chen%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rapid%2520pace%2520of%2520innovation%2520in%2520biological%2520microscopy%2520imaging%2520has%2520led%2520to%250Alarge%2520images%252C%2520putting%2520pressure%2520on%2520data%2520storage%2520and%2520impeding%2520efficient%2520sharing%252C%250Amanagement%252C%2520and%2520visualization.%2520This%2520necessitates%2520the%2520development%2520of%2520efficient%250Acompression%2520solutions.%2520Traditional%2520CODEC%2520methods%2520struggle%2520to%2520adapt%2520to%2520the%250Adiverse%2520bioimaging%2520data%2520and%2520often%2520suffer%2520from%2520sub-optimal%2520compression.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520an%2520adaptive%2520compression%2520workflow%2520based%2520on%2520Implicit%2520Neural%250ARepresentation%2520%2528INR%2529.%2520This%2520approach%2520permits%2520application-specific%2520compression%250Aobjectives%252C%2520capable%2520of%2520compressing%2520images%2520of%2520any%2520shape%2520and%2520arbitrary%2520pixel-wise%250Adecompression.%2520We%2520demonstrated%2520on%2520a%2520wide%2520range%2520of%2520microscopy%2520images%2520from%2520real%250Aapplications%2520that%2520our%2520workflow%2520not%2520only%2520achieved%2520high%252C%2520controllable%2520compression%250Aratios%2520%2528e.g.%252C%2520512x%2529%2520but%2520also%2520preserved%2520detailed%2520information%2520critical%2520for%250Adownstream%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Neural%20Image%20Field%20for%20Biological%20Microscopy%20Image%20Compression&entry.906535625=Gaole%20Dai%20and%20Cheng-Ching%20Tseng%20and%20Qingpo%20Wuwu%20and%20Rongyu%20Zhang%20and%20Shaokang%20Wang%20and%20Ming%20Lu%20and%20Tiejun%20Huang%20and%20Yu%20Zhou%20and%20Ali%20Ata%20Tuz%20and%20Matthias%20Gunzer%20and%20Jianxu%20Chen%20and%20Shanghang%20Zhang&entry.1292438233=%20%20The%20rapid%20pace%20of%20innovation%20in%20biological%20microscopy%20imaging%20has%20led%20to%0Alarge%20images%2C%20putting%20pressure%20on%20data%20storage%20and%20impeding%20efficient%20sharing%2C%0Amanagement%2C%20and%20visualization.%20This%20necessitates%20the%20development%20of%20efficient%0Acompression%20solutions.%20Traditional%20CODEC%20methods%20struggle%20to%20adapt%20to%20the%0Adiverse%20bioimaging%20data%20and%20often%20suffer%20from%20sub-optimal%20compression.%20In%20this%0Astudy%2C%20we%20propose%20an%20adaptive%20compression%20workflow%20based%20on%20Implicit%20Neural%0ARepresentation%20%28INR%29.%20This%20approach%20permits%20application-specific%20compression%0Aobjectives%2C%20capable%20of%20compressing%20images%20of%20any%20shape%20and%20arbitrary%20pixel-wise%0Adecompression.%20We%20demonstrated%20on%20a%20wide%20range%20of%20microscopy%20images%20from%20real%0Aapplications%20that%20our%20workflow%20not%20only%20achieved%20high%2C%20controllable%20compression%0Aratios%20%28e.g.%2C%20512x%29%20but%20also%20preserved%20detailed%20information%20critical%20for%0Adownstream%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19012v1&entry.124074799=Read"},
{"title": "Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning", "author": "Keltin Grimes and Collin Abidi and Cole Frank and Shannon Gallagher", "abstract": "  Machine learning models are vulnerable to adversarial attacks, including\nattacks that leak information about the model's training data. There has\nrecently been an increase in interest about how to best address privacy\nconcerns, especially in the presence of data-removal requests. Machine\nunlearning algorithms aim to efficiently update trained models to comply with\ndata deletion requests while maintaining performance and without having to\nresort to retraining the model from scratch, a costly endeavor. Several\nalgorithms in the machine unlearning literature demonstrate some level of\nprivacy gains, but they are often evaluated only on rudimentary membership\ninference attacks, which do not represent realistic threats. In this paper we\ndescribe and propose alternative evaluation methods for three key shortcomings\nin the current evaluation of unlearning algorithms. We show the utility of our\nalternative evaluations via a series of experiments of state-of-the-art\nunlearning algorithms on different computer vision datasets, presenting a more\ndetailed picture of the state of the field.\n", "link": "http://arxiv.org/abs/2405.19211v1", "date": "2024-05-29", "relevancy": 2.029, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5004}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gone%20but%20Not%20Forgotten%3A%20Improved%20Benchmarks%20for%20Machine%20Unlearning&body=Title%3A%20Gone%20but%20Not%20Forgotten%3A%20Improved%20Benchmarks%20for%20Machine%20Unlearning%0AAuthor%3A%20Keltin%20Grimes%20and%20Collin%20Abidi%20and%20Cole%20Frank%20and%20Shannon%20Gallagher%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20vulnerable%20to%20adversarial%20attacks%2C%20including%0Aattacks%20that%20leak%20information%20about%20the%20model%27s%20training%20data.%20There%20has%0Arecently%20been%20an%20increase%20in%20interest%20about%20how%20to%20best%20address%20privacy%0Aconcerns%2C%20especially%20in%20the%20presence%20of%20data-removal%20requests.%20Machine%0Aunlearning%20algorithms%20aim%20to%20efficiently%20update%20trained%20models%20to%20comply%20with%0Adata%20deletion%20requests%20while%20maintaining%20performance%20and%20without%20having%20to%0Aresort%20to%20retraining%20the%20model%20from%20scratch%2C%20a%20costly%20endeavor.%20Several%0Aalgorithms%20in%20the%20machine%20unlearning%20literature%20demonstrate%20some%20level%20of%0Aprivacy%20gains%2C%20but%20they%20are%20often%20evaluated%20only%20on%20rudimentary%20membership%0Ainference%20attacks%2C%20which%20do%20not%20represent%20realistic%20threats.%20In%20this%20paper%20we%0Adescribe%20and%20propose%20alternative%20evaluation%20methods%20for%20three%20key%20shortcomings%0Ain%20the%20current%20evaluation%20of%20unlearning%20algorithms.%20We%20show%20the%20utility%20of%20our%0Aalternative%20evaluations%20via%20a%20series%20of%20experiments%20of%20state-of-the-art%0Aunlearning%20algorithms%20on%20different%20computer%20vision%20datasets%2C%20presenting%20a%20more%0Adetailed%20picture%20of%20the%20state%20of%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGone%2520but%2520Not%2520Forgotten%253A%2520Improved%2520Benchmarks%2520for%2520Machine%2520Unlearning%26entry.906535625%3DKeltin%2520Grimes%2520and%2520Collin%2520Abidi%2520and%2520Cole%2520Frank%2520and%2520Shannon%2520Gallagher%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520including%250Aattacks%2520that%2520leak%2520information%2520about%2520the%2520model%2527s%2520training%2520data.%2520There%2520has%250Arecently%2520been%2520an%2520increase%2520in%2520interest%2520about%2520how%2520to%2520best%2520address%2520privacy%250Aconcerns%252C%2520especially%2520in%2520the%2520presence%2520of%2520data-removal%2520requests.%2520Machine%250Aunlearning%2520algorithms%2520aim%2520to%2520efficiently%2520update%2520trained%2520models%2520to%2520comply%2520with%250Adata%2520deletion%2520requests%2520while%2520maintaining%2520performance%2520and%2520without%2520having%2520to%250Aresort%2520to%2520retraining%2520the%2520model%2520from%2520scratch%252C%2520a%2520costly%2520endeavor.%2520Several%250Aalgorithms%2520in%2520the%2520machine%2520unlearning%2520literature%2520demonstrate%2520some%2520level%2520of%250Aprivacy%2520gains%252C%2520but%2520they%2520are%2520often%2520evaluated%2520only%2520on%2520rudimentary%2520membership%250Ainference%2520attacks%252C%2520which%2520do%2520not%2520represent%2520realistic%2520threats.%2520In%2520this%2520paper%2520we%250Adescribe%2520and%2520propose%2520alternative%2520evaluation%2520methods%2520for%2520three%2520key%2520shortcomings%250Ain%2520the%2520current%2520evaluation%2520of%2520unlearning%2520algorithms.%2520We%2520show%2520the%2520utility%2520of%2520our%250Aalternative%2520evaluations%2520via%2520a%2520series%2520of%2520experiments%2520of%2520state-of-the-art%250Aunlearning%2520algorithms%2520on%2520different%2520computer%2520vision%2520datasets%252C%2520presenting%2520a%2520more%250Adetailed%2520picture%2520of%2520the%2520state%2520of%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gone%20but%20Not%20Forgotten%3A%20Improved%20Benchmarks%20for%20Machine%20Unlearning&entry.906535625=Keltin%20Grimes%20and%20Collin%20Abidi%20and%20Cole%20Frank%20and%20Shannon%20Gallagher&entry.1292438233=%20%20Machine%20learning%20models%20are%20vulnerable%20to%20adversarial%20attacks%2C%20including%0Aattacks%20that%20leak%20information%20about%20the%20model%27s%20training%20data.%20There%20has%0Arecently%20been%20an%20increase%20in%20interest%20about%20how%20to%20best%20address%20privacy%0Aconcerns%2C%20especially%20in%20the%20presence%20of%20data-removal%20requests.%20Machine%0Aunlearning%20algorithms%20aim%20to%20efficiently%20update%20trained%20models%20to%20comply%20with%0Adata%20deletion%20requests%20while%20maintaining%20performance%20and%20without%20having%20to%0Aresort%20to%20retraining%20the%20model%20from%20scratch%2C%20a%20costly%20endeavor.%20Several%0Aalgorithms%20in%20the%20machine%20unlearning%20literature%20demonstrate%20some%20level%20of%0Aprivacy%20gains%2C%20but%20they%20are%20often%20evaluated%20only%20on%20rudimentary%20membership%0Ainference%20attacks%2C%20which%20do%20not%20represent%20realistic%20threats.%20In%20this%20paper%20we%0Adescribe%20and%20propose%20alternative%20evaluation%20methods%20for%20three%20key%20shortcomings%0Ain%20the%20current%20evaluation%20of%20unlearning%20algorithms.%20We%20show%20the%20utility%20of%20our%0Aalternative%20evaluations%20via%20a%20series%20of%20experiments%20of%20state-of-the-art%0Aunlearning%20algorithms%20on%20different%20computer%20vision%20datasets%2C%20presenting%20a%20more%0Adetailed%20picture%20of%20the%20state%20of%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19211v1&entry.124074799=Read"},
{"title": "Conditional Latent ODEs for Motion Prediction in Autonomous Driving", "author": "Khang Truong Giang and Yongjae Kim and Andrea Finazzi", "abstract": "  This paper addresses imitation learning for motion prediction problem in\nautonomous driving, especially in multi-agent setting. Different from previous\nmethods based on GAN, we present the conditional latent ordinary differential\nequation (cLODE) to leverage both the generative strength of conditional VAE\nand the continuous representation of neural ODE. Our network architecture is\ninspired from the Latent-ODE model. The experiment shows that our method\noutperform the baseline methods in the simulation of multi-agent driving and is\nvery efficient in term of GPU memory consumption. Our code and docker image are\npublicly available: https://github.com/TruongKhang/cLODE;\nhttps://hub.docker.com/r/kim4375731/clode.\n", "link": "http://arxiv.org/abs/2405.19183v1", "date": "2024-05-29", "relevancy": 1.0757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5857}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5183}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Latent%20ODEs%20for%20Motion%20Prediction%20in%20Autonomous%20Driving&body=Title%3A%20Conditional%20Latent%20ODEs%20for%20Motion%20Prediction%20in%20Autonomous%20Driving%0AAuthor%3A%20Khang%20Truong%20Giang%20and%20Yongjae%20Kim%20and%20Andrea%20Finazzi%0AAbstract%3A%20%20%20This%20paper%20addresses%20imitation%20learning%20for%20motion%20prediction%20problem%20in%0Aautonomous%20driving%2C%20especially%20in%20multi-agent%20setting.%20Different%20from%20previous%0Amethods%20based%20on%20GAN%2C%20we%20present%20the%20conditional%20latent%20ordinary%20differential%0Aequation%20%28cLODE%29%20to%20leverage%20both%20the%20generative%20strength%20of%20conditional%20VAE%0Aand%20the%20continuous%20representation%20of%20neural%20ODE.%20Our%20network%20architecture%20is%0Ainspired%20from%20the%20Latent-ODE%20model.%20The%20experiment%20shows%20that%20our%20method%0Aoutperform%20the%20baseline%20methods%20in%20the%20simulation%20of%20multi-agent%20driving%20and%20is%0Avery%20efficient%20in%20term%20of%20GPU%20memory%20consumption.%20Our%20code%20and%20docker%20image%20are%0Apublicly%20available%3A%20https%3A//github.com/TruongKhang/cLODE%3B%0Ahttps%3A//hub.docker.com/r/kim4375731/clode.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Latent%2520ODEs%2520for%2520Motion%2520Prediction%2520in%2520Autonomous%2520Driving%26entry.906535625%3DKhang%2520Truong%2520Giang%2520and%2520Yongjae%2520Kim%2520and%2520Andrea%2520Finazzi%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520imitation%2520learning%2520for%2520motion%2520prediction%2520problem%2520in%250Aautonomous%2520driving%252C%2520especially%2520in%2520multi-agent%2520setting.%2520Different%2520from%2520previous%250Amethods%2520based%2520on%2520GAN%252C%2520we%2520present%2520the%2520conditional%2520latent%2520ordinary%2520differential%250Aequation%2520%2528cLODE%2529%2520to%2520leverage%2520both%2520the%2520generative%2520strength%2520of%2520conditional%2520VAE%250Aand%2520the%2520continuous%2520representation%2520of%2520neural%2520ODE.%2520Our%2520network%2520architecture%2520is%250Ainspired%2520from%2520the%2520Latent-ODE%2520model.%2520The%2520experiment%2520shows%2520that%2520our%2520method%250Aoutperform%2520the%2520baseline%2520methods%2520in%2520the%2520simulation%2520of%2520multi-agent%2520driving%2520and%2520is%250Avery%2520efficient%2520in%2520term%2520of%2520GPU%2520memory%2520consumption.%2520Our%2520code%2520and%2520docker%2520image%2520are%250Apublicly%2520available%253A%2520https%253A//github.com/TruongKhang/cLODE%253B%250Ahttps%253A//hub.docker.com/r/kim4375731/clode.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Latent%20ODEs%20for%20Motion%20Prediction%20in%20Autonomous%20Driving&entry.906535625=Khang%20Truong%20Giang%20and%20Yongjae%20Kim%20and%20Andrea%20Finazzi&entry.1292438233=%20%20This%20paper%20addresses%20imitation%20learning%20for%20motion%20prediction%20problem%20in%0Aautonomous%20driving%2C%20especially%20in%20multi-agent%20setting.%20Different%20from%20previous%0Amethods%20based%20on%20GAN%2C%20we%20present%20the%20conditional%20latent%20ordinary%20differential%0Aequation%20%28cLODE%29%20to%20leverage%20both%20the%20generative%20strength%20of%20conditional%20VAE%0Aand%20the%20continuous%20representation%20of%20neural%20ODE.%20Our%20network%20architecture%20is%0Ainspired%20from%20the%20Latent-ODE%20model.%20The%20experiment%20shows%20that%20our%20method%0Aoutperform%20the%20baseline%20methods%20in%20the%20simulation%20of%20multi-agent%20driving%20and%20is%0Avery%20efficient%20in%20term%20of%20GPU%20memory%20consumption.%20Our%20code%20and%20docker%20image%20are%0Apublicly%20available%3A%20https%3A//github.com/TruongKhang/cLODE%3B%0Ahttps%3A//hub.docker.com/r/kim4375731/clode.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19183v1&entry.124074799=Read"},
{"title": "Dynamic Throwing with Robotic Material Handling Machines", "author": "Lennart Werner and Fang Nan and Pol Eyschen and Filippo A. Spinelli and Hongyi Yang and Marco Hutter", "abstract": "  Automation of hydraulic material handling machinery is currently limited to\nsemi-static pick-and-place cycles. Dynamic throwing motions which utilize the\npassive joints, can greatly improve time efficiency as well as increase the\ndumping workspace. In this work, we use Reinforcement Learning (RL) to design\ndynamic controllers for material handlers with underactuated arms as commonly\nused in logistics. The controllers are tested both in simulation and in\nreal-world experiments on a 12-ton test platform. The method is able to exploit\nthe passive joints of the gripper to perform dynamic throwing motions. With the\nproposed controllers, the machine is able to throw individual objects to\ntargets outside the static reachability zone with good accuracy for its\npractical applications. The work demonstrates the possibility of using RL to\nperform highly dynamic tasks with heavy machinery, suggesting a potential for\nimproving the efficiency and precision of autonomous material handling tasks.\n", "link": "http://arxiv.org/abs/2405.19001v1", "date": "2024-05-29", "relevancy": 1.5581, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines&body=Title%3A%20Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines%0AAuthor%3A%20Lennart%20Werner%20and%20Fang%20Nan%20and%20Pol%20Eyschen%20and%20Filippo%20A.%20Spinelli%20and%20Hongyi%20Yang%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Automation%20of%20hydraulic%20material%20handling%20machinery%20is%20currently%20limited%20to%0Asemi-static%20pick-and-place%20cycles.%20Dynamic%20throwing%20motions%20which%20utilize%20the%0Apassive%20joints%2C%20can%20greatly%20improve%20time%20efficiency%20as%20well%20as%20increase%20the%0Adumping%20workspace.%20In%20this%20work%2C%20we%20use%20Reinforcement%20Learning%20%28RL%29%20to%20design%0Adynamic%20controllers%20for%20material%20handlers%20with%20underactuated%20arms%20as%20commonly%0Aused%20in%20logistics.%20The%20controllers%20are%20tested%20both%20in%20simulation%20and%20in%0Areal-world%20experiments%20on%20a%2012-ton%20test%20platform.%20The%20method%20is%20able%20to%20exploit%0Athe%20passive%20joints%20of%20the%20gripper%20to%20perform%20dynamic%20throwing%20motions.%20With%20the%0Aproposed%20controllers%2C%20the%20machine%20is%20able%20to%20throw%20individual%20objects%20to%0Atargets%20outside%20the%20static%20reachability%20zone%20with%20good%20accuracy%20for%20its%0Apractical%20applications.%20The%20work%20demonstrates%20the%20possibility%20of%20using%20RL%20to%0Aperform%20highly%20dynamic%20tasks%20with%20heavy%20machinery%2C%20suggesting%20a%20potential%20for%0Aimproving%20the%20efficiency%20and%20precision%20of%20autonomous%20material%20handling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Throwing%2520with%2520Robotic%2520Material%2520Handling%2520Machines%26entry.906535625%3DLennart%2520Werner%2520and%2520Fang%2520Nan%2520and%2520Pol%2520Eyschen%2520and%2520Filippo%2520A.%2520Spinelli%2520and%2520Hongyi%2520Yang%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Automation%2520of%2520hydraulic%2520material%2520handling%2520machinery%2520is%2520currently%2520limited%2520to%250Asemi-static%2520pick-and-place%2520cycles.%2520Dynamic%2520throwing%2520motions%2520which%2520utilize%2520the%250Apassive%2520joints%252C%2520can%2520greatly%2520improve%2520time%2520efficiency%2520as%2520well%2520as%2520increase%2520the%250Adumping%2520workspace.%2520In%2520this%2520work%252C%2520we%2520use%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520design%250Adynamic%2520controllers%2520for%2520material%2520handlers%2520with%2520underactuated%2520arms%2520as%2520commonly%250Aused%2520in%2520logistics.%2520The%2520controllers%2520are%2520tested%2520both%2520in%2520simulation%2520and%2520in%250Areal-world%2520experiments%2520on%2520a%252012-ton%2520test%2520platform.%2520The%2520method%2520is%2520able%2520to%2520exploit%250Athe%2520passive%2520joints%2520of%2520the%2520gripper%2520to%2520perform%2520dynamic%2520throwing%2520motions.%2520With%2520the%250Aproposed%2520controllers%252C%2520the%2520machine%2520is%2520able%2520to%2520throw%2520individual%2520objects%2520to%250Atargets%2520outside%2520the%2520static%2520reachability%2520zone%2520with%2520good%2520accuracy%2520for%2520its%250Apractical%2520applications.%2520The%2520work%2520demonstrates%2520the%2520possibility%2520of%2520using%2520RL%2520to%250Aperform%2520highly%2520dynamic%2520tasks%2520with%2520heavy%2520machinery%252C%2520suggesting%2520a%2520potential%2520for%250Aimproving%2520the%2520efficiency%2520and%2520precision%2520of%2520autonomous%2520material%2520handling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines&entry.906535625=Lennart%20Werner%20and%20Fang%20Nan%20and%20Pol%20Eyschen%20and%20Filippo%20A.%20Spinelli%20and%20Hongyi%20Yang%20and%20Marco%20Hutter&entry.1292438233=%20%20Automation%20of%20hydraulic%20material%20handling%20machinery%20is%20currently%20limited%20to%0Asemi-static%20pick-and-place%20cycles.%20Dynamic%20throwing%20motions%20which%20utilize%20the%0Apassive%20joints%2C%20can%20greatly%20improve%20time%20efficiency%20as%20well%20as%20increase%20the%0Adumping%20workspace.%20In%20this%20work%2C%20we%20use%20Reinforcement%20Learning%20%28RL%29%20to%20design%0Adynamic%20controllers%20for%20material%20handlers%20with%20underactuated%20arms%20as%20commonly%0Aused%20in%20logistics.%20The%20controllers%20are%20tested%20both%20in%20simulation%20and%20in%0Areal-world%20experiments%20on%20a%2012-ton%20test%20platform.%20The%20method%20is%20able%20to%20exploit%0Athe%20passive%20joints%20of%20the%20gripper%20to%20perform%20dynamic%20throwing%20motions.%20With%20the%0Aproposed%20controllers%2C%20the%20machine%20is%20able%20to%20throw%20individual%20objects%20to%0Atargets%20outside%20the%20static%20reachability%20zone%20with%20good%20accuracy%20for%20its%0Apractical%20applications.%20The%20work%20demonstrates%20the%20possibility%20of%20using%20RL%20to%0Aperform%20highly%20dynamic%20tasks%20with%20heavy%20machinery%2C%20suggesting%20a%20potential%20for%0Aimproving%20the%20efficiency%20and%20precision%20of%20autonomous%20material%20handling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19001v1&entry.124074799=Read"},
{"title": "Understanding and Improving Training-free Loss-based Diffusion Guidance", "author": "Yifei Shen and Xinyang Jiang and Yezhen Wang and Yifan Yang and Dongqi Han and Dongsheng Li", "abstract": "  Adding additional control to pretrained diffusion models has become an\nincreasingly popular research area, with extensive applications in computer\nvision, reinforcement learning, and AI for science. Recently, several studies\nhave proposed training-free loss-based guidance by using off-the-shelf networks\npretrained on clean images. This approach enables zero-shot conditional\ngeneration for universal control formats, which appears to offer a free lunch\nin diffusion guidance. In this paper, we aim to develop a deeper understanding\nof training-free guidance, as well as overcome its limitations. We offer a\ntheoretical analysis that supports training-free guidance from the perspective\nof optimization, distinguishing it from classifier-based (or classifier-free)\nguidance. To elucidate their drawbacks, we theoretically demonstrate that\ntraining-free guidance is more susceptible to adversarial gradients and\nexhibits slower convergence rates compared to classifier guidance. We then\nintroduce a collection of techniques designed to overcome the limitations,\naccompanied by theoretical rationale and empirical evidence. Our experiments in\nimage and motion generation confirm the efficacy of these techniques.\n", "link": "http://arxiv.org/abs/2403.12404v2", "date": "2024-05-29", "relevancy": 1.6797, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5889}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5536}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Improving%20Training-free%20Loss-based%20Diffusion%20Guidance&body=Title%3A%20Understanding%20and%20Improving%20Training-free%20Loss-based%20Diffusion%20Guidance%0AAuthor%3A%20Yifei%20Shen%20and%20Xinyang%20Jiang%20and%20Yezhen%20Wang%20and%20Yifan%20Yang%20and%20Dongqi%20Han%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20Adding%20additional%20control%20to%20pretrained%20diffusion%20models%20has%20become%20an%0Aincreasingly%20popular%20research%20area%2C%20with%20extensive%20applications%20in%20computer%0Avision%2C%20reinforcement%20learning%2C%20and%20AI%20for%20science.%20Recently%2C%20several%20studies%0Ahave%20proposed%20training-free%20loss-based%20guidance%20by%20using%20off-the-shelf%20networks%0Apretrained%20on%20clean%20images.%20This%20approach%20enables%20zero-shot%20conditional%0Ageneration%20for%20universal%20control%20formats%2C%20which%20appears%20to%20offer%20a%20free%20lunch%0Ain%20diffusion%20guidance.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20deeper%20understanding%0Aof%20training-free%20guidance%2C%20as%20well%20as%20overcome%20its%20limitations.%20We%20offer%20a%0Atheoretical%20analysis%20that%20supports%20training-free%20guidance%20from%20the%20perspective%0Aof%20optimization%2C%20distinguishing%20it%20from%20classifier-based%20%28or%20classifier-free%29%0Aguidance.%20To%20elucidate%20their%20drawbacks%2C%20we%20theoretically%20demonstrate%20that%0Atraining-free%20guidance%20is%20more%20susceptible%20to%20adversarial%20gradients%20and%0Aexhibits%20slower%20convergence%20rates%20compared%20to%20classifier%20guidance.%20We%20then%0Aintroduce%20a%20collection%20of%20techniques%20designed%20to%20overcome%20the%20limitations%2C%0Aaccompanied%20by%20theoretical%20rationale%20and%20empirical%20evidence.%20Our%20experiments%20in%0Aimage%20and%20motion%20generation%20confirm%20the%20efficacy%20of%20these%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Improving%2520Training-free%2520Loss-based%2520Diffusion%2520Guidance%26entry.906535625%3DYifei%2520Shen%2520and%2520Xinyang%2520Jiang%2520and%2520Yezhen%2520Wang%2520and%2520Yifan%2520Yang%2520and%2520Dongqi%2520Han%2520and%2520Dongsheng%2520Li%26entry.1292438233%3D%2520%2520Adding%2520additional%2520control%2520to%2520pretrained%2520diffusion%2520models%2520has%2520become%2520an%250Aincreasingly%2520popular%2520research%2520area%252C%2520with%2520extensive%2520applications%2520in%2520computer%250Avision%252C%2520reinforcement%2520learning%252C%2520and%2520AI%2520for%2520science.%2520Recently%252C%2520several%2520studies%250Ahave%2520proposed%2520training-free%2520loss-based%2520guidance%2520by%2520using%2520off-the-shelf%2520networks%250Apretrained%2520on%2520clean%2520images.%2520This%2520approach%2520enables%2520zero-shot%2520conditional%250Ageneration%2520for%2520universal%2520control%2520formats%252C%2520which%2520appears%2520to%2520offer%2520a%2520free%2520lunch%250Ain%2520diffusion%2520guidance.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520develop%2520a%2520deeper%2520understanding%250Aof%2520training-free%2520guidance%252C%2520as%2520well%2520as%2520overcome%2520its%2520limitations.%2520We%2520offer%2520a%250Atheoretical%2520analysis%2520that%2520supports%2520training-free%2520guidance%2520from%2520the%2520perspective%250Aof%2520optimization%252C%2520distinguishing%2520it%2520from%2520classifier-based%2520%2528or%2520classifier-free%2529%250Aguidance.%2520To%2520elucidate%2520their%2520drawbacks%252C%2520we%2520theoretically%2520demonstrate%2520that%250Atraining-free%2520guidance%2520is%2520more%2520susceptible%2520to%2520adversarial%2520gradients%2520and%250Aexhibits%2520slower%2520convergence%2520rates%2520compared%2520to%2520classifier%2520guidance.%2520We%2520then%250Aintroduce%2520a%2520collection%2520of%2520techniques%2520designed%2520to%2520overcome%2520the%2520limitations%252C%250Aaccompanied%2520by%2520theoretical%2520rationale%2520and%2520empirical%2520evidence.%2520Our%2520experiments%2520in%250Aimage%2520and%2520motion%2520generation%2520confirm%2520the%2520efficacy%2520of%2520these%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Improving%20Training-free%20Loss-based%20Diffusion%20Guidance&entry.906535625=Yifei%20Shen%20and%20Xinyang%20Jiang%20and%20Yezhen%20Wang%20and%20Yifan%20Yang%20and%20Dongqi%20Han%20and%20Dongsheng%20Li&entry.1292438233=%20%20Adding%20additional%20control%20to%20pretrained%20diffusion%20models%20has%20become%20an%0Aincreasingly%20popular%20research%20area%2C%20with%20extensive%20applications%20in%20computer%0Avision%2C%20reinforcement%20learning%2C%20and%20AI%20for%20science.%20Recently%2C%20several%20studies%0Ahave%20proposed%20training-free%20loss-based%20guidance%20by%20using%20off-the-shelf%20networks%0Apretrained%20on%20clean%20images.%20This%20approach%20enables%20zero-shot%20conditional%0Ageneration%20for%20universal%20control%20formats%2C%20which%20appears%20to%20offer%20a%20free%20lunch%0Ain%20diffusion%20guidance.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20deeper%20understanding%0Aof%20training-free%20guidance%2C%20as%20well%20as%20overcome%20its%20limitations.%20We%20offer%20a%0Atheoretical%20analysis%20that%20supports%20training-free%20guidance%20from%20the%20perspective%0Aof%20optimization%2C%20distinguishing%20it%20from%20classifier-based%20%28or%20classifier-free%29%0Aguidance.%20To%20elucidate%20their%20drawbacks%2C%20we%20theoretically%20demonstrate%20that%0Atraining-free%20guidance%20is%20more%20susceptible%20to%20adversarial%20gradients%20and%0Aexhibits%20slower%20convergence%20rates%20compared%20to%20classifier%20guidance.%20We%20then%0Aintroduce%20a%20collection%20of%20techniques%20designed%20to%20overcome%20the%20limitations%2C%0Aaccompanied%20by%20theoretical%20rationale%20and%20empirical%20evidence.%20Our%20experiments%20in%0Aimage%20and%20motion%20generation%20confirm%20the%20efficacy%20of%20these%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12404v2&entry.124074799=Read"},
{"title": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection", "author": "Taeheon Kim and Sangyun Chung and Damin Yeom and Youngjoon Yu and Hak Gu Kim and Yong Man Ro", "abstract": "  Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in certain\ncases (e.g., thermal-obscured pedestrians), particularly due to the modality\nbias learned from statistically biased datasets. In this paper, we investigate\nhow to mitigate modality bias in multispectral pedestrian detection using Large\nLanguage Models (LLMs). Accordingly, we design a Multispectral Chain-of-Thought\n(MSCoT) prompting strategy, which prompts the LLM to perform multispectral\npedestrian detection. Moreover, we propose a novel Multispectral\nChain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting\ninto multispectral pedestrian detection. To this end, we design a\nLanguage-driven Multi-modal Fusion (LMF) strategy that enables fusing the\noutputs of MSCoT prompting with the detection results of vision-based\nmultispectral pedestrian detection models. Extensive experiments validate that\nMSCoTDet effectively mitigates modality biases and improves multispectral\npedestrian detection.\n", "link": "http://arxiv.org/abs/2403.15209v2", "date": "2024-05-29", "relevancy": 1.6467, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection&body=Title%3A%20MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection%0AAuthor%3A%20Taeheon%20Kim%20and%20Sangyun%20Chung%20and%20Damin%20Yeom%20and%20Youngjoon%20Yu%20and%20Hak%20Gu%20Kim%20and%20Yong%20Man%20Ro%0AAbstract%3A%20%20%20Multispectral%20pedestrian%20detection%20is%20attractive%20for%20around-the-clock%0Aapplications%20due%20to%20the%20complementary%20information%20between%20RGB%20and%20thermal%0Amodalities.%20However%2C%20current%20models%20often%20fail%20to%20detect%20pedestrians%20in%20certain%0Acases%20%28e.g.%2C%20thermal-obscured%20pedestrians%29%2C%20particularly%20due%20to%20the%20modality%0Abias%20learned%20from%20statistically%20biased%20datasets.%20In%20this%20paper%2C%20we%20investigate%0Ahow%20to%20mitigate%20modality%20bias%20in%20multispectral%20pedestrian%20detection%20using%20Large%0ALanguage%20Models%20%28LLMs%29.%20Accordingly%2C%20we%20design%20a%20Multispectral%20Chain-of-Thought%0A%28MSCoT%29%20prompting%20strategy%2C%20which%20prompts%20the%20LLM%20to%20perform%20multispectral%0Apedestrian%20detection.%20Moreover%2C%20we%20propose%20a%20novel%20Multispectral%0AChain-of-Thought%20Detection%20%28MSCoTDet%29%20framework%20that%20integrates%20MSCoT%20prompting%0Ainto%20multispectral%20pedestrian%20detection.%20To%20this%20end%2C%20we%20design%20a%0ALanguage-driven%20Multi-modal%20Fusion%20%28LMF%29%20strategy%20that%20enables%20fusing%20the%0Aoutputs%20of%20MSCoT%20prompting%20with%20the%20detection%20results%20of%20vision-based%0Amultispectral%20pedestrian%20detection%20models.%20Extensive%20experiments%20validate%20that%0AMSCoTDet%20effectively%20mitigates%20modality%20biases%20and%20improves%20multispectral%0Apedestrian%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCoTDet%253A%2520Language-driven%2520Multi-modal%2520Fusion%2520for%2520Improved%2520Multispectral%250A%2520%2520Pedestrian%2520Detection%26entry.906535625%3DTaeheon%2520Kim%2520and%2520Sangyun%2520Chung%2520and%2520Damin%2520Yeom%2520and%2520Youngjoon%2520Yu%2520and%2520Hak%2520Gu%2520Kim%2520and%2520Yong%2520Man%2520Ro%26entry.1292438233%3D%2520%2520Multispectral%2520pedestrian%2520detection%2520is%2520attractive%2520for%2520around-the-clock%250Aapplications%2520due%2520to%2520the%2520complementary%2520information%2520between%2520RGB%2520and%2520thermal%250Amodalities.%2520However%252C%2520current%2520models%2520often%2520fail%2520to%2520detect%2520pedestrians%2520in%2520certain%250Acases%2520%2528e.g.%252C%2520thermal-obscured%2520pedestrians%2529%252C%2520particularly%2520due%2520to%2520the%2520modality%250Abias%2520learned%2520from%2520statistically%2520biased%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520investigate%250Ahow%2520to%2520mitigate%2520modality%2520bias%2520in%2520multispectral%2520pedestrian%2520detection%2520using%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520Accordingly%252C%2520we%2520design%2520a%2520Multispectral%2520Chain-of-Thought%250A%2528MSCoT%2529%2520prompting%2520strategy%252C%2520which%2520prompts%2520the%2520LLM%2520to%2520perform%2520multispectral%250Apedestrian%2520detection.%2520Moreover%252C%2520we%2520propose%2520a%2520novel%2520Multispectral%250AChain-of-Thought%2520Detection%2520%2528MSCoTDet%2529%2520framework%2520that%2520integrates%2520MSCoT%2520prompting%250Ainto%2520multispectral%2520pedestrian%2520detection.%2520To%2520this%2520end%252C%2520we%2520design%2520a%250ALanguage-driven%2520Multi-modal%2520Fusion%2520%2528LMF%2529%2520strategy%2520that%2520enables%2520fusing%2520the%250Aoutputs%2520of%2520MSCoT%2520prompting%2520with%2520the%2520detection%2520results%2520of%2520vision-based%250Amultispectral%2520pedestrian%2520detection%2520models.%2520Extensive%2520experiments%2520validate%2520that%250AMSCoTDet%2520effectively%2520mitigates%2520modality%2520biases%2520and%2520improves%2520multispectral%250Apedestrian%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection&entry.906535625=Taeheon%20Kim%20and%20Sangyun%20Chung%20and%20Damin%20Yeom%20and%20Youngjoon%20Yu%20and%20Hak%20Gu%20Kim%20and%20Yong%20Man%20Ro&entry.1292438233=%20%20Multispectral%20pedestrian%20detection%20is%20attractive%20for%20around-the-clock%0Aapplications%20due%20to%20the%20complementary%20information%20between%20RGB%20and%20thermal%0Amodalities.%20However%2C%20current%20models%20often%20fail%20to%20detect%20pedestrians%20in%20certain%0Acases%20%28e.g.%2C%20thermal-obscured%20pedestrians%29%2C%20particularly%20due%20to%20the%20modality%0Abias%20learned%20from%20statistically%20biased%20datasets.%20In%20this%20paper%2C%20we%20investigate%0Ahow%20to%20mitigate%20modality%20bias%20in%20multispectral%20pedestrian%20detection%20using%20Large%0ALanguage%20Models%20%28LLMs%29.%20Accordingly%2C%20we%20design%20a%20Multispectral%20Chain-of-Thought%0A%28MSCoT%29%20prompting%20strategy%2C%20which%20prompts%20the%20LLM%20to%20perform%20multispectral%0Apedestrian%20detection.%20Moreover%2C%20we%20propose%20a%20novel%20Multispectral%0AChain-of-Thought%20Detection%20%28MSCoTDet%29%20framework%20that%20integrates%20MSCoT%20prompting%0Ainto%20multispectral%20pedestrian%20detection.%20To%20this%20end%2C%20we%20design%20a%0ALanguage-driven%20Multi-modal%20Fusion%20%28LMF%29%20strategy%20that%20enables%20fusing%20the%0Aoutputs%20of%20MSCoT%20prompting%20with%20the%20detection%20results%20of%20vision-based%0Amultispectral%20pedestrian%20detection%20models.%20Extensive%20experiments%20validate%20that%0AMSCoTDet%20effectively%20mitigates%20modality%20biases%20and%20improves%20multispectral%0Apedestrian%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15209v2&entry.124074799=Read"},
{"title": "DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories\n  Applied on Quadruped Robots", "author": "Maria Stamatopoulou and Jianwei Liu and Dimitrios Kanoulas", "abstract": "  We present DiPPeST, a novel image and goal conditioned diffusion-based\ntrajectory generator for quadrupedal robot path planning. DiPPeST is a\nzero-shot adaptation of our previously introduced diffusion-based 2D global\ntrajectory generator (DiPPeR). The introduced system incorporates a novel\nstrategy for local real-time path refinements, that is reactive to camera\ninput, without requiring any further training, image processing, or environment\ninterpretation techniques. DiPPeST achieves 92% success rate in obstacle\navoidance for nominal environments and an average of 88% success rate when\ntested in environments that are up to 3.5 times more complex in pixel variation\nthan DiPPeR. A visual-servoing framework is developed to allow for real-world\nexecution, tested on the quadruped robot, achieving 80% success rate in\ndifferent environments and showcasing improved behavior than complex\nstate-of-the-art local planners, in narrow environments.\n", "link": "http://arxiv.org/abs/2405.19232v1", "date": "2024-05-29", "relevancy": 1.045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5276}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiPPeST%3A%20Diffusion-based%20Path%20Planner%20for%20Synthesizing%20Trajectories%0A%20%20Applied%20on%20Quadruped%20Robots&body=Title%3A%20DiPPeST%3A%20Diffusion-based%20Path%20Planner%20for%20Synthesizing%20Trajectories%0A%20%20Applied%20on%20Quadruped%20Robots%0AAuthor%3A%20Maria%20Stamatopoulou%20and%20Jianwei%20Liu%20and%20Dimitrios%20Kanoulas%0AAbstract%3A%20%20%20We%20present%20DiPPeST%2C%20a%20novel%20image%20and%20goal%20conditioned%20diffusion-based%0Atrajectory%20generator%20for%20quadrupedal%20robot%20path%20planning.%20DiPPeST%20is%20a%0Azero-shot%20adaptation%20of%20our%20previously%20introduced%20diffusion-based%202D%20global%0Atrajectory%20generator%20%28DiPPeR%29.%20The%20introduced%20system%20incorporates%20a%20novel%0Astrategy%20for%20local%20real-time%20path%20refinements%2C%20that%20is%20reactive%20to%20camera%0Ainput%2C%20without%20requiring%20any%20further%20training%2C%20image%20processing%2C%20or%20environment%0Ainterpretation%20techniques.%20DiPPeST%20achieves%2092%25%20success%20rate%20in%20obstacle%0Aavoidance%20for%20nominal%20environments%20and%20an%20average%20of%2088%25%20success%20rate%20when%0Atested%20in%20environments%20that%20are%20up%20to%203.5%20times%20more%20complex%20in%20pixel%20variation%0Athan%20DiPPeR.%20A%20visual-servoing%20framework%20is%20developed%20to%20allow%20for%20real-world%0Aexecution%2C%20tested%20on%20the%20quadruped%20robot%2C%20achieving%2080%25%20success%20rate%20in%0Adifferent%20environments%20and%20showcasing%20improved%20behavior%20than%20complex%0Astate-of-the-art%20local%20planners%2C%20in%20narrow%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiPPeST%253A%2520Diffusion-based%2520Path%2520Planner%2520for%2520Synthesizing%2520Trajectories%250A%2520%2520Applied%2520on%2520Quadruped%2520Robots%26entry.906535625%3DMaria%2520Stamatopoulou%2520and%2520Jianwei%2520Liu%2520and%2520Dimitrios%2520Kanoulas%26entry.1292438233%3D%2520%2520We%2520present%2520DiPPeST%252C%2520a%2520novel%2520image%2520and%2520goal%2520conditioned%2520diffusion-based%250Atrajectory%2520generator%2520for%2520quadrupedal%2520robot%2520path%2520planning.%2520DiPPeST%2520is%2520a%250Azero-shot%2520adaptation%2520of%2520our%2520previously%2520introduced%2520diffusion-based%25202D%2520global%250Atrajectory%2520generator%2520%2528DiPPeR%2529.%2520The%2520introduced%2520system%2520incorporates%2520a%2520novel%250Astrategy%2520for%2520local%2520real-time%2520path%2520refinements%252C%2520that%2520is%2520reactive%2520to%2520camera%250Ainput%252C%2520without%2520requiring%2520any%2520further%2520training%252C%2520image%2520processing%252C%2520or%2520environment%250Ainterpretation%2520techniques.%2520DiPPeST%2520achieves%252092%2525%2520success%2520rate%2520in%2520obstacle%250Aavoidance%2520for%2520nominal%2520environments%2520and%2520an%2520average%2520of%252088%2525%2520success%2520rate%2520when%250Atested%2520in%2520environments%2520that%2520are%2520up%2520to%25203.5%2520times%2520more%2520complex%2520in%2520pixel%2520variation%250Athan%2520DiPPeR.%2520A%2520visual-servoing%2520framework%2520is%2520developed%2520to%2520allow%2520for%2520real-world%250Aexecution%252C%2520tested%2520on%2520the%2520quadruped%2520robot%252C%2520achieving%252080%2525%2520success%2520rate%2520in%250Adifferent%2520environments%2520and%2520showcasing%2520improved%2520behavior%2520than%2520complex%250Astate-of-the-art%2520local%2520planners%252C%2520in%2520narrow%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiPPeST%3A%20Diffusion-based%20Path%20Planner%20for%20Synthesizing%20Trajectories%0A%20%20Applied%20on%20Quadruped%20Robots&entry.906535625=Maria%20Stamatopoulou%20and%20Jianwei%20Liu%20and%20Dimitrios%20Kanoulas&entry.1292438233=%20%20We%20present%20DiPPeST%2C%20a%20novel%20image%20and%20goal%20conditioned%20diffusion-based%0Atrajectory%20generator%20for%20quadrupedal%20robot%20path%20planning.%20DiPPeST%20is%20a%0Azero-shot%20adaptation%20of%20our%20previously%20introduced%20diffusion-based%202D%20global%0Atrajectory%20generator%20%28DiPPeR%29.%20The%20introduced%20system%20incorporates%20a%20novel%0Astrategy%20for%20local%20real-time%20path%20refinements%2C%20that%20is%20reactive%20to%20camera%0Ainput%2C%20without%20requiring%20any%20further%20training%2C%20image%20processing%2C%20or%20environment%0Ainterpretation%20techniques.%20DiPPeST%20achieves%2092%25%20success%20rate%20in%20obstacle%0Aavoidance%20for%20nominal%20environments%20and%20an%20average%20of%2088%25%20success%20rate%20when%0Atested%20in%20environments%20that%20are%20up%20to%203.5%20times%20more%20complex%20in%20pixel%20variation%0Athan%20DiPPeR.%20A%20visual-servoing%20framework%20is%20developed%20to%20allow%20for%20real-world%0Aexecution%2C%20tested%20on%20the%20quadruped%20robot%2C%20achieving%2080%25%20success%20rate%20in%0Adifferent%20environments%20and%20showcasing%20improved%20behavior%20than%20complex%0Astate-of-the-art%20local%20planners%2C%20in%20narrow%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19232v1&entry.124074799=Read"},
{"title": "Membership Inference on Text-to-Image Diffusion Models via Conditional\n  Likelihood Discrepancy", "author": "Shengfang Zhai and Huanran Chen and Yinpeng Dong and Jiajun Li and Qingni Shen and Yansong Gao and Hang Su and Yang Liu", "abstract": "  Text-to-image diffusion models have achieved tremendous success in the field\nof controllable image generation, while also coming along with issues of\nprivacy leakage and data copyrights. Membership inference arises in these\ncontexts as a potential auditing method for detecting unauthorized data usage.\nWhile some efforts have been made on diffusion models, they are not applicable\nto text-to-image diffusion models due to the high computation overhead and\nenhanced generalization capabilities. In this paper, we first identify a\nconditional overfitting phenomenon in text-to-image diffusion models,\nindicating that these models tend to overfit the conditional distribution of\nimages given the text rather than the marginal distribution of images. Based on\nthis observation, we derive an analytical indicator, namely Conditional\nLikelihood Discrepancy (CLiD), to perform membership inference, which reduces\nthe stochasticity in estimating the memorization of individual samples.\nExperimental results demonstrate that our method significantly outperforms\nprevious methods across various data distributions and scales. Additionally,\nour method shows superior resistance to overfitting mitigation strategies such\nas early stopping and data augmentation.\n", "link": "http://arxiv.org/abs/2405.14800v2", "date": "2024-05-29", "relevancy": 1.716, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6018}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Membership%20Inference%20on%20Text-to-Image%20Diffusion%20Models%20via%20Conditional%0A%20%20Likelihood%20Discrepancy&body=Title%3A%20Membership%20Inference%20on%20Text-to-Image%20Diffusion%20Models%20via%20Conditional%0A%20%20Likelihood%20Discrepancy%0AAuthor%3A%20Shengfang%20Zhai%20and%20Huanran%20Chen%20and%20Yinpeng%20Dong%20and%20Jiajun%20Li%20and%20Qingni%20Shen%20and%20Yansong%20Gao%20and%20Hang%20Su%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20achieved%20tremendous%20success%20in%20the%20field%0Aof%20controllable%20image%20generation%2C%20while%20also%20coming%20along%20with%20issues%20of%0Aprivacy%20leakage%20and%20data%20copyrights.%20Membership%20inference%20arises%20in%20these%0Acontexts%20as%20a%20potential%20auditing%20method%20for%20detecting%20unauthorized%20data%20usage.%0AWhile%20some%20efforts%20have%20been%20made%20on%20diffusion%20models%2C%20they%20are%20not%20applicable%0Ato%20text-to-image%20diffusion%20models%20due%20to%20the%20high%20computation%20overhead%20and%0Aenhanced%20generalization%20capabilities.%20In%20this%20paper%2C%20we%20first%20identify%20a%0Aconditional%20overfitting%20phenomenon%20in%20text-to-image%20diffusion%20models%2C%0Aindicating%20that%20these%20models%20tend%20to%20overfit%20the%20conditional%20distribution%20of%0Aimages%20given%20the%20text%20rather%20than%20the%20marginal%20distribution%20of%20images.%20Based%20on%0Athis%20observation%2C%20we%20derive%20an%20analytical%20indicator%2C%20namely%20Conditional%0ALikelihood%20Discrepancy%20%28CLiD%29%2C%20to%20perform%20membership%20inference%2C%20which%20reduces%0Athe%20stochasticity%20in%20estimating%20the%20memorization%20of%20individual%20samples.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20methods%20across%20various%20data%20distributions%20and%20scales.%20Additionally%2C%0Aour%20method%20shows%20superior%20resistance%20to%20overfitting%20mitigation%20strategies%20such%0Aas%20early%20stopping%20and%20data%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMembership%2520Inference%2520on%2520Text-to-Image%2520Diffusion%2520Models%2520via%2520Conditional%250A%2520%2520Likelihood%2520Discrepancy%26entry.906535625%3DShengfang%2520Zhai%2520and%2520Huanran%2520Chen%2520and%2520Yinpeng%2520Dong%2520and%2520Jiajun%2520Li%2520and%2520Qingni%2520Shen%2520and%2520Yansong%2520Gao%2520and%2520Hang%2520Su%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520achieved%2520tremendous%2520success%2520in%2520the%2520field%250Aof%2520controllable%2520image%2520generation%252C%2520while%2520also%2520coming%2520along%2520with%2520issues%2520of%250Aprivacy%2520leakage%2520and%2520data%2520copyrights.%2520Membership%2520inference%2520arises%2520in%2520these%250Acontexts%2520as%2520a%2520potential%2520auditing%2520method%2520for%2520detecting%2520unauthorized%2520data%2520usage.%250AWhile%2520some%2520efforts%2520have%2520been%2520made%2520on%2520diffusion%2520models%252C%2520they%2520are%2520not%2520applicable%250Ato%2520text-to-image%2520diffusion%2520models%2520due%2520to%2520the%2520high%2520computation%2520overhead%2520and%250Aenhanced%2520generalization%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520first%2520identify%2520a%250Aconditional%2520overfitting%2520phenomenon%2520in%2520text-to-image%2520diffusion%2520models%252C%250Aindicating%2520that%2520these%2520models%2520tend%2520to%2520overfit%2520the%2520conditional%2520distribution%2520of%250Aimages%2520given%2520the%2520text%2520rather%2520than%2520the%2520marginal%2520distribution%2520of%2520images.%2520Based%2520on%250Athis%2520observation%252C%2520we%2520derive%2520an%2520analytical%2520indicator%252C%2520namely%2520Conditional%250ALikelihood%2520Discrepancy%2520%2528CLiD%2529%252C%2520to%2520perform%2520membership%2520inference%252C%2520which%2520reduces%250Athe%2520stochasticity%2520in%2520estimating%2520the%2520memorization%2520of%2520individual%2520samples.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Aprevious%2520methods%2520across%2520various%2520data%2520distributions%2520and%2520scales.%2520Additionally%252C%250Aour%2520method%2520shows%2520superior%2520resistance%2520to%2520overfitting%2520mitigation%2520strategies%2520such%250Aas%2520early%2520stopping%2520and%2520data%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membership%20Inference%20on%20Text-to-Image%20Diffusion%20Models%20via%20Conditional%0A%20%20Likelihood%20Discrepancy&entry.906535625=Shengfang%20Zhai%20and%20Huanran%20Chen%20and%20Yinpeng%20Dong%20and%20Jiajun%20Li%20and%20Qingni%20Shen%20and%20Yansong%20Gao%20and%20Hang%20Su%20and%20Yang%20Liu&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20achieved%20tremendous%20success%20in%20the%20field%0Aof%20controllable%20image%20generation%2C%20while%20also%20coming%20along%20with%20issues%20of%0Aprivacy%20leakage%20and%20data%20copyrights.%20Membership%20inference%20arises%20in%20these%0Acontexts%20as%20a%20potential%20auditing%20method%20for%20detecting%20unauthorized%20data%20usage.%0AWhile%20some%20efforts%20have%20been%20made%20on%20diffusion%20models%2C%20they%20are%20not%20applicable%0Ato%20text-to-image%20diffusion%20models%20due%20to%20the%20high%20computation%20overhead%20and%0Aenhanced%20generalization%20capabilities.%20In%20this%20paper%2C%20we%20first%20identify%20a%0Aconditional%20overfitting%20phenomenon%20in%20text-to-image%20diffusion%20models%2C%0Aindicating%20that%20these%20models%20tend%20to%20overfit%20the%20conditional%20distribution%20of%0Aimages%20given%20the%20text%20rather%20than%20the%20marginal%20distribution%20of%20images.%20Based%20on%0Athis%20observation%2C%20we%20derive%20an%20analytical%20indicator%2C%20namely%20Conditional%0ALikelihood%20Discrepancy%20%28CLiD%29%2C%20to%20perform%20membership%20inference%2C%20which%20reduces%0Athe%20stochasticity%20in%20estimating%20the%20memorization%20of%20individual%20samples.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20methods%20across%20various%20data%20distributions%20and%20scales.%20Additionally%2C%0Aour%20method%20shows%20superior%20resistance%20to%20overfitting%20mitigation%20strategies%20such%0Aas%20early%20stopping%20and%20data%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14800v2&entry.124074799=Read"},
{"title": "GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Domain-Specific Retrieval-Augmented Language Model", "author": "Xinzhe Li and Ming Liu and Shang Gao", "abstract": "  Retrieval-augmented Generation (RAG) systems have been actively studied and\ndeployed across various industries to query on domain-specific knowledge base.\nHowever, evaluating these systems presents unique challenges due to the\nscarcity of domain-specific queries and corresponding ground truths, as well as\na lack of systematic approaches to diagnosing the cause of failure cases --\nwhether they stem from knowledge deficits or issues related to system\nrobustness. To address these challenges, we introduce GRAMMAR (GRounded And\nModular Methodology for Assessment of RAG), an evaluation framework comprising\ntwo key elements: 1) a data generation process that leverages relational\ndatabases and LLMs to efficiently produce scalable query-answer pairs. This\nmethod facilitates the separation of query logic from linguistic variations for\nenhanced debugging capabilities; and 2) an evaluation framework that\ndifferentiates knowledge gaps from robustness and enables the identification of\ndefective modules. Our empirical results underscore the limitations of current\nreference-free evaluation approaches and the reliability of GRAMMAR to\naccurately identify model vulnerabilities.\n", "link": "http://arxiv.org/abs/2404.19232v4", "date": "2024-05-29", "relevancy": 1.8814, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4869}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4826}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Domain-Specific%20Retrieval-Augmented%20Language%20Model&body=Title%3A%20GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Domain-Specific%20Retrieval-Augmented%20Language%20Model%0AAuthor%3A%20Xinzhe%20Li%20and%20Ming%20Liu%20and%20Shang%20Gao%0AAbstract%3A%20%20%20Retrieval-augmented%20Generation%20%28RAG%29%20systems%20have%20been%20actively%20studied%20and%0Adeployed%20across%20various%20industries%20to%20query%20on%20domain-specific%20knowledge%20base.%0AHowever%2C%20evaluating%20these%20systems%20presents%20unique%20challenges%20due%20to%20the%0Ascarcity%20of%20domain-specific%20queries%20and%20corresponding%20ground%20truths%2C%20as%20well%20as%0Aa%20lack%20of%20systematic%20approaches%20to%20diagnosing%20the%20cause%20of%20failure%20cases%20--%0Awhether%20they%20stem%20from%20knowledge%20deficits%20or%20issues%20related%20to%20system%0Arobustness.%20To%20address%20these%20challenges%2C%20we%20introduce%20GRAMMAR%20%28GRounded%20And%0AModular%20Methodology%20for%20Assessment%20of%20RAG%29%2C%20an%20evaluation%20framework%20comprising%0Atwo%20key%20elements%3A%201%29%20a%20data%20generation%20process%20that%20leverages%20relational%0Adatabases%20and%20LLMs%20to%20efficiently%20produce%20scalable%20query-answer%20pairs.%20This%0Amethod%20facilitates%20the%20separation%20of%20query%20logic%20from%20linguistic%20variations%20for%0Aenhanced%20debugging%20capabilities%3B%20and%202%29%20an%20evaluation%20framework%20that%0Adifferentiates%20knowledge%20gaps%20from%20robustness%20and%20enables%20the%20identification%20of%0Adefective%20modules.%20Our%20empirical%20results%20underscore%20the%20limitations%20of%20current%0Areference-free%20evaluation%20approaches%20and%20the%20reliability%20of%20GRAMMAR%20to%0Aaccurately%20identify%20model%20vulnerabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19232v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAMMAR%253A%2520Grounded%2520and%2520Modular%2520Methodology%2520for%2520Assessment%2520of%250A%2520%2520Domain-Specific%2520Retrieval-Augmented%2520Language%2520Model%26entry.906535625%3DXinzhe%2520Li%2520and%2520Ming%2520Liu%2520and%2520Shang%2520Gao%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520Generation%2520%2528RAG%2529%2520systems%2520have%2520been%2520actively%2520studied%2520and%250Adeployed%2520across%2520various%2520industries%2520to%2520query%2520on%2520domain-specific%2520knowledge%2520base.%250AHowever%252C%2520evaluating%2520these%2520systems%2520presents%2520unique%2520challenges%2520due%2520to%2520the%250Ascarcity%2520of%2520domain-specific%2520queries%2520and%2520corresponding%2520ground%2520truths%252C%2520as%2520well%2520as%250Aa%2520lack%2520of%2520systematic%2520approaches%2520to%2520diagnosing%2520the%2520cause%2520of%2520failure%2520cases%2520--%250Awhether%2520they%2520stem%2520from%2520knowledge%2520deficits%2520or%2520issues%2520related%2520to%2520system%250Arobustness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520GRAMMAR%2520%2528GRounded%2520And%250AModular%2520Methodology%2520for%2520Assessment%2520of%2520RAG%2529%252C%2520an%2520evaluation%2520framework%2520comprising%250Atwo%2520key%2520elements%253A%25201%2529%2520a%2520data%2520generation%2520process%2520that%2520leverages%2520relational%250Adatabases%2520and%2520LLMs%2520to%2520efficiently%2520produce%2520scalable%2520query-answer%2520pairs.%2520This%250Amethod%2520facilitates%2520the%2520separation%2520of%2520query%2520logic%2520from%2520linguistic%2520variations%2520for%250Aenhanced%2520debugging%2520capabilities%253B%2520and%25202%2529%2520an%2520evaluation%2520framework%2520that%250Adifferentiates%2520knowledge%2520gaps%2520from%2520robustness%2520and%2520enables%2520the%2520identification%2520of%250Adefective%2520modules.%2520Our%2520empirical%2520results%2520underscore%2520the%2520limitations%2520of%2520current%250Areference-free%2520evaluation%2520approaches%2520and%2520the%2520reliability%2520of%2520GRAMMAR%2520to%250Aaccurately%2520identify%2520model%2520vulnerabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19232v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Domain-Specific%20Retrieval-Augmented%20Language%20Model&entry.906535625=Xinzhe%20Li%20and%20Ming%20Liu%20and%20Shang%20Gao&entry.1292438233=%20%20Retrieval-augmented%20Generation%20%28RAG%29%20systems%20have%20been%20actively%20studied%20and%0Adeployed%20across%20various%20industries%20to%20query%20on%20domain-specific%20knowledge%20base.%0AHowever%2C%20evaluating%20these%20systems%20presents%20unique%20challenges%20due%20to%20the%0Ascarcity%20of%20domain-specific%20queries%20and%20corresponding%20ground%20truths%2C%20as%20well%20as%0Aa%20lack%20of%20systematic%20approaches%20to%20diagnosing%20the%20cause%20of%20failure%20cases%20--%0Awhether%20they%20stem%20from%20knowledge%20deficits%20or%20issues%20related%20to%20system%0Arobustness.%20To%20address%20these%20challenges%2C%20we%20introduce%20GRAMMAR%20%28GRounded%20And%0AModular%20Methodology%20for%20Assessment%20of%20RAG%29%2C%20an%20evaluation%20framework%20comprising%0Atwo%20key%20elements%3A%201%29%20a%20data%20generation%20process%20that%20leverages%20relational%0Adatabases%20and%20LLMs%20to%20efficiently%20produce%20scalable%20query-answer%20pairs.%20This%0Amethod%20facilitates%20the%20separation%20of%20query%20logic%20from%20linguistic%20variations%20for%0Aenhanced%20debugging%20capabilities%3B%20and%202%29%20an%20evaluation%20framework%20that%0Adifferentiates%20knowledge%20gaps%20from%20robustness%20and%20enables%20the%20identification%20of%0Adefective%20modules.%20Our%20empirical%20results%20underscore%20the%20limitations%20of%20current%0Areference-free%20evaluation%20approaches%20and%20the%20reliability%20of%20GRAMMAR%20to%0Aaccurately%20identify%20model%20vulnerabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19232v4&entry.124074799=Read"},
{"title": "More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms", "author": "Hossein Zakerinia and Amin Behjati and Christoph H. Lampert", "abstract": "  We introduce a new framework for studying meta-learning methods using\nPAC-Bayesian theory. Its main advantage over previous work is that it allows\nfor more flexibility in how the transfer of knowledge between tasks is\nrealized. For previous approaches, this could only happen indirectly, by means\nof learning prior distributions over models. In contrast, the new\ngeneralization bounds that we prove express the process of meta-learning much\nmore directly as learning the learning algorithm that should be used for future\ntasks. The flexibility of our framework makes it suitable to analyze a wide\nrange of meta-learning mechanisms and even design new mechanisms. Other than\nour theoretical contributions we also show empirically that our framework\nimproves the prediction quality in practical meta-learning mechanisms.\n", "link": "http://arxiv.org/abs/2402.04054v2", "date": "2024-05-29", "relevancy": 1.5099, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5435}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Flexible%20PAC-Bayesian%20Meta-Learning%20by%20Learning%20Learning%20Algorithms&body=Title%3A%20More%20Flexible%20PAC-Bayesian%20Meta-Learning%20by%20Learning%20Learning%20Algorithms%0AAuthor%3A%20Hossein%20Zakerinia%20and%20Amin%20Behjati%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20framework%20for%20studying%20meta-learning%20methods%20using%0APAC-Bayesian%20theory.%20Its%20main%20advantage%20over%20previous%20work%20is%20that%20it%20allows%0Afor%20more%20flexibility%20in%20how%20the%20transfer%20of%20knowledge%20between%20tasks%20is%0Arealized.%20For%20previous%20approaches%2C%20this%20could%20only%20happen%20indirectly%2C%20by%20means%0Aof%20learning%20prior%20distributions%20over%20models.%20In%20contrast%2C%20the%20new%0Ageneralization%20bounds%20that%20we%20prove%20express%20the%20process%20of%20meta-learning%20much%0Amore%20directly%20as%20learning%20the%20learning%20algorithm%20that%20should%20be%20used%20for%20future%0Atasks.%20The%20flexibility%20of%20our%20framework%20makes%20it%20suitable%20to%20analyze%20a%20wide%0Arange%20of%20meta-learning%20mechanisms%20and%20even%20design%20new%20mechanisms.%20Other%20than%0Aour%20theoretical%20contributions%20we%20also%20show%20empirically%20that%20our%20framework%0Aimproves%20the%20prediction%20quality%20in%20practical%20meta-learning%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Flexible%2520PAC-Bayesian%2520Meta-Learning%2520by%2520Learning%2520Learning%2520Algorithms%26entry.906535625%3DHossein%2520Zakerinia%2520and%2520Amin%2520Behjati%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520framework%2520for%2520studying%2520meta-learning%2520methods%2520using%250APAC-Bayesian%2520theory.%2520Its%2520main%2520advantage%2520over%2520previous%2520work%2520is%2520that%2520it%2520allows%250Afor%2520more%2520flexibility%2520in%2520how%2520the%2520transfer%2520of%2520knowledge%2520between%2520tasks%2520is%250Arealized.%2520For%2520previous%2520approaches%252C%2520this%2520could%2520only%2520happen%2520indirectly%252C%2520by%2520means%250Aof%2520learning%2520prior%2520distributions%2520over%2520models.%2520In%2520contrast%252C%2520the%2520new%250Ageneralization%2520bounds%2520that%2520we%2520prove%2520express%2520the%2520process%2520of%2520meta-learning%2520much%250Amore%2520directly%2520as%2520learning%2520the%2520learning%2520algorithm%2520that%2520should%2520be%2520used%2520for%2520future%250Atasks.%2520The%2520flexibility%2520of%2520our%2520framework%2520makes%2520it%2520suitable%2520to%2520analyze%2520a%2520wide%250Arange%2520of%2520meta-learning%2520mechanisms%2520and%2520even%2520design%2520new%2520mechanisms.%2520Other%2520than%250Aour%2520theoretical%2520contributions%2520we%2520also%2520show%2520empirically%2520that%2520our%2520framework%250Aimproves%2520the%2520prediction%2520quality%2520in%2520practical%2520meta-learning%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Flexible%20PAC-Bayesian%20Meta-Learning%20by%20Learning%20Learning%20Algorithms&entry.906535625=Hossein%20Zakerinia%20and%20Amin%20Behjati%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20We%20introduce%20a%20new%20framework%20for%20studying%20meta-learning%20methods%20using%0APAC-Bayesian%20theory.%20Its%20main%20advantage%20over%20previous%20work%20is%20that%20it%20allows%0Afor%20more%20flexibility%20in%20how%20the%20transfer%20of%20knowledge%20between%20tasks%20is%0Arealized.%20For%20previous%20approaches%2C%20this%20could%20only%20happen%20indirectly%2C%20by%20means%0Aof%20learning%20prior%20distributions%20over%20models.%20In%20contrast%2C%20the%20new%0Ageneralization%20bounds%20that%20we%20prove%20express%20the%20process%20of%20meta-learning%20much%0Amore%20directly%20as%20learning%20the%20learning%20algorithm%20that%20should%20be%20used%20for%20future%0Atasks.%20The%20flexibility%20of%20our%20framework%20makes%20it%20suitable%20to%20analyze%20a%20wide%0Arange%20of%20meta-learning%20mechanisms%20and%20even%20design%20new%20mechanisms.%20Other%20than%0Aour%20theoretical%20contributions%20we%20also%20show%20empirically%20that%20our%20framework%0Aimproves%20the%20prediction%20quality%20in%20practical%20meta-learning%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04054v2&entry.124074799=Read"},
{"title": "MAGIC: Modular Auto-encoder for Generalisable Model Inversion with Bias\n  Corrections", "author": "Yihang She and Clement Atzberger and Andrew Blake and Adriano Gualandi and Srinivasan Keshav", "abstract": "  Scientists often model physical processes to understand the natural world and\nuncover the causation behind observations. Due to unavoidable simplification,\ndiscrepancies often arise between model predictions and actual observations, in\nthe form of systematic biases, whose impact varies with model completeness.\nClassical model inversion methods such as Bayesian inference or regressive\nneural networks tend either to overlook biases or make assumptions about their\nnature during data preprocessing, potentially leading to implausible results.\nInspired by recent work in inverse graphics, we replace the decoder stage of a\nstandard autoencoder with a physical model followed by a bias-correction layer.\nThis generalisable approach simultaneously inverts the model and corrects its\nbiases in an end-to-end manner without making strong assumptions about the\nnature of the biases. We demonstrate the effectiveness of our approach using\ntwo physical models from disparate domains: a complex radiative transfer model\nfrom remote sensing; and a volcanic deformation model from geodesy. Our method\nmatches or surpasses results from classical approaches without requiring biases\nto be explicitly filtered out, suggesting an effective pathway for\nunderstanding the causation of various physical processes.\n", "link": "http://arxiv.org/abs/2405.18953v1", "date": "2024-05-29", "relevancy": 1.6194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5939}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGIC%3A%20Modular%20Auto-encoder%20for%20Generalisable%20Model%20Inversion%20with%20Bias%0A%20%20Corrections&body=Title%3A%20MAGIC%3A%20Modular%20Auto-encoder%20for%20Generalisable%20Model%20Inversion%20with%20Bias%0A%20%20Corrections%0AAuthor%3A%20Yihang%20She%20and%20Clement%20Atzberger%20and%20Andrew%20Blake%20and%20Adriano%20Gualandi%20and%20Srinivasan%20Keshav%0AAbstract%3A%20%20%20Scientists%20often%20model%20physical%20processes%20to%20understand%20the%20natural%20world%20and%0Auncover%20the%20causation%20behind%20observations.%20Due%20to%20unavoidable%20simplification%2C%0Adiscrepancies%20often%20arise%20between%20model%20predictions%20and%20actual%20observations%2C%20in%0Athe%20form%20of%20systematic%20biases%2C%20whose%20impact%20varies%20with%20model%20completeness.%0AClassical%20model%20inversion%20methods%20such%20as%20Bayesian%20inference%20or%20regressive%0Aneural%20networks%20tend%20either%20to%20overlook%20biases%20or%20make%20assumptions%20about%20their%0Anature%20during%20data%20preprocessing%2C%20potentially%20leading%20to%20implausible%20results.%0AInspired%20by%20recent%20work%20in%20inverse%20graphics%2C%20we%20replace%20the%20decoder%20stage%20of%20a%0Astandard%20autoencoder%20with%20a%20physical%20model%20followed%20by%20a%20bias-correction%20layer.%0AThis%20generalisable%20approach%20simultaneously%20inverts%20the%20model%20and%20corrects%20its%0Abiases%20in%20an%20end-to-end%20manner%20without%20making%20strong%20assumptions%20about%20the%0Anature%20of%20the%20biases.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20using%0Atwo%20physical%20models%20from%20disparate%20domains%3A%20a%20complex%20radiative%20transfer%20model%0Afrom%20remote%20sensing%3B%20and%20a%20volcanic%20deformation%20model%20from%20geodesy.%20Our%20method%0Amatches%20or%20surpasses%20results%20from%20classical%20approaches%20without%20requiring%20biases%0Ato%20be%20explicitly%20filtered%20out%2C%20suggesting%20an%20effective%20pathway%20for%0Aunderstanding%20the%20causation%20of%20various%20physical%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGIC%253A%2520Modular%2520Auto-encoder%2520for%2520Generalisable%2520Model%2520Inversion%2520with%2520Bias%250A%2520%2520Corrections%26entry.906535625%3DYihang%2520She%2520and%2520Clement%2520Atzberger%2520and%2520Andrew%2520Blake%2520and%2520Adriano%2520Gualandi%2520and%2520Srinivasan%2520Keshav%26entry.1292438233%3D%2520%2520Scientists%2520often%2520model%2520physical%2520processes%2520to%2520understand%2520the%2520natural%2520world%2520and%250Auncover%2520the%2520causation%2520behind%2520observations.%2520Due%2520to%2520unavoidable%2520simplification%252C%250Adiscrepancies%2520often%2520arise%2520between%2520model%2520predictions%2520and%2520actual%2520observations%252C%2520in%250Athe%2520form%2520of%2520systematic%2520biases%252C%2520whose%2520impact%2520varies%2520with%2520model%2520completeness.%250AClassical%2520model%2520inversion%2520methods%2520such%2520as%2520Bayesian%2520inference%2520or%2520regressive%250Aneural%2520networks%2520tend%2520either%2520to%2520overlook%2520biases%2520or%2520make%2520assumptions%2520about%2520their%250Anature%2520during%2520data%2520preprocessing%252C%2520potentially%2520leading%2520to%2520implausible%2520results.%250AInspired%2520by%2520recent%2520work%2520in%2520inverse%2520graphics%252C%2520we%2520replace%2520the%2520decoder%2520stage%2520of%2520a%250Astandard%2520autoencoder%2520with%2520a%2520physical%2520model%2520followed%2520by%2520a%2520bias-correction%2520layer.%250AThis%2520generalisable%2520approach%2520simultaneously%2520inverts%2520the%2520model%2520and%2520corrects%2520its%250Abiases%2520in%2520an%2520end-to-end%2520manner%2520without%2520making%2520strong%2520assumptions%2520about%2520the%250Anature%2520of%2520the%2520biases.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520using%250Atwo%2520physical%2520models%2520from%2520disparate%2520domains%253A%2520a%2520complex%2520radiative%2520transfer%2520model%250Afrom%2520remote%2520sensing%253B%2520and%2520a%2520volcanic%2520deformation%2520model%2520from%2520geodesy.%2520Our%2520method%250Amatches%2520or%2520surpasses%2520results%2520from%2520classical%2520approaches%2520without%2520requiring%2520biases%250Ato%2520be%2520explicitly%2520filtered%2520out%252C%2520suggesting%2520an%2520effective%2520pathway%2520for%250Aunderstanding%2520the%2520causation%2520of%2520various%2520physical%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGIC%3A%20Modular%20Auto-encoder%20for%20Generalisable%20Model%20Inversion%20with%20Bias%0A%20%20Corrections&entry.906535625=Yihang%20She%20and%20Clement%20Atzberger%20and%20Andrew%20Blake%20and%20Adriano%20Gualandi%20and%20Srinivasan%20Keshav&entry.1292438233=%20%20Scientists%20often%20model%20physical%20processes%20to%20understand%20the%20natural%20world%20and%0Auncover%20the%20causation%20behind%20observations.%20Due%20to%20unavoidable%20simplification%2C%0Adiscrepancies%20often%20arise%20between%20model%20predictions%20and%20actual%20observations%2C%20in%0Athe%20form%20of%20systematic%20biases%2C%20whose%20impact%20varies%20with%20model%20completeness.%0AClassical%20model%20inversion%20methods%20such%20as%20Bayesian%20inference%20or%20regressive%0Aneural%20networks%20tend%20either%20to%20overlook%20biases%20or%20make%20assumptions%20about%20their%0Anature%20during%20data%20preprocessing%2C%20potentially%20leading%20to%20implausible%20results.%0AInspired%20by%20recent%20work%20in%20inverse%20graphics%2C%20we%20replace%20the%20decoder%20stage%20of%20a%0Astandard%20autoencoder%20with%20a%20physical%20model%20followed%20by%20a%20bias-correction%20layer.%0AThis%20generalisable%20approach%20simultaneously%20inverts%20the%20model%20and%20corrects%20its%0Abiases%20in%20an%20end-to-end%20manner%20without%20making%20strong%20assumptions%20about%20the%0Anature%20of%20the%20biases.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20using%0Atwo%20physical%20models%20from%20disparate%20domains%3A%20a%20complex%20radiative%20transfer%20model%0Afrom%20remote%20sensing%3B%20and%20a%20volcanic%20deformation%20model%20from%20geodesy.%20Our%20method%0Amatches%20or%20surpasses%20results%20from%20classical%20approaches%20without%20requiring%20biases%0Ato%20be%20explicitly%20filtered%20out%2C%20suggesting%20an%20effective%20pathway%20for%0Aunderstanding%20the%20causation%20of%20various%20physical%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18953v1&entry.124074799=Read"},
{"title": "Online Linear Regression in Dynamic Environments via Discounting", "author": "Andrew Jacobsen and Ashok Cutkosky", "abstract": "  We develop algorithms for online linear regression which achieve optimal\nstatic and dynamic regret guarantees \\emph{even in the complete absence of\nprior knowledge}. We present a novel analysis showing that a discounted variant\nof the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form\n$R_{T}(\\vec{u})\\le O\\left(d\\log(T)\\vee\n\\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\right)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a\nmeasure of variability of the comparator sequence, and show that the discount\nfactor achieving this result can be learned on-the-fly. We show that this\nresult is optimal by providing a matching lower bound. We also extend our\nresults to \\emph{strongly-adaptive} guarantees which hold over every\nsub-interval $[a,b]\\subseteq[1,T]$ simultaneously.\n", "link": "http://arxiv.org/abs/2405.19175v1", "date": "2024-05-29", "relevancy": 1.4679, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.462}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Linear%20Regression%20in%20Dynamic%20Environments%20via%20Discounting&body=Title%3A%20Online%20Linear%20Regression%20in%20Dynamic%20Environments%20via%20Discounting%0AAuthor%3A%20Andrew%20Jacobsen%20and%20Ashok%20Cutkosky%0AAbstract%3A%20%20%20We%20develop%20algorithms%20for%20online%20linear%20regression%20which%20achieve%20optimal%0Astatic%20and%20dynamic%20regret%20guarantees%20%5Cemph%7Beven%20in%20the%20complete%20absence%20of%0Aprior%20knowledge%7D.%20We%20present%20a%20novel%20analysis%20showing%20that%20a%20discounted%20variant%0Aof%20the%20Vovk-Azoury-Warmuth%20forecaster%20achieves%20dynamic%20regret%20of%20the%20form%0A%24R_%7BT%7D%28%5Cvec%7Bu%7D%29%5Cle%20O%5Cleft%28d%5Clog%28T%29%5Cvee%0A%5Csqrt%7BdP_%7BT%7D%5E%7B%5Cgamma%7D%28%5Cvec%7Bu%7D%29T%7D%5Cright%29%24%2C%20where%20%24P_%7BT%7D%5E%7B%5Cgamma%7D%28%5Cvec%7Bu%7D%29%24%20is%20a%0Ameasure%20of%20variability%20of%20the%20comparator%20sequence%2C%20and%20show%20that%20the%20discount%0Afactor%20achieving%20this%20result%20can%20be%20learned%20on-the-fly.%20We%20show%20that%20this%0Aresult%20is%20optimal%20by%20providing%20a%20matching%20lower%20bound.%20We%20also%20extend%20our%0Aresults%20to%20%5Cemph%7Bstrongly-adaptive%7D%20guarantees%20which%20hold%20over%20every%0Asub-interval%20%24%5Ba%2Cb%5D%5Csubseteq%5B1%2CT%5D%24%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Linear%2520Regression%2520in%2520Dynamic%2520Environments%2520via%2520Discounting%26entry.906535625%3DAndrew%2520Jacobsen%2520and%2520Ashok%2520Cutkosky%26entry.1292438233%3D%2520%2520We%2520develop%2520algorithms%2520for%2520online%2520linear%2520regression%2520which%2520achieve%2520optimal%250Astatic%2520and%2520dynamic%2520regret%2520guarantees%2520%255Cemph%257Beven%2520in%2520the%2520complete%2520absence%2520of%250Aprior%2520knowledge%257D.%2520We%2520present%2520a%2520novel%2520analysis%2520showing%2520that%2520a%2520discounted%2520variant%250Aof%2520the%2520Vovk-Azoury-Warmuth%2520forecaster%2520achieves%2520dynamic%2520regret%2520of%2520the%2520form%250A%2524R_%257BT%257D%2528%255Cvec%257Bu%257D%2529%255Cle%2520O%255Cleft%2528d%255Clog%2528T%2529%255Cvee%250A%255Csqrt%257BdP_%257BT%257D%255E%257B%255Cgamma%257D%2528%255Cvec%257Bu%257D%2529T%257D%255Cright%2529%2524%252C%2520where%2520%2524P_%257BT%257D%255E%257B%255Cgamma%257D%2528%255Cvec%257Bu%257D%2529%2524%2520is%2520a%250Ameasure%2520of%2520variability%2520of%2520the%2520comparator%2520sequence%252C%2520and%2520show%2520that%2520the%2520discount%250Afactor%2520achieving%2520this%2520result%2520can%2520be%2520learned%2520on-the-fly.%2520We%2520show%2520that%2520this%250Aresult%2520is%2520optimal%2520by%2520providing%2520a%2520matching%2520lower%2520bound.%2520We%2520also%2520extend%2520our%250Aresults%2520to%2520%255Cemph%257Bstrongly-adaptive%257D%2520guarantees%2520which%2520hold%2520over%2520every%250Asub-interval%2520%2524%255Ba%252Cb%255D%255Csubseteq%255B1%252CT%255D%2524%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Linear%20Regression%20in%20Dynamic%20Environments%20via%20Discounting&entry.906535625=Andrew%20Jacobsen%20and%20Ashok%20Cutkosky&entry.1292438233=%20%20We%20develop%20algorithms%20for%20online%20linear%20regression%20which%20achieve%20optimal%0Astatic%20and%20dynamic%20regret%20guarantees%20%5Cemph%7Beven%20in%20the%20complete%20absence%20of%0Aprior%20knowledge%7D.%20We%20present%20a%20novel%20analysis%20showing%20that%20a%20discounted%20variant%0Aof%20the%20Vovk-Azoury-Warmuth%20forecaster%20achieves%20dynamic%20regret%20of%20the%20form%0A%24R_%7BT%7D%28%5Cvec%7Bu%7D%29%5Cle%20O%5Cleft%28d%5Clog%28T%29%5Cvee%0A%5Csqrt%7BdP_%7BT%7D%5E%7B%5Cgamma%7D%28%5Cvec%7Bu%7D%29T%7D%5Cright%29%24%2C%20where%20%24P_%7BT%7D%5E%7B%5Cgamma%7D%28%5Cvec%7Bu%7D%29%24%20is%20a%0Ameasure%20of%20variability%20of%20the%20comparator%20sequence%2C%20and%20show%20that%20the%20discount%0Afactor%20achieving%20this%20result%20can%20be%20learned%20on-the-fly.%20We%20show%20that%20this%0Aresult%20is%20optimal%20by%20providing%20a%20matching%20lower%20bound.%20We%20also%20extend%20our%0Aresults%20to%20%5Cemph%7Bstrongly-adaptive%7D%20guarantees%20which%20hold%20over%20every%0Asub-interval%20%24%5Ba%2Cb%5D%5Csubseteq%5B1%2CT%5D%24%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19175v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


