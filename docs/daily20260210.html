<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260208.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars", "author": "Kelian Baert and Mae Younes and Francois Bourel and Marc Christie and Adnane Boukhayma", "abstract": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.", "link": "http://arxiv.org/abs/2512.09162v2", "date": "2026-02-09", "relevancy": 3.5379, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7369}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7369}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTAvatar%3A%20Bridging%20Gaussian%20Splatting%20and%20Texture%20Mapping%20for%20Relightable%20and%20Editable%20Gaussian%20Avatars&body=Title%3A%20GTAvatar%3A%20Bridging%20Gaussian%20Splatting%20and%20Texture%20Mapping%20for%20Relightable%20and%20Editable%20Gaussian%20Avatars%0AAuthor%3A%20Kelian%20Baert%20and%20Mae%20Younes%20and%20Francois%20Bourel%20and%20Marc%20Christie%20and%20Adnane%20Boukhayma%0AAbstract%3A%20Recent%20advancements%20in%20Gaussian%20Splatting%20have%20enabled%20increasingly%20accurate%20reconstruction%20of%20photorealistic%20head%20avatars%2C%20opening%20the%20door%20to%20numerous%20applications%20in%20visual%20effects%2C%20videoconferencing%2C%20and%20virtual%20reality.%20This%2C%20however%2C%20comes%20with%20the%20lack%20of%20intuitive%20editability%20offered%20by%20traditional%20triangle%20mesh-based%20methods.%20In%20contrast%2C%20we%20propose%20a%20method%20that%20combines%20the%20accuracy%20and%20fidelity%20of%202D%20Gaussian%20Splatting%20with%20the%20intuitiveness%20of%20UV%20texture%20mapping.%20By%20embedding%20each%20canonical%20Gaussian%20primitive%27s%20local%20frame%20into%20a%20patch%20in%20the%20UV%20space%20of%20a%20template%20mesh%20in%20a%20computationally%20efficient%20manner%2C%20we%20reconstruct%20continuous%20editable%20material%20head%20textures%20from%20a%20single%20monocular%20video%20on%20a%20conventional%20UV%20domain.%20Furthermore%2C%20we%20leverage%20an%20efficient%20physically%20based%20reflectance%20model%20to%20enable%20relighting%20and%20editing%20of%20these%20intrinsic%20material%20maps.%20Through%20extensive%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20demonstrate%20the%20accuracy%20of%20our%20reconstructions%2C%20the%20quality%20of%20our%20relighting%20results%2C%20and%20the%20ability%20to%20provide%20intuitive%20controls%20for%20modifying%20an%20avatar%27s%20appearance%20and%20geometry%20via%20texture%20mapping%20without%20additional%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTAvatar%253A%2520Bridging%2520Gaussian%2520Splatting%2520and%2520Texture%2520Mapping%2520for%2520Relightable%2520and%2520Editable%2520Gaussian%2520Avatars%26entry.906535625%3DKelian%2520Baert%2520and%2520Mae%2520Younes%2520and%2520Francois%2520Bourel%2520and%2520Marc%2520Christie%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3DRecent%2520advancements%2520in%2520Gaussian%2520Splatting%2520have%2520enabled%2520increasingly%2520accurate%2520reconstruction%2520of%2520photorealistic%2520head%2520avatars%252C%2520opening%2520the%2520door%2520to%2520numerous%2520applications%2520in%2520visual%2520effects%252C%2520videoconferencing%252C%2520and%2520virtual%2520reality.%2520This%252C%2520however%252C%2520comes%2520with%2520the%2520lack%2520of%2520intuitive%2520editability%2520offered%2520by%2520traditional%2520triangle%2520mesh-based%2520methods.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520method%2520that%2520combines%2520the%2520accuracy%2520and%2520fidelity%2520of%25202D%2520Gaussian%2520Splatting%2520with%2520the%2520intuitiveness%2520of%2520UV%2520texture%2520mapping.%2520By%2520embedding%2520each%2520canonical%2520Gaussian%2520primitive%2527s%2520local%2520frame%2520into%2520a%2520patch%2520in%2520the%2520UV%2520space%2520of%2520a%2520template%2520mesh%2520in%2520a%2520computationally%2520efficient%2520manner%252C%2520we%2520reconstruct%2520continuous%2520editable%2520material%2520head%2520textures%2520from%2520a%2520single%2520monocular%2520video%2520on%2520a%2520conventional%2520UV%2520domain.%2520Furthermore%252C%2520we%2520leverage%2520an%2520efficient%2520physically%2520based%2520reflectance%2520model%2520to%2520enable%2520relighting%2520and%2520editing%2520of%2520these%2520intrinsic%2520material%2520maps.%2520Through%2520extensive%2520comparisons%2520with%2520state-of-the-art%2520methods%252C%2520we%2520demonstrate%2520the%2520accuracy%2520of%2520our%2520reconstructions%252C%2520the%2520quality%2520of%2520our%2520relighting%2520results%252C%2520and%2520the%2520ability%2520to%2520provide%2520intuitive%2520controls%2520for%2520modifying%2520an%2520avatar%2527s%2520appearance%2520and%2520geometry%2520via%2520texture%2520mapping%2520without%2520additional%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTAvatar%3A%20Bridging%20Gaussian%20Splatting%20and%20Texture%20Mapping%20for%20Relightable%20and%20Editable%20Gaussian%20Avatars&entry.906535625=Kelian%20Baert%20and%20Mae%20Younes%20and%20Francois%20Bourel%20and%20Marc%20Christie%20and%20Adnane%20Boukhayma&entry.1292438233=Recent%20advancements%20in%20Gaussian%20Splatting%20have%20enabled%20increasingly%20accurate%20reconstruction%20of%20photorealistic%20head%20avatars%2C%20opening%20the%20door%20to%20numerous%20applications%20in%20visual%20effects%2C%20videoconferencing%2C%20and%20virtual%20reality.%20This%2C%20however%2C%20comes%20with%20the%20lack%20of%20intuitive%20editability%20offered%20by%20traditional%20triangle%20mesh-based%20methods.%20In%20contrast%2C%20we%20propose%20a%20method%20that%20combines%20the%20accuracy%20and%20fidelity%20of%202D%20Gaussian%20Splatting%20with%20the%20intuitiveness%20of%20UV%20texture%20mapping.%20By%20embedding%20each%20canonical%20Gaussian%20primitive%27s%20local%20frame%20into%20a%20patch%20in%20the%20UV%20space%20of%20a%20template%20mesh%20in%20a%20computationally%20efficient%20manner%2C%20we%20reconstruct%20continuous%20editable%20material%20head%20textures%20from%20a%20single%20monocular%20video%20on%20a%20conventional%20UV%20domain.%20Furthermore%2C%20we%20leverage%20an%20efficient%20physically%20based%20reflectance%20model%20to%20enable%20relighting%20and%20editing%20of%20these%20intrinsic%20material%20maps.%20Through%20extensive%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20demonstrate%20the%20accuracy%20of%20our%20reconstructions%2C%20the%20quality%20of%20our%20relighting%20results%2C%20and%20the%20ability%20to%20provide%20intuitive%20controls%20for%20modifying%20an%20avatar%27s%20appearance%20and%20geometry%20via%20texture%20mapping%20without%20additional%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2512.09162v2&entry.124074799=Read"},
{"title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit", "author": "Zhendong Wang and Cihan Ruan and Jingchuan Xiao and Chuqing Shi and Wei Jiang and Wei Wang and Wenjie Liu and Nam Ling", "abstract": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.", "link": "http://arxiv.org/abs/2602.08909v1", "date": "2026-02-09", "relevancy": 3.2834, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6425}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Converged%203D%20Gaussian%20Splatting%20Solutions%3A%20Density%20Effects%20and%20Prediction%20Limit&body=Title%3A%20Analysis%20of%20Converged%203D%20Gaussian%20Splatting%20Solutions%3A%20Density%20Effects%20and%20Prediction%20Limit%0AAuthor%3A%20Zhendong%20Wang%20and%20Cihan%20Ruan%20and%20Jingchuan%20Xiao%20and%20Chuqing%20Shi%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Wenjie%20Liu%20and%20Nam%20Ling%0AAbstract%3A%20We%20investigate%20what%20structure%20emerges%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20solutions%20from%20standard%20multi-view%20optimization.%20We%20term%20these%20Rendering-Optimal%20References%20%28RORs%29%20and%20analyze%20their%20statistical%20properties%2C%20revealing%20stable%20patterns%3A%20mixture-structured%20scales%20and%20bimodal%20radiance%20across%20diverse%20scenes.%20To%20understand%20what%20determines%20these%20parameters%2C%20we%20apply%20learnability%20probes%20by%20training%20predictors%20to%20reconstruct%20RORs%20from%20point%20clouds%20without%20rendering%20supervision.%20Our%20analysis%20uncovers%20fundamental%20density-stratification.%20Dense%20regions%20exhibit%20geometry-correlated%20parameters%20amenable%20to%20render-free%20prediction%2C%20while%20sparse%20regions%20show%20systematic%20failure%20across%20architectures.%20We%20formalize%20this%20through%20variance%20decomposition%2C%20demonstrating%20that%20visibility%20heterogeneity%20creates%20covariance-dominated%20coupling%20between%20geometric%20and%20appearance%20parameters%20in%20sparse%20regions.%20This%20reveals%20the%20dual%20character%20of%20RORs%3A%20geometric%20primitives%20where%20point%20clouds%20suffice%2C%20and%20view%20synthesis%20primitives%20where%20multi-view%20constraints%20are%20essential.%20We%20provide%20density-aware%20strategies%20that%20improve%20training%20robustness%20and%20discuss%20architectural%20implications%20for%20systems%20that%20adaptively%20balance%20feed-forward%20prediction%20and%20rendering-based%20refinement.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Converged%25203D%2520Gaussian%2520Splatting%2520Solutions%253A%2520Density%2520Effects%2520and%2520Prediction%2520Limit%26entry.906535625%3DZhendong%2520Wang%2520and%2520Cihan%2520Ruan%2520and%2520Jingchuan%2520Xiao%2520and%2520Chuqing%2520Shi%2520and%2520Wei%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Wenjie%2520Liu%2520and%2520Nam%2520Ling%26entry.1292438233%3DWe%2520investigate%2520what%2520structure%2520emerges%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520solutions%2520from%2520standard%2520multi-view%2520optimization.%2520We%2520term%2520these%2520Rendering-Optimal%2520References%2520%2528RORs%2529%2520and%2520analyze%2520their%2520statistical%2520properties%252C%2520revealing%2520stable%2520patterns%253A%2520mixture-structured%2520scales%2520and%2520bimodal%2520radiance%2520across%2520diverse%2520scenes.%2520To%2520understand%2520what%2520determines%2520these%2520parameters%252C%2520we%2520apply%2520learnability%2520probes%2520by%2520training%2520predictors%2520to%2520reconstruct%2520RORs%2520from%2520point%2520clouds%2520without%2520rendering%2520supervision.%2520Our%2520analysis%2520uncovers%2520fundamental%2520density-stratification.%2520Dense%2520regions%2520exhibit%2520geometry-correlated%2520parameters%2520amenable%2520to%2520render-free%2520prediction%252C%2520while%2520sparse%2520regions%2520show%2520systematic%2520failure%2520across%2520architectures.%2520We%2520formalize%2520this%2520through%2520variance%2520decomposition%252C%2520demonstrating%2520that%2520visibility%2520heterogeneity%2520creates%2520covariance-dominated%2520coupling%2520between%2520geometric%2520and%2520appearance%2520parameters%2520in%2520sparse%2520regions.%2520This%2520reveals%2520the%2520dual%2520character%2520of%2520RORs%253A%2520geometric%2520primitives%2520where%2520point%2520clouds%2520suffice%252C%2520and%2520view%2520synthesis%2520primitives%2520where%2520multi-view%2520constraints%2520are%2520essential.%2520We%2520provide%2520density-aware%2520strategies%2520that%2520improve%2520training%2520robustness%2520and%2520discuss%2520architectural%2520implications%2520for%2520systems%2520that%2520adaptively%2520balance%2520feed-forward%2520prediction%2520and%2520rendering-based%2520refinement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Converged%203D%20Gaussian%20Splatting%20Solutions%3A%20Density%20Effects%20and%20Prediction%20Limit&entry.906535625=Zhendong%20Wang%20and%20Cihan%20Ruan%20and%20Jingchuan%20Xiao%20and%20Chuqing%20Shi%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Wenjie%20Liu%20and%20Nam%20Ling&entry.1292438233=We%20investigate%20what%20structure%20emerges%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20solutions%20from%20standard%20multi-view%20optimization.%20We%20term%20these%20Rendering-Optimal%20References%20%28RORs%29%20and%20analyze%20their%20statistical%20properties%2C%20revealing%20stable%20patterns%3A%20mixture-structured%20scales%20and%20bimodal%20radiance%20across%20diverse%20scenes.%20To%20understand%20what%20determines%20these%20parameters%2C%20we%20apply%20learnability%20probes%20by%20training%20predictors%20to%20reconstruct%20RORs%20from%20point%20clouds%20without%20rendering%20supervision.%20Our%20analysis%20uncovers%20fundamental%20density-stratification.%20Dense%20regions%20exhibit%20geometry-correlated%20parameters%20amenable%20to%20render-free%20prediction%2C%20while%20sparse%20regions%20show%20systematic%20failure%20across%20architectures.%20We%20formalize%20this%20through%20variance%20decomposition%2C%20demonstrating%20that%20visibility%20heterogeneity%20creates%20covariance-dominated%20coupling%20between%20geometric%20and%20appearance%20parameters%20in%20sparse%20regions.%20This%20reveals%20the%20dual%20character%20of%20RORs%3A%20geometric%20primitives%20where%20point%20clouds%20suffice%2C%20and%20view%20synthesis%20primitives%20where%20multi-view%20constraints%20are%20essential.%20We%20provide%20density-aware%20strategies%20that%20improve%20training%20robustness%20and%20discuss%20architectural%20implications%20for%20systems%20that%20adaptively%20balance%20feed-forward%20prediction%20and%20rendering-based%20refinement.&entry.1838667208=http%3A//arxiv.org/abs/2602.08909v1&entry.124074799=Read"},
{"title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "author": "Yueyan Li and Chenggong Zhao and Zeyuan Zang and Caixia Yuan and Xiaojie Wang", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the \"what\" and \"where\" pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.", "link": "http://arxiv.org/abs/2509.19191v2", "date": "2026-02-09", "relevancy": 3.2116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%20Vision-Language%20Models&body=Title%3A%20Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%20Vision-Language%20Models%0AAuthor%3A%20Yueyan%20Li%20and%20Chenggong%20Zhao%20and%20Zeyuan%20Zang%20and%20Caixia%20Yuan%20and%20Xiaojie%20Wang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%20a%20variety%20of%20real-world%20tasks.%20However%2C%20existing%20VLMs%20typically%20process%20visual%20information%20by%20serializing%20images%2C%20a%20method%20that%20diverges%20significantly%20from%20the%20parallel%20nature%20of%20human%20vision.%20Moreover%2C%20their%20opaque%20internal%20mechanisms%20hinder%20both%20deeper%20understanding%20and%20architectural%20innovation.%20Inspired%20by%20the%20dual-stream%20hypothesis%20of%20human%20vision%2C%20which%20distinguishes%20the%20%22what%22%20and%20%22where%22%20pathways%2C%20we%20deconstruct%20the%20visual%20processing%20in%20VLMs%20into%20object%20recognition%20and%20spatial%20perception%20for%20separate%20study.%20For%20object%20recognition%2C%20we%20convert%20images%20into%20text%20token%20maps%20and%20find%20that%20the%20model%27s%20perception%20of%20image%20content%20unfolds%20as%20a%20two-stage%20process%20from%20shallow%20to%20deep%20layers%2C%20beginning%20with%20attribute%20recognition%20and%20culminating%20in%20semantic%20disambiguation.%20For%20spatial%20perception%2C%20we%20theoretically%20derive%20and%20empirically%20verify%20the%20geometric%20structure%20underlying%20the%20positional%20representation%20in%20VLMs.%20Based%20on%20these%20findings%2C%20we%20introduce%20an%20instruction-agnostic%20token%20compression%20algorithm%20based%20on%20a%20plug-and-play%20visual%20decoder%20to%20improve%20decoding%20efficiency%2C%20and%20a%20RoPE%20scaling%20technique%20to%20enhance%20spatial%20reasoning.%20Through%20rigorous%20experiments%2C%20our%20work%20validates%20these%20analyses%2C%20offering%20a%20deeper%20understanding%20of%20VLM%20internals%20and%20providing%20clear%20principles%20for%20designing%20more%20capable%20future%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520Images%2520Like%2520Texts%253A%2520Sequential%2520Image%2520Understanding%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYueyan%2520Li%2520and%2520Chenggong%2520Zhao%2520and%2520Zeyuan%2520Zang%2520and%2520Caixia%2520Yuan%2520and%2520Xiaojie%2520Wang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520a%2520variety%2520of%2520real-world%2520tasks.%2520However%252C%2520existing%2520VLMs%2520typically%2520process%2520visual%2520information%2520by%2520serializing%2520images%252C%2520a%2520method%2520that%2520diverges%2520significantly%2520from%2520the%2520parallel%2520nature%2520of%2520human%2520vision.%2520Moreover%252C%2520their%2520opaque%2520internal%2520mechanisms%2520hinder%2520both%2520deeper%2520understanding%2520and%2520architectural%2520innovation.%2520Inspired%2520by%2520the%2520dual-stream%2520hypothesis%2520of%2520human%2520vision%252C%2520which%2520distinguishes%2520the%2520%2522what%2522%2520and%2520%2522where%2522%2520pathways%252C%2520we%2520deconstruct%2520the%2520visual%2520processing%2520in%2520VLMs%2520into%2520object%2520recognition%2520and%2520spatial%2520perception%2520for%2520separate%2520study.%2520For%2520object%2520recognition%252C%2520we%2520convert%2520images%2520into%2520text%2520token%2520maps%2520and%2520find%2520that%2520the%2520model%2527s%2520perception%2520of%2520image%2520content%2520unfolds%2520as%2520a%2520two-stage%2520process%2520from%2520shallow%2520to%2520deep%2520layers%252C%2520beginning%2520with%2520attribute%2520recognition%2520and%2520culminating%2520in%2520semantic%2520disambiguation.%2520For%2520spatial%2520perception%252C%2520we%2520theoretically%2520derive%2520and%2520empirically%2520verify%2520the%2520geometric%2520structure%2520underlying%2520the%2520positional%2520representation%2520in%2520VLMs.%2520Based%2520on%2520these%2520findings%252C%2520we%2520introduce%2520an%2520instruction-agnostic%2520token%2520compression%2520algorithm%2520based%2520on%2520a%2520plug-and-play%2520visual%2520decoder%2520to%2520improve%2520decoding%2520efficiency%252C%2520and%2520a%2520RoPE%2520scaling%2520technique%2520to%2520enhance%2520spatial%2520reasoning.%2520Through%2520rigorous%2520experiments%252C%2520our%2520work%2520validates%2520these%2520analyses%252C%2520offering%2520a%2520deeper%2520understanding%2520of%2520VLM%2520internals%2520and%2520providing%2520clear%2520principles%2520for%2520designing%2520more%2520capable%2520future%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%20Vision-Language%20Models&entry.906535625=Yueyan%20Li%20and%20Chenggong%20Zhao%20and%20Zeyuan%20Zang%20and%20Caixia%20Yuan%20and%20Xiaojie%20Wang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%20a%20variety%20of%20real-world%20tasks.%20However%2C%20existing%20VLMs%20typically%20process%20visual%20information%20by%20serializing%20images%2C%20a%20method%20that%20diverges%20significantly%20from%20the%20parallel%20nature%20of%20human%20vision.%20Moreover%2C%20their%20opaque%20internal%20mechanisms%20hinder%20both%20deeper%20understanding%20and%20architectural%20innovation.%20Inspired%20by%20the%20dual-stream%20hypothesis%20of%20human%20vision%2C%20which%20distinguishes%20the%20%22what%22%20and%20%22where%22%20pathways%2C%20we%20deconstruct%20the%20visual%20processing%20in%20VLMs%20into%20object%20recognition%20and%20spatial%20perception%20for%20separate%20study.%20For%20object%20recognition%2C%20we%20convert%20images%20into%20text%20token%20maps%20and%20find%20that%20the%20model%27s%20perception%20of%20image%20content%20unfolds%20as%20a%20two-stage%20process%20from%20shallow%20to%20deep%20layers%2C%20beginning%20with%20attribute%20recognition%20and%20culminating%20in%20semantic%20disambiguation.%20For%20spatial%20perception%2C%20we%20theoretically%20derive%20and%20empirically%20verify%20the%20geometric%20structure%20underlying%20the%20positional%20representation%20in%20VLMs.%20Based%20on%20these%20findings%2C%20we%20introduce%20an%20instruction-agnostic%20token%20compression%20algorithm%20based%20on%20a%20plug-and-play%20visual%20decoder%20to%20improve%20decoding%20efficiency%2C%20and%20a%20RoPE%20scaling%20technique%20to%20enhance%20spatial%20reasoning.%20Through%20rigorous%20experiments%2C%20our%20work%20validates%20these%20analyses%2C%20offering%20a%20deeper%20understanding%20of%20VLM%20internals%20and%20providing%20clear%20principles%20for%20designing%20more%20capable%20future%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2509.19191v2&entry.124074799=Read"},
{"title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence", "author": "Feilong Tang and Xiang An and Yunyao Yan and Yin Xie and Bin Qin and Kaicheng Yang and Yifei Shen and Yuanhan Zhang and Chunyuan Li and Shikun Feng and Changrui Chen and Huajie Tan and Ming Hu and Manyuan Zhang and Bo Li and Ziyong Feng and Ziwei Liu and Zongyuan Ge and Jiankang Deng", "abstract": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.", "link": "http://arxiv.org/abs/2602.08683v1", "date": "2026-02-09", "relevancy": 3.194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6728}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneVision-Encoder%3A%20Codec-Aligned%20Sparsity%20as%20a%20Foundational%20Principle%20for%20Multimodal%20Intelligence&body=Title%3A%20OneVision-Encoder%3A%20Codec-Aligned%20Sparsity%20as%20a%20Foundational%20Principle%20for%20Multimodal%20Intelligence%0AAuthor%3A%20Feilong%20Tang%20and%20Xiang%20An%20and%20Yunyao%20Yan%20and%20Yin%20Xie%20and%20Bin%20Qin%20and%20Kaicheng%20Yang%20and%20Yifei%20Shen%20and%20Yuanhan%20Zhang%20and%20Chunyuan%20Li%20and%20Shikun%20Feng%20and%20Changrui%20Chen%20and%20Huajie%20Tan%20and%20Ming%20Hu%20and%20Manyuan%20Zhang%20and%20Bo%20Li%20and%20Ziyong%20Feng%20and%20Ziwei%20Liu%20and%20Zongyuan%20Ge%20and%20Jiankang%20Deng%0AAbstract%3A%20Hypothesis.%20Artificial%20general%20intelligence%20is%2C%20at%20its%20core%2C%20a%20compression%20problem.%20Effective%20compression%20demands%20resonance%3A%20deep%20learning%20scales%20best%20when%20its%20architecture%20aligns%20with%20the%20fundamental%20structure%20of%20the%20data.%20These%20are%20the%20fundamental%20principles.%20Yet%2C%20modern%20vision%20architectures%20have%20strayed%20from%20these%20truths%3A%20visual%20signals%20are%20highly%20redundant%2C%20while%20discriminative%20information%2C%20the%20surprise%2C%20is%20sparse.%20Current%20models%20process%20dense%20pixel%20grids%20uniformly%2C%20wasting%20vast%20compute%20on%20static%20background%20rather%20than%20focusing%20on%20the%20predictive%20residuals%20that%20define%20motion%20and%20meaning.%20We%20argue%20that%20to%20solve%20visual%20understanding%2C%20we%20must%20align%20our%20architectures%20with%20the%20information-theoretic%20principles%20of%20video%2C%20i.e.%2C%20Codecs.%0A%20%20Method.%20OneVision-Encoder%20encodes%20video%20by%20compressing%20predictive%20visual%20structure%20into%20semantic%20meaning.%20By%20adopting%20Codec%20Patchification%2C%20OV-Encoder%20abandons%20uniform%20computation%20to%20focus%20exclusively%20on%20the%203.1%25-25%25%20of%20regions%20rich%20in%20signal%20entropy.%20To%20unify%20spatial%20and%20temporal%20reasoning%20under%20irregular%20token%20layouts%2C%20OneVision-Encoder%20employs%20a%20shared%203D%20RoPE%20and%20is%20trained%20with%20a%20large-scale%20cluster%20discrimination%20objective%20over%20more%20than%20one%20million%20semantic%20concepts%2C%20jointly%20capturing%20object%20permanence%20and%20motion%20dynamics.%0A%20%20Evidence.%20The%20results%20validate%20our%20core%20hypothesis%3A%20efficiency%20and%20accuracy%20are%20not%20a%20trade-off%3B%20they%20are%20positively%20correlated.%20When%20integrated%20into%20LLM%2C%20it%20consistently%20outperforms%20strong%20vision%20backbones%20such%20as%20Qwen3-ViT%20and%20SigLIP2%20across%2016%20image%2C%20video%2C%20and%20document%20understanding%20benchmarks%2C%20despite%20using%20substantially%20fewer%20visual%20tokens%20and%20pretraining%20data.%20Notably%2C%20on%20video%20understanding%20tasks%2C%20OV-Encoder%20achieves%20an%20average%20improvement%20of%204.1%25%20over%20Qwen3-ViT.%20Codec-aligned%2C%20patch-level%20sparsity%20is%20a%20foundational%20principle%2C%20enabling%20OV-Encoder%20as%20a%20scalable%20engine%20for%20next-generation%20visual%20generalists.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneVision-Encoder%253A%2520Codec-Aligned%2520Sparsity%2520as%2520a%2520Foundational%2520Principle%2520for%2520Multimodal%2520Intelligence%26entry.906535625%3DFeilong%2520Tang%2520and%2520Xiang%2520An%2520and%2520Yunyao%2520Yan%2520and%2520Yin%2520Xie%2520and%2520Bin%2520Qin%2520and%2520Kaicheng%2520Yang%2520and%2520Yifei%2520Shen%2520and%2520Yuanhan%2520Zhang%2520and%2520Chunyuan%2520Li%2520and%2520Shikun%2520Feng%2520and%2520Changrui%2520Chen%2520and%2520Huajie%2520Tan%2520and%2520Ming%2520Hu%2520and%2520Manyuan%2520Zhang%2520and%2520Bo%2520Li%2520and%2520Ziyong%2520Feng%2520and%2520Ziwei%2520Liu%2520and%2520Zongyuan%2520Ge%2520and%2520Jiankang%2520Deng%26entry.1292438233%3DHypothesis.%2520Artificial%2520general%2520intelligence%2520is%252C%2520at%2520its%2520core%252C%2520a%2520compression%2520problem.%2520Effective%2520compression%2520demands%2520resonance%253A%2520deep%2520learning%2520scales%2520best%2520when%2520its%2520architecture%2520aligns%2520with%2520the%2520fundamental%2520structure%2520of%2520the%2520data.%2520These%2520are%2520the%2520fundamental%2520principles.%2520Yet%252C%2520modern%2520vision%2520architectures%2520have%2520strayed%2520from%2520these%2520truths%253A%2520visual%2520signals%2520are%2520highly%2520redundant%252C%2520while%2520discriminative%2520information%252C%2520the%2520surprise%252C%2520is%2520sparse.%2520Current%2520models%2520process%2520dense%2520pixel%2520grids%2520uniformly%252C%2520wasting%2520vast%2520compute%2520on%2520static%2520background%2520rather%2520than%2520focusing%2520on%2520the%2520predictive%2520residuals%2520that%2520define%2520motion%2520and%2520meaning.%2520We%2520argue%2520that%2520to%2520solve%2520visual%2520understanding%252C%2520we%2520must%2520align%2520our%2520architectures%2520with%2520the%2520information-theoretic%2520principles%2520of%2520video%252C%2520i.e.%252C%2520Codecs.%250A%2520%2520Method.%2520OneVision-Encoder%2520encodes%2520video%2520by%2520compressing%2520predictive%2520visual%2520structure%2520into%2520semantic%2520meaning.%2520By%2520adopting%2520Codec%2520Patchification%252C%2520OV-Encoder%2520abandons%2520uniform%2520computation%2520to%2520focus%2520exclusively%2520on%2520the%25203.1%2525-25%2525%2520of%2520regions%2520rich%2520in%2520signal%2520entropy.%2520To%2520unify%2520spatial%2520and%2520temporal%2520reasoning%2520under%2520irregular%2520token%2520layouts%252C%2520OneVision-Encoder%2520employs%2520a%2520shared%25203D%2520RoPE%2520and%2520is%2520trained%2520with%2520a%2520large-scale%2520cluster%2520discrimination%2520objective%2520over%2520more%2520than%2520one%2520million%2520semantic%2520concepts%252C%2520jointly%2520capturing%2520object%2520permanence%2520and%2520motion%2520dynamics.%250A%2520%2520Evidence.%2520The%2520results%2520validate%2520our%2520core%2520hypothesis%253A%2520efficiency%2520and%2520accuracy%2520are%2520not%2520a%2520trade-off%253B%2520they%2520are%2520positively%2520correlated.%2520When%2520integrated%2520into%2520LLM%252C%2520it%2520consistently%2520outperforms%2520strong%2520vision%2520backbones%2520such%2520as%2520Qwen3-ViT%2520and%2520SigLIP2%2520across%252016%2520image%252C%2520video%252C%2520and%2520document%2520understanding%2520benchmarks%252C%2520despite%2520using%2520substantially%2520fewer%2520visual%2520tokens%2520and%2520pretraining%2520data.%2520Notably%252C%2520on%2520video%2520understanding%2520tasks%252C%2520OV-Encoder%2520achieves%2520an%2520average%2520improvement%2520of%25204.1%2525%2520over%2520Qwen3-ViT.%2520Codec-aligned%252C%2520patch-level%2520sparsity%2520is%2520a%2520foundational%2520principle%252C%2520enabling%2520OV-Encoder%2520as%2520a%2520scalable%2520engine%2520for%2520next-generation%2520visual%2520generalists.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneVision-Encoder%3A%20Codec-Aligned%20Sparsity%20as%20a%20Foundational%20Principle%20for%20Multimodal%20Intelligence&entry.906535625=Feilong%20Tang%20and%20Xiang%20An%20and%20Yunyao%20Yan%20and%20Yin%20Xie%20and%20Bin%20Qin%20and%20Kaicheng%20Yang%20and%20Yifei%20Shen%20and%20Yuanhan%20Zhang%20and%20Chunyuan%20Li%20and%20Shikun%20Feng%20and%20Changrui%20Chen%20and%20Huajie%20Tan%20and%20Ming%20Hu%20and%20Manyuan%20Zhang%20and%20Bo%20Li%20and%20Ziyong%20Feng%20and%20Ziwei%20Liu%20and%20Zongyuan%20Ge%20and%20Jiankang%20Deng&entry.1292438233=Hypothesis.%20Artificial%20general%20intelligence%20is%2C%20at%20its%20core%2C%20a%20compression%20problem.%20Effective%20compression%20demands%20resonance%3A%20deep%20learning%20scales%20best%20when%20its%20architecture%20aligns%20with%20the%20fundamental%20structure%20of%20the%20data.%20These%20are%20the%20fundamental%20principles.%20Yet%2C%20modern%20vision%20architectures%20have%20strayed%20from%20these%20truths%3A%20visual%20signals%20are%20highly%20redundant%2C%20while%20discriminative%20information%2C%20the%20surprise%2C%20is%20sparse.%20Current%20models%20process%20dense%20pixel%20grids%20uniformly%2C%20wasting%20vast%20compute%20on%20static%20background%20rather%20than%20focusing%20on%20the%20predictive%20residuals%20that%20define%20motion%20and%20meaning.%20We%20argue%20that%20to%20solve%20visual%20understanding%2C%20we%20must%20align%20our%20architectures%20with%20the%20information-theoretic%20principles%20of%20video%2C%20i.e.%2C%20Codecs.%0A%20%20Method.%20OneVision-Encoder%20encodes%20video%20by%20compressing%20predictive%20visual%20structure%20into%20semantic%20meaning.%20By%20adopting%20Codec%20Patchification%2C%20OV-Encoder%20abandons%20uniform%20computation%20to%20focus%20exclusively%20on%20the%203.1%25-25%25%20of%20regions%20rich%20in%20signal%20entropy.%20To%20unify%20spatial%20and%20temporal%20reasoning%20under%20irregular%20token%20layouts%2C%20OneVision-Encoder%20employs%20a%20shared%203D%20RoPE%20and%20is%20trained%20with%20a%20large-scale%20cluster%20discrimination%20objective%20over%20more%20than%20one%20million%20semantic%20concepts%2C%20jointly%20capturing%20object%20permanence%20and%20motion%20dynamics.%0A%20%20Evidence.%20The%20results%20validate%20our%20core%20hypothesis%3A%20efficiency%20and%20accuracy%20are%20not%20a%20trade-off%3B%20they%20are%20positively%20correlated.%20When%20integrated%20into%20LLM%2C%20it%20consistently%20outperforms%20strong%20vision%20backbones%20such%20as%20Qwen3-ViT%20and%20SigLIP2%20across%2016%20image%2C%20video%2C%20and%20document%20understanding%20benchmarks%2C%20despite%20using%20substantially%20fewer%20visual%20tokens%20and%20pretraining%20data.%20Notably%2C%20on%20video%20understanding%20tasks%2C%20OV-Encoder%20achieves%20an%20average%20improvement%20of%204.1%25%20over%20Qwen3-ViT.%20Codec-aligned%2C%20patch-level%20sparsity%20is%20a%20foundational%20principle%2C%20enabling%20OV-Encoder%20as%20a%20scalable%20engine%20for%20next-generation%20visual%20generalists.&entry.1838667208=http%3A//arxiv.org/abs/2602.08683v1&entry.124074799=Read"},
{"title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "author": "Tianyu Sun and Zhoujie Fu and Bang Zhang and Guosheng Lin", "abstract": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "link": "http://arxiv.org/abs/2602.08753v1", "date": "2026-02-09", "relevancy": 3.1367, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.668}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVAnimate%3A%20Enhancing%20Character%20Animation%20with%20Multi-View%20Optimization&body=Title%3A%20MVAnimate%3A%20Enhancing%20Character%20Animation%20with%20Multi-View%20Optimization%0AAuthor%3A%20Tianyu%20Sun%20and%20Zhoujie%20Fu%20and%20Bang%20Zhang%20and%20Guosheng%20Lin%0AAbstract%3A%20The%20demand%20for%20realistic%20and%20versatile%20character%20animation%20has%20surged%2C%20driven%20by%20its%20wide-ranging%20applications%20in%20various%20domains.%20However%2C%20the%20animation%20generation%20algorithms%20modeling%20human%20pose%20with%202D%20or%203D%20structures%20all%20face%20various%20problems%2C%20including%20low-quality%20output%20content%20and%20training%20data%20deficiency%2C%20preventing%20the%20related%20algorithms%20from%20generating%20high-quality%20animation%20videos.%20Therefore%2C%20we%20introduce%20MVAnimate%2C%20a%20novel%20framework%20that%20synthesizes%20both%202D%20and%203D%20information%20of%20dynamic%20figures%20based%20on%20multi-view%20prior%20information%2C%20to%20enhance%20the%20generated%20video%20quality.%20Our%20approach%20leverages%20multi-view%20prior%20information%20to%20produce%20temporally%20consistent%20and%20spatially%20coherent%20animation%20outputs%2C%20demonstrating%20improvements%20over%20existing%20animation%20methods.%20Our%20MVAnimate%20also%20optimizes%20the%20multi-view%20videos%20of%20the%20target%20character%2C%20enhancing%20the%20video%20quality%20from%20different%20views.%20Experimental%20results%20on%20diverse%20datasets%20highlight%20the%20robustness%20of%20our%20method%20in%20handling%20various%20motion%20patterns%20and%20appearances.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVAnimate%253A%2520Enhancing%2520Character%2520Animation%2520with%2520Multi-View%2520Optimization%26entry.906535625%3DTianyu%2520Sun%2520and%2520Zhoujie%2520Fu%2520and%2520Bang%2520Zhang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3DThe%2520demand%2520for%2520realistic%2520and%2520versatile%2520character%2520animation%2520has%2520surged%252C%2520driven%2520by%2520its%2520wide-ranging%2520applications%2520in%2520various%2520domains.%2520However%252C%2520the%2520animation%2520generation%2520algorithms%2520modeling%2520human%2520pose%2520with%25202D%2520or%25203D%2520structures%2520all%2520face%2520various%2520problems%252C%2520including%2520low-quality%2520output%2520content%2520and%2520training%2520data%2520deficiency%252C%2520preventing%2520the%2520related%2520algorithms%2520from%2520generating%2520high-quality%2520animation%2520videos.%2520Therefore%252C%2520we%2520introduce%2520MVAnimate%252C%2520a%2520novel%2520framework%2520that%2520synthesizes%2520both%25202D%2520and%25203D%2520information%2520of%2520dynamic%2520figures%2520based%2520on%2520multi-view%2520prior%2520information%252C%2520to%2520enhance%2520the%2520generated%2520video%2520quality.%2520Our%2520approach%2520leverages%2520multi-view%2520prior%2520information%2520to%2520produce%2520temporally%2520consistent%2520and%2520spatially%2520coherent%2520animation%2520outputs%252C%2520demonstrating%2520improvements%2520over%2520existing%2520animation%2520methods.%2520Our%2520MVAnimate%2520also%2520optimizes%2520the%2520multi-view%2520videos%2520of%2520the%2520target%2520character%252C%2520enhancing%2520the%2520video%2520quality%2520from%2520different%2520views.%2520Experimental%2520results%2520on%2520diverse%2520datasets%2520highlight%2520the%2520robustness%2520of%2520our%2520method%2520in%2520handling%2520various%2520motion%2520patterns%2520and%2520appearances.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVAnimate%3A%20Enhancing%20Character%20Animation%20with%20Multi-View%20Optimization&entry.906535625=Tianyu%20Sun%20and%20Zhoujie%20Fu%20and%20Bang%20Zhang%20and%20Guosheng%20Lin&entry.1292438233=The%20demand%20for%20realistic%20and%20versatile%20character%20animation%20has%20surged%2C%20driven%20by%20its%20wide-ranging%20applications%20in%20various%20domains.%20However%2C%20the%20animation%20generation%20algorithms%20modeling%20human%20pose%20with%202D%20or%203D%20structures%20all%20face%20various%20problems%2C%20including%20low-quality%20output%20content%20and%20training%20data%20deficiency%2C%20preventing%20the%20related%20algorithms%20from%20generating%20high-quality%20animation%20videos.%20Therefore%2C%20we%20introduce%20MVAnimate%2C%20a%20novel%20framework%20that%20synthesizes%20both%202D%20and%203D%20information%20of%20dynamic%20figures%20based%20on%20multi-view%20prior%20information%2C%20to%20enhance%20the%20generated%20video%20quality.%20Our%20approach%20leverages%20multi-view%20prior%20information%20to%20produce%20temporally%20consistent%20and%20spatially%20coherent%20animation%20outputs%2C%20demonstrating%20improvements%20over%20existing%20animation%20methods.%20Our%20MVAnimate%20also%20optimizes%20the%20multi-view%20videos%20of%20the%20target%20character%2C%20enhancing%20the%20video%20quality%20from%20different%20views.%20Experimental%20results%20on%20diverse%20datasets%20highlight%20the%20robustness%20of%20our%20method%20in%20handling%20various%20motion%20patterns%20and%20appearances.&entry.1838667208=http%3A//arxiv.org/abs/2602.08753v1&entry.124074799=Read"},
{"title": "Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction", "author": "Hongyi Chen and Tony Dong and Tiancheng Wu and Liquan Wang and Yash Jangir and Yaru Niu and Yufei Ye and Homanga Bharadhwaj and Zackory Erickson and Jeffrey Ichnowski", "abstract": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.", "link": "http://arxiv.org/abs/2602.09013v1", "date": "2026-02-09", "relevancy": 3.1282, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6477}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6182}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dexterous%20Manipulation%20Policies%20from%20RGB%20Human%20Videos%20via%204D%20Hand-Object%20Trajectory%20Reconstruction&body=Title%3A%20Dexterous%20Manipulation%20Policies%20from%20RGB%20Human%20Videos%20via%204D%20Hand-Object%20Trajectory%20Reconstruction%0AAuthor%3A%20Hongyi%20Chen%20and%20Tony%20Dong%20and%20Tiancheng%20Wu%20and%20Liquan%20Wang%20and%20Yash%20Jangir%20and%20Yaru%20Niu%20and%20Yufei%20Ye%20and%20Homanga%20Bharadhwaj%20and%20Zackory%20Erickson%20and%20Jeffrey%20Ichnowski%0AAbstract%3A%20Multi-finger%20robotic%20hand%20manipulation%20and%20grasping%20are%20challenging%20due%20to%20the%20high-dimensional%20action%20space%20and%20the%20difficulty%20of%20acquiring%20large-scale%20training%20data.%20Existing%20approaches%20largely%20rely%20on%20human%20teleoperation%20with%20wearable%20devices%20or%20specialized%20sensing%20equipment%20to%20capture%20hand-object%20interactions%2C%20which%20limits%20scalability.%20In%20this%20work%2C%20we%20propose%20VIDEOMANIP%2C%20a%20device-free%20framework%20that%20learns%20dexterous%20manipulation%20directly%20from%20RGB%20human%20videos.%20Leveraging%20recent%20advances%20in%20computer%20vision%2C%20VIDEOMANIP%20reconstructs%20explicit%204D%20robot-object%20trajectories%20from%20monocular%20videos%20by%20estimating%20human%20hand%20poses%2C%20object%20meshes%2C%20and%20retargets%20the%20reconstructed%20human%20motions%20to%20robotic%20hands%20for%20manipulation%20learning.%20To%20make%20the%20reconstructed%20robot%20data%20suitable%20for%20dexterous%20manipulation%20training%2C%20we%20introduce%20hand-object%20contact%20optimization%20with%20interaction-centric%20grasp%20modeling%2C%20as%20well%20as%20a%20demonstration%20synthesis%20strategy%20that%20generates%20diverse%20training%20trajectories%20from%20a%20single%20video%2C%20enabling%20generalizable%20policy%20learning%20without%20additional%20robot%20demonstrations.%20In%20simulation%2C%20the%20learned%20grasping%20model%20achieves%20a%2070.25%25%20success%20rate%20across%2020%20diverse%20objects%20using%20the%20Inspire%20Hand.%20In%20the%20real%20world%2C%20manipulation%20policies%20trained%20from%20RGB%20videos%20achieve%20an%20average%2062.86%25%20success%20rate%20across%20seven%20tasks%20using%20the%20LEAP%20Hand%2C%20outperforming%20retargeting-based%20methods%20by%2015.87%25.%20Project%20videos%20are%20available%20at%20videomanip.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterous%2520Manipulation%2520Policies%2520from%2520RGB%2520Human%2520Videos%2520via%25204D%2520Hand-Object%2520Trajectory%2520Reconstruction%26entry.906535625%3DHongyi%2520Chen%2520and%2520Tony%2520Dong%2520and%2520Tiancheng%2520Wu%2520and%2520Liquan%2520Wang%2520and%2520Yash%2520Jangir%2520and%2520Yaru%2520Niu%2520and%2520Yufei%2520Ye%2520and%2520Homanga%2520Bharadhwaj%2520and%2520Zackory%2520Erickson%2520and%2520Jeffrey%2520Ichnowski%26entry.1292438233%3DMulti-finger%2520robotic%2520hand%2520manipulation%2520and%2520grasping%2520are%2520challenging%2520due%2520to%2520the%2520high-dimensional%2520action%2520space%2520and%2520the%2520difficulty%2520of%2520acquiring%2520large-scale%2520training%2520data.%2520Existing%2520approaches%2520largely%2520rely%2520on%2520human%2520teleoperation%2520with%2520wearable%2520devices%2520or%2520specialized%2520sensing%2520equipment%2520to%2520capture%2520hand-object%2520interactions%252C%2520which%2520limits%2520scalability.%2520In%2520this%2520work%252C%2520we%2520propose%2520VIDEOMANIP%252C%2520a%2520device-free%2520framework%2520that%2520learns%2520dexterous%2520manipulation%2520directly%2520from%2520RGB%2520human%2520videos.%2520Leveraging%2520recent%2520advances%2520in%2520computer%2520vision%252C%2520VIDEOMANIP%2520reconstructs%2520explicit%25204D%2520robot-object%2520trajectories%2520from%2520monocular%2520videos%2520by%2520estimating%2520human%2520hand%2520poses%252C%2520object%2520meshes%252C%2520and%2520retargets%2520the%2520reconstructed%2520human%2520motions%2520to%2520robotic%2520hands%2520for%2520manipulation%2520learning.%2520To%2520make%2520the%2520reconstructed%2520robot%2520data%2520suitable%2520for%2520dexterous%2520manipulation%2520training%252C%2520we%2520introduce%2520hand-object%2520contact%2520optimization%2520with%2520interaction-centric%2520grasp%2520modeling%252C%2520as%2520well%2520as%2520a%2520demonstration%2520synthesis%2520strategy%2520that%2520generates%2520diverse%2520training%2520trajectories%2520from%2520a%2520single%2520video%252C%2520enabling%2520generalizable%2520policy%2520learning%2520without%2520additional%2520robot%2520demonstrations.%2520In%2520simulation%252C%2520the%2520learned%2520grasping%2520model%2520achieves%2520a%252070.25%2525%2520success%2520rate%2520across%252020%2520diverse%2520objects%2520using%2520the%2520Inspire%2520Hand.%2520In%2520the%2520real%2520world%252C%2520manipulation%2520policies%2520trained%2520from%2520RGB%2520videos%2520achieve%2520an%2520average%252062.86%2525%2520success%2520rate%2520across%2520seven%2520tasks%2520using%2520the%2520LEAP%2520Hand%252C%2520outperforming%2520retargeting-based%2520methods%2520by%252015.87%2525.%2520Project%2520videos%2520are%2520available%2520at%2520videomanip.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dexterous%20Manipulation%20Policies%20from%20RGB%20Human%20Videos%20via%204D%20Hand-Object%20Trajectory%20Reconstruction&entry.906535625=Hongyi%20Chen%20and%20Tony%20Dong%20and%20Tiancheng%20Wu%20and%20Liquan%20Wang%20and%20Yash%20Jangir%20and%20Yaru%20Niu%20and%20Yufei%20Ye%20and%20Homanga%20Bharadhwaj%20and%20Zackory%20Erickson%20and%20Jeffrey%20Ichnowski&entry.1292438233=Multi-finger%20robotic%20hand%20manipulation%20and%20grasping%20are%20challenging%20due%20to%20the%20high-dimensional%20action%20space%20and%20the%20difficulty%20of%20acquiring%20large-scale%20training%20data.%20Existing%20approaches%20largely%20rely%20on%20human%20teleoperation%20with%20wearable%20devices%20or%20specialized%20sensing%20equipment%20to%20capture%20hand-object%20interactions%2C%20which%20limits%20scalability.%20In%20this%20work%2C%20we%20propose%20VIDEOMANIP%2C%20a%20device-free%20framework%20that%20learns%20dexterous%20manipulation%20directly%20from%20RGB%20human%20videos.%20Leveraging%20recent%20advances%20in%20computer%20vision%2C%20VIDEOMANIP%20reconstructs%20explicit%204D%20robot-object%20trajectories%20from%20monocular%20videos%20by%20estimating%20human%20hand%20poses%2C%20object%20meshes%2C%20and%20retargets%20the%20reconstructed%20human%20motions%20to%20robotic%20hands%20for%20manipulation%20learning.%20To%20make%20the%20reconstructed%20robot%20data%20suitable%20for%20dexterous%20manipulation%20training%2C%20we%20introduce%20hand-object%20contact%20optimization%20with%20interaction-centric%20grasp%20modeling%2C%20as%20well%20as%20a%20demonstration%20synthesis%20strategy%20that%20generates%20diverse%20training%20trajectories%20from%20a%20single%20video%2C%20enabling%20generalizable%20policy%20learning%20without%20additional%20robot%20demonstrations.%20In%20simulation%2C%20the%20learned%20grasping%20model%20achieves%20a%2070.25%25%20success%20rate%20across%2020%20diverse%20objects%20using%20the%20Inspire%20Hand.%20In%20the%20real%20world%2C%20manipulation%20policies%20trained%20from%20RGB%20videos%20achieve%20an%20average%2062.86%25%20success%20rate%20across%20seven%20tasks%20using%20the%20LEAP%20Hand%2C%20outperforming%20retargeting-based%20methods%20by%2015.87%25.%20Project%20videos%20are%20available%20at%20videomanip.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2602.09013v1&entry.124074799=Read"},
{"title": "GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion", "author": "Santiago Montiel-Mar\u00edn and Miguel Antunes-Garc\u00eda and Fabio S\u00e1nchez-Garc\u00eda and Angel Llamazares and Holger Caesar and Luis M. Bergasa", "abstract": "Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.", "link": "http://arxiv.org/abs/2602.08784v1", "date": "2026-02-09", "relevancy": 3.0847, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6733}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5935}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianCaR%3A%20Gaussian%20Splatting%20for%20Efficient%20Camera-Radar%20Fusion&body=Title%3A%20GaussianCaR%3A%20Gaussian%20Splatting%20for%20Efficient%20Camera-Radar%20Fusion%0AAuthor%3A%20Santiago%20Montiel-Mar%C3%ADn%20and%20Miguel%20Antunes-Garc%C3%ADa%20and%20Fabio%20S%C3%A1nchez-Garc%C3%ADa%20and%20Angel%20Llamazares%20and%20Holger%20Caesar%20and%20Luis%20M.%20Bergasa%0AAbstract%3A%20Robust%20and%20accurate%20perception%20of%20dynamic%20objects%20and%20map%20elements%20is%20crucial%20for%20autonomous%20vehicles%20performing%20safe%20navigation%20in%20complex%20traffic%20scenarios.%20While%20vision-only%20methods%20have%20become%20the%20de%20facto%20standard%20due%20to%20their%20technical%20advances%2C%20they%20can%20benefit%20from%20effective%20and%20cost-efficient%20fusion%20with%20radar%20measurements.%20In%20this%20work%2C%20we%20advance%20fusion%20methods%20by%20repurposing%20Gaussian%20Splatting%20as%20an%20efficient%20universal%20view%20transformer%20that%20bridges%20the%20view%20disparity%20gap%2C%20mapping%20both%20image%20pixels%20and%20radar%20points%20into%20a%20common%20Bird%27s-Eye%20View%20%28BEV%29%20representation.%20Our%20main%20contribution%20is%20GaussianCaR%2C%20an%20end-to-end%20network%20for%20BEV%20segmentation%20that%2C%20unlike%20prior%20BEV%20fusion%20methods%2C%20leverages%20Gaussian%20Splatting%20to%20map%20raw%20sensor%20information%20into%20latent%20features%20for%20efficient%20camera-radar%20fusion.%20Our%20architecture%20combines%20multi-scale%20fusion%20with%20a%20transformer%20decoder%20to%20efficiently%20extract%20BEV%20features.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20on%20par%20with%2C%20or%20even%20surpassing%2C%20the%20state%20of%20the%20art%20on%20BEV%20segmentation%20tasks%20%2857.3%25%2C%2082.9%25%2C%20and%2050.1%25%20IoU%20for%20vehicles%2C%20roads%2C%20and%20lane%20dividers%29%20on%20the%20nuScenes%20dataset%2C%20while%20maintaining%20a%203.2x%20faster%20inference%20runtime.%20Code%20and%20project%20page%20are%20available%20online.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianCaR%253A%2520Gaussian%2520Splatting%2520for%2520Efficient%2520Camera-Radar%2520Fusion%26entry.906535625%3DSantiago%2520Montiel-Mar%25C3%25ADn%2520and%2520Miguel%2520Antunes-Garc%25C3%25ADa%2520and%2520Fabio%2520S%25C3%25A1nchez-Garc%25C3%25ADa%2520and%2520Angel%2520Llamazares%2520and%2520Holger%2520Caesar%2520and%2520Luis%2520M.%2520Bergasa%26entry.1292438233%3DRobust%2520and%2520accurate%2520perception%2520of%2520dynamic%2520objects%2520and%2520map%2520elements%2520is%2520crucial%2520for%2520autonomous%2520vehicles%2520performing%2520safe%2520navigation%2520in%2520complex%2520traffic%2520scenarios.%2520While%2520vision-only%2520methods%2520have%2520become%2520the%2520de%2520facto%2520standard%2520due%2520to%2520their%2520technical%2520advances%252C%2520they%2520can%2520benefit%2520from%2520effective%2520and%2520cost-efficient%2520fusion%2520with%2520radar%2520measurements.%2520In%2520this%2520work%252C%2520we%2520advance%2520fusion%2520methods%2520by%2520repurposing%2520Gaussian%2520Splatting%2520as%2520an%2520efficient%2520universal%2520view%2520transformer%2520that%2520bridges%2520the%2520view%2520disparity%2520gap%252C%2520mapping%2520both%2520image%2520pixels%2520and%2520radar%2520points%2520into%2520a%2520common%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520representation.%2520Our%2520main%2520contribution%2520is%2520GaussianCaR%252C%2520an%2520end-to-end%2520network%2520for%2520BEV%2520segmentation%2520that%252C%2520unlike%2520prior%2520BEV%2520fusion%2520methods%252C%2520leverages%2520Gaussian%2520Splatting%2520to%2520map%2520raw%2520sensor%2520information%2520into%2520latent%2520features%2520for%2520efficient%2520camera-radar%2520fusion.%2520Our%2520architecture%2520combines%2520multi-scale%2520fusion%2520with%2520a%2520transformer%2520decoder%2520to%2520efficiently%2520extract%2520BEV%2520features.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520performance%2520on%2520par%2520with%252C%2520or%2520even%2520surpassing%252C%2520the%2520state%2520of%2520the%2520art%2520on%2520BEV%2520segmentation%2520tasks%2520%252857.3%2525%252C%252082.9%2525%252C%2520and%252050.1%2525%2520IoU%2520for%2520vehicles%252C%2520roads%252C%2520and%2520lane%2520dividers%2529%2520on%2520the%2520nuScenes%2520dataset%252C%2520while%2520maintaining%2520a%25203.2x%2520faster%2520inference%2520runtime.%2520Code%2520and%2520project%2520page%2520are%2520available%2520online.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianCaR%3A%20Gaussian%20Splatting%20for%20Efficient%20Camera-Radar%20Fusion&entry.906535625=Santiago%20Montiel-Mar%C3%ADn%20and%20Miguel%20Antunes-Garc%C3%ADa%20and%20Fabio%20S%C3%A1nchez-Garc%C3%ADa%20and%20Angel%20Llamazares%20and%20Holger%20Caesar%20and%20Luis%20M.%20Bergasa&entry.1292438233=Robust%20and%20accurate%20perception%20of%20dynamic%20objects%20and%20map%20elements%20is%20crucial%20for%20autonomous%20vehicles%20performing%20safe%20navigation%20in%20complex%20traffic%20scenarios.%20While%20vision-only%20methods%20have%20become%20the%20de%20facto%20standard%20due%20to%20their%20technical%20advances%2C%20they%20can%20benefit%20from%20effective%20and%20cost-efficient%20fusion%20with%20radar%20measurements.%20In%20this%20work%2C%20we%20advance%20fusion%20methods%20by%20repurposing%20Gaussian%20Splatting%20as%20an%20efficient%20universal%20view%20transformer%20that%20bridges%20the%20view%20disparity%20gap%2C%20mapping%20both%20image%20pixels%20and%20radar%20points%20into%20a%20common%20Bird%27s-Eye%20View%20%28BEV%29%20representation.%20Our%20main%20contribution%20is%20GaussianCaR%2C%20an%20end-to-end%20network%20for%20BEV%20segmentation%20that%2C%20unlike%20prior%20BEV%20fusion%20methods%2C%20leverages%20Gaussian%20Splatting%20to%20map%20raw%20sensor%20information%20into%20latent%20features%20for%20efficient%20camera-radar%20fusion.%20Our%20architecture%20combines%20multi-scale%20fusion%20with%20a%20transformer%20decoder%20to%20efficiently%20extract%20BEV%20features.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20on%20par%20with%2C%20or%20even%20surpassing%2C%20the%20state%20of%20the%20art%20on%20BEV%20segmentation%20tasks%20%2857.3%25%2C%2082.9%25%2C%20and%2050.1%25%20IoU%20for%20vehicles%2C%20roads%2C%20and%20lane%20dividers%29%20on%20the%20nuScenes%20dataset%2C%20while%20maintaining%20a%203.2x%20faster%20inference%20runtime.%20Code%20and%20project%20page%20are%20available%20online.&entry.1838667208=http%3A//arxiv.org/abs/2602.08784v1&entry.124074799=Read"},
{"title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation", "author": "He Wu and Xia Yan and Yanghui Xu and Liegang Xia and Jiazhou Chen", "abstract": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.", "link": "http://arxiv.org/abs/2602.08540v1", "date": "2026-02-09", "relevancy": 3.06, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.64}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6222}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIBR4D%3A%20Tracing-Guided%20Iterative%20Boundary%20Refinement%20for%20Efficient%204D%20Gaussian%20Segmentation&body=Title%3A%20TIBR4D%3A%20Tracing-Guided%20Iterative%20Boundary%20Refinement%20for%20Efficient%204D%20Gaussian%20Segmentation%0AAuthor%3A%20He%20Wu%20and%20Xia%20Yan%20and%20Yanghui%20Xu%20and%20Liegang%20Xia%20and%20Jiazhou%20Chen%0AAbstract%3A%20Object-level%20segmentation%20in%20dynamic%204D%20Gaussian%20scenes%20remains%20challenging%20due%20to%20complex%20motion%2C%20occlusions%2C%20and%20ambiguous%20boundaries.%20In%20this%20paper%2C%20we%20present%20an%20efficient%20learning-free%204D%20Gaussian%20segmentation%20framework%20that%20lifts%20video%20segmentation%20masks%20to%204D%20spaces%2C%20whose%20core%20is%20a%20two-stage%20iterative%20boundary%20refinement%2C%20TIBR4D.%20The%20first%20stage%20is%20an%20Iterative%20Gaussian%20Instance%20Tracing%20%28IGIT%29%20at%20the%20temporal%20segment%20level.%20It%20progressively%20refines%20Gaussian-to-instance%20probabilities%20through%20iterative%20tracing%2C%20and%20extracts%20corresponding%20Gaussian%20point%20clouds%20that%20better%20handle%20occlusions%20and%20preserve%20completeness%20of%20object%20structures%20compared%20to%20existing%20one-shot%20threshold-based%20methods.%20The%20second%20stage%20is%20a%20frame-wise%20Gaussian%20Rendering%20Range%20Control%20%28RCC%29%20via%20suppressing%20highly%20uncertain%20Gaussians%20near%20object%20boundaries%20while%20retaining%20their%20core%20contributions%20for%20more%20accurate%20boundaries.%20Furthermore%2C%20a%20temporal%20segmentation%20merging%20strategy%20is%20proposed%20for%20IGIT%20to%20balance%20identity%20consistency%20and%20dynamic%20awareness.%20Longer%20segments%20enforce%20stronger%20multi-frame%20constraints%20for%20stable%20identities%2C%20while%20shorter%20segments%20allow%20identity%20changes%20to%20be%20captured%20promptly.%20Experiments%20on%20HyperNeRF%20and%20Neu3D%20demonstrate%20that%20our%20method%20produces%20accurate%20object%20Gaussian%20point%20clouds%20with%20clearer%20boundaries%20and%20higher%20efficiency%20compared%20to%20SOTA%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIBR4D%253A%2520Tracing-Guided%2520Iterative%2520Boundary%2520Refinement%2520for%2520Efficient%25204D%2520Gaussian%2520Segmentation%26entry.906535625%3DHe%2520Wu%2520and%2520Xia%2520Yan%2520and%2520Yanghui%2520Xu%2520and%2520Liegang%2520Xia%2520and%2520Jiazhou%2520Chen%26entry.1292438233%3DObject-level%2520segmentation%2520in%2520dynamic%25204D%2520Gaussian%2520scenes%2520remains%2520challenging%2520due%2520to%2520complex%2520motion%252C%2520occlusions%252C%2520and%2520ambiguous%2520boundaries.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520efficient%2520learning-free%25204D%2520Gaussian%2520segmentation%2520framework%2520that%2520lifts%2520video%2520segmentation%2520masks%2520to%25204D%2520spaces%252C%2520whose%2520core%2520is%2520a%2520two-stage%2520iterative%2520boundary%2520refinement%252C%2520TIBR4D.%2520The%2520first%2520stage%2520is%2520an%2520Iterative%2520Gaussian%2520Instance%2520Tracing%2520%2528IGIT%2529%2520at%2520the%2520temporal%2520segment%2520level.%2520It%2520progressively%2520refines%2520Gaussian-to-instance%2520probabilities%2520through%2520iterative%2520tracing%252C%2520and%2520extracts%2520corresponding%2520Gaussian%2520point%2520clouds%2520that%2520better%2520handle%2520occlusions%2520and%2520preserve%2520completeness%2520of%2520object%2520structures%2520compared%2520to%2520existing%2520one-shot%2520threshold-based%2520methods.%2520The%2520second%2520stage%2520is%2520a%2520frame-wise%2520Gaussian%2520Rendering%2520Range%2520Control%2520%2528RCC%2529%2520via%2520suppressing%2520highly%2520uncertain%2520Gaussians%2520near%2520object%2520boundaries%2520while%2520retaining%2520their%2520core%2520contributions%2520for%2520more%2520accurate%2520boundaries.%2520Furthermore%252C%2520a%2520temporal%2520segmentation%2520merging%2520strategy%2520is%2520proposed%2520for%2520IGIT%2520to%2520balance%2520identity%2520consistency%2520and%2520dynamic%2520awareness.%2520Longer%2520segments%2520enforce%2520stronger%2520multi-frame%2520constraints%2520for%2520stable%2520identities%252C%2520while%2520shorter%2520segments%2520allow%2520identity%2520changes%2520to%2520be%2520captured%2520promptly.%2520Experiments%2520on%2520HyperNeRF%2520and%2520Neu3D%2520demonstrate%2520that%2520our%2520method%2520produces%2520accurate%2520object%2520Gaussian%2520point%2520clouds%2520with%2520clearer%2520boundaries%2520and%2520higher%2520efficiency%2520compared%2520to%2520SOTA%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIBR4D%3A%20Tracing-Guided%20Iterative%20Boundary%20Refinement%20for%20Efficient%204D%20Gaussian%20Segmentation&entry.906535625=He%20Wu%20and%20Xia%20Yan%20and%20Yanghui%20Xu%20and%20Liegang%20Xia%20and%20Jiazhou%20Chen&entry.1292438233=Object-level%20segmentation%20in%20dynamic%204D%20Gaussian%20scenes%20remains%20challenging%20due%20to%20complex%20motion%2C%20occlusions%2C%20and%20ambiguous%20boundaries.%20In%20this%20paper%2C%20we%20present%20an%20efficient%20learning-free%204D%20Gaussian%20segmentation%20framework%20that%20lifts%20video%20segmentation%20masks%20to%204D%20spaces%2C%20whose%20core%20is%20a%20two-stage%20iterative%20boundary%20refinement%2C%20TIBR4D.%20The%20first%20stage%20is%20an%20Iterative%20Gaussian%20Instance%20Tracing%20%28IGIT%29%20at%20the%20temporal%20segment%20level.%20It%20progressively%20refines%20Gaussian-to-instance%20probabilities%20through%20iterative%20tracing%2C%20and%20extracts%20corresponding%20Gaussian%20point%20clouds%20that%20better%20handle%20occlusions%20and%20preserve%20completeness%20of%20object%20structures%20compared%20to%20existing%20one-shot%20threshold-based%20methods.%20The%20second%20stage%20is%20a%20frame-wise%20Gaussian%20Rendering%20Range%20Control%20%28RCC%29%20via%20suppressing%20highly%20uncertain%20Gaussians%20near%20object%20boundaries%20while%20retaining%20their%20core%20contributions%20for%20more%20accurate%20boundaries.%20Furthermore%2C%20a%20temporal%20segmentation%20merging%20strategy%20is%20proposed%20for%20IGIT%20to%20balance%20identity%20consistency%20and%20dynamic%20awareness.%20Longer%20segments%20enforce%20stronger%20multi-frame%20constraints%20for%20stable%20identities%2C%20while%20shorter%20segments%20allow%20identity%20changes%20to%20be%20captured%20promptly.%20Experiments%20on%20HyperNeRF%20and%20Neu3D%20demonstrate%20that%20our%20method%20produces%20accurate%20object%20Gaussian%20point%20clouds%20with%20clearer%20boundaries%20and%20higher%20efficiency%20compared%20to%20SOTA%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.08540v1&entry.124074799=Read"},
{"title": "Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering", "author": "Geng Lin and Matthias Zwicker", "abstract": "Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.", "link": "http://arxiv.org/abs/2602.08724v1", "date": "2026-02-09", "relevancy": 3.0575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6228}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6145}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotated%20Lights%20for%20Consistent%20and%20Efficient%202D%20Gaussians%20Inverse%20Rendering&body=Title%3A%20Rotated%20Lights%20for%20Consistent%20and%20Efficient%202D%20Gaussians%20Inverse%20Rendering%0AAuthor%3A%20Geng%20Lin%20and%20Matthias%20Zwicker%0AAbstract%3A%20Inverse%20rendering%20aims%20to%20decompose%20a%20scene%20into%20its%20geometry%2C%20material%20properties%20and%20light%20conditions%20under%20a%20certain%20rendering%20model.%20It%20has%20wide%20applications%20like%20view%20synthesis%2C%20relighting%2C%20and%20scene%20editing.%20In%20recent%20years%2C%20inverse%20rendering%20methods%20have%20been%20inspired%20by%20view%20synthesis%20approaches%20like%20neural%20radiance%20fields%20and%20Gaussian%20splatting%2C%20which%20are%20capable%20of%20efficiently%20decomposing%20a%20scene%20into%20its%20geometry%20and%20radiance.%20They%20then%20further%20estimate%20the%20material%20and%20lighting%20that%20lead%20to%20the%20observed%20scene%20radiance.%20However%2C%20the%20latter%20step%20is%20highly%20ambiguous%20and%20prior%20works%20suffer%20from%20inaccurate%20color%20and%20baked%20shadows%20in%20their%20albedo%20estimation%20albeit%20their%20regularization.%20To%20this%20end%2C%20we%20propose%20RotLight%2C%20a%20simple%20capturing%20setup%2C%20to%20address%20the%20ambiguity.%20Compared%20to%20a%20usual%20capture%2C%20RotLight%20only%20requires%20the%20object%20to%20be%20rotated%20several%20times%20during%20the%20process.%20We%20show%20that%20as%20few%20as%20two%20rotations%20is%20effective%20in%20reducing%20artifacts.%20To%20further%20improve%202DGS-based%20inverse%20rendering%2C%20we%20additionally%20introduce%20a%20proxy%20mesh%20that%20not%20only%20allows%20accurate%20incident%20light%20tracing%2C%20but%20also%20enables%20a%20residual%20constraint%20and%20improves%20global%20illumination%20handling.%20We%20demonstrate%20with%20both%20synthetic%20and%20real%20world%20datasets%20that%20our%20method%20achieves%20superior%20albedo%20estimation%20while%20keeping%20efficient%20computation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotated%2520Lights%2520for%2520Consistent%2520and%2520Efficient%25202D%2520Gaussians%2520Inverse%2520Rendering%26entry.906535625%3DGeng%2520Lin%2520and%2520Matthias%2520Zwicker%26entry.1292438233%3DInverse%2520rendering%2520aims%2520to%2520decompose%2520a%2520scene%2520into%2520its%2520geometry%252C%2520material%2520properties%2520and%2520light%2520conditions%2520under%2520a%2520certain%2520rendering%2520model.%2520It%2520has%2520wide%2520applications%2520like%2520view%2520synthesis%252C%2520relighting%252C%2520and%2520scene%2520editing.%2520In%2520recent%2520years%252C%2520inverse%2520rendering%2520methods%2520have%2520been%2520inspired%2520by%2520view%2520synthesis%2520approaches%2520like%2520neural%2520radiance%2520fields%2520and%2520Gaussian%2520splatting%252C%2520which%2520are%2520capable%2520of%2520efficiently%2520decomposing%2520a%2520scene%2520into%2520its%2520geometry%2520and%2520radiance.%2520They%2520then%2520further%2520estimate%2520the%2520material%2520and%2520lighting%2520that%2520lead%2520to%2520the%2520observed%2520scene%2520radiance.%2520However%252C%2520the%2520latter%2520step%2520is%2520highly%2520ambiguous%2520and%2520prior%2520works%2520suffer%2520from%2520inaccurate%2520color%2520and%2520baked%2520shadows%2520in%2520their%2520albedo%2520estimation%2520albeit%2520their%2520regularization.%2520To%2520this%2520end%252C%2520we%2520propose%2520RotLight%252C%2520a%2520simple%2520capturing%2520setup%252C%2520to%2520address%2520the%2520ambiguity.%2520Compared%2520to%2520a%2520usual%2520capture%252C%2520RotLight%2520only%2520requires%2520the%2520object%2520to%2520be%2520rotated%2520several%2520times%2520during%2520the%2520process.%2520We%2520show%2520that%2520as%2520few%2520as%2520two%2520rotations%2520is%2520effective%2520in%2520reducing%2520artifacts.%2520To%2520further%2520improve%25202DGS-based%2520inverse%2520rendering%252C%2520we%2520additionally%2520introduce%2520a%2520proxy%2520mesh%2520that%2520not%2520only%2520allows%2520accurate%2520incident%2520light%2520tracing%252C%2520but%2520also%2520enables%2520a%2520residual%2520constraint%2520and%2520improves%2520global%2520illumination%2520handling.%2520We%2520demonstrate%2520with%2520both%2520synthetic%2520and%2520real%2520world%2520datasets%2520that%2520our%2520method%2520achieves%2520superior%2520albedo%2520estimation%2520while%2520keeping%2520efficient%2520computation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotated%20Lights%20for%20Consistent%20and%20Efficient%202D%20Gaussians%20Inverse%20Rendering&entry.906535625=Geng%20Lin%20and%20Matthias%20Zwicker&entry.1292438233=Inverse%20rendering%20aims%20to%20decompose%20a%20scene%20into%20its%20geometry%2C%20material%20properties%20and%20light%20conditions%20under%20a%20certain%20rendering%20model.%20It%20has%20wide%20applications%20like%20view%20synthesis%2C%20relighting%2C%20and%20scene%20editing.%20In%20recent%20years%2C%20inverse%20rendering%20methods%20have%20been%20inspired%20by%20view%20synthesis%20approaches%20like%20neural%20radiance%20fields%20and%20Gaussian%20splatting%2C%20which%20are%20capable%20of%20efficiently%20decomposing%20a%20scene%20into%20its%20geometry%20and%20radiance.%20They%20then%20further%20estimate%20the%20material%20and%20lighting%20that%20lead%20to%20the%20observed%20scene%20radiance.%20However%2C%20the%20latter%20step%20is%20highly%20ambiguous%20and%20prior%20works%20suffer%20from%20inaccurate%20color%20and%20baked%20shadows%20in%20their%20albedo%20estimation%20albeit%20their%20regularization.%20To%20this%20end%2C%20we%20propose%20RotLight%2C%20a%20simple%20capturing%20setup%2C%20to%20address%20the%20ambiguity.%20Compared%20to%20a%20usual%20capture%2C%20RotLight%20only%20requires%20the%20object%20to%20be%20rotated%20several%20times%20during%20the%20process.%20We%20show%20that%20as%20few%20as%20two%20rotations%20is%20effective%20in%20reducing%20artifacts.%20To%20further%20improve%202DGS-based%20inverse%20rendering%2C%20we%20additionally%20introduce%20a%20proxy%20mesh%20that%20not%20only%20allows%20accurate%20incident%20light%20tracing%2C%20but%20also%20enables%20a%20residual%20constraint%20and%20improves%20global%20illumination%20handling.%20We%20demonstrate%20with%20both%20synthetic%20and%20real%20world%20datasets%20that%20our%20method%20achieves%20superior%20albedo%20estimation%20while%20keeping%20efficient%20computation.&entry.1838667208=http%3A//arxiv.org/abs/2602.08724v1&entry.124074799=Read"},
{"title": "FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction", "author": "Guan Yuan Tan and Ngoc Tuan Vu and Arghya Pal and Sailaja Rajanala and Raphael Phan C. -W. and Mettu Srinivas and Chee-Ming Ting", "abstract": "We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.", "link": "http://arxiv.org/abs/2602.08558v1", "date": "2026-02-09", "relevancy": 3.0539, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6367}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6151}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAG-4D%3A%20Flow-Guided%20Local-Global%20Dual-Deformation%20Model%20for%204D%20Reconstruction&body=Title%3A%20FLAG-4D%3A%20Flow-Guided%20Local-Global%20Dual-Deformation%20Model%20for%204D%20Reconstruction%0AAuthor%3A%20Guan%20Yuan%20Tan%20and%20Ngoc%20Tuan%20Vu%20and%20Arghya%20Pal%20and%20Sailaja%20Rajanala%20and%20Raphael%20Phan%20C.%20-W.%20and%20Mettu%20Srinivas%20and%20Chee-Ming%20Ting%0AAbstract%3A%20We%20introduce%20FLAG-4D%2C%20a%20novel%20framework%20for%20generating%20novel%20views%20of%20dynamic%20scenes%20by%20reconstructing%20how%203D%20Gaussian%20primitives%20evolve%20through%20space%20and%20time.%20Existing%20methods%20typically%20rely%20on%20a%20single%20Multilayer%20Perceptron%20%28MLP%29%20to%20model%20temporal%20deformations%2C%20and%20they%20often%20struggle%20to%20capture%20complex%20point%20motions%20and%20fine-grained%20dynamic%20details%20consistently%20over%20time%2C%20especially%20from%20sparse%20input%20views.%20Our%20approach%2C%20FLAG-4D%2C%20overcomes%20this%20by%20employing%20a%20dual-deformation%20network%20that%20dynamically%20warps%20a%20canonical%20set%20of%203D%20Gaussians%20over%20time%20into%20new%20positions%20and%20anisotropic%20shapes.%20This%20dual-deformation%20network%20consists%20of%20an%20Instantaneous%20Deformation%20Network%20%28IDN%29%20for%20modeling%20fine-grained%2C%20local%20deformations%20and%20a%20Global%20Motion%20Network%20%28GMN%29%20for%20capturing%20long-range%20dynamics%2C%20refined%20through%20mutual%20learning.%20To%20ensure%20these%20deformations%20are%20both%20accurate%20and%20temporally%20smooth%2C%20FLAG-4D%20incorporates%20dense%20motion%20features%20from%20a%20pretrained%20optical%20flow%20backbone.%20We%20fuse%20these%20motion%20cues%20from%20adjacent%20timeframes%20and%20use%20a%20deformation-guided%20attention%20mechanism%20to%20align%20this%20flow%20information%20with%20the%20current%20state%20of%20each%20evolving%203D%20Gaussian.%20Extensive%20experiments%20demonstrate%20that%20FLAG-4D%20achieves%20higher-fidelity%20and%20more%20temporally%20coherent%20reconstructions%20with%20finer%20detail%20preservation%20than%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAG-4D%253A%2520Flow-Guided%2520Local-Global%2520Dual-Deformation%2520Model%2520for%25204D%2520Reconstruction%26entry.906535625%3DGuan%2520Yuan%2520Tan%2520and%2520Ngoc%2520Tuan%2520Vu%2520and%2520Arghya%2520Pal%2520and%2520Sailaja%2520Rajanala%2520and%2520Raphael%2520Phan%2520C.%2520-W.%2520and%2520Mettu%2520Srinivas%2520and%2520Chee-Ming%2520Ting%26entry.1292438233%3DWe%2520introduce%2520FLAG-4D%252C%2520a%2520novel%2520framework%2520for%2520generating%2520novel%2520views%2520of%2520dynamic%2520scenes%2520by%2520reconstructing%2520how%25203D%2520Gaussian%2520primitives%2520evolve%2520through%2520space%2520and%2520time.%2520Existing%2520methods%2520typically%2520rely%2520on%2520a%2520single%2520Multilayer%2520Perceptron%2520%2528MLP%2529%2520to%2520model%2520temporal%2520deformations%252C%2520and%2520they%2520often%2520struggle%2520to%2520capture%2520complex%2520point%2520motions%2520and%2520fine-grained%2520dynamic%2520details%2520consistently%2520over%2520time%252C%2520especially%2520from%2520sparse%2520input%2520views.%2520Our%2520approach%252C%2520FLAG-4D%252C%2520overcomes%2520this%2520by%2520employing%2520a%2520dual-deformation%2520network%2520that%2520dynamically%2520warps%2520a%2520canonical%2520set%2520of%25203D%2520Gaussians%2520over%2520time%2520into%2520new%2520positions%2520and%2520anisotropic%2520shapes.%2520This%2520dual-deformation%2520network%2520consists%2520of%2520an%2520Instantaneous%2520Deformation%2520Network%2520%2528IDN%2529%2520for%2520modeling%2520fine-grained%252C%2520local%2520deformations%2520and%2520a%2520Global%2520Motion%2520Network%2520%2528GMN%2529%2520for%2520capturing%2520long-range%2520dynamics%252C%2520refined%2520through%2520mutual%2520learning.%2520To%2520ensure%2520these%2520deformations%2520are%2520both%2520accurate%2520and%2520temporally%2520smooth%252C%2520FLAG-4D%2520incorporates%2520dense%2520motion%2520features%2520from%2520a%2520pretrained%2520optical%2520flow%2520backbone.%2520We%2520fuse%2520these%2520motion%2520cues%2520from%2520adjacent%2520timeframes%2520and%2520use%2520a%2520deformation-guided%2520attention%2520mechanism%2520to%2520align%2520this%2520flow%2520information%2520with%2520the%2520current%2520state%2520of%2520each%2520evolving%25203D%2520Gaussian.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FLAG-4D%2520achieves%2520higher-fidelity%2520and%2520more%2520temporally%2520coherent%2520reconstructions%2520with%2520finer%2520detail%2520preservation%2520than%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAG-4D%3A%20Flow-Guided%20Local-Global%20Dual-Deformation%20Model%20for%204D%20Reconstruction&entry.906535625=Guan%20Yuan%20Tan%20and%20Ngoc%20Tuan%20Vu%20and%20Arghya%20Pal%20and%20Sailaja%20Rajanala%20and%20Raphael%20Phan%20C.%20-W.%20and%20Mettu%20Srinivas%20and%20Chee-Ming%20Ting&entry.1292438233=We%20introduce%20FLAG-4D%2C%20a%20novel%20framework%20for%20generating%20novel%20views%20of%20dynamic%20scenes%20by%20reconstructing%20how%203D%20Gaussian%20primitives%20evolve%20through%20space%20and%20time.%20Existing%20methods%20typically%20rely%20on%20a%20single%20Multilayer%20Perceptron%20%28MLP%29%20to%20model%20temporal%20deformations%2C%20and%20they%20often%20struggle%20to%20capture%20complex%20point%20motions%20and%20fine-grained%20dynamic%20details%20consistently%20over%20time%2C%20especially%20from%20sparse%20input%20views.%20Our%20approach%2C%20FLAG-4D%2C%20overcomes%20this%20by%20employing%20a%20dual-deformation%20network%20that%20dynamically%20warps%20a%20canonical%20set%20of%203D%20Gaussians%20over%20time%20into%20new%20positions%20and%20anisotropic%20shapes.%20This%20dual-deformation%20network%20consists%20of%20an%20Instantaneous%20Deformation%20Network%20%28IDN%29%20for%20modeling%20fine-grained%2C%20local%20deformations%20and%20a%20Global%20Motion%20Network%20%28GMN%29%20for%20capturing%20long-range%20dynamics%2C%20refined%20through%20mutual%20learning.%20To%20ensure%20these%20deformations%20are%20both%20accurate%20and%20temporally%20smooth%2C%20FLAG-4D%20incorporates%20dense%20motion%20features%20from%20a%20pretrained%20optical%20flow%20backbone.%20We%20fuse%20these%20motion%20cues%20from%20adjacent%20timeframes%20and%20use%20a%20deformation-guided%20attention%20mechanism%20to%20align%20this%20flow%20information%20with%20the%20current%20state%20of%20each%20evolving%203D%20Gaussian.%20Extensive%20experiments%20demonstrate%20that%20FLAG-4D%20achieves%20higher-fidelity%20and%20more%20temporally%20coherent%20reconstructions%20with%20finer%20detail%20preservation%20than%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.08558v1&entry.124074799=Read"},
{"title": "Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?", "author": "Caterina Fuster-Barcel\u00f3 and Virginie Uhlmann", "abstract": "Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fr\u00e9chet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.", "link": "http://arxiv.org/abs/2602.08505v1", "date": "2026-02-09", "relevancy": 2.9738, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision%20Foundation%20Models%20Foundational%20for%20Electron%20Microscopy%20Image%20Segmentation%3F&body=Title%3A%20Are%20Vision%20Foundation%20Models%20Foundational%20for%20Electron%20Microscopy%20Image%20Segmentation%3F%0AAuthor%3A%20Caterina%20Fuster-Barcel%C3%B3%20and%20Virginie%20Uhlmann%0AAbstract%3A%20Although%20vision%20foundation%20models%20%28VFMs%29%20are%20increasingly%20reused%20for%20biomedical%20image%20analysis%2C%20it%20remains%20unclear%20whether%20the%20latent%20representations%20they%20provide%20are%20general%20enough%20to%20support%20effective%20transfer%20and%20reuse%20across%20heterogeneous%20microscopy%20image%20datasets.%20Here%2C%20we%20study%20this%20question%20for%20the%20problem%20of%20mitochondria%20segmentation%20in%20electron%20microscopy%20%28EM%29%20images%2C%20using%20two%20popular%20public%20EM%20datasets%20%28Lucchi%2B%2B%20and%20VNC%29%20and%20three%20recent%20representative%20VFMs%20%28DINOv2%2C%20DINOv3%2C%20and%20OpenCLIP%29.%20We%20evaluate%20two%20practical%20model%20adaptation%20regimes%3A%20a%20frozen-backbone%20setting%20in%20which%20only%20a%20lightweight%20segmentation%20head%20is%20trained%20on%20top%20of%20the%20VFM%2C%20and%20parameter-efficient%20fine-tuning%20%28PEFT%29%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20in%20which%20the%20VFM%20is%20fine-tuned%20in%20a%20targeted%20manner%20to%20a%20specific%20dataset.%20Across%20all%20backbones%2C%20we%20observe%20that%20training%20on%20a%20single%20EM%20dataset%20yields%20good%20segmentation%20performance%20%28quantified%20as%20foreground%20Intersection-over-Union%29%2C%20and%20that%20LoRA%20consistently%20improves%20in-domain%20performance.%20In%20contrast%2C%20training%20on%20multiple%20EM%20datasets%20leads%20to%20severe%20performance%20degradation%20for%20all%20models%20considered%2C%20with%20only%20marginal%20gains%20from%20PEFT.%20Exploration%20of%20the%20latent%20representation%20space%20through%20various%20techniques%20%28PCA%2C%20Fr%C3%A9chet%20Dinov2%20distance%2C%20and%20linear%20probes%29%20reveals%20a%20pronounced%20and%20persistent%20domain%20mismatch%20between%20the%20two%20considered%20EM%20datasets%20in%20spite%20of%20their%20visual%20similarity%2C%20which%20is%20consistent%20with%20the%20observed%20failure%20of%20paired%20training.%20These%20results%20suggest%20that%2C%20while%20VFMs%20can%20deliver%20competitive%20results%20for%20EM%20segmentation%20within%20a%20single%20domain%20under%20lightweight%20adaptation%2C%20current%20PEFT%20strategies%20are%20insufficient%20to%20obtain%20a%20single%20robust%20model%20across%20heterogeneous%20EM%20datasets%20without%20additional%20domain-alignment%20mechanisms.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision%2520Foundation%2520Models%2520Foundational%2520for%2520Electron%2520Microscopy%2520Image%2520Segmentation%253F%26entry.906535625%3DCaterina%2520Fuster-Barcel%25C3%25B3%2520and%2520Virginie%2520Uhlmann%26entry.1292438233%3DAlthough%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520are%2520increasingly%2520reused%2520for%2520biomedical%2520image%2520analysis%252C%2520it%2520remains%2520unclear%2520whether%2520the%2520latent%2520representations%2520they%2520provide%2520are%2520general%2520enough%2520to%2520support%2520effective%2520transfer%2520and%2520reuse%2520across%2520heterogeneous%2520microscopy%2520image%2520datasets.%2520Here%252C%2520we%2520study%2520this%2520question%2520for%2520the%2520problem%2520of%2520mitochondria%2520segmentation%2520in%2520electron%2520microscopy%2520%2528EM%2529%2520images%252C%2520using%2520two%2520popular%2520public%2520EM%2520datasets%2520%2528Lucchi%252B%252B%2520and%2520VNC%2529%2520and%2520three%2520recent%2520representative%2520VFMs%2520%2528DINOv2%252C%2520DINOv3%252C%2520and%2520OpenCLIP%2529.%2520We%2520evaluate%2520two%2520practical%2520model%2520adaptation%2520regimes%253A%2520a%2520frozen-backbone%2520setting%2520in%2520which%2520only%2520a%2520lightweight%2520segmentation%2520head%2520is%2520trained%2520on%2520top%2520of%2520the%2520VFM%252C%2520and%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520via%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520in%2520which%2520the%2520VFM%2520is%2520fine-tuned%2520in%2520a%2520targeted%2520manner%2520to%2520a%2520specific%2520dataset.%2520Across%2520all%2520backbones%252C%2520we%2520observe%2520that%2520training%2520on%2520a%2520single%2520EM%2520dataset%2520yields%2520good%2520segmentation%2520performance%2520%2528quantified%2520as%2520foreground%2520Intersection-over-Union%2529%252C%2520and%2520that%2520LoRA%2520consistently%2520improves%2520in-domain%2520performance.%2520In%2520contrast%252C%2520training%2520on%2520multiple%2520EM%2520datasets%2520leads%2520to%2520severe%2520performance%2520degradation%2520for%2520all%2520models%2520considered%252C%2520with%2520only%2520marginal%2520gains%2520from%2520PEFT.%2520Exploration%2520of%2520the%2520latent%2520representation%2520space%2520through%2520various%2520techniques%2520%2528PCA%252C%2520Fr%25C3%25A9chet%2520Dinov2%2520distance%252C%2520and%2520linear%2520probes%2529%2520reveals%2520a%2520pronounced%2520and%2520persistent%2520domain%2520mismatch%2520between%2520the%2520two%2520considered%2520EM%2520datasets%2520in%2520spite%2520of%2520their%2520visual%2520similarity%252C%2520which%2520is%2520consistent%2520with%2520the%2520observed%2520failure%2520of%2520paired%2520training.%2520These%2520results%2520suggest%2520that%252C%2520while%2520VFMs%2520can%2520deliver%2520competitive%2520results%2520for%2520EM%2520segmentation%2520within%2520a%2520single%2520domain%2520under%2520lightweight%2520adaptation%252C%2520current%2520PEFT%2520strategies%2520are%2520insufficient%2520to%2520obtain%2520a%2520single%2520robust%2520model%2520across%2520heterogeneous%2520EM%2520datasets%2520without%2520additional%2520domain-alignment%2520mechanisms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision%20Foundation%20Models%20Foundational%20for%20Electron%20Microscopy%20Image%20Segmentation%3F&entry.906535625=Caterina%20Fuster-Barcel%C3%B3%20and%20Virginie%20Uhlmann&entry.1292438233=Although%20vision%20foundation%20models%20%28VFMs%29%20are%20increasingly%20reused%20for%20biomedical%20image%20analysis%2C%20it%20remains%20unclear%20whether%20the%20latent%20representations%20they%20provide%20are%20general%20enough%20to%20support%20effective%20transfer%20and%20reuse%20across%20heterogeneous%20microscopy%20image%20datasets.%20Here%2C%20we%20study%20this%20question%20for%20the%20problem%20of%20mitochondria%20segmentation%20in%20electron%20microscopy%20%28EM%29%20images%2C%20using%20two%20popular%20public%20EM%20datasets%20%28Lucchi%2B%2B%20and%20VNC%29%20and%20three%20recent%20representative%20VFMs%20%28DINOv2%2C%20DINOv3%2C%20and%20OpenCLIP%29.%20We%20evaluate%20two%20practical%20model%20adaptation%20regimes%3A%20a%20frozen-backbone%20setting%20in%20which%20only%20a%20lightweight%20segmentation%20head%20is%20trained%20on%20top%20of%20the%20VFM%2C%20and%20parameter-efficient%20fine-tuning%20%28PEFT%29%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20in%20which%20the%20VFM%20is%20fine-tuned%20in%20a%20targeted%20manner%20to%20a%20specific%20dataset.%20Across%20all%20backbones%2C%20we%20observe%20that%20training%20on%20a%20single%20EM%20dataset%20yields%20good%20segmentation%20performance%20%28quantified%20as%20foreground%20Intersection-over-Union%29%2C%20and%20that%20LoRA%20consistently%20improves%20in-domain%20performance.%20In%20contrast%2C%20training%20on%20multiple%20EM%20datasets%20leads%20to%20severe%20performance%20degradation%20for%20all%20models%20considered%2C%20with%20only%20marginal%20gains%20from%20PEFT.%20Exploration%20of%20the%20latent%20representation%20space%20through%20various%20techniques%20%28PCA%2C%20Fr%C3%A9chet%20Dinov2%20distance%2C%20and%20linear%20probes%29%20reveals%20a%20pronounced%20and%20persistent%20domain%20mismatch%20between%20the%20two%20considered%20EM%20datasets%20in%20spite%20of%20their%20visual%20similarity%2C%20which%20is%20consistent%20with%20the%20observed%20failure%20of%20paired%20training.%20These%20results%20suggest%20that%2C%20while%20VFMs%20can%20deliver%20competitive%20results%20for%20EM%20segmentation%20within%20a%20single%20domain%20under%20lightweight%20adaptation%2C%20current%20PEFT%20strategies%20are%20insufficient%20to%20obtain%20a%20single%20robust%20model%20across%20heterogeneous%20EM%20datasets%20without%20additional%20domain-alignment%20mechanisms.&entry.1838667208=http%3A//arxiv.org/abs/2602.08505v1&entry.124074799=Read"},
{"title": "PAL-Net: A Point-Wise CNN with Patch-Attention for 3D Facial Landmark Localization", "author": "Ali Shadman Yazdi and Annalisa Cappella and Benedetta Baldini and Riccardo Solazzo and Gianluca Tartaglia and Chiarella Sforza and Giuseppe Baselli", "abstract": "Manual annotation of anatomical landmarks on 3D facial scans is a time-consuming and expertise-dependent task, yet it remains critical for clinical assessments, morphometric analysis, and craniofacial research. While several deep learning methods have been proposed for facial landmark localization, most focus on pseudo-landmarks or require complex input representations, limiting their clinical applicability. This study presents a fully automated deep learning pipeline (PAL-Net) for localizing 50 anatomical landmarks on stereo-photogrammetry facial models. The method combines coarse alignment, region-of-interest filtering, and an initial approximation of landmarks with a patch-based pointwise CNN enhanced by attention mechanisms. Trained and evaluated on 214 annotated scans from healthy adults, PAL-Net achieved a mean localization error of 3.686 mm and preserves relevant anatomical distances with a 2.822 mm average error, comparable to intra-observer variability. To assess generalization, the model was further evaluated on 700 subjects from the FaceScape dataset, achieving a point-wise error of 0.41\\,mm and a distance-wise error of 0.38\\,mm. Compared to existing methods, PAL-Net offers a favorable trade-off between accuracy and computational cost. While performance degrades in regions with poor mesh quality (e.g., ears, hairline), the method demonstrates consistent accuracy across most anatomical regions. PAL-Net generalizes effectively across datasets and facial regions, outperforming existing methods in both point-wise and structural evaluations. It provides a lightweight, scalable solution for high-throughput 3D anthropometric analysis, with potential to support clinical workflows and reduce reliance on manual annotation. Source code can be found at https://github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention", "link": "http://arxiv.org/abs/2510.00910v2", "date": "2026-02-09", "relevancy": 2.9602, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAL-Net%3A%20A%20Point-Wise%20CNN%20with%20Patch-Attention%20for%203D%20Facial%20Landmark%20Localization&body=Title%3A%20PAL-Net%3A%20A%20Point-Wise%20CNN%20with%20Patch-Attention%20for%203D%20Facial%20Landmark%20Localization%0AAuthor%3A%20Ali%20Shadman%20Yazdi%20and%20Annalisa%20Cappella%20and%20Benedetta%20Baldini%20and%20Riccardo%20Solazzo%20and%20Gianluca%20Tartaglia%20and%20Chiarella%20Sforza%20and%20Giuseppe%20Baselli%0AAbstract%3A%20Manual%20annotation%20of%20anatomical%20landmarks%20on%203D%20facial%20scans%20is%20a%20time-consuming%20and%20expertise-dependent%20task%2C%20yet%20it%20remains%20critical%20for%20clinical%20assessments%2C%20morphometric%20analysis%2C%20and%20craniofacial%20research.%20While%20several%20deep%20learning%20methods%20have%20been%20proposed%20for%20facial%20landmark%20localization%2C%20most%20focus%20on%20pseudo-landmarks%20or%20require%20complex%20input%20representations%2C%20limiting%20their%20clinical%20applicability.%20This%20study%20presents%20a%20fully%20automated%20deep%20learning%20pipeline%20%28PAL-Net%29%20for%20localizing%2050%20anatomical%20landmarks%20on%20stereo-photogrammetry%20facial%20models.%20The%20method%20combines%20coarse%20alignment%2C%20region-of-interest%20filtering%2C%20and%20an%20initial%20approximation%20of%20landmarks%20with%20a%20patch-based%20pointwise%20CNN%20enhanced%20by%20attention%20mechanisms.%20Trained%20and%20evaluated%20on%20214%20annotated%20scans%20from%20healthy%20adults%2C%20PAL-Net%20achieved%20a%20mean%20localization%20error%20of%203.686%20mm%20and%20preserves%20relevant%20anatomical%20distances%20with%20a%202.822%20mm%20average%20error%2C%20comparable%20to%20intra-observer%20variability.%20To%20assess%20generalization%2C%20the%20model%20was%20further%20evaluated%20on%20700%20subjects%20from%20the%20FaceScape%20dataset%2C%20achieving%20a%20point-wise%20error%20of%200.41%5C%2Cmm%20and%20a%20distance-wise%20error%20of%200.38%5C%2Cmm.%20Compared%20to%20existing%20methods%2C%20PAL-Net%20offers%20a%20favorable%20trade-off%20between%20accuracy%20and%20computational%20cost.%20While%20performance%20degrades%20in%20regions%20with%20poor%20mesh%20quality%20%28e.g.%2C%20ears%2C%20hairline%29%2C%20the%20method%20demonstrates%20consistent%20accuracy%20across%20most%20anatomical%20regions.%20PAL-Net%20generalizes%20effectively%20across%20datasets%20and%20facial%20regions%2C%20outperforming%20existing%20methods%20in%20both%20point-wise%20and%20structural%20evaluations.%20It%20provides%20a%20lightweight%2C%20scalable%20solution%20for%20high-throughput%203D%20anthropometric%20analysis%2C%20with%20potential%20to%20support%20clinical%20workflows%20and%20reduce%20reliance%20on%20manual%20annotation.%20Source%20code%20can%20be%20found%20at%20https%3A//github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention%0ALink%3A%20http%3A//arxiv.org/abs/2510.00910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAL-Net%253A%2520A%2520Point-Wise%2520CNN%2520with%2520Patch-Attention%2520for%25203D%2520Facial%2520Landmark%2520Localization%26entry.906535625%3DAli%2520Shadman%2520Yazdi%2520and%2520Annalisa%2520Cappella%2520and%2520Benedetta%2520Baldini%2520and%2520Riccardo%2520Solazzo%2520and%2520Gianluca%2520Tartaglia%2520and%2520Chiarella%2520Sforza%2520and%2520Giuseppe%2520Baselli%26entry.1292438233%3DManual%2520annotation%2520of%2520anatomical%2520landmarks%2520on%25203D%2520facial%2520scans%2520is%2520a%2520time-consuming%2520and%2520expertise-dependent%2520task%252C%2520yet%2520it%2520remains%2520critical%2520for%2520clinical%2520assessments%252C%2520morphometric%2520analysis%252C%2520and%2520craniofacial%2520research.%2520While%2520several%2520deep%2520learning%2520methods%2520have%2520been%2520proposed%2520for%2520facial%2520landmark%2520localization%252C%2520most%2520focus%2520on%2520pseudo-landmarks%2520or%2520require%2520complex%2520input%2520representations%252C%2520limiting%2520their%2520clinical%2520applicability.%2520This%2520study%2520presents%2520a%2520fully%2520automated%2520deep%2520learning%2520pipeline%2520%2528PAL-Net%2529%2520for%2520localizing%252050%2520anatomical%2520landmarks%2520on%2520stereo-photogrammetry%2520facial%2520models.%2520The%2520method%2520combines%2520coarse%2520alignment%252C%2520region-of-interest%2520filtering%252C%2520and%2520an%2520initial%2520approximation%2520of%2520landmarks%2520with%2520a%2520patch-based%2520pointwise%2520CNN%2520enhanced%2520by%2520attention%2520mechanisms.%2520Trained%2520and%2520evaluated%2520on%2520214%2520annotated%2520scans%2520from%2520healthy%2520adults%252C%2520PAL-Net%2520achieved%2520a%2520mean%2520localization%2520error%2520of%25203.686%2520mm%2520and%2520preserves%2520relevant%2520anatomical%2520distances%2520with%2520a%25202.822%2520mm%2520average%2520error%252C%2520comparable%2520to%2520intra-observer%2520variability.%2520To%2520assess%2520generalization%252C%2520the%2520model%2520was%2520further%2520evaluated%2520on%2520700%2520subjects%2520from%2520the%2520FaceScape%2520dataset%252C%2520achieving%2520a%2520point-wise%2520error%2520of%25200.41%255C%252Cmm%2520and%2520a%2520distance-wise%2520error%2520of%25200.38%255C%252Cmm.%2520Compared%2520to%2520existing%2520methods%252C%2520PAL-Net%2520offers%2520a%2520favorable%2520trade-off%2520between%2520accuracy%2520and%2520computational%2520cost.%2520While%2520performance%2520degrades%2520in%2520regions%2520with%2520poor%2520mesh%2520quality%2520%2528e.g.%252C%2520ears%252C%2520hairline%2529%252C%2520the%2520method%2520demonstrates%2520consistent%2520accuracy%2520across%2520most%2520anatomical%2520regions.%2520PAL-Net%2520generalizes%2520effectively%2520across%2520datasets%2520and%2520facial%2520regions%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520point-wise%2520and%2520structural%2520evaluations.%2520It%2520provides%2520a%2520lightweight%252C%2520scalable%2520solution%2520for%2520high-throughput%25203D%2520anthropometric%2520analysis%252C%2520with%2520potential%2520to%2520support%2520clinical%2520workflows%2520and%2520reduce%2520reliance%2520on%2520manual%2520annotation.%2520Source%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAL-Net%3A%20A%20Point-Wise%20CNN%20with%20Patch-Attention%20for%203D%20Facial%20Landmark%20Localization&entry.906535625=Ali%20Shadman%20Yazdi%20and%20Annalisa%20Cappella%20and%20Benedetta%20Baldini%20and%20Riccardo%20Solazzo%20and%20Gianluca%20Tartaglia%20and%20Chiarella%20Sforza%20and%20Giuseppe%20Baselli&entry.1292438233=Manual%20annotation%20of%20anatomical%20landmarks%20on%203D%20facial%20scans%20is%20a%20time-consuming%20and%20expertise-dependent%20task%2C%20yet%20it%20remains%20critical%20for%20clinical%20assessments%2C%20morphometric%20analysis%2C%20and%20craniofacial%20research.%20While%20several%20deep%20learning%20methods%20have%20been%20proposed%20for%20facial%20landmark%20localization%2C%20most%20focus%20on%20pseudo-landmarks%20or%20require%20complex%20input%20representations%2C%20limiting%20their%20clinical%20applicability.%20This%20study%20presents%20a%20fully%20automated%20deep%20learning%20pipeline%20%28PAL-Net%29%20for%20localizing%2050%20anatomical%20landmarks%20on%20stereo-photogrammetry%20facial%20models.%20The%20method%20combines%20coarse%20alignment%2C%20region-of-interest%20filtering%2C%20and%20an%20initial%20approximation%20of%20landmarks%20with%20a%20patch-based%20pointwise%20CNN%20enhanced%20by%20attention%20mechanisms.%20Trained%20and%20evaluated%20on%20214%20annotated%20scans%20from%20healthy%20adults%2C%20PAL-Net%20achieved%20a%20mean%20localization%20error%20of%203.686%20mm%20and%20preserves%20relevant%20anatomical%20distances%20with%20a%202.822%20mm%20average%20error%2C%20comparable%20to%20intra-observer%20variability.%20To%20assess%20generalization%2C%20the%20model%20was%20further%20evaluated%20on%20700%20subjects%20from%20the%20FaceScape%20dataset%2C%20achieving%20a%20point-wise%20error%20of%200.41%5C%2Cmm%20and%20a%20distance-wise%20error%20of%200.38%5C%2Cmm.%20Compared%20to%20existing%20methods%2C%20PAL-Net%20offers%20a%20favorable%20trade-off%20between%20accuracy%20and%20computational%20cost.%20While%20performance%20degrades%20in%20regions%20with%20poor%20mesh%20quality%20%28e.g.%2C%20ears%2C%20hairline%29%2C%20the%20method%20demonstrates%20consistent%20accuracy%20across%20most%20anatomical%20regions.%20PAL-Net%20generalizes%20effectively%20across%20datasets%20and%20facial%20regions%2C%20outperforming%20existing%20methods%20in%20both%20point-wise%20and%20structural%20evaluations.%20It%20provides%20a%20lightweight%2C%20scalable%20solution%20for%20high-throughput%203D%20anthropometric%20analysis%2C%20with%20potential%20to%20support%20clinical%20workflows%20and%20reduce%20reliance%20on%20manual%20annotation.%20Source%20code%20can%20be%20found%20at%20https%3A//github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention&entry.1838667208=http%3A//arxiv.org/abs/2510.00910v2&entry.124074799=Read"},
{"title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models", "author": "Masanari Oi and Koki Maeda and Ryuto Koike and Daisuke Oba and Nakamasa Inoue and Naoaki Okazaki", "abstract": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.", "link": "http://arxiv.org/abs/2602.08735v1", "date": "2026-02-09", "relevancy": 2.9576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Correspondence%20to%20Actions%3A%20Human-Like%20Multi-Image%20Spatial%20Reasoning%20in%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20From%20Correspondence%20to%20Actions%3A%20Human-Like%20Multi-Image%20Spatial%20Reasoning%20in%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Masanari%20Oi%20and%20Koki%20Maeda%20and%20Ryuto%20Koike%20and%20Daisuke%20Oba%20and%20Nakamasa%20Inoue%20and%20Naoaki%20Okazaki%0AAbstract%3A%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20substantial%20progress%20in%20single-image%20spatial%20reasoning%2C%20multi-image%20spatial%20reasoning%2C%20which%20requires%20integration%20of%20information%20from%20multiple%20viewpoints%2C%20remains%20challenging.%20Cognitive%20studies%20suggest%20that%20humans%20address%20such%20tasks%20through%20two%20mechanisms%3A%20cross-view%20correspondence%2C%20which%20identifies%20regions%20across%20different%20views%20that%20correspond%20to%20the%20same%20physical%20locations%2C%20and%20stepwise%20viewpoint%20transformation%2C%20which%20composes%20relative%20viewpoint%20changes%20sequentially.%20However%2C%20existing%20studies%20incorporate%20these%20mechanisms%20only%20partially%20and%20often%20implicitly%2C%20without%20explicit%20supervision%20for%20both.%20We%20propose%20Human-Aware%20Training%20for%20Cross-view%20correspondence%20and%20viewpoint%20cHange%20%28HATCH%29%2C%20a%20training%20framework%20with%20two%20complementary%20objectives%3A%20%281%29%20Patch-Level%20Spatial%20Alignment%2C%20which%20encourages%20patch%20representations%20to%20align%20across%20views%20for%20spatially%20corresponding%20regions%2C%20and%20%282%29%20Action-then-Answer%20Reasoning%2C%20which%20requires%20the%20model%20to%20generate%20explicit%20viewpoint%20transition%20actions%20before%20predicting%20the%20final%20answer.%20Experiments%20on%20three%20benchmarks%20demonstrate%20that%20HATCH%20consistently%20outperforms%20baselines%20of%20comparable%20size%20by%20a%20clear%20margin%20and%20achieves%20competitive%20results%20against%20much%20larger%20models%2C%20while%20preserving%20single-image%20reasoning%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Correspondence%2520to%2520Actions%253A%2520Human-Like%2520Multi-Image%2520Spatial%2520Reasoning%2520in%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DMasanari%2520Oi%2520and%2520Koki%2520Maeda%2520and%2520Ryuto%2520Koike%2520and%2520Daisuke%2520Oba%2520and%2520Nakamasa%2520Inoue%2520and%2520Naoaki%2520Okazaki%26entry.1292438233%3DWhile%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520substantial%2520progress%2520in%2520single-image%2520spatial%2520reasoning%252C%2520multi-image%2520spatial%2520reasoning%252C%2520which%2520requires%2520integration%2520of%2520information%2520from%2520multiple%2520viewpoints%252C%2520remains%2520challenging.%2520Cognitive%2520studies%2520suggest%2520that%2520humans%2520address%2520such%2520tasks%2520through%2520two%2520mechanisms%253A%2520cross-view%2520correspondence%252C%2520which%2520identifies%2520regions%2520across%2520different%2520views%2520that%2520correspond%2520to%2520the%2520same%2520physical%2520locations%252C%2520and%2520stepwise%2520viewpoint%2520transformation%252C%2520which%2520composes%2520relative%2520viewpoint%2520changes%2520sequentially.%2520However%252C%2520existing%2520studies%2520incorporate%2520these%2520mechanisms%2520only%2520partially%2520and%2520often%2520implicitly%252C%2520without%2520explicit%2520supervision%2520for%2520both.%2520We%2520propose%2520Human-Aware%2520Training%2520for%2520Cross-view%2520correspondence%2520and%2520viewpoint%2520cHange%2520%2528HATCH%2529%252C%2520a%2520training%2520framework%2520with%2520two%2520complementary%2520objectives%253A%2520%25281%2529%2520Patch-Level%2520Spatial%2520Alignment%252C%2520which%2520encourages%2520patch%2520representations%2520to%2520align%2520across%2520views%2520for%2520spatially%2520corresponding%2520regions%252C%2520and%2520%25282%2529%2520Action-then-Answer%2520Reasoning%252C%2520which%2520requires%2520the%2520model%2520to%2520generate%2520explicit%2520viewpoint%2520transition%2520actions%2520before%2520predicting%2520the%2520final%2520answer.%2520Experiments%2520on%2520three%2520benchmarks%2520demonstrate%2520that%2520HATCH%2520consistently%2520outperforms%2520baselines%2520of%2520comparable%2520size%2520by%2520a%2520clear%2520margin%2520and%2520achieves%2520competitive%2520results%2520against%2520much%2520larger%2520models%252C%2520while%2520preserving%2520single-image%2520reasoning%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Correspondence%20to%20Actions%3A%20Human-Like%20Multi-Image%20Spatial%20Reasoning%20in%20Multi-modal%20Large%20Language%20Models&entry.906535625=Masanari%20Oi%20and%20Koki%20Maeda%20and%20Ryuto%20Koike%20and%20Daisuke%20Oba%20and%20Nakamasa%20Inoue%20and%20Naoaki%20Okazaki&entry.1292438233=While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20substantial%20progress%20in%20single-image%20spatial%20reasoning%2C%20multi-image%20spatial%20reasoning%2C%20which%20requires%20integration%20of%20information%20from%20multiple%20viewpoints%2C%20remains%20challenging.%20Cognitive%20studies%20suggest%20that%20humans%20address%20such%20tasks%20through%20two%20mechanisms%3A%20cross-view%20correspondence%2C%20which%20identifies%20regions%20across%20different%20views%20that%20correspond%20to%20the%20same%20physical%20locations%2C%20and%20stepwise%20viewpoint%20transformation%2C%20which%20composes%20relative%20viewpoint%20changes%20sequentially.%20However%2C%20existing%20studies%20incorporate%20these%20mechanisms%20only%20partially%20and%20often%20implicitly%2C%20without%20explicit%20supervision%20for%20both.%20We%20propose%20Human-Aware%20Training%20for%20Cross-view%20correspondence%20and%20viewpoint%20cHange%20%28HATCH%29%2C%20a%20training%20framework%20with%20two%20complementary%20objectives%3A%20%281%29%20Patch-Level%20Spatial%20Alignment%2C%20which%20encourages%20patch%20representations%20to%20align%20across%20views%20for%20spatially%20corresponding%20regions%2C%20and%20%282%29%20Action-then-Answer%20Reasoning%2C%20which%20requires%20the%20model%20to%20generate%20explicit%20viewpoint%20transition%20actions%20before%20predicting%20the%20final%20answer.%20Experiments%20on%20three%20benchmarks%20demonstrate%20that%20HATCH%20consistently%20outperforms%20baselines%20of%20comparable%20size%20by%20a%20clear%20margin%20and%20achieves%20competitive%20results%20against%20much%20larger%20models%2C%20while%20preserving%20single-image%20reasoning%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2602.08735v1&entry.124074799=Read"},
{"title": "CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion", "author": "Mouad Abrini and Mohamed Chetouani", "abstract": "With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue", "link": "http://arxiv.org/abs/2602.08999v1", "date": "2026-02-09", "relevancy": 2.9502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5927}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLUE%3A%20Crossmodal%20disambiguation%20via%20Language-vision%20Understanding%20with%20attEntion&body=Title%3A%20CLUE%3A%20Crossmodal%20disambiguation%20via%20Language-vision%20Understanding%20with%20attEntion%0AAuthor%3A%20Mouad%20Abrini%20and%20Mohamed%20Chetouani%0AAbstract%3A%20With%20the%20increasing%20integration%20of%20robots%20into%20daily%20life%2C%20human-robot%20interaction%20has%20become%20more%20complex%20and%20multifaceted.%20A%20critical%20component%20of%20this%20interaction%20is%20Interactive%20Visual%20Grounding%20%28IVG%29%2C%20through%20which%20robots%20must%20interpret%20human%20intentions%20and%20resolve%20ambiguity.%20Existing%20IVG%20models%20generally%20lack%20a%20mechanism%20to%20determine%20when%20to%20ask%20clarification%20questions%2C%20as%20they%20implicitly%20rely%20on%20their%20learned%20representations.%20CLUE%20addresses%20this%20gap%20by%20converting%20the%20VLM%27s%20cross-modal%20attention%20into%20an%20explicit%2C%20spatially%20grounded%20signal%20for%20deciding%20when%20to%20ask.%20We%20extract%20text%20to%20image%20attention%20maps%20and%20pass%20them%20to%20a%20lightweight%20CNN%20to%20detect%20referential%20ambiguity%2C%20while%20a%20LoRA%20fine-tuned%20decoder%20conducts%20the%20dialog%20and%20emits%20grounding%20location%20tokens.%20We%20train%20on%20a%20real-world%20interactive%20dataset%20for%20IVG%2C%20and%20a%20mixed%20ambiguity%20set%20for%20the%20detector.%20With%20InViG-only%20supervision%2C%20our%20model%20surpasses%20a%20state-of-the-art%20method%20while%20using%20parameter-efficient%20fine-tuning.%20Similarly%2C%20the%20ambiguity%20detector%20outperforms%20prior%20baselines.%20Overall%2C%20CLUE%20turns%20the%20internal%20cross-modal%20attention%20of%20a%20VLM%20into%20an%20explicit%2C%20spatially%20grounded%20signal%20for%20deciding%20when%20to%20ask.%20The%20data%20and%20code%20are%20publicly%20available%20at%3A%20mouadabrini.github.io/clue%0ALink%3A%20http%3A//arxiv.org/abs/2602.08999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLUE%253A%2520Crossmodal%2520disambiguation%2520via%2520Language-vision%2520Understanding%2520with%2520attEntion%26entry.906535625%3DMouad%2520Abrini%2520and%2520Mohamed%2520Chetouani%26entry.1292438233%3DWith%2520the%2520increasing%2520integration%2520of%2520robots%2520into%2520daily%2520life%252C%2520human-robot%2520interaction%2520has%2520become%2520more%2520complex%2520and%2520multifaceted.%2520A%2520critical%2520component%2520of%2520this%2520interaction%2520is%2520Interactive%2520Visual%2520Grounding%2520%2528IVG%2529%252C%2520through%2520which%2520robots%2520must%2520interpret%2520human%2520intentions%2520and%2520resolve%2520ambiguity.%2520Existing%2520IVG%2520models%2520generally%2520lack%2520a%2520mechanism%2520to%2520determine%2520when%2520to%2520ask%2520clarification%2520questions%252C%2520as%2520they%2520implicitly%2520rely%2520on%2520their%2520learned%2520representations.%2520CLUE%2520addresses%2520this%2520gap%2520by%2520converting%2520the%2520VLM%2527s%2520cross-modal%2520attention%2520into%2520an%2520explicit%252C%2520spatially%2520grounded%2520signal%2520for%2520deciding%2520when%2520to%2520ask.%2520We%2520extract%2520text%2520to%2520image%2520attention%2520maps%2520and%2520pass%2520them%2520to%2520a%2520lightweight%2520CNN%2520to%2520detect%2520referential%2520ambiguity%252C%2520while%2520a%2520LoRA%2520fine-tuned%2520decoder%2520conducts%2520the%2520dialog%2520and%2520emits%2520grounding%2520location%2520tokens.%2520We%2520train%2520on%2520a%2520real-world%2520interactive%2520dataset%2520for%2520IVG%252C%2520and%2520a%2520mixed%2520ambiguity%2520set%2520for%2520the%2520detector.%2520With%2520InViG-only%2520supervision%252C%2520our%2520model%2520surpasses%2520a%2520state-of-the-art%2520method%2520while%2520using%2520parameter-efficient%2520fine-tuning.%2520Similarly%252C%2520the%2520ambiguity%2520detector%2520outperforms%2520prior%2520baselines.%2520Overall%252C%2520CLUE%2520turns%2520the%2520internal%2520cross-modal%2520attention%2520of%2520a%2520VLM%2520into%2520an%2520explicit%252C%2520spatially%2520grounded%2520signal%2520for%2520deciding%2520when%2520to%2520ask.%2520The%2520data%2520and%2520code%2520are%2520publicly%2520available%2520at%253A%2520mouadabrini.github.io/clue%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLUE%3A%20Crossmodal%20disambiguation%20via%20Language-vision%20Understanding%20with%20attEntion&entry.906535625=Mouad%20Abrini%20and%20Mohamed%20Chetouani&entry.1292438233=With%20the%20increasing%20integration%20of%20robots%20into%20daily%20life%2C%20human-robot%20interaction%20has%20become%20more%20complex%20and%20multifaceted.%20A%20critical%20component%20of%20this%20interaction%20is%20Interactive%20Visual%20Grounding%20%28IVG%29%2C%20through%20which%20robots%20must%20interpret%20human%20intentions%20and%20resolve%20ambiguity.%20Existing%20IVG%20models%20generally%20lack%20a%20mechanism%20to%20determine%20when%20to%20ask%20clarification%20questions%2C%20as%20they%20implicitly%20rely%20on%20their%20learned%20representations.%20CLUE%20addresses%20this%20gap%20by%20converting%20the%20VLM%27s%20cross-modal%20attention%20into%20an%20explicit%2C%20spatially%20grounded%20signal%20for%20deciding%20when%20to%20ask.%20We%20extract%20text%20to%20image%20attention%20maps%20and%20pass%20them%20to%20a%20lightweight%20CNN%20to%20detect%20referential%20ambiguity%2C%20while%20a%20LoRA%20fine-tuned%20decoder%20conducts%20the%20dialog%20and%20emits%20grounding%20location%20tokens.%20We%20train%20on%20a%20real-world%20interactive%20dataset%20for%20IVG%2C%20and%20a%20mixed%20ambiguity%20set%20for%20the%20detector.%20With%20InViG-only%20supervision%2C%20our%20model%20surpasses%20a%20state-of-the-art%20method%20while%20using%20parameter-efficient%20fine-tuning.%20Similarly%2C%20the%20ambiguity%20detector%20outperforms%20prior%20baselines.%20Overall%2C%20CLUE%20turns%20the%20internal%20cross-modal%20attention%20of%20a%20VLM%20into%20an%20explicit%2C%20spatially%20grounded%20signal%20for%20deciding%20when%20to%20ask.%20The%20data%20and%20code%20are%20publicly%20available%20at%3A%20mouadabrini.github.io/clue&entry.1838667208=http%3A//arxiv.org/abs/2602.08999v1&entry.124074799=Read"},
{"title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs", "author": "Benno Krojer and Shravan Nayak and Oscar Ma\u00f1as and Vaibhav Adlakha and Desmond Elliott and Siva Reddy and Marius Mosbach", "abstract": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.", "link": "http://arxiv.org/abs/2602.00462v2", "date": "2026-02-09", "relevancy": 2.9451, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatentLens%3A%20Revealing%20Highly%20Interpretable%20Visual%20Tokens%20in%20LLMs&body=Title%3A%20LatentLens%3A%20Revealing%20Highly%20Interpretable%20Visual%20Tokens%20in%20LLMs%0AAuthor%3A%20Benno%20Krojer%20and%20Shravan%20Nayak%20and%20Oscar%20Ma%C3%B1as%20and%20Vaibhav%20Adlakha%20and%20Desmond%20Elliott%20and%20Siva%20Reddy%20and%20Marius%20Mosbach%0AAbstract%3A%20Transforming%20a%20large%20language%20model%20%28LLM%29%20into%20a%20Vision-Language%20Model%20%28VLM%29%20can%20be%20achieved%20by%20mapping%20the%20visual%20tokens%20from%20a%20vision%20encoder%20into%20the%20embedding%20space%20of%20an%20LLM.%20Intriguingly%2C%20this%20mapping%20can%20be%20as%20simple%20as%20a%20shallow%20MLP%20transformation.%20To%20understand%20why%20LLMs%20can%20so%20readily%20process%20visual%20tokens%2C%20we%20need%20interpretability%20methods%20that%20reveal%20what%20is%20encoded%20in%20the%20visual%20token%20representations%20at%20every%20layer%20of%20LLM%20processing.%20In%20this%20work%2C%20we%20introduce%20LatentLens%2C%20a%20novel%20approach%20for%20mapping%20latent%20representations%20to%20descriptions%20in%20natural%20language.%20LatentLens%20works%20by%20encoding%20a%20large%20text%20corpus%20and%20storing%20contextualized%20token%20representations%20for%20each%20token%20in%20that%20corpus.%20Visual%20token%20representations%20are%20then%20compared%20to%20their%20contextualized%20textual%20representations%2C%20with%20the%20top-k%20nearest%20neighbor%20representations%20providing%20descriptions%20of%20the%20visual%20token.%20We%20evaluate%20this%20method%20on%2010%20different%20VLMs%2C%20showing%20that%20commonly%20used%20methods%2C%20such%20as%20LogitLens%2C%20substantially%20underestimate%20the%20interpretability%20of%20visual%20tokens.%20With%20LatentLens%20instead%2C%20the%20majority%20of%20visual%20tokens%20are%20interpretable%20across%20all%20studied%20models%20and%20all%20layers.%20Qualitatively%2C%20we%20show%20that%20the%20descriptions%20produced%20by%20LatentLens%20are%20semantically%20meaningful%20and%20provide%20more%20fine-grained%20interpretations%20for%20humans%20compared%20to%20individual%20tokens.%20More%20broadly%2C%20our%20findings%20contribute%20new%20evidence%20on%20the%20alignment%20between%20vision%20and%20language%20representations%2C%20opening%20up%20new%20directions%20for%20analyzing%20latent%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatentLens%253A%2520Revealing%2520Highly%2520Interpretable%2520Visual%2520Tokens%2520in%2520LLMs%26entry.906535625%3DBenno%2520Krojer%2520and%2520Shravan%2520Nayak%2520and%2520Oscar%2520Ma%25C3%25B1as%2520and%2520Vaibhav%2520Adlakha%2520and%2520Desmond%2520Elliott%2520and%2520Siva%2520Reddy%2520and%2520Marius%2520Mosbach%26entry.1292438233%3DTransforming%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520into%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%2520can%2520be%2520achieved%2520by%2520mapping%2520the%2520visual%2520tokens%2520from%2520a%2520vision%2520encoder%2520into%2520the%2520embedding%2520space%2520of%2520an%2520LLM.%2520Intriguingly%252C%2520this%2520mapping%2520can%2520be%2520as%2520simple%2520as%2520a%2520shallow%2520MLP%2520transformation.%2520To%2520understand%2520why%2520LLMs%2520can%2520so%2520readily%2520process%2520visual%2520tokens%252C%2520we%2520need%2520interpretability%2520methods%2520that%2520reveal%2520what%2520is%2520encoded%2520in%2520the%2520visual%2520token%2520representations%2520at%2520every%2520layer%2520of%2520LLM%2520processing.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LatentLens%252C%2520a%2520novel%2520approach%2520for%2520mapping%2520latent%2520representations%2520to%2520descriptions%2520in%2520natural%2520language.%2520LatentLens%2520works%2520by%2520encoding%2520a%2520large%2520text%2520corpus%2520and%2520storing%2520contextualized%2520token%2520representations%2520for%2520each%2520token%2520in%2520that%2520corpus.%2520Visual%2520token%2520representations%2520are%2520then%2520compared%2520to%2520their%2520contextualized%2520textual%2520representations%252C%2520with%2520the%2520top-k%2520nearest%2520neighbor%2520representations%2520providing%2520descriptions%2520of%2520the%2520visual%2520token.%2520We%2520evaluate%2520this%2520method%2520on%252010%2520different%2520VLMs%252C%2520showing%2520that%2520commonly%2520used%2520methods%252C%2520such%2520as%2520LogitLens%252C%2520substantially%2520underestimate%2520the%2520interpretability%2520of%2520visual%2520tokens.%2520With%2520LatentLens%2520instead%252C%2520the%2520majority%2520of%2520visual%2520tokens%2520are%2520interpretable%2520across%2520all%2520studied%2520models%2520and%2520all%2520layers.%2520Qualitatively%252C%2520we%2520show%2520that%2520the%2520descriptions%2520produced%2520by%2520LatentLens%2520are%2520semantically%2520meaningful%2520and%2520provide%2520more%2520fine-grained%2520interpretations%2520for%2520humans%2520compared%2520to%2520individual%2520tokens.%2520More%2520broadly%252C%2520our%2520findings%2520contribute%2520new%2520evidence%2520on%2520the%2520alignment%2520between%2520vision%2520and%2520language%2520representations%252C%2520opening%2520up%2520new%2520directions%2520for%2520analyzing%2520latent%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatentLens%3A%20Revealing%20Highly%20Interpretable%20Visual%20Tokens%20in%20LLMs&entry.906535625=Benno%20Krojer%20and%20Shravan%20Nayak%20and%20Oscar%20Ma%C3%B1as%20and%20Vaibhav%20Adlakha%20and%20Desmond%20Elliott%20and%20Siva%20Reddy%20and%20Marius%20Mosbach&entry.1292438233=Transforming%20a%20large%20language%20model%20%28LLM%29%20into%20a%20Vision-Language%20Model%20%28VLM%29%20can%20be%20achieved%20by%20mapping%20the%20visual%20tokens%20from%20a%20vision%20encoder%20into%20the%20embedding%20space%20of%20an%20LLM.%20Intriguingly%2C%20this%20mapping%20can%20be%20as%20simple%20as%20a%20shallow%20MLP%20transformation.%20To%20understand%20why%20LLMs%20can%20so%20readily%20process%20visual%20tokens%2C%20we%20need%20interpretability%20methods%20that%20reveal%20what%20is%20encoded%20in%20the%20visual%20token%20representations%20at%20every%20layer%20of%20LLM%20processing.%20In%20this%20work%2C%20we%20introduce%20LatentLens%2C%20a%20novel%20approach%20for%20mapping%20latent%20representations%20to%20descriptions%20in%20natural%20language.%20LatentLens%20works%20by%20encoding%20a%20large%20text%20corpus%20and%20storing%20contextualized%20token%20representations%20for%20each%20token%20in%20that%20corpus.%20Visual%20token%20representations%20are%20then%20compared%20to%20their%20contextualized%20textual%20representations%2C%20with%20the%20top-k%20nearest%20neighbor%20representations%20providing%20descriptions%20of%20the%20visual%20token.%20We%20evaluate%20this%20method%20on%2010%20different%20VLMs%2C%20showing%20that%20commonly%20used%20methods%2C%20such%20as%20LogitLens%2C%20substantially%20underestimate%20the%20interpretability%20of%20visual%20tokens.%20With%20LatentLens%20instead%2C%20the%20majority%20of%20visual%20tokens%20are%20interpretable%20across%20all%20studied%20models%20and%20all%20layers.%20Qualitatively%2C%20we%20show%20that%20the%20descriptions%20produced%20by%20LatentLens%20are%20semantically%20meaningful%20and%20provide%20more%20fine-grained%20interpretations%20for%20humans%20compared%20to%20individual%20tokens.%20More%20broadly%2C%20our%20findings%20contribute%20new%20evidence%20on%20the%20alignment%20between%20vision%20and%20language%20representations%2C%20opening%20up%20new%20directions%20for%20analyzing%20latent%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.00462v2&entry.124074799=Read"},
{"title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning", "author": "Enwei Tong and Yuanchao Bai and Yao Zhu and Junjun Jiang and Xianming Liu", "abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR.", "link": "http://arxiv.org/abs/2602.05809v2", "date": "2026-02-09", "relevancy": 2.9209, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning&body=Title%3A%20Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning%0AAuthor%3A%20Enwei%20Tong%20and%20Yuanchao%20Bai%20and%20Yao%20Zhu%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20often%20generate%20massive%20visual%20tokens%20that%20greatly%20increase%20inference%20latency%20and%20memory%20footprint%3B%20while%20training-free%20token%20pruning%20offers%20a%20practical%20remedy%2C%20existing%20methods%20still%20struggle%20to%20balance%20local%20evidence%20and%20global%20context%20under%20aggressive%20compression.%20We%20propose%20Focus-Scan-Refine%20%28FSR%29%2C%20a%20human-inspired%2C%20plug-and-play%20pruning%20framework%20that%20mimics%20how%20humans%20answer%20visual%20questions%3A%20focus%20on%20key%20evidence%2C%20then%20scan%20globally%20if%20needed%2C%20and%20refine%20the%20scanned%20context%20by%20aggregating%20relevant%20details.%20FSR%20first%20focuses%20on%20key%20evidence%20by%20combining%20visual%20importance%20with%20instruction%20relevance%2C%20avoiding%20the%20bias%20toward%20visually%20salient%20but%20query-irrelevant%20regions.%20It%20then%20scans%20for%20complementary%20context%20conditioned%20on%20the%20focused%20set%2C%20selecting%20tokens%20that%20are%20most%20different%20from%20the%20focused%20evidence.%20Finally%2C%20FSR%20refines%20the%20scanned%20context%20by%20aggregating%20nearby%20informative%20tokens%20into%20the%20scan%20anchors%20via%20similarity-based%20assignment%20and%20score-weighted%20merging%2C%20without%20increasing%20the%20token%20budget.%20Extensive%20experiments%20across%20multiple%20VLM%20backbones%20and%20vision-language%20benchmarks%20show%20that%20FSR%20consistently%20improves%20the%20accuracy-efficiency%20trade-off%20over%20existing%20state-of-the-art%20pruning%20methods.%20The%20source%20codes%20can%20be%20found%20at%20https%3A//github.com/ILOT-code/FSR.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus-Scan-Refine%253A%2520From%2520Human%2520Visual%2520Perception%2520to%2520Efficient%2520Visual%2520Token%2520Pruning%26entry.906535625%3DEnwei%2520Tong%2520and%2520Yuanchao%2520Bai%2520and%2520Yao%2520Zhu%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520often%2520generate%2520massive%2520visual%2520tokens%2520that%2520greatly%2520increase%2520inference%2520latency%2520and%2520memory%2520footprint%253B%2520while%2520training-free%2520token%2520pruning%2520offers%2520a%2520practical%2520remedy%252C%2520existing%2520methods%2520still%2520struggle%2520to%2520balance%2520local%2520evidence%2520and%2520global%2520context%2520under%2520aggressive%2520compression.%2520We%2520propose%2520Focus-Scan-Refine%2520%2528FSR%2529%252C%2520a%2520human-inspired%252C%2520plug-and-play%2520pruning%2520framework%2520that%2520mimics%2520how%2520humans%2520answer%2520visual%2520questions%253A%2520focus%2520on%2520key%2520evidence%252C%2520then%2520scan%2520globally%2520if%2520needed%252C%2520and%2520refine%2520the%2520scanned%2520context%2520by%2520aggregating%2520relevant%2520details.%2520FSR%2520first%2520focuses%2520on%2520key%2520evidence%2520by%2520combining%2520visual%2520importance%2520with%2520instruction%2520relevance%252C%2520avoiding%2520the%2520bias%2520toward%2520visually%2520salient%2520but%2520query-irrelevant%2520regions.%2520It%2520then%2520scans%2520for%2520complementary%2520context%2520conditioned%2520on%2520the%2520focused%2520set%252C%2520selecting%2520tokens%2520that%2520are%2520most%2520different%2520from%2520the%2520focused%2520evidence.%2520Finally%252C%2520FSR%2520refines%2520the%2520scanned%2520context%2520by%2520aggregating%2520nearby%2520informative%2520tokens%2520into%2520the%2520scan%2520anchors%2520via%2520similarity-based%2520assignment%2520and%2520score-weighted%2520merging%252C%2520without%2520increasing%2520the%2520token%2520budget.%2520Extensive%2520experiments%2520across%2520multiple%2520VLM%2520backbones%2520and%2520vision-language%2520benchmarks%2520show%2520that%2520FSR%2520consistently%2520improves%2520the%2520accuracy-efficiency%2520trade-off%2520over%2520existing%2520state-of-the-art%2520pruning%2520methods.%2520The%2520source%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/ILOT-code/FSR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning&entry.906535625=Enwei%20Tong%20and%20Yuanchao%20Bai%20and%20Yao%20Zhu%20and%20Junjun%20Jiang%20and%20Xianming%20Liu&entry.1292438233=Vision-language%20models%20%28VLMs%29%20often%20generate%20massive%20visual%20tokens%20that%20greatly%20increase%20inference%20latency%20and%20memory%20footprint%3B%20while%20training-free%20token%20pruning%20offers%20a%20practical%20remedy%2C%20existing%20methods%20still%20struggle%20to%20balance%20local%20evidence%20and%20global%20context%20under%20aggressive%20compression.%20We%20propose%20Focus-Scan-Refine%20%28FSR%29%2C%20a%20human-inspired%2C%20plug-and-play%20pruning%20framework%20that%20mimics%20how%20humans%20answer%20visual%20questions%3A%20focus%20on%20key%20evidence%2C%20then%20scan%20globally%20if%20needed%2C%20and%20refine%20the%20scanned%20context%20by%20aggregating%20relevant%20details.%20FSR%20first%20focuses%20on%20key%20evidence%20by%20combining%20visual%20importance%20with%20instruction%20relevance%2C%20avoiding%20the%20bias%20toward%20visually%20salient%20but%20query-irrelevant%20regions.%20It%20then%20scans%20for%20complementary%20context%20conditioned%20on%20the%20focused%20set%2C%20selecting%20tokens%20that%20are%20most%20different%20from%20the%20focused%20evidence.%20Finally%2C%20FSR%20refines%20the%20scanned%20context%20by%20aggregating%20nearby%20informative%20tokens%20into%20the%20scan%20anchors%20via%20similarity-based%20assignment%20and%20score-weighted%20merging%2C%20without%20increasing%20the%20token%20budget.%20Extensive%20experiments%20across%20multiple%20VLM%20backbones%20and%20vision-language%20benchmarks%20show%20that%20FSR%20consistently%20improves%20the%20accuracy-efficiency%20trade-off%20over%20existing%20state-of-the-art%20pruning%20methods.%20The%20source%20codes%20can%20be%20found%20at%20https%3A//github.com/ILOT-code/FSR.&entry.1838667208=http%3A//arxiv.org/abs/2602.05809v2&entry.124074799=Read"},
{"title": "Improving Reconstruction of Representation Autoencoder", "author": "Siyu Liu and Chujie Qin and Hubery Yin and Qixin Yan and Zheng-Peng Duan and Chen Li and Jing Lyu and Chun-Le Guo and Chongyi Li", "abstract": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.", "link": "http://arxiv.org/abs/2602.08620v1", "date": "2026-02-09", "relevancy": 2.9141, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6294}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Reconstruction%20of%20Representation%20Autoencoder&body=Title%3A%20Improving%20Reconstruction%20of%20Representation%20Autoencoder%0AAuthor%3A%20Siyu%20Liu%20and%20Chujie%20Qin%20and%20Hubery%20Yin%20and%20Qixin%20Yan%20and%20Zheng-Peng%20Duan%20and%20Chen%20Li%20and%20Jing%20Lyu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%0AAbstract%3A%20Recent%20work%20leverages%20Vision%20Foundation%20Models%20as%20image%20encoders%20to%20boost%20the%20generative%20performance%20of%20latent%20diffusion%20models%20%28LDMs%29%2C%20as%20their%20semantic%20feature%20distributions%20are%20easy%20to%20learn.%20However%2C%20such%20semantic%20features%20often%20lack%20low-level%20information%20%28%5Ceg%2C%20color%20and%20texture%29%2C%20leading%20to%20degraded%20reconstruction%20fidelity%2C%20which%20has%20emerged%20as%20a%20primary%20bottleneck%20in%20further%20scaling%20LDMs.%20To%20address%20this%20limitation%2C%20we%20propose%20LV-RAE%2C%20a%20representation%20autoencoder%20that%20augments%20semantic%20features%20with%20missing%20low-level%20information%2C%20enabling%20high-fidelity%20reconstruction%20while%20remaining%20highly%20aligned%20with%20the%20semantic%20distribution.%20We%20further%20observe%20that%20the%20resulting%20high-dimensional%2C%20information-rich%20latent%20make%20decoders%20sensitive%20to%20latent%20perturbations%2C%20causing%20severe%20artifacts%20when%20decoding%20generated%20latent%20and%20consequently%20degrading%20generation%20quality.%20Our%20analysis%20suggests%20that%20this%20sensitivity%20primarily%20stems%20from%20excessive%20decoder%20responses%20along%20directions%20off%20the%20data%20manifold.%20Building%20on%20these%20insights%2C%20we%20propose%20fine-tuning%20the%20decoder%20to%20increase%20its%20robustness%20and%20smoothing%20the%20generated%20latent%20via%20controlled%20noise%20injection%2C%20thereby%20enhancing%20generation%20quality.%20Experiments%20demonstrate%20that%20LV-RAE%20significantly%20improves%20reconstruction%20fidelity%20while%20preserving%20the%20semantic%20abstraction%20and%20achieving%20strong%20generative%20quality.%20Our%20code%20is%20available%20at%20https%3A//github.com/modyu-liu/LVRAE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Reconstruction%2520of%2520Representation%2520Autoencoder%26entry.906535625%3DSiyu%2520Liu%2520and%2520Chujie%2520Qin%2520and%2520Hubery%2520Yin%2520and%2520Qixin%2520Yan%2520and%2520Zheng-Peng%2520Duan%2520and%2520Chen%2520Li%2520and%2520Jing%2520Lyu%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%26entry.1292438233%3DRecent%2520work%2520leverages%2520Vision%2520Foundation%2520Models%2520as%2520image%2520encoders%2520to%2520boost%2520the%2520generative%2520performance%2520of%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%252C%2520as%2520their%2520semantic%2520feature%2520distributions%2520are%2520easy%2520to%2520learn.%2520However%252C%2520such%2520semantic%2520features%2520often%2520lack%2520low-level%2520information%2520%2528%255Ceg%252C%2520color%2520and%2520texture%2529%252C%2520leading%2520to%2520degraded%2520reconstruction%2520fidelity%252C%2520which%2520has%2520emerged%2520as%2520a%2520primary%2520bottleneck%2520in%2520further%2520scaling%2520LDMs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520LV-RAE%252C%2520a%2520representation%2520autoencoder%2520that%2520augments%2520semantic%2520features%2520with%2520missing%2520low-level%2520information%252C%2520enabling%2520high-fidelity%2520reconstruction%2520while%2520remaining%2520highly%2520aligned%2520with%2520the%2520semantic%2520distribution.%2520We%2520further%2520observe%2520that%2520the%2520resulting%2520high-dimensional%252C%2520information-rich%2520latent%2520make%2520decoders%2520sensitive%2520to%2520latent%2520perturbations%252C%2520causing%2520severe%2520artifacts%2520when%2520decoding%2520generated%2520latent%2520and%2520consequently%2520degrading%2520generation%2520quality.%2520Our%2520analysis%2520suggests%2520that%2520this%2520sensitivity%2520primarily%2520stems%2520from%2520excessive%2520decoder%2520responses%2520along%2520directions%2520off%2520the%2520data%2520manifold.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520fine-tuning%2520the%2520decoder%2520to%2520increase%2520its%2520robustness%2520and%2520smoothing%2520the%2520generated%2520latent%2520via%2520controlled%2520noise%2520injection%252C%2520thereby%2520enhancing%2520generation%2520quality.%2520Experiments%2520demonstrate%2520that%2520LV-RAE%2520significantly%2520improves%2520reconstruction%2520fidelity%2520while%2520preserving%2520the%2520semantic%2520abstraction%2520and%2520achieving%2520strong%2520generative%2520quality.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/modyu-liu/LVRAE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Reconstruction%20of%20Representation%20Autoencoder&entry.906535625=Siyu%20Liu%20and%20Chujie%20Qin%20and%20Hubery%20Yin%20and%20Qixin%20Yan%20and%20Zheng-Peng%20Duan%20and%20Chen%20Li%20and%20Jing%20Lyu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li&entry.1292438233=Recent%20work%20leverages%20Vision%20Foundation%20Models%20as%20image%20encoders%20to%20boost%20the%20generative%20performance%20of%20latent%20diffusion%20models%20%28LDMs%29%2C%20as%20their%20semantic%20feature%20distributions%20are%20easy%20to%20learn.%20However%2C%20such%20semantic%20features%20often%20lack%20low-level%20information%20%28%5Ceg%2C%20color%20and%20texture%29%2C%20leading%20to%20degraded%20reconstruction%20fidelity%2C%20which%20has%20emerged%20as%20a%20primary%20bottleneck%20in%20further%20scaling%20LDMs.%20To%20address%20this%20limitation%2C%20we%20propose%20LV-RAE%2C%20a%20representation%20autoencoder%20that%20augments%20semantic%20features%20with%20missing%20low-level%20information%2C%20enabling%20high-fidelity%20reconstruction%20while%20remaining%20highly%20aligned%20with%20the%20semantic%20distribution.%20We%20further%20observe%20that%20the%20resulting%20high-dimensional%2C%20information-rich%20latent%20make%20decoders%20sensitive%20to%20latent%20perturbations%2C%20causing%20severe%20artifacts%20when%20decoding%20generated%20latent%20and%20consequently%20degrading%20generation%20quality.%20Our%20analysis%20suggests%20that%20this%20sensitivity%20primarily%20stems%20from%20excessive%20decoder%20responses%20along%20directions%20off%20the%20data%20manifold.%20Building%20on%20these%20insights%2C%20we%20propose%20fine-tuning%20the%20decoder%20to%20increase%20its%20robustness%20and%20smoothing%20the%20generated%20latent%20via%20controlled%20noise%20injection%2C%20thereby%20enhancing%20generation%20quality.%20Experiments%20demonstrate%20that%20LV-RAE%20significantly%20improves%20reconstruction%20fidelity%20while%20preserving%20the%20semantic%20abstraction%20and%20achieving%20strong%20generative%20quality.%20Our%20code%20is%20available%20at%20https%3A//github.com/modyu-liu/LVRAE.&entry.1838667208=http%3A//arxiv.org/abs/2602.08620v1&entry.124074799=Read"},
{"title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving", "author": "Linger Deng and Yuliang Liu and Wenwen Yu and Zujia Zhang and Jianzhong Ju and Zhenbo Luo and Xiang Bai", "abstract": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus", "link": "http://arxiv.org/abs/2602.08524v1", "date": "2026-02-09", "relevancy": 2.8964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoFocus%3A%20Blending%20Efficient%20Global-to-Local%20Perception%20for%20Multimodal%20Geometry%20Problem-Solving&body=Title%3A%20GeoFocus%3A%20Blending%20Efficient%20Global-to-Local%20Perception%20for%20Multimodal%20Geometry%20Problem-Solving%0AAuthor%3A%20Linger%20Deng%20and%20Yuliang%20Liu%20and%20Wenwen%20Yu%20and%20Zujia%20Zhang%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Xiang%20Bai%0AAbstract%3A%20Geometry%20problem-solving%20remains%20a%20significant%20challenge%20for%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20requiring%20not%20only%20global%20shape%20recognition%20but%20also%20attention%20to%20intricate%20local%20relationships%20related%20to%20geometric%20theory.%20To%20address%20this%2C%20we%20propose%20GeoFocus%2C%20a%20novel%20framework%20comprising%20two%20core%20modules.%201%29%20Critical%20Local%20Perceptor%2C%20which%20automatically%20identifies%20and%20emphasizes%20critical%20local%20structure%20%28e.g.%2C%20angles%2C%20parallel%20lines%2C%20comparative%20distances%29%20through%20thirteen%20theory-based%20perception%20templates%2C%20boosting%20critical%20local%20feature%20coverage%20by%2061%25%20compared%20to%20previous%20methods.%202%29%20VertexLang%2C%20a%20compact%20topology%20formal%20language%2C%20encodes%20global%20figures%20through%20vertex%20coordinates%20and%20connectivity%20relations.%20By%20replacing%20bulky%20code-based%20encodings%2C%20VertexLang%20reduces%20global%20perception%20training%20time%20by%2020%25%20while%20improving%20topology%20recognition%20accuracy.%20When%20evaluated%20in%20Geo3K%2C%20GeoQA%2C%20and%20FormalGeo7K%2C%20GeoFocus%20achieves%20a%204.7%25%20accuracy%20improvement%20over%20leading%20specialized%20models%20and%20demonstrates%20superior%20robustness%20in%20MATHVERSE%20under%20diverse%20visual%20conditions.%20Project%20Page%20--%20https%3A//github.com/dle666/GeoFocus%0ALink%3A%20http%3A//arxiv.org/abs/2602.08524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoFocus%253A%2520Blending%2520Efficient%2520Global-to-Local%2520Perception%2520for%2520Multimodal%2520Geometry%2520Problem-Solving%26entry.906535625%3DLinger%2520Deng%2520and%2520Yuliang%2520Liu%2520and%2520Wenwen%2520Yu%2520and%2520Zujia%2520Zhang%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Xiang%2520Bai%26entry.1292438233%3DGeometry%2520problem-solving%2520remains%2520a%2520significant%2520challenge%2520for%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%252C%2520requiring%2520not%2520only%2520global%2520shape%2520recognition%2520but%2520also%2520attention%2520to%2520intricate%2520local%2520relationships%2520related%2520to%2520geometric%2520theory.%2520To%2520address%2520this%252C%2520we%2520propose%2520GeoFocus%252C%2520a%2520novel%2520framework%2520comprising%2520two%2520core%2520modules.%25201%2529%2520Critical%2520Local%2520Perceptor%252C%2520which%2520automatically%2520identifies%2520and%2520emphasizes%2520critical%2520local%2520structure%2520%2528e.g.%252C%2520angles%252C%2520parallel%2520lines%252C%2520comparative%2520distances%2529%2520through%2520thirteen%2520theory-based%2520perception%2520templates%252C%2520boosting%2520critical%2520local%2520feature%2520coverage%2520by%252061%2525%2520compared%2520to%2520previous%2520methods.%25202%2529%2520VertexLang%252C%2520a%2520compact%2520topology%2520formal%2520language%252C%2520encodes%2520global%2520figures%2520through%2520vertex%2520coordinates%2520and%2520connectivity%2520relations.%2520By%2520replacing%2520bulky%2520code-based%2520encodings%252C%2520VertexLang%2520reduces%2520global%2520perception%2520training%2520time%2520by%252020%2525%2520while%2520improving%2520topology%2520recognition%2520accuracy.%2520When%2520evaluated%2520in%2520Geo3K%252C%2520GeoQA%252C%2520and%2520FormalGeo7K%252C%2520GeoFocus%2520achieves%2520a%25204.7%2525%2520accuracy%2520improvement%2520over%2520leading%2520specialized%2520models%2520and%2520demonstrates%2520superior%2520robustness%2520in%2520MATHVERSE%2520under%2520diverse%2520visual%2520conditions.%2520Project%2520Page%2520--%2520https%253A//github.com/dle666/GeoFocus%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoFocus%3A%20Blending%20Efficient%20Global-to-Local%20Perception%20for%20Multimodal%20Geometry%20Problem-Solving&entry.906535625=Linger%20Deng%20and%20Yuliang%20Liu%20and%20Wenwen%20Yu%20and%20Zujia%20Zhang%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Xiang%20Bai&entry.1292438233=Geometry%20problem-solving%20remains%20a%20significant%20challenge%20for%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20requiring%20not%20only%20global%20shape%20recognition%20but%20also%20attention%20to%20intricate%20local%20relationships%20related%20to%20geometric%20theory.%20To%20address%20this%2C%20we%20propose%20GeoFocus%2C%20a%20novel%20framework%20comprising%20two%20core%20modules.%201%29%20Critical%20Local%20Perceptor%2C%20which%20automatically%20identifies%20and%20emphasizes%20critical%20local%20structure%20%28e.g.%2C%20angles%2C%20parallel%20lines%2C%20comparative%20distances%29%20through%20thirteen%20theory-based%20perception%20templates%2C%20boosting%20critical%20local%20feature%20coverage%20by%2061%25%20compared%20to%20previous%20methods.%202%29%20VertexLang%2C%20a%20compact%20topology%20formal%20language%2C%20encodes%20global%20figures%20through%20vertex%20coordinates%20and%20connectivity%20relations.%20By%20replacing%20bulky%20code-based%20encodings%2C%20VertexLang%20reduces%20global%20perception%20training%20time%20by%2020%25%20while%20improving%20topology%20recognition%20accuracy.%20When%20evaluated%20in%20Geo3K%2C%20GeoQA%2C%20and%20FormalGeo7K%2C%20GeoFocus%20achieves%20a%204.7%25%20accuracy%20improvement%20over%20leading%20specialized%20models%20and%20demonstrates%20superior%20robustness%20in%20MATHVERSE%20under%20diverse%20visual%20conditions.%20Project%20Page%20--%20https%3A//github.com/dle666/GeoFocus&entry.1838667208=http%3A//arxiv.org/abs/2602.08524v1&entry.124074799=Read"},
{"title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning", "author": "Hao Tan and Jun Lan and Senyuan Shi and Zichang Tan and Zijian Yu and Huijia Zhu and Weiqiang Wang and Jun Wan and Zhen Lei", "abstract": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "link": "http://arxiv.org/abs/2602.08828v1", "date": "2026-02-09", "relevancy": 2.8913, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5945}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5749}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoVeritas%3A%20AI-Generated%20Video%20Detection%20via%20Perception%20Pretext%20Reinforcement%20Learning&body=Title%3A%20VideoVeritas%3A%20AI-Generated%20Video%20Detection%20via%20Perception%20Pretext%20Reinforcement%20Learning%0AAuthor%3A%20Hao%20Tan%20and%20Jun%20Lan%20and%20Senyuan%20Shi%20and%20Zichang%20Tan%20and%20Zijian%20Yu%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jun%20Wan%20and%20Zhen%20Lei%0AAbstract%3A%20The%20growing%20capability%20of%20video%20generation%20poses%20escalating%20security%20risks%2C%20making%20reliable%20detection%20increasingly%20essential.%20In%20this%20paper%2C%20we%20introduce%20VideoVeritas%2C%20a%20framework%20that%20integrates%20fine-grained%20perception%20and%20fact-based%20reasoning.%20We%20observe%20that%20while%20current%20multi-modal%20large%20language%20models%20%28MLLMs%29%20exhibit%20strong%20reasoning%20capacity%2C%20their%20granular%20perception%20ability%20remains%20limited.%20To%20mitigate%20this%2C%20we%20introduce%20Joint%20Preference%20Alignment%20and%20Perception%20Pretext%20Reinforcement%20Learning%20%28PPRL%29.%20Specifically%2C%20rather%20than%20directly%20optimizing%20for%20detection%20task%2C%20we%20adopt%20general%20spatiotemporal%20grounding%20and%20self-supervised%20object%20counting%20in%20the%20RL%20stage%2C%20enhancing%20detection%20performance%20with%20simple%20perception%20pretext%20tasks.%20To%20facilitate%20robust%20evaluation%2C%20we%20further%20introduce%20MintVid%2C%20a%20light%20yet%20high-quality%20dataset%20containing%203K%20videos%20from%209%20state-of-the-art%20generators%2C%20along%20with%20a%20real-world%20collected%20subset%20that%20has%20factual%20errors%20in%20content.%20Experimental%20results%20demonstrate%20that%20existing%20methods%20tend%20to%20bias%20towards%20either%20superficial%20reasoning%20or%20mechanical%20analysis%2C%20while%20VideoVeritas%20achieves%20more%20balanced%20performance%20across%20diverse%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoVeritas%253A%2520AI-Generated%2520Video%2520Detection%2520via%2520Perception%2520Pretext%2520Reinforcement%2520Learning%26entry.906535625%3DHao%2520Tan%2520and%2520Jun%2520Lan%2520and%2520Senyuan%2520Shi%2520and%2520Zichang%2520Tan%2520and%2520Zijian%2520Yu%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Jun%2520Wan%2520and%2520Zhen%2520Lei%26entry.1292438233%3DThe%2520growing%2520capability%2520of%2520video%2520generation%2520poses%2520escalating%2520security%2520risks%252C%2520making%2520reliable%2520detection%2520increasingly%2520essential.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VideoVeritas%252C%2520a%2520framework%2520that%2520integrates%2520fine-grained%2520perception%2520and%2520fact-based%2520reasoning.%2520We%2520observe%2520that%2520while%2520current%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520exhibit%2520strong%2520reasoning%2520capacity%252C%2520their%2520granular%2520perception%2520ability%2520remains%2520limited.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%2520Joint%2520Preference%2520Alignment%2520and%2520Perception%2520Pretext%2520Reinforcement%2520Learning%2520%2528PPRL%2529.%2520Specifically%252C%2520rather%2520than%2520directly%2520optimizing%2520for%2520detection%2520task%252C%2520we%2520adopt%2520general%2520spatiotemporal%2520grounding%2520and%2520self-supervised%2520object%2520counting%2520in%2520the%2520RL%2520stage%252C%2520enhancing%2520detection%2520performance%2520with%2520simple%2520perception%2520pretext%2520tasks.%2520To%2520facilitate%2520robust%2520evaluation%252C%2520we%2520further%2520introduce%2520MintVid%252C%2520a%2520light%2520yet%2520high-quality%2520dataset%2520containing%25203K%2520videos%2520from%25209%2520state-of-the-art%2520generators%252C%2520along%2520with%2520a%2520real-world%2520collected%2520subset%2520that%2520has%2520factual%2520errors%2520in%2520content.%2520Experimental%2520results%2520demonstrate%2520that%2520existing%2520methods%2520tend%2520to%2520bias%2520towards%2520either%2520superficial%2520reasoning%2520or%2520mechanical%2520analysis%252C%2520while%2520VideoVeritas%2520achieves%2520more%2520balanced%2520performance%2520across%2520diverse%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoVeritas%3A%20AI-Generated%20Video%20Detection%20via%20Perception%20Pretext%20Reinforcement%20Learning&entry.906535625=Hao%20Tan%20and%20Jun%20Lan%20and%20Senyuan%20Shi%20and%20Zichang%20Tan%20and%20Zijian%20Yu%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jun%20Wan%20and%20Zhen%20Lei&entry.1292438233=The%20growing%20capability%20of%20video%20generation%20poses%20escalating%20security%20risks%2C%20making%20reliable%20detection%20increasingly%20essential.%20In%20this%20paper%2C%20we%20introduce%20VideoVeritas%2C%20a%20framework%20that%20integrates%20fine-grained%20perception%20and%20fact-based%20reasoning.%20We%20observe%20that%20while%20current%20multi-modal%20large%20language%20models%20%28MLLMs%29%20exhibit%20strong%20reasoning%20capacity%2C%20their%20granular%20perception%20ability%20remains%20limited.%20To%20mitigate%20this%2C%20we%20introduce%20Joint%20Preference%20Alignment%20and%20Perception%20Pretext%20Reinforcement%20Learning%20%28PPRL%29.%20Specifically%2C%20rather%20than%20directly%20optimizing%20for%20detection%20task%2C%20we%20adopt%20general%20spatiotemporal%20grounding%20and%20self-supervised%20object%20counting%20in%20the%20RL%20stage%2C%20enhancing%20detection%20performance%20with%20simple%20perception%20pretext%20tasks.%20To%20facilitate%20robust%20evaluation%2C%20we%20further%20introduce%20MintVid%2C%20a%20light%20yet%20high-quality%20dataset%20containing%203K%20videos%20from%209%20state-of-the-art%20generators%2C%20along%20with%20a%20real-world%20collected%20subset%20that%20has%20factual%20errors%20in%20content.%20Experimental%20results%20demonstrate%20that%20existing%20methods%20tend%20to%20bias%20towards%20either%20superficial%20reasoning%20or%20mechanical%20analysis%2C%20while%20VideoVeritas%20achieves%20more%20balanced%20performance%20across%20diverse%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.08828v1&entry.124074799=Read"},
{"title": "Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm", "author": "Xiaogang Xu and Kun Zhou and Tao Hu and Jiafei Wu and Ruixing Wang and Hao Peng and Bei Yu", "abstract": "Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.", "link": "http://arxiv.org/abs/2602.08699v1", "date": "2026-02-09", "relevancy": 2.8809, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Light%20Video%20Enhancement%20with%20An%20Effective%20Spatial-Temporal%20Decomposition%20Paradigm&body=Title%3A%20Low-Light%20Video%20Enhancement%20with%20An%20Effective%20Spatial-Temporal%20Decomposition%20Paradigm%0AAuthor%3A%20Xiaogang%20Xu%20and%20Kun%20Zhou%20and%20Tao%20Hu%20and%20Jiafei%20Wu%20and%20Ruixing%20Wang%20and%20Hao%20Peng%20and%20Bei%20Yu%0AAbstract%3A%20Low-Light%20Video%20Enhancement%20%28LLVE%29%20seeks%20to%20restore%20dynamic%20or%20static%20scenes%20plagued%20by%20severe%20invisibility%20and%20noise.%20In%20this%20paper%2C%20we%20present%20an%20innovative%20video%20decomposition%20strategy%20that%20incorporates%20view-independent%20and%20view-dependent%20components%20to%20enhance%20the%20performance%20of%20LLVE.%20The%20framework%20is%20called%20View-aware%20Low-light%20Video%20Enhancement%20%28VLLVE%29.%20We%20leverage%20dynamic%20cross-frame%20correspondences%20for%20the%20view-independent%20term%20%28which%20primarily%20captures%20intrinsic%20appearance%29%20and%20impose%20a%20scene-level%20continuity%20constraint%20on%20the%20view-dependent%20term%20%28which%20mainly%20describes%20the%20shading%20condition%29%20to%20achieve%20consistent%20and%20satisfactory%20decomposition%20results.%20To%20further%20ensure%20consistent%20decomposition%2C%20we%20introduce%20a%20dual-structure%20enhancement%20network%20featuring%20a%20cross-frame%20interaction%20mechanism.%20By%20supervising%20different%20frames%20simultaneously%2C%20this%20network%20encourages%20them%20to%20exhibit%20matching%20decomposition%20features.%20This%20mechanism%20can%20seamlessly%20integrate%20with%20encoder-decoder%20single-frame%20networks%2C%20incurring%20minimal%20additional%20parameter%20costs.%20Building%20upon%20VLLVE%2C%20we%20propose%20a%20more%20comprehensive%20decomposition%20strategy%20by%20introducing%20an%20additive%20residual%20term%2C%20resulting%20in%20VLLVE%2B%2B.%20This%20residual%20term%20can%20simulate%20scene-adaptive%20degradations%2C%20which%20are%20difficult%20to%20model%20using%20a%20decomposition%20formulation%20for%20common%20scenes%2C%20thereby%20further%20enhancing%20the%20ability%20to%20capture%20the%20overall%20content%20of%20videos.%20In%20addition%2C%20VLLVE%2B%2B%20enables%20bidirectional%20learning%20for%20both%20enhancement%20and%20degradation-aware%20correspondence%20refinement%20%28end-to-end%20manner%29%2C%20effectively%20increasing%20reliable%20correspondences%20while%20filtering%20out%20incorrect%20ones.%20Notably%2C%20VLLVE%2B%2B%20demonstrates%20strong%20capability%20in%20handling%20challenging%20cases%2C%20such%20as%20real-world%20scenes%20and%20videos%20with%20high%20dynamics.%20Extensive%20experiments%20are%20conducted%20on%20widely%20recognized%20LLVE%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Light%2520Video%2520Enhancement%2520with%2520An%2520Effective%2520Spatial-Temporal%2520Decomposition%2520Paradigm%26entry.906535625%3DXiaogang%2520Xu%2520and%2520Kun%2520Zhou%2520and%2520Tao%2520Hu%2520and%2520Jiafei%2520Wu%2520and%2520Ruixing%2520Wang%2520and%2520Hao%2520Peng%2520and%2520Bei%2520Yu%26entry.1292438233%3DLow-Light%2520Video%2520Enhancement%2520%2528LLVE%2529%2520seeks%2520to%2520restore%2520dynamic%2520or%2520static%2520scenes%2520plagued%2520by%2520severe%2520invisibility%2520and%2520noise.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520innovative%2520video%2520decomposition%2520strategy%2520that%2520incorporates%2520view-independent%2520and%2520view-dependent%2520components%2520to%2520enhance%2520the%2520performance%2520of%2520LLVE.%2520The%2520framework%2520is%2520called%2520View-aware%2520Low-light%2520Video%2520Enhancement%2520%2528VLLVE%2529.%2520We%2520leverage%2520dynamic%2520cross-frame%2520correspondences%2520for%2520the%2520view-independent%2520term%2520%2528which%2520primarily%2520captures%2520intrinsic%2520appearance%2529%2520and%2520impose%2520a%2520scene-level%2520continuity%2520constraint%2520on%2520the%2520view-dependent%2520term%2520%2528which%2520mainly%2520describes%2520the%2520shading%2520condition%2529%2520to%2520achieve%2520consistent%2520and%2520satisfactory%2520decomposition%2520results.%2520To%2520further%2520ensure%2520consistent%2520decomposition%252C%2520we%2520introduce%2520a%2520dual-structure%2520enhancement%2520network%2520featuring%2520a%2520cross-frame%2520interaction%2520mechanism.%2520By%2520supervising%2520different%2520frames%2520simultaneously%252C%2520this%2520network%2520encourages%2520them%2520to%2520exhibit%2520matching%2520decomposition%2520features.%2520This%2520mechanism%2520can%2520seamlessly%2520integrate%2520with%2520encoder-decoder%2520single-frame%2520networks%252C%2520incurring%2520minimal%2520additional%2520parameter%2520costs.%2520Building%2520upon%2520VLLVE%252C%2520we%2520propose%2520a%2520more%2520comprehensive%2520decomposition%2520strategy%2520by%2520introducing%2520an%2520additive%2520residual%2520term%252C%2520resulting%2520in%2520VLLVE%252B%252B.%2520This%2520residual%2520term%2520can%2520simulate%2520scene-adaptive%2520degradations%252C%2520which%2520are%2520difficult%2520to%2520model%2520using%2520a%2520decomposition%2520formulation%2520for%2520common%2520scenes%252C%2520thereby%2520further%2520enhancing%2520the%2520ability%2520to%2520capture%2520the%2520overall%2520content%2520of%2520videos.%2520In%2520addition%252C%2520VLLVE%252B%252B%2520enables%2520bidirectional%2520learning%2520for%2520both%2520enhancement%2520and%2520degradation-aware%2520correspondence%2520refinement%2520%2528end-to-end%2520manner%2529%252C%2520effectively%2520increasing%2520reliable%2520correspondences%2520while%2520filtering%2520out%2520incorrect%2520ones.%2520Notably%252C%2520VLLVE%252B%252B%2520demonstrates%2520strong%2520capability%2520in%2520handling%2520challenging%2520cases%252C%2520such%2520as%2520real-world%2520scenes%2520and%2520videos%2520with%2520high%2520dynamics.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520widely%2520recognized%2520LLVE%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Light%20Video%20Enhancement%20with%20An%20Effective%20Spatial-Temporal%20Decomposition%20Paradigm&entry.906535625=Xiaogang%20Xu%20and%20Kun%20Zhou%20and%20Tao%20Hu%20and%20Jiafei%20Wu%20and%20Ruixing%20Wang%20and%20Hao%20Peng%20and%20Bei%20Yu&entry.1292438233=Low-Light%20Video%20Enhancement%20%28LLVE%29%20seeks%20to%20restore%20dynamic%20or%20static%20scenes%20plagued%20by%20severe%20invisibility%20and%20noise.%20In%20this%20paper%2C%20we%20present%20an%20innovative%20video%20decomposition%20strategy%20that%20incorporates%20view-independent%20and%20view-dependent%20components%20to%20enhance%20the%20performance%20of%20LLVE.%20The%20framework%20is%20called%20View-aware%20Low-light%20Video%20Enhancement%20%28VLLVE%29.%20We%20leverage%20dynamic%20cross-frame%20correspondences%20for%20the%20view-independent%20term%20%28which%20primarily%20captures%20intrinsic%20appearance%29%20and%20impose%20a%20scene-level%20continuity%20constraint%20on%20the%20view-dependent%20term%20%28which%20mainly%20describes%20the%20shading%20condition%29%20to%20achieve%20consistent%20and%20satisfactory%20decomposition%20results.%20To%20further%20ensure%20consistent%20decomposition%2C%20we%20introduce%20a%20dual-structure%20enhancement%20network%20featuring%20a%20cross-frame%20interaction%20mechanism.%20By%20supervising%20different%20frames%20simultaneously%2C%20this%20network%20encourages%20them%20to%20exhibit%20matching%20decomposition%20features.%20This%20mechanism%20can%20seamlessly%20integrate%20with%20encoder-decoder%20single-frame%20networks%2C%20incurring%20minimal%20additional%20parameter%20costs.%20Building%20upon%20VLLVE%2C%20we%20propose%20a%20more%20comprehensive%20decomposition%20strategy%20by%20introducing%20an%20additive%20residual%20term%2C%20resulting%20in%20VLLVE%2B%2B.%20This%20residual%20term%20can%20simulate%20scene-adaptive%20degradations%2C%20which%20are%20difficult%20to%20model%20using%20a%20decomposition%20formulation%20for%20common%20scenes%2C%20thereby%20further%20enhancing%20the%20ability%20to%20capture%20the%20overall%20content%20of%20videos.%20In%20addition%2C%20VLLVE%2B%2B%20enables%20bidirectional%20learning%20for%20both%20enhancement%20and%20degradation-aware%20correspondence%20refinement%20%28end-to-end%20manner%29%2C%20effectively%20increasing%20reliable%20correspondences%20while%20filtering%20out%20incorrect%20ones.%20Notably%2C%20VLLVE%2B%2B%20demonstrates%20strong%20capability%20in%20handling%20challenging%20cases%2C%20such%20as%20real-world%20scenes%20and%20videos%20with%20high%20dynamics.%20Extensive%20experiments%20are%20conducted%20on%20widely%20recognized%20LLVE%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.08699v1&entry.124074799=Read"},
{"title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "author": "Haodong Li and Jingwei Wu and Quan Sun and Guopeng Li and Juanxi Tian and Huanyu Zhang and Yanlin Lai and Ruichuan An and Hongbo Peng and Yuhong Dai and Chenxi Li and Chunmei Qing and Jia Wang and Ziyang Meng and Zheng Ge and Xiangyu Zhang and Daxin Jiang", "abstract": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "link": "http://arxiv.org/abs/2602.09007v1", "date": "2026-02-09", "relevancy": 2.8669, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.593}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5667}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments&body=Title%3A%20GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments%0AAuthor%3A%20Haodong%20Li%20and%20Jingwei%20Wu%20and%20Quan%20Sun%20and%20Guopeng%20Li%20and%20Juanxi%20Tian%20and%20Huanyu%20Zhang%20and%20Yanlin%20Lai%20and%20Ruichuan%20An%20and%20Hongbo%20Peng%20and%20Yuhong%20Dai%20and%20Chenxi%20Li%20and%20Chunmei%20Qing%20and%20Jia%20Wang%20and%20Ziyang%20Meng%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%0AAbstract%3A%20Recent%20advancements%20in%20image%20generation%20models%20have%20enabled%20the%20prediction%20of%20future%20Graphical%20User%20Interface%20%28GUI%29%20states%20based%20on%20user%20instructions.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20general%20domain%20visual%20fidelity%2C%20leaving%20the%20evaluation%20of%20state%20transitions%20and%20temporal%20coherence%20in%20GUI-specific%20contexts%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20GEBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20dynamic%20interaction%20and%20temporal%20coherence%20in%20GUI%20generation.%20GEBench%20comprises%20700%20carefully%20curated%20samples%20spanning%20five%20task%20categories%2C%20covering%20both%20single-step%20interactions%20and%20multi-step%20trajectories%20across%20real-world%20and%20fictional%20scenarios%2C%20as%20well%20as%20grounding%20point%20localization.%20To%20support%20systematic%20evaluation%2C%20we%20propose%20GE-Score%2C%20a%20novel%20five-dimensional%20metric%20that%20assesses%20Goal%20Achievement%2C%20Interaction%20Logic%2C%20Content%20Consistency%2C%20UI%20Plausibility%2C%20and%20Visual%20Quality.%20Extensive%20evaluations%20on%20current%20models%20indicate%20that%20while%20they%20perform%20well%20on%20single-step%20transitions%2C%20they%20struggle%20significantly%20with%20maintaining%20temporal%20coherence%20and%20spatial%20grounding%20over%20longer%20interaction%20sequences.%20Our%20findings%20identify%20icon%20interpretation%2C%20text%20rendering%2C%20and%20localization%20precision%20as%20critical%20bottlenecks.%20This%20work%20provides%20a%20foundation%20for%20systematic%20assessment%20and%20suggests%20promising%20directions%20for%20future%20research%20toward%20building%20high-fidelity%20generative%20GUI%20environments.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/stepfun-ai/GEBench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEBench%253A%2520Benchmarking%2520Image%2520Generation%2520Models%2520as%2520GUI%2520Environments%26entry.906535625%3DHaodong%2520Li%2520and%2520Jingwei%2520Wu%2520and%2520Quan%2520Sun%2520and%2520Guopeng%2520Li%2520and%2520Juanxi%2520Tian%2520and%2520Huanyu%2520Zhang%2520and%2520Yanlin%2520Lai%2520and%2520Ruichuan%2520An%2520and%2520Hongbo%2520Peng%2520and%2520Yuhong%2520Dai%2520and%2520Chenxi%2520Li%2520and%2520Chunmei%2520Qing%2520and%2520Jia%2520Wang%2520and%2520Ziyang%2520Meng%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%26entry.1292438233%3DRecent%2520advancements%2520in%2520image%2520generation%2520models%2520have%2520enabled%2520the%2520prediction%2520of%2520future%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520states%2520based%2520on%2520user%2520instructions.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520general%2520domain%2520visual%2520fidelity%252C%2520leaving%2520the%2520evaluation%2520of%2520state%2520transitions%2520and%2520temporal%2520coherence%2520in%2520GUI-specific%2520contexts%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520GEBench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520dynamic%2520interaction%2520and%2520temporal%2520coherence%2520in%2520GUI%2520generation.%2520GEBench%2520comprises%2520700%2520carefully%2520curated%2520samples%2520spanning%2520five%2520task%2520categories%252C%2520covering%2520both%2520single-step%2520interactions%2520and%2520multi-step%2520trajectories%2520across%2520real-world%2520and%2520fictional%2520scenarios%252C%2520as%2520well%2520as%2520grounding%2520point%2520localization.%2520To%2520support%2520systematic%2520evaluation%252C%2520we%2520propose%2520GE-Score%252C%2520a%2520novel%2520five-dimensional%2520metric%2520that%2520assesses%2520Goal%2520Achievement%252C%2520Interaction%2520Logic%252C%2520Content%2520Consistency%252C%2520UI%2520Plausibility%252C%2520and%2520Visual%2520Quality.%2520Extensive%2520evaluations%2520on%2520current%2520models%2520indicate%2520that%2520while%2520they%2520perform%2520well%2520on%2520single-step%2520transitions%252C%2520they%2520struggle%2520significantly%2520with%2520maintaining%2520temporal%2520coherence%2520and%2520spatial%2520grounding%2520over%2520longer%2520interaction%2520sequences.%2520Our%2520findings%2520identify%2520icon%2520interpretation%252C%2520text%2520rendering%252C%2520and%2520localization%2520precision%2520as%2520critical%2520bottlenecks.%2520This%2520work%2520provides%2520a%2520foundation%2520for%2520systematic%2520assessment%2520and%2520suggests%2520promising%2520directions%2520for%2520future%2520research%2520toward%2520building%2520high-fidelity%2520generative%2520GUI%2520environments.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/stepfun-ai/GEBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments&entry.906535625=Haodong%20Li%20and%20Jingwei%20Wu%20and%20Quan%20Sun%20and%20Guopeng%20Li%20and%20Juanxi%20Tian%20and%20Huanyu%20Zhang%20and%20Yanlin%20Lai%20and%20Ruichuan%20An%20and%20Hongbo%20Peng%20and%20Yuhong%20Dai%20and%20Chenxi%20Li%20and%20Chunmei%20Qing%20and%20Jia%20Wang%20and%20Ziyang%20Meng%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang&entry.1292438233=Recent%20advancements%20in%20image%20generation%20models%20have%20enabled%20the%20prediction%20of%20future%20Graphical%20User%20Interface%20%28GUI%29%20states%20based%20on%20user%20instructions.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20general%20domain%20visual%20fidelity%2C%20leaving%20the%20evaluation%20of%20state%20transitions%20and%20temporal%20coherence%20in%20GUI-specific%20contexts%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20GEBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20dynamic%20interaction%20and%20temporal%20coherence%20in%20GUI%20generation.%20GEBench%20comprises%20700%20carefully%20curated%20samples%20spanning%20five%20task%20categories%2C%20covering%20both%20single-step%20interactions%20and%20multi-step%20trajectories%20across%20real-world%20and%20fictional%20scenarios%2C%20as%20well%20as%20grounding%20point%20localization.%20To%20support%20systematic%20evaluation%2C%20we%20propose%20GE-Score%2C%20a%20novel%20five-dimensional%20metric%20that%20assesses%20Goal%20Achievement%2C%20Interaction%20Logic%2C%20Content%20Consistency%2C%20UI%20Plausibility%2C%20and%20Visual%20Quality.%20Extensive%20evaluations%20on%20current%20models%20indicate%20that%20while%20they%20perform%20well%20on%20single-step%20transitions%2C%20they%20struggle%20significantly%20with%20maintaining%20temporal%20coherence%20and%20spatial%20grounding%20over%20longer%20interaction%20sequences.%20Our%20findings%20identify%20icon%20interpretation%2C%20text%20rendering%2C%20and%20localization%20precision%20as%20critical%20bottlenecks.%20This%20work%20provides%20a%20foundation%20for%20systematic%20assessment%20and%20suggests%20promising%20directions%20for%20future%20research%20toward%20building%20high-fidelity%20generative%20GUI%20environments.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/stepfun-ai/GEBench.&entry.1838667208=http%3A//arxiv.org/abs/2602.09007v1&entry.124074799=Read"},
{"title": "Thegra: Graph-based SLAM for Thermal Imagery", "author": "Anastasiia Kornilova and Ivan Moskalenko and Arabella Gromova and Gonzalo Ferrer and Alexander Menshchikov", "abstract": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.", "link": "http://arxiv.org/abs/2602.08531v1", "date": "2026-02-09", "relevancy": 2.8574, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thegra%3A%20Graph-based%20SLAM%20for%20Thermal%20Imagery&body=Title%3A%20Thegra%3A%20Graph-based%20SLAM%20for%20Thermal%20Imagery%0AAuthor%3A%20Anastasiia%20Kornilova%20and%20Ivan%20Moskalenko%20and%20Arabella%20Gromova%20and%20Gonzalo%20Ferrer%20and%20Alexander%20Menshchikov%0AAbstract%3A%20Thermal%20imaging%20provides%20a%20practical%20sensing%20modality%20for%20visual%20SLAM%20in%20visually%20degraded%20environments%20such%20as%20low%20illumination%2C%20smoke%2C%20or%20adverse%20weather.%20However%2C%20thermal%20imagery%20often%20exhibits%20low%20texture%2C%20low%20contrast%2C%20and%20high%20noise%2C%20complicating%20feature-based%20SLAM.%20In%20this%20work%2C%20we%20propose%20a%20sparse%20monocular%20graph-based%20SLAM%20system%20for%20thermal%20imagery%20that%20leverages%20general-purpose%20learned%20features%20--%20the%20SuperPoint%20detector%20and%20LightGlue%20matcher%2C%20trained%20on%20large-scale%20visible-spectrum%20data%20to%20improve%20cross-domain%20generalization.%20To%20adapt%20these%20components%20to%20thermal%20data%2C%20we%20introduce%20a%20preprocessing%20pipeline%20to%20enhance%20input%20suitability%20and%20modify%20core%20SLAM%20modules%20to%20handle%20sparse%20and%20outlier-prone%20feature%20matches.%20We%20further%20incorporate%20keypoint%20confidence%20scores%20from%20SuperPoint%20into%20a%20confidence-weighted%20factor%20graph%20to%20improve%20estimation%20robustness.%20Evaluations%20on%20public%20thermal%20datasets%20demonstrate%20that%20the%20proposed%20system%20achieves%20reliable%20performance%20without%20requiring%20dataset-specific%20training%20or%20fine-tuning%20a%20desired%20feature%20detector%2C%20given%20the%20scarcity%20of%20quality%20thermal%20data.%20Code%20will%20be%20made%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThegra%253A%2520Graph-based%2520SLAM%2520for%2520Thermal%2520Imagery%26entry.906535625%3DAnastasiia%2520Kornilova%2520and%2520Ivan%2520Moskalenko%2520and%2520Arabella%2520Gromova%2520and%2520Gonzalo%2520Ferrer%2520and%2520Alexander%2520Menshchikov%26entry.1292438233%3DThermal%2520imaging%2520provides%2520a%2520practical%2520sensing%2520modality%2520for%2520visual%2520SLAM%2520in%2520visually%2520degraded%2520environments%2520such%2520as%2520low%2520illumination%252C%2520smoke%252C%2520or%2520adverse%2520weather.%2520However%252C%2520thermal%2520imagery%2520often%2520exhibits%2520low%2520texture%252C%2520low%2520contrast%252C%2520and%2520high%2520noise%252C%2520complicating%2520feature-based%2520SLAM.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520sparse%2520monocular%2520graph-based%2520SLAM%2520system%2520for%2520thermal%2520imagery%2520that%2520leverages%2520general-purpose%2520learned%2520features%2520--%2520the%2520SuperPoint%2520detector%2520and%2520LightGlue%2520matcher%252C%2520trained%2520on%2520large-scale%2520visible-spectrum%2520data%2520to%2520improve%2520cross-domain%2520generalization.%2520To%2520adapt%2520these%2520components%2520to%2520thermal%2520data%252C%2520we%2520introduce%2520a%2520preprocessing%2520pipeline%2520to%2520enhance%2520input%2520suitability%2520and%2520modify%2520core%2520SLAM%2520modules%2520to%2520handle%2520sparse%2520and%2520outlier-prone%2520feature%2520matches.%2520We%2520further%2520incorporate%2520keypoint%2520confidence%2520scores%2520from%2520SuperPoint%2520into%2520a%2520confidence-weighted%2520factor%2520graph%2520to%2520improve%2520estimation%2520robustness.%2520Evaluations%2520on%2520public%2520thermal%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520system%2520achieves%2520reliable%2520performance%2520without%2520requiring%2520dataset-specific%2520training%2520or%2520fine-tuning%2520a%2520desired%2520feature%2520detector%252C%2520given%2520the%2520scarcity%2520of%2520quality%2520thermal%2520data.%2520Code%2520will%2520be%2520made%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thegra%3A%20Graph-based%20SLAM%20for%20Thermal%20Imagery&entry.906535625=Anastasiia%20Kornilova%20and%20Ivan%20Moskalenko%20and%20Arabella%20Gromova%20and%20Gonzalo%20Ferrer%20and%20Alexander%20Menshchikov&entry.1292438233=Thermal%20imaging%20provides%20a%20practical%20sensing%20modality%20for%20visual%20SLAM%20in%20visually%20degraded%20environments%20such%20as%20low%20illumination%2C%20smoke%2C%20or%20adverse%20weather.%20However%2C%20thermal%20imagery%20often%20exhibits%20low%20texture%2C%20low%20contrast%2C%20and%20high%20noise%2C%20complicating%20feature-based%20SLAM.%20In%20this%20work%2C%20we%20propose%20a%20sparse%20monocular%20graph-based%20SLAM%20system%20for%20thermal%20imagery%20that%20leverages%20general-purpose%20learned%20features%20--%20the%20SuperPoint%20detector%20and%20LightGlue%20matcher%2C%20trained%20on%20large-scale%20visible-spectrum%20data%20to%20improve%20cross-domain%20generalization.%20To%20adapt%20these%20components%20to%20thermal%20data%2C%20we%20introduce%20a%20preprocessing%20pipeline%20to%20enhance%20input%20suitability%20and%20modify%20core%20SLAM%20modules%20to%20handle%20sparse%20and%20outlier-prone%20feature%20matches.%20We%20further%20incorporate%20keypoint%20confidence%20scores%20from%20SuperPoint%20into%20a%20confidence-weighted%20factor%20graph%20to%20improve%20estimation%20robustness.%20Evaluations%20on%20public%20thermal%20datasets%20demonstrate%20that%20the%20proposed%20system%20achieves%20reliable%20performance%20without%20requiring%20dataset-specific%20training%20or%20fine-tuning%20a%20desired%20feature%20detector%2C%20given%20the%20scarcity%20of%20quality%20thermal%20data.%20Code%20will%20be%20made%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2602.08531v1&entry.124074799=Read"},
{"title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "author": "Hao Tang and Chenwei Xie and Xiaoyi Bao and Tingyu Weng and Pandeng Li and Yun Zheng and Liwei Wang", "abstract": "In this paper, we propose UniLIP, a unified framework that adapts CLIP for multimodal understanding, generation and editing. Although CLIP excels at understanding, it lacks reconstruction abilities required to be a unified visual encoder. However, previous CLIP-based unified methods fail to balance understanding and reconstruction, leading to semantic degradation or inconsistent reconstructions. In contrast, we introduce a novel two-stage training scheme with a self-distillation strategy that progressively endows CLIP with high-fidelity reconstruction abilities while preserving its original comprehension performance. For enhanced reasoning and consistency in generation and editing, we further develop a dual-condition architecture built upon the MetaQuery framework. Our architecture jointly utilizes multimodal hidden states for rich contextual details and learnable query embeddings to harness the powerful reasoning abilities of Multimodal Large Language Models (MLLMs). Leveraging advanced image representation and architectural design, UniLIP demonstrates superior instruction following and edit fidelity. With only 1B and 3B parameters, UniLIP can outperform larger unified models such as BAGEL (7B) and Uniworld-V1 (12B), achieving state-of-the-art performance of 0.90 on GenEval, 0.63 on WISE, and 3.94 on ImgEdit. These results demonstrate that UniLIP successfully expands the application of CLIP, establishing its continuous features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks. Code and models are available at https://github.com/nnnth/UniLIP.", "link": "http://arxiv.org/abs/2507.23278v3", "date": "2026-02-09", "relevancy": 2.8531, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.61}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLiP%3A%20Adapting%20CLIP%20for%20Unified%20Multimodal%20Understanding%2C%20Generation%20and%20Editing&body=Title%3A%20UniLiP%3A%20Adapting%20CLIP%20for%20Unified%20Multimodal%20Understanding%2C%20Generation%20and%20Editing%0AAuthor%3A%20Hao%20Tang%20and%20Chenwei%20Xie%20and%20Xiaoyi%20Bao%20and%20Tingyu%20Weng%20and%20Pandeng%20Li%20and%20Yun%20Zheng%20and%20Liwei%20Wang%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20UniLIP%2C%20a%20unified%20framework%20that%20adapts%20CLIP%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Although%20CLIP%20excels%20at%20understanding%2C%20it%20lacks%20reconstruction%20abilities%20required%20to%20be%20a%20unified%20visual%20encoder.%20However%2C%20previous%20CLIP-based%20unified%20methods%20fail%20to%20balance%20understanding%20and%20reconstruction%2C%20leading%20to%20semantic%20degradation%20or%20inconsistent%20reconstructions.%20In%20contrast%2C%20we%20introduce%20a%20novel%20two-stage%20training%20scheme%20with%20a%20self-distillation%20strategy%20that%20progressively%20endows%20CLIP%20with%20high-fidelity%20reconstruction%20abilities%20while%20preserving%20its%20original%20comprehension%20performance.%20For%20enhanced%20reasoning%20and%20consistency%20in%20generation%20and%20editing%2C%20we%20further%20develop%20a%20dual-condition%20architecture%20built%20upon%20the%20MetaQuery%20framework.%20Our%20architecture%20jointly%20utilizes%20multimodal%20hidden%20states%20for%20rich%20contextual%20details%20and%20learnable%20query%20embeddings%20to%20harness%20the%20powerful%20reasoning%20abilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Leveraging%20advanced%20image%20representation%20and%20architectural%20design%2C%20UniLIP%20demonstrates%20superior%20instruction%20following%20and%20edit%20fidelity.%20With%20only%201B%20and%203B%20parameters%2C%20UniLIP%20can%20outperform%20larger%20unified%20models%20such%20as%20BAGEL%20%287B%29%20and%20Uniworld-V1%20%2812B%29%2C%20achieving%20state-of-the-art%20performance%20of%200.90%20on%20GenEval%2C%200.63%20on%20WISE%2C%20and%203.94%20on%20ImgEdit.%20These%20results%20demonstrate%20that%20UniLIP%20successfully%20expands%20the%20application%20of%20CLIP%2C%20establishing%20its%20continuous%20features%20to%20not%20only%20serve%20as%20the%20optimal%20choice%20for%20understanding%20tasks%20but%20also%20achieve%20highly%20competitive%20performance%20in%20generation%20and%20editing%20tasks.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/nnnth/UniLIP.%0ALink%3A%20http%3A//arxiv.org/abs/2507.23278v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLiP%253A%2520Adapting%2520CLIP%2520for%2520Unified%2520Multimodal%2520Understanding%252C%2520Generation%2520and%2520Editing%26entry.906535625%3DHao%2520Tang%2520and%2520Chenwei%2520Xie%2520and%2520Xiaoyi%2520Bao%2520and%2520Tingyu%2520Weng%2520and%2520Pandeng%2520Li%2520and%2520Yun%2520Zheng%2520and%2520Liwei%2520Wang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520UniLIP%252C%2520a%2520unified%2520framework%2520that%2520adapts%2520CLIP%2520for%2520multimodal%2520understanding%252C%2520generation%2520and%2520editing.%2520Although%2520CLIP%2520excels%2520at%2520understanding%252C%2520it%2520lacks%2520reconstruction%2520abilities%2520required%2520to%2520be%2520a%2520unified%2520visual%2520encoder.%2520However%252C%2520previous%2520CLIP-based%2520unified%2520methods%2520fail%2520to%2520balance%2520understanding%2520and%2520reconstruction%252C%2520leading%2520to%2520semantic%2520degradation%2520or%2520inconsistent%2520reconstructions.%2520In%2520contrast%252C%2520we%2520introduce%2520a%2520novel%2520two-stage%2520training%2520scheme%2520with%2520a%2520self-distillation%2520strategy%2520that%2520progressively%2520endows%2520CLIP%2520with%2520high-fidelity%2520reconstruction%2520abilities%2520while%2520preserving%2520its%2520original%2520comprehension%2520performance.%2520For%2520enhanced%2520reasoning%2520and%2520consistency%2520in%2520generation%2520and%2520editing%252C%2520we%2520further%2520develop%2520a%2520dual-condition%2520architecture%2520built%2520upon%2520the%2520MetaQuery%2520framework.%2520Our%2520architecture%2520jointly%2520utilizes%2520multimodal%2520hidden%2520states%2520for%2520rich%2520contextual%2520details%2520and%2520learnable%2520query%2520embeddings%2520to%2520harness%2520the%2520powerful%2520reasoning%2520abilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Leveraging%2520advanced%2520image%2520representation%2520and%2520architectural%2520design%252C%2520UniLIP%2520demonstrates%2520superior%2520instruction%2520following%2520and%2520edit%2520fidelity.%2520With%2520only%25201B%2520and%25203B%2520parameters%252C%2520UniLIP%2520can%2520outperform%2520larger%2520unified%2520models%2520such%2520as%2520BAGEL%2520%25287B%2529%2520and%2520Uniworld-V1%2520%252812B%2529%252C%2520achieving%2520state-of-the-art%2520performance%2520of%25200.90%2520on%2520GenEval%252C%25200.63%2520on%2520WISE%252C%2520and%25203.94%2520on%2520ImgEdit.%2520These%2520results%2520demonstrate%2520that%2520UniLIP%2520successfully%2520expands%2520the%2520application%2520of%2520CLIP%252C%2520establishing%2520its%2520continuous%2520features%2520to%2520not%2520only%2520serve%2520as%2520the%2520optimal%2520choice%2520for%2520understanding%2520tasks%2520but%2520also%2520achieve%2520highly%2520competitive%2520performance%2520in%2520generation%2520and%2520editing%2520tasks.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/nnnth/UniLIP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23278v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLiP%3A%20Adapting%20CLIP%20for%20Unified%20Multimodal%20Understanding%2C%20Generation%20and%20Editing&entry.906535625=Hao%20Tang%20and%20Chenwei%20Xie%20and%20Xiaoyi%20Bao%20and%20Tingyu%20Weng%20and%20Pandeng%20Li%20and%20Yun%20Zheng%20and%20Liwei%20Wang&entry.1292438233=In%20this%20paper%2C%20we%20propose%20UniLIP%2C%20a%20unified%20framework%20that%20adapts%20CLIP%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Although%20CLIP%20excels%20at%20understanding%2C%20it%20lacks%20reconstruction%20abilities%20required%20to%20be%20a%20unified%20visual%20encoder.%20However%2C%20previous%20CLIP-based%20unified%20methods%20fail%20to%20balance%20understanding%20and%20reconstruction%2C%20leading%20to%20semantic%20degradation%20or%20inconsistent%20reconstructions.%20In%20contrast%2C%20we%20introduce%20a%20novel%20two-stage%20training%20scheme%20with%20a%20self-distillation%20strategy%20that%20progressively%20endows%20CLIP%20with%20high-fidelity%20reconstruction%20abilities%20while%20preserving%20its%20original%20comprehension%20performance.%20For%20enhanced%20reasoning%20and%20consistency%20in%20generation%20and%20editing%2C%20we%20further%20develop%20a%20dual-condition%20architecture%20built%20upon%20the%20MetaQuery%20framework.%20Our%20architecture%20jointly%20utilizes%20multimodal%20hidden%20states%20for%20rich%20contextual%20details%20and%20learnable%20query%20embeddings%20to%20harness%20the%20powerful%20reasoning%20abilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Leveraging%20advanced%20image%20representation%20and%20architectural%20design%2C%20UniLIP%20demonstrates%20superior%20instruction%20following%20and%20edit%20fidelity.%20With%20only%201B%20and%203B%20parameters%2C%20UniLIP%20can%20outperform%20larger%20unified%20models%20such%20as%20BAGEL%20%287B%29%20and%20Uniworld-V1%20%2812B%29%2C%20achieving%20state-of-the-art%20performance%20of%200.90%20on%20GenEval%2C%200.63%20on%20WISE%2C%20and%203.94%20on%20ImgEdit.%20These%20results%20demonstrate%20that%20UniLIP%20successfully%20expands%20the%20application%20of%20CLIP%2C%20establishing%20its%20continuous%20features%20to%20not%20only%20serve%20as%20the%20optimal%20choice%20for%20understanding%20tasks%20but%20also%20achieve%20highly%20competitive%20performance%20in%20generation%20and%20editing%20tasks.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/nnnth/UniLIP.&entry.1838667208=http%3A//arxiv.org/abs/2507.23278v3&entry.124074799=Read"},
{"title": "Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains", "author": "Jaesung Bae and Minje Kim", "abstract": "Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.", "link": "http://arxiv.org/abs/2602.02841v2", "date": "2026-02-09", "relevancy": 2.8485, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5829}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5687}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantics-Aware%20Generative%20Latent%20Data%20Augmentation%20for%20Learning%20in%20Low-Resource%20Domains&body=Title%3A%20Semantics-Aware%20Generative%20Latent%20Data%20Augmentation%20for%20Learning%20in%20Low-Resource%20Domains%0AAuthor%3A%20Jaesung%20Bae%20and%20Minje%20Kim%0AAbstract%3A%20Despite%20strong%20performance%20in%20data-rich%20regimes%2C%20deep%20learning%20often%20underperforms%20in%20the%20data-scarce%20settings%20common%20in%20practice.%20While%20foundation%20models%20%28FMs%29%20trained%20on%20massive%20datasets%20demonstrate%20strong%20generalization%20by%20extracting%20general-purpose%20features%2C%20they%20can%20still%20suffer%20from%20scarce%20labeled%20data%20during%20downstream%20fine-tuning.%20To%20address%20this%2C%20we%20propose%20GeLDA%2C%20a%20semantics-aware%20generative%20latent%20data%20augmentation%20framework%20that%20leverages%20conditional%20diffusion%20models%20to%20synthesize%20samples%20in%20an%20FM-induced%20latent%20space.%20Because%20this%20space%20is%20low-dimensional%20and%20concentrates%20task-relevant%20information%20compared%20to%20the%20input%20space%2C%20GeLDA%20enables%20efficient%2C%20high-quality%20data%20generation.%20GeLDA%20conditions%20generation%20on%20auxiliary%20feature%20vectors%20that%20capture%20semantic%20relationships%20among%20classes%20or%20subdomains%2C%20facilitating%20data%20augmentation%20in%20low-resource%20domains.%20We%20validate%20GeLDA%20in%20two%20large-scale%20recognition%20tasks%3A%20%28a%29%20in%20zero-shot%20language-specific%20speech%20emotion%20recognition%2C%20GeLDA%20improves%20the%20Whisper-large%20baseline%27s%20unweighted%20average%20recall%20by%206.13%25%3B%20and%20%28b%29%20in%20long-tailed%20image%20classification%2C%20it%20achieves%2074.7%25%20tail-class%20accuracy%20on%20ImageNet-LT%2C%20setting%20a%20new%20state-of-the-art%20result.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantics-Aware%2520Generative%2520Latent%2520Data%2520Augmentation%2520for%2520Learning%2520in%2520Low-Resource%2520Domains%26entry.906535625%3DJaesung%2520Bae%2520and%2520Minje%2520Kim%26entry.1292438233%3DDespite%2520strong%2520performance%2520in%2520data-rich%2520regimes%252C%2520deep%2520learning%2520often%2520underperforms%2520in%2520the%2520data-scarce%2520settings%2520common%2520in%2520practice.%2520While%2520foundation%2520models%2520%2528FMs%2529%2520trained%2520on%2520massive%2520datasets%2520demonstrate%2520strong%2520generalization%2520by%2520extracting%2520general-purpose%2520features%252C%2520they%2520can%2520still%2520suffer%2520from%2520scarce%2520labeled%2520data%2520during%2520downstream%2520fine-tuning.%2520To%2520address%2520this%252C%2520we%2520propose%2520GeLDA%252C%2520a%2520semantics-aware%2520generative%2520latent%2520data%2520augmentation%2520framework%2520that%2520leverages%2520conditional%2520diffusion%2520models%2520to%2520synthesize%2520samples%2520in%2520an%2520FM-induced%2520latent%2520space.%2520Because%2520this%2520space%2520is%2520low-dimensional%2520and%2520concentrates%2520task-relevant%2520information%2520compared%2520to%2520the%2520input%2520space%252C%2520GeLDA%2520enables%2520efficient%252C%2520high-quality%2520data%2520generation.%2520GeLDA%2520conditions%2520generation%2520on%2520auxiliary%2520feature%2520vectors%2520that%2520capture%2520semantic%2520relationships%2520among%2520classes%2520or%2520subdomains%252C%2520facilitating%2520data%2520augmentation%2520in%2520low-resource%2520domains.%2520We%2520validate%2520GeLDA%2520in%2520two%2520large-scale%2520recognition%2520tasks%253A%2520%2528a%2529%2520in%2520zero-shot%2520language-specific%2520speech%2520emotion%2520recognition%252C%2520GeLDA%2520improves%2520the%2520Whisper-large%2520baseline%2527s%2520unweighted%2520average%2520recall%2520by%25206.13%2525%253B%2520and%2520%2528b%2529%2520in%2520long-tailed%2520image%2520classification%252C%2520it%2520achieves%252074.7%2525%2520tail-class%2520accuracy%2520on%2520ImageNet-LT%252C%2520setting%2520a%2520new%2520state-of-the-art%2520result.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantics-Aware%20Generative%20Latent%20Data%20Augmentation%20for%20Learning%20in%20Low-Resource%20Domains&entry.906535625=Jaesung%20Bae%20and%20Minje%20Kim&entry.1292438233=Despite%20strong%20performance%20in%20data-rich%20regimes%2C%20deep%20learning%20often%20underperforms%20in%20the%20data-scarce%20settings%20common%20in%20practice.%20While%20foundation%20models%20%28FMs%29%20trained%20on%20massive%20datasets%20demonstrate%20strong%20generalization%20by%20extracting%20general-purpose%20features%2C%20they%20can%20still%20suffer%20from%20scarce%20labeled%20data%20during%20downstream%20fine-tuning.%20To%20address%20this%2C%20we%20propose%20GeLDA%2C%20a%20semantics-aware%20generative%20latent%20data%20augmentation%20framework%20that%20leverages%20conditional%20diffusion%20models%20to%20synthesize%20samples%20in%20an%20FM-induced%20latent%20space.%20Because%20this%20space%20is%20low-dimensional%20and%20concentrates%20task-relevant%20information%20compared%20to%20the%20input%20space%2C%20GeLDA%20enables%20efficient%2C%20high-quality%20data%20generation.%20GeLDA%20conditions%20generation%20on%20auxiliary%20feature%20vectors%20that%20capture%20semantic%20relationships%20among%20classes%20or%20subdomains%2C%20facilitating%20data%20augmentation%20in%20low-resource%20domains.%20We%20validate%20GeLDA%20in%20two%20large-scale%20recognition%20tasks%3A%20%28a%29%20in%20zero-shot%20language-specific%20speech%20emotion%20recognition%2C%20GeLDA%20improves%20the%20Whisper-large%20baseline%27s%20unweighted%20average%20recall%20by%206.13%25%3B%20and%20%28b%29%20in%20long-tailed%20image%20classification%2C%20it%20achieves%2074.7%25%20tail-class%20accuracy%20on%20ImageNet-LT%2C%20setting%20a%20new%20state-of-the-art%20result.&entry.1838667208=http%3A//arxiv.org/abs/2602.02841v2&entry.124074799=Read"},
{"title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars", "author": "Vineet Kumar Rakesh and Ahana Bhattacharjee and Soumya Mazumdar and Tapas Samanta and Hemendra Kumar Pandey and Amitabha Das and Sarbajit Pal", "abstract": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg", "link": "http://arxiv.org/abs/2602.08775v1", "date": "2026-02-09", "relevancy": 2.8054, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5675}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5675}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VedicTHG%3A%20Symbolic%20Vedic%20Computation%20for%20Low-Resource%20Talking-Head%20Generation%20in%20Educational%20Avatars&body=Title%3A%20VedicTHG%3A%20Symbolic%20Vedic%20Computation%20for%20Low-Resource%20Talking-Head%20Generation%20in%20Educational%20Avatars%0AAuthor%3A%20Vineet%20Kumar%20Rakesh%20and%20Ahana%20Bhattacharjee%20and%20Soumya%20Mazumdar%20and%20Tapas%20Samanta%20and%20Hemendra%20Kumar%20Pandey%20and%20Amitabha%20Das%20and%20Sarbajit%20Pal%0AAbstract%3A%20Talking-head%20avatars%20are%20increasingly%20adopted%20in%20educational%20technology%20to%20deliver%20content%20with%20social%20presence%20and%20improved%20engagement.%20However%2C%20many%20recent%20talking-head%20generation%20%28THG%29%20methods%20rely%20on%20GPU-centric%20neural%20rendering%2C%20large%20training%20sets%2C%20or%20high-capacity%20diffusion%20models%2C%20which%20limits%20deployment%20in%20offline%20or%20resource-constrained%20learning%20environments.%20A%20deterministic%20and%20CPU-oriented%20THG%20framework%20is%20described%2C%20termed%20Symbolic%20Vedic%20Computation%2C%20that%20converts%20speech%20to%20a%20time-aligned%20phoneme%20stream%2C%20maps%20phonemes%20to%20a%20compact%20viseme%20inventory%2C%20and%20produces%20smooth%20viseme%20trajectories%20through%20symbolic%20coarticulation%20inspired%20by%20Vedic%20sutra%20Urdhva%20Tiryakbhyam.%20A%20lightweight%202D%20renderer%20performs%20region-of-interest%20%28ROI%29%20warping%20and%20mouth%20compositing%20with%20stabilization%20to%20support%20real-time%20synthesis%20on%20commodity%20CPUs.%20Experiments%20report%20synchronization%20accuracy%2C%20temporal%20stability%2C%20and%20identity%20consistency%20under%20CPU-only%20execution%2C%20alongside%20benchmarking%20against%20representative%20CPU-feasible%20baselines.%20Results%20indicate%20that%20acceptable%20lip-sync%20quality%20can%20be%20achieved%20while%20substantially%20reducing%20computational%20load%20and%20latency%2C%20supporting%20practical%20educational%20avatars%20on%20low-end%20hardware.%20GitHub%3A%20https%3A//vineetkumarrakesh.github.io/vedicthg%0ALink%3A%20http%3A//arxiv.org/abs/2602.08775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVedicTHG%253A%2520Symbolic%2520Vedic%2520Computation%2520for%2520Low-Resource%2520Talking-Head%2520Generation%2520in%2520Educational%2520Avatars%26entry.906535625%3DVineet%2520Kumar%2520Rakesh%2520and%2520Ahana%2520Bhattacharjee%2520and%2520Soumya%2520Mazumdar%2520and%2520Tapas%2520Samanta%2520and%2520Hemendra%2520Kumar%2520Pandey%2520and%2520Amitabha%2520Das%2520and%2520Sarbajit%2520Pal%26entry.1292438233%3DTalking-head%2520avatars%2520are%2520increasingly%2520adopted%2520in%2520educational%2520technology%2520to%2520deliver%2520content%2520with%2520social%2520presence%2520and%2520improved%2520engagement.%2520However%252C%2520many%2520recent%2520talking-head%2520generation%2520%2528THG%2529%2520methods%2520rely%2520on%2520GPU-centric%2520neural%2520rendering%252C%2520large%2520training%2520sets%252C%2520or%2520high-capacity%2520diffusion%2520models%252C%2520which%2520limits%2520deployment%2520in%2520offline%2520or%2520resource-constrained%2520learning%2520environments.%2520A%2520deterministic%2520and%2520CPU-oriented%2520THG%2520framework%2520is%2520described%252C%2520termed%2520Symbolic%2520Vedic%2520Computation%252C%2520that%2520converts%2520speech%2520to%2520a%2520time-aligned%2520phoneme%2520stream%252C%2520maps%2520phonemes%2520to%2520a%2520compact%2520viseme%2520inventory%252C%2520and%2520produces%2520smooth%2520viseme%2520trajectories%2520through%2520symbolic%2520coarticulation%2520inspired%2520by%2520Vedic%2520sutra%2520Urdhva%2520Tiryakbhyam.%2520A%2520lightweight%25202D%2520renderer%2520performs%2520region-of-interest%2520%2528ROI%2529%2520warping%2520and%2520mouth%2520compositing%2520with%2520stabilization%2520to%2520support%2520real-time%2520synthesis%2520on%2520commodity%2520CPUs.%2520Experiments%2520report%2520synchronization%2520accuracy%252C%2520temporal%2520stability%252C%2520and%2520identity%2520consistency%2520under%2520CPU-only%2520execution%252C%2520alongside%2520benchmarking%2520against%2520representative%2520CPU-feasible%2520baselines.%2520Results%2520indicate%2520that%2520acceptable%2520lip-sync%2520quality%2520can%2520be%2520achieved%2520while%2520substantially%2520reducing%2520computational%2520load%2520and%2520latency%252C%2520supporting%2520practical%2520educational%2520avatars%2520on%2520low-end%2520hardware.%2520GitHub%253A%2520https%253A//vineetkumarrakesh.github.io/vedicthg%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VedicTHG%3A%20Symbolic%20Vedic%20Computation%20for%20Low-Resource%20Talking-Head%20Generation%20in%20Educational%20Avatars&entry.906535625=Vineet%20Kumar%20Rakesh%20and%20Ahana%20Bhattacharjee%20and%20Soumya%20Mazumdar%20and%20Tapas%20Samanta%20and%20Hemendra%20Kumar%20Pandey%20and%20Amitabha%20Das%20and%20Sarbajit%20Pal&entry.1292438233=Talking-head%20avatars%20are%20increasingly%20adopted%20in%20educational%20technology%20to%20deliver%20content%20with%20social%20presence%20and%20improved%20engagement.%20However%2C%20many%20recent%20talking-head%20generation%20%28THG%29%20methods%20rely%20on%20GPU-centric%20neural%20rendering%2C%20large%20training%20sets%2C%20or%20high-capacity%20diffusion%20models%2C%20which%20limits%20deployment%20in%20offline%20or%20resource-constrained%20learning%20environments.%20A%20deterministic%20and%20CPU-oriented%20THG%20framework%20is%20described%2C%20termed%20Symbolic%20Vedic%20Computation%2C%20that%20converts%20speech%20to%20a%20time-aligned%20phoneme%20stream%2C%20maps%20phonemes%20to%20a%20compact%20viseme%20inventory%2C%20and%20produces%20smooth%20viseme%20trajectories%20through%20symbolic%20coarticulation%20inspired%20by%20Vedic%20sutra%20Urdhva%20Tiryakbhyam.%20A%20lightweight%202D%20renderer%20performs%20region-of-interest%20%28ROI%29%20warping%20and%20mouth%20compositing%20with%20stabilization%20to%20support%20real-time%20synthesis%20on%20commodity%20CPUs.%20Experiments%20report%20synchronization%20accuracy%2C%20temporal%20stability%2C%20and%20identity%20consistency%20under%20CPU-only%20execution%2C%20alongside%20benchmarking%20against%20representative%20CPU-feasible%20baselines.%20Results%20indicate%20that%20acceptable%20lip-sync%20quality%20can%20be%20achieved%20while%20substantially%20reducing%20computational%20load%20and%20latency%2C%20supporting%20practical%20educational%20avatars%20on%20low-end%20hardware.%20GitHub%3A%20https%3A//vineetkumarrakesh.github.io/vedicthg&entry.1838667208=http%3A//arxiv.org/abs/2602.08775v1&entry.124074799=Read"},
{"title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding", "author": "Peiran Wu and Zhuorui Yu and Yunze Liu and Chi-Hao Wu and Enmin Zhou and Junxiao Shen", "abstract": "The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.", "link": "http://arxiv.org/abs/2510.07915v3", "date": "2026-02-09", "relevancy": 2.7606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding&body=Title%3A%20MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding%0AAuthor%3A%20Peiran%20Wu%20and%20Zhuorui%20Yu%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Enmin%20Zhou%20and%20Junxiao%20Shen%0AAbstract%3A%20The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%20laid%20the%20foundation%20for%20multimodal%20models.%20However%2C%20visual%20language%20models%20%28VLMs%29%20still%20face%20heavy%20computational%20costs%20when%20extended%20from%20images%20to%20videos%20due%20to%20high%20frame%20rates%20and%20long%20durations.%20Token%20compression%20is%20a%20promising%20solution%2C%20yet%20most%20existing%20training-free%20methods%20cause%20information%20loss%20and%20performance%20degradation.%20To%20overcome%20this%2C%20we%20propose%20%5Ctextbf%7BMemory-Augmented%20Reinforcement%20Learning-based%20Token%20Compression%20%28MARC%29%7D%2C%20which%20integrates%20structured%20retrieval%20and%20RL-based%20distillation.%20MARC%20adopts%20a%20%5Ctextit%7Bretrieve-then-compress%7D%20strategy%20using%20a%20%5Ctextbf%7BVisual%20Memory%20Retriever%20%28VMR%29%7D%20to%20select%20key%20clips%20and%20a%20%5Ctextbf%7BCompression%20Group%20Relative%20Policy%20Optimization%20%28C-GRPO%29%7D%20framework%20to%20distil%20reasoning%20ability%20from%20a%20teacher%20to%20a%20student%20model.%20Experiments%20on%20six%20video%20benchmarks%20show%20that%20MARC%20achieves%20near-baseline%20accuracy%20using%20only%20one%20frame%27s%20tokens%20--%20reducing%20visual%20tokens%20by%20%5Ctextbf%7B95%5C%25%7D%2C%20GPU%20memory%20by%20%5Ctextbf%7B72%5C%25%7D%2C%20and%20latency%20by%20%5Ctextbf%7B23.9%5C%25%7D.%20This%20demonstrates%20its%20potential%20for%20efficient%2C%20real-time%20video%20understanding%20in%20resource-constrained%20settings%20such%20as%20video%20QA%2C%20surveillance%2C%20and%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARC%253A%2520Memory-Augmented%2520RL%2520Token%2520Compression%2520for%2520Efficient%2520Video%2520Understanding%26entry.906535625%3DPeiran%2520Wu%2520and%2520Zhuorui%2520Yu%2520and%2520Yunze%2520Liu%2520and%2520Chi-Hao%2520Wu%2520and%2520Enmin%2520Zhou%2520and%2520Junxiao%2520Shen%26entry.1292438233%3DThe%2520rapid%2520progress%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520laid%2520the%2520foundation%2520for%2520multimodal%2520models.%2520However%252C%2520visual%2520language%2520models%2520%2528VLMs%2529%2520still%2520face%2520heavy%2520computational%2520costs%2520when%2520extended%2520from%2520images%2520to%2520videos%2520due%2520to%2520high%2520frame%2520rates%2520and%2520long%2520durations.%2520Token%2520compression%2520is%2520a%2520promising%2520solution%252C%2520yet%2520most%2520existing%2520training-free%2520methods%2520cause%2520information%2520loss%2520and%2520performance%2520degradation.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BMemory-Augmented%2520Reinforcement%2520Learning-based%2520Token%2520Compression%2520%2528MARC%2529%257D%252C%2520which%2520integrates%2520structured%2520retrieval%2520and%2520RL-based%2520distillation.%2520MARC%2520adopts%2520a%2520%255Ctextit%257Bretrieve-then-compress%257D%2520strategy%2520using%2520a%2520%255Ctextbf%257BVisual%2520Memory%2520Retriever%2520%2528VMR%2529%257D%2520to%2520select%2520key%2520clips%2520and%2520a%2520%255Ctextbf%257BCompression%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528C-GRPO%2529%257D%2520framework%2520to%2520distil%2520reasoning%2520ability%2520from%2520a%2520teacher%2520to%2520a%2520student%2520model.%2520Experiments%2520on%2520six%2520video%2520benchmarks%2520show%2520that%2520MARC%2520achieves%2520near-baseline%2520accuracy%2520using%2520only%2520one%2520frame%2527s%2520tokens%2520--%2520reducing%2520visual%2520tokens%2520by%2520%255Ctextbf%257B95%255C%2525%257D%252C%2520GPU%2520memory%2520by%2520%255Ctextbf%257B72%255C%2525%257D%252C%2520and%2520latency%2520by%2520%255Ctextbf%257B23.9%255C%2525%257D.%2520This%2520demonstrates%2520its%2520potential%2520for%2520efficient%252C%2520real-time%2520video%2520understanding%2520in%2520resource-constrained%2520settings%2520such%2520as%2520video%2520QA%252C%2520surveillance%252C%2520and%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding&entry.906535625=Peiran%20Wu%20and%20Zhuorui%20Yu%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Enmin%20Zhou%20and%20Junxiao%20Shen&entry.1292438233=The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%20laid%20the%20foundation%20for%20multimodal%20models.%20However%2C%20visual%20language%20models%20%28VLMs%29%20still%20face%20heavy%20computational%20costs%20when%20extended%20from%20images%20to%20videos%20due%20to%20high%20frame%20rates%20and%20long%20durations.%20Token%20compression%20is%20a%20promising%20solution%2C%20yet%20most%20existing%20training-free%20methods%20cause%20information%20loss%20and%20performance%20degradation.%20To%20overcome%20this%2C%20we%20propose%20%5Ctextbf%7BMemory-Augmented%20Reinforcement%20Learning-based%20Token%20Compression%20%28MARC%29%7D%2C%20which%20integrates%20structured%20retrieval%20and%20RL-based%20distillation.%20MARC%20adopts%20a%20%5Ctextit%7Bretrieve-then-compress%7D%20strategy%20using%20a%20%5Ctextbf%7BVisual%20Memory%20Retriever%20%28VMR%29%7D%20to%20select%20key%20clips%20and%20a%20%5Ctextbf%7BCompression%20Group%20Relative%20Policy%20Optimization%20%28C-GRPO%29%7D%20framework%20to%20distil%20reasoning%20ability%20from%20a%20teacher%20to%20a%20student%20model.%20Experiments%20on%20six%20video%20benchmarks%20show%20that%20MARC%20achieves%20near-baseline%20accuracy%20using%20only%20one%20frame%27s%20tokens%20--%20reducing%20visual%20tokens%20by%20%5Ctextbf%7B95%5C%25%7D%2C%20GPU%20memory%20by%20%5Ctextbf%7B72%5C%25%7D%2C%20and%20latency%20by%20%5Ctextbf%7B23.9%5C%25%7D.%20This%20demonstrates%20its%20potential%20for%20efficient%2C%20real-time%20video%20understanding%20in%20resource-constrained%20settings%20such%20as%20video%20QA%2C%20surveillance%2C%20and%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2510.07915v3&entry.124074799=Read"},
{"title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "author": "Yuhao Dong and Shulin Tian and Shuai Liu and Shuangrui Ding and Yuhang Zang and Xiaoyi Dong and Yuhang Cao and Jiaqi Wang and Ziwei Liu", "abstract": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "link": "http://arxiv.org/abs/2602.08439v1", "date": "2026-02-09", "relevancy": 2.7576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demo-ICL%3A%20In-Context%20Learning%20for%20Procedural%20Video%20Knowledge%20Acquisition&body=Title%3A%20Demo-ICL%3A%20In-Context%20Learning%20for%20Procedural%20Video%20Knowledge%20Acquisition%0AAuthor%3A%20Yuhao%20Dong%20and%20Shulin%20Tian%20and%20Shuai%20Liu%20and%20Shuangrui%20Ding%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Ziwei%20Liu%0AAbstract%3A%20Despite%20the%20growing%20video%20understanding%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20existing%20video%20benchmarks%20primarily%20assess%20understanding%20based%20on%20models%27%20static%2C%20internal%20knowledge%2C%20rather%20than%20their%20ability%20to%20learn%20and%20adapt%20from%20dynamic%2C%20novel%20contexts%20from%20few%20examples.%20To%20bridge%20this%20gap%2C%20we%20present%20Demo-driven%20Video%20In-Context%20Learning%2C%20a%20novel%20task%20focused%20on%20learning%20from%20in-context%20demonstrations%20to%20answer%20questions%20about%20the%20target%20videos.%20Alongside%20this%2C%20we%20propose%20Demo-ICL-Bench%2C%20a%20challenging%20benchmark%20designed%20to%20evaluate%20demo-driven%20video%20in-context%20learning%20capabilities.%20Demo-ICL-Bench%20is%20constructed%20from%201200%20instructional%20YouTube%20videos%20with%20associated%20questions%2C%20from%20which%20two%20types%20of%20demonstrations%20are%20derived%3A%20%28i%29%20summarizing%20video%20subtitles%20for%20text%20demonstration%3B%20and%20%28ii%29%20corresponding%20instructional%20videos%20as%20video%20demonstrations.%20To%20effectively%20tackle%20this%20new%20challenge%2C%20we%20develop%20Demo-ICL%2C%20an%20MLLM%20with%20a%20two-stage%20training%20strategy%3A%20video-supervised%20fine-tuning%20and%20information-assisted%20direct%20preference%20optimization%2C%20jointly%20enhancing%20the%20model%27s%20ability%20to%20learn%20from%20in-context%20examples.%20Extensive%20experiments%20with%20state-of-the-art%20MLLMs%20confirm%20the%20difficulty%20of%20Demo-ICL-Bench%2C%20demonstrate%20the%20effectiveness%20of%20Demo-ICL%2C%20and%20thereby%20unveil%20future%20research%20directions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemo-ICL%253A%2520In-Context%2520Learning%2520for%2520Procedural%2520Video%2520Knowledge%2520Acquisition%26entry.906535625%3DYuhao%2520Dong%2520and%2520Shulin%2520Tian%2520and%2520Shuai%2520Liu%2520and%2520Shuangrui%2520Ding%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Jiaqi%2520Wang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DDespite%2520the%2520growing%2520video%2520understanding%2520capabilities%2520of%2520recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520existing%2520video%2520benchmarks%2520primarily%2520assess%2520understanding%2520based%2520on%2520models%2527%2520static%252C%2520internal%2520knowledge%252C%2520rather%2520than%2520their%2520ability%2520to%2520learn%2520and%2520adapt%2520from%2520dynamic%252C%2520novel%2520contexts%2520from%2520few%2520examples.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520Demo-driven%2520Video%2520In-Context%2520Learning%252C%2520a%2520novel%2520task%2520focused%2520on%2520learning%2520from%2520in-context%2520demonstrations%2520to%2520answer%2520questions%2520about%2520the%2520target%2520videos.%2520Alongside%2520this%252C%2520we%2520propose%2520Demo-ICL-Bench%252C%2520a%2520challenging%2520benchmark%2520designed%2520to%2520evaluate%2520demo-driven%2520video%2520in-context%2520learning%2520capabilities.%2520Demo-ICL-Bench%2520is%2520constructed%2520from%25201200%2520instructional%2520YouTube%2520videos%2520with%2520associated%2520questions%252C%2520from%2520which%2520two%2520types%2520of%2520demonstrations%2520are%2520derived%253A%2520%2528i%2529%2520summarizing%2520video%2520subtitles%2520for%2520text%2520demonstration%253B%2520and%2520%2528ii%2529%2520corresponding%2520instructional%2520videos%2520as%2520video%2520demonstrations.%2520To%2520effectively%2520tackle%2520this%2520new%2520challenge%252C%2520we%2520develop%2520Demo-ICL%252C%2520an%2520MLLM%2520with%2520a%2520two-stage%2520training%2520strategy%253A%2520video-supervised%2520fine-tuning%2520and%2520information-assisted%2520direct%2520preference%2520optimization%252C%2520jointly%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520learn%2520from%2520in-context%2520examples.%2520Extensive%2520experiments%2520with%2520state-of-the-art%2520MLLMs%2520confirm%2520the%2520difficulty%2520of%2520Demo-ICL-Bench%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520Demo-ICL%252C%2520and%2520thereby%2520unveil%2520future%2520research%2520directions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demo-ICL%3A%20In-Context%20Learning%20for%20Procedural%20Video%20Knowledge%20Acquisition&entry.906535625=Yuhao%20Dong%20and%20Shulin%20Tian%20and%20Shuai%20Liu%20and%20Shuangrui%20Ding%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Ziwei%20Liu&entry.1292438233=Despite%20the%20growing%20video%20understanding%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20existing%20video%20benchmarks%20primarily%20assess%20understanding%20based%20on%20models%27%20static%2C%20internal%20knowledge%2C%20rather%20than%20their%20ability%20to%20learn%20and%20adapt%20from%20dynamic%2C%20novel%20contexts%20from%20few%20examples.%20To%20bridge%20this%20gap%2C%20we%20present%20Demo-driven%20Video%20In-Context%20Learning%2C%20a%20novel%20task%20focused%20on%20learning%20from%20in-context%20demonstrations%20to%20answer%20questions%20about%20the%20target%20videos.%20Alongside%20this%2C%20we%20propose%20Demo-ICL-Bench%2C%20a%20challenging%20benchmark%20designed%20to%20evaluate%20demo-driven%20video%20in-context%20learning%20capabilities.%20Demo-ICL-Bench%20is%20constructed%20from%201200%20instructional%20YouTube%20videos%20with%20associated%20questions%2C%20from%20which%20two%20types%20of%20demonstrations%20are%20derived%3A%20%28i%29%20summarizing%20video%20subtitles%20for%20text%20demonstration%3B%20and%20%28ii%29%20corresponding%20instructional%20videos%20as%20video%20demonstrations.%20To%20effectively%20tackle%20this%20new%20challenge%2C%20we%20develop%20Demo-ICL%2C%20an%20MLLM%20with%20a%20two-stage%20training%20strategy%3A%20video-supervised%20fine-tuning%20and%20information-assisted%20direct%20preference%20optimization%2C%20jointly%20enhancing%20the%20model%27s%20ability%20to%20learn%20from%20in-context%20examples.%20Extensive%20experiments%20with%20state-of-the-art%20MLLMs%20confirm%20the%20difficulty%20of%20Demo-ICL-Bench%2C%20demonstrate%20the%20effectiveness%20of%20Demo-ICL%2C%20and%20thereby%20unveil%20future%20research%20directions.&entry.1838667208=http%3A//arxiv.org/abs/2602.08439v1&entry.124074799=Read"},
{"title": "PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior", "author": "Zicong Fan and Edoardo Remelli and David Dimond and Fadime Sener and Liuhao Ge and Bugra Tekin and Cem Keskin and Shreyas Hampali", "abstract": "The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.", "link": "http://arxiv.org/abs/2511.05403v3", "date": "2026-02-09", "relevancy": 2.7485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5494}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior&body=Title%3A%20PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior%0AAuthor%3A%20Zicong%20Fan%20and%20Edoardo%20Remelli%20and%20David%20Dimond%20and%20Fadime%20Sener%20and%20Liuhao%20Ge%20and%20Bugra%20Tekin%20and%20Cem%20Keskin%20and%20Shreyas%20Hampali%0AAbstract%3A%20The%20ability%20to%20grasp%20objects%2C%20signal%20with%20gestures%2C%20and%20share%20emotion%20through%20touch%20all%20stem%20from%20the%20unique%20capabilities%20of%20human%20hands.%20Yet%20creating%20high-quality%20personalized%20hand%20avatars%20from%20images%20remains%20challenging%20due%20to%20complex%20geometry%2C%20appearance%2C%20and%20articulation%2C%20particularly%20under%20unconstrained%20lighting%20and%20limited%20views.%20Progress%20has%20also%20been%20limited%20by%20the%20lack%20of%20datasets%20that%20jointly%20provide%20accurate%203D%20geometry%2C%20high-resolution%20multiview%20imagery%2C%20and%20a%20diverse%20population%20of%20subjects.%20To%20address%20this%2C%20we%20present%20PALM%2C%20a%20large-scale%20dataset%20comprising%2013k%20high-quality%20hand%20scans%20from%20263%20subjects%20and%2090k%20multi-view%20images%2C%20capturing%20rich%20variation%20in%20skin%20tone%2C%20age%2C%20and%20geometry.%20To%20show%20its%20utility%2C%20we%20present%20a%20baseline%20PALM-Net%2C%20a%20multi-subject%20prior%20over%20hand%20geometry%20and%20material%20properties%20learned%20via%20physically%20based%20inverse%20rendering%2C%20enabling%20realistic%2C%20relightable%20single-image%20hand%20avatar%20personalization.%20PALM%27s%20scale%20and%20diversity%20make%20it%20a%20valuable%20real-world%20resource%20for%20hand%20modeling%20and%20related%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05403v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPALM%253A%2520A%2520Dataset%2520and%2520Baseline%2520for%2520Learning%2520Multi-subject%2520Hand%2520Prior%26entry.906535625%3DZicong%2520Fan%2520and%2520Edoardo%2520Remelli%2520and%2520David%2520Dimond%2520and%2520Fadime%2520Sener%2520and%2520Liuhao%2520Ge%2520and%2520Bugra%2520Tekin%2520and%2520Cem%2520Keskin%2520and%2520Shreyas%2520Hampali%26entry.1292438233%3DThe%2520ability%2520to%2520grasp%2520objects%252C%2520signal%2520with%2520gestures%252C%2520and%2520share%2520emotion%2520through%2520touch%2520all%2520stem%2520from%2520the%2520unique%2520capabilities%2520of%2520human%2520hands.%2520Yet%2520creating%2520high-quality%2520personalized%2520hand%2520avatars%2520from%2520images%2520remains%2520challenging%2520due%2520to%2520complex%2520geometry%252C%2520appearance%252C%2520and%2520articulation%252C%2520particularly%2520under%2520unconstrained%2520lighting%2520and%2520limited%2520views.%2520Progress%2520has%2520also%2520been%2520limited%2520by%2520the%2520lack%2520of%2520datasets%2520that%2520jointly%2520provide%2520accurate%25203D%2520geometry%252C%2520high-resolution%2520multiview%2520imagery%252C%2520and%2520a%2520diverse%2520population%2520of%2520subjects.%2520To%2520address%2520this%252C%2520we%2520present%2520PALM%252C%2520a%2520large-scale%2520dataset%2520comprising%252013k%2520high-quality%2520hand%2520scans%2520from%2520263%2520subjects%2520and%252090k%2520multi-view%2520images%252C%2520capturing%2520rich%2520variation%2520in%2520skin%2520tone%252C%2520age%252C%2520and%2520geometry.%2520To%2520show%2520its%2520utility%252C%2520we%2520present%2520a%2520baseline%2520PALM-Net%252C%2520a%2520multi-subject%2520prior%2520over%2520hand%2520geometry%2520and%2520material%2520properties%2520learned%2520via%2520physically%2520based%2520inverse%2520rendering%252C%2520enabling%2520realistic%252C%2520relightable%2520single-image%2520hand%2520avatar%2520personalization.%2520PALM%2527s%2520scale%2520and%2520diversity%2520make%2520it%2520a%2520valuable%2520real-world%2520resource%2520for%2520hand%2520modeling%2520and%2520related%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05403v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior&entry.906535625=Zicong%20Fan%20and%20Edoardo%20Remelli%20and%20David%20Dimond%20and%20Fadime%20Sener%20and%20Liuhao%20Ge%20and%20Bugra%20Tekin%20and%20Cem%20Keskin%20and%20Shreyas%20Hampali&entry.1292438233=The%20ability%20to%20grasp%20objects%2C%20signal%20with%20gestures%2C%20and%20share%20emotion%20through%20touch%20all%20stem%20from%20the%20unique%20capabilities%20of%20human%20hands.%20Yet%20creating%20high-quality%20personalized%20hand%20avatars%20from%20images%20remains%20challenging%20due%20to%20complex%20geometry%2C%20appearance%2C%20and%20articulation%2C%20particularly%20under%20unconstrained%20lighting%20and%20limited%20views.%20Progress%20has%20also%20been%20limited%20by%20the%20lack%20of%20datasets%20that%20jointly%20provide%20accurate%203D%20geometry%2C%20high-resolution%20multiview%20imagery%2C%20and%20a%20diverse%20population%20of%20subjects.%20To%20address%20this%2C%20we%20present%20PALM%2C%20a%20large-scale%20dataset%20comprising%2013k%20high-quality%20hand%20scans%20from%20263%20subjects%20and%2090k%20multi-view%20images%2C%20capturing%20rich%20variation%20in%20skin%20tone%2C%20age%2C%20and%20geometry.%20To%20show%20its%20utility%2C%20we%20present%20a%20baseline%20PALM-Net%2C%20a%20multi-subject%20prior%20over%20hand%20geometry%20and%20material%20properties%20learned%20via%20physically%20based%20inverse%20rendering%2C%20enabling%20realistic%2C%20relightable%20single-image%20hand%20avatar%20personalization.%20PALM%27s%20scale%20and%20diversity%20make%20it%20a%20valuable%20real-world%20resource%20for%20hand%20modeling%20and%20related%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.05403v3&entry.124074799=Read"},
{"title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "author": "Teng Wang and Rong Shan and Jianghao Lin and Junjie Wu and Tianyi Xu and Jianping Zhang and Wenteng Chen and Changwang Zhang and Zhaoxiang Wang and Weinan Zhang and Jun Wang", "abstract": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.", "link": "http://arxiv.org/abs/2602.08603v1", "date": "2026-02-09", "relevancy": 2.7451, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSCAR%3A%20Optimization-Steered%20Agentic%20Planning%20for%20Composed%20Image%20Retrieval&body=Title%3A%20OSCAR%3A%20Optimization-Steered%20Agentic%20Planning%20for%20Composed%20Image%20Retrieval%0AAuthor%3A%20Teng%20Wang%20and%20Rong%20Shan%20and%20Jianghao%20Lin%20and%20Junjie%20Wu%20and%20Tianyi%20Xu%20and%20Jianping%20Zhang%20and%20Wenteng%20Chen%20and%20Changwang%20Zhang%20and%20Zhaoxiang%20Wang%20and%20Weinan%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20Composed%20image%20retrieval%20%28CIR%29%20requires%20complex%20reasoning%20over%20heterogeneous%20visual%20and%20textual%20constraints.%20Existing%20approaches%20largely%20fall%20into%20two%20paradigms%3A%20unified%20embedding%20retrieval%2C%20which%20suffers%20from%20single-model%20myopia%2C%20and%20heuristic%20agentic%20retrieval%2C%20which%20is%20limited%20by%20suboptimal%2C%20trial-and-error%20orchestration.%20To%20this%20end%2C%20we%20propose%20OSCAR%2C%20an%20optimization-steered%20agentic%20planning%20framework%20for%20composed%20image%20retrieval.%20We%20are%20the%20first%20to%20reformulate%20agentic%20CIR%20from%20a%20heuristic%20search%20process%20into%20a%20principled%20trajectory%20optimization%20problem.%20Instead%20of%20relying%20on%20heuristic%20trial-and-error%20exploration%2C%20OSCAR%20employs%20a%20novel%20offline-online%20paradigm.%20In%20the%20offline%20phase%2C%20we%20model%20CIR%20via%20atomic%20retrieval%20selection%20and%20composition%20as%20a%20two-stage%20mixed-integer%20programming%20problem%2C%20mathematically%20deriving%20optimal%20trajectories%20that%20maximize%20ground-truth%20coverage%20for%20training%20samples%20via%20rigorous%20boolean%20set%20operations.%20These%20trajectories%20are%20then%20stored%20in%20a%20golden%20library%20to%20serve%20as%20in-context%20demonstrations%20for%20online%20steering%20of%20VLM%20planner%20at%20online%20inference%20time.%20Extensive%20experiments%20on%20three%20public%20benchmarks%20and%20a%20private%20industrial%20benchmark%20show%20that%20OSCAR%20consistently%20outperforms%20SOTA%20baselines.%20Notably%2C%20it%20achieves%20superior%20performance%20using%20only%2010%25%20of%20training%20data%2C%20demonstrating%20strong%20generalization%20of%20planning%20logic%20rather%20than%20dataset-specific%20memorization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSCAR%253A%2520Optimization-Steered%2520Agentic%2520Planning%2520for%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DTeng%2520Wang%2520and%2520Rong%2520Shan%2520and%2520Jianghao%2520Lin%2520and%2520Junjie%2520Wu%2520and%2520Tianyi%2520Xu%2520and%2520Jianping%2520Zhang%2520and%2520Wenteng%2520Chen%2520and%2520Changwang%2520Zhang%2520and%2520Zhaoxiang%2520Wang%2520and%2520Weinan%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3DComposed%2520image%2520retrieval%2520%2528CIR%2529%2520requires%2520complex%2520reasoning%2520over%2520heterogeneous%2520visual%2520and%2520textual%2520constraints.%2520Existing%2520approaches%2520largely%2520fall%2520into%2520two%2520paradigms%253A%2520unified%2520embedding%2520retrieval%252C%2520which%2520suffers%2520from%2520single-model%2520myopia%252C%2520and%2520heuristic%2520agentic%2520retrieval%252C%2520which%2520is%2520limited%2520by%2520suboptimal%252C%2520trial-and-error%2520orchestration.%2520To%2520this%2520end%252C%2520we%2520propose%2520OSCAR%252C%2520an%2520optimization-steered%2520agentic%2520planning%2520framework%2520for%2520composed%2520image%2520retrieval.%2520We%2520are%2520the%2520first%2520to%2520reformulate%2520agentic%2520CIR%2520from%2520a%2520heuristic%2520search%2520process%2520into%2520a%2520principled%2520trajectory%2520optimization%2520problem.%2520Instead%2520of%2520relying%2520on%2520heuristic%2520trial-and-error%2520exploration%252C%2520OSCAR%2520employs%2520a%2520novel%2520offline-online%2520paradigm.%2520In%2520the%2520offline%2520phase%252C%2520we%2520model%2520CIR%2520via%2520atomic%2520retrieval%2520selection%2520and%2520composition%2520as%2520a%2520two-stage%2520mixed-integer%2520programming%2520problem%252C%2520mathematically%2520deriving%2520optimal%2520trajectories%2520that%2520maximize%2520ground-truth%2520coverage%2520for%2520training%2520samples%2520via%2520rigorous%2520boolean%2520set%2520operations.%2520These%2520trajectories%2520are%2520then%2520stored%2520in%2520a%2520golden%2520library%2520to%2520serve%2520as%2520in-context%2520demonstrations%2520for%2520online%2520steering%2520of%2520VLM%2520planner%2520at%2520online%2520inference%2520time.%2520Extensive%2520experiments%2520on%2520three%2520public%2520benchmarks%2520and%2520a%2520private%2520industrial%2520benchmark%2520show%2520that%2520OSCAR%2520consistently%2520outperforms%2520SOTA%2520baselines.%2520Notably%252C%2520it%2520achieves%2520superior%2520performance%2520using%2520only%252010%2525%2520of%2520training%2520data%252C%2520demonstrating%2520strong%2520generalization%2520of%2520planning%2520logic%2520rather%2520than%2520dataset-specific%2520memorization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSCAR%3A%20Optimization-Steered%20Agentic%20Planning%20for%20Composed%20Image%20Retrieval&entry.906535625=Teng%20Wang%20and%20Rong%20Shan%20and%20Jianghao%20Lin%20and%20Junjie%20Wu%20and%20Tianyi%20Xu%20and%20Jianping%20Zhang%20and%20Wenteng%20Chen%20and%20Changwang%20Zhang%20and%20Zhaoxiang%20Wang%20and%20Weinan%20Zhang%20and%20Jun%20Wang&entry.1292438233=Composed%20image%20retrieval%20%28CIR%29%20requires%20complex%20reasoning%20over%20heterogeneous%20visual%20and%20textual%20constraints.%20Existing%20approaches%20largely%20fall%20into%20two%20paradigms%3A%20unified%20embedding%20retrieval%2C%20which%20suffers%20from%20single-model%20myopia%2C%20and%20heuristic%20agentic%20retrieval%2C%20which%20is%20limited%20by%20suboptimal%2C%20trial-and-error%20orchestration.%20To%20this%20end%2C%20we%20propose%20OSCAR%2C%20an%20optimization-steered%20agentic%20planning%20framework%20for%20composed%20image%20retrieval.%20We%20are%20the%20first%20to%20reformulate%20agentic%20CIR%20from%20a%20heuristic%20search%20process%20into%20a%20principled%20trajectory%20optimization%20problem.%20Instead%20of%20relying%20on%20heuristic%20trial-and-error%20exploration%2C%20OSCAR%20employs%20a%20novel%20offline-online%20paradigm.%20In%20the%20offline%20phase%2C%20we%20model%20CIR%20via%20atomic%20retrieval%20selection%20and%20composition%20as%20a%20two-stage%20mixed-integer%20programming%20problem%2C%20mathematically%20deriving%20optimal%20trajectories%20that%20maximize%20ground-truth%20coverage%20for%20training%20samples%20via%20rigorous%20boolean%20set%20operations.%20These%20trajectories%20are%20then%20stored%20in%20a%20golden%20library%20to%20serve%20as%20in-context%20demonstrations%20for%20online%20steering%20of%20VLM%20planner%20at%20online%20inference%20time.%20Extensive%20experiments%20on%20three%20public%20benchmarks%20and%20a%20private%20industrial%20benchmark%20show%20that%20OSCAR%20consistently%20outperforms%20SOTA%20baselines.%20Notably%2C%20it%20achieves%20superior%20performance%20using%20only%2010%25%20of%20training%20data%2C%20demonstrating%20strong%20generalization%20of%20planning%20logic%20rather%20than%20dataset-specific%20memorization.&entry.1838667208=http%3A//arxiv.org/abs/2602.08603v1&entry.124074799=Read"},
{"title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought", "author": "Junbo Qiao and Miaomiao Cai and Wei Li and Xudong Huang and Jie Hu and Xinghao Chen and Shaohui Lin and Hongkai Xiong", "abstract": "Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation.", "link": "http://arxiv.org/abs/2506.16796v3", "date": "2026-02-09", "relevancy": 2.7425, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealSR-R1%3A%20Reinforcement%20Learning%20for%20Real-World%20Image%20Super-Resolution%20with%20Vision-Language%20Chain-of-Thought&body=Title%3A%20RealSR-R1%3A%20Reinforcement%20Learning%20for%20Real-World%20Image%20Super-Resolution%20with%20Vision-Language%20Chain-of-Thought%0AAuthor%3A%20Junbo%20Qiao%20and%20Miaomiao%20Cai%20and%20Wei%20Li%20and%20Xudong%20Huang%20and%20Jie%20Hu%20and%20Xinghao%20Chen%20and%20Shaohui%20Lin%20and%20Hongkai%20Xiong%0AAbstract%3A%20Real-World%20Image%20Super-Resolution%20is%20one%20of%20the%20most%20challenging%20task%20in%20image%20restoration.%20However%2C%20existing%20methods%20struggle%20with%20an%20accurate%20understanding%20of%20degraded%20image%20content%2C%20leading%20to%20reconstructed%20results%20that%20are%20both%20low-fidelity%20and%20unnatural.%20We%20present%20RealSR-R1%20in%20this%20work%2C%20which%20empowers%20the%20RealSR%20models%20with%20understanding%20and%20reasoning%20capabilities.%20Inspired%20by%20the%20success%20of%20Chain%20of%20Thought%20%28CoT%29%20in%20large%20language%20models%20%28LLMs%29%2C%20we%20simulate%20the%20human%20process%20of%20handling%20degraded%20images%20and%20propose%20the%20VLCoT%20framework%2C%20which%20integrates%20vision%20and%20language%20reasoning.%20The%20framework%20aims%20to%20precisely%20restore%20image%20details%20by%20progressively%20generating%20more%20comprehensive%20text%20and%20higher-resolution%20images.%20To%20overcome%20the%20challenge%20of%20traditional%20supervised%20learning%20CoT%20failing%20to%20generalize%20to%20real-world%20scenarios%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20into%20the%20Real-World%20Image%20Super-Resolution%20task.%20We%20propose%20VLCoT-GRPO%20as%20a%20solution%2C%20which%20designs%20four%20reward%20functions%3A%20%281%29%20Format%20reward%2C%20used%20to%20standardize%20the%20CoT%20process%3B%20%282%29%20Degradation%20reward%2C%20to%20incentivize%20accurate%20degradation%20estimation%3B%20%283%29%20Understanding%20reward%2C%20to%20ensure%20the%20accuracy%20of%20the%20generated%20content%3B%20and%20%284%29%20Generation%20reward%2C%20where%20we%20propose%20using%20a%20visual%20expert%20model%20to%20evaluate%20the%20quality%20of%20generated%20images%2C%20encouraging%20the%20model%20to%20generate%20more%20realistic%20images.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20RealSR-R1%20can%20generate%20realistic%20details%20and%20accurately%20understand%20image%20content%2C%20particularly%20in%20semantically%20rich%20scenes%20or%20images%20with%20severe%20degradation.%0ALink%3A%20http%3A//arxiv.org/abs/2506.16796v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealSR-R1%253A%2520Reinforcement%2520Learning%2520for%2520Real-World%2520Image%2520Super-Resolution%2520with%2520Vision-Language%2520Chain-of-Thought%26entry.906535625%3DJunbo%2520Qiao%2520and%2520Miaomiao%2520Cai%2520and%2520Wei%2520Li%2520and%2520Xudong%2520Huang%2520and%2520Jie%2520Hu%2520and%2520Xinghao%2520Chen%2520and%2520Shaohui%2520Lin%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3DReal-World%2520Image%2520Super-Resolution%2520is%2520one%2520of%2520the%2520most%2520challenging%2520task%2520in%2520image%2520restoration.%2520However%252C%2520existing%2520methods%2520struggle%2520with%2520an%2520accurate%2520understanding%2520of%2520degraded%2520image%2520content%252C%2520leading%2520to%2520reconstructed%2520results%2520that%2520are%2520both%2520low-fidelity%2520and%2520unnatural.%2520We%2520present%2520RealSR-R1%2520in%2520this%2520work%252C%2520which%2520empowers%2520the%2520RealSR%2520models%2520with%2520understanding%2520and%2520reasoning%2520capabilities.%2520Inspired%2520by%2520the%2520success%2520of%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%2520simulate%2520the%2520human%2520process%2520of%2520handling%2520degraded%2520images%2520and%2520propose%2520the%2520VLCoT%2520framework%252C%2520which%2520integrates%2520vision%2520and%2520language%2520reasoning.%2520The%2520framework%2520aims%2520to%2520precisely%2520restore%2520image%2520details%2520by%2520progressively%2520generating%2520more%2520comprehensive%2520text%2520and%2520higher-resolution%2520images.%2520To%2520overcome%2520the%2520challenge%2520of%2520traditional%2520supervised%2520learning%2520CoT%2520failing%2520to%2520generalize%2520to%2520real-world%2520scenarios%252C%2520we%2520introduce%252C%2520for%2520the%2520first%2520time%252C%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520into%2520the%2520Real-World%2520Image%2520Super-Resolution%2520task.%2520We%2520propose%2520VLCoT-GRPO%2520as%2520a%2520solution%252C%2520which%2520designs%2520four%2520reward%2520functions%253A%2520%25281%2529%2520Format%2520reward%252C%2520used%2520to%2520standardize%2520the%2520CoT%2520process%253B%2520%25282%2529%2520Degradation%2520reward%252C%2520to%2520incentivize%2520accurate%2520degradation%2520estimation%253B%2520%25283%2529%2520Understanding%2520reward%252C%2520to%2520ensure%2520the%2520accuracy%2520of%2520the%2520generated%2520content%253B%2520and%2520%25284%2529%2520Generation%2520reward%252C%2520where%2520we%2520propose%2520using%2520a%2520visual%2520expert%2520model%2520to%2520evaluate%2520the%2520quality%2520of%2520generated%2520images%252C%2520encouraging%2520the%2520model%2520to%2520generate%2520more%2520realistic%2520images.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520RealSR-R1%2520can%2520generate%2520realistic%2520details%2520and%2520accurately%2520understand%2520image%2520content%252C%2520particularly%2520in%2520semantically%2520rich%2520scenes%2520or%2520images%2520with%2520severe%2520degradation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16796v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealSR-R1%3A%20Reinforcement%20Learning%20for%20Real-World%20Image%20Super-Resolution%20with%20Vision-Language%20Chain-of-Thought&entry.906535625=Junbo%20Qiao%20and%20Miaomiao%20Cai%20and%20Wei%20Li%20and%20Xudong%20Huang%20and%20Jie%20Hu%20and%20Xinghao%20Chen%20and%20Shaohui%20Lin%20and%20Hongkai%20Xiong&entry.1292438233=Real-World%20Image%20Super-Resolution%20is%20one%20of%20the%20most%20challenging%20task%20in%20image%20restoration.%20However%2C%20existing%20methods%20struggle%20with%20an%20accurate%20understanding%20of%20degraded%20image%20content%2C%20leading%20to%20reconstructed%20results%20that%20are%20both%20low-fidelity%20and%20unnatural.%20We%20present%20RealSR-R1%20in%20this%20work%2C%20which%20empowers%20the%20RealSR%20models%20with%20understanding%20and%20reasoning%20capabilities.%20Inspired%20by%20the%20success%20of%20Chain%20of%20Thought%20%28CoT%29%20in%20large%20language%20models%20%28LLMs%29%2C%20we%20simulate%20the%20human%20process%20of%20handling%20degraded%20images%20and%20propose%20the%20VLCoT%20framework%2C%20which%20integrates%20vision%20and%20language%20reasoning.%20The%20framework%20aims%20to%20precisely%20restore%20image%20details%20by%20progressively%20generating%20more%20comprehensive%20text%20and%20higher-resolution%20images.%20To%20overcome%20the%20challenge%20of%20traditional%20supervised%20learning%20CoT%20failing%20to%20generalize%20to%20real-world%20scenarios%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20into%20the%20Real-World%20Image%20Super-Resolution%20task.%20We%20propose%20VLCoT-GRPO%20as%20a%20solution%2C%20which%20designs%20four%20reward%20functions%3A%20%281%29%20Format%20reward%2C%20used%20to%20standardize%20the%20CoT%20process%3B%20%282%29%20Degradation%20reward%2C%20to%20incentivize%20accurate%20degradation%20estimation%3B%20%283%29%20Understanding%20reward%2C%20to%20ensure%20the%20accuracy%20of%20the%20generated%20content%3B%20and%20%284%29%20Generation%20reward%2C%20where%20we%20propose%20using%20a%20visual%20expert%20model%20to%20evaluate%20the%20quality%20of%20generated%20images%2C%20encouraging%20the%20model%20to%20generate%20more%20realistic%20images.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20RealSR-R1%20can%20generate%20realistic%20details%20and%20accurately%20understand%20image%20content%2C%20particularly%20in%20semantically%20rich%20scenes%20or%20images%20with%20severe%20degradation.&entry.1838667208=http%3A//arxiv.org/abs/2506.16796v3&entry.124074799=Read"},
{"title": "When LLaVA Meets Objects: Token Composition for Vision-Language-Models", "author": "Soumya Jahagirdar and Walid Bousselham and Anna Kukleva and Hilde Kuehne", "abstract": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.", "link": "http://arxiv.org/abs/2602.04864v2", "date": "2026-02-09", "relevancy": 2.7194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models&body=Title%3A%20When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models%0AAuthor%3A%20Soumya%20Jahagirdar%20and%20Walid%20Bousselham%20and%20Anna%20Kukleva%20and%20Hilde%20Kuehne%0AAbstract%3A%20Current%20autoregressive%20Vision%20Language%20Models%20%28VLMs%29%20usually%20rely%20on%20a%20large%20number%20of%20visual%20tokens%20to%20represent%20images%2C%20resulting%20in%20a%20need%20for%20more%20compute%20especially%20at%20inference%20time.%20To%20address%20this%20problem%2C%20we%20propose%20Mask-LLaVA%2C%20a%20framework%20that%20leverages%20different%20levels%20of%20visual%20features%20to%20create%20a%20compact%20yet%20information-rich%20visual%20representation%20for%20autoregressive%20VLMs.%20Namely%2C%20we%20combine%20mask-based%20object%20representations%20together%20with%20global%20tokens%20and%20local%20patch%20tokens.%20While%20all%20tokens%20are%20used%20during%20training%2C%20it%20shows%20that%20the%20resulting%20model%20can%20flexibly%20drop%20especially%20the%20number%20of%20mask-based%20object-tokens%20at%20test%20time%2C%20allowing%20to%20adapt%20the%20number%20of%20tokens%20during%20inference%20without%20the%20need%20to%20retrain%20the%20model%20and%20without%20a%20significant%20drop%20in%20performance.%20We%20evaluate%20the%20proposed%20approach%20on%20a%20suite%20of%20standard%20benchmarks%20showing%20results%20competitive%20to%20current%20token%20efficient%20methods%20and%20comparable%20to%20the%20original%20LLaVA%20baseline%20using%20only%20a%20fraction%20of%20visual%20tokens.%20Our%20analysis%20demonstrates%20that%20combining%20multi-level%20features%20enables%20efficient%20learning%20with%20fewer%20tokens%20while%20allowing%20dynamic%20token%20selection%20at%20test%20time%20for%20good%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LLaVA%2520Meets%2520Objects%253A%2520Token%2520Composition%2520for%2520Vision-Language-Models%26entry.906535625%3DSoumya%2520Jahagirdar%2520and%2520Walid%2520Bousselham%2520and%2520Anna%2520Kukleva%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3DCurrent%2520autoregressive%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520usually%2520rely%2520on%2520a%2520large%2520number%2520of%2520visual%2520tokens%2520to%2520represent%2520images%252C%2520resulting%2520in%2520a%2520need%2520for%2520more%2520compute%2520especially%2520at%2520inference%2520time.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Mask-LLaVA%252C%2520a%2520framework%2520that%2520leverages%2520different%2520levels%2520of%2520visual%2520features%2520to%2520create%2520a%2520compact%2520yet%2520information-rich%2520visual%2520representation%2520for%2520autoregressive%2520VLMs.%2520Namely%252C%2520we%2520combine%2520mask-based%2520object%2520representations%2520together%2520with%2520global%2520tokens%2520and%2520local%2520patch%2520tokens.%2520While%2520all%2520tokens%2520are%2520used%2520during%2520training%252C%2520it%2520shows%2520that%2520the%2520resulting%2520model%2520can%2520flexibly%2520drop%2520especially%2520the%2520number%2520of%2520mask-based%2520object-tokens%2520at%2520test%2520time%252C%2520allowing%2520to%2520adapt%2520the%2520number%2520of%2520tokens%2520during%2520inference%2520without%2520the%2520need%2520to%2520retrain%2520the%2520model%2520and%2520without%2520a%2520significant%2520drop%2520in%2520performance.%2520We%2520evaluate%2520the%2520proposed%2520approach%2520on%2520a%2520suite%2520of%2520standard%2520benchmarks%2520showing%2520results%2520competitive%2520to%2520current%2520token%2520efficient%2520methods%2520and%2520comparable%2520to%2520the%2520original%2520LLaVA%2520baseline%2520using%2520only%2520a%2520fraction%2520of%2520visual%2520tokens.%2520Our%2520analysis%2520demonstrates%2520that%2520combining%2520multi-level%2520features%2520enables%2520efficient%2520learning%2520with%2520fewer%2520tokens%2520while%2520allowing%2520dynamic%2520token%2520selection%2520at%2520test%2520time%2520for%2520good%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models&entry.906535625=Soumya%20Jahagirdar%20and%20Walid%20Bousselham%20and%20Anna%20Kukleva%20and%20Hilde%20Kuehne&entry.1292438233=Current%20autoregressive%20Vision%20Language%20Models%20%28VLMs%29%20usually%20rely%20on%20a%20large%20number%20of%20visual%20tokens%20to%20represent%20images%2C%20resulting%20in%20a%20need%20for%20more%20compute%20especially%20at%20inference%20time.%20To%20address%20this%20problem%2C%20we%20propose%20Mask-LLaVA%2C%20a%20framework%20that%20leverages%20different%20levels%20of%20visual%20features%20to%20create%20a%20compact%20yet%20information-rich%20visual%20representation%20for%20autoregressive%20VLMs.%20Namely%2C%20we%20combine%20mask-based%20object%20representations%20together%20with%20global%20tokens%20and%20local%20patch%20tokens.%20While%20all%20tokens%20are%20used%20during%20training%2C%20it%20shows%20that%20the%20resulting%20model%20can%20flexibly%20drop%20especially%20the%20number%20of%20mask-based%20object-tokens%20at%20test%20time%2C%20allowing%20to%20adapt%20the%20number%20of%20tokens%20during%20inference%20without%20the%20need%20to%20retrain%20the%20model%20and%20without%20a%20significant%20drop%20in%20performance.%20We%20evaluate%20the%20proposed%20approach%20on%20a%20suite%20of%20standard%20benchmarks%20showing%20results%20competitive%20to%20current%20token%20efficient%20methods%20and%20comparable%20to%20the%20original%20LLaVA%20baseline%20using%20only%20a%20fraction%20of%20visual%20tokens.%20Our%20analysis%20demonstrates%20that%20combining%20multi-level%20features%20enables%20efficient%20learning%20with%20fewer%20tokens%20while%20allowing%20dynamic%20token%20selection%20at%20test%20time%20for%20good%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2602.04864v2&entry.124074799=Read"},
{"title": "Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse", "author": "Shaojie Wang and Jinghui Wang and Yinghan Cui and Xuxing Chen and Chao Wang and Liang Huang and Xiaojiang Zhang and Junyi Peng and Li Wan and Haotian Zhang and Bin Chen", "abstract": "Agentic large language model (LLM) training often involves multi-turn interaction trajectories that branch into multiple execution paths due to concurrent tool use, think-mode, sub-agent, context management and other runtime designs. As a result, the token produced by a single task naturally forms a tree-structured token trajectory with shared prefixes, rather than a linear sequence. Existing training pipelines linearize such trajectories and treat each branch independently, leading to substantial redundant computation in both forward and backward passes. To eliminate such redundancy, we introduce Tree Training, an efficient training framework for tree-structured trajectories. Its core component, Gradient Restoration, enables correct gradient aggregation across shared prefixes, allowing each prefix to be computed exactly once while remaining mathematically equivalent to independent training on all branches. To support large trajectory trees in practice, we redesign the training engine to natively ingest tree-structured data and propose Tree Packing, a memory-efficient partitioning strategy that preserves high prefix reuse. Experiments conducted on dense and MOE models of real-world agentic trajectories show 6.2x training speedup for both supervised fine-tuning and the model update phase in reinforcement learning.", "link": "http://arxiv.org/abs/2511.00413v3", "date": "2026-02-09", "relevancy": 2.6749, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20Training%3A%20Accelerating%20Agentic%20LLMs%20Training%20via%20Shared%20Prefix%20Reuse&body=Title%3A%20Tree%20Training%3A%20Accelerating%20Agentic%20LLMs%20Training%20via%20Shared%20Prefix%20Reuse%0AAuthor%3A%20Shaojie%20Wang%20and%20Jinghui%20Wang%20and%20Yinghan%20Cui%20and%20Xuxing%20Chen%20and%20Chao%20Wang%20and%20Liang%20Huang%20and%20Xiaojiang%20Zhang%20and%20Junyi%20Peng%20and%20Li%20Wan%20and%20Haotian%20Zhang%20and%20Bin%20Chen%0AAbstract%3A%20Agentic%20large%20language%20model%20%28LLM%29%20training%20often%20involves%20multi-turn%20interaction%20trajectories%20that%20branch%20into%20multiple%20execution%20paths%20due%20to%20concurrent%20tool%20use%2C%20think-mode%2C%20sub-agent%2C%20context%20management%20and%20other%20runtime%20designs.%20As%20a%20result%2C%20the%20token%20produced%20by%20a%20single%20task%20naturally%20forms%20a%20tree-structured%20token%20trajectory%20with%20shared%20prefixes%2C%20rather%20than%20a%20linear%20sequence.%20Existing%20training%20pipelines%20linearize%20such%20trajectories%20and%20treat%20each%20branch%20independently%2C%20leading%20to%20substantial%20redundant%20computation%20in%20both%20forward%20and%20backward%20passes.%20To%20eliminate%20such%20redundancy%2C%20we%20introduce%20Tree%20Training%2C%20an%20efficient%20training%20framework%20for%20tree-structured%20trajectories.%20Its%20core%20component%2C%20Gradient%20Restoration%2C%20enables%20correct%20gradient%20aggregation%20across%20shared%20prefixes%2C%20allowing%20each%20prefix%20to%20be%20computed%20exactly%20once%20while%20remaining%20mathematically%20equivalent%20to%20independent%20training%20on%20all%20branches.%20To%20support%20large%20trajectory%20trees%20in%20practice%2C%20we%20redesign%20the%20training%20engine%20to%20natively%20ingest%20tree-structured%20data%20and%20propose%20Tree%20Packing%2C%20a%20memory-efficient%20partitioning%20strategy%20that%20preserves%20high%20prefix%20reuse.%20Experiments%20conducted%20on%20dense%20and%20MOE%20models%20of%20real-world%20agentic%20trajectories%20show%206.2x%20training%20speedup%20for%20both%20supervised%20fine-tuning%20and%20the%20model%20update%20phase%20in%20reinforcement%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00413v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520Training%253A%2520Accelerating%2520Agentic%2520LLMs%2520Training%2520via%2520Shared%2520Prefix%2520Reuse%26entry.906535625%3DShaojie%2520Wang%2520and%2520Jinghui%2520Wang%2520and%2520Yinghan%2520Cui%2520and%2520Xuxing%2520Chen%2520and%2520Chao%2520Wang%2520and%2520Liang%2520Huang%2520and%2520Xiaojiang%2520Zhang%2520and%2520Junyi%2520Peng%2520and%2520Li%2520Wan%2520and%2520Haotian%2520Zhang%2520and%2520Bin%2520Chen%26entry.1292438233%3DAgentic%2520large%2520language%2520model%2520%2528LLM%2529%2520training%2520often%2520involves%2520multi-turn%2520interaction%2520trajectories%2520that%2520branch%2520into%2520multiple%2520execution%2520paths%2520due%2520to%2520concurrent%2520tool%2520use%252C%2520think-mode%252C%2520sub-agent%252C%2520context%2520management%2520and%2520other%2520runtime%2520designs.%2520As%2520a%2520result%252C%2520the%2520token%2520produced%2520by%2520a%2520single%2520task%2520naturally%2520forms%2520a%2520tree-structured%2520token%2520trajectory%2520with%2520shared%2520prefixes%252C%2520rather%2520than%2520a%2520linear%2520sequence.%2520Existing%2520training%2520pipelines%2520linearize%2520such%2520trajectories%2520and%2520treat%2520each%2520branch%2520independently%252C%2520leading%2520to%2520substantial%2520redundant%2520computation%2520in%2520both%2520forward%2520and%2520backward%2520passes.%2520To%2520eliminate%2520such%2520redundancy%252C%2520we%2520introduce%2520Tree%2520Training%252C%2520an%2520efficient%2520training%2520framework%2520for%2520tree-structured%2520trajectories.%2520Its%2520core%2520component%252C%2520Gradient%2520Restoration%252C%2520enables%2520correct%2520gradient%2520aggregation%2520across%2520shared%2520prefixes%252C%2520allowing%2520each%2520prefix%2520to%2520be%2520computed%2520exactly%2520once%2520while%2520remaining%2520mathematically%2520equivalent%2520to%2520independent%2520training%2520on%2520all%2520branches.%2520To%2520support%2520large%2520trajectory%2520trees%2520in%2520practice%252C%2520we%2520redesign%2520the%2520training%2520engine%2520to%2520natively%2520ingest%2520tree-structured%2520data%2520and%2520propose%2520Tree%2520Packing%252C%2520a%2520memory-efficient%2520partitioning%2520strategy%2520that%2520preserves%2520high%2520prefix%2520reuse.%2520Experiments%2520conducted%2520on%2520dense%2520and%2520MOE%2520models%2520of%2520real-world%2520agentic%2520trajectories%2520show%25206.2x%2520training%2520speedup%2520for%2520both%2520supervised%2520fine-tuning%2520and%2520the%2520model%2520update%2520phase%2520in%2520reinforcement%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00413v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20Training%3A%20Accelerating%20Agentic%20LLMs%20Training%20via%20Shared%20Prefix%20Reuse&entry.906535625=Shaojie%20Wang%20and%20Jinghui%20Wang%20and%20Yinghan%20Cui%20and%20Xuxing%20Chen%20and%20Chao%20Wang%20and%20Liang%20Huang%20and%20Xiaojiang%20Zhang%20and%20Junyi%20Peng%20and%20Li%20Wan%20and%20Haotian%20Zhang%20and%20Bin%20Chen&entry.1292438233=Agentic%20large%20language%20model%20%28LLM%29%20training%20often%20involves%20multi-turn%20interaction%20trajectories%20that%20branch%20into%20multiple%20execution%20paths%20due%20to%20concurrent%20tool%20use%2C%20think-mode%2C%20sub-agent%2C%20context%20management%20and%20other%20runtime%20designs.%20As%20a%20result%2C%20the%20token%20produced%20by%20a%20single%20task%20naturally%20forms%20a%20tree-structured%20token%20trajectory%20with%20shared%20prefixes%2C%20rather%20than%20a%20linear%20sequence.%20Existing%20training%20pipelines%20linearize%20such%20trajectories%20and%20treat%20each%20branch%20independently%2C%20leading%20to%20substantial%20redundant%20computation%20in%20both%20forward%20and%20backward%20passes.%20To%20eliminate%20such%20redundancy%2C%20we%20introduce%20Tree%20Training%2C%20an%20efficient%20training%20framework%20for%20tree-structured%20trajectories.%20Its%20core%20component%2C%20Gradient%20Restoration%2C%20enables%20correct%20gradient%20aggregation%20across%20shared%20prefixes%2C%20allowing%20each%20prefix%20to%20be%20computed%20exactly%20once%20while%20remaining%20mathematically%20equivalent%20to%20independent%20training%20on%20all%20branches.%20To%20support%20large%20trajectory%20trees%20in%20practice%2C%20we%20redesign%20the%20training%20engine%20to%20natively%20ingest%20tree-structured%20data%20and%20propose%20Tree%20Packing%2C%20a%20memory-efficient%20partitioning%20strategy%20that%20preserves%20high%20prefix%20reuse.%20Experiments%20conducted%20on%20dense%20and%20MOE%20models%20of%20real-world%20agentic%20trajectories%20show%206.2x%20training%20speedup%20for%20both%20supervised%20fine-tuning%20and%20the%20model%20update%20phase%20in%20reinforcement%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2511.00413v3&entry.124074799=Read"},
{"title": "Restricted Receptive Fields for Face Verification", "author": "Kagan Ozturk and Aman Bhatta and Haiyu Wu and Patrick Flynn and Kevin W. Bowyer", "abstract": "Understanding how deep neural networks make decisions is crucial for analyzing their behavior and diagnosing failure cases. In computer vision, a common approach to improve interpretability is to assign importance to individual pixels using post-hoc methods. Although they are widely used to explain black-box models, their fidelity to the model's actual reasoning is uncertain due to the lack of reliable evaluation metrics. This limitation motivates an alternative approach, which is to design models whose decision processes are inherently interpretable. To this end, we propose a face similarity metric that breaks down global similarity into contributions from restricted receptive fields. Our method defines the similarity between two face images as the sum of patch-level similarity scores, providing a locally additive explanation without relying on post-hoc analysis. We show that the proposed approach achieves competitive verification performance even with patches as small as 28x28 within 112x112 face images, and surpasses state-of-the-art methods when using 56x56 patches.", "link": "http://arxiv.org/abs/2510.10753v2", "date": "2026-02-09", "relevancy": 2.6726, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restricted%20Receptive%20Fields%20for%20Face%20Verification&body=Title%3A%20Restricted%20Receptive%20Fields%20for%20Face%20Verification%0AAuthor%3A%20Kagan%20Ozturk%20and%20Aman%20Bhatta%20and%20Haiyu%20Wu%20and%20Patrick%20Flynn%20and%20Kevin%20W.%20Bowyer%0AAbstract%3A%20Understanding%20how%20deep%20neural%20networks%20make%20decisions%20is%20crucial%20for%20analyzing%20their%20behavior%20and%20diagnosing%20failure%20cases.%20In%20computer%20vision%2C%20a%20common%20approach%20to%20improve%20interpretability%20is%20to%20assign%20importance%20to%20individual%20pixels%20using%20post-hoc%20methods.%20Although%20they%20are%20widely%20used%20to%20explain%20black-box%20models%2C%20their%20fidelity%20to%20the%20model%27s%20actual%20reasoning%20is%20uncertain%20due%20to%20the%20lack%20of%20reliable%20evaluation%20metrics.%20This%20limitation%20motivates%20an%20alternative%20approach%2C%20which%20is%20to%20design%20models%20whose%20decision%20processes%20are%20inherently%20interpretable.%20To%20this%20end%2C%20we%20propose%20a%20face%20similarity%20metric%20that%20breaks%20down%20global%20similarity%20into%20contributions%20from%20restricted%20receptive%20fields.%20Our%20method%20defines%20the%20similarity%20between%20two%20face%20images%20as%20the%20sum%20of%20patch-level%20similarity%20scores%2C%20providing%20a%20locally%20additive%20explanation%20without%20relying%20on%20post-hoc%20analysis.%20We%20show%20that%20the%20proposed%20approach%20achieves%20competitive%20verification%20performance%20even%20with%20patches%20as%20small%20as%2028x28%20within%20112x112%20face%20images%2C%20and%20surpasses%20state-of-the-art%20methods%20when%20using%2056x56%20patches.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestricted%2520Receptive%2520Fields%2520for%2520Face%2520Verification%26entry.906535625%3DKagan%2520Ozturk%2520and%2520Aman%2520Bhatta%2520and%2520Haiyu%2520Wu%2520and%2520Patrick%2520Flynn%2520and%2520Kevin%2520W.%2520Bowyer%26entry.1292438233%3DUnderstanding%2520how%2520deep%2520neural%2520networks%2520make%2520decisions%2520is%2520crucial%2520for%2520analyzing%2520their%2520behavior%2520and%2520diagnosing%2520failure%2520cases.%2520In%2520computer%2520vision%252C%2520a%2520common%2520approach%2520to%2520improve%2520interpretability%2520is%2520to%2520assign%2520importance%2520to%2520individual%2520pixels%2520using%2520post-hoc%2520methods.%2520Although%2520they%2520are%2520widely%2520used%2520to%2520explain%2520black-box%2520models%252C%2520their%2520fidelity%2520to%2520the%2520model%2527s%2520actual%2520reasoning%2520is%2520uncertain%2520due%2520to%2520the%2520lack%2520of%2520reliable%2520evaluation%2520metrics.%2520This%2520limitation%2520motivates%2520an%2520alternative%2520approach%252C%2520which%2520is%2520to%2520design%2520models%2520whose%2520decision%2520processes%2520are%2520inherently%2520interpretable.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520face%2520similarity%2520metric%2520that%2520breaks%2520down%2520global%2520similarity%2520into%2520contributions%2520from%2520restricted%2520receptive%2520fields.%2520Our%2520method%2520defines%2520the%2520similarity%2520between%2520two%2520face%2520images%2520as%2520the%2520sum%2520of%2520patch-level%2520similarity%2520scores%252C%2520providing%2520a%2520locally%2520additive%2520explanation%2520without%2520relying%2520on%2520post-hoc%2520analysis.%2520We%2520show%2520that%2520the%2520proposed%2520approach%2520achieves%2520competitive%2520verification%2520performance%2520even%2520with%2520patches%2520as%2520small%2520as%252028x28%2520within%2520112x112%2520face%2520images%252C%2520and%2520surpasses%2520state-of-the-art%2520methods%2520when%2520using%252056x56%2520patches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restricted%20Receptive%20Fields%20for%20Face%20Verification&entry.906535625=Kagan%20Ozturk%20and%20Aman%20Bhatta%20and%20Haiyu%20Wu%20and%20Patrick%20Flynn%20and%20Kevin%20W.%20Bowyer&entry.1292438233=Understanding%20how%20deep%20neural%20networks%20make%20decisions%20is%20crucial%20for%20analyzing%20their%20behavior%20and%20diagnosing%20failure%20cases.%20In%20computer%20vision%2C%20a%20common%20approach%20to%20improve%20interpretability%20is%20to%20assign%20importance%20to%20individual%20pixels%20using%20post-hoc%20methods.%20Although%20they%20are%20widely%20used%20to%20explain%20black-box%20models%2C%20their%20fidelity%20to%20the%20model%27s%20actual%20reasoning%20is%20uncertain%20due%20to%20the%20lack%20of%20reliable%20evaluation%20metrics.%20This%20limitation%20motivates%20an%20alternative%20approach%2C%20which%20is%20to%20design%20models%20whose%20decision%20processes%20are%20inherently%20interpretable.%20To%20this%20end%2C%20we%20propose%20a%20face%20similarity%20metric%20that%20breaks%20down%20global%20similarity%20into%20contributions%20from%20restricted%20receptive%20fields.%20Our%20method%20defines%20the%20similarity%20between%20two%20face%20images%20as%20the%20sum%20of%20patch-level%20similarity%20scores%2C%20providing%20a%20locally%20additive%20explanation%20without%20relying%20on%20post-hoc%20analysis.%20We%20show%20that%20the%20proposed%20approach%20achieves%20competitive%20verification%20performance%20even%20with%20patches%20as%20small%20as%2028x28%20within%20112x112%20face%20images%2C%20and%20surpasses%20state-of-the-art%20methods%20when%20using%2056x56%20patches.&entry.1838667208=http%3A//arxiv.org/abs/2510.10753v2&entry.124074799=Read"},
{"title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views", "author": "Daniil Reutsky and Daniil Vladimirov and Yasin Mamedov and Georgy Perevozchikov and Nancy Mehta and Egor Ershov and Radu Timofte", "abstract": "Hyperspectral reconstruction (HSR) from RGB images is a fundamentally ill-posed problem due to severe spectral information loss. Existing approaches typically rely on a single RGB image, limiting reconstruction accuracy. In this work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR) framework that leverages a triple-camera smartphone system, where two lenses are equipped with carefully selected spectral filters. Our configuration, grounded in theoretical and empirical analysis, enables richer and more diverse spectral observations than conventional single-camera setups. To support this new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising aligned images from three smartphone cameras and a hyperspectral reference camera across diverse scenes. We show that the proposed HSR model achieves consistent improvements over existing methods on the newly proposed benchmark. In a nutshell, our setup allows 30% towards more accurately estimated spectra compared to an ordinary RGB camera. Our findings suggest that multi-view spectral filtering with commodity hardware can unlock more accurate and practical hyperspectral imaging solutions.", "link": "http://arxiv.org/abs/2507.01835v3", "date": "2026-02-09", "relevancy": 2.6428, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.525}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modulate%20and%20Reconstruct%3A%20Learning%20Hyperspectral%20Imaging%20from%20Misaligned%20Smartphone%20Views&body=Title%3A%20Modulate%20and%20Reconstruct%3A%20Learning%20Hyperspectral%20Imaging%20from%20Misaligned%20Smartphone%20Views%0AAuthor%3A%20Daniil%20Reutsky%20and%20Daniil%20Vladimirov%20and%20Yasin%20Mamedov%20and%20Georgy%20Perevozchikov%20and%20Nancy%20Mehta%20and%20Egor%20Ershov%20and%20Radu%20Timofte%0AAbstract%3A%20Hyperspectral%20reconstruction%20%28HSR%29%20from%20RGB%20images%20is%20a%20fundamentally%20ill-posed%20problem%20due%20to%20severe%20spectral%20information%20loss.%20Existing%20approaches%20typically%20rely%20on%20a%20single%20RGB%20image%2C%20limiting%20reconstruction%20accuracy.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multi-image-to-hyperspectral%20reconstruction%20%28MI-HSR%29%20framework%20that%20leverages%20a%20triple-camera%20smartphone%20system%2C%20where%20two%20lenses%20are%20equipped%20with%20carefully%20selected%20spectral%20filters.%20Our%20configuration%2C%20grounded%20in%20theoretical%20and%20empirical%20analysis%2C%20enables%20richer%20and%20more%20diverse%20spectral%20observations%20than%20conventional%20single-camera%20setups.%20To%20support%20this%20new%20paradigm%2C%20we%20introduce%20Doomer%2C%20the%20first%20dataset%20for%20MI-HSR%2C%20comprising%20aligned%20images%20from%20three%20smartphone%20cameras%20and%20a%20hyperspectral%20reference%20camera%20across%20diverse%20scenes.%20We%20show%20that%20the%20proposed%20HSR%20model%20achieves%20consistent%20improvements%20over%20existing%20methods%20on%20the%20newly%20proposed%20benchmark.%20In%20a%20nutshell%2C%20our%20setup%20allows%2030%25%20towards%20more%20accurately%20estimated%20spectra%20compared%20to%20an%20ordinary%20RGB%20camera.%20Our%20findings%20suggest%20that%20multi-view%20spectral%20filtering%20with%20commodity%20hardware%20can%20unlock%20more%20accurate%20and%20practical%20hyperspectral%20imaging%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01835v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModulate%2520and%2520Reconstruct%253A%2520Learning%2520Hyperspectral%2520Imaging%2520from%2520Misaligned%2520Smartphone%2520Views%26entry.906535625%3DDaniil%2520Reutsky%2520and%2520Daniil%2520Vladimirov%2520and%2520Yasin%2520Mamedov%2520and%2520Georgy%2520Perevozchikov%2520and%2520Nancy%2520Mehta%2520and%2520Egor%2520Ershov%2520and%2520Radu%2520Timofte%26entry.1292438233%3DHyperspectral%2520reconstruction%2520%2528HSR%2529%2520from%2520RGB%2520images%2520is%2520a%2520fundamentally%2520ill-posed%2520problem%2520due%2520to%2520severe%2520spectral%2520information%2520loss.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520a%2520single%2520RGB%2520image%252C%2520limiting%2520reconstruction%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520multi-image-to-hyperspectral%2520reconstruction%2520%2528MI-HSR%2529%2520framework%2520that%2520leverages%2520a%2520triple-camera%2520smartphone%2520system%252C%2520where%2520two%2520lenses%2520are%2520equipped%2520with%2520carefully%2520selected%2520spectral%2520filters.%2520Our%2520configuration%252C%2520grounded%2520in%2520theoretical%2520and%2520empirical%2520analysis%252C%2520enables%2520richer%2520and%2520more%2520diverse%2520spectral%2520observations%2520than%2520conventional%2520single-camera%2520setups.%2520To%2520support%2520this%2520new%2520paradigm%252C%2520we%2520introduce%2520Doomer%252C%2520the%2520first%2520dataset%2520for%2520MI-HSR%252C%2520comprising%2520aligned%2520images%2520from%2520three%2520smartphone%2520cameras%2520and%2520a%2520hyperspectral%2520reference%2520camera%2520across%2520diverse%2520scenes.%2520We%2520show%2520that%2520the%2520proposed%2520HSR%2520model%2520achieves%2520consistent%2520improvements%2520over%2520existing%2520methods%2520on%2520the%2520newly%2520proposed%2520benchmark.%2520In%2520a%2520nutshell%252C%2520our%2520setup%2520allows%252030%2525%2520towards%2520more%2520accurately%2520estimated%2520spectra%2520compared%2520to%2520an%2520ordinary%2520RGB%2520camera.%2520Our%2520findings%2520suggest%2520that%2520multi-view%2520spectral%2520filtering%2520with%2520commodity%2520hardware%2520can%2520unlock%2520more%2520accurate%2520and%2520practical%2520hyperspectral%2520imaging%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01835v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modulate%20and%20Reconstruct%3A%20Learning%20Hyperspectral%20Imaging%20from%20Misaligned%20Smartphone%20Views&entry.906535625=Daniil%20Reutsky%20and%20Daniil%20Vladimirov%20and%20Yasin%20Mamedov%20and%20Georgy%20Perevozchikov%20and%20Nancy%20Mehta%20and%20Egor%20Ershov%20and%20Radu%20Timofte&entry.1292438233=Hyperspectral%20reconstruction%20%28HSR%29%20from%20RGB%20images%20is%20a%20fundamentally%20ill-posed%20problem%20due%20to%20severe%20spectral%20information%20loss.%20Existing%20approaches%20typically%20rely%20on%20a%20single%20RGB%20image%2C%20limiting%20reconstruction%20accuracy.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multi-image-to-hyperspectral%20reconstruction%20%28MI-HSR%29%20framework%20that%20leverages%20a%20triple-camera%20smartphone%20system%2C%20where%20two%20lenses%20are%20equipped%20with%20carefully%20selected%20spectral%20filters.%20Our%20configuration%2C%20grounded%20in%20theoretical%20and%20empirical%20analysis%2C%20enables%20richer%20and%20more%20diverse%20spectral%20observations%20than%20conventional%20single-camera%20setups.%20To%20support%20this%20new%20paradigm%2C%20we%20introduce%20Doomer%2C%20the%20first%20dataset%20for%20MI-HSR%2C%20comprising%20aligned%20images%20from%20three%20smartphone%20cameras%20and%20a%20hyperspectral%20reference%20camera%20across%20diverse%20scenes.%20We%20show%20that%20the%20proposed%20HSR%20model%20achieves%20consistent%20improvements%20over%20existing%20methods%20on%20the%20newly%20proposed%20benchmark.%20In%20a%20nutshell%2C%20our%20setup%20allows%2030%25%20towards%20more%20accurately%20estimated%20spectra%20compared%20to%20an%20ordinary%20RGB%20camera.%20Our%20findings%20suggest%20that%20multi-view%20spectral%20filtering%20with%20commodity%20hardware%20can%20unlock%20more%20accurate%20and%20practical%20hyperspectral%20imaging%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2507.01835v3&entry.124074799=Read"},
{"title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation", "author": "Shanshan Wang and Ziying Feng and Xiaozheng Shen and Xun Yang and Pichao Wang and Zhenwei He and Xingyi Zhang", "abstract": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA", "link": "http://arxiv.org/abs/2602.08730v1", "date": "2026-02-09", "relevancy": 2.6307, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5622}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Confusion%20Loop%3A%20CLIP-Guided%20Alignment%20for%20Source-Free%20Domain%20Adaptation&body=Title%3A%20Closing%20the%20Confusion%20Loop%3A%20CLIP-Guided%20Alignment%20for%20Source-Free%20Domain%20Adaptation%0AAuthor%3A%20Shanshan%20Wang%20and%20Ziying%20Feng%20and%20Xiaozheng%20Shen%20and%20Xun%20Yang%20and%20Pichao%20Wang%20and%20Zhenwei%20He%20and%20Xingyi%20Zhang%0AAbstract%3A%20Source-Free%20Domain%20Adaptation%20%28SFDA%29%20tackles%20the%20problem%20of%20adapting%20a%20pre-trained%20source%20model%20to%20an%20unlabeled%20target%20domain%20without%20accessing%20any%20source%20data%2C%20which%20is%20quite%20suitable%20for%20the%20field%20of%20data%20security.%20Although%20recent%20advances%20have%20shown%20that%20pseudo-labeling%20strategies%20can%20be%20effective%2C%20they%20often%20fail%20in%20fine-grained%20scenarios%20due%20to%20subtle%20inter-class%20similarities.%20A%20critical%20but%20underexplored%20issue%20is%20the%20presence%20of%20asymmetric%20and%20dynamic%20class%20confusion%2C%20where%20visually%20similar%20classes%20are%20unequally%20and%20inconsistently%20misclassified%20by%20the%20source%20model.%20Existing%20methods%20typically%20ignore%20such%20confusion%20patterns%2C%20leading%20to%20noisy%20pseudo-labels%20and%20poor%20target%20discrimination.%20To%20address%20this%2C%20we%20propose%20CLIP-Guided%20Alignment%28CGA%29%2C%20a%20novel%20framework%20that%20explicitly%20models%20and%20mitigates%20class%20confusion%20in%20SFDA.%20Generally%2C%20our%20method%20consists%20of%20three%20parts%3A%20%281%29%20MCA%3A%20detects%20first%20directional%20confusion%20pairs%20by%20analyzing%20the%20predictions%20of%20the%20source%20model%20in%20the%20target%20domain%3B%20%282%29%20MCC%3A%20leverages%20CLIP%20to%20construct%20confusion-aware%20textual%20prompts%20%28e.g.%20a%20truck%20that%20looks%20like%20a%20bus%29%2C%20enabling%20more%20context-sensitive%20pseudo-labeling%3B%20and%20%283%29%20FAM%3A%20builds%20confusion-guided%20feature%20banks%20for%20both%20CLIP%20and%20the%20source%20model%20and%20aligns%20them%20using%20contrastive%20learning%20to%20reduce%20ambiguity%20in%20the%20representation%20space.%20Extensive%20experiments%20on%20various%20datasets%20demonstrate%20that%20CGA%20consistently%20outperforms%20state-of-the-art%20SFDA%20methods%2C%20with%20especially%20notable%20gains%20in%20confusion-prone%20and%20fine-grained%20scenarios.%20Our%20results%20highlight%20the%20importance%20of%20explicitly%20modeling%20inter-class%20confusion%20for%20effective%20source-free%20adaptation.%20Our%20code%20can%20be%20find%20at%20https%3A//github.com/soloiro/CGA%0ALink%3A%20http%3A//arxiv.org/abs/2602.08730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Confusion%2520Loop%253A%2520CLIP-Guided%2520Alignment%2520for%2520Source-Free%2520Domain%2520Adaptation%26entry.906535625%3DShanshan%2520Wang%2520and%2520Ziying%2520Feng%2520and%2520Xiaozheng%2520Shen%2520and%2520Xun%2520Yang%2520and%2520Pichao%2520Wang%2520and%2520Zhenwei%2520He%2520and%2520Xingyi%2520Zhang%26entry.1292438233%3DSource-Free%2520Domain%2520Adaptation%2520%2528SFDA%2529%2520tackles%2520the%2520problem%2520of%2520adapting%2520a%2520pre-trained%2520source%2520model%2520to%2520an%2520unlabeled%2520target%2520domain%2520without%2520accessing%2520any%2520source%2520data%252C%2520which%2520is%2520quite%2520suitable%2520for%2520the%2520field%2520of%2520data%2520security.%2520Although%2520recent%2520advances%2520have%2520shown%2520that%2520pseudo-labeling%2520strategies%2520can%2520be%2520effective%252C%2520they%2520often%2520fail%2520in%2520fine-grained%2520scenarios%2520due%2520to%2520subtle%2520inter-class%2520similarities.%2520A%2520critical%2520but%2520underexplored%2520issue%2520is%2520the%2520presence%2520of%2520asymmetric%2520and%2520dynamic%2520class%2520confusion%252C%2520where%2520visually%2520similar%2520classes%2520are%2520unequally%2520and%2520inconsistently%2520misclassified%2520by%2520the%2520source%2520model.%2520Existing%2520methods%2520typically%2520ignore%2520such%2520confusion%2520patterns%252C%2520leading%2520to%2520noisy%2520pseudo-labels%2520and%2520poor%2520target%2520discrimination.%2520To%2520address%2520this%252C%2520we%2520propose%2520CLIP-Guided%2520Alignment%2528CGA%2529%252C%2520a%2520novel%2520framework%2520that%2520explicitly%2520models%2520and%2520mitigates%2520class%2520confusion%2520in%2520SFDA.%2520Generally%252C%2520our%2520method%2520consists%2520of%2520three%2520parts%253A%2520%25281%2529%2520MCA%253A%2520detects%2520first%2520directional%2520confusion%2520pairs%2520by%2520analyzing%2520the%2520predictions%2520of%2520the%2520source%2520model%2520in%2520the%2520target%2520domain%253B%2520%25282%2529%2520MCC%253A%2520leverages%2520CLIP%2520to%2520construct%2520confusion-aware%2520textual%2520prompts%2520%2528e.g.%2520a%2520truck%2520that%2520looks%2520like%2520a%2520bus%2529%252C%2520enabling%2520more%2520context-sensitive%2520pseudo-labeling%253B%2520and%2520%25283%2529%2520FAM%253A%2520builds%2520confusion-guided%2520feature%2520banks%2520for%2520both%2520CLIP%2520and%2520the%2520source%2520model%2520and%2520aligns%2520them%2520using%2520contrastive%2520learning%2520to%2520reduce%2520ambiguity%2520in%2520the%2520representation%2520space.%2520Extensive%2520experiments%2520on%2520various%2520datasets%2520demonstrate%2520that%2520CGA%2520consistently%2520outperforms%2520state-of-the-art%2520SFDA%2520methods%252C%2520with%2520especially%2520notable%2520gains%2520in%2520confusion-prone%2520and%2520fine-grained%2520scenarios.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520explicitly%2520modeling%2520inter-class%2520confusion%2520for%2520effective%2520source-free%2520adaptation.%2520Our%2520code%2520can%2520be%2520find%2520at%2520https%253A//github.com/soloiro/CGA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Confusion%20Loop%3A%20CLIP-Guided%20Alignment%20for%20Source-Free%20Domain%20Adaptation&entry.906535625=Shanshan%20Wang%20and%20Ziying%20Feng%20and%20Xiaozheng%20Shen%20and%20Xun%20Yang%20and%20Pichao%20Wang%20and%20Zhenwei%20He%20and%20Xingyi%20Zhang&entry.1292438233=Source-Free%20Domain%20Adaptation%20%28SFDA%29%20tackles%20the%20problem%20of%20adapting%20a%20pre-trained%20source%20model%20to%20an%20unlabeled%20target%20domain%20without%20accessing%20any%20source%20data%2C%20which%20is%20quite%20suitable%20for%20the%20field%20of%20data%20security.%20Although%20recent%20advances%20have%20shown%20that%20pseudo-labeling%20strategies%20can%20be%20effective%2C%20they%20often%20fail%20in%20fine-grained%20scenarios%20due%20to%20subtle%20inter-class%20similarities.%20A%20critical%20but%20underexplored%20issue%20is%20the%20presence%20of%20asymmetric%20and%20dynamic%20class%20confusion%2C%20where%20visually%20similar%20classes%20are%20unequally%20and%20inconsistently%20misclassified%20by%20the%20source%20model.%20Existing%20methods%20typically%20ignore%20such%20confusion%20patterns%2C%20leading%20to%20noisy%20pseudo-labels%20and%20poor%20target%20discrimination.%20To%20address%20this%2C%20we%20propose%20CLIP-Guided%20Alignment%28CGA%29%2C%20a%20novel%20framework%20that%20explicitly%20models%20and%20mitigates%20class%20confusion%20in%20SFDA.%20Generally%2C%20our%20method%20consists%20of%20three%20parts%3A%20%281%29%20MCA%3A%20detects%20first%20directional%20confusion%20pairs%20by%20analyzing%20the%20predictions%20of%20the%20source%20model%20in%20the%20target%20domain%3B%20%282%29%20MCC%3A%20leverages%20CLIP%20to%20construct%20confusion-aware%20textual%20prompts%20%28e.g.%20a%20truck%20that%20looks%20like%20a%20bus%29%2C%20enabling%20more%20context-sensitive%20pseudo-labeling%3B%20and%20%283%29%20FAM%3A%20builds%20confusion-guided%20feature%20banks%20for%20both%20CLIP%20and%20the%20source%20model%20and%20aligns%20them%20using%20contrastive%20learning%20to%20reduce%20ambiguity%20in%20the%20representation%20space.%20Extensive%20experiments%20on%20various%20datasets%20demonstrate%20that%20CGA%20consistently%20outperforms%20state-of-the-art%20SFDA%20methods%2C%20with%20especially%20notable%20gains%20in%20confusion-prone%20and%20fine-grained%20scenarios.%20Our%20results%20highlight%20the%20importance%20of%20explicitly%20modeling%20inter-class%20confusion%20for%20effective%20source-free%20adaptation.%20Our%20code%20can%20be%20find%20at%20https%3A//github.com/soloiro/CGA&entry.1838667208=http%3A//arxiv.org/abs/2602.08730v1&entry.124074799=Read"},
{"title": "LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection", "author": "Dezheng Wang and Tong Chen and Guansong Pang and Congyan Chen and Shihua Li and Hongzhi Yin", "abstract": "As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.", "link": "http://arxiv.org/abs/2602.08638v1", "date": "2026-02-09", "relevancy": 2.6254, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5422}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5273}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEFT%3A%20Learnable%20Fusion%20of%20Tri-view%20Tokens%20for%20Unsupervised%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20LEFT%3A%20Learnable%20Fusion%20of%20Tri-view%20Tokens%20for%20Unsupervised%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Dezheng%20Wang%20and%20Tong%20Chen%20and%20Guansong%20Pang%20and%20Congyan%20Chen%20and%20Shihua%20Li%20and%20Hongzhi%20Yin%0AAbstract%3A%20As%20a%20fundamental%20data%20mining%20task%2C%20unsupervised%20time%20series%20anomaly%20detection%20%28TSAD%29%20aims%20to%20build%20a%20model%20for%20identifying%20abnormal%20timestamps%20without%20assuming%20the%20availability%20of%20annotations.%20A%20key%20challenge%20in%20unsupervised%20TSAD%20is%20that%20many%20anomalies%20are%20too%20subtle%20to%20exhibit%20detectable%20deviation%20in%20any%20single%20view%20%28e.g.%2C%20time%20domain%29%2C%20and%20instead%20manifest%20as%20inconsistencies%20across%20multiple%20views%20like%20time%2C%20frequency%2C%20and%20a%20mixture%20of%20resolutions.%20However%2C%20most%20cross-view%20methods%20rely%20on%20feature%20or%20score%20fusion%20and%20do%20not%20enforce%20analysis-synthesis%20consistency%2C%20meaning%20the%20frequency%20branch%20is%20not%20required%20to%20reconstruct%20the%20time%20signal%20through%20an%20inverse%20transform%2C%20and%20vice%20versa.%20In%20this%20paper%2C%20we%20present%20Learnable%20Fusion%20of%20Tri-view%20Tokens%20%28LEFT%29%2C%20a%20unified%20unsupervised%20TSAD%20framework%20that%20models%20anomalies%20as%20inconsistencies%20across%20complementary%20representations.%20LEFT%20learns%20feature%20tokens%20from%20three%20views%20of%20the%20same%20input%20time%20series%3A%20frequency-domain%20tokens%20that%20embed%20periodicity%20information%2C%20time-domain%20tokens%20that%20capture%20local%20dynamics%2C%20and%20multi-scale%20tokens%20that%20learns%20abnormal%20patterns%20at%20varying%20time%20series%20granularities.%20By%20learning%20a%20set%20of%20adaptive%20Nyquist-constrained%20spectral%20filters%2C%20the%20original%20time%20series%20is%20rescaled%20into%20multiple%20resolutions%20and%20then%20encoded%2C%20allowing%20these%20multi-scale%20tokens%20to%20complement%20the%20extracted%20frequency-%20and%20time-domain%20information.%20When%20generating%20the%20fused%20representation%2C%20we%20introduce%20a%20novel%20objective%20that%20reconstructs%20fine-grained%20targets%20from%20coarser%20multi-scale%20structure%2C%20and%20put%20forward%20an%20innovative%20time-frequency%20cycle%20consistency%20constraint%20to%20explicitly%20regularize%20cross-view%20agreement.%20Experiments%20on%20real-world%20benchmarks%20show%20that%20LEFT%20yields%20the%20best%20detection%20accuracy%20against%20SOTA%20baselines%2C%20while%20achieving%20a%205x%20reduction%20on%20FLOPs%20and%208x%20speed-up%20for%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEFT%253A%2520Learnable%2520Fusion%2520of%2520Tri-view%2520Tokens%2520for%2520Unsupervised%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DDezheng%2520Wang%2520and%2520Tong%2520Chen%2520and%2520Guansong%2520Pang%2520and%2520Congyan%2520Chen%2520and%2520Shihua%2520Li%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3DAs%2520a%2520fundamental%2520data%2520mining%2520task%252C%2520unsupervised%2520time%2520series%2520anomaly%2520detection%2520%2528TSAD%2529%2520aims%2520to%2520build%2520a%2520model%2520for%2520identifying%2520abnormal%2520timestamps%2520without%2520assuming%2520the%2520availability%2520of%2520annotations.%2520A%2520key%2520challenge%2520in%2520unsupervised%2520TSAD%2520is%2520that%2520many%2520anomalies%2520are%2520too%2520subtle%2520to%2520exhibit%2520detectable%2520deviation%2520in%2520any%2520single%2520view%2520%2528e.g.%252C%2520time%2520domain%2529%252C%2520and%2520instead%2520manifest%2520as%2520inconsistencies%2520across%2520multiple%2520views%2520like%2520time%252C%2520frequency%252C%2520and%2520a%2520mixture%2520of%2520resolutions.%2520However%252C%2520most%2520cross-view%2520methods%2520rely%2520on%2520feature%2520or%2520score%2520fusion%2520and%2520do%2520not%2520enforce%2520analysis-synthesis%2520consistency%252C%2520meaning%2520the%2520frequency%2520branch%2520is%2520not%2520required%2520to%2520reconstruct%2520the%2520time%2520signal%2520through%2520an%2520inverse%2520transform%252C%2520and%2520vice%2520versa.%2520In%2520this%2520paper%252C%2520we%2520present%2520Learnable%2520Fusion%2520of%2520Tri-view%2520Tokens%2520%2528LEFT%2529%252C%2520a%2520unified%2520unsupervised%2520TSAD%2520framework%2520that%2520models%2520anomalies%2520as%2520inconsistencies%2520across%2520complementary%2520representations.%2520LEFT%2520learns%2520feature%2520tokens%2520from%2520three%2520views%2520of%2520the%2520same%2520input%2520time%2520series%253A%2520frequency-domain%2520tokens%2520that%2520embed%2520periodicity%2520information%252C%2520time-domain%2520tokens%2520that%2520capture%2520local%2520dynamics%252C%2520and%2520multi-scale%2520tokens%2520that%2520learns%2520abnormal%2520patterns%2520at%2520varying%2520time%2520series%2520granularities.%2520By%2520learning%2520a%2520set%2520of%2520adaptive%2520Nyquist-constrained%2520spectral%2520filters%252C%2520the%2520original%2520time%2520series%2520is%2520rescaled%2520into%2520multiple%2520resolutions%2520and%2520then%2520encoded%252C%2520allowing%2520these%2520multi-scale%2520tokens%2520to%2520complement%2520the%2520extracted%2520frequency-%2520and%2520time-domain%2520information.%2520When%2520generating%2520the%2520fused%2520representation%252C%2520we%2520introduce%2520a%2520novel%2520objective%2520that%2520reconstructs%2520fine-grained%2520targets%2520from%2520coarser%2520multi-scale%2520structure%252C%2520and%2520put%2520forward%2520an%2520innovative%2520time-frequency%2520cycle%2520consistency%2520constraint%2520to%2520explicitly%2520regularize%2520cross-view%2520agreement.%2520Experiments%2520on%2520real-world%2520benchmarks%2520show%2520that%2520LEFT%2520yields%2520the%2520best%2520detection%2520accuracy%2520against%2520SOTA%2520baselines%252C%2520while%2520achieving%2520a%25205x%2520reduction%2520on%2520FLOPs%2520and%25208x%2520speed-up%2520for%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEFT%3A%20Learnable%20Fusion%20of%20Tri-view%20Tokens%20for%20Unsupervised%20Time%20Series%20Anomaly%20Detection&entry.906535625=Dezheng%20Wang%20and%20Tong%20Chen%20and%20Guansong%20Pang%20and%20Congyan%20Chen%20and%20Shihua%20Li%20and%20Hongzhi%20Yin&entry.1292438233=As%20a%20fundamental%20data%20mining%20task%2C%20unsupervised%20time%20series%20anomaly%20detection%20%28TSAD%29%20aims%20to%20build%20a%20model%20for%20identifying%20abnormal%20timestamps%20without%20assuming%20the%20availability%20of%20annotations.%20A%20key%20challenge%20in%20unsupervised%20TSAD%20is%20that%20many%20anomalies%20are%20too%20subtle%20to%20exhibit%20detectable%20deviation%20in%20any%20single%20view%20%28e.g.%2C%20time%20domain%29%2C%20and%20instead%20manifest%20as%20inconsistencies%20across%20multiple%20views%20like%20time%2C%20frequency%2C%20and%20a%20mixture%20of%20resolutions.%20However%2C%20most%20cross-view%20methods%20rely%20on%20feature%20or%20score%20fusion%20and%20do%20not%20enforce%20analysis-synthesis%20consistency%2C%20meaning%20the%20frequency%20branch%20is%20not%20required%20to%20reconstruct%20the%20time%20signal%20through%20an%20inverse%20transform%2C%20and%20vice%20versa.%20In%20this%20paper%2C%20we%20present%20Learnable%20Fusion%20of%20Tri-view%20Tokens%20%28LEFT%29%2C%20a%20unified%20unsupervised%20TSAD%20framework%20that%20models%20anomalies%20as%20inconsistencies%20across%20complementary%20representations.%20LEFT%20learns%20feature%20tokens%20from%20three%20views%20of%20the%20same%20input%20time%20series%3A%20frequency-domain%20tokens%20that%20embed%20periodicity%20information%2C%20time-domain%20tokens%20that%20capture%20local%20dynamics%2C%20and%20multi-scale%20tokens%20that%20learns%20abnormal%20patterns%20at%20varying%20time%20series%20granularities.%20By%20learning%20a%20set%20of%20adaptive%20Nyquist-constrained%20spectral%20filters%2C%20the%20original%20time%20series%20is%20rescaled%20into%20multiple%20resolutions%20and%20then%20encoded%2C%20allowing%20these%20multi-scale%20tokens%20to%20complement%20the%20extracted%20frequency-%20and%20time-domain%20information.%20When%20generating%20the%20fused%20representation%2C%20we%20introduce%20a%20novel%20objective%20that%20reconstructs%20fine-grained%20targets%20from%20coarser%20multi-scale%20structure%2C%20and%20put%20forward%20an%20innovative%20time-frequency%20cycle%20consistency%20constraint%20to%20explicitly%20regularize%20cross-view%20agreement.%20Experiments%20on%20real-world%20benchmarks%20show%20that%20LEFT%20yields%20the%20best%20detection%20accuracy%20against%20SOTA%20baselines%2C%20while%20achieving%20a%205x%20reduction%20on%20FLOPs%20and%208x%20speed-up%20for%20training.&entry.1838667208=http%3A//arxiv.org/abs/2602.08638v1&entry.124074799=Read"},
{"title": "SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning", "author": "Yicheng Di and Wei Yuan and Tieke He and Zhanjie Zhang and Ao Ma and Yuan Liu and Hongzhi Yin", "abstract": "Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \\textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.", "link": "http://arxiv.org/abs/2602.08590v1", "date": "2026-02-09", "relevancy": 2.6151, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDFed%3A%20Bridging%20Local%20Global%20Discrepancy%20via%20Subspace%20Refinement%20and%20Divergence%20Control%20in%20Federated%20Prompt%20Learning&body=Title%3A%20SDFed%3A%20Bridging%20Local%20Global%20Discrepancy%20via%20Subspace%20Refinement%20and%20Divergence%20Control%20in%20Federated%20Prompt%20Learning%0AAuthor%3A%20Yicheng%20Di%20and%20Wei%20Yuan%20and%20Tieke%20He%20and%20Zhanjie%20Zhang%20and%20Ao%20Ma%20and%20Yuan%20Liu%20and%20Hongzhi%20Yin%0AAbstract%3A%20Vision-language%20pretrained%20models%20offer%20strong%20transferable%20representations%2C%20yet%20adapting%20them%20in%20privacy-sensitive%20multi-party%20settings%20is%20challenging%20due%20to%20the%20high%20communication%20cost%20of%20federated%20optimization%20and%20the%20limited%20local%20data%20on%20clients.%20Federated%20prompt%20learning%20mitigates%20this%20issue%20by%20keeping%20the%20VLPM%20backbone%20frozen%20and%20collaboratively%20training%20lightweight%20prompt%20parameters.%20However%2C%20existing%20approaches%20typically%20enforce%20a%20unified%20prompt%20structure%20and%20length%20across%20clients%2C%20which%20is%20inadequate%20under%20practical%20client%20heterogeneity%20in%20both%20data%20distributions%20and%20system%20resources%2C%20and%20may%20further%20introduce%20conflicts%20between%20globally%20shared%20and%20locally%20optimal%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BSDFed%7D%2C%20a%20heterogeneous%20federated%20prompt%20learning%20framework%20that%20bridges%20Local-Global%20Discrepancy%20via%20Subspace%20Refinement%20and%20Divergence%20Control.%20SDFed%20maintains%20a%20fixed-length%20global%20prompt%20for%20efficient%20aggregation%20while%20allowing%20each%20client%20to%20learn%20a%20variable-length%20local%20prompt%20to%20better%20match%20its%20data%20characteristics%20and%20capacity.%20To%20mitigate%20local-global%20conflicts%20and%20facilitate%20effective%20knowledge%20transfer%2C%20SDFed%20introduces%20a%20subspace%20refinement%20method%20for%20local%20prompts%20and%20an%20information%20retention%20and%20divergence%20control%20strategy%20that%20preserves%20key%20local%20information%20while%20maintaining%20appropriate%20separability%20between%20global%20and%20local%20representations.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%20SDFed%20consistently%20improves%20performance%20and%20robustness%20in%20heterogeneous%20federated%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDFed%253A%2520Bridging%2520Local%2520Global%2520Discrepancy%2520via%2520Subspace%2520Refinement%2520and%2520Divergence%2520Control%2520in%2520Federated%2520Prompt%2520Learning%26entry.906535625%3DYicheng%2520Di%2520and%2520Wei%2520Yuan%2520and%2520Tieke%2520He%2520and%2520Zhanjie%2520Zhang%2520and%2520Ao%2520Ma%2520and%2520Yuan%2520Liu%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3DVision-language%2520pretrained%2520models%2520offer%2520strong%2520transferable%2520representations%252C%2520yet%2520adapting%2520them%2520in%2520privacy-sensitive%2520multi-party%2520settings%2520is%2520challenging%2520due%2520to%2520the%2520high%2520communication%2520cost%2520of%2520federated%2520optimization%2520and%2520the%2520limited%2520local%2520data%2520on%2520clients.%2520Federated%2520prompt%2520learning%2520mitigates%2520this%2520issue%2520by%2520keeping%2520the%2520VLPM%2520backbone%2520frozen%2520and%2520collaboratively%2520training%2520lightweight%2520prompt%2520parameters.%2520However%252C%2520existing%2520approaches%2520typically%2520enforce%2520a%2520unified%2520prompt%2520structure%2520and%2520length%2520across%2520clients%252C%2520which%2520is%2520inadequate%2520under%2520practical%2520client%2520heterogeneity%2520in%2520both%2520data%2520distributions%2520and%2520system%2520resources%252C%2520and%2520may%2520further%2520introduce%2520conflicts%2520between%2520globally%2520shared%2520and%2520locally%2520optimal%2520knowledge.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BSDFed%257D%252C%2520a%2520heterogeneous%2520federated%2520prompt%2520learning%2520framework%2520that%2520bridges%2520Local-Global%2520Discrepancy%2520via%2520Subspace%2520Refinement%2520and%2520Divergence%2520Control.%2520SDFed%2520maintains%2520a%2520fixed-length%2520global%2520prompt%2520for%2520efficient%2520aggregation%2520while%2520allowing%2520each%2520client%2520to%2520learn%2520a%2520variable-length%2520local%2520prompt%2520to%2520better%2520match%2520its%2520data%2520characteristics%2520and%2520capacity.%2520To%2520mitigate%2520local-global%2520conflicts%2520and%2520facilitate%2520effective%2520knowledge%2520transfer%252C%2520SDFed%2520introduces%2520a%2520subspace%2520refinement%2520method%2520for%2520local%2520prompts%2520and%2520an%2520information%2520retention%2520and%2520divergence%2520control%2520strategy%2520that%2520preserves%2520key%2520local%2520information%2520while%2520maintaining%2520appropriate%2520separability%2520between%2520global%2520and%2520local%2520representations.%2520Extensive%2520experiments%2520on%2520several%2520datasets%2520demonstrate%2520that%2520SDFed%2520consistently%2520improves%2520performance%2520and%2520robustness%2520in%2520heterogeneous%2520federated%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDFed%3A%20Bridging%20Local%20Global%20Discrepancy%20via%20Subspace%20Refinement%20and%20Divergence%20Control%20in%20Federated%20Prompt%20Learning&entry.906535625=Yicheng%20Di%20and%20Wei%20Yuan%20and%20Tieke%20He%20and%20Zhanjie%20Zhang%20and%20Ao%20Ma%20and%20Yuan%20Liu%20and%20Hongzhi%20Yin&entry.1292438233=Vision-language%20pretrained%20models%20offer%20strong%20transferable%20representations%2C%20yet%20adapting%20them%20in%20privacy-sensitive%20multi-party%20settings%20is%20challenging%20due%20to%20the%20high%20communication%20cost%20of%20federated%20optimization%20and%20the%20limited%20local%20data%20on%20clients.%20Federated%20prompt%20learning%20mitigates%20this%20issue%20by%20keeping%20the%20VLPM%20backbone%20frozen%20and%20collaboratively%20training%20lightweight%20prompt%20parameters.%20However%2C%20existing%20approaches%20typically%20enforce%20a%20unified%20prompt%20structure%20and%20length%20across%20clients%2C%20which%20is%20inadequate%20under%20practical%20client%20heterogeneity%20in%20both%20data%20distributions%20and%20system%20resources%2C%20and%20may%20further%20introduce%20conflicts%20between%20globally%20shared%20and%20locally%20optimal%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BSDFed%7D%2C%20a%20heterogeneous%20federated%20prompt%20learning%20framework%20that%20bridges%20Local-Global%20Discrepancy%20via%20Subspace%20Refinement%20and%20Divergence%20Control.%20SDFed%20maintains%20a%20fixed-length%20global%20prompt%20for%20efficient%20aggregation%20while%20allowing%20each%20client%20to%20learn%20a%20variable-length%20local%20prompt%20to%20better%20match%20its%20data%20characteristics%20and%20capacity.%20To%20mitigate%20local-global%20conflicts%20and%20facilitate%20effective%20knowledge%20transfer%2C%20SDFed%20introduces%20a%20subspace%20refinement%20method%20for%20local%20prompts%20and%20an%20information%20retention%20and%20divergence%20control%20strategy%20that%20preserves%20key%20local%20information%20while%20maintaining%20appropriate%20separability%20between%20global%20and%20local%20representations.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%20SDFed%20consistently%20improves%20performance%20and%20robustness%20in%20heterogeneous%20federated%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.08590v1&entry.124074799=Read"},
{"title": "DyMixOp: A Neural Operator Designed from a Complex Dynamics Perspective with Local-Global Mixing for Solving PDEs", "author": "Pengyu Lai and Yixiao Chen and Dewu Yang and Rui Wang and Feng Wang and Hui Xu", "abstract": "A primary challenge in using neural networks to approximate nonlinear dynamical systems governed by partial differential equations (PDEs) lies in recasting these systems into a tractable representation particularly when the dynamics are inherently non-linearizable or require infinite-dimensional spaces for linearization. To address this challenge, we introduce DyMixOp, a novel neural operator framework for PDEs that integrates theoretical insights from complex dynamical systems. Grounded in dynamics-aware priors and inertial manifold theory, DyMixOp projects the original infinite-dimensional PDE dynamics onto a finite-dimensional latent space. This reduction preserves both essential linear structures and dominant nonlinear interactions, thereby establishing a physically interpretable and computationally structured foundation. Central to this approach is the local-global mixing (LGM) transformation, a key architectural innovation inspired by the convective nonlinearity in turbulent flows. By multiplicatively coupling local fine-scale features with global spectral information, LGM effectively captures high-frequency details and complex nonlinear couplings while mitigating the spectral bias that plagues many existing neural operators. The framework is further enhanced by a dynamics-informed architecture that stacks multiple LGM layers in a hybrid configuration, incorporating timescale-adaptive gating and parallel aggregation of intermediate dynamics. This design enables robust approximation of general evolutionary dynamics across diverse physical regimes. Extensive experiments on seven benchmark PDE systems spanning 1D to 3D, elliptic to hyperbolic types demonstrate that DyMixOp achieves state-of-the-art performance on six of them, significantly reducing prediction errors (by up to 94.3% in chaotic regimes) while maintaining computational efficiency and strong scalability.", "link": "http://arxiv.org/abs/2508.13490v3", "date": "2026-02-09", "relevancy": 2.6028, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.546}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5116}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyMixOp%3A%20A%20Neural%20Operator%20Designed%20from%20a%20Complex%20Dynamics%20Perspective%20with%20Local-Global%20Mixing%20for%20Solving%20PDEs&body=Title%3A%20DyMixOp%3A%20A%20Neural%20Operator%20Designed%20from%20a%20Complex%20Dynamics%20Perspective%20with%20Local-Global%20Mixing%20for%20Solving%20PDEs%0AAuthor%3A%20Pengyu%20Lai%20and%20Yixiao%20Chen%20and%20Dewu%20Yang%20and%20Rui%20Wang%20and%20Feng%20Wang%20and%20Hui%20Xu%0AAbstract%3A%20A%20primary%20challenge%20in%20using%20neural%20networks%20to%20approximate%20nonlinear%20dynamical%20systems%20governed%20by%20partial%20differential%20equations%20%28PDEs%29%20lies%20in%20recasting%20these%20systems%20into%20a%20tractable%20representation%20particularly%20when%20the%20dynamics%20are%20inherently%20non-linearizable%20or%20require%20infinite-dimensional%20spaces%20for%20linearization.%20To%20address%20this%20challenge%2C%20we%20introduce%20DyMixOp%2C%20a%20novel%20neural%20operator%20framework%20for%20PDEs%20that%20integrates%20theoretical%20insights%20from%20complex%20dynamical%20systems.%20Grounded%20in%20dynamics-aware%20priors%20and%20inertial%20manifold%20theory%2C%20DyMixOp%20projects%20the%20original%20infinite-dimensional%20PDE%20dynamics%20onto%20a%20finite-dimensional%20latent%20space.%20This%20reduction%20preserves%20both%20essential%20linear%20structures%20and%20dominant%20nonlinear%20interactions%2C%20thereby%20establishing%20a%20physically%20interpretable%20and%20computationally%20structured%20foundation.%20Central%20to%20this%20approach%20is%20the%20local-global%20mixing%20%28LGM%29%20transformation%2C%20a%20key%20architectural%20innovation%20inspired%20by%20the%20convective%20nonlinearity%20in%20turbulent%20flows.%20By%20multiplicatively%20coupling%20local%20fine-scale%20features%20with%20global%20spectral%20information%2C%20LGM%20effectively%20captures%20high-frequency%20details%20and%20complex%20nonlinear%20couplings%20while%20mitigating%20the%20spectral%20bias%20that%20plagues%20many%20existing%20neural%20operators.%20The%20framework%20is%20further%20enhanced%20by%20a%20dynamics-informed%20architecture%20that%20stacks%20multiple%20LGM%20layers%20in%20a%20hybrid%20configuration%2C%20incorporating%20timescale-adaptive%20gating%20and%20parallel%20aggregation%20of%20intermediate%20dynamics.%20This%20design%20enables%20robust%20approximation%20of%20general%20evolutionary%20dynamics%20across%20diverse%20physical%20regimes.%20Extensive%20experiments%20on%20seven%20benchmark%20PDE%20systems%20spanning%201D%20to%203D%2C%20elliptic%20to%20hyperbolic%20types%20demonstrate%20that%20DyMixOp%20achieves%20state-of-the-art%20performance%20on%20six%20of%20them%2C%20significantly%20reducing%20prediction%20errors%20%28by%20up%20to%2094.3%25%20in%20chaotic%20regimes%29%20while%20maintaining%20computational%20efficiency%20and%20strong%20scalability.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13490v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyMixOp%253A%2520A%2520Neural%2520Operator%2520Designed%2520from%2520a%2520Complex%2520Dynamics%2520Perspective%2520with%2520Local-Global%2520Mixing%2520for%2520Solving%2520PDEs%26entry.906535625%3DPengyu%2520Lai%2520and%2520Yixiao%2520Chen%2520and%2520Dewu%2520Yang%2520and%2520Rui%2520Wang%2520and%2520Feng%2520Wang%2520and%2520Hui%2520Xu%26entry.1292438233%3DA%2520primary%2520challenge%2520in%2520using%2520neural%2520networks%2520to%2520approximate%2520nonlinear%2520dynamical%2520systems%2520governed%2520by%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520lies%2520in%2520recasting%2520these%2520systems%2520into%2520a%2520tractable%2520representation%2520particularly%2520when%2520the%2520dynamics%2520are%2520inherently%2520non-linearizable%2520or%2520require%2520infinite-dimensional%2520spaces%2520for%2520linearization.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520DyMixOp%252C%2520a%2520novel%2520neural%2520operator%2520framework%2520for%2520PDEs%2520that%2520integrates%2520theoretical%2520insights%2520from%2520complex%2520dynamical%2520systems.%2520Grounded%2520in%2520dynamics-aware%2520priors%2520and%2520inertial%2520manifold%2520theory%252C%2520DyMixOp%2520projects%2520the%2520original%2520infinite-dimensional%2520PDE%2520dynamics%2520onto%2520a%2520finite-dimensional%2520latent%2520space.%2520This%2520reduction%2520preserves%2520both%2520essential%2520linear%2520structures%2520and%2520dominant%2520nonlinear%2520interactions%252C%2520thereby%2520establishing%2520a%2520physically%2520interpretable%2520and%2520computationally%2520structured%2520foundation.%2520Central%2520to%2520this%2520approach%2520is%2520the%2520local-global%2520mixing%2520%2528LGM%2529%2520transformation%252C%2520a%2520key%2520architectural%2520innovation%2520inspired%2520by%2520the%2520convective%2520nonlinearity%2520in%2520turbulent%2520flows.%2520By%2520multiplicatively%2520coupling%2520local%2520fine-scale%2520features%2520with%2520global%2520spectral%2520information%252C%2520LGM%2520effectively%2520captures%2520high-frequency%2520details%2520and%2520complex%2520nonlinear%2520couplings%2520while%2520mitigating%2520the%2520spectral%2520bias%2520that%2520plagues%2520many%2520existing%2520neural%2520operators.%2520The%2520framework%2520is%2520further%2520enhanced%2520by%2520a%2520dynamics-informed%2520architecture%2520that%2520stacks%2520multiple%2520LGM%2520layers%2520in%2520a%2520hybrid%2520configuration%252C%2520incorporating%2520timescale-adaptive%2520gating%2520and%2520parallel%2520aggregation%2520of%2520intermediate%2520dynamics.%2520This%2520design%2520enables%2520robust%2520approximation%2520of%2520general%2520evolutionary%2520dynamics%2520across%2520diverse%2520physical%2520regimes.%2520Extensive%2520experiments%2520on%2520seven%2520benchmark%2520PDE%2520systems%2520spanning%25201D%2520to%25203D%252C%2520elliptic%2520to%2520hyperbolic%2520types%2520demonstrate%2520that%2520DyMixOp%2520achieves%2520state-of-the-art%2520performance%2520on%2520six%2520of%2520them%252C%2520significantly%2520reducing%2520prediction%2520errors%2520%2528by%2520up%2520to%252094.3%2525%2520in%2520chaotic%2520regimes%2529%2520while%2520maintaining%2520computational%2520efficiency%2520and%2520strong%2520scalability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13490v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyMixOp%3A%20A%20Neural%20Operator%20Designed%20from%20a%20Complex%20Dynamics%20Perspective%20with%20Local-Global%20Mixing%20for%20Solving%20PDEs&entry.906535625=Pengyu%20Lai%20and%20Yixiao%20Chen%20and%20Dewu%20Yang%20and%20Rui%20Wang%20and%20Feng%20Wang%20and%20Hui%20Xu&entry.1292438233=A%20primary%20challenge%20in%20using%20neural%20networks%20to%20approximate%20nonlinear%20dynamical%20systems%20governed%20by%20partial%20differential%20equations%20%28PDEs%29%20lies%20in%20recasting%20these%20systems%20into%20a%20tractable%20representation%20particularly%20when%20the%20dynamics%20are%20inherently%20non-linearizable%20or%20require%20infinite-dimensional%20spaces%20for%20linearization.%20To%20address%20this%20challenge%2C%20we%20introduce%20DyMixOp%2C%20a%20novel%20neural%20operator%20framework%20for%20PDEs%20that%20integrates%20theoretical%20insights%20from%20complex%20dynamical%20systems.%20Grounded%20in%20dynamics-aware%20priors%20and%20inertial%20manifold%20theory%2C%20DyMixOp%20projects%20the%20original%20infinite-dimensional%20PDE%20dynamics%20onto%20a%20finite-dimensional%20latent%20space.%20This%20reduction%20preserves%20both%20essential%20linear%20structures%20and%20dominant%20nonlinear%20interactions%2C%20thereby%20establishing%20a%20physically%20interpretable%20and%20computationally%20structured%20foundation.%20Central%20to%20this%20approach%20is%20the%20local-global%20mixing%20%28LGM%29%20transformation%2C%20a%20key%20architectural%20innovation%20inspired%20by%20the%20convective%20nonlinearity%20in%20turbulent%20flows.%20By%20multiplicatively%20coupling%20local%20fine-scale%20features%20with%20global%20spectral%20information%2C%20LGM%20effectively%20captures%20high-frequency%20details%20and%20complex%20nonlinear%20couplings%20while%20mitigating%20the%20spectral%20bias%20that%20plagues%20many%20existing%20neural%20operators.%20The%20framework%20is%20further%20enhanced%20by%20a%20dynamics-informed%20architecture%20that%20stacks%20multiple%20LGM%20layers%20in%20a%20hybrid%20configuration%2C%20incorporating%20timescale-adaptive%20gating%20and%20parallel%20aggregation%20of%20intermediate%20dynamics.%20This%20design%20enables%20robust%20approximation%20of%20general%20evolutionary%20dynamics%20across%20diverse%20physical%20regimes.%20Extensive%20experiments%20on%20seven%20benchmark%20PDE%20systems%20spanning%201D%20to%203D%2C%20elliptic%20to%20hyperbolic%20types%20demonstrate%20that%20DyMixOp%20achieves%20state-of-the-art%20performance%20on%20six%20of%20them%2C%20significantly%20reducing%20prediction%20errors%20%28by%20up%20to%2094.3%25%20in%20chaotic%20regimes%29%20while%20maintaining%20computational%20efficiency%20and%20strong%20scalability.&entry.1838667208=http%3A//arxiv.org/abs/2508.13490v3&entry.124074799=Read"},
{"title": "ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models", "author": "Raghav Singhal and Kaustubh Ponkshe and Rohit Vartak and Praneeth Vepakomma", "abstract": "Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget, a property we validate through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.", "link": "http://arxiv.org/abs/2505.14238v4", "date": "2026-02-09", "relevancy": 2.5975, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models&body=Title%3A%20ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models%0AAuthor%3A%20Raghav%20Singhal%20and%20Kaustubh%20Ponkshe%20and%20Rohit%20Vartak%20and%20Praneeth%20Vepakomma%0AAbstract%3A%20Large%20Language%20Models%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20but%20adapting%20them%20efficiently%20to%20new%20domains%20remains%20a%20key%20challenge.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%20introducing%20lightweight%2C%20trainable%20modules%20while%20keeping%20most%20pre-trained%20weights%20fixed.%20The%20prevailing%20approach%2C%20LoRA%2C%20models%20updates%20using%20a%20low-rank%20decomposition%2C%20but%20its%20expressivity%20is%20inherently%20constrained%20by%20the%20rank.%20Recent%20methods%20like%20HiRA%20aim%20to%20increase%20expressivity%20by%20incorporating%20a%20Hadamard%20product%20with%20the%20frozen%20weights%2C%20but%20still%20rely%20on%20the%20structure%20of%20the%20pre-trained%20model.%20We%20introduce%20ABBA%2C%20a%20new%20PEFT%20architecture%20that%20reparameterizes%20the%20update%20as%20a%20Hadamard%20product%20of%20two%20independently%20learnable%20low-rank%20matrices.%20In%20contrast%20to%20prior%20work%2C%20ABBA%20fully%20decouples%20the%20update%20from%20the%20pre-trained%20weights%2C%20enabling%20both%20components%20to%20be%20optimized%20freely.%20This%20leads%20to%20significantly%20higher%20expressivity%20under%20the%20same%20parameter%20budget%2C%20a%20property%20we%20validate%20through%20matrix%20reconstruction%20experiments.%20Empirically%2C%20ABBA%20achieves%20state-of-the-art%20results%20on%20arithmetic%20and%20commonsense%20reasoning%20benchmarks%2C%20consistently%20outperforming%20existing%20PEFT%20methods%20by%20a%20significant%20margin%20across%20multiple%20models.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/CERT-Lab/abba.%0ALink%3A%20http%3A//arxiv.org/abs/2505.14238v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABBA-Adapters%253A%2520Efficient%2520and%2520Expressive%2520Fine-Tuning%2520of%2520Foundation%2520Models%26entry.906535625%3DRaghav%2520Singhal%2520and%2520Kaustubh%2520Ponkshe%2520and%2520Rohit%2520Vartak%2520and%2520Praneeth%2520Vepakomma%26entry.1292438233%3DLarge%2520Language%2520Models%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520but%2520adapting%2520them%2520efficiently%2520to%2520new%2520domains%2520remains%2520a%2520key%2520challenge.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520address%2520this%2520by%2520introducing%2520lightweight%252C%2520trainable%2520modules%2520while%2520keeping%2520most%2520pre-trained%2520weights%2520fixed.%2520The%2520prevailing%2520approach%252C%2520LoRA%252C%2520models%2520updates%2520using%2520a%2520low-rank%2520decomposition%252C%2520but%2520its%2520expressivity%2520is%2520inherently%2520constrained%2520by%2520the%2520rank.%2520Recent%2520methods%2520like%2520HiRA%2520aim%2520to%2520increase%2520expressivity%2520by%2520incorporating%2520a%2520Hadamard%2520product%2520with%2520the%2520frozen%2520weights%252C%2520but%2520still%2520rely%2520on%2520the%2520structure%2520of%2520the%2520pre-trained%2520model.%2520We%2520introduce%2520ABBA%252C%2520a%2520new%2520PEFT%2520architecture%2520that%2520reparameterizes%2520the%2520update%2520as%2520a%2520Hadamard%2520product%2520of%2520two%2520independently%2520learnable%2520low-rank%2520matrices.%2520In%2520contrast%2520to%2520prior%2520work%252C%2520ABBA%2520fully%2520decouples%2520the%2520update%2520from%2520the%2520pre-trained%2520weights%252C%2520enabling%2520both%2520components%2520to%2520be%2520optimized%2520freely.%2520This%2520leads%2520to%2520significantly%2520higher%2520expressivity%2520under%2520the%2520same%2520parameter%2520budget%252C%2520a%2520property%2520we%2520validate%2520through%2520matrix%2520reconstruction%2520experiments.%2520Empirically%252C%2520ABBA%2520achieves%2520state-of-the-art%2520results%2520on%2520arithmetic%2520and%2520commonsense%2520reasoning%2520benchmarks%252C%2520consistently%2520outperforming%2520existing%2520PEFT%2520methods%2520by%2520a%2520significant%2520margin%2520across%2520multiple%2520models.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/CERT-Lab/abba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14238v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models&entry.906535625=Raghav%20Singhal%20and%20Kaustubh%20Ponkshe%20and%20Rohit%20Vartak%20and%20Praneeth%20Vepakomma&entry.1292438233=Large%20Language%20Models%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20but%20adapting%20them%20efficiently%20to%20new%20domains%20remains%20a%20key%20challenge.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%20introducing%20lightweight%2C%20trainable%20modules%20while%20keeping%20most%20pre-trained%20weights%20fixed.%20The%20prevailing%20approach%2C%20LoRA%2C%20models%20updates%20using%20a%20low-rank%20decomposition%2C%20but%20its%20expressivity%20is%20inherently%20constrained%20by%20the%20rank.%20Recent%20methods%20like%20HiRA%20aim%20to%20increase%20expressivity%20by%20incorporating%20a%20Hadamard%20product%20with%20the%20frozen%20weights%2C%20but%20still%20rely%20on%20the%20structure%20of%20the%20pre-trained%20model.%20We%20introduce%20ABBA%2C%20a%20new%20PEFT%20architecture%20that%20reparameterizes%20the%20update%20as%20a%20Hadamard%20product%20of%20two%20independently%20learnable%20low-rank%20matrices.%20In%20contrast%20to%20prior%20work%2C%20ABBA%20fully%20decouples%20the%20update%20from%20the%20pre-trained%20weights%2C%20enabling%20both%20components%20to%20be%20optimized%20freely.%20This%20leads%20to%20significantly%20higher%20expressivity%20under%20the%20same%20parameter%20budget%2C%20a%20property%20we%20validate%20through%20matrix%20reconstruction%20experiments.%20Empirically%2C%20ABBA%20achieves%20state-of-the-art%20results%20on%20arithmetic%20and%20commonsense%20reasoning%20benchmarks%2C%20consistently%20outperforming%20existing%20PEFT%20methods%20by%20a%20significant%20margin%20across%20multiple%20models.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/CERT-Lab/abba.&entry.1838667208=http%3A//arxiv.org/abs/2505.14238v4&entry.124074799=Read"},
{"title": "TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models", "author": "Tianyin Liao and Chunyu Hu and Yicheng Sui and Xingxuan Zhang and Peng Cui and Jianxin Li and Ziwei Zhang", "abstract": "Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.", "link": "http://arxiv.org/abs/2602.08592v1", "date": "2026-02-09", "relevancy": 2.5921, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TFMLinker%3A%20Universal%20Link%20Predictor%20by%20Graph%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&body=Title%3A%20TFMLinker%3A%20Universal%20Link%20Predictor%20by%20Graph%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models%0AAuthor%3A%20Tianyin%20Liao%20and%20Chunyu%20Hu%20and%20Yicheng%20Sui%20and%20Xingxuan%20Zhang%20and%20Peng%20Cui%20and%20Jianxin%20Li%20and%20Ziwei%20Zhang%0AAbstract%3A%20Link%20prediction%20is%20a%20fundamental%20task%20in%20graph%20machine%20learning%20with%20widespread%20applications%20such%20as%20recommendation%20systems%2C%20drug%20discovery%2C%20knowledge%20graphs%2C%20etc.%20In%20the%20foundation%20model%20era%2C%20how%20to%20develop%20universal%20link%20prediction%20methods%20across%20datasets%20and%20domains%20becomes%20a%20key%20problem%2C%20with%20some%20initial%20attempts%20adopting%20Graph%20Foundation%20Models%20utilizing%20Graph%20Neural%20Networks%20and%20Large%20Language%20Models.%20However%2C%20the%20existing%20methods%20face%20notable%20limitations%2C%20including%20limited%20pre-training%20scale%20or%20heavy%20reliance%20on%20textual%20information.%20Motivated%20by%20the%20success%20of%20tabular%20foundation%20models%20%28TFMs%29%20in%20achieving%20universal%20prediction%20across%20diverse%20tabular%20datasets%2C%20we%20explore%20an%20alternative%20approach%20by%20TFMs%2C%20which%20are%20pre-trained%20on%20diverse%20synthetic%20datasets%20sampled%20from%20structural%20causal%20models%20and%20support%20strong%20in-context%20learning%20independent%20of%20textual%20attributes.%20Nevertheless%2C%20adapting%20TFMs%20for%20link%20prediction%20faces%20severe%20technical%20challenges%20such%20as%20how%20to%20obtain%20the%20necessary%20context%20and%20capture%20link-centric%20topological%20information.%20To%20solve%20these%20challenges%2C%20we%20propose%20TFMLinker%20%28Tabular%20Foundation%20Model%20for%20Link%20Predictor%29%2C%20aiming%20to%20leverage%20the%20in-context%20learning%20capabilities%20of%20TFMs%20to%20perform%20link%20prediction%20across%20diverse%20graphs%20without%20requiring%20dataset-specific%20fine-tuning.%20Specifically%2C%20we%20first%20develop%20a%20prototype-augmented%20local-global%20context%20module%20to%20construct%20context%20that%20captures%20both%20graph-specific%20and%20cross-graph%20transferable%20patterns.%20Next%2C%20we%20design%20a%20universal%20topology-aware%20link%20encoder%20to%20capture%20link-centric%20topological%20information%20and%20generate%20link%20representations%20as%20inputs%20for%20the%20TFM.%20Finally%2C%20we%20employ%20the%20TFM%20to%20predict%20link%20existence%20through%20in-context%20learning.%20Experiments%20on%206%20graph%20benchmarks%20across%20diverse%20domains%20demonstrate%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20baselines%20without%20requiring%20dataset-specific%20finetuning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTFMLinker%253A%2520Universal%2520Link%2520Predictor%2520by%2520Graph%2520In-Context%2520Learning%2520with%2520Tabular%2520Foundation%2520Models%26entry.906535625%3DTianyin%2520Liao%2520and%2520Chunyu%2520Hu%2520and%2520Yicheng%2520Sui%2520and%2520Xingxuan%2520Zhang%2520and%2520Peng%2520Cui%2520and%2520Jianxin%2520Li%2520and%2520Ziwei%2520Zhang%26entry.1292438233%3DLink%2520prediction%2520is%2520a%2520fundamental%2520task%2520in%2520graph%2520machine%2520learning%2520with%2520widespread%2520applications%2520such%2520as%2520recommendation%2520systems%252C%2520drug%2520discovery%252C%2520knowledge%2520graphs%252C%2520etc.%2520In%2520the%2520foundation%2520model%2520era%252C%2520how%2520to%2520develop%2520universal%2520link%2520prediction%2520methods%2520across%2520datasets%2520and%2520domains%2520becomes%2520a%2520key%2520problem%252C%2520with%2520some%2520initial%2520attempts%2520adopting%2520Graph%2520Foundation%2520Models%2520utilizing%2520Graph%2520Neural%2520Networks%2520and%2520Large%2520Language%2520Models.%2520However%252C%2520the%2520existing%2520methods%2520face%2520notable%2520limitations%252C%2520including%2520limited%2520pre-training%2520scale%2520or%2520heavy%2520reliance%2520on%2520textual%2520information.%2520Motivated%2520by%2520the%2520success%2520of%2520tabular%2520foundation%2520models%2520%2528TFMs%2529%2520in%2520achieving%2520universal%2520prediction%2520across%2520diverse%2520tabular%2520datasets%252C%2520we%2520explore%2520an%2520alternative%2520approach%2520by%2520TFMs%252C%2520which%2520are%2520pre-trained%2520on%2520diverse%2520synthetic%2520datasets%2520sampled%2520from%2520structural%2520causal%2520models%2520and%2520support%2520strong%2520in-context%2520learning%2520independent%2520of%2520textual%2520attributes.%2520Nevertheless%252C%2520adapting%2520TFMs%2520for%2520link%2520prediction%2520faces%2520severe%2520technical%2520challenges%2520such%2520as%2520how%2520to%2520obtain%2520the%2520necessary%2520context%2520and%2520capture%2520link-centric%2520topological%2520information.%2520To%2520solve%2520these%2520challenges%252C%2520we%2520propose%2520TFMLinker%2520%2528Tabular%2520Foundation%2520Model%2520for%2520Link%2520Predictor%2529%252C%2520aiming%2520to%2520leverage%2520the%2520in-context%2520learning%2520capabilities%2520of%2520TFMs%2520to%2520perform%2520link%2520prediction%2520across%2520diverse%2520graphs%2520without%2520requiring%2520dataset-specific%2520fine-tuning.%2520Specifically%252C%2520we%2520first%2520develop%2520a%2520prototype-augmented%2520local-global%2520context%2520module%2520to%2520construct%2520context%2520that%2520captures%2520both%2520graph-specific%2520and%2520cross-graph%2520transferable%2520patterns.%2520Next%252C%2520we%2520design%2520a%2520universal%2520topology-aware%2520link%2520encoder%2520to%2520capture%2520link-centric%2520topological%2520information%2520and%2520generate%2520link%2520representations%2520as%2520inputs%2520for%2520the%2520TFM.%2520Finally%252C%2520we%2520employ%2520the%2520TFM%2520to%2520predict%2520link%2520existence%2520through%2520in-context%2520learning.%2520Experiments%2520on%25206%2520graph%2520benchmarks%2520across%2520diverse%2520domains%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520state-of-the-art%2520baselines%2520without%2520requiring%2520dataset-specific%2520finetuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFMLinker%3A%20Universal%20Link%20Predictor%20by%20Graph%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&entry.906535625=Tianyin%20Liao%20and%20Chunyu%20Hu%20and%20Yicheng%20Sui%20and%20Xingxuan%20Zhang%20and%20Peng%20Cui%20and%20Jianxin%20Li%20and%20Ziwei%20Zhang&entry.1292438233=Link%20prediction%20is%20a%20fundamental%20task%20in%20graph%20machine%20learning%20with%20widespread%20applications%20such%20as%20recommendation%20systems%2C%20drug%20discovery%2C%20knowledge%20graphs%2C%20etc.%20In%20the%20foundation%20model%20era%2C%20how%20to%20develop%20universal%20link%20prediction%20methods%20across%20datasets%20and%20domains%20becomes%20a%20key%20problem%2C%20with%20some%20initial%20attempts%20adopting%20Graph%20Foundation%20Models%20utilizing%20Graph%20Neural%20Networks%20and%20Large%20Language%20Models.%20However%2C%20the%20existing%20methods%20face%20notable%20limitations%2C%20including%20limited%20pre-training%20scale%20or%20heavy%20reliance%20on%20textual%20information.%20Motivated%20by%20the%20success%20of%20tabular%20foundation%20models%20%28TFMs%29%20in%20achieving%20universal%20prediction%20across%20diverse%20tabular%20datasets%2C%20we%20explore%20an%20alternative%20approach%20by%20TFMs%2C%20which%20are%20pre-trained%20on%20diverse%20synthetic%20datasets%20sampled%20from%20structural%20causal%20models%20and%20support%20strong%20in-context%20learning%20independent%20of%20textual%20attributes.%20Nevertheless%2C%20adapting%20TFMs%20for%20link%20prediction%20faces%20severe%20technical%20challenges%20such%20as%20how%20to%20obtain%20the%20necessary%20context%20and%20capture%20link-centric%20topological%20information.%20To%20solve%20these%20challenges%2C%20we%20propose%20TFMLinker%20%28Tabular%20Foundation%20Model%20for%20Link%20Predictor%29%2C%20aiming%20to%20leverage%20the%20in-context%20learning%20capabilities%20of%20TFMs%20to%20perform%20link%20prediction%20across%20diverse%20graphs%20without%20requiring%20dataset-specific%20fine-tuning.%20Specifically%2C%20we%20first%20develop%20a%20prototype-augmented%20local-global%20context%20module%20to%20construct%20context%20that%20captures%20both%20graph-specific%20and%20cross-graph%20transferable%20patterns.%20Next%2C%20we%20design%20a%20universal%20topology-aware%20link%20encoder%20to%20capture%20link-centric%20topological%20information%20and%20generate%20link%20representations%20as%20inputs%20for%20the%20TFM.%20Finally%2C%20we%20employ%20the%20TFM%20to%20predict%20link%20existence%20through%20in-context%20learning.%20Experiments%20on%206%20graph%20benchmarks%20across%20diverse%20domains%20demonstrate%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20baselines%20without%20requiring%20dataset-specific%20finetuning.&entry.1838667208=http%3A//arxiv.org/abs/2602.08592v1&entry.124074799=Read"},
{"title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications", "author": "Yao Pu and Yiming Shi and Zhenxi Zhang and Peixin Yu and Yitao Zhuang and Xiang Wang and Hongzhao Chen and Jing Cai and Ge Ren", "abstract": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.", "link": "http://arxiv.org/abs/2602.08822v1", "date": "2026-02-09", "relevancy": 2.5821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-to-All%20MRI%20Synthesis%3A%20A%20Unified%20Foundation%20Model%20for%20Nasopharyngeal%20Carcinoma%20and%20Its%20Downstream%20Applications&body=Title%3A%20Any-to-All%20MRI%20Synthesis%3A%20A%20Unified%20Foundation%20Model%20for%20Nasopharyngeal%20Carcinoma%20and%20Its%20Downstream%20Applications%0AAuthor%3A%20Yao%20Pu%20and%20Yiming%20Shi%20and%20Zhenxi%20Zhang%20and%20Peixin%20Yu%20and%20Yitao%20Zhuang%20and%20Xiang%20Wang%20and%20Hongzhao%20Chen%20and%20Jing%20Cai%20and%20Ge%20Ren%0AAbstract%3A%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20nasopharyngeal%20carcinoma%20%28NPC%29%20radiotherapy%20%28RT%29%2C%20but%20practical%20constraints%2C%20such%20as%20patient%20discomfort%2C%20long%20scan%20times%2C%20and%20high%20costs%20often%20lead%20to%20incomplete%20modalities%20in%20clinical%20practice%2C%20compromising%20RT%20planning%20accuracy.%20Traditional%20MRI%20synthesis%20methods%20are%20modality-specific%2C%20limited%20in%20anatomical%20adaptability%2C%20and%20lack%20clinical%20interpretability-failing%20to%20meet%20NPC%27s%20RT%20needs.%20Here%2C%20we%20developed%20a%20unified%20foundation%20model%20integrating%20contrastive%20visual%20representation%20learning%20and%20vision-language%20alignment%20%28VLA%29%20to%20enable%20any-to-all%20MRI%20synthesis.%20The%20model%20uses%20a%20contrastive%20encoder%20for%20modality-invariant%20representations%20and%20a%20CLIP-based%20text-informed%20decoder%20for%20semantically%20consistent%20synthesis%2C%20supporting%20any-to-all%20MRI%20synthesis%20via%20one%20unified%20foundation%20model.%20Trained%20on%2040%2C825%20images%20from%2013%20institutions%2C%20it%20achieves%20consistently%20high%20performance%20%28average%20SSIM%200.90%2C%20PSNR%2027%29%20across%2026%20internal/external%20validation%20sites%20%2815%2C748%20images%29%2C%20with%20superior%20synthesis%20fidelity%20and%20robustness%20to%20noise%20and%20domain%20shifts.%20Meanwhile%2C%20its%20unified%20representation%20enhances%20downstream%20RT-relevant%20tasks%20%28e.g.%2C%20segmentation%29.%20This%20work%20advances%20digital%20medicine%20solutions%20for%20NPC%20care%20by%20leveraging%20foundation%20models%20to%20bridge%20technical%20synthesis%20and%20clinical%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-to-All%2520MRI%2520Synthesis%253A%2520A%2520Unified%2520Foundation%2520Model%2520for%2520Nasopharyngeal%2520Carcinoma%2520and%2520Its%2520Downstream%2520Applications%26entry.906535625%3DYao%2520Pu%2520and%2520Yiming%2520Shi%2520and%2520Zhenxi%2520Zhang%2520and%2520Peixin%2520Yu%2520and%2520Yitao%2520Zhuang%2520and%2520Xiang%2520Wang%2520and%2520Hongzhao%2520Chen%2520and%2520Jing%2520Cai%2520and%2520Ge%2520Ren%26entry.1292438233%3DMagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520essential%2520for%2520nasopharyngeal%2520carcinoma%2520%2528NPC%2529%2520radiotherapy%2520%2528RT%2529%252C%2520but%2520practical%2520constraints%252C%2520such%2520as%2520patient%2520discomfort%252C%2520long%2520scan%2520times%252C%2520and%2520high%2520costs%2520often%2520lead%2520to%2520incomplete%2520modalities%2520in%2520clinical%2520practice%252C%2520compromising%2520RT%2520planning%2520accuracy.%2520Traditional%2520MRI%2520synthesis%2520methods%2520are%2520modality-specific%252C%2520limited%2520in%2520anatomical%2520adaptability%252C%2520and%2520lack%2520clinical%2520interpretability-failing%2520to%2520meet%2520NPC%2527s%2520RT%2520needs.%2520Here%252C%2520we%2520developed%2520a%2520unified%2520foundation%2520model%2520integrating%2520contrastive%2520visual%2520representation%2520learning%2520and%2520vision-language%2520alignment%2520%2528VLA%2529%2520to%2520enable%2520any-to-all%2520MRI%2520synthesis.%2520The%2520model%2520uses%2520a%2520contrastive%2520encoder%2520for%2520modality-invariant%2520representations%2520and%2520a%2520CLIP-based%2520text-informed%2520decoder%2520for%2520semantically%2520consistent%2520synthesis%252C%2520supporting%2520any-to-all%2520MRI%2520synthesis%2520via%2520one%2520unified%2520foundation%2520model.%2520Trained%2520on%252040%252C825%2520images%2520from%252013%2520institutions%252C%2520it%2520achieves%2520consistently%2520high%2520performance%2520%2528average%2520SSIM%25200.90%252C%2520PSNR%252027%2529%2520across%252026%2520internal/external%2520validation%2520sites%2520%252815%252C748%2520images%2529%252C%2520with%2520superior%2520synthesis%2520fidelity%2520and%2520robustness%2520to%2520noise%2520and%2520domain%2520shifts.%2520Meanwhile%252C%2520its%2520unified%2520representation%2520enhances%2520downstream%2520RT-relevant%2520tasks%2520%2528e.g.%252C%2520segmentation%2529.%2520This%2520work%2520advances%2520digital%2520medicine%2520solutions%2520for%2520NPC%2520care%2520by%2520leveraging%2520foundation%2520models%2520to%2520bridge%2520technical%2520synthesis%2520and%2520clinical%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-to-All%20MRI%20Synthesis%3A%20A%20Unified%20Foundation%20Model%20for%20Nasopharyngeal%20Carcinoma%20and%20Its%20Downstream%20Applications&entry.906535625=Yao%20Pu%20and%20Yiming%20Shi%20and%20Zhenxi%20Zhang%20and%20Peixin%20Yu%20and%20Yitao%20Zhuang%20and%20Xiang%20Wang%20and%20Hongzhao%20Chen%20and%20Jing%20Cai%20and%20Ge%20Ren&entry.1292438233=Magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20nasopharyngeal%20carcinoma%20%28NPC%29%20radiotherapy%20%28RT%29%2C%20but%20practical%20constraints%2C%20such%20as%20patient%20discomfort%2C%20long%20scan%20times%2C%20and%20high%20costs%20often%20lead%20to%20incomplete%20modalities%20in%20clinical%20practice%2C%20compromising%20RT%20planning%20accuracy.%20Traditional%20MRI%20synthesis%20methods%20are%20modality-specific%2C%20limited%20in%20anatomical%20adaptability%2C%20and%20lack%20clinical%20interpretability-failing%20to%20meet%20NPC%27s%20RT%20needs.%20Here%2C%20we%20developed%20a%20unified%20foundation%20model%20integrating%20contrastive%20visual%20representation%20learning%20and%20vision-language%20alignment%20%28VLA%29%20to%20enable%20any-to-all%20MRI%20synthesis.%20The%20model%20uses%20a%20contrastive%20encoder%20for%20modality-invariant%20representations%20and%20a%20CLIP-based%20text-informed%20decoder%20for%20semantically%20consistent%20synthesis%2C%20supporting%20any-to-all%20MRI%20synthesis%20via%20one%20unified%20foundation%20model.%20Trained%20on%2040%2C825%20images%20from%2013%20institutions%2C%20it%20achieves%20consistently%20high%20performance%20%28average%20SSIM%200.90%2C%20PSNR%2027%29%20across%2026%20internal/external%20validation%20sites%20%2815%2C748%20images%29%2C%20with%20superior%20synthesis%20fidelity%20and%20robustness%20to%20noise%20and%20domain%20shifts.%20Meanwhile%2C%20its%20unified%20representation%20enhances%20downstream%20RT-relevant%20tasks%20%28e.g.%2C%20segmentation%29.%20This%20work%20advances%20digital%20medicine%20solutions%20for%20NPC%20care%20by%20leveraging%20foundation%20models%20to%20bridge%20technical%20synthesis%20and%20clinical%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2602.08822v1&entry.124074799=Read"},
{"title": "Training Language Models to Explain Their Own Computations", "author": "Belinda Z. Li and Zifan Carl Guo and Vincent Huang and Jacob Steinhardt and Jacob Andreas", "abstract": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the explainer model is significantly more capable than the target). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods. Code and data at https://github.com/TransluceAI/introspective-interp", "link": "http://arxiv.org/abs/2511.08579v3", "date": "2026-02-09", "relevancy": 2.575, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Language%20Models%20to%20Explain%20Their%20Own%20Computations&body=Title%3A%20Training%20Language%20Models%20to%20Explain%20Their%20Own%20Computations%0AAuthor%3A%20Belinda%20Z.%20Li%20and%20Zifan%20Carl%20Guo%20and%20Vincent%20Huang%20and%20Jacob%20Steinhardt%20and%20Jacob%20Andreas%0AAbstract%3A%20Can%20language%20models%20%28LMs%29%20learn%20to%20faithfully%20describe%20their%20internal%20computations%3F%20Are%20they%20better%20able%20to%20describe%20themselves%20than%20other%20models%3F%20We%20study%20the%20extent%20to%20which%20LMs%27%20privileged%20access%20to%20their%20own%20internals%20can%20be%20leveraged%20to%20produce%20new%20techniques%20for%20explaining%20their%20behavior.%20Using%20existing%20interpretability%20techniques%20as%20a%20source%20of%20ground%20truth%2C%20we%20fine-tune%20LMs%20to%20generate%20natural%20language%20descriptions%20of%20%281%29%20the%20information%20encoded%20by%20LM%20features%2C%20%282%29%20the%20causal%20structure%20of%20LMs%27%20internal%20activations%2C%20and%20%283%29%20the%20influence%20of%20specific%20input%20tokens%20on%20LM%20outputs.%20When%20trained%20with%20only%20tens%20of%20thousands%20of%20example%20explanations%2C%20explainer%20models%20exhibit%20non-trivial%20generalization%20to%20new%20queries.%20This%20generalization%20appears%20partly%20attributable%20to%20explainer%20models%27%20privileged%20access%20to%20their%20own%20internals%3A%20using%20a%20model%20to%20explain%20its%20own%20computations%20generally%20works%20better%20than%20using%20a%20%2Adifferent%2A%20model%20to%20explain%20its%20computations%20%28even%20if%20the%20explainer%20model%20is%20significantly%20more%20capable%20than%20the%20target%29.%20Our%20results%20suggest%20not%20only%20that%20LMs%20can%20learn%20to%20reliably%20explain%20their%20internal%20computations%2C%20but%20that%20such%20explanations%20offer%20a%20scalable%20complement%20to%20existing%20interpretability%20methods.%20Code%20and%20data%20at%20https%3A//github.com/TransluceAI/introspective-interp%0ALink%3A%20http%3A//arxiv.org/abs/2511.08579v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Language%2520Models%2520to%2520Explain%2520Their%2520Own%2520Computations%26entry.906535625%3DBelinda%2520Z.%2520Li%2520and%2520Zifan%2520Carl%2520Guo%2520and%2520Vincent%2520Huang%2520and%2520Jacob%2520Steinhardt%2520and%2520Jacob%2520Andreas%26entry.1292438233%3DCan%2520language%2520models%2520%2528LMs%2529%2520learn%2520to%2520faithfully%2520describe%2520their%2520internal%2520computations%253F%2520Are%2520they%2520better%2520able%2520to%2520describe%2520themselves%2520than%2520other%2520models%253F%2520We%2520study%2520the%2520extent%2520to%2520which%2520LMs%2527%2520privileged%2520access%2520to%2520their%2520own%2520internals%2520can%2520be%2520leveraged%2520to%2520produce%2520new%2520techniques%2520for%2520explaining%2520their%2520behavior.%2520Using%2520existing%2520interpretability%2520techniques%2520as%2520a%2520source%2520of%2520ground%2520truth%252C%2520we%2520fine-tune%2520LMs%2520to%2520generate%2520natural%2520language%2520descriptions%2520of%2520%25281%2529%2520the%2520information%2520encoded%2520by%2520LM%2520features%252C%2520%25282%2529%2520the%2520causal%2520structure%2520of%2520LMs%2527%2520internal%2520activations%252C%2520and%2520%25283%2529%2520the%2520influence%2520of%2520specific%2520input%2520tokens%2520on%2520LM%2520outputs.%2520When%2520trained%2520with%2520only%2520tens%2520of%2520thousands%2520of%2520example%2520explanations%252C%2520explainer%2520models%2520exhibit%2520non-trivial%2520generalization%2520to%2520new%2520queries.%2520This%2520generalization%2520appears%2520partly%2520attributable%2520to%2520explainer%2520models%2527%2520privileged%2520access%2520to%2520their%2520own%2520internals%253A%2520using%2520a%2520model%2520to%2520explain%2520its%2520own%2520computations%2520generally%2520works%2520better%2520than%2520using%2520a%2520%252Adifferent%252A%2520model%2520to%2520explain%2520its%2520computations%2520%2528even%2520if%2520the%2520explainer%2520model%2520is%2520significantly%2520more%2520capable%2520than%2520the%2520target%2529.%2520Our%2520results%2520suggest%2520not%2520only%2520that%2520LMs%2520can%2520learn%2520to%2520reliably%2520explain%2520their%2520internal%2520computations%252C%2520but%2520that%2520such%2520explanations%2520offer%2520a%2520scalable%2520complement%2520to%2520existing%2520interpretability%2520methods.%2520Code%2520and%2520data%2520at%2520https%253A//github.com/TransluceAI/introspective-interp%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08579v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Language%20Models%20to%20Explain%20Their%20Own%20Computations&entry.906535625=Belinda%20Z.%20Li%20and%20Zifan%20Carl%20Guo%20and%20Vincent%20Huang%20and%20Jacob%20Steinhardt%20and%20Jacob%20Andreas&entry.1292438233=Can%20language%20models%20%28LMs%29%20learn%20to%20faithfully%20describe%20their%20internal%20computations%3F%20Are%20they%20better%20able%20to%20describe%20themselves%20than%20other%20models%3F%20We%20study%20the%20extent%20to%20which%20LMs%27%20privileged%20access%20to%20their%20own%20internals%20can%20be%20leveraged%20to%20produce%20new%20techniques%20for%20explaining%20their%20behavior.%20Using%20existing%20interpretability%20techniques%20as%20a%20source%20of%20ground%20truth%2C%20we%20fine-tune%20LMs%20to%20generate%20natural%20language%20descriptions%20of%20%281%29%20the%20information%20encoded%20by%20LM%20features%2C%20%282%29%20the%20causal%20structure%20of%20LMs%27%20internal%20activations%2C%20and%20%283%29%20the%20influence%20of%20specific%20input%20tokens%20on%20LM%20outputs.%20When%20trained%20with%20only%20tens%20of%20thousands%20of%20example%20explanations%2C%20explainer%20models%20exhibit%20non-trivial%20generalization%20to%20new%20queries.%20This%20generalization%20appears%20partly%20attributable%20to%20explainer%20models%27%20privileged%20access%20to%20their%20own%20internals%3A%20using%20a%20model%20to%20explain%20its%20own%20computations%20generally%20works%20better%20than%20using%20a%20%2Adifferent%2A%20model%20to%20explain%20its%20computations%20%28even%20if%20the%20explainer%20model%20is%20significantly%20more%20capable%20than%20the%20target%29.%20Our%20results%20suggest%20not%20only%20that%20LMs%20can%20learn%20to%20reliably%20explain%20their%20internal%20computations%2C%20but%20that%20such%20explanations%20offer%20a%20scalable%20complement%20to%20existing%20interpretability%20methods.%20Code%20and%20data%20at%20https%3A//github.com/TransluceAI/introspective-interp&entry.1838667208=http%3A//arxiv.org/abs/2511.08579v3&entry.124074799=Read"},
{"title": "A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches", "author": "Luca Ciampi and Ali Azmoudeh and Elif Ecem Akbaba and Erdi Sar\u0131ta\u015f and Ziya Ata Yaz\u0131c\u0131 and Haz\u0131m Kemal Ekenel and Giuseppe Amato and Fabrizio Falchi", "abstract": "Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories, a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of 30 CAC architectures and report their performance on gold-standard benchmarks, discussing key strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions.", "link": "http://arxiv.org/abs/2501.19184v4", "date": "2026-02-09", "relevancy": 2.5663, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%20to%20Open-World%20Text-Guided%20Approaches&body=Title%3A%20A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%20to%20Open-World%20Text-Guided%20Approaches%0AAuthor%3A%20Luca%20Ciampi%20and%20Ali%20Azmoudeh%20and%20Elif%20Ecem%20Akbaba%20and%20Erdi%20Sar%C4%B1ta%C5%9F%20and%20Ziya%20Ata%20Yaz%C4%B1c%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20Visual%20object%20counting%20has%20recently%20shifted%20towards%20class-agnostic%20counting%20%28CAC%29%2C%20which%20addresses%20the%20challenge%20of%20counting%20objects%20across%20arbitrary%20categories%2C%20a%20crucial%20capability%20for%20flexible%20and%20generalizable%20counting%20systems.%20Unlike%20humans%2C%20who%20effortlessly%20identify%20and%20count%20objects%20from%20diverse%20categories%20without%20prior%20knowledge%2C%20most%20existing%20counting%20methods%20are%20restricted%20to%20enumerating%20instances%20of%20known%20classes%2C%20requiring%20extensive%20labeled%20datasets%20for%20training%20and%20struggling%20in%20open-vocabulary%20settings.%20In%20contrast%2C%20CAC%20aims%20to%20count%20objects%20belonging%20to%20classes%20never%20seen%20during%20training%2C%20operating%20in%20a%20few-shot%20setting.%20In%20this%20paper%2C%20we%20present%20the%20first%20comprehensive%20review%20of%20CAC%20methodologies.%20We%20propose%20a%20taxonomy%20to%20categorize%20CAC%20approaches%20into%20three%20paradigms%20based%20on%20how%20target%20object%20classes%20can%20be%20specified%3A%20reference-based%2C%20reference-less%2C%20and%20open-world%20text-guided.%20Reference-based%20approaches%20achieve%20state-of-the-art%20performance%20by%20relying%20on%20exemplar-guided%20mechanisms.%20Reference-less%20methods%20eliminate%20exemplar%20dependency%20by%20leveraging%20inherent%20image%20patterns.%20Finally%2C%20open-world%20text-guided%20methods%20use%20vision-language%20models%2C%20enabling%20object%20class%20descriptions%20via%20textual%20prompts%2C%20offering%20a%20flexible%20and%20promising%20solution.%20Based%20on%20this%20taxonomy%2C%20we%20provide%20an%20overview%20of%2030%20CAC%20architectures%20and%20report%20their%20performance%20on%20gold-standard%20benchmarks%2C%20discussing%20key%20strengths%20and%20limitations.%20Specifically%2C%20we%20present%20results%20on%20the%20FSC-147%20dataset%2C%20setting%20a%20leaderboard%20using%20gold-standard%20metrics%2C%20and%20on%20the%20CARPK%20dataset%20to%20assess%20generalization%20capabilities.%20Finally%2C%20we%20offer%20a%20critical%20discussion%20of%20persistent%20challenges%2C%20such%20as%20annotation%20dependency%20and%20generalization%2C%20alongside%20future%20directions.%0ALink%3A%20http%3A//arxiv.org/abs/2501.19184v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Class-Agnostic%2520Counting%253A%2520Advancements%2520from%2520Reference-Based%2520to%2520Open-World%2520Text-Guided%2520Approaches%26entry.906535625%3DLuca%2520Ciampi%2520and%2520Ali%2520Azmoudeh%2520and%2520Elif%2520Ecem%2520Akbaba%2520and%2520Erdi%2520Sar%25C4%25B1ta%25C5%259F%2520and%2520Ziya%2520Ata%2520Yaz%25C4%25B1c%25C4%25B1%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3DVisual%2520object%2520counting%2520has%2520recently%2520shifted%2520towards%2520class-agnostic%2520counting%2520%2528CAC%2529%252C%2520which%2520addresses%2520the%2520challenge%2520of%2520counting%2520objects%2520across%2520arbitrary%2520categories%252C%2520a%2520crucial%2520capability%2520for%2520flexible%2520and%2520generalizable%2520counting%2520systems.%2520Unlike%2520humans%252C%2520who%2520effortlessly%2520identify%2520and%2520count%2520objects%2520from%2520diverse%2520categories%2520without%2520prior%2520knowledge%252C%2520most%2520existing%2520counting%2520methods%2520are%2520restricted%2520to%2520enumerating%2520instances%2520of%2520known%2520classes%252C%2520requiring%2520extensive%2520labeled%2520datasets%2520for%2520training%2520and%2520struggling%2520in%2520open-vocabulary%2520settings.%2520In%2520contrast%252C%2520CAC%2520aims%2520to%2520count%2520objects%2520belonging%2520to%2520classes%2520never%2520seen%2520during%2520training%252C%2520operating%2520in%2520a%2520few-shot%2520setting.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520review%2520of%2520CAC%2520methodologies.%2520We%2520propose%2520a%2520taxonomy%2520to%2520categorize%2520CAC%2520approaches%2520into%2520three%2520paradigms%2520based%2520on%2520how%2520target%2520object%2520classes%2520can%2520be%2520specified%253A%2520reference-based%252C%2520reference-less%252C%2520and%2520open-world%2520text-guided.%2520Reference-based%2520approaches%2520achieve%2520state-of-the-art%2520performance%2520by%2520relying%2520on%2520exemplar-guided%2520mechanisms.%2520Reference-less%2520methods%2520eliminate%2520exemplar%2520dependency%2520by%2520leveraging%2520inherent%2520image%2520patterns.%2520Finally%252C%2520open-world%2520text-guided%2520methods%2520use%2520vision-language%2520models%252C%2520enabling%2520object%2520class%2520descriptions%2520via%2520textual%2520prompts%252C%2520offering%2520a%2520flexible%2520and%2520promising%2520solution.%2520Based%2520on%2520this%2520taxonomy%252C%2520we%2520provide%2520an%2520overview%2520of%252030%2520CAC%2520architectures%2520and%2520report%2520their%2520performance%2520on%2520gold-standard%2520benchmarks%252C%2520discussing%2520key%2520strengths%2520and%2520limitations.%2520Specifically%252C%2520we%2520present%2520results%2520on%2520the%2520FSC-147%2520dataset%252C%2520setting%2520a%2520leaderboard%2520using%2520gold-standard%2520metrics%252C%2520and%2520on%2520the%2520CARPK%2520dataset%2520to%2520assess%2520generalization%2520capabilities.%2520Finally%252C%2520we%2520offer%2520a%2520critical%2520discussion%2520of%2520persistent%2520challenges%252C%2520such%2520as%2520annotation%2520dependency%2520and%2520generalization%252C%2520alongside%2520future%2520directions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19184v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%20to%20Open-World%20Text-Guided%20Approaches&entry.906535625=Luca%20Ciampi%20and%20Ali%20Azmoudeh%20and%20Elif%20Ecem%20Akbaba%20and%20Erdi%20Sar%C4%B1ta%C5%9F%20and%20Ziya%20Ata%20Yaz%C4%B1c%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=Visual%20object%20counting%20has%20recently%20shifted%20towards%20class-agnostic%20counting%20%28CAC%29%2C%20which%20addresses%20the%20challenge%20of%20counting%20objects%20across%20arbitrary%20categories%2C%20a%20crucial%20capability%20for%20flexible%20and%20generalizable%20counting%20systems.%20Unlike%20humans%2C%20who%20effortlessly%20identify%20and%20count%20objects%20from%20diverse%20categories%20without%20prior%20knowledge%2C%20most%20existing%20counting%20methods%20are%20restricted%20to%20enumerating%20instances%20of%20known%20classes%2C%20requiring%20extensive%20labeled%20datasets%20for%20training%20and%20struggling%20in%20open-vocabulary%20settings.%20In%20contrast%2C%20CAC%20aims%20to%20count%20objects%20belonging%20to%20classes%20never%20seen%20during%20training%2C%20operating%20in%20a%20few-shot%20setting.%20In%20this%20paper%2C%20we%20present%20the%20first%20comprehensive%20review%20of%20CAC%20methodologies.%20We%20propose%20a%20taxonomy%20to%20categorize%20CAC%20approaches%20into%20three%20paradigms%20based%20on%20how%20target%20object%20classes%20can%20be%20specified%3A%20reference-based%2C%20reference-less%2C%20and%20open-world%20text-guided.%20Reference-based%20approaches%20achieve%20state-of-the-art%20performance%20by%20relying%20on%20exemplar-guided%20mechanisms.%20Reference-less%20methods%20eliminate%20exemplar%20dependency%20by%20leveraging%20inherent%20image%20patterns.%20Finally%2C%20open-world%20text-guided%20methods%20use%20vision-language%20models%2C%20enabling%20object%20class%20descriptions%20via%20textual%20prompts%2C%20offering%20a%20flexible%20and%20promising%20solution.%20Based%20on%20this%20taxonomy%2C%20we%20provide%20an%20overview%20of%2030%20CAC%20architectures%20and%20report%20their%20performance%20on%20gold-standard%20benchmarks%2C%20discussing%20key%20strengths%20and%20limitations.%20Specifically%2C%20we%20present%20results%20on%20the%20FSC-147%20dataset%2C%20setting%20a%20leaderboard%20using%20gold-standard%20metrics%2C%20and%20on%20the%20CARPK%20dataset%20to%20assess%20generalization%20capabilities.%20Finally%2C%20we%20offer%20a%20critical%20discussion%20of%20persistent%20challenges%2C%20such%20as%20annotation%20dependency%20and%20generalization%2C%20alongside%20future%20directions.&entry.1838667208=http%3A//arxiv.org/abs/2501.19184v4&entry.124074799=Read"},
{"title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling", "author": "Yi Dao and Lankai Zhang and Hao Liu and Haiwei Zhang and Wenbo Wang", "abstract": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.", "link": "http://arxiv.org/abs/2602.08661v1", "date": "2026-02-09", "relevancy": 2.5633, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5373}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5028}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WiFlow%3A%20A%20Lightweight%20WiFi-based%20Continuous%20Human%20Pose%20Estimation%20Network%20with%20Spatio-Temporal%20Feature%20Decoupling&body=Title%3A%20WiFlow%3A%20A%20Lightweight%20WiFi-based%20Continuous%20Human%20Pose%20Estimation%20Network%20with%20Spatio-Temporal%20Feature%20Decoupling%0AAuthor%3A%20Yi%20Dao%20and%20Lankai%20Zhang%20and%20Hao%20Liu%20and%20Haiwei%20Zhang%20and%20Wenbo%20Wang%0AAbstract%3A%20Human%20pose%20estimation%20is%20fundamental%20to%20intelligent%20perception%20in%20the%20Internet%20of%20Things%20%28IoT%29%2C%20enabling%20applications%20ranging%20from%20smart%20healthcare%20to%20human-computer%20interaction.%20While%20WiFi-based%20methods%20have%20gained%20traction%2C%20they%20often%20struggle%20with%20continuous%20motion%20and%20high%20computational%20overhead.%20This%20work%20presents%20WiFlow%2C%20a%20novel%20framework%20for%20continuous%20human%20pose%20estimation%20using%20WiFi%20signals.%20Unlike%20vision-based%20approaches%20such%20as%20two-dimensional%20deep%20residual%20networks%20that%20treat%20Channel%20State%20Information%20%28CSI%29%20as%20images%2C%20WiFlow%20employs%20an%20encoder-decoder%20architecture.%20The%20encoder%20captures%20spatio-temporal%20features%20of%20CSI%20using%20temporal%20and%20asymmetric%20convolutions%2C%20preserving%20the%20original%20sequential%20structure%20of%20signals.%20It%20then%20refines%20keypoint%20features%20of%20human%20bodies%20to%20be%20tracked%20and%20capture%20their%20structural%20dependencies%20via%20axial%20attention.%20The%20decoder%20subsequently%20maps%20the%20encoded%20high-dimensional%20features%20into%20keypoint%20coordinates.%20Trained%20on%20a%20self-collected%20dataset%20of%20360%2C000%20synchronized%20CSI-pose%20samples%20from%205%20subjects%20performing%20continuous%20sequences%20of%208%20daily%20activities%2C%20WiFlow%20achieves%20a%20Percentage%20of%20Correct%20Keypoints%20%28PCK%29%20of%2097.00%25%20at%20a%20threshold%20of%2020%25%20%28PCK%4020%29%20and%2099.48%25%20at%20PCK%4050%2C%20with%20a%20mean%20per-joint%20position%20error%20of%200.008m.%20With%20only%204.82M%20parameters%2C%20WiFlow%20significantly%20reduces%20model%20complexity%20and%20computational%20cost%2C%20establishing%20a%20new%20performance%20baseline%20for%20practical%20WiFi-based%20human%20pose%20estimation.%20Our%20code%20and%20datasets%20are%20available%20at%20https%3A//github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWiFlow%253A%2520A%2520Lightweight%2520WiFi-based%2520Continuous%2520Human%2520Pose%2520Estimation%2520Network%2520with%2520Spatio-Temporal%2520Feature%2520Decoupling%26entry.906535625%3DYi%2520Dao%2520and%2520Lankai%2520Zhang%2520and%2520Hao%2520Liu%2520and%2520Haiwei%2520Zhang%2520and%2520Wenbo%2520Wang%26entry.1292438233%3DHuman%2520pose%2520estimation%2520is%2520fundamental%2520to%2520intelligent%2520perception%2520in%2520the%2520Internet%2520of%2520Things%2520%2528IoT%2529%252C%2520enabling%2520applications%2520ranging%2520from%2520smart%2520healthcare%2520to%2520human-computer%2520interaction.%2520While%2520WiFi-based%2520methods%2520have%2520gained%2520traction%252C%2520they%2520often%2520struggle%2520with%2520continuous%2520motion%2520and%2520high%2520computational%2520overhead.%2520This%2520work%2520presents%2520WiFlow%252C%2520a%2520novel%2520framework%2520for%2520continuous%2520human%2520pose%2520estimation%2520using%2520WiFi%2520signals.%2520Unlike%2520vision-based%2520approaches%2520such%2520as%2520two-dimensional%2520deep%2520residual%2520networks%2520that%2520treat%2520Channel%2520State%2520Information%2520%2528CSI%2529%2520as%2520images%252C%2520WiFlow%2520employs%2520an%2520encoder-decoder%2520architecture.%2520The%2520encoder%2520captures%2520spatio-temporal%2520features%2520of%2520CSI%2520using%2520temporal%2520and%2520asymmetric%2520convolutions%252C%2520preserving%2520the%2520original%2520sequential%2520structure%2520of%2520signals.%2520It%2520then%2520refines%2520keypoint%2520features%2520of%2520human%2520bodies%2520to%2520be%2520tracked%2520and%2520capture%2520their%2520structural%2520dependencies%2520via%2520axial%2520attention.%2520The%2520decoder%2520subsequently%2520maps%2520the%2520encoded%2520high-dimensional%2520features%2520into%2520keypoint%2520coordinates.%2520Trained%2520on%2520a%2520self-collected%2520dataset%2520of%2520360%252C000%2520synchronized%2520CSI-pose%2520samples%2520from%25205%2520subjects%2520performing%2520continuous%2520sequences%2520of%25208%2520daily%2520activities%252C%2520WiFlow%2520achieves%2520a%2520Percentage%2520of%2520Correct%2520Keypoints%2520%2528PCK%2529%2520of%252097.00%2525%2520at%2520a%2520threshold%2520of%252020%2525%2520%2528PCK%254020%2529%2520and%252099.48%2525%2520at%2520PCK%254050%252C%2520with%2520a%2520mean%2520per-joint%2520position%2520error%2520of%25200.008m.%2520With%2520only%25204.82M%2520parameters%252C%2520WiFlow%2520significantly%2520reduces%2520model%2520complexity%2520and%2520computational%2520cost%252C%2520establishing%2520a%2520new%2520performance%2520baseline%2520for%2520practical%2520WiFi-based%2520human%2520pose%2520estimation.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%2520https%253A//github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WiFlow%3A%20A%20Lightweight%20WiFi-based%20Continuous%20Human%20Pose%20Estimation%20Network%20with%20Spatio-Temporal%20Feature%20Decoupling&entry.906535625=Yi%20Dao%20and%20Lankai%20Zhang%20and%20Hao%20Liu%20and%20Haiwei%20Zhang%20and%20Wenbo%20Wang&entry.1292438233=Human%20pose%20estimation%20is%20fundamental%20to%20intelligent%20perception%20in%20the%20Internet%20of%20Things%20%28IoT%29%2C%20enabling%20applications%20ranging%20from%20smart%20healthcare%20to%20human-computer%20interaction.%20While%20WiFi-based%20methods%20have%20gained%20traction%2C%20they%20often%20struggle%20with%20continuous%20motion%20and%20high%20computational%20overhead.%20This%20work%20presents%20WiFlow%2C%20a%20novel%20framework%20for%20continuous%20human%20pose%20estimation%20using%20WiFi%20signals.%20Unlike%20vision-based%20approaches%20such%20as%20two-dimensional%20deep%20residual%20networks%20that%20treat%20Channel%20State%20Information%20%28CSI%29%20as%20images%2C%20WiFlow%20employs%20an%20encoder-decoder%20architecture.%20The%20encoder%20captures%20spatio-temporal%20features%20of%20CSI%20using%20temporal%20and%20asymmetric%20convolutions%2C%20preserving%20the%20original%20sequential%20structure%20of%20signals.%20It%20then%20refines%20keypoint%20features%20of%20human%20bodies%20to%20be%20tracked%20and%20capture%20their%20structural%20dependencies%20via%20axial%20attention.%20The%20decoder%20subsequently%20maps%20the%20encoded%20high-dimensional%20features%20into%20keypoint%20coordinates.%20Trained%20on%20a%20self-collected%20dataset%20of%20360%2C000%20synchronized%20CSI-pose%20samples%20from%205%20subjects%20performing%20continuous%20sequences%20of%208%20daily%20activities%2C%20WiFlow%20achieves%20a%20Percentage%20of%20Correct%20Keypoints%20%28PCK%29%20of%2097.00%25%20at%20a%20threshold%20of%2020%25%20%28PCK%4020%29%20and%2099.48%25%20at%20PCK%4050%2C%20with%20a%20mean%20per-joint%20position%20error%20of%200.008m.%20With%20only%204.82M%20parameters%2C%20WiFlow%20significantly%20reduces%20model%20complexity%20and%20computational%20cost%2C%20establishing%20a%20new%20performance%20baseline%20for%20practical%20WiFi-based%20human%20pose%20estimation.%20Our%20code%20and%20datasets%20are%20available%20at%20https%3A//github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.&entry.1838667208=http%3A//arxiv.org/abs/2602.08661v1&entry.124074799=Read"},
{"title": "NRR-Core: Non-Resolution Reasoning as a Computational Framework for Contextual Identity and Ambiguity Preservation", "author": "Kei Saito", "abstract": "Current artificial intelligence systems exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse--collapsing multiple valid interpretations into single outputs--stems from classical identity assumptions in neural architectures. We propose Non-Resolution Reasoning (NRR), a framework treating ambiguity retention as a valid reasoning mode. NRR introduces three principles: (1) Non-Identity ($A \\neq A$)--the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$)--entities share partial structural overlap without being identical; (3) Non-Resolution--conflicting interpretations coexist without forced convergence. We formalize these through Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \\neq A$ across inference. We illustrate NRR through case studies in paradox handling, creative generation, and context-dependent reasoning. Functional verification in a synthetic two-turn disambiguation task shows NRR-lite maintains high entropy ($H = 0.91$ bits, near-maximum $1.0$) at ambiguous turns while standard architectures collapse early ($H = 0.15$ bits), preserving interpretive flexibility until context arrives. NRR challenges the assumption that meaning must collapse to be useful. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "link": "http://arxiv.org/abs/2512.13478v8", "date": "2026-02-09", "relevancy": 2.5436, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NRR-Core%3A%20Non-Resolution%20Reasoning%20as%20a%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation&body=Title%3A%20NRR-Core%3A%20Non-Resolution%20Reasoning%20as%20a%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation%0AAuthor%3A%20Kei%20Saito%0AAbstract%3A%20Current%20artificial%20intelligence%20systems%20exhibit%20a%20fundamental%20architectural%20limitation%3A%20they%20resolve%20ambiguity%20prematurely.%20This%20premature%20semantic%20collapse--collapsing%20multiple%20valid%20interpretations%20into%20single%20outputs--stems%20from%20classical%20identity%20assumptions%20in%20neural%20architectures.%20We%20propose%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20a%20framework%20treating%20ambiguity%20retention%20as%20a%20valid%20reasoning%20mode.%20NRR%20introduces%20three%20principles%3A%20%281%29%20Non-Identity%20%28%24A%20%5Cneq%20A%24%29--the%20same%20symbol%20refers%20to%20different%20entities%20across%20contexts%3B%20%282%29%20Approximate%20Identity%20%28%24A%20%5Capprox%20A%24%29--entities%20share%20partial%20structural%20overlap%20without%20being%20identical%3B%20%283%29%20Non-Resolution--conflicting%20interpretations%20coexist%20without%20forced%20convergence.%20We%20formalize%20these%20through%20Multi-Vector%20Embeddings%20for%20context-dependent%20representation%2C%20Non-Collapsing%20Attention%20for%20parallel%20interpretation%20retention%2C%20and%20Contextual%20Identity%20Tracking%20%28CIT%29%20for%20maintaining%20%24A%20%5Cneq%20A%24%20across%20inference.%20We%20illustrate%20NRR%20through%20case%20studies%20in%20paradox%20handling%2C%20creative%20generation%2C%20and%20context-dependent%20reasoning.%20Functional%20verification%20in%20a%20synthetic%20two-turn%20disambiguation%20task%20shows%20NRR-lite%20maintains%20high%20entropy%20%28%24H%20%3D%200.91%24%20bits%2C%20near-maximum%20%241.0%24%29%20at%20ambiguous%20turns%20while%20standard%20architectures%20collapse%20early%20%28%24H%20%3D%200.15%24%20bits%29%2C%20preserving%20interpretive%20flexibility%20until%20context%20arrives.%20NRR%20challenges%20the%20assumption%20that%20meaning%20must%20collapse%20to%20be%20useful.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13478v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNRR-Core%253A%2520Non-Resolution%2520Reasoning%2520as%2520a%2520Computational%2520Framework%2520for%2520Contextual%2520Identity%2520and%2520Ambiguity%2520Preservation%26entry.906535625%3DKei%2520Saito%26entry.1292438233%3DCurrent%2520artificial%2520intelligence%2520systems%2520exhibit%2520a%2520fundamental%2520architectural%2520limitation%253A%2520they%2520resolve%2520ambiguity%2520prematurely.%2520This%2520premature%2520semantic%2520collapse--collapsing%2520multiple%2520valid%2520interpretations%2520into%2520single%2520outputs--stems%2520from%2520classical%2520identity%2520assumptions%2520in%2520neural%2520architectures.%2520We%2520propose%2520Non-Resolution%2520Reasoning%2520%2528NRR%2529%252C%2520a%2520framework%2520treating%2520ambiguity%2520retention%2520as%2520a%2520valid%2520reasoning%2520mode.%2520NRR%2520introduces%2520three%2520principles%253A%2520%25281%2529%2520Non-Identity%2520%2528%2524A%2520%255Cneq%2520A%2524%2529--the%2520same%2520symbol%2520refers%2520to%2520different%2520entities%2520across%2520contexts%253B%2520%25282%2529%2520Approximate%2520Identity%2520%2528%2524A%2520%255Capprox%2520A%2524%2529--entities%2520share%2520partial%2520structural%2520overlap%2520without%2520being%2520identical%253B%2520%25283%2529%2520Non-Resolution--conflicting%2520interpretations%2520coexist%2520without%2520forced%2520convergence.%2520We%2520formalize%2520these%2520through%2520Multi-Vector%2520Embeddings%2520for%2520context-dependent%2520representation%252C%2520Non-Collapsing%2520Attention%2520for%2520parallel%2520interpretation%2520retention%252C%2520and%2520Contextual%2520Identity%2520Tracking%2520%2528CIT%2529%2520for%2520maintaining%2520%2524A%2520%255Cneq%2520A%2524%2520across%2520inference.%2520We%2520illustrate%2520NRR%2520through%2520case%2520studies%2520in%2520paradox%2520handling%252C%2520creative%2520generation%252C%2520and%2520context-dependent%2520reasoning.%2520Functional%2520verification%2520in%2520a%2520synthetic%2520two-turn%2520disambiguation%2520task%2520shows%2520NRR-lite%2520maintains%2520high%2520entropy%2520%2528%2524H%2520%253D%25200.91%2524%2520bits%252C%2520near-maximum%2520%25241.0%2524%2529%2520at%2520ambiguous%2520turns%2520while%2520standard%2520architectures%2520collapse%2520early%2520%2528%2524H%2520%253D%25200.15%2524%2520bits%2529%252C%2520preserving%2520interpretive%2520flexibility%2520until%2520context%2520arrives.%2520NRR%2520challenges%2520the%2520assumption%2520that%2520meaning%2520must%2520collapse%2520to%2520be%2520useful.%2520The%2520question%2520is%2520not%2520whether%2520AI%2520should%2520resolve%2520ambiguity%252C%2520but%2520when%252C%2520how%252C%2520and%2520under%2520whose%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13478v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NRR-Core%3A%20Non-Resolution%20Reasoning%20as%20a%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation&entry.906535625=Kei%20Saito&entry.1292438233=Current%20artificial%20intelligence%20systems%20exhibit%20a%20fundamental%20architectural%20limitation%3A%20they%20resolve%20ambiguity%20prematurely.%20This%20premature%20semantic%20collapse--collapsing%20multiple%20valid%20interpretations%20into%20single%20outputs--stems%20from%20classical%20identity%20assumptions%20in%20neural%20architectures.%20We%20propose%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20a%20framework%20treating%20ambiguity%20retention%20as%20a%20valid%20reasoning%20mode.%20NRR%20introduces%20three%20principles%3A%20%281%29%20Non-Identity%20%28%24A%20%5Cneq%20A%24%29--the%20same%20symbol%20refers%20to%20different%20entities%20across%20contexts%3B%20%282%29%20Approximate%20Identity%20%28%24A%20%5Capprox%20A%24%29--entities%20share%20partial%20structural%20overlap%20without%20being%20identical%3B%20%283%29%20Non-Resolution--conflicting%20interpretations%20coexist%20without%20forced%20convergence.%20We%20formalize%20these%20through%20Multi-Vector%20Embeddings%20for%20context-dependent%20representation%2C%20Non-Collapsing%20Attention%20for%20parallel%20interpretation%20retention%2C%20and%20Contextual%20Identity%20Tracking%20%28CIT%29%20for%20maintaining%20%24A%20%5Cneq%20A%24%20across%20inference.%20We%20illustrate%20NRR%20through%20case%20studies%20in%20paradox%20handling%2C%20creative%20generation%2C%20and%20context-dependent%20reasoning.%20Functional%20verification%20in%20a%20synthetic%20two-turn%20disambiguation%20task%20shows%20NRR-lite%20maintains%20high%20entropy%20%28%24H%20%3D%200.91%24%20bits%2C%20near-maximum%20%241.0%24%29%20at%20ambiguous%20turns%20while%20standard%20architectures%20collapse%20early%20%28%24H%20%3D%200.15%24%20bits%29%2C%20preserving%20interpretive%20flexibility%20until%20context%20arrives.%20NRR%20challenges%20the%20assumption%20that%20meaning%20must%20collapse%20to%20be%20useful.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.13478v8&entry.124074799=Read"},
{"title": "Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning", "author": "Isaac Xu and Martin Gillis and Ayushi Sharma and Benjamin Misiuk and Craig J. Brown and Thomas Trappenberg", "abstract": "In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.", "link": "http://arxiv.org/abs/2602.08986v1", "date": "2026-02-09", "relevancy": 2.5373, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5165}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5095}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Detection%20of%20Rare%20Nodes%20in%20Hierarchical%20Multi-Label%20Learning&body=Title%3A%20Improving%20Detection%20of%20Rare%20Nodes%20in%20Hierarchical%20Multi-Label%20Learning%0AAuthor%3A%20Isaac%20Xu%20and%20Martin%20Gillis%20and%20Ayushi%20Sharma%20and%20Benjamin%20Misiuk%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg%0AAbstract%3A%20In%20hierarchical%20multi-label%20classification%2C%20a%20persistent%20challenge%20is%20enabling%20model%20predictions%20to%20reach%20deeper%20levels%20of%20the%20hierarchy%20for%20more%20detailed%20or%20fine-grained%20classifications.%20This%20difficulty%20partly%20arises%20from%20the%20natural%20rarity%20of%20certain%20classes%20%28or%20hierarchical%20nodes%29%20and%20the%20hierarchical%20constraint%20that%20ensures%20child%20nodes%20are%20almost%20always%20less%20frequent%20than%20their%20parents.%20To%20address%20this%2C%20we%20propose%20a%20weighted%20loss%20objective%20for%20neural%20networks%20that%20combines%20node-wise%20imbalance%20weighting%20with%20focal%20weighting%20components%2C%20the%20latter%20leveraging%20modern%20quantification%20of%20ensemble%20uncertainties.%20By%20emphasizing%20rare%20nodes%20rather%20than%20rare%20observations%20%28data%20points%29%2C%20and%20focusing%20on%20uncertain%20nodes%20for%20each%20model%20output%20distribution%20during%20training%2C%20we%20observe%20improvements%20in%20recall%20by%20up%20to%20a%20factor%20of%20five%20on%20benchmark%20datasets%2C%20along%20with%20statistically%20significant%20gains%20in%20%24F_%7B1%7D%24%20score.%20We%20also%20show%20our%20approach%20aids%20convolutional%20networks%20on%20challenging%20tasks%2C%20as%20in%20situations%20with%20suboptimal%20encoders%20or%20limited%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Detection%2520of%2520Rare%2520Nodes%2520in%2520Hierarchical%2520Multi-Label%2520Learning%26entry.906535625%3DIsaac%2520Xu%2520and%2520Martin%2520Gillis%2520and%2520Ayushi%2520Sharma%2520and%2520Benjamin%2520Misiuk%2520and%2520Craig%2520J.%2520Brown%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3DIn%2520hierarchical%2520multi-label%2520classification%252C%2520a%2520persistent%2520challenge%2520is%2520enabling%2520model%2520predictions%2520to%2520reach%2520deeper%2520levels%2520of%2520the%2520hierarchy%2520for%2520more%2520detailed%2520or%2520fine-grained%2520classifications.%2520This%2520difficulty%2520partly%2520arises%2520from%2520the%2520natural%2520rarity%2520of%2520certain%2520classes%2520%2528or%2520hierarchical%2520nodes%2529%2520and%2520the%2520hierarchical%2520constraint%2520that%2520ensures%2520child%2520nodes%2520are%2520almost%2520always%2520less%2520frequent%2520than%2520their%2520parents.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520weighted%2520loss%2520objective%2520for%2520neural%2520networks%2520that%2520combines%2520node-wise%2520imbalance%2520weighting%2520with%2520focal%2520weighting%2520components%252C%2520the%2520latter%2520leveraging%2520modern%2520quantification%2520of%2520ensemble%2520uncertainties.%2520By%2520emphasizing%2520rare%2520nodes%2520rather%2520than%2520rare%2520observations%2520%2528data%2520points%2529%252C%2520and%2520focusing%2520on%2520uncertain%2520nodes%2520for%2520each%2520model%2520output%2520distribution%2520during%2520training%252C%2520we%2520observe%2520improvements%2520in%2520recall%2520by%2520up%2520to%2520a%2520factor%2520of%2520five%2520on%2520benchmark%2520datasets%252C%2520along%2520with%2520statistically%2520significant%2520gains%2520in%2520%2524F_%257B1%257D%2524%2520score.%2520We%2520also%2520show%2520our%2520approach%2520aids%2520convolutional%2520networks%2520on%2520challenging%2520tasks%252C%2520as%2520in%2520situations%2520with%2520suboptimal%2520encoders%2520or%2520limited%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Detection%20of%20Rare%20Nodes%20in%20Hierarchical%20Multi-Label%20Learning&entry.906535625=Isaac%20Xu%20and%20Martin%20Gillis%20and%20Ayushi%20Sharma%20and%20Benjamin%20Misiuk%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg&entry.1292438233=In%20hierarchical%20multi-label%20classification%2C%20a%20persistent%20challenge%20is%20enabling%20model%20predictions%20to%20reach%20deeper%20levels%20of%20the%20hierarchy%20for%20more%20detailed%20or%20fine-grained%20classifications.%20This%20difficulty%20partly%20arises%20from%20the%20natural%20rarity%20of%20certain%20classes%20%28or%20hierarchical%20nodes%29%20and%20the%20hierarchical%20constraint%20that%20ensures%20child%20nodes%20are%20almost%20always%20less%20frequent%20than%20their%20parents.%20To%20address%20this%2C%20we%20propose%20a%20weighted%20loss%20objective%20for%20neural%20networks%20that%20combines%20node-wise%20imbalance%20weighting%20with%20focal%20weighting%20components%2C%20the%20latter%20leveraging%20modern%20quantification%20of%20ensemble%20uncertainties.%20By%20emphasizing%20rare%20nodes%20rather%20than%20rare%20observations%20%28data%20points%29%2C%20and%20focusing%20on%20uncertain%20nodes%20for%20each%20model%20output%20distribution%20during%20training%2C%20we%20observe%20improvements%20in%20recall%20by%20up%20to%20a%20factor%20of%20five%20on%20benchmark%20datasets%2C%20along%20with%20statistically%20significant%20gains%20in%20%24F_%7B1%7D%24%20score.%20We%20also%20show%20our%20approach%20aids%20convolutional%20networks%20on%20challenging%20tasks%2C%20as%20in%20situations%20with%20suboptimal%20encoders%20or%20limited%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.08986v1&entry.124074799=Read"},
{"title": "Lyria: A Genetic Algorithm-Driven Neuro-Symbolic Reasoning Framework for LLMs", "author": "Weizhi Tang and Kwabena Nuamah and Vaishak Belle", "abstract": "While LLMs have demonstrated impressive abilities across various domains, they struggle with two major issues. The first is that LLMs trap themselves into local optima and the second is that they lack exhaustive coverage of the solution space. To investigate and improve these two issues, we propose Lyria, a neuro-symbolic reasoning framework building on the integration of LLMs, genetic algorithms, and symbolic systems, comprising 7 essential components. Through conducting extensive experiments with 4 LLMs across 3 types of problems, we demonstrated the efficacy of Lyria. Furthermore, with 7 additional ablation experiments, we further systematically analyzed and elucidated the factors that affect its performance. In addition, based on Lyria, we extend the ideas to the fine-tuning process of LLMs and introduce LAFT which enables a weaker model to imitate the reasoning process of a stronger model that reason under the Lyria reasoning framework. We demonstrate that the significant effectiveness of LAFT by conducting extensive experiments against 9 constructed baselines. We finally reveal the limitations and provide insights into future directions.", "link": "http://arxiv.org/abs/2507.04034v2", "date": "2026-02-09", "relevancy": 2.5347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyria%3A%20A%20Genetic%20Algorithm-Driven%20Neuro-Symbolic%20Reasoning%20Framework%20for%20LLMs&body=Title%3A%20Lyria%3A%20A%20Genetic%20Algorithm-Driven%20Neuro-Symbolic%20Reasoning%20Framework%20for%20LLMs%0AAuthor%3A%20Weizhi%20Tang%20and%20Kwabena%20Nuamah%20and%20Vaishak%20Belle%0AAbstract%3A%20While%20LLMs%20have%20demonstrated%20impressive%20abilities%20across%20various%20domains%2C%20they%20struggle%20with%20two%20major%20issues.%20The%20first%20is%20that%20LLMs%20trap%20themselves%20into%20local%20optima%20and%20the%20second%20is%20that%20they%20lack%20exhaustive%20coverage%20of%20the%20solution%20space.%20To%20investigate%20and%20improve%20these%20two%20issues%2C%20we%20propose%20Lyria%2C%20a%20neuro-symbolic%20reasoning%20framework%20building%20on%20the%20integration%20of%20LLMs%2C%20genetic%20algorithms%2C%20and%20symbolic%20systems%2C%20comprising%207%20essential%20components.%20Through%20conducting%20extensive%20experiments%20with%204%20LLMs%20across%203%20types%20of%20problems%2C%20we%20demonstrated%20the%20efficacy%20of%20Lyria.%20Furthermore%2C%20with%207%20additional%20ablation%20experiments%2C%20we%20further%20systematically%20analyzed%20and%20elucidated%20the%20factors%20that%20affect%20its%20performance.%20In%20addition%2C%20based%20on%20Lyria%2C%20we%20extend%20the%20ideas%20to%20the%20fine-tuning%20process%20of%20LLMs%20and%20introduce%20LAFT%20which%20enables%20a%20weaker%20model%20to%20imitate%20the%20reasoning%20process%20of%20a%20stronger%20model%20that%20reason%20under%20the%20Lyria%20reasoning%20framework.%20We%20demonstrate%20that%20the%20significant%20effectiveness%20of%20LAFT%20by%20conducting%20extensive%20experiments%20against%209%20constructed%20baselines.%20We%20finally%20reveal%20the%20limitations%20and%20provide%20insights%20into%20future%20directions.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyria%253A%2520A%2520Genetic%2520Algorithm-Driven%2520Neuro-Symbolic%2520Reasoning%2520Framework%2520for%2520LLMs%26entry.906535625%3DWeizhi%2520Tang%2520and%2520Kwabena%2520Nuamah%2520and%2520Vaishak%2520Belle%26entry.1292438233%3DWhile%2520LLMs%2520have%2520demonstrated%2520impressive%2520abilities%2520across%2520various%2520domains%252C%2520they%2520struggle%2520with%2520two%2520major%2520issues.%2520The%2520first%2520is%2520that%2520LLMs%2520trap%2520themselves%2520into%2520local%2520optima%2520and%2520the%2520second%2520is%2520that%2520they%2520lack%2520exhaustive%2520coverage%2520of%2520the%2520solution%2520space.%2520To%2520investigate%2520and%2520improve%2520these%2520two%2520issues%252C%2520we%2520propose%2520Lyria%252C%2520a%2520neuro-symbolic%2520reasoning%2520framework%2520building%2520on%2520the%2520integration%2520of%2520LLMs%252C%2520genetic%2520algorithms%252C%2520and%2520symbolic%2520systems%252C%2520comprising%25207%2520essential%2520components.%2520Through%2520conducting%2520extensive%2520experiments%2520with%25204%2520LLMs%2520across%25203%2520types%2520of%2520problems%252C%2520we%2520demonstrated%2520the%2520efficacy%2520of%2520Lyria.%2520Furthermore%252C%2520with%25207%2520additional%2520ablation%2520experiments%252C%2520we%2520further%2520systematically%2520analyzed%2520and%2520elucidated%2520the%2520factors%2520that%2520affect%2520its%2520performance.%2520In%2520addition%252C%2520based%2520on%2520Lyria%252C%2520we%2520extend%2520the%2520ideas%2520to%2520the%2520fine-tuning%2520process%2520of%2520LLMs%2520and%2520introduce%2520LAFT%2520which%2520enables%2520a%2520weaker%2520model%2520to%2520imitate%2520the%2520reasoning%2520process%2520of%2520a%2520stronger%2520model%2520that%2520reason%2520under%2520the%2520Lyria%2520reasoning%2520framework.%2520We%2520demonstrate%2520that%2520the%2520significant%2520effectiveness%2520of%2520LAFT%2520by%2520conducting%2520extensive%2520experiments%2520against%25209%2520constructed%2520baselines.%2520We%2520finally%2520reveal%2520the%2520limitations%2520and%2520provide%2520insights%2520into%2520future%2520directions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyria%3A%20A%20Genetic%20Algorithm-Driven%20Neuro-Symbolic%20Reasoning%20Framework%20for%20LLMs&entry.906535625=Weizhi%20Tang%20and%20Kwabena%20Nuamah%20and%20Vaishak%20Belle&entry.1292438233=While%20LLMs%20have%20demonstrated%20impressive%20abilities%20across%20various%20domains%2C%20they%20struggle%20with%20two%20major%20issues.%20The%20first%20is%20that%20LLMs%20trap%20themselves%20into%20local%20optima%20and%20the%20second%20is%20that%20they%20lack%20exhaustive%20coverage%20of%20the%20solution%20space.%20To%20investigate%20and%20improve%20these%20two%20issues%2C%20we%20propose%20Lyria%2C%20a%20neuro-symbolic%20reasoning%20framework%20building%20on%20the%20integration%20of%20LLMs%2C%20genetic%20algorithms%2C%20and%20symbolic%20systems%2C%20comprising%207%20essential%20components.%20Through%20conducting%20extensive%20experiments%20with%204%20LLMs%20across%203%20types%20of%20problems%2C%20we%20demonstrated%20the%20efficacy%20of%20Lyria.%20Furthermore%2C%20with%207%20additional%20ablation%20experiments%2C%20we%20further%20systematically%20analyzed%20and%20elucidated%20the%20factors%20that%20affect%20its%20performance.%20In%20addition%2C%20based%20on%20Lyria%2C%20we%20extend%20the%20ideas%20to%20the%20fine-tuning%20process%20of%20LLMs%20and%20introduce%20LAFT%20which%20enables%20a%20weaker%20model%20to%20imitate%20the%20reasoning%20process%20of%20a%20stronger%20model%20that%20reason%20under%20the%20Lyria%20reasoning%20framework.%20We%20demonstrate%20that%20the%20significant%20effectiveness%20of%20LAFT%20by%20conducting%20extensive%20experiments%20against%209%20constructed%20baselines.%20We%20finally%20reveal%20the%20limitations%20and%20provide%20insights%20into%20future%20directions.&entry.1838667208=http%3A//arxiv.org/abs/2507.04034v2&entry.124074799=Read"},
{"title": "Deep networks learn to parse uniform-depth context-free languages from local statistics", "author": "Jack T. Parley and Francesco Cagnetta and Matthieu Wyart", "abstract": "Understanding how the structure of language can be learned from sentences alone is a central question in both cognitive science and machine learning. Studies of the internal representations of Large Language Models (LLMs) support their ability to parse text when predicting the next word, while representing semantic notions independently of surface form. Yet, which data statistics make these feats possible, and how much data is required, remain largely unknown. Probabilistic context-free grammars (PCFGs) provide a tractable testbed for studying these questions. However, prior work has focused either on the post-hoc characterization of the parsing-like algorithms used by trained networks; or on the learnability of PCFGs with fixed syntax, where parsing is unnecessary. Here, we (i) introduce a tunable class of PCFGs in which both the degree of ambiguity and the correlation structure across scales can be controlled; (ii) provide a learning mechanism -- an inference algorithm inspired by the structure of deep convolutional networks -- that links learnability and sample complexity to specific language statistics; and (iii) validate our predictions empirically across deep convolutional and transformer-based architectures. Overall, we propose a unifying framework where correlations at different scales lift local ambiguities, enabling the emergence of hierarchical representations of the data.", "link": "http://arxiv.org/abs/2602.06065v2", "date": "2026-02-09", "relevancy": 2.5288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20networks%20learn%20to%20parse%20uniform-depth%20context-free%20languages%20from%20local%20statistics&body=Title%3A%20Deep%20networks%20learn%20to%20parse%20uniform-depth%20context-free%20languages%20from%20local%20statistics%0AAuthor%3A%20Jack%20T.%20Parley%20and%20Francesco%20Cagnetta%20and%20Matthieu%20Wyart%0AAbstract%3A%20Understanding%20how%20the%20structure%20of%20language%20can%20be%20learned%20from%20sentences%20alone%20is%20a%20central%20question%20in%20both%20cognitive%20science%20and%20machine%20learning.%20Studies%20of%20the%20internal%20representations%20of%20Large%20Language%20Models%20%28LLMs%29%20support%20their%20ability%20to%20parse%20text%20when%20predicting%20the%20next%20word%2C%20while%20representing%20semantic%20notions%20independently%20of%20surface%20form.%20Yet%2C%20which%20data%20statistics%20make%20these%20feats%20possible%2C%20and%20how%20much%20data%20is%20required%2C%20remain%20largely%20unknown.%20Probabilistic%20context-free%20grammars%20%28PCFGs%29%20provide%20a%20tractable%20testbed%20for%20studying%20these%20questions.%20However%2C%20prior%20work%20has%20focused%20either%20on%20the%20post-hoc%20characterization%20of%20the%20parsing-like%20algorithms%20used%20by%20trained%20networks%3B%20or%20on%20the%20learnability%20of%20PCFGs%20with%20fixed%20syntax%2C%20where%20parsing%20is%20unnecessary.%20Here%2C%20we%20%28i%29%20introduce%20a%20tunable%20class%20of%20PCFGs%20in%20which%20both%20the%20degree%20of%20ambiguity%20and%20the%20correlation%20structure%20across%20scales%20can%20be%20controlled%3B%20%28ii%29%20provide%20a%20learning%20mechanism%20--%20an%20inference%20algorithm%20inspired%20by%20the%20structure%20of%20deep%20convolutional%20networks%20--%20that%20links%20learnability%20and%20sample%20complexity%20to%20specific%20language%20statistics%3B%20and%20%28iii%29%20validate%20our%20predictions%20empirically%20across%20deep%20convolutional%20and%20transformer-based%20architectures.%20Overall%2C%20we%20propose%20a%20unifying%20framework%20where%20correlations%20at%20different%20scales%20lift%20local%20ambiguities%2C%20enabling%20the%20emergence%20of%20hierarchical%20representations%20of%20the%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520networks%2520learn%2520to%2520parse%2520uniform-depth%2520context-free%2520languages%2520from%2520local%2520statistics%26entry.906535625%3DJack%2520T.%2520Parley%2520and%2520Francesco%2520Cagnetta%2520and%2520Matthieu%2520Wyart%26entry.1292438233%3DUnderstanding%2520how%2520the%2520structure%2520of%2520language%2520can%2520be%2520learned%2520from%2520sentences%2520alone%2520is%2520a%2520central%2520question%2520in%2520both%2520cognitive%2520science%2520and%2520machine%2520learning.%2520Studies%2520of%2520the%2520internal%2520representations%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520support%2520their%2520ability%2520to%2520parse%2520text%2520when%2520predicting%2520the%2520next%2520word%252C%2520while%2520representing%2520semantic%2520notions%2520independently%2520of%2520surface%2520form.%2520Yet%252C%2520which%2520data%2520statistics%2520make%2520these%2520feats%2520possible%252C%2520and%2520how%2520much%2520data%2520is%2520required%252C%2520remain%2520largely%2520unknown.%2520Probabilistic%2520context-free%2520grammars%2520%2528PCFGs%2529%2520provide%2520a%2520tractable%2520testbed%2520for%2520studying%2520these%2520questions.%2520However%252C%2520prior%2520work%2520has%2520focused%2520either%2520on%2520the%2520post-hoc%2520characterization%2520of%2520the%2520parsing-like%2520algorithms%2520used%2520by%2520trained%2520networks%253B%2520or%2520on%2520the%2520learnability%2520of%2520PCFGs%2520with%2520fixed%2520syntax%252C%2520where%2520parsing%2520is%2520unnecessary.%2520Here%252C%2520we%2520%2528i%2529%2520introduce%2520a%2520tunable%2520class%2520of%2520PCFGs%2520in%2520which%2520both%2520the%2520degree%2520of%2520ambiguity%2520and%2520the%2520correlation%2520structure%2520across%2520scales%2520can%2520be%2520controlled%253B%2520%2528ii%2529%2520provide%2520a%2520learning%2520mechanism%2520--%2520an%2520inference%2520algorithm%2520inspired%2520by%2520the%2520structure%2520of%2520deep%2520convolutional%2520networks%2520--%2520that%2520links%2520learnability%2520and%2520sample%2520complexity%2520to%2520specific%2520language%2520statistics%253B%2520and%2520%2528iii%2529%2520validate%2520our%2520predictions%2520empirically%2520across%2520deep%2520convolutional%2520and%2520transformer-based%2520architectures.%2520Overall%252C%2520we%2520propose%2520a%2520unifying%2520framework%2520where%2520correlations%2520at%2520different%2520scales%2520lift%2520local%2520ambiguities%252C%2520enabling%2520the%2520emergence%2520of%2520hierarchical%2520representations%2520of%2520the%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20networks%20learn%20to%20parse%20uniform-depth%20context-free%20languages%20from%20local%20statistics&entry.906535625=Jack%20T.%20Parley%20and%20Francesco%20Cagnetta%20and%20Matthieu%20Wyart&entry.1292438233=Understanding%20how%20the%20structure%20of%20language%20can%20be%20learned%20from%20sentences%20alone%20is%20a%20central%20question%20in%20both%20cognitive%20science%20and%20machine%20learning.%20Studies%20of%20the%20internal%20representations%20of%20Large%20Language%20Models%20%28LLMs%29%20support%20their%20ability%20to%20parse%20text%20when%20predicting%20the%20next%20word%2C%20while%20representing%20semantic%20notions%20independently%20of%20surface%20form.%20Yet%2C%20which%20data%20statistics%20make%20these%20feats%20possible%2C%20and%20how%20much%20data%20is%20required%2C%20remain%20largely%20unknown.%20Probabilistic%20context-free%20grammars%20%28PCFGs%29%20provide%20a%20tractable%20testbed%20for%20studying%20these%20questions.%20However%2C%20prior%20work%20has%20focused%20either%20on%20the%20post-hoc%20characterization%20of%20the%20parsing-like%20algorithms%20used%20by%20trained%20networks%3B%20or%20on%20the%20learnability%20of%20PCFGs%20with%20fixed%20syntax%2C%20where%20parsing%20is%20unnecessary.%20Here%2C%20we%20%28i%29%20introduce%20a%20tunable%20class%20of%20PCFGs%20in%20which%20both%20the%20degree%20of%20ambiguity%20and%20the%20correlation%20structure%20across%20scales%20can%20be%20controlled%3B%20%28ii%29%20provide%20a%20learning%20mechanism%20--%20an%20inference%20algorithm%20inspired%20by%20the%20structure%20of%20deep%20convolutional%20networks%20--%20that%20links%20learnability%20and%20sample%20complexity%20to%20specific%20language%20statistics%3B%20and%20%28iii%29%20validate%20our%20predictions%20empirically%20across%20deep%20convolutional%20and%20transformer-based%20architectures.%20Overall%2C%20we%20propose%20a%20unifying%20framework%20where%20correlations%20at%20different%20scales%20lift%20local%20ambiguities%2C%20enabling%20the%20emergence%20of%20hierarchical%20representations%20of%20the%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.06065v2&entry.124074799=Read"},
{"title": "SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation", "author": "Yu Xie and Xing Kai Ren and Ying Qi and Hu Yao", "abstract": "While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a \"Symmetric Conservatism\" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the \"Reward Collapse\" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a \"Boost Factor\" to high-potential cold start items to achieve super-linear updates and employs an \"Entropy Aware Penalty\" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.", "link": "http://arxiv.org/abs/2601.21452v2", "date": "2026-02-09", "relevancy": 2.5251, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4905}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGE%3A%20Sequence-level%20Adaptive%20Gradient%20Evolution%20for%20Generative%20Recommendation&body=Title%3A%20SAGE%3A%20Sequence-level%20Adaptive%20Gradient%20Evolution%20for%20Generative%20Recommendation%0AAuthor%3A%20Yu%20Xie%20and%20Xing%20Kai%20Ren%20and%20Ying%20Qi%20and%20Hu%20Yao%0AAbstract%3A%20While%20works%20such%20as%20OneRec%20have%20validated%20the%20scaling%20laws%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20recommender%20systems%2C%20they%20rely%20on%20a%20cumbersome%20separate%20vocabulary.%20This%20dependency%20prevents%20the%20model%20architecture%20from%20reusing%20native%20LLM%20vocabularies%2C%20resulting%20in%20high%20maintenance%20costs%20and%20poor%20scalability.%20In%20response%2C%20we%20aim%20to%20efficiently%20reuse%20open-source%20LLM%20architectures%20without%20constructing%20a%20separate%20tokenization%20vocabulary.%20Furthermore%2C%20we%20identify%20that%20the%20optimization%20strategy%20of%20OneRec%20Gradient%20Bounded%20Policy%20Optimization%20%28GBPO%29%2Csuffers%20from%20a%20%22Symmetric%20Conservatism%22%20problem%3A%20its%20static%20gradient%20boundaries%20structurally%20suppress%20the%20update%20momentum%20required%20for%20cold-start%20items%20and%20fail%20to%20prevent%20diversity%20collapse%20in%20high-noise%20environments.To%20address%20this%20issue%2C%20we%20propose%20SAGE%20%28Sequence-level%20Adaptive%20Gradient%20Evolution%29%2C%20a%20unified%20optimization%20framework%20tailored%20for%20list-wise%20generative%20recommendation.%20SAGE%20introduces%20two%20key%20innovations%3A%281%29%20Sequence-level%20Signal%20Decoupling%3A%20By%20combining%20a%20geometric%20mean%20importance%20ratio%20with%20decoupled%20multi-objective%20advantages%2C%20we%20eliminate%20token-level%20variance%20and%20resolve%20the%20%22Reward%20Collapse%22%20problem.%20%282%29%20Asymmetric%20Adaptive%20Dynamics%3A%20We%20construct%20a%20dynamic%20gradient%20manifold%20that%20applies%20a%20%22Boost%20Factor%22%20to%20high-potential%20cold%20start%20items%20to%20achieve%20super-linear%20updates%20and%20employs%20an%20%22Entropy%20Aware%20Penalty%22%20to%20break%20information%20cocoons.%20Theoretical%20analysis%20and%20empirical%20results%20demonstrate%20that%20SAGE%20effectively%20unblocks%20cold-start%20traffic%20and%20sustains%20recommendation%20diversity%2C%20all%20while%20retaining%20the%20numerical%20stability%20of%20GBPO.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGE%253A%2520Sequence-level%2520Adaptive%2520Gradient%2520Evolution%2520for%2520Generative%2520Recommendation%26entry.906535625%3DYu%2520Xie%2520and%2520Xing%2520Kai%2520Ren%2520and%2520Ying%2520Qi%2520and%2520Hu%2520Yao%26entry.1292438233%3DWhile%2520works%2520such%2520as%2520OneRec%2520have%2520validated%2520the%2520scaling%2520laws%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520recommender%2520systems%252C%2520they%2520rely%2520on%2520a%2520cumbersome%2520separate%2520vocabulary.%2520This%2520dependency%2520prevents%2520the%2520model%2520architecture%2520from%2520reusing%2520native%2520LLM%2520vocabularies%252C%2520resulting%2520in%2520high%2520maintenance%2520costs%2520and%2520poor%2520scalability.%2520In%2520response%252C%2520we%2520aim%2520to%2520efficiently%2520reuse%2520open-source%2520LLM%2520architectures%2520without%2520constructing%2520a%2520separate%2520tokenization%2520vocabulary.%2520Furthermore%252C%2520we%2520identify%2520that%2520the%2520optimization%2520strategy%2520of%2520OneRec%2520Gradient%2520Bounded%2520Policy%2520Optimization%2520%2528GBPO%2529%252Csuffers%2520from%2520a%2520%2522Symmetric%2520Conservatism%2522%2520problem%253A%2520its%2520static%2520gradient%2520boundaries%2520structurally%2520suppress%2520the%2520update%2520momentum%2520required%2520for%2520cold-start%2520items%2520and%2520fail%2520to%2520prevent%2520diversity%2520collapse%2520in%2520high-noise%2520environments.To%2520address%2520this%2520issue%252C%2520we%2520propose%2520SAGE%2520%2528Sequence-level%2520Adaptive%2520Gradient%2520Evolution%2529%252C%2520a%2520unified%2520optimization%2520framework%2520tailored%2520for%2520list-wise%2520generative%2520recommendation.%2520SAGE%2520introduces%2520two%2520key%2520innovations%253A%25281%2529%2520Sequence-level%2520Signal%2520Decoupling%253A%2520By%2520combining%2520a%2520geometric%2520mean%2520importance%2520ratio%2520with%2520decoupled%2520multi-objective%2520advantages%252C%2520we%2520eliminate%2520token-level%2520variance%2520and%2520resolve%2520the%2520%2522Reward%2520Collapse%2522%2520problem.%2520%25282%2529%2520Asymmetric%2520Adaptive%2520Dynamics%253A%2520We%2520construct%2520a%2520dynamic%2520gradient%2520manifold%2520that%2520applies%2520a%2520%2522Boost%2520Factor%2522%2520to%2520high-potential%2520cold%2520start%2520items%2520to%2520achieve%2520super-linear%2520updates%2520and%2520employs%2520an%2520%2522Entropy%2520Aware%2520Penalty%2522%2520to%2520break%2520information%2520cocoons.%2520Theoretical%2520analysis%2520and%2520empirical%2520results%2520demonstrate%2520that%2520SAGE%2520effectively%2520unblocks%2520cold-start%2520traffic%2520and%2520sustains%2520recommendation%2520diversity%252C%2520all%2520while%2520retaining%2520the%2520numerical%2520stability%2520of%2520GBPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGE%3A%20Sequence-level%20Adaptive%20Gradient%20Evolution%20for%20Generative%20Recommendation&entry.906535625=Yu%20Xie%20and%20Xing%20Kai%20Ren%20and%20Ying%20Qi%20and%20Hu%20Yao&entry.1292438233=While%20works%20such%20as%20OneRec%20have%20validated%20the%20scaling%20laws%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20recommender%20systems%2C%20they%20rely%20on%20a%20cumbersome%20separate%20vocabulary.%20This%20dependency%20prevents%20the%20model%20architecture%20from%20reusing%20native%20LLM%20vocabularies%2C%20resulting%20in%20high%20maintenance%20costs%20and%20poor%20scalability.%20In%20response%2C%20we%20aim%20to%20efficiently%20reuse%20open-source%20LLM%20architectures%20without%20constructing%20a%20separate%20tokenization%20vocabulary.%20Furthermore%2C%20we%20identify%20that%20the%20optimization%20strategy%20of%20OneRec%20Gradient%20Bounded%20Policy%20Optimization%20%28GBPO%29%2Csuffers%20from%20a%20%22Symmetric%20Conservatism%22%20problem%3A%20its%20static%20gradient%20boundaries%20structurally%20suppress%20the%20update%20momentum%20required%20for%20cold-start%20items%20and%20fail%20to%20prevent%20diversity%20collapse%20in%20high-noise%20environments.To%20address%20this%20issue%2C%20we%20propose%20SAGE%20%28Sequence-level%20Adaptive%20Gradient%20Evolution%29%2C%20a%20unified%20optimization%20framework%20tailored%20for%20list-wise%20generative%20recommendation.%20SAGE%20introduces%20two%20key%20innovations%3A%281%29%20Sequence-level%20Signal%20Decoupling%3A%20By%20combining%20a%20geometric%20mean%20importance%20ratio%20with%20decoupled%20multi-objective%20advantages%2C%20we%20eliminate%20token-level%20variance%20and%20resolve%20the%20%22Reward%20Collapse%22%20problem.%20%282%29%20Asymmetric%20Adaptive%20Dynamics%3A%20We%20construct%20a%20dynamic%20gradient%20manifold%20that%20applies%20a%20%22Boost%20Factor%22%20to%20high-potential%20cold%20start%20items%20to%20achieve%20super-linear%20updates%20and%20employs%20an%20%22Entropy%20Aware%20Penalty%22%20to%20break%20information%20cocoons.%20Theoretical%20analysis%20and%20empirical%20results%20demonstrate%20that%20SAGE%20effectively%20unblocks%20cold-start%20traffic%20and%20sustains%20recommendation%20diversity%2C%20all%20while%20retaining%20the%20numerical%20stability%20of%20GBPO.&entry.1838667208=http%3A//arxiv.org/abs/2601.21452v2&entry.124074799=Read"},
{"title": "Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization", "author": "Yang Qiu and Yixiong Zou and Jun Wang", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.", "link": "http://arxiv.org/abs/2602.08855v1", "date": "2026-02-09", "relevancy": 2.5175, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5079}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Graph%20Generalization%20through%20the%20Lens%20of%20Sharpness-Aware%20Minimization&body=Title%3A%20Rethinking%20Graph%20Generalization%20through%20the%20Lens%20of%20Sharpness-Aware%20Minimization%0AAuthor%3A%20Yang%20Qiu%20and%20Yixiong%20Zou%20and%20Jun%20Wang%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20various%20graph-based%20tasks%20but%20remain%20highly%20sensitive%20to%20distribution%20shifts.%20In%20this%20work%2C%20we%20focus%20on%20a%20prevalent%20yet%20under-explored%20phenomenon%20in%20graph%20generalization%2C%20Minimal%20Shift%20Flip%20%28MSF%29%2Cwhere%20test%20samples%20that%20slightly%20deviate%20from%20the%20training%20distribution%20are%20abruptly%20misclassified.%20To%20interpret%20this%20phenomenon%2C%20we%20revisit%20MSF%20through%20the%20lens%20of%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20which%20characterizes%20the%20local%20stability%20and%20sharpness%20of%20the%20loss%20landscape%20while%20providing%20a%20theoretical%20foundation%20for%20modeling%20generalization%20error.%20To%20quantify%20loss%20sharpness%2C%20we%20introduce%20the%20concept%20of%20Local%20Robust%20Radius%2C%20measuring%20the%20smallest%20perturbation%20required%20to%20flip%20a%20prediction%20and%20establishing%20a%20theoretical%20link%20between%20local%20stability%20and%20generalization.%20Building%20on%20this%20perspective%2C%20we%20further%20observe%20a%20continual%20decrease%20in%20the%20robust%20radius%20during%20training%2C%20indicating%20weakened%20local%20stability%20and%20an%20increasingly%20sharp%20loss%20landscape%20that%20gives%20rise%20to%20MSF.%20To%20jointly%20solve%20the%20MSF%20phenomenon%20and%20the%20intractability%20of%20radius%2C%20we%20develop%20an%20energy-based%20formulation%20that%20is%20theoretically%20proven%20to%20be%20monotonically%20correlated%20with%20the%20robust%20radius%2C%20offering%20a%20tractable%20and%20principled%20objective%20for%20modeling%20flatness%20and%20stability.%20Building%20on%20these%20insights%2C%20we%20propose%20an%20energy-driven%20generative%20augmentation%20framework%20%28E2A%29%20that%20leverages%20energy-guided%20latent%20perturbations%20to%20generate%20pseudo-OOD%20samples%20and%20enhance%20model%20generalization.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20E2A%20consistently%20improves%20graph%20OOD%20generalization%2C%20outperforming%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Graph%2520Generalization%2520through%2520the%2520Lens%2520of%2520Sharpness-Aware%2520Minimization%26entry.906535625%3DYang%2520Qiu%2520and%2520Yixiong%2520Zou%2520and%2520Jun%2520Wang%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520various%2520graph-based%2520tasks%2520but%2520remain%2520highly%2520sensitive%2520to%2520distribution%2520shifts.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%2520prevalent%2520yet%2520under-explored%2520phenomenon%2520in%2520graph%2520generalization%252C%2520Minimal%2520Shift%2520Flip%2520%2528MSF%2529%252Cwhere%2520test%2520samples%2520that%2520slightly%2520deviate%2520from%2520the%2520training%2520distribution%2520are%2520abruptly%2520misclassified.%2520To%2520interpret%2520this%2520phenomenon%252C%2520we%2520revisit%2520MSF%2520through%2520the%2520lens%2520of%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%252C%2520which%2520characterizes%2520the%2520local%2520stability%2520and%2520sharpness%2520of%2520the%2520loss%2520landscape%2520while%2520providing%2520a%2520theoretical%2520foundation%2520for%2520modeling%2520generalization%2520error.%2520To%2520quantify%2520loss%2520sharpness%252C%2520we%2520introduce%2520the%2520concept%2520of%2520Local%2520Robust%2520Radius%252C%2520measuring%2520the%2520smallest%2520perturbation%2520required%2520to%2520flip%2520a%2520prediction%2520and%2520establishing%2520a%2520theoretical%2520link%2520between%2520local%2520stability%2520and%2520generalization.%2520Building%2520on%2520this%2520perspective%252C%2520we%2520further%2520observe%2520a%2520continual%2520decrease%2520in%2520the%2520robust%2520radius%2520during%2520training%252C%2520indicating%2520weakened%2520local%2520stability%2520and%2520an%2520increasingly%2520sharp%2520loss%2520landscape%2520that%2520gives%2520rise%2520to%2520MSF.%2520To%2520jointly%2520solve%2520the%2520MSF%2520phenomenon%2520and%2520the%2520intractability%2520of%2520radius%252C%2520we%2520develop%2520an%2520energy-based%2520formulation%2520that%2520is%2520theoretically%2520proven%2520to%2520be%2520monotonically%2520correlated%2520with%2520the%2520robust%2520radius%252C%2520offering%2520a%2520tractable%2520and%2520principled%2520objective%2520for%2520modeling%2520flatness%2520and%2520stability.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520an%2520energy-driven%2520generative%2520augmentation%2520framework%2520%2528E2A%2529%2520that%2520leverages%2520energy-guided%2520latent%2520perturbations%2520to%2520generate%2520pseudo-OOD%2520samples%2520and%2520enhance%2520model%2520generalization.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520E2A%2520consistently%2520improves%2520graph%2520OOD%2520generalization%252C%2520outperforming%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Graph%20Generalization%20through%20the%20Lens%20of%20Sharpness-Aware%20Minimization&entry.906535625=Yang%20Qiu%20and%20Yixiong%20Zou%20and%20Jun%20Wang&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20various%20graph-based%20tasks%20but%20remain%20highly%20sensitive%20to%20distribution%20shifts.%20In%20this%20work%2C%20we%20focus%20on%20a%20prevalent%20yet%20under-explored%20phenomenon%20in%20graph%20generalization%2C%20Minimal%20Shift%20Flip%20%28MSF%29%2Cwhere%20test%20samples%20that%20slightly%20deviate%20from%20the%20training%20distribution%20are%20abruptly%20misclassified.%20To%20interpret%20this%20phenomenon%2C%20we%20revisit%20MSF%20through%20the%20lens%20of%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20which%20characterizes%20the%20local%20stability%20and%20sharpness%20of%20the%20loss%20landscape%20while%20providing%20a%20theoretical%20foundation%20for%20modeling%20generalization%20error.%20To%20quantify%20loss%20sharpness%2C%20we%20introduce%20the%20concept%20of%20Local%20Robust%20Radius%2C%20measuring%20the%20smallest%20perturbation%20required%20to%20flip%20a%20prediction%20and%20establishing%20a%20theoretical%20link%20between%20local%20stability%20and%20generalization.%20Building%20on%20this%20perspective%2C%20we%20further%20observe%20a%20continual%20decrease%20in%20the%20robust%20radius%20during%20training%2C%20indicating%20weakened%20local%20stability%20and%20an%20increasingly%20sharp%20loss%20landscape%20that%20gives%20rise%20to%20MSF.%20To%20jointly%20solve%20the%20MSF%20phenomenon%20and%20the%20intractability%20of%20radius%2C%20we%20develop%20an%20energy-based%20formulation%20that%20is%20theoretically%20proven%20to%20be%20monotonically%20correlated%20with%20the%20robust%20radius%2C%20offering%20a%20tractable%20and%20principled%20objective%20for%20modeling%20flatness%20and%20stability.%20Building%20on%20these%20insights%2C%20we%20propose%20an%20energy-driven%20generative%20augmentation%20framework%20%28E2A%29%20that%20leverages%20energy-guided%20latent%20perturbations%20to%20generate%20pseudo-OOD%20samples%20and%20enhance%20model%20generalization.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20E2A%20consistently%20improves%20graph%20OOD%20generalization%2C%20outperforming%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.08855v1&entry.124074799=Read"},
{"title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE", "author": "Ruijie Zhu and Jiahao Lu and Wenbo Hu and Xiaoguang Han and Jianfei Cai and Ying Shan and Chuanxia Zheng", "abstract": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "link": "http://arxiv.org/abs/2602.08961v1", "date": "2026-02-09", "relevancy": 2.5033, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6332}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6235}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionCrafter%3A%20Dense%20Geometry%20and%20Motion%20Reconstruction%20with%20a%204D%20VAE&body=Title%3A%20MotionCrafter%3A%20Dense%20Geometry%20and%20Motion%20Reconstruction%20with%20a%204D%20VAE%0AAuthor%3A%20Ruijie%20Zhu%20and%20Jiahao%20Lu%20and%20Wenbo%20Hu%20and%20Xiaoguang%20Han%20and%20Jianfei%20Cai%20and%20Ying%20Shan%20and%20Chuanxia%20Zheng%0AAbstract%3A%20We%20introduce%20MotionCrafter%2C%20a%20video%20diffusion-based%20framework%20that%20jointly%20reconstructs%204D%20geometry%20and%20estimates%20dense%20motion%20from%20a%20monocular%20video.%20The%20core%20of%20our%20method%20is%20a%20novel%20joint%20representation%20of%20dense%203D%20point%20maps%20and%203D%20scene%20flows%20in%20a%20shared%20coordinate%20system%2C%20and%20a%20novel%204D%20VAE%20to%20effectively%20learn%20this%20representation.%20Unlike%20prior%20work%20that%20forces%20the%203D%20value%20and%20latents%20to%20align%20strictly%20with%20RGB%20VAE%20latents-despite%20their%20fundamentally%20different%20distributions-we%20show%20that%20such%20alignment%20is%20unnecessary%20and%20leads%20to%20suboptimal%20performance.%20Instead%2C%20we%20introduce%20a%20new%20data%20normalization%20and%20VAE%20training%20strategy%20that%20better%20transfers%20diffusion%20priors%20and%20greatly%20improves%20reconstruction%20quality.%20Extensive%20experiments%20across%20multiple%20datasets%20demonstrate%20that%20MotionCrafter%20achieves%20state-of-the-art%20performance%20in%20both%20geometry%20reconstruction%20and%20dense%20scene%20flow%20estimation%2C%20delivering%2038.64%25%20and%2025.0%25%20improvements%20in%20geometry%20and%20motion%20reconstruction%2C%20respectively%2C%20all%20without%20any%20post-optimization.%20Project%20page%3A%20https%3A//ruijiezhu94.github.io/MotionCrafter_Page%0ALink%3A%20http%3A//arxiv.org/abs/2602.08961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionCrafter%253A%2520Dense%2520Geometry%2520and%2520Motion%2520Reconstruction%2520with%2520a%25204D%2520VAE%26entry.906535625%3DRuijie%2520Zhu%2520and%2520Jiahao%2520Lu%2520and%2520Wenbo%2520Hu%2520and%2520Xiaoguang%2520Han%2520and%2520Jianfei%2520Cai%2520and%2520Ying%2520Shan%2520and%2520Chuanxia%2520Zheng%26entry.1292438233%3DWe%2520introduce%2520MotionCrafter%252C%2520a%2520video%2520diffusion-based%2520framework%2520that%2520jointly%2520reconstructs%25204D%2520geometry%2520and%2520estimates%2520dense%2520motion%2520from%2520a%2520monocular%2520video.%2520The%2520core%2520of%2520our%2520method%2520is%2520a%2520novel%2520joint%2520representation%2520of%2520dense%25203D%2520point%2520maps%2520and%25203D%2520scene%2520flows%2520in%2520a%2520shared%2520coordinate%2520system%252C%2520and%2520a%2520novel%25204D%2520VAE%2520to%2520effectively%2520learn%2520this%2520representation.%2520Unlike%2520prior%2520work%2520that%2520forces%2520the%25203D%2520value%2520and%2520latents%2520to%2520align%2520strictly%2520with%2520RGB%2520VAE%2520latents-despite%2520their%2520fundamentally%2520different%2520distributions-we%2520show%2520that%2520such%2520alignment%2520is%2520unnecessary%2520and%2520leads%2520to%2520suboptimal%2520performance.%2520Instead%252C%2520we%2520introduce%2520a%2520new%2520data%2520normalization%2520and%2520VAE%2520training%2520strategy%2520that%2520better%2520transfers%2520diffusion%2520priors%2520and%2520greatly%2520improves%2520reconstruction%2520quality.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520MotionCrafter%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520geometry%2520reconstruction%2520and%2520dense%2520scene%2520flow%2520estimation%252C%2520delivering%252038.64%2525%2520and%252025.0%2525%2520improvements%2520in%2520geometry%2520and%2520motion%2520reconstruction%252C%2520respectively%252C%2520all%2520without%2520any%2520post-optimization.%2520Project%2520page%253A%2520https%253A//ruijiezhu94.github.io/MotionCrafter_Page%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionCrafter%3A%20Dense%20Geometry%20and%20Motion%20Reconstruction%20with%20a%204D%20VAE&entry.906535625=Ruijie%20Zhu%20and%20Jiahao%20Lu%20and%20Wenbo%20Hu%20and%20Xiaoguang%20Han%20and%20Jianfei%20Cai%20and%20Ying%20Shan%20and%20Chuanxia%20Zheng&entry.1292438233=We%20introduce%20MotionCrafter%2C%20a%20video%20diffusion-based%20framework%20that%20jointly%20reconstructs%204D%20geometry%20and%20estimates%20dense%20motion%20from%20a%20monocular%20video.%20The%20core%20of%20our%20method%20is%20a%20novel%20joint%20representation%20of%20dense%203D%20point%20maps%20and%203D%20scene%20flows%20in%20a%20shared%20coordinate%20system%2C%20and%20a%20novel%204D%20VAE%20to%20effectively%20learn%20this%20representation.%20Unlike%20prior%20work%20that%20forces%20the%203D%20value%20and%20latents%20to%20align%20strictly%20with%20RGB%20VAE%20latents-despite%20their%20fundamentally%20different%20distributions-we%20show%20that%20such%20alignment%20is%20unnecessary%20and%20leads%20to%20suboptimal%20performance.%20Instead%2C%20we%20introduce%20a%20new%20data%20normalization%20and%20VAE%20training%20strategy%20that%20better%20transfers%20diffusion%20priors%20and%20greatly%20improves%20reconstruction%20quality.%20Extensive%20experiments%20across%20multiple%20datasets%20demonstrate%20that%20MotionCrafter%20achieves%20state-of-the-art%20performance%20in%20both%20geometry%20reconstruction%20and%20dense%20scene%20flow%20estimation%2C%20delivering%2038.64%25%20and%2025.0%25%20improvements%20in%20geometry%20and%20motion%20reconstruction%2C%20respectively%2C%20all%20without%20any%20post-optimization.%20Project%20page%3A%20https%3A//ruijiezhu94.github.io/MotionCrafter_Page&entry.1838667208=http%3A//arxiv.org/abs/2602.08961v1&entry.124074799=Read"},
{"title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning", "author": "Hanzhen Wang and Jiaming Xu and Yushun Xiang and Jiayi Pan and Yongkang Zhou and Yong-Lu Li and Guohao Dai", "abstract": "Pruning is a typical acceleration technique for compute-bound models by removing computation on unimportant values. Recently, it has been applied to accelerate Vision-Language-Action (VLA) model inference. However, existing acceleration methods focus on local information from the current action step and ignore the global context, leading to >20% success rate drop and limited speedup in some scenarios. In this paper, we point out spatial-temporal consistency in VLA tasks: input images in consecutive steps exhibit high similarity, and propose the key insight that token selection should combine local information with global context of the model. Based on this, we propose SpecPrune-VLA, a training-free, two-level pruning method with heuristic control. (1) Action-level static pruning. We leverage global history and local attention to statically reduce visual tokens per action. (2) Layer-level dynamic pruning. We prune tokens adaptively per layer based on layer-wise importance. (3) Lightweight action-aware controller: We classify actions as coarse- or fine-grained by the speed of the end effector and adjust pruning aggressiveness accordingly. Extensive experiments show that SpecPrune-VLA achieves up to 1.57$\\times$ speedup in LIBERO simulation and 1.70$\\times$ on real-world tasks, with negligible success rate degradation.", "link": "http://arxiv.org/abs/2509.05614v2", "date": "2026-02-09", "relevancy": 2.4887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecPrune-VLA%3A%20Accelerating%20Vision-Language-Action%20Models%20via%20Action-Aware%20Self-Speculative%20Pruning&body=Title%3A%20SpecPrune-VLA%3A%20Accelerating%20Vision-Language-Action%20Models%20via%20Action-Aware%20Self-Speculative%20Pruning%0AAuthor%3A%20Hanzhen%20Wang%20and%20Jiaming%20Xu%20and%20Yushun%20Xiang%20and%20Jiayi%20Pan%20and%20Yongkang%20Zhou%20and%20Yong-Lu%20Li%20and%20Guohao%20Dai%0AAbstract%3A%20Pruning%20is%20a%20typical%20acceleration%20technique%20for%20compute-bound%20models%20by%20removing%20computation%20on%20unimportant%20values.%20Recently%2C%20it%20has%20been%20applied%20to%20accelerate%20Vision-Language-Action%20%28VLA%29%20model%20inference.%20However%2C%20existing%20acceleration%20methods%20focus%20on%20local%20information%20from%20the%20current%20action%20step%20and%20ignore%20the%20global%20context%2C%20leading%20to%20%3E20%25%20success%20rate%20drop%20and%20limited%20speedup%20in%20some%20scenarios.%20In%20this%20paper%2C%20we%20point%20out%20spatial-temporal%20consistency%20in%20VLA%20tasks%3A%20input%20images%20in%20consecutive%20steps%20exhibit%20high%20similarity%2C%20and%20propose%20the%20key%20insight%20that%20token%20selection%20should%20combine%20local%20information%20with%20global%20context%20of%20the%20model.%20Based%20on%20this%2C%20we%20propose%20SpecPrune-VLA%2C%20a%20training-free%2C%20two-level%20pruning%20method%20with%20heuristic%20control.%20%281%29%20Action-level%20static%20pruning.%20We%20leverage%20global%20history%20and%20local%20attention%20to%20statically%20reduce%20visual%20tokens%20per%20action.%20%282%29%20Layer-level%20dynamic%20pruning.%20We%20prune%20tokens%20adaptively%20per%20layer%20based%20on%20layer-wise%20importance.%20%283%29%20Lightweight%20action-aware%20controller%3A%20We%20classify%20actions%20as%20coarse-%20or%20fine-grained%20by%20the%20speed%20of%20the%20end%20effector%20and%20adjust%20pruning%20aggressiveness%20accordingly.%20Extensive%20experiments%20show%20that%20SpecPrune-VLA%20achieves%20up%20to%201.57%24%5Ctimes%24%20speedup%20in%20LIBERO%20simulation%20and%201.70%24%5Ctimes%24%20on%20real-world%20tasks%2C%20with%20negligible%20success%20rate%20degradation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.05614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecPrune-VLA%253A%2520Accelerating%2520Vision-Language-Action%2520Models%2520via%2520Action-Aware%2520Self-Speculative%2520Pruning%26entry.906535625%3DHanzhen%2520Wang%2520and%2520Jiaming%2520Xu%2520and%2520Yushun%2520Xiang%2520and%2520Jiayi%2520Pan%2520and%2520Yongkang%2520Zhou%2520and%2520Yong-Lu%2520Li%2520and%2520Guohao%2520Dai%26entry.1292438233%3DPruning%2520is%2520a%2520typical%2520acceleration%2520technique%2520for%2520compute-bound%2520models%2520by%2520removing%2520computation%2520on%2520unimportant%2520values.%2520Recently%252C%2520it%2520has%2520been%2520applied%2520to%2520accelerate%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520inference.%2520However%252C%2520existing%2520acceleration%2520methods%2520focus%2520on%2520local%2520information%2520from%2520the%2520current%2520action%2520step%2520and%2520ignore%2520the%2520global%2520context%252C%2520leading%2520to%2520%253E20%2525%2520success%2520rate%2520drop%2520and%2520limited%2520speedup%2520in%2520some%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520point%2520out%2520spatial-temporal%2520consistency%2520in%2520VLA%2520tasks%253A%2520input%2520images%2520in%2520consecutive%2520steps%2520exhibit%2520high%2520similarity%252C%2520and%2520propose%2520the%2520key%2520insight%2520that%2520token%2520selection%2520should%2520combine%2520local%2520information%2520with%2520global%2520context%2520of%2520the%2520model.%2520Based%2520on%2520this%252C%2520we%2520propose%2520SpecPrune-VLA%252C%2520a%2520training-free%252C%2520two-level%2520pruning%2520method%2520with%2520heuristic%2520control.%2520%25281%2529%2520Action-level%2520static%2520pruning.%2520We%2520leverage%2520global%2520history%2520and%2520local%2520attention%2520to%2520statically%2520reduce%2520visual%2520tokens%2520per%2520action.%2520%25282%2529%2520Layer-level%2520dynamic%2520pruning.%2520We%2520prune%2520tokens%2520adaptively%2520per%2520layer%2520based%2520on%2520layer-wise%2520importance.%2520%25283%2529%2520Lightweight%2520action-aware%2520controller%253A%2520We%2520classify%2520actions%2520as%2520coarse-%2520or%2520fine-grained%2520by%2520the%2520speed%2520of%2520the%2520end%2520effector%2520and%2520adjust%2520pruning%2520aggressiveness%2520accordingly.%2520Extensive%2520experiments%2520show%2520that%2520SpecPrune-VLA%2520achieves%2520up%2520to%25201.57%2524%255Ctimes%2524%2520speedup%2520in%2520LIBERO%2520simulation%2520and%25201.70%2524%255Ctimes%2524%2520on%2520real-world%2520tasks%252C%2520with%2520negligible%2520success%2520rate%2520degradation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecPrune-VLA%3A%20Accelerating%20Vision-Language-Action%20Models%20via%20Action-Aware%20Self-Speculative%20Pruning&entry.906535625=Hanzhen%20Wang%20and%20Jiaming%20Xu%20and%20Yushun%20Xiang%20and%20Jiayi%20Pan%20and%20Yongkang%20Zhou%20and%20Yong-Lu%20Li%20and%20Guohao%20Dai&entry.1292438233=Pruning%20is%20a%20typical%20acceleration%20technique%20for%20compute-bound%20models%20by%20removing%20computation%20on%20unimportant%20values.%20Recently%2C%20it%20has%20been%20applied%20to%20accelerate%20Vision-Language-Action%20%28VLA%29%20model%20inference.%20However%2C%20existing%20acceleration%20methods%20focus%20on%20local%20information%20from%20the%20current%20action%20step%20and%20ignore%20the%20global%20context%2C%20leading%20to%20%3E20%25%20success%20rate%20drop%20and%20limited%20speedup%20in%20some%20scenarios.%20In%20this%20paper%2C%20we%20point%20out%20spatial-temporal%20consistency%20in%20VLA%20tasks%3A%20input%20images%20in%20consecutive%20steps%20exhibit%20high%20similarity%2C%20and%20propose%20the%20key%20insight%20that%20token%20selection%20should%20combine%20local%20information%20with%20global%20context%20of%20the%20model.%20Based%20on%20this%2C%20we%20propose%20SpecPrune-VLA%2C%20a%20training-free%2C%20two-level%20pruning%20method%20with%20heuristic%20control.%20%281%29%20Action-level%20static%20pruning.%20We%20leverage%20global%20history%20and%20local%20attention%20to%20statically%20reduce%20visual%20tokens%20per%20action.%20%282%29%20Layer-level%20dynamic%20pruning.%20We%20prune%20tokens%20adaptively%20per%20layer%20based%20on%20layer-wise%20importance.%20%283%29%20Lightweight%20action-aware%20controller%3A%20We%20classify%20actions%20as%20coarse-%20or%20fine-grained%20by%20the%20speed%20of%20the%20end%20effector%20and%20adjust%20pruning%20aggressiveness%20accordingly.%20Extensive%20experiments%20show%20that%20SpecPrune-VLA%20achieves%20up%20to%201.57%24%5Ctimes%24%20speedup%20in%20LIBERO%20simulation%20and%201.70%24%5Ctimes%24%20on%20real-world%20tasks%2C%20with%20negligible%20success%20rate%20degradation.&entry.1838667208=http%3A//arxiv.org/abs/2509.05614v2&entry.124074799=Read"},
{"title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing", "author": "Hao Yang and Zhiyu Tan and Jia Gong and Luozheng Qin and Hesen Chen and Xiaomeng Yang and Yuqing Sun and Yuetan Lin and Mengping Yang and Hao Li", "abstract": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.", "link": "http://arxiv.org/abs/2602.08820v1", "date": "2026-02-09", "relevancy": 2.4695, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6601}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6344}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Video%202%3A%20Scaling%20MLLM-Conditioned%20Diffusion%20for%20Unified%20Video%20Generation%20and%20Editing&body=Title%3A%20Omni-Video%202%3A%20Scaling%20MLLM-Conditioned%20Diffusion%20for%20Unified%20Video%20Generation%20and%20Editing%0AAuthor%3A%20Hao%20Yang%20and%20Zhiyu%20Tan%20and%20Jia%20Gong%20and%20Luozheng%20Qin%20and%20Hesen%20Chen%20and%20Xiaomeng%20Yang%20and%20Yuqing%20Sun%20and%20Yuetan%20Lin%20and%20Mengping%20Yang%20and%20Hao%20Li%0AAbstract%3A%20We%20present%20Omni-Video%202%2C%20a%20scalable%20and%20computationally%20efficient%20model%20that%20connects%20pretrained%20multimodal%20large-language%20models%20%28MLLMs%29%20with%20video%20diffusion%20models%20for%20unified%20video%20generation%20and%20editing.%20Our%20key%20idea%20is%20to%20exploit%20the%20understanding%20and%20reasoning%20capabilities%20of%20MLLMs%20to%20produce%20explicit%20target%20captions%20to%20interpret%20user%20instructions.%20In%20this%20way%2C%20the%20rich%20contextual%20representations%20from%20the%20understanding%20model%20are%20directly%20used%20to%20guide%20the%20generative%20process%2C%20thereby%20improving%20performance%20on%20complex%20and%20compositional%20editing.%20Moreover%2C%20a%20lightweight%20adapter%20is%20developed%20to%20inject%20multimodal%20conditional%20tokens%20into%20pretrained%20text-to-video%20diffusion%20models%2C%20allowing%20maximum%20reuse%20of%20their%20powerful%20generative%20priors%20in%20a%20parameter-efficient%20manner.%20Benefiting%20from%20these%20designs%2C%20we%20scale%20up%20Omni-Video%202%20to%20a%2014B%20video%20diffusion%20model%20on%20meticulously%20curated%20training%20data%20with%20quality%2C%20supporting%20high%20quality%20text-to-video%20generation%20and%20various%20video%20editing%20tasks%20such%20as%20object%20removal%2C%20addition%2C%20background%20change%2C%20complex%20motion%20editing%2C%20%5Cemph%7Betc.%7D%20We%20evaluate%20the%20performance%20of%20Omni-Video%202%20on%20the%20FiVE%20benchmark%20for%20fine-grained%20video%20editing%20and%20the%20VBench%20benchmark%20for%20text-to-video%20generation.%20The%20results%20demonstrate%20its%20superior%20ability%20to%20follow%20complex%20compositional%20instructions%20in%20video%20editing%2C%20while%20also%20achieving%20competitive%20or%20superior%20quality%20in%20video%20generation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Video%25202%253A%2520Scaling%2520MLLM-Conditioned%2520Diffusion%2520for%2520Unified%2520Video%2520Generation%2520and%2520Editing%26entry.906535625%3DHao%2520Yang%2520and%2520Zhiyu%2520Tan%2520and%2520Jia%2520Gong%2520and%2520Luozheng%2520Qin%2520and%2520Hesen%2520Chen%2520and%2520Xiaomeng%2520Yang%2520and%2520Yuqing%2520Sun%2520and%2520Yuetan%2520Lin%2520and%2520Mengping%2520Yang%2520and%2520Hao%2520Li%26entry.1292438233%3DWe%2520present%2520Omni-Video%25202%252C%2520a%2520scalable%2520and%2520computationally%2520efficient%2520model%2520that%2520connects%2520pretrained%2520multimodal%2520large-language%2520models%2520%2528MLLMs%2529%2520with%2520video%2520diffusion%2520models%2520for%2520unified%2520video%2520generation%2520and%2520editing.%2520Our%2520key%2520idea%2520is%2520to%2520exploit%2520the%2520understanding%2520and%2520reasoning%2520capabilities%2520of%2520MLLMs%2520to%2520produce%2520explicit%2520target%2520captions%2520to%2520interpret%2520user%2520instructions.%2520In%2520this%2520way%252C%2520the%2520rich%2520contextual%2520representations%2520from%2520the%2520understanding%2520model%2520are%2520directly%2520used%2520to%2520guide%2520the%2520generative%2520process%252C%2520thereby%2520improving%2520performance%2520on%2520complex%2520and%2520compositional%2520editing.%2520Moreover%252C%2520a%2520lightweight%2520adapter%2520is%2520developed%2520to%2520inject%2520multimodal%2520conditional%2520tokens%2520into%2520pretrained%2520text-to-video%2520diffusion%2520models%252C%2520allowing%2520maximum%2520reuse%2520of%2520their%2520powerful%2520generative%2520priors%2520in%2520a%2520parameter-efficient%2520manner.%2520Benefiting%2520from%2520these%2520designs%252C%2520we%2520scale%2520up%2520Omni-Video%25202%2520to%2520a%252014B%2520video%2520diffusion%2520model%2520on%2520meticulously%2520curated%2520training%2520data%2520with%2520quality%252C%2520supporting%2520high%2520quality%2520text-to-video%2520generation%2520and%2520various%2520video%2520editing%2520tasks%2520such%2520as%2520object%2520removal%252C%2520addition%252C%2520background%2520change%252C%2520complex%2520motion%2520editing%252C%2520%255Cemph%257Betc.%257D%2520We%2520evaluate%2520the%2520performance%2520of%2520Omni-Video%25202%2520on%2520the%2520FiVE%2520benchmark%2520for%2520fine-grained%2520video%2520editing%2520and%2520the%2520VBench%2520benchmark%2520for%2520text-to-video%2520generation.%2520The%2520results%2520demonstrate%2520its%2520superior%2520ability%2520to%2520follow%2520complex%2520compositional%2520instructions%2520in%2520video%2520editing%252C%2520while%2520also%2520achieving%2520competitive%2520or%2520superior%2520quality%2520in%2520video%2520generation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Video%202%3A%20Scaling%20MLLM-Conditioned%20Diffusion%20for%20Unified%20Video%20Generation%20and%20Editing&entry.906535625=Hao%20Yang%20and%20Zhiyu%20Tan%20and%20Jia%20Gong%20and%20Luozheng%20Qin%20and%20Hesen%20Chen%20and%20Xiaomeng%20Yang%20and%20Yuqing%20Sun%20and%20Yuetan%20Lin%20and%20Mengping%20Yang%20and%20Hao%20Li&entry.1292438233=We%20present%20Omni-Video%202%2C%20a%20scalable%20and%20computationally%20efficient%20model%20that%20connects%20pretrained%20multimodal%20large-language%20models%20%28MLLMs%29%20with%20video%20diffusion%20models%20for%20unified%20video%20generation%20and%20editing.%20Our%20key%20idea%20is%20to%20exploit%20the%20understanding%20and%20reasoning%20capabilities%20of%20MLLMs%20to%20produce%20explicit%20target%20captions%20to%20interpret%20user%20instructions.%20In%20this%20way%2C%20the%20rich%20contextual%20representations%20from%20the%20understanding%20model%20are%20directly%20used%20to%20guide%20the%20generative%20process%2C%20thereby%20improving%20performance%20on%20complex%20and%20compositional%20editing.%20Moreover%2C%20a%20lightweight%20adapter%20is%20developed%20to%20inject%20multimodal%20conditional%20tokens%20into%20pretrained%20text-to-video%20diffusion%20models%2C%20allowing%20maximum%20reuse%20of%20their%20powerful%20generative%20priors%20in%20a%20parameter-efficient%20manner.%20Benefiting%20from%20these%20designs%2C%20we%20scale%20up%20Omni-Video%202%20to%20a%2014B%20video%20diffusion%20model%20on%20meticulously%20curated%20training%20data%20with%20quality%2C%20supporting%20high%20quality%20text-to-video%20generation%20and%20various%20video%20editing%20tasks%20such%20as%20object%20removal%2C%20addition%2C%20background%20change%2C%20complex%20motion%20editing%2C%20%5Cemph%7Betc.%7D%20We%20evaluate%20the%20performance%20of%20Omni-Video%202%20on%20the%20FiVE%20benchmark%20for%20fine-grained%20video%20editing%20and%20the%20VBench%20benchmark%20for%20text-to-video%20generation.%20The%20results%20demonstrate%20its%20superior%20ability%20to%20follow%20complex%20compositional%20instructions%20in%20video%20editing%2C%20while%20also%20achieving%20competitive%20or%20superior%20quality%20in%20video%20generation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.08820v1&entry.124074799=Read"},
{"title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models", "author": "Yuliang Liu and Yunchong Song and Yixuan Wang and Kewen Ge and Alex Lamb and Qipeng Guo and Kai Chen and Bowen Zhou and Zhouhan Lin", "abstract": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.", "link": "http://arxiv.org/abs/2602.08984v1", "date": "2026-02-09", "relevancy": 2.4631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20Concept%20Prediction%20in%20Discrete%20Latent%20Space%20Leads%20to%20Stronger%20Language%20Models&body=Title%3A%20Next%20Concept%20Prediction%20in%20Discrete%20Latent%20Space%20Leads%20to%20Stronger%20Language%20Models%0AAuthor%3A%20Yuliang%20Liu%20and%20Yunchong%20Song%20and%20Yixuan%20Wang%20and%20Kewen%20Ge%20and%20Alex%20Lamb%20and%20Qipeng%20Guo%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Zhouhan%20Lin%0AAbstract%3A%20We%20propose%20Next%20Concept%20Prediction%20%28NCP%29%2C%20a%20generative%20pretraining%20paradigm%20built%20on%20top%20of%20Next%20Token%20Prediction%20%28NTP%29.%20NCP%20predicts%20discrete%20concepts%20that%20span%20multiple%20tokens%2C%20thereby%20forming%20a%20more%20challenging%20pretraining%20objective.%20Our%20model%2C%20ConceptLM%2C%20quantizes%20hidden%20states%20using%20Vector%20Quantization%20and%20constructs%20a%20concept%20vocabulary.%20It%20leverages%20both%20NCP%20and%20NTP%20to%20drive%20parameter%20updates%20and%20generates%20a%20concept%20to%20guide%20the%20generation%20of%20the%20following%20tokens.%20We%20train%20ConceptLM%20from%20scratch%20at%20scales%20ranging%20from%2070M%20to%201.5B%20parameters%20with%20up%20to%20300B%20training%20data%2C%20including%20Pythia%20and%20GPT-2%20backbones.%20Results%20on%2013%20benchmarks%20show%20that%20NCP%20yields%20consistent%20performance%20gains%20over%20traditional%20token-level%20models.%20Furthermore%2C%20continual%20pretraining%20experiments%20on%20an%208B-parameter%20Llama%20model%20indicate%20that%20NCP%20can%20further%20improve%20an%20NTP-trained%20model.%20Our%20analysis%20suggests%20that%20NCP%20leads%20to%20more%20powerful%20language%20models%20by%20introducing%20a%20harder%20pretraining%20task%2C%20providing%20a%20promising%20path%20toward%20better%20language%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520Concept%2520Prediction%2520in%2520Discrete%2520Latent%2520Space%2520Leads%2520to%2520Stronger%2520Language%2520Models%26entry.906535625%3DYuliang%2520Liu%2520and%2520Yunchong%2520Song%2520and%2520Yixuan%2520Wang%2520and%2520Kewen%2520Ge%2520and%2520Alex%2520Lamb%2520and%2520Qipeng%2520Guo%2520and%2520Kai%2520Chen%2520and%2520Bowen%2520Zhou%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3DWe%2520propose%2520Next%2520Concept%2520Prediction%2520%2528NCP%2529%252C%2520a%2520generative%2520pretraining%2520paradigm%2520built%2520on%2520top%2520of%2520Next%2520Token%2520Prediction%2520%2528NTP%2529.%2520NCP%2520predicts%2520discrete%2520concepts%2520that%2520span%2520multiple%2520tokens%252C%2520thereby%2520forming%2520a%2520more%2520challenging%2520pretraining%2520objective.%2520Our%2520model%252C%2520ConceptLM%252C%2520quantizes%2520hidden%2520states%2520using%2520Vector%2520Quantization%2520and%2520constructs%2520a%2520concept%2520vocabulary.%2520It%2520leverages%2520both%2520NCP%2520and%2520NTP%2520to%2520drive%2520parameter%2520updates%2520and%2520generates%2520a%2520concept%2520to%2520guide%2520the%2520generation%2520of%2520the%2520following%2520tokens.%2520We%2520train%2520ConceptLM%2520from%2520scratch%2520at%2520scales%2520ranging%2520from%252070M%2520to%25201.5B%2520parameters%2520with%2520up%2520to%2520300B%2520training%2520data%252C%2520including%2520Pythia%2520and%2520GPT-2%2520backbones.%2520Results%2520on%252013%2520benchmarks%2520show%2520that%2520NCP%2520yields%2520consistent%2520performance%2520gains%2520over%2520traditional%2520token-level%2520models.%2520Furthermore%252C%2520continual%2520pretraining%2520experiments%2520on%2520an%25208B-parameter%2520Llama%2520model%2520indicate%2520that%2520NCP%2520can%2520further%2520improve%2520an%2520NTP-trained%2520model.%2520Our%2520analysis%2520suggests%2520that%2520NCP%2520leads%2520to%2520more%2520powerful%2520language%2520models%2520by%2520introducing%2520a%2520harder%2520pretraining%2520task%252C%2520providing%2520a%2520promising%2520path%2520toward%2520better%2520language%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20Concept%20Prediction%20in%20Discrete%20Latent%20Space%20Leads%20to%20Stronger%20Language%20Models&entry.906535625=Yuliang%20Liu%20and%20Yunchong%20Song%20and%20Yixuan%20Wang%20and%20Kewen%20Ge%20and%20Alex%20Lamb%20and%20Qipeng%20Guo%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Zhouhan%20Lin&entry.1292438233=We%20propose%20Next%20Concept%20Prediction%20%28NCP%29%2C%20a%20generative%20pretraining%20paradigm%20built%20on%20top%20of%20Next%20Token%20Prediction%20%28NTP%29.%20NCP%20predicts%20discrete%20concepts%20that%20span%20multiple%20tokens%2C%20thereby%20forming%20a%20more%20challenging%20pretraining%20objective.%20Our%20model%2C%20ConceptLM%2C%20quantizes%20hidden%20states%20using%20Vector%20Quantization%20and%20constructs%20a%20concept%20vocabulary.%20It%20leverages%20both%20NCP%20and%20NTP%20to%20drive%20parameter%20updates%20and%20generates%20a%20concept%20to%20guide%20the%20generation%20of%20the%20following%20tokens.%20We%20train%20ConceptLM%20from%20scratch%20at%20scales%20ranging%20from%2070M%20to%201.5B%20parameters%20with%20up%20to%20300B%20training%20data%2C%20including%20Pythia%20and%20GPT-2%20backbones.%20Results%20on%2013%20benchmarks%20show%20that%20NCP%20yields%20consistent%20performance%20gains%20over%20traditional%20token-level%20models.%20Furthermore%2C%20continual%20pretraining%20experiments%20on%20an%208B-parameter%20Llama%20model%20indicate%20that%20NCP%20can%20further%20improve%20an%20NTP-trained%20model.%20Our%20analysis%20suggests%20that%20NCP%20leads%20to%20more%20powerful%20language%20models%20by%20introducing%20a%20harder%20pretraining%20task%2C%20providing%20a%20promising%20path%20toward%20better%20language%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2602.08984v1&entry.124074799=Read"},
{"title": "Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity", "author": "Chenhe Du and Qing Wu and Xuanyu Tian and Jingyi Yu and Hongjiang Wei and Yuyao Zhang", "abstract": "3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS", "link": "http://arxiv.org/abs/2602.04162v2", "date": "2026-02-09", "relevancy": 2.4618, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6289}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6128}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%202D%20Diffusion%20Models%20for%203D%20Medical%20Imaging%20with%20Inter-Slice%20Consistent%20Stochasticity&body=Title%3A%20Improving%202D%20Diffusion%20Models%20for%203D%20Medical%20Imaging%20with%20Inter-Slice%20Consistent%20Stochasticity%0AAuthor%3A%20Chenhe%20Du%20and%20Qing%20Wu%20and%20Xuanyu%20Tian%20and%20Jingyi%20Yu%20and%20Hongjiang%20Wei%20and%20Yuyao%20Zhang%0AAbstract%3A%203D%20medical%20imaging%20is%20in%20high%20demand%20and%20essential%20for%20clinical%20diagnosis%20and%20scientific%20research.%20Currently%2C%20diffusion%20models%20%28DMs%29%20have%20become%20an%20effective%20tool%20for%20medical%20imaging%20reconstruction%20thanks%20to%20their%20ability%20to%20learn%20rich%2C%20high-quality%20data%20priors.%20However%2C%20learning%20the%203D%20data%20distribution%20with%20DMs%20in%20medical%20imaging%20is%20challenging%2C%20not%20only%20due%20to%20the%20difficulties%20in%20data%20collection%20but%20also%20because%20of%20the%20significant%20computational%20burden%20during%20model%20training.%20A%20common%20compromise%20is%20to%20train%20the%20DMs%20on%202D%20data%20priors%20and%20reconstruct%20stacked%202D%20slices%20to%20address%203D%20medical%20inverse%20problems.%20However%2C%20the%20intrinsic%20randomness%20of%20diffusion%20sampling%20causes%20severe%20inter-slice%20discontinuities%20of%20reconstructed%203D%20volumes.%20Existing%20methods%20often%20enforce%20continuity%20regularizations%20along%20the%20z-axis%2C%20which%20introduces%20sensitive%20hyper-parameters%20and%20may%20lead%20to%20over-smoothing%20results.%20In%20this%20work%2C%20we%20revisit%20the%20origin%20of%20stochasticity%20in%20diffusion%20sampling%20and%20introduce%20Inter-Slice%20Consistent%20Stochasticity%20%28ISCS%29%2C%20a%20simple%20yet%20effective%20strategy%20that%20encourages%20interslice%20consistency%20during%20diffusion%20sampling.%20Our%20key%20idea%20is%20to%20control%20the%20consistency%20of%20stochastic%20noise%20components%20during%20diffusion%20sampling%2C%20thereby%20aligning%20their%20sampling%20trajectories%20without%20adding%20any%20new%20loss%20terms%20or%20optimization%20steps.%20Importantly%2C%20the%20proposed%20ISCS%20is%20plug-and-play%20and%20can%20be%20dropped%20into%20any%202D%20trained%20diffusion%20based%203D%20reconstruction%20pipeline%20without%20additional%20computational%20cost.%20Experiments%20on%20several%20medical%20imaging%20problems%20show%20that%20our%20method%20can%20effectively%20improve%20the%20performance%20of%20medical%203D%20imaging%20problems%20based%20on%202D%20diffusion%20models.%20Our%20findings%20suggest%20that%20controlling%20inter-slice%20stochasticity%20is%20a%20principled%20and%20practically%20attractive%20route%20toward%20high-fidelity%203D%20medical%20imaging%20with%202D%20diffusion%20priors.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/duchenhe/ISCS%0ALink%3A%20http%3A//arxiv.org/abs/2602.04162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%25202D%2520Diffusion%2520Models%2520for%25203D%2520Medical%2520Imaging%2520with%2520Inter-Slice%2520Consistent%2520Stochasticity%26entry.906535625%3DChenhe%2520Du%2520and%2520Qing%2520Wu%2520and%2520Xuanyu%2520Tian%2520and%2520Jingyi%2520Yu%2520and%2520Hongjiang%2520Wei%2520and%2520Yuyao%2520Zhang%26entry.1292438233%3D3D%2520medical%2520imaging%2520is%2520in%2520high%2520demand%2520and%2520essential%2520for%2520clinical%2520diagnosis%2520and%2520scientific%2520research.%2520Currently%252C%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520become%2520an%2520effective%2520tool%2520for%2520medical%2520imaging%2520reconstruction%2520thanks%2520to%2520their%2520ability%2520to%2520learn%2520rich%252C%2520high-quality%2520data%2520priors.%2520However%252C%2520learning%2520the%25203D%2520data%2520distribution%2520with%2520DMs%2520in%2520medical%2520imaging%2520is%2520challenging%252C%2520not%2520only%2520due%2520to%2520the%2520difficulties%2520in%2520data%2520collection%2520but%2520also%2520because%2520of%2520the%2520significant%2520computational%2520burden%2520during%2520model%2520training.%2520A%2520common%2520compromise%2520is%2520to%2520train%2520the%2520DMs%2520on%25202D%2520data%2520priors%2520and%2520reconstruct%2520stacked%25202D%2520slices%2520to%2520address%25203D%2520medical%2520inverse%2520problems.%2520However%252C%2520the%2520intrinsic%2520randomness%2520of%2520diffusion%2520sampling%2520causes%2520severe%2520inter-slice%2520discontinuities%2520of%2520reconstructed%25203D%2520volumes.%2520Existing%2520methods%2520often%2520enforce%2520continuity%2520regularizations%2520along%2520the%2520z-axis%252C%2520which%2520introduces%2520sensitive%2520hyper-parameters%2520and%2520may%2520lead%2520to%2520over-smoothing%2520results.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520origin%2520of%2520stochasticity%2520in%2520diffusion%2520sampling%2520and%2520introduce%2520Inter-Slice%2520Consistent%2520Stochasticity%2520%2528ISCS%2529%252C%2520a%2520simple%2520yet%2520effective%2520strategy%2520that%2520encourages%2520interslice%2520consistency%2520during%2520diffusion%2520sampling.%2520Our%2520key%2520idea%2520is%2520to%2520control%2520the%2520consistency%2520of%2520stochastic%2520noise%2520components%2520during%2520diffusion%2520sampling%252C%2520thereby%2520aligning%2520their%2520sampling%2520trajectories%2520without%2520adding%2520any%2520new%2520loss%2520terms%2520or%2520optimization%2520steps.%2520Importantly%252C%2520the%2520proposed%2520ISCS%2520is%2520plug-and-play%2520and%2520can%2520be%2520dropped%2520into%2520any%25202D%2520trained%2520diffusion%2520based%25203D%2520reconstruction%2520pipeline%2520without%2520additional%2520computational%2520cost.%2520Experiments%2520on%2520several%2520medical%2520imaging%2520problems%2520show%2520that%2520our%2520method%2520can%2520effectively%2520improve%2520the%2520performance%2520of%2520medical%25203D%2520imaging%2520problems%2520based%2520on%25202D%2520diffusion%2520models.%2520Our%2520findings%2520suggest%2520that%2520controlling%2520inter-slice%2520stochasticity%2520is%2520a%2520principled%2520and%2520practically%2520attractive%2520route%2520toward%2520high-fidelity%25203D%2520medical%2520imaging%2520with%25202D%2520diffusion%2520priors.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/duchenhe/ISCS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%202D%20Diffusion%20Models%20for%203D%20Medical%20Imaging%20with%20Inter-Slice%20Consistent%20Stochasticity&entry.906535625=Chenhe%20Du%20and%20Qing%20Wu%20and%20Xuanyu%20Tian%20and%20Jingyi%20Yu%20and%20Hongjiang%20Wei%20and%20Yuyao%20Zhang&entry.1292438233=3D%20medical%20imaging%20is%20in%20high%20demand%20and%20essential%20for%20clinical%20diagnosis%20and%20scientific%20research.%20Currently%2C%20diffusion%20models%20%28DMs%29%20have%20become%20an%20effective%20tool%20for%20medical%20imaging%20reconstruction%20thanks%20to%20their%20ability%20to%20learn%20rich%2C%20high-quality%20data%20priors.%20However%2C%20learning%20the%203D%20data%20distribution%20with%20DMs%20in%20medical%20imaging%20is%20challenging%2C%20not%20only%20due%20to%20the%20difficulties%20in%20data%20collection%20but%20also%20because%20of%20the%20significant%20computational%20burden%20during%20model%20training.%20A%20common%20compromise%20is%20to%20train%20the%20DMs%20on%202D%20data%20priors%20and%20reconstruct%20stacked%202D%20slices%20to%20address%203D%20medical%20inverse%20problems.%20However%2C%20the%20intrinsic%20randomness%20of%20diffusion%20sampling%20causes%20severe%20inter-slice%20discontinuities%20of%20reconstructed%203D%20volumes.%20Existing%20methods%20often%20enforce%20continuity%20regularizations%20along%20the%20z-axis%2C%20which%20introduces%20sensitive%20hyper-parameters%20and%20may%20lead%20to%20over-smoothing%20results.%20In%20this%20work%2C%20we%20revisit%20the%20origin%20of%20stochasticity%20in%20diffusion%20sampling%20and%20introduce%20Inter-Slice%20Consistent%20Stochasticity%20%28ISCS%29%2C%20a%20simple%20yet%20effective%20strategy%20that%20encourages%20interslice%20consistency%20during%20diffusion%20sampling.%20Our%20key%20idea%20is%20to%20control%20the%20consistency%20of%20stochastic%20noise%20components%20during%20diffusion%20sampling%2C%20thereby%20aligning%20their%20sampling%20trajectories%20without%20adding%20any%20new%20loss%20terms%20or%20optimization%20steps.%20Importantly%2C%20the%20proposed%20ISCS%20is%20plug-and-play%20and%20can%20be%20dropped%20into%20any%202D%20trained%20diffusion%20based%203D%20reconstruction%20pipeline%20without%20additional%20computational%20cost.%20Experiments%20on%20several%20medical%20imaging%20problems%20show%20that%20our%20method%20can%20effectively%20improve%20the%20performance%20of%20medical%203D%20imaging%20problems%20based%20on%202D%20diffusion%20models.%20Our%20findings%20suggest%20that%20controlling%20inter-slice%20stochasticity%20is%20a%20principled%20and%20practically%20attractive%20route%20toward%20high-fidelity%203D%20medical%20imaging%20with%202D%20diffusion%20priors.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/duchenhe/ISCS&entry.1838667208=http%3A//arxiv.org/abs/2602.04162v2&entry.124074799=Read"},
{"title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor", "author": "Shaoang Zhang and Yazhe Niu", "abstract": "Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.", "link": "http://arxiv.org/abs/2602.08517v1", "date": "2026-02-09", "relevancy": 2.4417, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5073}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4852}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeTensor%3A%20Boost%20AI%20System%20on%20Nested%20Data%20with%20Constrained%20Tree-Like%20Tensor&body=Title%3A%20TreeTensor%3A%20Boost%20AI%20System%20on%20Nested%20Data%20with%20Constrained%20Tree-Like%20Tensor%0AAuthor%3A%20Shaoang%20Zhang%20and%20Yazhe%20Niu%0AAbstract%3A%20Tensor%20is%20the%20most%20basic%20and%20essential%20data%20structure%20of%20nowadays%20artificial%20intelligence%20%28AI%29%20system.%20The%20natural%20properties%20of%20Tensor%2C%20especially%20the%20memory-continuity%20and%20slice-independence%2C%20make%20it%20feasible%20for%20training%20system%20to%20leverage%20parallel%20computing%20unit%20like%20GPU%20to%20process%20data%20simultaneously%20in%20batch%2C%20spatial%20or%20temporal%20dimensions.%20However%2C%20if%20we%20look%20beyond%20perception%20tasks%2C%20the%20data%20in%20a%20complicated%20cognitive%20AI%20system%20usually%20has%20hierarchical%20structures%20%28i.e.%20nested%20data%29%20with%20various%20modalities.%20They%20are%20inconvenient%20and%20inefficient%20to%20program%20directly%20with%20conventional%20Tensor%20with%20fixed%20shape.%20To%20address%20this%20issue%2C%20we%20summarize%20two%20main%20computational%20patterns%20of%20nested%20data%2C%20and%20then%20propose%20a%20general%20nested%20data%20container%3A%20TreeTensor.%20Through%20various%20constraints%20and%20magic%20utilities%20of%20TreeTensor%2C%20one%20can%20apply%20arbitrary%20functions%20and%20operations%20to%20nested%20data%20with%20almost%20zero%20cost%2C%20including%20some%20famous%20machine%20learning%20libraries%2C%20such%20as%20Scikit-Learn%2C%20Numpy%20and%20PyTorch.%20Our%20approach%20utilizes%20a%20constrained%20tree-structure%20perspective%20to%20systematically%20model%20data%20relationships%2C%20and%20it%20can%20also%20easily%20be%20combined%20with%20other%20methods%20to%20extend%20more%20usages%2C%20such%20as%20asynchronous%20execution%20and%20variable-length%20data%20computation.%20Detailed%20examples%20and%20benchmarks%20show%20TreeTensor%20not%20only%20provides%20powerful%20usability%20in%20various%20problems%2C%20especially%20one%20of%20the%20most%20complicated%20AI%20systems%20at%20present%3A%20AlphaStar%20for%20StarCraftII%2C%20but%20also%20exhibits%20excellent%20runtime%20efficiency%20without%20any%20overhead.%20Our%20project%20is%20available%20at%20https%3A//github.com/opendilab/DI-treetensor.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeTensor%253A%2520Boost%2520AI%2520System%2520on%2520Nested%2520Data%2520with%2520Constrained%2520Tree-Like%2520Tensor%26entry.906535625%3DShaoang%2520Zhang%2520and%2520Yazhe%2520Niu%26entry.1292438233%3DTensor%2520is%2520the%2520most%2520basic%2520and%2520essential%2520data%2520structure%2520of%2520nowadays%2520artificial%2520intelligence%2520%2528AI%2529%2520system.%2520The%2520natural%2520properties%2520of%2520Tensor%252C%2520especially%2520the%2520memory-continuity%2520and%2520slice-independence%252C%2520make%2520it%2520feasible%2520for%2520training%2520system%2520to%2520leverage%2520parallel%2520computing%2520unit%2520like%2520GPU%2520to%2520process%2520data%2520simultaneously%2520in%2520batch%252C%2520spatial%2520or%2520temporal%2520dimensions.%2520However%252C%2520if%2520we%2520look%2520beyond%2520perception%2520tasks%252C%2520the%2520data%2520in%2520a%2520complicated%2520cognitive%2520AI%2520system%2520usually%2520has%2520hierarchical%2520structures%2520%2528i.e.%2520nested%2520data%2529%2520with%2520various%2520modalities.%2520They%2520are%2520inconvenient%2520and%2520inefficient%2520to%2520program%2520directly%2520with%2520conventional%2520Tensor%2520with%2520fixed%2520shape.%2520To%2520address%2520this%2520issue%252C%2520we%2520summarize%2520two%2520main%2520computational%2520patterns%2520of%2520nested%2520data%252C%2520and%2520then%2520propose%2520a%2520general%2520nested%2520data%2520container%253A%2520TreeTensor.%2520Through%2520various%2520constraints%2520and%2520magic%2520utilities%2520of%2520TreeTensor%252C%2520one%2520can%2520apply%2520arbitrary%2520functions%2520and%2520operations%2520to%2520nested%2520data%2520with%2520almost%2520zero%2520cost%252C%2520including%2520some%2520famous%2520machine%2520learning%2520libraries%252C%2520such%2520as%2520Scikit-Learn%252C%2520Numpy%2520and%2520PyTorch.%2520Our%2520approach%2520utilizes%2520a%2520constrained%2520tree-structure%2520perspective%2520to%2520systematically%2520model%2520data%2520relationships%252C%2520and%2520it%2520can%2520also%2520easily%2520be%2520combined%2520with%2520other%2520methods%2520to%2520extend%2520more%2520usages%252C%2520such%2520as%2520asynchronous%2520execution%2520and%2520variable-length%2520data%2520computation.%2520Detailed%2520examples%2520and%2520benchmarks%2520show%2520TreeTensor%2520not%2520only%2520provides%2520powerful%2520usability%2520in%2520various%2520problems%252C%2520especially%2520one%2520of%2520the%2520most%2520complicated%2520AI%2520systems%2520at%2520present%253A%2520AlphaStar%2520for%2520StarCraftII%252C%2520but%2520also%2520exhibits%2520excellent%2520runtime%2520efficiency%2520without%2520any%2520overhead.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//github.com/opendilab/DI-treetensor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeTensor%3A%20Boost%20AI%20System%20on%20Nested%20Data%20with%20Constrained%20Tree-Like%20Tensor&entry.906535625=Shaoang%20Zhang%20and%20Yazhe%20Niu&entry.1292438233=Tensor%20is%20the%20most%20basic%20and%20essential%20data%20structure%20of%20nowadays%20artificial%20intelligence%20%28AI%29%20system.%20The%20natural%20properties%20of%20Tensor%2C%20especially%20the%20memory-continuity%20and%20slice-independence%2C%20make%20it%20feasible%20for%20training%20system%20to%20leverage%20parallel%20computing%20unit%20like%20GPU%20to%20process%20data%20simultaneously%20in%20batch%2C%20spatial%20or%20temporal%20dimensions.%20However%2C%20if%20we%20look%20beyond%20perception%20tasks%2C%20the%20data%20in%20a%20complicated%20cognitive%20AI%20system%20usually%20has%20hierarchical%20structures%20%28i.e.%20nested%20data%29%20with%20various%20modalities.%20They%20are%20inconvenient%20and%20inefficient%20to%20program%20directly%20with%20conventional%20Tensor%20with%20fixed%20shape.%20To%20address%20this%20issue%2C%20we%20summarize%20two%20main%20computational%20patterns%20of%20nested%20data%2C%20and%20then%20propose%20a%20general%20nested%20data%20container%3A%20TreeTensor.%20Through%20various%20constraints%20and%20magic%20utilities%20of%20TreeTensor%2C%20one%20can%20apply%20arbitrary%20functions%20and%20operations%20to%20nested%20data%20with%20almost%20zero%20cost%2C%20including%20some%20famous%20machine%20learning%20libraries%2C%20such%20as%20Scikit-Learn%2C%20Numpy%20and%20PyTorch.%20Our%20approach%20utilizes%20a%20constrained%20tree-structure%20perspective%20to%20systematically%20model%20data%20relationships%2C%20and%20it%20can%20also%20easily%20be%20combined%20with%20other%20methods%20to%20extend%20more%20usages%2C%20such%20as%20asynchronous%20execution%20and%20variable-length%20data%20computation.%20Detailed%20examples%20and%20benchmarks%20show%20TreeTensor%20not%20only%20provides%20powerful%20usability%20in%20various%20problems%2C%20especially%20one%20of%20the%20most%20complicated%20AI%20systems%20at%20present%3A%20AlphaStar%20for%20StarCraftII%2C%20but%20also%20exhibits%20excellent%20runtime%20efficiency%20without%20any%20overhead.%20Our%20project%20is%20available%20at%20https%3A//github.com/opendilab/DI-treetensor.&entry.1838667208=http%3A//arxiv.org/abs/2602.08517v1&entry.124074799=Read"},
{"title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework", "author": "Johannes Thalhammer and Tina Dorosti and Sebastian Peterhansl and Daniela Pfeiffer and Franz Pfeiffer and Florian Schaff", "abstract": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.", "link": "http://arxiv.org/abs/2602.08727v1", "date": "2026-02-09", "relevancy": 2.4375, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6146}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6146}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artifact%20Reduction%20in%20Undersampled%203D%20Cone-Beam%20CTs%20using%20a%20Hybrid%202D-3D%20CNN%20Framework&body=Title%3A%20Artifact%20Reduction%20in%20Undersampled%203D%20Cone-Beam%20CTs%20using%20a%20Hybrid%202D-3D%20CNN%20Framework%0AAuthor%3A%20Johannes%20Thalhammer%20and%20Tina%20Dorosti%20and%20Sebastian%20Peterhansl%20and%20Daniela%20Pfeiffer%20and%20Franz%20Pfeiffer%20and%20Florian%20Schaff%0AAbstract%3A%20Undersampled%20CT%20volumes%20minimize%20acquisition%20time%20and%20radiation%20exposure%20but%20introduce%20artifacts%20degrading%20image%20quality%20and%20diagnostic%20utility.%20Reducing%20these%20artifacts%20is%20critical%20for%20high-quality%20imaging.%20We%20propose%20a%20computationally%20efficient%20hybrid%20deep-learning%20framework%20that%20combines%20the%20strengths%20of%202D%20and%203D%20models.%20First%2C%20a%202D%20U-Net%20operates%20on%20individual%20slices%20of%20undersampled%20CT%20volumes%20to%20extract%20feature%20maps.%20These%20slice-wise%20feature%20maps%20are%20then%20stacked%20across%20the%20volume%20and%20used%20as%20input%20to%20a%203D%20decoder%2C%20which%20utilizes%20contextual%20information%20across%20slices%20to%20predict%20an%20artifact-free%203D%20CT%20volume.%20The%20proposed%20two-stage%20approach%20balances%20the%20computational%20efficiency%20of%202D%20processing%20with%20the%20volumetric%20consistency%20provided%20by%203D%20modeling.%20The%20results%20show%20substantial%20improvements%20in%20inter-slice%20consistency%20in%20coronal%20and%20sagittal%20direction%20with%20low%20computational%20overhead.%20This%20hybrid%20framework%20presents%20a%20robust%20and%20efficient%20solution%20for%20high-quality%203D%20CT%20image%20post-processing.%20The%20code%20of%20this%20project%20can%20be%20found%20on%20github%3A%20https%3A//github.com/J-3TO/2D-3DCNN_sparseview/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtifact%2520Reduction%2520in%2520Undersampled%25203D%2520Cone-Beam%2520CTs%2520using%2520a%2520Hybrid%25202D-3D%2520CNN%2520Framework%26entry.906535625%3DJohannes%2520Thalhammer%2520and%2520Tina%2520Dorosti%2520and%2520Sebastian%2520Peterhansl%2520and%2520Daniela%2520Pfeiffer%2520and%2520Franz%2520Pfeiffer%2520and%2520Florian%2520Schaff%26entry.1292438233%3DUndersampled%2520CT%2520volumes%2520minimize%2520acquisition%2520time%2520and%2520radiation%2520exposure%2520but%2520introduce%2520artifacts%2520degrading%2520image%2520quality%2520and%2520diagnostic%2520utility.%2520Reducing%2520these%2520artifacts%2520is%2520critical%2520for%2520high-quality%2520imaging.%2520We%2520propose%2520a%2520computationally%2520efficient%2520hybrid%2520deep-learning%2520framework%2520that%2520combines%2520the%2520strengths%2520of%25202D%2520and%25203D%2520models.%2520First%252C%2520a%25202D%2520U-Net%2520operates%2520on%2520individual%2520slices%2520of%2520undersampled%2520CT%2520volumes%2520to%2520extract%2520feature%2520maps.%2520These%2520slice-wise%2520feature%2520maps%2520are%2520then%2520stacked%2520across%2520the%2520volume%2520and%2520used%2520as%2520input%2520to%2520a%25203D%2520decoder%252C%2520which%2520utilizes%2520contextual%2520information%2520across%2520slices%2520to%2520predict%2520an%2520artifact-free%25203D%2520CT%2520volume.%2520The%2520proposed%2520two-stage%2520approach%2520balances%2520the%2520computational%2520efficiency%2520of%25202D%2520processing%2520with%2520the%2520volumetric%2520consistency%2520provided%2520by%25203D%2520modeling.%2520The%2520results%2520show%2520substantial%2520improvements%2520in%2520inter-slice%2520consistency%2520in%2520coronal%2520and%2520sagittal%2520direction%2520with%2520low%2520computational%2520overhead.%2520This%2520hybrid%2520framework%2520presents%2520a%2520robust%2520and%2520efficient%2520solution%2520for%2520high-quality%25203D%2520CT%2520image%2520post-processing.%2520The%2520code%2520of%2520this%2520project%2520can%2520be%2520found%2520on%2520github%253A%2520https%253A//github.com/J-3TO/2D-3DCNN_sparseview/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artifact%20Reduction%20in%20Undersampled%203D%20Cone-Beam%20CTs%20using%20a%20Hybrid%202D-3D%20CNN%20Framework&entry.906535625=Johannes%20Thalhammer%20and%20Tina%20Dorosti%20and%20Sebastian%20Peterhansl%20and%20Daniela%20Pfeiffer%20and%20Franz%20Pfeiffer%20and%20Florian%20Schaff&entry.1292438233=Undersampled%20CT%20volumes%20minimize%20acquisition%20time%20and%20radiation%20exposure%20but%20introduce%20artifacts%20degrading%20image%20quality%20and%20diagnostic%20utility.%20Reducing%20these%20artifacts%20is%20critical%20for%20high-quality%20imaging.%20We%20propose%20a%20computationally%20efficient%20hybrid%20deep-learning%20framework%20that%20combines%20the%20strengths%20of%202D%20and%203D%20models.%20First%2C%20a%202D%20U-Net%20operates%20on%20individual%20slices%20of%20undersampled%20CT%20volumes%20to%20extract%20feature%20maps.%20These%20slice-wise%20feature%20maps%20are%20then%20stacked%20across%20the%20volume%20and%20used%20as%20input%20to%20a%203D%20decoder%2C%20which%20utilizes%20contextual%20information%20across%20slices%20to%20predict%20an%20artifact-free%203D%20CT%20volume.%20The%20proposed%20two-stage%20approach%20balances%20the%20computational%20efficiency%20of%202D%20processing%20with%20the%20volumetric%20consistency%20provided%20by%203D%20modeling.%20The%20results%20show%20substantial%20improvements%20in%20inter-slice%20consistency%20in%20coronal%20and%20sagittal%20direction%20with%20low%20computational%20overhead.%20This%20hybrid%20framework%20presents%20a%20robust%20and%20efficient%20solution%20for%20high-quality%203D%20CT%20image%20post-processing.%20The%20code%20of%20this%20project%20can%20be%20found%20on%20github%3A%20https%3A//github.com/J-3TO/2D-3DCNN_sparseview/.&entry.1838667208=http%3A//arxiv.org/abs/2602.08727v1&entry.124074799=Read"},
{"title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "author": "Yiyang Cao and Yunze Deng and Ziyu Lin and Bin Feng and Xinggang Wang and Wenyu Liu and Dandan Zheng and Jingdong Chen", "abstract": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.", "link": "http://arxiv.org/abs/2602.08462v1", "date": "2026-02-09", "relevancy": 2.4367, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6288}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6261}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriC-Motion%3A%20Tri-Domain%20Causal%20Modeling%20Grounded%20Text-to-Motion%20Generation&body=Title%3A%20TriC-Motion%3A%20Tri-Domain%20Causal%20Modeling%20Grounded%20Text-to-Motion%20Generation%0AAuthor%3A%20Yiyang%20Cao%20and%20Yunze%20Deng%20and%20Ziyu%20Lin%20and%20Bin%20Feng%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu%20and%20Dandan%20Zheng%20and%20Jingdong%20Chen%0AAbstract%3A%20Text-to-motion%20generation%2C%20a%20rapidly%20evolving%20field%20in%20computer%20vision%2C%20aims%20to%20produce%20realistic%20and%20text-aligned%20motion%20sequences.%20Current%20methods%20primarily%20focus%20on%20spatial-temporal%20modeling%20or%20independent%20frequency%20domain%20analysis%2C%20lacking%20a%20unified%20framework%20for%20joint%20optimization%20across%20spatial%2C%20temporal%2C%20and%20frequency%20domains.%20This%20limitation%20hinders%20the%20model%27s%20ability%20to%20leverage%20information%20from%20all%20domains%20simultaneously%2C%20leading%20to%20suboptimal%20generation%20quality.%20Additionally%2C%20in%20motion%20generation%20frameworks%2C%20motion-irrelevant%20cues%20caused%20by%20noise%20are%20often%20entangled%20with%20features%20that%20contribute%20positively%20to%20generation%2C%20thereby%20leading%20to%20motion%20distortion.%20To%20address%20these%20issues%2C%20we%20propose%20Tri-Domain%20Causal%20Text-to-Motion%20Generation%20%28TriC-Motion%29%2C%20a%20novel%20diffusion-based%20framework%20integrating%20spatial-temporal-frequency-domain%20modeling%20with%20causal%20intervention.%20TriC-Motion%20includes%20three%20core%20modeling%20modules%20for%20domain-specific%20modeling%2C%20namely%20Temporal%20Motion%20Encoding%2C%20Spatial%20Topology%20Modeling%2C%20and%20Hybrid%20Frequency%20Analysis.%20After%20comprehensive%20modeling%2C%20a%20Score-guided%20Tri-domain%20Fusion%20module%20integrates%20valuable%20information%20from%20the%20triple%20domains%2C%20simultaneously%20ensuring%20temporal%20consistency%2C%20spatial%20topology%2C%20motion%20trends%2C%20and%20dynamics.%20Moreover%2C%20the%20Causality-based%20Counterfactual%20Motion%20Disentangler%20is%20meticulously%20designed%20to%20expose%20motion-irrelevant%20cues%20to%20eliminate%20noise%2C%20disentangling%20the%20real%20modeling%20contributions%20of%20each%20domain%20for%20superior%20generation.%20Extensive%20experimental%20results%20validate%20that%20TriC-Motion%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20methods%2C%20attaining%20an%20outstanding%20R%401%20of%200.612%20on%20the%20HumanML3D%20dataset.%20These%20results%20demonstrate%20its%20capability%20to%20generate%20high-fidelity%2C%20coherent%2C%20diverse%2C%20and%20text-aligned%20motion%20sequences.%20Code%20is%20available%20at%3A%20https%3A//caoyiyang1105.github.io/TriC-Motion/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriC-Motion%253A%2520Tri-Domain%2520Causal%2520Modeling%2520Grounded%2520Text-to-Motion%2520Generation%26entry.906535625%3DYiyang%2520Cao%2520and%2520Yunze%2520Deng%2520and%2520Ziyu%2520Lin%2520and%2520Bin%2520Feng%2520and%2520Xinggang%2520Wang%2520and%2520Wenyu%2520Liu%2520and%2520Dandan%2520Zheng%2520and%2520Jingdong%2520Chen%26entry.1292438233%3DText-to-motion%2520generation%252C%2520a%2520rapidly%2520evolving%2520field%2520in%2520computer%2520vision%252C%2520aims%2520to%2520produce%2520realistic%2520and%2520text-aligned%2520motion%2520sequences.%2520Current%2520methods%2520primarily%2520focus%2520on%2520spatial-temporal%2520modeling%2520or%2520independent%2520frequency%2520domain%2520analysis%252C%2520lacking%2520a%2520unified%2520framework%2520for%2520joint%2520optimization%2520across%2520spatial%252C%2520temporal%252C%2520and%2520frequency%2520domains.%2520This%2520limitation%2520hinders%2520the%2520model%2527s%2520ability%2520to%2520leverage%2520information%2520from%2520all%2520domains%2520simultaneously%252C%2520leading%2520to%2520suboptimal%2520generation%2520quality.%2520Additionally%252C%2520in%2520motion%2520generation%2520frameworks%252C%2520motion-irrelevant%2520cues%2520caused%2520by%2520noise%2520are%2520often%2520entangled%2520with%2520features%2520that%2520contribute%2520positively%2520to%2520generation%252C%2520thereby%2520leading%2520to%2520motion%2520distortion.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Tri-Domain%2520Causal%2520Text-to-Motion%2520Generation%2520%2528TriC-Motion%2529%252C%2520a%2520novel%2520diffusion-based%2520framework%2520integrating%2520spatial-temporal-frequency-domain%2520modeling%2520with%2520causal%2520intervention.%2520TriC-Motion%2520includes%2520three%2520core%2520modeling%2520modules%2520for%2520domain-specific%2520modeling%252C%2520namely%2520Temporal%2520Motion%2520Encoding%252C%2520Spatial%2520Topology%2520Modeling%252C%2520and%2520Hybrid%2520Frequency%2520Analysis.%2520After%2520comprehensive%2520modeling%252C%2520a%2520Score-guided%2520Tri-domain%2520Fusion%2520module%2520integrates%2520valuable%2520information%2520from%2520the%2520triple%2520domains%252C%2520simultaneously%2520ensuring%2520temporal%2520consistency%252C%2520spatial%2520topology%252C%2520motion%2520trends%252C%2520and%2520dynamics.%2520Moreover%252C%2520the%2520Causality-based%2520Counterfactual%2520Motion%2520Disentangler%2520is%2520meticulously%2520designed%2520to%2520expose%2520motion-irrelevant%2520cues%2520to%2520eliminate%2520noise%252C%2520disentangling%2520the%2520real%2520modeling%2520contributions%2520of%2520each%2520domain%2520for%2520superior%2520generation.%2520Extensive%2520experimental%2520results%2520validate%2520that%2520TriC-Motion%2520achieves%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520attaining%2520an%2520outstanding%2520R%25401%2520of%25200.612%2520on%2520the%2520HumanML3D%2520dataset.%2520These%2520results%2520demonstrate%2520its%2520capability%2520to%2520generate%2520high-fidelity%252C%2520coherent%252C%2520diverse%252C%2520and%2520text-aligned%2520motion%2520sequences.%2520Code%2520is%2520available%2520at%253A%2520https%253A//caoyiyang1105.github.io/TriC-Motion/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriC-Motion%3A%20Tri-Domain%20Causal%20Modeling%20Grounded%20Text-to-Motion%20Generation&entry.906535625=Yiyang%20Cao%20and%20Yunze%20Deng%20and%20Ziyu%20Lin%20and%20Bin%20Feng%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu%20and%20Dandan%20Zheng%20and%20Jingdong%20Chen&entry.1292438233=Text-to-motion%20generation%2C%20a%20rapidly%20evolving%20field%20in%20computer%20vision%2C%20aims%20to%20produce%20realistic%20and%20text-aligned%20motion%20sequences.%20Current%20methods%20primarily%20focus%20on%20spatial-temporal%20modeling%20or%20independent%20frequency%20domain%20analysis%2C%20lacking%20a%20unified%20framework%20for%20joint%20optimization%20across%20spatial%2C%20temporal%2C%20and%20frequency%20domains.%20This%20limitation%20hinders%20the%20model%27s%20ability%20to%20leverage%20information%20from%20all%20domains%20simultaneously%2C%20leading%20to%20suboptimal%20generation%20quality.%20Additionally%2C%20in%20motion%20generation%20frameworks%2C%20motion-irrelevant%20cues%20caused%20by%20noise%20are%20often%20entangled%20with%20features%20that%20contribute%20positively%20to%20generation%2C%20thereby%20leading%20to%20motion%20distortion.%20To%20address%20these%20issues%2C%20we%20propose%20Tri-Domain%20Causal%20Text-to-Motion%20Generation%20%28TriC-Motion%29%2C%20a%20novel%20diffusion-based%20framework%20integrating%20spatial-temporal-frequency-domain%20modeling%20with%20causal%20intervention.%20TriC-Motion%20includes%20three%20core%20modeling%20modules%20for%20domain-specific%20modeling%2C%20namely%20Temporal%20Motion%20Encoding%2C%20Spatial%20Topology%20Modeling%2C%20and%20Hybrid%20Frequency%20Analysis.%20After%20comprehensive%20modeling%2C%20a%20Score-guided%20Tri-domain%20Fusion%20module%20integrates%20valuable%20information%20from%20the%20triple%20domains%2C%20simultaneously%20ensuring%20temporal%20consistency%2C%20spatial%20topology%2C%20motion%20trends%2C%20and%20dynamics.%20Moreover%2C%20the%20Causality-based%20Counterfactual%20Motion%20Disentangler%20is%20meticulously%20designed%20to%20expose%20motion-irrelevant%20cues%20to%20eliminate%20noise%2C%20disentangling%20the%20real%20modeling%20contributions%20of%20each%20domain%20for%20superior%20generation.%20Extensive%20experimental%20results%20validate%20that%20TriC-Motion%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20methods%2C%20attaining%20an%20outstanding%20R%401%20of%200.612%20on%20the%20HumanML3D%20dataset.%20These%20results%20demonstrate%20its%20capability%20to%20generate%20high-fidelity%2C%20coherent%2C%20diverse%2C%20and%20text-aligned%20motion%20sequences.%20Code%20is%20available%20at%3A%20https%3A//caoyiyang1105.github.io/TriC-Motion/.&entry.1838667208=http%3A//arxiv.org/abs/2602.08462v1&entry.124074799=Read"},
{"title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "author": "Elchanan Mossel", "abstract": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage.", "link": "http://arxiv.org/abs/2601.02380v2", "date": "2026-02-09", "relevancy": 2.42, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models&body=Title%3A%20The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models%0AAuthor%3A%20Elchanan%20Mossel%0AAbstract%3A%20Recent%20reports%20claim%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20the%20ability%20to%20derive%20new%20science%20and%20exhibit%20human-level%20general%20intelligence.%20We%20argue%20that%20such%20claims%20are%20not%20rigorous%20scientific%20claims%2C%20as%20they%20do%20not%20satisfy%20Popper%27s%20refutability%20principle%20%28often%20termed%20falsifiability%29%2C%20which%20requires%20that%20scientific%20statements%20be%20capable%20of%20being%20disproven.%20We%20identify%20several%20methodological%20pitfalls%20in%20current%20AI%20research%20on%20reasoning%2C%20including%20the%20inability%20to%20verify%20the%20novelty%20of%20findings%20due%20to%20opaque%20and%20non-searchable%20training%20data%2C%20the%20lack%20of%20reproducibility%20caused%20by%20continuous%20model%20updates%2C%20and%20the%20omission%20of%20human-interaction%20transcripts%2C%20which%20obscures%20the%20true%20source%20of%20scientific%20discovery.%20Additionally%2C%20the%20absence%20of%20counterfactuals%20and%20data%20on%20failed%20attempts%20creates%20a%20selection%20bias%20that%20may%20exaggerate%20LLM%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20guidelines%20for%20scientific%20transparency%20and%20reproducibility%20for%20research%20on%20reasoning%20by%20LLMs.%20Establishing%20such%20guidelines%20is%20crucial%20for%20both%20scientific%20integrity%20and%20the%20ongoing%20societal%20debates%20regarding%20fair%20data%20usage.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Refutability%2520Gap%253A%2520Challenges%2520in%2520Validating%2520Reasoning%2520by%2520Large%2520Language%2520Models%26entry.906535625%3DElchanan%2520Mossel%26entry.1292438233%3DRecent%2520reports%2520claim%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520the%2520ability%2520to%2520derive%2520new%2520science%2520and%2520exhibit%2520human-level%2520general%2520intelligence.%2520We%2520argue%2520that%2520such%2520claims%2520are%2520not%2520rigorous%2520scientific%2520claims%252C%2520as%2520they%2520do%2520not%2520satisfy%2520Popper%2527s%2520refutability%2520principle%2520%2528often%2520termed%2520falsifiability%2529%252C%2520which%2520requires%2520that%2520scientific%2520statements%2520be%2520capable%2520of%2520being%2520disproven.%2520We%2520identify%2520several%2520methodological%2520pitfalls%2520in%2520current%2520AI%2520research%2520on%2520reasoning%252C%2520including%2520the%2520inability%2520to%2520verify%2520the%2520novelty%2520of%2520findings%2520due%2520to%2520opaque%2520and%2520non-searchable%2520training%2520data%252C%2520the%2520lack%2520of%2520reproducibility%2520caused%2520by%2520continuous%2520model%2520updates%252C%2520and%2520the%2520omission%2520of%2520human-interaction%2520transcripts%252C%2520which%2520obscures%2520the%2520true%2520source%2520of%2520scientific%2520discovery.%2520Additionally%252C%2520the%2520absence%2520of%2520counterfactuals%2520and%2520data%2520on%2520failed%2520attempts%2520creates%2520a%2520selection%2520bias%2520that%2520may%2520exaggerate%2520LLM%2520capabilities.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520guidelines%2520for%2520scientific%2520transparency%2520and%2520reproducibility%2520for%2520research%2520on%2520reasoning%2520by%2520LLMs.%2520Establishing%2520such%2520guidelines%2520is%2520crucial%2520for%2520both%2520scientific%2520integrity%2520and%2520the%2520ongoing%2520societal%2520debates%2520regarding%2520fair%2520data%2520usage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models&entry.906535625=Elchanan%20Mossel&entry.1292438233=Recent%20reports%20claim%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20the%20ability%20to%20derive%20new%20science%20and%20exhibit%20human-level%20general%20intelligence.%20We%20argue%20that%20such%20claims%20are%20not%20rigorous%20scientific%20claims%2C%20as%20they%20do%20not%20satisfy%20Popper%27s%20refutability%20principle%20%28often%20termed%20falsifiability%29%2C%20which%20requires%20that%20scientific%20statements%20be%20capable%20of%20being%20disproven.%20We%20identify%20several%20methodological%20pitfalls%20in%20current%20AI%20research%20on%20reasoning%2C%20including%20the%20inability%20to%20verify%20the%20novelty%20of%20findings%20due%20to%20opaque%20and%20non-searchable%20training%20data%2C%20the%20lack%20of%20reproducibility%20caused%20by%20continuous%20model%20updates%2C%20and%20the%20omission%20of%20human-interaction%20transcripts%2C%20which%20obscures%20the%20true%20source%20of%20scientific%20discovery.%20Additionally%2C%20the%20absence%20of%20counterfactuals%20and%20data%20on%20failed%20attempts%20creates%20a%20selection%20bias%20that%20may%20exaggerate%20LLM%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20guidelines%20for%20scientific%20transparency%20and%20reproducibility%20for%20research%20on%20reasoning%20by%20LLMs.%20Establishing%20such%20guidelines%20is%20crucial%20for%20both%20scientific%20integrity%20and%20the%20ongoing%20societal%20debates%20regarding%20fair%20data%20usage.&entry.1838667208=http%3A//arxiv.org/abs/2601.02380v2&entry.124074799=Read"},
{"title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "author": "Ying Guo and Qijun Gan and Yifu Zhang and Jinlai Liu and Yifei Hu and Pan Xie and Dongjun Qian and Yu Zhang and Ruiqi Li and Yuqi Zhang and Ruibiao Lu and Xiaofeng Mei and Bo Han and Xiang Yin and Bingyue Peng and Zehuan Yuan", "abstract": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "link": "http://arxiv.org/abs/2602.08682v1", "date": "2026-02-09", "relevancy": 2.4149, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6168}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.607}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation&body=Title%3A%20ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation%0AAuthor%3A%20Ying%20Guo%20and%20Qijun%20Gan%20and%20Yifu%20Zhang%20and%20Jinlai%20Liu%20and%20Yifei%20Hu%20and%20Pan%20Xie%20and%20Dongjun%20Qian%20and%20Yu%20Zhang%20and%20Ruiqi%20Li%20and%20Yuqi%20Zhang%20and%20Ruibiao%20Lu%20and%20Xiaofeng%20Mei%20and%20Bo%20Han%20and%20Xiang%20Yin%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan%0AAbstract%3A%20Video%20generation%20is%20rapidly%20evolving%20towards%20unified%20audio-video%20generation.%20In%20this%20paper%2C%20we%20present%20ALIVE%2C%20a%20generation%20model%20that%20adapts%20a%20pretrained%20Text-to-Video%20%28T2V%29%20model%20to%20Sora-style%20audio-video%20generation%20and%20animation.%20In%20particular%2C%20the%20model%20unlocks%20the%20Text-to-Video%26Audio%20%28T2VA%29%20and%20Reference-to-Video%26Audio%20%28animation%29%20capabilities%20compared%20to%20the%20T2V%20foundation%20models.%20To%20support%20the%20audio-visual%20synchronization%20and%20reference%20animation%2C%20we%20augment%20the%20popular%20MMDiT%20architecture%20with%20a%20joint%20audio-video%20branch%20which%20includes%20TA-CrossAttn%20for%20temporally-aligned%20cross-modal%20fusion%20and%20UniTemp-RoPE%20for%20precise%20audio-visual%20alignment.%20Meanwhile%2C%20a%20comprehensive%20data%20pipeline%20consisting%20of%20audio-video%20captioning%2C%20quality%20control%2C%20etc.%2C%20is%20carefully%20designed%20to%20collect%20high-quality%20finetuning%20data.%20Additionally%2C%20we%20introduce%20a%20new%20benchmark%20to%20perform%20a%20comprehensive%20model%20test%20and%20comparison.%20After%20continue%20pretraining%20and%20finetuning%20on%20million-level%20high-quality%20data%2C%20ALIVE%20demonstrates%20outstanding%20performance%2C%20consistently%20outperforming%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%20commercial%20solutions.%20With%20detailed%20recipes%20and%20benchmarks%2C%20we%20hope%20ALIVE%20helps%20the%20community%20develop%20audio-video%20generation%20models%20more%20efficiently.%20Official%20page%3A%20https%3A//github.com/FoundationVision/Alive.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALIVE%253A%2520Animate%2520Your%2520World%2520with%2520Lifelike%2520Audio-Video%2520Generation%26entry.906535625%3DYing%2520Guo%2520and%2520Qijun%2520Gan%2520and%2520Yifu%2520Zhang%2520and%2520Jinlai%2520Liu%2520and%2520Yifei%2520Hu%2520and%2520Pan%2520Xie%2520and%2520Dongjun%2520Qian%2520and%2520Yu%2520Zhang%2520and%2520Ruiqi%2520Li%2520and%2520Yuqi%2520Zhang%2520and%2520Ruibiao%2520Lu%2520and%2520Xiaofeng%2520Mei%2520and%2520Bo%2520Han%2520and%2520Xiang%2520Yin%2520and%2520Bingyue%2520Peng%2520and%2520Zehuan%2520Yuan%26entry.1292438233%3DVideo%2520generation%2520is%2520rapidly%2520evolving%2520towards%2520unified%2520audio-video%2520generation.%2520In%2520this%2520paper%252C%2520we%2520present%2520ALIVE%252C%2520a%2520generation%2520model%2520that%2520adapts%2520a%2520pretrained%2520Text-to-Video%2520%2528T2V%2529%2520model%2520to%2520Sora-style%2520audio-video%2520generation%2520and%2520animation.%2520In%2520particular%252C%2520the%2520model%2520unlocks%2520the%2520Text-to-Video%2526Audio%2520%2528T2VA%2529%2520and%2520Reference-to-Video%2526Audio%2520%2528animation%2529%2520capabilities%2520compared%2520to%2520the%2520T2V%2520foundation%2520models.%2520To%2520support%2520the%2520audio-visual%2520synchronization%2520and%2520reference%2520animation%252C%2520we%2520augment%2520the%2520popular%2520MMDiT%2520architecture%2520with%2520a%2520joint%2520audio-video%2520branch%2520which%2520includes%2520TA-CrossAttn%2520for%2520temporally-aligned%2520cross-modal%2520fusion%2520and%2520UniTemp-RoPE%2520for%2520precise%2520audio-visual%2520alignment.%2520Meanwhile%252C%2520a%2520comprehensive%2520data%2520pipeline%2520consisting%2520of%2520audio-video%2520captioning%252C%2520quality%2520control%252C%2520etc.%252C%2520is%2520carefully%2520designed%2520to%2520collect%2520high-quality%2520finetuning%2520data.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520to%2520perform%2520a%2520comprehensive%2520model%2520test%2520and%2520comparison.%2520After%2520continue%2520pretraining%2520and%2520finetuning%2520on%2520million-level%2520high-quality%2520data%252C%2520ALIVE%2520demonstrates%2520outstanding%2520performance%252C%2520consistently%2520outperforming%2520open-source%2520models%2520and%2520matching%2520or%2520surpassing%2520state-of-the-art%2520commercial%2520solutions.%2520With%2520detailed%2520recipes%2520and%2520benchmarks%252C%2520we%2520hope%2520ALIVE%2520helps%2520the%2520community%2520develop%2520audio-video%2520generation%2520models%2520more%2520efficiently.%2520Official%2520page%253A%2520https%253A//github.com/FoundationVision/Alive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation&entry.906535625=Ying%20Guo%20and%20Qijun%20Gan%20and%20Yifu%20Zhang%20and%20Jinlai%20Liu%20and%20Yifei%20Hu%20and%20Pan%20Xie%20and%20Dongjun%20Qian%20and%20Yu%20Zhang%20and%20Ruiqi%20Li%20and%20Yuqi%20Zhang%20and%20Ruibiao%20Lu%20and%20Xiaofeng%20Mei%20and%20Bo%20Han%20and%20Xiang%20Yin%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan&entry.1292438233=Video%20generation%20is%20rapidly%20evolving%20towards%20unified%20audio-video%20generation.%20In%20this%20paper%2C%20we%20present%20ALIVE%2C%20a%20generation%20model%20that%20adapts%20a%20pretrained%20Text-to-Video%20%28T2V%29%20model%20to%20Sora-style%20audio-video%20generation%20and%20animation.%20In%20particular%2C%20the%20model%20unlocks%20the%20Text-to-Video%26Audio%20%28T2VA%29%20and%20Reference-to-Video%26Audio%20%28animation%29%20capabilities%20compared%20to%20the%20T2V%20foundation%20models.%20To%20support%20the%20audio-visual%20synchronization%20and%20reference%20animation%2C%20we%20augment%20the%20popular%20MMDiT%20architecture%20with%20a%20joint%20audio-video%20branch%20which%20includes%20TA-CrossAttn%20for%20temporally-aligned%20cross-modal%20fusion%20and%20UniTemp-RoPE%20for%20precise%20audio-visual%20alignment.%20Meanwhile%2C%20a%20comprehensive%20data%20pipeline%20consisting%20of%20audio-video%20captioning%2C%20quality%20control%2C%20etc.%2C%20is%20carefully%20designed%20to%20collect%20high-quality%20finetuning%20data.%20Additionally%2C%20we%20introduce%20a%20new%20benchmark%20to%20perform%20a%20comprehensive%20model%20test%20and%20comparison.%20After%20continue%20pretraining%20and%20finetuning%20on%20million-level%20high-quality%20data%2C%20ALIVE%20demonstrates%20outstanding%20performance%2C%20consistently%20outperforming%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%20commercial%20solutions.%20With%20detailed%20recipes%20and%20benchmarks%2C%20we%20hope%20ALIVE%20helps%20the%20community%20develop%20audio-video%20generation%20models%20more%20efficiently.%20Official%20page%3A%20https%3A//github.com/FoundationVision/Alive.&entry.1838667208=http%3A//arxiv.org/abs/2602.08682v1&entry.124074799=Read"},
{"title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization", "author": "Yu Li and Tian Lan and Zhengling Qi", "abstract": "Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (InSPO), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. InSPO serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs. Our Code is available at https://github.com/Skylanding/InSPO.", "link": "http://arxiv.org/abs/2512.23126v3", "date": "2026-02-09", "relevancy": 2.3951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InSPO%3A%20Unlocking%20Intrinsic%20Self-Reflection%20for%20LLM%20Preference%20Optimization&body=Title%3A%20InSPO%3A%20Unlocking%20Intrinsic%20Self-Reflection%20for%20LLM%20Preference%20Optimization%0AAuthor%3A%20Yu%20Li%20and%20Tian%20Lan%20and%20Zhengling%20Qi%0AAbstract%3A%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants%20have%20become%20standard%20for%20aligning%20Large%20Language%20Models%20due%20to%20their%20simplicity%20and%20offline%20stability.%20However%2C%20we%20identify%20two%20fundamental%20limitations.%20First%2C%20the%20optimal%20policy%20depends%20on%20arbitrary%20modeling%20choices%20%28scalarization%20function%2C%20reference%20policy%29%2C%20yielding%20behavior%20reflecting%20parameterization%20artifacts%20rather%20than%20true%20preferences.%20Second%2C%20treating%20response%20generation%20in%20isolation%20fails%20to%20leverage%20comparative%20information%20in%20pairwise%20data%2C%20leaving%20the%20model%27s%20capacity%20for%20intrinsic%20self-reflection%20untapped.%20To%20address%20it%2C%20we%20propose%20Intrinsic%20Self-reflective%20Preference%20Optimization%20%28InSPO%29%2C%20deriving%20a%20globally%20optimal%20policy%20conditioning%20on%20both%20context%20and%20alternative%20responses.%20We%20prove%20this%20formulation%20superior%20to%20DPO/RLHF%20while%20guaranteeing%20invariance%20to%20scalarization%20and%20reference%20choices.%20InSPO%20serves%20as%20a%20plug-and-play%20enhancement%20without%20architectural%20changes%20or%20inference%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20win%20rates%20and%20length-controlled%20metrics%2C%20validating%20that%20unlocking%20self-reflection%20yields%20more%20robust%2C%20human-aligned%20LLMs.%20Our%20Code%20is%20available%20at%20https%3A//github.com/Skylanding/InSPO.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23126v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInSPO%253A%2520Unlocking%2520Intrinsic%2520Self-Reflection%2520for%2520LLM%2520Preference%2520Optimization%26entry.906535625%3DYu%2520Li%2520and%2520Tian%2520Lan%2520and%2520Zhengling%2520Qi%26entry.1292438233%3DDirect%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520its%2520variants%2520have%2520become%2520standard%2520for%2520aligning%2520Large%2520Language%2520Models%2520due%2520to%2520their%2520simplicity%2520and%2520offline%2520stability.%2520However%252C%2520we%2520identify%2520two%2520fundamental%2520limitations.%2520First%252C%2520the%2520optimal%2520policy%2520depends%2520on%2520arbitrary%2520modeling%2520choices%2520%2528scalarization%2520function%252C%2520reference%2520policy%2529%252C%2520yielding%2520behavior%2520reflecting%2520parameterization%2520artifacts%2520rather%2520than%2520true%2520preferences.%2520Second%252C%2520treating%2520response%2520generation%2520in%2520isolation%2520fails%2520to%2520leverage%2520comparative%2520information%2520in%2520pairwise%2520data%252C%2520leaving%2520the%2520model%2527s%2520capacity%2520for%2520intrinsic%2520self-reflection%2520untapped.%2520To%2520address%2520it%252C%2520we%2520propose%2520Intrinsic%2520Self-reflective%2520Preference%2520Optimization%2520%2528InSPO%2529%252C%2520deriving%2520a%2520globally%2520optimal%2520policy%2520conditioning%2520on%2520both%2520context%2520and%2520alternative%2520responses.%2520We%2520prove%2520this%2520formulation%2520superior%2520to%2520DPO/RLHF%2520while%2520guaranteeing%2520invariance%2520to%2520scalarization%2520and%2520reference%2520choices.%2520InSPO%2520serves%2520as%2520a%2520plug-and-play%2520enhancement%2520without%2520architectural%2520changes%2520or%2520inference%2520overhead.%2520Experiments%2520demonstrate%2520consistent%2520improvements%2520in%2520win%2520rates%2520and%2520length-controlled%2520metrics%252C%2520validating%2520that%2520unlocking%2520self-reflection%2520yields%2520more%2520robust%252C%2520human-aligned%2520LLMs.%2520Our%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Skylanding/InSPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23126v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InSPO%3A%20Unlocking%20Intrinsic%20Self-Reflection%20for%20LLM%20Preference%20Optimization&entry.906535625=Yu%20Li%20and%20Tian%20Lan%20and%20Zhengling%20Qi&entry.1292438233=Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants%20have%20become%20standard%20for%20aligning%20Large%20Language%20Models%20due%20to%20their%20simplicity%20and%20offline%20stability.%20However%2C%20we%20identify%20two%20fundamental%20limitations.%20First%2C%20the%20optimal%20policy%20depends%20on%20arbitrary%20modeling%20choices%20%28scalarization%20function%2C%20reference%20policy%29%2C%20yielding%20behavior%20reflecting%20parameterization%20artifacts%20rather%20than%20true%20preferences.%20Second%2C%20treating%20response%20generation%20in%20isolation%20fails%20to%20leverage%20comparative%20information%20in%20pairwise%20data%2C%20leaving%20the%20model%27s%20capacity%20for%20intrinsic%20self-reflection%20untapped.%20To%20address%20it%2C%20we%20propose%20Intrinsic%20Self-reflective%20Preference%20Optimization%20%28InSPO%29%2C%20deriving%20a%20globally%20optimal%20policy%20conditioning%20on%20both%20context%20and%20alternative%20responses.%20We%20prove%20this%20formulation%20superior%20to%20DPO/RLHF%20while%20guaranteeing%20invariance%20to%20scalarization%20and%20reference%20choices.%20InSPO%20serves%20as%20a%20plug-and-play%20enhancement%20without%20architectural%20changes%20or%20inference%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20win%20rates%20and%20length-controlled%20metrics%2C%20validating%20that%20unlocking%20self-reflection%20yields%20more%20robust%2C%20human-aligned%20LLMs.%20Our%20Code%20is%20available%20at%20https%3A//github.com/Skylanding/InSPO.&entry.1838667208=http%3A//arxiv.org/abs/2512.23126v3&entry.124074799=Read"},
{"title": "ASIDE: Architectural Separation of Instructions and Data in Language Models", "author": "Egor Zverev and Evgenii Kortukov and Alexander Panfilov and Alexandra Volkova and Soroush Tabesh and Sebastian Lapuschkin and Wojciech Samek and Christoph H. Lampert", "abstract": "Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as the root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of token embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) achieves substantially higher instruction-data separation without performance loss and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at https://github.com/egozverev/aside.", "link": "http://arxiv.org/abs/2503.10566v4", "date": "2026-02-09", "relevancy": 2.3932, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%20Models&body=Title%3A%20ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%20Models%0AAuthor%3A%20Egor%20Zverev%20and%20Evgenii%20Kortukov%20and%20Alexander%20Panfilov%20and%20Alexandra%20Volkova%20and%20Soroush%20Tabesh%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20Despite%20their%20remarkable%20performance%2C%20large%20language%20models%20lack%20elementary%20safety%20features%2C%20making%20them%20susceptible%20to%20numerous%20malicious%20attacks.%20In%20particular%2C%20previous%20work%20has%20identified%20the%20absence%20of%20an%20intrinsic%20separation%20between%20instructions%20and%20data%20as%20the%20root%20cause%20of%20the%20success%20of%20prompt%20injection%20attacks.%20In%20this%20work%2C%20we%20propose%20a%20new%20architectural%20element%2C%20ASIDE%2C%20that%20allows%20language%20models%20to%20clearly%20separate%20instructions%20and%20data%20at%20the%20level%20of%20token%20embeddings.%20ASIDE%20applies%20an%20orthogonal%20rotation%20to%20the%20embeddings%20of%20data%20tokens%2C%20thus%20creating%20clearly%20distinct%20representations%20of%20instructions%20and%20data%20tokens%20without%20introducing%20any%20additional%20parameters.%20As%20we%20demonstrate%20experimentally%20across%20a%20range%20of%20models%2C%20instruction-tuning%20LLMs%20with%20ASIDE%20%281%29%20achieves%20substantially%20higher%20instruction-data%20separation%20without%20performance%20loss%20and%20%282%29%20makes%20the%20models%20more%20robust%20to%20prompt%20injection%20benchmarks%2C%20even%20without%20dedicated%20safety%20training.%20Additionally%2C%20we%20provide%20insights%20into%20the%20mechanism%20underlying%20our%20method%20through%20an%20analysis%20of%20the%20model%20representations.%20The%20source%20code%20and%20training%20scripts%20are%20openly%20accessible%20at%20https%3A//github.com/egozverev/aside.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10566v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASIDE%253A%2520Architectural%2520Separation%2520of%2520Instructions%2520and%2520Data%2520in%2520Language%2520Models%26entry.906535625%3DEgor%2520Zverev%2520and%2520Evgenii%2520Kortukov%2520and%2520Alexander%2520Panfilov%2520and%2520Alexandra%2520Volkova%2520and%2520Soroush%2520Tabesh%2520and%2520Sebastian%2520Lapuschkin%2520and%2520Wojciech%2520Samek%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3DDespite%2520their%2520remarkable%2520performance%252C%2520large%2520language%2520models%2520lack%2520elementary%2520safety%2520features%252C%2520making%2520them%2520susceptible%2520to%2520numerous%2520malicious%2520attacks.%2520In%2520particular%252C%2520previous%2520work%2520has%2520identified%2520the%2520absence%2520of%2520an%2520intrinsic%2520separation%2520between%2520instructions%2520and%2520data%2520as%2520the%2520root%2520cause%2520of%2520the%2520success%2520of%2520prompt%2520injection%2520attacks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520architectural%2520element%252C%2520ASIDE%252C%2520that%2520allows%2520language%2520models%2520to%2520clearly%2520separate%2520instructions%2520and%2520data%2520at%2520the%2520level%2520of%2520token%2520embeddings.%2520ASIDE%2520applies%2520an%2520orthogonal%2520rotation%2520to%2520the%2520embeddings%2520of%2520data%2520tokens%252C%2520thus%2520creating%2520clearly%2520distinct%2520representations%2520of%2520instructions%2520and%2520data%2520tokens%2520without%2520introducing%2520any%2520additional%2520parameters.%2520As%2520we%2520demonstrate%2520experimentally%2520across%2520a%2520range%2520of%2520models%252C%2520instruction-tuning%2520LLMs%2520with%2520ASIDE%2520%25281%2529%2520achieves%2520substantially%2520higher%2520instruction-data%2520separation%2520without%2520performance%2520loss%2520and%2520%25282%2529%2520makes%2520the%2520models%2520more%2520robust%2520to%2520prompt%2520injection%2520benchmarks%252C%2520even%2520without%2520dedicated%2520safety%2520training.%2520Additionally%252C%2520we%2520provide%2520insights%2520into%2520the%2520mechanism%2520underlying%2520our%2520method%2520through%2520an%2520analysis%2520of%2520the%2520model%2520representations.%2520The%2520source%2520code%2520and%2520training%2520scripts%2520are%2520openly%2520accessible%2520at%2520https%253A//github.com/egozverev/aside.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10566v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%20Models&entry.906535625=Egor%20Zverev%20and%20Evgenii%20Kortukov%20and%20Alexander%20Panfilov%20and%20Alexandra%20Volkova%20and%20Soroush%20Tabesh%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Christoph%20H.%20Lampert&entry.1292438233=Despite%20their%20remarkable%20performance%2C%20large%20language%20models%20lack%20elementary%20safety%20features%2C%20making%20them%20susceptible%20to%20numerous%20malicious%20attacks.%20In%20particular%2C%20previous%20work%20has%20identified%20the%20absence%20of%20an%20intrinsic%20separation%20between%20instructions%20and%20data%20as%20the%20root%20cause%20of%20the%20success%20of%20prompt%20injection%20attacks.%20In%20this%20work%2C%20we%20propose%20a%20new%20architectural%20element%2C%20ASIDE%2C%20that%20allows%20language%20models%20to%20clearly%20separate%20instructions%20and%20data%20at%20the%20level%20of%20token%20embeddings.%20ASIDE%20applies%20an%20orthogonal%20rotation%20to%20the%20embeddings%20of%20data%20tokens%2C%20thus%20creating%20clearly%20distinct%20representations%20of%20instructions%20and%20data%20tokens%20without%20introducing%20any%20additional%20parameters.%20As%20we%20demonstrate%20experimentally%20across%20a%20range%20of%20models%2C%20instruction-tuning%20LLMs%20with%20ASIDE%20%281%29%20achieves%20substantially%20higher%20instruction-data%20separation%20without%20performance%20loss%20and%20%282%29%20makes%20the%20models%20more%20robust%20to%20prompt%20injection%20benchmarks%2C%20even%20without%20dedicated%20safety%20training.%20Additionally%2C%20we%20provide%20insights%20into%20the%20mechanism%20underlying%20our%20method%20through%20an%20analysis%20of%20the%20model%20representations.%20The%20source%20code%20and%20training%20scripts%20are%20openly%20accessible%20at%20https%3A//github.com/egozverev/aside.&entry.1838667208=http%3A//arxiv.org/abs/2503.10566v4&entry.124074799=Read"},
{"title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill", "author": "Dalton Jones and Junyoung Park and Matthew Morse and Mingu Lee and Chris Lott and Harper Langston", "abstract": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.", "link": "http://arxiv.org/abs/2602.08722v1", "date": "2026-02-09", "relevancy": 2.3859, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5511}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4406}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QUOKA%3A%20Query-Oriented%20KV%20Selection%20For%20Efficient%20LLM%20Prefill&body=Title%3A%20QUOKA%3A%20Query-Oriented%20KV%20Selection%20For%20Efficient%20LLM%20Prefill%0AAuthor%3A%20Dalton%20Jones%20and%20Junyoung%20Park%20and%20Matthew%20Morse%20and%20Mingu%20Lee%20and%20Chris%20Lott%20and%20Harper%20Langston%0AAbstract%3A%20We%20present%20QUOKA%3A%20Query-oriented%20KV%20selection%20for%20efficient%20attention%2C%20a%20training-free%20and%20hardware%20agnostic%20sparse%20attention%20algorithm%20for%20accelerating%20transformer%20inference%20under%20chunked%20prefill.%20While%20many%20queries%20focus%20on%20a%20smaller%20group%20of%20keys%20in%20the%20attention%20operator%2C%20we%20observe%20that%20queries%20with%20low%20cosine%20similarity%20with%20respect%20to%20the%20mean%20query%20interact%20more%20strongly%20with%20more%20keys%20and%20have%20the%20greatest%20contribution%20to%20final%20attention%20logits.%20By%20prioritizing%20these%20low%20cosine%20similarity%20queries%2C%20the%20behavior%20of%20full%20attention%20during%20the%20prefill%20stage%20can%20be%20closely%20approximated.%20QUOKA%20leverages%20this%20observation%2C%20accelerating%20attention%20by%20%281%29%20first%20retaining%20a%20small%20set%20of%20representative%20queries%20and%20%282%29%20then%20subselectin%20the%20keys%20most%20aligned%20with%20those%20queries.%20Through%20experiments%20on%20Needle-In-A-Haystack%2C%20LongBench%2C%20RULER%2C%20and%20Math500%2C%20we%20show%20that%2C%20while%20realizing%20a%203x%20reduction%20in%20time-to-first-token%2C%205x%20speedup%20in%20attention%20on%20Nvidia%20GPUs%20and%20up%20to%20nearly%20a%207x%20speedup%20on%20Intel%20Xeon%20CPUs%2C%20QUOKA%20achieves%20near-baseline%20accuracy%2C%20utilizing%2088%25%20fewer%20key-value%20pairs%20per%20attention%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQUOKA%253A%2520Query-Oriented%2520KV%2520Selection%2520For%2520Efficient%2520LLM%2520Prefill%26entry.906535625%3DDalton%2520Jones%2520and%2520Junyoung%2520Park%2520and%2520Matthew%2520Morse%2520and%2520Mingu%2520Lee%2520and%2520Chris%2520Lott%2520and%2520Harper%2520Langston%26entry.1292438233%3DWe%2520present%2520QUOKA%253A%2520Query-oriented%2520KV%2520selection%2520for%2520efficient%2520attention%252C%2520a%2520training-free%2520and%2520hardware%2520agnostic%2520sparse%2520attention%2520algorithm%2520for%2520accelerating%2520transformer%2520inference%2520under%2520chunked%2520prefill.%2520While%2520many%2520queries%2520focus%2520on%2520a%2520smaller%2520group%2520of%2520keys%2520in%2520the%2520attention%2520operator%252C%2520we%2520observe%2520that%2520queries%2520with%2520low%2520cosine%2520similarity%2520with%2520respect%2520to%2520the%2520mean%2520query%2520interact%2520more%2520strongly%2520with%2520more%2520keys%2520and%2520have%2520the%2520greatest%2520contribution%2520to%2520final%2520attention%2520logits.%2520By%2520prioritizing%2520these%2520low%2520cosine%2520similarity%2520queries%252C%2520the%2520behavior%2520of%2520full%2520attention%2520during%2520the%2520prefill%2520stage%2520can%2520be%2520closely%2520approximated.%2520QUOKA%2520leverages%2520this%2520observation%252C%2520accelerating%2520attention%2520by%2520%25281%2529%2520first%2520retaining%2520a%2520small%2520set%2520of%2520representative%2520queries%2520and%2520%25282%2529%2520then%2520subselectin%2520the%2520keys%2520most%2520aligned%2520with%2520those%2520queries.%2520Through%2520experiments%2520on%2520Needle-In-A-Haystack%252C%2520LongBench%252C%2520RULER%252C%2520and%2520Math500%252C%2520we%2520show%2520that%252C%2520while%2520realizing%2520a%25203x%2520reduction%2520in%2520time-to-first-token%252C%25205x%2520speedup%2520in%2520attention%2520on%2520Nvidia%2520GPUs%2520and%2520up%2520to%2520nearly%2520a%25207x%2520speedup%2520on%2520Intel%2520Xeon%2520CPUs%252C%2520QUOKA%2520achieves%2520near-baseline%2520accuracy%252C%2520utilizing%252088%2525%2520fewer%2520key-value%2520pairs%2520per%2520attention%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUOKA%3A%20Query-Oriented%20KV%20Selection%20For%20Efficient%20LLM%20Prefill&entry.906535625=Dalton%20Jones%20and%20Junyoung%20Park%20and%20Matthew%20Morse%20and%20Mingu%20Lee%20and%20Chris%20Lott%20and%20Harper%20Langston&entry.1292438233=We%20present%20QUOKA%3A%20Query-oriented%20KV%20selection%20for%20efficient%20attention%2C%20a%20training-free%20and%20hardware%20agnostic%20sparse%20attention%20algorithm%20for%20accelerating%20transformer%20inference%20under%20chunked%20prefill.%20While%20many%20queries%20focus%20on%20a%20smaller%20group%20of%20keys%20in%20the%20attention%20operator%2C%20we%20observe%20that%20queries%20with%20low%20cosine%20similarity%20with%20respect%20to%20the%20mean%20query%20interact%20more%20strongly%20with%20more%20keys%20and%20have%20the%20greatest%20contribution%20to%20final%20attention%20logits.%20By%20prioritizing%20these%20low%20cosine%20similarity%20queries%2C%20the%20behavior%20of%20full%20attention%20during%20the%20prefill%20stage%20can%20be%20closely%20approximated.%20QUOKA%20leverages%20this%20observation%2C%20accelerating%20attention%20by%20%281%29%20first%20retaining%20a%20small%20set%20of%20representative%20queries%20and%20%282%29%20then%20subselectin%20the%20keys%20most%20aligned%20with%20those%20queries.%20Through%20experiments%20on%20Needle-In-A-Haystack%2C%20LongBench%2C%20RULER%2C%20and%20Math500%2C%20we%20show%20that%2C%20while%20realizing%20a%203x%20reduction%20in%20time-to-first-token%2C%205x%20speedup%20in%20attention%20on%20Nvidia%20GPUs%20and%20up%20to%20nearly%20a%207x%20speedup%20on%20Intel%20Xeon%20CPUs%2C%20QUOKA%20achieves%20near-baseline%20accuracy%2C%20utilizing%2088%25%20fewer%20key-value%20pairs%20per%20attention%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2602.08722v1&entry.124074799=Read"},
{"title": "ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection", "author": "Janghyun Baek and Mincheol Chang and Seokha Moon and Seung Joon Lee and Jinkyu Kim", "abstract": "Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.", "link": "http://arxiv.org/abs/2512.18187v2", "date": "2026-02-09", "relevancy": 2.3749, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6033}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5878}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALIGN%3A%20Advanced%20Query%20Initialization%20with%20LiDAR-Image%20Guidance%20for%20Occlusion-Robust%203D%20Object%20Detection&body=Title%3A%20ALIGN%3A%20Advanced%20Query%20Initialization%20with%20LiDAR-Image%20Guidance%20for%20Occlusion-Robust%203D%20Object%20Detection%0AAuthor%3A%20Janghyun%20Baek%20and%20Mincheol%20Chang%20and%20Seokha%20Moon%20and%20Seung%20Joon%20Lee%20and%20Jinkyu%20Kim%0AAbstract%3A%20Recent%20query-based%203D%20object%20detection%20methods%20using%20camera%20and%20LiDAR%20inputs%20have%20shown%20strong%20performance%2C%20but%20existing%20query%20initialization%20strategies%2Csuch%20as%20random%20sampling%20or%20BEV%20heatmap-based%20sampling%2C%20often%20result%20in%20inefficient%20query%20usage%20and%20reduced%20accuracy%2C%20particularly%20for%20occluded%20or%20crowded%20objects.%20To%20address%20this%20limitation%2C%20we%20propose%20ALIGN%20%28Advanced%20query%20initialization%20with%20LiDAR%20and%20Image%20GuidaNce%29%2C%20a%20novel%20approach%20for%20occlusion-robust%2C%20object-aware%20query%20initialization.%20Our%20model%20consists%20of%20three%20key%20components%3A%20%28i%29%20Occlusion-aware%20Center%20Estimation%20%28OCE%29%2C%20which%20integrates%20LiDAR%20geometry%20and%20image%20semantics%20to%20estimate%20object%20centers%20accurately%20%28ii%29%20Adaptive%20Neighbor%20Sampling%20%28ANS%29%2C%20which%20generates%20object%20candidates%20from%20LiDAR%20clustering%20and%20supplements%20each%20object%20by%20sampling%20spatially%20and%20semantically%20aligned%20points%20around%20it%20and%20%28iii%29%20Dynamic%20Query%20Balancing%20%28DQB%29%2C%20which%20adaptively%20balances%20queries%20between%20foreground%20and%20background%20regions.%20Our%20extensive%20experiments%20on%20the%20nuScenes%20benchmark%20demonstrate%20that%20ALIGN%20consistently%20improves%20performance%20across%20multiple%20state-of-the-art%20detectors%2C%20achieving%20gains%20of%20up%20to%20%2B0.9%20mAP%20and%20%2B1.2%20NDS%2C%20particularly%20in%20challenging%20scenes%20with%20occlusions%20or%20dense%20crowds.%20Our%20code%20will%20be%20publicly%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALIGN%253A%2520Advanced%2520Query%2520Initialization%2520with%2520LiDAR-Image%2520Guidance%2520for%2520Occlusion-Robust%25203D%2520Object%2520Detection%26entry.906535625%3DJanghyun%2520Baek%2520and%2520Mincheol%2520Chang%2520and%2520Seokha%2520Moon%2520and%2520Seung%2520Joon%2520Lee%2520and%2520Jinkyu%2520Kim%26entry.1292438233%3DRecent%2520query-based%25203D%2520object%2520detection%2520methods%2520using%2520camera%2520and%2520LiDAR%2520inputs%2520have%2520shown%2520strong%2520performance%252C%2520but%2520existing%2520query%2520initialization%2520strategies%252Csuch%2520as%2520random%2520sampling%2520or%2520BEV%2520heatmap-based%2520sampling%252C%2520often%2520result%2520in%2520inefficient%2520query%2520usage%2520and%2520reduced%2520accuracy%252C%2520particularly%2520for%2520occluded%2520or%2520crowded%2520objects.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520ALIGN%2520%2528Advanced%2520query%2520initialization%2520with%2520LiDAR%2520and%2520Image%2520GuidaNce%2529%252C%2520a%2520novel%2520approach%2520for%2520occlusion-robust%252C%2520object-aware%2520query%2520initialization.%2520Our%2520model%2520consists%2520of%2520three%2520key%2520components%253A%2520%2528i%2529%2520Occlusion-aware%2520Center%2520Estimation%2520%2528OCE%2529%252C%2520which%2520integrates%2520LiDAR%2520geometry%2520and%2520image%2520semantics%2520to%2520estimate%2520object%2520centers%2520accurately%2520%2528ii%2529%2520Adaptive%2520Neighbor%2520Sampling%2520%2528ANS%2529%252C%2520which%2520generates%2520object%2520candidates%2520from%2520LiDAR%2520clustering%2520and%2520supplements%2520each%2520object%2520by%2520sampling%2520spatially%2520and%2520semantically%2520aligned%2520points%2520around%2520it%2520and%2520%2528iii%2529%2520Dynamic%2520Query%2520Balancing%2520%2528DQB%2529%252C%2520which%2520adaptively%2520balances%2520queries%2520between%2520foreground%2520and%2520background%2520regions.%2520Our%2520extensive%2520experiments%2520on%2520the%2520nuScenes%2520benchmark%2520demonstrate%2520that%2520ALIGN%2520consistently%2520improves%2520performance%2520across%2520multiple%2520state-of-the-art%2520detectors%252C%2520achieving%2520gains%2520of%2520up%2520to%2520%252B0.9%2520mAP%2520and%2520%252B1.2%2520NDS%252C%2520particularly%2520in%2520challenging%2520scenes%2520with%2520occlusions%2520or%2520dense%2520crowds.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALIGN%3A%20Advanced%20Query%20Initialization%20with%20LiDAR-Image%20Guidance%20for%20Occlusion-Robust%203D%20Object%20Detection&entry.906535625=Janghyun%20Baek%20and%20Mincheol%20Chang%20and%20Seokha%20Moon%20and%20Seung%20Joon%20Lee%20and%20Jinkyu%20Kim&entry.1292438233=Recent%20query-based%203D%20object%20detection%20methods%20using%20camera%20and%20LiDAR%20inputs%20have%20shown%20strong%20performance%2C%20but%20existing%20query%20initialization%20strategies%2Csuch%20as%20random%20sampling%20or%20BEV%20heatmap-based%20sampling%2C%20often%20result%20in%20inefficient%20query%20usage%20and%20reduced%20accuracy%2C%20particularly%20for%20occluded%20or%20crowded%20objects.%20To%20address%20this%20limitation%2C%20we%20propose%20ALIGN%20%28Advanced%20query%20initialization%20with%20LiDAR%20and%20Image%20GuidaNce%29%2C%20a%20novel%20approach%20for%20occlusion-robust%2C%20object-aware%20query%20initialization.%20Our%20model%20consists%20of%20three%20key%20components%3A%20%28i%29%20Occlusion-aware%20Center%20Estimation%20%28OCE%29%2C%20which%20integrates%20LiDAR%20geometry%20and%20image%20semantics%20to%20estimate%20object%20centers%20accurately%20%28ii%29%20Adaptive%20Neighbor%20Sampling%20%28ANS%29%2C%20which%20generates%20object%20candidates%20from%20LiDAR%20clustering%20and%20supplements%20each%20object%20by%20sampling%20spatially%20and%20semantically%20aligned%20points%20around%20it%20and%20%28iii%29%20Dynamic%20Query%20Balancing%20%28DQB%29%2C%20which%20adaptively%20balances%20queries%20between%20foreground%20and%20background%20regions.%20Our%20extensive%20experiments%20on%20the%20nuScenes%20benchmark%20demonstrate%20that%20ALIGN%20consistently%20improves%20performance%20across%20multiple%20state-of-the-art%20detectors%2C%20achieving%20gains%20of%20up%20to%20%2B0.9%20mAP%20and%20%2B1.2%20NDS%2C%20particularly%20in%20challenging%20scenes%20with%20occlusions%20or%20dense%20crowds.%20Our%20code%20will%20be%20publicly%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2512.18187v2&entry.124074799=Read"},
{"title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation", "author": "Fei Zhang and Rob Chancia and Josie Clapp and Amirhossein Hassanzadeh and Dimah Dera and Richard MacKenzie and Jan van Aardt", "abstract": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.", "link": "http://arxiv.org/abs/2510.06582v3", "date": "2026-02-09", "relevancy": 2.3738, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6268}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through%20the%20Perspective%20of%20LiDAR%3A%20A%20Feature-Enriched%20and%20Uncertainty-Aware%20Annotation%20Pipeline%20for%20Terrestrial%20Point%20Cloud%20Segmentation&body=Title%3A%20Through%20the%20Perspective%20of%20LiDAR%3A%20A%20Feature-Enriched%20and%20Uncertainty-Aware%20Annotation%20Pipeline%20for%20Terrestrial%20Point%20Cloud%20Segmentation%0AAuthor%3A%20Fei%20Zhang%20and%20Rob%20Chancia%20and%20Josie%20Clapp%20and%20Amirhossein%20Hassanzadeh%20and%20Dimah%20Dera%20and%20Richard%20MacKenzie%20and%20Jan%20van%20Aardt%0AAbstract%3A%20Accurate%20semantic%20segmentation%20of%20terrestrial%20laser%20scanning%20%28TLS%29%20point%20clouds%20is%20limited%20by%20costly%20manual%20annotation.%20We%20propose%20a%20semi-automated%2C%20uncertainty-aware%20pipeline%20that%20integrates%20spherical%20projection%2C%20feature%20enrichment%2C%20ensemble%20learning%2C%20and%20targeted%20annotation%20to%20reduce%20labeling%20effort%2C%20while%20sustaining%20high%20accuracy.%20Our%20approach%20projects%203D%20points%20to%20a%202D%20spherical%20grid%2C%20enriches%20pixels%20with%20multi-source%20features%2C%20and%20trains%20an%20ensemble%20of%20segmentation%20networks%20to%20produce%20pseudo-labels%20and%20uncertainty%20maps%2C%20the%20latter%20guiding%20annotation%20of%20ambiguous%20regions.%20The%202D%20outputs%20are%20back-projected%20to%203D%2C%20yielding%20densely%20annotated%20point%20clouds%20supported%20by%20a%20three-tier%20visualization%20suite%20%282D%20feature%20maps%2C%203D%20colorized%20point%20clouds%2C%20and%20compact%20virtual%20spheres%29%20for%20rapid%20triage%20and%20reviewer%20guidance.%20Using%20this%20pipeline%2C%20we%20build%20Mangrove3D%2C%20a%20semantic%20segmentation%20TLS%20dataset%20for%20mangrove%20forests.%20We%20further%20evaluate%20data%20efficiency%20and%20feature%20importance%20to%20address%20two%20key%20questions%3A%20%281%29%20how%20much%20annotated%20data%20are%20needed%20and%20%282%29%20which%20features%20matter%20most.%20Results%20show%20that%20performance%20saturates%20after%20~12%20annotated%20scans%2C%20geometric%20features%20contribute%20the%20most%2C%20and%20compact%20nine-channel%20stacks%20capture%20nearly%20all%20discriminative%20power%2C%20with%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20plateauing%20at%20around%200.76.%20Finally%2C%20we%20confirm%20the%20generalization%20of%20our%20feature-enrichment%20strategy%20through%20cross-dataset%20tests%20on%20ForestSemantic%20and%20Semantic3D.%0A%20%20Our%20contributions%20include%3A%20%28i%29%20a%20robust%2C%20uncertainty-aware%20TLS%20annotation%20pipeline%20with%20visualization%20tools%3B%20%28ii%29%20the%20Mangrove3D%20dataset%3B%20and%20%28iii%29%20empirical%20guidance%20on%20data%20efficiency%20and%20feature%20importance%2C%20thus%20enabling%20scalable%2C%20high-quality%20segmentation%20of%20TLS%20point%20clouds%20for%20ecological%20monitoring%20and%20beyond.%20The%20dataset%20and%20processing%20scripts%20are%20publicly%20available%20at%20https%3A//fz-rit.github.io/through-the-lidars-eye/.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06582v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough%2520the%2520Perspective%2520of%2520LiDAR%253A%2520A%2520Feature-Enriched%2520and%2520Uncertainty-Aware%2520Annotation%2520Pipeline%2520for%2520Terrestrial%2520Point%2520Cloud%2520Segmentation%26entry.906535625%3DFei%2520Zhang%2520and%2520Rob%2520Chancia%2520and%2520Josie%2520Clapp%2520and%2520Amirhossein%2520Hassanzadeh%2520and%2520Dimah%2520Dera%2520and%2520Richard%2520MacKenzie%2520and%2520Jan%2520van%2520Aardt%26entry.1292438233%3DAccurate%2520semantic%2520segmentation%2520of%2520terrestrial%2520laser%2520scanning%2520%2528TLS%2529%2520point%2520clouds%2520is%2520limited%2520by%2520costly%2520manual%2520annotation.%2520We%2520propose%2520a%2520semi-automated%252C%2520uncertainty-aware%2520pipeline%2520that%2520integrates%2520spherical%2520projection%252C%2520feature%2520enrichment%252C%2520ensemble%2520learning%252C%2520and%2520targeted%2520annotation%2520to%2520reduce%2520labeling%2520effort%252C%2520while%2520sustaining%2520high%2520accuracy.%2520Our%2520approach%2520projects%25203D%2520points%2520to%2520a%25202D%2520spherical%2520grid%252C%2520enriches%2520pixels%2520with%2520multi-source%2520features%252C%2520and%2520trains%2520an%2520ensemble%2520of%2520segmentation%2520networks%2520to%2520produce%2520pseudo-labels%2520and%2520uncertainty%2520maps%252C%2520the%2520latter%2520guiding%2520annotation%2520of%2520ambiguous%2520regions.%2520The%25202D%2520outputs%2520are%2520back-projected%2520to%25203D%252C%2520yielding%2520densely%2520annotated%2520point%2520clouds%2520supported%2520by%2520a%2520three-tier%2520visualization%2520suite%2520%25282D%2520feature%2520maps%252C%25203D%2520colorized%2520point%2520clouds%252C%2520and%2520compact%2520virtual%2520spheres%2529%2520for%2520rapid%2520triage%2520and%2520reviewer%2520guidance.%2520Using%2520this%2520pipeline%252C%2520we%2520build%2520Mangrove3D%252C%2520a%2520semantic%2520segmentation%2520TLS%2520dataset%2520for%2520mangrove%2520forests.%2520We%2520further%2520evaluate%2520data%2520efficiency%2520and%2520feature%2520importance%2520to%2520address%2520two%2520key%2520questions%253A%2520%25281%2529%2520how%2520much%2520annotated%2520data%2520are%2520needed%2520and%2520%25282%2529%2520which%2520features%2520matter%2520most.%2520Results%2520show%2520that%2520performance%2520saturates%2520after%2520~12%2520annotated%2520scans%252C%2520geometric%2520features%2520contribute%2520the%2520most%252C%2520and%2520compact%2520nine-channel%2520stacks%2520capture%2520nearly%2520all%2520discriminative%2520power%252C%2520with%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520plateauing%2520at%2520around%25200.76.%2520Finally%252C%2520we%2520confirm%2520the%2520generalization%2520of%2520our%2520feature-enrichment%2520strategy%2520through%2520cross-dataset%2520tests%2520on%2520ForestSemantic%2520and%2520Semantic3D.%250A%2520%2520Our%2520contributions%2520include%253A%2520%2528i%2529%2520a%2520robust%252C%2520uncertainty-aware%2520TLS%2520annotation%2520pipeline%2520with%2520visualization%2520tools%253B%2520%2528ii%2529%2520the%2520Mangrove3D%2520dataset%253B%2520and%2520%2528iii%2529%2520empirical%2520guidance%2520on%2520data%2520efficiency%2520and%2520feature%2520importance%252C%2520thus%2520enabling%2520scalable%252C%2520high-quality%2520segmentation%2520of%2520TLS%2520point%2520clouds%2520for%2520ecological%2520monitoring%2520and%2520beyond.%2520The%2520dataset%2520and%2520processing%2520scripts%2520are%2520publicly%2520available%2520at%2520https%253A//fz-rit.github.io/through-the-lidars-eye/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06582v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through%20the%20Perspective%20of%20LiDAR%3A%20A%20Feature-Enriched%20and%20Uncertainty-Aware%20Annotation%20Pipeline%20for%20Terrestrial%20Point%20Cloud%20Segmentation&entry.906535625=Fei%20Zhang%20and%20Rob%20Chancia%20and%20Josie%20Clapp%20and%20Amirhossein%20Hassanzadeh%20and%20Dimah%20Dera%20and%20Richard%20MacKenzie%20and%20Jan%20van%20Aardt&entry.1292438233=Accurate%20semantic%20segmentation%20of%20terrestrial%20laser%20scanning%20%28TLS%29%20point%20clouds%20is%20limited%20by%20costly%20manual%20annotation.%20We%20propose%20a%20semi-automated%2C%20uncertainty-aware%20pipeline%20that%20integrates%20spherical%20projection%2C%20feature%20enrichment%2C%20ensemble%20learning%2C%20and%20targeted%20annotation%20to%20reduce%20labeling%20effort%2C%20while%20sustaining%20high%20accuracy.%20Our%20approach%20projects%203D%20points%20to%20a%202D%20spherical%20grid%2C%20enriches%20pixels%20with%20multi-source%20features%2C%20and%20trains%20an%20ensemble%20of%20segmentation%20networks%20to%20produce%20pseudo-labels%20and%20uncertainty%20maps%2C%20the%20latter%20guiding%20annotation%20of%20ambiguous%20regions.%20The%202D%20outputs%20are%20back-projected%20to%203D%2C%20yielding%20densely%20annotated%20point%20clouds%20supported%20by%20a%20three-tier%20visualization%20suite%20%282D%20feature%20maps%2C%203D%20colorized%20point%20clouds%2C%20and%20compact%20virtual%20spheres%29%20for%20rapid%20triage%20and%20reviewer%20guidance.%20Using%20this%20pipeline%2C%20we%20build%20Mangrove3D%2C%20a%20semantic%20segmentation%20TLS%20dataset%20for%20mangrove%20forests.%20We%20further%20evaluate%20data%20efficiency%20and%20feature%20importance%20to%20address%20two%20key%20questions%3A%20%281%29%20how%20much%20annotated%20data%20are%20needed%20and%20%282%29%20which%20features%20matter%20most.%20Results%20show%20that%20performance%20saturates%20after%20~12%20annotated%20scans%2C%20geometric%20features%20contribute%20the%20most%2C%20and%20compact%20nine-channel%20stacks%20capture%20nearly%20all%20discriminative%20power%2C%20with%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20plateauing%20at%20around%200.76.%20Finally%2C%20we%20confirm%20the%20generalization%20of%20our%20feature-enrichment%20strategy%20through%20cross-dataset%20tests%20on%20ForestSemantic%20and%20Semantic3D.%0A%20%20Our%20contributions%20include%3A%20%28i%29%20a%20robust%2C%20uncertainty-aware%20TLS%20annotation%20pipeline%20with%20visualization%20tools%3B%20%28ii%29%20the%20Mangrove3D%20dataset%3B%20and%20%28iii%29%20empirical%20guidance%20on%20data%20efficiency%20and%20feature%20importance%2C%20thus%20enabling%20scalable%2C%20high-quality%20segmentation%20of%20TLS%20point%20clouds%20for%20ecological%20monitoring%20and%20beyond.%20The%20dataset%20and%20processing%20scripts%20are%20publicly%20available%20at%20https%3A//fz-rit.github.io/through-the-lidars-eye/.&entry.1838667208=http%3A//arxiv.org/abs/2510.06582v3&entry.124074799=Read"},
{"title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning", "author": "Yuchen Yan and Liang Jiang and Jin Jiang and Shuaicheng Li and Zujie Wen and Zhiqiang Zhang and Jun Zhou and Jian Shao and Yueting Zhuang and Yongliang Shen", "abstract": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.", "link": "http://arxiv.org/abs/2602.06960v2", "date": "2026-02-09", "relevancy": 2.371, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.477}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4735}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InftyThink%2B%3A%20Effective%20and%20Efficient%20Infinite-Horizon%20Reasoning%20via%20Reinforcement%20Learning&body=Title%3A%20InftyThink%2B%3A%20Effective%20and%20Efficient%20Infinite-Horizon%20Reasoning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Yuchen%20Yan%20and%20Liang%20Jiang%20and%20Jin%20Jiang%20and%20Shuaicheng%20Li%20and%20Zujie%20Wen%20and%20Zhiqiang%20Zhang%20and%20Jun%20Zhou%20and%20Jian%20Shao%20and%20Yueting%20Zhuang%20and%20Yongliang%20Shen%0AAbstract%3A%20Large%20reasoning%20models%20achieve%20strong%20performance%20by%20scaling%20inference-time%20chain-of-thought%2C%20but%20this%20paradigm%20suffers%20from%20quadratic%20cost%2C%20context%20length%20limits%2C%20and%20degraded%20reasoning%20due%20to%20lost-in-the-middle%20effects.%20Iterative%20reasoning%20mitigates%20these%20issues%20by%20periodically%20summarizing%20intermediate%20thoughts%2C%20yet%20existing%20methods%20rely%20on%20supervised%20learning%20or%20fixed%20heuristics%20and%20fail%20to%20optimize%20when%20to%20summarize%2C%20what%20to%20preserve%2C%20and%20how%20to%20resume%20reasoning.%20We%20propose%20InftyThink%2B%2C%20an%20end-to-end%20reinforcement%20learning%20framework%20that%20optimizes%20the%20entire%20iterative%20reasoning%20trajectory%2C%20building%20on%20model-controlled%20iteration%20boundaries%20and%20explicit%20summarization.%20InftyThink%2B%20adopts%20a%20two-stage%20training%20scheme%20with%20supervised%20cold-start%20followed%20by%20trajectory-level%20reinforcement%20learning%2C%20enabling%20the%20model%20to%20learn%20strategic%20summarization%20and%20continuation%20decisions.%20Experiments%20on%20DeepSeek-R1-Distill-Qwen-1.5B%20show%20that%20InftyThink%2B%20improves%20accuracy%20by%2021%25%20on%20AIME24%20and%20outperforms%20conventional%20long%20chain-of-thought%20reinforcement%20learning%20by%20a%20clear%20margin%2C%20while%20also%20generalizing%20better%20to%20out-of-distribution%20benchmarks.%20Moreover%2C%20InftyThink%2B%20significantly%20reduces%20inference%20latency%20and%20accelerates%20reinforcement%20learning%20training%2C%20demonstrating%20improved%20reasoning%20efficiency%20alongside%20stronger%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInftyThink%252B%253A%2520Effective%2520and%2520Efficient%2520Infinite-Horizon%2520Reasoning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DYuchen%2520Yan%2520and%2520Liang%2520Jiang%2520and%2520Jin%2520Jiang%2520and%2520Shuaicheng%2520Li%2520and%2520Zujie%2520Wen%2520and%2520Zhiqiang%2520Zhang%2520and%2520Jun%2520Zhou%2520and%2520Jian%2520Shao%2520and%2520Yueting%2520Zhuang%2520and%2520Yongliang%2520Shen%26entry.1292438233%3DLarge%2520reasoning%2520models%2520achieve%2520strong%2520performance%2520by%2520scaling%2520inference-time%2520chain-of-thought%252C%2520but%2520this%2520paradigm%2520suffers%2520from%2520quadratic%2520cost%252C%2520context%2520length%2520limits%252C%2520and%2520degraded%2520reasoning%2520due%2520to%2520lost-in-the-middle%2520effects.%2520Iterative%2520reasoning%2520mitigates%2520these%2520issues%2520by%2520periodically%2520summarizing%2520intermediate%2520thoughts%252C%2520yet%2520existing%2520methods%2520rely%2520on%2520supervised%2520learning%2520or%2520fixed%2520heuristics%2520and%2520fail%2520to%2520optimize%2520when%2520to%2520summarize%252C%2520what%2520to%2520preserve%252C%2520and%2520how%2520to%2520resume%2520reasoning.%2520We%2520propose%2520InftyThink%252B%252C%2520an%2520end-to-end%2520reinforcement%2520learning%2520framework%2520that%2520optimizes%2520the%2520entire%2520iterative%2520reasoning%2520trajectory%252C%2520building%2520on%2520model-controlled%2520iteration%2520boundaries%2520and%2520explicit%2520summarization.%2520InftyThink%252B%2520adopts%2520a%2520two-stage%2520training%2520scheme%2520with%2520supervised%2520cold-start%2520followed%2520by%2520trajectory-level%2520reinforcement%2520learning%252C%2520enabling%2520the%2520model%2520to%2520learn%2520strategic%2520summarization%2520and%2520continuation%2520decisions.%2520Experiments%2520on%2520DeepSeek-R1-Distill-Qwen-1.5B%2520show%2520that%2520InftyThink%252B%2520improves%2520accuracy%2520by%252021%2525%2520on%2520AIME24%2520and%2520outperforms%2520conventional%2520long%2520chain-of-thought%2520reinforcement%2520learning%2520by%2520a%2520clear%2520margin%252C%2520while%2520also%2520generalizing%2520better%2520to%2520out-of-distribution%2520benchmarks.%2520Moreover%252C%2520InftyThink%252B%2520significantly%2520reduces%2520inference%2520latency%2520and%2520accelerates%2520reinforcement%2520learning%2520training%252C%2520demonstrating%2520improved%2520reasoning%2520efficiency%2520alongside%2520stronger%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InftyThink%2B%3A%20Effective%20and%20Efficient%20Infinite-Horizon%20Reasoning%20via%20Reinforcement%20Learning&entry.906535625=Yuchen%20Yan%20and%20Liang%20Jiang%20and%20Jin%20Jiang%20and%20Shuaicheng%20Li%20and%20Zujie%20Wen%20and%20Zhiqiang%20Zhang%20and%20Jun%20Zhou%20and%20Jian%20Shao%20and%20Yueting%20Zhuang%20and%20Yongliang%20Shen&entry.1292438233=Large%20reasoning%20models%20achieve%20strong%20performance%20by%20scaling%20inference-time%20chain-of-thought%2C%20but%20this%20paradigm%20suffers%20from%20quadratic%20cost%2C%20context%20length%20limits%2C%20and%20degraded%20reasoning%20due%20to%20lost-in-the-middle%20effects.%20Iterative%20reasoning%20mitigates%20these%20issues%20by%20periodically%20summarizing%20intermediate%20thoughts%2C%20yet%20existing%20methods%20rely%20on%20supervised%20learning%20or%20fixed%20heuristics%20and%20fail%20to%20optimize%20when%20to%20summarize%2C%20what%20to%20preserve%2C%20and%20how%20to%20resume%20reasoning.%20We%20propose%20InftyThink%2B%2C%20an%20end-to-end%20reinforcement%20learning%20framework%20that%20optimizes%20the%20entire%20iterative%20reasoning%20trajectory%2C%20building%20on%20model-controlled%20iteration%20boundaries%20and%20explicit%20summarization.%20InftyThink%2B%20adopts%20a%20two-stage%20training%20scheme%20with%20supervised%20cold-start%20followed%20by%20trajectory-level%20reinforcement%20learning%2C%20enabling%20the%20model%20to%20learn%20strategic%20summarization%20and%20continuation%20decisions.%20Experiments%20on%20DeepSeek-R1-Distill-Qwen-1.5B%20show%20that%20InftyThink%2B%20improves%20accuracy%20by%2021%25%20on%20AIME24%20and%20outperforms%20conventional%20long%20chain-of-thought%20reinforcement%20learning%20by%20a%20clear%20margin%2C%20while%20also%20generalizing%20better%20to%20out-of-distribution%20benchmarks.%20Moreover%2C%20InftyThink%2B%20significantly%20reduces%20inference%20latency%20and%20accelerates%20reinforcement%20learning%20training%2C%20demonstrating%20improved%20reasoning%20efficiency%20alongside%20stronger%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2602.06960v2&entry.124074799=Read"},
{"title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction", "author": "Shaobin Zhuang and Yiwei Guo and Canmiao Fu and Zhipeng Huang and Zeyue Tian and Xiaohui Li and Fangyikang Wang and Ying Zhang and Chen Li and Yali Wang", "abstract": "Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoder (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratio. On the ImageNet 50k validation set, at a high-fidelity setting, WeTok achieves a record-low zero-shot rFID of 0.12, outperforming leading continuous tokenizers like FLUX-VAE (0.18) and SD-VAE 3.5 (0.19) with 400% compression ratio. Furthermore, in a high-compression regime, WeTok achieves a zero-shot rFID of 3.49 at a 768$\\times$ compression ratio, substantially surpassing Cosmos, which scores 4.57 at only 50% our compression ratio. Code and models are available: https://github.com/zhuangshaobin/WeTok.", "link": "http://arxiv.org/abs/2508.05599v3", "date": "2026-02-09", "relevancy": 2.3516, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6361}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5585}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%20Reconstruction&body=Title%3A%20WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%20Reconstruction%0AAuthor%3A%20Shaobin%20Zhuang%20and%20Yiwei%20Guo%20and%20Canmiao%20Fu%20and%20Zhipeng%20Huang%20and%20Zeyue%20Tian%20and%20Xiaohui%20Li%20and%20Fangyikang%20Wang%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Yali%20Wang%0AAbstract%3A%20Visual%20tokenizer%20is%20a%20critical%20component%20for%20vision%20generation.%20However%2C%20the%20existing%20tokenizers%20often%20face%20unsatisfactory%20trade-off%20between%20compression%20ratios%20and%20reconstruction%20fidelity.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20powerful%20and%20concise%20WeTok%20tokenizer%2C%20which%20surpasses%20the%20previous%20leading%20tokenizers%20via%20two%20core%20innovations.%20%281%29%20Group-wise%20lookup-free%20Quantization%20%28GQ%29.%20We%20partition%20the%20latent%20features%20into%20groups%2C%20and%20perform%20lookup-free%20quantization%20for%20each%20group.%20As%20a%20result%2C%20GQ%20can%20efficiently%20overcome%20memory%20and%20computation%20limitations%20of%20prior%20tokenizers%2C%20while%20achieving%20a%20reconstruction%20breakthrough%20with%20more%20scalable%20codebooks.%20%282%29%20Generative%20Decoder%20%28GD%29.%20Different%20from%20prior%20tokenizers%2C%20we%20introduce%20a%20generative%20decoder%20with%20a%20prior%20of%20extra%20noise%20variable.%20In%20this%20case%2C%20GD%20can%20probabilistically%20model%20the%20distribution%20of%20visual%20data%20conditioned%20on%20discrete%20tokens%2C%20allowing%20WeTok%20to%20reconstruct%20visual%20details%2C%20especially%20at%20high%20compression%20ratio.%20On%20the%20ImageNet%2050k%20validation%20set%2C%20at%20a%20high-fidelity%20setting%2C%20WeTok%20achieves%20a%20record-low%20zero-shot%20rFID%20of%200.12%2C%20outperforming%20leading%20continuous%20tokenizers%20like%20FLUX-VAE%20%280.18%29%20and%20SD-VAE%203.5%20%280.19%29%20with%20400%25%20compression%20ratio.%20Furthermore%2C%20in%20a%20high-compression%20regime%2C%20WeTok%20achieves%20a%20zero-shot%20rFID%20of%203.49%20at%20a%20768%24%5Ctimes%24%20compression%20ratio%2C%20substantially%20surpassing%20Cosmos%2C%20which%20scores%204.57%20at%20only%2050%25%20our%20compression%20ratio.%20Code%20and%20models%20are%20available%3A%20https%3A//github.com/zhuangshaobin/WeTok.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05599v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeTok%253A%2520Powerful%2520Discrete%2520Tokenization%2520for%2520High-Fidelity%2520Visual%2520Reconstruction%26entry.906535625%3DShaobin%2520Zhuang%2520and%2520Yiwei%2520Guo%2520and%2520Canmiao%2520Fu%2520and%2520Zhipeng%2520Huang%2520and%2520Zeyue%2520Tian%2520and%2520Xiaohui%2520Li%2520and%2520Fangyikang%2520Wang%2520and%2520Ying%2520Zhang%2520and%2520Chen%2520Li%2520and%2520Yali%2520Wang%26entry.1292438233%3DVisual%2520tokenizer%2520is%2520a%2520critical%2520component%2520for%2520vision%2520generation.%2520However%252C%2520the%2520existing%2520tokenizers%2520often%2520face%2520unsatisfactory%2520trade-off%2520between%2520compression%2520ratios%2520and%2520reconstruction%2520fidelity.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520a%2520powerful%2520and%2520concise%2520WeTok%2520tokenizer%252C%2520which%2520surpasses%2520the%2520previous%2520leading%2520tokenizers%2520via%2520two%2520core%2520innovations.%2520%25281%2529%2520Group-wise%2520lookup-free%2520Quantization%2520%2528GQ%2529.%2520We%2520partition%2520the%2520latent%2520features%2520into%2520groups%252C%2520and%2520perform%2520lookup-free%2520quantization%2520for%2520each%2520group.%2520As%2520a%2520result%252C%2520GQ%2520can%2520efficiently%2520overcome%2520memory%2520and%2520computation%2520limitations%2520of%2520prior%2520tokenizers%252C%2520while%2520achieving%2520a%2520reconstruction%2520breakthrough%2520with%2520more%2520scalable%2520codebooks.%2520%25282%2529%2520Generative%2520Decoder%2520%2528GD%2529.%2520Different%2520from%2520prior%2520tokenizers%252C%2520we%2520introduce%2520a%2520generative%2520decoder%2520with%2520a%2520prior%2520of%2520extra%2520noise%2520variable.%2520In%2520this%2520case%252C%2520GD%2520can%2520probabilistically%2520model%2520the%2520distribution%2520of%2520visual%2520data%2520conditioned%2520on%2520discrete%2520tokens%252C%2520allowing%2520WeTok%2520to%2520reconstruct%2520visual%2520details%252C%2520especially%2520at%2520high%2520compression%2520ratio.%2520On%2520the%2520ImageNet%252050k%2520validation%2520set%252C%2520at%2520a%2520high-fidelity%2520setting%252C%2520WeTok%2520achieves%2520a%2520record-low%2520zero-shot%2520rFID%2520of%25200.12%252C%2520outperforming%2520leading%2520continuous%2520tokenizers%2520like%2520FLUX-VAE%2520%25280.18%2529%2520and%2520SD-VAE%25203.5%2520%25280.19%2529%2520with%2520400%2525%2520compression%2520ratio.%2520Furthermore%252C%2520in%2520a%2520high-compression%2520regime%252C%2520WeTok%2520achieves%2520a%2520zero-shot%2520rFID%2520of%25203.49%2520at%2520a%2520768%2524%255Ctimes%2524%2520compression%2520ratio%252C%2520substantially%2520surpassing%2520Cosmos%252C%2520which%2520scores%25204.57%2520at%2520only%252050%2525%2520our%2520compression%2520ratio.%2520Code%2520and%2520models%2520are%2520available%253A%2520https%253A//github.com/zhuangshaobin/WeTok.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05599v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%20Reconstruction&entry.906535625=Shaobin%20Zhuang%20and%20Yiwei%20Guo%20and%20Canmiao%20Fu%20and%20Zhipeng%20Huang%20and%20Zeyue%20Tian%20and%20Xiaohui%20Li%20and%20Fangyikang%20Wang%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Yali%20Wang&entry.1292438233=Visual%20tokenizer%20is%20a%20critical%20component%20for%20vision%20generation.%20However%2C%20the%20existing%20tokenizers%20often%20face%20unsatisfactory%20trade-off%20between%20compression%20ratios%20and%20reconstruction%20fidelity.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20powerful%20and%20concise%20WeTok%20tokenizer%2C%20which%20surpasses%20the%20previous%20leading%20tokenizers%20via%20two%20core%20innovations.%20%281%29%20Group-wise%20lookup-free%20Quantization%20%28GQ%29.%20We%20partition%20the%20latent%20features%20into%20groups%2C%20and%20perform%20lookup-free%20quantization%20for%20each%20group.%20As%20a%20result%2C%20GQ%20can%20efficiently%20overcome%20memory%20and%20computation%20limitations%20of%20prior%20tokenizers%2C%20while%20achieving%20a%20reconstruction%20breakthrough%20with%20more%20scalable%20codebooks.%20%282%29%20Generative%20Decoder%20%28GD%29.%20Different%20from%20prior%20tokenizers%2C%20we%20introduce%20a%20generative%20decoder%20with%20a%20prior%20of%20extra%20noise%20variable.%20In%20this%20case%2C%20GD%20can%20probabilistically%20model%20the%20distribution%20of%20visual%20data%20conditioned%20on%20discrete%20tokens%2C%20allowing%20WeTok%20to%20reconstruct%20visual%20details%2C%20especially%20at%20high%20compression%20ratio.%20On%20the%20ImageNet%2050k%20validation%20set%2C%20at%20a%20high-fidelity%20setting%2C%20WeTok%20achieves%20a%20record-low%20zero-shot%20rFID%20of%200.12%2C%20outperforming%20leading%20continuous%20tokenizers%20like%20FLUX-VAE%20%280.18%29%20and%20SD-VAE%203.5%20%280.19%29%20with%20400%25%20compression%20ratio.%20Furthermore%2C%20in%20a%20high-compression%20regime%2C%20WeTok%20achieves%20a%20zero-shot%20rFID%20of%203.49%20at%20a%20768%24%5Ctimes%24%20compression%20ratio%2C%20substantially%20surpassing%20Cosmos%2C%20which%20scores%204.57%20at%20only%2050%25%20our%20compression%20ratio.%20Code%20and%20models%20are%20available%3A%20https%3A//github.com/zhuangshaobin/WeTok.&entry.1838667208=http%3A//arxiv.org/abs/2508.05599v3&entry.124074799=Read"},
{"title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "author": "Kfir Goldberg and Elad Richardson and Yael Vinker", "abstract": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "link": "http://arxiv.org/abs/2602.08615v1", "date": "2026-02-09", "relevancy": 2.339, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6217}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration&body=Title%3A%20Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration%0AAuthor%3A%20Kfir%20Goldberg%20and%20Elad%20Richardson%20and%20Yael%20Vinker%0AAbstract%3A%20While%20generative%20models%20have%20become%20powerful%20tools%20for%20image%20synthesis%2C%20they%20are%20typically%20optimized%20for%20executing%20carefully%20crafted%20textual%20prompts%2C%20offering%20limited%20support%20for%20the%20open-ended%20visual%20exploration%20that%20often%20precedes%20idea%20formation.%20In%20contrast%2C%20designers%20frequently%20draw%20inspiration%20from%20loosely%20connected%20visual%20references%2C%20seeking%20emergent%20connections%20that%20spark%20new%20ideas.%20We%20propose%20Inspiration%20Seeds%2C%20a%20generative%20framework%20that%20shifts%20image%20generation%20from%20final%20execution%20to%20exploratory%20ideation.%20Given%20two%20input%20images%2C%20our%20model%20produces%20diverse%2C%20visually%20coherent%20compositions%20that%20reveal%20latent%20relationships%20between%20inputs%2C%20without%20relying%20on%20user-specified%20text%20prompts.%20Our%20approach%20is%20feed-forward%2C%20trained%20on%20synthetic%20triplets%20of%20decomposed%20visual%20aspects%20derived%20entirely%20through%20visual%20means%3A%20we%20use%20CLIP%20Sparse%20Autoencoders%20to%20extract%20editing%20directions%20in%20CLIP%20latent%20space%20and%20isolate%20concept%20pairs.%20By%20removing%20the%20reliance%20on%20language%20and%20enabling%20fast%2C%20intuitive%20recombination%2C%20our%20method%20supports%20visual%20ideation%20at%20the%20early%20and%20ambiguous%20stages%20of%20creative%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInspiration%2520Seeds%253A%2520Learning%2520Non-Literal%2520Visual%2520Combinations%2520for%2520Generative%2520Exploration%26entry.906535625%3DKfir%2520Goldberg%2520and%2520Elad%2520Richardson%2520and%2520Yael%2520Vinker%26entry.1292438233%3DWhile%2520generative%2520models%2520have%2520become%2520powerful%2520tools%2520for%2520image%2520synthesis%252C%2520they%2520are%2520typically%2520optimized%2520for%2520executing%2520carefully%2520crafted%2520textual%2520prompts%252C%2520offering%2520limited%2520support%2520for%2520the%2520open-ended%2520visual%2520exploration%2520that%2520often%2520precedes%2520idea%2520formation.%2520In%2520contrast%252C%2520designers%2520frequently%2520draw%2520inspiration%2520from%2520loosely%2520connected%2520visual%2520references%252C%2520seeking%2520emergent%2520connections%2520that%2520spark%2520new%2520ideas.%2520We%2520propose%2520Inspiration%2520Seeds%252C%2520a%2520generative%2520framework%2520that%2520shifts%2520image%2520generation%2520from%2520final%2520execution%2520to%2520exploratory%2520ideation.%2520Given%2520two%2520input%2520images%252C%2520our%2520model%2520produces%2520diverse%252C%2520visually%2520coherent%2520compositions%2520that%2520reveal%2520latent%2520relationships%2520between%2520inputs%252C%2520without%2520relying%2520on%2520user-specified%2520text%2520prompts.%2520Our%2520approach%2520is%2520feed-forward%252C%2520trained%2520on%2520synthetic%2520triplets%2520of%2520decomposed%2520visual%2520aspects%2520derived%2520entirely%2520through%2520visual%2520means%253A%2520we%2520use%2520CLIP%2520Sparse%2520Autoencoders%2520to%2520extract%2520editing%2520directions%2520in%2520CLIP%2520latent%2520space%2520and%2520isolate%2520concept%2520pairs.%2520By%2520removing%2520the%2520reliance%2520on%2520language%2520and%2520enabling%2520fast%252C%2520intuitive%2520recombination%252C%2520our%2520method%2520supports%2520visual%2520ideation%2520at%2520the%2520early%2520and%2520ambiguous%2520stages%2520of%2520creative%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration&entry.906535625=Kfir%20Goldberg%20and%20Elad%20Richardson%20and%20Yael%20Vinker&entry.1292438233=While%20generative%20models%20have%20become%20powerful%20tools%20for%20image%20synthesis%2C%20they%20are%20typically%20optimized%20for%20executing%20carefully%20crafted%20textual%20prompts%2C%20offering%20limited%20support%20for%20the%20open-ended%20visual%20exploration%20that%20often%20precedes%20idea%20formation.%20In%20contrast%2C%20designers%20frequently%20draw%20inspiration%20from%20loosely%20connected%20visual%20references%2C%20seeking%20emergent%20connections%20that%20spark%20new%20ideas.%20We%20propose%20Inspiration%20Seeds%2C%20a%20generative%20framework%20that%20shifts%20image%20generation%20from%20final%20execution%20to%20exploratory%20ideation.%20Given%20two%20input%20images%2C%20our%20model%20produces%20diverse%2C%20visually%20coherent%20compositions%20that%20reveal%20latent%20relationships%20between%20inputs%2C%20without%20relying%20on%20user-specified%20text%20prompts.%20Our%20approach%20is%20feed-forward%2C%20trained%20on%20synthetic%20triplets%20of%20decomposed%20visual%20aspects%20derived%20entirely%20through%20visual%20means%3A%20we%20use%20CLIP%20Sparse%20Autoencoders%20to%20extract%20editing%20directions%20in%20CLIP%20latent%20space%20and%20isolate%20concept%20pairs.%20By%20removing%20the%20reliance%20on%20language%20and%20enabling%20fast%2C%20intuitive%20recombination%2C%20our%20method%20supports%20visual%20ideation%20at%20the%20early%20and%20ambiguous%20stages%20of%20creative%20work.&entry.1838667208=http%3A//arxiv.org/abs/2602.08615v1&entry.124074799=Read"},
{"title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "author": "Weihan Luo and Lily Goli and Sherwin Bahmani and Felix Taubner and Andrea Tagliasacchi and David B. Lindell", "abstract": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "link": "http://arxiv.org/abs/2602.08958v1", "date": "2026-02-09", "relevancy": 2.3312, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5971}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5818}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields&body=Title%3A%20Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields%0AAuthor%3A%20Weihan%20Luo%20and%20Lily%20Goli%20and%20Sherwin%20Bahmani%20and%20Felix%20Taubner%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%0AAbstract%3A%20Modeling%20the%20time-varying%203D%20appearance%20of%20plants%20during%20their%20growth%20poses%20unique%20challenges%3A%20unlike%20many%20dynamic%20scenes%2C%20plants%20generate%20new%20geometry%20over%20time%20as%20they%20expand%2C%20branch%2C%20and%20differentiate.%20Recent%20motion%20modeling%20techniques%20are%20ill-suited%20to%20this%20problem%20setting.%20For%20example%2C%20deformation%20fields%20cannot%20introduce%20new%20geometry%2C%20and%204D%20Gaussian%20splatting%20constrains%20motion%20to%20a%20linear%20trajectory%20in%20space%20and%20time%20and%20cannot%20track%20the%20same%20set%20of%20Gaussians%20over%20time.%20Here%2C%20we%20introduce%20a%203D%20Gaussian%20flow%20field%20representation%20that%20models%20plant%20growth%20as%20a%20time-varying%20derivative%20over%20Gaussian%20parameters%20--%20position%2C%20scale%2C%20orientation%2C%20color%2C%20and%20opacity%20--%20enabling%20nonlinear%20and%20continuous-time%20growth%20dynamics.%20To%20initialize%20a%20sufficient%20set%20of%20Gaussian%20primitives%2C%20we%20reconstruct%20the%20mature%20plant%20and%20learn%20a%20process%20of%20reverse%20growth%2C%20effectively%20simulating%20the%20plant%27s%20developmental%20history%20in%20reverse.%20Our%20approach%20achieves%20superior%20image%20quality%20and%20geometric%20accuracy%20compared%20to%20prior%20methods%20on%20multi-view%20timelapse%20datasets%20of%20plant%20growth%2C%20providing%20a%20new%20approach%20for%20appearance%20modeling%20of%20growing%203D%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrow%2520with%2520the%2520Flow%253A%25204D%2520Reconstruction%2520of%2520Growing%2520Plants%2520with%2520Gaussian%2520Flow%2520Fields%26entry.906535625%3DWeihan%2520Luo%2520and%2520Lily%2520Goli%2520and%2520Sherwin%2520Bahmani%2520and%2520Felix%2520Taubner%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3DModeling%2520the%2520time-varying%25203D%2520appearance%2520of%2520plants%2520during%2520their%2520growth%2520poses%2520unique%2520challenges%253A%2520unlike%2520many%2520dynamic%2520scenes%252C%2520plants%2520generate%2520new%2520geometry%2520over%2520time%2520as%2520they%2520expand%252C%2520branch%252C%2520and%2520differentiate.%2520Recent%2520motion%2520modeling%2520techniques%2520are%2520ill-suited%2520to%2520this%2520problem%2520setting.%2520For%2520example%252C%2520deformation%2520fields%2520cannot%2520introduce%2520new%2520geometry%252C%2520and%25204D%2520Gaussian%2520splatting%2520constrains%2520motion%2520to%2520a%2520linear%2520trajectory%2520in%2520space%2520and%2520time%2520and%2520cannot%2520track%2520the%2520same%2520set%2520of%2520Gaussians%2520over%2520time.%2520Here%252C%2520we%2520introduce%2520a%25203D%2520Gaussian%2520flow%2520field%2520representation%2520that%2520models%2520plant%2520growth%2520as%2520a%2520time-varying%2520derivative%2520over%2520Gaussian%2520parameters%2520--%2520position%252C%2520scale%252C%2520orientation%252C%2520color%252C%2520and%2520opacity%2520--%2520enabling%2520nonlinear%2520and%2520continuous-time%2520growth%2520dynamics.%2520To%2520initialize%2520a%2520sufficient%2520set%2520of%2520Gaussian%2520primitives%252C%2520we%2520reconstruct%2520the%2520mature%2520plant%2520and%2520learn%2520a%2520process%2520of%2520reverse%2520growth%252C%2520effectively%2520simulating%2520the%2520plant%2527s%2520developmental%2520history%2520in%2520reverse.%2520Our%2520approach%2520achieves%2520superior%2520image%2520quality%2520and%2520geometric%2520accuracy%2520compared%2520to%2520prior%2520methods%2520on%2520multi-view%2520timelapse%2520datasets%2520of%2520plant%2520growth%252C%2520providing%2520a%2520new%2520approach%2520for%2520appearance%2520modeling%2520of%2520growing%25203D%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields&entry.906535625=Weihan%20Luo%20and%20Lily%20Goli%20and%20Sherwin%20Bahmani%20and%20Felix%20Taubner%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell&entry.1292438233=Modeling%20the%20time-varying%203D%20appearance%20of%20plants%20during%20their%20growth%20poses%20unique%20challenges%3A%20unlike%20many%20dynamic%20scenes%2C%20plants%20generate%20new%20geometry%20over%20time%20as%20they%20expand%2C%20branch%2C%20and%20differentiate.%20Recent%20motion%20modeling%20techniques%20are%20ill-suited%20to%20this%20problem%20setting.%20For%20example%2C%20deformation%20fields%20cannot%20introduce%20new%20geometry%2C%20and%204D%20Gaussian%20splatting%20constrains%20motion%20to%20a%20linear%20trajectory%20in%20space%20and%20time%20and%20cannot%20track%20the%20same%20set%20of%20Gaussians%20over%20time.%20Here%2C%20we%20introduce%20a%203D%20Gaussian%20flow%20field%20representation%20that%20models%20plant%20growth%20as%20a%20time-varying%20derivative%20over%20Gaussian%20parameters%20--%20position%2C%20scale%2C%20orientation%2C%20color%2C%20and%20opacity%20--%20enabling%20nonlinear%20and%20continuous-time%20growth%20dynamics.%20To%20initialize%20a%20sufficient%20set%20of%20Gaussian%20primitives%2C%20we%20reconstruct%20the%20mature%20plant%20and%20learn%20a%20process%20of%20reverse%20growth%2C%20effectively%20simulating%20the%20plant%27s%20developmental%20history%20in%20reverse.%20Our%20approach%20achieves%20superior%20image%20quality%20and%20geometric%20accuracy%20compared%20to%20prior%20methods%20on%20multi-view%20timelapse%20datasets%20of%20plant%20growth%2C%20providing%20a%20new%20approach%20for%20appearance%20modeling%20of%20growing%203D%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2602.08958v1&entry.124074799=Read"},
{"title": "HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training", "author": "Wen Xu and Zhetao Li and Yong Xiao and Pengpeng Qiao and Mianxiong Dong and Kaoru Ota", "abstract": "Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.", "link": "http://arxiv.org/abs/2602.08762v1", "date": "2026-02-09", "relevancy": 2.3182, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.475}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4618}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoGS%3A%20Homophily-Oriented%20Graph%20Synthesis%20for%20Local%20Differentially%20Private%20GNN%20Training&body=Title%3A%20HoGS%3A%20Homophily-Oriented%20Graph%20Synthesis%20for%20Local%20Differentially%20Private%20GNN%20Training%0AAuthor%3A%20Wen%20Xu%20and%20Zhetao%20Li%20and%20Yong%20Xiao%20and%20Pengpeng%20Qiao%20and%20Mianxiong%20Dong%20and%20Kaoru%20Ota%0AAbstract%3A%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20performance%20in%20various%20graph-based%20machine%20learning%20tasks%20by%20effectively%20modeling%20high-order%20interactions%20between%20nodes.%20However%2C%20training%20GNNs%20without%20protection%20may%20leak%20sensitive%20personal%20information%20in%20graph%20data%2C%20including%20links%20and%20node%20features.%20Local%20differential%20privacy%20%28LDP%29%20is%20an%20advanced%20technique%20for%20protecting%20data%20privacy%20in%20decentralized%20networks.%20Unfortunately%2C%20existing%20local%20differentially%20private%20GNNs%20either%20only%20preserve%20link%20privacy%20or%20suffer%20significant%20utility%20loss%20in%20the%20process%20of%20preserving%20link%20and%20node%20feature%20privacy.%20In%20this%20paper%2C%20we%20propose%20an%20effective%20LDP%20framework%2C%20called%20HoGS%2C%20which%20trains%20GNNs%20with%20link%20and%20feature%20protection%20by%20generating%20a%20synthetic%20graph.%20Concretely%2C%20HoGS%20first%20collects%20the%20link%20and%20feature%20information%20of%20the%20graph%20under%20LDP%2C%20and%20then%20utilizes%20the%20phenomenon%20of%20homophily%20in%20graph%20data%20to%20reconstruct%20the%20graph%20structure%20and%20node%20features%20separately%2C%20thereby%20effectively%20mitigating%20the%20negative%20impact%20of%20LDP%20on%20the%20downstream%20GNN%20training.%20We%20theoretically%20analyze%20the%20privacy%20guarantee%20of%20HoGS%20and%20conduct%20experiments%20using%20the%20generated%20synthetic%20graph%20as%20input%20to%20various%20state-of-the-art%20GNN%20architectures.%20Experimental%20results%20on%20three%20real-world%20datasets%20show%20that%20HoGS%20significantly%20outperforms%20baseline%20methods%20in%20the%20accuracy%20of%20training%20GNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoGS%253A%2520Homophily-Oriented%2520Graph%2520Synthesis%2520for%2520Local%2520Differentially%2520Private%2520GNN%2520Training%26entry.906535625%3DWen%2520Xu%2520and%2520Zhetao%2520Li%2520and%2520Yong%2520Xiao%2520and%2520Pengpeng%2520Qiao%2520and%2520Mianxiong%2520Dong%2520and%2520Kaoru%2520Ota%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520various%2520graph-based%2520machine%2520learning%2520tasks%2520by%2520effectively%2520modeling%2520high-order%2520interactions%2520between%2520nodes.%2520However%252C%2520training%2520GNNs%2520without%2520protection%2520may%2520leak%2520sensitive%2520personal%2520information%2520in%2520graph%2520data%252C%2520including%2520links%2520and%2520node%2520features.%2520Local%2520differential%2520privacy%2520%2528LDP%2529%2520is%2520an%2520advanced%2520technique%2520for%2520protecting%2520data%2520privacy%2520in%2520decentralized%2520networks.%2520Unfortunately%252C%2520existing%2520local%2520differentially%2520private%2520GNNs%2520either%2520only%2520preserve%2520link%2520privacy%2520or%2520suffer%2520significant%2520utility%2520loss%2520in%2520the%2520process%2520of%2520preserving%2520link%2520and%2520node%2520feature%2520privacy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520effective%2520LDP%2520framework%252C%2520called%2520HoGS%252C%2520which%2520trains%2520GNNs%2520with%2520link%2520and%2520feature%2520protection%2520by%2520generating%2520a%2520synthetic%2520graph.%2520Concretely%252C%2520HoGS%2520first%2520collects%2520the%2520link%2520and%2520feature%2520information%2520of%2520the%2520graph%2520under%2520LDP%252C%2520and%2520then%2520utilizes%2520the%2520phenomenon%2520of%2520homophily%2520in%2520graph%2520data%2520to%2520reconstruct%2520the%2520graph%2520structure%2520and%2520node%2520features%2520separately%252C%2520thereby%2520effectively%2520mitigating%2520the%2520negative%2520impact%2520of%2520LDP%2520on%2520the%2520downstream%2520GNN%2520training.%2520We%2520theoretically%2520analyze%2520the%2520privacy%2520guarantee%2520of%2520HoGS%2520and%2520conduct%2520experiments%2520using%2520the%2520generated%2520synthetic%2520graph%2520as%2520input%2520to%2520various%2520state-of-the-art%2520GNN%2520architectures.%2520Experimental%2520results%2520on%2520three%2520real-world%2520datasets%2520show%2520that%2520HoGS%2520significantly%2520outperforms%2520baseline%2520methods%2520in%2520the%2520accuracy%2520of%2520training%2520GNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoGS%3A%20Homophily-Oriented%20Graph%20Synthesis%20for%20Local%20Differentially%20Private%20GNN%20Training&entry.906535625=Wen%20Xu%20and%20Zhetao%20Li%20and%20Yong%20Xiao%20and%20Pengpeng%20Qiao%20and%20Mianxiong%20Dong%20and%20Kaoru%20Ota&entry.1292438233=Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20performance%20in%20various%20graph-based%20machine%20learning%20tasks%20by%20effectively%20modeling%20high-order%20interactions%20between%20nodes.%20However%2C%20training%20GNNs%20without%20protection%20may%20leak%20sensitive%20personal%20information%20in%20graph%20data%2C%20including%20links%20and%20node%20features.%20Local%20differential%20privacy%20%28LDP%29%20is%20an%20advanced%20technique%20for%20protecting%20data%20privacy%20in%20decentralized%20networks.%20Unfortunately%2C%20existing%20local%20differentially%20private%20GNNs%20either%20only%20preserve%20link%20privacy%20or%20suffer%20significant%20utility%20loss%20in%20the%20process%20of%20preserving%20link%20and%20node%20feature%20privacy.%20In%20this%20paper%2C%20we%20propose%20an%20effective%20LDP%20framework%2C%20called%20HoGS%2C%20which%20trains%20GNNs%20with%20link%20and%20feature%20protection%20by%20generating%20a%20synthetic%20graph.%20Concretely%2C%20HoGS%20first%20collects%20the%20link%20and%20feature%20information%20of%20the%20graph%20under%20LDP%2C%20and%20then%20utilizes%20the%20phenomenon%20of%20homophily%20in%20graph%20data%20to%20reconstruct%20the%20graph%20structure%20and%20node%20features%20separately%2C%20thereby%20effectively%20mitigating%20the%20negative%20impact%20of%20LDP%20on%20the%20downstream%20GNN%20training.%20We%20theoretically%20analyze%20the%20privacy%20guarantee%20of%20HoGS%20and%20conduct%20experiments%20using%20the%20generated%20synthetic%20graph%20as%20input%20to%20various%20state-of-the-art%20GNN%20architectures.%20Experimental%20results%20on%20three%20real-world%20datasets%20show%20that%20HoGS%20significantly%20outperforms%20baseline%20methods%20in%20the%20accuracy%20of%20training%20GNNs.&entry.1838667208=http%3A//arxiv.org/abs/2602.08762v1&entry.124074799=Read"},
{"title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "author": "SII-OpenMOSS Team and  : and Donghua Yu and Mingshu Chen and Qi Chen and Qi Luo and Qianyi Wu and Qinyuan Cheng and Ruixiao Li and Tianyi Liang and Wenbo Zhang and Wenming Tu and Xiangyu Peng and Yang Gao and Yanru Huo and Ying Zhu and Yinze Luo and Yiyang Zhang and Yuerong Song and Zhe Xu and Zhiyu Zhang and Chenchen Yang and Cheng Chang and Chushu Zhou and Hanfu Chen and Hongnan Ma and Jiaxi Li and Jingqi Tong and Junxi Liu and Ke Chen and Shimin Li and Songlin Wang and Wei Jiang and Zhaoye Fei and Zhiyuan Ning and Chunguo Li and Chenhui Li and Ziwei He and Zengfeng Huang and Xie Chen and Xipeng Qiu", "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "link": "http://arxiv.org/abs/2602.08794v1", "date": "2026-02-09", "relevancy": 2.3165, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5842}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5772}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOVA%3A%20Towards%20Scalable%20and%20Synchronized%20Video-Audio%20Generation&body=Title%3A%20MOVA%3A%20Towards%20Scalable%20and%20Synchronized%20Video-Audio%20Generation%0AAuthor%3A%20SII-OpenMOSS%20Team%20and%20%20%3A%20and%20Donghua%20Yu%20and%20Mingshu%20Chen%20and%20Qi%20Chen%20and%20Qi%20Luo%20and%20Qianyi%20Wu%20and%20Qinyuan%20Cheng%20and%20Ruixiao%20Li%20and%20Tianyi%20Liang%20and%20Wenbo%20Zhang%20and%20Wenming%20Tu%20and%20Xiangyu%20Peng%20and%20Yang%20Gao%20and%20Yanru%20Huo%20and%20Ying%20Zhu%20and%20Yinze%20Luo%20and%20Yiyang%20Zhang%20and%20Yuerong%20Song%20and%20Zhe%20Xu%20and%20Zhiyu%20Zhang%20and%20Chenchen%20Yang%20and%20Cheng%20Chang%20and%20Chushu%20Zhou%20and%20Hanfu%20Chen%20and%20Hongnan%20Ma%20and%20Jiaxi%20Li%20and%20Jingqi%20Tong%20and%20Junxi%20Liu%20and%20Ke%20Chen%20and%20Shimin%20Li%20and%20Songlin%20Wang%20and%20Wei%20Jiang%20and%20Zhaoye%20Fei%20and%20Zhiyuan%20Ning%20and%20Chunguo%20Li%20and%20Chenhui%20Li%20and%20Ziwei%20He%20and%20Zengfeng%20Huang%20and%20Xie%20Chen%20and%20Xipeng%20Qiu%0AAbstract%3A%20Audio%20is%20indispensable%20for%20real-world%20video%2C%20yet%20generation%20models%20have%20largely%20overlooked%20audio%20components.%20Current%20approaches%20to%20producing%20audio-visual%20content%20often%20rely%20on%20cascaded%20pipelines%2C%20which%20increase%20cost%2C%20accumulate%20errors%2C%20and%20degrade%20overall%20quality.%20While%20systems%20such%20as%20Veo%203%20and%20Sora%202%20emphasize%20the%20value%20of%20simultaneous%20generation%2C%20joint%20multimodal%20modeling%20introduces%20unique%20challenges%20in%20architecture%2C%20data%2C%20and%20training.%20Moreover%2C%20the%20closed-source%20nature%20of%20existing%20systems%20limits%20progress%20in%20the%20field.%20In%20this%20work%2C%20we%20introduce%20MOVA%20%28MOSS%20Video%20and%20Audio%29%2C%20an%20open-source%20model%20capable%20of%20generating%20high-quality%2C%20synchronized%20audio-visual%20content%2C%20including%20realistic%20lip-synced%20speech%2C%20environment-aware%20sound%20effects%2C%20and%20content-aligned%20music.%20MOVA%20employs%20a%20Mixture-of-Experts%20%28MoE%29%20architecture%2C%20with%20a%20total%20of%2032B%20parameters%2C%20of%20which%2018B%20are%20active%20during%20inference.%20It%20supports%20IT2VA%20%28Image-Text%20to%20Video-Audio%29%20generation%20task.%20By%20releasing%20the%20model%20weights%20and%20code%2C%20we%20aim%20to%20advance%20research%20and%20foster%20a%20vibrant%20community%20of%20creators.%20The%20released%20codebase%20features%20comprehensive%20support%20for%20efficient%20inference%2C%20LoRA%20fine-tuning%2C%20and%20prompt%20enhancement.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOVA%253A%2520Towards%2520Scalable%2520and%2520Synchronized%2520Video-Audio%2520Generation%26entry.906535625%3DSII-OpenMOSS%2520Team%2520and%2520%2520%253A%2520and%2520Donghua%2520Yu%2520and%2520Mingshu%2520Chen%2520and%2520Qi%2520Chen%2520and%2520Qi%2520Luo%2520and%2520Qianyi%2520Wu%2520and%2520Qinyuan%2520Cheng%2520and%2520Ruixiao%2520Li%2520and%2520Tianyi%2520Liang%2520and%2520Wenbo%2520Zhang%2520and%2520Wenming%2520Tu%2520and%2520Xiangyu%2520Peng%2520and%2520Yang%2520Gao%2520and%2520Yanru%2520Huo%2520and%2520Ying%2520Zhu%2520and%2520Yinze%2520Luo%2520and%2520Yiyang%2520Zhang%2520and%2520Yuerong%2520Song%2520and%2520Zhe%2520Xu%2520and%2520Zhiyu%2520Zhang%2520and%2520Chenchen%2520Yang%2520and%2520Cheng%2520Chang%2520and%2520Chushu%2520Zhou%2520and%2520Hanfu%2520Chen%2520and%2520Hongnan%2520Ma%2520and%2520Jiaxi%2520Li%2520and%2520Jingqi%2520Tong%2520and%2520Junxi%2520Liu%2520and%2520Ke%2520Chen%2520and%2520Shimin%2520Li%2520and%2520Songlin%2520Wang%2520and%2520Wei%2520Jiang%2520and%2520Zhaoye%2520Fei%2520and%2520Zhiyuan%2520Ning%2520and%2520Chunguo%2520Li%2520and%2520Chenhui%2520Li%2520and%2520Ziwei%2520He%2520and%2520Zengfeng%2520Huang%2520and%2520Xie%2520Chen%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DAudio%2520is%2520indispensable%2520for%2520real-world%2520video%252C%2520yet%2520generation%2520models%2520have%2520largely%2520overlooked%2520audio%2520components.%2520Current%2520approaches%2520to%2520producing%2520audio-visual%2520content%2520often%2520rely%2520on%2520cascaded%2520pipelines%252C%2520which%2520increase%2520cost%252C%2520accumulate%2520errors%252C%2520and%2520degrade%2520overall%2520quality.%2520While%2520systems%2520such%2520as%2520Veo%25203%2520and%2520Sora%25202%2520emphasize%2520the%2520value%2520of%2520simultaneous%2520generation%252C%2520joint%2520multimodal%2520modeling%2520introduces%2520unique%2520challenges%2520in%2520architecture%252C%2520data%252C%2520and%2520training.%2520Moreover%252C%2520the%2520closed-source%2520nature%2520of%2520existing%2520systems%2520limits%2520progress%2520in%2520the%2520field.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MOVA%2520%2528MOSS%2520Video%2520and%2520Audio%2529%252C%2520an%2520open-source%2520model%2520capable%2520of%2520generating%2520high-quality%252C%2520synchronized%2520audio-visual%2520content%252C%2520including%2520realistic%2520lip-synced%2520speech%252C%2520environment-aware%2520sound%2520effects%252C%2520and%2520content-aligned%2520music.%2520MOVA%2520employs%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%252C%2520with%2520a%2520total%2520of%252032B%2520parameters%252C%2520of%2520which%252018B%2520are%2520active%2520during%2520inference.%2520It%2520supports%2520IT2VA%2520%2528Image-Text%2520to%2520Video-Audio%2529%2520generation%2520task.%2520By%2520releasing%2520the%2520model%2520weights%2520and%2520code%252C%2520we%2520aim%2520to%2520advance%2520research%2520and%2520foster%2520a%2520vibrant%2520community%2520of%2520creators.%2520The%2520released%2520codebase%2520features%2520comprehensive%2520support%2520for%2520efficient%2520inference%252C%2520LoRA%2520fine-tuning%252C%2520and%2520prompt%2520enhancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOVA%3A%20Towards%20Scalable%20and%20Synchronized%20Video-Audio%20Generation&entry.906535625=SII-OpenMOSS%20Team%20and%20%20%3A%20and%20Donghua%20Yu%20and%20Mingshu%20Chen%20and%20Qi%20Chen%20and%20Qi%20Luo%20and%20Qianyi%20Wu%20and%20Qinyuan%20Cheng%20and%20Ruixiao%20Li%20and%20Tianyi%20Liang%20and%20Wenbo%20Zhang%20and%20Wenming%20Tu%20and%20Xiangyu%20Peng%20and%20Yang%20Gao%20and%20Yanru%20Huo%20and%20Ying%20Zhu%20and%20Yinze%20Luo%20and%20Yiyang%20Zhang%20and%20Yuerong%20Song%20and%20Zhe%20Xu%20and%20Zhiyu%20Zhang%20and%20Chenchen%20Yang%20and%20Cheng%20Chang%20and%20Chushu%20Zhou%20and%20Hanfu%20Chen%20and%20Hongnan%20Ma%20and%20Jiaxi%20Li%20and%20Jingqi%20Tong%20and%20Junxi%20Liu%20and%20Ke%20Chen%20and%20Shimin%20Li%20and%20Songlin%20Wang%20and%20Wei%20Jiang%20and%20Zhaoye%20Fei%20and%20Zhiyuan%20Ning%20and%20Chunguo%20Li%20and%20Chenhui%20Li%20and%20Ziwei%20He%20and%20Zengfeng%20Huang%20and%20Xie%20Chen%20and%20Xipeng%20Qiu&entry.1292438233=Audio%20is%20indispensable%20for%20real-world%20video%2C%20yet%20generation%20models%20have%20largely%20overlooked%20audio%20components.%20Current%20approaches%20to%20producing%20audio-visual%20content%20often%20rely%20on%20cascaded%20pipelines%2C%20which%20increase%20cost%2C%20accumulate%20errors%2C%20and%20degrade%20overall%20quality.%20While%20systems%20such%20as%20Veo%203%20and%20Sora%202%20emphasize%20the%20value%20of%20simultaneous%20generation%2C%20joint%20multimodal%20modeling%20introduces%20unique%20challenges%20in%20architecture%2C%20data%2C%20and%20training.%20Moreover%2C%20the%20closed-source%20nature%20of%20existing%20systems%20limits%20progress%20in%20the%20field.%20In%20this%20work%2C%20we%20introduce%20MOVA%20%28MOSS%20Video%20and%20Audio%29%2C%20an%20open-source%20model%20capable%20of%20generating%20high-quality%2C%20synchronized%20audio-visual%20content%2C%20including%20realistic%20lip-synced%20speech%2C%20environment-aware%20sound%20effects%2C%20and%20content-aligned%20music.%20MOVA%20employs%20a%20Mixture-of-Experts%20%28MoE%29%20architecture%2C%20with%20a%20total%20of%2032B%20parameters%2C%20of%20which%2018B%20are%20active%20during%20inference.%20It%20supports%20IT2VA%20%28Image-Text%20to%20Video-Audio%29%20generation%20task.%20By%20releasing%20the%20model%20weights%20and%20code%2C%20we%20aim%20to%20advance%20research%20and%20foster%20a%20vibrant%20community%20of%20creators.%20The%20released%20codebase%20features%20comprehensive%20support%20for%20efficient%20inference%2C%20LoRA%20fine-tuning%2C%20and%20prompt%20enhancement.&entry.1838667208=http%3A//arxiv.org/abs/2602.08794v1&entry.124074799=Read"},
{"title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries", "author": "Haocheng Lu and Nan Zhang and Wei Tao and Xiaoyang Qu and Guokuan Li and Jiguang Wan and Jianzong Wang", "abstract": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.", "link": "http://arxiv.org/abs/2602.08448v1", "date": "2026-02-09", "relevancy": 2.3159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vista%3A%20Scene-Aware%20Optimization%20for%20Streaming%20Video%20Question%20Answering%20under%20Post-Hoc%20Queries&body=Title%3A%20Vista%3A%20Scene-Aware%20Optimization%20for%20Streaming%20Video%20Question%20Answering%20under%20Post-Hoc%20Queries%0AAuthor%3A%20Haocheng%20Lu%20and%20Nan%20Zhang%20and%20Wei%20Tao%20and%20Xiaoyang%20Qu%20and%20Guokuan%20Li%20and%20Jiguang%20Wan%20and%20Jianzong%20Wang%0AAbstract%3A%20Streaming%20video%20question%20answering%20%28Streaming%20Video%20QA%29%20poses%20distinct%20challenges%20for%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20as%20video%20frames%20arrive%20sequentially%20and%20user%20queries%20can%20be%20issued%20at%20arbitrary%20time%20points.%20Existing%20solutions%20relying%20on%20fixed-size%20memory%20or%20naive%20compression%20often%20suffer%20from%20context%20loss%20or%20memory%20overflow%2C%20limiting%20their%20effectiveness%20in%20long-form%2C%20real-time%20scenarios.%20We%20present%20Vista%2C%20a%20novel%20framework%20for%20scene-aware%20streaming%20video%20QA%20that%20enables%20efficient%20and%20scalable%20reasoning%20over%20continuous%20video%20streams.%20The%20innovation%20of%20Vista%20can%20be%20summarized%20in%20three%20aspects%3A%20%281%29%20scene-aware%20segmentation%2C%20where%20Vista%20dynamically%20clusters%20incoming%20frames%20into%20temporally%20and%20visually%20coherent%20scene%20units%3B%20%282%29%20scene-aware%20compression%2C%20where%20each%20scene%20is%20compressed%20into%20a%20compact%20token%20representation%20and%20stored%20in%20GPU%20memory%20for%20efficient%20index-based%20retrieval%2C%20while%20full-resolution%20frames%20are%20offloaded%20to%20CPU%20memory%3B%20and%20%283%29%20scene-aware%20recall%2C%20where%20relevant%20scenes%20are%20selectively%20recalled%20and%20reintegrated%20into%20the%20model%20input%20upon%20receiving%20a%20query%2C%20enabling%20both%20efficiency%20and%20completeness.%20Vista%20is%20model-agnostic%20and%20integrates%20seamlessly%20with%20a%20variety%20of%20vision-language%20backbones%2C%20enabling%20long-context%20reasoning%20without%20compromising%20latency%20or%20memory%20efficiency.%20Extensive%20experiments%20on%20StreamingBench%20demonstrate%20that%20Vista%20achieves%20state-of-the-art%20performance%2C%20establishing%20a%20strong%20baseline%20for%20real-world%20streaming%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVista%253A%2520Scene-Aware%2520Optimization%2520for%2520Streaming%2520Video%2520Question%2520Answering%2520under%2520Post-Hoc%2520Queries%26entry.906535625%3DHaocheng%2520Lu%2520and%2520Nan%2520Zhang%2520and%2520Wei%2520Tao%2520and%2520Xiaoyang%2520Qu%2520and%2520Guokuan%2520Li%2520and%2520Jiguang%2520Wan%2520and%2520Jianzong%2520Wang%26entry.1292438233%3DStreaming%2520video%2520question%2520answering%2520%2528Streaming%2520Video%2520QA%2529%2520poses%2520distinct%2520challenges%2520for%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520as%2520video%2520frames%2520arrive%2520sequentially%2520and%2520user%2520queries%2520can%2520be%2520issued%2520at%2520arbitrary%2520time%2520points.%2520Existing%2520solutions%2520relying%2520on%2520fixed-size%2520memory%2520or%2520naive%2520compression%2520often%2520suffer%2520from%2520context%2520loss%2520or%2520memory%2520overflow%252C%2520limiting%2520their%2520effectiveness%2520in%2520long-form%252C%2520real-time%2520scenarios.%2520We%2520present%2520Vista%252C%2520a%2520novel%2520framework%2520for%2520scene-aware%2520streaming%2520video%2520QA%2520that%2520enables%2520efficient%2520and%2520scalable%2520reasoning%2520over%2520continuous%2520video%2520streams.%2520The%2520innovation%2520of%2520Vista%2520can%2520be%2520summarized%2520in%2520three%2520aspects%253A%2520%25281%2529%2520scene-aware%2520segmentation%252C%2520where%2520Vista%2520dynamically%2520clusters%2520incoming%2520frames%2520into%2520temporally%2520and%2520visually%2520coherent%2520scene%2520units%253B%2520%25282%2529%2520scene-aware%2520compression%252C%2520where%2520each%2520scene%2520is%2520compressed%2520into%2520a%2520compact%2520token%2520representation%2520and%2520stored%2520in%2520GPU%2520memory%2520for%2520efficient%2520index-based%2520retrieval%252C%2520while%2520full-resolution%2520frames%2520are%2520offloaded%2520to%2520CPU%2520memory%253B%2520and%2520%25283%2529%2520scene-aware%2520recall%252C%2520where%2520relevant%2520scenes%2520are%2520selectively%2520recalled%2520and%2520reintegrated%2520into%2520the%2520model%2520input%2520upon%2520receiving%2520a%2520query%252C%2520enabling%2520both%2520efficiency%2520and%2520completeness.%2520Vista%2520is%2520model-agnostic%2520and%2520integrates%2520seamlessly%2520with%2520a%2520variety%2520of%2520vision-language%2520backbones%252C%2520enabling%2520long-context%2520reasoning%2520without%2520compromising%2520latency%2520or%2520memory%2520efficiency.%2520Extensive%2520experiments%2520on%2520StreamingBench%2520demonstrate%2520that%2520Vista%2520achieves%2520state-of-the-art%2520performance%252C%2520establishing%2520a%2520strong%2520baseline%2520for%2520real-world%2520streaming%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vista%3A%20Scene-Aware%20Optimization%20for%20Streaming%20Video%20Question%20Answering%20under%20Post-Hoc%20Queries&entry.906535625=Haocheng%20Lu%20and%20Nan%20Zhang%20and%20Wei%20Tao%20and%20Xiaoyang%20Qu%20and%20Guokuan%20Li%20and%20Jiguang%20Wan%20and%20Jianzong%20Wang&entry.1292438233=Streaming%20video%20question%20answering%20%28Streaming%20Video%20QA%29%20poses%20distinct%20challenges%20for%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20as%20video%20frames%20arrive%20sequentially%20and%20user%20queries%20can%20be%20issued%20at%20arbitrary%20time%20points.%20Existing%20solutions%20relying%20on%20fixed-size%20memory%20or%20naive%20compression%20often%20suffer%20from%20context%20loss%20or%20memory%20overflow%2C%20limiting%20their%20effectiveness%20in%20long-form%2C%20real-time%20scenarios.%20We%20present%20Vista%2C%20a%20novel%20framework%20for%20scene-aware%20streaming%20video%20QA%20that%20enables%20efficient%20and%20scalable%20reasoning%20over%20continuous%20video%20streams.%20The%20innovation%20of%20Vista%20can%20be%20summarized%20in%20three%20aspects%3A%20%281%29%20scene-aware%20segmentation%2C%20where%20Vista%20dynamically%20clusters%20incoming%20frames%20into%20temporally%20and%20visually%20coherent%20scene%20units%3B%20%282%29%20scene-aware%20compression%2C%20where%20each%20scene%20is%20compressed%20into%20a%20compact%20token%20representation%20and%20stored%20in%20GPU%20memory%20for%20efficient%20index-based%20retrieval%2C%20while%20full-resolution%20frames%20are%20offloaded%20to%20CPU%20memory%3B%20and%20%283%29%20scene-aware%20recall%2C%20where%20relevant%20scenes%20are%20selectively%20recalled%20and%20reintegrated%20into%20the%20model%20input%20upon%20receiving%20a%20query%2C%20enabling%20both%20efficiency%20and%20completeness.%20Vista%20is%20model-agnostic%20and%20integrates%20seamlessly%20with%20a%20variety%20of%20vision-language%20backbones%2C%20enabling%20long-context%20reasoning%20without%20compromising%20latency%20or%20memory%20efficiency.%20Extensive%20experiments%20on%20StreamingBench%20demonstrate%20that%20Vista%20achieves%20state-of-the-art%20performance%2C%20establishing%20a%20strong%20baseline%20for%20real-world%20streaming%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2602.08448v1&entry.124074799=Read"},
{"title": "Interpretable Discovery of One-parameter Subgroups: A Modular Framework for Elliptical, Hyperbolic, and Parabolic Symmetries", "author": "Pavan Karjol and Vivek V Kashyap and Rohan Kashyap and Prathosh A P", "abstract": "We propose a modular, data-driven framework for jointly learning unknown functional mappings and discovering the underlying one-parameter symmetry subgroup governing the data. Unlike conventional geometric deep learning methods that assume known symmetries, our approach identifies the relevant continuous subgroup directly from data. We consider the broad class of one-parameter subgroups, which admit a canonical geometric classification into three regimes: elliptical, hyperbolic, and parabolic.\n  Given an assumed regime, our framework instantiates a corresponding symmetry discovery architecture with invariant and equivariant representation layers structured according to the Lie algebra of the subgroup, and learns the exact generator parameters end-to-end from data. This yields models whose invariance or equivariance is guaranteed by construction and admits formal proofs, enabling symmetry to be explicitly traced to identifiable components of the architecture. The approach is applicable to one-parameter subgroups of a wide range of matrix Lie groups, including $SO(n)$, $SL(n)$, and the Lorentz group. Experiments on synthetic and real-world systems, including moment of inertia prediction, double-pendulum dynamics, and high-energy \\textit{Top Quark Tagging}, demonstrate accurate subgroup recovery and strong predictive performance across both compact and non-compact regimes.", "link": "http://arxiv.org/abs/2509.22219v4", "date": "2026-02-09", "relevancy": 2.2969, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4634}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4611}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Discovery%20of%20One-parameter%20Subgroups%3A%20A%20Modular%20Framework%20for%20Elliptical%2C%20Hyperbolic%2C%20and%20Parabolic%20Symmetries&body=Title%3A%20Interpretable%20Discovery%20of%20One-parameter%20Subgroups%3A%20A%20Modular%20Framework%20for%20Elliptical%2C%20Hyperbolic%2C%20and%20Parabolic%20Symmetries%0AAuthor%3A%20Pavan%20Karjol%20and%20Vivek%20V%20Kashyap%20and%20Rohan%20Kashyap%20and%20Prathosh%20A%20P%0AAbstract%3A%20We%20propose%20a%20modular%2C%20data-driven%20framework%20for%20jointly%20learning%20unknown%20functional%20mappings%20and%20discovering%20the%20underlying%20one-parameter%20symmetry%20subgroup%20governing%20the%20data.%20Unlike%20conventional%20geometric%20deep%20learning%20methods%20that%20assume%20known%20symmetries%2C%20our%20approach%20identifies%20the%20relevant%20continuous%20subgroup%20directly%20from%20data.%20We%20consider%20the%20broad%20class%20of%20one-parameter%20subgroups%2C%20which%20admit%20a%20canonical%20geometric%20classification%20into%20three%20regimes%3A%20elliptical%2C%20hyperbolic%2C%20and%20parabolic.%0A%20%20Given%20an%20assumed%20regime%2C%20our%20framework%20instantiates%20a%20corresponding%20symmetry%20discovery%20architecture%20with%20invariant%20and%20equivariant%20representation%20layers%20structured%20according%20to%20the%20Lie%20algebra%20of%20the%20subgroup%2C%20and%20learns%20the%20exact%20generator%20parameters%20end-to-end%20from%20data.%20This%20yields%20models%20whose%20invariance%20or%20equivariance%20is%20guaranteed%20by%20construction%20and%20admits%20formal%20proofs%2C%20enabling%20symmetry%20to%20be%20explicitly%20traced%20to%20identifiable%20components%20of%20the%20architecture.%20The%20approach%20is%20applicable%20to%20one-parameter%20subgroups%20of%20a%20wide%20range%20of%20matrix%20Lie%20groups%2C%20including%20%24SO%28n%29%24%2C%20%24SL%28n%29%24%2C%20and%20the%20Lorentz%20group.%20Experiments%20on%20synthetic%20and%20real-world%20systems%2C%20including%20moment%20of%20inertia%20prediction%2C%20double-pendulum%20dynamics%2C%20and%20high-energy%20%5Ctextit%7BTop%20Quark%20Tagging%7D%2C%20demonstrate%20accurate%20subgroup%20recovery%20and%20strong%20predictive%20performance%20across%20both%20compact%20and%20non-compact%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22219v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Discovery%2520of%2520One-parameter%2520Subgroups%253A%2520A%2520Modular%2520Framework%2520for%2520Elliptical%252C%2520Hyperbolic%252C%2520and%2520Parabolic%2520Symmetries%26entry.906535625%3DPavan%2520Karjol%2520and%2520Vivek%2520V%2520Kashyap%2520and%2520Rohan%2520Kashyap%2520and%2520Prathosh%2520A%2520P%26entry.1292438233%3DWe%2520propose%2520a%2520modular%252C%2520data-driven%2520framework%2520for%2520jointly%2520learning%2520unknown%2520functional%2520mappings%2520and%2520discovering%2520the%2520underlying%2520one-parameter%2520symmetry%2520subgroup%2520governing%2520the%2520data.%2520Unlike%2520conventional%2520geometric%2520deep%2520learning%2520methods%2520that%2520assume%2520known%2520symmetries%252C%2520our%2520approach%2520identifies%2520the%2520relevant%2520continuous%2520subgroup%2520directly%2520from%2520data.%2520We%2520consider%2520the%2520broad%2520class%2520of%2520one-parameter%2520subgroups%252C%2520which%2520admit%2520a%2520canonical%2520geometric%2520classification%2520into%2520three%2520regimes%253A%2520elliptical%252C%2520hyperbolic%252C%2520and%2520parabolic.%250A%2520%2520Given%2520an%2520assumed%2520regime%252C%2520our%2520framework%2520instantiates%2520a%2520corresponding%2520symmetry%2520discovery%2520architecture%2520with%2520invariant%2520and%2520equivariant%2520representation%2520layers%2520structured%2520according%2520to%2520the%2520Lie%2520algebra%2520of%2520the%2520subgroup%252C%2520and%2520learns%2520the%2520exact%2520generator%2520parameters%2520end-to-end%2520from%2520data.%2520This%2520yields%2520models%2520whose%2520invariance%2520or%2520equivariance%2520is%2520guaranteed%2520by%2520construction%2520and%2520admits%2520formal%2520proofs%252C%2520enabling%2520symmetry%2520to%2520be%2520explicitly%2520traced%2520to%2520identifiable%2520components%2520of%2520the%2520architecture.%2520The%2520approach%2520is%2520applicable%2520to%2520one-parameter%2520subgroups%2520of%2520a%2520wide%2520range%2520of%2520matrix%2520Lie%2520groups%252C%2520including%2520%2524SO%2528n%2529%2524%252C%2520%2524SL%2528n%2529%2524%252C%2520and%2520the%2520Lorentz%2520group.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520systems%252C%2520including%2520moment%2520of%2520inertia%2520prediction%252C%2520double-pendulum%2520dynamics%252C%2520and%2520high-energy%2520%255Ctextit%257BTop%2520Quark%2520Tagging%257D%252C%2520demonstrate%2520accurate%2520subgroup%2520recovery%2520and%2520strong%2520predictive%2520performance%2520across%2520both%2520compact%2520and%2520non-compact%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22219v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Discovery%20of%20One-parameter%20Subgroups%3A%20A%20Modular%20Framework%20for%20Elliptical%2C%20Hyperbolic%2C%20and%20Parabolic%20Symmetries&entry.906535625=Pavan%20Karjol%20and%20Vivek%20V%20Kashyap%20and%20Rohan%20Kashyap%20and%20Prathosh%20A%20P&entry.1292438233=We%20propose%20a%20modular%2C%20data-driven%20framework%20for%20jointly%20learning%20unknown%20functional%20mappings%20and%20discovering%20the%20underlying%20one-parameter%20symmetry%20subgroup%20governing%20the%20data.%20Unlike%20conventional%20geometric%20deep%20learning%20methods%20that%20assume%20known%20symmetries%2C%20our%20approach%20identifies%20the%20relevant%20continuous%20subgroup%20directly%20from%20data.%20We%20consider%20the%20broad%20class%20of%20one-parameter%20subgroups%2C%20which%20admit%20a%20canonical%20geometric%20classification%20into%20three%20regimes%3A%20elliptical%2C%20hyperbolic%2C%20and%20parabolic.%0A%20%20Given%20an%20assumed%20regime%2C%20our%20framework%20instantiates%20a%20corresponding%20symmetry%20discovery%20architecture%20with%20invariant%20and%20equivariant%20representation%20layers%20structured%20according%20to%20the%20Lie%20algebra%20of%20the%20subgroup%2C%20and%20learns%20the%20exact%20generator%20parameters%20end-to-end%20from%20data.%20This%20yields%20models%20whose%20invariance%20or%20equivariance%20is%20guaranteed%20by%20construction%20and%20admits%20formal%20proofs%2C%20enabling%20symmetry%20to%20be%20explicitly%20traced%20to%20identifiable%20components%20of%20the%20architecture.%20The%20approach%20is%20applicable%20to%20one-parameter%20subgroups%20of%20a%20wide%20range%20of%20matrix%20Lie%20groups%2C%20including%20%24SO%28n%29%24%2C%20%24SL%28n%29%24%2C%20and%20the%20Lorentz%20group.%20Experiments%20on%20synthetic%20and%20real-world%20systems%2C%20including%20moment%20of%20inertia%20prediction%2C%20double-pendulum%20dynamics%2C%20and%20high-energy%20%5Ctextit%7BTop%20Quark%20Tagging%7D%2C%20demonstrate%20accurate%20subgroup%20recovery%20and%20strong%20predictive%20performance%20across%20both%20compact%20and%20non-compact%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2509.22219v4&entry.124074799=Read"},
{"title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale", "author": "Yifan Wu and Jiyue Jiang and Xichen Ye and Yiqi Wang and Chang Zhou and Yitao Xu and Jiayang Chen and He Hu and Weizhong Zhang and Cheng Jin and Jiao Yuan and Yu Li", "abstract": "Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.", "link": "http://arxiv.org/abs/2512.12932v2", "date": "2026-02-09", "relevancy": 2.2766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Data%20Pruning%20for%20Pretraining%20Biological%20Foundation%20Models%20at%20Scale&body=Title%3A%20Investigating%20Data%20Pruning%20for%20Pretraining%20Biological%20Foundation%20Models%20at%20Scale%0AAuthor%3A%20Yifan%20Wu%20and%20Jiyue%20Jiang%20and%20Xichen%20Ye%20and%20Yiqi%20Wang%20and%20Chang%20Zhou%20and%20Yitao%20Xu%20and%20Jiayang%20Chen%20and%20He%20Hu%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%20and%20Jiao%20Yuan%20and%20Yu%20Li%0AAbstract%3A%20Biological%20foundation%20models%20%28BioFMs%29%2C%20pretrained%20on%20large-scale%20biological%20sequences%2C%20have%20recently%20shown%20strong%20potential%20in%20providing%20meaningful%20representations%20for%20diverse%20downstream%20bioinformatics%20tasks.%20However%2C%20such%20models%20often%20rely%20on%20millions%20to%20billions%20of%20training%20sequences%20and%20billions%20of%20parameters%2C%20resulting%20in%20prohibitive%20computational%20costs%20and%20significant%20barriers%20to%20reproducibility%20and%20accessibility%2C%20particularly%20for%20academic%20labs.%20To%20address%20these%20challenges%2C%20we%20investigate%20the%20feasibility%20of%20data%20pruning%20for%20BioFM%20pretraining%20and%20propose%20a%20post-hoc%20influence-guided%20data%20pruning%20framework%20tailored%20to%20biological%20domains.%20Our%20approach%20introduces%20a%20subset-based%20self-influence%20formulation%20that%20enables%20efficient%20estimation%20of%20sample%20importance%20at%20low%20computational%20cost%2C%20and%20builds%20upon%20it%20two%20simple%20yet%20effective%20selection%20strategies%2C%20namely%20Top-k%20Influence%20%28Top%20I%29%20and%20Coverage-Centric%20Influence%20%28CCI%29.%20We%20empirically%20validate%20our%20method%20on%20two%20representative%20BioFMs%2C%20RNA-FM%20and%20ESM-C.%20For%20RNA%2C%20our%20framework%20consistently%20outperforms%20random%20selection%20baselines%20under%20an%20extreme%20pruning%20rate%20of%20over%2099%20percent%2C%20demonstrating%20its%20effectiveness.%20Furthermore%2C%20we%20show%20the%20generalizability%20of%20our%20framework%20on%20protein-related%20tasks%20using%20ESM-C.%20In%20particular%2C%20our%20coreset%20even%20outperforms%20random%20subsets%20that%20are%20ten%20times%20larger%20in%20both%20RNA%20and%20protein%20settings%2C%20revealing%20substantial%20redundancy%20in%20biological%20sequence%20datasets.%20These%20findings%20underscore%20the%20potential%20of%20influence-guided%20data%20pruning%20to%20substantially%20reduce%20the%20computational%20cost%20of%20BioFM%20pretraining%2C%20paving%20the%20way%20for%20more%20efficient%2C%20accessible%2C%20and%20sustainable%20biological%20AI%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12932v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Data%2520Pruning%2520for%2520Pretraining%2520Biological%2520Foundation%2520Models%2520at%2520Scale%26entry.906535625%3DYifan%2520Wu%2520and%2520Jiyue%2520Jiang%2520and%2520Xichen%2520Ye%2520and%2520Yiqi%2520Wang%2520and%2520Chang%2520Zhou%2520and%2520Yitao%2520Xu%2520and%2520Jiayang%2520Chen%2520and%2520He%2520Hu%2520and%2520Weizhong%2520Zhang%2520and%2520Cheng%2520Jin%2520and%2520Jiao%2520Yuan%2520and%2520Yu%2520Li%26entry.1292438233%3DBiological%2520foundation%2520models%2520%2528BioFMs%2529%252C%2520pretrained%2520on%2520large-scale%2520biological%2520sequences%252C%2520have%2520recently%2520shown%2520strong%2520potential%2520in%2520providing%2520meaningful%2520representations%2520for%2520diverse%2520downstream%2520bioinformatics%2520tasks.%2520However%252C%2520such%2520models%2520often%2520rely%2520on%2520millions%2520to%2520billions%2520of%2520training%2520sequences%2520and%2520billions%2520of%2520parameters%252C%2520resulting%2520in%2520prohibitive%2520computational%2520costs%2520and%2520significant%2520barriers%2520to%2520reproducibility%2520and%2520accessibility%252C%2520particularly%2520for%2520academic%2520labs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520investigate%2520the%2520feasibility%2520of%2520data%2520pruning%2520for%2520BioFM%2520pretraining%2520and%2520propose%2520a%2520post-hoc%2520influence-guided%2520data%2520pruning%2520framework%2520tailored%2520to%2520biological%2520domains.%2520Our%2520approach%2520introduces%2520a%2520subset-based%2520self-influence%2520formulation%2520that%2520enables%2520efficient%2520estimation%2520of%2520sample%2520importance%2520at%2520low%2520computational%2520cost%252C%2520and%2520builds%2520upon%2520it%2520two%2520simple%2520yet%2520effective%2520selection%2520strategies%252C%2520namely%2520Top-k%2520Influence%2520%2528Top%2520I%2529%2520and%2520Coverage-Centric%2520Influence%2520%2528CCI%2529.%2520We%2520empirically%2520validate%2520our%2520method%2520on%2520two%2520representative%2520BioFMs%252C%2520RNA-FM%2520and%2520ESM-C.%2520For%2520RNA%252C%2520our%2520framework%2520consistently%2520outperforms%2520random%2520selection%2520baselines%2520under%2520an%2520extreme%2520pruning%2520rate%2520of%2520over%252099%2520percent%252C%2520demonstrating%2520its%2520effectiveness.%2520Furthermore%252C%2520we%2520show%2520the%2520generalizability%2520of%2520our%2520framework%2520on%2520protein-related%2520tasks%2520using%2520ESM-C.%2520In%2520particular%252C%2520our%2520coreset%2520even%2520outperforms%2520random%2520subsets%2520that%2520are%2520ten%2520times%2520larger%2520in%2520both%2520RNA%2520and%2520protein%2520settings%252C%2520revealing%2520substantial%2520redundancy%2520in%2520biological%2520sequence%2520datasets.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%2520influence-guided%2520data%2520pruning%2520to%2520substantially%2520reduce%2520the%2520computational%2520cost%2520of%2520BioFM%2520pretraining%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%252C%2520accessible%252C%2520and%2520sustainable%2520biological%2520AI%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12932v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Data%20Pruning%20for%20Pretraining%20Biological%20Foundation%20Models%20at%20Scale&entry.906535625=Yifan%20Wu%20and%20Jiyue%20Jiang%20and%20Xichen%20Ye%20and%20Yiqi%20Wang%20and%20Chang%20Zhou%20and%20Yitao%20Xu%20and%20Jiayang%20Chen%20and%20He%20Hu%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%20and%20Jiao%20Yuan%20and%20Yu%20Li&entry.1292438233=Biological%20foundation%20models%20%28BioFMs%29%2C%20pretrained%20on%20large-scale%20biological%20sequences%2C%20have%20recently%20shown%20strong%20potential%20in%20providing%20meaningful%20representations%20for%20diverse%20downstream%20bioinformatics%20tasks.%20However%2C%20such%20models%20often%20rely%20on%20millions%20to%20billions%20of%20training%20sequences%20and%20billions%20of%20parameters%2C%20resulting%20in%20prohibitive%20computational%20costs%20and%20significant%20barriers%20to%20reproducibility%20and%20accessibility%2C%20particularly%20for%20academic%20labs.%20To%20address%20these%20challenges%2C%20we%20investigate%20the%20feasibility%20of%20data%20pruning%20for%20BioFM%20pretraining%20and%20propose%20a%20post-hoc%20influence-guided%20data%20pruning%20framework%20tailored%20to%20biological%20domains.%20Our%20approach%20introduces%20a%20subset-based%20self-influence%20formulation%20that%20enables%20efficient%20estimation%20of%20sample%20importance%20at%20low%20computational%20cost%2C%20and%20builds%20upon%20it%20two%20simple%20yet%20effective%20selection%20strategies%2C%20namely%20Top-k%20Influence%20%28Top%20I%29%20and%20Coverage-Centric%20Influence%20%28CCI%29.%20We%20empirically%20validate%20our%20method%20on%20two%20representative%20BioFMs%2C%20RNA-FM%20and%20ESM-C.%20For%20RNA%2C%20our%20framework%20consistently%20outperforms%20random%20selection%20baselines%20under%20an%20extreme%20pruning%20rate%20of%20over%2099%20percent%2C%20demonstrating%20its%20effectiveness.%20Furthermore%2C%20we%20show%20the%20generalizability%20of%20our%20framework%20on%20protein-related%20tasks%20using%20ESM-C.%20In%20particular%2C%20our%20coreset%20even%20outperforms%20random%20subsets%20that%20are%20ten%20times%20larger%20in%20both%20RNA%20and%20protein%20settings%2C%20revealing%20substantial%20redundancy%20in%20biological%20sequence%20datasets.%20These%20findings%20underscore%20the%20potential%20of%20influence-guided%20data%20pruning%20to%20substantially%20reduce%20the%20computational%20cost%20of%20BioFM%20pretraining%2C%20paving%20the%20way%20for%20more%20efficient%2C%20accessible%2C%20and%20sustainable%20biological%20AI%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.12932v2&entry.124074799=Read"},
{"title": "MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation", "author": "Zhenguo Sun and Bo-Sheng Huang and Yibo Peng and Xukun Li and Jingyu Ma and Yu Sun and Zhe Li and Haojun Jiang and Biao Gao and Zhenshan Bing and Xinlong Wang and Alois Knoll", "abstract": "Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.", "link": "http://arxiv.org/abs/2602.08594v1", "date": "2026-02-09", "relevancy": 2.274, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5763}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5639}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSAIC%3A%20Bridging%20the%20Sim-to-Real%20Gap%20in%20Generalist%20Humanoid%20Motion%20Tracking%20and%20Teleoperation%20with%20Rapid%20Residual%20Adaptation&body=Title%3A%20MOSAIC%3A%20Bridging%20the%20Sim-to-Real%20Gap%20in%20Generalist%20Humanoid%20Motion%20Tracking%20and%20Teleoperation%20with%20Rapid%20Residual%20Adaptation%0AAuthor%3A%20Zhenguo%20Sun%20and%20Bo-Sheng%20Huang%20and%20Yibo%20Peng%20and%20Xukun%20Li%20and%20Jingyu%20Ma%20and%20Yu%20Sun%20and%20Zhe%20Li%20and%20Haojun%20Jiang%20and%20Biao%20Gao%20and%20Zhenshan%20Bing%20and%20Xinlong%20Wang%20and%20Alois%20Knoll%0AAbstract%3A%20Generalist%20humanoid%20motion%20trackers%20have%20recently%20achieved%20strong%20simulation%20metrics%20by%20scaling%20data%20and%20training%2C%20yet%20often%20remain%20brittle%20on%20hardware%20during%20sustained%20teleoperation%20due%20to%20interface-%20and%20dynamics-induced%20errors.%20We%20present%20MOSAIC%2C%20an%20open-source%2C%20full-stack%20system%20for%20humanoid%20motion%20tracking%20and%20whole-body%20teleoperation%20across%20multiple%20interfaces.%20MOSAIC%20first%20learns%20a%20teleoperation-oriented%20general%20motion%20tracker%20via%20RL%20on%20a%20multi-source%20motion%20bank%20with%20adaptive%20resampling%20and%20rewards%20that%20emphasize%20world-frame%20motion%20consistency%2C%20which%20is%20critical%20for%20mobile%20teleoperation.%20To%20bridge%20the%20sim-to-real%20interface%20gap%20without%20sacrificing%20generality%2C%20MOSAIC%20then%20performs%20rapid%20residual%20adaptation%3A%20an%20interface-specific%20policy%20is%20trained%20using%20minimal%20interface-specific%20data%2C%20and%20then%20distilled%20into%20the%20general%20tracker%20through%20an%20additive%20residual%20module%2C%20outperforming%20naive%20fine-tuning%20or%20continual%20learning.%20We%20validate%20MOSAIC%20with%20systematic%20ablations%2C%20out-of-distribution%20benchmarking%2C%20and%20real-robot%20experiments%20demonstrating%20robust%20offline%20motion%20replay%20and%20online%20long-horizon%20teleoperation%20under%20realistic%20latency%20and%20noise.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSAIC%253A%2520Bridging%2520the%2520Sim-to-Real%2520Gap%2520in%2520Generalist%2520Humanoid%2520Motion%2520Tracking%2520and%2520Teleoperation%2520with%2520Rapid%2520Residual%2520Adaptation%26entry.906535625%3DZhenguo%2520Sun%2520and%2520Bo-Sheng%2520Huang%2520and%2520Yibo%2520Peng%2520and%2520Xukun%2520Li%2520and%2520Jingyu%2520Ma%2520and%2520Yu%2520Sun%2520and%2520Zhe%2520Li%2520and%2520Haojun%2520Jiang%2520and%2520Biao%2520Gao%2520and%2520Zhenshan%2520Bing%2520and%2520Xinlong%2520Wang%2520and%2520Alois%2520Knoll%26entry.1292438233%3DGeneralist%2520humanoid%2520motion%2520trackers%2520have%2520recently%2520achieved%2520strong%2520simulation%2520metrics%2520by%2520scaling%2520data%2520and%2520training%252C%2520yet%2520often%2520remain%2520brittle%2520on%2520hardware%2520during%2520sustained%2520teleoperation%2520due%2520to%2520interface-%2520and%2520dynamics-induced%2520errors.%2520We%2520present%2520MOSAIC%252C%2520an%2520open-source%252C%2520full-stack%2520system%2520for%2520humanoid%2520motion%2520tracking%2520and%2520whole-body%2520teleoperation%2520across%2520multiple%2520interfaces.%2520MOSAIC%2520first%2520learns%2520a%2520teleoperation-oriented%2520general%2520motion%2520tracker%2520via%2520RL%2520on%2520a%2520multi-source%2520motion%2520bank%2520with%2520adaptive%2520resampling%2520and%2520rewards%2520that%2520emphasize%2520world-frame%2520motion%2520consistency%252C%2520which%2520is%2520critical%2520for%2520mobile%2520teleoperation.%2520To%2520bridge%2520the%2520sim-to-real%2520interface%2520gap%2520without%2520sacrificing%2520generality%252C%2520MOSAIC%2520then%2520performs%2520rapid%2520residual%2520adaptation%253A%2520an%2520interface-specific%2520policy%2520is%2520trained%2520using%2520minimal%2520interface-specific%2520data%252C%2520and%2520then%2520distilled%2520into%2520the%2520general%2520tracker%2520through%2520an%2520additive%2520residual%2520module%252C%2520outperforming%2520naive%2520fine-tuning%2520or%2520continual%2520learning.%2520We%2520validate%2520MOSAIC%2520with%2520systematic%2520ablations%252C%2520out-of-distribution%2520benchmarking%252C%2520and%2520real-robot%2520experiments%2520demonstrating%2520robust%2520offline%2520motion%2520replay%2520and%2520online%2520long-horizon%2520teleoperation%2520under%2520realistic%2520latency%2520and%2520noise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSAIC%3A%20Bridging%20the%20Sim-to-Real%20Gap%20in%20Generalist%20Humanoid%20Motion%20Tracking%20and%20Teleoperation%20with%20Rapid%20Residual%20Adaptation&entry.906535625=Zhenguo%20Sun%20and%20Bo-Sheng%20Huang%20and%20Yibo%20Peng%20and%20Xukun%20Li%20and%20Jingyu%20Ma%20and%20Yu%20Sun%20and%20Zhe%20Li%20and%20Haojun%20Jiang%20and%20Biao%20Gao%20and%20Zhenshan%20Bing%20and%20Xinlong%20Wang%20and%20Alois%20Knoll&entry.1292438233=Generalist%20humanoid%20motion%20trackers%20have%20recently%20achieved%20strong%20simulation%20metrics%20by%20scaling%20data%20and%20training%2C%20yet%20often%20remain%20brittle%20on%20hardware%20during%20sustained%20teleoperation%20due%20to%20interface-%20and%20dynamics-induced%20errors.%20We%20present%20MOSAIC%2C%20an%20open-source%2C%20full-stack%20system%20for%20humanoid%20motion%20tracking%20and%20whole-body%20teleoperation%20across%20multiple%20interfaces.%20MOSAIC%20first%20learns%20a%20teleoperation-oriented%20general%20motion%20tracker%20via%20RL%20on%20a%20multi-source%20motion%20bank%20with%20adaptive%20resampling%20and%20rewards%20that%20emphasize%20world-frame%20motion%20consistency%2C%20which%20is%20critical%20for%20mobile%20teleoperation.%20To%20bridge%20the%20sim-to-real%20interface%20gap%20without%20sacrificing%20generality%2C%20MOSAIC%20then%20performs%20rapid%20residual%20adaptation%3A%20an%20interface-specific%20policy%20is%20trained%20using%20minimal%20interface-specific%20data%2C%20and%20then%20distilled%20into%20the%20general%20tracker%20through%20an%20additive%20residual%20module%2C%20outperforming%20naive%20fine-tuning%20or%20continual%20learning.%20We%20validate%20MOSAIC%20with%20systematic%20ablations%2C%20out-of-distribution%20benchmarking%2C%20and%20real-robot%20experiments%20demonstrating%20robust%20offline%20motion%20replay%20and%20online%20long-horizon%20teleoperation%20under%20realistic%20latency%20and%20noise.&entry.1838667208=http%3A//arxiv.org/abs/2602.08594v1&entry.124074799=Read"},
{"title": "f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment", "author": "Rajdeep Haldar and Lantao Mei and Guang Lin and Yue Xing and Qifan Song", "abstract": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.", "link": "http://arxiv.org/abs/2602.05946v2", "date": "2026-02-09", "relevancy": 2.2579, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.459}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20f-GRPO%20and%20Beyond%3A%20Divergence-Based%20Reinforcement%20Learning%20Algorithms%20for%20General%20LLM%20Alignment&body=Title%3A%20f-GRPO%20and%20Beyond%3A%20Divergence-Based%20Reinforcement%20Learning%20Algorithms%20for%20General%20LLM%20Alignment%0AAuthor%3A%20Rajdeep%20Haldar%20and%20Lantao%20Mei%20and%20Guang%20Lin%20and%20Yue%20Xing%20and%20Qifan%20Song%0AAbstract%3A%20Recent%20research%20shows%20that%20Preference%20Alignment%20%28PA%29%20objectives%20act%20as%20divergence%20estimators%20between%20aligned%20%28chosen%29%20and%20unaligned%20%28rejected%29%20response%20distributions.%20In%20this%20work%2C%20we%20extend%20this%20divergence-based%20perspective%20to%20general%20alignment%20settings%2C%20such%20as%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20where%20only%20environmental%20rewards%20are%20available.%20Within%20this%20unified%20framework%2C%20we%20propose%20f-Group%20Relative%20Policy%20Optimization%20%28f-GRPO%29%2C%20a%20class%20of%20on-policy%20reinforcement%20learning%2C%20and%20f-Hybrid%20Alignment%20Loss%20%28f-HAL%29%2C%20a%20hybrid%20on/off%20policy%20objectives%2C%20for%20general%20LLM%20alignment%20based%20on%20variational%20representation%20of%20f-divergences.%20We%20provide%20theoretical%20guarantees%20that%20these%20classes%20of%20objectives%20improve%20the%20average%20reward%20after%20alignment.%20Empirically%2C%20we%20validate%20our%20framework%20on%20both%20RLVR%20%28Math%20Reasoning%29%20and%20PA%20tasks%20%28Safety%20Alignment%29%2C%20demonstrating%20superior%20performance%20and%20flexibility%20compared%20to%20current%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Df-GRPO%2520and%2520Beyond%253A%2520Divergence-Based%2520Reinforcement%2520Learning%2520Algorithms%2520for%2520General%2520LLM%2520Alignment%26entry.906535625%3DRajdeep%2520Haldar%2520and%2520Lantao%2520Mei%2520and%2520Guang%2520Lin%2520and%2520Yue%2520Xing%2520and%2520Qifan%2520Song%26entry.1292438233%3DRecent%2520research%2520shows%2520that%2520Preference%2520Alignment%2520%2528PA%2529%2520objectives%2520act%2520as%2520divergence%2520estimators%2520between%2520aligned%2520%2528chosen%2529%2520and%2520unaligned%2520%2528rejected%2529%2520response%2520distributions.%2520In%2520this%2520work%252C%2520we%2520extend%2520this%2520divergence-based%2520perspective%2520to%2520general%2520alignment%2520settings%252C%2520such%2520as%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%252C%2520where%2520only%2520environmental%2520rewards%2520are%2520available.%2520Within%2520this%2520unified%2520framework%252C%2520we%2520propose%2520f-Group%2520Relative%2520Policy%2520Optimization%2520%2528f-GRPO%2529%252C%2520a%2520class%2520of%2520on-policy%2520reinforcement%2520learning%252C%2520and%2520f-Hybrid%2520Alignment%2520Loss%2520%2528f-HAL%2529%252C%2520a%2520hybrid%2520on/off%2520policy%2520objectives%252C%2520for%2520general%2520LLM%2520alignment%2520based%2520on%2520variational%2520representation%2520of%2520f-divergences.%2520We%2520provide%2520theoretical%2520guarantees%2520that%2520these%2520classes%2520of%2520objectives%2520improve%2520the%2520average%2520reward%2520after%2520alignment.%2520Empirically%252C%2520we%2520validate%2520our%2520framework%2520on%2520both%2520RLVR%2520%2528Math%2520Reasoning%2529%2520and%2520PA%2520tasks%2520%2528Safety%2520Alignment%2529%252C%2520demonstrating%2520superior%2520performance%2520and%2520flexibility%2520compared%2520to%2520current%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=f-GRPO%20and%20Beyond%3A%20Divergence-Based%20Reinforcement%20Learning%20Algorithms%20for%20General%20LLM%20Alignment&entry.906535625=Rajdeep%20Haldar%20and%20Lantao%20Mei%20and%20Guang%20Lin%20and%20Yue%20Xing%20and%20Qifan%20Song&entry.1292438233=Recent%20research%20shows%20that%20Preference%20Alignment%20%28PA%29%20objectives%20act%20as%20divergence%20estimators%20between%20aligned%20%28chosen%29%20and%20unaligned%20%28rejected%29%20response%20distributions.%20In%20this%20work%2C%20we%20extend%20this%20divergence-based%20perspective%20to%20general%20alignment%20settings%2C%20such%20as%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20where%20only%20environmental%20rewards%20are%20available.%20Within%20this%20unified%20framework%2C%20we%20propose%20f-Group%20Relative%20Policy%20Optimization%20%28f-GRPO%29%2C%20a%20class%20of%20on-policy%20reinforcement%20learning%2C%20and%20f-Hybrid%20Alignment%20Loss%20%28f-HAL%29%2C%20a%20hybrid%20on/off%20policy%20objectives%2C%20for%20general%20LLM%20alignment%20based%20on%20variational%20representation%20of%20f-divergences.%20We%20provide%20theoretical%20guarantees%20that%20these%20classes%20of%20objectives%20improve%20the%20average%20reward%20after%20alignment.%20Empirically%2C%20we%20validate%20our%20framework%20on%20both%20RLVR%20%28Math%20Reasoning%29%20and%20PA%20tasks%20%28Safety%20Alignment%29%2C%20demonstrating%20superior%20performance%20and%20flexibility%20compared%20to%20current%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.05946v2&entry.124074799=Read"},
{"title": "Positive Distribution Shift as a Framework for Understanding Tractable Learning", "author": "Marko Medvedev and Idan Attias and Elisabetta Cornacchia and Theodor Misiakiewicz and Gal Vardi and Nathan Srebro", "abstract": "We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.", "link": "http://arxiv.org/abs/2602.08907v1", "date": "2026-02-09", "relevancy": 2.254, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4583}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.454}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positive%20Distribution%20Shift%20as%20a%20Framework%20for%20Understanding%20Tractable%20Learning&body=Title%3A%20Positive%20Distribution%20Shift%20as%20a%20Framework%20for%20Understanding%20Tractable%20Learning%0AAuthor%3A%20Marko%20Medvedev%20and%20Idan%20Attias%20and%20Elisabetta%20Cornacchia%20and%20Theodor%20Misiakiewicz%20and%20Gal%20Vardi%20and%20Nathan%20Srebro%0AAbstract%3A%20We%20study%20a%20setting%20where%20the%20goal%20is%20to%20learn%20a%20target%20function%20f%28x%29%20with%20respect%20to%20a%20target%20distribution%20D%28x%29%2C%20but%20training%20is%20done%20on%20i.i.d.%20samples%20from%20a%20different%20training%20distribution%20D%27%28x%29%2C%20labeled%20by%20the%20true%20target%20f%28x%29.%20Such%20a%20distribution%20shift%20%28here%20in%20the%20form%20of%20covariate%20shift%29%20is%20usually%20viewed%20negatively%2C%20as%20hurting%20or%20making%20learning%20harder%2C%20and%20the%20traditional%20distribution%20shift%20literature%20is%20mostly%20concerned%20with%20limiting%20or%20avoiding%20this%20negative%20effect.%20In%20contrast%2C%20we%20argue%20that%20with%20a%20well-chosen%20D%27%28x%29%2C%20the%20shift%20can%20be%20positive%20and%20make%20learning%20easier%20--%20a%20perspective%20called%20Positive%20Distribution%20Shift%20%28PDS%29.%20Such%20a%20perspective%20is%20central%20to%20contemporary%20machine%20learning%2C%20where%20much%20of%20the%20innovation%20is%20in%20finding%20good%20training%20distributions%20D%27%28x%29%2C%20rather%20than%20changing%20the%20training%20algorithm.%20We%20further%20argue%20that%20the%20benefit%20is%20often%20computational%20rather%20than%20statistical%2C%20and%20that%20PDS%20allows%20computationally%20hard%20problems%20to%20become%20tractable%20even%20using%20standard%20gradient-based%20training.%20We%20formalize%20different%20variants%20of%20PDS%2C%20show%20how%20certain%20hard%20classes%20are%20easily%20learnable%20under%20PDS%2C%20and%20make%20connections%20with%20membership%20query%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositive%2520Distribution%2520Shift%2520as%2520a%2520Framework%2520for%2520Understanding%2520Tractable%2520Learning%26entry.906535625%3DMarko%2520Medvedev%2520and%2520Idan%2520Attias%2520and%2520Elisabetta%2520Cornacchia%2520and%2520Theodor%2520Misiakiewicz%2520and%2520Gal%2520Vardi%2520and%2520Nathan%2520Srebro%26entry.1292438233%3DWe%2520study%2520a%2520setting%2520where%2520the%2520goal%2520is%2520to%2520learn%2520a%2520target%2520function%2520f%2528x%2529%2520with%2520respect%2520to%2520a%2520target%2520distribution%2520D%2528x%2529%252C%2520but%2520training%2520is%2520done%2520on%2520i.i.d.%2520samples%2520from%2520a%2520different%2520training%2520distribution%2520D%2527%2528x%2529%252C%2520labeled%2520by%2520the%2520true%2520target%2520f%2528x%2529.%2520Such%2520a%2520distribution%2520shift%2520%2528here%2520in%2520the%2520form%2520of%2520covariate%2520shift%2529%2520is%2520usually%2520viewed%2520negatively%252C%2520as%2520hurting%2520or%2520making%2520learning%2520harder%252C%2520and%2520the%2520traditional%2520distribution%2520shift%2520literature%2520is%2520mostly%2520concerned%2520with%2520limiting%2520or%2520avoiding%2520this%2520negative%2520effect.%2520In%2520contrast%252C%2520we%2520argue%2520that%2520with%2520a%2520well-chosen%2520D%2527%2528x%2529%252C%2520the%2520shift%2520can%2520be%2520positive%2520and%2520make%2520learning%2520easier%2520--%2520a%2520perspective%2520called%2520Positive%2520Distribution%2520Shift%2520%2528PDS%2529.%2520Such%2520a%2520perspective%2520is%2520central%2520to%2520contemporary%2520machine%2520learning%252C%2520where%2520much%2520of%2520the%2520innovation%2520is%2520in%2520finding%2520good%2520training%2520distributions%2520D%2527%2528x%2529%252C%2520rather%2520than%2520changing%2520the%2520training%2520algorithm.%2520We%2520further%2520argue%2520that%2520the%2520benefit%2520is%2520often%2520computational%2520rather%2520than%2520statistical%252C%2520and%2520that%2520PDS%2520allows%2520computationally%2520hard%2520problems%2520to%2520become%2520tractable%2520even%2520using%2520standard%2520gradient-based%2520training.%2520We%2520formalize%2520different%2520variants%2520of%2520PDS%252C%2520show%2520how%2520certain%2520hard%2520classes%2520are%2520easily%2520learnable%2520under%2520PDS%252C%2520and%2520make%2520connections%2520with%2520membership%2520query%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positive%20Distribution%20Shift%20as%20a%20Framework%20for%20Understanding%20Tractable%20Learning&entry.906535625=Marko%20Medvedev%20and%20Idan%20Attias%20and%20Elisabetta%20Cornacchia%20and%20Theodor%20Misiakiewicz%20and%20Gal%20Vardi%20and%20Nathan%20Srebro&entry.1292438233=We%20study%20a%20setting%20where%20the%20goal%20is%20to%20learn%20a%20target%20function%20f%28x%29%20with%20respect%20to%20a%20target%20distribution%20D%28x%29%2C%20but%20training%20is%20done%20on%20i.i.d.%20samples%20from%20a%20different%20training%20distribution%20D%27%28x%29%2C%20labeled%20by%20the%20true%20target%20f%28x%29.%20Such%20a%20distribution%20shift%20%28here%20in%20the%20form%20of%20covariate%20shift%29%20is%20usually%20viewed%20negatively%2C%20as%20hurting%20or%20making%20learning%20harder%2C%20and%20the%20traditional%20distribution%20shift%20literature%20is%20mostly%20concerned%20with%20limiting%20or%20avoiding%20this%20negative%20effect.%20In%20contrast%2C%20we%20argue%20that%20with%20a%20well-chosen%20D%27%28x%29%2C%20the%20shift%20can%20be%20positive%20and%20make%20learning%20easier%20--%20a%20perspective%20called%20Positive%20Distribution%20Shift%20%28PDS%29.%20Such%20a%20perspective%20is%20central%20to%20contemporary%20machine%20learning%2C%20where%20much%20of%20the%20innovation%20is%20in%20finding%20good%20training%20distributions%20D%27%28x%29%2C%20rather%20than%20changing%20the%20training%20algorithm.%20We%20further%20argue%20that%20the%20benefit%20is%20often%20computational%20rather%20than%20statistical%2C%20and%20that%20PDS%20allows%20computationally%20hard%20problems%20to%20become%20tractable%20even%20using%20standard%20gradient-based%20training.%20We%20formalize%20different%20variants%20of%20PDS%2C%20show%20how%20certain%20hard%20classes%20are%20easily%20learnable%20under%20PDS%2C%20and%20make%20connections%20with%20membership%20query%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.08907v1&entry.124074799=Read"},
{"title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing", "author": "Shih-Fang Chen and Jun-Cheng Chen and I-Hong Jhuo and Yen-Yu Lin", "abstract": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.", "link": "http://arxiv.org/abs/2602.08550v1", "date": "2026-02-09", "relevancy": 2.2446, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5641}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5606}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOT-Edit%3A%20Geometry-Aware%20Generic%20Object%20Tracking%20via%20Online%20Model%20Editing&body=Title%3A%20GOT-Edit%3A%20Geometry-Aware%20Generic%20Object%20Tracking%20via%20Online%20Model%20Editing%0AAuthor%3A%20Shih-Fang%20Chen%20and%20Jun-Cheng%20Chen%20and%20I-Hong%20Jhuo%20and%20Yen-Yu%20Lin%0AAbstract%3A%20Human%20perception%20for%20effective%20object%20tracking%20in%20a%202D%20video%20stream%20arises%20from%20the%20implicit%20use%20of%20prior%203D%20knowledge%20combined%20with%20semantic%20reasoning.%20In%20contrast%2C%20most%20generic%20object%20tracking%20%28GOT%29%20methods%20primarily%20rely%20on%202D%20features%20of%20the%20target%20and%20its%20surroundings%20while%20neglecting%203D%20geometric%20cues%2C%20which%20makes%20them%20susceptible%20to%20partial%20occlusion%2C%20distractors%2C%20and%20variations%20in%20geometry%20and%20appearance.%20To%20address%20this%20limitation%2C%20we%20introduce%20GOT-Edit%2C%20an%20online%20cross-modality%20model%20editing%20approach%20that%20integrates%20geometry-aware%20cues%20into%20a%20generic%20object%20tracker%20from%20a%202D%20video%20stream.%20Our%20approach%20leverages%20features%20from%20a%20pre-trained%20Visual%20Geometry%20Grounded%20Transformer%20to%20enable%20geometric%20cue%20inference%20from%20only%20a%20few%202D%20images.%20To%20tackle%20the%20challenge%20of%20seamlessly%20combining%20geometry%20and%20semantics%2C%20GOT-Edit%20performs%20online%20model%20editing%20with%20null-space%20constrained%20updates%20that%20incorporate%20geometric%20information%20while%20preserving%20semantic%20discrimination%2C%20yielding%20consistently%20better%20performance%20across%20diverse%20scenarios.%20Extensive%20experiments%20on%20multiple%20GOT%20benchmarks%20demonstrate%20that%20GOT-Edit%20achieves%20superior%20robustness%20and%20accuracy%2C%20particularly%20under%20occlusion%20and%20clutter%2C%20establishing%20a%20new%20paradigm%20for%20combining%202D%20semantics%20with%203D%20geometric%20reasoning%20for%20generic%20object%20tracking.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOT-Edit%253A%2520Geometry-Aware%2520Generic%2520Object%2520Tracking%2520via%2520Online%2520Model%2520Editing%26entry.906535625%3DShih-Fang%2520Chen%2520and%2520Jun-Cheng%2520Chen%2520and%2520I-Hong%2520Jhuo%2520and%2520Yen-Yu%2520Lin%26entry.1292438233%3DHuman%2520perception%2520for%2520effective%2520object%2520tracking%2520in%2520a%25202D%2520video%2520stream%2520arises%2520from%2520the%2520implicit%2520use%2520of%2520prior%25203D%2520knowledge%2520combined%2520with%2520semantic%2520reasoning.%2520In%2520contrast%252C%2520most%2520generic%2520object%2520tracking%2520%2528GOT%2529%2520methods%2520primarily%2520rely%2520on%25202D%2520features%2520of%2520the%2520target%2520and%2520its%2520surroundings%2520while%2520neglecting%25203D%2520geometric%2520cues%252C%2520which%2520makes%2520them%2520susceptible%2520to%2520partial%2520occlusion%252C%2520distractors%252C%2520and%2520variations%2520in%2520geometry%2520and%2520appearance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520GOT-Edit%252C%2520an%2520online%2520cross-modality%2520model%2520editing%2520approach%2520that%2520integrates%2520geometry-aware%2520cues%2520into%2520a%2520generic%2520object%2520tracker%2520from%2520a%25202D%2520video%2520stream.%2520Our%2520approach%2520leverages%2520features%2520from%2520a%2520pre-trained%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520to%2520enable%2520geometric%2520cue%2520inference%2520from%2520only%2520a%2520few%25202D%2520images.%2520To%2520tackle%2520the%2520challenge%2520of%2520seamlessly%2520combining%2520geometry%2520and%2520semantics%252C%2520GOT-Edit%2520performs%2520online%2520model%2520editing%2520with%2520null-space%2520constrained%2520updates%2520that%2520incorporate%2520geometric%2520information%2520while%2520preserving%2520semantic%2520discrimination%252C%2520yielding%2520consistently%2520better%2520performance%2520across%2520diverse%2520scenarios.%2520Extensive%2520experiments%2520on%2520multiple%2520GOT%2520benchmarks%2520demonstrate%2520that%2520GOT-Edit%2520achieves%2520superior%2520robustness%2520and%2520accuracy%252C%2520particularly%2520under%2520occlusion%2520and%2520clutter%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520combining%25202D%2520semantics%2520with%25203D%2520geometric%2520reasoning%2520for%2520generic%2520object%2520tracking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOT-Edit%3A%20Geometry-Aware%20Generic%20Object%20Tracking%20via%20Online%20Model%20Editing&entry.906535625=Shih-Fang%20Chen%20and%20Jun-Cheng%20Chen%20and%20I-Hong%20Jhuo%20and%20Yen-Yu%20Lin&entry.1292438233=Human%20perception%20for%20effective%20object%20tracking%20in%20a%202D%20video%20stream%20arises%20from%20the%20implicit%20use%20of%20prior%203D%20knowledge%20combined%20with%20semantic%20reasoning.%20In%20contrast%2C%20most%20generic%20object%20tracking%20%28GOT%29%20methods%20primarily%20rely%20on%202D%20features%20of%20the%20target%20and%20its%20surroundings%20while%20neglecting%203D%20geometric%20cues%2C%20which%20makes%20them%20susceptible%20to%20partial%20occlusion%2C%20distractors%2C%20and%20variations%20in%20geometry%20and%20appearance.%20To%20address%20this%20limitation%2C%20we%20introduce%20GOT-Edit%2C%20an%20online%20cross-modality%20model%20editing%20approach%20that%20integrates%20geometry-aware%20cues%20into%20a%20generic%20object%20tracker%20from%20a%202D%20video%20stream.%20Our%20approach%20leverages%20features%20from%20a%20pre-trained%20Visual%20Geometry%20Grounded%20Transformer%20to%20enable%20geometric%20cue%20inference%20from%20only%20a%20few%202D%20images.%20To%20tackle%20the%20challenge%20of%20seamlessly%20combining%20geometry%20and%20semantics%2C%20GOT-Edit%20performs%20online%20model%20editing%20with%20null-space%20constrained%20updates%20that%20incorporate%20geometric%20information%20while%20preserving%20semantic%20discrimination%2C%20yielding%20consistently%20better%20performance%20across%20diverse%20scenarios.%20Extensive%20experiments%20on%20multiple%20GOT%20benchmarks%20demonstrate%20that%20GOT-Edit%20achieves%20superior%20robustness%20and%20accuracy%2C%20particularly%20under%20occlusion%20and%20clutter%2C%20establishing%20a%20new%20paradigm%20for%20combining%202D%20semantics%20with%203D%20geometric%20reasoning%20for%20generic%20object%20tracking.&entry.1838667208=http%3A//arxiv.org/abs/2602.08550v1&entry.124074799=Read"},
{"title": "Overview and Comparison of AVS Point Cloud Compression Standard", "author": "Wei Gao and Wenxu Gao and Xingming Mu and Changhao Peng and Ge Li", "abstract": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.", "link": "http://arxiv.org/abs/2602.08613v1", "date": "2026-02-09", "relevancy": 2.2431, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4797}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overview%20and%20Comparison%20of%20AVS%20Point%20Cloud%20Compression%20Standard&body=Title%3A%20Overview%20and%20Comparison%20of%20AVS%20Point%20Cloud%20Compression%20Standard%0AAuthor%3A%20Wei%20Gao%20and%20Wenxu%20Gao%20and%20Xingming%20Mu%20and%20Changhao%20Peng%20and%20Ge%20Li%0AAbstract%3A%20Point%20cloud%20is%20a%20prevalent%203D%20data%20representation%20format%20with%20significant%20application%20values%20in%20immersive%20media%2C%20autonomous%20driving%2C%20digital%20heritage%20protection%2C%20etc.%20However%2C%20the%20large%20data%20size%20of%20point%20clouds%20poses%20challenges%20to%20transmission%20and%20storage%2C%20which%20influences%20the%20wide%20deployments.%20Therefore%2C%20point%20cloud%20compression%20plays%20a%20crucial%20role%20in%20practical%20applications%20for%20both%20human%20and%20machine%20perception%20optimization.%20To%20this%20end%2C%20the%20Moving%20Picture%20Experts%20Group%20%28MPEG%29%20has%20established%20two%20standards%20for%20point%20cloud%20compression%2C%20including%20Geometry-based%20Point%20Cloud%20Compression%20%28G-PCC%29%20and%20Video-based%20Point%20Cloud%20Compression%20%28V-PCC%29.%20In%20the%20meantime%2C%20the%20Audio%20Video%20coding%20Standard%20%28AVS%29%20Workgroup%20of%20China%20also%20have%20launched%20and%20completed%20the%20development%20for%20its%20first%20generation%20point%20cloud%20compression%20standard%2C%20namely%20AVS%20PCC.%20This%20new%20standardization%20effort%20has%20adopted%20many%20new%20coding%20tools%20and%20techniques%2C%20which%20are%20different%20from%20the%20other%20counterpart%20standards.%20This%20paper%20reviews%20the%20AVS%20PCC%20standard%20from%20two%20perspectives%2C%20i.e.%2C%20the%20related%20technologies%20and%20performance%20comparisons.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverview%2520and%2520Comparison%2520of%2520AVS%2520Point%2520Cloud%2520Compression%2520Standard%26entry.906535625%3DWei%2520Gao%2520and%2520Wenxu%2520Gao%2520and%2520Xingming%2520Mu%2520and%2520Changhao%2520Peng%2520and%2520Ge%2520Li%26entry.1292438233%3DPoint%2520cloud%2520is%2520a%2520prevalent%25203D%2520data%2520representation%2520format%2520with%2520significant%2520application%2520values%2520in%2520immersive%2520media%252C%2520autonomous%2520driving%252C%2520digital%2520heritage%2520protection%252C%2520etc.%2520However%252C%2520the%2520large%2520data%2520size%2520of%2520point%2520clouds%2520poses%2520challenges%2520to%2520transmission%2520and%2520storage%252C%2520which%2520influences%2520the%2520wide%2520deployments.%2520Therefore%252C%2520point%2520cloud%2520compression%2520plays%2520a%2520crucial%2520role%2520in%2520practical%2520applications%2520for%2520both%2520human%2520and%2520machine%2520perception%2520optimization.%2520To%2520this%2520end%252C%2520the%2520Moving%2520Picture%2520Experts%2520Group%2520%2528MPEG%2529%2520has%2520established%2520two%2520standards%2520for%2520point%2520cloud%2520compression%252C%2520including%2520Geometry-based%2520Point%2520Cloud%2520Compression%2520%2528G-PCC%2529%2520and%2520Video-based%2520Point%2520Cloud%2520Compression%2520%2528V-PCC%2529.%2520In%2520the%2520meantime%252C%2520the%2520Audio%2520Video%2520coding%2520Standard%2520%2528AVS%2529%2520Workgroup%2520of%2520China%2520also%2520have%2520launched%2520and%2520completed%2520the%2520development%2520for%2520its%2520first%2520generation%2520point%2520cloud%2520compression%2520standard%252C%2520namely%2520AVS%2520PCC.%2520This%2520new%2520standardization%2520effort%2520has%2520adopted%2520many%2520new%2520coding%2520tools%2520and%2520techniques%252C%2520which%2520are%2520different%2520from%2520the%2520other%2520counterpart%2520standards.%2520This%2520paper%2520reviews%2520the%2520AVS%2520PCC%2520standard%2520from%2520two%2520perspectives%252C%2520i.e.%252C%2520the%2520related%2520technologies%2520and%2520performance%2520comparisons.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overview%20and%20Comparison%20of%20AVS%20Point%20Cloud%20Compression%20Standard&entry.906535625=Wei%20Gao%20and%20Wenxu%20Gao%20and%20Xingming%20Mu%20and%20Changhao%20Peng%20and%20Ge%20Li&entry.1292438233=Point%20cloud%20is%20a%20prevalent%203D%20data%20representation%20format%20with%20significant%20application%20values%20in%20immersive%20media%2C%20autonomous%20driving%2C%20digital%20heritage%20protection%2C%20etc.%20However%2C%20the%20large%20data%20size%20of%20point%20clouds%20poses%20challenges%20to%20transmission%20and%20storage%2C%20which%20influences%20the%20wide%20deployments.%20Therefore%2C%20point%20cloud%20compression%20plays%20a%20crucial%20role%20in%20practical%20applications%20for%20both%20human%20and%20machine%20perception%20optimization.%20To%20this%20end%2C%20the%20Moving%20Picture%20Experts%20Group%20%28MPEG%29%20has%20established%20two%20standards%20for%20point%20cloud%20compression%2C%20including%20Geometry-based%20Point%20Cloud%20Compression%20%28G-PCC%29%20and%20Video-based%20Point%20Cloud%20Compression%20%28V-PCC%29.%20In%20the%20meantime%2C%20the%20Audio%20Video%20coding%20Standard%20%28AVS%29%20Workgroup%20of%20China%20also%20have%20launched%20and%20completed%20the%20development%20for%20its%20first%20generation%20point%20cloud%20compression%20standard%2C%20namely%20AVS%20PCC.%20This%20new%20standardization%20effort%20has%20adopted%20many%20new%20coding%20tools%20and%20techniques%2C%20which%20are%20different%20from%20the%20other%20counterpart%20standards.%20This%20paper%20reviews%20the%20AVS%20PCC%20standard%20from%20two%20perspectives%2C%20i.e.%2C%20the%20related%20technologies%20and%20performance%20comparisons.&entry.1838667208=http%3A//arxiv.org/abs/2602.08613v1&entry.124074799=Read"},
{"title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions", "author": "Linli Yao and Yuancheng Wei and Yaojie Zhang and Lei Li and Xinlong Chen and Feifan Song and Ziyue Wang and Kun Ouyang and Yuanxin Liu and Lingpeng Kong and Qi Liu and Pengfei Wan and Kun Gai and Yuanxing Zhang and Xu Sun", "abstract": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.", "link": "http://arxiv.org/abs/2602.08711v1", "date": "2026-02-09", "relevancy": 2.2333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeChat-Captioner%3A%20Scripting%20Multi-Scene%20Videos%20with%20Time-Aware%20and%20Structural%20Audio-Visual%20Captions&body=Title%3A%20TimeChat-Captioner%3A%20Scripting%20Multi-Scene%20Videos%20with%20Time-Aware%20and%20Structural%20Audio-Visual%20Captions%0AAuthor%3A%20Linli%20Yao%20and%20Yuancheng%20Wei%20and%20Yaojie%20Zhang%20and%20Lei%20Li%20and%20Xinlong%20Chen%20and%20Feifan%20Song%20and%20Ziyue%20Wang%20and%20Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Lingpeng%20Kong%20and%20Qi%20Liu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Yuanxing%20Zhang%20and%20Xu%20Sun%0AAbstract%3A%20This%20paper%20proposes%20Omni%20Dense%20Captioning%2C%20a%20novel%20task%20designed%20to%20generate%20continuous%2C%20fine-grained%2C%20and%20structured%20audio-visual%20narratives%20with%20explicit%20timestamps.%20To%20ensure%20dense%20semantic%20coverage%2C%20we%20introduce%20a%20six-dimensional%20structural%20schema%20to%20create%20%22script-like%22%20captions%2C%20enabling%20readers%20to%20vividly%20imagine%20the%20video%20content%20scene%20by%20scene%2C%20akin%20to%20a%20cinematographic%20screenplay.%20To%20facilitate%20research%2C%20we%20construct%20OmniDCBench%2C%20a%20high-quality%2C%20human-annotated%20benchmark%2C%20and%20propose%20SodaM%2C%20a%20unified%20metric%20that%20evaluates%20time-aware%20detailed%20descriptions%20while%20mitigating%20scene%20boundary%20ambiguity.%20Furthermore%2C%20we%20construct%20a%20training%20dataset%2C%20TimeChatCap-42K%2C%20and%20present%20TimeChat-Captioner-7B%2C%20a%20strong%20baseline%20trained%20via%20SFT%20and%20GRPO%20with%20task-specific%20rewards.%20Extensive%20experiments%20demonstrate%20that%20TimeChat-Captioner-7B%20achieves%20state-of-the-art%20performance%2C%20surpassing%20Gemini-2.5-Pro%2C%20while%20its%20generated%20dense%20descriptions%20significantly%20boost%20downstream%20capabilities%20in%20audio-visual%20reasoning%20%28DailyOmni%20and%20WorldSense%29%20and%20temporal%20grounding%20%28Charades-STA%29.%20All%20datasets%2C%20models%2C%20and%20code%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/yaolinli/TimeChat-Captioner.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeChat-Captioner%253A%2520Scripting%2520Multi-Scene%2520Videos%2520with%2520Time-Aware%2520and%2520Structural%2520Audio-Visual%2520Captions%26entry.906535625%3DLinli%2520Yao%2520and%2520Yuancheng%2520Wei%2520and%2520Yaojie%2520Zhang%2520and%2520Lei%2520Li%2520and%2520Xinlong%2520Chen%2520and%2520Feifan%2520Song%2520and%2520Ziyue%2520Wang%2520and%2520Kun%2520Ouyang%2520and%2520Yuanxin%2520Liu%2520and%2520Lingpeng%2520Kong%2520and%2520Qi%2520Liu%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Yuanxing%2520Zhang%2520and%2520Xu%2520Sun%26entry.1292438233%3DThis%2520paper%2520proposes%2520Omni%2520Dense%2520Captioning%252C%2520a%2520novel%2520task%2520designed%2520to%2520generate%2520continuous%252C%2520fine-grained%252C%2520and%2520structured%2520audio-visual%2520narratives%2520with%2520explicit%2520timestamps.%2520To%2520ensure%2520dense%2520semantic%2520coverage%252C%2520we%2520introduce%2520a%2520six-dimensional%2520structural%2520schema%2520to%2520create%2520%2522script-like%2522%2520captions%252C%2520enabling%2520readers%2520to%2520vividly%2520imagine%2520the%2520video%2520content%2520scene%2520by%2520scene%252C%2520akin%2520to%2520a%2520cinematographic%2520screenplay.%2520To%2520facilitate%2520research%252C%2520we%2520construct%2520OmniDCBench%252C%2520a%2520high-quality%252C%2520human-annotated%2520benchmark%252C%2520and%2520propose%2520SodaM%252C%2520a%2520unified%2520metric%2520that%2520evaluates%2520time-aware%2520detailed%2520descriptions%2520while%2520mitigating%2520scene%2520boundary%2520ambiguity.%2520Furthermore%252C%2520we%2520construct%2520a%2520training%2520dataset%252C%2520TimeChatCap-42K%252C%2520and%2520present%2520TimeChat-Captioner-7B%252C%2520a%2520strong%2520baseline%2520trained%2520via%2520SFT%2520and%2520GRPO%2520with%2520task-specific%2520rewards.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TimeChat-Captioner-7B%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520Gemini-2.5-Pro%252C%2520while%2520its%2520generated%2520dense%2520descriptions%2520significantly%2520boost%2520downstream%2520capabilities%2520in%2520audio-visual%2520reasoning%2520%2528DailyOmni%2520and%2520WorldSense%2529%2520and%2520temporal%2520grounding%2520%2528Charades-STA%2529.%2520All%2520datasets%252C%2520models%252C%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/yaolinli/TimeChat-Captioner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeChat-Captioner%3A%20Scripting%20Multi-Scene%20Videos%20with%20Time-Aware%20and%20Structural%20Audio-Visual%20Captions&entry.906535625=Linli%20Yao%20and%20Yuancheng%20Wei%20and%20Yaojie%20Zhang%20and%20Lei%20Li%20and%20Xinlong%20Chen%20and%20Feifan%20Song%20and%20Ziyue%20Wang%20and%20Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Lingpeng%20Kong%20and%20Qi%20Liu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Yuanxing%20Zhang%20and%20Xu%20Sun&entry.1292438233=This%20paper%20proposes%20Omni%20Dense%20Captioning%2C%20a%20novel%20task%20designed%20to%20generate%20continuous%2C%20fine-grained%2C%20and%20structured%20audio-visual%20narratives%20with%20explicit%20timestamps.%20To%20ensure%20dense%20semantic%20coverage%2C%20we%20introduce%20a%20six-dimensional%20structural%20schema%20to%20create%20%22script-like%22%20captions%2C%20enabling%20readers%20to%20vividly%20imagine%20the%20video%20content%20scene%20by%20scene%2C%20akin%20to%20a%20cinematographic%20screenplay.%20To%20facilitate%20research%2C%20we%20construct%20OmniDCBench%2C%20a%20high-quality%2C%20human-annotated%20benchmark%2C%20and%20propose%20SodaM%2C%20a%20unified%20metric%20that%20evaluates%20time-aware%20detailed%20descriptions%20while%20mitigating%20scene%20boundary%20ambiguity.%20Furthermore%2C%20we%20construct%20a%20training%20dataset%2C%20TimeChatCap-42K%2C%20and%20present%20TimeChat-Captioner-7B%2C%20a%20strong%20baseline%20trained%20via%20SFT%20and%20GRPO%20with%20task-specific%20rewards.%20Extensive%20experiments%20demonstrate%20that%20TimeChat-Captioner-7B%20achieves%20state-of-the-art%20performance%2C%20surpassing%20Gemini-2.5-Pro%2C%20while%20its%20generated%20dense%20descriptions%20significantly%20boost%20downstream%20capabilities%20in%20audio-visual%20reasoning%20%28DailyOmni%20and%20WorldSense%29%20and%20temporal%20grounding%20%28Charades-STA%29.%20All%20datasets%2C%20models%2C%20and%20code%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/yaolinli/TimeChat-Captioner.&entry.1838667208=http%3A//arxiv.org/abs/2602.08711v1&entry.124074799=Read"},
{"title": "EgoFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving", "author": "Haisheng Su and Wei Wu and Zhenjie Yang and Isabel Guan", "abstract": "Current End-to-End Autonomous Driving (E2E-AD) methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized with a fully differentiable framework in a planning-oriented manner, existing end-to-end driving systems lacking ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, due to rasterized scene representation learning and redundant information transmission. In this paper, we propose an ego-centric fully sparse paradigm, named EgoFSD, for end-to-end self-driving. Specifically, EgoFSD consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. In addition, position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thereby enhancing the training stability and convergence speed. Extensive experiments are conducted on nuScenes and Bench2Drive datasets, which significantly reduces the average L2 error by 59% and collision rate by 92% than UniAD while achieves 6.9x faster running efficiency.", "link": "http://arxiv.org/abs/2409.09777v6", "date": "2026-02-09", "relevancy": 2.2332, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5883}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5395}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoFSD%3A%20Ego-Centric%20Fully%20Sparse%20Paradigm%20with%20Uncertainty%20Denoising%20and%20Iterative%20Refinement%20for%20Efficient%20End-to-End%20Self-Driving&body=Title%3A%20EgoFSD%3A%20Ego-Centric%20Fully%20Sparse%20Paradigm%20with%20Uncertainty%20Denoising%20and%20Iterative%20Refinement%20for%20Efficient%20End-to-End%20Self-Driving%0AAuthor%3A%20Haisheng%20Su%20and%20Wei%20Wu%20and%20Zhenjie%20Yang%20and%20Isabel%20Guan%0AAbstract%3A%20Current%20End-to-End%20Autonomous%20Driving%20%28E2E-AD%29%20methods%20resort%20to%20unifying%20modular%20designs%20for%20various%20tasks%20%28e.g.%20perception%2C%20prediction%20and%20planning%29.%20Although%20optimized%20with%20a%20fully%20differentiable%20framework%20in%20a%20planning-oriented%20manner%2C%20existing%20end-to-end%20driving%20systems%20lacking%20ego-centric%20designs%20still%20suffer%20from%20unsatisfactory%20performance%20and%20inferior%20efficiency%2C%20due%20to%20rasterized%20scene%20representation%20learning%20and%20redundant%20information%20transmission.%20In%20this%20paper%2C%20we%20propose%20an%20ego-centric%20fully%20sparse%20paradigm%2C%20named%20EgoFSD%2C%20for%20end-to-end%20self-driving.%20Specifically%2C%20EgoFSD%20consists%20of%20sparse%20perception%2C%20hierarchical%20interaction%20and%20iterative%20motion%20planner.%20The%20sparse%20perception%20module%20performs%20detection%20and%20online%20mapping%20based%20on%20sparse%20representation%20of%20the%20driving%20scene.%20The%20hierarchical%20interaction%20module%20aims%20to%20select%20the%20Closest%20In-Path%20Vehicle%20/%20Stationary%20%28CIPV%20/%20CIPS%29%20from%20coarse%20to%20fine%2C%20benefiting%20from%20an%20additional%20geometric%20prior.%20As%20for%20the%20iterative%20motion%20planner%2C%20both%20selected%20interactive%20agents%20and%20ego-vehicle%20are%20considered%20for%20joint%20motion%20prediction%2C%20where%20the%20output%20multi-modal%20ego-trajectories%20are%20optimized%20in%20an%20iterative%20fashion.%20In%20addition%2C%20position-level%20motion%20diffusion%20and%20trajectory-level%20planning%20denoising%20are%20introduced%20for%20uncertainty%20modeling%2C%20thereby%20enhancing%20the%20training%20stability%20and%20convergence%20speed.%20Extensive%20experiments%20are%20conducted%20on%20nuScenes%20and%20Bench2Drive%20datasets%2C%20which%20significantly%20reduces%20the%20average%20L2%20error%20by%2059%25%20and%20collision%20rate%20by%2092%25%20than%20UniAD%20while%20achieves%206.9x%20faster%20running%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2409.09777v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoFSD%253A%2520Ego-Centric%2520Fully%2520Sparse%2520Paradigm%2520with%2520Uncertainty%2520Denoising%2520and%2520Iterative%2520Refinement%2520for%2520Efficient%2520End-to-End%2520Self-Driving%26entry.906535625%3DHaisheng%2520Su%2520and%2520Wei%2520Wu%2520and%2520Zhenjie%2520Yang%2520and%2520Isabel%2520Guan%26entry.1292438233%3DCurrent%2520End-to-End%2520Autonomous%2520Driving%2520%2528E2E-AD%2529%2520methods%2520resort%2520to%2520unifying%2520modular%2520designs%2520for%2520various%2520tasks%2520%2528e.g.%2520perception%252C%2520prediction%2520and%2520planning%2529.%2520Although%2520optimized%2520with%2520a%2520fully%2520differentiable%2520framework%2520in%2520a%2520planning-oriented%2520manner%252C%2520existing%2520end-to-end%2520driving%2520systems%2520lacking%2520ego-centric%2520designs%2520still%2520suffer%2520from%2520unsatisfactory%2520performance%2520and%2520inferior%2520efficiency%252C%2520due%2520to%2520rasterized%2520scene%2520representation%2520learning%2520and%2520redundant%2520information%2520transmission.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520ego-centric%2520fully%2520sparse%2520paradigm%252C%2520named%2520EgoFSD%252C%2520for%2520end-to-end%2520self-driving.%2520Specifically%252C%2520EgoFSD%2520consists%2520of%2520sparse%2520perception%252C%2520hierarchical%2520interaction%2520and%2520iterative%2520motion%2520planner.%2520The%2520sparse%2520perception%2520module%2520performs%2520detection%2520and%2520online%2520mapping%2520based%2520on%2520sparse%2520representation%2520of%2520the%2520driving%2520scene.%2520The%2520hierarchical%2520interaction%2520module%2520aims%2520to%2520select%2520the%2520Closest%2520In-Path%2520Vehicle%2520/%2520Stationary%2520%2528CIPV%2520/%2520CIPS%2529%2520from%2520coarse%2520to%2520fine%252C%2520benefiting%2520from%2520an%2520additional%2520geometric%2520prior.%2520As%2520for%2520the%2520iterative%2520motion%2520planner%252C%2520both%2520selected%2520interactive%2520agents%2520and%2520ego-vehicle%2520are%2520considered%2520for%2520joint%2520motion%2520prediction%252C%2520where%2520the%2520output%2520multi-modal%2520ego-trajectories%2520are%2520optimized%2520in%2520an%2520iterative%2520fashion.%2520In%2520addition%252C%2520position-level%2520motion%2520diffusion%2520and%2520trajectory-level%2520planning%2520denoising%2520are%2520introduced%2520for%2520uncertainty%2520modeling%252C%2520thereby%2520enhancing%2520the%2520training%2520stability%2520and%2520convergence%2520speed.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520nuScenes%2520and%2520Bench2Drive%2520datasets%252C%2520which%2520significantly%2520reduces%2520the%2520average%2520L2%2520error%2520by%252059%2525%2520and%2520collision%2520rate%2520by%252092%2525%2520than%2520UniAD%2520while%2520achieves%25206.9x%2520faster%2520running%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09777v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoFSD%3A%20Ego-Centric%20Fully%20Sparse%20Paradigm%20with%20Uncertainty%20Denoising%20and%20Iterative%20Refinement%20for%20Efficient%20End-to-End%20Self-Driving&entry.906535625=Haisheng%20Su%20and%20Wei%20Wu%20and%20Zhenjie%20Yang%20and%20Isabel%20Guan&entry.1292438233=Current%20End-to-End%20Autonomous%20Driving%20%28E2E-AD%29%20methods%20resort%20to%20unifying%20modular%20designs%20for%20various%20tasks%20%28e.g.%20perception%2C%20prediction%20and%20planning%29.%20Although%20optimized%20with%20a%20fully%20differentiable%20framework%20in%20a%20planning-oriented%20manner%2C%20existing%20end-to-end%20driving%20systems%20lacking%20ego-centric%20designs%20still%20suffer%20from%20unsatisfactory%20performance%20and%20inferior%20efficiency%2C%20due%20to%20rasterized%20scene%20representation%20learning%20and%20redundant%20information%20transmission.%20In%20this%20paper%2C%20we%20propose%20an%20ego-centric%20fully%20sparse%20paradigm%2C%20named%20EgoFSD%2C%20for%20end-to-end%20self-driving.%20Specifically%2C%20EgoFSD%20consists%20of%20sparse%20perception%2C%20hierarchical%20interaction%20and%20iterative%20motion%20planner.%20The%20sparse%20perception%20module%20performs%20detection%20and%20online%20mapping%20based%20on%20sparse%20representation%20of%20the%20driving%20scene.%20The%20hierarchical%20interaction%20module%20aims%20to%20select%20the%20Closest%20In-Path%20Vehicle%20/%20Stationary%20%28CIPV%20/%20CIPS%29%20from%20coarse%20to%20fine%2C%20benefiting%20from%20an%20additional%20geometric%20prior.%20As%20for%20the%20iterative%20motion%20planner%2C%20both%20selected%20interactive%20agents%20and%20ego-vehicle%20are%20considered%20for%20joint%20motion%20prediction%2C%20where%20the%20output%20multi-modal%20ego-trajectories%20are%20optimized%20in%20an%20iterative%20fashion.%20In%20addition%2C%20position-level%20motion%20diffusion%20and%20trajectory-level%20planning%20denoising%20are%20introduced%20for%20uncertainty%20modeling%2C%20thereby%20enhancing%20the%20training%20stability%20and%20convergence%20speed.%20Extensive%20experiments%20are%20conducted%20on%20nuScenes%20and%20Bench2Drive%20datasets%2C%20which%20significantly%20reduces%20the%20average%20L2%20error%20by%2059%25%20and%20collision%20rate%20by%2092%25%20than%20UniAD%20while%20achieves%206.9x%20faster%20running%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2409.09777v6&entry.124074799=Read"},
{"title": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation", "author": "Ning Yang and Chengzhi Wang and Yibo Liu and Baoliang Tian and Haijun Zhang", "abstract": "Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.", "link": "http://arxiv.org/abs/2602.08686v1", "date": "2026-02-09", "relevancy": 2.2293, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompilerKV%3A%20Risk-Adaptive%20KV%20Compression%20via%20Offline%20Experience%20Compilation&body=Title%3A%20CompilerKV%3A%20Risk-Adaptive%20KV%20Compression%20via%20Offline%20Experience%20Compilation%0AAuthor%3A%20Ning%20Yang%20and%20Chengzhi%20Wang%20and%20Yibo%20Liu%20and%20Baoliang%20Tian%20and%20Haijun%20Zhang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20in%20long-context%20scenarios%20are%20severely%20constrained%20by%20the%20linear%20growth%20of%20Key-Value%20%28KV%29%20cache%20memory.%20Existing%20KV%20compression%20methods%20rely%20either%20on%20static%20thresholds%20and%20attention-only%20heuristics%20or%20on%20coarse%20memory%20budget%20allocation.%20Under%20tight%20memory%20budgets%2C%20these%20methods%20overlook%20two%20key%20factors%3A%20prompt-dependent%20variation%20in%20compression%20risk%20and%20functional%20heterogeneity%20across%20attention%20heads%2C%20which%20destabilize%20token%20selection%20and%20lead%20to%20tail%20failures.%20To%20address%20these%20challenges%2C%20we%20propose%20CompilerKV%2C%20a%20risk-adaptive%20and%20head-aware%20compression%20framework%20that%20compiles%20offline%20experience%20into%20reusable%20decision%20tables%20for%20prefill-only%20deployment.%20CompilerKV%20integrates%20two%20key%20synergistic%20components%3A%20%28i%29%20a%20Head%20Heterogeneity%20Table%2C%20learned%20via%20offline%20contextual%20bandits%2C%20which%20assigns%20head-specific%20reliability%20weights%20to%20govern%20functional%20differences%20across%20attention%20heads%20explicitly%3B%20and%20%28ii%29%20a%20Risk-Adaptive%20Threshold%20Gating%20mechanism%20that%20jointly%20models%20attention%20entropy%20and%20local%20perplexity%2C%20transforming%20prompt-level%20risk%20into%20deployable%20retention%20thresholds.%20Experiments%20on%20LongBench%20show%20CompilerKV%20dominates%20SOTA%20methods%20under%20a%20512-token%20budget%2C%20recovering%2097.7%5C%25%20of%20FullKV%20performance%20while%20achieving%20up%20to%20%2B5.2%20points%20gain%20over%20the%20strongest%20competitor.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompilerKV%253A%2520Risk-Adaptive%2520KV%2520Compression%2520via%2520Offline%2520Experience%2520Compilation%26entry.906535625%3DNing%2520Yang%2520and%2520Chengzhi%2520Wang%2520and%2520Yibo%2520Liu%2520and%2520Baoliang%2520Tian%2520and%2520Haijun%2520Zhang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520long-context%2520scenarios%2520are%2520severely%2520constrained%2520by%2520the%2520linear%2520growth%2520of%2520Key-Value%2520%2528KV%2529%2520cache%2520memory.%2520Existing%2520KV%2520compression%2520methods%2520rely%2520either%2520on%2520static%2520thresholds%2520and%2520attention-only%2520heuristics%2520or%2520on%2520coarse%2520memory%2520budget%2520allocation.%2520Under%2520tight%2520memory%2520budgets%252C%2520these%2520methods%2520overlook%2520two%2520key%2520factors%253A%2520prompt-dependent%2520variation%2520in%2520compression%2520risk%2520and%2520functional%2520heterogeneity%2520across%2520attention%2520heads%252C%2520which%2520destabilize%2520token%2520selection%2520and%2520lead%2520to%2520tail%2520failures.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520CompilerKV%252C%2520a%2520risk-adaptive%2520and%2520head-aware%2520compression%2520framework%2520that%2520compiles%2520offline%2520experience%2520into%2520reusable%2520decision%2520tables%2520for%2520prefill-only%2520deployment.%2520CompilerKV%2520integrates%2520two%2520key%2520synergistic%2520components%253A%2520%2528i%2529%2520a%2520Head%2520Heterogeneity%2520Table%252C%2520learned%2520via%2520offline%2520contextual%2520bandits%252C%2520which%2520assigns%2520head-specific%2520reliability%2520weights%2520to%2520govern%2520functional%2520differences%2520across%2520attention%2520heads%2520explicitly%253B%2520and%2520%2528ii%2529%2520a%2520Risk-Adaptive%2520Threshold%2520Gating%2520mechanism%2520that%2520jointly%2520models%2520attention%2520entropy%2520and%2520local%2520perplexity%252C%2520transforming%2520prompt-level%2520risk%2520into%2520deployable%2520retention%2520thresholds.%2520Experiments%2520on%2520LongBench%2520show%2520CompilerKV%2520dominates%2520SOTA%2520methods%2520under%2520a%2520512-token%2520budget%252C%2520recovering%252097.7%255C%2525%2520of%2520FullKV%2520performance%2520while%2520achieving%2520up%2520to%2520%252B5.2%2520points%2520gain%2520over%2520the%2520strongest%2520competitor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompilerKV%3A%20Risk-Adaptive%20KV%20Compression%20via%20Offline%20Experience%20Compilation&entry.906535625=Ning%20Yang%20and%20Chengzhi%20Wang%20and%20Yibo%20Liu%20and%20Baoliang%20Tian%20and%20Haijun%20Zhang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20in%20long-context%20scenarios%20are%20severely%20constrained%20by%20the%20linear%20growth%20of%20Key-Value%20%28KV%29%20cache%20memory.%20Existing%20KV%20compression%20methods%20rely%20either%20on%20static%20thresholds%20and%20attention-only%20heuristics%20or%20on%20coarse%20memory%20budget%20allocation.%20Under%20tight%20memory%20budgets%2C%20these%20methods%20overlook%20two%20key%20factors%3A%20prompt-dependent%20variation%20in%20compression%20risk%20and%20functional%20heterogeneity%20across%20attention%20heads%2C%20which%20destabilize%20token%20selection%20and%20lead%20to%20tail%20failures.%20To%20address%20these%20challenges%2C%20we%20propose%20CompilerKV%2C%20a%20risk-adaptive%20and%20head-aware%20compression%20framework%20that%20compiles%20offline%20experience%20into%20reusable%20decision%20tables%20for%20prefill-only%20deployment.%20CompilerKV%20integrates%20two%20key%20synergistic%20components%3A%20%28i%29%20a%20Head%20Heterogeneity%20Table%2C%20learned%20via%20offline%20contextual%20bandits%2C%20which%20assigns%20head-specific%20reliability%20weights%20to%20govern%20functional%20differences%20across%20attention%20heads%20explicitly%3B%20and%20%28ii%29%20a%20Risk-Adaptive%20Threshold%20Gating%20mechanism%20that%20jointly%20models%20attention%20entropy%20and%20local%20perplexity%2C%20transforming%20prompt-level%20risk%20into%20deployable%20retention%20thresholds.%20Experiments%20on%20LongBench%20show%20CompilerKV%20dominates%20SOTA%20methods%20under%20a%20512-token%20budget%2C%20recovering%2097.7%5C%25%20of%20FullKV%20performance%20while%20achieving%20up%20to%20%2B5.2%20points%20gain%20over%20the%20strongest%20competitor.&entry.1838667208=http%3A//arxiv.org/abs/2602.08686v1&entry.124074799=Read"},
{"title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling", "author": "Yilang Zhang and Bingcong Li and Niao He and Georgios B. Giannakis", "abstract": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.", "link": "http://arxiv.org/abs/2602.09009v1", "date": "2026-02-09", "relevancy": 2.2289, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5898}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5582}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANCRe%3A%20Adaptive%20Neural%20Connection%20Reassignment%20for%20Efficient%20Depth%20Scaling&body=Title%3A%20ANCRe%3A%20Adaptive%20Neural%20Connection%20Reassignment%20for%20Efficient%20Depth%20Scaling%0AAuthor%3A%20Yilang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He%20and%20Georgios%20B.%20Giannakis%0AAbstract%3A%20Scaling%20network%20depth%20has%20been%20a%20central%20driver%20behind%20the%20success%20of%20modern%20foundation%20models%2C%20yet%20recent%20investigations%20suggest%20that%20deep%20layers%20are%20often%20underutilized.%20This%20paper%20revisits%20the%20default%20mechanism%20for%20deepening%20neural%20networks%2C%20namely%20residual%20connections%2C%20from%20an%20optimization%20perspective.%20Rigorous%20analysis%20proves%20that%20the%20layout%20of%20residual%20connections%20can%20fundamentally%20shape%20convergence%20behavior%2C%20and%20even%20induces%20an%20exponential%20gap%20in%20convergence%20rates.%20Prompted%20by%20this%20insight%2C%20we%20introduce%20adaptive%20neural%20connection%20reassignment%20%28ANCRe%29%2C%20a%20principled%20and%20lightweight%20framework%20that%20parameterizes%20and%20learns%20residual%20connectivities%20from%20the%20data.%20ANCRe%20adaptively%20reassigns%20residual%20connections%20with%20negligible%20computational%20and%20memory%20overhead%20%28%24%3C1%5C%25%24%29%2C%20while%20enabling%20more%20effective%20utilization%20of%20network%20depth.%20Extensive%20numerical%20tests%20across%20pre-training%20of%20large%20language%20models%2C%20diffusion%20models%2C%20and%20deep%20ResNets%20demonstrate%20consistently%20accelerated%20convergence%2C%20boosted%20performance%2C%20and%20enhanced%20depth%20efficiency%20over%20conventional%20residual%20connections.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANCRe%253A%2520Adaptive%2520Neural%2520Connection%2520Reassignment%2520for%2520Efficient%2520Depth%2520Scaling%26entry.906535625%3DYilang%2520Zhang%2520and%2520Bingcong%2520Li%2520and%2520Niao%2520He%2520and%2520Georgios%2520B.%2520Giannakis%26entry.1292438233%3DScaling%2520network%2520depth%2520has%2520been%2520a%2520central%2520driver%2520behind%2520the%2520success%2520of%2520modern%2520foundation%2520models%252C%2520yet%2520recent%2520investigations%2520suggest%2520that%2520deep%2520layers%2520are%2520often%2520underutilized.%2520This%2520paper%2520revisits%2520the%2520default%2520mechanism%2520for%2520deepening%2520neural%2520networks%252C%2520namely%2520residual%2520connections%252C%2520from%2520an%2520optimization%2520perspective.%2520Rigorous%2520analysis%2520proves%2520that%2520the%2520layout%2520of%2520residual%2520connections%2520can%2520fundamentally%2520shape%2520convergence%2520behavior%252C%2520and%2520even%2520induces%2520an%2520exponential%2520gap%2520in%2520convergence%2520rates.%2520Prompted%2520by%2520this%2520insight%252C%2520we%2520introduce%2520adaptive%2520neural%2520connection%2520reassignment%2520%2528ANCRe%2529%252C%2520a%2520principled%2520and%2520lightweight%2520framework%2520that%2520parameterizes%2520and%2520learns%2520residual%2520connectivities%2520from%2520the%2520data.%2520ANCRe%2520adaptively%2520reassigns%2520residual%2520connections%2520with%2520negligible%2520computational%2520and%2520memory%2520overhead%2520%2528%2524%253C1%255C%2525%2524%2529%252C%2520while%2520enabling%2520more%2520effective%2520utilization%2520of%2520network%2520depth.%2520Extensive%2520numerical%2520tests%2520across%2520pre-training%2520of%2520large%2520language%2520models%252C%2520diffusion%2520models%252C%2520and%2520deep%2520ResNets%2520demonstrate%2520consistently%2520accelerated%2520convergence%252C%2520boosted%2520performance%252C%2520and%2520enhanced%2520depth%2520efficiency%2520over%2520conventional%2520residual%2520connections.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANCRe%3A%20Adaptive%20Neural%20Connection%20Reassignment%20for%20Efficient%20Depth%20Scaling&entry.906535625=Yilang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He%20and%20Georgios%20B.%20Giannakis&entry.1292438233=Scaling%20network%20depth%20has%20been%20a%20central%20driver%20behind%20the%20success%20of%20modern%20foundation%20models%2C%20yet%20recent%20investigations%20suggest%20that%20deep%20layers%20are%20often%20underutilized.%20This%20paper%20revisits%20the%20default%20mechanism%20for%20deepening%20neural%20networks%2C%20namely%20residual%20connections%2C%20from%20an%20optimization%20perspective.%20Rigorous%20analysis%20proves%20that%20the%20layout%20of%20residual%20connections%20can%20fundamentally%20shape%20convergence%20behavior%2C%20and%20even%20induces%20an%20exponential%20gap%20in%20convergence%20rates.%20Prompted%20by%20this%20insight%2C%20we%20introduce%20adaptive%20neural%20connection%20reassignment%20%28ANCRe%29%2C%20a%20principled%20and%20lightweight%20framework%20that%20parameterizes%20and%20learns%20residual%20connectivities%20from%20the%20data.%20ANCRe%20adaptively%20reassigns%20residual%20connections%20with%20negligible%20computational%20and%20memory%20overhead%20%28%24%3C1%5C%25%24%29%2C%20while%20enabling%20more%20effective%20utilization%20of%20network%20depth.%20Extensive%20numerical%20tests%20across%20pre-training%20of%20large%20language%20models%2C%20diffusion%20models%2C%20and%20deep%20ResNets%20demonstrate%20consistently%20accelerated%20convergence%2C%20boosted%20performance%2C%20and%20enhanced%20depth%20efficiency%20over%20conventional%20residual%20connections.&entry.1838667208=http%3A//arxiv.org/abs/2602.09009v1&entry.124074799=Read"},
{"title": "Latent Domain Modeling Improves Robustness to Geographic Shifts", "author": "Ruth Crasto and Esther Rolf", "abstract": "Geographic distribution shift arises when the distribution of locations on Earth in a training dataset is different from what is seen at inference time. Using standard empirical risk minimization (ERM) in this setting can lead to uneven generalization across different spatially-determined groups of interest such as continents or biomes. The most common approaches to tackling geographic distribution shift apply domain adaptation methods using discrete group labels, ignoring geographic coordinates that are often available as metadata. On the other hand, modeling methods that integrate geographic coordinates have been shown to improve overall performance, but their impact on geographic domain generalization has not been studied. In this work, we propose a general modeling framework for improving robustness to geographic distribution shift. The key idea is to model continuous, latent domain assignment using location encoders and to condition the main task predictor on the jointly-trained latents. On four diverse geo-tagged image datasets with different group splits, we show that instances of our framework achieve significant improvements in worst-group performance compared to existing domain adaptation and location-aware modeling methods. In particular, we achieve new state-of-the-art results on two datasets from the WILDS benchmark.", "link": "http://arxiv.org/abs/2503.02036v3", "date": "2026-02-09", "relevancy": 2.2238, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5786}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5438}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Domain%20Modeling%20Improves%20Robustness%20to%20Geographic%20Shifts&body=Title%3A%20Latent%20Domain%20Modeling%20Improves%20Robustness%20to%20Geographic%20Shifts%0AAuthor%3A%20Ruth%20Crasto%20and%20Esther%20Rolf%0AAbstract%3A%20Geographic%20distribution%20shift%20arises%20when%20the%20distribution%20of%20locations%20on%20Earth%20in%20a%20training%20dataset%20is%20different%20from%20what%20is%20seen%20at%20inference%20time.%20Using%20standard%20empirical%20risk%20minimization%20%28ERM%29%20in%20this%20setting%20can%20lead%20to%20uneven%20generalization%20across%20different%20spatially-determined%20groups%20of%20interest%20such%20as%20continents%20or%20biomes.%20The%20most%20common%20approaches%20to%20tackling%20geographic%20distribution%20shift%20apply%20domain%20adaptation%20methods%20using%20discrete%20group%20labels%2C%20ignoring%20geographic%20coordinates%20that%20are%20often%20available%20as%20metadata.%20On%20the%20other%20hand%2C%20modeling%20methods%20that%20integrate%20geographic%20coordinates%20have%20been%20shown%20to%20improve%20overall%20performance%2C%20but%20their%20impact%20on%20geographic%20domain%20generalization%20has%20not%20been%20studied.%20In%20this%20work%2C%20we%20propose%20a%20general%20modeling%20framework%20for%20improving%20robustness%20to%20geographic%20distribution%20shift.%20The%20key%20idea%20is%20to%20model%20continuous%2C%20latent%20domain%20assignment%20using%20location%20encoders%20and%20to%20condition%20the%20main%20task%20predictor%20on%20the%20jointly-trained%20latents.%20On%20four%20diverse%20geo-tagged%20image%20datasets%20with%20different%20group%20splits%2C%20we%20show%20that%20instances%20of%20our%20framework%20achieve%20significant%20improvements%20in%20worst-group%20performance%20compared%20to%20existing%20domain%20adaptation%20and%20location-aware%20modeling%20methods.%20In%20particular%2C%20we%20achieve%20new%20state-of-the-art%20results%20on%20two%20datasets%20from%20the%20WILDS%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2503.02036v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Domain%2520Modeling%2520Improves%2520Robustness%2520to%2520Geographic%2520Shifts%26entry.906535625%3DRuth%2520Crasto%2520and%2520Esther%2520Rolf%26entry.1292438233%3DGeographic%2520distribution%2520shift%2520arises%2520when%2520the%2520distribution%2520of%2520locations%2520on%2520Earth%2520in%2520a%2520training%2520dataset%2520is%2520different%2520from%2520what%2520is%2520seen%2520at%2520inference%2520time.%2520Using%2520standard%2520empirical%2520risk%2520minimization%2520%2528ERM%2529%2520in%2520this%2520setting%2520can%2520lead%2520to%2520uneven%2520generalization%2520across%2520different%2520spatially-determined%2520groups%2520of%2520interest%2520such%2520as%2520continents%2520or%2520biomes.%2520The%2520most%2520common%2520approaches%2520to%2520tackling%2520geographic%2520distribution%2520shift%2520apply%2520domain%2520adaptation%2520methods%2520using%2520discrete%2520group%2520labels%252C%2520ignoring%2520geographic%2520coordinates%2520that%2520are%2520often%2520available%2520as%2520metadata.%2520On%2520the%2520other%2520hand%252C%2520modeling%2520methods%2520that%2520integrate%2520geographic%2520coordinates%2520have%2520been%2520shown%2520to%2520improve%2520overall%2520performance%252C%2520but%2520their%2520impact%2520on%2520geographic%2520domain%2520generalization%2520has%2520not%2520been%2520studied.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%2520modeling%2520framework%2520for%2520improving%2520robustness%2520to%2520geographic%2520distribution%2520shift.%2520The%2520key%2520idea%2520is%2520to%2520model%2520continuous%252C%2520latent%2520domain%2520assignment%2520using%2520location%2520encoders%2520and%2520to%2520condition%2520the%2520main%2520task%2520predictor%2520on%2520the%2520jointly-trained%2520latents.%2520On%2520four%2520diverse%2520geo-tagged%2520image%2520datasets%2520with%2520different%2520group%2520splits%252C%2520we%2520show%2520that%2520instances%2520of%2520our%2520framework%2520achieve%2520significant%2520improvements%2520in%2520worst-group%2520performance%2520compared%2520to%2520existing%2520domain%2520adaptation%2520and%2520location-aware%2520modeling%2520methods.%2520In%2520particular%252C%2520we%2520achieve%2520new%2520state-of-the-art%2520results%2520on%2520two%2520datasets%2520from%2520the%2520WILDS%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02036v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Domain%20Modeling%20Improves%20Robustness%20to%20Geographic%20Shifts&entry.906535625=Ruth%20Crasto%20and%20Esther%20Rolf&entry.1292438233=Geographic%20distribution%20shift%20arises%20when%20the%20distribution%20of%20locations%20on%20Earth%20in%20a%20training%20dataset%20is%20different%20from%20what%20is%20seen%20at%20inference%20time.%20Using%20standard%20empirical%20risk%20minimization%20%28ERM%29%20in%20this%20setting%20can%20lead%20to%20uneven%20generalization%20across%20different%20spatially-determined%20groups%20of%20interest%20such%20as%20continents%20or%20biomes.%20The%20most%20common%20approaches%20to%20tackling%20geographic%20distribution%20shift%20apply%20domain%20adaptation%20methods%20using%20discrete%20group%20labels%2C%20ignoring%20geographic%20coordinates%20that%20are%20often%20available%20as%20metadata.%20On%20the%20other%20hand%2C%20modeling%20methods%20that%20integrate%20geographic%20coordinates%20have%20been%20shown%20to%20improve%20overall%20performance%2C%20but%20their%20impact%20on%20geographic%20domain%20generalization%20has%20not%20been%20studied.%20In%20this%20work%2C%20we%20propose%20a%20general%20modeling%20framework%20for%20improving%20robustness%20to%20geographic%20distribution%20shift.%20The%20key%20idea%20is%20to%20model%20continuous%2C%20latent%20domain%20assignment%20using%20location%20encoders%20and%20to%20condition%20the%20main%20task%20predictor%20on%20the%20jointly-trained%20latents.%20On%20four%20diverse%20geo-tagged%20image%20datasets%20with%20different%20group%20splits%2C%20we%20show%20that%20instances%20of%20our%20framework%20achieve%20significant%20improvements%20in%20worst-group%20performance%20compared%20to%20existing%20domain%20adaptation%20and%20location-aware%20modeling%20methods.%20In%20particular%2C%20we%20achieve%20new%20state-of-the-art%20results%20on%20two%20datasets%20from%20the%20WILDS%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2503.02036v3&entry.124074799=Read"},
{"title": "A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation", "author": "Kenghou Hoi and Yuze Wu and Annan Ding and Junjie Wang and Anke Zhao and Chengqian Zhang and Fei Gao", "abstract": "Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.", "link": "http://arxiv.org/abs/2602.08599v1", "date": "2026-02-09", "relevancy": 2.2171, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5684}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Precise%20Real-Time%20Force-Aware%20Grasping%20System%20for%20Robust%20Aerial%20Manipulation&body=Title%3A%20A%20Precise%20Real-Time%20Force-Aware%20Grasping%20System%20for%20Robust%20Aerial%20Manipulation%0AAuthor%3A%20Kenghou%20Hoi%20and%20Yuze%20Wu%20and%20Annan%20Ding%20and%20Junjie%20Wang%20and%20Anke%20Zhao%20and%20Chengqian%20Zhang%20and%20Fei%20Gao%0AAbstract%3A%20Aerial%20manipulation%20requires%20force-aware%20capabilities%20to%20enable%20safe%20and%20effective%20grasping%20and%20physical%20interaction.%20Previous%20works%20often%20rely%20on%20heavy%2C%20expensive%20force%20sensors%20unsuitable%20for%20typical%20quadrotor%20platforms%2C%20or%20perform%20grasping%20without%20force%20feedback%2C%20risking%20damage%20to%20fragile%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20force-aware%20grasping%20framework%20incorporating%20six%20low-cost%2C%20sensitive%20skin-like%20tactile%20sensors.%20We%20introduce%20a%20magnetic-based%20tactile%20sensing%20module%20that%20provides%20high-precision%20three-dimensional%20force%20measurements.%20We%20eliminate%20geomagnetic%20interference%20through%20a%20reference%20Hall%20sensor%20and%20simplify%20the%20calibration%20process%20compared%20to%20previous%20work.%20The%20proposed%20framework%20enables%20precise%20force-aware%20grasping%20control%2C%20allowing%20safe%20manipulation%20of%20fragile%20objects%20and%20real-time%20weight%20measurement%20of%20grasped%20items.%20The%20system%20is%20validated%20through%20comprehensive%20real-world%20experiments%2C%20including%20balloon%20grasping%2C%20dynamic%20load%20variation%20tests%2C%20and%20ablation%20studies%2C%20demonstrating%20its%20effectiveness%20in%20various%20aerial%20manipulation%20scenarios.%20Our%20approach%20achieves%20fully%20onboard%20operation%20without%20external%20motion%20capture%20systems%2C%20significantly%20enhancing%20the%20practicality%20of%20force-sensitive%20aerial%20manipulation.%20The%20supplementary%20video%20is%20available%20at%3A%20https%3A//www.youtube.com/watch%3Fv%3DmbcZkrJEf1I.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Precise%2520Real-Time%2520Force-Aware%2520Grasping%2520System%2520for%2520Robust%2520Aerial%2520Manipulation%26entry.906535625%3DKenghou%2520Hoi%2520and%2520Yuze%2520Wu%2520and%2520Annan%2520Ding%2520and%2520Junjie%2520Wang%2520and%2520Anke%2520Zhao%2520and%2520Chengqian%2520Zhang%2520and%2520Fei%2520Gao%26entry.1292438233%3DAerial%2520manipulation%2520requires%2520force-aware%2520capabilities%2520to%2520enable%2520safe%2520and%2520effective%2520grasping%2520and%2520physical%2520interaction.%2520Previous%2520works%2520often%2520rely%2520on%2520heavy%252C%2520expensive%2520force%2520sensors%2520unsuitable%2520for%2520typical%2520quadrotor%2520platforms%252C%2520or%2520perform%2520grasping%2520without%2520force%2520feedback%252C%2520risking%2520damage%2520to%2520fragile%2520objects.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520force-aware%2520grasping%2520framework%2520incorporating%2520six%2520low-cost%252C%2520sensitive%2520skin-like%2520tactile%2520sensors.%2520We%2520introduce%2520a%2520magnetic-based%2520tactile%2520sensing%2520module%2520that%2520provides%2520high-precision%2520three-dimensional%2520force%2520measurements.%2520We%2520eliminate%2520geomagnetic%2520interference%2520through%2520a%2520reference%2520Hall%2520sensor%2520and%2520simplify%2520the%2520calibration%2520process%2520compared%2520to%2520previous%2520work.%2520The%2520proposed%2520framework%2520enables%2520precise%2520force-aware%2520grasping%2520control%252C%2520allowing%2520safe%2520manipulation%2520of%2520fragile%2520objects%2520and%2520real-time%2520weight%2520measurement%2520of%2520grasped%2520items.%2520The%2520system%2520is%2520validated%2520through%2520comprehensive%2520real-world%2520experiments%252C%2520including%2520balloon%2520grasping%252C%2520dynamic%2520load%2520variation%2520tests%252C%2520and%2520ablation%2520studies%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520various%2520aerial%2520manipulation%2520scenarios.%2520Our%2520approach%2520achieves%2520fully%2520onboard%2520operation%2520without%2520external%2520motion%2520capture%2520systems%252C%2520significantly%2520enhancing%2520the%2520practicality%2520of%2520force-sensitive%2520aerial%2520manipulation.%2520The%2520supplementary%2520video%2520is%2520available%2520at%253A%2520https%253A//www.youtube.com/watch%253Fv%253DmbcZkrJEf1I.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Precise%20Real-Time%20Force-Aware%20Grasping%20System%20for%20Robust%20Aerial%20Manipulation&entry.906535625=Kenghou%20Hoi%20and%20Yuze%20Wu%20and%20Annan%20Ding%20and%20Junjie%20Wang%20and%20Anke%20Zhao%20and%20Chengqian%20Zhang%20and%20Fei%20Gao&entry.1292438233=Aerial%20manipulation%20requires%20force-aware%20capabilities%20to%20enable%20safe%20and%20effective%20grasping%20and%20physical%20interaction.%20Previous%20works%20often%20rely%20on%20heavy%2C%20expensive%20force%20sensors%20unsuitable%20for%20typical%20quadrotor%20platforms%2C%20or%20perform%20grasping%20without%20force%20feedback%2C%20risking%20damage%20to%20fragile%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20force-aware%20grasping%20framework%20incorporating%20six%20low-cost%2C%20sensitive%20skin-like%20tactile%20sensors.%20We%20introduce%20a%20magnetic-based%20tactile%20sensing%20module%20that%20provides%20high-precision%20three-dimensional%20force%20measurements.%20We%20eliminate%20geomagnetic%20interference%20through%20a%20reference%20Hall%20sensor%20and%20simplify%20the%20calibration%20process%20compared%20to%20previous%20work.%20The%20proposed%20framework%20enables%20precise%20force-aware%20grasping%20control%2C%20allowing%20safe%20manipulation%20of%20fragile%20objects%20and%20real-time%20weight%20measurement%20of%20grasped%20items.%20The%20system%20is%20validated%20through%20comprehensive%20real-world%20experiments%2C%20including%20balloon%20grasping%2C%20dynamic%20load%20variation%20tests%2C%20and%20ablation%20studies%2C%20demonstrating%20its%20effectiveness%20in%20various%20aerial%20manipulation%20scenarios.%20Our%20approach%20achieves%20fully%20onboard%20operation%20without%20external%20motion%20capture%20systems%2C%20significantly%20enhancing%20the%20practicality%20of%20force-sensitive%20aerial%20manipulation.%20The%20supplementary%20video%20is%20available%20at%3A%20https%3A//www.youtube.com/watch%3Fv%3DmbcZkrJEf1I.&entry.1838667208=http%3A//arxiv.org/abs/2602.08599v1&entry.124074799=Read"},
{"title": "UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach", "author": "Dao Lan Vy Dinh and Anh Nguyen Thi Mai and Hung Tran and Giang Quynh Le Vu and Tu Dac Ho and Zhenni Pan and Vo Nhan Van and Symeon Chatzinotas and Dinh-Hieu Tran", "abstract": "This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.", "link": "http://arxiv.org/abs/2511.07366v2", "date": "2026-02-09", "relevancy": 2.2165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4454}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV-Assisted%20Resilience%20in%206G%20and%20Beyond%20Network%20Energy%20Saving%3A%20A%20Multi-Agent%20DRL%20Approach&body=Title%3A%20UAV-Assisted%20Resilience%20in%206G%20and%20Beyond%20Network%20Energy%20Saving%3A%20A%20Multi-Agent%20DRL%20Approach%0AAuthor%3A%20Dao%20Lan%20Vy%20Dinh%20and%20Anh%20Nguyen%20Thi%20Mai%20and%20Hung%20Tran%20and%20Giang%20Quynh%20Le%20Vu%20and%20Tu%20Dac%20Ho%20and%20Zhenni%20Pan%20and%20Vo%20Nhan%20Van%20and%20Symeon%20Chatzinotas%20and%20Dinh-Hieu%20Tran%0AAbstract%3A%20This%20paper%20investigates%20the%20unmanned%20aerial%20vehicle%20%28UAV%29-assisted%20resilience%20perspective%20in%20the%206G%20network%20energy%20saving%20%28NES%29%20scenario.%20More%20specifically%2C%20we%20consider%20multiple%20ground%20base%20stations%20%28GBSs%29%20and%20each%20GBS%20has%20three%20different%20sectors/cells%20in%20the%20terrestrial%20networks%2C%20and%20multiple%20cells%20are%20turned%20off%20due%20to%20NES%20or%20incidents%2C%20e.g.%2C%20disasters%2C%20hardware%20failures%2C%20or%20outages.%20To%20address%20this%2C%20we%20propose%20a%20Multi-Agent%20Deep%20Deterministic%20Policy%20Gradient%20%28MADDPG%29%20framework%20to%20enable%20UAV-assisted%20communication%20by%20jointly%20optimizing%20UAV%20trajectories%2C%20transmission%20power%2C%20and%20user-UAV%20association%20under%20a%20sleeping%20ground%20base%20station%20%28GBS%29%20strategy.%20This%20framework%20aims%20to%20ensure%20the%20resilience%20of%20active%20users%20in%20the%20network%20and%20the%20long-term%20operability%20of%20UAVs.%20Specifically%2C%20it%20maximizes%20service%20coverage%20for%20users%20during%20power%20outages%20or%20NES%20zones%2C%20while%20minimizing%20the%20energy%20consumption%20of%20UAVs.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20MADDPG%20policy%20consistently%20achieves%20high%20coverage%20ratio%20across%20different%20testing%20episodes%2C%20outperforming%20other%20baselines.%20Moreover%2C%20the%20MADDPG%20framework%20attains%20the%20lowest%20total%20energy%20consumption%2C%20with%20a%20reduction%20of%20approximately%2024%5C%25%20compared%20to%20the%20conventional%20all%20GBS%20ON%20configuration%2C%20while%20maintaining%20a%20comparable%20user%20service%20rate.%20These%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20achieving%20a%20superior%20trade-off%20between%20energy%20efficiency%20and%20service%20performance%2C%20supporting%20the%20development%20of%20sustainable%20and%20resilient%20UAV-assisted%20cellular%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV-Assisted%2520Resilience%2520in%25206G%2520and%2520Beyond%2520Network%2520Energy%2520Saving%253A%2520A%2520Multi-Agent%2520DRL%2520Approach%26entry.906535625%3DDao%2520Lan%2520Vy%2520Dinh%2520and%2520Anh%2520Nguyen%2520Thi%2520Mai%2520and%2520Hung%2520Tran%2520and%2520Giang%2520Quynh%2520Le%2520Vu%2520and%2520Tu%2520Dac%2520Ho%2520and%2520Zhenni%2520Pan%2520and%2520Vo%2520Nhan%2520Van%2520and%2520Symeon%2520Chatzinotas%2520and%2520Dinh-Hieu%2520Tran%26entry.1292438233%3DThis%2520paper%2520investigates%2520the%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529-assisted%2520resilience%2520perspective%2520in%2520the%25206G%2520network%2520energy%2520saving%2520%2528NES%2529%2520scenario.%2520More%2520specifically%252C%2520we%2520consider%2520multiple%2520ground%2520base%2520stations%2520%2528GBSs%2529%2520and%2520each%2520GBS%2520has%2520three%2520different%2520sectors/cells%2520in%2520the%2520terrestrial%2520networks%252C%2520and%2520multiple%2520cells%2520are%2520turned%2520off%2520due%2520to%2520NES%2520or%2520incidents%252C%2520e.g.%252C%2520disasters%252C%2520hardware%2520failures%252C%2520or%2520outages.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Multi-Agent%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528MADDPG%2529%2520framework%2520to%2520enable%2520UAV-assisted%2520communication%2520by%2520jointly%2520optimizing%2520UAV%2520trajectories%252C%2520transmission%2520power%252C%2520and%2520user-UAV%2520association%2520under%2520a%2520sleeping%2520ground%2520base%2520station%2520%2528GBS%2529%2520strategy.%2520This%2520framework%2520aims%2520to%2520ensure%2520the%2520resilience%2520of%2520active%2520users%2520in%2520the%2520network%2520and%2520the%2520long-term%2520operability%2520of%2520UAVs.%2520Specifically%252C%2520it%2520maximizes%2520service%2520coverage%2520for%2520users%2520during%2520power%2520outages%2520or%2520NES%2520zones%252C%2520while%2520minimizing%2520the%2520energy%2520consumption%2520of%2520UAVs.%2520Simulation%2520results%2520demonstrate%2520that%2520the%2520proposed%2520MADDPG%2520policy%2520consistently%2520achieves%2520high%2520coverage%2520ratio%2520across%2520different%2520testing%2520episodes%252C%2520outperforming%2520other%2520baselines.%2520Moreover%252C%2520the%2520MADDPG%2520framework%2520attains%2520the%2520lowest%2520total%2520energy%2520consumption%252C%2520with%2520a%2520reduction%2520of%2520approximately%252024%255C%2525%2520compared%2520to%2520the%2520conventional%2520all%2520GBS%2520ON%2520configuration%252C%2520while%2520maintaining%2520a%2520comparable%2520user%2520service%2520rate.%2520These%2520results%2520confirm%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520in%2520achieving%2520a%2520superior%2520trade-off%2520between%2520energy%2520efficiency%2520and%2520service%2520performance%252C%2520supporting%2520the%2520development%2520of%2520sustainable%2520and%2520resilient%2520UAV-assisted%2520cellular%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV-Assisted%20Resilience%20in%206G%20and%20Beyond%20Network%20Energy%20Saving%3A%20A%20Multi-Agent%20DRL%20Approach&entry.906535625=Dao%20Lan%20Vy%20Dinh%20and%20Anh%20Nguyen%20Thi%20Mai%20and%20Hung%20Tran%20and%20Giang%20Quynh%20Le%20Vu%20and%20Tu%20Dac%20Ho%20and%20Zhenni%20Pan%20and%20Vo%20Nhan%20Van%20and%20Symeon%20Chatzinotas%20and%20Dinh-Hieu%20Tran&entry.1292438233=This%20paper%20investigates%20the%20unmanned%20aerial%20vehicle%20%28UAV%29-assisted%20resilience%20perspective%20in%20the%206G%20network%20energy%20saving%20%28NES%29%20scenario.%20More%20specifically%2C%20we%20consider%20multiple%20ground%20base%20stations%20%28GBSs%29%20and%20each%20GBS%20has%20three%20different%20sectors/cells%20in%20the%20terrestrial%20networks%2C%20and%20multiple%20cells%20are%20turned%20off%20due%20to%20NES%20or%20incidents%2C%20e.g.%2C%20disasters%2C%20hardware%20failures%2C%20or%20outages.%20To%20address%20this%2C%20we%20propose%20a%20Multi-Agent%20Deep%20Deterministic%20Policy%20Gradient%20%28MADDPG%29%20framework%20to%20enable%20UAV-assisted%20communication%20by%20jointly%20optimizing%20UAV%20trajectories%2C%20transmission%20power%2C%20and%20user-UAV%20association%20under%20a%20sleeping%20ground%20base%20station%20%28GBS%29%20strategy.%20This%20framework%20aims%20to%20ensure%20the%20resilience%20of%20active%20users%20in%20the%20network%20and%20the%20long-term%20operability%20of%20UAVs.%20Specifically%2C%20it%20maximizes%20service%20coverage%20for%20users%20during%20power%20outages%20or%20NES%20zones%2C%20while%20minimizing%20the%20energy%20consumption%20of%20UAVs.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20MADDPG%20policy%20consistently%20achieves%20high%20coverage%20ratio%20across%20different%20testing%20episodes%2C%20outperforming%20other%20baselines.%20Moreover%2C%20the%20MADDPG%20framework%20attains%20the%20lowest%20total%20energy%20consumption%2C%20with%20a%20reduction%20of%20approximately%2024%5C%25%20compared%20to%20the%20conventional%20all%20GBS%20ON%20configuration%2C%20while%20maintaining%20a%20comparable%20user%20service%20rate.%20These%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20achieving%20a%20superior%20trade-off%20between%20energy%20efficiency%20and%20service%20performance%2C%20supporting%20the%20development%20of%20sustainable%20and%20resilient%20UAV-assisted%20cellular%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2511.07366v2&entry.124074799=Read"},
{"title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images", "author": "Farnaz Khun Jush and Grit Werner and Mark Klemens and Matthias Lenga", "abstract": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.", "link": "http://arxiv.org/abs/2602.08717v1", "date": "2026-02-09", "relevancy": 2.2152, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20System%20for%20Automatic%20Body%20Region%20Detection%20for%20Volumetric%20CT%20and%20MR%20Images&body=Title%3A%20Zero-shot%20System%20for%20Automatic%20Body%20Region%20Detection%20for%20Volumetric%20CT%20and%20MR%20Images%0AAuthor%3A%20Farnaz%20Khun%20Jush%20and%20Grit%20Werner%20and%20Mark%20Klemens%20and%20Matthias%20Lenga%0AAbstract%3A%20Reliable%20identification%20of%20anatomical%20body%20regions%20is%20a%20prerequisite%20for%20many%20automated%20medical%20imaging%20workflows%2C%20yet%20existing%20solutions%20remain%20heavily%20dependent%20on%20unreliable%20DICOM%20metadata.%20Current%20solutions%20mainly%20use%20supervised%20learning%2C%20which%20limits%20their%20applicability%20in%20many%20real-world%20scenarios.%20In%20this%20work%2C%20we%20investigate%20whether%20body%20region%20detection%20in%20volumetric%20CT%20and%20MR%20images%20can%20be%20achieved%20in%20a%20fully%20zero-shot%20manner%20by%20using%20knowledge%20embedded%20in%20large%20pre-trained%20foundation%20models.%20We%20propose%20and%20systematically%20evaluate%20three%20training-free%20pipelines%3A%20%281%29%20a%20segmentation-driven%20rule-based%20system%20leveraging%20pre-trained%20multi-organ%20segmentation%20models%2C%20%282%29%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20guided%20by%20radiologist-defined%20rules%2C%20and%20%283%29%20a%20segmentation-aware%20MLLM%20that%20combines%20visual%20input%20with%20explicit%20anatomical%20evidence.%20All%20methods%20are%20evaluated%20on%20887%20heterogeneous%20CT%20and%20MR%20scans%20with%20manually%20verified%20anatomical%20region%20labels.%20The%20segmentation-driven%20rule-based%20approach%20achieves%20the%20strongest%20and%20most%20consistent%20performance%2C%20with%20weighted%20F1-scores%20of%200.947%20%28CT%29%20and%200.914%20%28MR%29%2C%20demonstrating%20robustness%20across%20modalities%20and%20atypical%20scan%20coverage.%20The%20MLLM%20performs%20competitively%20in%20visually%20distinctive%20regions%2C%20while%20the%20segmentation-aware%20MLLM%20reveals%20fundamental%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520System%2520for%2520Automatic%2520Body%2520Region%2520Detection%2520for%2520Volumetric%2520CT%2520and%2520MR%2520Images%26entry.906535625%3DFarnaz%2520Khun%2520Jush%2520and%2520Grit%2520Werner%2520and%2520Mark%2520Klemens%2520and%2520Matthias%2520Lenga%26entry.1292438233%3DReliable%2520identification%2520of%2520anatomical%2520body%2520regions%2520is%2520a%2520prerequisite%2520for%2520many%2520automated%2520medical%2520imaging%2520workflows%252C%2520yet%2520existing%2520solutions%2520remain%2520heavily%2520dependent%2520on%2520unreliable%2520DICOM%2520metadata.%2520Current%2520solutions%2520mainly%2520use%2520supervised%2520learning%252C%2520which%2520limits%2520their%2520applicability%2520in%2520many%2520real-world%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520body%2520region%2520detection%2520in%2520volumetric%2520CT%2520and%2520MR%2520images%2520can%2520be%2520achieved%2520in%2520a%2520fully%2520zero-shot%2520manner%2520by%2520using%2520knowledge%2520embedded%2520in%2520large%2520pre-trained%2520foundation%2520models.%2520We%2520propose%2520and%2520systematically%2520evaluate%2520three%2520training-free%2520pipelines%253A%2520%25281%2529%2520a%2520segmentation-driven%2520rule-based%2520system%2520leveraging%2520pre-trained%2520multi-organ%2520segmentation%2520models%252C%2520%25282%2529%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520guided%2520by%2520radiologist-defined%2520rules%252C%2520and%2520%25283%2529%2520a%2520segmentation-aware%2520MLLM%2520that%2520combines%2520visual%2520input%2520with%2520explicit%2520anatomical%2520evidence.%2520All%2520methods%2520are%2520evaluated%2520on%2520887%2520heterogeneous%2520CT%2520and%2520MR%2520scans%2520with%2520manually%2520verified%2520anatomical%2520region%2520labels.%2520The%2520segmentation-driven%2520rule-based%2520approach%2520achieves%2520the%2520strongest%2520and%2520most%2520consistent%2520performance%252C%2520with%2520weighted%2520F1-scores%2520of%25200.947%2520%2528CT%2529%2520and%25200.914%2520%2528MR%2529%252C%2520demonstrating%2520robustness%2520across%2520modalities%2520and%2520atypical%2520scan%2520coverage.%2520The%2520MLLM%2520performs%2520competitively%2520in%2520visually%2520distinctive%2520regions%252C%2520while%2520the%2520segmentation-aware%2520MLLM%2520reveals%2520fundamental%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20System%20for%20Automatic%20Body%20Region%20Detection%20for%20Volumetric%20CT%20and%20MR%20Images&entry.906535625=Farnaz%20Khun%20Jush%20and%20Grit%20Werner%20and%20Mark%20Klemens%20and%20Matthias%20Lenga&entry.1292438233=Reliable%20identification%20of%20anatomical%20body%20regions%20is%20a%20prerequisite%20for%20many%20automated%20medical%20imaging%20workflows%2C%20yet%20existing%20solutions%20remain%20heavily%20dependent%20on%20unreliable%20DICOM%20metadata.%20Current%20solutions%20mainly%20use%20supervised%20learning%2C%20which%20limits%20their%20applicability%20in%20many%20real-world%20scenarios.%20In%20this%20work%2C%20we%20investigate%20whether%20body%20region%20detection%20in%20volumetric%20CT%20and%20MR%20images%20can%20be%20achieved%20in%20a%20fully%20zero-shot%20manner%20by%20using%20knowledge%20embedded%20in%20large%20pre-trained%20foundation%20models.%20We%20propose%20and%20systematically%20evaluate%20three%20training-free%20pipelines%3A%20%281%29%20a%20segmentation-driven%20rule-based%20system%20leveraging%20pre-trained%20multi-organ%20segmentation%20models%2C%20%282%29%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20guided%20by%20radiologist-defined%20rules%2C%20and%20%283%29%20a%20segmentation-aware%20MLLM%20that%20combines%20visual%20input%20with%20explicit%20anatomical%20evidence.%20All%20methods%20are%20evaluated%20on%20887%20heterogeneous%20CT%20and%20MR%20scans%20with%20manually%20verified%20anatomical%20region%20labels.%20The%20segmentation-driven%20rule-based%20approach%20achieves%20the%20strongest%20and%20most%20consistent%20performance%2C%20with%20weighted%20F1-scores%20of%200.947%20%28CT%29%20and%200.914%20%28MR%29%2C%20demonstrating%20robustness%20across%20modalities%20and%20atypical%20scan%20coverage.%20The%20MLLM%20performs%20competitively%20in%20visually%20distinctive%20regions%2C%20while%20the%20segmentation-aware%20MLLM%20reveals%20fundamental%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2602.08717v1&entry.124074799=Read"},
{"title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer", "author": "Yisu Liu and Chenxing Li and Wanqian Zhang and Wenfu Wang and Meng Yu and Ruibo Fu and Zheng Lin and Weiping Wang and Dong Yu", "abstract": "Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.", "link": "http://arxiv.org/abs/2508.13786v2", "date": "2026-02-09", "relevancy": 2.215, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5936}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5464}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%20Diffusion%20Transformer&body=Title%3A%20DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%20Diffusion%20Transformer%0AAuthor%3A%20Yisu%20Liu%20and%20Chenxing%20Li%20and%20Wanqian%20Zhang%20and%20Wenfu%20Wang%20and%20Meng%20Yu%20and%20Ruibo%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang%20and%20Dong%20Yu%0AAbstract%3A%20Controllable%20text-to-audio%20generation%20aims%20to%20synthesize%20audio%20from%20textual%20descriptions%20while%20satisfying%20user-specified%20constraints%2C%20including%20event%20types%2C%20temporal%20sequences%2C%20and%20onset%20and%20offset%20timestamps.%20This%20enables%20precise%20control%20over%20both%20the%20content%20and%20temporal%20structure%20of%20the%20generated%20audio.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20face%20inherent%20trade-offs%20among%20accurate%20temporal%20localization%2C%20open-vocabulary%20scalability%2C%20and%20practical%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20DegDiT%2C%20a%20novel%20dynamic%20event%20graph-guided%20diffusion%20transformer%20framework%20for%20open-vocabulary%20controllable%20audio%20generation.%20DegDiT%20encodes%20the%20events%20in%20the%20description%20as%20structured%20dynamic%20graphs.%20The%20nodes%20in%20each%20graph%20are%20designed%20to%20represent%20three%20aspects%3A%20semantic%20features%2C%20temporal%20attributes%2C%20and%20inter-event%20connections.%20A%20graph%20transformer%20is%20employed%20to%20integrate%20these%20nodes%20and%20produce%20contextualized%20event%20embeddings%20that%20serve%20as%20guidance%20for%20the%20diffusion%20model.%20To%20ensure%20high-quality%20and%20diverse%20training%20data%2C%20we%20introduce%20a%20quality-balanced%20data%20selection%20pipeline%20that%20combines%20hierarchical%20event%20annotation%20with%20multi-criteria%20quality%20scoring%2C%20resulting%20in%20a%20curated%20dataset%20with%20semantic%20diversity.%20Furthermore%2C%20we%20present%20consensus%20preference%20optimization%2C%20facilitating%20audio%20generation%20through%20consensus%20among%20multiple%20reward%20signals.%20Extensive%20experiments%20on%20AudioCondition%2C%20DESED%2C%20and%20AudioTime%20datasets%20demonstrate%20that%20DegDiT%20achieves%20state-of-the-art%20performances%20across%20a%20variety%20of%20objective%20and%20subjective%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegDiT%253A%2520Controllable%2520Audio%2520Generation%2520with%2520Dynamic%2520Event%2520Graph%2520Guided%2520Diffusion%2520Transformer%26entry.906535625%3DYisu%2520Liu%2520and%2520Chenxing%2520Li%2520and%2520Wanqian%2520Zhang%2520and%2520Wenfu%2520Wang%2520and%2520Meng%2520Yu%2520and%2520Ruibo%2520Fu%2520and%2520Zheng%2520Lin%2520and%2520Weiping%2520Wang%2520and%2520Dong%2520Yu%26entry.1292438233%3DControllable%2520text-to-audio%2520generation%2520aims%2520to%2520synthesize%2520audio%2520from%2520textual%2520descriptions%2520while%2520satisfying%2520user-specified%2520constraints%252C%2520including%2520event%2520types%252C%2520temporal%2520sequences%252C%2520and%2520onset%2520and%2520offset%2520timestamps.%2520This%2520enables%2520precise%2520control%2520over%2520both%2520the%2520content%2520and%2520temporal%2520structure%2520of%2520the%2520generated%2520audio.%2520Despite%2520recent%2520progress%252C%2520existing%2520methods%2520still%2520face%2520inherent%2520trade-offs%2520among%2520accurate%2520temporal%2520localization%252C%2520open-vocabulary%2520scalability%252C%2520and%2520practical%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DegDiT%252C%2520a%2520novel%2520dynamic%2520event%2520graph-guided%2520diffusion%2520transformer%2520framework%2520for%2520open-vocabulary%2520controllable%2520audio%2520generation.%2520DegDiT%2520encodes%2520the%2520events%2520in%2520the%2520description%2520as%2520structured%2520dynamic%2520graphs.%2520The%2520nodes%2520in%2520each%2520graph%2520are%2520designed%2520to%2520represent%2520three%2520aspects%253A%2520semantic%2520features%252C%2520temporal%2520attributes%252C%2520and%2520inter-event%2520connections.%2520A%2520graph%2520transformer%2520is%2520employed%2520to%2520integrate%2520these%2520nodes%2520and%2520produce%2520contextualized%2520event%2520embeddings%2520that%2520serve%2520as%2520guidance%2520for%2520the%2520diffusion%2520model.%2520To%2520ensure%2520high-quality%2520and%2520diverse%2520training%2520data%252C%2520we%2520introduce%2520a%2520quality-balanced%2520data%2520selection%2520pipeline%2520that%2520combines%2520hierarchical%2520event%2520annotation%2520with%2520multi-criteria%2520quality%2520scoring%252C%2520resulting%2520in%2520a%2520curated%2520dataset%2520with%2520semantic%2520diversity.%2520Furthermore%252C%2520we%2520present%2520consensus%2520preference%2520optimization%252C%2520facilitating%2520audio%2520generation%2520through%2520consensus%2520among%2520multiple%2520reward%2520signals.%2520Extensive%2520experiments%2520on%2520AudioCondition%252C%2520DESED%252C%2520and%2520AudioTime%2520datasets%2520demonstrate%2520that%2520DegDiT%2520achieves%2520state-of-the-art%2520performances%2520across%2520a%2520variety%2520of%2520objective%2520and%2520subjective%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%20Diffusion%20Transformer&entry.906535625=Yisu%20Liu%20and%20Chenxing%20Li%20and%20Wanqian%20Zhang%20and%20Wenfu%20Wang%20and%20Meng%20Yu%20and%20Ruibo%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang%20and%20Dong%20Yu&entry.1292438233=Controllable%20text-to-audio%20generation%20aims%20to%20synthesize%20audio%20from%20textual%20descriptions%20while%20satisfying%20user-specified%20constraints%2C%20including%20event%20types%2C%20temporal%20sequences%2C%20and%20onset%20and%20offset%20timestamps.%20This%20enables%20precise%20control%20over%20both%20the%20content%20and%20temporal%20structure%20of%20the%20generated%20audio.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20face%20inherent%20trade-offs%20among%20accurate%20temporal%20localization%2C%20open-vocabulary%20scalability%2C%20and%20practical%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20DegDiT%2C%20a%20novel%20dynamic%20event%20graph-guided%20diffusion%20transformer%20framework%20for%20open-vocabulary%20controllable%20audio%20generation.%20DegDiT%20encodes%20the%20events%20in%20the%20description%20as%20structured%20dynamic%20graphs.%20The%20nodes%20in%20each%20graph%20are%20designed%20to%20represent%20three%20aspects%3A%20semantic%20features%2C%20temporal%20attributes%2C%20and%20inter-event%20connections.%20A%20graph%20transformer%20is%20employed%20to%20integrate%20these%20nodes%20and%20produce%20contextualized%20event%20embeddings%20that%20serve%20as%20guidance%20for%20the%20diffusion%20model.%20To%20ensure%20high-quality%20and%20diverse%20training%20data%2C%20we%20introduce%20a%20quality-balanced%20data%20selection%20pipeline%20that%20combines%20hierarchical%20event%20annotation%20with%20multi-criteria%20quality%20scoring%2C%20resulting%20in%20a%20curated%20dataset%20with%20semantic%20diversity.%20Furthermore%2C%20we%20present%20consensus%20preference%20optimization%2C%20facilitating%20audio%20generation%20through%20consensus%20among%20multiple%20reward%20signals.%20Extensive%20experiments%20on%20AudioCondition%2C%20DESED%2C%20and%20AudioTime%20datasets%20demonstrate%20that%20DegDiT%20achieves%20state-of-the-art%20performances%20across%20a%20variety%20of%20objective%20and%20subjective%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2508.13786v2&entry.124074799=Read"},
{"title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models", "author": "Xiangtian Zheng and Zishuo Wang and Yuxin Peng", "abstract": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.", "link": "http://arxiv.org/abs/2602.08861v1", "date": "2026-02-09", "relevancy": 2.2107, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.57}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiFRe%3A%20Text-guided%20Video%20Frame%20Reduction%20for%20Efficient%20Video%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20TiFRe%3A%20Text-guided%20Video%20Frame%20Reduction%20for%20Efficient%20Video%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Xiangtian%20Zheng%20and%20Zishuo%20Wang%20and%20Yuxin%20Peng%0AAbstract%3A%20With%20the%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20Video%20Multi-Modal%20Large%20Language%20Models%20%28Video%20MLLMs%29%20have%20achieved%20remarkable%20performance%20in%20video-language%20tasks%20such%20as%20video%20understanding%20and%20question%20answering.%20However%2C%20Video%20MLLMs%20face%20high%20computational%20costs%2C%20particularly%20in%20processing%20numerous%20video%20frames%20as%20input%2C%20which%20leads%20to%20significant%20attention%20computation%20overhead.%20A%20straightforward%20approach%20to%20reduce%20computational%20costs%20is%20to%20decrease%20the%20number%20of%20input%20video%20frames.%20However%2C%20simply%20selecting%20key%20frames%20at%20a%20fixed%20frame%20rate%20%28FPS%29%20often%20overlooks%20valuable%20information%20in%20non-key%20frames%2C%20resulting%20in%20notable%20performance%20degradation.%20To%20address%20this%2C%20we%20propose%20Text-guided%20Video%20Frame%20Reduction%20%28TiFRe%29%2C%20a%20framework%20that%20reduces%20input%20frames%20while%20preserving%20essential%20video%20information.%20TiFRe%20uses%20a%20Text-guided%20Frame%20Sampling%20%28TFS%29%20strategy%20to%20select%20key%20frames%20based%20on%20user%20input%2C%20which%20is%20processed%20by%20an%20LLM%20to%20generate%20a%20CLIP-style%20prompt.%20Pre-trained%20CLIP%20encoders%20calculate%20the%20semantic%20similarity%20between%20the%20prompt%20and%20each%20frame%2C%20selecting%20the%20most%20relevant%20frames%20as%20key%20frames.%20To%20preserve%20video%20semantics%2C%20TiFRe%20employs%20a%20Frame%20Matching%20and%20Merging%20%28FMM%29%20mechanism%2C%20which%20integrates%20non-key%20frame%20information%20into%20the%20selected%20key%20frames%2C%20minimizing%20information%20loss.%20Experiments%20show%20that%20TiFRe%20effectively%20reduces%20computational%20costs%20while%20improving%20performance%20on%20video-language%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiFRe%253A%2520Text-guided%2520Video%2520Frame%2520Reduction%2520for%2520Efficient%2520Video%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DXiangtian%2520Zheng%2520and%2520Zishuo%2520Wang%2520and%2520Yuxin%2520Peng%26entry.1292438233%3DWith%2520the%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Video%2520Multi-Modal%2520Large%2520Language%2520Models%2520%2528Video%2520MLLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520in%2520video-language%2520tasks%2520such%2520as%2520video%2520understanding%2520and%2520question%2520answering.%2520However%252C%2520Video%2520MLLMs%2520face%2520high%2520computational%2520costs%252C%2520particularly%2520in%2520processing%2520numerous%2520video%2520frames%2520as%2520input%252C%2520which%2520leads%2520to%2520significant%2520attention%2520computation%2520overhead.%2520A%2520straightforward%2520approach%2520to%2520reduce%2520computational%2520costs%2520is%2520to%2520decrease%2520the%2520number%2520of%2520input%2520video%2520frames.%2520However%252C%2520simply%2520selecting%2520key%2520frames%2520at%2520a%2520fixed%2520frame%2520rate%2520%2528FPS%2529%2520often%2520overlooks%2520valuable%2520information%2520in%2520non-key%2520frames%252C%2520resulting%2520in%2520notable%2520performance%2520degradation.%2520To%2520address%2520this%252C%2520we%2520propose%2520Text-guided%2520Video%2520Frame%2520Reduction%2520%2528TiFRe%2529%252C%2520a%2520framework%2520that%2520reduces%2520input%2520frames%2520while%2520preserving%2520essential%2520video%2520information.%2520TiFRe%2520uses%2520a%2520Text-guided%2520Frame%2520Sampling%2520%2528TFS%2529%2520strategy%2520to%2520select%2520key%2520frames%2520based%2520on%2520user%2520input%252C%2520which%2520is%2520processed%2520by%2520an%2520LLM%2520to%2520generate%2520a%2520CLIP-style%2520prompt.%2520Pre-trained%2520CLIP%2520encoders%2520calculate%2520the%2520semantic%2520similarity%2520between%2520the%2520prompt%2520and%2520each%2520frame%252C%2520selecting%2520the%2520most%2520relevant%2520frames%2520as%2520key%2520frames.%2520To%2520preserve%2520video%2520semantics%252C%2520TiFRe%2520employs%2520a%2520Frame%2520Matching%2520and%2520Merging%2520%2528FMM%2529%2520mechanism%252C%2520which%2520integrates%2520non-key%2520frame%2520information%2520into%2520the%2520selected%2520key%2520frames%252C%2520minimizing%2520information%2520loss.%2520Experiments%2520show%2520that%2520TiFRe%2520effectively%2520reduces%2520computational%2520costs%2520while%2520improving%2520performance%2520on%2520video-language%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiFRe%3A%20Text-guided%20Video%20Frame%20Reduction%20for%20Efficient%20Video%20Multi-modal%20Large%20Language%20Models&entry.906535625=Xiangtian%20Zheng%20and%20Zishuo%20Wang%20and%20Yuxin%20Peng&entry.1292438233=With%20the%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20Video%20Multi-Modal%20Large%20Language%20Models%20%28Video%20MLLMs%29%20have%20achieved%20remarkable%20performance%20in%20video-language%20tasks%20such%20as%20video%20understanding%20and%20question%20answering.%20However%2C%20Video%20MLLMs%20face%20high%20computational%20costs%2C%20particularly%20in%20processing%20numerous%20video%20frames%20as%20input%2C%20which%20leads%20to%20significant%20attention%20computation%20overhead.%20A%20straightforward%20approach%20to%20reduce%20computational%20costs%20is%20to%20decrease%20the%20number%20of%20input%20video%20frames.%20However%2C%20simply%20selecting%20key%20frames%20at%20a%20fixed%20frame%20rate%20%28FPS%29%20often%20overlooks%20valuable%20information%20in%20non-key%20frames%2C%20resulting%20in%20notable%20performance%20degradation.%20To%20address%20this%2C%20we%20propose%20Text-guided%20Video%20Frame%20Reduction%20%28TiFRe%29%2C%20a%20framework%20that%20reduces%20input%20frames%20while%20preserving%20essential%20video%20information.%20TiFRe%20uses%20a%20Text-guided%20Frame%20Sampling%20%28TFS%29%20strategy%20to%20select%20key%20frames%20based%20on%20user%20input%2C%20which%20is%20processed%20by%20an%20LLM%20to%20generate%20a%20CLIP-style%20prompt.%20Pre-trained%20CLIP%20encoders%20calculate%20the%20semantic%20similarity%20between%20the%20prompt%20and%20each%20frame%2C%20selecting%20the%20most%20relevant%20frames%20as%20key%20frames.%20To%20preserve%20video%20semantics%2C%20TiFRe%20employs%20a%20Frame%20Matching%20and%20Merging%20%28FMM%29%20mechanism%2C%20which%20integrates%20non-key%20frame%20information%20into%20the%20selected%20key%20frames%2C%20minimizing%20information%20loss.%20Experiments%20show%20that%20TiFRe%20effectively%20reduces%20computational%20costs%20while%20improving%20performance%20on%20video-language%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.08861v1&entry.124074799=Read"},
{"title": "Two-dimensional RMSD projections for reaction path visualization and validation", "author": "Rohit Goswami", "abstract": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and are often history dependent. This precludes the ability to compare optimization histories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension projection defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using a gradient aware derivative Gaussian Process. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We demonstrate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements, along with the ratification of the visualization for more complex reactions, a grignard rearrangement, and a bicyclobutadiene rearrangement.", "link": "http://arxiv.org/abs/2512.07329v2", "date": "2026-02-09", "relevancy": 2.2107, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4469}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-dimensional%20RMSD%20projections%20for%20reaction%20path%20visualization%20and%20validation&body=Title%3A%20Two-dimensional%20RMSD%20projections%20for%20reaction%20path%20visualization%20and%20validation%0AAuthor%3A%20Rohit%20Goswami%0AAbstract%3A%20Transition%20state%20or%20minimum%20energy%20path%20finding%20methods%20constitute%20a%20routine%20component%20of%20the%20computational%20chemistry%20toolkit.%20Standard%20analysis%20involves%20trajectories%20conventionally%20plotted%20in%20terms%20of%20the%20relative%20energy%20to%20the%20initial%20state%20against%20a%20cumulative%20displacement%20variable%2C%20or%20the%20image%20number.%20These%20dimensional%20reductions%20obscure%20structural%20rearrangements%20in%20high%20dimensions%20and%20are%20often%20history%20dependent.%20This%20precludes%20the%20ability%20to%20compare%20optimization%20histories%20of%20different%20methods%20beyond%20the%20number%20of%20calculations%2C%20time%20taken%2C%20and%20final%20saddle%20geometry.%20We%20present%20a%20method%20mapping%20trajectories%20onto%20a%20two-dimension%20projection%20defined%20by%20a%20permutation%20corrected%20root%20mean%20square%20deviation%20from%20the%20reactant%20and%20product%20configurations.%20Energy%20is%20represented%20as%20an%20interpolated%20color-mapped%20surface%20constructed%20from%20all%20optimization%20steps%20using%20a%20gradient%20aware%20derivative%20Gaussian%20Process.%20This%20representation%20highlights%20optimization%20trajectories%2C%20identifies%20endpoint%20basins%2C%20and%20diagnoses%20convergence%20concerns%20invisible%20in%20one-dimensional%20profiles.%20We%20demonstrate%20the%20framework%20on%20a%20cycloaddition%20reaction%2C%20showing%20that%20a%20machine-learned%20potential%20saddle%20and%20density%20functional%20theory%20reference%20lie%20on%20comparable%20energy%20contours%20despite%20geometric%20displacements%2C%20along%20with%20the%20ratification%20of%20the%20visualization%20for%20more%20complex%20reactions%2C%20a%20grignard%20rearrangement%2C%20and%20a%20bicyclobutadiene%20rearrangement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-dimensional%2520RMSD%2520projections%2520for%2520reaction%2520path%2520visualization%2520and%2520validation%26entry.906535625%3DRohit%2520Goswami%26entry.1292438233%3DTransition%2520state%2520or%2520minimum%2520energy%2520path%2520finding%2520methods%2520constitute%2520a%2520routine%2520component%2520of%2520the%2520computational%2520chemistry%2520toolkit.%2520Standard%2520analysis%2520involves%2520trajectories%2520conventionally%2520plotted%2520in%2520terms%2520of%2520the%2520relative%2520energy%2520to%2520the%2520initial%2520state%2520against%2520a%2520cumulative%2520displacement%2520variable%252C%2520or%2520the%2520image%2520number.%2520These%2520dimensional%2520reductions%2520obscure%2520structural%2520rearrangements%2520in%2520high%2520dimensions%2520and%2520are%2520often%2520history%2520dependent.%2520This%2520precludes%2520the%2520ability%2520to%2520compare%2520optimization%2520histories%2520of%2520different%2520methods%2520beyond%2520the%2520number%2520of%2520calculations%252C%2520time%2520taken%252C%2520and%2520final%2520saddle%2520geometry.%2520We%2520present%2520a%2520method%2520mapping%2520trajectories%2520onto%2520a%2520two-dimension%2520projection%2520defined%2520by%2520a%2520permutation%2520corrected%2520root%2520mean%2520square%2520deviation%2520from%2520the%2520reactant%2520and%2520product%2520configurations.%2520Energy%2520is%2520represented%2520as%2520an%2520interpolated%2520color-mapped%2520surface%2520constructed%2520from%2520all%2520optimization%2520steps%2520using%2520a%2520gradient%2520aware%2520derivative%2520Gaussian%2520Process.%2520This%2520representation%2520highlights%2520optimization%2520trajectories%252C%2520identifies%2520endpoint%2520basins%252C%2520and%2520diagnoses%2520convergence%2520concerns%2520invisible%2520in%2520one-dimensional%2520profiles.%2520We%2520demonstrate%2520the%2520framework%2520on%2520a%2520cycloaddition%2520reaction%252C%2520showing%2520that%2520a%2520machine-learned%2520potential%2520saddle%2520and%2520density%2520functional%2520theory%2520reference%2520lie%2520on%2520comparable%2520energy%2520contours%2520despite%2520geometric%2520displacements%252C%2520along%2520with%2520the%2520ratification%2520of%2520the%2520visualization%2520for%2520more%2520complex%2520reactions%252C%2520a%2520grignard%2520rearrangement%252C%2520and%2520a%2520bicyclobutadiene%2520rearrangement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-dimensional%20RMSD%20projections%20for%20reaction%20path%20visualization%20and%20validation&entry.906535625=Rohit%20Goswami&entry.1292438233=Transition%20state%20or%20minimum%20energy%20path%20finding%20methods%20constitute%20a%20routine%20component%20of%20the%20computational%20chemistry%20toolkit.%20Standard%20analysis%20involves%20trajectories%20conventionally%20plotted%20in%20terms%20of%20the%20relative%20energy%20to%20the%20initial%20state%20against%20a%20cumulative%20displacement%20variable%2C%20or%20the%20image%20number.%20These%20dimensional%20reductions%20obscure%20structural%20rearrangements%20in%20high%20dimensions%20and%20are%20often%20history%20dependent.%20This%20precludes%20the%20ability%20to%20compare%20optimization%20histories%20of%20different%20methods%20beyond%20the%20number%20of%20calculations%2C%20time%20taken%2C%20and%20final%20saddle%20geometry.%20We%20present%20a%20method%20mapping%20trajectories%20onto%20a%20two-dimension%20projection%20defined%20by%20a%20permutation%20corrected%20root%20mean%20square%20deviation%20from%20the%20reactant%20and%20product%20configurations.%20Energy%20is%20represented%20as%20an%20interpolated%20color-mapped%20surface%20constructed%20from%20all%20optimization%20steps%20using%20a%20gradient%20aware%20derivative%20Gaussian%20Process.%20This%20representation%20highlights%20optimization%20trajectories%2C%20identifies%20endpoint%20basins%2C%20and%20diagnoses%20convergence%20concerns%20invisible%20in%20one-dimensional%20profiles.%20We%20demonstrate%20the%20framework%20on%20a%20cycloaddition%20reaction%2C%20showing%20that%20a%20machine-learned%20potential%20saddle%20and%20density%20functional%20theory%20reference%20lie%20on%20comparable%20energy%20contours%20despite%20geometric%20displacements%2C%20along%20with%20the%20ratification%20of%20the%20visualization%20for%20more%20complex%20reactions%2C%20a%20grignard%20rearrangement%2C%20and%20a%20bicyclobutadiene%20rearrangement.&entry.1838667208=http%3A//arxiv.org/abs/2512.07329v2&entry.124074799=Read"},
{"title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing", "author": "Yongwen Lai and Chaoqun Wang and Shaobo Min", "abstract": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.", "link": "http://arxiv.org/abs/2602.08725v1", "date": "2026-02-09", "relevancy": 1.6873, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5922}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5664}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionEdit%3A%20Semantic%20Fusion%20and%20Attention%20Modulation%20for%20Training-Free%20Image%20Editing&body=Title%3A%20FusionEdit%3A%20Semantic%20Fusion%20and%20Attention%20Modulation%20for%20Training-Free%20Image%20Editing%0AAuthor%3A%20Yongwen%20Lai%20and%20Chaoqun%20Wang%20and%20Shaobo%20Min%0AAbstract%3A%20Text-guided%20image%20editing%20aims%20to%20modify%20specific%20regions%20according%20to%20the%20target%20prompt%20while%20preserving%20the%20identity%20of%20the%20source%20image.%20Recent%20methods%20exploit%20explicit%20binary%20masks%20to%20constrain%20editing%2C%20but%20hard%20mask%20boundaries%20introduce%20artifacts%20and%20reduce%20editability.%20To%20address%20these%20issues%2C%20we%20propose%20FusionEdit%2C%20a%20training-free%20image%20editing%20framework%20that%20achieves%20precise%20and%20controllable%20edits.%20First%2C%20editing%20and%20preserved%20regions%20are%20automatically%20identified%20by%20measuring%20semantic%20discrepancies%20between%20source%20and%20target%20prompts.%20To%20mitigate%20boundary%20artifacts%2C%20FusionEdit%20performs%20distance-aware%20latent%20fusion%20along%20region%20boundaries%20to%20yield%20the%20soft%20and%20accurate%20mask%2C%20and%20employs%20a%20total%20variation%20loss%20to%20enforce%20smooth%20transitions%2C%20obtaining%20natural%20editing%20results.%20Second%2C%20FusionEdit%20leverages%20AdaIN-based%20modulation%20within%20DiT%20attention%20layers%20to%20perform%20a%20statistical%20attention%20fusion%20in%20the%20editing%20region%2C%20enhancing%20editability%20while%20preserving%20global%20consistency%20with%20the%20source%20image.%20Extensive%20experiments%20demonstrate%20that%20our%20FusionEdit%20significantly%20outperforms%20state-of-the-art%20methods.%20Code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Yvan1001/FusionEdit%7D%7Bhttps%3A//github.com/Yvan1001/FusionEdit%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionEdit%253A%2520Semantic%2520Fusion%2520and%2520Attention%2520Modulation%2520for%2520Training-Free%2520Image%2520Editing%26entry.906535625%3DYongwen%2520Lai%2520and%2520Chaoqun%2520Wang%2520and%2520Shaobo%2520Min%26entry.1292438233%3DText-guided%2520image%2520editing%2520aims%2520to%2520modify%2520specific%2520regions%2520according%2520to%2520the%2520target%2520prompt%2520while%2520preserving%2520the%2520identity%2520of%2520the%2520source%2520image.%2520Recent%2520methods%2520exploit%2520explicit%2520binary%2520masks%2520to%2520constrain%2520editing%252C%2520but%2520hard%2520mask%2520boundaries%2520introduce%2520artifacts%2520and%2520reduce%2520editability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FusionEdit%252C%2520a%2520training-free%2520image%2520editing%2520framework%2520that%2520achieves%2520precise%2520and%2520controllable%2520edits.%2520First%252C%2520editing%2520and%2520preserved%2520regions%2520are%2520automatically%2520identified%2520by%2520measuring%2520semantic%2520discrepancies%2520between%2520source%2520and%2520target%2520prompts.%2520To%2520mitigate%2520boundary%2520artifacts%252C%2520FusionEdit%2520performs%2520distance-aware%2520latent%2520fusion%2520along%2520region%2520boundaries%2520to%2520yield%2520the%2520soft%2520and%2520accurate%2520mask%252C%2520and%2520employs%2520a%2520total%2520variation%2520loss%2520to%2520enforce%2520smooth%2520transitions%252C%2520obtaining%2520natural%2520editing%2520results.%2520Second%252C%2520FusionEdit%2520leverages%2520AdaIN-based%2520modulation%2520within%2520DiT%2520attention%2520layers%2520to%2520perform%2520a%2520statistical%2520attention%2520fusion%2520in%2520the%2520editing%2520region%252C%2520enhancing%2520editability%2520while%2520preserving%2520global%2520consistency%2520with%2520the%2520source%2520image.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520FusionEdit%2520significantly%2520outperforms%2520state-of-the-art%2520methods.%2520Code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Yvan1001/FusionEdit%257D%257Bhttps%253A//github.com/Yvan1001/FusionEdit%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionEdit%3A%20Semantic%20Fusion%20and%20Attention%20Modulation%20for%20Training-Free%20Image%20Editing&entry.906535625=Yongwen%20Lai%20and%20Chaoqun%20Wang%20and%20Shaobo%20Min&entry.1292438233=Text-guided%20image%20editing%20aims%20to%20modify%20specific%20regions%20according%20to%20the%20target%20prompt%20while%20preserving%20the%20identity%20of%20the%20source%20image.%20Recent%20methods%20exploit%20explicit%20binary%20masks%20to%20constrain%20editing%2C%20but%20hard%20mask%20boundaries%20introduce%20artifacts%20and%20reduce%20editability.%20To%20address%20these%20issues%2C%20we%20propose%20FusionEdit%2C%20a%20training-free%20image%20editing%20framework%20that%20achieves%20precise%20and%20controllable%20edits.%20First%2C%20editing%20and%20preserved%20regions%20are%20automatically%20identified%20by%20measuring%20semantic%20discrepancies%20between%20source%20and%20target%20prompts.%20To%20mitigate%20boundary%20artifacts%2C%20FusionEdit%20performs%20distance-aware%20latent%20fusion%20along%20region%20boundaries%20to%20yield%20the%20soft%20and%20accurate%20mask%2C%20and%20employs%20a%20total%20variation%20loss%20to%20enforce%20smooth%20transitions%2C%20obtaining%20natural%20editing%20results.%20Second%2C%20FusionEdit%20leverages%20AdaIN-based%20modulation%20within%20DiT%20attention%20layers%20to%20perform%20a%20statistical%20attention%20fusion%20in%20the%20editing%20region%2C%20enhancing%20editability%20while%20preserving%20global%20consistency%20with%20the%20source%20image.%20Extensive%20experiments%20demonstrate%20that%20our%20FusionEdit%20significantly%20outperforms%20state-of-the-art%20methods.%20Code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Yvan1001/FusionEdit%7D%7Bhttps%3A//github.com/Yvan1001/FusionEdit%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.08725v1&entry.124074799=Read"},
{"title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs", "author": "Yapei Chang and Kyle Lo and Mohit Iyyer and Luca Soldaini", "abstract": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.", "link": "http://arxiv.org/abs/2602.08808v1", "date": "2026-02-09", "relevancy": 1.9113, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How2Everything%3A%20Mining%20the%20Web%20for%20How-To%20Procedures%20to%20Evaluate%20and%20Improve%20LLMs&body=Title%3A%20How2Everything%3A%20Mining%20the%20Web%20for%20How-To%20Procedures%20to%20Evaluate%20and%20Improve%20LLMs%0AAuthor%3A%20Yapei%20Chang%20and%20Kyle%20Lo%20and%20Mohit%20Iyyer%20and%20Luca%20Soldaini%0AAbstract%3A%20Generating%20step-by-step%20%22how-to%22%20procedures%20is%20a%20key%20LLM%20capability%3A%20how-to%20advice%20is%20commonly%20requested%20in%20chatbots%2C%20and%20step-by-step%20planning%20is%20critical%20for%20reasoning%20over%20complex%20tasks.%20Yet%2C%20measuring%20and%20improving%20procedural%20validity%20at%20scale%20on%20real-world%20tasks%20remains%20challenging%20and%20understudied.%20To%20address%20this%2C%20we%20introduce%20How2Everything%2C%20a%20scalable%20framework%20to%20evaluate%20and%20improve%20goal-conditioned%20procedure%20generation.%20Our%20framework%20includes%20How2Mine%2C%20which%20mines%20351K%20procedures%20from%20980K%20web%20pages%20across%2014%20topics%20and%20readily%20scales%20to%20larger%20corpora.%20From%20this%20pool%20we%20build%20How2Bench%2C%20a%207K-example%20evaluation%20set%20balanced%20across%20topics.%20To%20reliably%20score%20model%20outputs%2C%20we%20develop%20How2Score%2C%20an%20evaluation%20protocol%20that%20uses%20an%20LLM%20judge%20to%20detect%20whether%20a%20generation%20contains%20any%20critical%20failure%20that%20would%20prevent%20achieving%20the%20goal.%20For%20low-cost%2C%20reproducible%20evaluation%2C%20we%20distill%20a%20frontier%20model%20into%20an%20open%208B%20model%2C%20achieving%2080.5%25%20agreement%20with%20human%20annotators.%20How2Bench%20reveals%20clear%20scaling%20trends%20across%20model%20sizes%20and%20training%20stages%2C%20providing%20signal%20early%20in%20pretraining.%20Finally%2C%20RL%20using%20How2Score%20as%20a%20reward%20improves%20performance%20on%20How2Bench%20by%20%3E10%20points%20across%20three%20models%20without%20systematic%20regressions%20on%20standard%20benchmarks%2C%20with%20gains%20robust%20to%20superficial%20source-document%20memorization%20or%20format%20compliance.%20Taken%20together%2C%20How2Everything%20shows%20how%20pretraining%20web%20data%20can%20support%20a%20closed%20loop%20of%20capability%20evaluation%20and%20improvement%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow2Everything%253A%2520Mining%2520the%2520Web%2520for%2520How-To%2520Procedures%2520to%2520Evaluate%2520and%2520Improve%2520LLMs%26entry.906535625%3DYapei%2520Chang%2520and%2520Kyle%2520Lo%2520and%2520Mohit%2520Iyyer%2520and%2520Luca%2520Soldaini%26entry.1292438233%3DGenerating%2520step-by-step%2520%2522how-to%2522%2520procedures%2520is%2520a%2520key%2520LLM%2520capability%253A%2520how-to%2520advice%2520is%2520commonly%2520requested%2520in%2520chatbots%252C%2520and%2520step-by-step%2520planning%2520is%2520critical%2520for%2520reasoning%2520over%2520complex%2520tasks.%2520Yet%252C%2520measuring%2520and%2520improving%2520procedural%2520validity%2520at%2520scale%2520on%2520real-world%2520tasks%2520remains%2520challenging%2520and%2520understudied.%2520To%2520address%2520this%252C%2520we%2520introduce%2520How2Everything%252C%2520a%2520scalable%2520framework%2520to%2520evaluate%2520and%2520improve%2520goal-conditioned%2520procedure%2520generation.%2520Our%2520framework%2520includes%2520How2Mine%252C%2520which%2520mines%2520351K%2520procedures%2520from%2520980K%2520web%2520pages%2520across%252014%2520topics%2520and%2520readily%2520scales%2520to%2520larger%2520corpora.%2520From%2520this%2520pool%2520we%2520build%2520How2Bench%252C%2520a%25207K-example%2520evaluation%2520set%2520balanced%2520across%2520topics.%2520To%2520reliably%2520score%2520model%2520outputs%252C%2520we%2520develop%2520How2Score%252C%2520an%2520evaluation%2520protocol%2520that%2520uses%2520an%2520LLM%2520judge%2520to%2520detect%2520whether%2520a%2520generation%2520contains%2520any%2520critical%2520failure%2520that%2520would%2520prevent%2520achieving%2520the%2520goal.%2520For%2520low-cost%252C%2520reproducible%2520evaluation%252C%2520we%2520distill%2520a%2520frontier%2520model%2520into%2520an%2520open%25208B%2520model%252C%2520achieving%252080.5%2525%2520agreement%2520with%2520human%2520annotators.%2520How2Bench%2520reveals%2520clear%2520scaling%2520trends%2520across%2520model%2520sizes%2520and%2520training%2520stages%252C%2520providing%2520signal%2520early%2520in%2520pretraining.%2520Finally%252C%2520RL%2520using%2520How2Score%2520as%2520a%2520reward%2520improves%2520performance%2520on%2520How2Bench%2520by%2520%253E10%2520points%2520across%2520three%2520models%2520without%2520systematic%2520regressions%2520on%2520standard%2520benchmarks%252C%2520with%2520gains%2520robust%2520to%2520superficial%2520source-document%2520memorization%2520or%2520format%2520compliance.%2520Taken%2520together%252C%2520How2Everything%2520shows%2520how%2520pretraining%2520web%2520data%2520can%2520support%2520a%2520closed%2520loop%2520of%2520capability%2520evaluation%2520and%2520improvement%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How2Everything%3A%20Mining%20the%20Web%20for%20How-To%20Procedures%20to%20Evaluate%20and%20Improve%20LLMs&entry.906535625=Yapei%20Chang%20and%20Kyle%20Lo%20and%20Mohit%20Iyyer%20and%20Luca%20Soldaini&entry.1292438233=Generating%20step-by-step%20%22how-to%22%20procedures%20is%20a%20key%20LLM%20capability%3A%20how-to%20advice%20is%20commonly%20requested%20in%20chatbots%2C%20and%20step-by-step%20planning%20is%20critical%20for%20reasoning%20over%20complex%20tasks.%20Yet%2C%20measuring%20and%20improving%20procedural%20validity%20at%20scale%20on%20real-world%20tasks%20remains%20challenging%20and%20understudied.%20To%20address%20this%2C%20we%20introduce%20How2Everything%2C%20a%20scalable%20framework%20to%20evaluate%20and%20improve%20goal-conditioned%20procedure%20generation.%20Our%20framework%20includes%20How2Mine%2C%20which%20mines%20351K%20procedures%20from%20980K%20web%20pages%20across%2014%20topics%20and%20readily%20scales%20to%20larger%20corpora.%20From%20this%20pool%20we%20build%20How2Bench%2C%20a%207K-example%20evaluation%20set%20balanced%20across%20topics.%20To%20reliably%20score%20model%20outputs%2C%20we%20develop%20How2Score%2C%20an%20evaluation%20protocol%20that%20uses%20an%20LLM%20judge%20to%20detect%20whether%20a%20generation%20contains%20any%20critical%20failure%20that%20would%20prevent%20achieving%20the%20goal.%20For%20low-cost%2C%20reproducible%20evaluation%2C%20we%20distill%20a%20frontier%20model%20into%20an%20open%208B%20model%2C%20achieving%2080.5%25%20agreement%20with%20human%20annotators.%20How2Bench%20reveals%20clear%20scaling%20trends%20across%20model%20sizes%20and%20training%20stages%2C%20providing%20signal%20early%20in%20pretraining.%20Finally%2C%20RL%20using%20How2Score%20as%20a%20reward%20improves%20performance%20on%20How2Bench%20by%20%3E10%20points%20across%20three%20models%20without%20systematic%20regressions%20on%20standard%20benchmarks%2C%20with%20gains%20robust%20to%20superficial%20source-document%20memorization%20or%20format%20compliance.%20Taken%20together%2C%20How2Everything%20shows%20how%20pretraining%20web%20data%20can%20support%20a%20closed%20loop%20of%20capability%20evaluation%20and%20improvement%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2602.08808v1&entry.124074799=Read"},
{"title": "Diffusion-Inspired Masked Fine-Tuning for Knowledge Injection in Autoregressive LLMs", "author": "Xu Pan and Ely Hahami and Jingxuan Fan and Ziqian Xie and Haim Sompolinsky", "abstract": "Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language models (dLLMs) require fewer training samples to achieve lower loss in pre-training and are more resistant to the reversal curse, suggesting dLLMs may learn new knowledge more easily than autoregressive LLMs (arLLMs). We test this hypothesis in controlled knowledge fine-tuning experiments and find that while arLLMs rely on paraphrase augmentation to generalize knowledge text into question-answering (QA) capability, dLLMs do not require paraphrases to achieve high QA accuracy. To further investigate whether the demasking objective alone can induce such a knowledge injection advantage in dLLMs regardless of their diffusion denoising paradigm, we propose masked fine-tuning for arLLMs, which prompts an arLLM to reconstruct the original text given a masked version in context. The masked fine-tuning for arLLMs substantially improves the efficacy of knowledge injection, i.e. no paraphrase needed and resistant to the reversal curse, closing the gap between arLLMs and dLLMs. We also demonstrate that the same demasking objective improves supervised fine-tuning (SFT) on math tasks over standard SFT, suggesting broader applicability of the demasking objective.", "link": "http://arxiv.org/abs/2510.09885v4", "date": "2026-02-09", "relevancy": 1.0245, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5215}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.508}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Inspired%20Masked%20Fine-Tuning%20for%20Knowledge%20Injection%20in%20Autoregressive%20LLMs&body=Title%3A%20Diffusion-Inspired%20Masked%20Fine-Tuning%20for%20Knowledge%20Injection%20in%20Autoregressive%20LLMs%0AAuthor%3A%20Xu%20Pan%20and%20Ely%20Hahami%20and%20Jingxuan%20Fan%20and%20Ziqian%20Xie%20and%20Haim%20Sompolinsky%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20often%20used%20in%20environments%20where%20facts%20evolve%2C%20yet%20factual%20knowledge%20updates%20via%20fine-tuning%20on%20unstructured%20text%20often%20suffers%20from%201%29%20reliance%20on%20compute-heavy%20paraphrase%20augmentation%20and%202%29%20the%20reversal%20curse.%20Recent%20studies%20show%20diffusion%20large%20language%20models%20%28dLLMs%29%20require%20fewer%20training%20samples%20to%20achieve%20lower%20loss%20in%20pre-training%20and%20are%20more%20resistant%20to%20the%20reversal%20curse%2C%20suggesting%20dLLMs%20may%20learn%20new%20knowledge%20more%20easily%20than%20autoregressive%20LLMs%20%28arLLMs%29.%20We%20test%20this%20hypothesis%20in%20controlled%20knowledge%20fine-tuning%20experiments%20and%20find%20that%20while%20arLLMs%20rely%20on%20paraphrase%20augmentation%20to%20generalize%20knowledge%20text%20into%20question-answering%20%28QA%29%20capability%2C%20dLLMs%20do%20not%20require%20paraphrases%20to%20achieve%20high%20QA%20accuracy.%20To%20further%20investigate%20whether%20the%20demasking%20objective%20alone%20can%20induce%20such%20a%20knowledge%20injection%20advantage%20in%20dLLMs%20regardless%20of%20their%20diffusion%20denoising%20paradigm%2C%20we%20propose%20masked%20fine-tuning%20for%20arLLMs%2C%20which%20prompts%20an%20arLLM%20to%20reconstruct%20the%20original%20text%20given%20a%20masked%20version%20in%20context.%20The%20masked%20fine-tuning%20for%20arLLMs%20substantially%20improves%20the%20efficacy%20of%20knowledge%20injection%2C%20i.e.%20no%20paraphrase%20needed%20and%20resistant%20to%20the%20reversal%20curse%2C%20closing%20the%20gap%20between%20arLLMs%20and%20dLLMs.%20We%20also%20demonstrate%20that%20the%20same%20demasking%20objective%20improves%20supervised%20fine-tuning%20%28SFT%29%20on%20math%20tasks%20over%20standard%20SFT%2C%20suggesting%20broader%20applicability%20of%20the%20demasking%20objective.%0ALink%3A%20http%3A//arxiv.org/abs/2510.09885v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Inspired%2520Masked%2520Fine-Tuning%2520for%2520Knowledge%2520Injection%2520in%2520Autoregressive%2520LLMs%26entry.906535625%3DXu%2520Pan%2520and%2520Ely%2520Hahami%2520and%2520Jingxuan%2520Fan%2520and%2520Ziqian%2520Xie%2520and%2520Haim%2520Sompolinsky%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520often%2520used%2520in%2520environments%2520where%2520facts%2520evolve%252C%2520yet%2520factual%2520knowledge%2520updates%2520via%2520fine-tuning%2520on%2520unstructured%2520text%2520often%2520suffers%2520from%25201%2529%2520reliance%2520on%2520compute-heavy%2520paraphrase%2520augmentation%2520and%25202%2529%2520the%2520reversal%2520curse.%2520Recent%2520studies%2520show%2520diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520require%2520fewer%2520training%2520samples%2520to%2520achieve%2520lower%2520loss%2520in%2520pre-training%2520and%2520are%2520more%2520resistant%2520to%2520the%2520reversal%2520curse%252C%2520suggesting%2520dLLMs%2520may%2520learn%2520new%2520knowledge%2520more%2520easily%2520than%2520autoregressive%2520LLMs%2520%2528arLLMs%2529.%2520We%2520test%2520this%2520hypothesis%2520in%2520controlled%2520knowledge%2520fine-tuning%2520experiments%2520and%2520find%2520that%2520while%2520arLLMs%2520rely%2520on%2520paraphrase%2520augmentation%2520to%2520generalize%2520knowledge%2520text%2520into%2520question-answering%2520%2528QA%2529%2520capability%252C%2520dLLMs%2520do%2520not%2520require%2520paraphrases%2520to%2520achieve%2520high%2520QA%2520accuracy.%2520To%2520further%2520investigate%2520whether%2520the%2520demasking%2520objective%2520alone%2520can%2520induce%2520such%2520a%2520knowledge%2520injection%2520advantage%2520in%2520dLLMs%2520regardless%2520of%2520their%2520diffusion%2520denoising%2520paradigm%252C%2520we%2520propose%2520masked%2520fine-tuning%2520for%2520arLLMs%252C%2520which%2520prompts%2520an%2520arLLM%2520to%2520reconstruct%2520the%2520original%2520text%2520given%2520a%2520masked%2520version%2520in%2520context.%2520The%2520masked%2520fine-tuning%2520for%2520arLLMs%2520substantially%2520improves%2520the%2520efficacy%2520of%2520knowledge%2520injection%252C%2520i.e.%2520no%2520paraphrase%2520needed%2520and%2520resistant%2520to%2520the%2520reversal%2520curse%252C%2520closing%2520the%2520gap%2520between%2520arLLMs%2520and%2520dLLMs.%2520We%2520also%2520demonstrate%2520that%2520the%2520same%2520demasking%2520objective%2520improves%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520math%2520tasks%2520over%2520standard%2520SFT%252C%2520suggesting%2520broader%2520applicability%2520of%2520the%2520demasking%2520objective.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09885v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Inspired%20Masked%20Fine-Tuning%20for%20Knowledge%20Injection%20in%20Autoregressive%20LLMs&entry.906535625=Xu%20Pan%20and%20Ely%20Hahami%20and%20Jingxuan%20Fan%20and%20Ziqian%20Xie%20and%20Haim%20Sompolinsky&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20often%20used%20in%20environments%20where%20facts%20evolve%2C%20yet%20factual%20knowledge%20updates%20via%20fine-tuning%20on%20unstructured%20text%20often%20suffers%20from%201%29%20reliance%20on%20compute-heavy%20paraphrase%20augmentation%20and%202%29%20the%20reversal%20curse.%20Recent%20studies%20show%20diffusion%20large%20language%20models%20%28dLLMs%29%20require%20fewer%20training%20samples%20to%20achieve%20lower%20loss%20in%20pre-training%20and%20are%20more%20resistant%20to%20the%20reversal%20curse%2C%20suggesting%20dLLMs%20may%20learn%20new%20knowledge%20more%20easily%20than%20autoregressive%20LLMs%20%28arLLMs%29.%20We%20test%20this%20hypothesis%20in%20controlled%20knowledge%20fine-tuning%20experiments%20and%20find%20that%20while%20arLLMs%20rely%20on%20paraphrase%20augmentation%20to%20generalize%20knowledge%20text%20into%20question-answering%20%28QA%29%20capability%2C%20dLLMs%20do%20not%20require%20paraphrases%20to%20achieve%20high%20QA%20accuracy.%20To%20further%20investigate%20whether%20the%20demasking%20objective%20alone%20can%20induce%20such%20a%20knowledge%20injection%20advantage%20in%20dLLMs%20regardless%20of%20their%20diffusion%20denoising%20paradigm%2C%20we%20propose%20masked%20fine-tuning%20for%20arLLMs%2C%20which%20prompts%20an%20arLLM%20to%20reconstruct%20the%20original%20text%20given%20a%20masked%20version%20in%20context.%20The%20masked%20fine-tuning%20for%20arLLMs%20substantially%20improves%20the%20efficacy%20of%20knowledge%20injection%2C%20i.e.%20no%20paraphrase%20needed%20and%20resistant%20to%20the%20reversal%20curse%2C%20closing%20the%20gap%20between%20arLLMs%20and%20dLLMs.%20We%20also%20demonstrate%20that%20the%20same%20demasking%20objective%20improves%20supervised%20fine-tuning%20%28SFT%29%20on%20math%20tasks%20over%20standard%20SFT%2C%20suggesting%20broader%20applicability%20of%20the%20demasking%20objective.&entry.1838667208=http%3A//arxiv.org/abs/2510.09885v4&entry.124074799=Read"},
{"title": "Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms", "author": "Nobuyuki Ota", "abstract": "Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that\n  researchers can examine. Here we present CDT-II, an \"AI microscope\" whose attention maps are directly interpretable as regulatory structure.\n  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA\n  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional\n  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in\n  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B\n  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \\times 10^{-17}$). Two distinct attention mechanisms converge on an RNA\n  processing module ($P = 1 \\times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing\n  regulatory structure rather than merely optimizing predictions.", "link": "http://arxiv.org/abs/2602.08751v1", "date": "2026-02-09", "relevancy": 1.9079, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms&body=Title%3A%20Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms%0AAuthor%3A%20Nobuyuki%20Ota%0AAbstract%3A%20Current%20biological%20AI%20models%20lack%20interpretability%20--%20their%20internal%20representations%20do%20not%20correspond%20to%20biological%20relationships%20that%0A%20%20researchers%20can%20examine.%20Here%20we%20present%20CDT-II%2C%20an%20%22AI%20microscope%22%20whose%20attention%20maps%20are%20directly%20interpretable%20as%20regulatory%20structure.%0A%20%20By%20mirroring%20the%20central%20dogma%20in%20its%20architecture%2C%20each%20attention%20mechanism%20corresponds%20to%20a%20specific%20biological%20relationship%3A%20DNA%0A%20%20self-attention%20for%20genomic%20relationships%2C%20RNA%20self-attention%20for%20gene%20co-regulation%2C%20and%20DNA-to-RNA%20cross-attention%20for%20transcriptional%0A%20%20control.%20Using%20only%20genomic%20embeddings%20and%20raw%20per-cell%20expression%2C%20CDT-II%20enables%20experimental%20biologists%20to%20observe%20regulatory%20networks%20in%0A%20%20their%20own%20data.%20Applied%20to%20K562%20CRISPRi%20data%2C%20CDT-II%20predicts%20perturbation%20effects%20%28per-gene%20mean%20%24r%20%3D%200.84%24%29%20and%20recovers%20the%20GFI1B%0A%20%20regulatory%20network%20without%20supervision%20%286.6-fold%20enrichment%2C%20%24P%20%3D%203.5%20%5Ctimes%2010%5E%7B-17%7D%24%29.%20Two%20distinct%20attention%20mechanisms%20converge%20on%20an%20RNA%0A%20%20processing%20module%20%28%24P%20%3D%201%20%5Ctimes%2010%5E%7B-16%7D%24%29.%20CDT-II%20establishes%20mechanism-oriented%20AI%20as%20an%20alternative%20to%20task-oriented%20approaches%2C%20revealing%0A%20%20regulatory%20structure%20rather%20than%20merely%20optimizing%20predictions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentral%2520Dogma%2520Transformer%2520II%253A%2520An%2520AI%2520Microscope%2520for%2520Understanding%2520Cellular%2520Regulatory%2520Mechanisms%26entry.906535625%3DNobuyuki%2520Ota%26entry.1292438233%3DCurrent%2520biological%2520AI%2520models%2520lack%2520interpretability%2520--%2520their%2520internal%2520representations%2520do%2520not%2520correspond%2520to%2520biological%2520relationships%2520that%250A%2520%2520researchers%2520can%2520examine.%2520Here%2520we%2520present%2520CDT-II%252C%2520an%2520%2522AI%2520microscope%2522%2520whose%2520attention%2520maps%2520are%2520directly%2520interpretable%2520as%2520regulatory%2520structure.%250A%2520%2520By%2520mirroring%2520the%2520central%2520dogma%2520in%2520its%2520architecture%252C%2520each%2520attention%2520mechanism%2520corresponds%2520to%2520a%2520specific%2520biological%2520relationship%253A%2520DNA%250A%2520%2520self-attention%2520for%2520genomic%2520relationships%252C%2520RNA%2520self-attention%2520for%2520gene%2520co-regulation%252C%2520and%2520DNA-to-RNA%2520cross-attention%2520for%2520transcriptional%250A%2520%2520control.%2520Using%2520only%2520genomic%2520embeddings%2520and%2520raw%2520per-cell%2520expression%252C%2520CDT-II%2520enables%2520experimental%2520biologists%2520to%2520observe%2520regulatory%2520networks%2520in%250A%2520%2520their%2520own%2520data.%2520Applied%2520to%2520K562%2520CRISPRi%2520data%252C%2520CDT-II%2520predicts%2520perturbation%2520effects%2520%2528per-gene%2520mean%2520%2524r%2520%253D%25200.84%2524%2529%2520and%2520recovers%2520the%2520GFI1B%250A%2520%2520regulatory%2520network%2520without%2520supervision%2520%25286.6-fold%2520enrichment%252C%2520%2524P%2520%253D%25203.5%2520%255Ctimes%252010%255E%257B-17%257D%2524%2529.%2520Two%2520distinct%2520attention%2520mechanisms%2520converge%2520on%2520an%2520RNA%250A%2520%2520processing%2520module%2520%2528%2524P%2520%253D%25201%2520%255Ctimes%252010%255E%257B-16%257D%2524%2529.%2520CDT-II%2520establishes%2520mechanism-oriented%2520AI%2520as%2520an%2520alternative%2520to%2520task-oriented%2520approaches%252C%2520revealing%250A%2520%2520regulatory%2520structure%2520rather%2520than%2520merely%2520optimizing%2520predictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms&entry.906535625=Nobuyuki%20Ota&entry.1292438233=Current%20biological%20AI%20models%20lack%20interpretability%20--%20their%20internal%20representations%20do%20not%20correspond%20to%20biological%20relationships%20that%0A%20%20researchers%20can%20examine.%20Here%20we%20present%20CDT-II%2C%20an%20%22AI%20microscope%22%20whose%20attention%20maps%20are%20directly%20interpretable%20as%20regulatory%20structure.%0A%20%20By%20mirroring%20the%20central%20dogma%20in%20its%20architecture%2C%20each%20attention%20mechanism%20corresponds%20to%20a%20specific%20biological%20relationship%3A%20DNA%0A%20%20self-attention%20for%20genomic%20relationships%2C%20RNA%20self-attention%20for%20gene%20co-regulation%2C%20and%20DNA-to-RNA%20cross-attention%20for%20transcriptional%0A%20%20control.%20Using%20only%20genomic%20embeddings%20and%20raw%20per-cell%20expression%2C%20CDT-II%20enables%20experimental%20biologists%20to%20observe%20regulatory%20networks%20in%0A%20%20their%20own%20data.%20Applied%20to%20K562%20CRISPRi%20data%2C%20CDT-II%20predicts%20perturbation%20effects%20%28per-gene%20mean%20%24r%20%3D%200.84%24%29%20and%20recovers%20the%20GFI1B%0A%20%20regulatory%20network%20without%20supervision%20%286.6-fold%20enrichment%2C%20%24P%20%3D%203.5%20%5Ctimes%2010%5E%7B-17%7D%24%29.%20Two%20distinct%20attention%20mechanisms%20converge%20on%20an%20RNA%0A%20%20processing%20module%20%28%24P%20%3D%201%20%5Ctimes%2010%5E%7B-16%7D%24%29.%20CDT-II%20establishes%20mechanism-oriented%20AI%20as%20an%20alternative%20to%20task-oriented%20approaches%2C%20revealing%0A%20%20regulatory%20structure%20rather%20than%20merely%20optimizing%20predictions.&entry.1838667208=http%3A//arxiv.org/abs/2602.08751v1&entry.124074799=Read"},
{"title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs", "author": "Yukun Jiang and Hai Huang and Mingjie Li and Yage Zhang and Michael Backes and Yang Zhang", "abstract": "By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.", "link": "http://arxiv.org/abs/2602.08621v1", "date": "2026-02-09", "relevancy": 0.9634, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4799}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Models%2C%20Sparse%20Safety%3A%20Unsafe%20Routes%20in%20Mixture-of-Experts%20LLMs&body=Title%3A%20Sparse%20Models%2C%20Sparse%20Safety%3A%20Unsafe%20Routes%20in%20Mixture-of-Experts%20LLMs%0AAuthor%3A%20Yukun%20Jiang%20and%20Hai%20Huang%20and%20Mingjie%20Li%20and%20Yage%20Zhang%20and%20Michael%20Backes%20and%20Yang%20Zhang%0AAbstract%3A%20By%20introducing%20routers%20to%20selectively%20activate%20experts%20in%20Transformer%20layers%2C%20the%20mixture-of-experts%20%28MoE%29%20architecture%20significantly%20reduces%20computational%20costs%20in%20large%20language%20models%20%28LLMs%29%20while%20maintaining%20competitive%20performance%2C%20especially%20for%20models%20with%20massive%20parameters.%20However%2C%20prior%20work%20has%20largely%20focused%20on%20utility%20and%20efficiency%2C%20leaving%20the%20safety%20risks%20associated%20with%20this%20sparse%20architecture%20underexplored.%20In%20this%20work%2C%20we%20show%20that%20the%20safety%20of%20MoE%20LLMs%20is%20as%20sparse%20as%20their%20architecture%20by%20discovering%20unsafe%20routes%3A%20routing%20configurations%20that%2C%20once%20activated%2C%20convert%20safe%20outputs%20into%20harmful%20ones.%20Specifically%2C%20we%20first%20introduce%20the%20Router%20Safety%20importance%20score%20%28RoSais%29%20to%20quantify%20the%20safety%20criticality%20of%20each%20layer%27s%20router.%20Manipulation%20of%20only%20the%20high-RoSais%20router%28s%29%20can%20flip%20the%20default%20route%20into%20an%20unsafe%20one.%20For%20instance%2C%20on%20JailbreakBench%2C%20masking%205%20routers%20in%20DeepSeek-V2-Lite%20increases%20attack%20success%20rate%20%28ASR%29%20by%20over%204%24%5Ctimes%24%20to%200.79%2C%20highlighting%20an%20inherent%20risk%20that%20router%20manipulation%20may%20naturally%20occur%20in%20MoE%20LLMs.%20We%20further%20propose%20a%20Fine-grained%20token-layer-wise%20Stochastic%20Optimization%20framework%20to%20discover%20more%20concrete%20Unsafe%20Routes%20%28F-SOUR%29%2C%20which%20explicitly%20considers%20the%20sequentiality%20and%20dynamics%20of%20input%20tokens.%20Across%20four%20representative%20MoE%20LLM%20families%2C%20F-SOUR%20achieves%20an%20average%20ASR%20of%200.90%20and%200.98%20on%20JailbreakBench%20and%20AdvBench%2C%20respectively.%20Finally%2C%20we%20outline%20defensive%20perspectives%2C%20including%20safety-aware%20route%20disabling%20and%20router%20training%2C%20as%20promising%20directions%20to%20safeguard%20MoE%20LLMs.%20We%20hope%20our%20work%20can%20inform%20future%20red-teaming%20and%20safeguarding%20of%20MoE%20LLMs.%20Our%20code%20is%20provided%20in%20https%3A//github.com/TrustAIRLab/UnsafeMoE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Models%252C%2520Sparse%2520Safety%253A%2520Unsafe%2520Routes%2520in%2520Mixture-of-Experts%2520LLMs%26entry.906535625%3DYukun%2520Jiang%2520and%2520Hai%2520Huang%2520and%2520Mingjie%2520Li%2520and%2520Yage%2520Zhang%2520and%2520Michael%2520Backes%2520and%2520Yang%2520Zhang%26entry.1292438233%3DBy%2520introducing%2520routers%2520to%2520selectively%2520activate%2520experts%2520in%2520Transformer%2520layers%252C%2520the%2520mixture-of-experts%2520%2528MoE%2529%2520architecture%2520significantly%2520reduces%2520computational%2520costs%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520while%2520maintaining%2520competitive%2520performance%252C%2520especially%2520for%2520models%2520with%2520massive%2520parameters.%2520However%252C%2520prior%2520work%2520has%2520largely%2520focused%2520on%2520utility%2520and%2520efficiency%252C%2520leaving%2520the%2520safety%2520risks%2520associated%2520with%2520this%2520sparse%2520architecture%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520safety%2520of%2520MoE%2520LLMs%2520is%2520as%2520sparse%2520as%2520their%2520architecture%2520by%2520discovering%2520unsafe%2520routes%253A%2520routing%2520configurations%2520that%252C%2520once%2520activated%252C%2520convert%2520safe%2520outputs%2520into%2520harmful%2520ones.%2520Specifically%252C%2520we%2520first%2520introduce%2520the%2520Router%2520Safety%2520importance%2520score%2520%2528RoSais%2529%2520to%2520quantify%2520the%2520safety%2520criticality%2520of%2520each%2520layer%2527s%2520router.%2520Manipulation%2520of%2520only%2520the%2520high-RoSais%2520router%2528s%2529%2520can%2520flip%2520the%2520default%2520route%2520into%2520an%2520unsafe%2520one.%2520For%2520instance%252C%2520on%2520JailbreakBench%252C%2520masking%25205%2520routers%2520in%2520DeepSeek-V2-Lite%2520increases%2520attack%2520success%2520rate%2520%2528ASR%2529%2520by%2520over%25204%2524%255Ctimes%2524%2520to%25200.79%252C%2520highlighting%2520an%2520inherent%2520risk%2520that%2520router%2520manipulation%2520may%2520naturally%2520occur%2520in%2520MoE%2520LLMs.%2520We%2520further%2520propose%2520a%2520Fine-grained%2520token-layer-wise%2520Stochastic%2520Optimization%2520framework%2520to%2520discover%2520more%2520concrete%2520Unsafe%2520Routes%2520%2528F-SOUR%2529%252C%2520which%2520explicitly%2520considers%2520the%2520sequentiality%2520and%2520dynamics%2520of%2520input%2520tokens.%2520Across%2520four%2520representative%2520MoE%2520LLM%2520families%252C%2520F-SOUR%2520achieves%2520an%2520average%2520ASR%2520of%25200.90%2520and%25200.98%2520on%2520JailbreakBench%2520and%2520AdvBench%252C%2520respectively.%2520Finally%252C%2520we%2520outline%2520defensive%2520perspectives%252C%2520including%2520safety-aware%2520route%2520disabling%2520and%2520router%2520training%252C%2520as%2520promising%2520directions%2520to%2520safeguard%2520MoE%2520LLMs.%2520We%2520hope%2520our%2520work%2520can%2520inform%2520future%2520red-teaming%2520and%2520safeguarding%2520of%2520MoE%2520LLMs.%2520Our%2520code%2520is%2520provided%2520in%2520https%253A//github.com/TrustAIRLab/UnsafeMoE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Models%2C%20Sparse%20Safety%3A%20Unsafe%20Routes%20in%20Mixture-of-Experts%20LLMs&entry.906535625=Yukun%20Jiang%20and%20Hai%20Huang%20and%20Mingjie%20Li%20and%20Yage%20Zhang%20and%20Michael%20Backes%20and%20Yang%20Zhang&entry.1292438233=By%20introducing%20routers%20to%20selectively%20activate%20experts%20in%20Transformer%20layers%2C%20the%20mixture-of-experts%20%28MoE%29%20architecture%20significantly%20reduces%20computational%20costs%20in%20large%20language%20models%20%28LLMs%29%20while%20maintaining%20competitive%20performance%2C%20especially%20for%20models%20with%20massive%20parameters.%20However%2C%20prior%20work%20has%20largely%20focused%20on%20utility%20and%20efficiency%2C%20leaving%20the%20safety%20risks%20associated%20with%20this%20sparse%20architecture%20underexplored.%20In%20this%20work%2C%20we%20show%20that%20the%20safety%20of%20MoE%20LLMs%20is%20as%20sparse%20as%20their%20architecture%20by%20discovering%20unsafe%20routes%3A%20routing%20configurations%20that%2C%20once%20activated%2C%20convert%20safe%20outputs%20into%20harmful%20ones.%20Specifically%2C%20we%20first%20introduce%20the%20Router%20Safety%20importance%20score%20%28RoSais%29%20to%20quantify%20the%20safety%20criticality%20of%20each%20layer%27s%20router.%20Manipulation%20of%20only%20the%20high-RoSais%20router%28s%29%20can%20flip%20the%20default%20route%20into%20an%20unsafe%20one.%20For%20instance%2C%20on%20JailbreakBench%2C%20masking%205%20routers%20in%20DeepSeek-V2-Lite%20increases%20attack%20success%20rate%20%28ASR%29%20by%20over%204%24%5Ctimes%24%20to%200.79%2C%20highlighting%20an%20inherent%20risk%20that%20router%20manipulation%20may%20naturally%20occur%20in%20MoE%20LLMs.%20We%20further%20propose%20a%20Fine-grained%20token-layer-wise%20Stochastic%20Optimization%20framework%20to%20discover%20more%20concrete%20Unsafe%20Routes%20%28F-SOUR%29%2C%20which%20explicitly%20considers%20the%20sequentiality%20and%20dynamics%20of%20input%20tokens.%20Across%20four%20representative%20MoE%20LLM%20families%2C%20F-SOUR%20achieves%20an%20average%20ASR%20of%200.90%20and%200.98%20on%20JailbreakBench%20and%20AdvBench%2C%20respectively.%20Finally%2C%20we%20outline%20defensive%20perspectives%2C%20including%20safety-aware%20route%20disabling%20and%20router%20training%2C%20as%20promising%20directions%20to%20safeguard%20MoE%20LLMs.%20We%20hope%20our%20work%20can%20inform%20future%20red-teaming%20and%20safeguarding%20of%20MoE%20LLMs.%20Our%20code%20is%20provided%20in%20https%3A//github.com/TrustAIRLab/UnsafeMoE.&entry.1838667208=http%3A//arxiv.org/abs/2602.08621v1&entry.124074799=Read"},
{"title": "iGRPO: Self-Feedback-Driven LLM Reasoning", "author": "Ali Hatamizadeh and Shrimai Prabhumoye and Igor Gitman and Ximing Lu and Seungju Han and Wei Ping and Yejin Choi and Jan Kautz", "abstract": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.", "link": "http://arxiv.org/abs/2602.09000v1", "date": "2026-02-09", "relevancy": 1.4846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.51}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iGRPO%3A%20Self-Feedback-Driven%20LLM%20Reasoning&body=Title%3A%20iGRPO%3A%20Self-Feedback-Driven%20LLM%20Reasoning%0AAuthor%3A%20Ali%20Hatamizadeh%20and%20Shrimai%20Prabhumoye%20and%20Igor%20Gitman%20and%20Ximing%20Lu%20and%20Seungju%20Han%20and%20Wei%20Ping%20and%20Yejin%20Choi%20and%20Jan%20Kautz%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20solving%20complex%20mathematical%20problems%2C%20yet%20they%20still%20fall%20short%20of%20producing%20accurate%20and%20consistent%20solutions.%20Reinforcement%20Learning%20%28RL%29%20is%20a%20framework%20for%20aligning%20these%20models%20with%20task-specific%20rewards%2C%20improving%20overall%20quality%20and%20reliability.%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20is%20an%20efficient%2C%20value-function-free%20alternative%20to%20Proximal%20Policy%20Optimization%20%28PPO%29%20that%20leverages%20group-relative%20reward%20normalization.%20We%20introduce%20Iterative%20Group%20Relative%20Policy%20Optimization%20%28iGRPO%29%2C%20a%20two-stage%20extension%20of%20GRPO%20that%20adds%20dynamic%20self-conditioning%20through%20model-generated%20drafts.%20In%20Stage%201%2C%20iGRPO%20samples%20multiple%20exploratory%20drafts%20and%20selects%20the%20highest-reward%20draft%20using%20the%20same%20scalar%20reward%20signal%20used%20for%20optimization.%20In%20Stage%202%2C%20it%20appends%20this%20best%20draft%20to%20the%20original%20prompt%20and%20applies%20a%20GRPO-style%20update%20on%20draft-conditioned%20refinements%2C%20training%20the%20policy%20to%20improve%20beyond%20its%20strongest%20prior%20attempt.%20Under%20matched%20rollout%20budgets%2C%20iGRPO%20consistently%20outperforms%20GRPO%20across%20base%20models%20%28e.g.%2C%20Nemotron-H-8B-Base-8K%20and%20DeepSeek-R1%20Distilled%29%2C%20validating%20its%20effectiveness%20on%20diverse%20reasoning%20benchmarks.%20Moreover%2C%20applying%20iGRPO%20to%20OpenReasoning-Nemotron-7B%20trained%20on%20AceReason-Math%20achieves%20new%20state-of-the-art%20results%20of%2085.62%5C%25%20and%2079.64%5C%25%20on%20AIME24%20and%20AIME25%2C%20respectively.%20Ablations%20further%20show%20that%20the%20refinement%20wrapper%20generalizes%20beyond%20GRPO%20variants%2C%20benefits%20from%20a%20generative%20judge%2C%20and%20alters%20learning%20dynamics%20by%20delaying%20entropy%20collapse.%20These%20results%20underscore%20the%20potential%20of%20iterative%2C%20self-feedback-based%20RL%20for%20advancing%20verifiable%20mathematical%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiGRPO%253A%2520Self-Feedback-Driven%2520LLM%2520Reasoning%26entry.906535625%3DAli%2520Hatamizadeh%2520and%2520Shrimai%2520Prabhumoye%2520and%2520Igor%2520Gitman%2520and%2520Ximing%2520Lu%2520and%2520Seungju%2520Han%2520and%2520Wei%2520Ping%2520and%2520Yejin%2520Choi%2520and%2520Jan%2520Kautz%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520solving%2520complex%2520mathematical%2520problems%252C%2520yet%2520they%2520still%2520fall%2520short%2520of%2520producing%2520accurate%2520and%2520consistent%2520solutions.%2520Reinforcement%2520Learning%2520%2528RL%2529%2520is%2520a%2520framework%2520for%2520aligning%2520these%2520models%2520with%2520task-specific%2520rewards%252C%2520improving%2520overall%2520quality%2520and%2520reliability.%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520is%2520an%2520efficient%252C%2520value-function-free%2520alternative%2520to%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520that%2520leverages%2520group-relative%2520reward%2520normalization.%2520We%2520introduce%2520Iterative%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528iGRPO%2529%252C%2520a%2520two-stage%2520extension%2520of%2520GRPO%2520that%2520adds%2520dynamic%2520self-conditioning%2520through%2520model-generated%2520drafts.%2520In%2520Stage%25201%252C%2520iGRPO%2520samples%2520multiple%2520exploratory%2520drafts%2520and%2520selects%2520the%2520highest-reward%2520draft%2520using%2520the%2520same%2520scalar%2520reward%2520signal%2520used%2520for%2520optimization.%2520In%2520Stage%25202%252C%2520it%2520appends%2520this%2520best%2520draft%2520to%2520the%2520original%2520prompt%2520and%2520applies%2520a%2520GRPO-style%2520update%2520on%2520draft-conditioned%2520refinements%252C%2520training%2520the%2520policy%2520to%2520improve%2520beyond%2520its%2520strongest%2520prior%2520attempt.%2520Under%2520matched%2520rollout%2520budgets%252C%2520iGRPO%2520consistently%2520outperforms%2520GRPO%2520across%2520base%2520models%2520%2528e.g.%252C%2520Nemotron-H-8B-Base-8K%2520and%2520DeepSeek-R1%2520Distilled%2529%252C%2520validating%2520its%2520effectiveness%2520on%2520diverse%2520reasoning%2520benchmarks.%2520Moreover%252C%2520applying%2520iGRPO%2520to%2520OpenReasoning-Nemotron-7B%2520trained%2520on%2520AceReason-Math%2520achieves%2520new%2520state-of-the-art%2520results%2520of%252085.62%255C%2525%2520and%252079.64%255C%2525%2520on%2520AIME24%2520and%2520AIME25%252C%2520respectively.%2520Ablations%2520further%2520show%2520that%2520the%2520refinement%2520wrapper%2520generalizes%2520beyond%2520GRPO%2520variants%252C%2520benefits%2520from%2520a%2520generative%2520judge%252C%2520and%2520alters%2520learning%2520dynamics%2520by%2520delaying%2520entropy%2520collapse.%2520These%2520results%2520underscore%2520the%2520potential%2520of%2520iterative%252C%2520self-feedback-based%2520RL%2520for%2520advancing%2520verifiable%2520mathematical%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iGRPO%3A%20Self-Feedback-Driven%20LLM%20Reasoning&entry.906535625=Ali%20Hatamizadeh%20and%20Shrimai%20Prabhumoye%20and%20Igor%20Gitman%20and%20Ximing%20Lu%20and%20Seungju%20Han%20and%20Wei%20Ping%20and%20Yejin%20Choi%20and%20Jan%20Kautz&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20solving%20complex%20mathematical%20problems%2C%20yet%20they%20still%20fall%20short%20of%20producing%20accurate%20and%20consistent%20solutions.%20Reinforcement%20Learning%20%28RL%29%20is%20a%20framework%20for%20aligning%20these%20models%20with%20task-specific%20rewards%2C%20improving%20overall%20quality%20and%20reliability.%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20is%20an%20efficient%2C%20value-function-free%20alternative%20to%20Proximal%20Policy%20Optimization%20%28PPO%29%20that%20leverages%20group-relative%20reward%20normalization.%20We%20introduce%20Iterative%20Group%20Relative%20Policy%20Optimization%20%28iGRPO%29%2C%20a%20two-stage%20extension%20of%20GRPO%20that%20adds%20dynamic%20self-conditioning%20through%20model-generated%20drafts.%20In%20Stage%201%2C%20iGRPO%20samples%20multiple%20exploratory%20drafts%20and%20selects%20the%20highest-reward%20draft%20using%20the%20same%20scalar%20reward%20signal%20used%20for%20optimization.%20In%20Stage%202%2C%20it%20appends%20this%20best%20draft%20to%20the%20original%20prompt%20and%20applies%20a%20GRPO-style%20update%20on%20draft-conditioned%20refinements%2C%20training%20the%20policy%20to%20improve%20beyond%20its%20strongest%20prior%20attempt.%20Under%20matched%20rollout%20budgets%2C%20iGRPO%20consistently%20outperforms%20GRPO%20across%20base%20models%20%28e.g.%2C%20Nemotron-H-8B-Base-8K%20and%20DeepSeek-R1%20Distilled%29%2C%20validating%20its%20effectiveness%20on%20diverse%20reasoning%20benchmarks.%20Moreover%2C%20applying%20iGRPO%20to%20OpenReasoning-Nemotron-7B%20trained%20on%20AceReason-Math%20achieves%20new%20state-of-the-art%20results%20of%2085.62%5C%25%20and%2079.64%5C%25%20on%20AIME24%20and%20AIME25%2C%20respectively.%20Ablations%20further%20show%20that%20the%20refinement%20wrapper%20generalizes%20beyond%20GRPO%20variants%2C%20benefits%20from%20a%20generative%20judge%2C%20and%20alters%20learning%20dynamics%20by%20delaying%20entropy%20collapse.%20These%20results%20underscore%20the%20potential%20of%20iterative%2C%20self-feedback-based%20RL%20for%20advancing%20verifiable%20mathematical%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.09000v1&entry.124074799=Read"},
{"title": "Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond", "author": "Kazuma Sawaya", "abstract": "We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.\n  Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.", "link": "http://arxiv.org/abs/2512.04696v2", "date": "2026-02-09", "relevancy": 1.8675, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20FDR%20Control%20for%20Deep%20Feature%20Selection%3A%20Deep%20MLPs%20and%20Beyond&body=Title%3A%20Provable%20FDR%20Control%20for%20Deep%20Feature%20Selection%3A%20Deep%20MLPs%20and%20Beyond%0AAuthor%3A%20Kazuma%20Sawaya%0AAbstract%3A%20We%20develop%20a%20flexible%20feature%20selection%20framework%20based%20on%20deep%20neural%20networks%20that%20approximately%20controls%20the%20false%20discovery%20rate%20%28FDR%29%2C%20a%20measure%20of%20Type-I%20error.%20The%20method%20applies%20to%20architectures%20whose%20first%20layer%20is%20fully%20connected.%20From%20the%20second%20layer%20onward%2C%20it%20accommodates%20multilayer%20perceptrons%20%28MLPs%29%20of%20arbitrary%20width%20and%20depth%2C%20convolutional%20and%20recurrent%20networks%2C%20attention%20mechanisms%2C%20residual%20connections%2C%20and%20dropout.%20The%20procedure%20also%20accommodates%20stochastic%20gradient%20descent%20with%20data-independent%20initializations%20and%20learning%20rates.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20provide%20a%20theoretical%20guarantee%20of%20FDR%20control%20for%20feature%20selection%20within%20such%20a%20general%20deep%20learning%20setting.%0A%20%20Our%20analysis%20is%20built%20upon%20a%20multi-index%20data-generating%20model%20and%20an%20asymptotic%20regime%20in%20which%20the%20feature%20dimension%20%24n%24%20diverges%20faster%20than%20the%20latent%20dimension%20%24q%5E%7B%2A%7D%24%2C%20while%20the%20sample%20size%2C%20the%20number%20of%20training%20iterations%2C%20the%20network%20depth%2C%20and%20hidden%20layer%20widths%20are%20left%20unrestricted.%20Under%20this%20setting%2C%20we%20show%20that%20each%20coordinate%20of%20the%20gradient-based%20feature-importance%20vector%20admits%20a%20marginal%20normal%20approximation%2C%20thereby%20supporting%20the%20validity%20of%20asymptotic%20FDR%20control.%20As%20a%20theoretical%20limitation%2C%20we%20assume%20%24%5Cmathbf%7BB%7D%24-right%20orthogonal%20invariance%20of%20the%20design%20matrix%2C%20and%20we%20discuss%20broader%20generalizations.%20We%20also%20present%20numerical%20experiments%20that%20underscore%20the%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520FDR%2520Control%2520for%2520Deep%2520Feature%2520Selection%253A%2520Deep%2520MLPs%2520and%2520Beyond%26entry.906535625%3DKazuma%2520Sawaya%26entry.1292438233%3DWe%2520develop%2520a%2520flexible%2520feature%2520selection%2520framework%2520based%2520on%2520deep%2520neural%2520networks%2520that%2520approximately%2520controls%2520the%2520false%2520discovery%2520rate%2520%2528FDR%2529%252C%2520a%2520measure%2520of%2520Type-I%2520error.%2520The%2520method%2520applies%2520to%2520architectures%2520whose%2520first%2520layer%2520is%2520fully%2520connected.%2520From%2520the%2520second%2520layer%2520onward%252C%2520it%2520accommodates%2520multilayer%2520perceptrons%2520%2528MLPs%2529%2520of%2520arbitrary%2520width%2520and%2520depth%252C%2520convolutional%2520and%2520recurrent%2520networks%252C%2520attention%2520mechanisms%252C%2520residual%2520connections%252C%2520and%2520dropout.%2520The%2520procedure%2520also%2520accommodates%2520stochastic%2520gradient%2520descent%2520with%2520data-independent%2520initializations%2520and%2520learning%2520rates.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520provide%2520a%2520theoretical%2520guarantee%2520of%2520FDR%2520control%2520for%2520feature%2520selection%2520within%2520such%2520a%2520general%2520deep%2520learning%2520setting.%250A%2520%2520Our%2520analysis%2520is%2520built%2520upon%2520a%2520multi-index%2520data-generating%2520model%2520and%2520an%2520asymptotic%2520regime%2520in%2520which%2520the%2520feature%2520dimension%2520%2524n%2524%2520diverges%2520faster%2520than%2520the%2520latent%2520dimension%2520%2524q%255E%257B%252A%257D%2524%252C%2520while%2520the%2520sample%2520size%252C%2520the%2520number%2520of%2520training%2520iterations%252C%2520the%2520network%2520depth%252C%2520and%2520hidden%2520layer%2520widths%2520are%2520left%2520unrestricted.%2520Under%2520this%2520setting%252C%2520we%2520show%2520that%2520each%2520coordinate%2520of%2520the%2520gradient-based%2520feature-importance%2520vector%2520admits%2520a%2520marginal%2520normal%2520approximation%252C%2520thereby%2520supporting%2520the%2520validity%2520of%2520asymptotic%2520FDR%2520control.%2520As%2520a%2520theoretical%2520limitation%252C%2520we%2520assume%2520%2524%255Cmathbf%257BB%257D%2524-right%2520orthogonal%2520invariance%2520of%2520the%2520design%2520matrix%252C%2520and%2520we%2520discuss%2520broader%2520generalizations.%2520We%2520also%2520present%2520numerical%2520experiments%2520that%2520underscore%2520the%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20FDR%20Control%20for%20Deep%20Feature%20Selection%3A%20Deep%20MLPs%20and%20Beyond&entry.906535625=Kazuma%20Sawaya&entry.1292438233=We%20develop%20a%20flexible%20feature%20selection%20framework%20based%20on%20deep%20neural%20networks%20that%20approximately%20controls%20the%20false%20discovery%20rate%20%28FDR%29%2C%20a%20measure%20of%20Type-I%20error.%20The%20method%20applies%20to%20architectures%20whose%20first%20layer%20is%20fully%20connected.%20From%20the%20second%20layer%20onward%2C%20it%20accommodates%20multilayer%20perceptrons%20%28MLPs%29%20of%20arbitrary%20width%20and%20depth%2C%20convolutional%20and%20recurrent%20networks%2C%20attention%20mechanisms%2C%20residual%20connections%2C%20and%20dropout.%20The%20procedure%20also%20accommodates%20stochastic%20gradient%20descent%20with%20data-independent%20initializations%20and%20learning%20rates.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20provide%20a%20theoretical%20guarantee%20of%20FDR%20control%20for%20feature%20selection%20within%20such%20a%20general%20deep%20learning%20setting.%0A%20%20Our%20analysis%20is%20built%20upon%20a%20multi-index%20data-generating%20model%20and%20an%20asymptotic%20regime%20in%20which%20the%20feature%20dimension%20%24n%24%20diverges%20faster%20than%20the%20latent%20dimension%20%24q%5E%7B%2A%7D%24%2C%20while%20the%20sample%20size%2C%20the%20number%20of%20training%20iterations%2C%20the%20network%20depth%2C%20and%20hidden%20layer%20widths%20are%20left%20unrestricted.%20Under%20this%20setting%2C%20we%20show%20that%20each%20coordinate%20of%20the%20gradient-based%20feature-importance%20vector%20admits%20a%20marginal%20normal%20approximation%2C%20thereby%20supporting%20the%20validity%20of%20asymptotic%20FDR%20control.%20As%20a%20theoretical%20limitation%2C%20we%20assume%20%24%5Cmathbf%7BB%7D%24-right%20orthogonal%20invariance%20of%20the%20design%20matrix%2C%20and%20we%20discuss%20broader%20generalizations.%20We%20also%20present%20numerical%20experiments%20that%20underscore%20the%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2512.04696v2&entry.124074799=Read"},
{"title": "GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems", "author": "Kate\u0159ina Henclov\u00e1 and V\u00e1clav \u0160m\u00eddl", "abstract": "Selecting interpretable feature sets in underdetermined ($n \\ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.\n  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.\n  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.\n  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.", "link": "http://arxiv.org/abs/2602.08913v1", "date": "2026-02-09", "relevancy": 2.0684, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5211}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5161}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEMSS%3A%20A%20Variational%20Bayesian%20Method%20for%20Discovering%20Multiple%20Sparse%20Solutions%20in%20Classification%20and%20Regression%20Problems&body=Title%3A%20GEMSS%3A%20A%20Variational%20Bayesian%20Method%20for%20Discovering%20Multiple%20Sparse%20Solutions%20in%20Classification%20and%20Regression%20Problems%0AAuthor%3A%20Kate%C5%99ina%20Henclov%C3%A1%20and%20V%C3%A1clav%20%C5%A0m%C3%ADdl%0AAbstract%3A%20Selecting%20interpretable%20feature%20sets%20in%20underdetermined%20%28%24n%20%5Cll%20p%24%29%20and%20highly%20correlated%20regimes%20constitutes%20a%20fundamental%20challenge%20in%20data%20science%2C%20particularly%20when%20analyzing%20physical%20measurements.%20In%20such%20settings%2C%20multiple%20distinct%20sparse%20subsets%20may%20explain%20the%20response%20equally%20well.%20Identifying%20these%20alternatives%20is%20crucial%20for%20generating%20domain-specific%20insights%20into%20the%20underlying%20mechanisms%2C%20yet%20conventional%20methods%20typically%20isolate%20a%20single%20solution%2C%20obscuring%20the%20full%20spectrum%20of%20plausible%20explanations.%0A%20%20We%20present%20GEMSS%20%28Gaussian%20Ensemble%20for%20Multiple%20Sparse%20Solutions%29%2C%20a%20variational%20Bayesian%20framework%20specifically%20designed%20to%20simultaneously%20discover%20multiple%2C%20diverse%20sparse%20feature%20combinations.%20The%20method%20employs%20a%20structured%20spike-and-slab%20prior%20for%20sparsity%2C%20a%20mixture%20of%20Gaussians%20to%20approximate%20the%20intractable%20multimodal%20posterior%2C%20and%20a%20Jaccard-based%20penalty%20to%20further%20control%20solution%20diversity.%20Unlike%20sequential%20greedy%20approaches%2C%20GEMSS%20optimizes%20the%20entire%20ensemble%20of%20solutions%20within%20a%20single%20objective%20function%20via%20stochastic%20gradient%20descent.%0A%20%20The%20method%20is%20validated%20on%20a%20comprehensive%20benchmark%20comprising%20128%20synthetic%20experiments%20across%20classification%20and%20regression%20tasks.%20Results%20demonstrate%20that%20GEMSS%20scales%20effectively%20to%20high-dimensional%20settings%20%28%24p%3D5000%24%29%20with%20sample%20size%20as%20small%20as%20%24n%20%3D%2050%24%2C%20generalizes%20seamlessly%20to%20continuous%20targets%2C%20handles%20missing%20data%20natively%2C%20and%20exhibits%20remarkable%20robustness%20to%20class%20imbalance%20and%20Gaussian%20noise.%0A%20%20GEMSS%20is%20available%20as%20a%20Python%20package%20%27gemss%27%20at%20PyPI.%20The%20full%20GitHub%20repository%20at%20https%3A//github.com/kat-er-ina/gemss/%20also%20includes%20a%20free%2C%20easy-to-use%20application%20suitable%20for%20non-coders.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEMSS%253A%2520A%2520Variational%2520Bayesian%2520Method%2520for%2520Discovering%2520Multiple%2520Sparse%2520Solutions%2520in%2520Classification%2520and%2520Regression%2520Problems%26entry.906535625%3DKate%25C5%2599ina%2520Henclov%25C3%25A1%2520and%2520V%25C3%25A1clav%2520%25C5%25A0m%25C3%25ADdl%26entry.1292438233%3DSelecting%2520interpretable%2520feature%2520sets%2520in%2520underdetermined%2520%2528%2524n%2520%255Cll%2520p%2524%2529%2520and%2520highly%2520correlated%2520regimes%2520constitutes%2520a%2520fundamental%2520challenge%2520in%2520data%2520science%252C%2520particularly%2520when%2520analyzing%2520physical%2520measurements.%2520In%2520such%2520settings%252C%2520multiple%2520distinct%2520sparse%2520subsets%2520may%2520explain%2520the%2520response%2520equally%2520well.%2520Identifying%2520these%2520alternatives%2520is%2520crucial%2520for%2520generating%2520domain-specific%2520insights%2520into%2520the%2520underlying%2520mechanisms%252C%2520yet%2520conventional%2520methods%2520typically%2520isolate%2520a%2520single%2520solution%252C%2520obscuring%2520the%2520full%2520spectrum%2520of%2520plausible%2520explanations.%250A%2520%2520We%2520present%2520GEMSS%2520%2528Gaussian%2520Ensemble%2520for%2520Multiple%2520Sparse%2520Solutions%2529%252C%2520a%2520variational%2520Bayesian%2520framework%2520specifically%2520designed%2520to%2520simultaneously%2520discover%2520multiple%252C%2520diverse%2520sparse%2520feature%2520combinations.%2520The%2520method%2520employs%2520a%2520structured%2520spike-and-slab%2520prior%2520for%2520sparsity%252C%2520a%2520mixture%2520of%2520Gaussians%2520to%2520approximate%2520the%2520intractable%2520multimodal%2520posterior%252C%2520and%2520a%2520Jaccard-based%2520penalty%2520to%2520further%2520control%2520solution%2520diversity.%2520Unlike%2520sequential%2520greedy%2520approaches%252C%2520GEMSS%2520optimizes%2520the%2520entire%2520ensemble%2520of%2520solutions%2520within%2520a%2520single%2520objective%2520function%2520via%2520stochastic%2520gradient%2520descent.%250A%2520%2520The%2520method%2520is%2520validated%2520on%2520a%2520comprehensive%2520benchmark%2520comprising%2520128%2520synthetic%2520experiments%2520across%2520classification%2520and%2520regression%2520tasks.%2520Results%2520demonstrate%2520that%2520GEMSS%2520scales%2520effectively%2520to%2520high-dimensional%2520settings%2520%2528%2524p%253D5000%2524%2529%2520with%2520sample%2520size%2520as%2520small%2520as%2520%2524n%2520%253D%252050%2524%252C%2520generalizes%2520seamlessly%2520to%2520continuous%2520targets%252C%2520handles%2520missing%2520data%2520natively%252C%2520and%2520exhibits%2520remarkable%2520robustness%2520to%2520class%2520imbalance%2520and%2520Gaussian%2520noise.%250A%2520%2520GEMSS%2520is%2520available%2520as%2520a%2520Python%2520package%2520%2527gemss%2527%2520at%2520PyPI.%2520The%2520full%2520GitHub%2520repository%2520at%2520https%253A//github.com/kat-er-ina/gemss/%2520also%2520includes%2520a%2520free%252C%2520easy-to-use%2520application%2520suitable%2520for%2520non-coders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEMSS%3A%20A%20Variational%20Bayesian%20Method%20for%20Discovering%20Multiple%20Sparse%20Solutions%20in%20Classification%20and%20Regression%20Problems&entry.906535625=Kate%C5%99ina%20Henclov%C3%A1%20and%20V%C3%A1clav%20%C5%A0m%C3%ADdl&entry.1292438233=Selecting%20interpretable%20feature%20sets%20in%20underdetermined%20%28%24n%20%5Cll%20p%24%29%20and%20highly%20correlated%20regimes%20constitutes%20a%20fundamental%20challenge%20in%20data%20science%2C%20particularly%20when%20analyzing%20physical%20measurements.%20In%20such%20settings%2C%20multiple%20distinct%20sparse%20subsets%20may%20explain%20the%20response%20equally%20well.%20Identifying%20these%20alternatives%20is%20crucial%20for%20generating%20domain-specific%20insights%20into%20the%20underlying%20mechanisms%2C%20yet%20conventional%20methods%20typically%20isolate%20a%20single%20solution%2C%20obscuring%20the%20full%20spectrum%20of%20plausible%20explanations.%0A%20%20We%20present%20GEMSS%20%28Gaussian%20Ensemble%20for%20Multiple%20Sparse%20Solutions%29%2C%20a%20variational%20Bayesian%20framework%20specifically%20designed%20to%20simultaneously%20discover%20multiple%2C%20diverse%20sparse%20feature%20combinations.%20The%20method%20employs%20a%20structured%20spike-and-slab%20prior%20for%20sparsity%2C%20a%20mixture%20of%20Gaussians%20to%20approximate%20the%20intractable%20multimodal%20posterior%2C%20and%20a%20Jaccard-based%20penalty%20to%20further%20control%20solution%20diversity.%20Unlike%20sequential%20greedy%20approaches%2C%20GEMSS%20optimizes%20the%20entire%20ensemble%20of%20solutions%20within%20a%20single%20objective%20function%20via%20stochastic%20gradient%20descent.%0A%20%20The%20method%20is%20validated%20on%20a%20comprehensive%20benchmark%20comprising%20128%20synthetic%20experiments%20across%20classification%20and%20regression%20tasks.%20Results%20demonstrate%20that%20GEMSS%20scales%20effectively%20to%20high-dimensional%20settings%20%28%24p%3D5000%24%29%20with%20sample%20size%20as%20small%20as%20%24n%20%3D%2050%24%2C%20generalizes%20seamlessly%20to%20continuous%20targets%2C%20handles%20missing%20data%20natively%2C%20and%20exhibits%20remarkable%20robustness%20to%20class%20imbalance%20and%20Gaussian%20noise.%0A%20%20GEMSS%20is%20available%20as%20a%20Python%20package%20%27gemss%27%20at%20PyPI.%20The%20full%20GitHub%20repository%20at%20https%3A//github.com/kat-er-ina/gemss/%20also%20includes%20a%20free%2C%20easy-to-use%20application%20suitable%20for%20non-coders.&entry.1838667208=http%3A//arxiv.org/abs/2602.08913v1&entry.124074799=Read"},
{"title": "Trajectory Stitching for Solving Inverse Problems with Flow-Based Models", "author": "Alexander Denker and Moshe Eliasof and Zeljko Kereta and Carola-Bibiane Sch\u00f6nlieb", "abstract": "Flow-based generative models have emerged as powerful priors for solving inverse problems. One option is to directly optimize the initial latent code (noise), such that the flow output solves the inverse problem. However, this requires backpropagating through the entire generative trajectory, incurring high memory costs and numerical instability. We propose MS-Flow, which represents the trajectory as a sequence of intermediate latent states rather than a single initial code. By enforcing the flow dynamics locally and coupling segments through trajectory-matching penalties, MS-Flow alternates between updating intermediate latent states and enforcing consistency with observed data. This reduces memory consumption while improving reconstruction quality. We demonstrate the effectiveness of MS-Flow over existing methods on image recovery and inverse problems, including inpainting, super-resolution, and computed tomography.", "link": "http://arxiv.org/abs/2602.08538v1", "date": "2026-02-09", "relevancy": 1.5775, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5882}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5111}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Stitching%20for%20Solving%20Inverse%20Problems%20with%20Flow-Based%20Models&body=Title%3A%20Trajectory%20Stitching%20for%20Solving%20Inverse%20Problems%20with%20Flow-Based%20Models%0AAuthor%3A%20Alexander%20Denker%20and%20Moshe%20Eliasof%20and%20Zeljko%20Kereta%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20Flow-based%20generative%20models%20have%20emerged%20as%20powerful%20priors%20for%20solving%20inverse%20problems.%20One%20option%20is%20to%20directly%20optimize%20the%20initial%20latent%20code%20%28noise%29%2C%20such%20that%20the%20flow%20output%20solves%20the%20inverse%20problem.%20However%2C%20this%20requires%20backpropagating%20through%20the%20entire%20generative%20trajectory%2C%20incurring%20high%20memory%20costs%20and%20numerical%20instability.%20We%20propose%20MS-Flow%2C%20which%20represents%20the%20trajectory%20as%20a%20sequence%20of%20intermediate%20latent%20states%20rather%20than%20a%20single%20initial%20code.%20By%20enforcing%20the%20flow%20dynamics%20locally%20and%20coupling%20segments%20through%20trajectory-matching%20penalties%2C%20MS-Flow%20alternates%20between%20updating%20intermediate%20latent%20states%20and%20enforcing%20consistency%20with%20observed%20data.%20This%20reduces%20memory%20consumption%20while%20improving%20reconstruction%20quality.%20We%20demonstrate%20the%20effectiveness%20of%20MS-Flow%20over%20existing%20methods%20on%20image%20recovery%20and%20inverse%20problems%2C%20including%20inpainting%2C%20super-resolution%2C%20and%20computed%20tomography.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Stitching%2520for%2520Solving%2520Inverse%2520Problems%2520with%2520Flow-Based%2520Models%26entry.906535625%3DAlexander%2520Denker%2520and%2520Moshe%2520Eliasof%2520and%2520Zeljko%2520Kereta%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3DFlow-based%2520generative%2520models%2520have%2520emerged%2520as%2520powerful%2520priors%2520for%2520solving%2520inverse%2520problems.%2520One%2520option%2520is%2520to%2520directly%2520optimize%2520the%2520initial%2520latent%2520code%2520%2528noise%2529%252C%2520such%2520that%2520the%2520flow%2520output%2520solves%2520the%2520inverse%2520problem.%2520However%252C%2520this%2520requires%2520backpropagating%2520through%2520the%2520entire%2520generative%2520trajectory%252C%2520incurring%2520high%2520memory%2520costs%2520and%2520numerical%2520instability.%2520We%2520propose%2520MS-Flow%252C%2520which%2520represents%2520the%2520trajectory%2520as%2520a%2520sequence%2520of%2520intermediate%2520latent%2520states%2520rather%2520than%2520a%2520single%2520initial%2520code.%2520By%2520enforcing%2520the%2520flow%2520dynamics%2520locally%2520and%2520coupling%2520segments%2520through%2520trajectory-matching%2520penalties%252C%2520MS-Flow%2520alternates%2520between%2520updating%2520intermediate%2520latent%2520states%2520and%2520enforcing%2520consistency%2520with%2520observed%2520data.%2520This%2520reduces%2520memory%2520consumption%2520while%2520improving%2520reconstruction%2520quality.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520MS-Flow%2520over%2520existing%2520methods%2520on%2520image%2520recovery%2520and%2520inverse%2520problems%252C%2520including%2520inpainting%252C%2520super-resolution%252C%2520and%2520computed%2520tomography.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Stitching%20for%20Solving%20Inverse%20Problems%20with%20Flow-Based%20Models&entry.906535625=Alexander%20Denker%20and%20Moshe%20Eliasof%20and%20Zeljko%20Kereta%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=Flow-based%20generative%20models%20have%20emerged%20as%20powerful%20priors%20for%20solving%20inverse%20problems.%20One%20option%20is%20to%20directly%20optimize%20the%20initial%20latent%20code%20%28noise%29%2C%20such%20that%20the%20flow%20output%20solves%20the%20inverse%20problem.%20However%2C%20this%20requires%20backpropagating%20through%20the%20entire%20generative%20trajectory%2C%20incurring%20high%20memory%20costs%20and%20numerical%20instability.%20We%20propose%20MS-Flow%2C%20which%20represents%20the%20trajectory%20as%20a%20sequence%20of%20intermediate%20latent%20states%20rather%20than%20a%20single%20initial%20code.%20By%20enforcing%20the%20flow%20dynamics%20locally%20and%20coupling%20segments%20through%20trajectory-matching%20penalties%2C%20MS-Flow%20alternates%20between%20updating%20intermediate%20latent%20states%20and%20enforcing%20consistency%20with%20observed%20data.%20This%20reduces%20memory%20consumption%20while%20improving%20reconstruction%20quality.%20We%20demonstrate%20the%20effectiveness%20of%20MS-Flow%20over%20existing%20methods%20on%20image%20recovery%20and%20inverse%20problems%2C%20including%20inpainting%2C%20super-resolution%2C%20and%20computed%20tomography.&entry.1838667208=http%3A//arxiv.org/abs/2602.08538v1&entry.124074799=Read"},
{"title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models", "author": "Zehan Wang and Tengfei Wang and Haiyu Zhang and Xuhui Zuo and Junta Wu and Haoyuan Wang and Wenqiang Sun and Zhenwei Wang and Chenjie Cao and Hengshuang Zhao and Chunchao Guo and Zhou Zhao", "abstract": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.", "link": "http://arxiv.org/abs/2602.09022v1", "date": "2026-02-09", "relevancy": 1.6787, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6192}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5436}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldCompass%3A%20Reinforcement%20Learning%20for%20Long-Horizon%20World%20Models&body=Title%3A%20WorldCompass%3A%20Reinforcement%20Learning%20for%20Long-Horizon%20World%20Models%0AAuthor%3A%20Zehan%20Wang%20and%20Tengfei%20Wang%20and%20Haiyu%20Zhang%20and%20Xuhui%20Zuo%20and%20Junta%20Wu%20and%20Haoyuan%20Wang%20and%20Wenqiang%20Sun%20and%20Zhenwei%20Wang%20and%20Chenjie%20Cao%20and%20Hengshuang%20Zhao%20and%20Chunchao%20Guo%20and%20Zhou%20Zhao%0AAbstract%3A%20This%20work%20presents%20WorldCompass%2C%20a%20novel%20Reinforcement%20Learning%20%28RL%29%20post-training%20framework%20for%20the%20long-horizon%2C%20interactive%20video-based%20world%20models%2C%20enabling%20them%20to%20explore%20the%20world%20more%20accurately%20and%20consistently%20based%20on%20interaction%20signals.%20To%20effectively%20%22steer%22%20the%20world%20model%27s%20exploration%2C%20we%20introduce%20three%20core%20innovations%20tailored%20to%20the%20autoregressive%20video%20generation%20paradigm%3A%201%29%20Clip-level%20rollout%20Strategy%3A%20We%20generate%20and%20evaluate%20multiple%20samples%20at%20a%20single%20target%20clip%2C%20which%20significantly%20boosts%20rollout%20efficiency%20and%20provides%20fine-grained%20reward%20signals.%202%29%20Complementary%20Reward%20Functions%3A%20We%20design%20reward%20functions%20for%20both%20interaction-following%20accuracy%20and%20visual%20quality%2C%20which%20provide%20direct%20supervision%20and%20effectively%20suppress%20reward-hacking%20behaviors.%203%29%20Efficient%20RL%20Algorithm%3A%20We%20employ%20the%20negative-aware%20fine-tuning%20strategy%20coupled%20with%20various%20efficiency%20optimizations%20to%20efficiently%20and%20effectively%20enhance%20model%20capacity.%20Evaluations%20on%20the%20SoTA%20open-source%20world%20model%2C%20WorldPlay%2C%20demonstrate%20that%20WorldCompass%20significantly%20improves%20interaction%20accuracy%20and%20visual%20fidelity%20across%20various%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldCompass%253A%2520Reinforcement%2520Learning%2520for%2520Long-Horizon%2520World%2520Models%26entry.906535625%3DZehan%2520Wang%2520and%2520Tengfei%2520Wang%2520and%2520Haiyu%2520Zhang%2520and%2520Xuhui%2520Zuo%2520and%2520Junta%2520Wu%2520and%2520Haoyuan%2520Wang%2520and%2520Wenqiang%2520Sun%2520and%2520Zhenwei%2520Wang%2520and%2520Chenjie%2520Cao%2520and%2520Hengshuang%2520Zhao%2520and%2520Chunchao%2520Guo%2520and%2520Zhou%2520Zhao%26entry.1292438233%3DThis%2520work%2520presents%2520WorldCompass%252C%2520a%2520novel%2520Reinforcement%2520Learning%2520%2528RL%2529%2520post-training%2520framework%2520for%2520the%2520long-horizon%252C%2520interactive%2520video-based%2520world%2520models%252C%2520enabling%2520them%2520to%2520explore%2520the%2520world%2520more%2520accurately%2520and%2520consistently%2520based%2520on%2520interaction%2520signals.%2520To%2520effectively%2520%2522steer%2522%2520the%2520world%2520model%2527s%2520exploration%252C%2520we%2520introduce%2520three%2520core%2520innovations%2520tailored%2520to%2520the%2520autoregressive%2520video%2520generation%2520paradigm%253A%25201%2529%2520Clip-level%2520rollout%2520Strategy%253A%2520We%2520generate%2520and%2520evaluate%2520multiple%2520samples%2520at%2520a%2520single%2520target%2520clip%252C%2520which%2520significantly%2520boosts%2520rollout%2520efficiency%2520and%2520provides%2520fine-grained%2520reward%2520signals.%25202%2529%2520Complementary%2520Reward%2520Functions%253A%2520We%2520design%2520reward%2520functions%2520for%2520both%2520interaction-following%2520accuracy%2520and%2520visual%2520quality%252C%2520which%2520provide%2520direct%2520supervision%2520and%2520effectively%2520suppress%2520reward-hacking%2520behaviors.%25203%2529%2520Efficient%2520RL%2520Algorithm%253A%2520We%2520employ%2520the%2520negative-aware%2520fine-tuning%2520strategy%2520coupled%2520with%2520various%2520efficiency%2520optimizations%2520to%2520efficiently%2520and%2520effectively%2520enhance%2520model%2520capacity.%2520Evaluations%2520on%2520the%2520SoTA%2520open-source%2520world%2520model%252C%2520WorldPlay%252C%2520demonstrate%2520that%2520WorldCompass%2520significantly%2520improves%2520interaction%2520accuracy%2520and%2520visual%2520fidelity%2520across%2520various%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldCompass%3A%20Reinforcement%20Learning%20for%20Long-Horizon%20World%20Models&entry.906535625=Zehan%20Wang%20and%20Tengfei%20Wang%20and%20Haiyu%20Zhang%20and%20Xuhui%20Zuo%20and%20Junta%20Wu%20and%20Haoyuan%20Wang%20and%20Wenqiang%20Sun%20and%20Zhenwei%20Wang%20and%20Chenjie%20Cao%20and%20Hengshuang%20Zhao%20and%20Chunchao%20Guo%20and%20Zhou%20Zhao&entry.1292438233=This%20work%20presents%20WorldCompass%2C%20a%20novel%20Reinforcement%20Learning%20%28RL%29%20post-training%20framework%20for%20the%20long-horizon%2C%20interactive%20video-based%20world%20models%2C%20enabling%20them%20to%20explore%20the%20world%20more%20accurately%20and%20consistently%20based%20on%20interaction%20signals.%20To%20effectively%20%22steer%22%20the%20world%20model%27s%20exploration%2C%20we%20introduce%20three%20core%20innovations%20tailored%20to%20the%20autoregressive%20video%20generation%20paradigm%3A%201%29%20Clip-level%20rollout%20Strategy%3A%20We%20generate%20and%20evaluate%20multiple%20samples%20at%20a%20single%20target%20clip%2C%20which%20significantly%20boosts%20rollout%20efficiency%20and%20provides%20fine-grained%20reward%20signals.%202%29%20Complementary%20Reward%20Functions%3A%20We%20design%20reward%20functions%20for%20both%20interaction-following%20accuracy%20and%20visual%20quality%2C%20which%20provide%20direct%20supervision%20and%20effectively%20suppress%20reward-hacking%20behaviors.%203%29%20Efficient%20RL%20Algorithm%3A%20We%20employ%20the%20negative-aware%20fine-tuning%20strategy%20coupled%20with%20various%20efficiency%20optimizations%20to%20efficiently%20and%20effectively%20enhance%20model%20capacity.%20Evaluations%20on%20the%20SoTA%20open-source%20world%20model%2C%20WorldPlay%2C%20demonstrate%20that%20WorldCompass%20significantly%20improves%20interaction%20accuracy%20and%20visual%20fidelity%20across%20various%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.09022v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


