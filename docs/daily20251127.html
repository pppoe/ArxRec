<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251126.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy", "author": "Yuxuan Xue and Xianghui Xie and Riccardo Marin and Gerard Pons-Moll", "abstract": "Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D objects and avatars with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on https://yuxuan-xue.com/gen-3diffusion.", "link": "http://arxiv.org/abs/2412.06698v2", "date": "2025-11-26", "relevancy": 3.5681, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7343}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7059}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gen-3Diffusion%3A%20Realistic%20Image-to-3D%20Generation%20via%202D%20%26%203D%20Diffusion%20Synergy&body=Title%3A%20Gen-3Diffusion%3A%20Realistic%20Image-to-3D%20Generation%20via%202D%20%26%203D%20Diffusion%20Synergy%0AAuthor%3A%20Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Riccardo%20Marin%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20Creating%20realistic%203D%20objects%20and%20clothed%20avatars%20from%20a%20single%20RGB%20image%20is%20an%20attractive%20yet%20challenging%20problem.%20Due%20to%20its%20ill-posed%20nature%2C%20recent%20works%20leverage%20powerful%20prior%20from%202D%20diffusion%20models%20pretrained%20on%20large%20datasets.%20Although%202D%20diffusion%20models%20demonstrate%20strong%20generalization%20capability%2C%20they%20cannot%20guarantee%20the%20generated%20multi-view%20images%20are%203D%20consistent.%20In%20this%20paper%2C%20we%20propose%20Gen-3Diffusion%3A%20Realistic%20Image-to-3D%20Generation%20via%202D%20%26%203D%20Diffusion%20Synergy.%20We%20leverage%20a%20pre-trained%202D%20diffusion%20model%20and%20a%203D%20diffusion%20model%20via%20our%20elegantly%20designed%20process%20that%20synchronizes%20two%20diffusion%20models%20at%20both%20training%20and%20sampling%20time.%20The%20synergy%20between%20the%202D%20and%203D%20diffusion%20models%20brings%20two%20major%20advantages%3A%201%29%202D%20helps%203D%20in%20generalization%3A%20the%20pretrained%202D%20model%20has%20strong%20generalization%20ability%20to%20unseen%20images%2C%20providing%20strong%20shape%20priors%20for%20the%203D%20diffusion%20model%3B%202%29%203D%20helps%202D%20in%20multi-view%20consistency%3A%20the%203D%20diffusion%20model%20enhances%20the%203D%20consistency%20of%202D%20multi-view%20sampling%20process%2C%20resulting%20in%20more%20accurate%20multi-view%20generation.%20We%20validate%20our%20idea%20through%20extensive%20experiments%20in%20image-based%20objects%20and%20clothed%20avatar%20generation%20tasks.%20Results%20show%20that%20our%20method%20generates%20realistic%203D%20objects%20and%20avatars%20with%20high-fidelity%20geometry%20and%20texture.%20Extensive%20ablations%20also%20validate%20our%20design%20choices%20and%20demonstrate%20the%20strong%20generalization%20ability%20to%20diverse%20clothing%20and%20compositional%20shapes.%20Our%20code%20and%20pretrained%20models%20will%20be%20publicly%20released%20on%20https%3A//yuxuan-xue.com/gen-3diffusion.%0ALink%3A%20http%3A//arxiv.org/abs/2412.06698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGen-3Diffusion%253A%2520Realistic%2520Image-to-3D%2520Generation%2520via%25202D%2520%2526%25203D%2520Diffusion%2520Synergy%26entry.906535625%3DYuxuan%2520Xue%2520and%2520Xianghui%2520Xie%2520and%2520Riccardo%2520Marin%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3DCreating%2520realistic%25203D%2520objects%2520and%2520clothed%2520avatars%2520from%2520a%2520single%2520RGB%2520image%2520is%2520an%2520attractive%2520yet%2520challenging%2520problem.%2520Due%2520to%2520its%2520ill-posed%2520nature%252C%2520recent%2520works%2520leverage%2520powerful%2520prior%2520from%25202D%2520diffusion%2520models%2520pretrained%2520on%2520large%2520datasets.%2520Although%25202D%2520diffusion%2520models%2520demonstrate%2520strong%2520generalization%2520capability%252C%2520they%2520cannot%2520guarantee%2520the%2520generated%2520multi-view%2520images%2520are%25203D%2520consistent.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Gen-3Diffusion%253A%2520Realistic%2520Image-to-3D%2520Generation%2520via%25202D%2520%2526%25203D%2520Diffusion%2520Synergy.%2520We%2520leverage%2520a%2520pre-trained%25202D%2520diffusion%2520model%2520and%2520a%25203D%2520diffusion%2520model%2520via%2520our%2520elegantly%2520designed%2520process%2520that%2520synchronizes%2520two%2520diffusion%2520models%2520at%2520both%2520training%2520and%2520sampling%2520time.%2520The%2520synergy%2520between%2520the%25202D%2520and%25203D%2520diffusion%2520models%2520brings%2520two%2520major%2520advantages%253A%25201%2529%25202D%2520helps%25203D%2520in%2520generalization%253A%2520the%2520pretrained%25202D%2520model%2520has%2520strong%2520generalization%2520ability%2520to%2520unseen%2520images%252C%2520providing%2520strong%2520shape%2520priors%2520for%2520the%25203D%2520diffusion%2520model%253B%25202%2529%25203D%2520helps%25202D%2520in%2520multi-view%2520consistency%253A%2520the%25203D%2520diffusion%2520model%2520enhances%2520the%25203D%2520consistency%2520of%25202D%2520multi-view%2520sampling%2520process%252C%2520resulting%2520in%2520more%2520accurate%2520multi-view%2520generation.%2520We%2520validate%2520our%2520idea%2520through%2520extensive%2520experiments%2520in%2520image-based%2520objects%2520and%2520clothed%2520avatar%2520generation%2520tasks.%2520Results%2520show%2520that%2520our%2520method%2520generates%2520realistic%25203D%2520objects%2520and%2520avatars%2520with%2520high-fidelity%2520geometry%2520and%2520texture.%2520Extensive%2520ablations%2520also%2520validate%2520our%2520design%2520choices%2520and%2520demonstrate%2520the%2520strong%2520generalization%2520ability%2520to%2520diverse%2520clothing%2520and%2520compositional%2520shapes.%2520Our%2520code%2520and%2520pretrained%2520models%2520will%2520be%2520publicly%2520released%2520on%2520https%253A//yuxuan-xue.com/gen-3diffusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen-3Diffusion%3A%20Realistic%20Image-to-3D%20Generation%20via%202D%20%26%203D%20Diffusion%20Synergy&entry.906535625=Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Riccardo%20Marin%20and%20Gerard%20Pons-Moll&entry.1292438233=Creating%20realistic%203D%20objects%20and%20clothed%20avatars%20from%20a%20single%20RGB%20image%20is%20an%20attractive%20yet%20challenging%20problem.%20Due%20to%20its%20ill-posed%20nature%2C%20recent%20works%20leverage%20powerful%20prior%20from%202D%20diffusion%20models%20pretrained%20on%20large%20datasets.%20Although%202D%20diffusion%20models%20demonstrate%20strong%20generalization%20capability%2C%20they%20cannot%20guarantee%20the%20generated%20multi-view%20images%20are%203D%20consistent.%20In%20this%20paper%2C%20we%20propose%20Gen-3Diffusion%3A%20Realistic%20Image-to-3D%20Generation%20via%202D%20%26%203D%20Diffusion%20Synergy.%20We%20leverage%20a%20pre-trained%202D%20diffusion%20model%20and%20a%203D%20diffusion%20model%20via%20our%20elegantly%20designed%20process%20that%20synchronizes%20two%20diffusion%20models%20at%20both%20training%20and%20sampling%20time.%20The%20synergy%20between%20the%202D%20and%203D%20diffusion%20models%20brings%20two%20major%20advantages%3A%201%29%202D%20helps%203D%20in%20generalization%3A%20the%20pretrained%202D%20model%20has%20strong%20generalization%20ability%20to%20unseen%20images%2C%20providing%20strong%20shape%20priors%20for%20the%203D%20diffusion%20model%3B%202%29%203D%20helps%202D%20in%20multi-view%20consistency%3A%20the%203D%20diffusion%20model%20enhances%20the%203D%20consistency%20of%202D%20multi-view%20sampling%20process%2C%20resulting%20in%20more%20accurate%20multi-view%20generation.%20We%20validate%20our%20idea%20through%20extensive%20experiments%20in%20image-based%20objects%20and%20clothed%20avatar%20generation%20tasks.%20Results%20show%20that%20our%20method%20generates%20realistic%203D%20objects%20and%20avatars%20with%20high-fidelity%20geometry%20and%20texture.%20Extensive%20ablations%20also%20validate%20our%20design%20choices%20and%20demonstrate%20the%20strong%20generalization%20ability%20to%20diverse%20clothing%20and%20compositional%20shapes.%20Our%20code%20and%20pretrained%20models%20will%20be%20publicly%20released%20on%20https%3A//yuxuan-xue.com/gen-3diffusion.&entry.1838667208=http%3A//arxiv.org/abs/2412.06698v2&entry.124074799=Read"},
{"title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment", "author": "Qing Li and Huifang Feng and Xun Gong and Yu-Shen Liu", "abstract": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.", "link": "http://arxiv.org/abs/2510.11473v2", "date": "2025-11-26", "relevancy": 3.4073, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6961}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6777}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VA-GS%3A%20Enhancing%20the%20Geometric%20Representation%20of%20Gaussian%20Splatting%20via%20View%20Alignment&body=Title%3A%20VA-GS%3A%20Enhancing%20the%20Geometric%20Representation%20of%20Gaussian%20Splatting%20via%20View%20Alignment%0AAuthor%3A%20Qing%20Li%20and%20Huifang%20Feng%20and%20Xun%20Gong%20and%20Yu-Shen%20Liu%0AAbstract%3A%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20an%20efficient%20solution%20for%20high-quality%20and%20real-time%20novel%20view%20synthesis.%20However%2C%20its%20capability%20for%20accurate%20surface%20reconstruction%20remains%20underexplored.%20Due%20to%20the%20discrete%20and%20unstructured%20nature%20of%20Gaussians%2C%20supervision%20based%20solely%20on%20image%20rendering%20loss%20often%20leads%20to%20inaccurate%20geometry%20and%20inconsistent%20multi-view%20alignment.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20that%20enhances%20the%20geometric%20representation%20of%203D%20Gaussians%20through%20view%20alignment%20%28VA%29.%20Specifically%2C%20we%20incorporate%20edge-aware%20image%20cues%20into%20the%20rendering%20loss%20to%20improve%20surface%20boundary%20delineation.%20To%20enforce%20geometric%20consistency%20across%20views%2C%20we%20introduce%20a%20visibility-aware%20photometric%20alignment%20loss%20that%20models%20occlusions%20and%20encourages%20accurate%20spatial%20relationships%20among%20Gaussians.%20To%20further%20mitigate%20ambiguities%20caused%20by%20lighting%20variations%2C%20we%20incorporate%20normal-based%20constraints%20to%20refine%20the%20spatial%20orientation%20of%20Gaussians%20and%20improve%20local%20surface%20estimation.%20Additionally%2C%20we%20leverage%20deep%20image%20feature%20embeddings%20to%20enforce%20cross-view%20consistency%2C%20enhancing%20the%20robustness%20of%20the%20learned%20geometry%20under%20varying%20viewpoints%20and%20illumination.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20both%20surface%20reconstruction%20and%20novel%20view%20synthesis.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/LeoQLi/VA-GS.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVA-GS%253A%2520Enhancing%2520the%2520Geometric%2520Representation%2520of%2520Gaussian%2520Splatting%2520via%2520View%2520Alignment%26entry.906535625%3DQing%2520Li%2520and%2520Huifang%2520Feng%2520and%2520Xun%2520Gong%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520has%2520recently%2520emerged%2520as%2520an%2520efficient%2520solution%2520for%2520high-quality%2520and%2520real-time%2520novel%2520view%2520synthesis.%2520However%252C%2520its%2520capability%2520for%2520accurate%2520surface%2520reconstruction%2520remains%2520underexplored.%2520Due%2520to%2520the%2520discrete%2520and%2520unstructured%2520nature%2520of%2520Gaussians%252C%2520supervision%2520based%2520solely%2520on%2520image%2520rendering%2520loss%2520often%2520leads%2520to%2520inaccurate%2520geometry%2520and%2520inconsistent%2520multi-view%2520alignment.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%2520enhances%2520the%2520geometric%2520representation%2520of%25203D%2520Gaussians%2520through%2520view%2520alignment%2520%2528VA%2529.%2520Specifically%252C%2520we%2520incorporate%2520edge-aware%2520image%2520cues%2520into%2520the%2520rendering%2520loss%2520to%2520improve%2520surface%2520boundary%2520delineation.%2520To%2520enforce%2520geometric%2520consistency%2520across%2520views%252C%2520we%2520introduce%2520a%2520visibility-aware%2520photometric%2520alignment%2520loss%2520that%2520models%2520occlusions%2520and%2520encourages%2520accurate%2520spatial%2520relationships%2520among%2520Gaussians.%2520To%2520further%2520mitigate%2520ambiguities%2520caused%2520by%2520lighting%2520variations%252C%2520we%2520incorporate%2520normal-based%2520constraints%2520to%2520refine%2520the%2520spatial%2520orientation%2520of%2520Gaussians%2520and%2520improve%2520local%2520surface%2520estimation.%2520Additionally%252C%2520we%2520leverage%2520deep%2520image%2520feature%2520embeddings%2520to%2520enforce%2520cross-view%2520consistency%252C%2520enhancing%2520the%2520robustness%2520of%2520the%2520learned%2520geometry%2520under%2520varying%2520viewpoints%2520and%2520illumination.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520surface%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/LeoQLi/VA-GS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VA-GS%3A%20Enhancing%20the%20Geometric%20Representation%20of%20Gaussian%20Splatting%20via%20View%20Alignment&entry.906535625=Qing%20Li%20and%20Huifang%20Feng%20and%20Xun%20Gong%20and%20Yu-Shen%20Liu&entry.1292438233=3D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20an%20efficient%20solution%20for%20high-quality%20and%20real-time%20novel%20view%20synthesis.%20However%2C%20its%20capability%20for%20accurate%20surface%20reconstruction%20remains%20underexplored.%20Due%20to%20the%20discrete%20and%20unstructured%20nature%20of%20Gaussians%2C%20supervision%20based%20solely%20on%20image%20rendering%20loss%20often%20leads%20to%20inaccurate%20geometry%20and%20inconsistent%20multi-view%20alignment.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20that%20enhances%20the%20geometric%20representation%20of%203D%20Gaussians%20through%20view%20alignment%20%28VA%29.%20Specifically%2C%20we%20incorporate%20edge-aware%20image%20cues%20into%20the%20rendering%20loss%20to%20improve%20surface%20boundary%20delineation.%20To%20enforce%20geometric%20consistency%20across%20views%2C%20we%20introduce%20a%20visibility-aware%20photometric%20alignment%20loss%20that%20models%20occlusions%20and%20encourages%20accurate%20spatial%20relationships%20among%20Gaussians.%20To%20further%20mitigate%20ambiguities%20caused%20by%20lighting%20variations%2C%20we%20incorporate%20normal-based%20constraints%20to%20refine%20the%20spatial%20orientation%20of%20Gaussians%20and%20improve%20local%20surface%20estimation.%20Additionally%2C%20we%20leverage%20deep%20image%20feature%20embeddings%20to%20enforce%20cross-view%20consistency%2C%20enhancing%20the%20robustness%20of%20the%20learned%20geometry%20under%20varying%20viewpoints%20and%20illumination.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20both%20surface%20reconstruction%20and%20novel%20view%20synthesis.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/LeoQLi/VA-GS.&entry.1838667208=http%3A//arxiv.org/abs/2510.11473v2&entry.124074799=Read"},
{"title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting", "author": "Juncheng Chen and Chao Xu and Yanjun Cao", "abstract": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.", "link": "http://arxiv.org/abs/2511.21265v1", "date": "2025-11-26", "relevancy": 3.2455, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6669}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Zero-shot%20Potential%20of%20Semi-dense%20Image%20Matching%20via%20Gaussian%20Splatting&body=Title%3A%20Unlocking%20Zero-shot%20Potential%20of%20Semi-dense%20Image%20Matching%20via%20Gaussian%20Splatting%0AAuthor%3A%20Juncheng%20Chen%20and%20Chao%20Xu%20and%20Yanjun%20Cao%0AAbstract%3A%20Learning-based%20image%20matching%20critically%20depends%20on%20large-scale%2C%20diverse%2C%20and%20geometrically%20accurate%20training%20data.%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20photorealistic%20novel-view%20synthesis%20and%20thus%20is%20attractive%20for%20data%20generation.%20However%2C%20its%20geometric%20inaccuracies%20and%20biased%20depth%20rendering%20currently%20prevent%20robust%20correspondence%20labeling.%20To%20address%20this%2C%20we%20introduce%20MatchGS%2C%20the%20first%20framework%20designed%20to%20systematically%20correct%20and%20leverage%203DGS%20for%20robust%2C%20zero-shot%20image%20matching.%20Our%20approach%20is%20twofold%3A%20%281%29%20a%20geometrically-faithful%20data%20generation%20pipeline%20that%20refines%203DGS%20geometry%20to%20produce%20highly%20precise%20correspondence%20labels%2C%20enabling%20the%20synthesis%20of%20a%20vast%20and%20diverse%20range%20of%20viewpoints%20without%20compromising%20rendering%20fidelity%3B%20and%20%282%29%20a%202D-3D%20representation%20alignment%20strategy%20that%20infuses%203DGS%27%20explicit%203D%20knowledge%20into%20the%202D%20matcher%2C%20guiding%202D%20semi-dense%20matchers%20to%20learn%20viewpoint-invariant%203D%20representations.%20Our%20generated%20ground-truth%20correspondences%20reduce%20the%20epipolar%20error%20by%20up%20to%2040%20times%20compared%20to%20existing%20datasets%2C%20enable%20supervision%20under%20extreme%20viewpoint%20changes%2C%20and%20provide%20self-supervisory%20signals%20through%20Gaussian%20attributes.%20Consequently%2C%20state-of-the-art%20matchers%20trained%20solely%20on%20our%20data%20achieve%20significant%20zero-shot%20performance%20gains%20on%20public%20benchmarks%2C%20with%20improvements%20of%20up%20to%2017.7%25.%20Our%20work%20demonstrates%20that%20with%20proper%20geometric%20refinement%2C%203DGS%20can%20serve%20as%20a%20scalable%2C%20high-fidelity%2C%20and%20structurally-rich%20data%20source%2C%20paving%20the%20way%20for%20a%20new%20generation%20of%20robust%20zero-shot%20image%20matchers.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Zero-shot%2520Potential%2520of%2520Semi-dense%2520Image%2520Matching%2520via%2520Gaussian%2520Splatting%26entry.906535625%3DJuncheng%2520Chen%2520and%2520Chao%2520Xu%2520and%2520Yanjun%2520Cao%26entry.1292438233%3DLearning-based%2520image%2520matching%2520critically%2520depends%2520on%2520large-scale%252C%2520diverse%252C%2520and%2520geometrically%2520accurate%2520training%2520data.%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520photorealistic%2520novel-view%2520synthesis%2520and%2520thus%2520is%2520attractive%2520for%2520data%2520generation.%2520However%252C%2520its%2520geometric%2520inaccuracies%2520and%2520biased%2520depth%2520rendering%2520currently%2520prevent%2520robust%2520correspondence%2520labeling.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MatchGS%252C%2520the%2520first%2520framework%2520designed%2520to%2520systematically%2520correct%2520and%2520leverage%25203DGS%2520for%2520robust%252C%2520zero-shot%2520image%2520matching.%2520Our%2520approach%2520is%2520twofold%253A%2520%25281%2529%2520a%2520geometrically-faithful%2520data%2520generation%2520pipeline%2520that%2520refines%25203DGS%2520geometry%2520to%2520produce%2520highly%2520precise%2520correspondence%2520labels%252C%2520enabling%2520the%2520synthesis%2520of%2520a%2520vast%2520and%2520diverse%2520range%2520of%2520viewpoints%2520without%2520compromising%2520rendering%2520fidelity%253B%2520and%2520%25282%2529%2520a%25202D-3D%2520representation%2520alignment%2520strategy%2520that%2520infuses%25203DGS%2527%2520explicit%25203D%2520knowledge%2520into%2520the%25202D%2520matcher%252C%2520guiding%25202D%2520semi-dense%2520matchers%2520to%2520learn%2520viewpoint-invariant%25203D%2520representations.%2520Our%2520generated%2520ground-truth%2520correspondences%2520reduce%2520the%2520epipolar%2520error%2520by%2520up%2520to%252040%2520times%2520compared%2520to%2520existing%2520datasets%252C%2520enable%2520supervision%2520under%2520extreme%2520viewpoint%2520changes%252C%2520and%2520provide%2520self-supervisory%2520signals%2520through%2520Gaussian%2520attributes.%2520Consequently%252C%2520state-of-the-art%2520matchers%2520trained%2520solely%2520on%2520our%2520data%2520achieve%2520significant%2520zero-shot%2520performance%2520gains%2520on%2520public%2520benchmarks%252C%2520with%2520improvements%2520of%2520up%2520to%252017.7%2525.%2520Our%2520work%2520demonstrates%2520that%2520with%2520proper%2520geometric%2520refinement%252C%25203DGS%2520can%2520serve%2520as%2520a%2520scalable%252C%2520high-fidelity%252C%2520and%2520structurally-rich%2520data%2520source%252C%2520paving%2520the%2520way%2520for%2520a%2520new%2520generation%2520of%2520robust%2520zero-shot%2520image%2520matchers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Zero-shot%20Potential%20of%20Semi-dense%20Image%20Matching%20via%20Gaussian%20Splatting&entry.906535625=Juncheng%20Chen%20and%20Chao%20Xu%20and%20Yanjun%20Cao&entry.1292438233=Learning-based%20image%20matching%20critically%20depends%20on%20large-scale%2C%20diverse%2C%20and%20geometrically%20accurate%20training%20data.%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20photorealistic%20novel-view%20synthesis%20and%20thus%20is%20attractive%20for%20data%20generation.%20However%2C%20its%20geometric%20inaccuracies%20and%20biased%20depth%20rendering%20currently%20prevent%20robust%20correspondence%20labeling.%20To%20address%20this%2C%20we%20introduce%20MatchGS%2C%20the%20first%20framework%20designed%20to%20systematically%20correct%20and%20leverage%203DGS%20for%20robust%2C%20zero-shot%20image%20matching.%20Our%20approach%20is%20twofold%3A%20%281%29%20a%20geometrically-faithful%20data%20generation%20pipeline%20that%20refines%203DGS%20geometry%20to%20produce%20highly%20precise%20correspondence%20labels%2C%20enabling%20the%20synthesis%20of%20a%20vast%20and%20diverse%20range%20of%20viewpoints%20without%20compromising%20rendering%20fidelity%3B%20and%20%282%29%20a%202D-3D%20representation%20alignment%20strategy%20that%20infuses%203DGS%27%20explicit%203D%20knowledge%20into%20the%202D%20matcher%2C%20guiding%202D%20semi-dense%20matchers%20to%20learn%20viewpoint-invariant%203D%20representations.%20Our%20generated%20ground-truth%20correspondences%20reduce%20the%20epipolar%20error%20by%20up%20to%2040%20times%20compared%20to%20existing%20datasets%2C%20enable%20supervision%20under%20extreme%20viewpoint%20changes%2C%20and%20provide%20self-supervisory%20signals%20through%20Gaussian%20attributes.%20Consequently%2C%20state-of-the-art%20matchers%20trained%20solely%20on%20our%20data%20achieve%20significant%20zero-shot%20performance%20gains%20on%20public%20benchmarks%2C%20with%20improvements%20of%20up%20to%2017.7%25.%20Our%20work%20demonstrates%20that%20with%20proper%20geometric%20refinement%2C%203DGS%20can%20serve%20as%20a%20scalable%2C%20high-fidelity%2C%20and%20structurally-rich%20data%20source%2C%20paving%20the%20way%20for%20a%20new%20generation%20of%20robust%20zero-shot%20image%20matchers.&entry.1838667208=http%3A//arxiv.org/abs/2511.21265v1&entry.124074799=Read"},
{"title": "Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes", "author": "Yangle Liu and Fengze Li and Kan Liu and Jieming Ma", "abstract": "Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.", "link": "http://arxiv.org/abs/2511.21367v1", "date": "2025-11-26", "relevancy": 3.2401, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6672}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6659}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Endo-G%24%5E%7B2%7D%24T%3A%20Geometry-Guided%20%26%20Temporally%20Aware%20Time-Embedded%204DGS%20For%20Endoscopic%20Scenes&body=Title%3A%20Endo-G%24%5E%7B2%7D%24T%3A%20Geometry-Guided%20%26%20Temporally%20Aware%20Time-Embedded%204DGS%20For%20Endoscopic%20Scenes%0AAuthor%3A%20Yangle%20Liu%20and%20Fengze%20Li%20and%20Kan%20Liu%20and%20Jieming%20Ma%0AAbstract%3A%20Endoscopic%20%28endo%29%20video%20exhibits%20strong%20view-dependent%20effects%20such%20as%20specularities%2C%20wet%20reflections%2C%20and%20occlusions.%20Pure%20photometric%20supervision%20misaligns%20with%20geometry%20and%20triggers%20early%20geometric%20drift%2C%20where%20erroneous%20shapes%20are%20reinforced%20during%20densification%20and%20become%20hard%20to%20correct.%20We%20ask%20how%20to%20anchor%20geometry%20early%20for%204D%20Gaussian%20splatting%20%284DGS%29%20while%20maintaining%20temporal%20consistency%20and%20efficiency%20in%20dynamic%20endoscopic%20scenes.%20Thus%2C%20we%20present%20Endo-G%24%5E%7B2%7D%24T%2C%20a%20geometry-guided%20and%20temporally%20aware%20training%20scheme%20for%20time-embedded%204DGS.%20First%2C%20geo-guided%20prior%20distillation%20converts%20confidence-gated%20monocular%20depth%20into%20supervision%20with%20scale-invariant%20depth%20and%20depth-gradient%20losses%2C%20using%20a%20warm-up-to-cap%20schedule%20to%20inject%20priors%20softly%20and%20avoid%20early%20overfitting.%20Second%2C%20a%20time-embedded%20Gaussian%20field%20represents%20dynamics%20in%20XYZT%20with%20a%20rotor-like%20rotation%20parameterization%2C%20yielding%20temporally%20coherent%20geometry%20with%20lightweight%20regularization%20that%20favors%20smooth%20motion%20and%20crisp%20opacity%20boundaries.%20Third%2C%20keyframe-constrained%20streaming%20improves%20efficiency%20and%20long-horizon%20stability%20through%20keyframe-focused%20optimization%20under%20a%20max-points%20budget%2C%20while%20non-keyframes%20advance%20with%20lightweight%20updates.%20Across%20EndoNeRF%20and%20StereoMIS-P1%20datasets%2C%20Endo-G%24%5E%7B2%7D%24T%20achieves%20state-of-the-art%20results%20among%20monocular%20reconstruction%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndo-G%2524%255E%257B2%257D%2524T%253A%2520Geometry-Guided%2520%2526%2520Temporally%2520Aware%2520Time-Embedded%25204DGS%2520For%2520Endoscopic%2520Scenes%26entry.906535625%3DYangle%2520Liu%2520and%2520Fengze%2520Li%2520and%2520Kan%2520Liu%2520and%2520Jieming%2520Ma%26entry.1292438233%3DEndoscopic%2520%2528endo%2529%2520video%2520exhibits%2520strong%2520view-dependent%2520effects%2520such%2520as%2520specularities%252C%2520wet%2520reflections%252C%2520and%2520occlusions.%2520Pure%2520photometric%2520supervision%2520misaligns%2520with%2520geometry%2520and%2520triggers%2520early%2520geometric%2520drift%252C%2520where%2520erroneous%2520shapes%2520are%2520reinforced%2520during%2520densification%2520and%2520become%2520hard%2520to%2520correct.%2520We%2520ask%2520how%2520to%2520anchor%2520geometry%2520early%2520for%25204D%2520Gaussian%2520splatting%2520%25284DGS%2529%2520while%2520maintaining%2520temporal%2520consistency%2520and%2520efficiency%2520in%2520dynamic%2520endoscopic%2520scenes.%2520Thus%252C%2520we%2520present%2520Endo-G%2524%255E%257B2%257D%2524T%252C%2520a%2520geometry-guided%2520and%2520temporally%2520aware%2520training%2520scheme%2520for%2520time-embedded%25204DGS.%2520First%252C%2520geo-guided%2520prior%2520distillation%2520converts%2520confidence-gated%2520monocular%2520depth%2520into%2520supervision%2520with%2520scale-invariant%2520depth%2520and%2520depth-gradient%2520losses%252C%2520using%2520a%2520warm-up-to-cap%2520schedule%2520to%2520inject%2520priors%2520softly%2520and%2520avoid%2520early%2520overfitting.%2520Second%252C%2520a%2520time-embedded%2520Gaussian%2520field%2520represents%2520dynamics%2520in%2520XYZT%2520with%2520a%2520rotor-like%2520rotation%2520parameterization%252C%2520yielding%2520temporally%2520coherent%2520geometry%2520with%2520lightweight%2520regularization%2520that%2520favors%2520smooth%2520motion%2520and%2520crisp%2520opacity%2520boundaries.%2520Third%252C%2520keyframe-constrained%2520streaming%2520improves%2520efficiency%2520and%2520long-horizon%2520stability%2520through%2520keyframe-focused%2520optimization%2520under%2520a%2520max-points%2520budget%252C%2520while%2520non-keyframes%2520advance%2520with%2520lightweight%2520updates.%2520Across%2520EndoNeRF%2520and%2520StereoMIS-P1%2520datasets%252C%2520Endo-G%2524%255E%257B2%257D%2524T%2520achieves%2520state-of-the-art%2520results%2520among%2520monocular%2520reconstruction%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Endo-G%24%5E%7B2%7D%24T%3A%20Geometry-Guided%20%26%20Temporally%20Aware%20Time-Embedded%204DGS%20For%20Endoscopic%20Scenes&entry.906535625=Yangle%20Liu%20and%20Fengze%20Li%20and%20Kan%20Liu%20and%20Jieming%20Ma&entry.1292438233=Endoscopic%20%28endo%29%20video%20exhibits%20strong%20view-dependent%20effects%20such%20as%20specularities%2C%20wet%20reflections%2C%20and%20occlusions.%20Pure%20photometric%20supervision%20misaligns%20with%20geometry%20and%20triggers%20early%20geometric%20drift%2C%20where%20erroneous%20shapes%20are%20reinforced%20during%20densification%20and%20become%20hard%20to%20correct.%20We%20ask%20how%20to%20anchor%20geometry%20early%20for%204D%20Gaussian%20splatting%20%284DGS%29%20while%20maintaining%20temporal%20consistency%20and%20efficiency%20in%20dynamic%20endoscopic%20scenes.%20Thus%2C%20we%20present%20Endo-G%24%5E%7B2%7D%24T%2C%20a%20geometry-guided%20and%20temporally%20aware%20training%20scheme%20for%20time-embedded%204DGS.%20First%2C%20geo-guided%20prior%20distillation%20converts%20confidence-gated%20monocular%20depth%20into%20supervision%20with%20scale-invariant%20depth%20and%20depth-gradient%20losses%2C%20using%20a%20warm-up-to-cap%20schedule%20to%20inject%20priors%20softly%20and%20avoid%20early%20overfitting.%20Second%2C%20a%20time-embedded%20Gaussian%20field%20represents%20dynamics%20in%20XYZT%20with%20a%20rotor-like%20rotation%20parameterization%2C%20yielding%20temporally%20coherent%20geometry%20with%20lightweight%20regularization%20that%20favors%20smooth%20motion%20and%20crisp%20opacity%20boundaries.%20Third%2C%20keyframe-constrained%20streaming%20improves%20efficiency%20and%20long-horizon%20stability%20through%20keyframe-focused%20optimization%20under%20a%20max-points%20budget%2C%20while%20non-keyframes%20advance%20with%20lightweight%20updates.%20Across%20EndoNeRF%20and%20StereoMIS-P1%20datasets%2C%20Endo-G%24%5E%7B2%7D%24T%20achieves%20state-of-the-art%20results%20among%20monocular%20reconstruction%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.21367v1&entry.124074799=Read"},
{"title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals", "author": "Nate Gillman and Charles Herrmann and Michael Freeman and Daksh Aggarwal and Evan Luo and Deqing Sun and Chen Sun", "abstract": "Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied. In this work, we investigate using physical forces as a control signal for video generation and propose force prompts which enable users to interact with images through both localized point forces, such as poking a plant, and global wind force fields, such as wind blowing on fabric. We demonstrate that these force prompts can enable videos to respond realistically to physical control signals by leveraging the visual and motion prior in the original pretrained model, without using any 3D asset or physics simulator at inference. The primary challenge of force prompting is the difficulty in obtaining high quality paired force-video training data, both in the real world due to the difficulty of obtaining force signals, and in synthetic data due to limitations in the visual quality and domain diversity of physics simulators. Our key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. Our method can generate videos which simulate forces across diverse geometries, settings, and materials. We also try to understand the source of this generalization and perform ablations that reveal two key elements: visual diversity and the use of specific text keywords during training. Our approach is trained on only around 15k training examples for a single day on four A100 GPUs, and outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions. We release all datasets, code, weights, and interactive video demos at our project page.", "link": "http://arxiv.org/abs/2505.19386v2", "date": "2025-11-26", "relevancy": 3.1669, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7161}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.603}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Force%20Prompting%3A%20Video%20Generation%20Models%20Can%20Learn%20and%20Generalize%20Physics-based%20Control%20Signals&body=Title%3A%20Force%20Prompting%3A%20Video%20Generation%20Models%20Can%20Learn%20and%20Generalize%20Physics-based%20Control%20Signals%0AAuthor%3A%20Nate%20Gillman%20and%20Charles%20Herrmann%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Evan%20Luo%20and%20Deqing%20Sun%20and%20Chen%20Sun%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20models%20have%20sparked%20interest%20in%20world%20models%20capable%20of%20simulating%20realistic%20environments.%20While%20navigation%20has%20been%20well-explored%2C%20physically%20meaningful%20interactions%20that%20mimic%20real-world%20forces%20remain%20largely%20understudied.%20In%20this%20work%2C%20we%20investigate%20using%20physical%20forces%20as%20a%20control%20signal%20for%20video%20generation%20and%20propose%20force%20prompts%20which%20enable%20users%20to%20interact%20with%20images%20through%20both%20localized%20point%20forces%2C%20such%20as%20poking%20a%20plant%2C%20and%20global%20wind%20force%20fields%2C%20such%20as%20wind%20blowing%20on%20fabric.%20We%20demonstrate%20that%20these%20force%20prompts%20can%20enable%20videos%20to%20respond%20realistically%20to%20physical%20control%20signals%20by%20leveraging%20the%20visual%20and%20motion%20prior%20in%20the%20original%20pretrained%20model%2C%20without%20using%20any%203D%20asset%20or%20physics%20simulator%20at%20inference.%20The%20primary%20challenge%20of%20force%20prompting%20is%20the%20difficulty%20in%20obtaining%20high%20quality%20paired%20force-video%20training%20data%2C%20both%20in%20the%20real%20world%20due%20to%20the%20difficulty%20of%20obtaining%20force%20signals%2C%20and%20in%20synthetic%20data%20due%20to%20limitations%20in%20the%20visual%20quality%20and%20domain%20diversity%20of%20physics%20simulators.%20Our%20key%20finding%20is%20that%20video%20generation%20models%20can%20generalize%20remarkably%20well%20when%20adapted%20to%20follow%20physical%20force%20conditioning%20from%20videos%20synthesized%20by%20Blender%2C%20even%20with%20limited%20demonstrations%20of%20few%20objects.%20Our%20method%20can%20generate%20videos%20which%20simulate%20forces%20across%20diverse%20geometries%2C%20settings%2C%20and%20materials.%20We%20also%20try%20to%20understand%20the%20source%20of%20this%20generalization%20and%20perform%20ablations%20that%20reveal%20two%20key%20elements%3A%20visual%20diversity%20and%20the%20use%20of%20specific%20text%20keywords%20during%20training.%20Our%20approach%20is%20trained%20on%20only%20around%2015k%20training%20examples%20for%20a%20single%20day%20on%20four%20A100%20GPUs%2C%20and%20outperforms%20existing%20methods%20on%20force%20adherence%20and%20physics%20realism%2C%20bringing%20world%20models%20closer%20to%20real-world%20physics%20interactions.%20We%20release%20all%20datasets%2C%20code%2C%20weights%2C%20and%20interactive%20video%20demos%20at%20our%20project%20page.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForce%2520Prompting%253A%2520Video%2520Generation%2520Models%2520Can%2520Learn%2520and%2520Generalize%2520Physics-based%2520Control%2520Signals%26entry.906535625%3DNate%2520Gillman%2520and%2520Charles%2520Herrmann%2520and%2520Michael%2520Freeman%2520and%2520Daksh%2520Aggarwal%2520and%2520Evan%2520Luo%2520and%2520Deqing%2520Sun%2520and%2520Chen%2520Sun%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520models%2520have%2520sparked%2520interest%2520in%2520world%2520models%2520capable%2520of%2520simulating%2520realistic%2520environments.%2520While%2520navigation%2520has%2520been%2520well-explored%252C%2520physically%2520meaningful%2520interactions%2520that%2520mimic%2520real-world%2520forces%2520remain%2520largely%2520understudied.%2520In%2520this%2520work%252C%2520we%2520investigate%2520using%2520physical%2520forces%2520as%2520a%2520control%2520signal%2520for%2520video%2520generation%2520and%2520propose%2520force%2520prompts%2520which%2520enable%2520users%2520to%2520interact%2520with%2520images%2520through%2520both%2520localized%2520point%2520forces%252C%2520such%2520as%2520poking%2520a%2520plant%252C%2520and%2520global%2520wind%2520force%2520fields%252C%2520such%2520as%2520wind%2520blowing%2520on%2520fabric.%2520We%2520demonstrate%2520that%2520these%2520force%2520prompts%2520can%2520enable%2520videos%2520to%2520respond%2520realistically%2520to%2520physical%2520control%2520signals%2520by%2520leveraging%2520the%2520visual%2520and%2520motion%2520prior%2520in%2520the%2520original%2520pretrained%2520model%252C%2520without%2520using%2520any%25203D%2520asset%2520or%2520physics%2520simulator%2520at%2520inference.%2520The%2520primary%2520challenge%2520of%2520force%2520prompting%2520is%2520the%2520difficulty%2520in%2520obtaining%2520high%2520quality%2520paired%2520force-video%2520training%2520data%252C%2520both%2520in%2520the%2520real%2520world%2520due%2520to%2520the%2520difficulty%2520of%2520obtaining%2520force%2520signals%252C%2520and%2520in%2520synthetic%2520data%2520due%2520to%2520limitations%2520in%2520the%2520visual%2520quality%2520and%2520domain%2520diversity%2520of%2520physics%2520simulators.%2520Our%2520key%2520finding%2520is%2520that%2520video%2520generation%2520models%2520can%2520generalize%2520remarkably%2520well%2520when%2520adapted%2520to%2520follow%2520physical%2520force%2520conditioning%2520from%2520videos%2520synthesized%2520by%2520Blender%252C%2520even%2520with%2520limited%2520demonstrations%2520of%2520few%2520objects.%2520Our%2520method%2520can%2520generate%2520videos%2520which%2520simulate%2520forces%2520across%2520diverse%2520geometries%252C%2520settings%252C%2520and%2520materials.%2520We%2520also%2520try%2520to%2520understand%2520the%2520source%2520of%2520this%2520generalization%2520and%2520perform%2520ablations%2520that%2520reveal%2520two%2520key%2520elements%253A%2520visual%2520diversity%2520and%2520the%2520use%2520of%2520specific%2520text%2520keywords%2520during%2520training.%2520Our%2520approach%2520is%2520trained%2520on%2520only%2520around%252015k%2520training%2520examples%2520for%2520a%2520single%2520day%2520on%2520four%2520A100%2520GPUs%252C%2520and%2520outperforms%2520existing%2520methods%2520on%2520force%2520adherence%2520and%2520physics%2520realism%252C%2520bringing%2520world%2520models%2520closer%2520to%2520real-world%2520physics%2520interactions.%2520We%2520release%2520all%2520datasets%252C%2520code%252C%2520weights%252C%2520and%2520interactive%2520video%2520demos%2520at%2520our%2520project%2520page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Force%20Prompting%3A%20Video%20Generation%20Models%20Can%20Learn%20and%20Generalize%20Physics-based%20Control%20Signals&entry.906535625=Nate%20Gillman%20and%20Charles%20Herrmann%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Evan%20Luo%20and%20Deqing%20Sun%20and%20Chen%20Sun&entry.1292438233=Recent%20advances%20in%20video%20generation%20models%20have%20sparked%20interest%20in%20world%20models%20capable%20of%20simulating%20realistic%20environments.%20While%20navigation%20has%20been%20well-explored%2C%20physically%20meaningful%20interactions%20that%20mimic%20real-world%20forces%20remain%20largely%20understudied.%20In%20this%20work%2C%20we%20investigate%20using%20physical%20forces%20as%20a%20control%20signal%20for%20video%20generation%20and%20propose%20force%20prompts%20which%20enable%20users%20to%20interact%20with%20images%20through%20both%20localized%20point%20forces%2C%20such%20as%20poking%20a%20plant%2C%20and%20global%20wind%20force%20fields%2C%20such%20as%20wind%20blowing%20on%20fabric.%20We%20demonstrate%20that%20these%20force%20prompts%20can%20enable%20videos%20to%20respond%20realistically%20to%20physical%20control%20signals%20by%20leveraging%20the%20visual%20and%20motion%20prior%20in%20the%20original%20pretrained%20model%2C%20without%20using%20any%203D%20asset%20or%20physics%20simulator%20at%20inference.%20The%20primary%20challenge%20of%20force%20prompting%20is%20the%20difficulty%20in%20obtaining%20high%20quality%20paired%20force-video%20training%20data%2C%20both%20in%20the%20real%20world%20due%20to%20the%20difficulty%20of%20obtaining%20force%20signals%2C%20and%20in%20synthetic%20data%20due%20to%20limitations%20in%20the%20visual%20quality%20and%20domain%20diversity%20of%20physics%20simulators.%20Our%20key%20finding%20is%20that%20video%20generation%20models%20can%20generalize%20remarkably%20well%20when%20adapted%20to%20follow%20physical%20force%20conditioning%20from%20videos%20synthesized%20by%20Blender%2C%20even%20with%20limited%20demonstrations%20of%20few%20objects.%20Our%20method%20can%20generate%20videos%20which%20simulate%20forces%20across%20diverse%20geometries%2C%20settings%2C%20and%20materials.%20We%20also%20try%20to%20understand%20the%20source%20of%20this%20generalization%20and%20perform%20ablations%20that%20reveal%20two%20key%20elements%3A%20visual%20diversity%20and%20the%20use%20of%20specific%20text%20keywords%20during%20training.%20Our%20approach%20is%20trained%20on%20only%20around%2015k%20training%20examples%20for%20a%20single%20day%20on%20four%20A100%20GPUs%2C%20and%20outperforms%20existing%20methods%20on%20force%20adherence%20and%20physics%20realism%2C%20bringing%20world%20models%20closer%20to%20real-world%20physics%20interactions.%20We%20release%20all%20datasets%2C%20code%2C%20weights%2C%20and%20interactive%20video%20demos%20at%20our%20project%20page.&entry.1838667208=http%3A//arxiv.org/abs/2505.19386v2&entry.124074799=Read"},
{"title": "Without Paired Labeled Data: End-to-End Self-Supervised Learning for Drone-view Geo-Localization", "author": "Zhongwei Chen and Zhao-Xu Yang and Hai-Jun Rong and Guoqi Li", "abstract": "Drone-view Geo-Localization (DVGL) aims to achieve accurate localization of drones by retrieving the most relevant GPS-tagged satellite images. However, most existing methods heavily rely on strictly pre-paired drone-satellite images for supervised learning. When the target region shifts, new paired samples are typically required to adapt to the distribution changes. The high cost of annotation and the limited transferability of these methods significantly hinder the practical deployment of DVGL in open-world scenarios. To address these limitations, we propose a novel end-to-end self-supervised learning method with a shallow backbone network, called the dynamic memory-driven and neighborhood information learning (DMNIL) method. It employs a clustering algorithm to generate pseudo-labels and adopts a dual-path contrastive learning framework to learn discriminative intra-view representations. Furthermore, DMNIL incorporates two core modules, including the dynamic hierarchical memory learning (DHML) module and the information consistency evolution learning (ICEL) module. The DHML module combines short-term and long-term memory to enhance intra-view feature consistency and discriminability. Meanwhile, the ICEL module utilizes a neighborhood-driven dynamic constraint mechanism to systematically capture implicit cross-view semantic correlations, consequently improving cross-view feature alignment. To further stabilize and strengthen the self-supervised training process, a pseudo-label enhancement strategy is introduced to enhance the quality of pseudo supervision. Extensive experiments on three public benchmark datasets demonstrate that the proposed method consistently outperforms existing self-supervised methods and even surpasses several state-of-the-art supervised methods. Our code is available at https://github.com/ISChenawei/DMNIL.", "link": "http://arxiv.org/abs/2502.11381v5", "date": "2025-11-26", "relevancy": 3.0854, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5969}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%20Drone-view%20Geo-Localization&body=Title%3A%20Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%20Drone-view%20Geo-Localization%0AAuthor%3A%20Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Guoqi%20Li%0AAbstract%3A%20Drone-view%20Geo-Localization%20%28DVGL%29%20aims%20to%20achieve%20accurate%20localization%20of%20drones%20by%20retrieving%20the%20most%20relevant%20GPS-tagged%20satellite%20images.%20However%2C%20most%20existing%20methods%20heavily%20rely%20on%20strictly%20pre-paired%20drone-satellite%20images%20for%20supervised%20learning.%20When%20the%20target%20region%20shifts%2C%20new%20paired%20samples%20are%20typically%20required%20to%20adapt%20to%20the%20distribution%20changes.%20The%20high%20cost%20of%20annotation%20and%20the%20limited%20transferability%20of%20these%20methods%20significantly%20hinder%20the%20practical%20deployment%20of%20DVGL%20in%20open-world%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20end-to-end%20self-supervised%20learning%20method%20with%20a%20shallow%20backbone%20network%2C%20called%20the%20dynamic%20memory-driven%20and%20neighborhood%20information%20learning%20%28DMNIL%29%20method.%20It%20employs%20a%20clustering%20algorithm%20to%20generate%20pseudo-labels%20and%20adopts%20a%20dual-path%20contrastive%20learning%20framework%20to%20learn%20discriminative%20intra-view%20representations.%20Furthermore%2C%20DMNIL%20incorporates%20two%20core%20modules%2C%20including%20the%20dynamic%20hierarchical%20memory%20learning%20%28DHML%29%20module%20and%20the%20information%20consistency%20evolution%20learning%20%28ICEL%29%20module.%20The%20DHML%20module%20combines%20short-term%20and%20long-term%20memory%20to%20enhance%20intra-view%20feature%20consistency%20and%20discriminability.%20Meanwhile%2C%20the%20ICEL%20module%20utilizes%20a%20neighborhood-driven%20dynamic%20constraint%20mechanism%20to%20systematically%20capture%20implicit%20cross-view%20semantic%20correlations%2C%20consequently%20improving%20cross-view%20feature%20alignment.%20To%20further%20stabilize%20and%20strengthen%20the%20self-supervised%20training%20process%2C%20a%20pseudo-label%20enhancement%20strategy%20is%20introduced%20to%20enhance%20the%20quality%20of%20pseudo%20supervision.%20Extensive%20experiments%20on%20three%20public%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%20self-supervised%20methods%20and%20even%20surpasses%20several%20state-of-the-art%20supervised%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/DMNIL.%0ALink%3A%20http%3A//arxiv.org/abs/2502.11381v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWithout%2520Paired%2520Labeled%2520Data%253A%2520End-to-End%2520Self-Supervised%2520Learning%2520for%2520Drone-view%2520Geo-Localization%26entry.906535625%3DZhongwei%2520Chen%2520and%2520Zhao-Xu%2520Yang%2520and%2520Hai-Jun%2520Rong%2520and%2520Guoqi%2520Li%26entry.1292438233%3DDrone-view%2520Geo-Localization%2520%2528DVGL%2529%2520aims%2520to%2520achieve%2520accurate%2520localization%2520of%2520drones%2520by%2520retrieving%2520the%2520most%2520relevant%2520GPS-tagged%2520satellite%2520images.%2520However%252C%2520most%2520existing%2520methods%2520heavily%2520rely%2520on%2520strictly%2520pre-paired%2520drone-satellite%2520images%2520for%2520supervised%2520learning.%2520When%2520the%2520target%2520region%2520shifts%252C%2520new%2520paired%2520samples%2520are%2520typically%2520required%2520to%2520adapt%2520to%2520the%2520distribution%2520changes.%2520The%2520high%2520cost%2520of%2520annotation%2520and%2520the%2520limited%2520transferability%2520of%2520these%2520methods%2520significantly%2520hinder%2520the%2520practical%2520deployment%2520of%2520DVGL%2520in%2520open-world%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520self-supervised%2520learning%2520method%2520with%2520a%2520shallow%2520backbone%2520network%252C%2520called%2520the%2520dynamic%2520memory-driven%2520and%2520neighborhood%2520information%2520learning%2520%2528DMNIL%2529%2520method.%2520It%2520employs%2520a%2520clustering%2520algorithm%2520to%2520generate%2520pseudo-labels%2520and%2520adopts%2520a%2520dual-path%2520contrastive%2520learning%2520framework%2520to%2520learn%2520discriminative%2520intra-view%2520representations.%2520Furthermore%252C%2520DMNIL%2520incorporates%2520two%2520core%2520modules%252C%2520including%2520the%2520dynamic%2520hierarchical%2520memory%2520learning%2520%2528DHML%2529%2520module%2520and%2520the%2520information%2520consistency%2520evolution%2520learning%2520%2528ICEL%2529%2520module.%2520The%2520DHML%2520module%2520combines%2520short-term%2520and%2520long-term%2520memory%2520to%2520enhance%2520intra-view%2520feature%2520consistency%2520and%2520discriminability.%2520Meanwhile%252C%2520the%2520ICEL%2520module%2520utilizes%2520a%2520neighborhood-driven%2520dynamic%2520constraint%2520mechanism%2520to%2520systematically%2520capture%2520implicit%2520cross-view%2520semantic%2520correlations%252C%2520consequently%2520improving%2520cross-view%2520feature%2520alignment.%2520To%2520further%2520stabilize%2520and%2520strengthen%2520the%2520self-supervised%2520training%2520process%252C%2520a%2520pseudo-label%2520enhancement%2520strategy%2520is%2520introduced%2520to%2520enhance%2520the%2520quality%2520of%2520pseudo%2520supervision.%2520Extensive%2520experiments%2520on%2520three%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520existing%2520self-supervised%2520methods%2520and%2520even%2520surpasses%2520several%2520state-of-the-art%2520supervised%2520methods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ISChenawei/DMNIL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11381v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%20Drone-view%20Geo-Localization&entry.906535625=Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Guoqi%20Li&entry.1292438233=Drone-view%20Geo-Localization%20%28DVGL%29%20aims%20to%20achieve%20accurate%20localization%20of%20drones%20by%20retrieving%20the%20most%20relevant%20GPS-tagged%20satellite%20images.%20However%2C%20most%20existing%20methods%20heavily%20rely%20on%20strictly%20pre-paired%20drone-satellite%20images%20for%20supervised%20learning.%20When%20the%20target%20region%20shifts%2C%20new%20paired%20samples%20are%20typically%20required%20to%20adapt%20to%20the%20distribution%20changes.%20The%20high%20cost%20of%20annotation%20and%20the%20limited%20transferability%20of%20these%20methods%20significantly%20hinder%20the%20practical%20deployment%20of%20DVGL%20in%20open-world%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20end-to-end%20self-supervised%20learning%20method%20with%20a%20shallow%20backbone%20network%2C%20called%20the%20dynamic%20memory-driven%20and%20neighborhood%20information%20learning%20%28DMNIL%29%20method.%20It%20employs%20a%20clustering%20algorithm%20to%20generate%20pseudo-labels%20and%20adopts%20a%20dual-path%20contrastive%20learning%20framework%20to%20learn%20discriminative%20intra-view%20representations.%20Furthermore%2C%20DMNIL%20incorporates%20two%20core%20modules%2C%20including%20the%20dynamic%20hierarchical%20memory%20learning%20%28DHML%29%20module%20and%20the%20information%20consistency%20evolution%20learning%20%28ICEL%29%20module.%20The%20DHML%20module%20combines%20short-term%20and%20long-term%20memory%20to%20enhance%20intra-view%20feature%20consistency%20and%20discriminability.%20Meanwhile%2C%20the%20ICEL%20module%20utilizes%20a%20neighborhood-driven%20dynamic%20constraint%20mechanism%20to%20systematically%20capture%20implicit%20cross-view%20semantic%20correlations%2C%20consequently%20improving%20cross-view%20feature%20alignment.%20To%20further%20stabilize%20and%20strengthen%20the%20self-supervised%20training%20process%2C%20a%20pseudo-label%20enhancement%20strategy%20is%20introduced%20to%20enhance%20the%20quality%20of%20pseudo%20supervision.%20Extensive%20experiments%20on%20three%20public%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%20self-supervised%20methods%20and%20even%20surpasses%20several%20state-of-the-art%20supervised%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/DMNIL.&entry.1838667208=http%3A//arxiv.org/abs/2502.11381v5&entry.124074799=Read"},
{"title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning", "author": "Wenbo Hu and Jingli Lin and Yilin Long and Yunlong Ran and Lihan Jiang and Yifan Wang and Chenming Zhu and Runsen Xu and Tai Wang and Jiangmiao Pang", "abstract": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.", "link": "http://arxiv.org/abs/2511.21688v1", "date": "2025-11-26", "relevancy": 2.9825, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G%24%5E2%24VLM%3A%20Geometry%20Grounded%20Vision%20Language%20Model%20with%20Unified%203D%20Reconstruction%20and%20Spatial%20Reasoning&body=Title%3A%20G%24%5E2%24VLM%3A%20Geometry%20Grounded%20Vision%20Language%20Model%20with%20Unified%203D%20Reconstruction%20and%20Spatial%20Reasoning%0AAuthor%3A%20Wenbo%20Hu%20and%20Jingli%20Lin%20and%20Yilin%20Long%20and%20Yunlong%20Ran%20and%20Lihan%20Jiang%20and%20Yifan%20Wang%20and%20Chenming%20Zhu%20and%20Runsen%20Xu%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20still%20lack%20robustness%20in%20spatial%20intelligence%2C%20demonstrating%20poor%20performance%20on%20spatial%20understanding%20and%20reasoning%20tasks.%20We%20attribute%20this%20gap%20to%20the%20absence%20of%20a%20visual%20geometry%20learning%20process%20capable%20of%20reconstructing%203D%20space%20from%202D%20images.%20We%20present%20G%24%5E2%24VLM%2C%20a%20geometry%20grounded%20vision-language%20model%20that%20bridges%20two%20fundamental%20aspects%20of%20spatial%20intelligence%3A%20spatial%203D%20reconstruction%20and%20spatial%20understanding.%20G%24%5E2%24VLM%20natively%20leverages%20learned%203D%20visual%20geometry%20features%20to%20directly%20predict%203D%20attributes%20and%20enhance%20spatial%20reasoning%20tasks%20via%20in-context%20learning%20and%20interleaved%20reasoning.%20Our%20unified%20design%20is%20highly%20scalable%20for%20spatial%20understanding%3A%20it%20trains%20on%20abundant%20multi-view%20image%20and%20video%20data%2C%20while%20simultaneously%20leveraging%20the%20benefits%20of%203D%20visual%20priors%20that%20are%20typically%20only%20derived%20from%20hard-to-collect%20annotations.%20Experimental%20results%20demonstrate%20G%24%5E2%24VLM%20is%20proficient%20in%20both%20tasks%2C%20achieving%20comparable%20results%20to%20state-of-the-art%20feed-forward%203D%20reconstruction%20models%20and%20achieving%20better%20or%20competitive%20results%20across%20spatial%20understanding%20and%20reasoning%20tasks.%20By%20unifying%20a%20semantically%20strong%20VLM%20with%20low-level%203D%20vision%20tasks%2C%20we%20hope%20G%24%5E2%24VLM%20can%20serve%20as%20a%20strong%20baseline%20for%20the%20community%20and%20unlock%20more%20future%20applications%2C%20such%20as%203D%20scene%20editing.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG%2524%255E2%2524VLM%253A%2520Geometry%2520Grounded%2520Vision%2520Language%2520Model%2520with%2520Unified%25203D%2520Reconstruction%2520and%2520Spatial%2520Reasoning%26entry.906535625%3DWenbo%2520Hu%2520and%2520Jingli%2520Lin%2520and%2520Yilin%2520Long%2520and%2520Yunlong%2520Ran%2520and%2520Lihan%2520Jiang%2520and%2520Yifan%2520Wang%2520and%2520Chenming%2520Zhu%2520and%2520Runsen%2520Xu%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520still%2520lack%2520robustness%2520in%2520spatial%2520intelligence%252C%2520demonstrating%2520poor%2520performance%2520on%2520spatial%2520understanding%2520and%2520reasoning%2520tasks.%2520We%2520attribute%2520this%2520gap%2520to%2520the%2520absence%2520of%2520a%2520visual%2520geometry%2520learning%2520process%2520capable%2520of%2520reconstructing%25203D%2520space%2520from%25202D%2520images.%2520We%2520present%2520G%2524%255E2%2524VLM%252C%2520a%2520geometry%2520grounded%2520vision-language%2520model%2520that%2520bridges%2520two%2520fundamental%2520aspects%2520of%2520spatial%2520intelligence%253A%2520spatial%25203D%2520reconstruction%2520and%2520spatial%2520understanding.%2520G%2524%255E2%2524VLM%2520natively%2520leverages%2520learned%25203D%2520visual%2520geometry%2520features%2520to%2520directly%2520predict%25203D%2520attributes%2520and%2520enhance%2520spatial%2520reasoning%2520tasks%2520via%2520in-context%2520learning%2520and%2520interleaved%2520reasoning.%2520Our%2520unified%2520design%2520is%2520highly%2520scalable%2520for%2520spatial%2520understanding%253A%2520it%2520trains%2520on%2520abundant%2520multi-view%2520image%2520and%2520video%2520data%252C%2520while%2520simultaneously%2520leveraging%2520the%2520benefits%2520of%25203D%2520visual%2520priors%2520that%2520are%2520typically%2520only%2520derived%2520from%2520hard-to-collect%2520annotations.%2520Experimental%2520results%2520demonstrate%2520G%2524%255E2%2524VLM%2520is%2520proficient%2520in%2520both%2520tasks%252C%2520achieving%2520comparable%2520results%2520to%2520state-of-the-art%2520feed-forward%25203D%2520reconstruction%2520models%2520and%2520achieving%2520better%2520or%2520competitive%2520results%2520across%2520spatial%2520understanding%2520and%2520reasoning%2520tasks.%2520By%2520unifying%2520a%2520semantically%2520strong%2520VLM%2520with%2520low-level%25203D%2520vision%2520tasks%252C%2520we%2520hope%2520G%2524%255E2%2524VLM%2520can%2520serve%2520as%2520a%2520strong%2520baseline%2520for%2520the%2520community%2520and%2520unlock%2520more%2520future%2520applications%252C%2520such%2520as%25203D%2520scene%2520editing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G%24%5E2%24VLM%3A%20Geometry%20Grounded%20Vision%20Language%20Model%20with%20Unified%203D%20Reconstruction%20and%20Spatial%20Reasoning&entry.906535625=Wenbo%20Hu%20and%20Jingli%20Lin%20and%20Yilin%20Long%20and%20Yunlong%20Ran%20and%20Lihan%20Jiang%20and%20Yifan%20Wang%20and%20Chenming%20Zhu%20and%20Runsen%20Xu%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20still%20lack%20robustness%20in%20spatial%20intelligence%2C%20demonstrating%20poor%20performance%20on%20spatial%20understanding%20and%20reasoning%20tasks.%20We%20attribute%20this%20gap%20to%20the%20absence%20of%20a%20visual%20geometry%20learning%20process%20capable%20of%20reconstructing%203D%20space%20from%202D%20images.%20We%20present%20G%24%5E2%24VLM%2C%20a%20geometry%20grounded%20vision-language%20model%20that%20bridges%20two%20fundamental%20aspects%20of%20spatial%20intelligence%3A%20spatial%203D%20reconstruction%20and%20spatial%20understanding.%20G%24%5E2%24VLM%20natively%20leverages%20learned%203D%20visual%20geometry%20features%20to%20directly%20predict%203D%20attributes%20and%20enhance%20spatial%20reasoning%20tasks%20via%20in-context%20learning%20and%20interleaved%20reasoning.%20Our%20unified%20design%20is%20highly%20scalable%20for%20spatial%20understanding%3A%20it%20trains%20on%20abundant%20multi-view%20image%20and%20video%20data%2C%20while%20simultaneously%20leveraging%20the%20benefits%20of%203D%20visual%20priors%20that%20are%20typically%20only%20derived%20from%20hard-to-collect%20annotations.%20Experimental%20results%20demonstrate%20G%24%5E2%24VLM%20is%20proficient%20in%20both%20tasks%2C%20achieving%20comparable%20results%20to%20state-of-the-art%20feed-forward%203D%20reconstruction%20models%20and%20achieving%20better%20or%20competitive%20results%20across%20spatial%20understanding%20and%20reasoning%20tasks.%20By%20unifying%20a%20semantically%20strong%20VLM%20with%20low-level%203D%20vision%20tasks%2C%20we%20hope%20G%24%5E2%24VLM%20can%20serve%20as%20a%20strong%20baseline%20for%20the%20community%20and%20unlock%20more%20future%20applications%2C%20such%20as%203D%20scene%20editing.&entry.1838667208=http%3A//arxiv.org/abs/2511.21688v1&entry.124074799=Read"},
{"title": "Qwen3-VL Technical Report", "author": "Shuai Bai and Yuxuan Cai and Ruizhe Chen and Keqin Chen and Xionghui Chen and Zesen Cheng and Lianghao Deng and Wei Ding and Chang Gao and Chunjiang Ge and Wenbin Ge and Zhifang Guo and Qidong Huang and Jie Huang and Fei Huang and Binyuan Hui and Shutong Jiang and Zhaohai Li and Mingsheng Li and Mei Li and Kaixin Li and Zicheng Lin and Junyang Lin and Xuejing Liu and Jiawei Liu and Chenglong Liu and Yang Liu and Dayiheng Liu and Shixuan Liu and Dunjie Lu and Ruilin Luo and Chenxu Lv and Rui Men and Lingchen Meng and Xuancheng Ren and Xingzhang Ren and Sibo Song and Yuchong Sun and Jun Tang and Jianhong Tu and Jianqiang Wan and Peng Wang and Pengfei Wang and Qiuyue Wang and Yuxuan Wang and Tianbao Xie and Yiheng Xu and Haiyang Xu and Jin Xu and Zhibo Yang and Mingkun Yang and Jianxin Yang and An Yang and Bowen Yu and Fei Zhang and Hang Zhang and Xi Zhang and Bo Zheng and Humen Zhong and Jingren Zhou and Fan Zhou and Jing Zhou and Yuanzhi Zhu and Ke Zhu", "abstract": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "link": "http://arxiv.org/abs/2511.21631v1", "date": "2025-11-26", "relevancy": 2.9505, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen3-VL%20Technical%20Report&body=Title%3A%20Qwen3-VL%20Technical%20Report%0AAuthor%3A%20Shuai%20Bai%20and%20Yuxuan%20Cai%20and%20Ruizhe%20Chen%20and%20Keqin%20Chen%20and%20Xionghui%20Chen%20and%20Zesen%20Cheng%20and%20Lianghao%20Deng%20and%20Wei%20Ding%20and%20Chang%20Gao%20and%20Chunjiang%20Ge%20and%20Wenbin%20Ge%20and%20Zhifang%20Guo%20and%20Qidong%20Huang%20and%20Jie%20Huang%20and%20Fei%20Huang%20and%20Binyuan%20Hui%20and%20Shutong%20Jiang%20and%20Zhaohai%20Li%20and%20Mingsheng%20Li%20and%20Mei%20Li%20and%20Kaixin%20Li%20and%20Zicheng%20Lin%20and%20Junyang%20Lin%20and%20Xuejing%20Liu%20and%20Jiawei%20Liu%20and%20Chenglong%20Liu%20and%20Yang%20Liu%20and%20Dayiheng%20Liu%20and%20Shixuan%20Liu%20and%20Dunjie%20Lu%20and%20Ruilin%20Luo%20and%20Chenxu%20Lv%20and%20Rui%20Men%20and%20Lingchen%20Meng%20and%20Xuancheng%20Ren%20and%20Xingzhang%20Ren%20and%20Sibo%20Song%20and%20Yuchong%20Sun%20and%20Jun%20Tang%20and%20Jianhong%20Tu%20and%20Jianqiang%20Wan%20and%20Peng%20Wang%20and%20Pengfei%20Wang%20and%20Qiuyue%20Wang%20and%20Yuxuan%20Wang%20and%20Tianbao%20Xie%20and%20Yiheng%20Xu%20and%20Haiyang%20Xu%20and%20Jin%20Xu%20and%20Zhibo%20Yang%20and%20Mingkun%20Yang%20and%20Jianxin%20Yang%20and%20An%20Yang%20and%20Bowen%20Yu%20and%20Fei%20Zhang%20and%20Hang%20Zhang%20and%20Xi%20Zhang%20and%20Bo%20Zheng%20and%20Humen%20Zhong%20and%20Jingren%20Zhou%20and%20Fan%20Zhou%20and%20Jing%20Zhou%20and%20Yuanzhi%20Zhu%20and%20Ke%20Zhu%0AAbstract%3A%20We%20introduce%20Qwen3-VL%2C%20the%20most%20capable%20vision-language%20model%20in%20the%20Qwen%20series%20to%20date%2C%20achieving%20superior%20performance%20across%20a%20broad%20range%20of%20multimodal%20benchmarks.%20It%20natively%20supports%20interleaved%20contexts%20of%20up%20to%20256K%20tokens%2C%20seamlessly%20integrating%20text%2C%20images%2C%20and%20video.%20The%20model%20family%20includes%20both%20dense%20%282B/4B/8B/32B%29%20and%20mixture-of-experts%20%2830B-A3B/235B-A22B%29%20variants%20to%20accommodate%20diverse%20latency-quality%20trade-offs.%20Qwen3-VL%20delivers%20three%20core%20pillars%3A%20%28i%29%20markedly%20stronger%20pure-text%20understanding%2C%20surpassing%20comparable%20text-only%20backbones%20in%20several%20cases%3B%20%28ii%29%20robust%20long-context%20comprehension%20with%20a%20native%20256K-token%20window%20for%20both%20text%20and%20interleaved%20multimodal%20inputs%2C%20enabling%20faithful%20retention%2C%20retrieval%2C%20and%20cross-referencing%20across%20long%20documents%20and%20videos%3B%20and%20%28iii%29%20advanced%20multimodal%20reasoning%20across%20single-image%2C%20multi-image%2C%20and%20video%20tasks%2C%20demonstrating%20leading%20performance%20on%20comprehensive%20evaluations%20such%20as%20MMMU%20and%20visual-math%20benchmarks%20%28e.g.%2C%20MathVista%20and%20MathVision%29.%20Architecturally%2C%20we%20introduce%20three%20key%20upgrades%3A%20%28i%29%20an%20enhanced%20interleaved-MRoPE%20for%20stronger%20spatial-temporal%20modeling%20across%20images%20and%20video%3B%20%28ii%29%20DeepStack%20integration%2C%20which%20effectively%20leverages%20multi-level%20ViT%20features%20to%20tighten%20vision-language%20alignment%3B%20and%20%28iii%29%20text-based%20time%20alignment%20for%20video%2C%20evolving%20from%20T-RoPE%20to%20explicit%20textual%20timestamp%20alignment%20for%20more%20precise%20temporal%20grounding.%20Under%20comparable%20token%20budgets%20and%20latency%20constraints%2C%20Qwen3-VL%20achieves%20superior%20performance%20in%20both%20dense%20and%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20envision%20Qwen3-VL%20serving%20as%20a%20foundational%20engine%20for%20image-grounded%20reasoning%2C%20agentic%20decision-making%2C%20and%20multimodal%20code%20intelligence%20in%20real-world%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen3-VL%2520Technical%2520Report%26entry.906535625%3DShuai%2520Bai%2520and%2520Yuxuan%2520Cai%2520and%2520Ruizhe%2520Chen%2520and%2520Keqin%2520Chen%2520and%2520Xionghui%2520Chen%2520and%2520Zesen%2520Cheng%2520and%2520Lianghao%2520Deng%2520and%2520Wei%2520Ding%2520and%2520Chang%2520Gao%2520and%2520Chunjiang%2520Ge%2520and%2520Wenbin%2520Ge%2520and%2520Zhifang%2520Guo%2520and%2520Qidong%2520Huang%2520and%2520Jie%2520Huang%2520and%2520Fei%2520Huang%2520and%2520Binyuan%2520Hui%2520and%2520Shutong%2520Jiang%2520and%2520Zhaohai%2520Li%2520and%2520Mingsheng%2520Li%2520and%2520Mei%2520Li%2520and%2520Kaixin%2520Li%2520and%2520Zicheng%2520Lin%2520and%2520Junyang%2520Lin%2520and%2520Xuejing%2520Liu%2520and%2520Jiawei%2520Liu%2520and%2520Chenglong%2520Liu%2520and%2520Yang%2520Liu%2520and%2520Dayiheng%2520Liu%2520and%2520Shixuan%2520Liu%2520and%2520Dunjie%2520Lu%2520and%2520Ruilin%2520Luo%2520and%2520Chenxu%2520Lv%2520and%2520Rui%2520Men%2520and%2520Lingchen%2520Meng%2520and%2520Xuancheng%2520Ren%2520and%2520Xingzhang%2520Ren%2520and%2520Sibo%2520Song%2520and%2520Yuchong%2520Sun%2520and%2520Jun%2520Tang%2520and%2520Jianhong%2520Tu%2520and%2520Jianqiang%2520Wan%2520and%2520Peng%2520Wang%2520and%2520Pengfei%2520Wang%2520and%2520Qiuyue%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Tianbao%2520Xie%2520and%2520Yiheng%2520Xu%2520and%2520Haiyang%2520Xu%2520and%2520Jin%2520Xu%2520and%2520Zhibo%2520Yang%2520and%2520Mingkun%2520Yang%2520and%2520Jianxin%2520Yang%2520and%2520An%2520Yang%2520and%2520Bowen%2520Yu%2520and%2520Fei%2520Zhang%2520and%2520Hang%2520Zhang%2520and%2520Xi%2520Zhang%2520and%2520Bo%2520Zheng%2520and%2520Humen%2520Zhong%2520and%2520Jingren%2520Zhou%2520and%2520Fan%2520Zhou%2520and%2520Jing%2520Zhou%2520and%2520Yuanzhi%2520Zhu%2520and%2520Ke%2520Zhu%26entry.1292438233%3DWe%2520introduce%2520Qwen3-VL%252C%2520the%2520most%2520capable%2520vision-language%2520model%2520in%2520the%2520Qwen%2520series%2520to%2520date%252C%2520achieving%2520superior%2520performance%2520across%2520a%2520broad%2520range%2520of%2520multimodal%2520benchmarks.%2520It%2520natively%2520supports%2520interleaved%2520contexts%2520of%2520up%2520to%2520256K%2520tokens%252C%2520seamlessly%2520integrating%2520text%252C%2520images%252C%2520and%2520video.%2520The%2520model%2520family%2520includes%2520both%2520dense%2520%25282B/4B/8B/32B%2529%2520and%2520mixture-of-experts%2520%252830B-A3B/235B-A22B%2529%2520variants%2520to%2520accommodate%2520diverse%2520latency-quality%2520trade-offs.%2520Qwen3-VL%2520delivers%2520three%2520core%2520pillars%253A%2520%2528i%2529%2520markedly%2520stronger%2520pure-text%2520understanding%252C%2520surpassing%2520comparable%2520text-only%2520backbones%2520in%2520several%2520cases%253B%2520%2528ii%2529%2520robust%2520long-context%2520comprehension%2520with%2520a%2520native%2520256K-token%2520window%2520for%2520both%2520text%2520and%2520interleaved%2520multimodal%2520inputs%252C%2520enabling%2520faithful%2520retention%252C%2520retrieval%252C%2520and%2520cross-referencing%2520across%2520long%2520documents%2520and%2520videos%253B%2520and%2520%2528iii%2529%2520advanced%2520multimodal%2520reasoning%2520across%2520single-image%252C%2520multi-image%252C%2520and%2520video%2520tasks%252C%2520demonstrating%2520leading%2520performance%2520on%2520comprehensive%2520evaluations%2520such%2520as%2520MMMU%2520and%2520visual-math%2520benchmarks%2520%2528e.g.%252C%2520MathVista%2520and%2520MathVision%2529.%2520Architecturally%252C%2520we%2520introduce%2520three%2520key%2520upgrades%253A%2520%2528i%2529%2520an%2520enhanced%2520interleaved-MRoPE%2520for%2520stronger%2520spatial-temporal%2520modeling%2520across%2520images%2520and%2520video%253B%2520%2528ii%2529%2520DeepStack%2520integration%252C%2520which%2520effectively%2520leverages%2520multi-level%2520ViT%2520features%2520to%2520tighten%2520vision-language%2520alignment%253B%2520and%2520%2528iii%2529%2520text-based%2520time%2520alignment%2520for%2520video%252C%2520evolving%2520from%2520T-RoPE%2520to%2520explicit%2520textual%2520timestamp%2520alignment%2520for%2520more%2520precise%2520temporal%2520grounding.%2520Under%2520comparable%2520token%2520budgets%2520and%2520latency%2520constraints%252C%2520Qwen3-VL%2520achieves%2520superior%2520performance%2520in%2520both%2520dense%2520and%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures.%2520We%2520envision%2520Qwen3-VL%2520serving%2520as%2520a%2520foundational%2520engine%2520for%2520image-grounded%2520reasoning%252C%2520agentic%2520decision-making%252C%2520and%2520multimodal%2520code%2520intelligence%2520in%2520real-world%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen3-VL%20Technical%20Report&entry.906535625=Shuai%20Bai%20and%20Yuxuan%20Cai%20and%20Ruizhe%20Chen%20and%20Keqin%20Chen%20and%20Xionghui%20Chen%20and%20Zesen%20Cheng%20and%20Lianghao%20Deng%20and%20Wei%20Ding%20and%20Chang%20Gao%20and%20Chunjiang%20Ge%20and%20Wenbin%20Ge%20and%20Zhifang%20Guo%20and%20Qidong%20Huang%20and%20Jie%20Huang%20and%20Fei%20Huang%20and%20Binyuan%20Hui%20and%20Shutong%20Jiang%20and%20Zhaohai%20Li%20and%20Mingsheng%20Li%20and%20Mei%20Li%20and%20Kaixin%20Li%20and%20Zicheng%20Lin%20and%20Junyang%20Lin%20and%20Xuejing%20Liu%20and%20Jiawei%20Liu%20and%20Chenglong%20Liu%20and%20Yang%20Liu%20and%20Dayiheng%20Liu%20and%20Shixuan%20Liu%20and%20Dunjie%20Lu%20and%20Ruilin%20Luo%20and%20Chenxu%20Lv%20and%20Rui%20Men%20and%20Lingchen%20Meng%20and%20Xuancheng%20Ren%20and%20Xingzhang%20Ren%20and%20Sibo%20Song%20and%20Yuchong%20Sun%20and%20Jun%20Tang%20and%20Jianhong%20Tu%20and%20Jianqiang%20Wan%20and%20Peng%20Wang%20and%20Pengfei%20Wang%20and%20Qiuyue%20Wang%20and%20Yuxuan%20Wang%20and%20Tianbao%20Xie%20and%20Yiheng%20Xu%20and%20Haiyang%20Xu%20and%20Jin%20Xu%20and%20Zhibo%20Yang%20and%20Mingkun%20Yang%20and%20Jianxin%20Yang%20and%20An%20Yang%20and%20Bowen%20Yu%20and%20Fei%20Zhang%20and%20Hang%20Zhang%20and%20Xi%20Zhang%20and%20Bo%20Zheng%20and%20Humen%20Zhong%20and%20Jingren%20Zhou%20and%20Fan%20Zhou%20and%20Jing%20Zhou%20and%20Yuanzhi%20Zhu%20and%20Ke%20Zhu&entry.1292438233=We%20introduce%20Qwen3-VL%2C%20the%20most%20capable%20vision-language%20model%20in%20the%20Qwen%20series%20to%20date%2C%20achieving%20superior%20performance%20across%20a%20broad%20range%20of%20multimodal%20benchmarks.%20It%20natively%20supports%20interleaved%20contexts%20of%20up%20to%20256K%20tokens%2C%20seamlessly%20integrating%20text%2C%20images%2C%20and%20video.%20The%20model%20family%20includes%20both%20dense%20%282B/4B/8B/32B%29%20and%20mixture-of-experts%20%2830B-A3B/235B-A22B%29%20variants%20to%20accommodate%20diverse%20latency-quality%20trade-offs.%20Qwen3-VL%20delivers%20three%20core%20pillars%3A%20%28i%29%20markedly%20stronger%20pure-text%20understanding%2C%20surpassing%20comparable%20text-only%20backbones%20in%20several%20cases%3B%20%28ii%29%20robust%20long-context%20comprehension%20with%20a%20native%20256K-token%20window%20for%20both%20text%20and%20interleaved%20multimodal%20inputs%2C%20enabling%20faithful%20retention%2C%20retrieval%2C%20and%20cross-referencing%20across%20long%20documents%20and%20videos%3B%20and%20%28iii%29%20advanced%20multimodal%20reasoning%20across%20single-image%2C%20multi-image%2C%20and%20video%20tasks%2C%20demonstrating%20leading%20performance%20on%20comprehensive%20evaluations%20such%20as%20MMMU%20and%20visual-math%20benchmarks%20%28e.g.%2C%20MathVista%20and%20MathVision%29.%20Architecturally%2C%20we%20introduce%20three%20key%20upgrades%3A%20%28i%29%20an%20enhanced%20interleaved-MRoPE%20for%20stronger%20spatial-temporal%20modeling%20across%20images%20and%20video%3B%20%28ii%29%20DeepStack%20integration%2C%20which%20effectively%20leverages%20multi-level%20ViT%20features%20to%20tighten%20vision-language%20alignment%3B%20and%20%28iii%29%20text-based%20time%20alignment%20for%20video%2C%20evolving%20from%20T-RoPE%20to%20explicit%20textual%20timestamp%20alignment%20for%20more%20precise%20temporal%20grounding.%20Under%20comparable%20token%20budgets%20and%20latency%20constraints%2C%20Qwen3-VL%20achieves%20superior%20performance%20in%20both%20dense%20and%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20envision%20Qwen3-VL%20serving%20as%20a%20foundational%20engine%20for%20image-grounded%20reasoning%2C%20agentic%20decision-making%2C%20and%20multimodal%20code%20intelligence%20in%20real-world%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2511.21631v1&entry.124074799=Read"},
{"title": "From Limited Labels to Open Domains:An Efficient Learning Method for Drone-view Geo-Localization", "author": "Zhongwei Chen and Zhao-Xu Yang and Hai-Jun Rong and Jiawei Lang and Guoqi Li", "abstract": "Traditional supervised drone-view geo-localization (DVGL) methods heavily depend on paired training data and encounter difficulties in learning cross-view correlations from unpaired data. Moreover, when deployed in a new domain, these methods require obtaining the new paired data and subsequent retraining for model adaptation, which significantly increases computational overhead. Existing unsupervised methods have enabled to generate pseudo-labels based on cross-view similarity to infer the pairing relationships. However, geographical similarity and spatial continuity often cause visually analogous features at different geographical locations. The feature confusion compromises the reliability of pseudo-label generation, where incorrect pseudo-labels drive negative optimization. Given these challenges inherent in both supervised and unsupervised DVGL methods, we propose a novel cross-domain invariant knowledge transfer network (CDIKTNet) with limited supervision, whose architecture consists of a cross-domain invariance sub-network (CDIS) and a cross-domain transfer sub-network (CDTS). This architecture facilitates a closed-loop framework for invariance feature learning and knowledge transfer. The CDIS is designed to learn cross-view structural and spatial invariance from a small amount of paired data that serves as prior knowledge. It endows the shared feature space of unpaired data with similar implicit cross-view correlations at initialization, which alleviates feature confusion. Based on this, the CDTS employs dual-path contrastive learning to further optimize each subspace while preserving consistency in a shared feature space. Extensive experiments demonstrate that CDIKTNet achieves state-of-the-art performance under full supervision compared with those supervised methods, and further surpasses existing unsupervised methods in both few-shot and cross-domain initialization.", "link": "http://arxiv.org/abs/2503.07520v3", "date": "2025-11-26", "relevancy": 2.9154, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6371}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Limited%20Labels%20to%20Open%20Domains%3AAn%20Efficient%20Learning%20Method%20for%20Drone-view%20Geo-Localization&body=Title%3A%20From%20Limited%20Labels%20to%20Open%20Domains%3AAn%20Efficient%20Learning%20Method%20for%20Drone-view%20Geo-Localization%0AAuthor%3A%20Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Jiawei%20Lang%20and%20Guoqi%20Li%0AAbstract%3A%20Traditional%20supervised%20drone-view%20geo-localization%20%28DVGL%29%20methods%20heavily%20depend%20on%20paired%20training%20data%20and%20encounter%20difficulties%20in%20learning%20cross-view%20correlations%20from%20unpaired%20data.%20Moreover%2C%20when%20deployed%20in%20a%20new%20domain%2C%20these%20methods%20require%20obtaining%20the%20new%20paired%20data%20and%20subsequent%20retraining%20for%20model%20adaptation%2C%20which%20significantly%20increases%20computational%20overhead.%20Existing%20unsupervised%20methods%20have%20enabled%20to%20generate%20pseudo-labels%20based%20on%20cross-view%20similarity%20to%20infer%20the%20pairing%20relationships.%20However%2C%20geographical%20similarity%20and%20spatial%20continuity%20often%20cause%20visually%20analogous%20features%20at%20different%20geographical%20locations.%20The%20feature%20confusion%20compromises%20the%20reliability%20of%20pseudo-label%20generation%2C%20where%20incorrect%20pseudo-labels%20drive%20negative%20optimization.%20Given%20these%20challenges%20inherent%20in%20both%20supervised%20and%20unsupervised%20DVGL%20methods%2C%20we%20propose%20a%20novel%20cross-domain%20invariant%20knowledge%20transfer%20network%20%28CDIKTNet%29%20with%20limited%20supervision%2C%20whose%20architecture%20consists%20of%20a%20cross-domain%20invariance%20sub-network%20%28CDIS%29%20and%20a%20cross-domain%20transfer%20sub-network%20%28CDTS%29.%20This%20architecture%20facilitates%20a%20closed-loop%20framework%20for%20invariance%20feature%20learning%20and%20knowledge%20transfer.%20The%20CDIS%20is%20designed%20to%20learn%20cross-view%20structural%20and%20spatial%20invariance%20from%20a%20small%20amount%20of%20paired%20data%20that%20serves%20as%20prior%20knowledge.%20It%20endows%20the%20shared%20feature%20space%20of%20unpaired%20data%20with%20similar%20implicit%20cross-view%20correlations%20at%20initialization%2C%20which%20alleviates%20feature%20confusion.%20Based%20on%20this%2C%20the%20CDTS%20employs%20dual-path%20contrastive%20learning%20to%20further%20optimize%20each%20subspace%20while%20preserving%20consistency%20in%20a%20shared%20feature%20space.%20Extensive%20experiments%20demonstrate%20that%20CDIKTNet%20achieves%20state-of-the-art%20performance%20under%20full%20supervision%20compared%20with%20those%20supervised%20methods%2C%20and%20further%20surpasses%20existing%20unsupervised%20methods%20in%20both%20few-shot%20and%20cross-domain%20initialization.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07520v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Limited%2520Labels%2520to%2520Open%2520Domains%253AAn%2520Efficient%2520Learning%2520Method%2520for%2520Drone-view%2520Geo-Localization%26entry.906535625%3DZhongwei%2520Chen%2520and%2520Zhao-Xu%2520Yang%2520and%2520Hai-Jun%2520Rong%2520and%2520Jiawei%2520Lang%2520and%2520Guoqi%2520Li%26entry.1292438233%3DTraditional%2520supervised%2520drone-view%2520geo-localization%2520%2528DVGL%2529%2520methods%2520heavily%2520depend%2520on%2520paired%2520training%2520data%2520and%2520encounter%2520difficulties%2520in%2520learning%2520cross-view%2520correlations%2520from%2520unpaired%2520data.%2520Moreover%252C%2520when%2520deployed%2520in%2520a%2520new%2520domain%252C%2520these%2520methods%2520require%2520obtaining%2520the%2520new%2520paired%2520data%2520and%2520subsequent%2520retraining%2520for%2520model%2520adaptation%252C%2520which%2520significantly%2520increases%2520computational%2520overhead.%2520Existing%2520unsupervised%2520methods%2520have%2520enabled%2520to%2520generate%2520pseudo-labels%2520based%2520on%2520cross-view%2520similarity%2520to%2520infer%2520the%2520pairing%2520relationships.%2520However%252C%2520geographical%2520similarity%2520and%2520spatial%2520continuity%2520often%2520cause%2520visually%2520analogous%2520features%2520at%2520different%2520geographical%2520locations.%2520The%2520feature%2520confusion%2520compromises%2520the%2520reliability%2520of%2520pseudo-label%2520generation%252C%2520where%2520incorrect%2520pseudo-labels%2520drive%2520negative%2520optimization.%2520Given%2520these%2520challenges%2520inherent%2520in%2520both%2520supervised%2520and%2520unsupervised%2520DVGL%2520methods%252C%2520we%2520propose%2520a%2520novel%2520cross-domain%2520invariant%2520knowledge%2520transfer%2520network%2520%2528CDIKTNet%2529%2520with%2520limited%2520supervision%252C%2520whose%2520architecture%2520consists%2520of%2520a%2520cross-domain%2520invariance%2520sub-network%2520%2528CDIS%2529%2520and%2520a%2520cross-domain%2520transfer%2520sub-network%2520%2528CDTS%2529.%2520This%2520architecture%2520facilitates%2520a%2520closed-loop%2520framework%2520for%2520invariance%2520feature%2520learning%2520and%2520knowledge%2520transfer.%2520The%2520CDIS%2520is%2520designed%2520to%2520learn%2520cross-view%2520structural%2520and%2520spatial%2520invariance%2520from%2520a%2520small%2520amount%2520of%2520paired%2520data%2520that%2520serves%2520as%2520prior%2520knowledge.%2520It%2520endows%2520the%2520shared%2520feature%2520space%2520of%2520unpaired%2520data%2520with%2520similar%2520implicit%2520cross-view%2520correlations%2520at%2520initialization%252C%2520which%2520alleviates%2520feature%2520confusion.%2520Based%2520on%2520this%252C%2520the%2520CDTS%2520employs%2520dual-path%2520contrastive%2520learning%2520to%2520further%2520optimize%2520each%2520subspace%2520while%2520preserving%2520consistency%2520in%2520a%2520shared%2520feature%2520space.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CDIKTNet%2520achieves%2520state-of-the-art%2520performance%2520under%2520full%2520supervision%2520compared%2520with%2520those%2520supervised%2520methods%252C%2520and%2520further%2520surpasses%2520existing%2520unsupervised%2520methods%2520in%2520both%2520few-shot%2520and%2520cross-domain%2520initialization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07520v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Limited%20Labels%20to%20Open%20Domains%3AAn%20Efficient%20Learning%20Method%20for%20Drone-view%20Geo-Localization&entry.906535625=Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Jiawei%20Lang%20and%20Guoqi%20Li&entry.1292438233=Traditional%20supervised%20drone-view%20geo-localization%20%28DVGL%29%20methods%20heavily%20depend%20on%20paired%20training%20data%20and%20encounter%20difficulties%20in%20learning%20cross-view%20correlations%20from%20unpaired%20data.%20Moreover%2C%20when%20deployed%20in%20a%20new%20domain%2C%20these%20methods%20require%20obtaining%20the%20new%20paired%20data%20and%20subsequent%20retraining%20for%20model%20adaptation%2C%20which%20significantly%20increases%20computational%20overhead.%20Existing%20unsupervised%20methods%20have%20enabled%20to%20generate%20pseudo-labels%20based%20on%20cross-view%20similarity%20to%20infer%20the%20pairing%20relationships.%20However%2C%20geographical%20similarity%20and%20spatial%20continuity%20often%20cause%20visually%20analogous%20features%20at%20different%20geographical%20locations.%20The%20feature%20confusion%20compromises%20the%20reliability%20of%20pseudo-label%20generation%2C%20where%20incorrect%20pseudo-labels%20drive%20negative%20optimization.%20Given%20these%20challenges%20inherent%20in%20both%20supervised%20and%20unsupervised%20DVGL%20methods%2C%20we%20propose%20a%20novel%20cross-domain%20invariant%20knowledge%20transfer%20network%20%28CDIKTNet%29%20with%20limited%20supervision%2C%20whose%20architecture%20consists%20of%20a%20cross-domain%20invariance%20sub-network%20%28CDIS%29%20and%20a%20cross-domain%20transfer%20sub-network%20%28CDTS%29.%20This%20architecture%20facilitates%20a%20closed-loop%20framework%20for%20invariance%20feature%20learning%20and%20knowledge%20transfer.%20The%20CDIS%20is%20designed%20to%20learn%20cross-view%20structural%20and%20spatial%20invariance%20from%20a%20small%20amount%20of%20paired%20data%20that%20serves%20as%20prior%20knowledge.%20It%20endows%20the%20shared%20feature%20space%20of%20unpaired%20data%20with%20similar%20implicit%20cross-view%20correlations%20at%20initialization%2C%20which%20alleviates%20feature%20confusion.%20Based%20on%20this%2C%20the%20CDTS%20employs%20dual-path%20contrastive%20learning%20to%20further%20optimize%20each%20subspace%20while%20preserving%20consistency%20in%20a%20shared%20feature%20space.%20Extensive%20experiments%20demonstrate%20that%20CDIKTNet%20achieves%20state-of-the-art%20performance%20under%20full%20supervision%20compared%20with%20those%20supervised%20methods%2C%20and%20further%20surpasses%20existing%20unsupervised%20methods%20in%20both%20few-shot%20and%20cross-domain%20initialization.&entry.1838667208=http%3A//arxiv.org/abs/2503.07520v3&entry.124074799=Read"},
{"title": "SARVLM: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery", "author": "Qiwei Ma and Zhiyu Wang and Wang Liu and Xukun Lu and Bin Deng and Puhong Duan and Xudong Kang and Shutao Li", "abstract": "Synthetic Aperture Radar (SAR) is a crucial imaging modality thanks to its all-weather capability. Although recent advances in self-supervised learning and masked image modeling (MIM) have enabled SAR foundation models, these methods largely emphasize low-level visual features and often overlook multimodal alignment and zero-shot target recognition in SAR imagery. To address this, we construct SARVLM-1M, a large-scale vision-language dataset with over one million image-text pairs aggregated from existing datasets. We further propose a domain transfer training strategy to mitigate the large gap between natural and SAR imagery. Building on this, we develop SARVLM, the first vision language foundation model (VLM) tailored to SAR, comprising SARCLIP and SARCap. SARVLM is trained with a vision-language contrastive objective under the proposed domain transfer strategy, bridging SAR imagery and textual descriptions. Extensive experiments on image text retrieval, zero-shot classification, semantic localization, and imagery captioning demonstrate that SARVLM delivers superior feature extraction and interpretation, outperforming state-of-the-art VLMs and advancing SAR semantic understanding. Code and datasets will be released soon.", "link": "http://arxiv.org/abs/2510.22665v2", "date": "2025-11-26", "relevancy": 2.9105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARVLM%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Semantic%20Understanding%20and%20Target%20Recognition%20in%20SAR%20Imagery&body=Title%3A%20SARVLM%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Semantic%20Understanding%20and%20Target%20Recognition%20in%20SAR%20Imagery%0AAuthor%3A%20Qiwei%20Ma%20and%20Zhiyu%20Wang%20and%20Wang%20Liu%20and%20Xukun%20Lu%20and%20Bin%20Deng%20and%20Puhong%20Duan%20and%20Xudong%20Kang%20and%20Shutao%20Li%0AAbstract%3A%20Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20a%20crucial%20imaging%20modality%20thanks%20to%20its%20all-weather%20capability.%20Although%20recent%20advances%20in%20self-supervised%20learning%20and%20masked%20image%20modeling%20%28MIM%29%20have%20enabled%20SAR%20foundation%20models%2C%20these%20methods%20largely%20emphasize%20low-level%20visual%20features%20and%20often%20overlook%20multimodal%20alignment%20and%20zero-shot%20target%20recognition%20in%20SAR%20imagery.%20To%20address%20this%2C%20we%20construct%20SARVLM-1M%2C%20a%20large-scale%20vision-language%20dataset%20with%20over%20one%20million%20image-text%20pairs%20aggregated%20from%20existing%20datasets.%20We%20further%20propose%20a%20domain%20transfer%20training%20strategy%20to%20mitigate%20the%20large%20gap%20between%20natural%20and%20SAR%20imagery.%20Building%20on%20this%2C%20we%20develop%20SARVLM%2C%20the%20first%20vision%20language%20foundation%20model%20%28VLM%29%20tailored%20to%20SAR%2C%20comprising%20SARCLIP%20and%20SARCap.%20SARVLM%20is%20trained%20with%20a%20vision-language%20contrastive%20objective%20under%20the%20proposed%20domain%20transfer%20strategy%2C%20bridging%20SAR%20imagery%20and%20textual%20descriptions.%20Extensive%20experiments%20on%20image%20text%20retrieval%2C%20zero-shot%20classification%2C%20semantic%20localization%2C%20and%20imagery%20captioning%20demonstrate%20that%20SARVLM%20delivers%20superior%20feature%20extraction%20and%20interpretation%2C%20outperforming%20state-of-the-art%20VLMs%20and%20advancing%20SAR%20semantic%20understanding.%20Code%20and%20datasets%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARVLM%253A%2520A%2520Vision%2520Language%2520Foundation%2520Model%2520for%2520Semantic%2520Understanding%2520and%2520Target%2520Recognition%2520in%2520SAR%2520Imagery%26entry.906535625%3DQiwei%2520Ma%2520and%2520Zhiyu%2520Wang%2520and%2520Wang%2520Liu%2520and%2520Xukun%2520Lu%2520and%2520Bin%2520Deng%2520and%2520Puhong%2520Duan%2520and%2520Xudong%2520Kang%2520and%2520Shutao%2520Li%26entry.1292438233%3DSynthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520is%2520a%2520crucial%2520imaging%2520modality%2520thanks%2520to%2520its%2520all-weather%2520capability.%2520Although%2520recent%2520advances%2520in%2520self-supervised%2520learning%2520and%2520masked%2520image%2520modeling%2520%2528MIM%2529%2520have%2520enabled%2520SAR%2520foundation%2520models%252C%2520these%2520methods%2520largely%2520emphasize%2520low-level%2520visual%2520features%2520and%2520often%2520overlook%2520multimodal%2520alignment%2520and%2520zero-shot%2520target%2520recognition%2520in%2520SAR%2520imagery.%2520To%2520address%2520this%252C%2520we%2520construct%2520SARVLM-1M%252C%2520a%2520large-scale%2520vision-language%2520dataset%2520with%2520over%2520one%2520million%2520image-text%2520pairs%2520aggregated%2520from%2520existing%2520datasets.%2520We%2520further%2520propose%2520a%2520domain%2520transfer%2520training%2520strategy%2520to%2520mitigate%2520the%2520large%2520gap%2520between%2520natural%2520and%2520SAR%2520imagery.%2520Building%2520on%2520this%252C%2520we%2520develop%2520SARVLM%252C%2520the%2520first%2520vision%2520language%2520foundation%2520model%2520%2528VLM%2529%2520tailored%2520to%2520SAR%252C%2520comprising%2520SARCLIP%2520and%2520SARCap.%2520SARVLM%2520is%2520trained%2520with%2520a%2520vision-language%2520contrastive%2520objective%2520under%2520the%2520proposed%2520domain%2520transfer%2520strategy%252C%2520bridging%2520SAR%2520imagery%2520and%2520textual%2520descriptions.%2520Extensive%2520experiments%2520on%2520image%2520text%2520retrieval%252C%2520zero-shot%2520classification%252C%2520semantic%2520localization%252C%2520and%2520imagery%2520captioning%2520demonstrate%2520that%2520SARVLM%2520delivers%2520superior%2520feature%2520extraction%2520and%2520interpretation%252C%2520outperforming%2520state-of-the-art%2520VLMs%2520and%2520advancing%2520SAR%2520semantic%2520understanding.%2520Code%2520and%2520datasets%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARVLM%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Semantic%20Understanding%20and%20Target%20Recognition%20in%20SAR%20Imagery&entry.906535625=Qiwei%20Ma%20and%20Zhiyu%20Wang%20and%20Wang%20Liu%20and%20Xukun%20Lu%20and%20Bin%20Deng%20and%20Puhong%20Duan%20and%20Xudong%20Kang%20and%20Shutao%20Li&entry.1292438233=Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20a%20crucial%20imaging%20modality%20thanks%20to%20its%20all-weather%20capability.%20Although%20recent%20advances%20in%20self-supervised%20learning%20and%20masked%20image%20modeling%20%28MIM%29%20have%20enabled%20SAR%20foundation%20models%2C%20these%20methods%20largely%20emphasize%20low-level%20visual%20features%20and%20often%20overlook%20multimodal%20alignment%20and%20zero-shot%20target%20recognition%20in%20SAR%20imagery.%20To%20address%20this%2C%20we%20construct%20SARVLM-1M%2C%20a%20large-scale%20vision-language%20dataset%20with%20over%20one%20million%20image-text%20pairs%20aggregated%20from%20existing%20datasets.%20We%20further%20propose%20a%20domain%20transfer%20training%20strategy%20to%20mitigate%20the%20large%20gap%20between%20natural%20and%20SAR%20imagery.%20Building%20on%20this%2C%20we%20develop%20SARVLM%2C%20the%20first%20vision%20language%20foundation%20model%20%28VLM%29%20tailored%20to%20SAR%2C%20comprising%20SARCLIP%20and%20SARCap.%20SARVLM%20is%20trained%20with%20a%20vision-language%20contrastive%20objective%20under%20the%20proposed%20domain%20transfer%20strategy%2C%20bridging%20SAR%20imagery%20and%20textual%20descriptions.%20Extensive%20experiments%20on%20image%20text%20retrieval%2C%20zero-shot%20classification%2C%20semantic%20localization%2C%20and%20imagery%20captioning%20demonstrate%20that%20SARVLM%20delivers%20superior%20feature%20extraction%20and%20interpretation%2C%20outperforming%20state-of-the-art%20VLMs%20and%20advancing%20SAR%20semantic%20understanding.%20Code%20and%20datasets%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2510.22665v2&entry.124074799=Read"},
{"title": "Co-Training Vision Language Models for Remote Sensing Multi-task Learning", "author": "Qingyun Li and Shuran Ma and Junwei Luo and Yi Yu and Yue Zhou and Fengxiang Wang and Xudong Lu and Xiaoxing Wang and Xin He and Yushi Chen and Xue Yang and Junchi Yan", "abstract": "With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.", "link": "http://arxiv.org/abs/2511.21272v1", "date": "2025-11-26", "relevancy": 2.9103, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning&body=Title%3A%20Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning%0AAuthor%3A%20Qingyun%20Li%20and%20Shuran%20Ma%20and%20Junwei%20Luo%20and%20Yi%20Yu%20and%20Yue%20Zhou%20and%20Fengxiang%20Wang%20and%20Xudong%20Lu%20and%20Xiaoxing%20Wang%20and%20Xin%20He%20and%20Yushi%20Chen%20and%20Xue%20Yang%20and%20Junchi%20Yan%0AAbstract%3A%20With%20Transformers%20achieving%20outstanding%20performance%20on%20individual%20remote%20sensing%20%28RS%29%20tasks%2C%20we%20are%20now%20approaching%20the%20realization%20of%20a%20unified%20model%20that%20excels%20across%20multiple%20tasks%20through%20multi-task%20learning%20%28MTL%29.%20Compared%20to%20single-task%20approaches%2C%20MTL%20methods%20offer%20improved%20generalization%2C%20enhanced%20scalability%2C%20and%20greater%20practical%20applicability.%20Recently%2C%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20promising%20results%20in%20RS%20image%20understanding%2C%20grounding%2C%20and%20ultra-high-resolution%20%28UHR%29%20image%20reasoning%2C%20respectively.%20Moreover%2C%20the%20unified%20text-based%20interface%20demonstrates%20significant%20potential%20for%20MTL.%20Hence%2C%20in%20this%20work%2C%20we%20present%20RSCoVLM%2C%20a%20simple%20yet%20flexible%20VLM%20baseline%20for%20RS%20MTL.%20Firstly%2C%20we%20create%20the%20data%20curation%20engine%2C%20including%20data%20acquisition%2C%20offline%20processing%20and%20integrating%2C%20as%20well%20as%20online%20loading%20and%20weighting.%20This%20data%20engine%20effectively%20addresses%20complex%20RS%20data%20enviroment%20and%20generates%20flexible%20vision-language%20conversations.%20Furthermore%2C%20we%20propose%20a%20unified%20dynamic-resolution%20strategy%20to%20address%20the%20diverse%20image%20scales%20inherent%20in%20RS%20imagery.%20For%20UHR%20images%2C%20we%20introduce%20the%20Zoom-in%20Chain%20mechanism%20together%20with%20its%20corresponding%20dataset%2C%20LRS-VQA-Zoom.%20The%20strategies%20are%20flexible%20and%20effectively%20mitigate%20the%20computational%20burdens.%20Additionally%2C%20we%20significantly%20enhance%20the%20model%27s%20object%20detection%20capability%20and%20propose%20a%20novel%20evaluation%20protocol%20that%20ensures%20fair%20comparison%20between%20VLMs%20and%20conventional%20detection%20models.%20Extensive%20experiments%20demonstrate%20that%20RSCoVLM%20achieves%20state-of-the-art%20performance%20across%20diverse%20tasks%2C%20outperforming%20existing%20RS%20VLMs%20and%20even%20rivaling%20specialized%20expert%20models.%20All%20the%20training%20and%20evaluating%20tools%2C%20model%20weights%2C%20and%20datasets%20have%20been%20fully%20open-sourced%20to%20support%20reproducibility.%20We%20expect%20that%20this%20baseline%20will%20promote%20further%20progress%20toward%20general-purpose%20RS%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Training%2520Vision%2520Language%2520Models%2520for%2520Remote%2520Sensing%2520Multi-task%2520Learning%26entry.906535625%3DQingyun%2520Li%2520and%2520Shuran%2520Ma%2520and%2520Junwei%2520Luo%2520and%2520Yi%2520Yu%2520and%2520Yue%2520Zhou%2520and%2520Fengxiang%2520Wang%2520and%2520Xudong%2520Lu%2520and%2520Xiaoxing%2520Wang%2520and%2520Xin%2520He%2520and%2520Yushi%2520Chen%2520and%2520Xue%2520Yang%2520and%2520Junchi%2520Yan%26entry.1292438233%3DWith%2520Transformers%2520achieving%2520outstanding%2520performance%2520on%2520individual%2520remote%2520sensing%2520%2528RS%2529%2520tasks%252C%2520we%2520are%2520now%2520approaching%2520the%2520realization%2520of%2520a%2520unified%2520model%2520that%2520excels%2520across%2520multiple%2520tasks%2520through%2520multi-task%2520learning%2520%2528MTL%2529.%2520Compared%2520to%2520single-task%2520approaches%252C%2520MTL%2520methods%2520offer%2520improved%2520generalization%252C%2520enhanced%2520scalability%252C%2520and%2520greater%2520practical%2520applicability.%2520Recently%252C%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520promising%2520results%2520in%2520RS%2520image%2520understanding%252C%2520grounding%252C%2520and%2520ultra-high-resolution%2520%2528UHR%2529%2520image%2520reasoning%252C%2520respectively.%2520Moreover%252C%2520the%2520unified%2520text-based%2520interface%2520demonstrates%2520significant%2520potential%2520for%2520MTL.%2520Hence%252C%2520in%2520this%2520work%252C%2520we%2520present%2520RSCoVLM%252C%2520a%2520simple%2520yet%2520flexible%2520VLM%2520baseline%2520for%2520RS%2520MTL.%2520Firstly%252C%2520we%2520create%2520the%2520data%2520curation%2520engine%252C%2520including%2520data%2520acquisition%252C%2520offline%2520processing%2520and%2520integrating%252C%2520as%2520well%2520as%2520online%2520loading%2520and%2520weighting.%2520This%2520data%2520engine%2520effectively%2520addresses%2520complex%2520RS%2520data%2520enviroment%2520and%2520generates%2520flexible%2520vision-language%2520conversations.%2520Furthermore%252C%2520we%2520propose%2520a%2520unified%2520dynamic-resolution%2520strategy%2520to%2520address%2520the%2520diverse%2520image%2520scales%2520inherent%2520in%2520RS%2520imagery.%2520For%2520UHR%2520images%252C%2520we%2520introduce%2520the%2520Zoom-in%2520Chain%2520mechanism%2520together%2520with%2520its%2520corresponding%2520dataset%252C%2520LRS-VQA-Zoom.%2520The%2520strategies%2520are%2520flexible%2520and%2520effectively%2520mitigate%2520the%2520computational%2520burdens.%2520Additionally%252C%2520we%2520significantly%2520enhance%2520the%2520model%2527s%2520object%2520detection%2520capability%2520and%2520propose%2520a%2520novel%2520evaluation%2520protocol%2520that%2520ensures%2520fair%2520comparison%2520between%2520VLMs%2520and%2520conventional%2520detection%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RSCoVLM%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%2520tasks%252C%2520outperforming%2520existing%2520RS%2520VLMs%2520and%2520even%2520rivaling%2520specialized%2520expert%2520models.%2520All%2520the%2520training%2520and%2520evaluating%2520tools%252C%2520model%2520weights%252C%2520and%2520datasets%2520have%2520been%2520fully%2520open-sourced%2520to%2520support%2520reproducibility.%2520We%2520expect%2520that%2520this%2520baseline%2520will%2520promote%2520further%2520progress%2520toward%2520general-purpose%2520RS%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning&entry.906535625=Qingyun%20Li%20and%20Shuran%20Ma%20and%20Junwei%20Luo%20and%20Yi%20Yu%20and%20Yue%20Zhou%20and%20Fengxiang%20Wang%20and%20Xudong%20Lu%20and%20Xiaoxing%20Wang%20and%20Xin%20He%20and%20Yushi%20Chen%20and%20Xue%20Yang%20and%20Junchi%20Yan&entry.1292438233=With%20Transformers%20achieving%20outstanding%20performance%20on%20individual%20remote%20sensing%20%28RS%29%20tasks%2C%20we%20are%20now%20approaching%20the%20realization%20of%20a%20unified%20model%20that%20excels%20across%20multiple%20tasks%20through%20multi-task%20learning%20%28MTL%29.%20Compared%20to%20single-task%20approaches%2C%20MTL%20methods%20offer%20improved%20generalization%2C%20enhanced%20scalability%2C%20and%20greater%20practical%20applicability.%20Recently%2C%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20promising%20results%20in%20RS%20image%20understanding%2C%20grounding%2C%20and%20ultra-high-resolution%20%28UHR%29%20image%20reasoning%2C%20respectively.%20Moreover%2C%20the%20unified%20text-based%20interface%20demonstrates%20significant%20potential%20for%20MTL.%20Hence%2C%20in%20this%20work%2C%20we%20present%20RSCoVLM%2C%20a%20simple%20yet%20flexible%20VLM%20baseline%20for%20RS%20MTL.%20Firstly%2C%20we%20create%20the%20data%20curation%20engine%2C%20including%20data%20acquisition%2C%20offline%20processing%20and%20integrating%2C%20as%20well%20as%20online%20loading%20and%20weighting.%20This%20data%20engine%20effectively%20addresses%20complex%20RS%20data%20enviroment%20and%20generates%20flexible%20vision-language%20conversations.%20Furthermore%2C%20we%20propose%20a%20unified%20dynamic-resolution%20strategy%20to%20address%20the%20diverse%20image%20scales%20inherent%20in%20RS%20imagery.%20For%20UHR%20images%2C%20we%20introduce%20the%20Zoom-in%20Chain%20mechanism%20together%20with%20its%20corresponding%20dataset%2C%20LRS-VQA-Zoom.%20The%20strategies%20are%20flexible%20and%20effectively%20mitigate%20the%20computational%20burdens.%20Additionally%2C%20we%20significantly%20enhance%20the%20model%27s%20object%20detection%20capability%20and%20propose%20a%20novel%20evaluation%20protocol%20that%20ensures%20fair%20comparison%20between%20VLMs%20and%20conventional%20detection%20models.%20Extensive%20experiments%20demonstrate%20that%20RSCoVLM%20achieves%20state-of-the-art%20performance%20across%20diverse%20tasks%2C%20outperforming%20existing%20RS%20VLMs%20and%20even%20rivaling%20specialized%20expert%20models.%20All%20the%20training%20and%20evaluating%20tools%2C%20model%20weights%2C%20and%20datasets%20have%20been%20fully%20open-sourced%20to%20support%20reproducibility.%20We%20expect%20that%20this%20baseline%20will%20promote%20further%20progress%20toward%20general-purpose%20RS%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.21272v1&entry.124074799=Read"},
{"title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding", "author": "Boshen Xu and Zihan Xiao and Jiaze Li and Jianzhong Ju and Zhenbo Luo and Jian Luan and Qin Jin", "abstract": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.", "link": "http://arxiv.org/abs/2511.16595v2", "date": "2025-11-26", "relevancy": 2.8734, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding&body=Title%3A%20TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding%0AAuthor%3A%20Boshen%20Xu%20and%20Zihan%20Xiao%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Qin%20Jin%0AAbstract%3A%20We%20introduce%20TimeViper%2C%20a%20hybrid%20vision-language%20model%20designed%20to%20tackle%20challenges%20of%20long%20video%20understanding.%20Processing%20long%20videos%20demands%20both%20an%20efficient%20model%20architecture%20and%20an%20effective%20mechanism%20for%20handling%20extended%20temporal%20contexts.%20To%20this%20end%2C%20TimeViper%20adopts%20a%20hybrid%20Mamba-Transformer%20backbone%20that%20combines%20the%20efficiency%20of%20state-space%20models%20with%20the%20expressivity%20of%20attention%20mechanisms.%20Through%20this%20hybrid%20design%2C%20we%20reveal%20the%20vision-to-text%20information%20aggregation%20phenomenon%2C%20where%20information%20progressively%20flows%20from%20vision%20tokens%20to%20text%20tokens%20across%20increasing%20LLM%20depth%2C%20resulting%20in%20severe%20vision%20token%20redundancy.%20Motivated%20by%20this%20observation%2C%20we%20propose%20TransV%2C%20a%20token%20information%20transfer%20module%20that%20transfers%20and%20compresses%20vision%20tokens%20into%20instruction%20tokens%20while%20maintaining%20multimodal%20understanding%20capabilities.%20This%20design%20enables%20TimeViper%20to%20process%20hour-long%20videos%20exceeding%2010%2C000%20frames.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20TimeViper%20competes%20with%20state-of-the-art%20models%20while%20extending%20frame%20numbers.%20We%20further%20analyze%20attention%20behaviors%20of%20both%20Mamba%20and%20Transformer%20layers%2C%20offering%20new%20insights%20into%20hybrid%20model%20interpretability.%20This%20work%20represents%20an%20initial%20step%20towards%20developing%2C%20interpreting%2C%20and%20compressing%20hybrid%20Mamba-Transformer%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeViper%253A%2520A%2520Hybrid%2520Mamba-Transformer%2520Vision-Language%2520Model%2520for%2520Efficient%2520Long%2520Video%2520Understanding%26entry.906535625%3DBoshen%2520Xu%2520and%2520Zihan%2520Xiao%2520and%2520Jiaze%2520Li%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Jian%2520Luan%2520and%2520Qin%2520Jin%26entry.1292438233%3DWe%2520introduce%2520TimeViper%252C%2520a%2520hybrid%2520vision-language%2520model%2520designed%2520to%2520tackle%2520challenges%2520of%2520long%2520video%2520understanding.%2520Processing%2520long%2520videos%2520demands%2520both%2520an%2520efficient%2520model%2520architecture%2520and%2520an%2520effective%2520mechanism%2520for%2520handling%2520extended%2520temporal%2520contexts.%2520To%2520this%2520end%252C%2520TimeViper%2520adopts%2520a%2520hybrid%2520Mamba-Transformer%2520backbone%2520that%2520combines%2520the%2520efficiency%2520of%2520state-space%2520models%2520with%2520the%2520expressivity%2520of%2520attention%2520mechanisms.%2520Through%2520this%2520hybrid%2520design%252C%2520we%2520reveal%2520the%2520vision-to-text%2520information%2520aggregation%2520phenomenon%252C%2520where%2520information%2520progressively%2520flows%2520from%2520vision%2520tokens%2520to%2520text%2520tokens%2520across%2520increasing%2520LLM%2520depth%252C%2520resulting%2520in%2520severe%2520vision%2520token%2520redundancy.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520TransV%252C%2520a%2520token%2520information%2520transfer%2520module%2520that%2520transfers%2520and%2520compresses%2520vision%2520tokens%2520into%2520instruction%2520tokens%2520while%2520maintaining%2520multimodal%2520understanding%2520capabilities.%2520This%2520design%2520enables%2520TimeViper%2520to%2520process%2520hour-long%2520videos%2520exceeding%252010%252C000%2520frames.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520TimeViper%2520competes%2520with%2520state-of-the-art%2520models%2520while%2520extending%2520frame%2520numbers.%2520We%2520further%2520analyze%2520attention%2520behaviors%2520of%2520both%2520Mamba%2520and%2520Transformer%2520layers%252C%2520offering%2520new%2520insights%2520into%2520hybrid%2520model%2520interpretability.%2520This%2520work%2520represents%2520an%2520initial%2520step%2520towards%2520developing%252C%2520interpreting%252C%2520and%2520compressing%2520hybrid%2520Mamba-Transformer%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding&entry.906535625=Boshen%20Xu%20and%20Zihan%20Xiao%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Qin%20Jin&entry.1292438233=We%20introduce%20TimeViper%2C%20a%20hybrid%20vision-language%20model%20designed%20to%20tackle%20challenges%20of%20long%20video%20understanding.%20Processing%20long%20videos%20demands%20both%20an%20efficient%20model%20architecture%20and%20an%20effective%20mechanism%20for%20handling%20extended%20temporal%20contexts.%20To%20this%20end%2C%20TimeViper%20adopts%20a%20hybrid%20Mamba-Transformer%20backbone%20that%20combines%20the%20efficiency%20of%20state-space%20models%20with%20the%20expressivity%20of%20attention%20mechanisms.%20Through%20this%20hybrid%20design%2C%20we%20reveal%20the%20vision-to-text%20information%20aggregation%20phenomenon%2C%20where%20information%20progressively%20flows%20from%20vision%20tokens%20to%20text%20tokens%20across%20increasing%20LLM%20depth%2C%20resulting%20in%20severe%20vision%20token%20redundancy.%20Motivated%20by%20this%20observation%2C%20we%20propose%20TransV%2C%20a%20token%20information%20transfer%20module%20that%20transfers%20and%20compresses%20vision%20tokens%20into%20instruction%20tokens%20while%20maintaining%20multimodal%20understanding%20capabilities.%20This%20design%20enables%20TimeViper%20to%20process%20hour-long%20videos%20exceeding%2010%2C000%20frames.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20TimeViper%20competes%20with%20state-of-the-art%20models%20while%20extending%20frame%20numbers.%20We%20further%20analyze%20attention%20behaviors%20of%20both%20Mamba%20and%20Transformer%20layers%2C%20offering%20new%20insights%20into%20hybrid%20model%20interpretability.%20This%20work%20represents%20an%20initial%20step%20towards%20developing%2C%20interpreting%2C%20and%20compressing%20hybrid%20Mamba-Transformer%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2511.16595v2&entry.124074799=Read"},
{"title": "DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures", "author": "Shengqi Dang and Fu Chai and Jiaxin Li and Chao Yuan and Wei Ye and Nan Cao", "abstract": "The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.", "link": "http://arxiv.org/abs/2511.09298v2", "date": "2025-11-26", "relevancy": 2.8598, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5767}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5767}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DensiCrafter%3A%20Physically-Constrained%20Generation%20and%20Fabrication%20of%20Self-Supporting%20Hollow%20Structures&body=Title%3A%20DensiCrafter%3A%20Physically-Constrained%20Generation%20and%20Fabrication%20of%20Self-Supporting%20Hollow%20Structures%0AAuthor%3A%20Shengqi%20Dang%20and%20Fu%20Chai%20and%20Jiaxin%20Li%20and%20Chao%20Yuan%20and%20Wei%20Ye%20and%20Nan%20Cao%0AAbstract%3A%20The%20rise%20of%203D%20generative%20models%20has%20enabled%20automatic%203D%20geometry%20and%20texture%20synthesis%20from%20multimodal%20inputs%20%28e.g.%2C%20text%20or%20images%29.%20However%2C%20these%20methods%20often%20ignore%20physical%20constraints%20and%20manufacturability%20considerations.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20producing%203D%20designs%20that%20are%20both%20lightweight%20and%20self-supporting.%20We%20present%20DensiCrafter%2C%20a%20framework%20for%20generating%20lightweight%2C%20self-supporting%203D%20hollow%20structures%20by%20optimizing%20the%20density%20field.%20Starting%20from%20coarse%20voxel%20grids%20produced%20by%20Trellis%2C%20we%20interpret%20these%20as%20continuous%20density%20fields%20to%20optimize%20and%20introduce%20three%20differentiable%2C%20physically%20constrained%2C%20and%20simulation-free%20loss%20terms.%20Additionally%2C%20a%20mass%20regularization%20penalizes%20unnecessary%20material%2C%20while%20a%20restricted%20optimization%20domain%20preserves%20the%20outer%20surface.%20Our%20method%20seamlessly%20integrates%20with%20pretrained%20Trellis-based%20models%20%28e.g.%2C%20Trellis%2C%20DSO%29%20without%20any%20architectural%20changes.%20In%20extensive%20evaluations%2C%20we%20achieve%20up%20to%2043%25%20reduction%20in%20material%20mass%20on%20the%20text-to-3D%20task.%20Compared%20to%20state-of-the-art%20baselines%2C%20our%20method%20could%20improve%20the%20stability%20and%20maintain%20high%20geometric%20fidelity.%20Real-world%203D-printing%20experiments%20confirm%20that%20our%20hollow%20designs%20can%20be%20reliably%20fabricated%20and%20could%20be%20self-supporting.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDensiCrafter%253A%2520Physically-Constrained%2520Generation%2520and%2520Fabrication%2520of%2520Self-Supporting%2520Hollow%2520Structures%26entry.906535625%3DShengqi%2520Dang%2520and%2520Fu%2520Chai%2520and%2520Jiaxin%2520Li%2520and%2520Chao%2520Yuan%2520and%2520Wei%2520Ye%2520and%2520Nan%2520Cao%26entry.1292438233%3DThe%2520rise%2520of%25203D%2520generative%2520models%2520has%2520enabled%2520automatic%25203D%2520geometry%2520and%2520texture%2520synthesis%2520from%2520multimodal%2520inputs%2520%2528e.g.%252C%2520text%2520or%2520images%2529.%2520However%252C%2520these%2520methods%2520often%2520ignore%2520physical%2520constraints%2520and%2520manufacturability%2520considerations.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenge%2520of%2520producing%25203D%2520designs%2520that%2520are%2520both%2520lightweight%2520and%2520self-supporting.%2520We%2520present%2520DensiCrafter%252C%2520a%2520framework%2520for%2520generating%2520lightweight%252C%2520self-supporting%25203D%2520hollow%2520structures%2520by%2520optimizing%2520the%2520density%2520field.%2520Starting%2520from%2520coarse%2520voxel%2520grids%2520produced%2520by%2520Trellis%252C%2520we%2520interpret%2520these%2520as%2520continuous%2520density%2520fields%2520to%2520optimize%2520and%2520introduce%2520three%2520differentiable%252C%2520physically%2520constrained%252C%2520and%2520simulation-free%2520loss%2520terms.%2520Additionally%252C%2520a%2520mass%2520regularization%2520penalizes%2520unnecessary%2520material%252C%2520while%2520a%2520restricted%2520optimization%2520domain%2520preserves%2520the%2520outer%2520surface.%2520Our%2520method%2520seamlessly%2520integrates%2520with%2520pretrained%2520Trellis-based%2520models%2520%2528e.g.%252C%2520Trellis%252C%2520DSO%2529%2520without%2520any%2520architectural%2520changes.%2520In%2520extensive%2520evaluations%252C%2520we%2520achieve%2520up%2520to%252043%2525%2520reduction%2520in%2520material%2520mass%2520on%2520the%2520text-to-3D%2520task.%2520Compared%2520to%2520state-of-the-art%2520baselines%252C%2520our%2520method%2520could%2520improve%2520the%2520stability%2520and%2520maintain%2520high%2520geometric%2520fidelity.%2520Real-world%25203D-printing%2520experiments%2520confirm%2520that%2520our%2520hollow%2520designs%2520can%2520be%2520reliably%2520fabricated%2520and%2520could%2520be%2520self-supporting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DensiCrafter%3A%20Physically-Constrained%20Generation%20and%20Fabrication%20of%20Self-Supporting%20Hollow%20Structures&entry.906535625=Shengqi%20Dang%20and%20Fu%20Chai%20and%20Jiaxin%20Li%20and%20Chao%20Yuan%20and%20Wei%20Ye%20and%20Nan%20Cao&entry.1292438233=The%20rise%20of%203D%20generative%20models%20has%20enabled%20automatic%203D%20geometry%20and%20texture%20synthesis%20from%20multimodal%20inputs%20%28e.g.%2C%20text%20or%20images%29.%20However%2C%20these%20methods%20often%20ignore%20physical%20constraints%20and%20manufacturability%20considerations.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20producing%203D%20designs%20that%20are%20both%20lightweight%20and%20self-supporting.%20We%20present%20DensiCrafter%2C%20a%20framework%20for%20generating%20lightweight%2C%20self-supporting%203D%20hollow%20structures%20by%20optimizing%20the%20density%20field.%20Starting%20from%20coarse%20voxel%20grids%20produced%20by%20Trellis%2C%20we%20interpret%20these%20as%20continuous%20density%20fields%20to%20optimize%20and%20introduce%20three%20differentiable%2C%20physically%20constrained%2C%20and%20simulation-free%20loss%20terms.%20Additionally%2C%20a%20mass%20regularization%20penalizes%20unnecessary%20material%2C%20while%20a%20restricted%20optimization%20domain%20preserves%20the%20outer%20surface.%20Our%20method%20seamlessly%20integrates%20with%20pretrained%20Trellis-based%20models%20%28e.g.%2C%20Trellis%2C%20DSO%29%20without%20any%20architectural%20changes.%20In%20extensive%20evaluations%2C%20we%20achieve%20up%20to%2043%25%20reduction%20in%20material%20mass%20on%20the%20text-to-3D%20task.%20Compared%20to%20state-of-the-art%20baselines%2C%20our%20method%20could%20improve%20the%20stability%20and%20maintain%20high%20geometric%20fidelity.%20Real-world%203D-printing%20experiments%20confirm%20that%20our%20hollow%20designs%20can%20be%20reliably%20fabricated%20and%20could%20be%20self-supporting.&entry.1838667208=http%3A//arxiv.org/abs/2511.09298v2&entry.124074799=Read"},
{"title": "Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning", "author": "Liqin Luo and Guangyao Chen and Xiawu Zheng and Yongxing Dai and Yixiong Zou and Yonghong Tian", "abstract": "Visual grounding, the task of linking textual queries to specific regions within images, plays a pivotal role in vision-language integration. Existing methods typically rely on extensive task-specific annotations and fine-tuning, limiting their ability to generalize effectively to novel or out-of-distribution scenarios. To address these limitations, we introduce GroundingAgent, a novel agentic visual grounding framework that operates without any task-specific fine-tuning. GroundingAgent employs a structured, iterative reasoning mechanism that integrates pretrained open-vocabulary object detectors, multimodal large language models (MLLMs), and large language models (LLMs) to progressively refine candidate regions through joint semantic and spatial analyses. Remarkably, GroundingAgent achieves an average zero-shot grounding accuracy of 65.1 % on widely-used benchmarks (RefCOCO, RefCOCO+, RefCOCOg), entirely without fine-tuning. Furthermore, by substituting MLLM-generated captions with the original query texts, the accuracy at the selection stage alone reaches approximately 90 %, closely matching supervised performance and underscoring the critical role of LLM reasoning capabilities. GroundingAgent also offers strong interpretability, transparently illustrating each reasoning step and providing clear insights into its decision-making process.", "link": "http://arxiv.org/abs/2511.19516v2", "date": "2025-11-26", "relevancy": 2.8246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20the%20Dots%3A%20Training-Free%20Visual%20Grounding%20via%20Agentic%20Reasoning&body=Title%3A%20Connecting%20the%20Dots%3A%20Training-Free%20Visual%20Grounding%20via%20Agentic%20Reasoning%0AAuthor%3A%20Liqin%20Luo%20and%20Guangyao%20Chen%20and%20Xiawu%20Zheng%20and%20Yongxing%20Dai%20and%20Yixiong%20Zou%20and%20Yonghong%20Tian%0AAbstract%3A%20Visual%20grounding%2C%20the%20task%20of%20linking%20textual%20queries%20to%20specific%20regions%20within%20images%2C%20plays%20a%20pivotal%20role%20in%20vision-language%20integration.%20Existing%20methods%20typically%20rely%20on%20extensive%20task-specific%20annotations%20and%20fine-tuning%2C%20limiting%20their%20ability%20to%20generalize%20effectively%20to%20novel%20or%20out-of-distribution%20scenarios.%20To%20address%20these%20limitations%2C%20we%20introduce%20GroundingAgent%2C%20a%20novel%20agentic%20visual%20grounding%20framework%20that%20operates%20without%20any%20task-specific%20fine-tuning.%20GroundingAgent%20employs%20a%20structured%2C%20iterative%20reasoning%20mechanism%20that%20integrates%20pretrained%20open-vocabulary%20object%20detectors%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20and%20large%20language%20models%20%28LLMs%29%20to%20progressively%20refine%20candidate%20regions%20through%20joint%20semantic%20and%20spatial%20analyses.%20Remarkably%2C%20GroundingAgent%20achieves%20an%20average%20zero-shot%20grounding%20accuracy%20of%2065.1%20%25%20on%20widely-used%20benchmarks%20%28RefCOCO%2C%20RefCOCO%2B%2C%20RefCOCOg%29%2C%20entirely%20without%20fine-tuning.%20Furthermore%2C%20by%20substituting%20MLLM-generated%20captions%20with%20the%20original%20query%20texts%2C%20the%20accuracy%20at%20the%20selection%20stage%20alone%20reaches%20approximately%2090%20%25%2C%20closely%20matching%20supervised%20performance%20and%20underscoring%20the%20critical%20role%20of%20LLM%20reasoning%20capabilities.%20GroundingAgent%20also%20offers%20strong%20interpretability%2C%20transparently%20illustrating%20each%20reasoning%20step%20and%20providing%20clear%20insights%20into%20its%20decision-making%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520the%2520Dots%253A%2520Training-Free%2520Visual%2520Grounding%2520via%2520Agentic%2520Reasoning%26entry.906535625%3DLiqin%2520Luo%2520and%2520Guangyao%2520Chen%2520and%2520Xiawu%2520Zheng%2520and%2520Yongxing%2520Dai%2520and%2520Yixiong%2520Zou%2520and%2520Yonghong%2520Tian%26entry.1292438233%3DVisual%2520grounding%252C%2520the%2520task%2520of%2520linking%2520textual%2520queries%2520to%2520specific%2520regions%2520within%2520images%252C%2520plays%2520a%2520pivotal%2520role%2520in%2520vision-language%2520integration.%2520Existing%2520methods%2520typically%2520rely%2520on%2520extensive%2520task-specific%2520annotations%2520and%2520fine-tuning%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520effectively%2520to%2520novel%2520or%2520out-of-distribution%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520GroundingAgent%252C%2520a%2520novel%2520agentic%2520visual%2520grounding%2520framework%2520that%2520operates%2520without%2520any%2520task-specific%2520fine-tuning.%2520GroundingAgent%2520employs%2520a%2520structured%252C%2520iterative%2520reasoning%2520mechanism%2520that%2520integrates%2520pretrained%2520open-vocabulary%2520object%2520detectors%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520progressively%2520refine%2520candidate%2520regions%2520through%2520joint%2520semantic%2520and%2520spatial%2520analyses.%2520Remarkably%252C%2520GroundingAgent%2520achieves%2520an%2520average%2520zero-shot%2520grounding%2520accuracy%2520of%252065.1%2520%2525%2520on%2520widely-used%2520benchmarks%2520%2528RefCOCO%252C%2520RefCOCO%252B%252C%2520RefCOCOg%2529%252C%2520entirely%2520without%2520fine-tuning.%2520Furthermore%252C%2520by%2520substituting%2520MLLM-generated%2520captions%2520with%2520the%2520original%2520query%2520texts%252C%2520the%2520accuracy%2520at%2520the%2520selection%2520stage%2520alone%2520reaches%2520approximately%252090%2520%2525%252C%2520closely%2520matching%2520supervised%2520performance%2520and%2520underscoring%2520the%2520critical%2520role%2520of%2520LLM%2520reasoning%2520capabilities.%2520GroundingAgent%2520also%2520offers%2520strong%2520interpretability%252C%2520transparently%2520illustrating%2520each%2520reasoning%2520step%2520and%2520providing%2520clear%2520insights%2520into%2520its%2520decision-making%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20the%20Dots%3A%20Training-Free%20Visual%20Grounding%20via%20Agentic%20Reasoning&entry.906535625=Liqin%20Luo%20and%20Guangyao%20Chen%20and%20Xiawu%20Zheng%20and%20Yongxing%20Dai%20and%20Yixiong%20Zou%20and%20Yonghong%20Tian&entry.1292438233=Visual%20grounding%2C%20the%20task%20of%20linking%20textual%20queries%20to%20specific%20regions%20within%20images%2C%20plays%20a%20pivotal%20role%20in%20vision-language%20integration.%20Existing%20methods%20typically%20rely%20on%20extensive%20task-specific%20annotations%20and%20fine-tuning%2C%20limiting%20their%20ability%20to%20generalize%20effectively%20to%20novel%20or%20out-of-distribution%20scenarios.%20To%20address%20these%20limitations%2C%20we%20introduce%20GroundingAgent%2C%20a%20novel%20agentic%20visual%20grounding%20framework%20that%20operates%20without%20any%20task-specific%20fine-tuning.%20GroundingAgent%20employs%20a%20structured%2C%20iterative%20reasoning%20mechanism%20that%20integrates%20pretrained%20open-vocabulary%20object%20detectors%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20and%20large%20language%20models%20%28LLMs%29%20to%20progressively%20refine%20candidate%20regions%20through%20joint%20semantic%20and%20spatial%20analyses.%20Remarkably%2C%20GroundingAgent%20achieves%20an%20average%20zero-shot%20grounding%20accuracy%20of%2065.1%20%25%20on%20widely-used%20benchmarks%20%28RefCOCO%2C%20RefCOCO%2B%2C%20RefCOCOg%29%2C%20entirely%20without%20fine-tuning.%20Furthermore%2C%20by%20substituting%20MLLM-generated%20captions%20with%20the%20original%20query%20texts%2C%20the%20accuracy%20at%20the%20selection%20stage%20alone%20reaches%20approximately%2090%20%25%2C%20closely%20matching%20supervised%20performance%20and%20underscoring%20the%20critical%20role%20of%20LLM%20reasoning%20capabilities.%20GroundingAgent%20also%20offers%20strong%20interpretability%2C%20transparently%20illustrating%20each%20reasoning%20step%20and%20providing%20clear%20insights%20into%20its%20decision-making%20process.&entry.1838667208=http%3A//arxiv.org/abs/2511.19516v2&entry.124074799=Read"},
{"title": "Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer", "author": "Emma Collins and Myungseo wong and Kim Yun and Finn Kingston and Hana Satou", "abstract": "Despite progress in geometry-aware domain adaptation, current methods such as GAMA still suffer from two unresolved issues: (1) insufficient disentanglement of task-relevant and task-irrelevant manifold dimensions, and (2) rigid perturbation schemes that ignore per-class alignment asymmetries. To address this, we propose GAMA++, a novel framework that introduces (i) latent space disentanglement to isolate label-consistent manifold directions from nuisance factors, and (ii) an adaptive contrastive perturbation strategy that tailors both on- and off-manifold exploration to class-specific manifold curvature and alignment discrepancy. We further propose a cross-domain contrastive consistency loss that encourages local semantic clusters to align while preserving intra-domain diversity. Our method achieves state-of-the-art results on DomainNet, Office-Home, and VisDA benchmarks under both standard and few-shot settings, with notable improvements in class-level alignment fidelity and boundary robustness. GAMA++ sets a new standard for semantic geometry alignment in transfer learning.", "link": "http://arxiv.org/abs/2505.15241v2", "date": "2025-11-26", "relevancy": 2.8219, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.583}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Geometric%20Alignment%20with%20Adaptive%20Contrastive%20Perturbation%20for%20Reliable%20Domain%20Transfer&body=Title%3A%20Disentangled%20Geometric%20Alignment%20with%20Adaptive%20Contrastive%20Perturbation%20for%20Reliable%20Domain%20Transfer%0AAuthor%3A%20Emma%20Collins%20and%20Myungseo%20wong%20and%20Kim%20Yun%20and%20Finn%20Kingston%20and%20Hana%20Satou%0AAbstract%3A%20Despite%20progress%20in%20geometry-aware%20domain%20adaptation%2C%20current%20methods%20such%20as%20GAMA%20still%20suffer%20from%20two%20unresolved%20issues%3A%20%281%29%20insufficient%20disentanglement%20of%20task-relevant%20and%20task-irrelevant%20manifold%20dimensions%2C%20and%20%282%29%20rigid%20perturbation%20schemes%20that%20ignore%20per-class%20alignment%20asymmetries.%20To%20address%20this%2C%20we%20propose%20GAMA%2B%2B%2C%20a%20novel%20framework%20that%20introduces%20%28i%29%20latent%20space%20disentanglement%20to%20isolate%20label-consistent%20manifold%20directions%20from%20nuisance%20factors%2C%20and%20%28ii%29%20an%20adaptive%20contrastive%20perturbation%20strategy%20that%20tailors%20both%20on-%20and%20off-manifold%20exploration%20to%20class-specific%20manifold%20curvature%20and%20alignment%20discrepancy.%20We%20further%20propose%20a%20cross-domain%20contrastive%20consistency%20loss%20that%20encourages%20local%20semantic%20clusters%20to%20align%20while%20preserving%20intra-domain%20diversity.%20Our%20method%20achieves%20state-of-the-art%20results%20on%20DomainNet%2C%20Office-Home%2C%20and%20VisDA%20benchmarks%20under%20both%20standard%20and%20few-shot%20settings%2C%20with%20notable%20improvements%20in%20class-level%20alignment%20fidelity%20and%20boundary%20robustness.%20GAMA%2B%2B%20sets%20a%20new%20standard%20for%20semantic%20geometry%20alignment%20in%20transfer%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Geometric%2520Alignment%2520with%2520Adaptive%2520Contrastive%2520Perturbation%2520for%2520Reliable%2520Domain%2520Transfer%26entry.906535625%3DEmma%2520Collins%2520and%2520Myungseo%2520wong%2520and%2520Kim%2520Yun%2520and%2520Finn%2520Kingston%2520and%2520Hana%2520Satou%26entry.1292438233%3DDespite%2520progress%2520in%2520geometry-aware%2520domain%2520adaptation%252C%2520current%2520methods%2520such%2520as%2520GAMA%2520still%2520suffer%2520from%2520two%2520unresolved%2520issues%253A%2520%25281%2529%2520insufficient%2520disentanglement%2520of%2520task-relevant%2520and%2520task-irrelevant%2520manifold%2520dimensions%252C%2520and%2520%25282%2529%2520rigid%2520perturbation%2520schemes%2520that%2520ignore%2520per-class%2520alignment%2520asymmetries.%2520To%2520address%2520this%252C%2520we%2520propose%2520GAMA%252B%252B%252C%2520a%2520novel%2520framework%2520that%2520introduces%2520%2528i%2529%2520latent%2520space%2520disentanglement%2520to%2520isolate%2520label-consistent%2520manifold%2520directions%2520from%2520nuisance%2520factors%252C%2520and%2520%2528ii%2529%2520an%2520adaptive%2520contrastive%2520perturbation%2520strategy%2520that%2520tailors%2520both%2520on-%2520and%2520off-manifold%2520exploration%2520to%2520class-specific%2520manifold%2520curvature%2520and%2520alignment%2520discrepancy.%2520We%2520further%2520propose%2520a%2520cross-domain%2520contrastive%2520consistency%2520loss%2520that%2520encourages%2520local%2520semantic%2520clusters%2520to%2520align%2520while%2520preserving%2520intra-domain%2520diversity.%2520Our%2520method%2520achieves%2520state-of-the-art%2520results%2520on%2520DomainNet%252C%2520Office-Home%252C%2520and%2520VisDA%2520benchmarks%2520under%2520both%2520standard%2520and%2520few-shot%2520settings%252C%2520with%2520notable%2520improvements%2520in%2520class-level%2520alignment%2520fidelity%2520and%2520boundary%2520robustness.%2520GAMA%252B%252B%2520sets%2520a%2520new%2520standard%2520for%2520semantic%2520geometry%2520alignment%2520in%2520transfer%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Geometric%20Alignment%20with%20Adaptive%20Contrastive%20Perturbation%20for%20Reliable%20Domain%20Transfer&entry.906535625=Emma%20Collins%20and%20Myungseo%20wong%20and%20Kim%20Yun%20and%20Finn%20Kingston%20and%20Hana%20Satou&entry.1292438233=Despite%20progress%20in%20geometry-aware%20domain%20adaptation%2C%20current%20methods%20such%20as%20GAMA%20still%20suffer%20from%20two%20unresolved%20issues%3A%20%281%29%20insufficient%20disentanglement%20of%20task-relevant%20and%20task-irrelevant%20manifold%20dimensions%2C%20and%20%282%29%20rigid%20perturbation%20schemes%20that%20ignore%20per-class%20alignment%20asymmetries.%20To%20address%20this%2C%20we%20propose%20GAMA%2B%2B%2C%20a%20novel%20framework%20that%20introduces%20%28i%29%20latent%20space%20disentanglement%20to%20isolate%20label-consistent%20manifold%20directions%20from%20nuisance%20factors%2C%20and%20%28ii%29%20an%20adaptive%20contrastive%20perturbation%20strategy%20that%20tailors%20both%20on-%20and%20off-manifold%20exploration%20to%20class-specific%20manifold%20curvature%20and%20alignment%20discrepancy.%20We%20further%20propose%20a%20cross-domain%20contrastive%20consistency%20loss%20that%20encourages%20local%20semantic%20clusters%20to%20align%20while%20preserving%20intra-domain%20diversity.%20Our%20method%20achieves%20state-of-the-art%20results%20on%20DomainNet%2C%20Office-Home%2C%20and%20VisDA%20benchmarks%20under%20both%20standard%20and%20few-shot%20settings%2C%20with%20notable%20improvements%20in%20class-level%20alignment%20fidelity%20and%20boundary%20robustness.%20GAMA%2B%2B%20sets%20a%20new%20standard%20for%20semantic%20geometry%20alignment%20in%20transfer%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2505.15241v2&entry.124074799=Read"},
{"title": "ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation", "author": "Sanghyun Jo and Wooyeol Lee and Ziseok Lee and Kyungsu Kim", "abstract": "Text-to-image diffusion models have recently become highly capable, yet their behavior in multi-object scenes remains unreliable: models often produce an incorrect number of instances and exhibit semantics leaking across objects. We trace these failures to vague instance boundaries; self-attention already reveals instance layouts early in the denoising process, but existing approaches act only on semantic signals. We introduce $\\textbf{ISAC}$ ($\\textbf{I}$nstance-to-$\\textbf{S}$emantic $\\textbf{A}$ttention $\\textbf{C}$ontrol), a training-free, model-agnostic objective that performs hierarchical attention control by first carving out instance layouts from self-attention and then binding semantics to these instances. In Phase 1, ISAC clusters self-attention into the number of instances and repels overlaps, establishing an instance-level structural hierarchy; in Phase 2, it injects these instance cues into cross-attention to obtain instance-aware semantic masks and decomposes mixing semantics by tying attributes within each instance. ISAC yields consistent gains on T2I-CompBench, HRS-Bench, and IntraCompBench, our new benchmark for intra-class compositions where failures are most frequent, with improvements of at least 50% in multi-class accuracy and 7% in multi-instance accuracy on IntraCompBench, without any fine-tuning or external models. Beyond text-to-image setups, ISAC also strengthens layout-to-image controllers under overlapping boxes by refining coarse box layouts into dense instance masks, indicating that hierarchical decoupling of instance formation and semantic assignment is a key principle for robust, controllable multi-object generation. Code will be released upon publication.", "link": "http://arxiv.org/abs/2505.20935v2", "date": "2025-11-26", "relevancy": 2.787, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ISAC%3A%20Training-Free%20Instance-to-Semantic%20Attention%20Control%20for%20Improving%20Multi-Instance%20Generation&body=Title%3A%20ISAC%3A%20Training-Free%20Instance-to-Semantic%20Attention%20Control%20for%20Improving%20Multi-Instance%20Generation%0AAuthor%3A%20Sanghyun%20Jo%20and%20Wooyeol%20Lee%20and%20Ziseok%20Lee%20and%20Kyungsu%20Kim%0AAbstract%3A%20Text-to-image%20diffusion%20models%20have%20recently%20become%20highly%20capable%2C%20yet%20their%20behavior%20in%20multi-object%20scenes%20remains%20unreliable%3A%20models%20often%20produce%20an%20incorrect%20number%20of%20instances%20and%20exhibit%20semantics%20leaking%20across%20objects.%20We%20trace%20these%20failures%20to%20vague%20instance%20boundaries%3B%20self-attention%20already%20reveals%20instance%20layouts%20early%20in%20the%20denoising%20process%2C%20but%20existing%20approaches%20act%20only%20on%20semantic%20signals.%20We%20introduce%20%24%5Ctextbf%7BISAC%7D%24%20%28%24%5Ctextbf%7BI%7D%24nstance-to-%24%5Ctextbf%7BS%7D%24emantic%20%24%5Ctextbf%7BA%7D%24ttention%20%24%5Ctextbf%7BC%7D%24ontrol%29%2C%20a%20training-free%2C%20model-agnostic%20objective%20that%20performs%20hierarchical%20attention%20control%20by%20first%20carving%20out%20instance%20layouts%20from%20self-attention%20and%20then%20binding%20semantics%20to%20these%20instances.%20In%20Phase%201%2C%20ISAC%20clusters%20self-attention%20into%20the%20number%20of%20instances%20and%20repels%20overlaps%2C%20establishing%20an%20instance-level%20structural%20hierarchy%3B%20in%20Phase%202%2C%20it%20injects%20these%20instance%20cues%20into%20cross-attention%20to%20obtain%20instance-aware%20semantic%20masks%20and%20decomposes%20mixing%20semantics%20by%20tying%20attributes%20within%20each%20instance.%20ISAC%20yields%20consistent%20gains%20on%20T2I-CompBench%2C%20HRS-Bench%2C%20and%20IntraCompBench%2C%20our%20new%20benchmark%20for%20intra-class%20compositions%20where%20failures%20are%20most%20frequent%2C%20with%20improvements%20of%20at%20least%2050%25%20in%20multi-class%20accuracy%20and%207%25%20in%20multi-instance%20accuracy%20on%20IntraCompBench%2C%20without%20any%20fine-tuning%20or%20external%20models.%20Beyond%20text-to-image%20setups%2C%20ISAC%20also%20strengthens%20layout-to-image%20controllers%20under%20overlapping%20boxes%20by%20refining%20coarse%20box%20layouts%20into%20dense%20instance%20masks%2C%20indicating%20that%20hierarchical%20decoupling%20of%20instance%20formation%20and%20semantic%20assignment%20is%20a%20key%20principle%20for%20robust%2C%20controllable%20multi-object%20generation.%20Code%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DISAC%253A%2520Training-Free%2520Instance-to-Semantic%2520Attention%2520Control%2520for%2520Improving%2520Multi-Instance%2520Generation%26entry.906535625%3DSanghyun%2520Jo%2520and%2520Wooyeol%2520Lee%2520and%2520Ziseok%2520Lee%2520and%2520Kyungsu%2520Kim%26entry.1292438233%3DText-to-image%2520diffusion%2520models%2520have%2520recently%2520become%2520highly%2520capable%252C%2520yet%2520their%2520behavior%2520in%2520multi-object%2520scenes%2520remains%2520unreliable%253A%2520models%2520often%2520produce%2520an%2520incorrect%2520number%2520of%2520instances%2520and%2520exhibit%2520semantics%2520leaking%2520across%2520objects.%2520We%2520trace%2520these%2520failures%2520to%2520vague%2520instance%2520boundaries%253B%2520self-attention%2520already%2520reveals%2520instance%2520layouts%2520early%2520in%2520the%2520denoising%2520process%252C%2520but%2520existing%2520approaches%2520act%2520only%2520on%2520semantic%2520signals.%2520We%2520introduce%2520%2524%255Ctextbf%257BISAC%257D%2524%2520%2528%2524%255Ctextbf%257BI%257D%2524nstance-to-%2524%255Ctextbf%257BS%257D%2524emantic%2520%2524%255Ctextbf%257BA%257D%2524ttention%2520%2524%255Ctextbf%257BC%257D%2524ontrol%2529%252C%2520a%2520training-free%252C%2520model-agnostic%2520objective%2520that%2520performs%2520hierarchical%2520attention%2520control%2520by%2520first%2520carving%2520out%2520instance%2520layouts%2520from%2520self-attention%2520and%2520then%2520binding%2520semantics%2520to%2520these%2520instances.%2520In%2520Phase%25201%252C%2520ISAC%2520clusters%2520self-attention%2520into%2520the%2520number%2520of%2520instances%2520and%2520repels%2520overlaps%252C%2520establishing%2520an%2520instance-level%2520structural%2520hierarchy%253B%2520in%2520Phase%25202%252C%2520it%2520injects%2520these%2520instance%2520cues%2520into%2520cross-attention%2520to%2520obtain%2520instance-aware%2520semantic%2520masks%2520and%2520decomposes%2520mixing%2520semantics%2520by%2520tying%2520attributes%2520within%2520each%2520instance.%2520ISAC%2520yields%2520consistent%2520gains%2520on%2520T2I-CompBench%252C%2520HRS-Bench%252C%2520and%2520IntraCompBench%252C%2520our%2520new%2520benchmark%2520for%2520intra-class%2520compositions%2520where%2520failures%2520are%2520most%2520frequent%252C%2520with%2520improvements%2520of%2520at%2520least%252050%2525%2520in%2520multi-class%2520accuracy%2520and%25207%2525%2520in%2520multi-instance%2520accuracy%2520on%2520IntraCompBench%252C%2520without%2520any%2520fine-tuning%2520or%2520external%2520models.%2520Beyond%2520text-to-image%2520setups%252C%2520ISAC%2520also%2520strengthens%2520layout-to-image%2520controllers%2520under%2520overlapping%2520boxes%2520by%2520refining%2520coarse%2520box%2520layouts%2520into%2520dense%2520instance%2520masks%252C%2520indicating%2520that%2520hierarchical%2520decoupling%2520of%2520instance%2520formation%2520and%2520semantic%2520assignment%2520is%2520a%2520key%2520principle%2520for%2520robust%252C%2520controllable%2520multi-object%2520generation.%2520Code%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ISAC%3A%20Training-Free%20Instance-to-Semantic%20Attention%20Control%20for%20Improving%20Multi-Instance%20Generation&entry.906535625=Sanghyun%20Jo%20and%20Wooyeol%20Lee%20and%20Ziseok%20Lee%20and%20Kyungsu%20Kim&entry.1292438233=Text-to-image%20diffusion%20models%20have%20recently%20become%20highly%20capable%2C%20yet%20their%20behavior%20in%20multi-object%20scenes%20remains%20unreliable%3A%20models%20often%20produce%20an%20incorrect%20number%20of%20instances%20and%20exhibit%20semantics%20leaking%20across%20objects.%20We%20trace%20these%20failures%20to%20vague%20instance%20boundaries%3B%20self-attention%20already%20reveals%20instance%20layouts%20early%20in%20the%20denoising%20process%2C%20but%20existing%20approaches%20act%20only%20on%20semantic%20signals.%20We%20introduce%20%24%5Ctextbf%7BISAC%7D%24%20%28%24%5Ctextbf%7BI%7D%24nstance-to-%24%5Ctextbf%7BS%7D%24emantic%20%24%5Ctextbf%7BA%7D%24ttention%20%24%5Ctextbf%7BC%7D%24ontrol%29%2C%20a%20training-free%2C%20model-agnostic%20objective%20that%20performs%20hierarchical%20attention%20control%20by%20first%20carving%20out%20instance%20layouts%20from%20self-attention%20and%20then%20binding%20semantics%20to%20these%20instances.%20In%20Phase%201%2C%20ISAC%20clusters%20self-attention%20into%20the%20number%20of%20instances%20and%20repels%20overlaps%2C%20establishing%20an%20instance-level%20structural%20hierarchy%3B%20in%20Phase%202%2C%20it%20injects%20these%20instance%20cues%20into%20cross-attention%20to%20obtain%20instance-aware%20semantic%20masks%20and%20decomposes%20mixing%20semantics%20by%20tying%20attributes%20within%20each%20instance.%20ISAC%20yields%20consistent%20gains%20on%20T2I-CompBench%2C%20HRS-Bench%2C%20and%20IntraCompBench%2C%20our%20new%20benchmark%20for%20intra-class%20compositions%20where%20failures%20are%20most%20frequent%2C%20with%20improvements%20of%20at%20least%2050%25%20in%20multi-class%20accuracy%20and%207%25%20in%20multi-instance%20accuracy%20on%20IntraCompBench%2C%20without%20any%20fine-tuning%20or%20external%20models.%20Beyond%20text-to-image%20setups%2C%20ISAC%20also%20strengthens%20layout-to-image%20controllers%20under%20overlapping%20boxes%20by%20refining%20coarse%20box%20layouts%20into%20dense%20instance%20masks%2C%20indicating%20that%20hierarchical%20decoupling%20of%20instance%20formation%20and%20semantic%20assignment%20is%20a%20key%20principle%20for%20robust%2C%20controllable%20multi-object%20generation.%20Code%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2505.20935v2&entry.124074799=Read"},
{"title": "Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering", "author": "Qing Li and Huifang Feng and Xun Gong and Yu-Shen Liu", "abstract": "Estimating normals for noisy point clouds is a persistent challenge in 3D geometry processing, particularly for end-to-end oriented normal estimation. Existing methods generally address relatively clean data and rely on supervised priors to fit local surfaces within specific neighborhoods. In this paper, we propose a novel approach for learning normals from noisy point clouds through local gradient-aware surface filtering. Our method projects noisy points onto the underlying surface by utilizing normals and distances derived from an implicit function constrained by local gradients. We start by introducing a distance measurement operator for global surface fitting on noisy data, which integrates projected distances along normals. Following this, we develop an implicit field-based filtering approach for surface point construction, adding projection constraints on these points during filtering. To address issues of over-smoothing and gradient degradation, we further incorporate local gradient consistency constraints, as well as local gradient orientation and aggregation. Comprehensive experiments on normal estimation, surface reconstruction, and point cloud denoising demonstrate the state-of-the-art performance of our method. The source code and trained models are available at https://github.com/LeoQLi/LGSF.", "link": "http://arxiv.org/abs/2507.03394v2", "date": "2025-11-26", "relevancy": 2.7666, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.562}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5582}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Normals%20of%20Noisy%20Points%20by%20Local%20Gradient-Aware%20Surface%20Filtering&body=Title%3A%20Learning%20Normals%20of%20Noisy%20Points%20by%20Local%20Gradient-Aware%20Surface%20Filtering%0AAuthor%3A%20Qing%20Li%20and%20Huifang%20Feng%20and%20Xun%20Gong%20and%20Yu-Shen%20Liu%0AAbstract%3A%20Estimating%20normals%20for%20noisy%20point%20clouds%20is%20a%20persistent%20challenge%20in%203D%20geometry%20processing%2C%20particularly%20for%20end-to-end%20oriented%20normal%20estimation.%20Existing%20methods%20generally%20address%20relatively%20clean%20data%20and%20rely%20on%20supervised%20priors%20to%20fit%20local%20surfaces%20within%20specific%20neighborhoods.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20learning%20normals%20from%20noisy%20point%20clouds%20through%20local%20gradient-aware%20surface%20filtering.%20Our%20method%20projects%20noisy%20points%20onto%20the%20underlying%20surface%20by%20utilizing%20normals%20and%20distances%20derived%20from%20an%20implicit%20function%20constrained%20by%20local%20gradients.%20We%20start%20by%20introducing%20a%20distance%20measurement%20operator%20for%20global%20surface%20fitting%20on%20noisy%20data%2C%20which%20integrates%20projected%20distances%20along%20normals.%20Following%20this%2C%20we%20develop%20an%20implicit%20field-based%20filtering%20approach%20for%20surface%20point%20construction%2C%20adding%20projection%20constraints%20on%20these%20points%20during%20filtering.%20To%20address%20issues%20of%20over-smoothing%20and%20gradient%20degradation%2C%20we%20further%20incorporate%20local%20gradient%20consistency%20constraints%2C%20as%20well%20as%20local%20gradient%20orientation%20and%20aggregation.%20Comprehensive%20experiments%20on%20normal%20estimation%2C%20surface%20reconstruction%2C%20and%20point%20cloud%20denoising%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%20method.%20The%20source%20code%20and%20trained%20models%20are%20available%20at%20https%3A//github.com/LeoQLi/LGSF.%0ALink%3A%20http%3A//arxiv.org/abs/2507.03394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Normals%2520of%2520Noisy%2520Points%2520by%2520Local%2520Gradient-Aware%2520Surface%2520Filtering%26entry.906535625%3DQing%2520Li%2520and%2520Huifang%2520Feng%2520and%2520Xun%2520Gong%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3DEstimating%2520normals%2520for%2520noisy%2520point%2520clouds%2520is%2520a%2520persistent%2520challenge%2520in%25203D%2520geometry%2520processing%252C%2520particularly%2520for%2520end-to-end%2520oriented%2520normal%2520estimation.%2520Existing%2520methods%2520generally%2520address%2520relatively%2520clean%2520data%2520and%2520rely%2520on%2520supervised%2520priors%2520to%2520fit%2520local%2520surfaces%2520within%2520specific%2520neighborhoods.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520learning%2520normals%2520from%2520noisy%2520point%2520clouds%2520through%2520local%2520gradient-aware%2520surface%2520filtering.%2520Our%2520method%2520projects%2520noisy%2520points%2520onto%2520the%2520underlying%2520surface%2520by%2520utilizing%2520normals%2520and%2520distances%2520derived%2520from%2520an%2520implicit%2520function%2520constrained%2520by%2520local%2520gradients.%2520We%2520start%2520by%2520introducing%2520a%2520distance%2520measurement%2520operator%2520for%2520global%2520surface%2520fitting%2520on%2520noisy%2520data%252C%2520which%2520integrates%2520projected%2520distances%2520along%2520normals.%2520Following%2520this%252C%2520we%2520develop%2520an%2520implicit%2520field-based%2520filtering%2520approach%2520for%2520surface%2520point%2520construction%252C%2520adding%2520projection%2520constraints%2520on%2520these%2520points%2520during%2520filtering.%2520To%2520address%2520issues%2520of%2520over-smoothing%2520and%2520gradient%2520degradation%252C%2520we%2520further%2520incorporate%2520local%2520gradient%2520consistency%2520constraints%252C%2520as%2520well%2520as%2520local%2520gradient%2520orientation%2520and%2520aggregation.%2520Comprehensive%2520experiments%2520on%2520normal%2520estimation%252C%2520surface%2520reconstruction%252C%2520and%2520point%2520cloud%2520denoising%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520our%2520method.%2520The%2520source%2520code%2520and%2520trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/LeoQLi/LGSF.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Normals%20of%20Noisy%20Points%20by%20Local%20Gradient-Aware%20Surface%20Filtering&entry.906535625=Qing%20Li%20and%20Huifang%20Feng%20and%20Xun%20Gong%20and%20Yu-Shen%20Liu&entry.1292438233=Estimating%20normals%20for%20noisy%20point%20clouds%20is%20a%20persistent%20challenge%20in%203D%20geometry%20processing%2C%20particularly%20for%20end-to-end%20oriented%20normal%20estimation.%20Existing%20methods%20generally%20address%20relatively%20clean%20data%20and%20rely%20on%20supervised%20priors%20to%20fit%20local%20surfaces%20within%20specific%20neighborhoods.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20learning%20normals%20from%20noisy%20point%20clouds%20through%20local%20gradient-aware%20surface%20filtering.%20Our%20method%20projects%20noisy%20points%20onto%20the%20underlying%20surface%20by%20utilizing%20normals%20and%20distances%20derived%20from%20an%20implicit%20function%20constrained%20by%20local%20gradients.%20We%20start%20by%20introducing%20a%20distance%20measurement%20operator%20for%20global%20surface%20fitting%20on%20noisy%20data%2C%20which%20integrates%20projected%20distances%20along%20normals.%20Following%20this%2C%20we%20develop%20an%20implicit%20field-based%20filtering%20approach%20for%20surface%20point%20construction%2C%20adding%20projection%20constraints%20on%20these%20points%20during%20filtering.%20To%20address%20issues%20of%20over-smoothing%20and%20gradient%20degradation%2C%20we%20further%20incorporate%20local%20gradient%20consistency%20constraints%2C%20as%20well%20as%20local%20gradient%20orientation%20and%20aggregation.%20Comprehensive%20experiments%20on%20normal%20estimation%2C%20surface%20reconstruction%2C%20and%20point%20cloud%20denoising%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%20method.%20The%20source%20code%20and%20trained%20models%20are%20available%20at%20https%3A//github.com/LeoQLi/LGSF.&entry.1838667208=http%3A//arxiv.org/abs/2507.03394v2&entry.124074799=Read"},
{"title": "HTTM: Head-wise Temporal Token Merging for Faster VGGT", "author": "Weitian Wang and Lukas Meiner and Rai Shubham and Cecilia De La Parra and Akash Kumar", "abstract": "The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.", "link": "http://arxiv.org/abs/2511.21317v1", "date": "2025-11-26", "relevancy": 2.7621, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HTTM%3A%20Head-wise%20Temporal%20Token%20Merging%20for%20Faster%20VGGT&body=Title%3A%20HTTM%3A%20Head-wise%20Temporal%20Token%20Merging%20for%20Faster%20VGGT%0AAuthor%3A%20Weitian%20Wang%20and%20Lukas%20Meiner%20and%20Rai%20Shubham%20and%20Cecilia%20De%20La%20Parra%20and%20Akash%20Kumar%0AAbstract%3A%20The%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20marks%20a%20significant%20leap%20forward%20in%203D%20scene%20reconstruction%2C%20as%20it%20is%20the%20first%20model%20that%20directly%20infers%20all%20key%203D%20attributes%20%28camera%20poses%2C%20depths%2C%20and%20dense%20geometry%29%20jointly%20in%20one%20pass.%20However%2C%20this%20joint%20inference%20mechanism%20requires%20global%20attention%20layers%20that%20perform%20all-to-all%20attention%20computation%20on%20tokens%20from%20all%20views.%20For%20reconstruction%20of%20large%20scenes%20with%20long-sequence%20inputs%2C%20this%20causes%20a%20significant%20latency%20bottleneck.%20In%20this%20paper%2C%20we%20propose%20head-wise%20temporal%20merging%20%28HTTM%29%2C%20a%20training-free%203D%20token%20merging%20method%20for%20accelerating%20VGGT.%20Existing%20merging%20techniques%20merge%20tokens%20uniformly%20across%20different%20attention%20heads%2C%20resulting%20in%20identical%20tokens%20in%20the%20layers%27%20output%2C%20which%20hinders%20the%20model%27s%20representational%20ability.%20HTTM%20tackles%20this%20problem%20by%20merging%20tokens%20in%20multi-head%20granularity%2C%20which%20preserves%20the%20uniqueness%20of%20feature%20tokens%20after%20head%20concatenation.%20Additionally%2C%20this%20enables%20HTTM%20to%20leverage%20the%20spatial%20locality%20and%20temporal%20correspondence%20observed%20at%20the%20head%20level%20to%20achieve%20higher%20merging%20ratios%20with%20lower%20merging%20costs%20compared%20to%20existing%20methods.%20Thus%2C%20HTTM%20achieves%20up%20to%207x%20acceleration%20with%20negligible%20performance%20drops%20in%20a%20GPU-based%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHTTM%253A%2520Head-wise%2520Temporal%2520Token%2520Merging%2520for%2520Faster%2520VGGT%26entry.906535625%3DWeitian%2520Wang%2520and%2520Lukas%2520Meiner%2520and%2520Rai%2520Shubham%2520and%2520Cecilia%2520De%2520La%2520Parra%2520and%2520Akash%2520Kumar%26entry.1292438233%3DThe%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528VGGT%2529%2520marks%2520a%2520significant%2520leap%2520forward%2520in%25203D%2520scene%2520reconstruction%252C%2520as%2520it%2520is%2520the%2520first%2520model%2520that%2520directly%2520infers%2520all%2520key%25203D%2520attributes%2520%2528camera%2520poses%252C%2520depths%252C%2520and%2520dense%2520geometry%2529%2520jointly%2520in%2520one%2520pass.%2520However%252C%2520this%2520joint%2520inference%2520mechanism%2520requires%2520global%2520attention%2520layers%2520that%2520perform%2520all-to-all%2520attention%2520computation%2520on%2520tokens%2520from%2520all%2520views.%2520For%2520reconstruction%2520of%2520large%2520scenes%2520with%2520long-sequence%2520inputs%252C%2520this%2520causes%2520a%2520significant%2520latency%2520bottleneck.%2520In%2520this%2520paper%252C%2520we%2520propose%2520head-wise%2520temporal%2520merging%2520%2528HTTM%2529%252C%2520a%2520training-free%25203D%2520token%2520merging%2520method%2520for%2520accelerating%2520VGGT.%2520Existing%2520merging%2520techniques%2520merge%2520tokens%2520uniformly%2520across%2520different%2520attention%2520heads%252C%2520resulting%2520in%2520identical%2520tokens%2520in%2520the%2520layers%2527%2520output%252C%2520which%2520hinders%2520the%2520model%2527s%2520representational%2520ability.%2520HTTM%2520tackles%2520this%2520problem%2520by%2520merging%2520tokens%2520in%2520multi-head%2520granularity%252C%2520which%2520preserves%2520the%2520uniqueness%2520of%2520feature%2520tokens%2520after%2520head%2520concatenation.%2520Additionally%252C%2520this%2520enables%2520HTTM%2520to%2520leverage%2520the%2520spatial%2520locality%2520and%2520temporal%2520correspondence%2520observed%2520at%2520the%2520head%2520level%2520to%2520achieve%2520higher%2520merging%2520ratios%2520with%2520lower%2520merging%2520costs%2520compared%2520to%2520existing%2520methods.%2520Thus%252C%2520HTTM%2520achieves%2520up%2520to%25207x%2520acceleration%2520with%2520negligible%2520performance%2520drops%2520in%2520a%2520GPU-based%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HTTM%3A%20Head-wise%20Temporal%20Token%20Merging%20for%20Faster%20VGGT&entry.906535625=Weitian%20Wang%20and%20Lukas%20Meiner%20and%20Rai%20Shubham%20and%20Cecilia%20De%20La%20Parra%20and%20Akash%20Kumar&entry.1292438233=The%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20marks%20a%20significant%20leap%20forward%20in%203D%20scene%20reconstruction%2C%20as%20it%20is%20the%20first%20model%20that%20directly%20infers%20all%20key%203D%20attributes%20%28camera%20poses%2C%20depths%2C%20and%20dense%20geometry%29%20jointly%20in%20one%20pass.%20However%2C%20this%20joint%20inference%20mechanism%20requires%20global%20attention%20layers%20that%20perform%20all-to-all%20attention%20computation%20on%20tokens%20from%20all%20views.%20For%20reconstruction%20of%20large%20scenes%20with%20long-sequence%20inputs%2C%20this%20causes%20a%20significant%20latency%20bottleneck.%20In%20this%20paper%2C%20we%20propose%20head-wise%20temporal%20merging%20%28HTTM%29%2C%20a%20training-free%203D%20token%20merging%20method%20for%20accelerating%20VGGT.%20Existing%20merging%20techniques%20merge%20tokens%20uniformly%20across%20different%20attention%20heads%2C%20resulting%20in%20identical%20tokens%20in%20the%20layers%27%20output%2C%20which%20hinders%20the%20model%27s%20representational%20ability.%20HTTM%20tackles%20this%20problem%20by%20merging%20tokens%20in%20multi-head%20granularity%2C%20which%20preserves%20the%20uniqueness%20of%20feature%20tokens%20after%20head%20concatenation.%20Additionally%2C%20this%20enables%20HTTM%20to%20leverage%20the%20spatial%20locality%20and%20temporal%20correspondence%20observed%20at%20the%20head%20level%20to%20achieve%20higher%20merging%20ratios%20with%20lower%20merging%20costs%20compared%20to%20existing%20methods.%20Thus%2C%20HTTM%20achieves%20up%20to%207x%20acceleration%20with%20negligible%20performance%20drops%20in%20a%20GPU-based%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2511.21317v1&entry.124074799=Read"},
{"title": "FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision", "author": "Chen Ling and Henglin Shi and Hedvig Kjellstr\u00f6m", "abstract": "Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.", "link": "http://arxiv.org/abs/2511.21245v1", "date": "2025-11-26", "relevancy": 2.7144, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision&body=Title%3A%20FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%0AAuthor%3A%20Chen%20Ling%20and%20Henglin%20Shi%20and%20Hedvig%20Kjellstr%C3%B6m%0AAbstract%3A%20Facial%20expressions%20convey%20the%20bulk%20of%20emotional%20information%20in%20human%20communication%2C%20yet%20existing%203D%20face%20reconstruction%20methods%20often%20miss%20subtle%20affective%20details%20due%20to%20reliance%20on%202D%20supervision%20and%20lack%20of%203D%20ground%20truth.%20We%20propose%20FIELDS%20%28Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%29%20to%20address%20these%20limitations%20by%20extending%20self-supervised%202D%20image%20consistency%20cues%20with%20direct%203D%20expression%20parameter%20supervision%20and%20an%20auxiliary%20emotion%20recognition%20branch.%20Our%20encoder%20is%20guided%20by%20authentic%20expression%20parameters%20from%20spontaneous%204D%20facial%20scans%2C%20while%20an%20intensity-aware%20emotion%20loss%20encourages%20the%203D%20expression%20parameters%20to%20capture%20genuine%20emotion%20content%20without%20exaggeration.%20This%20dual-supervision%20strategy%20bridges%20the%202D/3D%20domain%20gap%20and%20mitigates%20expression-intensity%20bias%2C%20yielding%20high-fidelity%203D%20reconstructions%20that%20preserve%20subtle%20emotional%20cues.%20From%20a%20single%20image%2C%20FIELDS%20produces%20emotion-rich%20face%20models%20with%20highly%20realistic%20expressions%2C%20significantly%20improving%20in-the-wild%20facial%20expression%20recognition%20performance%20without%20sacrificing%20naturalness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIELDS%253A%2520Face%2520reconstruction%2520with%2520accurate%2520Inference%2520of%2520Expression%2520using%2520Learning%2520with%2520Direct%2520Supervision%26entry.906535625%3DChen%2520Ling%2520and%2520Henglin%2520Shi%2520and%2520Hedvig%2520Kjellstr%25C3%25B6m%26entry.1292438233%3DFacial%2520expressions%2520convey%2520the%2520bulk%2520of%2520emotional%2520information%2520in%2520human%2520communication%252C%2520yet%2520existing%25203D%2520face%2520reconstruction%2520methods%2520often%2520miss%2520subtle%2520affective%2520details%2520due%2520to%2520reliance%2520on%25202D%2520supervision%2520and%2520lack%2520of%25203D%2520ground%2520truth.%2520We%2520propose%2520FIELDS%2520%2528Face%2520reconstruction%2520with%2520accurate%2520Inference%2520of%2520Expression%2520using%2520Learning%2520with%2520Direct%2520Supervision%2529%2520to%2520address%2520these%2520limitations%2520by%2520extending%2520self-supervised%25202D%2520image%2520consistency%2520cues%2520with%2520direct%25203D%2520expression%2520parameter%2520supervision%2520and%2520an%2520auxiliary%2520emotion%2520recognition%2520branch.%2520Our%2520encoder%2520is%2520guided%2520by%2520authentic%2520expression%2520parameters%2520from%2520spontaneous%25204D%2520facial%2520scans%252C%2520while%2520an%2520intensity-aware%2520emotion%2520loss%2520encourages%2520the%25203D%2520expression%2520parameters%2520to%2520capture%2520genuine%2520emotion%2520content%2520without%2520exaggeration.%2520This%2520dual-supervision%2520strategy%2520bridges%2520the%25202D/3D%2520domain%2520gap%2520and%2520mitigates%2520expression-intensity%2520bias%252C%2520yielding%2520high-fidelity%25203D%2520reconstructions%2520that%2520preserve%2520subtle%2520emotional%2520cues.%2520From%2520a%2520single%2520image%252C%2520FIELDS%2520produces%2520emotion-rich%2520face%2520models%2520with%2520highly%2520realistic%2520expressions%252C%2520significantly%2520improving%2520in-the-wild%2520facial%2520expression%2520recognition%2520performance%2520without%2520sacrificing%2520naturalness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision&entry.906535625=Chen%20Ling%20and%20Henglin%20Shi%20and%20Hedvig%20Kjellstr%C3%B6m&entry.1292438233=Facial%20expressions%20convey%20the%20bulk%20of%20emotional%20information%20in%20human%20communication%2C%20yet%20existing%203D%20face%20reconstruction%20methods%20often%20miss%20subtle%20affective%20details%20due%20to%20reliance%20on%202D%20supervision%20and%20lack%20of%203D%20ground%20truth.%20We%20propose%20FIELDS%20%28Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%29%20to%20address%20these%20limitations%20by%20extending%20self-supervised%202D%20image%20consistency%20cues%20with%20direct%203D%20expression%20parameter%20supervision%20and%20an%20auxiliary%20emotion%20recognition%20branch.%20Our%20encoder%20is%20guided%20by%20authentic%20expression%20parameters%20from%20spontaneous%204D%20facial%20scans%2C%20while%20an%20intensity-aware%20emotion%20loss%20encourages%20the%203D%20expression%20parameters%20to%20capture%20genuine%20emotion%20content%20without%20exaggeration.%20This%20dual-supervision%20strategy%20bridges%20the%202D/3D%20domain%20gap%20and%20mitigates%20expression-intensity%20bias%2C%20yielding%20high-fidelity%203D%20reconstructions%20that%20preserve%20subtle%20emotional%20cues.%20From%20a%20single%20image%2C%20FIELDS%20produces%20emotion-rich%20face%20models%20with%20highly%20realistic%20expressions%2C%20significantly%20improving%20in-the-wild%20facial%20expression%20recognition%20performance%20without%20sacrificing%20naturalness.&entry.1838667208=http%3A//arxiv.org/abs/2511.21245v1&entry.124074799=Read"},
{"title": "Adaptive Object Detection for Indoor Navigation Assistance: A Performance Evaluation of Real-Time Algorithms", "author": "Abhinav Pratap and Sushant Kumar and Suchinton Chakravarty", "abstract": "This study addresses the need for accurate and efficient object detection in assistive technologies for visually impaired individuals. We evaluate four real-time object detection algorithms YOLO, SSD, Faster R-CNN, and Mask R-CNN within the context of indoor navigation assistance. Using the Indoor Objects Detection dataset, we analyze detection accuracy, processing speed, and adaptability to indoor environments. Our findings highlight the trade-offs between precision and efficiency, offering insights into selecting optimal algorithms for realtime assistive navigation. This research advances adaptive machine learning applications, enhancing indoor navigation solutions for the visually impaired and promoting accessibility.", "link": "http://arxiv.org/abs/2501.18444v2", "date": "2025-11-26", "relevancy": 2.6858, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Object%20Detection%20for%20Indoor%20Navigation%20Assistance%3A%20A%20Performance%20Evaluation%20of%20Real-Time%20Algorithms&body=Title%3A%20Adaptive%20Object%20Detection%20for%20Indoor%20Navigation%20Assistance%3A%20A%20Performance%20Evaluation%20of%20Real-Time%20Algorithms%0AAuthor%3A%20Abhinav%20Pratap%20and%20Sushant%20Kumar%20and%20Suchinton%20Chakravarty%0AAbstract%3A%20This%20study%20addresses%20the%20need%20for%20accurate%20and%20efficient%20object%20detection%20in%20assistive%20technologies%20for%20visually%20impaired%20individuals.%20We%20evaluate%20four%20real-time%20object%20detection%20algorithms%20YOLO%2C%20SSD%2C%20Faster%20R-CNN%2C%20and%20Mask%20R-CNN%20within%20the%20context%20of%20indoor%20navigation%20assistance.%20Using%20the%20Indoor%20Objects%20Detection%20dataset%2C%20we%20analyze%20detection%20accuracy%2C%20processing%20speed%2C%20and%20adaptability%20to%20indoor%20environments.%20Our%20findings%20highlight%20the%20trade-offs%20between%20precision%20and%20efficiency%2C%20offering%20insights%20into%20selecting%20optimal%20algorithms%20for%20realtime%20assistive%20navigation.%20This%20research%20advances%20adaptive%20machine%20learning%20applications%2C%20enhancing%20indoor%20navigation%20solutions%20for%20the%20visually%20impaired%20and%20promoting%20accessibility.%0ALink%3A%20http%3A//arxiv.org/abs/2501.18444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Object%2520Detection%2520for%2520Indoor%2520Navigation%2520Assistance%253A%2520A%2520Performance%2520Evaluation%2520of%2520Real-Time%2520Algorithms%26entry.906535625%3DAbhinav%2520Pratap%2520and%2520Sushant%2520Kumar%2520and%2520Suchinton%2520Chakravarty%26entry.1292438233%3DThis%2520study%2520addresses%2520the%2520need%2520for%2520accurate%2520and%2520efficient%2520object%2520detection%2520in%2520assistive%2520technologies%2520for%2520visually%2520impaired%2520individuals.%2520We%2520evaluate%2520four%2520real-time%2520object%2520detection%2520algorithms%2520YOLO%252C%2520SSD%252C%2520Faster%2520R-CNN%252C%2520and%2520Mask%2520R-CNN%2520within%2520the%2520context%2520of%2520indoor%2520navigation%2520assistance.%2520Using%2520the%2520Indoor%2520Objects%2520Detection%2520dataset%252C%2520we%2520analyze%2520detection%2520accuracy%252C%2520processing%2520speed%252C%2520and%2520adaptability%2520to%2520indoor%2520environments.%2520Our%2520findings%2520highlight%2520the%2520trade-offs%2520between%2520precision%2520and%2520efficiency%252C%2520offering%2520insights%2520into%2520selecting%2520optimal%2520algorithms%2520for%2520realtime%2520assistive%2520navigation.%2520This%2520research%2520advances%2520adaptive%2520machine%2520learning%2520applications%252C%2520enhancing%2520indoor%2520navigation%2520solutions%2520for%2520the%2520visually%2520impaired%2520and%2520promoting%2520accessibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Object%20Detection%20for%20Indoor%20Navigation%20Assistance%3A%20A%20Performance%20Evaluation%20of%20Real-Time%20Algorithms&entry.906535625=Abhinav%20Pratap%20and%20Sushant%20Kumar%20and%20Suchinton%20Chakravarty&entry.1292438233=This%20study%20addresses%20the%20need%20for%20accurate%20and%20efficient%20object%20detection%20in%20assistive%20technologies%20for%20visually%20impaired%20individuals.%20We%20evaluate%20four%20real-time%20object%20detection%20algorithms%20YOLO%2C%20SSD%2C%20Faster%20R-CNN%2C%20and%20Mask%20R-CNN%20within%20the%20context%20of%20indoor%20navigation%20assistance.%20Using%20the%20Indoor%20Objects%20Detection%20dataset%2C%20we%20analyze%20detection%20accuracy%2C%20processing%20speed%2C%20and%20adaptability%20to%20indoor%20environments.%20Our%20findings%20highlight%20the%20trade-offs%20between%20precision%20and%20efficiency%2C%20offering%20insights%20into%20selecting%20optimal%20algorithms%20for%20realtime%20assistive%20navigation.%20This%20research%20advances%20adaptive%20machine%20learning%20applications%2C%20enhancing%20indoor%20navigation%20solutions%20for%20the%20visually%20impaired%20and%20promoting%20accessibility.&entry.1838667208=http%3A//arxiv.org/abs/2501.18444v2&entry.124074799=Read"},
{"title": "QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression", "author": "Lei Huang and Rui Zhang and Jiaming Guo and Yang Zhang and Di Huang and Shuyao Cheng and Pengwei Jin and Chongxiao Li and Zidong Du and Xing Hu and Qi Guo and Yunji Chen", "abstract": "Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.", "link": "http://arxiv.org/abs/2511.20099v2", "date": "2025-11-26", "relevancy": 2.6618, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QiMeng-CRUX%3A%20Narrowing%20the%20Gap%20between%20Natural%20Language%20and%20Verilog%20via%20Core%20Refined%20Understanding%20eXpression&body=Title%3A%20QiMeng-CRUX%3A%20Narrowing%20the%20Gap%20between%20Natural%20Language%20and%20Verilog%20via%20Core%20Refined%20Understanding%20eXpression%0AAuthor%3A%20Lei%20Huang%20and%20Rui%20Zhang%20and%20Jiaming%20Guo%20and%20Yang%20Zhang%20and%20Di%20Huang%20and%20Shuyao%20Cheng%20and%20Pengwei%20Jin%20and%20Chongxiao%20Li%20and%20Zidong%20Du%20and%20Xing%20Hu%20and%20Qi%20Guo%20and%20Yunji%20Chen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promising%20capabilities%20in%20hardware%20description%20language%20%28HDL%29%20generation.%20However%2C%20existing%20approaches%20often%20rely%20on%20free-form%20natural%20language%20descriptions%20that%20are%20often%20ambiguous%2C%20redundant%2C%20and%20unstructured%2C%20which%20poses%20significant%20challenges%20for%20downstream%20Verilog%20code%20generation.%20We%20treat%20hardware%20code%20generation%20as%20a%20complex%20transformation%20from%20an%20open-ended%20natural%20language%20space%20to%20a%20domain-specific%2C%20highly%20constrained%20target%20space.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Core%20Refined%20Understanding%20eXpression%20%28CRUX%29%2C%20a%20structured%20intermediate%20space%20that%20captures%20the%20essential%20semantics%20of%20user%20intent%20while%20organizing%20the%20expression%20for%20precise%20Verilog%20code%20generation.%20We%20further%20design%20a%20two-stage%20training%20framework%2C%20comprising%20Joint%20Expression%20Modeling%20and%20Dual-Space%20Optimization%2C%20to%20enhance%20the%20quality%20of%20both%20CRUX%20and%20Verilog%20code.%20Experiments%20across%20multiple%20Verilog%20generation%20benchmarks%20demonstrate%20that%20our%20model%2C%20CRUX-V%2C%20achieves%20state-of-the-art%20performance%20among%20general%20models%2C%20particularly%20under%20challenging%20design%20tasks.%20Furthermore%2C%20the%20CRUX%20space%20proves%20transferable%20and%20beneficial%20when%20used%20as%20input%20prompts%20for%20other%20code%20models%2C%20highlighting%20its%20effectiveness%20in%20narrowing%20the%20gap%20between%20free-form%20natural%20language%20descriptions%20and%20precise%20Verilog%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQiMeng-CRUX%253A%2520Narrowing%2520the%2520Gap%2520between%2520Natural%2520Language%2520and%2520Verilog%2520via%2520Core%2520Refined%2520Understanding%2520eXpression%26entry.906535625%3DLei%2520Huang%2520and%2520Rui%2520Zhang%2520and%2520Jiaming%2520Guo%2520and%2520Yang%2520Zhang%2520and%2520Di%2520Huang%2520and%2520Shuyao%2520Cheng%2520and%2520Pengwei%2520Jin%2520and%2520Chongxiao%2520Li%2520and%2520Zidong%2520Du%2520and%2520Xing%2520Hu%2520and%2520Qi%2520Guo%2520and%2520Yunji%2520Chen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promising%2520capabilities%2520in%2520hardware%2520description%2520language%2520%2528HDL%2529%2520generation.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%2520free-form%2520natural%2520language%2520descriptions%2520that%2520are%2520often%2520ambiguous%252C%2520redundant%252C%2520and%2520unstructured%252C%2520which%2520poses%2520significant%2520challenges%2520for%2520downstream%2520Verilog%2520code%2520generation.%2520We%2520treat%2520hardware%2520code%2520generation%2520as%2520a%2520complex%2520transformation%2520from%2520an%2520open-ended%2520natural%2520language%2520space%2520to%2520a%2520domain-specific%252C%2520highly%2520constrained%2520target%2520space.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Core%2520Refined%2520Understanding%2520eXpression%2520%2528CRUX%2529%252C%2520a%2520structured%2520intermediate%2520space%2520that%2520captures%2520the%2520essential%2520semantics%2520of%2520user%2520intent%2520while%2520organizing%2520the%2520expression%2520for%2520precise%2520Verilog%2520code%2520generation.%2520We%2520further%2520design%2520a%2520two-stage%2520training%2520framework%252C%2520comprising%2520Joint%2520Expression%2520Modeling%2520and%2520Dual-Space%2520Optimization%252C%2520to%2520enhance%2520the%2520quality%2520of%2520both%2520CRUX%2520and%2520Verilog%2520code.%2520Experiments%2520across%2520multiple%2520Verilog%2520generation%2520benchmarks%2520demonstrate%2520that%2520our%2520model%252C%2520CRUX-V%252C%2520achieves%2520state-of-the-art%2520performance%2520among%2520general%2520models%252C%2520particularly%2520under%2520challenging%2520design%2520tasks.%2520Furthermore%252C%2520the%2520CRUX%2520space%2520proves%2520transferable%2520and%2520beneficial%2520when%2520used%2520as%2520input%2520prompts%2520for%2520other%2520code%2520models%252C%2520highlighting%2520its%2520effectiveness%2520in%2520narrowing%2520the%2520gap%2520between%2520free-form%2520natural%2520language%2520descriptions%2520and%2520precise%2520Verilog%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QiMeng-CRUX%3A%20Narrowing%20the%20Gap%20between%20Natural%20Language%20and%20Verilog%20via%20Core%20Refined%20Understanding%20eXpression&entry.906535625=Lei%20Huang%20and%20Rui%20Zhang%20and%20Jiaming%20Guo%20and%20Yang%20Zhang%20and%20Di%20Huang%20and%20Shuyao%20Cheng%20and%20Pengwei%20Jin%20and%20Chongxiao%20Li%20and%20Zidong%20Du%20and%20Xing%20Hu%20and%20Qi%20Guo%20and%20Yunji%20Chen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20shown%20promising%20capabilities%20in%20hardware%20description%20language%20%28HDL%29%20generation.%20However%2C%20existing%20approaches%20often%20rely%20on%20free-form%20natural%20language%20descriptions%20that%20are%20often%20ambiguous%2C%20redundant%2C%20and%20unstructured%2C%20which%20poses%20significant%20challenges%20for%20downstream%20Verilog%20code%20generation.%20We%20treat%20hardware%20code%20generation%20as%20a%20complex%20transformation%20from%20an%20open-ended%20natural%20language%20space%20to%20a%20domain-specific%2C%20highly%20constrained%20target%20space.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Core%20Refined%20Understanding%20eXpression%20%28CRUX%29%2C%20a%20structured%20intermediate%20space%20that%20captures%20the%20essential%20semantics%20of%20user%20intent%20while%20organizing%20the%20expression%20for%20precise%20Verilog%20code%20generation.%20We%20further%20design%20a%20two-stage%20training%20framework%2C%20comprising%20Joint%20Expression%20Modeling%20and%20Dual-Space%20Optimization%2C%20to%20enhance%20the%20quality%20of%20both%20CRUX%20and%20Verilog%20code.%20Experiments%20across%20multiple%20Verilog%20generation%20benchmarks%20demonstrate%20that%20our%20model%2C%20CRUX-V%2C%20achieves%20state-of-the-art%20performance%20among%20general%20models%2C%20particularly%20under%20challenging%20design%20tasks.%20Furthermore%2C%20the%20CRUX%20space%20proves%20transferable%20and%20beneficial%20when%20used%20as%20input%20prompts%20for%20other%20code%20models%2C%20highlighting%20its%20effectiveness%20in%20narrowing%20the%20gap%20between%20free-form%20natural%20language%20descriptions%20and%20precise%20Verilog%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.20099v2&entry.124074799=Read"},
{"title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices", "author": "Shuai Zhang and Bao Tang and Siyuan Yu and Yueting Zhu and Jingfeng Yao and Ya Zou and Shanglin Yuan and Li Yu and Wenyu Liu and Xinggang Wang", "abstract": "Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.", "link": "http://arxiv.org/abs/2511.21475v1", "date": "2025-11-26", "relevancy": 2.6324, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6715}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6504}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MobileI2V%3A%20Fast%20and%20High-Resolution%20Image-to-Video%20on%20Mobile%20Devices&body=Title%3A%20MobileI2V%3A%20Fast%20and%20High-Resolution%20Image-to-Video%20on%20Mobile%20Devices%0AAuthor%3A%20Shuai%20Zhang%20and%20Bao%20Tang%20and%20Siyuan%20Yu%20and%20Yueting%20Zhu%20and%20Jingfeng%20Yao%20and%20Ya%20Zou%20and%20Shanglin%20Yuan%20and%20Li%20Yu%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20Recently%2C%20video%20generation%20has%20witnessed%20rapid%20advancements%2C%20drawing%20increasing%20attention%20to%20image-to-video%20%28I2V%29%20synthesis%20on%20mobile%20devices.%20However%2C%20the%20substantial%20computational%20complexity%20and%20slow%20generation%20speed%20of%20diffusion%20models%20pose%20significant%20challenges%20for%20real-time%2C%20high-resolution%20video%20generation%20on%20resource-constrained%20mobile%20devices.%20In%20this%20work%2C%20we%20propose%20MobileI2V%2C%20a%20270M%20lightweight%20diffusion%20model%20for%20real-time%20image-to-video%20generation%20on%20mobile%20devices.%20The%20core%20lies%20in%3A%20%281%29%20We%20analyzed%20the%20performance%20of%20linear%20attention%20modules%20and%20softmax%20attention%20modules%20on%20mobile%20devices%2C%20and%20proposed%20a%20linear%20hybrid%20architecture%20denoiser%20that%20balances%20generation%20efficiency%20and%20quality.%20%282%29%20We%20design%20a%20time-step%20distillation%20strategy%20that%20compresses%20the%20I2V%20sampling%20steps%20from%20more%20than%2020%20to%20only%20two%20without%20significant%20quality%20loss%2C%20resulting%20in%20a%2010-fold%20increase%20in%20generation%20speed.%20%283%29%20We%20apply%20mobile-specific%20attention%20optimizations%20that%20yield%20a%202-fold%20speed-up%20for%20attention%20operations%20during%20on-device%20inference.%20MobileI2V%20enables%2C%20for%20the%20first%20time%2C%20fast%20720p%20image-to-video%20generation%20on%20mobile%20devices%2C%20with%20quality%20comparable%20to%20existing%20models.%20Under%20one-step%20conditions%2C%20the%20generation%20speed%20of%20each%20frame%20of%20720p%20video%20is%20less%20than%20100%20ms.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/hustvl/MobileI2V.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobileI2V%253A%2520Fast%2520and%2520High-Resolution%2520Image-to-Video%2520on%2520Mobile%2520Devices%26entry.906535625%3DShuai%2520Zhang%2520and%2520Bao%2520Tang%2520and%2520Siyuan%2520Yu%2520and%2520Yueting%2520Zhu%2520and%2520Jingfeng%2520Yao%2520and%2520Ya%2520Zou%2520and%2520Shanglin%2520Yuan%2520and%2520Li%2520Yu%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DRecently%252C%2520video%2520generation%2520has%2520witnessed%2520rapid%2520advancements%252C%2520drawing%2520increasing%2520attention%2520to%2520image-to-video%2520%2528I2V%2529%2520synthesis%2520on%2520mobile%2520devices.%2520However%252C%2520the%2520substantial%2520computational%2520complexity%2520and%2520slow%2520generation%2520speed%2520of%2520diffusion%2520models%2520pose%2520significant%2520challenges%2520for%2520real-time%252C%2520high-resolution%2520video%2520generation%2520on%2520resource-constrained%2520mobile%2520devices.%2520In%2520this%2520work%252C%2520we%2520propose%2520MobileI2V%252C%2520a%2520270M%2520lightweight%2520diffusion%2520model%2520for%2520real-time%2520image-to-video%2520generation%2520on%2520mobile%2520devices.%2520The%2520core%2520lies%2520in%253A%2520%25281%2529%2520We%2520analyzed%2520the%2520performance%2520of%2520linear%2520attention%2520modules%2520and%2520softmax%2520attention%2520modules%2520on%2520mobile%2520devices%252C%2520and%2520proposed%2520a%2520linear%2520hybrid%2520architecture%2520denoiser%2520that%2520balances%2520generation%2520efficiency%2520and%2520quality.%2520%25282%2529%2520We%2520design%2520a%2520time-step%2520distillation%2520strategy%2520that%2520compresses%2520the%2520I2V%2520sampling%2520steps%2520from%2520more%2520than%252020%2520to%2520only%2520two%2520without%2520significant%2520quality%2520loss%252C%2520resulting%2520in%2520a%252010-fold%2520increase%2520in%2520generation%2520speed.%2520%25283%2529%2520We%2520apply%2520mobile-specific%2520attention%2520optimizations%2520that%2520yield%2520a%25202-fold%2520speed-up%2520for%2520attention%2520operations%2520during%2520on-device%2520inference.%2520MobileI2V%2520enables%252C%2520for%2520the%2520first%2520time%252C%2520fast%2520720p%2520image-to-video%2520generation%2520on%2520mobile%2520devices%252C%2520with%2520quality%2520comparable%2520to%2520existing%2520models.%2520Under%2520one-step%2520conditions%252C%2520the%2520generation%2520speed%2520of%2520each%2520frame%2520of%2520720p%2520video%2520is%2520less%2520than%2520100%2520ms.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/hustvl/MobileI2V.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MobileI2V%3A%20Fast%20and%20High-Resolution%20Image-to-Video%20on%20Mobile%20Devices&entry.906535625=Shuai%20Zhang%20and%20Bao%20Tang%20and%20Siyuan%20Yu%20and%20Yueting%20Zhu%20and%20Jingfeng%20Yao%20and%20Ya%20Zou%20and%20Shanglin%20Yuan%20and%20Li%20Yu%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=Recently%2C%20video%20generation%20has%20witnessed%20rapid%20advancements%2C%20drawing%20increasing%20attention%20to%20image-to-video%20%28I2V%29%20synthesis%20on%20mobile%20devices.%20However%2C%20the%20substantial%20computational%20complexity%20and%20slow%20generation%20speed%20of%20diffusion%20models%20pose%20significant%20challenges%20for%20real-time%2C%20high-resolution%20video%20generation%20on%20resource-constrained%20mobile%20devices.%20In%20this%20work%2C%20we%20propose%20MobileI2V%2C%20a%20270M%20lightweight%20diffusion%20model%20for%20real-time%20image-to-video%20generation%20on%20mobile%20devices.%20The%20core%20lies%20in%3A%20%281%29%20We%20analyzed%20the%20performance%20of%20linear%20attention%20modules%20and%20softmax%20attention%20modules%20on%20mobile%20devices%2C%20and%20proposed%20a%20linear%20hybrid%20architecture%20denoiser%20that%20balances%20generation%20efficiency%20and%20quality.%20%282%29%20We%20design%20a%20time-step%20distillation%20strategy%20that%20compresses%20the%20I2V%20sampling%20steps%20from%20more%20than%2020%20to%20only%20two%20without%20significant%20quality%20loss%2C%20resulting%20in%20a%2010-fold%20increase%20in%20generation%20speed.%20%283%29%20We%20apply%20mobile-specific%20attention%20optimizations%20that%20yield%20a%202-fold%20speed-up%20for%20attention%20operations%20during%20on-device%20inference.%20MobileI2V%20enables%2C%20for%20the%20first%20time%2C%20fast%20720p%20image-to-video%20generation%20on%20mobile%20devices%2C%20with%20quality%20comparable%20to%20existing%20models.%20Under%20one-step%20conditions%2C%20the%20generation%20speed%20of%20each%20frame%20of%20720p%20video%20is%20less%20than%20100%20ms.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/hustvl/MobileI2V.&entry.1838667208=http%3A//arxiv.org/abs/2511.21475v1&entry.124074799=Read"},
{"title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?", "author": "Pierre Adorni and Minh-Tan Pham and St\u00e9phane May and S\u00e9bastien Lef\u00e8vre", "abstract": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.", "link": "http://arxiv.org/abs/2511.21523v1", "date": "2025-11-26", "relevancy": 2.6293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F&body=Title%3A%20EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F%0AAuthor%3A%20Pierre%20Adorni%20and%20Minh-Tan%20Pham%20and%20St%C3%A9phane%20May%20and%20S%C3%A9bastien%20Lef%C3%A8vre%0AAbstract%3A%20Recent%20advances%20in%20foundation%20models%20have%20shown%20great%20promise%20in%20domains%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20and%20similar%20efforts%20are%20now%20emerging%20in%20the%20Earth%20Observation%20community.%20These%20models%20aim%20to%20generalize%20across%20tasks%20with%20limited%20supervision%2C%20reducing%20the%20need%20for%20training%20separate%20models%20for%20each%20task.%20However%2C%20current%20strategies%2C%20which%20largely%20focus%20on%20scaling%20model%20size%20and%20dataset%20volume%2C%20require%20prohibitive%20computational%20and%20data%20resources%2C%20limiting%20accessibility%20to%20only%20a%20few%20large%20institutions.%20Moreover%2C%20this%20paradigm%20of%20ever-larger%20models%20stands%20in%20stark%20contrast%20with%20the%20principles%20of%20sustainable%20and%20environmentally%20responsible%20AI%2C%20as%20it%20leads%20to%20immense%20carbon%20footprints%20and%20resource%20inefficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20and%20efficient%20alternative%3A%20an%20Ensemble-of-Specialists%20framework%20for%20building%20Remote%20Sensing%20Foundation%20Models%20%28RSFMs%29.%20Our%20method%20decomposes%20the%20training%20process%20into%20lightweight%2C%20task-specific%20ConvNeXtV2%20specialists%20that%20can%20be%20frozen%20and%20reused.%20This%20modular%20approach%20offers%20strong%20advantages%20in%20efficiency%2C%20interpretability%2C%20and%20extensibility.%20Moreover%2C%20it%20naturally%20supports%20federated%20training%2C%20pruning%2C%20and%20continuous%20specialist%20integration%2C%20making%20it%20particularly%20well-suited%20for%20collaborative%20and%20resource-constrained%20settings.%20Our%20framework%20sets%20a%20new%20direction%20for%20building%20scalable%20and%20efficient%20RSFMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEoS-FM%253A%2520Can%2520an%2520Ensemble%2520of%2520Specialist%2520Models%2520act%2520as%2520a%2520Generalist%2520Feature%2520Extractor%253F%26entry.906535625%3DPierre%2520Adorni%2520and%2520Minh-Tan%2520Pham%2520and%2520St%25C3%25A9phane%2520May%2520and%2520S%25C3%25A9bastien%2520Lef%25C3%25A8vre%26entry.1292438233%3DRecent%2520advances%2520in%2520foundation%2520models%2520have%2520shown%2520great%2520promise%2520in%2520domains%2520such%2520as%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%2520and%2520similar%2520efforts%2520are%2520now%2520emerging%2520in%2520the%2520Earth%2520Observation%2520community.%2520These%2520models%2520aim%2520to%2520generalize%2520across%2520tasks%2520with%2520limited%2520supervision%252C%2520reducing%2520the%2520need%2520for%2520training%2520separate%2520models%2520for%2520each%2520task.%2520However%252C%2520current%2520strategies%252C%2520which%2520largely%2520focus%2520on%2520scaling%2520model%2520size%2520and%2520dataset%2520volume%252C%2520require%2520prohibitive%2520computational%2520and%2520data%2520resources%252C%2520limiting%2520accessibility%2520to%2520only%2520a%2520few%2520large%2520institutions.%2520Moreover%252C%2520this%2520paradigm%2520of%2520ever-larger%2520models%2520stands%2520in%2520stark%2520contrast%2520with%2520the%2520principles%2520of%2520sustainable%2520and%2520environmentally%2520responsible%2520AI%252C%2520as%2520it%2520leads%2520to%2520immense%2520carbon%2520footprints%2520and%2520resource%2520inefficiency.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520and%2520efficient%2520alternative%253A%2520an%2520Ensemble-of-Specialists%2520framework%2520for%2520building%2520Remote%2520Sensing%2520Foundation%2520Models%2520%2528RSFMs%2529.%2520Our%2520method%2520decomposes%2520the%2520training%2520process%2520into%2520lightweight%252C%2520task-specific%2520ConvNeXtV2%2520specialists%2520that%2520can%2520be%2520frozen%2520and%2520reused.%2520This%2520modular%2520approach%2520offers%2520strong%2520advantages%2520in%2520efficiency%252C%2520interpretability%252C%2520and%2520extensibility.%2520Moreover%252C%2520it%2520naturally%2520supports%2520federated%2520training%252C%2520pruning%252C%2520and%2520continuous%2520specialist%2520integration%252C%2520making%2520it%2520particularly%2520well-suited%2520for%2520collaborative%2520and%2520resource-constrained%2520settings.%2520Our%2520framework%2520sets%2520a%2520new%2520direction%2520for%2520building%2520scalable%2520and%2520efficient%2520RSFMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F&entry.906535625=Pierre%20Adorni%20and%20Minh-Tan%20Pham%20and%20St%C3%A9phane%20May%20and%20S%C3%A9bastien%20Lef%C3%A8vre&entry.1292438233=Recent%20advances%20in%20foundation%20models%20have%20shown%20great%20promise%20in%20domains%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20and%20similar%20efforts%20are%20now%20emerging%20in%20the%20Earth%20Observation%20community.%20These%20models%20aim%20to%20generalize%20across%20tasks%20with%20limited%20supervision%2C%20reducing%20the%20need%20for%20training%20separate%20models%20for%20each%20task.%20However%2C%20current%20strategies%2C%20which%20largely%20focus%20on%20scaling%20model%20size%20and%20dataset%20volume%2C%20require%20prohibitive%20computational%20and%20data%20resources%2C%20limiting%20accessibility%20to%20only%20a%20few%20large%20institutions.%20Moreover%2C%20this%20paradigm%20of%20ever-larger%20models%20stands%20in%20stark%20contrast%20with%20the%20principles%20of%20sustainable%20and%20environmentally%20responsible%20AI%2C%20as%20it%20leads%20to%20immense%20carbon%20footprints%20and%20resource%20inefficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20and%20efficient%20alternative%3A%20an%20Ensemble-of-Specialists%20framework%20for%20building%20Remote%20Sensing%20Foundation%20Models%20%28RSFMs%29.%20Our%20method%20decomposes%20the%20training%20process%20into%20lightweight%2C%20task-specific%20ConvNeXtV2%20specialists%20that%20can%20be%20frozen%20and%20reused.%20This%20modular%20approach%20offers%20strong%20advantages%20in%20efficiency%2C%20interpretability%2C%20and%20extensibility.%20Moreover%2C%20it%20naturally%20supports%20federated%20training%2C%20pruning%2C%20and%20continuous%20specialist%20integration%2C%20making%20it%20particularly%20well-suited%20for%20collaborative%20and%20resource-constrained%20settings.%20Our%20framework%20sets%20a%20new%20direction%20for%20building%20scalable%20and%20efficient%20RSFMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.21523v1&entry.124074799=Read"},
{"title": "ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images", "author": "M. Naseer Subhani", "abstract": "Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.", "link": "http://arxiv.org/abs/2511.21606v1", "date": "2025-11-26", "relevancy": 2.6258, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5366}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReSAM%3A%20Refine%2C%20Requery%2C%20and%20Reinforce%3A%20Self-Prompting%20Point-Supervised%20Segmentation%20for%20Remote%20Sensing%20Images&body=Title%3A%20ReSAM%3A%20Refine%2C%20Requery%2C%20and%20Reinforce%3A%20Self-Prompting%20Point-Supervised%20Segmentation%20for%20Remote%20Sensing%20Images%0AAuthor%3A%20M.%20Naseer%20Subhani%0AAbstract%3A%20Interactive%20segmentation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20have%20demonstrated%20remarkable%20generalization%20on%20natural%20images%2C%20but%20perform%20suboptimally%20on%20remote%20sensing%20imagery%20%28RSI%29%20due%20to%20severe%20domain%20shift%20and%20the%20scarcity%20of%20dense%20annotations.%20To%20address%20this%2C%20we%20propose%20a%20self-prompting%2C%20point-supervised%20framework%20that%20adapts%20SAM%20to%20RSIs%20using%20only%20sparse%20point%20annotations.%20Our%20method%20employs%20a%20Refine-Requery-Reinforce%20loop%2C%20where%20coarse%20pseudo-masks%20are%20generated%20from%20initial%20points%20%28Refine%29%2C%20improved%20with%20self-constructed%20box%20prompts%20%28Requery%29%2C%20and%20embeddings%20are%20aligned%20across%20iterations%20to%20reduce%20confirmation%20bias%20%28Reinforce%29.%20Without%20relying%20on%20full-mask%20supervision%2C%20our%20approach%20progressively%20enhances%20SAM%27s%20segmentation%20quality%20and%20domain%20robustness%20through%20self-guided%20prompt%20adaptation%20.%20We%20evaluate%20our%20proposed%20method%20on%20three%20RSI%20benchmark%20datasets%2C%20including%20WHU%2C%20HRSID%2C%20and%20NWPU%20VHR-10%2C%20showing%20that%20our%20method%20consistently%20surpasses%20pretrained%20SAM%20and%20recent%20point-supervised%20segmentation%20methods.%20Our%20results%20demonstrate%20that%20self-prompting%20and%20semantic%20alignment%20provide%20an%20efficient%20path%20towards%20scalable%2C%20point-level%20adaptation%20of%20foundation%20segmentation%20models%20for%20remote%20sensing%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReSAM%253A%2520Refine%252C%2520Requery%252C%2520and%2520Reinforce%253A%2520Self-Prompting%2520Point-Supervised%2520Segmentation%2520for%2520Remote%2520Sensing%2520Images%26entry.906535625%3DM.%2520Naseer%2520Subhani%26entry.1292438233%3DInteractive%2520segmentation%2520models%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520have%2520demonstrated%2520remarkable%2520generalization%2520on%2520natural%2520images%252C%2520but%2520perform%2520suboptimally%2520on%2520remote%2520sensing%2520imagery%2520%2528RSI%2529%2520due%2520to%2520severe%2520domain%2520shift%2520and%2520the%2520scarcity%2520of%2520dense%2520annotations.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520self-prompting%252C%2520point-supervised%2520framework%2520that%2520adapts%2520SAM%2520to%2520RSIs%2520using%2520only%2520sparse%2520point%2520annotations.%2520Our%2520method%2520employs%2520a%2520Refine-Requery-Reinforce%2520loop%252C%2520where%2520coarse%2520pseudo-masks%2520are%2520generated%2520from%2520initial%2520points%2520%2528Refine%2529%252C%2520improved%2520with%2520self-constructed%2520box%2520prompts%2520%2528Requery%2529%252C%2520and%2520embeddings%2520are%2520aligned%2520across%2520iterations%2520to%2520reduce%2520confirmation%2520bias%2520%2528Reinforce%2529.%2520Without%2520relying%2520on%2520full-mask%2520supervision%252C%2520our%2520approach%2520progressively%2520enhances%2520SAM%2527s%2520segmentation%2520quality%2520and%2520domain%2520robustness%2520through%2520self-guided%2520prompt%2520adaptation%2520.%2520We%2520evaluate%2520our%2520proposed%2520method%2520on%2520three%2520RSI%2520benchmark%2520datasets%252C%2520including%2520WHU%252C%2520HRSID%252C%2520and%2520NWPU%2520VHR-10%252C%2520showing%2520that%2520our%2520method%2520consistently%2520surpasses%2520pretrained%2520SAM%2520and%2520recent%2520point-supervised%2520segmentation%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520self-prompting%2520and%2520semantic%2520alignment%2520provide%2520an%2520efficient%2520path%2520towards%2520scalable%252C%2520point-level%2520adaptation%2520of%2520foundation%2520segmentation%2520models%2520for%2520remote%2520sensing%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReSAM%3A%20Refine%2C%20Requery%2C%20and%20Reinforce%3A%20Self-Prompting%20Point-Supervised%20Segmentation%20for%20Remote%20Sensing%20Images&entry.906535625=M.%20Naseer%20Subhani&entry.1292438233=Interactive%20segmentation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20have%20demonstrated%20remarkable%20generalization%20on%20natural%20images%2C%20but%20perform%20suboptimally%20on%20remote%20sensing%20imagery%20%28RSI%29%20due%20to%20severe%20domain%20shift%20and%20the%20scarcity%20of%20dense%20annotations.%20To%20address%20this%2C%20we%20propose%20a%20self-prompting%2C%20point-supervised%20framework%20that%20adapts%20SAM%20to%20RSIs%20using%20only%20sparse%20point%20annotations.%20Our%20method%20employs%20a%20Refine-Requery-Reinforce%20loop%2C%20where%20coarse%20pseudo-masks%20are%20generated%20from%20initial%20points%20%28Refine%29%2C%20improved%20with%20self-constructed%20box%20prompts%20%28Requery%29%2C%20and%20embeddings%20are%20aligned%20across%20iterations%20to%20reduce%20confirmation%20bias%20%28Reinforce%29.%20Without%20relying%20on%20full-mask%20supervision%2C%20our%20approach%20progressively%20enhances%20SAM%27s%20segmentation%20quality%20and%20domain%20robustness%20through%20self-guided%20prompt%20adaptation%20.%20We%20evaluate%20our%20proposed%20method%20on%20three%20RSI%20benchmark%20datasets%2C%20including%20WHU%2C%20HRSID%2C%20and%20NWPU%20VHR-10%2C%20showing%20that%20our%20method%20consistently%20surpasses%20pretrained%20SAM%20and%20recent%20point-supervised%20segmentation%20methods.%20Our%20results%20demonstrate%20that%20self-prompting%20and%20semantic%20alignment%20provide%20an%20efficient%20path%20towards%20scalable%2C%20point-level%20adaptation%20of%20foundation%20segmentation%20models%20for%20remote%20sensing%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.21606v1&entry.124074799=Read"},
{"title": "Active Learning for GCN-based Action Recognition", "author": "Hichem Sahbi", "abstract": "Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.", "link": "http://arxiv.org/abs/2511.21625v1", "date": "2025-11-26", "relevancy": 2.5772, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5187}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20GCN-based%20Action%20Recognition&body=Title%3A%20Active%20Learning%20for%20GCN-based%20Action%20Recognition%0AAuthor%3A%20Hichem%20Sahbi%0AAbstract%3A%20Despite%20the%20notable%20success%20of%20graph%20convolutional%20networks%20%28GCNs%29%20in%20skeleton-based%20action%20recognition%2C%20their%20performance%20often%20depends%20on%20large%20volumes%20of%20labeled%20data%2C%20which%20are%20frequently%20scarce%20in%20practical%20settings.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20label-efficient%20GCN%20model.%20Our%20work%20makes%20two%20primary%20contributions.%20First%2C%20we%20develop%20a%20novel%20acquisition%20function%20that%20employs%20an%20adversarial%20strategy%20to%20identify%20a%20compact%20set%20of%20informative%20exemplars%20for%20labeling.%20This%20selection%20process%20balances%20representativeness%2C%20diversity%2C%20and%20uncertainty.%20Second%2C%20we%20introduce%20bidirectional%20and%20stable%20GCN%20architectures.%20These%20enhanced%20networks%20facilitate%20a%20more%20effective%20mapping%20between%20the%20ambient%20and%20latent%20data%20spaces%2C%20enabling%20a%20better%20understanding%20of%20the%20learned%20exemplar%20distribution.%20Extensive%20evaluations%20on%20two%20challenging%20skeleton-based%20action%20recognition%20benchmarks%20reveal%20significant%20improvements%20achieved%20by%20our%20label-efficient%20GCNs%20compared%20to%20prior%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520GCN-based%2520Action%2520Recognition%26entry.906535625%3DHichem%2520Sahbi%26entry.1292438233%3DDespite%2520the%2520notable%2520success%2520of%2520graph%2520convolutional%2520networks%2520%2528GCNs%2529%2520in%2520skeleton-based%2520action%2520recognition%252C%2520their%2520performance%2520often%2520depends%2520on%2520large%2520volumes%2520of%2520labeled%2520data%252C%2520which%2520are%2520frequently%2520scarce%2520in%2520practical%2520settings.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520label-efficient%2520GCN%2520model.%2520Our%2520work%2520makes%2520two%2520primary%2520contributions.%2520First%252C%2520we%2520develop%2520a%2520novel%2520acquisition%2520function%2520that%2520employs%2520an%2520adversarial%2520strategy%2520to%2520identify%2520a%2520compact%2520set%2520of%2520informative%2520exemplars%2520for%2520labeling.%2520This%2520selection%2520process%2520balances%2520representativeness%252C%2520diversity%252C%2520and%2520uncertainty.%2520Second%252C%2520we%2520introduce%2520bidirectional%2520and%2520stable%2520GCN%2520architectures.%2520These%2520enhanced%2520networks%2520facilitate%2520a%2520more%2520effective%2520mapping%2520between%2520the%2520ambient%2520and%2520latent%2520data%2520spaces%252C%2520enabling%2520a%2520better%2520understanding%2520of%2520the%2520learned%2520exemplar%2520distribution.%2520Extensive%2520evaluations%2520on%2520two%2520challenging%2520skeleton-based%2520action%2520recognition%2520benchmarks%2520reveal%2520significant%2520improvements%2520achieved%2520by%2520our%2520label-efficient%2520GCNs%2520compared%2520to%2520prior%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20GCN-based%20Action%20Recognition&entry.906535625=Hichem%20Sahbi&entry.1292438233=Despite%20the%20notable%20success%20of%20graph%20convolutional%20networks%20%28GCNs%29%20in%20skeleton-based%20action%20recognition%2C%20their%20performance%20often%20depends%20on%20large%20volumes%20of%20labeled%20data%2C%20which%20are%20frequently%20scarce%20in%20practical%20settings.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20label-efficient%20GCN%20model.%20Our%20work%20makes%20two%20primary%20contributions.%20First%2C%20we%20develop%20a%20novel%20acquisition%20function%20that%20employs%20an%20adversarial%20strategy%20to%20identify%20a%20compact%20set%20of%20informative%20exemplars%20for%20labeling.%20This%20selection%20process%20balances%20representativeness%2C%20diversity%2C%20and%20uncertainty.%20Second%2C%20we%20introduce%20bidirectional%20and%20stable%20GCN%20architectures.%20These%20enhanced%20networks%20facilitate%20a%20more%20effective%20mapping%20between%20the%20ambient%20and%20latent%20data%20spaces%2C%20enabling%20a%20better%20understanding%20of%20the%20learned%20exemplar%20distribution.%20Extensive%20evaluations%20on%20two%20challenging%20skeleton-based%20action%20recognition%20benchmarks%20reveal%20significant%20improvements%20achieved%20by%20our%20label-efficient%20GCNs%20compared%20to%20prior%20work.&entry.1838667208=http%3A//arxiv.org/abs/2511.21625v1&entry.124074799=Read"},
{"title": "PFF-Net: Patch Feature Fitting for Point Cloud Normal Estimation", "author": "Qing Li and Huifang Feng and Kanle Shi and Yue Gao and Yi Fang and Yu-Shen Liu and Zhizhong Han", "abstract": "Estimating the normal of a point requires constructing a local patch to provide center-surrounding context, but determining the appropriate neighborhood size is difficult when dealing with different data or geometries. Existing methods commonly employ various parameter-heavy strategies to extract a full feature description from the input patch. However, they still have difficulties in accurately and efficiently predicting normals for various point clouds. In this work, we present a new idea of feature extraction for robust normal estimation of point clouds. We use the fusion of multi-scale features from different neighborhood sizes to address the issue of selecting reasonable patch sizes for various data or geometries. We seek to model a patch feature fitting (PFF) based on multi-scale features to approximate the optimal geometric description for normal estimation and implement the approximation process via multi-scale feature aggregation and cross-scale feature compensation. The feature aggregation module progressively aggregates the patch features of different scales to the center of the patch and shrinks the patch size by removing points far from the center. It not only enables the network to precisely capture the structure characteristic in a wide range, but also describes highly detailed geometries. The feature compensation module ensures the reusability of features from earlier layers of large scales and reveals associated information in different patch sizes. Our approximation strategy based on aggregating the features of multiple scales enables the model to achieve scale adaptation of varying local patches and deliver the optimal feature description. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets with fewer network parameters and running time.", "link": "http://arxiv.org/abs/2511.21365v1", "date": "2025-11-26", "relevancy": 2.5449, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFF-Net%3A%20Patch%20Feature%20Fitting%20for%20Point%20Cloud%20Normal%20Estimation&body=Title%3A%20PFF-Net%3A%20Patch%20Feature%20Fitting%20for%20Point%20Cloud%20Normal%20Estimation%0AAuthor%3A%20Qing%20Li%20and%20Huifang%20Feng%20and%20Kanle%20Shi%20and%20Yue%20Gao%20and%20Yi%20Fang%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20Estimating%20the%20normal%20of%20a%20point%20requires%20constructing%20a%20local%20patch%20to%20provide%20center-surrounding%20context%2C%20but%20determining%20the%20appropriate%20neighborhood%20size%20is%20difficult%20when%20dealing%20with%20different%20data%20or%20geometries.%20Existing%20methods%20commonly%20employ%20various%20parameter-heavy%20strategies%20to%20extract%20a%20full%20feature%20description%20from%20the%20input%20patch.%20However%2C%20they%20still%20have%20difficulties%20in%20accurately%20and%20efficiently%20predicting%20normals%20for%20various%20point%20clouds.%20In%20this%20work%2C%20we%20present%20a%20new%20idea%20of%20feature%20extraction%20for%20robust%20normal%20estimation%20of%20point%20clouds.%20We%20use%20the%20fusion%20of%20multi-scale%20features%20from%20different%20neighborhood%20sizes%20to%20address%20the%20issue%20of%20selecting%20reasonable%20patch%20sizes%20for%20various%20data%20or%20geometries.%20We%20seek%20to%20model%20a%20patch%20feature%20fitting%20%28PFF%29%20based%20on%20multi-scale%20features%20to%20approximate%20the%20optimal%20geometric%20description%20for%20normal%20estimation%20and%20implement%20the%20approximation%20process%20via%20multi-scale%20feature%20aggregation%20and%20cross-scale%20feature%20compensation.%20The%20feature%20aggregation%20module%20progressively%20aggregates%20the%20patch%20features%20of%20different%20scales%20to%20the%20center%20of%20the%20patch%20and%20shrinks%20the%20patch%20size%20by%20removing%20points%20far%20from%20the%20center.%20It%20not%20only%20enables%20the%20network%20to%20precisely%20capture%20the%20structure%20characteristic%20in%20a%20wide%20range%2C%20but%20also%20describes%20highly%20detailed%20geometries.%20The%20feature%20compensation%20module%20ensures%20the%20reusability%20of%20features%20from%20earlier%20layers%20of%20large%20scales%20and%20reveals%20associated%20information%20in%20different%20patch%20sizes.%20Our%20approximation%20strategy%20based%20on%20aggregating%20the%20features%20of%20multiple%20scales%20enables%20the%20model%20to%20achieve%20scale%20adaptation%20of%20varying%20local%20patches%20and%20deliver%20the%20optimal%20feature%20description.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20datasets%20with%20fewer%20network%20parameters%20and%20running%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFF-Net%253A%2520Patch%2520Feature%2520Fitting%2520for%2520Point%2520Cloud%2520Normal%2520Estimation%26entry.906535625%3DQing%2520Li%2520and%2520Huifang%2520Feng%2520and%2520Kanle%2520Shi%2520and%2520Yue%2520Gao%2520and%2520Yi%2520Fang%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3DEstimating%2520the%2520normal%2520of%2520a%2520point%2520requires%2520constructing%2520a%2520local%2520patch%2520to%2520provide%2520center-surrounding%2520context%252C%2520but%2520determining%2520the%2520appropriate%2520neighborhood%2520size%2520is%2520difficult%2520when%2520dealing%2520with%2520different%2520data%2520or%2520geometries.%2520Existing%2520methods%2520commonly%2520employ%2520various%2520parameter-heavy%2520strategies%2520to%2520extract%2520a%2520full%2520feature%2520description%2520from%2520the%2520input%2520patch.%2520However%252C%2520they%2520still%2520have%2520difficulties%2520in%2520accurately%2520and%2520efficiently%2520predicting%2520normals%2520for%2520various%2520point%2520clouds.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520idea%2520of%2520feature%2520extraction%2520for%2520robust%2520normal%2520estimation%2520of%2520point%2520clouds.%2520We%2520use%2520the%2520fusion%2520of%2520multi-scale%2520features%2520from%2520different%2520neighborhood%2520sizes%2520to%2520address%2520the%2520issue%2520of%2520selecting%2520reasonable%2520patch%2520sizes%2520for%2520various%2520data%2520or%2520geometries.%2520We%2520seek%2520to%2520model%2520a%2520patch%2520feature%2520fitting%2520%2528PFF%2529%2520based%2520on%2520multi-scale%2520features%2520to%2520approximate%2520the%2520optimal%2520geometric%2520description%2520for%2520normal%2520estimation%2520and%2520implement%2520the%2520approximation%2520process%2520via%2520multi-scale%2520feature%2520aggregation%2520and%2520cross-scale%2520feature%2520compensation.%2520The%2520feature%2520aggregation%2520module%2520progressively%2520aggregates%2520the%2520patch%2520features%2520of%2520different%2520scales%2520to%2520the%2520center%2520of%2520the%2520patch%2520and%2520shrinks%2520the%2520patch%2520size%2520by%2520removing%2520points%2520far%2520from%2520the%2520center.%2520It%2520not%2520only%2520enables%2520the%2520network%2520to%2520precisely%2520capture%2520the%2520structure%2520characteristic%2520in%2520a%2520wide%2520range%252C%2520but%2520also%2520describes%2520highly%2520detailed%2520geometries.%2520The%2520feature%2520compensation%2520module%2520ensures%2520the%2520reusability%2520of%2520features%2520from%2520earlier%2520layers%2520of%2520large%2520scales%2520and%2520reveals%2520associated%2520information%2520in%2520different%2520patch%2520sizes.%2520Our%2520approximation%2520strategy%2520based%2520on%2520aggregating%2520the%2520features%2520of%2520multiple%2520scales%2520enables%2520the%2520model%2520to%2520achieve%2520scale%2520adaptation%2520of%2520varying%2520local%2520patches%2520and%2520deliver%2520the%2520optimal%2520feature%2520description.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520with%2520fewer%2520network%2520parameters%2520and%2520running%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFF-Net%3A%20Patch%20Feature%20Fitting%20for%20Point%20Cloud%20Normal%20Estimation&entry.906535625=Qing%20Li%20and%20Huifang%20Feng%20and%20Kanle%20Shi%20and%20Yue%20Gao%20and%20Yi%20Fang%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=Estimating%20the%20normal%20of%20a%20point%20requires%20constructing%20a%20local%20patch%20to%20provide%20center-surrounding%20context%2C%20but%20determining%20the%20appropriate%20neighborhood%20size%20is%20difficult%20when%20dealing%20with%20different%20data%20or%20geometries.%20Existing%20methods%20commonly%20employ%20various%20parameter-heavy%20strategies%20to%20extract%20a%20full%20feature%20description%20from%20the%20input%20patch.%20However%2C%20they%20still%20have%20difficulties%20in%20accurately%20and%20efficiently%20predicting%20normals%20for%20various%20point%20clouds.%20In%20this%20work%2C%20we%20present%20a%20new%20idea%20of%20feature%20extraction%20for%20robust%20normal%20estimation%20of%20point%20clouds.%20We%20use%20the%20fusion%20of%20multi-scale%20features%20from%20different%20neighborhood%20sizes%20to%20address%20the%20issue%20of%20selecting%20reasonable%20patch%20sizes%20for%20various%20data%20or%20geometries.%20We%20seek%20to%20model%20a%20patch%20feature%20fitting%20%28PFF%29%20based%20on%20multi-scale%20features%20to%20approximate%20the%20optimal%20geometric%20description%20for%20normal%20estimation%20and%20implement%20the%20approximation%20process%20via%20multi-scale%20feature%20aggregation%20and%20cross-scale%20feature%20compensation.%20The%20feature%20aggregation%20module%20progressively%20aggregates%20the%20patch%20features%20of%20different%20scales%20to%20the%20center%20of%20the%20patch%20and%20shrinks%20the%20patch%20size%20by%20removing%20points%20far%20from%20the%20center.%20It%20not%20only%20enables%20the%20network%20to%20precisely%20capture%20the%20structure%20characteristic%20in%20a%20wide%20range%2C%20but%20also%20describes%20highly%20detailed%20geometries.%20The%20feature%20compensation%20module%20ensures%20the%20reusability%20of%20features%20from%20earlier%20layers%20of%20large%20scales%20and%20reveals%20associated%20information%20in%20different%20patch%20sizes.%20Our%20approximation%20strategy%20based%20on%20aggregating%20the%20features%20of%20multiple%20scales%20enables%20the%20model%20to%20achieve%20scale%20adaptation%20of%20varying%20local%20patches%20and%20deliver%20the%20optimal%20feature%20description.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20datasets%20with%20fewer%20network%20parameters%20and%20running%20time.&entry.1838667208=http%3A//arxiv.org/abs/2511.21365v1&entry.124074799=Read"},
{"title": "EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation", "author": "Futian Wang and Fan Zhang and Xiao Wang and Mengqi Wang and Dexing Huang and Jin Tang", "abstract": "Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.", "link": "http://arxiv.org/abs/2511.21439v1", "date": "2025-11-26", "relevancy": 2.5072, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5086}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvRainDrop%3A%20HyperGraph-guided%20Completion%20for%20Effective%20Frame%20and%20Event%20Stream%20Aggregation&body=Title%3A%20EvRainDrop%3A%20HyperGraph-guided%20Completion%20for%20Effective%20Frame%20and%20Event%20Stream%20Aggregation%0AAuthor%3A%20Futian%20Wang%20and%20Fan%20Zhang%20and%20Xiao%20Wang%20and%20Mengqi%20Wang%20and%20Dexing%20Huang%20and%20Jin%20Tang%0AAbstract%3A%20Event%20cameras%20produce%20asynchronous%20event%20streams%20that%20are%20spatially%20sparse%20yet%20temporally%20dense.%20Mainstream%20event%20representation%20learning%20algorithms%20typically%20use%20event%20frames%2C%20voxels%2C%20or%20tensors%20as%20input.%20Although%20these%20approaches%20have%20achieved%20notable%20progress%2C%20they%20struggle%20to%20address%20the%20undersampling%20problem%20caused%20by%20spatial%20sparsity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hypergraph-guided%20spatio-temporal%20event%20stream%20completion%20mechanism%2C%20which%20connects%20event%20tokens%20across%20different%20times%20and%20spatial%20locations%20via%20hypergraphs%20and%20leverages%20contextual%20information%20message%20passing%20to%20complete%20these%20sparse%20events.%20The%20proposed%20method%20can%20flexibly%20incorporate%20RGB%20tokens%20as%20nodes%20in%20the%20hypergraph%20within%20this%20completion%20framework%2C%20enabling%20multi-modal%20hypergraph-based%20information%20completion.%20Subsequently%2C%20we%20aggregate%20hypergraph%20node%20information%20across%20different%20time%20steps%20through%20self-attention%2C%20enabling%20effective%20learning%20and%20fusion%20of%20multi-modal%20features.%20Extensive%20experiments%20on%20both%20single-%20and%20multi-label%20event%20classification%20tasks%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%20framework.%20The%20source%20code%20of%20this%20paper%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/EvRainDrop.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvRainDrop%253A%2520HyperGraph-guided%2520Completion%2520for%2520Effective%2520Frame%2520and%2520Event%2520Stream%2520Aggregation%26entry.906535625%3DFutian%2520Wang%2520and%2520Fan%2520Zhang%2520and%2520Xiao%2520Wang%2520and%2520Mengqi%2520Wang%2520and%2520Dexing%2520Huang%2520and%2520Jin%2520Tang%26entry.1292438233%3DEvent%2520cameras%2520produce%2520asynchronous%2520event%2520streams%2520that%2520are%2520spatially%2520sparse%2520yet%2520temporally%2520dense.%2520Mainstream%2520event%2520representation%2520learning%2520algorithms%2520typically%2520use%2520event%2520frames%252C%2520voxels%252C%2520or%2520tensors%2520as%2520input.%2520Although%2520these%2520approaches%2520have%2520achieved%2520notable%2520progress%252C%2520they%2520struggle%2520to%2520address%2520the%2520undersampling%2520problem%2520caused%2520by%2520spatial%2520sparsity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520hypergraph-guided%2520spatio-temporal%2520event%2520stream%2520completion%2520mechanism%252C%2520which%2520connects%2520event%2520tokens%2520across%2520different%2520times%2520and%2520spatial%2520locations%2520via%2520hypergraphs%2520and%2520leverages%2520contextual%2520information%2520message%2520passing%2520to%2520complete%2520these%2520sparse%2520events.%2520The%2520proposed%2520method%2520can%2520flexibly%2520incorporate%2520RGB%2520tokens%2520as%2520nodes%2520in%2520the%2520hypergraph%2520within%2520this%2520completion%2520framework%252C%2520enabling%2520multi-modal%2520hypergraph-based%2520information%2520completion.%2520Subsequently%252C%2520we%2520aggregate%2520hypergraph%2520node%2520information%2520across%2520different%2520time%2520steps%2520through%2520self-attention%252C%2520enabling%2520effective%2520learning%2520and%2520fusion%2520of%2520multi-modal%2520features.%2520Extensive%2520experiments%2520on%2520both%2520single-%2520and%2520multi-label%2520event%2520classification%2520tasks%2520fully%2520validated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework.%2520The%2520source%2520code%2520of%2520this%2520paper%2520will%2520be%2520released%2520on%2520https%253A//github.com/Event-AHU/EvRainDrop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvRainDrop%3A%20HyperGraph-guided%20Completion%20for%20Effective%20Frame%20and%20Event%20Stream%20Aggregation&entry.906535625=Futian%20Wang%20and%20Fan%20Zhang%20and%20Xiao%20Wang%20and%20Mengqi%20Wang%20and%20Dexing%20Huang%20and%20Jin%20Tang&entry.1292438233=Event%20cameras%20produce%20asynchronous%20event%20streams%20that%20are%20spatially%20sparse%20yet%20temporally%20dense.%20Mainstream%20event%20representation%20learning%20algorithms%20typically%20use%20event%20frames%2C%20voxels%2C%20or%20tensors%20as%20input.%20Although%20these%20approaches%20have%20achieved%20notable%20progress%2C%20they%20struggle%20to%20address%20the%20undersampling%20problem%20caused%20by%20spatial%20sparsity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hypergraph-guided%20spatio-temporal%20event%20stream%20completion%20mechanism%2C%20which%20connects%20event%20tokens%20across%20different%20times%20and%20spatial%20locations%20via%20hypergraphs%20and%20leverages%20contextual%20information%20message%20passing%20to%20complete%20these%20sparse%20events.%20The%20proposed%20method%20can%20flexibly%20incorporate%20RGB%20tokens%20as%20nodes%20in%20the%20hypergraph%20within%20this%20completion%20framework%2C%20enabling%20multi-modal%20hypergraph-based%20information%20completion.%20Subsequently%2C%20we%20aggregate%20hypergraph%20node%20information%20across%20different%20time%20steps%20through%20self-attention%2C%20enabling%20effective%20learning%20and%20fusion%20of%20multi-modal%20features.%20Extensive%20experiments%20on%20both%20single-%20and%20multi-label%20event%20classification%20tasks%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%20framework.%20The%20source%20code%20of%20this%20paper%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/EvRainDrop.&entry.1838667208=http%3A//arxiv.org/abs/2511.21439v1&entry.124074799=Read"},
{"title": "CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation", "author": "Shizhe Sun and Wataru Ohyama", "abstract": "We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD", "link": "http://arxiv.org/abs/2511.21503v1", "date": "2025-11-26", "relevancy": 2.4938, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanKD%3A%20Cross-Attention-based%20Non-local%20operation%20for%20Feature-based%20Knowledge%20Distillation&body=Title%3A%20CanKD%3A%20Cross-Attention-based%20Non-local%20operation%20for%20Feature-based%20Knowledge%20Distillation%0AAuthor%3A%20Shizhe%20Sun%20and%20Wataru%20Ohyama%0AAbstract%3A%20We%20propose%20Cross-Attention-based%20Non-local%20Knowledge%20Distillation%20%28CanKD%29%2C%20a%20novel%20feature-based%20knowledge%20distillation%20framework%20that%20leverages%20cross-attention%20mechanisms%20to%20enhance%20the%20knowledge%20transfer%20process.%20Unlike%20traditional%20self-attention-based%20distillation%20methods%20that%20align%20teacher%20and%20student%20feature%20maps%20independently%2C%20CanKD%20enables%20each%20pixel%20in%20the%20student%20feature%20map%20to%20dynamically%20consider%20all%20pixels%20in%20the%20teacher%20feature%20map.%20This%20non-local%20knowledge%20transfer%20more%20thoroughly%20captures%20pixel-wise%20relationships%2C%20improving%20feature%20representation%20learning.%20Our%20method%20introduces%20only%20an%20additional%20loss%20function%20to%20achieve%20superior%20performance%20compared%20with%20existing%20attention-guided%20distillation%20methods.%20Extensive%20experiments%20on%20object%20detection%20and%20image%20segmentation%20tasks%20demonstrate%20that%20CanKD%20outperforms%20state-of-the-art%20feature%20and%20hybrid%20distillation%20methods.%20These%20experimental%20results%20highlight%20CanKD%27s%20potential%20as%20a%20new%20paradigm%20for%20attention-guided%20distillation%20in%20computer%20vision%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/tori-hotaru/CanKD%0ALink%3A%20http%3A//arxiv.org/abs/2511.21503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanKD%253A%2520Cross-Attention-based%2520Non-local%2520operation%2520for%2520Feature-based%2520Knowledge%2520Distillation%26entry.906535625%3DShizhe%2520Sun%2520and%2520Wataru%2520Ohyama%26entry.1292438233%3DWe%2520propose%2520Cross-Attention-based%2520Non-local%2520Knowledge%2520Distillation%2520%2528CanKD%2529%252C%2520a%2520novel%2520feature-based%2520knowledge%2520distillation%2520framework%2520that%2520leverages%2520cross-attention%2520mechanisms%2520to%2520enhance%2520the%2520knowledge%2520transfer%2520process.%2520Unlike%2520traditional%2520self-attention-based%2520distillation%2520methods%2520that%2520align%2520teacher%2520and%2520student%2520feature%2520maps%2520independently%252C%2520CanKD%2520enables%2520each%2520pixel%2520in%2520the%2520student%2520feature%2520map%2520to%2520dynamically%2520consider%2520all%2520pixels%2520in%2520the%2520teacher%2520feature%2520map.%2520This%2520non-local%2520knowledge%2520transfer%2520more%2520thoroughly%2520captures%2520pixel-wise%2520relationships%252C%2520improving%2520feature%2520representation%2520learning.%2520Our%2520method%2520introduces%2520only%2520an%2520additional%2520loss%2520function%2520to%2520achieve%2520superior%2520performance%2520compared%2520with%2520existing%2520attention-guided%2520distillation%2520methods.%2520Extensive%2520experiments%2520on%2520object%2520detection%2520and%2520image%2520segmentation%2520tasks%2520demonstrate%2520that%2520CanKD%2520outperforms%2520state-of-the-art%2520feature%2520and%2520hybrid%2520distillation%2520methods.%2520These%2520experimental%2520results%2520highlight%2520CanKD%2527s%2520potential%2520as%2520a%2520new%2520paradigm%2520for%2520attention-guided%2520distillation%2520in%2520computer%2520vision%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/tori-hotaru/CanKD%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanKD%3A%20Cross-Attention-based%20Non-local%20operation%20for%20Feature-based%20Knowledge%20Distillation&entry.906535625=Shizhe%20Sun%20and%20Wataru%20Ohyama&entry.1292438233=We%20propose%20Cross-Attention-based%20Non-local%20Knowledge%20Distillation%20%28CanKD%29%2C%20a%20novel%20feature-based%20knowledge%20distillation%20framework%20that%20leverages%20cross-attention%20mechanisms%20to%20enhance%20the%20knowledge%20transfer%20process.%20Unlike%20traditional%20self-attention-based%20distillation%20methods%20that%20align%20teacher%20and%20student%20feature%20maps%20independently%2C%20CanKD%20enables%20each%20pixel%20in%20the%20student%20feature%20map%20to%20dynamically%20consider%20all%20pixels%20in%20the%20teacher%20feature%20map.%20This%20non-local%20knowledge%20transfer%20more%20thoroughly%20captures%20pixel-wise%20relationships%2C%20improving%20feature%20representation%20learning.%20Our%20method%20introduces%20only%20an%20additional%20loss%20function%20to%20achieve%20superior%20performance%20compared%20with%20existing%20attention-guided%20distillation%20methods.%20Extensive%20experiments%20on%20object%20detection%20and%20image%20segmentation%20tasks%20demonstrate%20that%20CanKD%20outperforms%20state-of-the-art%20feature%20and%20hybrid%20distillation%20methods.%20These%20experimental%20results%20highlight%20CanKD%27s%20potential%20as%20a%20new%20paradigm%20for%20attention-guided%20distillation%20in%20computer%20vision%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/tori-hotaru/CanKD&entry.1838667208=http%3A//arxiv.org/abs/2511.21503v1&entry.124074799=Read"},
{"title": "Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation", "author": "Hang Chen and Maoyuan Ye and Peng Yang and Haibin He and Juhua Liu and Bo Du", "abstract": "Power transmission corridor hazard segmentation (PTCHS) aims to separate transmission equipment and surrounding hazards from complex background, conveying great significance to maintaining electric power transmission safety. Recently, the Segment Anything Model (SAM) has emerged as a foundational vision model and pushed the boundaries of segmentation tasks. However, SAM struggles to deal with the target objects in complex transmission corridor scenario, especially those with fine structure. In this paper, we propose ELE-SAM, adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt Adapter to achieve better prompt tokens via incorporating global-local features and focusing more on key regions. Subsequently, to tackle the hazard objects with fine structure in complex background, we design a High-Fidelity Mask Decoder by leveraging multi-granularity mask features and then scaling them to a higher resolution. Moreover, to train ELE-SAM and advance this field, we construct the ELE-40K benchmark, the first large-scale and real-world dataset for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K demonstrate the superior performance that ELE-SAM outperforms the baseline model with the average 16.8% mIoU and 20.6% mBIoU performance improvement. Moreover, compared with the state-of-the-art method on HQSeg-44K, the average 2.9% mIoU and 3.8% mBIoU absolute improvements further validate the effectiveness of our method on high-quality generic object segmentation. The source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.", "link": "http://arxiv.org/abs/2505.22105v2", "date": "2025-11-26", "relevancy": 2.4844, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Segment%20Anything%20Model%20for%20Power%20Transmission%20Corridor%20Hazard%20Segmentation&body=Title%3A%20Adapting%20Segment%20Anything%20Model%20for%20Power%20Transmission%20Corridor%20Hazard%20Segmentation%0AAuthor%3A%20Hang%20Chen%20and%20Maoyuan%20Ye%20and%20Peng%20Yang%20and%20Haibin%20He%20and%20Juhua%20Liu%20and%20Bo%20Du%0AAbstract%3A%20Power%20transmission%20corridor%20hazard%20segmentation%20%28PTCHS%29%20aims%20to%20separate%20transmission%20equipment%20and%20surrounding%20hazards%20from%20complex%20background%2C%20conveying%20great%20significance%20to%20maintaining%20electric%20power%20transmission%20safety.%20Recently%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20emerged%20as%20a%20foundational%20vision%20model%20and%20pushed%20the%20boundaries%20of%20segmentation%20tasks.%20However%2C%20SAM%20struggles%20to%20deal%20with%20the%20target%20objects%20in%20complex%20transmission%20corridor%20scenario%2C%20especially%20those%20with%20fine%20structure.%20In%20this%20paper%2C%20we%20propose%20ELE-SAM%2C%20adapting%20SAM%20for%20the%20PTCHS%20task.%20Technically%2C%20we%20develop%20a%20Context-Aware%20Prompt%20Adapter%20to%20achieve%20better%20prompt%20tokens%20via%20incorporating%20global-local%20features%20and%20focusing%20more%20on%20key%20regions.%20Subsequently%2C%20to%20tackle%20the%20hazard%20objects%20with%20fine%20structure%20in%20complex%20background%2C%20we%20design%20a%20High-Fidelity%20Mask%20Decoder%20by%20leveraging%20multi-granularity%20mask%20features%20and%20then%20scaling%20them%20to%20a%20higher%20resolution.%20Moreover%2C%20to%20train%20ELE-SAM%20and%20advance%20this%20field%2C%20we%20construct%20the%20ELE-40K%20benchmark%2C%20the%20first%20large-scale%20and%20real-world%20dataset%20for%20PTCHS%20including%2044%2C094%20image-mask%20pairs.%20Experimental%20results%20for%20ELE-40K%20demonstrate%20the%20superior%20performance%20that%20ELE-SAM%20outperforms%20the%20baseline%20model%20with%20the%20average%2016.8%25%20mIoU%20and%2020.6%25%20mBIoU%20performance%20improvement.%20Moreover%2C%20compared%20with%20the%20state-of-the-art%20method%20on%20HQSeg-44K%2C%20the%20average%202.9%25%20mIoU%20and%203.8%25%20mBIoU%20absolute%20improvements%20further%20validate%20the%20effectiveness%20of%20our%20method%20on%20high-quality%20generic%20object%20segmentation.%20The%20source%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/Hhaizee/ELE-SAM.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Segment%2520Anything%2520Model%2520for%2520Power%2520Transmission%2520Corridor%2520Hazard%2520Segmentation%26entry.906535625%3DHang%2520Chen%2520and%2520Maoyuan%2520Ye%2520and%2520Peng%2520Yang%2520and%2520Haibin%2520He%2520and%2520Juhua%2520Liu%2520and%2520Bo%2520Du%26entry.1292438233%3DPower%2520transmission%2520corridor%2520hazard%2520segmentation%2520%2528PTCHS%2529%2520aims%2520to%2520separate%2520transmission%2520equipment%2520and%2520surrounding%2520hazards%2520from%2520complex%2520background%252C%2520conveying%2520great%2520significance%2520to%2520maintaining%2520electric%2520power%2520transmission%2520safety.%2520Recently%252C%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520emerged%2520as%2520a%2520foundational%2520vision%2520model%2520and%2520pushed%2520the%2520boundaries%2520of%2520segmentation%2520tasks.%2520However%252C%2520SAM%2520struggles%2520to%2520deal%2520with%2520the%2520target%2520objects%2520in%2520complex%2520transmission%2520corridor%2520scenario%252C%2520especially%2520those%2520with%2520fine%2520structure.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ELE-SAM%252C%2520adapting%2520SAM%2520for%2520the%2520PTCHS%2520task.%2520Technically%252C%2520we%2520develop%2520a%2520Context-Aware%2520Prompt%2520Adapter%2520to%2520achieve%2520better%2520prompt%2520tokens%2520via%2520incorporating%2520global-local%2520features%2520and%2520focusing%2520more%2520on%2520key%2520regions.%2520Subsequently%252C%2520to%2520tackle%2520the%2520hazard%2520objects%2520with%2520fine%2520structure%2520in%2520complex%2520background%252C%2520we%2520design%2520a%2520High-Fidelity%2520Mask%2520Decoder%2520by%2520leveraging%2520multi-granularity%2520mask%2520features%2520and%2520then%2520scaling%2520them%2520to%2520a%2520higher%2520resolution.%2520Moreover%252C%2520to%2520train%2520ELE-SAM%2520and%2520advance%2520this%2520field%252C%2520we%2520construct%2520the%2520ELE-40K%2520benchmark%252C%2520the%2520first%2520large-scale%2520and%2520real-world%2520dataset%2520for%2520PTCHS%2520including%252044%252C094%2520image-mask%2520pairs.%2520Experimental%2520results%2520for%2520ELE-40K%2520demonstrate%2520the%2520superior%2520performance%2520that%2520ELE-SAM%2520outperforms%2520the%2520baseline%2520model%2520with%2520the%2520average%252016.8%2525%2520mIoU%2520and%252020.6%2525%2520mBIoU%2520performance%2520improvement.%2520Moreover%252C%2520compared%2520with%2520the%2520state-of-the-art%2520method%2520on%2520HQSeg-44K%252C%2520the%2520average%25202.9%2525%2520mIoU%2520and%25203.8%2525%2520mBIoU%2520absolute%2520improvements%2520further%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520high-quality%2520generic%2520object%2520segmentation.%2520The%2520source%2520code%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/Hhaizee/ELE-SAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Segment%20Anything%20Model%20for%20Power%20Transmission%20Corridor%20Hazard%20Segmentation&entry.906535625=Hang%20Chen%20and%20Maoyuan%20Ye%20and%20Peng%20Yang%20and%20Haibin%20He%20and%20Juhua%20Liu%20and%20Bo%20Du&entry.1292438233=Power%20transmission%20corridor%20hazard%20segmentation%20%28PTCHS%29%20aims%20to%20separate%20transmission%20equipment%20and%20surrounding%20hazards%20from%20complex%20background%2C%20conveying%20great%20significance%20to%20maintaining%20electric%20power%20transmission%20safety.%20Recently%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20emerged%20as%20a%20foundational%20vision%20model%20and%20pushed%20the%20boundaries%20of%20segmentation%20tasks.%20However%2C%20SAM%20struggles%20to%20deal%20with%20the%20target%20objects%20in%20complex%20transmission%20corridor%20scenario%2C%20especially%20those%20with%20fine%20structure.%20In%20this%20paper%2C%20we%20propose%20ELE-SAM%2C%20adapting%20SAM%20for%20the%20PTCHS%20task.%20Technically%2C%20we%20develop%20a%20Context-Aware%20Prompt%20Adapter%20to%20achieve%20better%20prompt%20tokens%20via%20incorporating%20global-local%20features%20and%20focusing%20more%20on%20key%20regions.%20Subsequently%2C%20to%20tackle%20the%20hazard%20objects%20with%20fine%20structure%20in%20complex%20background%2C%20we%20design%20a%20High-Fidelity%20Mask%20Decoder%20by%20leveraging%20multi-granularity%20mask%20features%20and%20then%20scaling%20them%20to%20a%20higher%20resolution.%20Moreover%2C%20to%20train%20ELE-SAM%20and%20advance%20this%20field%2C%20we%20construct%20the%20ELE-40K%20benchmark%2C%20the%20first%20large-scale%20and%20real-world%20dataset%20for%20PTCHS%20including%2044%2C094%20image-mask%20pairs.%20Experimental%20results%20for%20ELE-40K%20demonstrate%20the%20superior%20performance%20that%20ELE-SAM%20outperforms%20the%20baseline%20model%20with%20the%20average%2016.8%25%20mIoU%20and%2020.6%25%20mBIoU%20performance%20improvement.%20Moreover%2C%20compared%20with%20the%20state-of-the-art%20method%20on%20HQSeg-44K%2C%20the%20average%202.9%25%20mIoU%20and%203.8%25%20mBIoU%20absolute%20improvements%20further%20validate%20the%20effectiveness%20of%20our%20method%20on%20high-quality%20generic%20object%20segmentation.%20The%20source%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/Hhaizee/ELE-SAM.&entry.1838667208=http%3A//arxiv.org/abs/2505.22105v2&entry.124074799=Read"},
{"title": "Towards Consistent and Controllable Image Synthesis for Face Editing", "author": "Mengting Wei and Tuomas Varanka and Yante Li and Xingxun Jiang and Huai-Qian Khor and Guoying Zhao", "abstract": "Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models.", "link": "http://arxiv.org/abs/2502.02465v3", "date": "2025-11-26", "relevancy": 2.479, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6263}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Consistent%20and%20Controllable%20Image%20Synthesis%20for%20Face%20Editing&body=Title%3A%20Towards%20Consistent%20and%20Controllable%20Image%20Synthesis%20for%20Face%20Editing%0AAuthor%3A%20Mengting%20Wei%20and%20Tuomas%20Varanka%20and%20Yante%20Li%20and%20Xingxun%20Jiang%20and%20Huai-Qian%20Khor%20and%20Guoying%20Zhao%0AAbstract%3A%20Face%20editing%20methods%2C%20essential%20for%20tasks%20like%20virtual%20avatars%2C%20digital%20human%20synthesis%20and%20identity%20preservation%2C%20have%20traditionally%20been%20built%20upon%20GAN-based%20techniques%2C%20while%20recent%20focus%20has%20shifted%20to%20diffusion-based%20models%20due%20to%20their%20success%20in%20image%20reconstruction.%20However%2C%20diffusion%20models%20still%20face%20challenges%20in%20controlling%20specific%20attributes%20and%20preserving%20the%20consistency%20of%20other%20unchanged%20attributes%20especially%20the%20identity%20characteristics.%20To%20address%20these%20issues%20and%20facilitate%20more%20convenient%20editing%20of%20face%20images%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20the%20power%20of%20Stable-Diffusion%20%28SD%29%20models%20and%20crude%203D%20face%20models%20to%20control%20the%20lighting%2C%20facial%20expression%20and%20head%20pose%20of%20a%20portrait%20photo.%20We%20observe%20that%20this%20task%20essentially%20involves%20the%20combinations%20of%20target%20background%2C%20identity%20and%20face%20attributes%20aimed%20to%20edit.%20We%20strive%20to%20sufficiently%20disentangle%20the%20control%20of%20these%20factors%20to%20enable%20consistency%20of%20face%20editing.%20Specifically%2C%20our%20method%2C%20coined%20as%20RigFace%2C%20contains%3A%201%29%20A%20Spatial%20Attribute%20Encoder%20that%20provides%20presise%20and%20decoupled%20conditions%20of%20background%2C%20pose%2C%20expression%20and%20lighting%3B%202%29%20A%20high-consistency%20FaceFusion%20method%20that%20transfers%20identity%20features%20from%20the%20Identity%20Encoder%20to%20the%20denoising%20UNet%20of%20a%20pre-trained%20SD%20model%3B%203%29%20An%20Attribute%20Rigger%20that%20injects%20those%20conditions%20into%20the%20denoising%20UNet.%20Our%20model%20achieves%20comparable%20or%20even%20superior%20performance%20in%20both%20identity%20preservation%20and%20photorealism%20compared%20to%20existing%20face%20editing%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2502.02465v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Consistent%2520and%2520Controllable%2520Image%2520Synthesis%2520for%2520Face%2520Editing%26entry.906535625%3DMengting%2520Wei%2520and%2520Tuomas%2520Varanka%2520and%2520Yante%2520Li%2520and%2520Xingxun%2520Jiang%2520and%2520Huai-Qian%2520Khor%2520and%2520Guoying%2520Zhao%26entry.1292438233%3DFace%2520editing%2520methods%252C%2520essential%2520for%2520tasks%2520like%2520virtual%2520avatars%252C%2520digital%2520human%2520synthesis%2520and%2520identity%2520preservation%252C%2520have%2520traditionally%2520been%2520built%2520upon%2520GAN-based%2520techniques%252C%2520while%2520recent%2520focus%2520has%2520shifted%2520to%2520diffusion-based%2520models%2520due%2520to%2520their%2520success%2520in%2520image%2520reconstruction.%2520However%252C%2520diffusion%2520models%2520still%2520face%2520challenges%2520in%2520controlling%2520specific%2520attributes%2520and%2520preserving%2520the%2520consistency%2520of%2520other%2520unchanged%2520attributes%2520especially%2520the%2520identity%2520characteristics.%2520To%2520address%2520these%2520issues%2520and%2520facilitate%2520more%2520convenient%2520editing%2520of%2520face%2520images%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520leverages%2520the%2520power%2520of%2520Stable-Diffusion%2520%2528SD%2529%2520models%2520and%2520crude%25203D%2520face%2520models%2520to%2520control%2520the%2520lighting%252C%2520facial%2520expression%2520and%2520head%2520pose%2520of%2520a%2520portrait%2520photo.%2520We%2520observe%2520that%2520this%2520task%2520essentially%2520involves%2520the%2520combinations%2520of%2520target%2520background%252C%2520identity%2520and%2520face%2520attributes%2520aimed%2520to%2520edit.%2520We%2520strive%2520to%2520sufficiently%2520disentangle%2520the%2520control%2520of%2520these%2520factors%2520to%2520enable%2520consistency%2520of%2520face%2520editing.%2520Specifically%252C%2520our%2520method%252C%2520coined%2520as%2520RigFace%252C%2520contains%253A%25201%2529%2520A%2520Spatial%2520Attribute%2520Encoder%2520that%2520provides%2520presise%2520and%2520decoupled%2520conditions%2520of%2520background%252C%2520pose%252C%2520expression%2520and%2520lighting%253B%25202%2529%2520A%2520high-consistency%2520FaceFusion%2520method%2520that%2520transfers%2520identity%2520features%2520from%2520the%2520Identity%2520Encoder%2520to%2520the%2520denoising%2520UNet%2520of%2520a%2520pre-trained%2520SD%2520model%253B%25203%2529%2520An%2520Attribute%2520Rigger%2520that%2520injects%2520those%2520conditions%2520into%2520the%2520denoising%2520UNet.%2520Our%2520model%2520achieves%2520comparable%2520or%2520even%2520superior%2520performance%2520in%2520both%2520identity%2520preservation%2520and%2520photorealism%2520compared%2520to%2520existing%2520face%2520editing%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02465v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Consistent%20and%20Controllable%20Image%20Synthesis%20for%20Face%20Editing&entry.906535625=Mengting%20Wei%20and%20Tuomas%20Varanka%20and%20Yante%20Li%20and%20Xingxun%20Jiang%20and%20Huai-Qian%20Khor%20and%20Guoying%20Zhao&entry.1292438233=Face%20editing%20methods%2C%20essential%20for%20tasks%20like%20virtual%20avatars%2C%20digital%20human%20synthesis%20and%20identity%20preservation%2C%20have%20traditionally%20been%20built%20upon%20GAN-based%20techniques%2C%20while%20recent%20focus%20has%20shifted%20to%20diffusion-based%20models%20due%20to%20their%20success%20in%20image%20reconstruction.%20However%2C%20diffusion%20models%20still%20face%20challenges%20in%20controlling%20specific%20attributes%20and%20preserving%20the%20consistency%20of%20other%20unchanged%20attributes%20especially%20the%20identity%20characteristics.%20To%20address%20these%20issues%20and%20facilitate%20more%20convenient%20editing%20of%20face%20images%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20the%20power%20of%20Stable-Diffusion%20%28SD%29%20models%20and%20crude%203D%20face%20models%20to%20control%20the%20lighting%2C%20facial%20expression%20and%20head%20pose%20of%20a%20portrait%20photo.%20We%20observe%20that%20this%20task%20essentially%20involves%20the%20combinations%20of%20target%20background%2C%20identity%20and%20face%20attributes%20aimed%20to%20edit.%20We%20strive%20to%20sufficiently%20disentangle%20the%20control%20of%20these%20factors%20to%20enable%20consistency%20of%20face%20editing.%20Specifically%2C%20our%20method%2C%20coined%20as%20RigFace%2C%20contains%3A%201%29%20A%20Spatial%20Attribute%20Encoder%20that%20provides%20presise%20and%20decoupled%20conditions%20of%20background%2C%20pose%2C%20expression%20and%20lighting%3B%202%29%20A%20high-consistency%20FaceFusion%20method%20that%20transfers%20identity%20features%20from%20the%20Identity%20Encoder%20to%20the%20denoising%20UNet%20of%20a%20pre-trained%20SD%20model%3B%203%29%20An%20Attribute%20Rigger%20that%20injects%20those%20conditions%20into%20the%20denoising%20UNet.%20Our%20model%20achieves%20comparable%20or%20even%20superior%20performance%20in%20both%20identity%20preservation%20and%20photorealism%20compared%20to%20existing%20face%20editing%20models.&entry.1838667208=http%3A//arxiv.org/abs/2502.02465v3&entry.124074799=Read"},
{"title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training", "author": "Haotian Xue and Qi Chen and Zhonghao Wang and Xun Huang and Eli Shechtman and Jinrong Xie and Yongxin Chen", "abstract": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.", "link": "http://arxiv.org/abs/2511.21592v1", "date": "2025-11-26", "relevancy": 2.4767, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGAN%3A%20Improving%20Motion%20Quality%20in%20Video%20Diffusion%20via%20Few-Step%20Motion%20Adversarial%20Post-Training&body=Title%3A%20MoGAN%3A%20Improving%20Motion%20Quality%20in%20Video%20Diffusion%20via%20Few-Step%20Motion%20Adversarial%20Post-Training%0AAuthor%3A%20Haotian%20Xue%20and%20Qi%20Chen%20and%20Zhonghao%20Wang%20and%20Xun%20Huang%20and%20Eli%20Shechtman%20and%20Jinrong%20Xie%20and%20Yongxin%20Chen%0AAbstract%3A%20Video%20diffusion%20models%20achieve%20strong%20frame-level%20fidelity%20but%20still%20struggle%20with%20motion%20coherence%2C%20dynamics%20and%20realism%2C%20often%20producing%20jitter%2C%20ghosting%2C%20or%20implausible%20dynamics.%20A%20key%20limitation%20is%20that%20the%20standard%20denoising%20MSE%20objective%20provides%20no%20direct%20supervision%20on%20temporal%20consistency%2C%20allowing%20models%20to%20achieve%20low%20loss%20while%20still%20generating%20poor%20motion.%20We%20propose%20MoGAN%2C%20a%20motion-centric%20post-training%20framework%20that%20improves%20motion%20realism%20without%20reward%20models%20or%20human%20preference%20data.%20Built%20atop%20a%203-step%20distilled%20video%20diffusion%20model%2C%20we%20train%20a%20DiT-based%20optical-flow%20discriminator%20to%20differentiate%20real%20from%20generated%20motion%2C%20combined%20with%20a%20distribution-matching%20regularizer%20to%20preserve%20visual%20fidelity.%20With%20experiments%20on%20Wan2.1-T2V-1.3B%2C%20MoGAN%20substantially%20improves%20motion%20quality%20across%20benchmarks.%20On%20VBench%2C%20MoGAN%20boosts%20motion%20score%20by%20%2B7.3%25%20over%20the%2050-step%20teacher%20and%20%2B13.3%25%20over%20the%203-step%20DMD%20model.%20On%20VideoJAM-Bench%2C%20MoGAN%20improves%20motion%20score%20by%20%2B7.4%25%20over%20the%20teacher%20and%20%2B8.8%25%20over%20DMD%2C%20while%20maintaining%20comparable%20or%20even%20better%20aesthetic%20and%20image-quality%20scores.%20A%20human%20study%20further%20confirms%20that%20MoGAN%20is%20preferred%20for%20motion%20quality%20%2852%25%20vs.%2038%25%20for%20the%20teacher%3B%2056%25%20vs.%2029%25%20for%20DMD%29.%20Overall%2C%20MoGAN%20delivers%20significantly%20more%20realistic%20motion%20without%20sacrificing%20visual%20fidelity%20or%20efficiency%2C%20offering%20a%20practical%20path%20toward%20fast%2C%20high-quality%20video%20generation.%20Project%20webpage%20is%3A%20https%3A//xavihart.github.io/mogan.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGAN%253A%2520Improving%2520Motion%2520Quality%2520in%2520Video%2520Diffusion%2520via%2520Few-Step%2520Motion%2520Adversarial%2520Post-Training%26entry.906535625%3DHaotian%2520Xue%2520and%2520Qi%2520Chen%2520and%2520Zhonghao%2520Wang%2520and%2520Xun%2520Huang%2520and%2520Eli%2520Shechtman%2520and%2520Jinrong%2520Xie%2520and%2520Yongxin%2520Chen%26entry.1292438233%3DVideo%2520diffusion%2520models%2520achieve%2520strong%2520frame-level%2520fidelity%2520but%2520still%2520struggle%2520with%2520motion%2520coherence%252C%2520dynamics%2520and%2520realism%252C%2520often%2520producing%2520jitter%252C%2520ghosting%252C%2520or%2520implausible%2520dynamics.%2520A%2520key%2520limitation%2520is%2520that%2520the%2520standard%2520denoising%2520MSE%2520objective%2520provides%2520no%2520direct%2520supervision%2520on%2520temporal%2520consistency%252C%2520allowing%2520models%2520to%2520achieve%2520low%2520loss%2520while%2520still%2520generating%2520poor%2520motion.%2520We%2520propose%2520MoGAN%252C%2520a%2520motion-centric%2520post-training%2520framework%2520that%2520improves%2520motion%2520realism%2520without%2520reward%2520models%2520or%2520human%2520preference%2520data.%2520Built%2520atop%2520a%25203-step%2520distilled%2520video%2520diffusion%2520model%252C%2520we%2520train%2520a%2520DiT-based%2520optical-flow%2520discriminator%2520to%2520differentiate%2520real%2520from%2520generated%2520motion%252C%2520combined%2520with%2520a%2520distribution-matching%2520regularizer%2520to%2520preserve%2520visual%2520fidelity.%2520With%2520experiments%2520on%2520Wan2.1-T2V-1.3B%252C%2520MoGAN%2520substantially%2520improves%2520motion%2520quality%2520across%2520benchmarks.%2520On%2520VBench%252C%2520MoGAN%2520boosts%2520motion%2520score%2520by%2520%252B7.3%2525%2520over%2520the%252050-step%2520teacher%2520and%2520%252B13.3%2525%2520over%2520the%25203-step%2520DMD%2520model.%2520On%2520VideoJAM-Bench%252C%2520MoGAN%2520improves%2520motion%2520score%2520by%2520%252B7.4%2525%2520over%2520the%2520teacher%2520and%2520%252B8.8%2525%2520over%2520DMD%252C%2520while%2520maintaining%2520comparable%2520or%2520even%2520better%2520aesthetic%2520and%2520image-quality%2520scores.%2520A%2520human%2520study%2520further%2520confirms%2520that%2520MoGAN%2520is%2520preferred%2520for%2520motion%2520quality%2520%252852%2525%2520vs.%252038%2525%2520for%2520the%2520teacher%253B%252056%2525%2520vs.%252029%2525%2520for%2520DMD%2529.%2520Overall%252C%2520MoGAN%2520delivers%2520significantly%2520more%2520realistic%2520motion%2520without%2520sacrificing%2520visual%2520fidelity%2520or%2520efficiency%252C%2520offering%2520a%2520practical%2520path%2520toward%2520fast%252C%2520high-quality%2520video%2520generation.%2520Project%2520webpage%2520is%253A%2520https%253A//xavihart.github.io/mogan.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGAN%3A%20Improving%20Motion%20Quality%20in%20Video%20Diffusion%20via%20Few-Step%20Motion%20Adversarial%20Post-Training&entry.906535625=Haotian%20Xue%20and%20Qi%20Chen%20and%20Zhonghao%20Wang%20and%20Xun%20Huang%20and%20Eli%20Shechtman%20and%20Jinrong%20Xie%20and%20Yongxin%20Chen&entry.1292438233=Video%20diffusion%20models%20achieve%20strong%20frame-level%20fidelity%20but%20still%20struggle%20with%20motion%20coherence%2C%20dynamics%20and%20realism%2C%20often%20producing%20jitter%2C%20ghosting%2C%20or%20implausible%20dynamics.%20A%20key%20limitation%20is%20that%20the%20standard%20denoising%20MSE%20objective%20provides%20no%20direct%20supervision%20on%20temporal%20consistency%2C%20allowing%20models%20to%20achieve%20low%20loss%20while%20still%20generating%20poor%20motion.%20We%20propose%20MoGAN%2C%20a%20motion-centric%20post-training%20framework%20that%20improves%20motion%20realism%20without%20reward%20models%20or%20human%20preference%20data.%20Built%20atop%20a%203-step%20distilled%20video%20diffusion%20model%2C%20we%20train%20a%20DiT-based%20optical-flow%20discriminator%20to%20differentiate%20real%20from%20generated%20motion%2C%20combined%20with%20a%20distribution-matching%20regularizer%20to%20preserve%20visual%20fidelity.%20With%20experiments%20on%20Wan2.1-T2V-1.3B%2C%20MoGAN%20substantially%20improves%20motion%20quality%20across%20benchmarks.%20On%20VBench%2C%20MoGAN%20boosts%20motion%20score%20by%20%2B7.3%25%20over%20the%2050-step%20teacher%20and%20%2B13.3%25%20over%20the%203-step%20DMD%20model.%20On%20VideoJAM-Bench%2C%20MoGAN%20improves%20motion%20score%20by%20%2B7.4%25%20over%20the%20teacher%20and%20%2B8.8%25%20over%20DMD%2C%20while%20maintaining%20comparable%20or%20even%20better%20aesthetic%20and%20image-quality%20scores.%20A%20human%20study%20further%20confirms%20that%20MoGAN%20is%20preferred%20for%20motion%20quality%20%2852%25%20vs.%2038%25%20for%20the%20teacher%3B%2056%25%20vs.%2029%25%20for%20DMD%29.%20Overall%2C%20MoGAN%20delivers%20significantly%20more%20realistic%20motion%20without%20sacrificing%20visual%20fidelity%20or%20efficiency%2C%20offering%20a%20practical%20path%20toward%20fast%2C%20high-quality%20video%20generation.%20Project%20webpage%20is%3A%20https%3A//xavihart.github.io/mogan.&entry.1838667208=http%3A//arxiv.org/abs/2511.21592v1&entry.124074799=Read"},
{"title": "LaGen: Towards Autoregressive LiDAR Scene Generation", "author": "Sizhuo Zhou and Xiaosong Jia and Fanrui Zhang and Junjie Li and Juyong Zhang and Yukang Feng and Jianwen Sun and Songbur Wong and Junqi You and Junchi Yan", "abstract": "Generative world models for autonomous driving (AD) have become a trending topic. Unlike the widely studied image modality, in this work we explore generative world models for LiDAR data. Existing generation methods for LiDAR data only support single frame generation, while existing prediction approaches require multiple frames of historical input and can only deterministically predict multiple frames at once, lacking interactivity. Both paradigms fail to support long-horizon interactive generation. To this end, we introduce LaGen, which to the best of our knowledge is the first framework capable of frame-by-frame autoregressive generation of long-horizon LiDAR scenes. LaGen is able to take a single-frame LiDAR input as a starting point and effectively utilize bounding box information as conditions to generate high-fidelity 4D scene point clouds. In addition, we introduce a scene decoupling estimation module to enhance the model's interactive generation capability for object-level content, as well as a noise modulation module to mitigate error accumulation during long-horizon generation. We construct a protocol based on nuScenes for evaluating long-horizon LiDAR scene generation. Experimental results comprehensively demonstrate LaGen outperforms state-of-the-art LiDAR generation and prediction models, especially on the later frames.", "link": "http://arxiv.org/abs/2511.21256v1", "date": "2025-11-26", "relevancy": 2.4427, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6312}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6242}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaGen%3A%20Towards%20Autoregressive%20LiDAR%20Scene%20Generation&body=Title%3A%20LaGen%3A%20Towards%20Autoregressive%20LiDAR%20Scene%20Generation%0AAuthor%3A%20Sizhuo%20Zhou%20and%20Xiaosong%20Jia%20and%20Fanrui%20Zhang%20and%20Junjie%20Li%20and%20Juyong%20Zhang%20and%20Yukang%20Feng%20and%20Jianwen%20Sun%20and%20Songbur%20Wong%20and%20Junqi%20You%20and%20Junchi%20Yan%0AAbstract%3A%20Generative%20world%20models%20for%20autonomous%20driving%20%28AD%29%20have%20become%20a%20trending%20topic.%20Unlike%20the%20widely%20studied%20image%20modality%2C%20in%20this%20work%20we%20explore%20generative%20world%20models%20for%20LiDAR%20data.%20Existing%20generation%20methods%20for%20LiDAR%20data%20only%20support%20single%20frame%20generation%2C%20while%20existing%20prediction%20approaches%20require%20multiple%20frames%20of%20historical%20input%20and%20can%20only%20deterministically%20predict%20multiple%20frames%20at%20once%2C%20lacking%20interactivity.%20Both%20paradigms%20fail%20to%20support%20long-horizon%20interactive%20generation.%20To%20this%20end%2C%20we%20introduce%20LaGen%2C%20which%20to%20the%20best%20of%20our%20knowledge%20is%20the%20first%20framework%20capable%20of%20frame-by-frame%20autoregressive%20generation%20of%20long-horizon%20LiDAR%20scenes.%20LaGen%20is%20able%20to%20take%20a%20single-frame%20LiDAR%20input%20as%20a%20starting%20point%20and%20effectively%20utilize%20bounding%20box%20information%20as%20conditions%20to%20generate%20high-fidelity%204D%20scene%20point%20clouds.%20In%20addition%2C%20we%20introduce%20a%20scene%20decoupling%20estimation%20module%20to%20enhance%20the%20model%27s%20interactive%20generation%20capability%20for%20object-level%20content%2C%20as%20well%20as%20a%20noise%20modulation%20module%20to%20mitigate%20error%20accumulation%20during%20long-horizon%20generation.%20We%20construct%20a%20protocol%20based%20on%20nuScenes%20for%20evaluating%20long-horizon%20LiDAR%20scene%20generation.%20Experimental%20results%20comprehensively%20demonstrate%20LaGen%20outperforms%20state-of-the-art%20LiDAR%20generation%20and%20prediction%20models%2C%20especially%20on%20the%20later%20frames.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaGen%253A%2520Towards%2520Autoregressive%2520LiDAR%2520Scene%2520Generation%26entry.906535625%3DSizhuo%2520Zhou%2520and%2520Xiaosong%2520Jia%2520and%2520Fanrui%2520Zhang%2520and%2520Junjie%2520Li%2520and%2520Juyong%2520Zhang%2520and%2520Yukang%2520Feng%2520and%2520Jianwen%2520Sun%2520and%2520Songbur%2520Wong%2520and%2520Junqi%2520You%2520and%2520Junchi%2520Yan%26entry.1292438233%3DGenerative%2520world%2520models%2520for%2520autonomous%2520driving%2520%2528AD%2529%2520have%2520become%2520a%2520trending%2520topic.%2520Unlike%2520the%2520widely%2520studied%2520image%2520modality%252C%2520in%2520this%2520work%2520we%2520explore%2520generative%2520world%2520models%2520for%2520LiDAR%2520data.%2520Existing%2520generation%2520methods%2520for%2520LiDAR%2520data%2520only%2520support%2520single%2520frame%2520generation%252C%2520while%2520existing%2520prediction%2520approaches%2520require%2520multiple%2520frames%2520of%2520historical%2520input%2520and%2520can%2520only%2520deterministically%2520predict%2520multiple%2520frames%2520at%2520once%252C%2520lacking%2520interactivity.%2520Both%2520paradigms%2520fail%2520to%2520support%2520long-horizon%2520interactive%2520generation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520LaGen%252C%2520which%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520is%2520the%2520first%2520framework%2520capable%2520of%2520frame-by-frame%2520autoregressive%2520generation%2520of%2520long-horizon%2520LiDAR%2520scenes.%2520LaGen%2520is%2520able%2520to%2520take%2520a%2520single-frame%2520LiDAR%2520input%2520as%2520a%2520starting%2520point%2520and%2520effectively%2520utilize%2520bounding%2520box%2520information%2520as%2520conditions%2520to%2520generate%2520high-fidelity%25204D%2520scene%2520point%2520clouds.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520scene%2520decoupling%2520estimation%2520module%2520to%2520enhance%2520the%2520model%2527s%2520interactive%2520generation%2520capability%2520for%2520object-level%2520content%252C%2520as%2520well%2520as%2520a%2520noise%2520modulation%2520module%2520to%2520mitigate%2520error%2520accumulation%2520during%2520long-horizon%2520generation.%2520We%2520construct%2520a%2520protocol%2520based%2520on%2520nuScenes%2520for%2520evaluating%2520long-horizon%2520LiDAR%2520scene%2520generation.%2520Experimental%2520results%2520comprehensively%2520demonstrate%2520LaGen%2520outperforms%2520state-of-the-art%2520LiDAR%2520generation%2520and%2520prediction%2520models%252C%2520especially%2520on%2520the%2520later%2520frames.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaGen%3A%20Towards%20Autoregressive%20LiDAR%20Scene%20Generation&entry.906535625=Sizhuo%20Zhou%20and%20Xiaosong%20Jia%20and%20Fanrui%20Zhang%20and%20Junjie%20Li%20and%20Juyong%20Zhang%20and%20Yukang%20Feng%20and%20Jianwen%20Sun%20and%20Songbur%20Wong%20and%20Junqi%20You%20and%20Junchi%20Yan&entry.1292438233=Generative%20world%20models%20for%20autonomous%20driving%20%28AD%29%20have%20become%20a%20trending%20topic.%20Unlike%20the%20widely%20studied%20image%20modality%2C%20in%20this%20work%20we%20explore%20generative%20world%20models%20for%20LiDAR%20data.%20Existing%20generation%20methods%20for%20LiDAR%20data%20only%20support%20single%20frame%20generation%2C%20while%20existing%20prediction%20approaches%20require%20multiple%20frames%20of%20historical%20input%20and%20can%20only%20deterministically%20predict%20multiple%20frames%20at%20once%2C%20lacking%20interactivity.%20Both%20paradigms%20fail%20to%20support%20long-horizon%20interactive%20generation.%20To%20this%20end%2C%20we%20introduce%20LaGen%2C%20which%20to%20the%20best%20of%20our%20knowledge%20is%20the%20first%20framework%20capable%20of%20frame-by-frame%20autoregressive%20generation%20of%20long-horizon%20LiDAR%20scenes.%20LaGen%20is%20able%20to%20take%20a%20single-frame%20LiDAR%20input%20as%20a%20starting%20point%20and%20effectively%20utilize%20bounding%20box%20information%20as%20conditions%20to%20generate%20high-fidelity%204D%20scene%20point%20clouds.%20In%20addition%2C%20we%20introduce%20a%20scene%20decoupling%20estimation%20module%20to%20enhance%20the%20model%27s%20interactive%20generation%20capability%20for%20object-level%20content%2C%20as%20well%20as%20a%20noise%20modulation%20module%20to%20mitigate%20error%20accumulation%20during%20long-horizon%20generation.%20We%20construct%20a%20protocol%20based%20on%20nuScenes%20for%20evaluating%20long-horizon%20LiDAR%20scene%20generation.%20Experimental%20results%20comprehensively%20demonstrate%20LaGen%20outperforms%20state-of-the-art%20LiDAR%20generation%20and%20prediction%20models%2C%20especially%20on%20the%20later%20frames.&entry.1838667208=http%3A//arxiv.org/abs/2511.21256v1&entry.124074799=Read"},
{"title": "A decoupled alignment kernel for peptide membrane permeability predictions", "author": "Ali Amirahmadi and G\u00f6k\u00e7e Geylan and Leonardo De Maria and Farzaneh Etminani and Mattias Ohlsson and Alessandro Tibo", "abstract": "Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.", "link": "http://arxiv.org/abs/2511.21566v1", "date": "2025-11-26", "relevancy": 2.4375, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.503}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4836}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20decoupled%20alignment%20kernel%20for%20peptide%20membrane%20permeability%20predictions&body=Title%3A%20A%20decoupled%20alignment%20kernel%20for%20peptide%20membrane%20permeability%20predictions%0AAuthor%3A%20Ali%20Amirahmadi%20and%20G%C3%B6k%C3%A7e%20Geylan%20and%20Leonardo%20De%20Maria%20and%20Farzaneh%20Etminani%20and%20Mattias%20Ohlsson%20and%20Alessandro%20Tibo%0AAbstract%3A%20Cyclic%20peptides%20are%20promising%20modalities%20for%20targeting%20intracellular%20sites%3B%20however%2C%20cell-membrane%20permeability%20remains%20a%20key%20bottleneck%2C%20exacerbated%20by%20limited%20public%20data%20and%20the%20need%20for%20well-calibrated%20uncertainty.%20Instead%20of%20relying%20on%20data-eager%20complex%20deep%20learning%20architecture%2C%20we%20propose%20a%20monomer-aware%20decoupled%20global%20alignment%20kernel%20%28MD-GAK%29%2C%20which%20couples%20chemically%20meaningful%20residue-residue%20similarity%20with%20sequence%20alignment%20while%20decoupling%20local%20matches%20from%20gap%20penalties.%20MD-GAK%20is%20a%20relatively%20simple%20kernel.%20To%20further%20demonstrate%20the%20robustness%20of%20our%20framework%2C%20we%20also%20introduce%20a%20variant%2C%20PMD-GAK%2C%20which%20incorporates%20a%20triangular%20positional%20prior.%20As%20we%20will%20show%20in%20the%20experimental%20section%2C%20PMD-GAK%20can%20offer%20additional%20advantages%20over%20MD-GAK%2C%20particularly%20in%20reducing%20calibration%20errors.%20Since%20our%20focus%20is%20on%20uncertainty%20estimation%2C%20we%20use%20Gaussian%20Processes%20as%20the%20predictive%20model%2C%20as%20both%20MD-GAK%20and%20PMD-GAK%20can%20be%20directly%20applied%20within%20this%20framework.%20We%20demonstrate%20the%20effectiveness%20of%20our%20methods%20through%20an%20extensive%20set%20of%20experiments%2C%20comparing%20our%20fully%20reproducible%20approach%20against%20state-of-the-art%20models%2C%20and%20show%20that%20it%20outperforms%20them%20across%20all%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520decoupled%2520alignment%2520kernel%2520for%2520peptide%2520membrane%2520permeability%2520predictions%26entry.906535625%3DAli%2520Amirahmadi%2520and%2520G%25C3%25B6k%25C3%25A7e%2520Geylan%2520and%2520Leonardo%2520De%2520Maria%2520and%2520Farzaneh%2520Etminani%2520and%2520Mattias%2520Ohlsson%2520and%2520Alessandro%2520Tibo%26entry.1292438233%3DCyclic%2520peptides%2520are%2520promising%2520modalities%2520for%2520targeting%2520intracellular%2520sites%253B%2520however%252C%2520cell-membrane%2520permeability%2520remains%2520a%2520key%2520bottleneck%252C%2520exacerbated%2520by%2520limited%2520public%2520data%2520and%2520the%2520need%2520for%2520well-calibrated%2520uncertainty.%2520Instead%2520of%2520relying%2520on%2520data-eager%2520complex%2520deep%2520learning%2520architecture%252C%2520we%2520propose%2520a%2520monomer-aware%2520decoupled%2520global%2520alignment%2520kernel%2520%2528MD-GAK%2529%252C%2520which%2520couples%2520chemically%2520meaningful%2520residue-residue%2520similarity%2520with%2520sequence%2520alignment%2520while%2520decoupling%2520local%2520matches%2520from%2520gap%2520penalties.%2520MD-GAK%2520is%2520a%2520relatively%2520simple%2520kernel.%2520To%2520further%2520demonstrate%2520the%2520robustness%2520of%2520our%2520framework%252C%2520we%2520also%2520introduce%2520a%2520variant%252C%2520PMD-GAK%252C%2520which%2520incorporates%2520a%2520triangular%2520positional%2520prior.%2520As%2520we%2520will%2520show%2520in%2520the%2520experimental%2520section%252C%2520PMD-GAK%2520can%2520offer%2520additional%2520advantages%2520over%2520MD-GAK%252C%2520particularly%2520in%2520reducing%2520calibration%2520errors.%2520Since%2520our%2520focus%2520is%2520on%2520uncertainty%2520estimation%252C%2520we%2520use%2520Gaussian%2520Processes%2520as%2520the%2520predictive%2520model%252C%2520as%2520both%2520MD-GAK%2520and%2520PMD-GAK%2520can%2520be%2520directly%2520applied%2520within%2520this%2520framework.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520methods%2520through%2520an%2520extensive%2520set%2520of%2520experiments%252C%2520comparing%2520our%2520fully%2520reproducible%2520approach%2520against%2520state-of-the-art%2520models%252C%2520and%2520show%2520that%2520it%2520outperforms%2520them%2520across%2520all%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20decoupled%20alignment%20kernel%20for%20peptide%20membrane%20permeability%20predictions&entry.906535625=Ali%20Amirahmadi%20and%20G%C3%B6k%C3%A7e%20Geylan%20and%20Leonardo%20De%20Maria%20and%20Farzaneh%20Etminani%20and%20Mattias%20Ohlsson%20and%20Alessandro%20Tibo&entry.1292438233=Cyclic%20peptides%20are%20promising%20modalities%20for%20targeting%20intracellular%20sites%3B%20however%2C%20cell-membrane%20permeability%20remains%20a%20key%20bottleneck%2C%20exacerbated%20by%20limited%20public%20data%20and%20the%20need%20for%20well-calibrated%20uncertainty.%20Instead%20of%20relying%20on%20data-eager%20complex%20deep%20learning%20architecture%2C%20we%20propose%20a%20monomer-aware%20decoupled%20global%20alignment%20kernel%20%28MD-GAK%29%2C%20which%20couples%20chemically%20meaningful%20residue-residue%20similarity%20with%20sequence%20alignment%20while%20decoupling%20local%20matches%20from%20gap%20penalties.%20MD-GAK%20is%20a%20relatively%20simple%20kernel.%20To%20further%20demonstrate%20the%20robustness%20of%20our%20framework%2C%20we%20also%20introduce%20a%20variant%2C%20PMD-GAK%2C%20which%20incorporates%20a%20triangular%20positional%20prior.%20As%20we%20will%20show%20in%20the%20experimental%20section%2C%20PMD-GAK%20can%20offer%20additional%20advantages%20over%20MD-GAK%2C%20particularly%20in%20reducing%20calibration%20errors.%20Since%20our%20focus%20is%20on%20uncertainty%20estimation%2C%20we%20use%20Gaussian%20Processes%20as%20the%20predictive%20model%2C%20as%20both%20MD-GAK%20and%20PMD-GAK%20can%20be%20directly%20applied%20within%20this%20framework.%20We%20demonstrate%20the%20effectiveness%20of%20our%20methods%20through%20an%20extensive%20set%20of%20experiments%2C%20comparing%20our%20fully%20reproducible%20approach%20against%20state-of-the-art%20models%2C%20and%20show%20that%20it%20outperforms%20them%20across%20all%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.21566v1&entry.124074799=Read"},
{"title": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners", "author": "Daniel Herbst and Lea Karbevska and Divyanshu Kumar and Akanksha Ahuja and Fatemeh Gholamzadeh Nasrabadi and Fabrizio Frasca", "abstract": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.", "link": "http://arxiv.org/abs/2511.10234v2", "date": "2025-11-26", "relevancy": 2.4312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners&body=Title%3A%20Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners%0AAuthor%3A%20Daniel%20Herbst%20and%20Lea%20Karbevska%20and%20Divyanshu%20Kumar%20and%20Akanksha%20Ahuja%20and%20Fatemeh%20Gholamzadeh%20Nasrabadi%20and%20Fabrizio%20Frasca%0AAbstract%3A%20While%20promising%2C%20graph%20reasoners%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20lack%20built-in%20invariance%20to%20symmetries%20in%20graph%20representations.%20Operating%20on%20sequential%20graph%20serializations%2C%20LLMs%20can%20produce%20different%20outputs%20under%20node%20reindexing%2C%20edge%20reordering%2C%20or%20formatting%20changes%2C%20raising%20robustness%20concerns.%20We%20systematically%20analyze%20these%20effects%2C%20studying%20how%20fine-tuning%20impacts%20encoding%20sensitivity%20as%20well%20generalization%20on%20unseen%20tasks.%20We%20propose%20a%20principled%20decomposition%20of%20graph%20serializations%20into%20node%20labeling%2C%20edge%20encoding%2C%20and%20syntax%2C%20and%20evaluate%20LLM%20robustness%20to%20variations%20of%20each%20of%20these%20factors%20on%20a%20comprehensive%20benchmarking%20suite.%20We%20also%20contribute%20a%20novel%20set%20of%20spectral%20tasks%20to%20further%20assess%20generalization%20abilities%20of%20fine-tuned%20reasoners.%20Results%20show%20that%20larger%20%28non-fine-tuned%29%20models%20are%20more%20robust.%20Fine-tuning%20reduces%20sensitivity%20to%20node%20relabeling%20but%20may%20increase%20it%20to%20variations%20in%20structure%20and%20format%2C%20while%20it%20does%20not%20consistently%20improve%20performance%20on%20unseen%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Serialization%253A%2520Invariance%2520and%2520Generalization%2520of%2520LLM%2520Graph%2520Reasoners%26entry.906535625%3DDaniel%2520Herbst%2520and%2520Lea%2520Karbevska%2520and%2520Divyanshu%2520Kumar%2520and%2520Akanksha%2520Ahuja%2520and%2520Fatemeh%2520Gholamzadeh%2520Nasrabadi%2520and%2520Fabrizio%2520Frasca%26entry.1292438233%3DWhile%2520promising%252C%2520graph%2520reasoners%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520lack%2520built-in%2520invariance%2520to%2520symmetries%2520in%2520graph%2520representations.%2520Operating%2520on%2520sequential%2520graph%2520serializations%252C%2520LLMs%2520can%2520produce%2520different%2520outputs%2520under%2520node%2520reindexing%252C%2520edge%2520reordering%252C%2520or%2520formatting%2520changes%252C%2520raising%2520robustness%2520concerns.%2520We%2520systematically%2520analyze%2520these%2520effects%252C%2520studying%2520how%2520fine-tuning%2520impacts%2520encoding%2520sensitivity%2520as%2520well%2520generalization%2520on%2520unseen%2520tasks.%2520We%2520propose%2520a%2520principled%2520decomposition%2520of%2520graph%2520serializations%2520into%2520node%2520labeling%252C%2520edge%2520encoding%252C%2520and%2520syntax%252C%2520and%2520evaluate%2520LLM%2520robustness%2520to%2520variations%2520of%2520each%2520of%2520these%2520factors%2520on%2520a%2520comprehensive%2520benchmarking%2520suite.%2520We%2520also%2520contribute%2520a%2520novel%2520set%2520of%2520spectral%2520tasks%2520to%2520further%2520assess%2520generalization%2520abilities%2520of%2520fine-tuned%2520reasoners.%2520Results%2520show%2520that%2520larger%2520%2528non-fine-tuned%2529%2520models%2520are%2520more%2520robust.%2520Fine-tuning%2520reduces%2520sensitivity%2520to%2520node%2520relabeling%2520but%2520may%2520increase%2520it%2520to%2520variations%2520in%2520structure%2520and%2520format%252C%2520while%2520it%2520does%2520not%2520consistently%2520improve%2520performance%2520on%2520unseen%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners&entry.906535625=Daniel%20Herbst%20and%20Lea%20Karbevska%20and%20Divyanshu%20Kumar%20and%20Akanksha%20Ahuja%20and%20Fatemeh%20Gholamzadeh%20Nasrabadi%20and%20Fabrizio%20Frasca&entry.1292438233=While%20promising%2C%20graph%20reasoners%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20lack%20built-in%20invariance%20to%20symmetries%20in%20graph%20representations.%20Operating%20on%20sequential%20graph%20serializations%2C%20LLMs%20can%20produce%20different%20outputs%20under%20node%20reindexing%2C%20edge%20reordering%2C%20or%20formatting%20changes%2C%20raising%20robustness%20concerns.%20We%20systematically%20analyze%20these%20effects%2C%20studying%20how%20fine-tuning%20impacts%20encoding%20sensitivity%20as%20well%20generalization%20on%20unseen%20tasks.%20We%20propose%20a%20principled%20decomposition%20of%20graph%20serializations%20into%20node%20labeling%2C%20edge%20encoding%2C%20and%20syntax%2C%20and%20evaluate%20LLM%20robustness%20to%20variations%20of%20each%20of%20these%20factors%20on%20a%20comprehensive%20benchmarking%20suite.%20We%20also%20contribute%20a%20novel%20set%20of%20spectral%20tasks%20to%20further%20assess%20generalization%20abilities%20of%20fine-tuned%20reasoners.%20Results%20show%20that%20larger%20%28non-fine-tuned%29%20models%20are%20more%20robust.%20Fine-tuning%20reduces%20sensitivity%20to%20node%20relabeling%20but%20may%20increase%20it%20to%20variations%20in%20structure%20and%20format%2C%20while%20it%20does%20not%20consistently%20improve%20performance%20on%20unseen%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.10234v2&entry.124074799=Read"},
{"title": "Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation", "author": "Qian Hong and Cheng Bian and Xiao Zhou and Xiaoyu Li and Yelei Li and Zijing Zeng", "abstract": "Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs transformation accuracy, especially in capturing critical features like ABP peaks. Conventional synchronization methods often rely on strong similarity assumptions or manual tuning, while existing Learning with Noisy Labels (LNL) approaches are ineffective under time-shifted supervision, either discarding excessive data or failing to correct label shifts. To address this challenge, we propose ShiftSyncNet, a meta-learning-based bi-level optimization framework that automatically mitigates performance degradation due to time misalignment. It comprises a transformation network (TransNet) and a time-shift correction network (SyncNet), where SyncNet learns time offsets between training pairs and applies Fourier phase shifts to align supervision signals. Experiments on one real-world industrial dataset and two public datasets show that ShiftSyncNet outperforms strong baselines by 9.4%, 6.0%, and 12.8%, respectively. The results highlight its effectiveness in correcting time shifts, improving label quality, and enhancing transformation accuracy across diverse misalignment scenarios, pointing toward a unified direction for addressing temporal inconsistencies in multimodal physiological transformation.", "link": "http://arxiv.org/abs/2511.21500v1", "date": "2025-11-26", "relevancy": 2.4244, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4883}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Time%3F%20A%20Meta-Learning%20Framework%20for%20Time-Shift-Tolerant%20Physiological%20Signal%20Transformation&body=Title%3A%20Lost%20in%20Time%3F%20A%20Meta-Learning%20Framework%20for%20Time-Shift-Tolerant%20Physiological%20Signal%20Transformation%0AAuthor%3A%20Qian%20Hong%20and%20Cheng%20Bian%20and%20Xiao%20Zhou%20and%20Xiaoyu%20Li%20and%20Yelei%20Li%20and%20Zijing%20Zeng%0AAbstract%3A%20Translating%20non-invasive%20signals%20such%20as%20photoplethysmography%20%28PPG%29%20and%20ballistocardiography%20%28BCG%29%20into%20clinically%20meaningful%20signals%20like%20arterial%20blood%20pressure%20%28ABP%29%20is%20vital%20for%20continuous%2C%20low-cost%20healthcare%20monitoring.%20However%2C%20temporal%20misalignment%20in%20multimodal%20signal%20transformation%20impairs%20transformation%20accuracy%2C%20especially%20in%20capturing%20critical%20features%20like%20ABP%20peaks.%20Conventional%20synchronization%20methods%20often%20rely%20on%20strong%20similarity%20assumptions%20or%20manual%20tuning%2C%20while%20existing%20Learning%20with%20Noisy%20Labels%20%28LNL%29%20approaches%20are%20ineffective%20under%20time-shifted%20supervision%2C%20either%20discarding%20excessive%20data%20or%20failing%20to%20correct%20label%20shifts.%20To%20address%20this%20challenge%2C%20we%20propose%20ShiftSyncNet%2C%20a%20meta-learning-based%20bi-level%20optimization%20framework%20that%20automatically%20mitigates%20performance%20degradation%20due%20to%20time%20misalignment.%20It%20comprises%20a%20transformation%20network%20%28TransNet%29%20and%20a%20time-shift%20correction%20network%20%28SyncNet%29%2C%20where%20SyncNet%20learns%20time%20offsets%20between%20training%20pairs%20and%20applies%20Fourier%20phase%20shifts%20to%20align%20supervision%20signals.%20Experiments%20on%20one%20real-world%20industrial%20dataset%20and%20two%20public%20datasets%20show%20that%20ShiftSyncNet%20outperforms%20strong%20baselines%20by%209.4%25%2C%206.0%25%2C%20and%2012.8%25%2C%20respectively.%20The%20results%20highlight%20its%20effectiveness%20in%20correcting%20time%20shifts%2C%20improving%20label%20quality%2C%20and%20enhancing%20transformation%20accuracy%20across%20diverse%20misalignment%20scenarios%2C%20pointing%20toward%20a%20unified%20direction%20for%20addressing%20temporal%20inconsistencies%20in%20multimodal%20physiological%20transformation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Time%253F%2520A%2520Meta-Learning%2520Framework%2520for%2520Time-Shift-Tolerant%2520Physiological%2520Signal%2520Transformation%26entry.906535625%3DQian%2520Hong%2520and%2520Cheng%2520Bian%2520and%2520Xiao%2520Zhou%2520and%2520Xiaoyu%2520Li%2520and%2520Yelei%2520Li%2520and%2520Zijing%2520Zeng%26entry.1292438233%3DTranslating%2520non-invasive%2520signals%2520such%2520as%2520photoplethysmography%2520%2528PPG%2529%2520and%2520ballistocardiography%2520%2528BCG%2529%2520into%2520clinically%2520meaningful%2520signals%2520like%2520arterial%2520blood%2520pressure%2520%2528ABP%2529%2520is%2520vital%2520for%2520continuous%252C%2520low-cost%2520healthcare%2520monitoring.%2520However%252C%2520temporal%2520misalignment%2520in%2520multimodal%2520signal%2520transformation%2520impairs%2520transformation%2520accuracy%252C%2520especially%2520in%2520capturing%2520critical%2520features%2520like%2520ABP%2520peaks.%2520Conventional%2520synchronization%2520methods%2520often%2520rely%2520on%2520strong%2520similarity%2520assumptions%2520or%2520manual%2520tuning%252C%2520while%2520existing%2520Learning%2520with%2520Noisy%2520Labels%2520%2528LNL%2529%2520approaches%2520are%2520ineffective%2520under%2520time-shifted%2520supervision%252C%2520either%2520discarding%2520excessive%2520data%2520or%2520failing%2520to%2520correct%2520label%2520shifts.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ShiftSyncNet%252C%2520a%2520meta-learning-based%2520bi-level%2520optimization%2520framework%2520that%2520automatically%2520mitigates%2520performance%2520degradation%2520due%2520to%2520time%2520misalignment.%2520It%2520comprises%2520a%2520transformation%2520network%2520%2528TransNet%2529%2520and%2520a%2520time-shift%2520correction%2520network%2520%2528SyncNet%2529%252C%2520where%2520SyncNet%2520learns%2520time%2520offsets%2520between%2520training%2520pairs%2520and%2520applies%2520Fourier%2520phase%2520shifts%2520to%2520align%2520supervision%2520signals.%2520Experiments%2520on%2520one%2520real-world%2520industrial%2520dataset%2520and%2520two%2520public%2520datasets%2520show%2520that%2520ShiftSyncNet%2520outperforms%2520strong%2520baselines%2520by%25209.4%2525%252C%25206.0%2525%252C%2520and%252012.8%2525%252C%2520respectively.%2520The%2520results%2520highlight%2520its%2520effectiveness%2520in%2520correcting%2520time%2520shifts%252C%2520improving%2520label%2520quality%252C%2520and%2520enhancing%2520transformation%2520accuracy%2520across%2520diverse%2520misalignment%2520scenarios%252C%2520pointing%2520toward%2520a%2520unified%2520direction%2520for%2520addressing%2520temporal%2520inconsistencies%2520in%2520multimodal%2520physiological%2520transformation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Time%3F%20A%20Meta-Learning%20Framework%20for%20Time-Shift-Tolerant%20Physiological%20Signal%20Transformation&entry.906535625=Qian%20Hong%20and%20Cheng%20Bian%20and%20Xiao%20Zhou%20and%20Xiaoyu%20Li%20and%20Yelei%20Li%20and%20Zijing%20Zeng&entry.1292438233=Translating%20non-invasive%20signals%20such%20as%20photoplethysmography%20%28PPG%29%20and%20ballistocardiography%20%28BCG%29%20into%20clinically%20meaningful%20signals%20like%20arterial%20blood%20pressure%20%28ABP%29%20is%20vital%20for%20continuous%2C%20low-cost%20healthcare%20monitoring.%20However%2C%20temporal%20misalignment%20in%20multimodal%20signal%20transformation%20impairs%20transformation%20accuracy%2C%20especially%20in%20capturing%20critical%20features%20like%20ABP%20peaks.%20Conventional%20synchronization%20methods%20often%20rely%20on%20strong%20similarity%20assumptions%20or%20manual%20tuning%2C%20while%20existing%20Learning%20with%20Noisy%20Labels%20%28LNL%29%20approaches%20are%20ineffective%20under%20time-shifted%20supervision%2C%20either%20discarding%20excessive%20data%20or%20failing%20to%20correct%20label%20shifts.%20To%20address%20this%20challenge%2C%20we%20propose%20ShiftSyncNet%2C%20a%20meta-learning-based%20bi-level%20optimization%20framework%20that%20automatically%20mitigates%20performance%20degradation%20due%20to%20time%20misalignment.%20It%20comprises%20a%20transformation%20network%20%28TransNet%29%20and%20a%20time-shift%20correction%20network%20%28SyncNet%29%2C%20where%20SyncNet%20learns%20time%20offsets%20between%20training%20pairs%20and%20applies%20Fourier%20phase%20shifts%20to%20align%20supervision%20signals.%20Experiments%20on%20one%20real-world%20industrial%20dataset%20and%20two%20public%20datasets%20show%20that%20ShiftSyncNet%20outperforms%20strong%20baselines%20by%209.4%25%2C%206.0%25%2C%20and%2012.8%25%2C%20respectively.%20The%20results%20highlight%20its%20effectiveness%20in%20correcting%20time%20shifts%2C%20improving%20label%20quality%2C%20and%20enhancing%20transformation%20accuracy%20across%20diverse%20misalignment%20scenarios%2C%20pointing%20toward%20a%20unified%20direction%20for%20addressing%20temporal%20inconsistencies%20in%20multimodal%20physiological%20transformation.&entry.1838667208=http%3A//arxiv.org/abs/2511.21500v1&entry.124074799=Read"},
{"title": "3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization", "author": "Shuhan Xia and Xuannan Liu and Xing Cui and Peipei Li", "abstract": "Recently, partial audio forgery has emerged as a new form of audio manipulation. Attackers selectively modify partial but semantically critical frames while preserving the overall perceptual authenticity, making such forgeries particularly difficult to detect. Existing methods focus on independently detecting whether a single frame is forged, lacking the hierarchical structure to capture both transient and sustained anomalies across different temporal levels. To address these limitations, We identify three key levels relevant to partial audio forgery detection and present T3-Tracer, the first framework that jointly analyzes audio at the frame, segment, and audio levels to comprehensively detect forgery traces. T3-Tracer consists of two complementary core modules: the Frame-Audio Feature Aggregation Module (FA-FAM) and the Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM). FA-FAM is designed to detect the authenticity of each audio frame. It combines both frame-level and audio-level temporal information to detect intra-frame forgery cues and global semantic inconsistencies. To further refine and correct frame detection, we introduce SMDAM to detect forgery boundaries at the segment level. It adopts a dual-branch architecture that jointly models frame features and inter-frame differences across multi-scale temporal windows, effectively identifying abrupt anomalies that appeared on the forged boundaries. Extensive experiments conducted on three challenging datasets demonstrate that our approach achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2511.21237v1", "date": "2025-11-26", "relevancy": 2.4211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4846}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203-Tracer%3A%20A%20Tri-level%20Temporal-Aware%20Framework%20for%20Audio%20Forgery%20Detection%20and%20Localization&body=Title%3A%203-Tracer%3A%20A%20Tri-level%20Temporal-Aware%20Framework%20for%20Audio%20Forgery%20Detection%20and%20Localization%0AAuthor%3A%20Shuhan%20Xia%20and%20Xuannan%20Liu%20and%20Xing%20Cui%20and%20Peipei%20Li%0AAbstract%3A%20Recently%2C%20partial%20audio%20forgery%20has%20emerged%20as%20a%20new%20form%20of%20audio%20manipulation.%20Attackers%20selectively%20modify%20partial%20but%20semantically%20critical%20frames%20while%20preserving%20the%20overall%20perceptual%20authenticity%2C%20making%20such%20forgeries%20particularly%20difficult%20to%20detect.%20Existing%20methods%20focus%20on%20independently%20detecting%20whether%20a%20single%20frame%20is%20forged%2C%20lacking%20the%20hierarchical%20structure%20to%20capture%20both%20transient%20and%20sustained%20anomalies%20across%20different%20temporal%20levels.%20To%20address%20these%20limitations%2C%20We%20identify%20three%20key%20levels%20relevant%20to%20partial%20audio%20forgery%20detection%20and%20present%20T3-Tracer%2C%20the%20first%20framework%20that%20jointly%20analyzes%20audio%20at%20the%20frame%2C%20segment%2C%20and%20audio%20levels%20to%20comprehensively%20detect%20forgery%20traces.%20T3-Tracer%20consists%20of%20two%20complementary%20core%20modules%3A%20the%20Frame-Audio%20Feature%20Aggregation%20Module%20%28FA-FAM%29%20and%20the%20Segment-level%20Multi-Scale%20Discrepancy-Aware%20Module%20%28SMDAM%29.%20FA-FAM%20is%20designed%20to%20detect%20the%20authenticity%20of%20each%20audio%20frame.%20It%20combines%20both%20frame-level%20and%20audio-level%20temporal%20information%20to%20detect%20intra-frame%20forgery%20cues%20and%20global%20semantic%20inconsistencies.%20To%20further%20refine%20and%20correct%20frame%20detection%2C%20we%20introduce%20SMDAM%20to%20detect%20forgery%20boundaries%20at%20the%20segment%20level.%20It%20adopts%20a%20dual-branch%20architecture%20that%20jointly%20models%20frame%20features%20and%20inter-frame%20differences%20across%20multi-scale%20temporal%20windows%2C%20effectively%20identifying%20abrupt%20anomalies%20that%20appeared%20on%20the%20forged%20boundaries.%20Extensive%20experiments%20conducted%20on%20three%20challenging%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3-Tracer%253A%2520A%2520Tri-level%2520Temporal-Aware%2520Framework%2520for%2520Audio%2520Forgery%2520Detection%2520and%2520Localization%26entry.906535625%3DShuhan%2520Xia%2520and%2520Xuannan%2520Liu%2520and%2520Xing%2520Cui%2520and%2520Peipei%2520Li%26entry.1292438233%3DRecently%252C%2520partial%2520audio%2520forgery%2520has%2520emerged%2520as%2520a%2520new%2520form%2520of%2520audio%2520manipulation.%2520Attackers%2520selectively%2520modify%2520partial%2520but%2520semantically%2520critical%2520frames%2520while%2520preserving%2520the%2520overall%2520perceptual%2520authenticity%252C%2520making%2520such%2520forgeries%2520particularly%2520difficult%2520to%2520detect.%2520Existing%2520methods%2520focus%2520on%2520independently%2520detecting%2520whether%2520a%2520single%2520frame%2520is%2520forged%252C%2520lacking%2520the%2520hierarchical%2520structure%2520to%2520capture%2520both%2520transient%2520and%2520sustained%2520anomalies%2520across%2520different%2520temporal%2520levels.%2520To%2520address%2520these%2520limitations%252C%2520We%2520identify%2520three%2520key%2520levels%2520relevant%2520to%2520partial%2520audio%2520forgery%2520detection%2520and%2520present%2520T3-Tracer%252C%2520the%2520first%2520framework%2520that%2520jointly%2520analyzes%2520audio%2520at%2520the%2520frame%252C%2520segment%252C%2520and%2520audio%2520levels%2520to%2520comprehensively%2520detect%2520forgery%2520traces.%2520T3-Tracer%2520consists%2520of%2520two%2520complementary%2520core%2520modules%253A%2520the%2520Frame-Audio%2520Feature%2520Aggregation%2520Module%2520%2528FA-FAM%2529%2520and%2520the%2520Segment-level%2520Multi-Scale%2520Discrepancy-Aware%2520Module%2520%2528SMDAM%2529.%2520FA-FAM%2520is%2520designed%2520to%2520detect%2520the%2520authenticity%2520of%2520each%2520audio%2520frame.%2520It%2520combines%2520both%2520frame-level%2520and%2520audio-level%2520temporal%2520information%2520to%2520detect%2520intra-frame%2520forgery%2520cues%2520and%2520global%2520semantic%2520inconsistencies.%2520To%2520further%2520refine%2520and%2520correct%2520frame%2520detection%252C%2520we%2520introduce%2520SMDAM%2520to%2520detect%2520forgery%2520boundaries%2520at%2520the%2520segment%2520level.%2520It%2520adopts%2520a%2520dual-branch%2520architecture%2520that%2520jointly%2520models%2520frame%2520features%2520and%2520inter-frame%2520differences%2520across%2520multi-scale%2520temporal%2520windows%252C%2520effectively%2520identifying%2520abrupt%2520anomalies%2520that%2520appeared%2520on%2520the%2520forged%2520boundaries.%2520Extensive%2520experiments%2520conducted%2520on%2520three%2520challenging%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3-Tracer%3A%20A%20Tri-level%20Temporal-Aware%20Framework%20for%20Audio%20Forgery%20Detection%20and%20Localization&entry.906535625=Shuhan%20Xia%20and%20Xuannan%20Liu%20and%20Xing%20Cui%20and%20Peipei%20Li&entry.1292438233=Recently%2C%20partial%20audio%20forgery%20has%20emerged%20as%20a%20new%20form%20of%20audio%20manipulation.%20Attackers%20selectively%20modify%20partial%20but%20semantically%20critical%20frames%20while%20preserving%20the%20overall%20perceptual%20authenticity%2C%20making%20such%20forgeries%20particularly%20difficult%20to%20detect.%20Existing%20methods%20focus%20on%20independently%20detecting%20whether%20a%20single%20frame%20is%20forged%2C%20lacking%20the%20hierarchical%20structure%20to%20capture%20both%20transient%20and%20sustained%20anomalies%20across%20different%20temporal%20levels.%20To%20address%20these%20limitations%2C%20We%20identify%20three%20key%20levels%20relevant%20to%20partial%20audio%20forgery%20detection%20and%20present%20T3-Tracer%2C%20the%20first%20framework%20that%20jointly%20analyzes%20audio%20at%20the%20frame%2C%20segment%2C%20and%20audio%20levels%20to%20comprehensively%20detect%20forgery%20traces.%20T3-Tracer%20consists%20of%20two%20complementary%20core%20modules%3A%20the%20Frame-Audio%20Feature%20Aggregation%20Module%20%28FA-FAM%29%20and%20the%20Segment-level%20Multi-Scale%20Discrepancy-Aware%20Module%20%28SMDAM%29.%20FA-FAM%20is%20designed%20to%20detect%20the%20authenticity%20of%20each%20audio%20frame.%20It%20combines%20both%20frame-level%20and%20audio-level%20temporal%20information%20to%20detect%20intra-frame%20forgery%20cues%20and%20global%20semantic%20inconsistencies.%20To%20further%20refine%20and%20correct%20frame%20detection%2C%20we%20introduce%20SMDAM%20to%20detect%20forgery%20boundaries%20at%20the%20segment%20level.%20It%20adopts%20a%20dual-branch%20architecture%20that%20jointly%20models%20frame%20features%20and%20inter-frame%20differences%20across%20multi-scale%20temporal%20windows%2C%20effectively%20identifying%20abrupt%20anomalies%20that%20appeared%20on%20the%20forged%20boundaries.%20Extensive%20experiments%20conducted%20on%20three%20challenging%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.21237v1&entry.124074799=Read"},
{"title": "Characterizing Pattern Matching and Its Limits on Compositional Task Structures", "author": "Hoyeon Chang and Jinho Park and Hanseul Cho and Sohee Yang and Miyoung Ko and Hyeonbin Hwang and Seungpil Won and Dohaeng Lee and Youbin Ahn and Minjoon Seo", "abstract": "Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks. However, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations. To address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant. Then, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism. Our formalism yields predictive and quantitative insights: (1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence. (2) We prove a tight sample complexity bound of learning a two-hop structure by identifying the exponent of the data scaling law for perfect in-domain generalization. Our empirical results align with the theoretical prediction, under 20x parameter scaling and across architectures. (3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability. (4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity. Hence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.", "link": "http://arxiv.org/abs/2505.20278v2", "date": "2025-11-26", "relevancy": 2.4131, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Pattern%20Matching%20and%20Its%20Limits%20on%20Compositional%20Task%20Structures&body=Title%3A%20Characterizing%20Pattern%20Matching%20and%20Its%20Limits%20on%20Compositional%20Task%20Structures%0AAuthor%3A%20Hoyeon%20Chang%20and%20Jinho%20Park%20and%20Hanseul%20Cho%20and%20Sohee%20Yang%20and%20Miyoung%20Ko%20and%20Hyeonbin%20Hwang%20and%20Seungpil%20Won%20and%20Dohaeng%20Lee%20and%20Youbin%20Ahn%20and%20Minjoon%20Seo%0AAbstract%3A%20Despite%20impressive%20capabilities%2C%20LLMs%27%20successes%20often%20rely%20on%20pattern-matching%20behaviors%2C%20yet%20these%20are%20also%20linked%20to%20OOD%20generalization%20failures%20in%20compositional%20tasks.%20However%2C%20behavioral%20studies%20commonly%20employ%20task%20setups%20that%20allow%20multiple%20generalization%20sources%20%28e.g.%2C%20algebraic%20invariances%2C%20structural%20repetition%29%2C%20obscuring%20a%20precise%20and%20testable%20account%20of%20how%20well%20LLMs%20perform%20generalization%20through%20pattern%20matching%20and%20their%20limitations.%20To%20address%20this%20ambiguity%2C%20we%20first%20formalize%20pattern%20matching%20as%20functional%20equivalence%2C%20i.e.%2C%20identifying%20pairs%20of%20subsequences%20of%20inputs%20that%20consistently%20lead%20to%20identical%20results%20when%20the%20rest%20of%20the%20input%20is%20held%20constant.%20Then%2C%20we%20systematically%20study%20how%20decoder-only%20Transformer%20and%20Mamba%20behave%20in%20controlled%20tasks%20with%20compositional%20structures%20that%20isolate%20this%20mechanism.%20Our%20formalism%20yields%20predictive%20and%20quantitative%20insights%3A%20%281%29%20Instance-wise%20success%20of%20pattern%20matching%20is%20well%20predicted%20by%20the%20number%20of%20contexts%20witnessing%20the%20relevant%20functional%20equivalence.%20%282%29%20We%20prove%20a%20tight%20sample%20complexity%20bound%20of%20learning%20a%20two-hop%20structure%20by%20identifying%20the%20exponent%20of%20the%20data%20scaling%20law%20for%20perfect%20in-domain%20generalization.%20Our%20empirical%20results%20align%20with%20the%20theoretical%20prediction%2C%20under%2020x%20parameter%20scaling%20and%20across%20architectures.%20%283%29%20Path%20ambiguity%20is%20a%20structural%20barrier%3A%20when%20a%20variable%20influences%20the%20output%20via%20multiple%20paths%2C%20models%20fail%20to%20form%20unified%20intermediate%20state%20representations%2C%20impairing%20accuracy%20and%20interpretability.%20%284%29%20Chain-of-Thought%20reduces%20data%20requirements%20yet%20does%20not%20resolve%20path%20ambiguity.%20Hence%2C%20we%20provide%20a%20predictive%2C%20falsifiable%20boundary%20for%20pattern%20matching%20and%20a%20foundational%20diagnostic%20for%20disentangling%20mixed%20generalization%20mechanisms.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Pattern%2520Matching%2520and%2520Its%2520Limits%2520on%2520Compositional%2520Task%2520Structures%26entry.906535625%3DHoyeon%2520Chang%2520and%2520Jinho%2520Park%2520and%2520Hanseul%2520Cho%2520and%2520Sohee%2520Yang%2520and%2520Miyoung%2520Ko%2520and%2520Hyeonbin%2520Hwang%2520and%2520Seungpil%2520Won%2520and%2520Dohaeng%2520Lee%2520and%2520Youbin%2520Ahn%2520and%2520Minjoon%2520Seo%26entry.1292438233%3DDespite%2520impressive%2520capabilities%252C%2520LLMs%2527%2520successes%2520often%2520rely%2520on%2520pattern-matching%2520behaviors%252C%2520yet%2520these%2520are%2520also%2520linked%2520to%2520OOD%2520generalization%2520failures%2520in%2520compositional%2520tasks.%2520However%252C%2520behavioral%2520studies%2520commonly%2520employ%2520task%2520setups%2520that%2520allow%2520multiple%2520generalization%2520sources%2520%2528e.g.%252C%2520algebraic%2520invariances%252C%2520structural%2520repetition%2529%252C%2520obscuring%2520a%2520precise%2520and%2520testable%2520account%2520of%2520how%2520well%2520LLMs%2520perform%2520generalization%2520through%2520pattern%2520matching%2520and%2520their%2520limitations.%2520To%2520address%2520this%2520ambiguity%252C%2520we%2520first%2520formalize%2520pattern%2520matching%2520as%2520functional%2520equivalence%252C%2520i.e.%252C%2520identifying%2520pairs%2520of%2520subsequences%2520of%2520inputs%2520that%2520consistently%2520lead%2520to%2520identical%2520results%2520when%2520the%2520rest%2520of%2520the%2520input%2520is%2520held%2520constant.%2520Then%252C%2520we%2520systematically%2520study%2520how%2520decoder-only%2520Transformer%2520and%2520Mamba%2520behave%2520in%2520controlled%2520tasks%2520with%2520compositional%2520structures%2520that%2520isolate%2520this%2520mechanism.%2520Our%2520formalism%2520yields%2520predictive%2520and%2520quantitative%2520insights%253A%2520%25281%2529%2520Instance-wise%2520success%2520of%2520pattern%2520matching%2520is%2520well%2520predicted%2520by%2520the%2520number%2520of%2520contexts%2520witnessing%2520the%2520relevant%2520functional%2520equivalence.%2520%25282%2529%2520We%2520prove%2520a%2520tight%2520sample%2520complexity%2520bound%2520of%2520learning%2520a%2520two-hop%2520structure%2520by%2520identifying%2520the%2520exponent%2520of%2520the%2520data%2520scaling%2520law%2520for%2520perfect%2520in-domain%2520generalization.%2520Our%2520empirical%2520results%2520align%2520with%2520the%2520theoretical%2520prediction%252C%2520under%252020x%2520parameter%2520scaling%2520and%2520across%2520architectures.%2520%25283%2529%2520Path%2520ambiguity%2520is%2520a%2520structural%2520barrier%253A%2520when%2520a%2520variable%2520influences%2520the%2520output%2520via%2520multiple%2520paths%252C%2520models%2520fail%2520to%2520form%2520unified%2520intermediate%2520state%2520representations%252C%2520impairing%2520accuracy%2520and%2520interpretability.%2520%25284%2529%2520Chain-of-Thought%2520reduces%2520data%2520requirements%2520yet%2520does%2520not%2520resolve%2520path%2520ambiguity.%2520Hence%252C%2520we%2520provide%2520a%2520predictive%252C%2520falsifiable%2520boundary%2520for%2520pattern%2520matching%2520and%2520a%2520foundational%2520diagnostic%2520for%2520disentangling%2520mixed%2520generalization%2520mechanisms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Pattern%20Matching%20and%20Its%20Limits%20on%20Compositional%20Task%20Structures&entry.906535625=Hoyeon%20Chang%20and%20Jinho%20Park%20and%20Hanseul%20Cho%20and%20Sohee%20Yang%20and%20Miyoung%20Ko%20and%20Hyeonbin%20Hwang%20and%20Seungpil%20Won%20and%20Dohaeng%20Lee%20and%20Youbin%20Ahn%20and%20Minjoon%20Seo&entry.1292438233=Despite%20impressive%20capabilities%2C%20LLMs%27%20successes%20often%20rely%20on%20pattern-matching%20behaviors%2C%20yet%20these%20are%20also%20linked%20to%20OOD%20generalization%20failures%20in%20compositional%20tasks.%20However%2C%20behavioral%20studies%20commonly%20employ%20task%20setups%20that%20allow%20multiple%20generalization%20sources%20%28e.g.%2C%20algebraic%20invariances%2C%20structural%20repetition%29%2C%20obscuring%20a%20precise%20and%20testable%20account%20of%20how%20well%20LLMs%20perform%20generalization%20through%20pattern%20matching%20and%20their%20limitations.%20To%20address%20this%20ambiguity%2C%20we%20first%20formalize%20pattern%20matching%20as%20functional%20equivalence%2C%20i.e.%2C%20identifying%20pairs%20of%20subsequences%20of%20inputs%20that%20consistently%20lead%20to%20identical%20results%20when%20the%20rest%20of%20the%20input%20is%20held%20constant.%20Then%2C%20we%20systematically%20study%20how%20decoder-only%20Transformer%20and%20Mamba%20behave%20in%20controlled%20tasks%20with%20compositional%20structures%20that%20isolate%20this%20mechanism.%20Our%20formalism%20yields%20predictive%20and%20quantitative%20insights%3A%20%281%29%20Instance-wise%20success%20of%20pattern%20matching%20is%20well%20predicted%20by%20the%20number%20of%20contexts%20witnessing%20the%20relevant%20functional%20equivalence.%20%282%29%20We%20prove%20a%20tight%20sample%20complexity%20bound%20of%20learning%20a%20two-hop%20structure%20by%20identifying%20the%20exponent%20of%20the%20data%20scaling%20law%20for%20perfect%20in-domain%20generalization.%20Our%20empirical%20results%20align%20with%20the%20theoretical%20prediction%2C%20under%2020x%20parameter%20scaling%20and%20across%20architectures.%20%283%29%20Path%20ambiguity%20is%20a%20structural%20barrier%3A%20when%20a%20variable%20influences%20the%20output%20via%20multiple%20paths%2C%20models%20fail%20to%20form%20unified%20intermediate%20state%20representations%2C%20impairing%20accuracy%20and%20interpretability.%20%284%29%20Chain-of-Thought%20reduces%20data%20requirements%20yet%20does%20not%20resolve%20path%20ambiguity.%20Hence%2C%20we%20provide%20a%20predictive%2C%20falsifiable%20boundary%20for%20pattern%20matching%20and%20a%20foundational%20diagnostic%20for%20disentangling%20mixed%20generalization%20mechanisms.&entry.1838667208=http%3A//arxiv.org/abs/2505.20278v2&entry.124074799=Read"},
{"title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC", "author": "Beichen Zhang and Yuhang Zang and Xiaoyi Dong and Yuhang Cao and Haodong Duan and Dahua Lin and Jiaqi Wang", "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33\\% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/InternLM/ARC-VL.", "link": "http://arxiv.org/abs/2511.15703v2", "date": "2025-11-26", "relevancy": 2.4124, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC&body=Title%3A%20Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC%0AAuthor%3A%20Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20Abstract%20reasoning%20from%20minimal%20examples%20remains%20a%20core%20unsolved%20problem%20for%20frontier%20foundation%20models%20such%20as%20GPT-5%20and%20Grok%204.%20These%20models%20still%20fail%20to%20infer%20structured%20transformation%20rules%20from%20a%20handful%20of%20examples%2C%20which%20is%20a%20key%20hallmark%20of%20human%20intelligence.%20The%20Abstraction%20and%20Reasoning%20Corpus%20for%20Artificial%20General%20Intelligence%20%28ARC-AGI%29%20provides%20a%20rigorous%20testbed%20for%20this%20capability%2C%20demanding%20conceptual%20rule%20induction%20and%20transfer%20to%20novel%20tasks.%20Most%20existing%20methods%20treat%20ARC-AGI%20as%20a%20purely%20textual%20reasoning%20task%2C%20overlooking%20the%20fact%20that%20humans%20rely%20heavily%20on%20visual%20abstraction%20when%20solving%20such%20puzzles.%20However%2C%20our%20pilot%20experiments%20reveal%20a%20paradox%3A%20naively%20rendering%20ARC-AGI%20grids%20as%20images%20degrades%20performance%20due%20to%20imprecise%20rule%20execution.%20This%20leads%20to%20our%20central%20hypothesis%20that%20vision%20and%20language%20possess%20complementary%20strengths%20across%20distinct%20reasoning%20stages%3A%20vision%20supports%20global%20pattern%20abstraction%20and%20verification%2C%20whereas%20language%20specializes%20in%20symbolic%20rule%20formulation%20and%20precise%20execution.%20Building%20on%20this%20insight%2C%20we%20introduce%20two%20synergistic%20strategies%3A%20%281%29%20Vision-Language%20Synergy%20Reasoning%20%28VLSR%29%2C%20which%20decomposes%20ARC-AGI%20into%20modality-aligned%20subtasks%3B%20and%20%282%29%20Modality-Switch%20Self-Correction%20%28MSSC%29%2C%20which%20leverages%20vision%20to%20verify%20text-based%20reasoning%20for%20intrinsic%20error%20correction.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20yields%20up%20to%20a%204.33%5C%25%20improvement%20over%20text-only%20baselines%20across%20diverse%20flagship%20models%20and%20multiple%20ARC-AGI%20tasks.%20Our%20findings%20suggest%20that%20unifying%20visual%20abstraction%20with%20linguistic%20reasoning%20is%20a%20crucial%20step%20toward%20achieving%20generalizable%2C%20human-like%20intelligence%20in%20future%20foundation%20models.%20Source%20code%20is%20released%20at%20https%3A//github.com/InternLM/ARC-VL.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Visually%252C%2520Reason%2520Textually%253A%2520Vision-Language%2520Synergy%2520in%2520ARC%26entry.906535625%3DBeichen%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Haodong%2520Duan%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DAbstract%2520reasoning%2520from%2520minimal%2520examples%2520remains%2520a%2520core%2520unsolved%2520problem%2520for%2520frontier%2520foundation%2520models%2520such%2520as%2520GPT-5%2520and%2520Grok%25204.%2520These%2520models%2520still%2520fail%2520to%2520infer%2520structured%2520transformation%2520rules%2520from%2520a%2520handful%2520of%2520examples%252C%2520which%2520is%2520a%2520key%2520hallmark%2520of%2520human%2520intelligence.%2520The%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520for%2520Artificial%2520General%2520Intelligence%2520%2528ARC-AGI%2529%2520provides%2520a%2520rigorous%2520testbed%2520for%2520this%2520capability%252C%2520demanding%2520conceptual%2520rule%2520induction%2520and%2520transfer%2520to%2520novel%2520tasks.%2520Most%2520existing%2520methods%2520treat%2520ARC-AGI%2520as%2520a%2520purely%2520textual%2520reasoning%2520task%252C%2520overlooking%2520the%2520fact%2520that%2520humans%2520rely%2520heavily%2520on%2520visual%2520abstraction%2520when%2520solving%2520such%2520puzzles.%2520However%252C%2520our%2520pilot%2520experiments%2520reveal%2520a%2520paradox%253A%2520naively%2520rendering%2520ARC-AGI%2520grids%2520as%2520images%2520degrades%2520performance%2520due%2520to%2520imprecise%2520rule%2520execution.%2520This%2520leads%2520to%2520our%2520central%2520hypothesis%2520that%2520vision%2520and%2520language%2520possess%2520complementary%2520strengths%2520across%2520distinct%2520reasoning%2520stages%253A%2520vision%2520supports%2520global%2520pattern%2520abstraction%2520and%2520verification%252C%2520whereas%2520language%2520specializes%2520in%2520symbolic%2520rule%2520formulation%2520and%2520precise%2520execution.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520two%2520synergistic%2520strategies%253A%2520%25281%2529%2520Vision-Language%2520Synergy%2520Reasoning%2520%2528VLSR%2529%252C%2520which%2520decomposes%2520ARC-AGI%2520into%2520modality-aligned%2520subtasks%253B%2520and%2520%25282%2529%2520Modality-Switch%2520Self-Correction%2520%2528MSSC%2529%252C%2520which%2520leverages%2520vision%2520to%2520verify%2520text-based%2520reasoning%2520for%2520intrinsic%2520error%2520correction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520yields%2520up%2520to%2520a%25204.33%255C%2525%2520improvement%2520over%2520text-only%2520baselines%2520across%2520diverse%2520flagship%2520models%2520and%2520multiple%2520ARC-AGI%2520tasks.%2520Our%2520findings%2520suggest%2520that%2520unifying%2520visual%2520abstraction%2520with%2520linguistic%2520reasoning%2520is%2520a%2520crucial%2520step%2520toward%2520achieving%2520generalizable%252C%2520human-like%2520intelligence%2520in%2520future%2520foundation%2520models.%2520Source%2520code%2520is%2520released%2520at%2520https%253A//github.com/InternLM/ARC-VL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC&entry.906535625=Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=Abstract%20reasoning%20from%20minimal%20examples%20remains%20a%20core%20unsolved%20problem%20for%20frontier%20foundation%20models%20such%20as%20GPT-5%20and%20Grok%204.%20These%20models%20still%20fail%20to%20infer%20structured%20transformation%20rules%20from%20a%20handful%20of%20examples%2C%20which%20is%20a%20key%20hallmark%20of%20human%20intelligence.%20The%20Abstraction%20and%20Reasoning%20Corpus%20for%20Artificial%20General%20Intelligence%20%28ARC-AGI%29%20provides%20a%20rigorous%20testbed%20for%20this%20capability%2C%20demanding%20conceptual%20rule%20induction%20and%20transfer%20to%20novel%20tasks.%20Most%20existing%20methods%20treat%20ARC-AGI%20as%20a%20purely%20textual%20reasoning%20task%2C%20overlooking%20the%20fact%20that%20humans%20rely%20heavily%20on%20visual%20abstraction%20when%20solving%20such%20puzzles.%20However%2C%20our%20pilot%20experiments%20reveal%20a%20paradox%3A%20naively%20rendering%20ARC-AGI%20grids%20as%20images%20degrades%20performance%20due%20to%20imprecise%20rule%20execution.%20This%20leads%20to%20our%20central%20hypothesis%20that%20vision%20and%20language%20possess%20complementary%20strengths%20across%20distinct%20reasoning%20stages%3A%20vision%20supports%20global%20pattern%20abstraction%20and%20verification%2C%20whereas%20language%20specializes%20in%20symbolic%20rule%20formulation%20and%20precise%20execution.%20Building%20on%20this%20insight%2C%20we%20introduce%20two%20synergistic%20strategies%3A%20%281%29%20Vision-Language%20Synergy%20Reasoning%20%28VLSR%29%2C%20which%20decomposes%20ARC-AGI%20into%20modality-aligned%20subtasks%3B%20and%20%282%29%20Modality-Switch%20Self-Correction%20%28MSSC%29%2C%20which%20leverages%20vision%20to%20verify%20text-based%20reasoning%20for%20intrinsic%20error%20correction.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20yields%20up%20to%20a%204.33%5C%25%20improvement%20over%20text-only%20baselines%20across%20diverse%20flagship%20models%20and%20multiple%20ARC-AGI%20tasks.%20Our%20findings%20suggest%20that%20unifying%20visual%20abstraction%20with%20linguistic%20reasoning%20is%20a%20crucial%20step%20toward%20achieving%20generalizable%2C%20human-like%20intelligence%20in%20future%20foundation%20models.%20Source%20code%20is%20released%20at%20https%3A//github.com/InternLM/ARC-VL.&entry.1838667208=http%3A//arxiv.org/abs/2511.15703v2&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Visual Object Pose Estimation", "author": "Lorenzo Shaikewitz and Charis Georgiou and Luca Carlone", "abstract": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.", "link": "http://arxiv.org/abs/2511.21666v1", "date": "2025-11-26", "relevancy": 2.4118, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6303}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6255}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Visual%20Object%20Pose%20Estimation&body=Title%3A%20Uncertainty%20Quantification%20for%20Visual%20Object%20Pose%20Estimation%0AAuthor%3A%20Lorenzo%20Shaikewitz%20and%20Charis%20Georgiou%20and%20Luca%20Carlone%0AAbstract%3A%20Quantifying%20the%20uncertainty%20of%20an%20object%27s%20pose%20estimate%20is%20essential%20for%20robust%20control%20and%20planning.%20Although%20pose%20estimation%20is%20a%20well-studied%20robotics%20problem%2C%20attaching%20statistically%20rigorous%20uncertainty%20is%20not%20well%20understood%20without%20strict%20distributional%20assumptions.%20We%20develop%20distribution-free%20pose%20uncertainty%20bounds%20about%20a%20given%20pose%20estimate%20in%20the%20monocular%20setting.%20Our%20pose%20uncertainty%20only%20requires%20high%20probability%20noise%20bounds%20on%20pixel%20detections%20of%202D%20semantic%20keypoints%20on%20a%20known%20object.%20This%20noise%20model%20induces%20an%20implicit%2C%20non-convex%20set%20of%20pose%20uncertainty%20constraints.%20Our%20key%20contribution%20is%20SLUE%20%28S-Lemma%20Uncertainty%20Estimation%29%2C%20a%20convex%20program%20to%20reduce%20this%20set%20to%20a%20single%20ellipsoidal%20uncertainty%20bound%20that%20is%20guaranteed%20to%20contain%20the%20true%20object%20pose%20with%20high%20probability.%20SLUE%20solves%20a%20relaxation%20of%20the%20minimum%20volume%20bounding%20ellipsoid%20problem%20inspired%20by%20the%20celebrated%20S-lemma.%20It%20requires%20no%20initial%20guess%20of%20the%20bound%27s%20shape%20or%20size%20and%20is%20guaranteed%20to%20contain%20the%20true%20object%20pose%20with%20high%20probability.%20For%20tighter%20uncertainty%20bounds%20at%20the%20same%20confidence%2C%20we%20extend%20SLUE%20to%20a%20sum-of-squares%20relaxation%20hierarchy%20which%20is%20guaranteed%20to%20converge%20to%20the%20minimum%20volume%20ellipsoidal%20uncertainty%20bound%20for%20a%20given%20set%20of%20keypoint%20constraints.%20We%20show%20this%20pose%20uncertainty%20bound%20can%20easily%20be%20projected%20to%20independent%20translation%20and%20axis-angle%20orientation%20bounds.%20We%20evaluate%20SLUE%20on%20two%20pose%20estimation%20datasets%20and%20a%20real-world%20drone%20tracking%20scenario.%20Compared%20to%20prior%20work%2C%20SLUE%20generates%20substantially%20smaller%20translation%20bounds%20and%20competitive%20orientation%20bounds.%20We%20release%20code%20at%20https%3A//github.com/MIT-SPARK/PoseUncertaintySets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Visual%2520Object%2520Pose%2520Estimation%26entry.906535625%3DLorenzo%2520Shaikewitz%2520and%2520Charis%2520Georgiou%2520and%2520Luca%2520Carlone%26entry.1292438233%3DQuantifying%2520the%2520uncertainty%2520of%2520an%2520object%2527s%2520pose%2520estimate%2520is%2520essential%2520for%2520robust%2520control%2520and%2520planning.%2520Although%2520pose%2520estimation%2520is%2520a%2520well-studied%2520robotics%2520problem%252C%2520attaching%2520statistically%2520rigorous%2520uncertainty%2520is%2520not%2520well%2520understood%2520without%2520strict%2520distributional%2520assumptions.%2520We%2520develop%2520distribution-free%2520pose%2520uncertainty%2520bounds%2520about%2520a%2520given%2520pose%2520estimate%2520in%2520the%2520monocular%2520setting.%2520Our%2520pose%2520uncertainty%2520only%2520requires%2520high%2520probability%2520noise%2520bounds%2520on%2520pixel%2520detections%2520of%25202D%2520semantic%2520keypoints%2520on%2520a%2520known%2520object.%2520This%2520noise%2520model%2520induces%2520an%2520implicit%252C%2520non-convex%2520set%2520of%2520pose%2520uncertainty%2520constraints.%2520Our%2520key%2520contribution%2520is%2520SLUE%2520%2528S-Lemma%2520Uncertainty%2520Estimation%2529%252C%2520a%2520convex%2520program%2520to%2520reduce%2520this%2520set%2520to%2520a%2520single%2520ellipsoidal%2520uncertainty%2520bound%2520that%2520is%2520guaranteed%2520to%2520contain%2520the%2520true%2520object%2520pose%2520with%2520high%2520probability.%2520SLUE%2520solves%2520a%2520relaxation%2520of%2520the%2520minimum%2520volume%2520bounding%2520ellipsoid%2520problem%2520inspired%2520by%2520the%2520celebrated%2520S-lemma.%2520It%2520requires%2520no%2520initial%2520guess%2520of%2520the%2520bound%2527s%2520shape%2520or%2520size%2520and%2520is%2520guaranteed%2520to%2520contain%2520the%2520true%2520object%2520pose%2520with%2520high%2520probability.%2520For%2520tighter%2520uncertainty%2520bounds%2520at%2520the%2520same%2520confidence%252C%2520we%2520extend%2520SLUE%2520to%2520a%2520sum-of-squares%2520relaxation%2520hierarchy%2520which%2520is%2520guaranteed%2520to%2520converge%2520to%2520the%2520minimum%2520volume%2520ellipsoidal%2520uncertainty%2520bound%2520for%2520a%2520given%2520set%2520of%2520keypoint%2520constraints.%2520We%2520show%2520this%2520pose%2520uncertainty%2520bound%2520can%2520easily%2520be%2520projected%2520to%2520independent%2520translation%2520and%2520axis-angle%2520orientation%2520bounds.%2520We%2520evaluate%2520SLUE%2520on%2520two%2520pose%2520estimation%2520datasets%2520and%2520a%2520real-world%2520drone%2520tracking%2520scenario.%2520Compared%2520to%2520prior%2520work%252C%2520SLUE%2520generates%2520substantially%2520smaller%2520translation%2520bounds%2520and%2520competitive%2520orientation%2520bounds.%2520We%2520release%2520code%2520at%2520https%253A//github.com/MIT-SPARK/PoseUncertaintySets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Visual%20Object%20Pose%20Estimation&entry.906535625=Lorenzo%20Shaikewitz%20and%20Charis%20Georgiou%20and%20Luca%20Carlone&entry.1292438233=Quantifying%20the%20uncertainty%20of%20an%20object%27s%20pose%20estimate%20is%20essential%20for%20robust%20control%20and%20planning.%20Although%20pose%20estimation%20is%20a%20well-studied%20robotics%20problem%2C%20attaching%20statistically%20rigorous%20uncertainty%20is%20not%20well%20understood%20without%20strict%20distributional%20assumptions.%20We%20develop%20distribution-free%20pose%20uncertainty%20bounds%20about%20a%20given%20pose%20estimate%20in%20the%20monocular%20setting.%20Our%20pose%20uncertainty%20only%20requires%20high%20probability%20noise%20bounds%20on%20pixel%20detections%20of%202D%20semantic%20keypoints%20on%20a%20known%20object.%20This%20noise%20model%20induces%20an%20implicit%2C%20non-convex%20set%20of%20pose%20uncertainty%20constraints.%20Our%20key%20contribution%20is%20SLUE%20%28S-Lemma%20Uncertainty%20Estimation%29%2C%20a%20convex%20program%20to%20reduce%20this%20set%20to%20a%20single%20ellipsoidal%20uncertainty%20bound%20that%20is%20guaranteed%20to%20contain%20the%20true%20object%20pose%20with%20high%20probability.%20SLUE%20solves%20a%20relaxation%20of%20the%20minimum%20volume%20bounding%20ellipsoid%20problem%20inspired%20by%20the%20celebrated%20S-lemma.%20It%20requires%20no%20initial%20guess%20of%20the%20bound%27s%20shape%20or%20size%20and%20is%20guaranteed%20to%20contain%20the%20true%20object%20pose%20with%20high%20probability.%20For%20tighter%20uncertainty%20bounds%20at%20the%20same%20confidence%2C%20we%20extend%20SLUE%20to%20a%20sum-of-squares%20relaxation%20hierarchy%20which%20is%20guaranteed%20to%20converge%20to%20the%20minimum%20volume%20ellipsoidal%20uncertainty%20bound%20for%20a%20given%20set%20of%20keypoint%20constraints.%20We%20show%20this%20pose%20uncertainty%20bound%20can%20easily%20be%20projected%20to%20independent%20translation%20and%20axis-angle%20orientation%20bounds.%20We%20evaluate%20SLUE%20on%20two%20pose%20estimation%20datasets%20and%20a%20real-world%20drone%20tracking%20scenario.%20Compared%20to%20prior%20work%2C%20SLUE%20generates%20substantially%20smaller%20translation%20bounds%20and%20competitive%20orientation%20bounds.%20We%20release%20code%20at%20https%3A//github.com/MIT-SPARK/PoseUncertaintySets.&entry.1838667208=http%3A//arxiv.org/abs/2511.21666v1&entry.124074799=Read"},
{"title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "author": "Seungjae Lee and Yoonkyo Jung and Inkook Chun and Yao-Chih Lee and Zikui Cai and Hongjia Huang and Aayush Talreja and Tan Dat Dao and Yongyuan Liang and Jia-Bin Huang and Furong Huang", "abstract": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "link": "http://arxiv.org/abs/2511.21690v1", "date": "2025-11-26", "relevancy": 2.4087, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6095}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6052}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TraceGen%3A%20World%20Modeling%20in%203D%20Trace%20Space%20Enables%20Learning%20from%20Cross-Embodiment%20Videos&body=Title%3A%20TraceGen%3A%20World%20Modeling%20in%203D%20Trace%20Space%20Enables%20Learning%20from%20Cross-Embodiment%20Videos%0AAuthor%3A%20Seungjae%20Lee%20and%20Yoonkyo%20Jung%20and%20Inkook%20Chun%20and%20Yao-Chih%20Lee%20and%20Zikui%20Cai%20and%20Hongjia%20Huang%20and%20Aayush%20Talreja%20and%20Tan%20Dat%20Dao%20and%20Yongyuan%20Liang%20and%20Jia-Bin%20Huang%20and%20Furong%20Huang%0AAbstract%3A%20Learning%20new%20robot%20tasks%20on%20new%20platforms%20and%20in%20new%20scenes%20from%20only%20a%20handful%20of%20demonstrations%20remains%20challenging.%20While%20videos%20of%20other%20embodiments%20-%20humans%20and%20different%20robots%20-%20are%20abundant%2C%20differences%20in%20embodiment%2C%20camera%2C%20and%20environment%20hinder%20their%20direct%20use.%20We%20address%20the%20small-data%20problem%20by%20introducing%20a%20unifying%2C%20symbolic%20representation%20-%20a%20compact%203D%20%22trace-space%22%20of%20scene-level%20trajectories%20-%20that%20enables%20learning%20from%20cross-embodiment%2C%20cross-environment%2C%20and%20cross-task%20videos.%20We%20present%20TraceGen%2C%20a%20world%20model%20that%20predicts%20future%20motion%20in%20trace-space%20rather%20than%20pixel%20space%2C%20abstracting%20away%20appearance%20while%20retaining%20the%20geometric%20structure%20needed%20for%20manipulation.%20To%20train%20TraceGen%20at%20scale%2C%20we%20develop%20TraceForge%2C%20a%20data%20pipeline%20that%20transforms%20heterogeneous%20human%20and%20robot%20videos%20into%20consistent%203D%20traces%2C%20yielding%20a%20corpus%20of%20123K%20videos%20and%201.8M%20observation-trace-language%20triplets.%20Pretraining%20on%20this%20corpus%20produces%20a%20transferable%203D%20motion%20prior%20that%20adapts%20efficiently%3A%20with%20just%20five%20target%20robot%20videos%2C%20TraceGen%20attains%2080%25%20success%20across%20four%20tasks%20while%20offering%2050-600x%20faster%20inference%20than%20state-of-the-art%20video-based%20world%20models.%20In%20the%20more%20challenging%20case%20where%20only%20five%20uncalibrated%20human%20demonstration%20videos%20captured%20on%20a%20handheld%20phone%20are%20available%2C%20it%20still%20reaches%2067.5%25%20success%20on%20a%20real%20robot%2C%20highlighting%20TraceGen%27s%20ability%20to%20adapt%20across%20embodiments%20without%20relying%20on%20object%20detectors%20or%20heavy%20pixel-space%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraceGen%253A%2520World%2520Modeling%2520in%25203D%2520Trace%2520Space%2520Enables%2520Learning%2520from%2520Cross-Embodiment%2520Videos%26entry.906535625%3DSeungjae%2520Lee%2520and%2520Yoonkyo%2520Jung%2520and%2520Inkook%2520Chun%2520and%2520Yao-Chih%2520Lee%2520and%2520Zikui%2520Cai%2520and%2520Hongjia%2520Huang%2520and%2520Aayush%2520Talreja%2520and%2520Tan%2520Dat%2520Dao%2520and%2520Yongyuan%2520Liang%2520and%2520Jia-Bin%2520Huang%2520and%2520Furong%2520Huang%26entry.1292438233%3DLearning%2520new%2520robot%2520tasks%2520on%2520new%2520platforms%2520and%2520in%2520new%2520scenes%2520from%2520only%2520a%2520handful%2520of%2520demonstrations%2520remains%2520challenging.%2520While%2520videos%2520of%2520other%2520embodiments%2520-%2520humans%2520and%2520different%2520robots%2520-%2520are%2520abundant%252C%2520differences%2520in%2520embodiment%252C%2520camera%252C%2520and%2520environment%2520hinder%2520their%2520direct%2520use.%2520We%2520address%2520the%2520small-data%2520problem%2520by%2520introducing%2520a%2520unifying%252C%2520symbolic%2520representation%2520-%2520a%2520compact%25203D%2520%2522trace-space%2522%2520of%2520scene-level%2520trajectories%2520-%2520that%2520enables%2520learning%2520from%2520cross-embodiment%252C%2520cross-environment%252C%2520and%2520cross-task%2520videos.%2520We%2520present%2520TraceGen%252C%2520a%2520world%2520model%2520that%2520predicts%2520future%2520motion%2520in%2520trace-space%2520rather%2520than%2520pixel%2520space%252C%2520abstracting%2520away%2520appearance%2520while%2520retaining%2520the%2520geometric%2520structure%2520needed%2520for%2520manipulation.%2520To%2520train%2520TraceGen%2520at%2520scale%252C%2520we%2520develop%2520TraceForge%252C%2520a%2520data%2520pipeline%2520that%2520transforms%2520heterogeneous%2520human%2520and%2520robot%2520videos%2520into%2520consistent%25203D%2520traces%252C%2520yielding%2520a%2520corpus%2520of%2520123K%2520videos%2520and%25201.8M%2520observation-trace-language%2520triplets.%2520Pretraining%2520on%2520this%2520corpus%2520produces%2520a%2520transferable%25203D%2520motion%2520prior%2520that%2520adapts%2520efficiently%253A%2520with%2520just%2520five%2520target%2520robot%2520videos%252C%2520TraceGen%2520attains%252080%2525%2520success%2520across%2520four%2520tasks%2520while%2520offering%252050-600x%2520faster%2520inference%2520than%2520state-of-the-art%2520video-based%2520world%2520models.%2520In%2520the%2520more%2520challenging%2520case%2520where%2520only%2520five%2520uncalibrated%2520human%2520demonstration%2520videos%2520captured%2520on%2520a%2520handheld%2520phone%2520are%2520available%252C%2520it%2520still%2520reaches%252067.5%2525%2520success%2520on%2520a%2520real%2520robot%252C%2520highlighting%2520TraceGen%2527s%2520ability%2520to%2520adapt%2520across%2520embodiments%2520without%2520relying%2520on%2520object%2520detectors%2520or%2520heavy%2520pixel-space%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TraceGen%3A%20World%20Modeling%20in%203D%20Trace%20Space%20Enables%20Learning%20from%20Cross-Embodiment%20Videos&entry.906535625=Seungjae%20Lee%20and%20Yoonkyo%20Jung%20and%20Inkook%20Chun%20and%20Yao-Chih%20Lee%20and%20Zikui%20Cai%20and%20Hongjia%20Huang%20and%20Aayush%20Talreja%20and%20Tan%20Dat%20Dao%20and%20Yongyuan%20Liang%20and%20Jia-Bin%20Huang%20and%20Furong%20Huang&entry.1292438233=Learning%20new%20robot%20tasks%20on%20new%20platforms%20and%20in%20new%20scenes%20from%20only%20a%20handful%20of%20demonstrations%20remains%20challenging.%20While%20videos%20of%20other%20embodiments%20-%20humans%20and%20different%20robots%20-%20are%20abundant%2C%20differences%20in%20embodiment%2C%20camera%2C%20and%20environment%20hinder%20their%20direct%20use.%20We%20address%20the%20small-data%20problem%20by%20introducing%20a%20unifying%2C%20symbolic%20representation%20-%20a%20compact%203D%20%22trace-space%22%20of%20scene-level%20trajectories%20-%20that%20enables%20learning%20from%20cross-embodiment%2C%20cross-environment%2C%20and%20cross-task%20videos.%20We%20present%20TraceGen%2C%20a%20world%20model%20that%20predicts%20future%20motion%20in%20trace-space%20rather%20than%20pixel%20space%2C%20abstracting%20away%20appearance%20while%20retaining%20the%20geometric%20structure%20needed%20for%20manipulation.%20To%20train%20TraceGen%20at%20scale%2C%20we%20develop%20TraceForge%2C%20a%20data%20pipeline%20that%20transforms%20heterogeneous%20human%20and%20robot%20videos%20into%20consistent%203D%20traces%2C%20yielding%20a%20corpus%20of%20123K%20videos%20and%201.8M%20observation-trace-language%20triplets.%20Pretraining%20on%20this%20corpus%20produces%20a%20transferable%203D%20motion%20prior%20that%20adapts%20efficiently%3A%20with%20just%20five%20target%20robot%20videos%2C%20TraceGen%20attains%2080%25%20success%20across%20four%20tasks%20while%20offering%2050-600x%20faster%20inference%20than%20state-of-the-art%20video-based%20world%20models.%20In%20the%20more%20challenging%20case%20where%20only%20five%20uncalibrated%20human%20demonstration%20videos%20captured%20on%20a%20handheld%20phone%20are%20available%2C%20it%20still%20reaches%2067.5%25%20success%20on%20a%20real%20robot%2C%20highlighting%20TraceGen%27s%20ability%20to%20adapt%20across%20embodiments%20without%20relying%20on%20object%20detectors%20or%20heavy%20pixel-space%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.21690v1&entry.124074799=Read"},
{"title": "One-Step Diffusion-Based Image Compression with Semantic Distillation", "author": "Naifu Xue and Zhaoyang Jia and Jiahao Li and Bin Li and Yuan Zhang and Yan Lu", "abstract": "While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/", "link": "http://arxiv.org/abs/2505.16687v2", "date": "2025-11-26", "relevancy": 2.401, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6792}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5853}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation&body=Title%3A%20One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation%0AAuthor%3A%20Naifu%20Xue%20and%20Zhaoyang%20Jia%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yuan%20Zhang%20and%20Yan%20Lu%0AAbstract%3A%20While%20recent%20diffusion-based%20generative%20image%20codecs%20have%20shown%20impressive%20performance%2C%20their%20iterative%20sampling%20process%20introduces%20unpleasing%20latency.%20In%20this%20work%2C%20we%20revisit%20the%20design%20of%20a%20diffusion-based%20codec%20and%20argue%20that%20multi-step%20sampling%20is%20not%20necessary%20for%20generative%20compression.%20Based%20on%20this%20insight%2C%20we%20propose%20OneDC%2C%20a%20One-step%20Diffusion-based%20generative%20image%20Codec%20--%20that%20integrates%20a%20latent%20compression%20module%20with%20a%20one-step%20diffusion%20generator.%20Recognizing%20the%20critical%20role%20of%20semantic%20guidance%20in%20one-step%20diffusion%2C%20we%20propose%20using%20the%20hyperprior%20as%20a%20semantic%20signal%2C%20overcoming%20the%20limitations%20of%20text%20prompts%20in%20representing%20complex%20visual%20content.%20To%20further%20enhance%20the%20semantic%20capability%20of%20the%20hyperprior%2C%20we%20introduce%20a%20semantic%20distillation%20mechanism%20that%20transfers%20knowledge%20from%20a%20pretrained%20generative%20tokenizer%20to%20the%20hyperprior%20codec.%20Additionally%2C%20we%20adopt%20a%20hybrid%20pixel-%20and%20latent-domain%20optimization%20to%20jointly%20enhance%20both%20reconstruction%20fidelity%20and%20perceptual%20realism.%20Extensive%20experiments%20demonstrate%20that%20OneDC%20achieves%20SOTA%20perceptual%20quality%20even%20with%20one-step%20generation%2C%20offering%20over%2039%25%20bitrate%20reduction%20and%2020x%20faster%20decoding%20compared%20to%20prior%20multi-step%20diffusion-based%20codecs.%20Project%3A%20https%3A//onedc-codec.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2505.16687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Step%2520Diffusion-Based%2520Image%2520Compression%2520with%2520Semantic%2520Distillation%26entry.906535625%3DNaifu%2520Xue%2520and%2520Zhaoyang%2520Jia%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Yuan%2520Zhang%2520and%2520Yan%2520Lu%26entry.1292438233%3DWhile%2520recent%2520diffusion-based%2520generative%2520image%2520codecs%2520have%2520shown%2520impressive%2520performance%252C%2520their%2520iterative%2520sampling%2520process%2520introduces%2520unpleasing%2520latency.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520design%2520of%2520a%2520diffusion-based%2520codec%2520and%2520argue%2520that%2520multi-step%2520sampling%2520is%2520not%2520necessary%2520for%2520generative%2520compression.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520OneDC%252C%2520a%2520One-step%2520Diffusion-based%2520generative%2520image%2520Codec%2520--%2520that%2520integrates%2520a%2520latent%2520compression%2520module%2520with%2520a%2520one-step%2520diffusion%2520generator.%2520Recognizing%2520the%2520critical%2520role%2520of%2520semantic%2520guidance%2520in%2520one-step%2520diffusion%252C%2520we%2520propose%2520using%2520the%2520hyperprior%2520as%2520a%2520semantic%2520signal%252C%2520overcoming%2520the%2520limitations%2520of%2520text%2520prompts%2520in%2520representing%2520complex%2520visual%2520content.%2520To%2520further%2520enhance%2520the%2520semantic%2520capability%2520of%2520the%2520hyperprior%252C%2520we%2520introduce%2520a%2520semantic%2520distillation%2520mechanism%2520that%2520transfers%2520knowledge%2520from%2520a%2520pretrained%2520generative%2520tokenizer%2520to%2520the%2520hyperprior%2520codec.%2520Additionally%252C%2520we%2520adopt%2520a%2520hybrid%2520pixel-%2520and%2520latent-domain%2520optimization%2520to%2520jointly%2520enhance%2520both%2520reconstruction%2520fidelity%2520and%2520perceptual%2520realism.%2520Extensive%2520experiments%2520demonstrate%2520that%2520OneDC%2520achieves%2520SOTA%2520perceptual%2520quality%2520even%2520with%2520one-step%2520generation%252C%2520offering%2520over%252039%2525%2520bitrate%2520reduction%2520and%252020x%2520faster%2520decoding%2520compared%2520to%2520prior%2520multi-step%2520diffusion-based%2520codecs.%2520Project%253A%2520https%253A//onedc-codec.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation&entry.906535625=Naifu%20Xue%20and%20Zhaoyang%20Jia%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yuan%20Zhang%20and%20Yan%20Lu&entry.1292438233=While%20recent%20diffusion-based%20generative%20image%20codecs%20have%20shown%20impressive%20performance%2C%20their%20iterative%20sampling%20process%20introduces%20unpleasing%20latency.%20In%20this%20work%2C%20we%20revisit%20the%20design%20of%20a%20diffusion-based%20codec%20and%20argue%20that%20multi-step%20sampling%20is%20not%20necessary%20for%20generative%20compression.%20Based%20on%20this%20insight%2C%20we%20propose%20OneDC%2C%20a%20One-step%20Diffusion-based%20generative%20image%20Codec%20--%20that%20integrates%20a%20latent%20compression%20module%20with%20a%20one-step%20diffusion%20generator.%20Recognizing%20the%20critical%20role%20of%20semantic%20guidance%20in%20one-step%20diffusion%2C%20we%20propose%20using%20the%20hyperprior%20as%20a%20semantic%20signal%2C%20overcoming%20the%20limitations%20of%20text%20prompts%20in%20representing%20complex%20visual%20content.%20To%20further%20enhance%20the%20semantic%20capability%20of%20the%20hyperprior%2C%20we%20introduce%20a%20semantic%20distillation%20mechanism%20that%20transfers%20knowledge%20from%20a%20pretrained%20generative%20tokenizer%20to%20the%20hyperprior%20codec.%20Additionally%2C%20we%20adopt%20a%20hybrid%20pixel-%20and%20latent-domain%20optimization%20to%20jointly%20enhance%20both%20reconstruction%20fidelity%20and%20perceptual%20realism.%20Extensive%20experiments%20demonstrate%20that%20OneDC%20achieves%20SOTA%20perceptual%20quality%20even%20with%20one-step%20generation%2C%20offering%20over%2039%25%20bitrate%20reduction%20and%2020x%20faster%20decoding%20compared%20to%20prior%20multi-step%20diffusion-based%20codecs.%20Project%3A%20https%3A//onedc-codec.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2505.16687v2&entry.124074799=Read"},
{"title": "A Systematic Study of Model Merging Techniques in Large Language Models", "author": "O\u011fuz Ka\u011fan Hitit and Leander Girrbach and Zeynep Akata", "abstract": "Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.", "link": "http://arxiv.org/abs/2511.21437v1", "date": "2025-11-26", "relevancy": 2.3934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Study%20of%20Model%20Merging%20Techniques%20in%20Large%20Language%20Models&body=Title%3A%20A%20Systematic%20Study%20of%20Model%20Merging%20Techniques%20in%20Large%20Language%20Models%0AAuthor%3A%20O%C4%9Fuz%20Ka%C4%9Fan%20Hitit%20and%20Leander%20Girrbach%20and%20Zeynep%20Akata%0AAbstract%3A%20Model%20merging%20combines%20multiple%20fine-tuned%20checkpoints%20into%20a%20single%20model%20without%20additional%20training%2C%20offering%20an%20attractive%20approach%20to%20reusing%20models%20and%20efficiently%20improving%20performance.%20However%2C%20it%20remains%20unclear%20whether%20the%20advantages%20reported%20for%20smaller%20models%20and%20classifiers%20generalize%20to%20LLMs.%20We%20present%20a%20large-scale%2C%20systematic%20evaluation%20of%20six%20state-of-the-art%20merging%20methods%2C%20including%20recent%20subspace%20methods%2C%20across%20four%20open-weight%20LLMs%2C%20twelve%20fine-tuned%20checkpoints%20per%20base%20model%2C%20and%20sixteen%20standard%20LLM%20benchmarks.%20Evaluating%20through%20standardized%20benchmarks%2C%20we%20measure%20both%20the%20probability%20that%20a%20merged%20model%20outperforms%20the%20base%20model%20and%20relative%20gains%20over%20the%20best%20individual%20checkpoint.%20Our%20results%20show%20that%20the%20oldest%20and%20simplest%20method%2C%20Task%20Arithmetic%2C%20is%20the%20only%20approach%20that%20reliably%20yields%20performance%20gains%20on%20LLMs.%20Other%20interference-aware%20and%20subspace%20merging%20methods%20typically%20result%20in%20significant%20performance%20drops.%20Our%20findings%20indicate%20that%20current%20merging%20techniques%20do%20not%20directly%20transfer%20to%20modern%20LLMs.%20This%20motivates%20the%20design%20of%20LLM-specific%20merging%20algorithms%20and%20merging-aware%20fine-tuning%20methods.%20Code%20will%20be%20released%20upon%20acceptance%20of%20this%20paper.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Study%2520of%2520Model%2520Merging%2520Techniques%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DO%25C4%259Fuz%2520Ka%25C4%259Fan%2520Hitit%2520and%2520Leander%2520Girrbach%2520and%2520Zeynep%2520Akata%26entry.1292438233%3DModel%2520merging%2520combines%2520multiple%2520fine-tuned%2520checkpoints%2520into%2520a%2520single%2520model%2520without%2520additional%2520training%252C%2520offering%2520an%2520attractive%2520approach%2520to%2520reusing%2520models%2520and%2520efficiently%2520improving%2520performance.%2520However%252C%2520it%2520remains%2520unclear%2520whether%2520the%2520advantages%2520reported%2520for%2520smaller%2520models%2520and%2520classifiers%2520generalize%2520to%2520LLMs.%2520We%2520present%2520a%2520large-scale%252C%2520systematic%2520evaluation%2520of%2520six%2520state-of-the-art%2520merging%2520methods%252C%2520including%2520recent%2520subspace%2520methods%252C%2520across%2520four%2520open-weight%2520LLMs%252C%2520twelve%2520fine-tuned%2520checkpoints%2520per%2520base%2520model%252C%2520and%2520sixteen%2520standard%2520LLM%2520benchmarks.%2520Evaluating%2520through%2520standardized%2520benchmarks%252C%2520we%2520measure%2520both%2520the%2520probability%2520that%2520a%2520merged%2520model%2520outperforms%2520the%2520base%2520model%2520and%2520relative%2520gains%2520over%2520the%2520best%2520individual%2520checkpoint.%2520Our%2520results%2520show%2520that%2520the%2520oldest%2520and%2520simplest%2520method%252C%2520Task%2520Arithmetic%252C%2520is%2520the%2520only%2520approach%2520that%2520reliably%2520yields%2520performance%2520gains%2520on%2520LLMs.%2520Other%2520interference-aware%2520and%2520subspace%2520merging%2520methods%2520typically%2520result%2520in%2520significant%2520performance%2520drops.%2520Our%2520findings%2520indicate%2520that%2520current%2520merging%2520techniques%2520do%2520not%2520directly%2520transfer%2520to%2520modern%2520LLMs.%2520This%2520motivates%2520the%2520design%2520of%2520LLM-specific%2520merging%2520algorithms%2520and%2520merging-aware%2520fine-tuning%2520methods.%2520Code%2520will%2520be%2520released%2520upon%2520acceptance%2520of%2520this%2520paper.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Study%20of%20Model%20Merging%20Techniques%20in%20Large%20Language%20Models&entry.906535625=O%C4%9Fuz%20Ka%C4%9Fan%20Hitit%20and%20Leander%20Girrbach%20and%20Zeynep%20Akata&entry.1292438233=Model%20merging%20combines%20multiple%20fine-tuned%20checkpoints%20into%20a%20single%20model%20without%20additional%20training%2C%20offering%20an%20attractive%20approach%20to%20reusing%20models%20and%20efficiently%20improving%20performance.%20However%2C%20it%20remains%20unclear%20whether%20the%20advantages%20reported%20for%20smaller%20models%20and%20classifiers%20generalize%20to%20LLMs.%20We%20present%20a%20large-scale%2C%20systematic%20evaluation%20of%20six%20state-of-the-art%20merging%20methods%2C%20including%20recent%20subspace%20methods%2C%20across%20four%20open-weight%20LLMs%2C%20twelve%20fine-tuned%20checkpoints%20per%20base%20model%2C%20and%20sixteen%20standard%20LLM%20benchmarks.%20Evaluating%20through%20standardized%20benchmarks%2C%20we%20measure%20both%20the%20probability%20that%20a%20merged%20model%20outperforms%20the%20base%20model%20and%20relative%20gains%20over%20the%20best%20individual%20checkpoint.%20Our%20results%20show%20that%20the%20oldest%20and%20simplest%20method%2C%20Task%20Arithmetic%2C%20is%20the%20only%20approach%20that%20reliably%20yields%20performance%20gains%20on%20LLMs.%20Other%20interference-aware%20and%20subspace%20merging%20methods%20typically%20result%20in%20significant%20performance%20drops.%20Our%20findings%20indicate%20that%20current%20merging%20techniques%20do%20not%20directly%20transfer%20to%20modern%20LLMs.%20This%20motivates%20the%20design%20of%20LLM-specific%20merging%20algorithms%20and%20merging-aware%20fine-tuning%20methods.%20Code%20will%20be%20released%20upon%20acceptance%20of%20this%20paper.&entry.1838667208=http%3A//arxiv.org/abs/2511.21437v1&entry.124074799=Read"},
{"title": "Reasoning Transfer for an Extremely Low-Resource and Endangered Language: Bridging Languages Through Sample-Efficient Language Understanding", "author": "Khanh-Tung Tran and Barry O'Sullivan and Hoang D. Nguyen", "abstract": "Recent advances have enabled Large Language Models (LLMs) to tackle reasoning tasks by generating chain-of-thought (CoT) rationales, yet these gains have largely applied to high-resource languages, leaving low-resource languages behind. In this work, we first investigate CoT techniques in extremely low-resource scenarios through previous prompting, model-editing, and fine-tuning approaches. We introduce English-Pivoted CoT Training, leveraging the insight that LLMs internally operate in a latent space aligned toward the dominant language. Given input in a low-resource language, we perform supervised fine-tuning to generate CoT in English and output the final response in the target language. Across mathematical reasoning benchmarks, our approach outperforms other baselines with up to 28.33% improvement in low-resource scenarios. Our analysis and additional experiments, including Mixed-Language CoT and Two-Stage Training, show that explicitly separating language understanding from reasoning enhances cross-lingual reasoning abilities. To facilitate future work, we also release \\emph{LC2024}, the first benchmark for mathematical tasks in Irish, an extremely low-resource and endangered language. Our results and resources highlight a practical pathway to multilingual reasoning without extensive retraining in every extremely low-resource language, despite data scarcity.", "link": "http://arxiv.org/abs/2504.02890v2", "date": "2025-11-26", "relevancy": 2.3901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Transfer%20for%20an%20Extremely%20Low-Resource%20and%20Endangered%20Language%3A%20Bridging%20Languages%20Through%20Sample-Efficient%20Language%20Understanding&body=Title%3A%20Reasoning%20Transfer%20for%20an%20Extremely%20Low-Resource%20and%20Endangered%20Language%3A%20Bridging%20Languages%20Through%20Sample-Efficient%20Language%20Understanding%0AAuthor%3A%20Khanh-Tung%20Tran%20and%20Barry%20O%27Sullivan%20and%20Hoang%20D.%20Nguyen%0AAbstract%3A%20Recent%20advances%20have%20enabled%20Large%20Language%20Models%20%28LLMs%29%20to%20tackle%20reasoning%20tasks%20by%20generating%20chain-of-thought%20%28CoT%29%20rationales%2C%20yet%20these%20gains%20have%20largely%20applied%20to%20high-resource%20languages%2C%20leaving%20low-resource%20languages%20behind.%20In%20this%20work%2C%20we%20first%20investigate%20CoT%20techniques%20in%20extremely%20low-resource%20scenarios%20through%20previous%20prompting%2C%20model-editing%2C%20and%20fine-tuning%20approaches.%20We%20introduce%20English-Pivoted%20CoT%20Training%2C%20leveraging%20the%20insight%20that%20LLMs%20internally%20operate%20in%20a%20latent%20space%20aligned%20toward%20the%20dominant%20language.%20Given%20input%20in%20a%20low-resource%20language%2C%20we%20perform%20supervised%20fine-tuning%20to%20generate%20CoT%20in%20English%20and%20output%20the%20final%20response%20in%20the%20target%20language.%20Across%20mathematical%20reasoning%20benchmarks%2C%20our%20approach%20outperforms%20other%20baselines%20with%20up%20to%2028.33%25%20improvement%20in%20low-resource%20scenarios.%20Our%20analysis%20and%20additional%20experiments%2C%20including%20Mixed-Language%20CoT%20and%20Two-Stage%20Training%2C%20show%20that%20explicitly%20separating%20language%20understanding%20from%20reasoning%20enhances%20cross-lingual%20reasoning%20abilities.%20To%20facilitate%20future%20work%2C%20we%20also%20release%20%5Cemph%7BLC2024%7D%2C%20the%20first%20benchmark%20for%20mathematical%20tasks%20in%20Irish%2C%20an%20extremely%20low-resource%20and%20endangered%20language.%20Our%20results%20and%20resources%20highlight%20a%20practical%20pathway%20to%20multilingual%20reasoning%20without%20extensive%20retraining%20in%20every%20extremely%20low-resource%20language%2C%20despite%20data%20scarcity.%0ALink%3A%20http%3A//arxiv.org/abs/2504.02890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Transfer%2520for%2520an%2520Extremely%2520Low-Resource%2520and%2520Endangered%2520Language%253A%2520Bridging%2520Languages%2520Through%2520Sample-Efficient%2520Language%2520Understanding%26entry.906535625%3DKhanh-Tung%2520Tran%2520and%2520Barry%2520O%2527Sullivan%2520and%2520Hoang%2520D.%2520Nguyen%26entry.1292438233%3DRecent%2520advances%2520have%2520enabled%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520tackle%2520reasoning%2520tasks%2520by%2520generating%2520chain-of-thought%2520%2528CoT%2529%2520rationales%252C%2520yet%2520these%2520gains%2520have%2520largely%2520applied%2520to%2520high-resource%2520languages%252C%2520leaving%2520low-resource%2520languages%2520behind.%2520In%2520this%2520work%252C%2520we%2520first%2520investigate%2520CoT%2520techniques%2520in%2520extremely%2520low-resource%2520scenarios%2520through%2520previous%2520prompting%252C%2520model-editing%252C%2520and%2520fine-tuning%2520approaches.%2520We%2520introduce%2520English-Pivoted%2520CoT%2520Training%252C%2520leveraging%2520the%2520insight%2520that%2520LLMs%2520internally%2520operate%2520in%2520a%2520latent%2520space%2520aligned%2520toward%2520the%2520dominant%2520language.%2520Given%2520input%2520in%2520a%2520low-resource%2520language%252C%2520we%2520perform%2520supervised%2520fine-tuning%2520to%2520generate%2520CoT%2520in%2520English%2520and%2520output%2520the%2520final%2520response%2520in%2520the%2520target%2520language.%2520Across%2520mathematical%2520reasoning%2520benchmarks%252C%2520our%2520approach%2520outperforms%2520other%2520baselines%2520with%2520up%2520to%252028.33%2525%2520improvement%2520in%2520low-resource%2520scenarios.%2520Our%2520analysis%2520and%2520additional%2520experiments%252C%2520including%2520Mixed-Language%2520CoT%2520and%2520Two-Stage%2520Training%252C%2520show%2520that%2520explicitly%2520separating%2520language%2520understanding%2520from%2520reasoning%2520enhances%2520cross-lingual%2520reasoning%2520abilities.%2520To%2520facilitate%2520future%2520work%252C%2520we%2520also%2520release%2520%255Cemph%257BLC2024%257D%252C%2520the%2520first%2520benchmark%2520for%2520mathematical%2520tasks%2520in%2520Irish%252C%2520an%2520extremely%2520low-resource%2520and%2520endangered%2520language.%2520Our%2520results%2520and%2520resources%2520highlight%2520a%2520practical%2520pathway%2520to%2520multilingual%2520reasoning%2520without%2520extensive%2520retraining%2520in%2520every%2520extremely%2520low-resource%2520language%252C%2520despite%2520data%2520scarcity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Transfer%20for%20an%20Extremely%20Low-Resource%20and%20Endangered%20Language%3A%20Bridging%20Languages%20Through%20Sample-Efficient%20Language%20Understanding&entry.906535625=Khanh-Tung%20Tran%20and%20Barry%20O%27Sullivan%20and%20Hoang%20D.%20Nguyen&entry.1292438233=Recent%20advances%20have%20enabled%20Large%20Language%20Models%20%28LLMs%29%20to%20tackle%20reasoning%20tasks%20by%20generating%20chain-of-thought%20%28CoT%29%20rationales%2C%20yet%20these%20gains%20have%20largely%20applied%20to%20high-resource%20languages%2C%20leaving%20low-resource%20languages%20behind.%20In%20this%20work%2C%20we%20first%20investigate%20CoT%20techniques%20in%20extremely%20low-resource%20scenarios%20through%20previous%20prompting%2C%20model-editing%2C%20and%20fine-tuning%20approaches.%20We%20introduce%20English-Pivoted%20CoT%20Training%2C%20leveraging%20the%20insight%20that%20LLMs%20internally%20operate%20in%20a%20latent%20space%20aligned%20toward%20the%20dominant%20language.%20Given%20input%20in%20a%20low-resource%20language%2C%20we%20perform%20supervised%20fine-tuning%20to%20generate%20CoT%20in%20English%20and%20output%20the%20final%20response%20in%20the%20target%20language.%20Across%20mathematical%20reasoning%20benchmarks%2C%20our%20approach%20outperforms%20other%20baselines%20with%20up%20to%2028.33%25%20improvement%20in%20low-resource%20scenarios.%20Our%20analysis%20and%20additional%20experiments%2C%20including%20Mixed-Language%20CoT%20and%20Two-Stage%20Training%2C%20show%20that%20explicitly%20separating%20language%20understanding%20from%20reasoning%20enhances%20cross-lingual%20reasoning%20abilities.%20To%20facilitate%20future%20work%2C%20we%20also%20release%20%5Cemph%7BLC2024%7D%2C%20the%20first%20benchmark%20for%20mathematical%20tasks%20in%20Irish%2C%20an%20extremely%20low-resource%20and%20endangered%20language.%20Our%20results%20and%20resources%20highlight%20a%20practical%20pathway%20to%20multilingual%20reasoning%20without%20extensive%20retraining%20in%20every%20extremely%20low-resource%20language%2C%20despite%20data%20scarcity.&entry.1838667208=http%3A//arxiv.org/abs/2504.02890v2&entry.124074799=Read"},
{"title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "author": "Basant Sharma and Prajyot Jadhav and Pranjal Paul and K. Madhava Krishna and Arun Kumar Singh", "abstract": "Navigating unknown environments with a single RGB camera is challenging, as the lack of depth information prevents reliable collision-checking. While some methods use estimated depth to build collision maps, we found that depth estimates from vision foundation models are too noisy for zero-shot navigation in cluttered environments. We propose an alternative approach: instead of using noisy estimated depth for direct collision-checking, we use it as a rich context input to a learned collision model. This model predicts the distribution of minimum obstacle clearance that the robot can expect for a given control sequence. At inference, these predictions inform a risk-aware MPC planner that minimizes estimated collision risk. We proposed a joint learning pipeline that co-trains the collision model and risk metric using both safe and unsafe trajectories. Crucially, our joint-training ensures well calibrated uncertainty in our collision model that improves navigation in highly cluttered environments. Consequently, real-world experiments show reductions in collision-rate and improvements in goal reaching and speed over several strong baselines.", "link": "http://arxiv.org/abs/2508.07387v3", "date": "2025-11-26", "relevancy": 2.3888, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6171}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5946}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoMPC%3A%20Monocular%20Vision%20Based%20Navigation%20with%20Learned%20Collision%20Model%20and%20Risk-Aware%20Model%20Predictive%20Control&body=Title%3A%20MonoMPC%3A%20Monocular%20Vision%20Based%20Navigation%20with%20Learned%20Collision%20Model%20and%20Risk-Aware%20Model%20Predictive%20Control%0AAuthor%3A%20Basant%20Sharma%20and%20Prajyot%20Jadhav%20and%20Pranjal%20Paul%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh%0AAbstract%3A%20Navigating%20unknown%20environments%20with%20a%20single%20RGB%20camera%20is%20challenging%2C%20as%20the%20lack%20of%20depth%20information%20prevents%20reliable%20collision-checking.%20While%20some%20methods%20use%20estimated%20depth%20to%20build%20collision%20maps%2C%20we%20found%20that%20depth%20estimates%20from%20vision%20foundation%20models%20are%20too%20noisy%20for%20zero-shot%20navigation%20in%20cluttered%20environments.%20We%20propose%20an%20alternative%20approach%3A%20instead%20of%20using%20noisy%20estimated%20depth%20for%20direct%20collision-checking%2C%20we%20use%20it%20as%20a%20rich%20context%20input%20to%20a%20learned%20collision%20model.%20This%20model%20predicts%20the%20distribution%20of%20minimum%20obstacle%20clearance%20that%20the%20robot%20can%20expect%20for%20a%20given%20control%20sequence.%20At%20inference%2C%20these%20predictions%20inform%20a%20risk-aware%20MPC%20planner%20that%20minimizes%20estimated%20collision%20risk.%20We%20proposed%20a%20joint%20learning%20pipeline%20that%20co-trains%20the%20collision%20model%20and%20risk%20metric%20using%20both%20safe%20and%20unsafe%20trajectories.%20Crucially%2C%20our%20joint-training%20ensures%20well%20calibrated%20uncertainty%20in%20our%20collision%20model%20that%20improves%20navigation%20in%20highly%20cluttered%20environments.%20Consequently%2C%20real-world%20experiments%20show%20reductions%20in%20collision-rate%20and%20improvements%20in%20goal%20reaching%20and%20speed%20over%20several%20strong%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoMPC%253A%2520Monocular%2520Vision%2520Based%2520Navigation%2520with%2520Learned%2520Collision%2520Model%2520and%2520Risk-Aware%2520Model%2520Predictive%2520Control%26entry.906535625%3DBasant%2520Sharma%2520and%2520Prajyot%2520Jadhav%2520and%2520Pranjal%2520Paul%2520and%2520K.%2520Madhava%2520Krishna%2520and%2520Arun%2520Kumar%2520Singh%26entry.1292438233%3DNavigating%2520unknown%2520environments%2520with%2520a%2520single%2520RGB%2520camera%2520is%2520challenging%252C%2520as%2520the%2520lack%2520of%2520depth%2520information%2520prevents%2520reliable%2520collision-checking.%2520While%2520some%2520methods%2520use%2520estimated%2520depth%2520to%2520build%2520collision%2520maps%252C%2520we%2520found%2520that%2520depth%2520estimates%2520from%2520vision%2520foundation%2520models%2520are%2520too%2520noisy%2520for%2520zero-shot%2520navigation%2520in%2520cluttered%2520environments.%2520We%2520propose%2520an%2520alternative%2520approach%253A%2520instead%2520of%2520using%2520noisy%2520estimated%2520depth%2520for%2520direct%2520collision-checking%252C%2520we%2520use%2520it%2520as%2520a%2520rich%2520context%2520input%2520to%2520a%2520learned%2520collision%2520model.%2520This%2520model%2520predicts%2520the%2520distribution%2520of%2520minimum%2520obstacle%2520clearance%2520that%2520the%2520robot%2520can%2520expect%2520for%2520a%2520given%2520control%2520sequence.%2520At%2520inference%252C%2520these%2520predictions%2520inform%2520a%2520risk-aware%2520MPC%2520planner%2520that%2520minimizes%2520estimated%2520collision%2520risk.%2520We%2520proposed%2520a%2520joint%2520learning%2520pipeline%2520that%2520co-trains%2520the%2520collision%2520model%2520and%2520risk%2520metric%2520using%2520both%2520safe%2520and%2520unsafe%2520trajectories.%2520Crucially%252C%2520our%2520joint-training%2520ensures%2520well%2520calibrated%2520uncertainty%2520in%2520our%2520collision%2520model%2520that%2520improves%2520navigation%2520in%2520highly%2520cluttered%2520environments.%2520Consequently%252C%2520real-world%2520experiments%2520show%2520reductions%2520in%2520collision-rate%2520and%2520improvements%2520in%2520goal%2520reaching%2520and%2520speed%2520over%2520several%2520strong%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoMPC%3A%20Monocular%20Vision%20Based%20Navigation%20with%20Learned%20Collision%20Model%20and%20Risk-Aware%20Model%20Predictive%20Control&entry.906535625=Basant%20Sharma%20and%20Prajyot%20Jadhav%20and%20Pranjal%20Paul%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh&entry.1292438233=Navigating%20unknown%20environments%20with%20a%20single%20RGB%20camera%20is%20challenging%2C%20as%20the%20lack%20of%20depth%20information%20prevents%20reliable%20collision-checking.%20While%20some%20methods%20use%20estimated%20depth%20to%20build%20collision%20maps%2C%20we%20found%20that%20depth%20estimates%20from%20vision%20foundation%20models%20are%20too%20noisy%20for%20zero-shot%20navigation%20in%20cluttered%20environments.%20We%20propose%20an%20alternative%20approach%3A%20instead%20of%20using%20noisy%20estimated%20depth%20for%20direct%20collision-checking%2C%20we%20use%20it%20as%20a%20rich%20context%20input%20to%20a%20learned%20collision%20model.%20This%20model%20predicts%20the%20distribution%20of%20minimum%20obstacle%20clearance%20that%20the%20robot%20can%20expect%20for%20a%20given%20control%20sequence.%20At%20inference%2C%20these%20predictions%20inform%20a%20risk-aware%20MPC%20planner%20that%20minimizes%20estimated%20collision%20risk.%20We%20proposed%20a%20joint%20learning%20pipeline%20that%20co-trains%20the%20collision%20model%20and%20risk%20metric%20using%20both%20safe%20and%20unsafe%20trajectories.%20Crucially%2C%20our%20joint-training%20ensures%20well%20calibrated%20uncertainty%20in%20our%20collision%20model%20that%20improves%20navigation%20in%20highly%20cluttered%20environments.%20Consequently%2C%20real-world%20experiments%20show%20reductions%20in%20collision-rate%20and%20improvements%20in%20goal%20reaching%20and%20speed%20over%20several%20strong%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2508.07387v3&entry.124074799=Read"},
{"title": "Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation", "author": "Lina Conti and Dennis Fucci and Marco Gaido and Matteo Negri and Guillaume Wisniewski and Luisa Bentivogli", "abstract": "Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.", "link": "http://arxiv.org/abs/2511.21517v1", "date": "2025-11-26", "relevancy": 2.3811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voice%2C%20Bias%2C%20and%20Coreference%3A%20An%20Interpretability%20Study%20of%20Gender%20in%20Speech%20Translation&body=Title%3A%20Voice%2C%20Bias%2C%20and%20Coreference%3A%20An%20Interpretability%20Study%20of%20Gender%20in%20Speech%20Translation%0AAuthor%3A%20Lina%20Conti%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Guillaume%20Wisniewski%20and%20Luisa%20Bentivogli%0AAbstract%3A%20Unlike%20text%2C%20speech%20conveys%20information%20about%20the%20speaker%2C%20such%20as%20gender%2C%20through%20acoustic%20cues%20like%20pitch.%20This%20gives%20rise%20to%20modality-specific%20bias%20concerns.%20For%20example%2C%20in%20speech%20translation%20%28ST%29%2C%20when%20translating%20from%20languages%20with%20notional%20gender%2C%20such%20as%20English%2C%20into%20languages%20where%20gender-ambiguous%20terms%20referring%20to%20the%20speaker%20are%20assigned%20grammatical%20gender%2C%20the%20speaker%27s%20vocal%20characteristics%20may%20play%20a%20role%20in%20gender%20assignment.%20This%20risks%20misgendering%20speakers%2C%20whether%20through%20masculine%20defaults%20or%20vocal-based%20assumptions.%20Yet%2C%20how%20ST%20models%20make%20these%20decisions%20remains%20poorly%20understood.%20We%20investigate%20the%20mechanisms%20ST%20models%20use%20to%20assign%20gender%20to%20speaker-referring%20terms%20across%20three%20language%20pairs%20%28en-es/fr/it%29%2C%20examining%20how%20training%20data%20patterns%2C%20internal%20language%20model%20%28ILM%29%20biases%2C%20and%20acoustic%20information%20interact.%20We%20find%20that%20models%20do%20not%20simply%20replicate%20term-specific%20gender%20associations%20from%20training%20data%2C%20but%20learn%20broader%20patterns%20of%20masculine%20prevalence.%20While%20the%20ILM%20exhibits%20strong%20masculine%20bias%2C%20models%20can%20override%20these%20preferences%20based%20on%20acoustic%20input.%20Using%20contrastive%20feature%20attribution%20on%20spectrograms%2C%20we%20reveal%20that%20the%20model%20with%20higher%20gender%20accuracy%20relies%20on%20a%20previously%20unknown%20mechanism%3A%20using%20first-person%20pronouns%20to%20link%20gendered%20terms%20back%20to%20the%20speaker%2C%20accessing%20gender%20information%20distributed%20across%20the%20frequency%20spectrum%20rather%20than%20concentrated%20in%20pitch.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoice%252C%2520Bias%252C%2520and%2520Coreference%253A%2520An%2520Interpretability%2520Study%2520of%2520Gender%2520in%2520Speech%2520Translation%26entry.906535625%3DLina%2520Conti%2520and%2520Dennis%2520Fucci%2520and%2520Marco%2520Gaido%2520and%2520Matteo%2520Negri%2520and%2520Guillaume%2520Wisniewski%2520and%2520Luisa%2520Bentivogli%26entry.1292438233%3DUnlike%2520text%252C%2520speech%2520conveys%2520information%2520about%2520the%2520speaker%252C%2520such%2520as%2520gender%252C%2520through%2520acoustic%2520cues%2520like%2520pitch.%2520This%2520gives%2520rise%2520to%2520modality-specific%2520bias%2520concerns.%2520For%2520example%252C%2520in%2520speech%2520translation%2520%2528ST%2529%252C%2520when%2520translating%2520from%2520languages%2520with%2520notional%2520gender%252C%2520such%2520as%2520English%252C%2520into%2520languages%2520where%2520gender-ambiguous%2520terms%2520referring%2520to%2520the%2520speaker%2520are%2520assigned%2520grammatical%2520gender%252C%2520the%2520speaker%2527s%2520vocal%2520characteristics%2520may%2520play%2520a%2520role%2520in%2520gender%2520assignment.%2520This%2520risks%2520misgendering%2520speakers%252C%2520whether%2520through%2520masculine%2520defaults%2520or%2520vocal-based%2520assumptions.%2520Yet%252C%2520how%2520ST%2520models%2520make%2520these%2520decisions%2520remains%2520poorly%2520understood.%2520We%2520investigate%2520the%2520mechanisms%2520ST%2520models%2520use%2520to%2520assign%2520gender%2520to%2520speaker-referring%2520terms%2520across%2520three%2520language%2520pairs%2520%2528en-es/fr/it%2529%252C%2520examining%2520how%2520training%2520data%2520patterns%252C%2520internal%2520language%2520model%2520%2528ILM%2529%2520biases%252C%2520and%2520acoustic%2520information%2520interact.%2520We%2520find%2520that%2520models%2520do%2520not%2520simply%2520replicate%2520term-specific%2520gender%2520associations%2520from%2520training%2520data%252C%2520but%2520learn%2520broader%2520patterns%2520of%2520masculine%2520prevalence.%2520While%2520the%2520ILM%2520exhibits%2520strong%2520masculine%2520bias%252C%2520models%2520can%2520override%2520these%2520preferences%2520based%2520on%2520acoustic%2520input.%2520Using%2520contrastive%2520feature%2520attribution%2520on%2520spectrograms%252C%2520we%2520reveal%2520that%2520the%2520model%2520with%2520higher%2520gender%2520accuracy%2520relies%2520on%2520a%2520previously%2520unknown%2520mechanism%253A%2520using%2520first-person%2520pronouns%2520to%2520link%2520gendered%2520terms%2520back%2520to%2520the%2520speaker%252C%2520accessing%2520gender%2520information%2520distributed%2520across%2520the%2520frequency%2520spectrum%2520rather%2520than%2520concentrated%2520in%2520pitch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voice%2C%20Bias%2C%20and%20Coreference%3A%20An%20Interpretability%20Study%20of%20Gender%20in%20Speech%20Translation&entry.906535625=Lina%20Conti%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Guillaume%20Wisniewski%20and%20Luisa%20Bentivogli&entry.1292438233=Unlike%20text%2C%20speech%20conveys%20information%20about%20the%20speaker%2C%20such%20as%20gender%2C%20through%20acoustic%20cues%20like%20pitch.%20This%20gives%20rise%20to%20modality-specific%20bias%20concerns.%20For%20example%2C%20in%20speech%20translation%20%28ST%29%2C%20when%20translating%20from%20languages%20with%20notional%20gender%2C%20such%20as%20English%2C%20into%20languages%20where%20gender-ambiguous%20terms%20referring%20to%20the%20speaker%20are%20assigned%20grammatical%20gender%2C%20the%20speaker%27s%20vocal%20characteristics%20may%20play%20a%20role%20in%20gender%20assignment.%20This%20risks%20misgendering%20speakers%2C%20whether%20through%20masculine%20defaults%20or%20vocal-based%20assumptions.%20Yet%2C%20how%20ST%20models%20make%20these%20decisions%20remains%20poorly%20understood.%20We%20investigate%20the%20mechanisms%20ST%20models%20use%20to%20assign%20gender%20to%20speaker-referring%20terms%20across%20three%20language%20pairs%20%28en-es/fr/it%29%2C%20examining%20how%20training%20data%20patterns%2C%20internal%20language%20model%20%28ILM%29%20biases%2C%20and%20acoustic%20information%20interact.%20We%20find%20that%20models%20do%20not%20simply%20replicate%20term-specific%20gender%20associations%20from%20training%20data%2C%20but%20learn%20broader%20patterns%20of%20masculine%20prevalence.%20While%20the%20ILM%20exhibits%20strong%20masculine%20bias%2C%20models%20can%20override%20these%20preferences%20based%20on%20acoustic%20input.%20Using%20contrastive%20feature%20attribution%20on%20spectrograms%2C%20we%20reveal%20that%20the%20model%20with%20higher%20gender%20accuracy%20relies%20on%20a%20previously%20unknown%20mechanism%3A%20using%20first-person%20pronouns%20to%20link%20gendered%20terms%20back%20to%20the%20speaker%2C%20accessing%20gender%20information%20distributed%20across%20the%20frequency%20spectrum%20rather%20than%20concentrated%20in%20pitch.&entry.1838667208=http%3A//arxiv.org/abs/2511.21517v1&entry.124074799=Read"},
{"title": "E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework", "author": "Adeela Islam and Stefano Fiorini and Manuel Lecha and Theodore Tsesmelis and Stuart James and Pietro Morerio and Alessio Del Bue", "abstract": "3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.", "link": "http://arxiv.org/abs/2511.21422v1", "date": "2025-11-26", "relevancy": 2.3733, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5959}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-M3RF%3A%20An%20Equivariant%20Multimodal%203D%20Re-assembly%20Framework&body=Title%3A%20E-M3RF%3A%20An%20Equivariant%20Multimodal%203D%20Re-assembly%20Framework%0AAuthor%3A%20Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Manuel%20Lecha%20and%20Theodore%20Tsesmelis%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%0AAbstract%3A%203D%20reassembly%20is%20a%20fundamental%20geometric%20problem%2C%20and%20in%20recent%20years%20it%20has%20increasingly%20been%20challenged%20by%20deep%20learning%20methods%20rather%20than%20classical%20optimization.%20While%20learning%20approaches%20have%20shown%20promising%20results%2C%20most%20still%20rely%20primarily%20on%20geometric%20features%20to%20assemble%20a%20whole%20from%20its%20parts.%20As%20a%20result%2C%20methods%20struggle%20when%20geometry%20alone%20is%20insufficient%20or%20ambiguous%2C%20for%20example%2C%20for%20small%2C%20eroded%2C%20or%20symmetric%20fragments.%20Additionally%2C%20solutions%20do%20not%20impose%20physical%20constraints%20that%20explicitly%20prevent%20overlapping%20assemblies.%20To%20address%20these%20limitations%2C%20we%20introduce%20E-M3RF%2C%20an%20equivariant%20multimodal%203D%20reassembly%20framework%20that%20takes%20as%20input%20the%20point%20clouds%2C%20containing%20both%20point%20positions%20and%20colors%20of%20fractured%20fragments%2C%20and%20predicts%20the%20transformations%20required%20to%20reassemble%20them%20using%20SE%283%29%20flow%20matching.%20Each%20fragment%20is%20represented%20by%20both%20geometric%20and%20color%20features%3A%20i%29%203D%20point%20positions%20are%20encoded%20as%20rotationconsistent%20geometric%20features%20using%20a%20rotation-equivariant%20encoder%2C%20ii%29%20the%20colors%20at%20each%203D%20point%20are%20encoded%20with%20a%20transformer.%20The%20two%20feature%20sets%20are%20then%20combined%20to%20form%20a%20multimodal%20representation.%20We%20experimented%20on%20four%20datasets%3A%20two%20synthetic%20datasets%2C%20Breaking%20Bad%20and%20Fantastic%20Breaks%2C%20and%20two%20real-world%20cultural%20heritage%20datasets%2C%20RePAIR%20and%20Presious%2C%20demonstrating%20that%20E-M3RF%20on%20the%20RePAIR%20dataset%20reduces%20rotation%20error%20by%2023.1%25%20and%20translation%20error%20by%2013.2%25%2C%20while%20Chamfer%20Distance%20decreases%20by%2018.4%25%20compared%20to%20competing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-M3RF%253A%2520An%2520Equivariant%2520Multimodal%25203D%2520Re-assembly%2520Framework%26entry.906535625%3DAdeela%2520Islam%2520and%2520Stefano%2520Fiorini%2520and%2520Manuel%2520Lecha%2520and%2520Theodore%2520Tsesmelis%2520and%2520Stuart%2520James%2520and%2520Pietro%2520Morerio%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D3D%2520reassembly%2520is%2520a%2520fundamental%2520geometric%2520problem%252C%2520and%2520in%2520recent%2520years%2520it%2520has%2520increasingly%2520been%2520challenged%2520by%2520deep%2520learning%2520methods%2520rather%2520than%2520classical%2520optimization.%2520While%2520learning%2520approaches%2520have%2520shown%2520promising%2520results%252C%2520most%2520still%2520rely%2520primarily%2520on%2520geometric%2520features%2520to%2520assemble%2520a%2520whole%2520from%2520its%2520parts.%2520As%2520a%2520result%252C%2520methods%2520struggle%2520when%2520geometry%2520alone%2520is%2520insufficient%2520or%2520ambiguous%252C%2520for%2520example%252C%2520for%2520small%252C%2520eroded%252C%2520or%2520symmetric%2520fragments.%2520Additionally%252C%2520solutions%2520do%2520not%2520impose%2520physical%2520constraints%2520that%2520explicitly%2520prevent%2520overlapping%2520assemblies.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520E-M3RF%252C%2520an%2520equivariant%2520multimodal%25203D%2520reassembly%2520framework%2520that%2520takes%2520as%2520input%2520the%2520point%2520clouds%252C%2520containing%2520both%2520point%2520positions%2520and%2520colors%2520of%2520fractured%2520fragments%252C%2520and%2520predicts%2520the%2520transformations%2520required%2520to%2520reassemble%2520them%2520using%2520SE%25283%2529%2520flow%2520matching.%2520Each%2520fragment%2520is%2520represented%2520by%2520both%2520geometric%2520and%2520color%2520features%253A%2520i%2529%25203D%2520point%2520positions%2520are%2520encoded%2520as%2520rotationconsistent%2520geometric%2520features%2520using%2520a%2520rotation-equivariant%2520encoder%252C%2520ii%2529%2520the%2520colors%2520at%2520each%25203D%2520point%2520are%2520encoded%2520with%2520a%2520transformer.%2520The%2520two%2520feature%2520sets%2520are%2520then%2520combined%2520to%2520form%2520a%2520multimodal%2520representation.%2520We%2520experimented%2520on%2520four%2520datasets%253A%2520two%2520synthetic%2520datasets%252C%2520Breaking%2520Bad%2520and%2520Fantastic%2520Breaks%252C%2520and%2520two%2520real-world%2520cultural%2520heritage%2520datasets%252C%2520RePAIR%2520and%2520Presious%252C%2520demonstrating%2520that%2520E-M3RF%2520on%2520the%2520RePAIR%2520dataset%2520reduces%2520rotation%2520error%2520by%252023.1%2525%2520and%2520translation%2520error%2520by%252013.2%2525%252C%2520while%2520Chamfer%2520Distance%2520decreases%2520by%252018.4%2525%2520compared%2520to%2520competing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-M3RF%3A%20An%20Equivariant%20Multimodal%203D%20Re-assembly%20Framework&entry.906535625=Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Manuel%20Lecha%20and%20Theodore%20Tsesmelis%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue&entry.1292438233=3D%20reassembly%20is%20a%20fundamental%20geometric%20problem%2C%20and%20in%20recent%20years%20it%20has%20increasingly%20been%20challenged%20by%20deep%20learning%20methods%20rather%20than%20classical%20optimization.%20While%20learning%20approaches%20have%20shown%20promising%20results%2C%20most%20still%20rely%20primarily%20on%20geometric%20features%20to%20assemble%20a%20whole%20from%20its%20parts.%20As%20a%20result%2C%20methods%20struggle%20when%20geometry%20alone%20is%20insufficient%20or%20ambiguous%2C%20for%20example%2C%20for%20small%2C%20eroded%2C%20or%20symmetric%20fragments.%20Additionally%2C%20solutions%20do%20not%20impose%20physical%20constraints%20that%20explicitly%20prevent%20overlapping%20assemblies.%20To%20address%20these%20limitations%2C%20we%20introduce%20E-M3RF%2C%20an%20equivariant%20multimodal%203D%20reassembly%20framework%20that%20takes%20as%20input%20the%20point%20clouds%2C%20containing%20both%20point%20positions%20and%20colors%20of%20fractured%20fragments%2C%20and%20predicts%20the%20transformations%20required%20to%20reassemble%20them%20using%20SE%283%29%20flow%20matching.%20Each%20fragment%20is%20represented%20by%20both%20geometric%20and%20color%20features%3A%20i%29%203D%20point%20positions%20are%20encoded%20as%20rotationconsistent%20geometric%20features%20using%20a%20rotation-equivariant%20encoder%2C%20ii%29%20the%20colors%20at%20each%203D%20point%20are%20encoded%20with%20a%20transformer.%20The%20two%20feature%20sets%20are%20then%20combined%20to%20form%20a%20multimodal%20representation.%20We%20experimented%20on%20four%20datasets%3A%20two%20synthetic%20datasets%2C%20Breaking%20Bad%20and%20Fantastic%20Breaks%2C%20and%20two%20real-world%20cultural%20heritage%20datasets%2C%20RePAIR%20and%20Presious%2C%20demonstrating%20that%20E-M3RF%20on%20the%20RePAIR%20dataset%20reduces%20rotation%20error%20by%2023.1%25%20and%20translation%20error%20by%2013.2%25%2C%20while%20Chamfer%20Distance%20decreases%20by%2018.4%25%20compared%20to%20competing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.21422v1&entry.124074799=Read"},
{"title": "Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams", "author": "Enes Bektas and Fazli Can", "abstract": "Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.", "link": "http://arxiv.org/abs/2511.21465v1", "date": "2025-11-26", "relevancy": 2.3697, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.498}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4824}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20Performance%20Through%20the%20Lens%20of%20Linear%20Independence%20of%20Classifier%20Votes%20in%20Data%20Streams&body=Title%3A%20Ensemble%20Performance%20Through%20the%20Lens%20of%20Linear%20Independence%20of%20Classifier%20Votes%20in%20Data%20Streams%0AAuthor%3A%20Enes%20Bektas%20and%20Fazli%20Can%0AAbstract%3A%20Ensemble%20learning%20improves%20classification%20performance%20by%20combining%20multiple%20base%20classifiers.%20While%20increasing%20the%20number%20of%20classifiers%20generally%20enhances%20accuracy%2C%20excessively%20large%20ensembles%20can%20lead%20to%20computational%20inefficiency%20and%20diminishing%20returns.%20This%20paper%20investigates%20the%20relationship%20between%20ensemble%20size%20and%20performance%20through%20the%20lens%20of%20linear%20independence%20among%20classifier%20votes%20in%20data%20streams.%20We%20propose%20that%20ensembles%20composed%20of%20linearly%20independent%20classifiers%20maximize%20representational%20capacity%2C%20particularly%20under%20a%20geometric%20model.%20We%20then%20generalize%20the%20importance%20of%20linear%20independence%20to%20the%20weighted%20majority%20voting%20problem.%20By%20modeling%20the%20probability%20of%20achieving%20linear%20independence%20among%20classifier%20outputs%2C%20we%20derive%20a%20theoretical%20framework%20that%20explains%20the%20trade-off%20between%20ensemble%20size%20and%20accuracy.%20Our%20analysis%20leads%20to%20a%20theoretical%20estimate%20of%20the%20ensemble%20size%20required%20to%20achieve%20a%20user-specified%20probability%20of%20linear%20independence.%20We%20validate%20our%20theory%20through%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%20using%20two%20ensemble%20methods%2C%20OzaBagging%20and%20GOOWE.%20Our%20results%20confirm%20that%20this%20theoretical%20estimate%20effectively%20identifies%20the%20point%20of%20performance%20saturation%20for%20robust%20ensembles%20like%20OzaBagging.%20Conversely%2C%20for%20complex%20weighting%20schemes%20like%20GOOWE%2C%20our%20framework%20reveals%20that%20high%20theoretical%20diversity%20can%20trigger%20algorithmic%20instability.%20Our%20implementation%20is%20publicly%20available%20to%20support%20reproducibility%20and%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520Performance%2520Through%2520the%2520Lens%2520of%2520Linear%2520Independence%2520of%2520Classifier%2520Votes%2520in%2520Data%2520Streams%26entry.906535625%3DEnes%2520Bektas%2520and%2520Fazli%2520Can%26entry.1292438233%3DEnsemble%2520learning%2520improves%2520classification%2520performance%2520by%2520combining%2520multiple%2520base%2520classifiers.%2520While%2520increasing%2520the%2520number%2520of%2520classifiers%2520generally%2520enhances%2520accuracy%252C%2520excessively%2520large%2520ensembles%2520can%2520lead%2520to%2520computational%2520inefficiency%2520and%2520diminishing%2520returns.%2520This%2520paper%2520investigates%2520the%2520relationship%2520between%2520ensemble%2520size%2520and%2520performance%2520through%2520the%2520lens%2520of%2520linear%2520independence%2520among%2520classifier%2520votes%2520in%2520data%2520streams.%2520We%2520propose%2520that%2520ensembles%2520composed%2520of%2520linearly%2520independent%2520classifiers%2520maximize%2520representational%2520capacity%252C%2520particularly%2520under%2520a%2520geometric%2520model.%2520We%2520then%2520generalize%2520the%2520importance%2520of%2520linear%2520independence%2520to%2520the%2520weighted%2520majority%2520voting%2520problem.%2520By%2520modeling%2520the%2520probability%2520of%2520achieving%2520linear%2520independence%2520among%2520classifier%2520outputs%252C%2520we%2520derive%2520a%2520theoretical%2520framework%2520that%2520explains%2520the%2520trade-off%2520between%2520ensemble%2520size%2520and%2520accuracy.%2520Our%2520analysis%2520leads%2520to%2520a%2520theoretical%2520estimate%2520of%2520the%2520ensemble%2520size%2520required%2520to%2520achieve%2520a%2520user-specified%2520probability%2520of%2520linear%2520independence.%2520We%2520validate%2520our%2520theory%2520through%2520experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520datasets%2520using%2520two%2520ensemble%2520methods%252C%2520OzaBagging%2520and%2520GOOWE.%2520Our%2520results%2520confirm%2520that%2520this%2520theoretical%2520estimate%2520effectively%2520identifies%2520the%2520point%2520of%2520performance%2520saturation%2520for%2520robust%2520ensembles%2520like%2520OzaBagging.%2520Conversely%252C%2520for%2520complex%2520weighting%2520schemes%2520like%2520GOOWE%252C%2520our%2520framework%2520reveals%2520that%2520high%2520theoretical%2520diversity%2520can%2520trigger%2520algorithmic%2520instability.%2520Our%2520implementation%2520is%2520publicly%2520available%2520to%2520support%2520reproducibility%2520and%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20Performance%20Through%20the%20Lens%20of%20Linear%20Independence%20of%20Classifier%20Votes%20in%20Data%20Streams&entry.906535625=Enes%20Bektas%20and%20Fazli%20Can&entry.1292438233=Ensemble%20learning%20improves%20classification%20performance%20by%20combining%20multiple%20base%20classifiers.%20While%20increasing%20the%20number%20of%20classifiers%20generally%20enhances%20accuracy%2C%20excessively%20large%20ensembles%20can%20lead%20to%20computational%20inefficiency%20and%20diminishing%20returns.%20This%20paper%20investigates%20the%20relationship%20between%20ensemble%20size%20and%20performance%20through%20the%20lens%20of%20linear%20independence%20among%20classifier%20votes%20in%20data%20streams.%20We%20propose%20that%20ensembles%20composed%20of%20linearly%20independent%20classifiers%20maximize%20representational%20capacity%2C%20particularly%20under%20a%20geometric%20model.%20We%20then%20generalize%20the%20importance%20of%20linear%20independence%20to%20the%20weighted%20majority%20voting%20problem.%20By%20modeling%20the%20probability%20of%20achieving%20linear%20independence%20among%20classifier%20outputs%2C%20we%20derive%20a%20theoretical%20framework%20that%20explains%20the%20trade-off%20between%20ensemble%20size%20and%20accuracy.%20Our%20analysis%20leads%20to%20a%20theoretical%20estimate%20of%20the%20ensemble%20size%20required%20to%20achieve%20a%20user-specified%20probability%20of%20linear%20independence.%20We%20validate%20our%20theory%20through%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%20using%20two%20ensemble%20methods%2C%20OzaBagging%20and%20GOOWE.%20Our%20results%20confirm%20that%20this%20theoretical%20estimate%20effectively%20identifies%20the%20point%20of%20performance%20saturation%20for%20robust%20ensembles%20like%20OzaBagging.%20Conversely%2C%20for%20complex%20weighting%20schemes%20like%20GOOWE%2C%20our%20framework%20reveals%20that%20high%20theoretical%20diversity%20can%20trigger%20algorithmic%20instability.%20Our%20implementation%20is%20publicly%20available%20to%20support%20reproducibility%20and%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.21465v1&entry.124074799=Read"},
{"title": "Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings", "author": "Leanne Nortje and Dan Oneata and Gabriel Pirlogeanu and Herman Kamper", "abstract": "Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a speech collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded speech model trained on paired images and unlabelled speech. But all experiments were done on English. Moreover, transcriptions were used to get positive and negative pairs for the contrastive loss. This paper introduces a few-shot learning scheme to mine pairs automatically without transcriptions. On English, this results in only a small drop in performance. We also - for the first time - consider VPKL on a real low-resource language, Yoruba. While scores are reasonable, here we see a bigger drop in performance compared to using ground truth pairs because the mining is less accurate in Yoruba.", "link": "http://arxiv.org/abs/2409.06013v2", "date": "2025-11-26", "relevancy": 2.3568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Visually%20Prompted%20Keyword%20Localisation%20in%20Real%20Low-Resource%20Settings&body=Title%3A%20Improved%20Visually%20Prompted%20Keyword%20Localisation%20in%20Real%20Low-Resource%20Settings%0AAuthor%3A%20Leanne%20Nortje%20and%20Dan%20Oneata%20and%20Gabriel%20Pirlogeanu%20and%20Herman%20Kamper%0AAbstract%3A%20Given%20an%20image%20query%2C%20visually%20prompted%20keyword%20localisation%20%28VPKL%29%20aims%20to%20find%20occurrences%20of%20the%20depicted%20word%20in%20a%20speech%20collection.%20This%20can%20be%20useful%20when%20transcriptions%20are%20not%20available%20for%20a%20low-resource%20language%20%28e.g.%20if%20it%20is%20unwritten%29.%20Previous%20work%20showed%20that%20VPKL%20can%20be%20performed%20with%20a%20visually%20grounded%20speech%20model%20trained%20on%20paired%20images%20and%20unlabelled%20speech.%20But%20all%20experiments%20were%20done%20on%20English.%20Moreover%2C%20transcriptions%20were%20used%20to%20get%20positive%20and%20negative%20pairs%20for%20the%20contrastive%20loss.%20This%20paper%20introduces%20a%20few-shot%20learning%20scheme%20to%20mine%20pairs%20automatically%20without%20transcriptions.%20On%20English%2C%20this%20results%20in%20only%20a%20small%20drop%20in%20performance.%20We%20also%20-%20for%20the%20first%20time%20-%20consider%20VPKL%20on%20a%20real%20low-resource%20language%2C%20Yoruba.%20While%20scores%20are%20reasonable%2C%20here%20we%20see%20a%20bigger%20drop%20in%20performance%20compared%20to%20using%20ground%20truth%20pairs%20because%20the%20mining%20is%20less%20accurate%20in%20Yoruba.%0ALink%3A%20http%3A//arxiv.org/abs/2409.06013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Visually%2520Prompted%2520Keyword%2520Localisation%2520in%2520Real%2520Low-Resource%2520Settings%26entry.906535625%3DLeanne%2520Nortje%2520and%2520Dan%2520Oneata%2520and%2520Gabriel%2520Pirlogeanu%2520and%2520Herman%2520Kamper%26entry.1292438233%3DGiven%2520an%2520image%2520query%252C%2520visually%2520prompted%2520keyword%2520localisation%2520%2528VPKL%2529%2520aims%2520to%2520find%2520occurrences%2520of%2520the%2520depicted%2520word%2520in%2520a%2520speech%2520collection.%2520This%2520can%2520be%2520useful%2520when%2520transcriptions%2520are%2520not%2520available%2520for%2520a%2520low-resource%2520language%2520%2528e.g.%2520if%2520it%2520is%2520unwritten%2529.%2520Previous%2520work%2520showed%2520that%2520VPKL%2520can%2520be%2520performed%2520with%2520a%2520visually%2520grounded%2520speech%2520model%2520trained%2520on%2520paired%2520images%2520and%2520unlabelled%2520speech.%2520But%2520all%2520experiments%2520were%2520done%2520on%2520English.%2520Moreover%252C%2520transcriptions%2520were%2520used%2520to%2520get%2520positive%2520and%2520negative%2520pairs%2520for%2520the%2520contrastive%2520loss.%2520This%2520paper%2520introduces%2520a%2520few-shot%2520learning%2520scheme%2520to%2520mine%2520pairs%2520automatically%2520without%2520transcriptions.%2520On%2520English%252C%2520this%2520results%2520in%2520only%2520a%2520small%2520drop%2520in%2520performance.%2520We%2520also%2520-%2520for%2520the%2520first%2520time%2520-%2520consider%2520VPKL%2520on%2520a%2520real%2520low-resource%2520language%252C%2520Yoruba.%2520While%2520scores%2520are%2520reasonable%252C%2520here%2520we%2520see%2520a%2520bigger%2520drop%2520in%2520performance%2520compared%2520to%2520using%2520ground%2520truth%2520pairs%2520because%2520the%2520mining%2520is%2520less%2520accurate%2520in%2520Yoruba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Visually%20Prompted%20Keyword%20Localisation%20in%20Real%20Low-Resource%20Settings&entry.906535625=Leanne%20Nortje%20and%20Dan%20Oneata%20and%20Gabriel%20Pirlogeanu%20and%20Herman%20Kamper&entry.1292438233=Given%20an%20image%20query%2C%20visually%20prompted%20keyword%20localisation%20%28VPKL%29%20aims%20to%20find%20occurrences%20of%20the%20depicted%20word%20in%20a%20speech%20collection.%20This%20can%20be%20useful%20when%20transcriptions%20are%20not%20available%20for%20a%20low-resource%20language%20%28e.g.%20if%20it%20is%20unwritten%29.%20Previous%20work%20showed%20that%20VPKL%20can%20be%20performed%20with%20a%20visually%20grounded%20speech%20model%20trained%20on%20paired%20images%20and%20unlabelled%20speech.%20But%20all%20experiments%20were%20done%20on%20English.%20Moreover%2C%20transcriptions%20were%20used%20to%20get%20positive%20and%20negative%20pairs%20for%20the%20contrastive%20loss.%20This%20paper%20introduces%20a%20few-shot%20learning%20scheme%20to%20mine%20pairs%20automatically%20without%20transcriptions.%20On%20English%2C%20this%20results%20in%20only%20a%20small%20drop%20in%20performance.%20We%20also%20-%20for%20the%20first%20time%20-%20consider%20VPKL%20on%20a%20real%20low-resource%20language%2C%20Yoruba.%20While%20scores%20are%20reasonable%2C%20here%20we%20see%20a%20bigger%20drop%20in%20performance%20compared%20to%20using%20ground%20truth%20pairs%20because%20the%20mining%20is%20less%20accurate%20in%20Yoruba.&entry.1838667208=http%3A//arxiv.org/abs/2409.06013v2&entry.124074799=Read"},
{"title": "An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter", "author": "Naoki Masuyama and Yuichiro Toda and Yusuke Nojima and Hisao Ishibuchi", "abstract": "Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT", "link": "http://arxiv.org/abs/2511.17983v2", "date": "2025-11-26", "relevancy": 2.3566, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Resonance%20Theory-based%20Topological%20Clustering%20Algorithm%20with%20a%20Self-Adjusting%20Vigilance%20Parameter&body=Title%3A%20An%20Adaptive%20Resonance%20Theory-based%20Topological%20Clustering%20Algorithm%20with%20a%20Self-Adjusting%20Vigilance%20Parameter%0AAuthor%3A%20Naoki%20Masuyama%20and%20Yuichiro%20Toda%20and%20Yusuke%20Nojima%20and%20Hisao%20Ishibuchi%0AAbstract%3A%20Clustering%20in%20stationary%20and%20nonstationary%20settings%2C%20where%20data%20distributions%20remain%20static%20or%20evolve%20over%20time%2C%20requires%20models%20that%20can%20adapt%20to%20distributional%20shifts%20while%20preserving%20previously%20learned%20cluster%20structures.%20This%20paper%20proposes%20an%20Adaptive%20Resonance%20Theory%20%28ART%29-based%20topological%20clustering%20algorithm%20that%20autonomously%20adjusts%20its%20recalculation%20interval%20and%20vigilance%20threshold%20through%20a%20diversity-driven%20adaptation%20mechanism.%20This%20mechanism%20enables%20hyperparameter-free%20learning%20that%20maintains%20cluster%20stability%20and%20continuity%20in%20dynamic%20environments.%20Experiments%20on%2024%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20algorithm%20outperforms%20state-of-the-art%20methods%20in%20both%20clustering%20performance%20and%20continual%20learning%20capability.%20These%20results%20highlight%20the%20effectiveness%20of%20the%20proposed%20parameter%20adaptation%20in%20mitigating%20catastrophic%20forgetting%20and%20maintaining%20consistent%20clustering%20in%20evolving%20data%20streams.%20Source%20code%20is%20available%20at%20https%3A//github.com/Masuyama-lab/IDAT%0ALink%3A%20http%3A//arxiv.org/abs/2511.17983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Resonance%2520Theory-based%2520Topological%2520Clustering%2520Algorithm%2520with%2520a%2520Self-Adjusting%2520Vigilance%2520Parameter%26entry.906535625%3DNaoki%2520Masuyama%2520and%2520Yuichiro%2520Toda%2520and%2520Yusuke%2520Nojima%2520and%2520Hisao%2520Ishibuchi%26entry.1292438233%3DClustering%2520in%2520stationary%2520and%2520nonstationary%2520settings%252C%2520where%2520data%2520distributions%2520remain%2520static%2520or%2520evolve%2520over%2520time%252C%2520requires%2520models%2520that%2520can%2520adapt%2520to%2520distributional%2520shifts%2520while%2520preserving%2520previously%2520learned%2520cluster%2520structures.%2520This%2520paper%2520proposes%2520an%2520Adaptive%2520Resonance%2520Theory%2520%2528ART%2529-based%2520topological%2520clustering%2520algorithm%2520that%2520autonomously%2520adjusts%2520its%2520recalculation%2520interval%2520and%2520vigilance%2520threshold%2520through%2520a%2520diversity-driven%2520adaptation%2520mechanism.%2520This%2520mechanism%2520enables%2520hyperparameter-free%2520learning%2520that%2520maintains%2520cluster%2520stability%2520and%2520continuity%2520in%2520dynamic%2520environments.%2520Experiments%2520on%252024%2520real-world%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520algorithm%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520clustering%2520performance%2520and%2520continual%2520learning%2520capability.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%2520the%2520proposed%2520parameter%2520adaptation%2520in%2520mitigating%2520catastrophic%2520forgetting%2520and%2520maintaining%2520consistent%2520clustering%2520in%2520evolving%2520data%2520streams.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//github.com/Masuyama-lab/IDAT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Resonance%20Theory-based%20Topological%20Clustering%20Algorithm%20with%20a%20Self-Adjusting%20Vigilance%20Parameter&entry.906535625=Naoki%20Masuyama%20and%20Yuichiro%20Toda%20and%20Yusuke%20Nojima%20and%20Hisao%20Ishibuchi&entry.1292438233=Clustering%20in%20stationary%20and%20nonstationary%20settings%2C%20where%20data%20distributions%20remain%20static%20or%20evolve%20over%20time%2C%20requires%20models%20that%20can%20adapt%20to%20distributional%20shifts%20while%20preserving%20previously%20learned%20cluster%20structures.%20This%20paper%20proposes%20an%20Adaptive%20Resonance%20Theory%20%28ART%29-based%20topological%20clustering%20algorithm%20that%20autonomously%20adjusts%20its%20recalculation%20interval%20and%20vigilance%20threshold%20through%20a%20diversity-driven%20adaptation%20mechanism.%20This%20mechanism%20enables%20hyperparameter-free%20learning%20that%20maintains%20cluster%20stability%20and%20continuity%20in%20dynamic%20environments.%20Experiments%20on%2024%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20algorithm%20outperforms%20state-of-the-art%20methods%20in%20both%20clustering%20performance%20and%20continual%20learning%20capability.%20These%20results%20highlight%20the%20effectiveness%20of%20the%20proposed%20parameter%20adaptation%20in%20mitigating%20catastrophic%20forgetting%20and%20maintaining%20consistent%20clustering%20in%20evolving%20data%20streams.%20Source%20code%20is%20available%20at%20https%3A//github.com/Masuyama-lab/IDAT&entry.1838667208=http%3A//arxiv.org/abs/2511.17983v2&entry.124074799=Read"},
{"title": "SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery", "author": "Da Li and Jiping Jin and Xuanlong Yu and Wei Liu and Xiaodong Cun and Kai Chen and Rui Fan and Jiangang Kong and Xi Shen", "abstract": "Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.", "link": "http://arxiv.org/abs/2511.20157v2", "date": "2025-11-26", "relevancy": 2.3537, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6072}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery&body=Title%3A%20SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery%0AAuthor%3A%20Da%20Li%20and%20Jiping%20Jin%20and%20Xuanlong%20Yu%20and%20Wei%20Liu%20and%20Xiaodong%20Cun%20and%20Kai%20Chen%20and%20Rui%20Fan%20and%20Jiangang%20Kong%20and%20Xi%20Shen%0AAbstract%3A%20Parametric%203D%20human%20models%20such%20as%20SMPL%20have%20driven%20significant%20advances%20in%20human%20pose%20and%20shape%20estimation%2C%20yet%20their%20simplified%20kinematics%20limit%20biomechanical%20realism.%20The%20recently%20proposed%20SKEL%20model%20addresses%20this%20limitation%20by%20re-rigging%20SMPL%20with%20an%20anatomically%20accurate%20skeleton.%20However%2C%20estimating%20SKEL%20parameters%20directly%20remains%20challenging%20due%20to%20limited%20training%20data%2C%20perspective%20ambiguities%2C%20and%20the%20inherent%20complexity%20of%20human%20articulation.%20We%20introduce%20SKEL-CF%2C%20a%20coarse-to-fine%20framework%20for%20SKEL%20parameter%20estimation.%20SKEL-CF%20employs%20a%20transformer-based%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20predicts%20coarse%20camera%20and%20SKEL%20parameters%2C%20and%20the%20decoder%20progressively%20refines%20them%20in%20successive%20layers.%20To%20ensure%20anatomically%20consistent%20supervision%2C%20we%20convert%20the%20existing%20SMPL-based%20dataset%204DHuman%20into%20a%20SKEL-aligned%20version%2C%204DHuman-SKEL%2C%20providing%20high-quality%20training%20data%20for%20SKEL%20estimation.%20In%20addition%2C%20to%20mitigate%20depth%20and%20scale%20ambiguities%2C%20we%20explicitly%20incorporate%20camera%20modeling%20into%20the%20SKEL-CF%20pipeline%20and%20demonstrate%20its%20importance%20across%20diverse%20viewpoints.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20the%20proposed%20design.%20On%20the%20challenging%20MOYO%20dataset%2C%20SKEL-CF%20achieves%2085.0%20MPJPE%20/%2051.4%20PA-MPJPE%2C%20significantly%20outperforming%20the%20previous%20SKEL-based%20state-of-the-art%20HSMR%20%28104.5%20/%2079.6%29.%20These%20results%20establish%20SKEL-CF%20as%20a%20scalable%20and%20anatomically%20faithful%20framework%20for%20human%20motion%20analysis%2C%20bridging%20the%20gap%20between%20computer%20vision%20and%20biomechanics.%20Our%20implementation%20is%20available%20on%20the%20project%20page%3A%20https%3A//pokerman8.github.io/SKEL-CF/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKEL-CF%253A%2520Coarse-to-Fine%2520Biomechanical%2520Skeleton%2520and%2520Surface%2520Mesh%2520Recovery%26entry.906535625%3DDa%2520Li%2520and%2520Jiping%2520Jin%2520and%2520Xuanlong%2520Yu%2520and%2520Wei%2520Liu%2520and%2520Xiaodong%2520Cun%2520and%2520Kai%2520Chen%2520and%2520Rui%2520Fan%2520and%2520Jiangang%2520Kong%2520and%2520Xi%2520Shen%26entry.1292438233%3DParametric%25203D%2520human%2520models%2520such%2520as%2520SMPL%2520have%2520driven%2520significant%2520advances%2520in%2520human%2520pose%2520and%2520shape%2520estimation%252C%2520yet%2520their%2520simplified%2520kinematics%2520limit%2520biomechanical%2520realism.%2520The%2520recently%2520proposed%2520SKEL%2520model%2520addresses%2520this%2520limitation%2520by%2520re-rigging%2520SMPL%2520with%2520an%2520anatomically%2520accurate%2520skeleton.%2520However%252C%2520estimating%2520SKEL%2520parameters%2520directly%2520remains%2520challenging%2520due%2520to%2520limited%2520training%2520data%252C%2520perspective%2520ambiguities%252C%2520and%2520the%2520inherent%2520complexity%2520of%2520human%2520articulation.%2520We%2520introduce%2520SKEL-CF%252C%2520a%2520coarse-to-fine%2520framework%2520for%2520SKEL%2520parameter%2520estimation.%2520SKEL-CF%2520employs%2520a%2520transformer-based%2520encoder-decoder%2520architecture%252C%2520where%2520the%2520encoder%2520predicts%2520coarse%2520camera%2520and%2520SKEL%2520parameters%252C%2520and%2520the%2520decoder%2520progressively%2520refines%2520them%2520in%2520successive%2520layers.%2520To%2520ensure%2520anatomically%2520consistent%2520supervision%252C%2520we%2520convert%2520the%2520existing%2520SMPL-based%2520dataset%25204DHuman%2520into%2520a%2520SKEL-aligned%2520version%252C%25204DHuman-SKEL%252C%2520providing%2520high-quality%2520training%2520data%2520for%2520SKEL%2520estimation.%2520In%2520addition%252C%2520to%2520mitigate%2520depth%2520and%2520scale%2520ambiguities%252C%2520we%2520explicitly%2520incorporate%2520camera%2520modeling%2520into%2520the%2520SKEL-CF%2520pipeline%2520and%2520demonstrate%2520its%2520importance%2520across%2520diverse%2520viewpoints.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520design.%2520On%2520the%2520challenging%2520MOYO%2520dataset%252C%2520SKEL-CF%2520achieves%252085.0%2520MPJPE%2520/%252051.4%2520PA-MPJPE%252C%2520significantly%2520outperforming%2520the%2520previous%2520SKEL-based%2520state-of-the-art%2520HSMR%2520%2528104.5%2520/%252079.6%2529.%2520These%2520results%2520establish%2520SKEL-CF%2520as%2520a%2520scalable%2520and%2520anatomically%2520faithful%2520framework%2520for%2520human%2520motion%2520analysis%252C%2520bridging%2520the%2520gap%2520between%2520computer%2520vision%2520and%2520biomechanics.%2520Our%2520implementation%2520is%2520available%2520on%2520the%2520project%2520page%253A%2520https%253A//pokerman8.github.io/SKEL-CF/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery&entry.906535625=Da%20Li%20and%20Jiping%20Jin%20and%20Xuanlong%20Yu%20and%20Wei%20Liu%20and%20Xiaodong%20Cun%20and%20Kai%20Chen%20and%20Rui%20Fan%20and%20Jiangang%20Kong%20and%20Xi%20Shen&entry.1292438233=Parametric%203D%20human%20models%20such%20as%20SMPL%20have%20driven%20significant%20advances%20in%20human%20pose%20and%20shape%20estimation%2C%20yet%20their%20simplified%20kinematics%20limit%20biomechanical%20realism.%20The%20recently%20proposed%20SKEL%20model%20addresses%20this%20limitation%20by%20re-rigging%20SMPL%20with%20an%20anatomically%20accurate%20skeleton.%20However%2C%20estimating%20SKEL%20parameters%20directly%20remains%20challenging%20due%20to%20limited%20training%20data%2C%20perspective%20ambiguities%2C%20and%20the%20inherent%20complexity%20of%20human%20articulation.%20We%20introduce%20SKEL-CF%2C%20a%20coarse-to-fine%20framework%20for%20SKEL%20parameter%20estimation.%20SKEL-CF%20employs%20a%20transformer-based%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20predicts%20coarse%20camera%20and%20SKEL%20parameters%2C%20and%20the%20decoder%20progressively%20refines%20them%20in%20successive%20layers.%20To%20ensure%20anatomically%20consistent%20supervision%2C%20we%20convert%20the%20existing%20SMPL-based%20dataset%204DHuman%20into%20a%20SKEL-aligned%20version%2C%204DHuman-SKEL%2C%20providing%20high-quality%20training%20data%20for%20SKEL%20estimation.%20In%20addition%2C%20to%20mitigate%20depth%20and%20scale%20ambiguities%2C%20we%20explicitly%20incorporate%20camera%20modeling%20into%20the%20SKEL-CF%20pipeline%20and%20demonstrate%20its%20importance%20across%20diverse%20viewpoints.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20the%20proposed%20design.%20On%20the%20challenging%20MOYO%20dataset%2C%20SKEL-CF%20achieves%2085.0%20MPJPE%20/%2051.4%20PA-MPJPE%2C%20significantly%20outperforming%20the%20previous%20SKEL-based%20state-of-the-art%20HSMR%20%28104.5%20/%2079.6%29.%20These%20results%20establish%20SKEL-CF%20as%20a%20scalable%20and%20anatomically%20faithful%20framework%20for%20human%20motion%20analysis%2C%20bridging%20the%20gap%20between%20computer%20vision%20and%20biomechanics.%20Our%20implementation%20is%20available%20on%20the%20project%20page%3A%20https%3A//pokerman8.github.io/SKEL-CF/.&entry.1838667208=http%3A//arxiv.org/abs/2511.20157v2&entry.124074799=Read"},
{"title": "Seeing without Pixels: Perception from Camera Trajectories", "author": "Zihui Xue and Kristen Grauman and Dima Damen and Andrew Zisserman and Tengda Han", "abstract": "Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, \"how you move\" can indeed reveal \"what you are doing\" (egocentric) or \"observing\" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.", "link": "http://arxiv.org/abs/2511.21681v1", "date": "2025-11-26", "relevancy": 2.3398, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20without%20Pixels%3A%20Perception%20from%20Camera%20Trajectories&body=Title%3A%20Seeing%20without%20Pixels%3A%20Perception%20from%20Camera%20Trajectories%0AAuthor%3A%20Zihui%20Xue%20and%20Kristen%20Grauman%20and%20Dima%20Damen%20and%20Andrew%20Zisserman%20and%20Tengda%20Han%0AAbstract%3A%20Can%20one%20perceive%20a%20video%27s%20content%20without%20seeing%20its%20pixels%2C%20just%20from%20the%20camera%20trajectory-the%20path%20it%20carves%20through%20space%3F%20This%20paper%20is%20the%20first%20to%20systematically%20investigate%20this%20seemingly%20implausible%20question.%20Towards%20this%20end%2C%20we%20propose%20a%20contrastive%20learning%20framework%20to%20train%20CamFormer%2C%20a%20dedicated%20encoder%20that%20projects%20camera%20pose%20trajectories%20into%20a%20joint%20embedding%20space%2C%20aligning%20them%20with%20natural%20language.%20We%20find%20that%2C%20contrary%20to%20its%20apparent%20simplicity%2C%20the%20camera%20trajectory%20is%20a%20remarkably%20informative%20signal%20to%20uncover%20video%20content.%20In%20other%20words%2C%20%22how%20you%20move%22%20can%20indeed%20reveal%20%22what%20you%20are%20doing%22%20%28egocentric%29%20or%20%22observing%22%20%28exocentric%29.%20We%20demonstrate%20the%20versatility%20of%20our%20learned%20CamFormer%20embeddings%20on%20a%20diverse%20suite%20of%20downstream%20tasks%2C%20ranging%20from%20cross-modal%20alignment%20to%20classification%20and%20temporal%20analysis.%20Importantly%2C%20our%20representations%20are%20robust%20across%20diverse%20camera%20pose%20estimation%20methods%2C%20including%20both%20high-fidelity%20multi-sensored%20and%20standard%20RGB-only%20estimators.%20Our%20findings%20establish%20camera%20trajectory%20as%20a%20lightweight%2C%20robust%2C%20and%20versatile%20modality%20for%20perceiving%20video%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520without%2520Pixels%253A%2520Perception%2520from%2520Camera%2520Trajectories%26entry.906535625%3DZihui%2520Xue%2520and%2520Kristen%2520Grauman%2520and%2520Dima%2520Damen%2520and%2520Andrew%2520Zisserman%2520and%2520Tengda%2520Han%26entry.1292438233%3DCan%2520one%2520perceive%2520a%2520video%2527s%2520content%2520without%2520seeing%2520its%2520pixels%252C%2520just%2520from%2520the%2520camera%2520trajectory-the%2520path%2520it%2520carves%2520through%2520space%253F%2520This%2520paper%2520is%2520the%2520first%2520to%2520systematically%2520investigate%2520this%2520seemingly%2520implausible%2520question.%2520Towards%2520this%2520end%252C%2520we%2520propose%2520a%2520contrastive%2520learning%2520framework%2520to%2520train%2520CamFormer%252C%2520a%2520dedicated%2520encoder%2520that%2520projects%2520camera%2520pose%2520trajectories%2520into%2520a%2520joint%2520embedding%2520space%252C%2520aligning%2520them%2520with%2520natural%2520language.%2520We%2520find%2520that%252C%2520contrary%2520to%2520its%2520apparent%2520simplicity%252C%2520the%2520camera%2520trajectory%2520is%2520a%2520remarkably%2520informative%2520signal%2520to%2520uncover%2520video%2520content.%2520In%2520other%2520words%252C%2520%2522how%2520you%2520move%2522%2520can%2520indeed%2520reveal%2520%2522what%2520you%2520are%2520doing%2522%2520%2528egocentric%2529%2520or%2520%2522observing%2522%2520%2528exocentric%2529.%2520We%2520demonstrate%2520the%2520versatility%2520of%2520our%2520learned%2520CamFormer%2520embeddings%2520on%2520a%2520diverse%2520suite%2520of%2520downstream%2520tasks%252C%2520ranging%2520from%2520cross-modal%2520alignment%2520to%2520classification%2520and%2520temporal%2520analysis.%2520Importantly%252C%2520our%2520representations%2520are%2520robust%2520across%2520diverse%2520camera%2520pose%2520estimation%2520methods%252C%2520including%2520both%2520high-fidelity%2520multi-sensored%2520and%2520standard%2520RGB-only%2520estimators.%2520Our%2520findings%2520establish%2520camera%2520trajectory%2520as%2520a%2520lightweight%252C%2520robust%252C%2520and%2520versatile%2520modality%2520for%2520perceiving%2520video%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20without%20Pixels%3A%20Perception%20from%20Camera%20Trajectories&entry.906535625=Zihui%20Xue%20and%20Kristen%20Grauman%20and%20Dima%20Damen%20and%20Andrew%20Zisserman%20and%20Tengda%20Han&entry.1292438233=Can%20one%20perceive%20a%20video%27s%20content%20without%20seeing%20its%20pixels%2C%20just%20from%20the%20camera%20trajectory-the%20path%20it%20carves%20through%20space%3F%20This%20paper%20is%20the%20first%20to%20systematically%20investigate%20this%20seemingly%20implausible%20question.%20Towards%20this%20end%2C%20we%20propose%20a%20contrastive%20learning%20framework%20to%20train%20CamFormer%2C%20a%20dedicated%20encoder%20that%20projects%20camera%20pose%20trajectories%20into%20a%20joint%20embedding%20space%2C%20aligning%20them%20with%20natural%20language.%20We%20find%20that%2C%20contrary%20to%20its%20apparent%20simplicity%2C%20the%20camera%20trajectory%20is%20a%20remarkably%20informative%20signal%20to%20uncover%20video%20content.%20In%20other%20words%2C%20%22how%20you%20move%22%20can%20indeed%20reveal%20%22what%20you%20are%20doing%22%20%28egocentric%29%20or%20%22observing%22%20%28exocentric%29.%20We%20demonstrate%20the%20versatility%20of%20our%20learned%20CamFormer%20embeddings%20on%20a%20diverse%20suite%20of%20downstream%20tasks%2C%20ranging%20from%20cross-modal%20alignment%20to%20classification%20and%20temporal%20analysis.%20Importantly%2C%20our%20representations%20are%20robust%20across%20diverse%20camera%20pose%20estimation%20methods%2C%20including%20both%20high-fidelity%20multi-sensored%20and%20standard%20RGB-only%20estimators.%20Our%20findings%20establish%20camera%20trajectory%20as%20a%20lightweight%2C%20robust%2C%20and%20versatile%20modality%20for%20perceiving%20video%20content.&entry.1838667208=http%3A//arxiv.org/abs/2511.21681v1&entry.124074799=Read"},
{"title": "HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal", "author": "Kexin Li and Xiao Hu and Ilya Grishchenko and David Lie", "abstract": "The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.\n  We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.", "link": "http://arxiv.org/abs/2511.21577v1", "date": "2025-11-26", "relevancy": 2.3281, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4855}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4672}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HarmonicAttack%3A%20An%20Adaptive%20Cross-Domain%20Audio%20Watermark%20Removal&body=Title%3A%20HarmonicAttack%3A%20An%20Adaptive%20Cross-Domain%20Audio%20Watermark%20Removal%0AAuthor%3A%20Kexin%20Li%20and%20Xiao%20Hu%20and%20Ilya%20Grishchenko%20and%20David%20Lie%0AAbstract%3A%20The%20availability%20of%20high-quality%2C%20AI-generated%20audio%20raises%20security%20challenges%20such%20as%20misinformation%20campaigns%20and%20voice-cloning%20fraud.%20A%20key%20defense%20against%20the%20misuse%20of%20AI-generated%20audio%20is%20by%20watermarking%20it%2C%20so%20that%20it%20can%20be%20easily%20distinguished%20from%20genuine%20audio.%20As%20those%20seeking%20to%20misuse%20AI-generated%20audio%20may%20thus%20seek%20to%20remove%20audio%20watermarks%2C%20studying%20effective%20watermark%20removal%20techniques%20is%20critical%20to%20being%20able%20to%20objectively%20evaluate%20the%20robustness%20of%20audio%20watermarks%20against%20removal.%20Previous%20watermark%20removal%20schemes%20either%20assume%20impractical%20knowledge%20of%20the%20watermarks%20they%20are%20designed%20to%20remove%20or%20are%20computationally%20expensive%2C%20potentially%20generating%20a%20false%20sense%20of%20confidence%20in%20current%20watermark%20schemes.%0A%20%20We%20introduce%20HarmonicAttack%2C%20an%20efficient%20audio%20watermark%20removal%20method%20that%20only%20requires%20the%20basic%20ability%20to%20generate%20the%20watermarks%20from%20the%20targeted%20scheme%20and%20nothing%20else.%20With%20this%2C%20we%20are%20able%20to%20train%20a%20general%20watermark%20removal%20model%20that%20is%20able%20to%20remove%20the%20watermarks%20generated%20by%20the%20targeted%20scheme%20from%20any%20watermarked%20audio%20sample.%20HarmonicAttack%20employs%20a%20dual-path%20convolutional%20autoencoder%20that%20operates%20in%20both%20temporal%20and%20frequency%20domains%2C%20along%20with%20GAN-style%20training%2C%20to%20separate%20the%20watermark%20from%20the%20original%20audio.%20When%20evaluated%20against%20state-of-the-art%20watermark%20schemes%20AudioSeal%2C%20WavMark%2C%20and%20Silentcipher%2C%20HarmonicAttack%20demonstrates%20greater%20watermark%20removal%20ability%20than%20previous%20watermark%20removal%20methods%20with%20near%20real-time%20performance.%20Moreover%2C%20while%20HarmonicAttack%20requires%20training%2C%20we%20find%20that%20it%20is%20able%20to%20transfer%20to%20out-of-distribution%20samples%20with%20minimal%20degradation%20in%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonicAttack%253A%2520An%2520Adaptive%2520Cross-Domain%2520Audio%2520Watermark%2520Removal%26entry.906535625%3DKexin%2520Li%2520and%2520Xiao%2520Hu%2520and%2520Ilya%2520Grishchenko%2520and%2520David%2520Lie%26entry.1292438233%3DThe%2520availability%2520of%2520high-quality%252C%2520AI-generated%2520audio%2520raises%2520security%2520challenges%2520such%2520as%2520misinformation%2520campaigns%2520and%2520voice-cloning%2520fraud.%2520A%2520key%2520defense%2520against%2520the%2520misuse%2520of%2520AI-generated%2520audio%2520is%2520by%2520watermarking%2520it%252C%2520so%2520that%2520it%2520can%2520be%2520easily%2520distinguished%2520from%2520genuine%2520audio.%2520As%2520those%2520seeking%2520to%2520misuse%2520AI-generated%2520audio%2520may%2520thus%2520seek%2520to%2520remove%2520audio%2520watermarks%252C%2520studying%2520effective%2520watermark%2520removal%2520techniques%2520is%2520critical%2520to%2520being%2520able%2520to%2520objectively%2520evaluate%2520the%2520robustness%2520of%2520audio%2520watermarks%2520against%2520removal.%2520Previous%2520watermark%2520removal%2520schemes%2520either%2520assume%2520impractical%2520knowledge%2520of%2520the%2520watermarks%2520they%2520are%2520designed%2520to%2520remove%2520or%2520are%2520computationally%2520expensive%252C%2520potentially%2520generating%2520a%2520false%2520sense%2520of%2520confidence%2520in%2520current%2520watermark%2520schemes.%250A%2520%2520We%2520introduce%2520HarmonicAttack%252C%2520an%2520efficient%2520audio%2520watermark%2520removal%2520method%2520that%2520only%2520requires%2520the%2520basic%2520ability%2520to%2520generate%2520the%2520watermarks%2520from%2520the%2520targeted%2520scheme%2520and%2520nothing%2520else.%2520With%2520this%252C%2520we%2520are%2520able%2520to%2520train%2520a%2520general%2520watermark%2520removal%2520model%2520that%2520is%2520able%2520to%2520remove%2520the%2520watermarks%2520generated%2520by%2520the%2520targeted%2520scheme%2520from%2520any%2520watermarked%2520audio%2520sample.%2520HarmonicAttack%2520employs%2520a%2520dual-path%2520convolutional%2520autoencoder%2520that%2520operates%2520in%2520both%2520temporal%2520and%2520frequency%2520domains%252C%2520along%2520with%2520GAN-style%2520training%252C%2520to%2520separate%2520the%2520watermark%2520from%2520the%2520original%2520audio.%2520When%2520evaluated%2520against%2520state-of-the-art%2520watermark%2520schemes%2520AudioSeal%252C%2520WavMark%252C%2520and%2520Silentcipher%252C%2520HarmonicAttack%2520demonstrates%2520greater%2520watermark%2520removal%2520ability%2520than%2520previous%2520watermark%2520removal%2520methods%2520with%2520near%2520real-time%2520performance.%2520Moreover%252C%2520while%2520HarmonicAttack%2520requires%2520training%252C%2520we%2520find%2520that%2520it%2520is%2520able%2520to%2520transfer%2520to%2520out-of-distribution%2520samples%2520with%2520minimal%2520degradation%2520in%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HarmonicAttack%3A%20An%20Adaptive%20Cross-Domain%20Audio%20Watermark%20Removal&entry.906535625=Kexin%20Li%20and%20Xiao%20Hu%20and%20Ilya%20Grishchenko%20and%20David%20Lie&entry.1292438233=The%20availability%20of%20high-quality%2C%20AI-generated%20audio%20raises%20security%20challenges%20such%20as%20misinformation%20campaigns%20and%20voice-cloning%20fraud.%20A%20key%20defense%20against%20the%20misuse%20of%20AI-generated%20audio%20is%20by%20watermarking%20it%2C%20so%20that%20it%20can%20be%20easily%20distinguished%20from%20genuine%20audio.%20As%20those%20seeking%20to%20misuse%20AI-generated%20audio%20may%20thus%20seek%20to%20remove%20audio%20watermarks%2C%20studying%20effective%20watermark%20removal%20techniques%20is%20critical%20to%20being%20able%20to%20objectively%20evaluate%20the%20robustness%20of%20audio%20watermarks%20against%20removal.%20Previous%20watermark%20removal%20schemes%20either%20assume%20impractical%20knowledge%20of%20the%20watermarks%20they%20are%20designed%20to%20remove%20or%20are%20computationally%20expensive%2C%20potentially%20generating%20a%20false%20sense%20of%20confidence%20in%20current%20watermark%20schemes.%0A%20%20We%20introduce%20HarmonicAttack%2C%20an%20efficient%20audio%20watermark%20removal%20method%20that%20only%20requires%20the%20basic%20ability%20to%20generate%20the%20watermarks%20from%20the%20targeted%20scheme%20and%20nothing%20else.%20With%20this%2C%20we%20are%20able%20to%20train%20a%20general%20watermark%20removal%20model%20that%20is%20able%20to%20remove%20the%20watermarks%20generated%20by%20the%20targeted%20scheme%20from%20any%20watermarked%20audio%20sample.%20HarmonicAttack%20employs%20a%20dual-path%20convolutional%20autoencoder%20that%20operates%20in%20both%20temporal%20and%20frequency%20domains%2C%20along%20with%20GAN-style%20training%2C%20to%20separate%20the%20watermark%20from%20the%20original%20audio.%20When%20evaluated%20against%20state-of-the-art%20watermark%20schemes%20AudioSeal%2C%20WavMark%2C%20and%20Silentcipher%2C%20HarmonicAttack%20demonstrates%20greater%20watermark%20removal%20ability%20than%20previous%20watermark%20removal%20methods%20with%20near%20real-time%20performance.%20Moreover%2C%20while%20HarmonicAttack%20requires%20training%2C%20we%20find%20that%20it%20is%20able%20to%20transfer%20to%20out-of-distribution%20samples%20with%20minimal%20degradation%20in%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.21577v1&entry.124074799=Read"},
{"title": "Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance", "author": "Bram Silue and Santiago Amaya-Corredor and Patrick Mannion and Lander Willem and Pieter Libin", "abstract": "Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.", "link": "http://arxiv.org/abs/2511.21356v1", "date": "2025-11-26", "relevancy": 2.3258, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4718}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4676}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-AIRL%3A%20Enhancing%20Inverse%20Reinforcement%20Learning%20with%20Supervised%20Expert%20Guidance&body=Title%3A%20Hybrid-AIRL%3A%20Enhancing%20Inverse%20Reinforcement%20Learning%20with%20Supervised%20Expert%20Guidance%0AAuthor%3A%20Bram%20Silue%20and%20Santiago%20Amaya-Corredor%20and%20Patrick%20Mannion%20and%20Lander%20Willem%20and%20Pieter%20Libin%0AAbstract%3A%20Adversarial%20Inverse%20Reinforcement%20Learning%20%28AIRL%29%20has%20shown%20promise%20in%20addressing%20the%20sparse%20reward%20problem%20in%20reinforcement%20learning%20%28RL%29%20by%20inferring%20dense%20reward%20functions%20from%20expert%20demonstrations.%20However%2C%20its%20performance%20in%20highly%20complex%2C%20imperfect-information%20settings%20remains%20largely%20unexplored.%20To%20explore%20this%20gap%2C%20we%20evaluate%20AIRL%20in%20the%20context%20of%20Heads-Up%20Limit%20Hold%27em%20%28HULHE%29%20poker%2C%20a%20domain%20characterized%20by%20sparse%2C%20delayed%20rewards%20and%20significant%20uncertainty.%20In%20this%20setting%2C%20we%20find%20that%20AIRL%20struggles%20to%20infer%20a%20sufficiently%20informative%20reward%20function.%20To%20overcome%20this%20limitation%2C%20we%20contribute%20Hybrid-AIRL%20%28H-AIRL%29%2C%20an%20extension%20that%20enhances%20reward%20inference%20and%20policy%20learning%20by%20incorporating%20a%20supervised%20loss%20derived%20from%20expert%20data%20and%20a%20stochastic%20regularization%20mechanism.%20We%20evaluate%20H-AIRL%20on%20a%20carefully%20selected%20set%20of%20Gymnasium%20benchmarks%20and%20the%20HULHE%20poker%20setting.%20Additionally%2C%20we%20analyze%20the%20learned%20reward%20function%20through%20visualization%20to%20gain%20deeper%20insights%20into%20the%20learning%20process.%20Our%20experimental%20results%20show%20that%20H-AIRL%20achieves%20higher%20sample%20efficiency%20and%20more%20stable%20learning%20compared%20to%20AIRL.%20This%20highlights%20the%20benefits%20of%20incorporating%20supervised%20signals%20into%20inverse%20RL%20and%20establishes%20H-AIRL%20as%20a%20promising%20framework%20for%20tackling%20challenging%2C%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-AIRL%253A%2520Enhancing%2520Inverse%2520Reinforcement%2520Learning%2520with%2520Supervised%2520Expert%2520Guidance%26entry.906535625%3DBram%2520Silue%2520and%2520Santiago%2520Amaya-Corredor%2520and%2520Patrick%2520Mannion%2520and%2520Lander%2520Willem%2520and%2520Pieter%2520Libin%26entry.1292438233%3DAdversarial%2520Inverse%2520Reinforcement%2520Learning%2520%2528AIRL%2529%2520has%2520shown%2520promise%2520in%2520addressing%2520the%2520sparse%2520reward%2520problem%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520by%2520inferring%2520dense%2520reward%2520functions%2520from%2520expert%2520demonstrations.%2520However%252C%2520its%2520performance%2520in%2520highly%2520complex%252C%2520imperfect-information%2520settings%2520remains%2520largely%2520unexplored.%2520To%2520explore%2520this%2520gap%252C%2520we%2520evaluate%2520AIRL%2520in%2520the%2520context%2520of%2520Heads-Up%2520Limit%2520Hold%2527em%2520%2528HULHE%2529%2520poker%252C%2520a%2520domain%2520characterized%2520by%2520sparse%252C%2520delayed%2520rewards%2520and%2520significant%2520uncertainty.%2520In%2520this%2520setting%252C%2520we%2520find%2520that%2520AIRL%2520struggles%2520to%2520infer%2520a%2520sufficiently%2520informative%2520reward%2520function.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520contribute%2520Hybrid-AIRL%2520%2528H-AIRL%2529%252C%2520an%2520extension%2520that%2520enhances%2520reward%2520inference%2520and%2520policy%2520learning%2520by%2520incorporating%2520a%2520supervised%2520loss%2520derived%2520from%2520expert%2520data%2520and%2520a%2520stochastic%2520regularization%2520mechanism.%2520We%2520evaluate%2520H-AIRL%2520on%2520a%2520carefully%2520selected%2520set%2520of%2520Gymnasium%2520benchmarks%2520and%2520the%2520HULHE%2520poker%2520setting.%2520Additionally%252C%2520we%2520analyze%2520the%2520learned%2520reward%2520function%2520through%2520visualization%2520to%2520gain%2520deeper%2520insights%2520into%2520the%2520learning%2520process.%2520Our%2520experimental%2520results%2520show%2520that%2520H-AIRL%2520achieves%2520higher%2520sample%2520efficiency%2520and%2520more%2520stable%2520learning%2520compared%2520to%2520AIRL.%2520This%2520highlights%2520the%2520benefits%2520of%2520incorporating%2520supervised%2520signals%2520into%2520inverse%2520RL%2520and%2520establishes%2520H-AIRL%2520as%2520a%2520promising%2520framework%2520for%2520tackling%2520challenging%252C%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-AIRL%3A%20Enhancing%20Inverse%20Reinforcement%20Learning%20with%20Supervised%20Expert%20Guidance&entry.906535625=Bram%20Silue%20and%20Santiago%20Amaya-Corredor%20and%20Patrick%20Mannion%20and%20Lander%20Willem%20and%20Pieter%20Libin&entry.1292438233=Adversarial%20Inverse%20Reinforcement%20Learning%20%28AIRL%29%20has%20shown%20promise%20in%20addressing%20the%20sparse%20reward%20problem%20in%20reinforcement%20learning%20%28RL%29%20by%20inferring%20dense%20reward%20functions%20from%20expert%20demonstrations.%20However%2C%20its%20performance%20in%20highly%20complex%2C%20imperfect-information%20settings%20remains%20largely%20unexplored.%20To%20explore%20this%20gap%2C%20we%20evaluate%20AIRL%20in%20the%20context%20of%20Heads-Up%20Limit%20Hold%27em%20%28HULHE%29%20poker%2C%20a%20domain%20characterized%20by%20sparse%2C%20delayed%20rewards%20and%20significant%20uncertainty.%20In%20this%20setting%2C%20we%20find%20that%20AIRL%20struggles%20to%20infer%20a%20sufficiently%20informative%20reward%20function.%20To%20overcome%20this%20limitation%2C%20we%20contribute%20Hybrid-AIRL%20%28H-AIRL%29%2C%20an%20extension%20that%20enhances%20reward%20inference%20and%20policy%20learning%20by%20incorporating%20a%20supervised%20loss%20derived%20from%20expert%20data%20and%20a%20stochastic%20regularization%20mechanism.%20We%20evaluate%20H-AIRL%20on%20a%20carefully%20selected%20set%20of%20Gymnasium%20benchmarks%20and%20the%20HULHE%20poker%20setting.%20Additionally%2C%20we%20analyze%20the%20learned%20reward%20function%20through%20visualization%20to%20gain%20deeper%20insights%20into%20the%20learning%20process.%20Our%20experimental%20results%20show%20that%20H-AIRL%20achieves%20higher%20sample%20efficiency%20and%20more%20stable%20learning%20compared%20to%20AIRL.%20This%20highlights%20the%20benefits%20of%20incorporating%20supervised%20signals%20into%20inverse%20RL%20and%20establishes%20H-AIRL%20as%20a%20promising%20framework%20for%20tackling%20challenging%2C%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.21356v1&entry.124074799=Read"},
{"title": "CaliTex: Geometry-Calibrated Attention for View-Coherent 3D Texture Generation", "author": "Chenyu Liu and Hongze Chen and Jingzhi Bao and Lingting Zhu and Runze Zhang and Weikai Chen and Zeyu Hu and Yingda Yin and Keyang Luo and Xin Wang", "abstract": "Despite major advances brought by diffusion-based models, current 3D texture generation systems remain hindered by cross-view inconsistency -- textures that appear convincing from one viewpoint often fail to align across others. We find that this issue arises from attention ambiguity, where unstructured full attention is applied indiscriminately across tokens and modalities, causing geometric confusion and unstable appearance-structure coupling. To address this, we introduce CaliTex, a framework of geometry-calibrated attention that explicitly aligns attention with 3D structure. It introduces two modules: Part-Aligned Attention that enforces spatial alignment across semantically matched parts, and Condition-Routed Attention which routes appearance information through geometry-conditioned pathways to maintain spatial fidelity. Coupled with a two-stage diffusion transformer, CaliTex makes geometric coherence an inherent behavior of the network rather than a byproduct of optimization. Empirically, CaliTex produces seamless and view-consistent textures and outperforms both open-source and commercial baselines.", "link": "http://arxiv.org/abs/2511.21309v1", "date": "2025-11-26", "relevancy": 2.3196, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation&body=Title%3A%20CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation%0AAuthor%3A%20Chenyu%20Liu%20and%20Hongze%20Chen%20and%20Jingzhi%20Bao%20and%20Lingting%20Zhu%20and%20Runze%20Zhang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Yingda%20Yin%20and%20Keyang%20Luo%20and%20Xin%20Wang%0AAbstract%3A%20Despite%20major%20advances%20brought%20by%20diffusion-based%20models%2C%20current%203D%20texture%20generation%20systems%20remain%20hindered%20by%20cross-view%20inconsistency%20--%20textures%20that%20appear%20convincing%20from%20one%20viewpoint%20often%20fail%20to%20align%20across%20others.%20We%20find%20that%20this%20issue%20arises%20from%20attention%20ambiguity%2C%20where%20unstructured%20full%20attention%20is%20applied%20indiscriminately%20across%20tokens%20and%20modalities%2C%20causing%20geometric%20confusion%20and%20unstable%20appearance-structure%20coupling.%20To%20address%20this%2C%20we%20introduce%20CaliTex%2C%20a%20framework%20of%20geometry-calibrated%20attention%20that%20explicitly%20aligns%20attention%20with%203D%20structure.%20It%20introduces%20two%20modules%3A%20Part-Aligned%20Attention%20that%20enforces%20spatial%20alignment%20across%20semantically%20matched%20parts%2C%20and%20Condition-Routed%20Attention%20which%20routes%20appearance%20information%20through%20geometry-conditioned%20pathways%20to%20maintain%20spatial%20fidelity.%20Coupled%20with%20a%20two-stage%20diffusion%20transformer%2C%20CaliTex%20makes%20geometric%20coherence%20an%20inherent%20behavior%20of%20the%20network%20rather%20than%20a%20byproduct%20of%20optimization.%20Empirically%2C%20CaliTex%20produces%20seamless%20and%20view-consistent%20textures%20and%20outperforms%20both%20open-source%20and%20commercial%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaliTex%253A%2520Geometry-Calibrated%2520Attention%2520for%2520View-Coherent%25203D%2520Texture%2520Generation%26entry.906535625%3DChenyu%2520Liu%2520and%2520Hongze%2520Chen%2520and%2520Jingzhi%2520Bao%2520and%2520Lingting%2520Zhu%2520and%2520Runze%2520Zhang%2520and%2520Weikai%2520Chen%2520and%2520Zeyu%2520Hu%2520and%2520Yingda%2520Yin%2520and%2520Keyang%2520Luo%2520and%2520Xin%2520Wang%26entry.1292438233%3DDespite%2520major%2520advances%2520brought%2520by%2520diffusion-based%2520models%252C%2520current%25203D%2520texture%2520generation%2520systems%2520remain%2520hindered%2520by%2520cross-view%2520inconsistency%2520--%2520textures%2520that%2520appear%2520convincing%2520from%2520one%2520viewpoint%2520often%2520fail%2520to%2520align%2520across%2520others.%2520We%2520find%2520that%2520this%2520issue%2520arises%2520from%2520attention%2520ambiguity%252C%2520where%2520unstructured%2520full%2520attention%2520is%2520applied%2520indiscriminately%2520across%2520tokens%2520and%2520modalities%252C%2520causing%2520geometric%2520confusion%2520and%2520unstable%2520appearance-structure%2520coupling.%2520To%2520address%2520this%252C%2520we%2520introduce%2520CaliTex%252C%2520a%2520framework%2520of%2520geometry-calibrated%2520attention%2520that%2520explicitly%2520aligns%2520attention%2520with%25203D%2520structure.%2520It%2520introduces%2520two%2520modules%253A%2520Part-Aligned%2520Attention%2520that%2520enforces%2520spatial%2520alignment%2520across%2520semantically%2520matched%2520parts%252C%2520and%2520Condition-Routed%2520Attention%2520which%2520routes%2520appearance%2520information%2520through%2520geometry-conditioned%2520pathways%2520to%2520maintain%2520spatial%2520fidelity.%2520Coupled%2520with%2520a%2520two-stage%2520diffusion%2520transformer%252C%2520CaliTex%2520makes%2520geometric%2520coherence%2520an%2520inherent%2520behavior%2520of%2520the%2520network%2520rather%2520than%2520a%2520byproduct%2520of%2520optimization.%2520Empirically%252C%2520CaliTex%2520produces%2520seamless%2520and%2520view-consistent%2520textures%2520and%2520outperforms%2520both%2520open-source%2520and%2520commercial%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation&entry.906535625=Chenyu%20Liu%20and%20Hongze%20Chen%20and%20Jingzhi%20Bao%20and%20Lingting%20Zhu%20and%20Runze%20Zhang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Yingda%20Yin%20and%20Keyang%20Luo%20and%20Xin%20Wang&entry.1292438233=Despite%20major%20advances%20brought%20by%20diffusion-based%20models%2C%20current%203D%20texture%20generation%20systems%20remain%20hindered%20by%20cross-view%20inconsistency%20--%20textures%20that%20appear%20convincing%20from%20one%20viewpoint%20often%20fail%20to%20align%20across%20others.%20We%20find%20that%20this%20issue%20arises%20from%20attention%20ambiguity%2C%20where%20unstructured%20full%20attention%20is%20applied%20indiscriminately%20across%20tokens%20and%20modalities%2C%20causing%20geometric%20confusion%20and%20unstable%20appearance-structure%20coupling.%20To%20address%20this%2C%20we%20introduce%20CaliTex%2C%20a%20framework%20of%20geometry-calibrated%20attention%20that%20explicitly%20aligns%20attention%20with%203D%20structure.%20It%20introduces%20two%20modules%3A%20Part-Aligned%20Attention%20that%20enforces%20spatial%20alignment%20across%20semantically%20matched%20parts%2C%20and%20Condition-Routed%20Attention%20which%20routes%20appearance%20information%20through%20geometry-conditioned%20pathways%20to%20maintain%20spatial%20fidelity.%20Coupled%20with%20a%20two-stage%20diffusion%20transformer%2C%20CaliTex%20makes%20geometric%20coherence%20an%20inherent%20behavior%20of%20the%20network%20rather%20than%20a%20byproduct%20of%20optimization.%20Empirically%2C%20CaliTex%20produces%20seamless%20and%20view-consistent%20textures%20and%20outperforms%20both%20open-source%20and%20commercial%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.21309v1&entry.124074799=Read"},
{"title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language", "author": "Qixun Wang and Yang Shi and Yifei Wang and Yuanxing Zhang and Pengfei Wan and Kun Gai and Xianghua Ying and Yisen Wang", "abstract": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.", "link": "http://arxiv.org/abs/2511.21395v1", "date": "2025-11-26", "relevancy": 2.3157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monet%3A%20Reasoning%20in%20Latent%20Visual%20Space%20Beyond%20Images%20and%20Language&body=Title%3A%20Monet%3A%20Reasoning%20in%20Latent%20Visual%20Space%20Beyond%20Images%20and%20Language%0AAuthor%3A%20Qixun%20Wang%20and%20Yang%20Shi%20and%20Yifei%20Wang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Xianghua%20Ying%20and%20Yisen%20Wang%0AAbstract%3A%20%22Thinking%20with%20images%22%20has%20emerged%20as%20an%20effective%20paradigm%20for%20advancing%20visual%20reasoning%2C%20extending%20beyond%20text-only%20chains%20of%20thought%20by%20injecting%20visual%20evidence%20into%20intermediate%20reasoning%20steps.%20However%2C%20existing%20methods%20fall%20short%20of%20human-like%20abstract%20visual%20thinking%2C%20as%20their%20flexibility%20is%20fundamentally%20limited%20by%20external%20tools.%20In%20this%20work%2C%20we%20introduce%20Monet%2C%20a%20training%20framework%20that%20enables%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20reason%20directly%20within%20the%20latent%20visual%20space%20by%20generating%20continuous%20embeddings%20that%20function%20as%20intermediate%20visual%20thoughts.%20We%20identify%20two%20core%20challenges%20in%20training%20MLLMs%20for%20latent%20visual%20reasoning%3A%20high%20computational%20cost%20in%20latent-vision%20alignment%20and%20insufficient%20supervision%20over%20latent%20embeddings%2C%20and%20address%20them%20with%20a%20three-stage%20distillation-based%20supervised%20fine-tuning%20%28SFT%29%20pipeline.%20We%20further%20reveal%20a%20limitation%20of%20applying%20GRPO%20to%20latent%20reasoning%3A%20it%20primarily%20enhances%20text-based%20reasoning%20rather%20than%20latent%20reasoning.%20To%20overcome%20this%2C%20we%20propose%20VLPO%20%28Visual-latent%20Policy%20Optimization%29%2C%20a%20reinforcement%20learning%20method%20that%20explicitly%20incorporates%20latent%20embeddings%20into%20policy%20gradient%20updates.%20To%20support%20SFT%2C%20we%20construct%20Monet-SFT-125K%2C%20a%20high-quality%20text-image%20interleaved%20CoT%20dataset%20containing%20125K%20real-world%2C%20chart%2C%20OCR%2C%20and%20geometry%20CoTs.%20Our%20model%2C%20Monet-7B%2C%20shows%20consistent%20gains%20across%20real-world%20perception%20and%20reasoning%20benchmarks%20and%20exhibits%20strong%20out-of-distribution%20generalization%20on%20challenging%20abstract%20visual%20reasoning%20tasks.%20We%20also%20empirically%20analyze%20the%20role%20of%20each%20training%20component%20and%20discuss%20our%20early%20unsuccessful%20attempts%2C%20providing%20insights%20for%20future%20developments%20in%20visual%20latent%20reasoning.%20Our%20model%2C%20data%2C%20and%20code%20are%20available%20at%20https%3A//github.com/NOVAglow646/Monet.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonet%253A%2520Reasoning%2520in%2520Latent%2520Visual%2520Space%2520Beyond%2520Images%2520and%2520Language%26entry.906535625%3DQixun%2520Wang%2520and%2520Yang%2520Shi%2520and%2520Yifei%2520Wang%2520and%2520Yuanxing%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Xianghua%2520Ying%2520and%2520Yisen%2520Wang%26entry.1292438233%3D%2522Thinking%2520with%2520images%2522%2520has%2520emerged%2520as%2520an%2520effective%2520paradigm%2520for%2520advancing%2520visual%2520reasoning%252C%2520extending%2520beyond%2520text-only%2520chains%2520of%2520thought%2520by%2520injecting%2520visual%2520evidence%2520into%2520intermediate%2520reasoning%2520steps.%2520However%252C%2520existing%2520methods%2520fall%2520short%2520of%2520human-like%2520abstract%2520visual%2520thinking%252C%2520as%2520their%2520flexibility%2520is%2520fundamentally%2520limited%2520by%2520external%2520tools.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Monet%252C%2520a%2520training%2520framework%2520that%2520enables%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520reason%2520directly%2520within%2520the%2520latent%2520visual%2520space%2520by%2520generating%2520continuous%2520embeddings%2520that%2520function%2520as%2520intermediate%2520visual%2520thoughts.%2520We%2520identify%2520two%2520core%2520challenges%2520in%2520training%2520MLLMs%2520for%2520latent%2520visual%2520reasoning%253A%2520high%2520computational%2520cost%2520in%2520latent-vision%2520alignment%2520and%2520insufficient%2520supervision%2520over%2520latent%2520embeddings%252C%2520and%2520address%2520them%2520with%2520a%2520three-stage%2520distillation-based%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520pipeline.%2520We%2520further%2520reveal%2520a%2520limitation%2520of%2520applying%2520GRPO%2520to%2520latent%2520reasoning%253A%2520it%2520primarily%2520enhances%2520text-based%2520reasoning%2520rather%2520than%2520latent%2520reasoning.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520VLPO%2520%2528Visual-latent%2520Policy%2520Optimization%2529%252C%2520a%2520reinforcement%2520learning%2520method%2520that%2520explicitly%2520incorporates%2520latent%2520embeddings%2520into%2520policy%2520gradient%2520updates.%2520To%2520support%2520SFT%252C%2520we%2520construct%2520Monet-SFT-125K%252C%2520a%2520high-quality%2520text-image%2520interleaved%2520CoT%2520dataset%2520containing%2520125K%2520real-world%252C%2520chart%252C%2520OCR%252C%2520and%2520geometry%2520CoTs.%2520Our%2520model%252C%2520Monet-7B%252C%2520shows%2520consistent%2520gains%2520across%2520real-world%2520perception%2520and%2520reasoning%2520benchmarks%2520and%2520exhibits%2520strong%2520out-of-distribution%2520generalization%2520on%2520challenging%2520abstract%2520visual%2520reasoning%2520tasks.%2520We%2520also%2520empirically%2520analyze%2520the%2520role%2520of%2520each%2520training%2520component%2520and%2520discuss%2520our%2520early%2520unsuccessful%2520attempts%252C%2520providing%2520insights%2520for%2520future%2520developments%2520in%2520visual%2520latent%2520reasoning.%2520Our%2520model%252C%2520data%252C%2520and%2520code%2520are%2520available%2520at%2520https%253A//github.com/NOVAglow646/Monet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monet%3A%20Reasoning%20in%20Latent%20Visual%20Space%20Beyond%20Images%20and%20Language&entry.906535625=Qixun%20Wang%20and%20Yang%20Shi%20and%20Yifei%20Wang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Xianghua%20Ying%20and%20Yisen%20Wang&entry.1292438233=%22Thinking%20with%20images%22%20has%20emerged%20as%20an%20effective%20paradigm%20for%20advancing%20visual%20reasoning%2C%20extending%20beyond%20text-only%20chains%20of%20thought%20by%20injecting%20visual%20evidence%20into%20intermediate%20reasoning%20steps.%20However%2C%20existing%20methods%20fall%20short%20of%20human-like%20abstract%20visual%20thinking%2C%20as%20their%20flexibility%20is%20fundamentally%20limited%20by%20external%20tools.%20In%20this%20work%2C%20we%20introduce%20Monet%2C%20a%20training%20framework%20that%20enables%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20reason%20directly%20within%20the%20latent%20visual%20space%20by%20generating%20continuous%20embeddings%20that%20function%20as%20intermediate%20visual%20thoughts.%20We%20identify%20two%20core%20challenges%20in%20training%20MLLMs%20for%20latent%20visual%20reasoning%3A%20high%20computational%20cost%20in%20latent-vision%20alignment%20and%20insufficient%20supervision%20over%20latent%20embeddings%2C%20and%20address%20them%20with%20a%20three-stage%20distillation-based%20supervised%20fine-tuning%20%28SFT%29%20pipeline.%20We%20further%20reveal%20a%20limitation%20of%20applying%20GRPO%20to%20latent%20reasoning%3A%20it%20primarily%20enhances%20text-based%20reasoning%20rather%20than%20latent%20reasoning.%20To%20overcome%20this%2C%20we%20propose%20VLPO%20%28Visual-latent%20Policy%20Optimization%29%2C%20a%20reinforcement%20learning%20method%20that%20explicitly%20incorporates%20latent%20embeddings%20into%20policy%20gradient%20updates.%20To%20support%20SFT%2C%20we%20construct%20Monet-SFT-125K%2C%20a%20high-quality%20text-image%20interleaved%20CoT%20dataset%20containing%20125K%20real-world%2C%20chart%2C%20OCR%2C%20and%20geometry%20CoTs.%20Our%20model%2C%20Monet-7B%2C%20shows%20consistent%20gains%20across%20real-world%20perception%20and%20reasoning%20benchmarks%20and%20exhibits%20strong%20out-of-distribution%20generalization%20on%20challenging%20abstract%20visual%20reasoning%20tasks.%20We%20also%20empirically%20analyze%20the%20role%20of%20each%20training%20component%20and%20discuss%20our%20early%20unsuccessful%20attempts%2C%20providing%20insights%20for%20future%20developments%20in%20visual%20latent%20reasoning.%20Our%20model%2C%20data%2C%20and%20code%20are%20available%20at%20https%3A//github.com/NOVAglow646/Monet.&entry.1838667208=http%3A//arxiv.org/abs/2511.21395v1&entry.124074799=Read"},
{"title": "Neural NMPC through Signed Distance Field Encoding for Collision Avoidance", "author": "Martin Jacquet and Marvin Harms and Kostas Alexis", "abstract": "This paper introduces a neural Nonlinear Model Predictive Control (NMPC) framework for mapless, collision-free navigation in unknown environments with Aerial Robots, using onboard range sensing. We leverage deep neural networks to encode a single range image, capturing all the available information about the environment, into a Signed Distance Function (SDF). The proposed neural architecture consists of two cascaded networks: a convolutional encoder that compresses the input image into a low-dimensional latent vector, and a Multi-Layer Perceptron that approximates the corresponding spatial SDF. This latter network parametrizes an explicit position constraint used for collision avoidance, which is embedded in a velocity-tracking NMPC that outputs thrust and attitude commands to the robot. First, a theoretical analysis of the contributed NMPC is conducted, verifying recursive feasibility and stability properties under fixed observations. Subsequently, we evaluate the open-loop performance of the learning-based components as well as the closed-loop performance of the controller in simulations and experiments. The simulation study includes an ablation study, comparisons with two state-of-the-art local navigation methods, and an assessment of the resilience to drifting odometry. The real-world experiments are conducted in forest environments, demonstrating that the neural NMPC effectively performs collision avoidance in cluttered settings against an adversarial reference velocity input and drifting position estimates.", "link": "http://arxiv.org/abs/2511.21312v1", "date": "2025-11-26", "relevancy": 2.3141, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5858}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20NMPC%20through%20Signed%20Distance%20Field%20Encoding%20for%20Collision%20Avoidance&body=Title%3A%20Neural%20NMPC%20through%20Signed%20Distance%20Field%20Encoding%20for%20Collision%20Avoidance%0AAuthor%3A%20Martin%20Jacquet%20and%20Marvin%20Harms%20and%20Kostas%20Alexis%0AAbstract%3A%20This%20paper%20introduces%20a%20neural%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20framework%20for%20mapless%2C%20collision-free%20navigation%20in%20unknown%20environments%20with%20Aerial%20Robots%2C%20using%20onboard%20range%20sensing.%20We%20leverage%20deep%20neural%20networks%20to%20encode%20a%20single%20range%20image%2C%20capturing%20all%20the%20available%20information%20about%20the%20environment%2C%20into%20a%20Signed%20Distance%20Function%20%28SDF%29.%20The%20proposed%20neural%20architecture%20consists%20of%20two%20cascaded%20networks%3A%20a%20convolutional%20encoder%20that%20compresses%20the%20input%20image%20into%20a%20low-dimensional%20latent%20vector%2C%20and%20a%20Multi-Layer%20Perceptron%20that%20approximates%20the%20corresponding%20spatial%20SDF.%20This%20latter%20network%20parametrizes%20an%20explicit%20position%20constraint%20used%20for%20collision%20avoidance%2C%20which%20is%20embedded%20in%20a%20velocity-tracking%20NMPC%20that%20outputs%20thrust%20and%20attitude%20commands%20to%20the%20robot.%20First%2C%20a%20theoretical%20analysis%20of%20the%20contributed%20NMPC%20is%20conducted%2C%20verifying%20recursive%20feasibility%20and%20stability%20properties%20under%20fixed%20observations.%20Subsequently%2C%20we%20evaluate%20the%20open-loop%20performance%20of%20the%20learning-based%20components%20as%20well%20as%20the%20closed-loop%20performance%20of%20the%20controller%20in%20simulations%20and%20experiments.%20The%20simulation%20study%20includes%20an%20ablation%20study%2C%20comparisons%20with%20two%20state-of-the-art%20local%20navigation%20methods%2C%20and%20an%20assessment%20of%20the%20resilience%20to%20drifting%20odometry.%20The%20real-world%20experiments%20are%20conducted%20in%20forest%20environments%2C%20demonstrating%20that%20the%20neural%20NMPC%20effectively%20performs%20collision%20avoidance%20in%20cluttered%20settings%20against%20an%20adversarial%20reference%20velocity%20input%20and%20drifting%20position%20estimates.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520NMPC%2520through%2520Signed%2520Distance%2520Field%2520Encoding%2520for%2520Collision%2520Avoidance%26entry.906535625%3DMartin%2520Jacquet%2520and%2520Marvin%2520Harms%2520and%2520Kostas%2520Alexis%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520neural%2520Nonlinear%2520Model%2520Predictive%2520Control%2520%2528NMPC%2529%2520framework%2520for%2520mapless%252C%2520collision-free%2520navigation%2520in%2520unknown%2520environments%2520with%2520Aerial%2520Robots%252C%2520using%2520onboard%2520range%2520sensing.%2520We%2520leverage%2520deep%2520neural%2520networks%2520to%2520encode%2520a%2520single%2520range%2520image%252C%2520capturing%2520all%2520the%2520available%2520information%2520about%2520the%2520environment%252C%2520into%2520a%2520Signed%2520Distance%2520Function%2520%2528SDF%2529.%2520The%2520proposed%2520neural%2520architecture%2520consists%2520of%2520two%2520cascaded%2520networks%253A%2520a%2520convolutional%2520encoder%2520that%2520compresses%2520the%2520input%2520image%2520into%2520a%2520low-dimensional%2520latent%2520vector%252C%2520and%2520a%2520Multi-Layer%2520Perceptron%2520that%2520approximates%2520the%2520corresponding%2520spatial%2520SDF.%2520This%2520latter%2520network%2520parametrizes%2520an%2520explicit%2520position%2520constraint%2520used%2520for%2520collision%2520avoidance%252C%2520which%2520is%2520embedded%2520in%2520a%2520velocity-tracking%2520NMPC%2520that%2520outputs%2520thrust%2520and%2520attitude%2520commands%2520to%2520the%2520robot.%2520First%252C%2520a%2520theoretical%2520analysis%2520of%2520the%2520contributed%2520NMPC%2520is%2520conducted%252C%2520verifying%2520recursive%2520feasibility%2520and%2520stability%2520properties%2520under%2520fixed%2520observations.%2520Subsequently%252C%2520we%2520evaluate%2520the%2520open-loop%2520performance%2520of%2520the%2520learning-based%2520components%2520as%2520well%2520as%2520the%2520closed-loop%2520performance%2520of%2520the%2520controller%2520in%2520simulations%2520and%2520experiments.%2520The%2520simulation%2520study%2520includes%2520an%2520ablation%2520study%252C%2520comparisons%2520with%2520two%2520state-of-the-art%2520local%2520navigation%2520methods%252C%2520and%2520an%2520assessment%2520of%2520the%2520resilience%2520to%2520drifting%2520odometry.%2520The%2520real-world%2520experiments%2520are%2520conducted%2520in%2520forest%2520environments%252C%2520demonstrating%2520that%2520the%2520neural%2520NMPC%2520effectively%2520performs%2520collision%2520avoidance%2520in%2520cluttered%2520settings%2520against%2520an%2520adversarial%2520reference%2520velocity%2520input%2520and%2520drifting%2520position%2520estimates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20NMPC%20through%20Signed%20Distance%20Field%20Encoding%20for%20Collision%20Avoidance&entry.906535625=Martin%20Jacquet%20and%20Marvin%20Harms%20and%20Kostas%20Alexis&entry.1292438233=This%20paper%20introduces%20a%20neural%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20framework%20for%20mapless%2C%20collision-free%20navigation%20in%20unknown%20environments%20with%20Aerial%20Robots%2C%20using%20onboard%20range%20sensing.%20We%20leverage%20deep%20neural%20networks%20to%20encode%20a%20single%20range%20image%2C%20capturing%20all%20the%20available%20information%20about%20the%20environment%2C%20into%20a%20Signed%20Distance%20Function%20%28SDF%29.%20The%20proposed%20neural%20architecture%20consists%20of%20two%20cascaded%20networks%3A%20a%20convolutional%20encoder%20that%20compresses%20the%20input%20image%20into%20a%20low-dimensional%20latent%20vector%2C%20and%20a%20Multi-Layer%20Perceptron%20that%20approximates%20the%20corresponding%20spatial%20SDF.%20This%20latter%20network%20parametrizes%20an%20explicit%20position%20constraint%20used%20for%20collision%20avoidance%2C%20which%20is%20embedded%20in%20a%20velocity-tracking%20NMPC%20that%20outputs%20thrust%20and%20attitude%20commands%20to%20the%20robot.%20First%2C%20a%20theoretical%20analysis%20of%20the%20contributed%20NMPC%20is%20conducted%2C%20verifying%20recursive%20feasibility%20and%20stability%20properties%20under%20fixed%20observations.%20Subsequently%2C%20we%20evaluate%20the%20open-loop%20performance%20of%20the%20learning-based%20components%20as%20well%20as%20the%20closed-loop%20performance%20of%20the%20controller%20in%20simulations%20and%20experiments.%20The%20simulation%20study%20includes%20an%20ablation%20study%2C%20comparisons%20with%20two%20state-of-the-art%20local%20navigation%20methods%2C%20and%20an%20assessment%20of%20the%20resilience%20to%20drifting%20odometry.%20The%20real-world%20experiments%20are%20conducted%20in%20forest%20environments%2C%20demonstrating%20that%20the%20neural%20NMPC%20effectively%20performs%20collision%20avoidance%20in%20cluttered%20settings%20against%20an%20adversarial%20reference%20velocity%20input%20and%20drifting%20position%20estimates.&entry.1838667208=http%3A//arxiv.org/abs/2511.21312v1&entry.124074799=Read"},
{"title": "Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss", "author": "Chou Mo and Yehyun Suh and J. Ryan Martin and Daniel Moyer", "abstract": "Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.", "link": "http://arxiv.org/abs/2511.21575v1", "date": "2025-11-26", "relevancy": 2.3123, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5913}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5697}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Landmark%20Detection%20Model%20in%20Pelvic%20Fluoroscopy%20using%202D/3D%20Registration%20Loss&body=Title%3A%20Enhanced%20Landmark%20Detection%20Model%20in%20Pelvic%20Fluoroscopy%20using%202D/3D%20Registration%20Loss%0AAuthor%3A%20Chou%20Mo%20and%20Yehyun%20Suh%20and%20J.%20Ryan%20Martin%20and%20Daniel%20Moyer%0AAbstract%3A%20Automated%20landmark%20detection%20offers%20an%20efficient%20approach%20for%20medical%20professionals%20to%20understand%20patient%20anatomic%20structure%20and%20positioning%20using%20intra-operative%20imaging.%20While%20current%20detection%20methods%20for%20pelvic%20fluoroscopy%20demonstrate%20promising%20accuracy%2C%20most%20assume%20a%20fixed%20Antero-Posterior%20view%20of%20the%20pelvis.%20However%2C%20orientation%20often%20deviates%20from%20this%20standard%20view%2C%20either%20due%20to%20repositioning%20of%20the%20imaging%20unit%20or%20of%20the%20target%20structure%20itself.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20framework%20that%20incorporates%202D/3D%20landmark%20registration%20into%20the%20training%20of%20a%20U-Net%20landmark%20prediction%20model.%20We%20analyze%20the%20performance%20difference%20by%20comparing%20landmark%20detection%20accuracy%20between%20the%20baseline%20U-Net%2C%20U-Net%20trained%20with%20Pose%20Estimation%20Loss%2C%20and%20U-Net%20fine-tuned%20with%20Pose%20Estimation%20Loss%20under%20realistic%20intra-operative%20conditions%20where%20patient%20pose%20is%20variable.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Landmark%2520Detection%2520Model%2520in%2520Pelvic%2520Fluoroscopy%2520using%25202D/3D%2520Registration%2520Loss%26entry.906535625%3DChou%2520Mo%2520and%2520Yehyun%2520Suh%2520and%2520J.%2520Ryan%2520Martin%2520and%2520Daniel%2520Moyer%26entry.1292438233%3DAutomated%2520landmark%2520detection%2520offers%2520an%2520efficient%2520approach%2520for%2520medical%2520professionals%2520to%2520understand%2520patient%2520anatomic%2520structure%2520and%2520positioning%2520using%2520intra-operative%2520imaging.%2520While%2520current%2520detection%2520methods%2520for%2520pelvic%2520fluoroscopy%2520demonstrate%2520promising%2520accuracy%252C%2520most%2520assume%2520a%2520fixed%2520Antero-Posterior%2520view%2520of%2520the%2520pelvis.%2520However%252C%2520orientation%2520often%2520deviates%2520from%2520this%2520standard%2520view%252C%2520either%2520due%2520to%2520repositioning%2520of%2520the%2520imaging%2520unit%2520or%2520of%2520the%2520target%2520structure%2520itself.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520incorporates%25202D/3D%2520landmark%2520registration%2520into%2520the%2520training%2520of%2520a%2520U-Net%2520landmark%2520prediction%2520model.%2520We%2520analyze%2520the%2520performance%2520difference%2520by%2520comparing%2520landmark%2520detection%2520accuracy%2520between%2520the%2520baseline%2520U-Net%252C%2520U-Net%2520trained%2520with%2520Pose%2520Estimation%2520Loss%252C%2520and%2520U-Net%2520fine-tuned%2520with%2520Pose%2520Estimation%2520Loss%2520under%2520realistic%2520intra-operative%2520conditions%2520where%2520patient%2520pose%2520is%2520variable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Landmark%20Detection%20Model%20in%20Pelvic%20Fluoroscopy%20using%202D/3D%20Registration%20Loss&entry.906535625=Chou%20Mo%20and%20Yehyun%20Suh%20and%20J.%20Ryan%20Martin%20and%20Daniel%20Moyer&entry.1292438233=Automated%20landmark%20detection%20offers%20an%20efficient%20approach%20for%20medical%20professionals%20to%20understand%20patient%20anatomic%20structure%20and%20positioning%20using%20intra-operative%20imaging.%20While%20current%20detection%20methods%20for%20pelvic%20fluoroscopy%20demonstrate%20promising%20accuracy%2C%20most%20assume%20a%20fixed%20Antero-Posterior%20view%20of%20the%20pelvis.%20However%2C%20orientation%20often%20deviates%20from%20this%20standard%20view%2C%20either%20due%20to%20repositioning%20of%20the%20imaging%20unit%20or%20of%20the%20target%20structure%20itself.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20framework%20that%20incorporates%202D/3D%20landmark%20registration%20into%20the%20training%20of%20a%20U-Net%20landmark%20prediction%20model.%20We%20analyze%20the%20performance%20difference%20by%20comparing%20landmark%20detection%20accuracy%20between%20the%20baseline%20U-Net%2C%20U-Net%20trained%20with%20Pose%20Estimation%20Loss%2C%20and%20U-Net%20fine-tuned%20with%20Pose%20Estimation%20Loss%20under%20realistic%20intra-operative%20conditions%20where%20patient%20pose%20is%20variable.&entry.1838667208=http%3A//arxiv.org/abs/2511.21575v1&entry.124074799=Read"},
{"title": "SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition", "author": "Peiran Xu and Sudong Wang and Yao Zhu and Jianing Li and Yunjian Zhang", "abstract": "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.", "link": "http://arxiv.org/abs/2511.21471v1", "date": "2025-11-26", "relevancy": 2.2697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialBench%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Spatial%20Cognition&body=Title%3A%20SpatialBench%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Spatial%20Cognition%0AAuthor%3A%20Peiran%20Xu%20and%20Sudong%20Wang%20and%20Yao%20Zhu%20and%20Jianing%20Li%20and%20Yunjian%20Zhang%0AAbstract%3A%20Spatial%20cognition%20is%20fundamental%20to%20real-world%20multimodal%20intelligence%2C%20allowing%20models%20to%20effectively%20interact%20with%20the%20physical%20environment.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20strides%2C%20existing%20benchmarks%20often%20oversimplify%20spatial%20cognition%2C%20reducing%20it%20to%20a%20single-dimensional%20metric%2C%20which%20fails%20to%20capture%20the%20hierarchical%20structure%20and%20interdependence%20of%20spatial%20abilities.%20To%20address%20this%20gap%2C%20we%20propose%20a%20hierarchical%20spatial%20cognition%20framework%20that%20decomposes%20spatial%20intelligence%20into%20five%20progressively%20complex%20levels%20from%20basic%20observation%20to%20high-level%20planning.%20Building%20upon%20this%20taxonomy%2C%20we%20construct%20SpatialBench%2C%20a%20large-scale%2C%20fine-grained%20benchmark%20covering%2015%20tasks%20aligned%20with%20these%20cognitive%20levels.%20To%20provide%20a%20unified%20evaluation%20across%20heterogeneous%20tasks%2C%20we%20further%20introduce%20a%20high-level%20capability-oriented%20metric%20that%20reliably%20assesses%20a%20model%27s%20overall%20spatial%20reasoning%20ability.%20Extensive%20experiments%20over%20massive%20MLLMs%20reveal%20distinct%20performance%20stratification%20across%20cognitive%20levels%3A%20models%20exhibit%20strong%20perceptual%20grounding%20yet%20remain%20limited%20in%20symbolic%20reasoning%2C%20causal%20inference%2C%20and%20planning.%20Additional%20human%20tests%20demonstrate%20that%20humans%20perform%20selective%2C%20goal-directed%20abstraction%2C%20while%20MLLMs%20tend%20to%20over-attend%20to%20surface%20details%20without%20coherent%20spatial%20intent.%20Our%20work%20establishes%20the%20first%20systematic%20framework%20for%20measuring%20hierarchical%20spatial%20cognition%20in%20MLLMs%2C%20laying%20the%20foundation%20for%20future%20spatially%20intelligent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialBench%253A%2520Benchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Spatial%2520Cognition%26entry.906535625%3DPeiran%2520Xu%2520and%2520Sudong%2520Wang%2520and%2520Yao%2520Zhu%2520and%2520Jianing%2520Li%2520and%2520Yunjian%2520Zhang%26entry.1292438233%3DSpatial%2520cognition%2520is%2520fundamental%2520to%2520real-world%2520multimodal%2520intelligence%252C%2520allowing%2520models%2520to%2520effectively%2520interact%2520with%2520the%2520physical%2520environment.%2520While%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520strides%252C%2520existing%2520benchmarks%2520often%2520oversimplify%2520spatial%2520cognition%252C%2520reducing%2520it%2520to%2520a%2520single-dimensional%2520metric%252C%2520which%2520fails%2520to%2520capture%2520the%2520hierarchical%2520structure%2520and%2520interdependence%2520of%2520spatial%2520abilities.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520hierarchical%2520spatial%2520cognition%2520framework%2520that%2520decomposes%2520spatial%2520intelligence%2520into%2520five%2520progressively%2520complex%2520levels%2520from%2520basic%2520observation%2520to%2520high-level%2520planning.%2520Building%2520upon%2520this%2520taxonomy%252C%2520we%2520construct%2520SpatialBench%252C%2520a%2520large-scale%252C%2520fine-grained%2520benchmark%2520covering%252015%2520tasks%2520aligned%2520with%2520these%2520cognitive%2520levels.%2520To%2520provide%2520a%2520unified%2520evaluation%2520across%2520heterogeneous%2520tasks%252C%2520we%2520further%2520introduce%2520a%2520high-level%2520capability-oriented%2520metric%2520that%2520reliably%2520assesses%2520a%2520model%2527s%2520overall%2520spatial%2520reasoning%2520ability.%2520Extensive%2520experiments%2520over%2520massive%2520MLLMs%2520reveal%2520distinct%2520performance%2520stratification%2520across%2520cognitive%2520levels%253A%2520models%2520exhibit%2520strong%2520perceptual%2520grounding%2520yet%2520remain%2520limited%2520in%2520symbolic%2520reasoning%252C%2520causal%2520inference%252C%2520and%2520planning.%2520Additional%2520human%2520tests%2520demonstrate%2520that%2520humans%2520perform%2520selective%252C%2520goal-directed%2520abstraction%252C%2520while%2520MLLMs%2520tend%2520to%2520over-attend%2520to%2520surface%2520details%2520without%2520coherent%2520spatial%2520intent.%2520Our%2520work%2520establishes%2520the%2520first%2520systematic%2520framework%2520for%2520measuring%2520hierarchical%2520spatial%2520cognition%2520in%2520MLLMs%252C%2520laying%2520the%2520foundation%2520for%2520future%2520spatially%2520intelligent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialBench%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Spatial%20Cognition&entry.906535625=Peiran%20Xu%20and%20Sudong%20Wang%20and%20Yao%20Zhu%20and%20Jianing%20Li%20and%20Yunjian%20Zhang&entry.1292438233=Spatial%20cognition%20is%20fundamental%20to%20real-world%20multimodal%20intelligence%2C%20allowing%20models%20to%20effectively%20interact%20with%20the%20physical%20environment.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20strides%2C%20existing%20benchmarks%20often%20oversimplify%20spatial%20cognition%2C%20reducing%20it%20to%20a%20single-dimensional%20metric%2C%20which%20fails%20to%20capture%20the%20hierarchical%20structure%20and%20interdependence%20of%20spatial%20abilities.%20To%20address%20this%20gap%2C%20we%20propose%20a%20hierarchical%20spatial%20cognition%20framework%20that%20decomposes%20spatial%20intelligence%20into%20five%20progressively%20complex%20levels%20from%20basic%20observation%20to%20high-level%20planning.%20Building%20upon%20this%20taxonomy%2C%20we%20construct%20SpatialBench%2C%20a%20large-scale%2C%20fine-grained%20benchmark%20covering%2015%20tasks%20aligned%20with%20these%20cognitive%20levels.%20To%20provide%20a%20unified%20evaluation%20across%20heterogeneous%20tasks%2C%20we%20further%20introduce%20a%20high-level%20capability-oriented%20metric%20that%20reliably%20assesses%20a%20model%27s%20overall%20spatial%20reasoning%20ability.%20Extensive%20experiments%20over%20massive%20MLLMs%20reveal%20distinct%20performance%20stratification%20across%20cognitive%20levels%3A%20models%20exhibit%20strong%20perceptual%20grounding%20yet%20remain%20limited%20in%20symbolic%20reasoning%2C%20causal%20inference%2C%20and%20planning.%20Additional%20human%20tests%20demonstrate%20that%20humans%20perform%20selective%2C%20goal-directed%20abstraction%2C%20while%20MLLMs%20tend%20to%20over-attend%20to%20surface%20details%20without%20coherent%20spatial%20intent.%20Our%20work%20establishes%20the%20first%20systematic%20framework%20for%20measuring%20hierarchical%20spatial%20cognition%20in%20MLLMs%2C%20laying%20the%20foundation%20for%20future%20spatially%20intelligent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.21471v1&entry.124074799=Read"},
{"title": "g-DPO: Scalable Preference Optimization for Protein Language Models", "author": "Constance Ferragu and Jonathan D. Ziegler and Nicolas Deutschmann and Arthur Lindoulsi and Eli Bixby and Cradle ML Team", "abstract": "Direct Preference Optimization (DPO) is an effective approach for aligning protein language models with experimental design goals. However, DPO faces a scalability bottleneck: the number of possible training pairs grows quadratically with the number of labeled sequences, leading to prohibitive training times even for modestly sized datasets. We introduce g-DPO, a framework that (i) uses sequence space clustering to prune redundant pairs while preserving training signal, and (ii) amortizes likelihood computations with group-based approximations. Across three protein engineering tasks, g-DPO maintains in silico and in vitro performance that is statistically indistinguishable from standard DPO, while converging 1.7x to 5.4x times faster, with speedups that scale with dataset size and the structure of the underlying mutational landscape.", "link": "http://arxiv.org/abs/2510.19474v2", "date": "2025-11-26", "relevancy": 2.2679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20g-DPO%3A%20Scalable%20Preference%20Optimization%20for%20Protein%20Language%20Models&body=Title%3A%20g-DPO%3A%20Scalable%20Preference%20Optimization%20for%20Protein%20Language%20Models%0AAuthor%3A%20Constance%20Ferragu%20and%20Jonathan%20D.%20Ziegler%20and%20Nicolas%20Deutschmann%20and%20Arthur%20Lindoulsi%20and%20Eli%20Bixby%20and%20Cradle%20ML%20Team%0AAbstract%3A%20Direct%20Preference%20Optimization%20%28DPO%29%20is%20an%20effective%20approach%20for%20aligning%20protein%20language%20models%20with%20experimental%20design%20goals.%20However%2C%20DPO%20faces%20a%20scalability%20bottleneck%3A%20the%20number%20of%20possible%20training%20pairs%20grows%20quadratically%20with%20the%20number%20of%20labeled%20sequences%2C%20leading%20to%20prohibitive%20training%20times%20even%20for%20modestly%20sized%20datasets.%20We%20introduce%20g-DPO%2C%20a%20framework%20that%20%28i%29%20uses%20sequence%20space%20clustering%20to%20prune%20redundant%20pairs%20while%20preserving%20training%20signal%2C%20and%20%28ii%29%20amortizes%20likelihood%20computations%20with%20group-based%20approximations.%20Across%20three%20protein%20engineering%20tasks%2C%20g-DPO%20maintains%20in%20silico%20and%20in%20vitro%20performance%20that%20is%20statistically%20indistinguishable%20from%20standard%20DPO%2C%20while%20converging%201.7x%20to%205.4x%20times%20faster%2C%20with%20speedups%20that%20scale%20with%20dataset%20size%20and%20the%20structure%20of%20the%20underlying%20mutational%20landscape.%0ALink%3A%20http%3A//arxiv.org/abs/2510.19474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dg-DPO%253A%2520Scalable%2520Preference%2520Optimization%2520for%2520Protein%2520Language%2520Models%26entry.906535625%3DConstance%2520Ferragu%2520and%2520Jonathan%2520D.%2520Ziegler%2520and%2520Nicolas%2520Deutschmann%2520and%2520Arthur%2520Lindoulsi%2520and%2520Eli%2520Bixby%2520and%2520Cradle%2520ML%2520Team%26entry.1292438233%3DDirect%2520Preference%2520Optimization%2520%2528DPO%2529%2520is%2520an%2520effective%2520approach%2520for%2520aligning%2520protein%2520language%2520models%2520with%2520experimental%2520design%2520goals.%2520However%252C%2520DPO%2520faces%2520a%2520scalability%2520bottleneck%253A%2520the%2520number%2520of%2520possible%2520training%2520pairs%2520grows%2520quadratically%2520with%2520the%2520number%2520of%2520labeled%2520sequences%252C%2520leading%2520to%2520prohibitive%2520training%2520times%2520even%2520for%2520modestly%2520sized%2520datasets.%2520We%2520introduce%2520g-DPO%252C%2520a%2520framework%2520that%2520%2528i%2529%2520uses%2520sequence%2520space%2520clustering%2520to%2520prune%2520redundant%2520pairs%2520while%2520preserving%2520training%2520signal%252C%2520and%2520%2528ii%2529%2520amortizes%2520likelihood%2520computations%2520with%2520group-based%2520approximations.%2520Across%2520three%2520protein%2520engineering%2520tasks%252C%2520g-DPO%2520maintains%2520in%2520silico%2520and%2520in%2520vitro%2520performance%2520that%2520is%2520statistically%2520indistinguishable%2520from%2520standard%2520DPO%252C%2520while%2520converging%25201.7x%2520to%25205.4x%2520times%2520faster%252C%2520with%2520speedups%2520that%2520scale%2520with%2520dataset%2520size%2520and%2520the%2520structure%2520of%2520the%2520underlying%2520mutational%2520landscape.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=g-DPO%3A%20Scalable%20Preference%20Optimization%20for%20Protein%20Language%20Models&entry.906535625=Constance%20Ferragu%20and%20Jonathan%20D.%20Ziegler%20and%20Nicolas%20Deutschmann%20and%20Arthur%20Lindoulsi%20and%20Eli%20Bixby%20and%20Cradle%20ML%20Team&entry.1292438233=Direct%20Preference%20Optimization%20%28DPO%29%20is%20an%20effective%20approach%20for%20aligning%20protein%20language%20models%20with%20experimental%20design%20goals.%20However%2C%20DPO%20faces%20a%20scalability%20bottleneck%3A%20the%20number%20of%20possible%20training%20pairs%20grows%20quadratically%20with%20the%20number%20of%20labeled%20sequences%2C%20leading%20to%20prohibitive%20training%20times%20even%20for%20modestly%20sized%20datasets.%20We%20introduce%20g-DPO%2C%20a%20framework%20that%20%28i%29%20uses%20sequence%20space%20clustering%20to%20prune%20redundant%20pairs%20while%20preserving%20training%20signal%2C%20and%20%28ii%29%20amortizes%20likelihood%20computations%20with%20group-based%20approximations.%20Across%20three%20protein%20engineering%20tasks%2C%20g-DPO%20maintains%20in%20silico%20and%20in%20vitro%20performance%20that%20is%20statistically%20indistinguishable%20from%20standard%20DPO%2C%20while%20converging%201.7x%20to%205.4x%20times%20faster%2C%20with%20speedups%20that%20scale%20with%20dataset%20size%20and%20the%20structure%20of%20the%20underlying%20mutational%20landscape.&entry.1838667208=http%3A//arxiv.org/abs/2510.19474v2&entry.124074799=Read"},
{"title": "Multimodal Robust Prompt Distillation for 3D Point Cloud Models", "author": "Xiang Gu and Liming Lu and Xu Zheng and Anan Du and Yongbin Zhou and Shuchao Pang", "abstract": "Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.", "link": "http://arxiv.org/abs/2511.21574v1", "date": "2025-11-26", "relevancy": 2.2546, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Robust%20Prompt%20Distillation%20for%203D%20Point%20Cloud%20Models&body=Title%3A%20Multimodal%20Robust%20Prompt%20Distillation%20for%203D%20Point%20Cloud%20Models%0AAuthor%3A%20Xiang%20Gu%20and%20Liming%20Lu%20and%20Xu%20Zheng%20and%20Anan%20Du%20and%20Yongbin%20Zhou%20and%20Shuchao%20Pang%0AAbstract%3A%20Adversarial%20attacks%20pose%20a%20significant%20threat%20to%20learning-based%203D%20point%20cloud%20models%2C%20critically%20undermining%20their%20reliability%20in%20security-sensitive%20applications.%20Existing%20defense%20methods%20often%20suffer%20from%20%281%29%20high%20computational%20overhead%20and%20%282%29%20poor%20generalization%20ability%20across%20diverse%20attack%20types.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20novel%20yet%20efficient%20teacher-student%20framework%2C%20namely%20Multimodal%20Robust%20Prompt%20Distillation%20%28MRPD%29%20for%20distilling%20robust%203D%20point%20cloud%20model.%20It%20learns%20lightweight%20prompts%20by%20aligning%20student%20point%20cloud%20model%27s%20features%20with%20robust%20embeddings%20from%20three%20distinct%20teachers%3A%20a%20vision%20model%20processing%20depth%20projections%2C%20a%20high-performance%203D%20model%2C%20and%20a%20text%20encoder.%20To%20ensure%20a%20reliable%20knowledge%20transfer%2C%20this%20distillation%20is%20guided%20by%20a%20confidence-gated%20mechanism%20which%20dynamically%20balances%20the%20contribution%20of%20all%20input%20modalities.%20Notably%2C%20since%20the%20distillation%20is%20all%20during%20the%20training%20stage%2C%20there%20is%20no%20additional%20computational%20cost%20at%20inference.%20Extensive%20experiments%20demonstrate%20that%20MRPD%20substantially%20outperforms%20state-of-the-art%20defense%20methods%20against%20a%20wide%20range%20of%20white-box%20and%20black-box%20attacks%2C%20while%20even%20achieving%20better%20performance%20on%20clean%20data.%20Our%20work%20presents%20a%20new%2C%20practical%20paradigm%20for%20building%20robust%203D%20vision%20systems%20by%20efficiently%20harnessing%20multimodal%20knowledge.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Robust%2520Prompt%2520Distillation%2520for%25203D%2520Point%2520Cloud%2520Models%26entry.906535625%3DXiang%2520Gu%2520and%2520Liming%2520Lu%2520and%2520Xu%2520Zheng%2520and%2520Anan%2520Du%2520and%2520Yongbin%2520Zhou%2520and%2520Shuchao%2520Pang%26entry.1292438233%3DAdversarial%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520learning-based%25203D%2520point%2520cloud%2520models%252C%2520critically%2520undermining%2520their%2520reliability%2520in%2520security-sensitive%2520applications.%2520Existing%2520defense%2520methods%2520often%2520suffer%2520from%2520%25281%2529%2520high%2520computational%2520overhead%2520and%2520%25282%2529%2520poor%2520generalization%2520ability%2520across%2520diverse%2520attack%2520types.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520propose%2520a%2520novel%2520yet%2520efficient%2520teacher-student%2520framework%252C%2520namely%2520Multimodal%2520Robust%2520Prompt%2520Distillation%2520%2528MRPD%2529%2520for%2520distilling%2520robust%25203D%2520point%2520cloud%2520model.%2520It%2520learns%2520lightweight%2520prompts%2520by%2520aligning%2520student%2520point%2520cloud%2520model%2527s%2520features%2520with%2520robust%2520embeddings%2520from%2520three%2520distinct%2520teachers%253A%2520a%2520vision%2520model%2520processing%2520depth%2520projections%252C%2520a%2520high-performance%25203D%2520model%252C%2520and%2520a%2520text%2520encoder.%2520To%2520ensure%2520a%2520reliable%2520knowledge%2520transfer%252C%2520this%2520distillation%2520is%2520guided%2520by%2520a%2520confidence-gated%2520mechanism%2520which%2520dynamically%2520balances%2520the%2520contribution%2520of%2520all%2520input%2520modalities.%2520Notably%252C%2520since%2520the%2520distillation%2520is%2520all%2520during%2520the%2520training%2520stage%252C%2520there%2520is%2520no%2520additional%2520computational%2520cost%2520at%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MRPD%2520substantially%2520outperforms%2520state-of-the-art%2520defense%2520methods%2520against%2520a%2520wide%2520range%2520of%2520white-box%2520and%2520black-box%2520attacks%252C%2520while%2520even%2520achieving%2520better%2520performance%2520on%2520clean%2520data.%2520Our%2520work%2520presents%2520a%2520new%252C%2520practical%2520paradigm%2520for%2520building%2520robust%25203D%2520vision%2520systems%2520by%2520efficiently%2520harnessing%2520multimodal%2520knowledge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Robust%20Prompt%20Distillation%20for%203D%20Point%20Cloud%20Models&entry.906535625=Xiang%20Gu%20and%20Liming%20Lu%20and%20Xu%20Zheng%20and%20Anan%20Du%20and%20Yongbin%20Zhou%20and%20Shuchao%20Pang&entry.1292438233=Adversarial%20attacks%20pose%20a%20significant%20threat%20to%20learning-based%203D%20point%20cloud%20models%2C%20critically%20undermining%20their%20reliability%20in%20security-sensitive%20applications.%20Existing%20defense%20methods%20often%20suffer%20from%20%281%29%20high%20computational%20overhead%20and%20%282%29%20poor%20generalization%20ability%20across%20diverse%20attack%20types.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20novel%20yet%20efficient%20teacher-student%20framework%2C%20namely%20Multimodal%20Robust%20Prompt%20Distillation%20%28MRPD%29%20for%20distilling%20robust%203D%20point%20cloud%20model.%20It%20learns%20lightweight%20prompts%20by%20aligning%20student%20point%20cloud%20model%27s%20features%20with%20robust%20embeddings%20from%20three%20distinct%20teachers%3A%20a%20vision%20model%20processing%20depth%20projections%2C%20a%20high-performance%203D%20model%2C%20and%20a%20text%20encoder.%20To%20ensure%20a%20reliable%20knowledge%20transfer%2C%20this%20distillation%20is%20guided%20by%20a%20confidence-gated%20mechanism%20which%20dynamically%20balances%20the%20contribution%20of%20all%20input%20modalities.%20Notably%2C%20since%20the%20distillation%20is%20all%20during%20the%20training%20stage%2C%20there%20is%20no%20additional%20computational%20cost%20at%20inference.%20Extensive%20experiments%20demonstrate%20that%20MRPD%20substantially%20outperforms%20state-of-the-art%20defense%20methods%20against%20a%20wide%20range%20of%20white-box%20and%20black-box%20attacks%2C%20while%20even%20achieving%20better%20performance%20on%20clean%20data.%20Our%20work%20presents%20a%20new%2C%20practical%20paradigm%20for%20building%20robust%203D%20vision%20systems%20by%20efficiently%20harnessing%20multimodal%20knowledge.&entry.1838667208=http%3A//arxiv.org/abs/2511.21574v1&entry.124074799=Read"},
{"title": "Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis", "author": "Jiyun Bae and Hyunjong Ok and Sangwoo Mo and Jaeho Lee", "abstract": "How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.", "link": "http://arxiv.org/abs/2511.21397v1", "date": "2025-11-26", "relevancy": 2.2481, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Reasoning%20Vision-Language%20Models%20Inversely%20Scale%20in%20Test-Time%20Compute%3F%20A%20Distractor-centric%20Empirical%20Analysis&body=Title%3A%20Do%20Reasoning%20Vision-Language%20Models%20Inversely%20Scale%20in%20Test-Time%20Compute%3F%20A%20Distractor-centric%20Empirical%20Analysis%0AAuthor%3A%20Jiyun%20Bae%20and%20Hyunjong%20Ok%20and%20Sangwoo%20Mo%20and%20Jaeho%20Lee%0AAbstract%3A%20How%20does%20irrelevant%20information%20%28i.e.%2C%20distractors%29%20affect%20test-time%20scaling%20in%20vision-language%20models%20%28VLMs%29%3F%20Prior%20studies%20on%20language%20models%20have%20reported%20an%20inverse%20scaling%20effect%2C%20where%20textual%20distractors%20lead%20to%20longer%20but%20less%20effective%20reasoning.%20To%20investigate%20whether%20similar%20phenomena%20occur%20in%20multimodal%20settings%2C%20we%20introduce%20Idis%20%28Images%20with%20distractors%29%2C%20a%20visual%20question-answering%20dataset%20that%20systematically%20varies%20distractors%20along%20semantic%2C%20numerical%2C%20and%20spatial%20dimensions.%20Our%20analyses%20reveal%20that%20visual%20distractors%20differ%20fundamentally%20from%20textual%20ones%3A%20although%20inverse%20scaling%20persists%2C%20adding%20visual%20distractors%20reduces%20accuracy%20without%20increasing%20reasoning%20length.%20We%20further%20show%20that%20tracking%20attribute%20counts%20within%20reasoning%20traces%20provides%20key%20insights%20into%20how%20distractors%2C%20reasoning%20length%2C%20and%20accuracy%20interact.%20Finally%2C%20we%20demonstrate%20that%20these%20trends%20extend%20to%20established%20visual%20bias%20benchmarks%20such%20as%20Waterbirds%2C%20and%20we%20propose%20a%20simple%20prompting%20strategy%20to%20mitigate%20bias-driven%20predictions%20in%20reasoning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Reasoning%2520Vision-Language%2520Models%2520Inversely%2520Scale%2520in%2520Test-Time%2520Compute%253F%2520A%2520Distractor-centric%2520Empirical%2520Analysis%26entry.906535625%3DJiyun%2520Bae%2520and%2520Hyunjong%2520Ok%2520and%2520Sangwoo%2520Mo%2520and%2520Jaeho%2520Lee%26entry.1292438233%3DHow%2520does%2520irrelevant%2520information%2520%2528i.e.%252C%2520distractors%2529%2520affect%2520test-time%2520scaling%2520in%2520vision-language%2520models%2520%2528VLMs%2529%253F%2520Prior%2520studies%2520on%2520language%2520models%2520have%2520reported%2520an%2520inverse%2520scaling%2520effect%252C%2520where%2520textual%2520distractors%2520lead%2520to%2520longer%2520but%2520less%2520effective%2520reasoning.%2520To%2520investigate%2520whether%2520similar%2520phenomena%2520occur%2520in%2520multimodal%2520settings%252C%2520we%2520introduce%2520Idis%2520%2528Images%2520with%2520distractors%2529%252C%2520a%2520visual%2520question-answering%2520dataset%2520that%2520systematically%2520varies%2520distractors%2520along%2520semantic%252C%2520numerical%252C%2520and%2520spatial%2520dimensions.%2520Our%2520analyses%2520reveal%2520that%2520visual%2520distractors%2520differ%2520fundamentally%2520from%2520textual%2520ones%253A%2520although%2520inverse%2520scaling%2520persists%252C%2520adding%2520visual%2520distractors%2520reduces%2520accuracy%2520without%2520increasing%2520reasoning%2520length.%2520We%2520further%2520show%2520that%2520tracking%2520attribute%2520counts%2520within%2520reasoning%2520traces%2520provides%2520key%2520insights%2520into%2520how%2520distractors%252C%2520reasoning%2520length%252C%2520and%2520accuracy%2520interact.%2520Finally%252C%2520we%2520demonstrate%2520that%2520these%2520trends%2520extend%2520to%2520established%2520visual%2520bias%2520benchmarks%2520such%2520as%2520Waterbirds%252C%2520and%2520we%2520propose%2520a%2520simple%2520prompting%2520strategy%2520to%2520mitigate%2520bias-driven%2520predictions%2520in%2520reasoning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Reasoning%20Vision-Language%20Models%20Inversely%20Scale%20in%20Test-Time%20Compute%3F%20A%20Distractor-centric%20Empirical%20Analysis&entry.906535625=Jiyun%20Bae%20and%20Hyunjong%20Ok%20and%20Sangwoo%20Mo%20and%20Jaeho%20Lee&entry.1292438233=How%20does%20irrelevant%20information%20%28i.e.%2C%20distractors%29%20affect%20test-time%20scaling%20in%20vision-language%20models%20%28VLMs%29%3F%20Prior%20studies%20on%20language%20models%20have%20reported%20an%20inverse%20scaling%20effect%2C%20where%20textual%20distractors%20lead%20to%20longer%20but%20less%20effective%20reasoning.%20To%20investigate%20whether%20similar%20phenomena%20occur%20in%20multimodal%20settings%2C%20we%20introduce%20Idis%20%28Images%20with%20distractors%29%2C%20a%20visual%20question-answering%20dataset%20that%20systematically%20varies%20distractors%20along%20semantic%2C%20numerical%2C%20and%20spatial%20dimensions.%20Our%20analyses%20reveal%20that%20visual%20distractors%20differ%20fundamentally%20from%20textual%20ones%3A%20although%20inverse%20scaling%20persists%2C%20adding%20visual%20distractors%20reduces%20accuracy%20without%20increasing%20reasoning%20length.%20We%20further%20show%20that%20tracking%20attribute%20counts%20within%20reasoning%20traces%20provides%20key%20insights%20into%20how%20distractors%2C%20reasoning%20length%2C%20and%20accuracy%20interact.%20Finally%2C%20we%20demonstrate%20that%20these%20trends%20extend%20to%20established%20visual%20bias%20benchmarks%20such%20as%20Waterbirds%2C%20and%20we%20propose%20a%20simple%20prompting%20strategy%20to%20mitigate%20bias-driven%20predictions%20in%20reasoning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.21397v1&entry.124074799=Read"},
{"title": "Frequency-Aware Token Reduction for Efficient Vision Transformer", "author": "Dong-Jae Lee and Jiwan Hur and Jaehyun Choi and Jaemyung Yu and Junmo Kim", "abstract": "Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.", "link": "http://arxiv.org/abs/2511.21477v1", "date": "2025-11-26", "relevancy": 2.248, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6398}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5446}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Aware%20Token%20Reduction%20for%20Efficient%20Vision%20Transformer&body=Title%3A%20Frequency-Aware%20Token%20Reduction%20for%20Efficient%20Vision%20Transformer%0AAuthor%3A%20Dong-Jae%20Lee%20and%20Jiwan%20Hur%20and%20Jaehyun%20Choi%20and%20Jaemyung%20Yu%20and%20Junmo%20Kim%0AAbstract%3A%20Vision%20Transformers%20have%20demonstrated%20exceptional%20performance%20across%20various%20computer%20vision%20tasks%2C%20yet%20their%20quadratic%20computational%20complexity%20concerning%20token%20length%20remains%20a%20significant%20challenge.%20To%20address%20this%2C%20token%20reduction%20methods%20have%20been%20widely%20explored.%20However%2C%20existing%20approaches%20often%20overlook%20the%20frequency%20characteristics%20of%20self-attention%2C%20such%20as%20rank%20collapsing%20and%20over-smoothing%20phenomenon.%20In%20this%20paper%2C%20we%20propose%20a%20frequency-aware%20token%20reduction%20strategy%20that%20improves%20computational%20efficiency%20while%20preserving%20performance%20by%20mitigating%20rank%20collapsing.%20Our%20method%20partitions%20tokens%20into%20high-frequency%20tokens%20and%20low-frequency%20tokens.%20high-frequency%20tokens%20are%20selectively%20preserved%2C%20while%20low-frequency%20tokens%20are%20aggregated%20into%20a%20compact%20direct%20current%20token%20to%20retain%20essential%20low-frequency%20components.%20Through%20extensive%20experiments%20and%20analysis%2C%20we%20demonstrate%20that%20our%20approach%20significantly%20improves%20accuracy%20while%20reducing%20computational%20overhead%20and%20mitigating%20rank%20collapsing%20and%20over%20smoothing.%20Furthermore%2C%20we%20analyze%20the%20previous%20methods%2C%20shedding%20light%20on%20their%20implicit%20frequency%20characteristics%20and%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Aware%2520Token%2520Reduction%2520for%2520Efficient%2520Vision%2520Transformer%26entry.906535625%3DDong-Jae%2520Lee%2520and%2520Jiwan%2520Hur%2520and%2520Jaehyun%2520Choi%2520and%2520Jaemyung%2520Yu%2520and%2520Junmo%2520Kim%26entry.1292438233%3DVision%2520Transformers%2520have%2520demonstrated%2520exceptional%2520performance%2520across%2520various%2520computer%2520vision%2520tasks%252C%2520yet%2520their%2520quadratic%2520computational%2520complexity%2520concerning%2520token%2520length%2520remains%2520a%2520significant%2520challenge.%2520To%2520address%2520this%252C%2520token%2520reduction%2520methods%2520have%2520been%2520widely%2520explored.%2520However%252C%2520existing%2520approaches%2520often%2520overlook%2520the%2520frequency%2520characteristics%2520of%2520self-attention%252C%2520such%2520as%2520rank%2520collapsing%2520and%2520over-smoothing%2520phenomenon.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520frequency-aware%2520token%2520reduction%2520strategy%2520that%2520improves%2520computational%2520efficiency%2520while%2520preserving%2520performance%2520by%2520mitigating%2520rank%2520collapsing.%2520Our%2520method%2520partitions%2520tokens%2520into%2520high-frequency%2520tokens%2520and%2520low-frequency%2520tokens.%2520high-frequency%2520tokens%2520are%2520selectively%2520preserved%252C%2520while%2520low-frequency%2520tokens%2520are%2520aggregated%2520into%2520a%2520compact%2520direct%2520current%2520token%2520to%2520retain%2520essential%2520low-frequency%2520components.%2520Through%2520extensive%2520experiments%2520and%2520analysis%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%2520accuracy%2520while%2520reducing%2520computational%2520overhead%2520and%2520mitigating%2520rank%2520collapsing%2520and%2520over%2520smoothing.%2520Furthermore%252C%2520we%2520analyze%2520the%2520previous%2520methods%252C%2520shedding%2520light%2520on%2520their%2520implicit%2520frequency%2520characteristics%2520and%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Aware%20Token%20Reduction%20for%20Efficient%20Vision%20Transformer&entry.906535625=Dong-Jae%20Lee%20and%20Jiwan%20Hur%20and%20Jaehyun%20Choi%20and%20Jaemyung%20Yu%20and%20Junmo%20Kim&entry.1292438233=Vision%20Transformers%20have%20demonstrated%20exceptional%20performance%20across%20various%20computer%20vision%20tasks%2C%20yet%20their%20quadratic%20computational%20complexity%20concerning%20token%20length%20remains%20a%20significant%20challenge.%20To%20address%20this%2C%20token%20reduction%20methods%20have%20been%20widely%20explored.%20However%2C%20existing%20approaches%20often%20overlook%20the%20frequency%20characteristics%20of%20self-attention%2C%20such%20as%20rank%20collapsing%20and%20over-smoothing%20phenomenon.%20In%20this%20paper%2C%20we%20propose%20a%20frequency-aware%20token%20reduction%20strategy%20that%20improves%20computational%20efficiency%20while%20preserving%20performance%20by%20mitigating%20rank%20collapsing.%20Our%20method%20partitions%20tokens%20into%20high-frequency%20tokens%20and%20low-frequency%20tokens.%20high-frequency%20tokens%20are%20selectively%20preserved%2C%20while%20low-frequency%20tokens%20are%20aggregated%20into%20a%20compact%20direct%20current%20token%20to%20retain%20essential%20low-frequency%20components.%20Through%20extensive%20experiments%20and%20analysis%2C%20we%20demonstrate%20that%20our%20approach%20significantly%20improves%20accuracy%20while%20reducing%20computational%20overhead%20and%20mitigating%20rank%20collapsing%20and%20over%20smoothing.%20Furthermore%2C%20we%20analyze%20the%20previous%20methods%2C%20shedding%20light%20on%20their%20implicit%20frequency%20characteristics%20and%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2511.21477v1&entry.124074799=Read"},
{"title": "HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks", "author": "Raffaele Mura and Giuseppe Floris and Luca Scionis and Giorgio Piras and Maura Pintor and Ambra Demontis and Giorgio Giacinto and Battista Biggio and Fabio Roli", "abstract": "Gradient-based attacks are a primary tool to evaluate robustness of machine-learning models. However, many attacks tend to provide overly-optimistic evaluations as they use fixed loss functions, optimizers, step-size schedulers, and default hyperparameters. In this work, we tackle these limitations by proposing a parametric variation of the well-known fast minimum-norm attack algorithm, whose loss, optimizer, step-size scheduler, and hyperparameters can be dynamically adjusted. We re-evaluate 12 robust models, showing that our attack finds smaller adversarial perturbations without requiring any additional tuning. This also enables reporting adversarial robustness as a function of the perturbation budget, providing a more complete evaluation than that offered by fixed-budget attacks, while remaining efficient. We release our open-source code at https://github.com/pralab/HO-FMN.", "link": "http://arxiv.org/abs/2407.08806v3", "date": "2025-11-26", "relevancy": 2.2373, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4802}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4466}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks&body=Title%3A%20HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks%0AAuthor%3A%20Raffaele%20Mura%20and%20Giuseppe%20Floris%20and%20Luca%20Scionis%20and%20Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Giorgio%20Giacinto%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20Gradient-based%20attacks%20are%20a%20primary%20tool%20to%20evaluate%20robustness%20of%20machine-learning%20models.%20However%2C%20many%20attacks%20tend%20to%20provide%20overly-optimistic%20evaluations%20as%20they%20use%20fixed%20loss%20functions%2C%20optimizers%2C%20step-size%20schedulers%2C%20and%20default%20hyperparameters.%20In%20this%20work%2C%20we%20tackle%20these%20limitations%20by%20proposing%20a%20parametric%20variation%20of%20the%20well-known%20fast%20minimum-norm%20attack%20algorithm%2C%20whose%20loss%2C%20optimizer%2C%20step-size%20scheduler%2C%20and%20hyperparameters%20can%20be%20dynamically%20adjusted.%20We%20re-evaluate%2012%20robust%20models%2C%20showing%20that%20our%20attack%20finds%20smaller%20adversarial%20perturbations%20without%20requiring%20any%20additional%20tuning.%20This%20also%20enables%20reporting%20adversarial%20robustness%20as%20a%20function%20of%20the%20perturbation%20budget%2C%20providing%20a%20more%20complete%20evaluation%20than%20that%20offered%20by%20fixed-budget%20attacks%2C%20while%20remaining%20efficient.%20We%20release%20our%20open-source%20code%20at%20https%3A//github.com/pralab/HO-FMN.%0ALink%3A%20http%3A//arxiv.org/abs/2407.08806v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHO-FMN%253A%2520Hyperparameter%2520Optimization%2520for%2520Fast%2520Minimum-Norm%2520Attacks%26entry.906535625%3DRaffaele%2520Mura%2520and%2520Giuseppe%2520Floris%2520and%2520Luca%2520Scionis%2520and%2520Giorgio%2520Piras%2520and%2520Maura%2520Pintor%2520and%2520Ambra%2520Demontis%2520and%2520Giorgio%2520Giacinto%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3DGradient-based%2520attacks%2520are%2520a%2520primary%2520tool%2520to%2520evaluate%2520robustness%2520of%2520machine-learning%2520models.%2520However%252C%2520many%2520attacks%2520tend%2520to%2520provide%2520overly-optimistic%2520evaluations%2520as%2520they%2520use%2520fixed%2520loss%2520functions%252C%2520optimizers%252C%2520step-size%2520schedulers%252C%2520and%2520default%2520hyperparameters.%2520In%2520this%2520work%252C%2520we%2520tackle%2520these%2520limitations%2520by%2520proposing%2520a%2520parametric%2520variation%2520of%2520the%2520well-known%2520fast%2520minimum-norm%2520attack%2520algorithm%252C%2520whose%2520loss%252C%2520optimizer%252C%2520step-size%2520scheduler%252C%2520and%2520hyperparameters%2520can%2520be%2520dynamically%2520adjusted.%2520We%2520re-evaluate%252012%2520robust%2520models%252C%2520showing%2520that%2520our%2520attack%2520finds%2520smaller%2520adversarial%2520perturbations%2520without%2520requiring%2520any%2520additional%2520tuning.%2520This%2520also%2520enables%2520reporting%2520adversarial%2520robustness%2520as%2520a%2520function%2520of%2520the%2520perturbation%2520budget%252C%2520providing%2520a%2520more%2520complete%2520evaluation%2520than%2520that%2520offered%2520by%2520fixed-budget%2520attacks%252C%2520while%2520remaining%2520efficient.%2520We%2520release%2520our%2520open-source%2520code%2520at%2520https%253A//github.com/pralab/HO-FMN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08806v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks&entry.906535625=Raffaele%20Mura%20and%20Giuseppe%20Floris%20and%20Luca%20Scionis%20and%20Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Giorgio%20Giacinto%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=Gradient-based%20attacks%20are%20a%20primary%20tool%20to%20evaluate%20robustness%20of%20machine-learning%20models.%20However%2C%20many%20attacks%20tend%20to%20provide%20overly-optimistic%20evaluations%20as%20they%20use%20fixed%20loss%20functions%2C%20optimizers%2C%20step-size%20schedulers%2C%20and%20default%20hyperparameters.%20In%20this%20work%2C%20we%20tackle%20these%20limitations%20by%20proposing%20a%20parametric%20variation%20of%20the%20well-known%20fast%20minimum-norm%20attack%20algorithm%2C%20whose%20loss%2C%20optimizer%2C%20step-size%20scheduler%2C%20and%20hyperparameters%20can%20be%20dynamically%20adjusted.%20We%20re-evaluate%2012%20robust%20models%2C%20showing%20that%20our%20attack%20finds%20smaller%20adversarial%20perturbations%20without%20requiring%20any%20additional%20tuning.%20This%20also%20enables%20reporting%20adversarial%20robustness%20as%20a%20function%20of%20the%20perturbation%20budget%2C%20providing%20a%20more%20complete%20evaluation%20than%20that%20offered%20by%20fixed-budget%20attacks%2C%20while%20remaining%20efficient.%20We%20release%20our%20open-source%20code%20at%20https%3A//github.com/pralab/HO-FMN.&entry.1838667208=http%3A//arxiv.org/abs/2407.08806v3&entry.124074799=Read"},
{"title": "Mechanisms of Non-Monotonic Scaling in Vision Transformers", "author": "Anantha Padmanaban Krishna Kumar", "abstract": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.", "link": "http://arxiv.org/abs/2511.21635v1", "date": "2025-11-26", "relevancy": 2.2311, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6339}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanisms%20of%20Non-Monotonic%20Scaling%20in%20Vision%20Transformers&body=Title%3A%20Mechanisms%20of%20Non-Monotonic%20Scaling%20in%20Vision%20Transformers%0AAuthor%3A%20Anantha%20Padmanaban%20Krishna%20Kumar%0AAbstract%3A%20Deeper%20Vision%20Transformers%20often%20perform%20worse%20than%20shallower%20ones%2C%20which%20challenges%20common%20scaling%20assumptions.%20Through%20a%20systematic%20empirical%20analysis%20of%20ViT-S%2C%20ViT-B%2C%20and%20ViT-L%20on%20ImageNet%2C%20we%20identify%20a%20consistent%20three-phase%20Cliff-Plateau-Climb%20pattern%20that%20governs%20how%20representations%20evolve%20with%20depth.%20We%20observe%20that%20better%20performance%20is%20associated%20with%20progressive%20marginalization%20of%20the%20%5BCLS%5D%20token%2C%20originally%20designed%20as%20a%20global%20aggregation%20hub%2C%20in%20favor%20of%20distributed%20consensus%20among%20patch%20tokens.%20We%20quantify%20patterns%20of%20information%20mixing%20with%20an%20Information%20Scrambling%20Index%2C%20and%20show%20that%20in%20ViT-L%20the%20information-task%20tradeoff%20emerges%20roughly%2010%20layers%20later%20than%20in%20ViT-B%2C%20and%20that%20these%20additional%20layers%20correlate%20with%20increased%20information%20diffusion%20rather%20than%20improved%20task%20performance.%20Taken%20together%2C%20these%20results%20suggest%20that%20transformer%20architectures%20in%20this%20regime%20may%20benefit%20more%20from%20carefully%20calibrated%20depth%20that%20executes%20clean%20phase%20transitions%20than%20from%20simply%20increasing%20parameter%20count.%20The%20Information%20Scrambling%20Index%20provides%20a%20useful%20diagnostic%20for%20existing%20models%20and%20suggests%20a%20potential%20design%20target%20for%20future%20architectures.%20All%20code%20is%20available%20at%3A%20https%3A//github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanisms%2520of%2520Non-Monotonic%2520Scaling%2520in%2520Vision%2520Transformers%26entry.906535625%3DAnantha%2520Padmanaban%2520Krishna%2520Kumar%26entry.1292438233%3DDeeper%2520Vision%2520Transformers%2520often%2520perform%2520worse%2520than%2520shallower%2520ones%252C%2520which%2520challenges%2520common%2520scaling%2520assumptions.%2520Through%2520a%2520systematic%2520empirical%2520analysis%2520of%2520ViT-S%252C%2520ViT-B%252C%2520and%2520ViT-L%2520on%2520ImageNet%252C%2520we%2520identify%2520a%2520consistent%2520three-phase%2520Cliff-Plateau-Climb%2520pattern%2520that%2520governs%2520how%2520representations%2520evolve%2520with%2520depth.%2520We%2520observe%2520that%2520better%2520performance%2520is%2520associated%2520with%2520progressive%2520marginalization%2520of%2520the%2520%255BCLS%255D%2520token%252C%2520originally%2520designed%2520as%2520a%2520global%2520aggregation%2520hub%252C%2520in%2520favor%2520of%2520distributed%2520consensus%2520among%2520patch%2520tokens.%2520We%2520quantify%2520patterns%2520of%2520information%2520mixing%2520with%2520an%2520Information%2520Scrambling%2520Index%252C%2520and%2520show%2520that%2520in%2520ViT-L%2520the%2520information-task%2520tradeoff%2520emerges%2520roughly%252010%2520layers%2520later%2520than%2520in%2520ViT-B%252C%2520and%2520that%2520these%2520additional%2520layers%2520correlate%2520with%2520increased%2520information%2520diffusion%2520rather%2520than%2520improved%2520task%2520performance.%2520Taken%2520together%252C%2520these%2520results%2520suggest%2520that%2520transformer%2520architectures%2520in%2520this%2520regime%2520may%2520benefit%2520more%2520from%2520carefully%2520calibrated%2520depth%2520that%2520executes%2520clean%2520phase%2520transitions%2520than%2520from%2520simply%2520increasing%2520parameter%2520count.%2520The%2520Information%2520Scrambling%2520Index%2520provides%2520a%2520useful%2520diagnostic%2520for%2520existing%2520models%2520and%2520suggests%2520a%2520potential%2520design%2520target%2520for%2520future%2520architectures.%2520All%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanisms%20of%20Non-Monotonic%20Scaling%20in%20Vision%20Transformers&entry.906535625=Anantha%20Padmanaban%20Krishna%20Kumar&entry.1292438233=Deeper%20Vision%20Transformers%20often%20perform%20worse%20than%20shallower%20ones%2C%20which%20challenges%20common%20scaling%20assumptions.%20Through%20a%20systematic%20empirical%20analysis%20of%20ViT-S%2C%20ViT-B%2C%20and%20ViT-L%20on%20ImageNet%2C%20we%20identify%20a%20consistent%20three-phase%20Cliff-Plateau-Climb%20pattern%20that%20governs%20how%20representations%20evolve%20with%20depth.%20We%20observe%20that%20better%20performance%20is%20associated%20with%20progressive%20marginalization%20of%20the%20%5BCLS%5D%20token%2C%20originally%20designed%20as%20a%20global%20aggregation%20hub%2C%20in%20favor%20of%20distributed%20consensus%20among%20patch%20tokens.%20We%20quantify%20patterns%20of%20information%20mixing%20with%20an%20Information%20Scrambling%20Index%2C%20and%20show%20that%20in%20ViT-L%20the%20information-task%20tradeoff%20emerges%20roughly%2010%20layers%20later%20than%20in%20ViT-B%2C%20and%20that%20these%20additional%20layers%20correlate%20with%20increased%20information%20diffusion%20rather%20than%20improved%20task%20performance.%20Taken%20together%2C%20these%20results%20suggest%20that%20transformer%20architectures%20in%20this%20regime%20may%20benefit%20more%20from%20carefully%20calibrated%20depth%20that%20executes%20clean%20phase%20transitions%20than%20from%20simply%20increasing%20parameter%20count.%20The%20Information%20Scrambling%20Index%20provides%20a%20useful%20diagnostic%20for%20existing%20models%20and%20suggests%20a%20potential%20design%20target%20for%20future%20architectures.%20All%20code%20is%20available%20at%3A%20https%3A//github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.&entry.1838667208=http%3A//arxiv.org/abs/2511.21635v1&entry.124074799=Read"},
{"title": "SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding", "author": "Tae-Min Choi and Tae Kyeong Jeong and Garam Kim and Jaemin Lee and Yeongyoon Koh and In Cheul Choi and Jae-Ho Chung and Jong Woong Park and Juyoun Park", "abstract": "Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.", "link": "http://arxiv.org/abs/2511.21339v1", "date": "2025-11-26", "relevancy": 2.2306, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgMLLMBench%3A%20A%20Multimodal%20Large%20Language%20Model%20Benchmark%20Dataset%20for%20Surgical%20Scene%20Understanding&body=Title%3A%20SurgMLLMBench%3A%20A%20Multimodal%20Large%20Language%20Model%20Benchmark%20Dataset%20for%20Surgical%20Scene%20Understanding%0AAuthor%3A%20Tae-Min%20Choi%20and%20Tae%20Kyeong%20Jeong%20and%20Garam%20Kim%20and%20Jaemin%20Lee%20and%20Yeongyoon%20Koh%20and%20In%20Cheul%20Choi%20and%20Jae-Ho%20Chung%20and%20Jong%20Woong%20Park%20and%20Juyoun%20Park%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20their%20potential%20for%20medical%20and%20surgical%20applications.%20However%2C%20existing%20surgical%20datasets%20predominantly%20adopt%20a%20Visual%20Question%20Answering%20%28VQA%29%20format%20with%20heterogeneous%20taxonomies%20and%20lack%20support%20for%20pixel-level%20segmentation%2C%20limiting%20consistent%20evaluation%20and%20applicability.%20We%20present%20SurgMLLMBench%2C%20a%20unified%20multimodal%20benchmark%20explicitly%20designed%20for%20developing%20and%20evaluating%20interactive%20multimodal%20LLMs%20for%20surgical%20scene%20understanding%2C%20including%20the%20newly%20collected%20Micro-surgical%20Artificial%20Vascular%20anastomosIS%20%28MAVIS%29%20dataset.%20It%20integrates%20pixel-level%20instrument%20segmentation%20masks%20and%20structured%20VQA%20annotations%20across%20laparoscopic%2C%20robot-assisted%2C%20and%20micro-surgical%20domains%20under%20a%20unified%20taxonomy%2C%20enabling%20comprehensive%20evaluation%20beyond%20traditional%20VQA%20tasks%20and%20richer%20visual-conversational%20interactions.%20Extensive%20baseline%20experiments%20show%20that%20a%20single%20model%20trained%20on%20SurgMLLMBench%20achieves%20consistent%20performance%20across%20domains%20and%20generalizes%20effectively%20to%20unseen%20datasets.%20SurgMLLMBench%20will%20be%20publicly%20released%20as%20a%20robust%20resource%20to%20advance%20multimodal%20surgical%20AI%20research%2C%20supporting%20reproducible%20evaluation%20and%20development%20of%20interactive%20surgical%20reasoning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgMLLMBench%253A%2520A%2520Multimodal%2520Large%2520Language%2520Model%2520Benchmark%2520Dataset%2520for%2520Surgical%2520Scene%2520Understanding%26entry.906535625%3DTae-Min%2520Choi%2520and%2520Tae%2520Kyeong%2520Jeong%2520and%2520Garam%2520Kim%2520and%2520Jaemin%2520Lee%2520and%2520Yeongyoon%2520Koh%2520and%2520In%2520Cheul%2520Choi%2520and%2520Jae-Ho%2520Chung%2520and%2520Jong%2520Woong%2520Park%2520and%2520Juyoun%2520Park%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520highlighted%2520their%2520potential%2520for%2520medical%2520and%2520surgical%2520applications.%2520However%252C%2520existing%2520surgical%2520datasets%2520predominantly%2520adopt%2520a%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520format%2520with%2520heterogeneous%2520taxonomies%2520and%2520lack%2520support%2520for%2520pixel-level%2520segmentation%252C%2520limiting%2520consistent%2520evaluation%2520and%2520applicability.%2520We%2520present%2520SurgMLLMBench%252C%2520a%2520unified%2520multimodal%2520benchmark%2520explicitly%2520designed%2520for%2520developing%2520and%2520evaluating%2520interactive%2520multimodal%2520LLMs%2520for%2520surgical%2520scene%2520understanding%252C%2520including%2520the%2520newly%2520collected%2520Micro-surgical%2520Artificial%2520Vascular%2520anastomosIS%2520%2528MAVIS%2529%2520dataset.%2520It%2520integrates%2520pixel-level%2520instrument%2520segmentation%2520masks%2520and%2520structured%2520VQA%2520annotations%2520across%2520laparoscopic%252C%2520robot-assisted%252C%2520and%2520micro-surgical%2520domains%2520under%2520a%2520unified%2520taxonomy%252C%2520enabling%2520comprehensive%2520evaluation%2520beyond%2520traditional%2520VQA%2520tasks%2520and%2520richer%2520visual-conversational%2520interactions.%2520Extensive%2520baseline%2520experiments%2520show%2520that%2520a%2520single%2520model%2520trained%2520on%2520SurgMLLMBench%2520achieves%2520consistent%2520performance%2520across%2520domains%2520and%2520generalizes%2520effectively%2520to%2520unseen%2520datasets.%2520SurgMLLMBench%2520will%2520be%2520publicly%2520released%2520as%2520a%2520robust%2520resource%2520to%2520advance%2520multimodal%2520surgical%2520AI%2520research%252C%2520supporting%2520reproducible%2520evaluation%2520and%2520development%2520of%2520interactive%2520surgical%2520reasoning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgMLLMBench%3A%20A%20Multimodal%20Large%20Language%20Model%20Benchmark%20Dataset%20for%20Surgical%20Scene%20Understanding&entry.906535625=Tae-Min%20Choi%20and%20Tae%20Kyeong%20Jeong%20and%20Garam%20Kim%20and%20Jaemin%20Lee%20and%20Yeongyoon%20Koh%20and%20In%20Cheul%20Choi%20and%20Jae-Ho%20Chung%20and%20Jong%20Woong%20Park%20and%20Juyoun%20Park&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20their%20potential%20for%20medical%20and%20surgical%20applications.%20However%2C%20existing%20surgical%20datasets%20predominantly%20adopt%20a%20Visual%20Question%20Answering%20%28VQA%29%20format%20with%20heterogeneous%20taxonomies%20and%20lack%20support%20for%20pixel-level%20segmentation%2C%20limiting%20consistent%20evaluation%20and%20applicability.%20We%20present%20SurgMLLMBench%2C%20a%20unified%20multimodal%20benchmark%20explicitly%20designed%20for%20developing%20and%20evaluating%20interactive%20multimodal%20LLMs%20for%20surgical%20scene%20understanding%2C%20including%20the%20newly%20collected%20Micro-surgical%20Artificial%20Vascular%20anastomosIS%20%28MAVIS%29%20dataset.%20It%20integrates%20pixel-level%20instrument%20segmentation%20masks%20and%20structured%20VQA%20annotations%20across%20laparoscopic%2C%20robot-assisted%2C%20and%20micro-surgical%20domains%20under%20a%20unified%20taxonomy%2C%20enabling%20comprehensive%20evaluation%20beyond%20traditional%20VQA%20tasks%20and%20richer%20visual-conversational%20interactions.%20Extensive%20baseline%20experiments%20show%20that%20a%20single%20model%20trained%20on%20SurgMLLMBench%20achieves%20consistent%20performance%20across%20domains%20and%20generalizes%20effectively%20to%20unseen%20datasets.%20SurgMLLMBench%20will%20be%20publicly%20released%20as%20a%20robust%20resource%20to%20advance%20multimodal%20surgical%20AI%20research%2C%20supporting%20reproducible%20evaluation%20and%20development%20of%20interactive%20surgical%20reasoning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.21339v1&entry.124074799=Read"},
{"title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues", "author": "Wei Huang and Jiaxin Li and Zang Wan and Huijun Di and Wei Liang and Zhu Yang", "abstract": "Guiding an agent to a specific target in indoor environments based solely on RGB inputs and a floor plan is a promising yet challenging problem. Although existing methods have made significant progress, two challenges remain unresolved. First, the modality gap between egocentric RGB observations and the floor plan hinders the integration of visual and spatial information for both local obstacle avoidance and global planning. Second, accurate localization is critical for navigation performance, but remains challenging at deployment in unseen environments due to the lack of explicit geometric alignment between RGB inputs and floor plans. We propose a novel diffusion-based policy, denoted as GlocDiff, which integrates global path planning from the floor plan with local depth-aware features derived from RGB observations. The floor plan offers explicit global guidance, while the depth features provide implicit geometric cues, collectively enabling precise prediction of optimal navigation directions and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation during training to enhance robustness against pose estimation errors, and we find that combining this with a relatively stable VO module during inference results in significantly improved navigation performance. Extensive experiments on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in achieving superior navigation performance, and the success of real-world deployments also highlights its potential for widespread practical applications.", "link": "http://arxiv.org/abs/2511.01493v2", "date": "2025-11-26", "relevancy": 2.229, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5614}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Floor%20Plan-Guided%20Visual%20Navigation%20Incorporating%20Depth%20and%20Directional%20Cues&body=Title%3A%20Floor%20Plan-Guided%20Visual%20Navigation%20Incorporating%20Depth%20and%20Directional%20Cues%0AAuthor%3A%20Wei%20Huang%20and%20Jiaxin%20Li%20and%20Zang%20Wan%20and%20Huijun%20Di%20and%20Wei%20Liang%20and%20Zhu%20Yang%0AAbstract%3A%20Guiding%20an%20agent%20to%20a%20specific%20target%20in%20indoor%20environments%20based%20solely%20on%20RGB%20inputs%20and%20a%20floor%20plan%20is%20a%20promising%20yet%20challenging%20problem.%20Although%20existing%20methods%20have%20made%20significant%20progress%2C%20two%20challenges%20remain%20unresolved.%20First%2C%20the%20modality%20gap%20between%20egocentric%20RGB%20observations%20and%20the%20floor%20plan%20hinders%20the%20integration%20of%20visual%20and%20spatial%20information%20for%20both%20local%20obstacle%20avoidance%20and%20global%20planning.%20Second%2C%20accurate%20localization%20is%20critical%20for%20navigation%20performance%2C%20but%20remains%20challenging%20at%20deployment%20in%20unseen%20environments%20due%20to%20the%20lack%20of%20explicit%20geometric%20alignment%20between%20RGB%20inputs%20and%20floor%20plans.%20We%20propose%20a%20novel%20diffusion-based%20policy%2C%20denoted%20as%20GlocDiff%2C%20which%20integrates%20global%20path%20planning%20from%20the%20floor%20plan%20with%20local%20depth-aware%20features%20derived%20from%20RGB%20observations.%20The%20floor%20plan%20offers%20explicit%20global%20guidance%2C%20while%20the%20depth%20features%20provide%20implicit%20geometric%20cues%2C%20collectively%20enabling%20precise%20prediction%20of%20optimal%20navigation%20directions%20and%20robust%20obstacle%20avoidance.%20Moreover%2C%20GlocDiff%20introduces%20noise%20perturbation%20during%20training%20to%20enhance%20robustness%20against%20pose%20estimation%20errors%2C%20and%20we%20find%20that%20combining%20this%20with%20a%20relatively%20stable%20VO%20module%20during%20inference%20results%20in%20significantly%20improved%20navigation%20performance.%20Extensive%20experiments%20on%20the%20FloNa%20benchmark%20demonstrate%20GlocDiff%27s%20efficiency%20and%20effectiveness%20in%20achieving%20superior%20navigation%20performance%2C%20and%20the%20success%20of%20real-world%20deployments%20also%20highlights%20its%20potential%20for%20widespread%20practical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01493v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloor%2520Plan-Guided%2520Visual%2520Navigation%2520Incorporating%2520Depth%2520and%2520Directional%2520Cues%26entry.906535625%3DWei%2520Huang%2520and%2520Jiaxin%2520Li%2520and%2520Zang%2520Wan%2520and%2520Huijun%2520Di%2520and%2520Wei%2520Liang%2520and%2520Zhu%2520Yang%26entry.1292438233%3DGuiding%2520an%2520agent%2520to%2520a%2520specific%2520target%2520in%2520indoor%2520environments%2520based%2520solely%2520on%2520RGB%2520inputs%2520and%2520a%2520floor%2520plan%2520is%2520a%2520promising%2520yet%2520challenging%2520problem.%2520Although%2520existing%2520methods%2520have%2520made%2520significant%2520progress%252C%2520two%2520challenges%2520remain%2520unresolved.%2520First%252C%2520the%2520modality%2520gap%2520between%2520egocentric%2520RGB%2520observations%2520and%2520the%2520floor%2520plan%2520hinders%2520the%2520integration%2520of%2520visual%2520and%2520spatial%2520information%2520for%2520both%2520local%2520obstacle%2520avoidance%2520and%2520global%2520planning.%2520Second%252C%2520accurate%2520localization%2520is%2520critical%2520for%2520navigation%2520performance%252C%2520but%2520remains%2520challenging%2520at%2520deployment%2520in%2520unseen%2520environments%2520due%2520to%2520the%2520lack%2520of%2520explicit%2520geometric%2520alignment%2520between%2520RGB%2520inputs%2520and%2520floor%2520plans.%2520We%2520propose%2520a%2520novel%2520diffusion-based%2520policy%252C%2520denoted%2520as%2520GlocDiff%252C%2520which%2520integrates%2520global%2520path%2520planning%2520from%2520the%2520floor%2520plan%2520with%2520local%2520depth-aware%2520features%2520derived%2520from%2520RGB%2520observations.%2520The%2520floor%2520plan%2520offers%2520explicit%2520global%2520guidance%252C%2520while%2520the%2520depth%2520features%2520provide%2520implicit%2520geometric%2520cues%252C%2520collectively%2520enabling%2520precise%2520prediction%2520of%2520optimal%2520navigation%2520directions%2520and%2520robust%2520obstacle%2520avoidance.%2520Moreover%252C%2520GlocDiff%2520introduces%2520noise%2520perturbation%2520during%2520training%2520to%2520enhance%2520robustness%2520against%2520pose%2520estimation%2520errors%252C%2520and%2520we%2520find%2520that%2520combining%2520this%2520with%2520a%2520relatively%2520stable%2520VO%2520module%2520during%2520inference%2520results%2520in%2520significantly%2520improved%2520navigation%2520performance.%2520Extensive%2520experiments%2520on%2520the%2520FloNa%2520benchmark%2520demonstrate%2520GlocDiff%2527s%2520efficiency%2520and%2520effectiveness%2520in%2520achieving%2520superior%2520navigation%2520performance%252C%2520and%2520the%2520success%2520of%2520real-world%2520deployments%2520also%2520highlights%2520its%2520potential%2520for%2520widespread%2520practical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01493v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Floor%20Plan-Guided%20Visual%20Navigation%20Incorporating%20Depth%20and%20Directional%20Cues&entry.906535625=Wei%20Huang%20and%20Jiaxin%20Li%20and%20Zang%20Wan%20and%20Huijun%20Di%20and%20Wei%20Liang%20and%20Zhu%20Yang&entry.1292438233=Guiding%20an%20agent%20to%20a%20specific%20target%20in%20indoor%20environments%20based%20solely%20on%20RGB%20inputs%20and%20a%20floor%20plan%20is%20a%20promising%20yet%20challenging%20problem.%20Although%20existing%20methods%20have%20made%20significant%20progress%2C%20two%20challenges%20remain%20unresolved.%20First%2C%20the%20modality%20gap%20between%20egocentric%20RGB%20observations%20and%20the%20floor%20plan%20hinders%20the%20integration%20of%20visual%20and%20spatial%20information%20for%20both%20local%20obstacle%20avoidance%20and%20global%20planning.%20Second%2C%20accurate%20localization%20is%20critical%20for%20navigation%20performance%2C%20but%20remains%20challenging%20at%20deployment%20in%20unseen%20environments%20due%20to%20the%20lack%20of%20explicit%20geometric%20alignment%20between%20RGB%20inputs%20and%20floor%20plans.%20We%20propose%20a%20novel%20diffusion-based%20policy%2C%20denoted%20as%20GlocDiff%2C%20which%20integrates%20global%20path%20planning%20from%20the%20floor%20plan%20with%20local%20depth-aware%20features%20derived%20from%20RGB%20observations.%20The%20floor%20plan%20offers%20explicit%20global%20guidance%2C%20while%20the%20depth%20features%20provide%20implicit%20geometric%20cues%2C%20collectively%20enabling%20precise%20prediction%20of%20optimal%20navigation%20directions%20and%20robust%20obstacle%20avoidance.%20Moreover%2C%20GlocDiff%20introduces%20noise%20perturbation%20during%20training%20to%20enhance%20robustness%20against%20pose%20estimation%20errors%2C%20and%20we%20find%20that%20combining%20this%20with%20a%20relatively%20stable%20VO%20module%20during%20inference%20results%20in%20significantly%20improved%20navigation%20performance.%20Extensive%20experiments%20on%20the%20FloNa%20benchmark%20demonstrate%20GlocDiff%27s%20efficiency%20and%20effectiveness%20in%20achieving%20superior%20navigation%20performance%2C%20and%20the%20success%20of%20real-world%20deployments%20also%20highlights%20its%20potential%20for%20widespread%20practical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.01493v2&entry.124074799=Read"},
{"title": "SUPN: Shallow Universal Polynomial Networks", "author": "Zachary Morrow and Michael Penwarden and Brian Chen and Aurya Javeed and Akil Narayan and John D. Jakeman", "abstract": "Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.", "link": "http://arxiv.org/abs/2511.21414v1", "date": "2025-11-26", "relevancy": 2.2156, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4374}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUPN%3A%20Shallow%20Universal%20Polynomial%20Networks&body=Title%3A%20SUPN%3A%20Shallow%20Universal%20Polynomial%20Networks%0AAuthor%3A%20Zachary%20Morrow%20and%20Michael%20Penwarden%20and%20Brian%20Chen%20and%20Aurya%20Javeed%20and%20Akil%20Narayan%20and%20John%20D.%20Jakeman%0AAbstract%3A%20Deep%20neural%20networks%20%28DNNs%29%20and%20Kolmogorov-Arnold%20networks%20%28KANs%29%20are%20popular%20methods%20for%20function%20approximation%20due%20to%20their%20flexibility%20and%20expressivity.%20However%2C%20they%20typically%20require%20a%20large%20number%20of%20trainable%20parameters%20to%20produce%20a%20suitable%20approximation.%20Beyond%20making%20the%20resulting%20network%20less%20transparent%2C%20overparameterization%20creates%20a%20large%20optimization%20space%2C%20likely%20producing%20local%20minima%20in%20training%20that%20have%20quite%20different%20generalization%20errors.%20In%20this%20case%2C%20network%20initialization%20can%20have%20an%20outsize%20impact%20on%20the%20model%27s%20out-of-sample%20accuracy.%20For%20these%20reasons%2C%20we%20propose%20shallow%20universal%20polynomial%20networks%20%28SUPNs%29.%20These%20networks%20replace%20all%20but%20the%20last%20hidden%20layer%20with%20a%20single%20layer%20of%20polynomials%20with%20learnable%20coefficients%2C%20leveraging%20the%20strengths%20of%20DNNs%20and%20polynomials%20to%20achieve%20sufficient%20expressivity%20with%20far%20fewer%20parameters.%20We%20prove%20that%20SUPNs%20converge%20at%20the%20same%20rate%20as%20the%20best%20polynomial%20approximation%20of%20the%20same%20degree%2C%20and%20we%20derive%20explicit%20formulas%20for%20quasi-optimal%20SUPN%20parameters.%20We%20complement%20theory%20with%20an%20extensive%20suite%20of%20numerical%20experiments%20involving%20SUPNs%2C%20DNNs%2C%20KANs%2C%20and%20polynomial%20projection%20in%20one%2C%20two%2C%20and%20ten%20dimensions%2C%20consisting%20of%20over%2013%2C000%20trained%20models.%20On%20the%20target%20functions%20we%20numerically%20studied%2C%20for%20a%20given%20number%20of%20trainable%20parameters%2C%20the%20approximation%20error%20and%20variability%20are%20often%20lower%20for%20SUPNs%20than%20for%20DNNs%20and%20KANs%20by%20an%20order%20of%20magnitude.%20In%20our%20examples%2C%20SUPNs%20even%20outperform%20polynomial%20projection%20on%20non-smooth%20functions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUPN%253A%2520Shallow%2520Universal%2520Polynomial%2520Networks%26entry.906535625%3DZachary%2520Morrow%2520and%2520Michael%2520Penwarden%2520and%2520Brian%2520Chen%2520and%2520Aurya%2520Javeed%2520and%2520Akil%2520Narayan%2520and%2520John%2520D.%2520Jakeman%26entry.1292438233%3DDeep%2520neural%2520networks%2520%2528DNNs%2529%2520and%2520Kolmogorov-Arnold%2520networks%2520%2528KANs%2529%2520are%2520popular%2520methods%2520for%2520function%2520approximation%2520due%2520to%2520their%2520flexibility%2520and%2520expressivity.%2520However%252C%2520they%2520typically%2520require%2520a%2520large%2520number%2520of%2520trainable%2520parameters%2520to%2520produce%2520a%2520suitable%2520approximation.%2520Beyond%2520making%2520the%2520resulting%2520network%2520less%2520transparent%252C%2520overparameterization%2520creates%2520a%2520large%2520optimization%2520space%252C%2520likely%2520producing%2520local%2520minima%2520in%2520training%2520that%2520have%2520quite%2520different%2520generalization%2520errors.%2520In%2520this%2520case%252C%2520network%2520initialization%2520can%2520have%2520an%2520outsize%2520impact%2520on%2520the%2520model%2527s%2520out-of-sample%2520accuracy.%2520For%2520these%2520reasons%252C%2520we%2520propose%2520shallow%2520universal%2520polynomial%2520networks%2520%2528SUPNs%2529.%2520These%2520networks%2520replace%2520all%2520but%2520the%2520last%2520hidden%2520layer%2520with%2520a%2520single%2520layer%2520of%2520polynomials%2520with%2520learnable%2520coefficients%252C%2520leveraging%2520the%2520strengths%2520of%2520DNNs%2520and%2520polynomials%2520to%2520achieve%2520sufficient%2520expressivity%2520with%2520far%2520fewer%2520parameters.%2520We%2520prove%2520that%2520SUPNs%2520converge%2520at%2520the%2520same%2520rate%2520as%2520the%2520best%2520polynomial%2520approximation%2520of%2520the%2520same%2520degree%252C%2520and%2520we%2520derive%2520explicit%2520formulas%2520for%2520quasi-optimal%2520SUPN%2520parameters.%2520We%2520complement%2520theory%2520with%2520an%2520extensive%2520suite%2520of%2520numerical%2520experiments%2520involving%2520SUPNs%252C%2520DNNs%252C%2520KANs%252C%2520and%2520polynomial%2520projection%2520in%2520one%252C%2520two%252C%2520and%2520ten%2520dimensions%252C%2520consisting%2520of%2520over%252013%252C000%2520trained%2520models.%2520On%2520the%2520target%2520functions%2520we%2520numerically%2520studied%252C%2520for%2520a%2520given%2520number%2520of%2520trainable%2520parameters%252C%2520the%2520approximation%2520error%2520and%2520variability%2520are%2520often%2520lower%2520for%2520SUPNs%2520than%2520for%2520DNNs%2520and%2520KANs%2520by%2520an%2520order%2520of%2520magnitude.%2520In%2520our%2520examples%252C%2520SUPNs%2520even%2520outperform%2520polynomial%2520projection%2520on%2520non-smooth%2520functions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPN%3A%20Shallow%20Universal%20Polynomial%20Networks&entry.906535625=Zachary%20Morrow%20and%20Michael%20Penwarden%20and%20Brian%20Chen%20and%20Aurya%20Javeed%20and%20Akil%20Narayan%20and%20John%20D.%20Jakeman&entry.1292438233=Deep%20neural%20networks%20%28DNNs%29%20and%20Kolmogorov-Arnold%20networks%20%28KANs%29%20are%20popular%20methods%20for%20function%20approximation%20due%20to%20their%20flexibility%20and%20expressivity.%20However%2C%20they%20typically%20require%20a%20large%20number%20of%20trainable%20parameters%20to%20produce%20a%20suitable%20approximation.%20Beyond%20making%20the%20resulting%20network%20less%20transparent%2C%20overparameterization%20creates%20a%20large%20optimization%20space%2C%20likely%20producing%20local%20minima%20in%20training%20that%20have%20quite%20different%20generalization%20errors.%20In%20this%20case%2C%20network%20initialization%20can%20have%20an%20outsize%20impact%20on%20the%20model%27s%20out-of-sample%20accuracy.%20For%20these%20reasons%2C%20we%20propose%20shallow%20universal%20polynomial%20networks%20%28SUPNs%29.%20These%20networks%20replace%20all%20but%20the%20last%20hidden%20layer%20with%20a%20single%20layer%20of%20polynomials%20with%20learnable%20coefficients%2C%20leveraging%20the%20strengths%20of%20DNNs%20and%20polynomials%20to%20achieve%20sufficient%20expressivity%20with%20far%20fewer%20parameters.%20We%20prove%20that%20SUPNs%20converge%20at%20the%20same%20rate%20as%20the%20best%20polynomial%20approximation%20of%20the%20same%20degree%2C%20and%20we%20derive%20explicit%20formulas%20for%20quasi-optimal%20SUPN%20parameters.%20We%20complement%20theory%20with%20an%20extensive%20suite%20of%20numerical%20experiments%20involving%20SUPNs%2C%20DNNs%2C%20KANs%2C%20and%20polynomial%20projection%20in%20one%2C%20two%2C%20and%20ten%20dimensions%2C%20consisting%20of%20over%2013%2C000%20trained%20models.%20On%20the%20target%20functions%20we%20numerically%20studied%2C%20for%20a%20given%20number%20of%20trainable%20parameters%2C%20the%20approximation%20error%20and%20variability%20are%20often%20lower%20for%20SUPNs%20than%20for%20DNNs%20and%20KANs%20by%20an%20order%20of%20magnitude.%20In%20our%20examples%2C%20SUPNs%20even%20outperform%20polynomial%20projection%20on%20non-smooth%20functions.&entry.1838667208=http%3A//arxiv.org/abs/2511.21414v1&entry.124074799=Read"},
{"title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation", "author": "Hui Zhou and Siyuan Huang and Minxing Li and Hao Zhang and Lue Fan and Shaoshuai Shi", "abstract": "Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.", "link": "http://arxiv.org/abs/2511.21557v1", "date": "2025-11-26", "relevancy": 2.2085, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5539}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VacuumVLA%3A%20Boosting%20VLA%20Capabilities%20via%20a%20Unified%20Suction%20and%20Gripping%20Tool%20for%20Complex%20Robotic%20Manipulation&body=Title%3A%20VacuumVLA%3A%20Boosting%20VLA%20Capabilities%20via%20a%20Unified%20Suction%20and%20Gripping%20Tool%20for%20Complex%20Robotic%20Manipulation%0AAuthor%3A%20Hui%20Zhou%20and%20Siyuan%20Huang%20and%20Minxing%20Li%20and%20Hao%20Zhang%20and%20Lue%20Fan%20and%20Shaoshuai%20Shi%0AAbstract%3A%20Vision%20Language%20Action%20models%20have%20significantly%20advanced%20general%20purpose%20robotic%20manipulation%20by%20harnessing%20large%20scale%20pretrained%20vision%20and%20language%20representations.%20Among%20existing%20approaches%2C%20a%20majority%20of%20current%20VLA%20systems%20employ%20parallel%20two%20finger%20grippers%20as%20their%20default%20end%20effectors.%20However%2C%20such%20grippers%20face%20inherent%20limitations%20in%20handling%20certain%20real%20world%20tasks%20such%20as%20wiping%20glass%20surfaces%20or%20opening%20drawers%20without%20handles%20due%20to%20insufficient%20contact%20area%20or%20lack%20of%20adhesion.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20low%20cost%2C%20integrated%20hardware%20design%20that%20combines%20a%20mechanical%20two%20finger%20gripper%20with%20a%20vacuum%20suction%20unit%2C%20enabling%20dual%20mode%20manipulation%20within%20a%20single%20end%20effector.%20Our%20system%20supports%20flexible%20switching%20or%20synergistic%20use%20of%20both%20modalities%2C%20expanding%20the%20range%20of%20feasible%20tasks.%20We%20validate%20the%20efficiency%20and%20practicality%20of%20our%20design%20within%20two%20state%20of%20the%20art%20VLA%20frameworks%3A%20DexVLA%20and%20Pi0.%20Experimental%20results%20demonstrate%20that%20with%20the%20proposed%20hybrid%20end%20effector%2C%20robots%20can%20successfully%20perform%20multiple%20complex%20tasks%20that%20are%20infeasible%20for%20conventional%20two%20finger%20grippers%20alone.%20All%20hardware%20designs%20and%20controlling%20systems%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVacuumVLA%253A%2520Boosting%2520VLA%2520Capabilities%2520via%2520a%2520Unified%2520Suction%2520and%2520Gripping%2520Tool%2520for%2520Complex%2520Robotic%2520Manipulation%26entry.906535625%3DHui%2520Zhou%2520and%2520Siyuan%2520Huang%2520and%2520Minxing%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Lue%2520Fan%2520and%2520Shaoshuai%2520Shi%26entry.1292438233%3DVision%2520Language%2520Action%2520models%2520have%2520significantly%2520advanced%2520general%2520purpose%2520robotic%2520manipulation%2520by%2520harnessing%2520large%2520scale%2520pretrained%2520vision%2520and%2520language%2520representations.%2520Among%2520existing%2520approaches%252C%2520a%2520majority%2520of%2520current%2520VLA%2520systems%2520employ%2520parallel%2520two%2520finger%2520grippers%2520as%2520their%2520default%2520end%2520effectors.%2520However%252C%2520such%2520grippers%2520face%2520inherent%2520limitations%2520in%2520handling%2520certain%2520real%2520world%2520tasks%2520such%2520as%2520wiping%2520glass%2520surfaces%2520or%2520opening%2520drawers%2520without%2520handles%2520due%2520to%2520insufficient%2520contact%2520area%2520or%2520lack%2520of%2520adhesion.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520a%2520low%2520cost%252C%2520integrated%2520hardware%2520design%2520that%2520combines%2520a%2520mechanical%2520two%2520finger%2520gripper%2520with%2520a%2520vacuum%2520suction%2520unit%252C%2520enabling%2520dual%2520mode%2520manipulation%2520within%2520a%2520single%2520end%2520effector.%2520Our%2520system%2520supports%2520flexible%2520switching%2520or%2520synergistic%2520use%2520of%2520both%2520modalities%252C%2520expanding%2520the%2520range%2520of%2520feasible%2520tasks.%2520We%2520validate%2520the%2520efficiency%2520and%2520practicality%2520of%2520our%2520design%2520within%2520two%2520state%2520of%2520the%2520art%2520VLA%2520frameworks%253A%2520DexVLA%2520and%2520Pi0.%2520Experimental%2520results%2520demonstrate%2520that%2520with%2520the%2520proposed%2520hybrid%2520end%2520effector%252C%2520robots%2520can%2520successfully%2520perform%2520multiple%2520complex%2520tasks%2520that%2520are%2520infeasible%2520for%2520conventional%2520two%2520finger%2520grippers%2520alone.%2520All%2520hardware%2520designs%2520and%2520controlling%2520systems%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VacuumVLA%3A%20Boosting%20VLA%20Capabilities%20via%20a%20Unified%20Suction%20and%20Gripping%20Tool%20for%20Complex%20Robotic%20Manipulation&entry.906535625=Hui%20Zhou%20and%20Siyuan%20Huang%20and%20Minxing%20Li%20and%20Hao%20Zhang%20and%20Lue%20Fan%20and%20Shaoshuai%20Shi&entry.1292438233=Vision%20Language%20Action%20models%20have%20significantly%20advanced%20general%20purpose%20robotic%20manipulation%20by%20harnessing%20large%20scale%20pretrained%20vision%20and%20language%20representations.%20Among%20existing%20approaches%2C%20a%20majority%20of%20current%20VLA%20systems%20employ%20parallel%20two%20finger%20grippers%20as%20their%20default%20end%20effectors.%20However%2C%20such%20grippers%20face%20inherent%20limitations%20in%20handling%20certain%20real%20world%20tasks%20such%20as%20wiping%20glass%20surfaces%20or%20opening%20drawers%20without%20handles%20due%20to%20insufficient%20contact%20area%20or%20lack%20of%20adhesion.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20low%20cost%2C%20integrated%20hardware%20design%20that%20combines%20a%20mechanical%20two%20finger%20gripper%20with%20a%20vacuum%20suction%20unit%2C%20enabling%20dual%20mode%20manipulation%20within%20a%20single%20end%20effector.%20Our%20system%20supports%20flexible%20switching%20or%20synergistic%20use%20of%20both%20modalities%2C%20expanding%20the%20range%20of%20feasible%20tasks.%20We%20validate%20the%20efficiency%20and%20practicality%20of%20our%20design%20within%20two%20state%20of%20the%20art%20VLA%20frameworks%3A%20DexVLA%20and%20Pi0.%20Experimental%20results%20demonstrate%20that%20with%20the%20proposed%20hybrid%20end%20effector%2C%20robots%20can%20successfully%20perform%20multiple%20complex%20tasks%20that%20are%20infeasible%20for%20conventional%20two%20finger%20grippers%20alone.%20All%20hardware%20designs%20and%20controlling%20systems%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.21557v1&entry.124074799=Read"},
{"title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy", "author": "Teng Hu and Zhentao Yu and Guozhen Zhang and Zihan Su and Zhengguang Zhou and Youliang Zhang and Yuan Zhou and Qinglin Lu and Ran Yi", "abstract": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.", "link": "http://arxiv.org/abs/2511.21579v1", "date": "2025-11-26", "relevancy": 2.2053, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5696}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy&body=Title%3A%20Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy%0AAuthor%3A%20Teng%20Hu%20and%20Zhentao%20Yu%20and%20Guozhen%20Zhang%20and%20Zihan%20Su%20and%20Zhengguang%20Zhou%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi%0AAbstract%3A%20The%20synthesis%20of%20synchronized%20audio-visual%20content%20is%20a%20key%20challenge%20in%20generative%20AI%2C%20with%20open-source%20models%20facing%20challenges%20in%20robust%20audio-video%20alignment.%20Our%20analysis%20reveals%20that%20this%20issue%20is%20rooted%20in%20three%20fundamental%20challenges%20of%20the%20joint%20diffusion%20process%3A%20%281%29%20Correspondence%20Drift%2C%20where%20concurrently%20evolving%20noisy%20latents%20impede%20stable%20learning%20of%20alignment%3B%20%282%29%20inefficient%20global%20attention%20mechanisms%20that%20fail%20to%20capture%20fine-grained%20temporal%20cues%3B%20and%20%283%29%20the%20intra-modal%20bias%20of%20conventional%20Classifier-Free%20Guidance%20%28CFG%29%2C%20which%20enhances%20conditionality%20but%20not%20cross-modal%20synchronization.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20Harmony%2C%20a%20novel%20framework%20that%20mechanistically%20enforces%20audio-visual%20synchronization.%20We%20first%20propose%20a%20Cross-Task%20Synergy%20training%20paradigm%20to%20mitigate%20drift%20by%20leveraging%20strong%20supervisory%20signals%20from%20audio-driven%20video%20and%20video-driven%20audio%20generation%20tasks.%20Then%2C%20we%20design%20a%20Global-Local%20Decoupled%20Interaction%20Module%20for%20efficient%20and%20precise%20temporal-style%20alignment.%20Finally%2C%20we%20present%20a%20novel%20Synchronization-Enhanced%20CFG%20%28SyncCFG%29%20that%20explicitly%20isolates%20and%20amplifies%20the%20alignment%20signal%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%20Harmony%20establishes%20a%20new%20state-of-the-art%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20generation%20fidelity%20and%2C%20critically%2C%20in%20achieving%20fine-grained%20audio-visual%20synchronization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmony%253A%2520Harmonizing%2520Audio%2520and%2520Video%2520Generation%2520through%2520Cross-Task%2520Synergy%26entry.906535625%3DTeng%2520Hu%2520and%2520Zhentao%2520Yu%2520and%2520Guozhen%2520Zhang%2520and%2520Zihan%2520Su%2520and%2520Zhengguang%2520Zhou%2520and%2520Youliang%2520Zhang%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Ran%2520Yi%26entry.1292438233%3DThe%2520synthesis%2520of%2520synchronized%2520audio-visual%2520content%2520is%2520a%2520key%2520challenge%2520in%2520generative%2520AI%252C%2520with%2520open-source%2520models%2520facing%2520challenges%2520in%2520robust%2520audio-video%2520alignment.%2520Our%2520analysis%2520reveals%2520that%2520this%2520issue%2520is%2520rooted%2520in%2520three%2520fundamental%2520challenges%2520of%2520the%2520joint%2520diffusion%2520process%253A%2520%25281%2529%2520Correspondence%2520Drift%252C%2520where%2520concurrently%2520evolving%2520noisy%2520latents%2520impede%2520stable%2520learning%2520of%2520alignment%253B%2520%25282%2529%2520inefficient%2520global%2520attention%2520mechanisms%2520that%2520fail%2520to%2520capture%2520fine-grained%2520temporal%2520cues%253B%2520and%2520%25283%2529%2520the%2520intra-modal%2520bias%2520of%2520conventional%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520which%2520enhances%2520conditionality%2520but%2520not%2520cross-modal%2520synchronization.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520Harmony%252C%2520a%2520novel%2520framework%2520that%2520mechanistically%2520enforces%2520audio-visual%2520synchronization.%2520We%2520first%2520propose%2520a%2520Cross-Task%2520Synergy%2520training%2520paradigm%2520to%2520mitigate%2520drift%2520by%2520leveraging%2520strong%2520supervisory%2520signals%2520from%2520audio-driven%2520video%2520and%2520video-driven%2520audio%2520generation%2520tasks.%2520Then%252C%2520we%2520design%2520a%2520Global-Local%2520Decoupled%2520Interaction%2520Module%2520for%2520efficient%2520and%2520precise%2520temporal-style%2520alignment.%2520Finally%252C%2520we%2520present%2520a%2520novel%2520Synchronization-Enhanced%2520CFG%2520%2528SyncCFG%2529%2520that%2520explicitly%2520isolates%2520and%2520amplifies%2520the%2520alignment%2520signal%2520during%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Harmony%2520establishes%2520a%2520new%2520state-of-the-art%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520both%2520generation%2520fidelity%2520and%252C%2520critically%252C%2520in%2520achieving%2520fine-grained%2520audio-visual%2520synchronization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy&entry.906535625=Teng%20Hu%20and%20Zhentao%20Yu%20and%20Guozhen%20Zhang%20and%20Zihan%20Su%20and%20Zhengguang%20Zhou%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi&entry.1292438233=The%20synthesis%20of%20synchronized%20audio-visual%20content%20is%20a%20key%20challenge%20in%20generative%20AI%2C%20with%20open-source%20models%20facing%20challenges%20in%20robust%20audio-video%20alignment.%20Our%20analysis%20reveals%20that%20this%20issue%20is%20rooted%20in%20three%20fundamental%20challenges%20of%20the%20joint%20diffusion%20process%3A%20%281%29%20Correspondence%20Drift%2C%20where%20concurrently%20evolving%20noisy%20latents%20impede%20stable%20learning%20of%20alignment%3B%20%282%29%20inefficient%20global%20attention%20mechanisms%20that%20fail%20to%20capture%20fine-grained%20temporal%20cues%3B%20and%20%283%29%20the%20intra-modal%20bias%20of%20conventional%20Classifier-Free%20Guidance%20%28CFG%29%2C%20which%20enhances%20conditionality%20but%20not%20cross-modal%20synchronization.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20Harmony%2C%20a%20novel%20framework%20that%20mechanistically%20enforces%20audio-visual%20synchronization.%20We%20first%20propose%20a%20Cross-Task%20Synergy%20training%20paradigm%20to%20mitigate%20drift%20by%20leveraging%20strong%20supervisory%20signals%20from%20audio-driven%20video%20and%20video-driven%20audio%20generation%20tasks.%20Then%2C%20we%20design%20a%20Global-Local%20Decoupled%20Interaction%20Module%20for%20efficient%20and%20precise%20temporal-style%20alignment.%20Finally%2C%20we%20present%20a%20novel%20Synchronization-Enhanced%20CFG%20%28SyncCFG%29%20that%20explicitly%20isolates%20and%20amplifies%20the%20alignment%20signal%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%20Harmony%20establishes%20a%20new%20state-of-the-art%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20generation%20fidelity%20and%2C%20critically%2C%20in%20achieving%20fine-grained%20audio-visual%20synchronization.&entry.1838667208=http%3A//arxiv.org/abs/2511.21579v1&entry.124074799=Read"},
{"title": "Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects", "author": "Husne Ara Rubaiyeat and Hasan Mahmud and Md Kamrul Hasan", "abstract": "Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].", "link": "http://arxiv.org/abs/2511.21533v1", "date": "2025-11-26", "relevancy": 2.1998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4419}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bangla%20Sign%20Language%20Translation%3A%20Dataset%20Creation%20Challenges%2C%20Benchmarking%20and%20Prospects&body=Title%3A%20Bangla%20Sign%20Language%20Translation%3A%20Dataset%20Creation%20Challenges%2C%20Benchmarking%20and%20Prospects%0AAuthor%3A%20Husne%20Ara%20Rubaiyeat%20and%20Hasan%20Mahmud%20and%20Md%20Kamrul%20Hasan%0AAbstract%3A%20Bangla%20Sign%20Language%20Translation%20%28BdSLT%29%20has%20been%20severely%20constrained%20so%20far%20as%20the%20language%20itself%20is%20very%20low%20resource.%20Standard%20sentence%20level%20dataset%20creation%20for%20BdSLT%20is%20of%20immense%20importance%20for%20developing%20AI%20based%20assistive%20tools%20for%20deaf%20and%20hard%20of%20hearing%20people%20of%20Bangla%20speaking%20community.%20In%20this%20paper%2C%20we%20present%20a%20dataset%2C%20IsharaKhobor%20%2C%20and%20two%20subset%20of%20it%20for%20enabling%20research.%20We%20also%20present%20the%20challenges%20towards%20developing%20the%20dataset%20and%20present%20some%20way%20forward%20by%20benchmarking%20with%20landmark%20based%20raw%20and%20RQE%20embedding.%20We%20do%20some%20ablation%20on%20vocabulary%20restriction%20and%20canonicalization%20of%20the%20same%20within%20the%20dataset%2C%20which%20resulted%20in%20two%20more%20datasets%2C%20IsharaKhobor_small%20and%20IsharaKhobor_canonical_small.%20The%20dataset%20is%20publicly%20available%20at%3A%20www.kaggle.com/datasets/hasanssl/isharakhobor%20%5B1%5D.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBangla%2520Sign%2520Language%2520Translation%253A%2520Dataset%2520Creation%2520Challenges%252C%2520Benchmarking%2520and%2520Prospects%26entry.906535625%3DHusne%2520Ara%2520Rubaiyeat%2520and%2520Hasan%2520Mahmud%2520and%2520Md%2520Kamrul%2520Hasan%26entry.1292438233%3DBangla%2520Sign%2520Language%2520Translation%2520%2528BdSLT%2529%2520has%2520been%2520severely%2520constrained%2520so%2520far%2520as%2520the%2520language%2520itself%2520is%2520very%2520low%2520resource.%2520Standard%2520sentence%2520level%2520dataset%2520creation%2520for%2520BdSLT%2520is%2520of%2520immense%2520importance%2520for%2520developing%2520AI%2520based%2520assistive%2520tools%2520for%2520deaf%2520and%2520hard%2520of%2520hearing%2520people%2520of%2520Bangla%2520speaking%2520community.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520dataset%252C%2520IsharaKhobor%2520%252C%2520and%2520two%2520subset%2520of%2520it%2520for%2520enabling%2520research.%2520We%2520also%2520present%2520the%2520challenges%2520towards%2520developing%2520the%2520dataset%2520and%2520present%2520some%2520way%2520forward%2520by%2520benchmarking%2520with%2520landmark%2520based%2520raw%2520and%2520RQE%2520embedding.%2520We%2520do%2520some%2520ablation%2520on%2520vocabulary%2520restriction%2520and%2520canonicalization%2520of%2520the%2520same%2520within%2520the%2520dataset%252C%2520which%2520resulted%2520in%2520two%2520more%2520datasets%252C%2520IsharaKhobor_small%2520and%2520IsharaKhobor_canonical_small.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%253A%2520www.kaggle.com/datasets/hasanssl/isharakhobor%2520%255B1%255D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bangla%20Sign%20Language%20Translation%3A%20Dataset%20Creation%20Challenges%2C%20Benchmarking%20and%20Prospects&entry.906535625=Husne%20Ara%20Rubaiyeat%20and%20Hasan%20Mahmud%20and%20Md%20Kamrul%20Hasan&entry.1292438233=Bangla%20Sign%20Language%20Translation%20%28BdSLT%29%20has%20been%20severely%20constrained%20so%20far%20as%20the%20language%20itself%20is%20very%20low%20resource.%20Standard%20sentence%20level%20dataset%20creation%20for%20BdSLT%20is%20of%20immense%20importance%20for%20developing%20AI%20based%20assistive%20tools%20for%20deaf%20and%20hard%20of%20hearing%20people%20of%20Bangla%20speaking%20community.%20In%20this%20paper%2C%20we%20present%20a%20dataset%2C%20IsharaKhobor%20%2C%20and%20two%20subset%20of%20it%20for%20enabling%20research.%20We%20also%20present%20the%20challenges%20towards%20developing%20the%20dataset%20and%20present%20some%20way%20forward%20by%20benchmarking%20with%20landmark%20based%20raw%20and%20RQE%20embedding.%20We%20do%20some%20ablation%20on%20vocabulary%20restriction%20and%20canonicalization%20of%20the%20same%20within%20the%20dataset%2C%20which%20resulted%20in%20two%20more%20datasets%2C%20IsharaKhobor_small%20and%20IsharaKhobor_canonical_small.%20The%20dataset%20is%20publicly%20available%20at%3A%20www.kaggle.com/datasets/hasanssl/isharakhobor%20%5B1%5D.&entry.1838667208=http%3A//arxiv.org/abs/2511.21533v1&entry.124074799=Read"},
{"title": "Unsupervised Segmentation by Diffusing, Walking and Cutting", "author": "Daniela Ivanova and Marco Aversa and Paul Henderson and John Williamson", "abstract": "We propose an unsupervised image segmentation method using features from pre-trained text-to-image diffusion models. Inspired by classic spectral clustering approaches, we construct adjacency matrices from self-attention layers between image patches and recursively partition using Normalised Cuts. A key insight is that self-attention probability distributions, which capture semantic relations between patches, can be interpreted as a transition matrix for random walks across the image. We leverage this by first using Random Walk Normalized Cuts directly on these self-attention activations to partition the image, minimizing transition probabilities between clusters while maximizing coherence within clusters. Applied recursively, this yields a hierarchical segmentation that reflects the rich semantics in the pre-trained attention layers, without any additional training. Next, we explore other ways to build the NCuts adjacency matrix from features, and how we can use the random walk interpretation of self-attention to capture long-range relationships. Finally, we propose an approach to automatically determine the NCut cost criterion, avoiding the need to tune this manually. We quantitatively analyse the effect incorporating different features, a constant versus dynamic NCut threshold, and incorporating multi-node paths when constructing the NCuts adjacency matrix. We show that our approach surpasses all existing methods for zero-shot unsupervised segmentation, achieving state-of-the-art results on COCO-Stuff-27 and Cityscapes.", "link": "http://arxiv.org/abs/2412.04678v2", "date": "2025-11-26", "relevancy": 2.1967, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5584}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5443}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Segmentation%20by%20Diffusing%2C%20Walking%20and%20Cutting&body=Title%3A%20Unsupervised%20Segmentation%20by%20Diffusing%2C%20Walking%20and%20Cutting%0AAuthor%3A%20Daniela%20Ivanova%20and%20Marco%20Aversa%20and%20Paul%20Henderson%20and%20John%20Williamson%0AAbstract%3A%20We%20propose%20an%20unsupervised%20image%20segmentation%20method%20using%20features%20from%20pre-trained%20text-to-image%20diffusion%20models.%20Inspired%20by%20classic%20spectral%20clustering%20approaches%2C%20we%20construct%20adjacency%20matrices%20from%20self-attention%20layers%20between%20image%20patches%20and%20recursively%20partition%20using%20Normalised%20Cuts.%20A%20key%20insight%20is%20that%20self-attention%20probability%20distributions%2C%20which%20capture%20semantic%20relations%20between%20patches%2C%20can%20be%20interpreted%20as%20a%20transition%20matrix%20for%20random%20walks%20across%20the%20image.%20We%20leverage%20this%20by%20first%20using%20Random%20Walk%20Normalized%20Cuts%20directly%20on%20these%20self-attention%20activations%20to%20partition%20the%20image%2C%20minimizing%20transition%20probabilities%20between%20clusters%20while%20maximizing%20coherence%20within%20clusters.%20Applied%20recursively%2C%20this%20yields%20a%20hierarchical%20segmentation%20that%20reflects%20the%20rich%20semantics%20in%20the%20pre-trained%20attention%20layers%2C%20without%20any%20additional%20training.%20Next%2C%20we%20explore%20other%20ways%20to%20build%20the%20NCuts%20adjacency%20matrix%20from%20features%2C%20and%20how%20we%20can%20use%20the%20random%20walk%20interpretation%20of%20self-attention%20to%20capture%20long-range%20relationships.%20Finally%2C%20we%20propose%20an%20approach%20to%20automatically%20determine%20the%20NCut%20cost%20criterion%2C%20avoiding%20the%20need%20to%20tune%20this%20manually.%20We%20quantitatively%20analyse%20the%20effect%20incorporating%20different%20features%2C%20a%20constant%20versus%20dynamic%20NCut%20threshold%2C%20and%20incorporating%20multi-node%20paths%20when%20constructing%20the%20NCuts%20adjacency%20matrix.%20We%20show%20that%20our%20approach%20surpasses%20all%20existing%20methods%20for%20zero-shot%20unsupervised%20segmentation%2C%20achieving%20state-of-the-art%20results%20on%20COCO-Stuff-27%20and%20Cityscapes.%0ALink%3A%20http%3A//arxiv.org/abs/2412.04678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Segmentation%2520by%2520Diffusing%252C%2520Walking%2520and%2520Cutting%26entry.906535625%3DDaniela%2520Ivanova%2520and%2520Marco%2520Aversa%2520and%2520Paul%2520Henderson%2520and%2520John%2520Williamson%26entry.1292438233%3DWe%2520propose%2520an%2520unsupervised%2520image%2520segmentation%2520method%2520using%2520features%2520from%2520pre-trained%2520text-to-image%2520diffusion%2520models.%2520Inspired%2520by%2520classic%2520spectral%2520clustering%2520approaches%252C%2520we%2520construct%2520adjacency%2520matrices%2520from%2520self-attention%2520layers%2520between%2520image%2520patches%2520and%2520recursively%2520partition%2520using%2520Normalised%2520Cuts.%2520A%2520key%2520insight%2520is%2520that%2520self-attention%2520probability%2520distributions%252C%2520which%2520capture%2520semantic%2520relations%2520between%2520patches%252C%2520can%2520be%2520interpreted%2520as%2520a%2520transition%2520matrix%2520for%2520random%2520walks%2520across%2520the%2520image.%2520We%2520leverage%2520this%2520by%2520first%2520using%2520Random%2520Walk%2520Normalized%2520Cuts%2520directly%2520on%2520these%2520self-attention%2520activations%2520to%2520partition%2520the%2520image%252C%2520minimizing%2520transition%2520probabilities%2520between%2520clusters%2520while%2520maximizing%2520coherence%2520within%2520clusters.%2520Applied%2520recursively%252C%2520this%2520yields%2520a%2520hierarchical%2520segmentation%2520that%2520reflects%2520the%2520rich%2520semantics%2520in%2520the%2520pre-trained%2520attention%2520layers%252C%2520without%2520any%2520additional%2520training.%2520Next%252C%2520we%2520explore%2520other%2520ways%2520to%2520build%2520the%2520NCuts%2520adjacency%2520matrix%2520from%2520features%252C%2520and%2520how%2520we%2520can%2520use%2520the%2520random%2520walk%2520interpretation%2520of%2520self-attention%2520to%2520capture%2520long-range%2520relationships.%2520Finally%252C%2520we%2520propose%2520an%2520approach%2520to%2520automatically%2520determine%2520the%2520NCut%2520cost%2520criterion%252C%2520avoiding%2520the%2520need%2520to%2520tune%2520this%2520manually.%2520We%2520quantitatively%2520analyse%2520the%2520effect%2520incorporating%2520different%2520features%252C%2520a%2520constant%2520versus%2520dynamic%2520NCut%2520threshold%252C%2520and%2520incorporating%2520multi-node%2520paths%2520when%2520constructing%2520the%2520NCuts%2520adjacency%2520matrix.%2520We%2520show%2520that%2520our%2520approach%2520surpasses%2520all%2520existing%2520methods%2520for%2520zero-shot%2520unsupervised%2520segmentation%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520COCO-Stuff-27%2520and%2520Cityscapes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Segmentation%20by%20Diffusing%2C%20Walking%20and%20Cutting&entry.906535625=Daniela%20Ivanova%20and%20Marco%20Aversa%20and%20Paul%20Henderson%20and%20John%20Williamson&entry.1292438233=We%20propose%20an%20unsupervised%20image%20segmentation%20method%20using%20features%20from%20pre-trained%20text-to-image%20diffusion%20models.%20Inspired%20by%20classic%20spectral%20clustering%20approaches%2C%20we%20construct%20adjacency%20matrices%20from%20self-attention%20layers%20between%20image%20patches%20and%20recursively%20partition%20using%20Normalised%20Cuts.%20A%20key%20insight%20is%20that%20self-attention%20probability%20distributions%2C%20which%20capture%20semantic%20relations%20between%20patches%2C%20can%20be%20interpreted%20as%20a%20transition%20matrix%20for%20random%20walks%20across%20the%20image.%20We%20leverage%20this%20by%20first%20using%20Random%20Walk%20Normalized%20Cuts%20directly%20on%20these%20self-attention%20activations%20to%20partition%20the%20image%2C%20minimizing%20transition%20probabilities%20between%20clusters%20while%20maximizing%20coherence%20within%20clusters.%20Applied%20recursively%2C%20this%20yields%20a%20hierarchical%20segmentation%20that%20reflects%20the%20rich%20semantics%20in%20the%20pre-trained%20attention%20layers%2C%20without%20any%20additional%20training.%20Next%2C%20we%20explore%20other%20ways%20to%20build%20the%20NCuts%20adjacency%20matrix%20from%20features%2C%20and%20how%20we%20can%20use%20the%20random%20walk%20interpretation%20of%20self-attention%20to%20capture%20long-range%20relationships.%20Finally%2C%20we%20propose%20an%20approach%20to%20automatically%20determine%20the%20NCut%20cost%20criterion%2C%20avoiding%20the%20need%20to%20tune%20this%20manually.%20We%20quantitatively%20analyse%20the%20effect%20incorporating%20different%20features%2C%20a%20constant%20versus%20dynamic%20NCut%20threshold%2C%20and%20incorporating%20multi-node%20paths%20when%20constructing%20the%20NCuts%20adjacency%20matrix.%20We%20show%20that%20our%20approach%20surpasses%20all%20existing%20methods%20for%20zero-shot%20unsupervised%20segmentation%2C%20achieving%20state-of-the-art%20results%20on%20COCO-Stuff-27%20and%20Cityscapes.&entry.1838667208=http%3A//arxiv.org/abs/2412.04678v2&entry.124074799=Read"},
{"title": "Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning", "author": "Kaifeng Hong and Yinglong Zhang and Xiaoying Hong and Xuewen Xia and Xing Xu", "abstract": "Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.", "link": "http://arxiv.org/abs/2511.21416v1", "date": "2025-11-26", "relevancy": 2.1912, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5732}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Odin%3A%20Oriented%20Dual-module%20Integration%20for%20Text-rich%20Network%20Representation%20Learning&body=Title%3A%20Odin%3A%20Oriented%20Dual-module%20Integration%20for%20Text-rich%20Network%20Representation%20Learning%0AAuthor%3A%20Kaifeng%20Hong%20and%20Yinglong%20Zhang%20and%20Xiaoying%20Hong%20and%20Xuewen%20Xia%20and%20Xing%20Xu%0AAbstract%3A%20Text-attributed%20graphs%20require%20models%20to%20effectively%20combine%20strong%20textual%20understanding%20with%20structurally%20informed%20reasoning.%20Existing%20approaches%20either%20rely%20on%20GNNs--limited%20by%20over-smoothing%20and%20hop-dependent%20diffusion--or%20employ%20Transformers%20that%20overlook%20graph%20topology%20and%20treat%20nodes%20as%20isolated%20sequences.%20We%20propose%20Odin%20%28Oriented%20Dual-module%20INtegration%29%2C%20a%20new%20architecture%20that%20injects%20graph%20structure%20into%20Transformers%20at%20selected%20depths%20through%20an%20oriented%20dual-module%20mechanism.Unlike%20message-passing%20GNNs%2C%20Odin%20does%20not%20rely%20on%20multi-hop%20diffusion%3B%20instead%2C%20multi-hop%20structures%20are%20integrated%20at%20specific%20Transformer%20layers%2C%20yielding%20low-%2C%20mid-%2C%20and%20high-level%20structural%20abstraction%20aligned%20with%20the%20model%27s%20semantic%20hierarchy.%20Because%20aggregation%20operates%20on%20the%20global%20%5BCLS%5D%20representation%2C%20Odin%20fundamentally%20avoids%20over-smoothing%20and%20decouples%20structural%20abstraction%20from%20neighborhood%20size%20or%20graph%20topology.%20We%20further%20establish%20that%20Odin%27s%20expressive%20power%20strictly%20contains%20that%20of%20both%20pure%20Transformers%20and%20GNNs.To%20make%20the%20design%20efficient%20in%20large-scale%20or%20low-resource%20settings%2C%20we%20introduce%20Light%20Odin%2C%20a%20lightweight%20variant%20that%20preserves%20the%20same%20layer-aligned%20structural%20abstraction%20for%20faster%20training%20and%20inference.%20Experiments%20on%20multiple%20text-rich%20graph%20benchmarks%20show%20that%20Odin%20achieves%20state-of-the-art%20accuracy%2C%20while%20Light%20Odin%20delivers%20competitive%20performance%20with%20significantly%20reduced%20computational%20cost.%20Together%2C%20Odin%20and%20Light%20Odin%20form%20a%20unified%2C%20hop-free%20framework%20for%20principled%20structure-text%20integration.%20The%20source%20code%20of%20this%20model%20has%20been%20released%20at%20https%3A//github.com/hongkaifeng/Odin.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOdin%253A%2520Oriented%2520Dual-module%2520Integration%2520for%2520Text-rich%2520Network%2520Representation%2520Learning%26entry.906535625%3DKaifeng%2520Hong%2520and%2520Yinglong%2520Zhang%2520and%2520Xiaoying%2520Hong%2520and%2520Xuewen%2520Xia%2520and%2520Xing%2520Xu%26entry.1292438233%3DText-attributed%2520graphs%2520require%2520models%2520to%2520effectively%2520combine%2520strong%2520textual%2520understanding%2520with%2520structurally%2520informed%2520reasoning.%2520Existing%2520approaches%2520either%2520rely%2520on%2520GNNs--limited%2520by%2520over-smoothing%2520and%2520hop-dependent%2520diffusion--or%2520employ%2520Transformers%2520that%2520overlook%2520graph%2520topology%2520and%2520treat%2520nodes%2520as%2520isolated%2520sequences.%2520We%2520propose%2520Odin%2520%2528Oriented%2520Dual-module%2520INtegration%2529%252C%2520a%2520new%2520architecture%2520that%2520injects%2520graph%2520structure%2520into%2520Transformers%2520at%2520selected%2520depths%2520through%2520an%2520oriented%2520dual-module%2520mechanism.Unlike%2520message-passing%2520GNNs%252C%2520Odin%2520does%2520not%2520rely%2520on%2520multi-hop%2520diffusion%253B%2520instead%252C%2520multi-hop%2520structures%2520are%2520integrated%2520at%2520specific%2520Transformer%2520layers%252C%2520yielding%2520low-%252C%2520mid-%252C%2520and%2520high-level%2520structural%2520abstraction%2520aligned%2520with%2520the%2520model%2527s%2520semantic%2520hierarchy.%2520Because%2520aggregation%2520operates%2520on%2520the%2520global%2520%255BCLS%255D%2520representation%252C%2520Odin%2520fundamentally%2520avoids%2520over-smoothing%2520and%2520decouples%2520structural%2520abstraction%2520from%2520neighborhood%2520size%2520or%2520graph%2520topology.%2520We%2520further%2520establish%2520that%2520Odin%2527s%2520expressive%2520power%2520strictly%2520contains%2520that%2520of%2520both%2520pure%2520Transformers%2520and%2520GNNs.To%2520make%2520the%2520design%2520efficient%2520in%2520large-scale%2520or%2520low-resource%2520settings%252C%2520we%2520introduce%2520Light%2520Odin%252C%2520a%2520lightweight%2520variant%2520that%2520preserves%2520the%2520same%2520layer-aligned%2520structural%2520abstraction%2520for%2520faster%2520training%2520and%2520inference.%2520Experiments%2520on%2520multiple%2520text-rich%2520graph%2520benchmarks%2520show%2520that%2520Odin%2520achieves%2520state-of-the-art%2520accuracy%252C%2520while%2520Light%2520Odin%2520delivers%2520competitive%2520performance%2520with%2520significantly%2520reduced%2520computational%2520cost.%2520Together%252C%2520Odin%2520and%2520Light%2520Odin%2520form%2520a%2520unified%252C%2520hop-free%2520framework%2520for%2520principled%2520structure-text%2520integration.%2520The%2520source%2520code%2520of%2520this%2520model%2520has%2520been%2520released%2520at%2520https%253A//github.com/hongkaifeng/Odin.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Odin%3A%20Oriented%20Dual-module%20Integration%20for%20Text-rich%20Network%20Representation%20Learning&entry.906535625=Kaifeng%20Hong%20and%20Yinglong%20Zhang%20and%20Xiaoying%20Hong%20and%20Xuewen%20Xia%20and%20Xing%20Xu&entry.1292438233=Text-attributed%20graphs%20require%20models%20to%20effectively%20combine%20strong%20textual%20understanding%20with%20structurally%20informed%20reasoning.%20Existing%20approaches%20either%20rely%20on%20GNNs--limited%20by%20over-smoothing%20and%20hop-dependent%20diffusion--or%20employ%20Transformers%20that%20overlook%20graph%20topology%20and%20treat%20nodes%20as%20isolated%20sequences.%20We%20propose%20Odin%20%28Oriented%20Dual-module%20INtegration%29%2C%20a%20new%20architecture%20that%20injects%20graph%20structure%20into%20Transformers%20at%20selected%20depths%20through%20an%20oriented%20dual-module%20mechanism.Unlike%20message-passing%20GNNs%2C%20Odin%20does%20not%20rely%20on%20multi-hop%20diffusion%3B%20instead%2C%20multi-hop%20structures%20are%20integrated%20at%20specific%20Transformer%20layers%2C%20yielding%20low-%2C%20mid-%2C%20and%20high-level%20structural%20abstraction%20aligned%20with%20the%20model%27s%20semantic%20hierarchy.%20Because%20aggregation%20operates%20on%20the%20global%20%5BCLS%5D%20representation%2C%20Odin%20fundamentally%20avoids%20over-smoothing%20and%20decouples%20structural%20abstraction%20from%20neighborhood%20size%20or%20graph%20topology.%20We%20further%20establish%20that%20Odin%27s%20expressive%20power%20strictly%20contains%20that%20of%20both%20pure%20Transformers%20and%20GNNs.To%20make%20the%20design%20efficient%20in%20large-scale%20or%20low-resource%20settings%2C%20we%20introduce%20Light%20Odin%2C%20a%20lightweight%20variant%20that%20preserves%20the%20same%20layer-aligned%20structural%20abstraction%20for%20faster%20training%20and%20inference.%20Experiments%20on%20multiple%20text-rich%20graph%20benchmarks%20show%20that%20Odin%20achieves%20state-of-the-art%20accuracy%2C%20while%20Light%20Odin%20delivers%20competitive%20performance%20with%20significantly%20reduced%20computational%20cost.%20Together%2C%20Odin%20and%20Light%20Odin%20form%20a%20unified%2C%20hop-free%20framework%20for%20principled%20structure-text%20integration.%20The%20source%20code%20of%20this%20model%20has%20been%20released%20at%20https%3A//github.com/hongkaifeng/Odin.&entry.1838667208=http%3A//arxiv.org/abs/2511.21416v1&entry.124074799=Read"},
{"title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "author": "Weihao Bo and Shan Zhang and Yanpeng Sun and Jingjing Wu and Qunyi Xie and Xiao Tan and Kunbin Chen and Wei He and Xiaofan Li and Na Zhao and Jingdong Wang and Zechao Li", "abstract": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "link": "http://arxiv.org/abs/2511.21678v1", "date": "2025-11-26", "relevancy": 2.1756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Learner%20with%20Grow-and-Refine%20Multimodal%20Semantic%20Memory&body=Title%3A%20Agentic%20Learner%20with%20Grow-and-Refine%20Multimodal%20Semantic%20Memory%0AAuthor%3A%20Weihao%20Bo%20and%20Shan%20Zhang%20and%20Yanpeng%20Sun%20and%20Jingjing%20Wu%20and%20Qunyi%20Xie%20and%20Xiao%20Tan%20and%20Kunbin%20Chen%20and%20Wei%20He%20and%20Xiaofan%20Li%20and%20Na%20Zhao%20and%20Jingdong%20Wang%20and%20Zechao%20Li%0AAbstract%3A%20MLLMs%20exhibit%20strong%20reasoning%20on%20isolated%20queries%2C%20yet%20they%20operate%20de%20novo%20--%20solving%20each%20problem%20independently%20and%20often%20repeating%20the%20same%20mistakes.%20Existing%20memory-augmented%20agents%20mainly%20store%20past%20trajectories%20for%20reuse.%20However%2C%20trajectory-based%20memory%20suffers%20from%20brevity%20bias%2C%20gradually%20losing%20essential%20domain%20knowledge.%20More%20critically%2C%20even%20in%20truly%20multimodal%20problem-solving%20settings%2C%20it%20records%20only%20a%20single-modality%20trace%20of%20past%20behavior%2C%20failing%20to%20preserve%20how%20visual%20attention%20and%20logical%20reasoning%20jointly%20contributed%20to%20the%20solution.%20This%20is%20fundamentally%20misaligned%20with%20human%20cognition%3A%20semantic%20memory%20is%20both%20multimodal%20and%20integrated%2C%20preserving%20visual%20and%20abstract%20knowledge%20through%20coordinated%20but%20distinct%20representational%20streams.%20We%20thus%20introduce%20ViLoMem%2C%20a%20dual-stream%20memory%20framework%20that%20constructs%20compact%2C%20schema-based%20memory.%20It%20separately%20encodes%20visual%20distraction%20patterns%20and%20logical%20reasoning%20errors%2C%20enabling%20MLLMs%20to%20learn%20from%20their%20successful%20and%20failed%20experiences.%20Following%20a%20grow-and-refine%20principle%2C%20the%20system%20incrementally%20accumulates%20and%20updates%20multimodal%20semantic%20knowledge%20--%20preserving%20stable%2C%20generalizable%20strategies%20while%20avoiding%20catastrophic%20forgetting.%20Across%20six%20multimodal%20benchmarks%2C%20ViLoMem%20consistently%20improves%20pass%401%20accuracy%20and%20substantially%20reduces%20repeated%20visual%20and%20logical%20errors.%20Ablations%20confirm%20the%20necessity%20of%20dual-stream%20memory%20with%20explicit%20distraction--hallucination%20separation%2C%20demonstrating%20the%20value%20of%20error-aware%20multimodal%20memory%20for%20lifelong%20and%20cross-domain%20agentic%20learning.%20Our%20project%20page%20will%20be%20available%20at%20https%3A//weihao-bo.github.io/ViLoMeo-page.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Learner%2520with%2520Grow-and-Refine%2520Multimodal%2520Semantic%2520Memory%26entry.906535625%3DWeihao%2520Bo%2520and%2520Shan%2520Zhang%2520and%2520Yanpeng%2520Sun%2520and%2520Jingjing%2520Wu%2520and%2520Qunyi%2520Xie%2520and%2520Xiao%2520Tan%2520and%2520Kunbin%2520Chen%2520and%2520Wei%2520He%2520and%2520Xiaofan%2520Li%2520and%2520Na%2520Zhao%2520and%2520Jingdong%2520Wang%2520and%2520Zechao%2520Li%26entry.1292438233%3DMLLMs%2520exhibit%2520strong%2520reasoning%2520on%2520isolated%2520queries%252C%2520yet%2520they%2520operate%2520de%2520novo%2520--%2520solving%2520each%2520problem%2520independently%2520and%2520often%2520repeating%2520the%2520same%2520mistakes.%2520Existing%2520memory-augmented%2520agents%2520mainly%2520store%2520past%2520trajectories%2520for%2520reuse.%2520However%252C%2520trajectory-based%2520memory%2520suffers%2520from%2520brevity%2520bias%252C%2520gradually%2520losing%2520essential%2520domain%2520knowledge.%2520More%2520critically%252C%2520even%2520in%2520truly%2520multimodal%2520problem-solving%2520settings%252C%2520it%2520records%2520only%2520a%2520single-modality%2520trace%2520of%2520past%2520behavior%252C%2520failing%2520to%2520preserve%2520how%2520visual%2520attention%2520and%2520logical%2520reasoning%2520jointly%2520contributed%2520to%2520the%2520solution.%2520This%2520is%2520fundamentally%2520misaligned%2520with%2520human%2520cognition%253A%2520semantic%2520memory%2520is%2520both%2520multimodal%2520and%2520integrated%252C%2520preserving%2520visual%2520and%2520abstract%2520knowledge%2520through%2520coordinated%2520but%2520distinct%2520representational%2520streams.%2520We%2520thus%2520introduce%2520ViLoMem%252C%2520a%2520dual-stream%2520memory%2520framework%2520that%2520constructs%2520compact%252C%2520schema-based%2520memory.%2520It%2520separately%2520encodes%2520visual%2520distraction%2520patterns%2520and%2520logical%2520reasoning%2520errors%252C%2520enabling%2520MLLMs%2520to%2520learn%2520from%2520their%2520successful%2520and%2520failed%2520experiences.%2520Following%2520a%2520grow-and-refine%2520principle%252C%2520the%2520system%2520incrementally%2520accumulates%2520and%2520updates%2520multimodal%2520semantic%2520knowledge%2520--%2520preserving%2520stable%252C%2520generalizable%2520strategies%2520while%2520avoiding%2520catastrophic%2520forgetting.%2520Across%2520six%2520multimodal%2520benchmarks%252C%2520ViLoMem%2520consistently%2520improves%2520pass%25401%2520accuracy%2520and%2520substantially%2520reduces%2520repeated%2520visual%2520and%2520logical%2520errors.%2520Ablations%2520confirm%2520the%2520necessity%2520of%2520dual-stream%2520memory%2520with%2520explicit%2520distraction--hallucination%2520separation%252C%2520demonstrating%2520the%2520value%2520of%2520error-aware%2520multimodal%2520memory%2520for%2520lifelong%2520and%2520cross-domain%2520agentic%2520learning.%2520Our%2520project%2520page%2520will%2520be%2520available%2520at%2520https%253A//weihao-bo.github.io/ViLoMeo-page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Learner%20with%20Grow-and-Refine%20Multimodal%20Semantic%20Memory&entry.906535625=Weihao%20Bo%20and%20Shan%20Zhang%20and%20Yanpeng%20Sun%20and%20Jingjing%20Wu%20and%20Qunyi%20Xie%20and%20Xiao%20Tan%20and%20Kunbin%20Chen%20and%20Wei%20He%20and%20Xiaofan%20Li%20and%20Na%20Zhao%20and%20Jingdong%20Wang%20and%20Zechao%20Li&entry.1292438233=MLLMs%20exhibit%20strong%20reasoning%20on%20isolated%20queries%2C%20yet%20they%20operate%20de%20novo%20--%20solving%20each%20problem%20independently%20and%20often%20repeating%20the%20same%20mistakes.%20Existing%20memory-augmented%20agents%20mainly%20store%20past%20trajectories%20for%20reuse.%20However%2C%20trajectory-based%20memory%20suffers%20from%20brevity%20bias%2C%20gradually%20losing%20essential%20domain%20knowledge.%20More%20critically%2C%20even%20in%20truly%20multimodal%20problem-solving%20settings%2C%20it%20records%20only%20a%20single-modality%20trace%20of%20past%20behavior%2C%20failing%20to%20preserve%20how%20visual%20attention%20and%20logical%20reasoning%20jointly%20contributed%20to%20the%20solution.%20This%20is%20fundamentally%20misaligned%20with%20human%20cognition%3A%20semantic%20memory%20is%20both%20multimodal%20and%20integrated%2C%20preserving%20visual%20and%20abstract%20knowledge%20through%20coordinated%20but%20distinct%20representational%20streams.%20We%20thus%20introduce%20ViLoMem%2C%20a%20dual-stream%20memory%20framework%20that%20constructs%20compact%2C%20schema-based%20memory.%20It%20separately%20encodes%20visual%20distraction%20patterns%20and%20logical%20reasoning%20errors%2C%20enabling%20MLLMs%20to%20learn%20from%20their%20successful%20and%20failed%20experiences.%20Following%20a%20grow-and-refine%20principle%2C%20the%20system%20incrementally%20accumulates%20and%20updates%20multimodal%20semantic%20knowledge%20--%20preserving%20stable%2C%20generalizable%20strategies%20while%20avoiding%20catastrophic%20forgetting.%20Across%20six%20multimodal%20benchmarks%2C%20ViLoMem%20consistently%20improves%20pass%401%20accuracy%20and%20substantially%20reduces%20repeated%20visual%20and%20logical%20errors.%20Ablations%20confirm%20the%20necessity%20of%20dual-stream%20memory%20with%20explicit%20distraction--hallucination%20separation%2C%20demonstrating%20the%20value%20of%20error-aware%20multimodal%20memory%20for%20lifelong%20and%20cross-domain%20agentic%20learning.%20Our%20project%20page%20will%20be%20available%20at%20https%3A//weihao-bo.github.io/ViLoMeo-page.&entry.1838667208=http%3A//arxiv.org/abs/2511.21678v1&entry.124074799=Read"},
{"title": "BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations", "author": "Simone Giovannini and Fabio Coppini and Andrea Gemelli and Simone Marinai", "abstract": "We present a unified dataset for document Question-Answering (QA), which is obtained combining several public datasets related to Document AI and visually rich document understanding (VRDU). Our main contribution is twofold: on the one hand we reformulate existing Document AI tasks, such as Information Extraction (IE), into a Question-Answering task, making it a suitable resource for training and evaluating Large Language Models; on the other hand, we release the OCR of all the documents and include the exact position of the answer to be found in the document image as a bounding box. Using this dataset, we explore the impact of different prompting techniques (that might include bounding box information) on the performance of open-weight models, identifying the most effective approaches for document comprehension.", "link": "http://arxiv.org/abs/2501.03403v2", "date": "2025-11-26", "relevancy": 2.1756, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoundingDocs%3A%20a%20Unified%20Dataset%20for%20Document%20Question%20Answering%20with%20Spatial%20Annotations&body=Title%3A%20BoundingDocs%3A%20a%20Unified%20Dataset%20for%20Document%20Question%20Answering%20with%20Spatial%20Annotations%0AAuthor%3A%20Simone%20Giovannini%20and%20Fabio%20Coppini%20and%20Andrea%20Gemelli%20and%20Simone%20Marinai%0AAbstract%3A%20We%20present%20a%20unified%20dataset%20for%20document%20Question-Answering%20%28QA%29%2C%20which%20is%20obtained%20combining%20several%20public%20datasets%20related%20to%20Document%20AI%20and%20visually%20rich%20document%20understanding%20%28VRDU%29.%20Our%20main%20contribution%20is%20twofold%3A%20on%20the%20one%20hand%20we%20reformulate%20existing%20Document%20AI%20tasks%2C%20such%20as%20Information%20Extraction%20%28IE%29%2C%20into%20a%20Question-Answering%20task%2C%20making%20it%20a%20suitable%20resource%20for%20training%20and%20evaluating%20Large%20Language%20Models%3B%20on%20the%20other%20hand%2C%20we%20release%20the%20OCR%20of%20all%20the%20documents%20and%20include%20the%20exact%20position%20of%20the%20answer%20to%20be%20found%20in%20the%20document%20image%20as%20a%20bounding%20box.%20Using%20this%20dataset%2C%20we%20explore%20the%20impact%20of%20different%20prompting%20techniques%20%28that%20might%20include%20bounding%20box%20information%29%20on%20the%20performance%20of%20open-weight%20models%2C%20identifying%20the%20most%20effective%20approaches%20for%20document%20comprehension.%0ALink%3A%20http%3A//arxiv.org/abs/2501.03403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundingDocs%253A%2520a%2520Unified%2520Dataset%2520for%2520Document%2520Question%2520Answering%2520with%2520Spatial%2520Annotations%26entry.906535625%3DSimone%2520Giovannini%2520and%2520Fabio%2520Coppini%2520and%2520Andrea%2520Gemelli%2520and%2520Simone%2520Marinai%26entry.1292438233%3DWe%2520present%2520a%2520unified%2520dataset%2520for%2520document%2520Question-Answering%2520%2528QA%2529%252C%2520which%2520is%2520obtained%2520combining%2520several%2520public%2520datasets%2520related%2520to%2520Document%2520AI%2520and%2520visually%2520rich%2520document%2520understanding%2520%2528VRDU%2529.%2520Our%2520main%2520contribution%2520is%2520twofold%253A%2520on%2520the%2520one%2520hand%2520we%2520reformulate%2520existing%2520Document%2520AI%2520tasks%252C%2520such%2520as%2520Information%2520Extraction%2520%2528IE%2529%252C%2520into%2520a%2520Question-Answering%2520task%252C%2520making%2520it%2520a%2520suitable%2520resource%2520for%2520training%2520and%2520evaluating%2520Large%2520Language%2520Models%253B%2520on%2520the%2520other%2520hand%252C%2520we%2520release%2520the%2520OCR%2520of%2520all%2520the%2520documents%2520and%2520include%2520the%2520exact%2520position%2520of%2520the%2520answer%2520to%2520be%2520found%2520in%2520the%2520document%2520image%2520as%2520a%2520bounding%2520box.%2520Using%2520this%2520dataset%252C%2520we%2520explore%2520the%2520impact%2520of%2520different%2520prompting%2520techniques%2520%2528that%2520might%2520include%2520bounding%2520box%2520information%2529%2520on%2520the%2520performance%2520of%2520open-weight%2520models%252C%2520identifying%2520the%2520most%2520effective%2520approaches%2520for%2520document%2520comprehension.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoundingDocs%3A%20a%20Unified%20Dataset%20for%20Document%20Question%20Answering%20with%20Spatial%20Annotations&entry.906535625=Simone%20Giovannini%20and%20Fabio%20Coppini%20and%20Andrea%20Gemelli%20and%20Simone%20Marinai&entry.1292438233=We%20present%20a%20unified%20dataset%20for%20document%20Question-Answering%20%28QA%29%2C%20which%20is%20obtained%20combining%20several%20public%20datasets%20related%20to%20Document%20AI%20and%20visually%20rich%20document%20understanding%20%28VRDU%29.%20Our%20main%20contribution%20is%20twofold%3A%20on%20the%20one%20hand%20we%20reformulate%20existing%20Document%20AI%20tasks%2C%20such%20as%20Information%20Extraction%20%28IE%29%2C%20into%20a%20Question-Answering%20task%2C%20making%20it%20a%20suitable%20resource%20for%20training%20and%20evaluating%20Large%20Language%20Models%3B%20on%20the%20other%20hand%2C%20we%20release%20the%20OCR%20of%20all%20the%20documents%20and%20include%20the%20exact%20position%20of%20the%20answer%20to%20be%20found%20in%20the%20document%20image%20as%20a%20bounding%20box.%20Using%20this%20dataset%2C%20we%20explore%20the%20impact%20of%20different%20prompting%20techniques%20%28that%20might%20include%20bounding%20box%20information%29%20on%20the%20performance%20of%20open-weight%20models%2C%20identifying%20the%20most%20effective%20approaches%20for%20document%20comprehension.&entry.1838667208=http%3A//arxiv.org/abs/2501.03403v2&entry.124074799=Read"},
{"title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction", "author": "Alex Ning and Vainateya Rangaraju", "abstract": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.", "link": "http://arxiv.org/abs/2511.21594v1", "date": "2025-11-26", "relevancy": 2.1753, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20LLM%20Latent%20Space%20Geometry%20Through%20Dimensionality%20Reduction&body=Title%3A%20Visualizing%20LLM%20Latent%20Space%20Geometry%20Through%20Dimensionality%20Reduction%0AAuthor%3A%20Alex%20Ning%20and%20Vainateya%20Rangaraju%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20achieve%20state-of-the-art%20results%20across%20many%20natural%20language%20tasks%2C%20but%20their%20internal%20mechanisms%20remain%20difficult%20to%20interpret.%20In%20this%20work%2C%20we%20extract%2C%20process%2C%20and%20visualize%20latent%20state%20geometries%20in%20Transformer-based%20language%20models%20through%20dimensionality%20reduction.%20We%20capture%20layerwise%20activations%20at%20multiple%20points%20within%20Transformer%20blocks%20and%20enable%20systematic%20analysis%20through%20Principal%20Component%20Analysis%20%28PCA%29%20and%20Uniform%20Manifold%20Approximation%20%28UMAP%29.%20We%20demonstrate%20experiments%20on%20GPT-2%20and%20LLaMa%20models%2C%20where%20we%20uncover%20interesting%20geometric%20patterns%20in%20latent%20space.%20Notably%2C%20we%20identify%20a%20clear%20separation%20between%20attention%20and%20MLP%20component%20outputs%20across%20intermediate%20layers%2C%20a%20pattern%20not%20documented%20in%20prior%20work%20to%20our%20knowledge.%20We%20also%20characterize%20the%20high%20norm%20of%20latent%20states%20at%20the%20initial%20sequence%20position%20and%20visualize%20the%20layerwise%20evolution%20of%20latent%20states.%20Additionally%2C%20we%20demonstrate%20the%20high-dimensional%20helical%20structure%20of%20GPT-2%27s%20positional%20embeddings%2C%20the%20sequence-wise%20geometric%20patterns%20in%20LLaMa%2C%20and%20experiment%20with%20repeating%20token%20sequences.%20We%20aim%20to%20support%20systematic%20analysis%20of%20Transformer%20internals%20with%20the%20goal%20of%20enabling%20further%20reproducible%20interpretability%20research.%20We%20make%20our%20code%20available%20at%20https%3A//github.com/Vainateya/Feature_Geometry_Visualization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520LLM%2520Latent%2520Space%2520Geometry%2520Through%2520Dimensionality%2520Reduction%26entry.906535625%3DAlex%2520Ning%2520and%2520Vainateya%2520Rangaraju%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520state-of-the-art%2520results%2520across%2520many%2520natural%2520language%2520tasks%252C%2520but%2520their%2520internal%2520mechanisms%2520remain%2520difficult%2520to%2520interpret.%2520In%2520this%2520work%252C%2520we%2520extract%252C%2520process%252C%2520and%2520visualize%2520latent%2520state%2520geometries%2520in%2520Transformer-based%2520language%2520models%2520through%2520dimensionality%2520reduction.%2520We%2520capture%2520layerwise%2520activations%2520at%2520multiple%2520points%2520within%2520Transformer%2520blocks%2520and%2520enable%2520systematic%2520analysis%2520through%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520and%2520Uniform%2520Manifold%2520Approximation%2520%2528UMAP%2529.%2520We%2520demonstrate%2520experiments%2520on%2520GPT-2%2520and%2520LLaMa%2520models%252C%2520where%2520we%2520uncover%2520interesting%2520geometric%2520patterns%2520in%2520latent%2520space.%2520Notably%252C%2520we%2520identify%2520a%2520clear%2520separation%2520between%2520attention%2520and%2520MLP%2520component%2520outputs%2520across%2520intermediate%2520layers%252C%2520a%2520pattern%2520not%2520documented%2520in%2520prior%2520work%2520to%2520our%2520knowledge.%2520We%2520also%2520characterize%2520the%2520high%2520norm%2520of%2520latent%2520states%2520at%2520the%2520initial%2520sequence%2520position%2520and%2520visualize%2520the%2520layerwise%2520evolution%2520of%2520latent%2520states.%2520Additionally%252C%2520we%2520demonstrate%2520the%2520high-dimensional%2520helical%2520structure%2520of%2520GPT-2%2527s%2520positional%2520embeddings%252C%2520the%2520sequence-wise%2520geometric%2520patterns%2520in%2520LLaMa%252C%2520and%2520experiment%2520with%2520repeating%2520token%2520sequences.%2520We%2520aim%2520to%2520support%2520systematic%2520analysis%2520of%2520Transformer%2520internals%2520with%2520the%2520goal%2520of%2520enabling%2520further%2520reproducible%2520interpretability%2520research.%2520We%2520make%2520our%2520code%2520available%2520at%2520https%253A//github.com/Vainateya/Feature_Geometry_Visualization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20LLM%20Latent%20Space%20Geometry%20Through%20Dimensionality%20Reduction&entry.906535625=Alex%20Ning%20and%20Vainateya%20Rangaraju&entry.1292438233=Large%20language%20models%20%28LLMs%29%20achieve%20state-of-the-art%20results%20across%20many%20natural%20language%20tasks%2C%20but%20their%20internal%20mechanisms%20remain%20difficult%20to%20interpret.%20In%20this%20work%2C%20we%20extract%2C%20process%2C%20and%20visualize%20latent%20state%20geometries%20in%20Transformer-based%20language%20models%20through%20dimensionality%20reduction.%20We%20capture%20layerwise%20activations%20at%20multiple%20points%20within%20Transformer%20blocks%20and%20enable%20systematic%20analysis%20through%20Principal%20Component%20Analysis%20%28PCA%29%20and%20Uniform%20Manifold%20Approximation%20%28UMAP%29.%20We%20demonstrate%20experiments%20on%20GPT-2%20and%20LLaMa%20models%2C%20where%20we%20uncover%20interesting%20geometric%20patterns%20in%20latent%20space.%20Notably%2C%20we%20identify%20a%20clear%20separation%20between%20attention%20and%20MLP%20component%20outputs%20across%20intermediate%20layers%2C%20a%20pattern%20not%20documented%20in%20prior%20work%20to%20our%20knowledge.%20We%20also%20characterize%20the%20high%20norm%20of%20latent%20states%20at%20the%20initial%20sequence%20position%20and%20visualize%20the%20layerwise%20evolution%20of%20latent%20states.%20Additionally%2C%20we%20demonstrate%20the%20high-dimensional%20helical%20structure%20of%20GPT-2%27s%20positional%20embeddings%2C%20the%20sequence-wise%20geometric%20patterns%20in%20LLaMa%2C%20and%20experiment%20with%20repeating%20token%20sequences.%20We%20aim%20to%20support%20systematic%20analysis%20of%20Transformer%20internals%20with%20the%20goal%20of%20enabling%20further%20reproducible%20interpretability%20research.%20We%20make%20our%20code%20available%20at%20https%3A//github.com/Vainateya/Feature_Geometry_Visualization.&entry.1838667208=http%3A//arxiv.org/abs/2511.21594v1&entry.124074799=Read"},
{"title": "Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning", "author": "Xin Gu and Haoji Zhang and Qihang Fan and Jingxuan Niu and Zhipeng Zhang and Libo Zhang and Guang Chen and Fan Chen and Longyin Wen and Sijie Zhu", "abstract": "Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\\% m\\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.", "link": "http://arxiv.org/abs/2511.21375v1", "date": "2025-11-26", "relevancy": 2.1752, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5458}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5433}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20With%20Bounding%20Boxes%3A%20Enhancing%20Spatio-Temporal%20Video%20Grounding%20via%20Reinforcement%20Fine-Tuning&body=Title%3A%20Thinking%20With%20Bounding%20Boxes%3A%20Enhancing%20Spatio-Temporal%20Video%20Grounding%20via%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Xin%20Gu%20and%20Haoji%20Zhang%20and%20Qihang%20Fan%20and%20Jingxuan%20Niu%20and%20Zhipeng%20Zhang%20and%20Libo%20Zhang%20and%20Guang%20Chen%20and%20Fan%20Chen%20and%20Longyin%20Wen%20and%20Sijie%20Zhu%0AAbstract%3A%20Spatio-temporal%20video%20grounding%20%28STVG%29%20requires%20localizing%20a%20target%20object%20in%20untrimmed%20videos%20both%20temporally%20and%20spatially%20from%20natural%20language%20descriptions.%20Despite%20their%20strong%20language%20understanding%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20underperform%20on%20STVG%20due%20to%20misaligned%20training%20objectives%20and%20weak%20fine-grained%20region-word%20alignment%20in%20standard%20visual%20encoders.%20To%20address%20this%2C%20we%20propose%20STVG-o1%2C%20the%20first%20framework%20that%20enables%20off-the-shelf%20MLLMs%20to%20achieve%20state-of-the-art%20STVG%20performance%20without%20any%20architectural%20modifications.%20Our%20method%20introduces%20a%20bounding-box%20chain-of-thought%20mechanism%20that%20explicitly%20reasons%20about%20spatio-temporal%20locations%20in%20an%20intermediate%20step%20before%20producing%20the%20final%20prediction.%20We%20further%20design%20a%20multi-dimensional%20reinforcement%20reward%20function%20consisting%20of%20format%2C%20consistency%2C%20temporal%2C%20spatial%2C%20and%20think%20rewards%2C%20which%20provides%20geometry-aware%20supervision%20through%20reinforcement%20fine-tuning.%20Evaluated%20on%20HCSTVG-v1/v2%20and%20VidSTG%2C%20STVG-o1%20sets%20new%20state-of-the-art%20results%20on%20HCSTVG%2C%20outperforming%20the%20best%20task-specific%20method%20by%207.3%5C%25%20m%5C_tIoU%20on%20HCSTVG-v1%2C%20matching%20specialized%20models%20on%20VidSTG%2C%20and%20surpassing%20all%20existing%20MLLM-based%20approaches%20by%20large%20margins.%20It%20also%20demonstrates%20strong%20open-vocabulary%20generalization%20across%20datasets%2C%20establishing%20MLLMs%20as%20viable%20and%20powerful%20backbones%20for%20precise%20spatio-temporal%20grounding.%20Our%20code%20and%20models%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520With%2520Bounding%2520Boxes%253A%2520Enhancing%2520Spatio-Temporal%2520Video%2520Grounding%2520via%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DXin%2520Gu%2520and%2520Haoji%2520Zhang%2520and%2520Qihang%2520Fan%2520and%2520Jingxuan%2520Niu%2520and%2520Zhipeng%2520Zhang%2520and%2520Libo%2520Zhang%2520and%2520Guang%2520Chen%2520and%2520Fan%2520Chen%2520and%2520Longyin%2520Wen%2520and%2520Sijie%2520Zhu%26entry.1292438233%3DSpatio-temporal%2520video%2520grounding%2520%2528STVG%2529%2520requires%2520localizing%2520a%2520target%2520object%2520in%2520untrimmed%2520videos%2520both%2520temporally%2520and%2520spatially%2520from%2520natural%2520language%2520descriptions.%2520Despite%2520their%2520strong%2520language%2520understanding%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520underperform%2520on%2520STVG%2520due%2520to%2520misaligned%2520training%2520objectives%2520and%2520weak%2520fine-grained%2520region-word%2520alignment%2520in%2520standard%2520visual%2520encoders.%2520To%2520address%2520this%252C%2520we%2520propose%2520STVG-o1%252C%2520the%2520first%2520framework%2520that%2520enables%2520off-the-shelf%2520MLLMs%2520to%2520achieve%2520state-of-the-art%2520STVG%2520performance%2520without%2520any%2520architectural%2520modifications.%2520Our%2520method%2520introduces%2520a%2520bounding-box%2520chain-of-thought%2520mechanism%2520that%2520explicitly%2520reasons%2520about%2520spatio-temporal%2520locations%2520in%2520an%2520intermediate%2520step%2520before%2520producing%2520the%2520final%2520prediction.%2520We%2520further%2520design%2520a%2520multi-dimensional%2520reinforcement%2520reward%2520function%2520consisting%2520of%2520format%252C%2520consistency%252C%2520temporal%252C%2520spatial%252C%2520and%2520think%2520rewards%252C%2520which%2520provides%2520geometry-aware%2520supervision%2520through%2520reinforcement%2520fine-tuning.%2520Evaluated%2520on%2520HCSTVG-v1/v2%2520and%2520VidSTG%252C%2520STVG-o1%2520sets%2520new%2520state-of-the-art%2520results%2520on%2520HCSTVG%252C%2520outperforming%2520the%2520best%2520task-specific%2520method%2520by%25207.3%255C%2525%2520m%255C_tIoU%2520on%2520HCSTVG-v1%252C%2520matching%2520specialized%2520models%2520on%2520VidSTG%252C%2520and%2520surpassing%2520all%2520existing%2520MLLM-based%2520approaches%2520by%2520large%2520margins.%2520It%2520also%2520demonstrates%2520strong%2520open-vocabulary%2520generalization%2520across%2520datasets%252C%2520establishing%2520MLLMs%2520as%2520viable%2520and%2520powerful%2520backbones%2520for%2520precise%2520spatio-temporal%2520grounding.%2520Our%2520code%2520and%2520models%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20With%20Bounding%20Boxes%3A%20Enhancing%20Spatio-Temporal%20Video%20Grounding%20via%20Reinforcement%20Fine-Tuning&entry.906535625=Xin%20Gu%20and%20Haoji%20Zhang%20and%20Qihang%20Fan%20and%20Jingxuan%20Niu%20and%20Zhipeng%20Zhang%20and%20Libo%20Zhang%20and%20Guang%20Chen%20and%20Fan%20Chen%20and%20Longyin%20Wen%20and%20Sijie%20Zhu&entry.1292438233=Spatio-temporal%20video%20grounding%20%28STVG%29%20requires%20localizing%20a%20target%20object%20in%20untrimmed%20videos%20both%20temporally%20and%20spatially%20from%20natural%20language%20descriptions.%20Despite%20their%20strong%20language%20understanding%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20underperform%20on%20STVG%20due%20to%20misaligned%20training%20objectives%20and%20weak%20fine-grained%20region-word%20alignment%20in%20standard%20visual%20encoders.%20To%20address%20this%2C%20we%20propose%20STVG-o1%2C%20the%20first%20framework%20that%20enables%20off-the-shelf%20MLLMs%20to%20achieve%20state-of-the-art%20STVG%20performance%20without%20any%20architectural%20modifications.%20Our%20method%20introduces%20a%20bounding-box%20chain-of-thought%20mechanism%20that%20explicitly%20reasons%20about%20spatio-temporal%20locations%20in%20an%20intermediate%20step%20before%20producing%20the%20final%20prediction.%20We%20further%20design%20a%20multi-dimensional%20reinforcement%20reward%20function%20consisting%20of%20format%2C%20consistency%2C%20temporal%2C%20spatial%2C%20and%20think%20rewards%2C%20which%20provides%20geometry-aware%20supervision%20through%20reinforcement%20fine-tuning.%20Evaluated%20on%20HCSTVG-v1/v2%20and%20VidSTG%2C%20STVG-o1%20sets%20new%20state-of-the-art%20results%20on%20HCSTVG%2C%20outperforming%20the%20best%20task-specific%20method%20by%207.3%5C%25%20m%5C_tIoU%20on%20HCSTVG-v1%2C%20matching%20specialized%20models%20on%20VidSTG%2C%20and%20surpassing%20all%20existing%20MLLM-based%20approaches%20by%20large%20margins.%20It%20also%20demonstrates%20strong%20open-vocabulary%20generalization%20across%20datasets%2C%20establishing%20MLLMs%20as%20viable%20and%20powerful%20backbones%20for%20precise%20spatio-temporal%20grounding.%20Our%20code%20and%20models%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.21375v1&entry.124074799=Read"},
{"title": "UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes", "author": "Kang Du and Xue Liao and Junpeng Xia and Chaozheng Guo and Yi Gu and Yirui Guan and Duotun Wang and  ShengHuang and Zeyu Wang", "abstract": "Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.", "link": "http://arxiv.org/abs/2511.21565v1", "date": "2025-11-26", "relevancy": 2.1737, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5475}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAVLight%3A%20A%20Benchmark%20for%20Illumination-Robust%203D%20Reconstruction%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20Scenes&body=Title%3A%20UAVLight%3A%20A%20Benchmark%20for%20Illumination-Robust%203D%20Reconstruction%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20Scenes%0AAuthor%3A%20Kang%20Du%20and%20Xue%20Liao%20and%20Junpeng%20Xia%20and%20Chaozheng%20Guo%20and%20Yi%20Gu%20and%20Yirui%20Guan%20and%20Duotun%20Wang%20and%20%20ShengHuang%20and%20Zeyu%20Wang%0AAbstract%3A%20Illumination%20inconsistency%20is%20a%20fundamental%20challenge%20in%20multi-view%203D%20reconstruction.%20Variations%20in%20sunlight%20direction%2C%20cloud%20cover%2C%20and%20shadows%20break%20the%20constant-lighting%20assumption%20underlying%20both%20classical%20multi-view%20stereo%20%28MVS%29%20and%20structure%20from%20motion%20%28SfM%29%20pipelines%20and%20recent%20neural%20rendering%20methods%2C%20leading%20to%20geometry%20drift%2C%20color%20inconsistency%2C%20and%20shadow%20imprinting.%20This%20issue%20is%20especially%20critical%20in%20UAV-based%20reconstruction%2C%20where%20long%20flight%20durations%20and%20outdoor%20environments%20make%20lighting%20changes%20unavoidable.%20However%2C%20existing%20datasets%20either%20restrict%20capture%20to%20short%20time%20windows%2C%20thus%20lacking%20meaningful%20illumination%20diversity%2C%20or%20span%20months%20and%20seasons%2C%20where%20geometric%20and%20semantic%20changes%20confound%20the%20isolated%20study%20of%20lighting%20robustness.%20We%20introduce%20UAVLight%2C%20a%20controlled-yet-real%20benchmark%20for%20illumination-robust%203D%20reconstruction.%20Each%20scene%20is%20captured%20along%20repeatable%2C%20geo-referenced%20flight%20paths%20at%20multiple%20fixed%20times%20of%20day%2C%20producing%20natural%20lighting%20variation%20under%20consistent%20geometry%2C%20calibration%2C%20and%20viewpoints.%20With%20standardized%20evaluation%20protocols%20across%20lighting%20conditions%2C%20UAVLight%20provides%20a%20reliable%20foundation%20for%20developing%20and%20benchmarking%20reconstruction%20methods%20that%20are%20consistent%2C%20faithful%2C%20and%20relightable%20in%20real%20outdoor%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAVLight%253A%2520A%2520Benchmark%2520for%2520Illumination-Robust%25203D%2520Reconstruction%2520in%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520Scenes%26entry.906535625%3DKang%2520Du%2520and%2520Xue%2520Liao%2520and%2520Junpeng%2520Xia%2520and%2520Chaozheng%2520Guo%2520and%2520Yi%2520Gu%2520and%2520Yirui%2520Guan%2520and%2520Duotun%2520Wang%2520and%2520%2520ShengHuang%2520and%2520Zeyu%2520Wang%26entry.1292438233%3DIllumination%2520inconsistency%2520is%2520a%2520fundamental%2520challenge%2520in%2520multi-view%25203D%2520reconstruction.%2520Variations%2520in%2520sunlight%2520direction%252C%2520cloud%2520cover%252C%2520and%2520shadows%2520break%2520the%2520constant-lighting%2520assumption%2520underlying%2520both%2520classical%2520multi-view%2520stereo%2520%2528MVS%2529%2520and%2520structure%2520from%2520motion%2520%2528SfM%2529%2520pipelines%2520and%2520recent%2520neural%2520rendering%2520methods%252C%2520leading%2520to%2520geometry%2520drift%252C%2520color%2520inconsistency%252C%2520and%2520shadow%2520imprinting.%2520This%2520issue%2520is%2520especially%2520critical%2520in%2520UAV-based%2520reconstruction%252C%2520where%2520long%2520flight%2520durations%2520and%2520outdoor%2520environments%2520make%2520lighting%2520changes%2520unavoidable.%2520However%252C%2520existing%2520datasets%2520either%2520restrict%2520capture%2520to%2520short%2520time%2520windows%252C%2520thus%2520lacking%2520meaningful%2520illumination%2520diversity%252C%2520or%2520span%2520months%2520and%2520seasons%252C%2520where%2520geometric%2520and%2520semantic%2520changes%2520confound%2520the%2520isolated%2520study%2520of%2520lighting%2520robustness.%2520We%2520introduce%2520UAVLight%252C%2520a%2520controlled-yet-real%2520benchmark%2520for%2520illumination-robust%25203D%2520reconstruction.%2520Each%2520scene%2520is%2520captured%2520along%2520repeatable%252C%2520geo-referenced%2520flight%2520paths%2520at%2520multiple%2520fixed%2520times%2520of%2520day%252C%2520producing%2520natural%2520lighting%2520variation%2520under%2520consistent%2520geometry%252C%2520calibration%252C%2520and%2520viewpoints.%2520With%2520standardized%2520evaluation%2520protocols%2520across%2520lighting%2520conditions%252C%2520UAVLight%2520provides%2520a%2520reliable%2520foundation%2520for%2520developing%2520and%2520benchmarking%2520reconstruction%2520methods%2520that%2520are%2520consistent%252C%2520faithful%252C%2520and%2520relightable%2520in%2520real%2520outdoor%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAVLight%3A%20A%20Benchmark%20for%20Illumination-Robust%203D%20Reconstruction%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20Scenes&entry.906535625=Kang%20Du%20and%20Xue%20Liao%20and%20Junpeng%20Xia%20and%20Chaozheng%20Guo%20and%20Yi%20Gu%20and%20Yirui%20Guan%20and%20Duotun%20Wang%20and%20%20ShengHuang%20and%20Zeyu%20Wang&entry.1292438233=Illumination%20inconsistency%20is%20a%20fundamental%20challenge%20in%20multi-view%203D%20reconstruction.%20Variations%20in%20sunlight%20direction%2C%20cloud%20cover%2C%20and%20shadows%20break%20the%20constant-lighting%20assumption%20underlying%20both%20classical%20multi-view%20stereo%20%28MVS%29%20and%20structure%20from%20motion%20%28SfM%29%20pipelines%20and%20recent%20neural%20rendering%20methods%2C%20leading%20to%20geometry%20drift%2C%20color%20inconsistency%2C%20and%20shadow%20imprinting.%20This%20issue%20is%20especially%20critical%20in%20UAV-based%20reconstruction%2C%20where%20long%20flight%20durations%20and%20outdoor%20environments%20make%20lighting%20changes%20unavoidable.%20However%2C%20existing%20datasets%20either%20restrict%20capture%20to%20short%20time%20windows%2C%20thus%20lacking%20meaningful%20illumination%20diversity%2C%20or%20span%20months%20and%20seasons%2C%20where%20geometric%20and%20semantic%20changes%20confound%20the%20isolated%20study%20of%20lighting%20robustness.%20We%20introduce%20UAVLight%2C%20a%20controlled-yet-real%20benchmark%20for%20illumination-robust%203D%20reconstruction.%20Each%20scene%20is%20captured%20along%20repeatable%2C%20geo-referenced%20flight%20paths%20at%20multiple%20fixed%20times%20of%20day%2C%20producing%20natural%20lighting%20variation%20under%20consistent%20geometry%2C%20calibration%2C%20and%20viewpoints.%20With%20standardized%20evaluation%20protocols%20across%20lighting%20conditions%2C%20UAVLight%20provides%20a%20reliable%20foundation%20for%20developing%20and%20benchmarking%20reconstruction%20methods%20that%20are%20consistent%2C%20faithful%2C%20and%20relightable%20in%20real%20outdoor%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.21565v1&entry.124074799=Read"},
{"title": "MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers", "author": "Audrey Pei-Hsuan Chen", "abstract": "Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with Scrublet, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.", "link": "http://arxiv.org/abs/2511.20382v2", "date": "2025-11-26", "relevancy": 2.1688, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5868}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5131}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRE%3A%20Batch-Robust%20Multi-Omics%20Representations%20from%20Frozen%20Pre-trained%20Transformers&body=Title%3A%20MoRE%3A%20Batch-Robust%20Multi-Omics%20Representations%20from%20Frozen%20Pre-trained%20Transformers%0AAuthor%3A%20Audrey%20Pei-Hsuan%20Chen%0AAbstract%3A%20Representation%20learning%20on%20multi-omics%20data%20is%20challenging%20due%20to%20extreme%20dimensionality%2C%20modality%20heterogeneity%2C%20and%20cohort-specific%20batch%20effects.%20While%20pre-trained%20transformer%20backbones%20have%20shown%20broad%20generalization%20capabilities%20in%20biological%20sequence%20modeling%2C%20their%20application%20to%20multi-omics%20integration%20remains%20underexplored.%20We%20present%20MoRE%20%28Multi-Omics%20Representation%20Embedding%29%2C%20a%20framework%20that%20repurposes%20frozen%20pre-trained%20transformers%20to%20align%20heterogeneous%20assays%20into%20a%20shared%20latent%20space.%20Unlike%20purely%20generative%20approaches%2C%20MoRE%20employs%20a%20parameter-efficient%20fine-tuning%20%28PEFT%29%20strategy%2C%20prioritizing%20cross-sample%20and%20cross-modality%20alignment%20over%20simple%20sequence%20reconstruction.%20Specifically%2C%20MoRE%20attaches%20lightweight%2C%20modality-specific%20adapters%20and%20a%20task-adaptive%20fusion%20layer%20to%20the%20frozen%20backbone.%20It%20optimizes%20a%20masked%20modeling%20objective%20jointly%20with%20supervised%20contrastive%20and%20batch-invariant%20alignment%20losses%2C%20yielding%20structure-preserving%20embeddings%20that%20generalize%20across%20unseen%20cell%20types%20and%20platforms.%20We%20benchmark%20MoRE%20against%20established%20baselines%2C%20including%20scGPT%2C%20scVI%2C%20and%20Harmony%20with%20Scrublet%2C%20evaluating%20integration%20fidelity%2C%20rare%20population%20detection%2C%20and%20modality%20transfer.%20Our%20results%20demonstrate%20that%20MoRE%20achieves%20competitive%20batch%20robustness%20and%20biological%20conservation%20while%20significantly%20reducing%20trainable%20parameters%20compared%20to%20fully%20fine-tuned%20models.%20This%20work%20positions%20MoRE%20as%20a%20practical%20step%20toward%20general-purpose%20omics%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRE%253A%2520Batch-Robust%2520Multi-Omics%2520Representations%2520from%2520Frozen%2520Pre-trained%2520Transformers%26entry.906535625%3DAudrey%2520Pei-Hsuan%2520Chen%26entry.1292438233%3DRepresentation%2520learning%2520on%2520multi-omics%2520data%2520is%2520challenging%2520due%2520to%2520extreme%2520dimensionality%252C%2520modality%2520heterogeneity%252C%2520and%2520cohort-specific%2520batch%2520effects.%2520While%2520pre-trained%2520transformer%2520backbones%2520have%2520shown%2520broad%2520generalization%2520capabilities%2520in%2520biological%2520sequence%2520modeling%252C%2520their%2520application%2520to%2520multi-omics%2520integration%2520remains%2520underexplored.%2520We%2520present%2520MoRE%2520%2528Multi-Omics%2520Representation%2520Embedding%2529%252C%2520a%2520framework%2520that%2520repurposes%2520frozen%2520pre-trained%2520transformers%2520to%2520align%2520heterogeneous%2520assays%2520into%2520a%2520shared%2520latent%2520space.%2520Unlike%2520purely%2520generative%2520approaches%252C%2520MoRE%2520employs%2520a%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520strategy%252C%2520prioritizing%2520cross-sample%2520and%2520cross-modality%2520alignment%2520over%2520simple%2520sequence%2520reconstruction.%2520Specifically%252C%2520MoRE%2520attaches%2520lightweight%252C%2520modality-specific%2520adapters%2520and%2520a%2520task-adaptive%2520fusion%2520layer%2520to%2520the%2520frozen%2520backbone.%2520It%2520optimizes%2520a%2520masked%2520modeling%2520objective%2520jointly%2520with%2520supervised%2520contrastive%2520and%2520batch-invariant%2520alignment%2520losses%252C%2520yielding%2520structure-preserving%2520embeddings%2520that%2520generalize%2520across%2520unseen%2520cell%2520types%2520and%2520platforms.%2520We%2520benchmark%2520MoRE%2520against%2520established%2520baselines%252C%2520including%2520scGPT%252C%2520scVI%252C%2520and%2520Harmony%2520with%2520Scrublet%252C%2520evaluating%2520integration%2520fidelity%252C%2520rare%2520population%2520detection%252C%2520and%2520modality%2520transfer.%2520Our%2520results%2520demonstrate%2520that%2520MoRE%2520achieves%2520competitive%2520batch%2520robustness%2520and%2520biological%2520conservation%2520while%2520significantly%2520reducing%2520trainable%2520parameters%2520compared%2520to%2520fully%2520fine-tuned%2520models.%2520This%2520work%2520positions%2520MoRE%2520as%2520a%2520practical%2520step%2520toward%2520general-purpose%2520omics%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRE%3A%20Batch-Robust%20Multi-Omics%20Representations%20from%20Frozen%20Pre-trained%20Transformers&entry.906535625=Audrey%20Pei-Hsuan%20Chen&entry.1292438233=Representation%20learning%20on%20multi-omics%20data%20is%20challenging%20due%20to%20extreme%20dimensionality%2C%20modality%20heterogeneity%2C%20and%20cohort-specific%20batch%20effects.%20While%20pre-trained%20transformer%20backbones%20have%20shown%20broad%20generalization%20capabilities%20in%20biological%20sequence%20modeling%2C%20their%20application%20to%20multi-omics%20integration%20remains%20underexplored.%20We%20present%20MoRE%20%28Multi-Omics%20Representation%20Embedding%29%2C%20a%20framework%20that%20repurposes%20frozen%20pre-trained%20transformers%20to%20align%20heterogeneous%20assays%20into%20a%20shared%20latent%20space.%20Unlike%20purely%20generative%20approaches%2C%20MoRE%20employs%20a%20parameter-efficient%20fine-tuning%20%28PEFT%29%20strategy%2C%20prioritizing%20cross-sample%20and%20cross-modality%20alignment%20over%20simple%20sequence%20reconstruction.%20Specifically%2C%20MoRE%20attaches%20lightweight%2C%20modality-specific%20adapters%20and%20a%20task-adaptive%20fusion%20layer%20to%20the%20frozen%20backbone.%20It%20optimizes%20a%20masked%20modeling%20objective%20jointly%20with%20supervised%20contrastive%20and%20batch-invariant%20alignment%20losses%2C%20yielding%20structure-preserving%20embeddings%20that%20generalize%20across%20unseen%20cell%20types%20and%20platforms.%20We%20benchmark%20MoRE%20against%20established%20baselines%2C%20including%20scGPT%2C%20scVI%2C%20and%20Harmony%20with%20Scrublet%2C%20evaluating%20integration%20fidelity%2C%20rare%20population%20detection%2C%20and%20modality%20transfer.%20Our%20results%20demonstrate%20that%20MoRE%20achieves%20competitive%20batch%20robustness%20and%20biological%20conservation%20while%20significantly%20reducing%20trainable%20parameters%20compared%20to%20fully%20fine-tuned%20models.%20This%20work%20positions%20MoRE%20as%20a%20practical%20step%20toward%20general-purpose%20omics%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.20382v2&entry.124074799=Read"},
{"title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings", "author": "Jiajie Zhang and S\u00f6ren Schwertfeger and Alexander Kleiner", "abstract": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.", "link": "http://arxiv.org/abs/2511.21428v1", "date": "2025-11-26", "relevancy": 2.167, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5589}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Observation%20to%20Action%3A%20Latent%20Action-based%20Primitive%20Segmentation%20for%20VLA%20Pre-training%20in%20Industrial%20Settings&body=Title%3A%20From%20Observation%20to%20Action%3A%20Latent%20Action-based%20Primitive%20Segmentation%20for%20VLA%20Pre-training%20in%20Industrial%20Settings%0AAuthor%3A%20Jiajie%20Zhang%20and%20S%C3%B6ren%20Schwertfeger%20and%20Alexander%20Kleiner%0AAbstract%3A%20We%20present%20a%20novel%20unsupervised%20framework%20to%20unlock%20vast%20unlabeled%20human%20demonstration%20data%20from%20continuous%20industrial%20video%20streams%20for%20Vision-Language-Action%20%28VLA%29%20model%20pre-training.%20Our%20method%20first%20trains%20a%20lightweight%20motion%20tokenizer%20to%20encode%20motion%20dynamics%2C%20then%20employs%20an%20unsupervised%20action%20segmenter%20leveraging%20a%20novel%20%22Latent%20Action%20Energy%22%20metric%20to%20discover%20and%20segment%20semantically%20coherent%20action%20primitives.%20The%20pipeline%20outputs%20both%20segmented%20video%20clips%20and%20their%20corresponding%20latent%20action%20sequences%2C%20providing%20structured%20data%20directly%20suitable%20for%20VLA%20pre-training.%20Evaluations%20on%20public%20benchmarks%20and%20a%20proprietary%20electric%20motor%20assembly%20dataset%20demonstrate%20effective%20segmentation%20of%20key%20tasks%20performed%20by%20humans%20at%20workstations.%20Further%20clustering%20and%20quantitative%20assessment%20via%20a%20Vision-Language%20Model%20confirm%20the%20semantic%20coherence%20of%20the%20discovered%20action%20primitives.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20fully%20automated%20end-to-end%20system%20for%20extracting%20and%20organizing%20VLA%20pre-training%20data%20from%20unstructured%20industrial%20videos%2C%20offering%20a%20scalable%20solution%20for%20embodied%20AI%20integration%20in%20manufacturing.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Observation%2520to%2520Action%253A%2520Latent%2520Action-based%2520Primitive%2520Segmentation%2520for%2520VLA%2520Pre-training%2520in%2520Industrial%2520Settings%26entry.906535625%3DJiajie%2520Zhang%2520and%2520S%25C3%25B6ren%2520Schwertfeger%2520and%2520Alexander%2520Kleiner%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520unsupervised%2520framework%2520to%2520unlock%2520vast%2520unlabeled%2520human%2520demonstration%2520data%2520from%2520continuous%2520industrial%2520video%2520streams%2520for%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520pre-training.%2520Our%2520method%2520first%2520trains%2520a%2520lightweight%2520motion%2520tokenizer%2520to%2520encode%2520motion%2520dynamics%252C%2520then%2520employs%2520an%2520unsupervised%2520action%2520segmenter%2520leveraging%2520a%2520novel%2520%2522Latent%2520Action%2520Energy%2522%2520metric%2520to%2520discover%2520and%2520segment%2520semantically%2520coherent%2520action%2520primitives.%2520The%2520pipeline%2520outputs%2520both%2520segmented%2520video%2520clips%2520and%2520their%2520corresponding%2520latent%2520action%2520sequences%252C%2520providing%2520structured%2520data%2520directly%2520suitable%2520for%2520VLA%2520pre-training.%2520Evaluations%2520on%2520public%2520benchmarks%2520and%2520a%2520proprietary%2520electric%2520motor%2520assembly%2520dataset%2520demonstrate%2520effective%2520segmentation%2520of%2520key%2520tasks%2520performed%2520by%2520humans%2520at%2520workstations.%2520Further%2520clustering%2520and%2520quantitative%2520assessment%2520via%2520a%2520Vision-Language%2520Model%2520confirm%2520the%2520semantic%2520coherence%2520of%2520the%2520discovered%2520action%2520primitives.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520fully%2520automated%2520end-to-end%2520system%2520for%2520extracting%2520and%2520organizing%2520VLA%2520pre-training%2520data%2520from%2520unstructured%2520industrial%2520videos%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520embodied%2520AI%2520integration%2520in%2520manufacturing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Observation%20to%20Action%3A%20Latent%20Action-based%20Primitive%20Segmentation%20for%20VLA%20Pre-training%20in%20Industrial%20Settings&entry.906535625=Jiajie%20Zhang%20and%20S%C3%B6ren%20Schwertfeger%20and%20Alexander%20Kleiner&entry.1292438233=We%20present%20a%20novel%20unsupervised%20framework%20to%20unlock%20vast%20unlabeled%20human%20demonstration%20data%20from%20continuous%20industrial%20video%20streams%20for%20Vision-Language-Action%20%28VLA%29%20model%20pre-training.%20Our%20method%20first%20trains%20a%20lightweight%20motion%20tokenizer%20to%20encode%20motion%20dynamics%2C%20then%20employs%20an%20unsupervised%20action%20segmenter%20leveraging%20a%20novel%20%22Latent%20Action%20Energy%22%20metric%20to%20discover%20and%20segment%20semantically%20coherent%20action%20primitives.%20The%20pipeline%20outputs%20both%20segmented%20video%20clips%20and%20their%20corresponding%20latent%20action%20sequences%2C%20providing%20structured%20data%20directly%20suitable%20for%20VLA%20pre-training.%20Evaluations%20on%20public%20benchmarks%20and%20a%20proprietary%20electric%20motor%20assembly%20dataset%20demonstrate%20effective%20segmentation%20of%20key%20tasks%20performed%20by%20humans%20at%20workstations.%20Further%20clustering%20and%20quantitative%20assessment%20via%20a%20Vision-Language%20Model%20confirm%20the%20semantic%20coherence%20of%20the%20discovered%20action%20primitives.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20fully%20automated%20end-to-end%20system%20for%20extracting%20and%20organizing%20VLA%20pre-training%20data%20from%20unstructured%20industrial%20videos%2C%20offering%20a%20scalable%20solution%20for%20embodied%20AI%20integration%20in%20manufacturing.&entry.1838667208=http%3A//arxiv.org/abs/2511.21428v1&entry.124074799=Read"},
{"title": "Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training", "author": "Alan Mitkiy and James Smith and Myungseo wong and Hana Satou and Hiroshi Tanaka and Emily Johnson", "abstract": "Adversarial training is among the most effective strategies for defending deep neural networks against adversarial examples. A key limitation of existing adversarial training approaches lies in their reliance on a fixed perturbation budget, which fails to account for instance-specific robustness characteristics. While prior works such as IAAT and MMA introduce instance-level adaptations, they often rely on heuristic or static approximations of data robustness. In this paper, we propose Dynamic Epsilon Scheduling (DES), a novel framework that adaptively adjusts the adversarial perturbation budget per instance and per training iteration. DES integrates three key factors: (1) the distance to the decision boundary approximated via gradient-based proxies, (2) prediction confidence derived from softmax entropy, and (3) model uncertainty estimated via Monte Carlo dropout. By combining these cues into a unified scheduling strategy, DES tailors the perturbation budget dynamically to guide more effective adversarial learning. Experimental results on CIFAR-10 and CIFAR-100 show that our method consistently improves both adversarial robustness and standard accuracy compared to fixed-epsilon baselines and prior adaptive methods. Moreover, we provide theoretical insights into the stability and convergence of our scheduling policy. This work opens a new avenue for instance-aware, data-driven adversarial training methods.", "link": "http://arxiv.org/abs/2506.04263v2", "date": "2025-11-26", "relevancy": 2.151, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.589}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5054}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Epsilon%20Scheduling%3A%20A%20Multi-Factor%20Adaptive%20Perturbation%20Budget%20for%20Adversarial%20Training&body=Title%3A%20Dynamic%20Epsilon%20Scheduling%3A%20A%20Multi-Factor%20Adaptive%20Perturbation%20Budget%20for%20Adversarial%20Training%0AAuthor%3A%20Alan%20Mitkiy%20and%20James%20Smith%20and%20Myungseo%20wong%20and%20Hana%20Satou%20and%20Hiroshi%20Tanaka%20and%20Emily%20Johnson%0AAbstract%3A%20Adversarial%20training%20is%20among%20the%20most%20effective%20strategies%20for%20defending%20deep%20neural%20networks%20against%20adversarial%20examples.%20A%20key%20limitation%20of%20existing%20adversarial%20training%20approaches%20lies%20in%20their%20reliance%20on%20a%20fixed%20perturbation%20budget%2C%20which%20fails%20to%20account%20for%20instance-specific%20robustness%20characteristics.%20While%20prior%20works%20such%20as%20IAAT%20and%20MMA%20introduce%20instance-level%20adaptations%2C%20they%20often%20rely%20on%20heuristic%20or%20static%20approximations%20of%20data%20robustness.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Epsilon%20Scheduling%20%28DES%29%2C%20a%20novel%20framework%20that%20adaptively%20adjusts%20the%20adversarial%20perturbation%20budget%20per%20instance%20and%20per%20training%20iteration.%20DES%20integrates%20three%20key%20factors%3A%20%281%29%20the%20distance%20to%20the%20decision%20boundary%20approximated%20via%20gradient-based%20proxies%2C%20%282%29%20prediction%20confidence%20derived%20from%20softmax%20entropy%2C%20and%20%283%29%20model%20uncertainty%20estimated%20via%20Monte%20Carlo%20dropout.%20By%20combining%20these%20cues%20into%20a%20unified%20scheduling%20strategy%2C%20DES%20tailors%20the%20perturbation%20budget%20dynamically%20to%20guide%20more%20effective%20adversarial%20learning.%20Experimental%20results%20on%20CIFAR-10%20and%20CIFAR-100%20show%20that%20our%20method%20consistently%20improves%20both%20adversarial%20robustness%20and%20standard%20accuracy%20compared%20to%20fixed-epsilon%20baselines%20and%20prior%20adaptive%20methods.%20Moreover%2C%20we%20provide%20theoretical%20insights%20into%20the%20stability%20and%20convergence%20of%20our%20scheduling%20policy.%20This%20work%20opens%20a%20new%20avenue%20for%20instance-aware%2C%20data-driven%20adversarial%20training%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2506.04263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Epsilon%2520Scheduling%253A%2520A%2520Multi-Factor%2520Adaptive%2520Perturbation%2520Budget%2520for%2520Adversarial%2520Training%26entry.906535625%3DAlan%2520Mitkiy%2520and%2520James%2520Smith%2520and%2520Myungseo%2520wong%2520and%2520Hana%2520Satou%2520and%2520Hiroshi%2520Tanaka%2520and%2520Emily%2520Johnson%26entry.1292438233%3DAdversarial%2520training%2520is%2520among%2520the%2520most%2520effective%2520strategies%2520for%2520defending%2520deep%2520neural%2520networks%2520against%2520adversarial%2520examples.%2520A%2520key%2520limitation%2520of%2520existing%2520adversarial%2520training%2520approaches%2520lies%2520in%2520their%2520reliance%2520on%2520a%2520fixed%2520perturbation%2520budget%252C%2520which%2520fails%2520to%2520account%2520for%2520instance-specific%2520robustness%2520characteristics.%2520While%2520prior%2520works%2520such%2520as%2520IAAT%2520and%2520MMA%2520introduce%2520instance-level%2520adaptations%252C%2520they%2520often%2520rely%2520on%2520heuristic%2520or%2520static%2520approximations%2520of%2520data%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dynamic%2520Epsilon%2520Scheduling%2520%2528DES%2529%252C%2520a%2520novel%2520framework%2520that%2520adaptively%2520adjusts%2520the%2520adversarial%2520perturbation%2520budget%2520per%2520instance%2520and%2520per%2520training%2520iteration.%2520DES%2520integrates%2520three%2520key%2520factors%253A%2520%25281%2529%2520the%2520distance%2520to%2520the%2520decision%2520boundary%2520approximated%2520via%2520gradient-based%2520proxies%252C%2520%25282%2529%2520prediction%2520confidence%2520derived%2520from%2520softmax%2520entropy%252C%2520and%2520%25283%2529%2520model%2520uncertainty%2520estimated%2520via%2520Monte%2520Carlo%2520dropout.%2520By%2520combining%2520these%2520cues%2520into%2520a%2520unified%2520scheduling%2520strategy%252C%2520DES%2520tailors%2520the%2520perturbation%2520budget%2520dynamically%2520to%2520guide%2520more%2520effective%2520adversarial%2520learning.%2520Experimental%2520results%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520show%2520that%2520our%2520method%2520consistently%2520improves%2520both%2520adversarial%2520robustness%2520and%2520standard%2520accuracy%2520compared%2520to%2520fixed-epsilon%2520baselines%2520and%2520prior%2520adaptive%2520methods.%2520Moreover%252C%2520we%2520provide%2520theoretical%2520insights%2520into%2520the%2520stability%2520and%2520convergence%2520of%2520our%2520scheduling%2520policy.%2520This%2520work%2520opens%2520a%2520new%2520avenue%2520for%2520instance-aware%252C%2520data-driven%2520adversarial%2520training%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Epsilon%20Scheduling%3A%20A%20Multi-Factor%20Adaptive%20Perturbation%20Budget%20for%20Adversarial%20Training&entry.906535625=Alan%20Mitkiy%20and%20James%20Smith%20and%20Myungseo%20wong%20and%20Hana%20Satou%20and%20Hiroshi%20Tanaka%20and%20Emily%20Johnson&entry.1292438233=Adversarial%20training%20is%20among%20the%20most%20effective%20strategies%20for%20defending%20deep%20neural%20networks%20against%20adversarial%20examples.%20A%20key%20limitation%20of%20existing%20adversarial%20training%20approaches%20lies%20in%20their%20reliance%20on%20a%20fixed%20perturbation%20budget%2C%20which%20fails%20to%20account%20for%20instance-specific%20robustness%20characteristics.%20While%20prior%20works%20such%20as%20IAAT%20and%20MMA%20introduce%20instance-level%20adaptations%2C%20they%20often%20rely%20on%20heuristic%20or%20static%20approximations%20of%20data%20robustness.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Epsilon%20Scheduling%20%28DES%29%2C%20a%20novel%20framework%20that%20adaptively%20adjusts%20the%20adversarial%20perturbation%20budget%20per%20instance%20and%20per%20training%20iteration.%20DES%20integrates%20three%20key%20factors%3A%20%281%29%20the%20distance%20to%20the%20decision%20boundary%20approximated%20via%20gradient-based%20proxies%2C%20%282%29%20prediction%20confidence%20derived%20from%20softmax%20entropy%2C%20and%20%283%29%20model%20uncertainty%20estimated%20via%20Monte%20Carlo%20dropout.%20By%20combining%20these%20cues%20into%20a%20unified%20scheduling%20strategy%2C%20DES%20tailors%20the%20perturbation%20budget%20dynamically%20to%20guide%20more%20effective%20adversarial%20learning.%20Experimental%20results%20on%20CIFAR-10%20and%20CIFAR-100%20show%20that%20our%20method%20consistently%20improves%20both%20adversarial%20robustness%20and%20standard%20accuracy%20compared%20to%20fixed-epsilon%20baselines%20and%20prior%20adaptive%20methods.%20Moreover%2C%20we%20provide%20theoretical%20insights%20into%20the%20stability%20and%20convergence%20of%20our%20scheduling%20policy.%20This%20work%20opens%20a%20new%20avenue%20for%20instance-aware%2C%20data-driven%20adversarial%20training%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2506.04263v2&entry.124074799=Read"},
{"title": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators", "author": "Armand Kassa\u00ef Koupa\u00ef and Lise Le Boudec and Louis Serrano and Patrick Gallinari", "abstract": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.", "link": "http://arxiv.org/abs/2506.06158v2", "date": "2025-11-26", "relevancy": 2.1475, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5278}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&body=Title%3A%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators%0AAuthor%3A%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari%0AAbstract%3A%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20remains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%20generalizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%20is%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%20to%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%20to%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%20predicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%20autoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%20generation.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%20latent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%20a%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%20learning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%20trajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%20is%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%20supports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENMA%253A%2520Tokenwise%2520Autoregression%2520for%2520Generative%2520Neural%2520PDE%2520Operators%26entry.906535625%3DArmand%2520Kassa%25C3%25AF%2520Koupa%25C3%25AF%2520and%2520Lise%2520Le%2520Boudec%2520and%2520Louis%2520Serrano%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3DSolving%2520time-dependent%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520remains%2520a%2520fundamental%2520challenge%2520for%2520neural%2520solvers%252C%2520particularly%2520when%2520generalizing%2520across%2520a%2520wide%2520range%2520of%2520physical%2520parameters%2520and%2520dynamics.%2520When%2520data%2520is%2520uncertain%2520or%2520incomplete-as%2520is%2520often%2520the%2520case-a%2520natural%2520approach%2520is%2520to%2520turn%2520to%2520generative%2520models.%2520We%2520introduce%2520ENMA%252C%2520a%2520generative%2520neural%2520operator%2520designed%2520to%2520model%2520spatio-temporal%2520dynamics%2520arising%2520from%2520physical%2520phenomena.%2520ENMA%2520predicts%2520future%2520dynamics%2520in%2520a%2520compressed%2520latent%2520space%2520using%2520a%2520generative%2520masked%2520autoregressive%2520transformer%2520trained%2520with%2520flow%2520matching%2520loss%252C%2520enabling%2520tokenwise%2520generation.%2520Irregularly%2520sampled%2520spatial%2520observations%2520are%2520encoded%2520into%2520uniform%2520latent%2520representations%2520via%2520attention%2520mechanisms%2520and%2520further%2520compressed%2520through%2520a%2520spatio-temporal%2520convolutional%2520encoder.%2520This%2520allows%2520ENMA%2520to%2520perform%2520in-context%2520learning%2520at%2520inference%2520time%2520by%2520conditioning%2520on%2520either%2520past%2520states%2520of%2520the%2520target%2520trajectory%2520or%2520auxiliary%2520context%2520trajectories%2520with%2520similar%2520dynamics.%2520The%2520result%2520is%2520a%2520robust%2520and%2520adaptable%2520framework%2520that%2520generalizes%2520to%2520new%2520PDE%2520regimes%2520and%2520supports%2520one-shot%2520surrogate%2520modeling%2520of%2520time-dependent%2520parametric%2520PDEs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&entry.906535625=Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari&entry.1292438233=Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20remains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%20generalizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%20is%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%20to%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%20to%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%20predicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%20autoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%20generation.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%20latent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%20a%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%20learning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%20trajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%20is%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%20supports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.&entry.1838667208=http%3A//arxiv.org/abs/2506.06158v2&entry.124074799=Read"},
{"title": "ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition", "author": "Mengqi Xue and Qihan Huang and Haofei Zhang and Jingwen Hu and Jie Song and Mingli Song and Canghong Jin", "abstract": "Prototypical part network (ProtoPNet) has drawn wide attention and boosted many follow-up studies due to its self-explanatory property for explainable artificial intelligence (XAI). However, when directly applying ProtoPNet on vision transformer (ViT) backbones, learned prototypes have a \"distraction\" problem: they have a relatively high probability of being activated by the background and pay less attention to the foreground. The powerful capability of modeling long-term dependency makes the transformer-based ProtoPNet hard to focus on prototypical parts, thus severely impairing its inherent interpretability. This paper proposes prototypical part transformer (ProtoPFormer) for appropriately and effectively applying the prototype-based method with ViTs for interpretable image recognition. The proposed method introduces global and local prototypes for capturing and highlighting the representative holistic and partial features of targets according to the architectural characteristics of ViTs. The global prototypes are adopted to provide the global view of objects to guide local prototypes to concentrate on the foreground while eliminating the influence of the background. Afterwards, local prototypes are explicitly supervised to concentrate on their respective prototypical visual parts, increasing the overall interpretability. Extensive experiments demonstrate that our proposed global and local prototypes can mutually correct each other and jointly make final decisions, which faithfully and transparently reason the decision-making processes associatively from the whole and local perspectives, respectively. Moreover, ProtoPFormer consistently achieves superior performance and visualization results over the state-of-the-art (SOTA) prototype-based baselines. Our code has been released at https://github.com/zju-vipa/ProtoPFormer.", "link": "http://arxiv.org/abs/2208.10431v3", "date": "2025-11-26", "relevancy": 2.1473, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoPFormer%3A%20Concentrating%20on%20Prototypical%20Parts%20in%20Vision%20Transformers%20for%20Interpretable%20Image%20Recognition&body=Title%3A%20ProtoPFormer%3A%20Concentrating%20on%20Prototypical%20Parts%20in%20Vision%20Transformers%20for%20Interpretable%20Image%20Recognition%0AAuthor%3A%20Mengqi%20Xue%20and%20Qihan%20Huang%20and%20Haofei%20Zhang%20and%20Jingwen%20Hu%20and%20Jie%20Song%20and%20Mingli%20Song%20and%20Canghong%20Jin%0AAbstract%3A%20Prototypical%20part%20network%20%28ProtoPNet%29%20has%20drawn%20wide%20attention%20and%20boosted%20many%20follow-up%20studies%20due%20to%20its%20self-explanatory%20property%20for%20explainable%20artificial%20intelligence%20%28XAI%29.%20However%2C%20when%20directly%20applying%20ProtoPNet%20on%20vision%20transformer%20%28ViT%29%20backbones%2C%20learned%20prototypes%20have%20a%20%22distraction%22%20problem%3A%20they%20have%20a%20relatively%20high%20probability%20of%20being%20activated%20by%20the%20background%20and%20pay%20less%20attention%20to%20the%20foreground.%20The%20powerful%20capability%20of%20modeling%20long-term%20dependency%20makes%20the%20transformer-based%20ProtoPNet%20hard%20to%20focus%20on%20prototypical%20parts%2C%20thus%20severely%20impairing%20its%20inherent%20interpretability.%20This%20paper%20proposes%20prototypical%20part%20transformer%20%28ProtoPFormer%29%20for%20appropriately%20and%20effectively%20applying%20the%20prototype-based%20method%20with%20ViTs%20for%20interpretable%20image%20recognition.%20The%20proposed%20method%20introduces%20global%20and%20local%20prototypes%20for%20capturing%20and%20highlighting%20the%20representative%20holistic%20and%20partial%20features%20of%20targets%20according%20to%20the%20architectural%20characteristics%20of%20ViTs.%20The%20global%20prototypes%20are%20adopted%20to%20provide%20the%20global%20view%20of%20objects%20to%20guide%20local%20prototypes%20to%20concentrate%20on%20the%20foreground%20while%20eliminating%20the%20influence%20of%20the%20background.%20Afterwards%2C%20local%20prototypes%20are%20explicitly%20supervised%20to%20concentrate%20on%20their%20respective%20prototypical%20visual%20parts%2C%20increasing%20the%20overall%20interpretability.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20global%20and%20local%20prototypes%20can%20mutually%20correct%20each%20other%20and%20jointly%20make%20final%20decisions%2C%20which%20faithfully%20and%20transparently%20reason%20the%20decision-making%20processes%20associatively%20from%20the%20whole%20and%20local%20perspectives%2C%20respectively.%20Moreover%2C%20ProtoPFormer%20consistently%20achieves%20superior%20performance%20and%20visualization%20results%20over%20the%20state-of-the-art%20%28SOTA%29%20prototype-based%20baselines.%20Our%20code%20has%20been%20released%20at%20https%3A//github.com/zju-vipa/ProtoPFormer.%0ALink%3A%20http%3A//arxiv.org/abs/2208.10431v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoPFormer%253A%2520Concentrating%2520on%2520Prototypical%2520Parts%2520in%2520Vision%2520Transformers%2520for%2520Interpretable%2520Image%2520Recognition%26entry.906535625%3DMengqi%2520Xue%2520and%2520Qihan%2520Huang%2520and%2520Haofei%2520Zhang%2520and%2520Jingwen%2520Hu%2520and%2520Jie%2520Song%2520and%2520Mingli%2520Song%2520and%2520Canghong%2520Jin%26entry.1292438233%3DPrototypical%2520part%2520network%2520%2528ProtoPNet%2529%2520has%2520drawn%2520wide%2520attention%2520and%2520boosted%2520many%2520follow-up%2520studies%2520due%2520to%2520its%2520self-explanatory%2520property%2520for%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529.%2520However%252C%2520when%2520directly%2520applying%2520ProtoPNet%2520on%2520vision%2520transformer%2520%2528ViT%2529%2520backbones%252C%2520learned%2520prototypes%2520have%2520a%2520%2522distraction%2522%2520problem%253A%2520they%2520have%2520a%2520relatively%2520high%2520probability%2520of%2520being%2520activated%2520by%2520the%2520background%2520and%2520pay%2520less%2520attention%2520to%2520the%2520foreground.%2520The%2520powerful%2520capability%2520of%2520modeling%2520long-term%2520dependency%2520makes%2520the%2520transformer-based%2520ProtoPNet%2520hard%2520to%2520focus%2520on%2520prototypical%2520parts%252C%2520thus%2520severely%2520impairing%2520its%2520inherent%2520interpretability.%2520This%2520paper%2520proposes%2520prototypical%2520part%2520transformer%2520%2528ProtoPFormer%2529%2520for%2520appropriately%2520and%2520effectively%2520applying%2520the%2520prototype-based%2520method%2520with%2520ViTs%2520for%2520interpretable%2520image%2520recognition.%2520The%2520proposed%2520method%2520introduces%2520global%2520and%2520local%2520prototypes%2520for%2520capturing%2520and%2520highlighting%2520the%2520representative%2520holistic%2520and%2520partial%2520features%2520of%2520targets%2520according%2520to%2520the%2520architectural%2520characteristics%2520of%2520ViTs.%2520The%2520global%2520prototypes%2520are%2520adopted%2520to%2520provide%2520the%2520global%2520view%2520of%2520objects%2520to%2520guide%2520local%2520prototypes%2520to%2520concentrate%2520on%2520the%2520foreground%2520while%2520eliminating%2520the%2520influence%2520of%2520the%2520background.%2520Afterwards%252C%2520local%2520prototypes%2520are%2520explicitly%2520supervised%2520to%2520concentrate%2520on%2520their%2520respective%2520prototypical%2520visual%2520parts%252C%2520increasing%2520the%2520overall%2520interpretability.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520global%2520and%2520local%2520prototypes%2520can%2520mutually%2520correct%2520each%2520other%2520and%2520jointly%2520make%2520final%2520decisions%252C%2520which%2520faithfully%2520and%2520transparently%2520reason%2520the%2520decision-making%2520processes%2520associatively%2520from%2520the%2520whole%2520and%2520local%2520perspectives%252C%2520respectively.%2520Moreover%252C%2520ProtoPFormer%2520consistently%2520achieves%2520superior%2520performance%2520and%2520visualization%2520results%2520over%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520prototype-based%2520baselines.%2520Our%2520code%2520has%2520been%2520released%2520at%2520https%253A//github.com/zju-vipa/ProtoPFormer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.10431v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoPFormer%3A%20Concentrating%20on%20Prototypical%20Parts%20in%20Vision%20Transformers%20for%20Interpretable%20Image%20Recognition&entry.906535625=Mengqi%20Xue%20and%20Qihan%20Huang%20and%20Haofei%20Zhang%20and%20Jingwen%20Hu%20and%20Jie%20Song%20and%20Mingli%20Song%20and%20Canghong%20Jin&entry.1292438233=Prototypical%20part%20network%20%28ProtoPNet%29%20has%20drawn%20wide%20attention%20and%20boosted%20many%20follow-up%20studies%20due%20to%20its%20self-explanatory%20property%20for%20explainable%20artificial%20intelligence%20%28XAI%29.%20However%2C%20when%20directly%20applying%20ProtoPNet%20on%20vision%20transformer%20%28ViT%29%20backbones%2C%20learned%20prototypes%20have%20a%20%22distraction%22%20problem%3A%20they%20have%20a%20relatively%20high%20probability%20of%20being%20activated%20by%20the%20background%20and%20pay%20less%20attention%20to%20the%20foreground.%20The%20powerful%20capability%20of%20modeling%20long-term%20dependency%20makes%20the%20transformer-based%20ProtoPNet%20hard%20to%20focus%20on%20prototypical%20parts%2C%20thus%20severely%20impairing%20its%20inherent%20interpretability.%20This%20paper%20proposes%20prototypical%20part%20transformer%20%28ProtoPFormer%29%20for%20appropriately%20and%20effectively%20applying%20the%20prototype-based%20method%20with%20ViTs%20for%20interpretable%20image%20recognition.%20The%20proposed%20method%20introduces%20global%20and%20local%20prototypes%20for%20capturing%20and%20highlighting%20the%20representative%20holistic%20and%20partial%20features%20of%20targets%20according%20to%20the%20architectural%20characteristics%20of%20ViTs.%20The%20global%20prototypes%20are%20adopted%20to%20provide%20the%20global%20view%20of%20objects%20to%20guide%20local%20prototypes%20to%20concentrate%20on%20the%20foreground%20while%20eliminating%20the%20influence%20of%20the%20background.%20Afterwards%2C%20local%20prototypes%20are%20explicitly%20supervised%20to%20concentrate%20on%20their%20respective%20prototypical%20visual%20parts%2C%20increasing%20the%20overall%20interpretability.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20global%20and%20local%20prototypes%20can%20mutually%20correct%20each%20other%20and%20jointly%20make%20final%20decisions%2C%20which%20faithfully%20and%20transparently%20reason%20the%20decision-making%20processes%20associatively%20from%20the%20whole%20and%20local%20perspectives%2C%20respectively.%20Moreover%2C%20ProtoPFormer%20consistently%20achieves%20superior%20performance%20and%20visualization%20results%20over%20the%20state-of-the-art%20%28SOTA%29%20prototype-based%20baselines.%20Our%20code%20has%20been%20released%20at%20https%3A//github.com/zju-vipa/ProtoPFormer.&entry.1838667208=http%3A//arxiv.org/abs/2208.10431v3&entry.124074799=Read"},
{"title": "TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks", "author": "Xuanle Zhao and Shuxin Zeng and Xinyuan Cai and Xiang Cheng and Duzhen Zhang and Xiuyi Chen and Bo Xu", "abstract": "While Vision Language Models (VLMs) have demonstrated remarkable capabilities in general visual understanding, their application in the chemical domain has been limited, with previous works predominantly focusing on text and thus overlooking critical visual information, such as molecular structures. Current approaches that directly adopt standard VLMs for chemical tasks suffer from two primary issues: (i) computational inefficiency of processing entire chemical images with non-informative backgrounds. (ii) a narrow scope on molecular-level tasks that restricts progress in chemical reasoning. In this work, we propose \\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages visual token reduction and reaction-level tasks to improve model efficiency and reasoning capacity. Also, we propose \\textbf{ChemRxn-V}, a reaction-level benchmark for assessing vision-based reaction recognition and prediction tasks. Directly predicting reaction products from molecular images poses a non-trivial challenge, as it requires models to integrate both recognition and reasoning capacities. Our results demonstrate that with only 4B parameters, TinyChemVL achieves superior performance on both molecular and reaction tasks while demonstrating faster inference and training speeds compared to existing models. Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the visual tokens. This work builds efficient yet powerful VLMs for chemical domains by co-designing model architecture and task complexity.", "link": "http://arxiv.org/abs/2511.06283v2", "date": "2025-11-26", "relevancy": 2.1299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinyChemVL%3A%20Advancing%20Chemical%20Vision-Language%20Models%20via%20Efficient%20Visual%20Token%20Reduction%20and%20Complex%20Reaction%20Tasks&body=Title%3A%20TinyChemVL%3A%20Advancing%20Chemical%20Vision-Language%20Models%20via%20Efficient%20Visual%20Token%20Reduction%20and%20Complex%20Reaction%20Tasks%0AAuthor%3A%20Xuanle%20Zhao%20and%20Shuxin%20Zeng%20and%20Xinyuan%20Cai%20and%20Xiang%20Cheng%20and%20Duzhen%20Zhang%20and%20Xiuyi%20Chen%20and%20Bo%20Xu%0AAbstract%3A%20While%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20general%20visual%20understanding%2C%20their%20application%20in%20the%20chemical%20domain%20has%20been%20limited%2C%20with%20previous%20works%20predominantly%20focusing%20on%20text%20and%20thus%20overlooking%20critical%20visual%20information%2C%20such%20as%20molecular%20structures.%20Current%20approaches%20that%20directly%20adopt%20standard%20VLMs%20for%20chemical%20tasks%20suffer%20from%20two%20primary%20issues%3A%20%28i%29%20computational%20inefficiency%20of%20processing%20entire%20chemical%20images%20with%20non-informative%20backgrounds.%20%28ii%29%20a%20narrow%20scope%20on%20molecular-level%20tasks%20that%20restricts%20progress%20in%20chemical%20reasoning.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BTinyChemVL%7D%2C%20an%20efficient%20and%20powerful%20chemical%20VLM%20that%20leverages%20visual%20token%20reduction%20and%20reaction-level%20tasks%20to%20improve%20model%20efficiency%20and%20reasoning%20capacity.%20Also%2C%20we%20propose%20%5Ctextbf%7BChemRxn-V%7D%2C%20a%20reaction-level%20benchmark%20for%20assessing%20vision-based%20reaction%20recognition%20and%20prediction%20tasks.%20Directly%20predicting%20reaction%20products%20from%20molecular%20images%20poses%20a%20non-trivial%20challenge%2C%20as%20it%20requires%20models%20to%20integrate%20both%20recognition%20and%20reasoning%20capacities.%20Our%20results%20demonstrate%20that%20with%20only%204B%20parameters%2C%20TinyChemVL%20achieves%20superior%20performance%20on%20both%20molecular%20and%20reaction%20tasks%20while%20demonstrating%20faster%20inference%20and%20training%20speeds%20compared%20to%20existing%20models.%20Notably%2C%20TinyChemVL%20outperforms%20ChemVLM%20while%20utilizing%20only%201/16th%20of%20the%20visual%20tokens.%20This%20work%20builds%20efficient%20yet%20powerful%20VLMs%20for%20chemical%20domains%20by%20co-designing%20model%20architecture%20and%20task%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinyChemVL%253A%2520Advancing%2520Chemical%2520Vision-Language%2520Models%2520via%2520Efficient%2520Visual%2520Token%2520Reduction%2520and%2520Complex%2520Reaction%2520Tasks%26entry.906535625%3DXuanle%2520Zhao%2520and%2520Shuxin%2520Zeng%2520and%2520Xinyuan%2520Cai%2520and%2520Xiang%2520Cheng%2520and%2520Duzhen%2520Zhang%2520and%2520Xiuyi%2520Chen%2520and%2520Bo%2520Xu%26entry.1292438233%3DWhile%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520general%2520visual%2520understanding%252C%2520their%2520application%2520in%2520the%2520chemical%2520domain%2520has%2520been%2520limited%252C%2520with%2520previous%2520works%2520predominantly%2520focusing%2520on%2520text%2520and%2520thus%2520overlooking%2520critical%2520visual%2520information%252C%2520such%2520as%2520molecular%2520structures.%2520Current%2520approaches%2520that%2520directly%2520adopt%2520standard%2520VLMs%2520for%2520chemical%2520tasks%2520suffer%2520from%2520two%2520primary%2520issues%253A%2520%2528i%2529%2520computational%2520inefficiency%2520of%2520processing%2520entire%2520chemical%2520images%2520with%2520non-informative%2520backgrounds.%2520%2528ii%2529%2520a%2520narrow%2520scope%2520on%2520molecular-level%2520tasks%2520that%2520restricts%2520progress%2520in%2520chemical%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Ctextbf%257BTinyChemVL%257D%252C%2520an%2520efficient%2520and%2520powerful%2520chemical%2520VLM%2520that%2520leverages%2520visual%2520token%2520reduction%2520and%2520reaction-level%2520tasks%2520to%2520improve%2520model%2520efficiency%2520and%2520reasoning%2520capacity.%2520Also%252C%2520we%2520propose%2520%255Ctextbf%257BChemRxn-V%257D%252C%2520a%2520reaction-level%2520benchmark%2520for%2520assessing%2520vision-based%2520reaction%2520recognition%2520and%2520prediction%2520tasks.%2520Directly%2520predicting%2520reaction%2520products%2520from%2520molecular%2520images%2520poses%2520a%2520non-trivial%2520challenge%252C%2520as%2520it%2520requires%2520models%2520to%2520integrate%2520both%2520recognition%2520and%2520reasoning%2520capacities.%2520Our%2520results%2520demonstrate%2520that%2520with%2520only%25204B%2520parameters%252C%2520TinyChemVL%2520achieves%2520superior%2520performance%2520on%2520both%2520molecular%2520and%2520reaction%2520tasks%2520while%2520demonstrating%2520faster%2520inference%2520and%2520training%2520speeds%2520compared%2520to%2520existing%2520models.%2520Notably%252C%2520TinyChemVL%2520outperforms%2520ChemVLM%2520while%2520utilizing%2520only%25201/16th%2520of%2520the%2520visual%2520tokens.%2520This%2520work%2520builds%2520efficient%2520yet%2520powerful%2520VLMs%2520for%2520chemical%2520domains%2520by%2520co-designing%2520model%2520architecture%2520and%2520task%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyChemVL%3A%20Advancing%20Chemical%20Vision-Language%20Models%20via%20Efficient%20Visual%20Token%20Reduction%20and%20Complex%20Reaction%20Tasks&entry.906535625=Xuanle%20Zhao%20and%20Shuxin%20Zeng%20and%20Xinyuan%20Cai%20and%20Xiang%20Cheng%20and%20Duzhen%20Zhang%20and%20Xiuyi%20Chen%20and%20Bo%20Xu&entry.1292438233=While%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20general%20visual%20understanding%2C%20their%20application%20in%20the%20chemical%20domain%20has%20been%20limited%2C%20with%20previous%20works%20predominantly%20focusing%20on%20text%20and%20thus%20overlooking%20critical%20visual%20information%2C%20such%20as%20molecular%20structures.%20Current%20approaches%20that%20directly%20adopt%20standard%20VLMs%20for%20chemical%20tasks%20suffer%20from%20two%20primary%20issues%3A%20%28i%29%20computational%20inefficiency%20of%20processing%20entire%20chemical%20images%20with%20non-informative%20backgrounds.%20%28ii%29%20a%20narrow%20scope%20on%20molecular-level%20tasks%20that%20restricts%20progress%20in%20chemical%20reasoning.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BTinyChemVL%7D%2C%20an%20efficient%20and%20powerful%20chemical%20VLM%20that%20leverages%20visual%20token%20reduction%20and%20reaction-level%20tasks%20to%20improve%20model%20efficiency%20and%20reasoning%20capacity.%20Also%2C%20we%20propose%20%5Ctextbf%7BChemRxn-V%7D%2C%20a%20reaction-level%20benchmark%20for%20assessing%20vision-based%20reaction%20recognition%20and%20prediction%20tasks.%20Directly%20predicting%20reaction%20products%20from%20molecular%20images%20poses%20a%20non-trivial%20challenge%2C%20as%20it%20requires%20models%20to%20integrate%20both%20recognition%20and%20reasoning%20capacities.%20Our%20results%20demonstrate%20that%20with%20only%204B%20parameters%2C%20TinyChemVL%20achieves%20superior%20performance%20on%20both%20molecular%20and%20reaction%20tasks%20while%20demonstrating%20faster%20inference%20and%20training%20speeds%20compared%20to%20existing%20models.%20Notably%2C%20TinyChemVL%20outperforms%20ChemVLM%20while%20utilizing%20only%201/16th%20of%20the%20visual%20tokens.%20This%20work%20builds%20efficient%20yet%20powerful%20VLMs%20for%20chemical%20domains%20by%20co-designing%20model%20architecture%20and%20task%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2511.06283v2&entry.124074799=Read"},
{"title": "PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery", "author": "Jules Decaestecker and Nicolas Vigne", "abstract": "Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive.", "link": "http://arxiv.org/abs/2511.21298v1", "date": "2025-11-26", "relevancy": 2.1271, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathMamba%3A%20A%20Hybrid%20Mamba-Transformer%20for%20Topologically%20Coherent%20Road%20Segmentation%20in%20Satellite%20Imagery&body=Title%3A%20PathMamba%3A%20A%20Hybrid%20Mamba-Transformer%20for%20Topologically%20Coherent%20Road%20Segmentation%20in%20Satellite%20Imagery%0AAuthor%3A%20Jules%20Decaestecker%20and%20Nicolas%20Vigne%0AAbstract%3A%20Achieving%20both%20high%20accuracy%20and%20topological%20continuity%20in%20road%20segmentation%20from%20satellite%20imagery%20is%20a%20critical%20goal%20for%20applications%20ranging%20from%20urban%20planning%20to%20disaster%20response.%20State-of-the-art%20methods%20often%20rely%20on%20Vision%20Transformers%2C%20which%20excel%20at%20capturing%20global%20context%2C%20yet%20their%20quadratic%20complexity%20is%20a%20significant%20barrier%20to%20efficient%20deployment%2C%20particularly%20for%20on-board%20processing%20in%20resource-constrained%20platforms.%20In%20contrast%2C%20emerging%20State%20Space%20Models%20like%20Mamba%20offer%20linear-time%20efficiency%20and%20are%20inherently%20suited%20to%20modeling%20long%2C%20continuous%20structures.%20We%20posit%20that%20these%20architectures%20have%20complementary%20strengths.%20To%20this%20end%2C%20we%20introduce%20PathMamba%2C%20a%20novel%20hybrid%20architecture%20that%20integrates%20Mamba%27s%20sequential%20modeling%20with%20the%20Transformer%27s%20global%20reasoning.%20Our%20design%20strategically%20uses%20Mamba%20blocks%20to%20trace%20the%20continuous%20nature%20of%20road%20networks%2C%20preserving%20topological%20structure%2C%20while%20integrating%20Transformer%20blocks%20to%20refine%20features%20with%20global%20context.%20This%20approach%20yields%20topologically%20superior%20segmentation%20maps%20without%20the%20prohibitive%20scaling%20costs%20of%20pure%20attention-based%20models.%20Our%20experiments%20on%20the%20DeepGlobe%20Road%20Extraction%20and%20Massachusetts%20Roads%20datasets%20demonstrate%20that%20PathMamba%20sets%20a%20new%20state-of-the-art.%20Notably%2C%20it%20significantly%20improves%20topological%20continuity%2C%20as%20measured%20by%20the%20APLS%20metric%2C%20setting%20a%20new%20benchmark%20while%20remaining%20computationally%20competitive.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathMamba%253A%2520A%2520Hybrid%2520Mamba-Transformer%2520for%2520Topologically%2520Coherent%2520Road%2520Segmentation%2520in%2520Satellite%2520Imagery%26entry.906535625%3DJules%2520Decaestecker%2520and%2520Nicolas%2520Vigne%26entry.1292438233%3DAchieving%2520both%2520high%2520accuracy%2520and%2520topological%2520continuity%2520in%2520road%2520segmentation%2520from%2520satellite%2520imagery%2520is%2520a%2520critical%2520goal%2520for%2520applications%2520ranging%2520from%2520urban%2520planning%2520to%2520disaster%2520response.%2520State-of-the-art%2520methods%2520often%2520rely%2520on%2520Vision%2520Transformers%252C%2520which%2520excel%2520at%2520capturing%2520global%2520context%252C%2520yet%2520their%2520quadratic%2520complexity%2520is%2520a%2520significant%2520barrier%2520to%2520efficient%2520deployment%252C%2520particularly%2520for%2520on-board%2520processing%2520in%2520resource-constrained%2520platforms.%2520In%2520contrast%252C%2520emerging%2520State%2520Space%2520Models%2520like%2520Mamba%2520offer%2520linear-time%2520efficiency%2520and%2520are%2520inherently%2520suited%2520to%2520modeling%2520long%252C%2520continuous%2520structures.%2520We%2520posit%2520that%2520these%2520architectures%2520have%2520complementary%2520strengths.%2520To%2520this%2520end%252C%2520we%2520introduce%2520PathMamba%252C%2520a%2520novel%2520hybrid%2520architecture%2520that%2520integrates%2520Mamba%2527s%2520sequential%2520modeling%2520with%2520the%2520Transformer%2527s%2520global%2520reasoning.%2520Our%2520design%2520strategically%2520uses%2520Mamba%2520blocks%2520to%2520trace%2520the%2520continuous%2520nature%2520of%2520road%2520networks%252C%2520preserving%2520topological%2520structure%252C%2520while%2520integrating%2520Transformer%2520blocks%2520to%2520refine%2520features%2520with%2520global%2520context.%2520This%2520approach%2520yields%2520topologically%2520superior%2520segmentation%2520maps%2520without%2520the%2520prohibitive%2520scaling%2520costs%2520of%2520pure%2520attention-based%2520models.%2520Our%2520experiments%2520on%2520the%2520DeepGlobe%2520Road%2520Extraction%2520and%2520Massachusetts%2520Roads%2520datasets%2520demonstrate%2520that%2520PathMamba%2520sets%2520a%2520new%2520state-of-the-art.%2520Notably%252C%2520it%2520significantly%2520improves%2520topological%2520continuity%252C%2520as%2520measured%2520by%2520the%2520APLS%2520metric%252C%2520setting%2520a%2520new%2520benchmark%2520while%2520remaining%2520computationally%2520competitive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathMamba%3A%20A%20Hybrid%20Mamba-Transformer%20for%20Topologically%20Coherent%20Road%20Segmentation%20in%20Satellite%20Imagery&entry.906535625=Jules%20Decaestecker%20and%20Nicolas%20Vigne&entry.1292438233=Achieving%20both%20high%20accuracy%20and%20topological%20continuity%20in%20road%20segmentation%20from%20satellite%20imagery%20is%20a%20critical%20goal%20for%20applications%20ranging%20from%20urban%20planning%20to%20disaster%20response.%20State-of-the-art%20methods%20often%20rely%20on%20Vision%20Transformers%2C%20which%20excel%20at%20capturing%20global%20context%2C%20yet%20their%20quadratic%20complexity%20is%20a%20significant%20barrier%20to%20efficient%20deployment%2C%20particularly%20for%20on-board%20processing%20in%20resource-constrained%20platforms.%20In%20contrast%2C%20emerging%20State%20Space%20Models%20like%20Mamba%20offer%20linear-time%20efficiency%20and%20are%20inherently%20suited%20to%20modeling%20long%2C%20continuous%20structures.%20We%20posit%20that%20these%20architectures%20have%20complementary%20strengths.%20To%20this%20end%2C%20we%20introduce%20PathMamba%2C%20a%20novel%20hybrid%20architecture%20that%20integrates%20Mamba%27s%20sequential%20modeling%20with%20the%20Transformer%27s%20global%20reasoning.%20Our%20design%20strategically%20uses%20Mamba%20blocks%20to%20trace%20the%20continuous%20nature%20of%20road%20networks%2C%20preserving%20topological%20structure%2C%20while%20integrating%20Transformer%20blocks%20to%20refine%20features%20with%20global%20context.%20This%20approach%20yields%20topologically%20superior%20segmentation%20maps%20without%20the%20prohibitive%20scaling%20costs%20of%20pure%20attention-based%20models.%20Our%20experiments%20on%20the%20DeepGlobe%20Road%20Extraction%20and%20Massachusetts%20Roads%20datasets%20demonstrate%20that%20PathMamba%20sets%20a%20new%20state-of-the-art.%20Notably%2C%20it%20significantly%20improves%20topological%20continuity%2C%20as%20measured%20by%20the%20APLS%20metric%2C%20setting%20a%20new%20benchmark%20while%20remaining%20computationally%20competitive.&entry.1838667208=http%3A//arxiv.org/abs/2511.21298v1&entry.124074799=Read"},
{"title": "Restoration-Oriented Video Frame Interpolation with Region-Distinguishable Priors from SAM", "author": "Yan Han and Xiaogang Xu and Yingqi Lin and Jiafei Wu and Zhe Liu and Ming-Hsuan Yang", "abstract": "In existing restoration-oriented Video Frame Interpolation (VFI) approaches, the motion estimation between neighboring frames plays a crucial role. However, the estimation accuracy in existing methods remains a challenge, primarily due to the inherent ambiguity in identifying corresponding areas in adjacent frames for interpolation. Therefore, enhancing accuracy by distinguishing different regions before motion estimation is of utmost importance. In this paper, we introduce a novel solution involving the utilization of open-world segmentation models, e.g., SAM2 (Segment Anything Model2) for frames, to derive Region-Distinguishable Priors (RDPs) in different frames. These RDPs are represented as spatial-varying Gaussian mixtures, distinguishing an arbitrary number of areas with a unified modality. RDPs can be integrated into existing motion-based VFI methods to enhance features for motion estimation, facilitated by our designed play-and-plug Hierarchical Region-aware Feature Fusion Module (HRFFM). HRFFM incorporates RDP into various hierarchical stages of VFI's encoder, using RDP-guided Feature Normalization (RDPFN) in a residual learning manner. With HRFFM and RDP, the features within VFI's encoder exhibit similar representations for matched regions in neighboring frames, thus improving the synthesis of intermediate frames. Extensive experiments demonstrate that HRFFM consistently enhances VFI performance across various scenes.", "link": "http://arxiv.org/abs/2312.15868v2", "date": "2025-11-26", "relevancy": 2.1248, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5588}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5134}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restoration-Oriented%20Video%20Frame%20Interpolation%20with%20Region-Distinguishable%20Priors%20from%20SAM&body=Title%3A%20Restoration-Oriented%20Video%20Frame%20Interpolation%20with%20Region-Distinguishable%20Priors%20from%20SAM%0AAuthor%3A%20Yan%20Han%20and%20Xiaogang%20Xu%20and%20Yingqi%20Lin%20and%20Jiafei%20Wu%20and%20Zhe%20Liu%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20In%20existing%20restoration-oriented%20Video%20Frame%20Interpolation%20%28VFI%29%20approaches%2C%20the%20motion%20estimation%20between%20neighboring%20frames%20plays%20a%20crucial%20role.%20However%2C%20the%20estimation%20accuracy%20in%20existing%20methods%20remains%20a%20challenge%2C%20primarily%20due%20to%20the%20inherent%20ambiguity%20in%20identifying%20corresponding%20areas%20in%20adjacent%20frames%20for%20interpolation.%20Therefore%2C%20enhancing%20accuracy%20by%20distinguishing%20different%20regions%20before%20motion%20estimation%20is%20of%20utmost%20importance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20solution%20involving%20the%20utilization%20of%20open-world%20segmentation%20models%2C%20e.g.%2C%20SAM2%20%28Segment%20Anything%20Model2%29%20for%20frames%2C%20to%20derive%20Region-Distinguishable%20Priors%20%28RDPs%29%20in%20different%20frames.%20These%20RDPs%20are%20represented%20as%20spatial-varying%20Gaussian%20mixtures%2C%20distinguishing%20an%20arbitrary%20number%20of%20areas%20with%20a%20unified%20modality.%20RDPs%20can%20be%20integrated%20into%20existing%20motion-based%20VFI%20methods%20to%20enhance%20features%20for%20motion%20estimation%2C%20facilitated%20by%20our%20designed%20play-and-plug%20Hierarchical%20Region-aware%20Feature%20Fusion%20Module%20%28HRFFM%29.%20HRFFM%20incorporates%20RDP%20into%20various%20hierarchical%20stages%20of%20VFI%27s%20encoder%2C%20using%20RDP-guided%20Feature%20Normalization%20%28RDPFN%29%20in%20a%20residual%20learning%20manner.%20With%20HRFFM%20and%20RDP%2C%20the%20features%20within%20VFI%27s%20encoder%20exhibit%20similar%20representations%20for%20matched%20regions%20in%20neighboring%20frames%2C%20thus%20improving%20the%20synthesis%20of%20intermediate%20frames.%20Extensive%20experiments%20demonstrate%20that%20HRFFM%20consistently%20enhances%20VFI%20performance%20across%20various%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2312.15868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestoration-Oriented%2520Video%2520Frame%2520Interpolation%2520with%2520Region-Distinguishable%2520Priors%2520from%2520SAM%26entry.906535625%3DYan%2520Han%2520and%2520Xiaogang%2520Xu%2520and%2520Yingqi%2520Lin%2520and%2520Jiafei%2520Wu%2520and%2520Zhe%2520Liu%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3DIn%2520existing%2520restoration-oriented%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520approaches%252C%2520the%2520motion%2520estimation%2520between%2520neighboring%2520frames%2520plays%2520a%2520crucial%2520role.%2520However%252C%2520the%2520estimation%2520accuracy%2520in%2520existing%2520methods%2520remains%2520a%2520challenge%252C%2520primarily%2520due%2520to%2520the%2520inherent%2520ambiguity%2520in%2520identifying%2520corresponding%2520areas%2520in%2520adjacent%2520frames%2520for%2520interpolation.%2520Therefore%252C%2520enhancing%2520accuracy%2520by%2520distinguishing%2520different%2520regions%2520before%2520motion%2520estimation%2520is%2520of%2520utmost%2520importance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520solution%2520involving%2520the%2520utilization%2520of%2520open-world%2520segmentation%2520models%252C%2520e.g.%252C%2520SAM2%2520%2528Segment%2520Anything%2520Model2%2529%2520for%2520frames%252C%2520to%2520derive%2520Region-Distinguishable%2520Priors%2520%2528RDPs%2529%2520in%2520different%2520frames.%2520These%2520RDPs%2520are%2520represented%2520as%2520spatial-varying%2520Gaussian%2520mixtures%252C%2520distinguishing%2520an%2520arbitrary%2520number%2520of%2520areas%2520with%2520a%2520unified%2520modality.%2520RDPs%2520can%2520be%2520integrated%2520into%2520existing%2520motion-based%2520VFI%2520methods%2520to%2520enhance%2520features%2520for%2520motion%2520estimation%252C%2520facilitated%2520by%2520our%2520designed%2520play-and-plug%2520Hierarchical%2520Region-aware%2520Feature%2520Fusion%2520Module%2520%2528HRFFM%2529.%2520HRFFM%2520incorporates%2520RDP%2520into%2520various%2520hierarchical%2520stages%2520of%2520VFI%2527s%2520encoder%252C%2520using%2520RDP-guided%2520Feature%2520Normalization%2520%2528RDPFN%2529%2520in%2520a%2520residual%2520learning%2520manner.%2520With%2520HRFFM%2520and%2520RDP%252C%2520the%2520features%2520within%2520VFI%2527s%2520encoder%2520exhibit%2520similar%2520representations%2520for%2520matched%2520regions%2520in%2520neighboring%2520frames%252C%2520thus%2520improving%2520the%2520synthesis%2520of%2520intermediate%2520frames.%2520Extensive%2520experiments%2520demonstrate%2520that%2520HRFFM%2520consistently%2520enhances%2520VFI%2520performance%2520across%2520various%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restoration-Oriented%20Video%20Frame%20Interpolation%20with%20Region-Distinguishable%20Priors%20from%20SAM&entry.906535625=Yan%20Han%20and%20Xiaogang%20Xu%20and%20Yingqi%20Lin%20and%20Jiafei%20Wu%20and%20Zhe%20Liu%20and%20Ming-Hsuan%20Yang&entry.1292438233=In%20existing%20restoration-oriented%20Video%20Frame%20Interpolation%20%28VFI%29%20approaches%2C%20the%20motion%20estimation%20between%20neighboring%20frames%20plays%20a%20crucial%20role.%20However%2C%20the%20estimation%20accuracy%20in%20existing%20methods%20remains%20a%20challenge%2C%20primarily%20due%20to%20the%20inherent%20ambiguity%20in%20identifying%20corresponding%20areas%20in%20adjacent%20frames%20for%20interpolation.%20Therefore%2C%20enhancing%20accuracy%20by%20distinguishing%20different%20regions%20before%20motion%20estimation%20is%20of%20utmost%20importance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20solution%20involving%20the%20utilization%20of%20open-world%20segmentation%20models%2C%20e.g.%2C%20SAM2%20%28Segment%20Anything%20Model2%29%20for%20frames%2C%20to%20derive%20Region-Distinguishable%20Priors%20%28RDPs%29%20in%20different%20frames.%20These%20RDPs%20are%20represented%20as%20spatial-varying%20Gaussian%20mixtures%2C%20distinguishing%20an%20arbitrary%20number%20of%20areas%20with%20a%20unified%20modality.%20RDPs%20can%20be%20integrated%20into%20existing%20motion-based%20VFI%20methods%20to%20enhance%20features%20for%20motion%20estimation%2C%20facilitated%20by%20our%20designed%20play-and-plug%20Hierarchical%20Region-aware%20Feature%20Fusion%20Module%20%28HRFFM%29.%20HRFFM%20incorporates%20RDP%20into%20various%20hierarchical%20stages%20of%20VFI%27s%20encoder%2C%20using%20RDP-guided%20Feature%20Normalization%20%28RDPFN%29%20in%20a%20residual%20learning%20manner.%20With%20HRFFM%20and%20RDP%2C%20the%20features%20within%20VFI%27s%20encoder%20exhibit%20similar%20representations%20for%20matched%20regions%20in%20neighboring%20frames%2C%20thus%20improving%20the%20synthesis%20of%20intermediate%20frames.%20Extensive%20experiments%20demonstrate%20that%20HRFFM%20consistently%20enhances%20VFI%20performance%20across%20various%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2312.15868v2&entry.124074799=Read"},
{"title": "MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors", "author": "Thai-Khanh Nguyen and Uyen Vo and Tan M. Nguyen and Thieu N. Vo and Trung-Hieu Le and Cuong Pham", "abstract": "Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications.", "link": "http://arxiv.org/abs/2511.21550v1", "date": "2025-11-26", "relevancy": 2.1247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5616}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5273}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMA%3A%20A%20Momentum%20Mamba%20Architecture%20for%20Human%20Activity%20Recognition%20with%20Inertial%20Sensors&body=Title%3A%20MMA%3A%20A%20Momentum%20Mamba%20Architecture%20for%20Human%20Activity%20Recognition%20with%20Inertial%20Sensors%0AAuthor%3A%20Thai-Khanh%20Nguyen%20and%20Uyen%20Vo%20and%20Tan%20M.%20Nguyen%20and%20Thieu%20N.%20Vo%20and%20Trung-Hieu%20Le%20and%20Cuong%20Pham%0AAbstract%3A%20Human%20activity%20recognition%20%28HAR%29%20from%20inertial%20sensors%20is%20essential%20for%20ubiquitous%20computing%2C%20mobile%20health%2C%20and%20ambient%20intelligence.%20Conventional%20deep%20models%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%20and%20transformers%20have%20advanced%20HAR%20but%20remain%20limited%20by%20vanishing%20or%20exloding%20gradients%2C%20high%20computational%20cost%2C%20and%20difficulty%20in%20capturing%20long-range%20dependencies.%20Structured%20state-space%20models%20%28SSMs%29%20like%20Mamba%20address%20these%20challenges%20with%20linear%20complexity%20and%20effective%20temporal%20modeling%2C%20yet%20they%20are%20restricted%20to%20first-order%20dynamics%20without%20stable%20longterm%20memory%20mechanisms.%20We%20introduce%20Momentum%20Mamba%2C%20a%20momentum-augmented%20SSM%20that%20incorporates%20second-order%20dynamics%20to%20improve%20stability%20of%20information%20flow%20across%20time%20steps%2C%20robustness%2C%20and%20long-sequence%20modeling.%20Two%20extensions%20further%20expand%20its%20capacity%3A%20Complex%20Momentum%20Mamba%20for%20frequency-selective%20memory%20scaling.%20Experiments%20on%20multiple%20HAR%20benchmarks%20demonstrate%20consistent%20gains%20over%20vanilla%20Mamba%20and%20Transformer%20baselines%20in%20accuracy%2C%20robustness%2C%20and%20convergence%20speed.%20With%20only%20moderate%20increases%20in%20training%20cost%2C%20momentum-augmented%20SSMs%20offer%20a%20favorable%20accuracy-efficiency%20balance%2C%20establishing%20them%20as%20a%20scalable%20paradigm%20for%20HAR%20and%20a%20promising%20principal%20framework%20for%20broader%20sequence%20modeling%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMA%253A%2520A%2520Momentum%2520Mamba%2520Architecture%2520for%2520Human%2520Activity%2520Recognition%2520with%2520Inertial%2520Sensors%26entry.906535625%3DThai-Khanh%2520Nguyen%2520and%2520Uyen%2520Vo%2520and%2520Tan%2520M.%2520Nguyen%2520and%2520Thieu%2520N.%2520Vo%2520and%2520Trung-Hieu%2520Le%2520and%2520Cuong%2520Pham%26entry.1292438233%3DHuman%2520activity%2520recognition%2520%2528HAR%2529%2520from%2520inertial%2520sensors%2520is%2520essential%2520for%2520ubiquitous%2520computing%252C%2520mobile%2520health%252C%2520and%2520ambient%2520intelligence.%2520Conventional%2520deep%2520models%2520such%2520as%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%252C%2520and%2520transformers%2520have%2520advanced%2520HAR%2520but%2520remain%2520limited%2520by%2520vanishing%2520or%2520exloding%2520gradients%252C%2520high%2520computational%2520cost%252C%2520and%2520difficulty%2520in%2520capturing%2520long-range%2520dependencies.%2520Structured%2520state-space%2520models%2520%2528SSMs%2529%2520like%2520Mamba%2520address%2520these%2520challenges%2520with%2520linear%2520complexity%2520and%2520effective%2520temporal%2520modeling%252C%2520yet%2520they%2520are%2520restricted%2520to%2520first-order%2520dynamics%2520without%2520stable%2520longterm%2520memory%2520mechanisms.%2520We%2520introduce%2520Momentum%2520Mamba%252C%2520a%2520momentum-augmented%2520SSM%2520that%2520incorporates%2520second-order%2520dynamics%2520to%2520improve%2520stability%2520of%2520information%2520flow%2520across%2520time%2520steps%252C%2520robustness%252C%2520and%2520long-sequence%2520modeling.%2520Two%2520extensions%2520further%2520expand%2520its%2520capacity%253A%2520Complex%2520Momentum%2520Mamba%2520for%2520frequency-selective%2520memory%2520scaling.%2520Experiments%2520on%2520multiple%2520HAR%2520benchmarks%2520demonstrate%2520consistent%2520gains%2520over%2520vanilla%2520Mamba%2520and%2520Transformer%2520baselines%2520in%2520accuracy%252C%2520robustness%252C%2520and%2520convergence%2520speed.%2520With%2520only%2520moderate%2520increases%2520in%2520training%2520cost%252C%2520momentum-augmented%2520SSMs%2520offer%2520a%2520favorable%2520accuracy-efficiency%2520balance%252C%2520establishing%2520them%2520as%2520a%2520scalable%2520paradigm%2520for%2520HAR%2520and%2520a%2520promising%2520principal%2520framework%2520for%2520broader%2520sequence%2520modeling%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMA%3A%20A%20Momentum%20Mamba%20Architecture%20for%20Human%20Activity%20Recognition%20with%20Inertial%20Sensors&entry.906535625=Thai-Khanh%20Nguyen%20and%20Uyen%20Vo%20and%20Tan%20M.%20Nguyen%20and%20Thieu%20N.%20Vo%20and%20Trung-Hieu%20Le%20and%20Cuong%20Pham&entry.1292438233=Human%20activity%20recognition%20%28HAR%29%20from%20inertial%20sensors%20is%20essential%20for%20ubiquitous%20computing%2C%20mobile%20health%2C%20and%20ambient%20intelligence.%20Conventional%20deep%20models%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%20and%20transformers%20have%20advanced%20HAR%20but%20remain%20limited%20by%20vanishing%20or%20exloding%20gradients%2C%20high%20computational%20cost%2C%20and%20difficulty%20in%20capturing%20long-range%20dependencies.%20Structured%20state-space%20models%20%28SSMs%29%20like%20Mamba%20address%20these%20challenges%20with%20linear%20complexity%20and%20effective%20temporal%20modeling%2C%20yet%20they%20are%20restricted%20to%20first-order%20dynamics%20without%20stable%20longterm%20memory%20mechanisms.%20We%20introduce%20Momentum%20Mamba%2C%20a%20momentum-augmented%20SSM%20that%20incorporates%20second-order%20dynamics%20to%20improve%20stability%20of%20information%20flow%20across%20time%20steps%2C%20robustness%2C%20and%20long-sequence%20modeling.%20Two%20extensions%20further%20expand%20its%20capacity%3A%20Complex%20Momentum%20Mamba%20for%20frequency-selective%20memory%20scaling.%20Experiments%20on%20multiple%20HAR%20benchmarks%20demonstrate%20consistent%20gains%20over%20vanilla%20Mamba%20and%20Transformer%20baselines%20in%20accuracy%2C%20robustness%2C%20and%20convergence%20speed.%20With%20only%20moderate%20increases%20in%20training%20cost%2C%20momentum-augmented%20SSMs%20offer%20a%20favorable%20accuracy-efficiency%20balance%2C%20establishing%20them%20as%20a%20scalable%20paradigm%20for%20HAR%20and%20a%20promising%20principal%20framework%20for%20broader%20sequence%20modeling%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.21550v1&entry.124074799=Read"},
{"title": "Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations", "author": "Tingkai Xue and Chin Chun Ooi and Zhengwei Ge and Fong Yew Leong and Hongying Li and Chang Wei Kang", "abstract": "Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.", "link": "http://arxiv.org/abs/2511.21369v1", "date": "2025-11-26", "relevancy": 2.1176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5493}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5291}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Physics-Neural%20Models%20enable%20Learning%20of%20Non-Markovian%20Closures%20for%20Accelerated%20Coarse-Grained%20Physics%20Simulations&body=Title%3A%20Differentiable%20Physics-Neural%20Models%20enable%20Learning%20of%20Non-Markovian%20Closures%20for%20Accelerated%20Coarse-Grained%20Physics%20Simulations%0AAuthor%3A%20Tingkai%20Xue%20and%20Chin%20Chun%20Ooi%20and%20Zhengwei%20Ge%20and%20Fong%20Yew%20Leong%20and%20Hongying%20Li%20and%20Chang%20Wei%20Kang%0AAbstract%3A%20Numerical%20simulations%20provide%20key%20insights%20into%20many%20physical%2C%20real-world%20problems.%20However%2C%20while%20these%20simulations%20are%20solved%20on%20a%20full%203D%20domain%2C%20most%20analysis%20only%20require%20a%20reduced%20set%20of%20metrics%20%28e.g.%20plane-level%20concentrations%29.%20This%20work%20presents%20a%20hybrid%20physics-neural%20model%20that%20predicts%20scalar%20transport%20in%20a%20complex%20domain%20orders%20of%20magnitude%20faster%20than%20the%203D%20simulation%20%28from%20hours%20to%20less%20than%201%20min%29.%20This%20end-to-end%20differentiable%20framework%20jointly%20learns%20the%20physical%20model%20parameterization%20%28i.e.%20orthotropic%20diffusivity%29%20and%20a%20non-Markovian%20neural%20closure%20model%20to%20capture%20unresolved%2C%20%27coarse-grained%27%20effects%2C%20thereby%20enabling%20stable%2C%20long%20time%20horizon%20rollouts.%20This%20proposed%20model%20is%20data-efficient%20%28learning%20with%2026%20training%20data%29%2C%20and%20can%20be%20flexibly%20extended%20to%20an%20out-of-distribution%20scenario%20%28with%20a%20moving%20source%29%2C%20achieving%20a%20Spearman%20correlation%20coefficient%20of%200.96%20at%20the%20final%20simulation%20time.%20Overall%20results%20show%20that%20this%20differentiable%20physics-neural%20framework%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20coarse-grained%20surrogates%20for%20physical%20phenomena.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Physics-Neural%2520Models%2520enable%2520Learning%2520of%2520Non-Markovian%2520Closures%2520for%2520Accelerated%2520Coarse-Grained%2520Physics%2520Simulations%26entry.906535625%3DTingkai%2520Xue%2520and%2520Chin%2520Chun%2520Ooi%2520and%2520Zhengwei%2520Ge%2520and%2520Fong%2520Yew%2520Leong%2520and%2520Hongying%2520Li%2520and%2520Chang%2520Wei%2520Kang%26entry.1292438233%3DNumerical%2520simulations%2520provide%2520key%2520insights%2520into%2520many%2520physical%252C%2520real-world%2520problems.%2520However%252C%2520while%2520these%2520simulations%2520are%2520solved%2520on%2520a%2520full%25203D%2520domain%252C%2520most%2520analysis%2520only%2520require%2520a%2520reduced%2520set%2520of%2520metrics%2520%2528e.g.%2520plane-level%2520concentrations%2529.%2520This%2520work%2520presents%2520a%2520hybrid%2520physics-neural%2520model%2520that%2520predicts%2520scalar%2520transport%2520in%2520a%2520complex%2520domain%2520orders%2520of%2520magnitude%2520faster%2520than%2520the%25203D%2520simulation%2520%2528from%2520hours%2520to%2520less%2520than%25201%2520min%2529.%2520This%2520end-to-end%2520differentiable%2520framework%2520jointly%2520learns%2520the%2520physical%2520model%2520parameterization%2520%2528i.e.%2520orthotropic%2520diffusivity%2529%2520and%2520a%2520non-Markovian%2520neural%2520closure%2520model%2520to%2520capture%2520unresolved%252C%2520%2527coarse-grained%2527%2520effects%252C%2520thereby%2520enabling%2520stable%252C%2520long%2520time%2520horizon%2520rollouts.%2520This%2520proposed%2520model%2520is%2520data-efficient%2520%2528learning%2520with%252026%2520training%2520data%2529%252C%2520and%2520can%2520be%2520flexibly%2520extended%2520to%2520an%2520out-of-distribution%2520scenario%2520%2528with%2520a%2520moving%2520source%2529%252C%2520achieving%2520a%2520Spearman%2520correlation%2520coefficient%2520of%25200.96%2520at%2520the%2520final%2520simulation%2520time.%2520Overall%2520results%2520show%2520that%2520this%2520differentiable%2520physics-neural%2520framework%2520enables%2520fast%252C%2520accurate%252C%2520and%2520generalizable%2520coarse-grained%2520surrogates%2520for%2520physical%2520phenomena.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Physics-Neural%20Models%20enable%20Learning%20of%20Non-Markovian%20Closures%20for%20Accelerated%20Coarse-Grained%20Physics%20Simulations&entry.906535625=Tingkai%20Xue%20and%20Chin%20Chun%20Ooi%20and%20Zhengwei%20Ge%20and%20Fong%20Yew%20Leong%20and%20Hongying%20Li%20and%20Chang%20Wei%20Kang&entry.1292438233=Numerical%20simulations%20provide%20key%20insights%20into%20many%20physical%2C%20real-world%20problems.%20However%2C%20while%20these%20simulations%20are%20solved%20on%20a%20full%203D%20domain%2C%20most%20analysis%20only%20require%20a%20reduced%20set%20of%20metrics%20%28e.g.%20plane-level%20concentrations%29.%20This%20work%20presents%20a%20hybrid%20physics-neural%20model%20that%20predicts%20scalar%20transport%20in%20a%20complex%20domain%20orders%20of%20magnitude%20faster%20than%20the%203D%20simulation%20%28from%20hours%20to%20less%20than%201%20min%29.%20This%20end-to-end%20differentiable%20framework%20jointly%20learns%20the%20physical%20model%20parameterization%20%28i.e.%20orthotropic%20diffusivity%29%20and%20a%20non-Markovian%20neural%20closure%20model%20to%20capture%20unresolved%2C%20%27coarse-grained%27%20effects%2C%20thereby%20enabling%20stable%2C%20long%20time%20horizon%20rollouts.%20This%20proposed%20model%20is%20data-efficient%20%28learning%20with%2026%20training%20data%29%2C%20and%20can%20be%20flexibly%20extended%20to%20an%20out-of-distribution%20scenario%20%28with%20a%20moving%20source%29%2C%20achieving%20a%20Spearman%20correlation%20coefficient%20of%200.96%20at%20the%20final%20simulation%20time.%20Overall%20results%20show%20that%20this%20differentiable%20physics-neural%20framework%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20coarse-grained%20surrogates%20for%20physical%20phenomena.&entry.1838667208=http%3A//arxiv.org/abs/2511.21369v1&entry.124074799=Read"},
{"title": "DWFF-Net : A Multi-Scale Farmland System Habitat Identification Method with Adaptive Dynamic Weight", "author": "Kesong Zheng and Zhi Song and Peizhou Li and Shuyi Yao and Zhenxing Bian", "abstract": "Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of the habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 69.79% and an F1-score of 80.49%, outperforming the baseline network by 2.1% and 1.61%, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes. (The complete code repository can be accessed via GitHub at the following URL: https://github.com/sysau/DWFF-Net)", "link": "http://arxiv.org/abs/2511.11659v2", "date": "2025-11-26", "relevancy": 2.1137, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5163}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DWFF-Net%20%3A%20A%20Multi-Scale%20Farmland%20System%20Habitat%20Identification%20Method%20with%20Adaptive%20Dynamic%20Weight&body=Title%3A%20DWFF-Net%20%3A%20A%20Multi-Scale%20Farmland%20System%20Habitat%20Identification%20Method%20with%20Adaptive%20Dynamic%20Weight%0AAuthor%3A%20Kesong%20Zheng%20and%20Zhi%20Song%20and%20Peizhou%20Li%20and%20Shuyi%20Yao%20and%20Zhenxing%20Bian%0AAbstract%3A%20Addressing%20the%20current%20lack%20of%20a%20standardized%20habitat%20classification%20system%20for%20cultivated%20land%20ecosystems%2C%20incomplete%20coverage%20of%20the%20habitat%20types%2C%20and%20the%20inability%20of%20existing%20models%20to%20effectively%20integrate%20semantic%20and%20texture%20features-resulting%20in%20insufficient%20segmentation%20accuracy%20and%20blurred%20boundaries%20for%20multi-scale%20habitats%20%28e.g.%2C%20large-scale%20field%20plots%20and%20micro-habitats%29-this%20study%20developed%20a%20comprehensively%20annotated%20ultra-high-resolution%20remote%20sensing%20image%20dataset%20encompassing%2015%20categories%20of%20cultivated%20land%20system%20habitats.%20Furthermore%2C%20we%20propose%20a%20Dynamic-Weighted%20Feature%20Fusion%20Network%20%28DWFF-Net%29.%20The%20encoder%20of%20this%20model%20utilizes%20a%20frozen-parameter%20DINOv3%20to%20extract%20foundational%20features.%20By%20analyzing%20the%20relationships%20between%20different%20category%20images%20and%20feature%20maps%2C%20we%20introduce%20a%20data-level%20adaptive%20dynamic%20weighting%20strategy%20for%20feature%20fusion.%20The%20decoder%20incorporates%20a%20dynamic%20weight%20computation%20network%20to%20achieve%20thorough%20integration%20of%20multi-layer%20features%2C%20and%20a%20hybrid%20loss%20function%20is%20adopted%20to%20optimize%20model%20training.%20Experimental%20results%20on%20the%20constructed%20dataset%20demonstrate%20that%20the%20proposed%20model%20achieves%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2069.79%25%20and%20an%20F1-score%20of%2080.49%25%2C%20outperforming%20the%20baseline%20network%20by%202.1%25%20and%201.61%25%2C%20respectively.%20Ablation%20studies%20further%20confirm%20the%20complementary%20nature%20of%20multi-layer%20feature%20fusion%2C%20which%20effectively%20improves%20the%20IoU%20for%20micro-habitat%20categories%20such%20as%20field%20ridges.%20This%20study%20establishes%20a%20habitat%20identification%20framework%20for%20cultivated%20land%20systems%20based%20on%20adaptive%20multi-layer%20feature%20fusion%2C%20enabling%20sub-meter%20precision%20habitat%20mapping%20at%20a%20low%20cost%20and%20providing%20robust%20technical%20support%20for%20fine-grained%20habitat%20monitoring%20in%20cultivated%20landscapes.%20%28The%20complete%20code%20repository%20can%20be%20accessed%20via%20GitHub%20at%20the%20following%20URL%3A%20https%3A//github.com/sysau/DWFF-Net%29%0ALink%3A%20http%3A//arxiv.org/abs/2511.11659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDWFF-Net%2520%253A%2520A%2520Multi-Scale%2520Farmland%2520System%2520Habitat%2520Identification%2520Method%2520with%2520Adaptive%2520Dynamic%2520Weight%26entry.906535625%3DKesong%2520Zheng%2520and%2520Zhi%2520Song%2520and%2520Peizhou%2520Li%2520and%2520Shuyi%2520Yao%2520and%2520Zhenxing%2520Bian%26entry.1292438233%3DAddressing%2520the%2520current%2520lack%2520of%2520a%2520standardized%2520habitat%2520classification%2520system%2520for%2520cultivated%2520land%2520ecosystems%252C%2520incomplete%2520coverage%2520of%2520the%2520habitat%2520types%252C%2520and%2520the%2520inability%2520of%2520existing%2520models%2520to%2520effectively%2520integrate%2520semantic%2520and%2520texture%2520features-resulting%2520in%2520insufficient%2520segmentation%2520accuracy%2520and%2520blurred%2520boundaries%2520for%2520multi-scale%2520habitats%2520%2528e.g.%252C%2520large-scale%2520field%2520plots%2520and%2520micro-habitats%2529-this%2520study%2520developed%2520a%2520comprehensively%2520annotated%2520ultra-high-resolution%2520remote%2520sensing%2520image%2520dataset%2520encompassing%252015%2520categories%2520of%2520cultivated%2520land%2520system%2520habitats.%2520Furthermore%252C%2520we%2520propose%2520a%2520Dynamic-Weighted%2520Feature%2520Fusion%2520Network%2520%2528DWFF-Net%2529.%2520The%2520encoder%2520of%2520this%2520model%2520utilizes%2520a%2520frozen-parameter%2520DINOv3%2520to%2520extract%2520foundational%2520features.%2520By%2520analyzing%2520the%2520relationships%2520between%2520different%2520category%2520images%2520and%2520feature%2520maps%252C%2520we%2520introduce%2520a%2520data-level%2520adaptive%2520dynamic%2520weighting%2520strategy%2520for%2520feature%2520fusion.%2520The%2520decoder%2520incorporates%2520a%2520dynamic%2520weight%2520computation%2520network%2520to%2520achieve%2520thorough%2520integration%2520of%2520multi-layer%2520features%252C%2520and%2520a%2520hybrid%2520loss%2520function%2520is%2520adopted%2520to%2520optimize%2520model%2520training.%2520Experimental%2520results%2520on%2520the%2520constructed%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520model%2520achieves%2520a%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252069.79%2525%2520and%2520an%2520F1-score%2520of%252080.49%2525%252C%2520outperforming%2520the%2520baseline%2520network%2520by%25202.1%2525%2520and%25201.61%2525%252C%2520respectively.%2520Ablation%2520studies%2520further%2520confirm%2520the%2520complementary%2520nature%2520of%2520multi-layer%2520feature%2520fusion%252C%2520which%2520effectively%2520improves%2520the%2520IoU%2520for%2520micro-habitat%2520categories%2520such%2520as%2520field%2520ridges.%2520This%2520study%2520establishes%2520a%2520habitat%2520identification%2520framework%2520for%2520cultivated%2520land%2520systems%2520based%2520on%2520adaptive%2520multi-layer%2520feature%2520fusion%252C%2520enabling%2520sub-meter%2520precision%2520habitat%2520mapping%2520at%2520a%2520low%2520cost%2520and%2520providing%2520robust%2520technical%2520support%2520for%2520fine-grained%2520habitat%2520monitoring%2520in%2520cultivated%2520landscapes.%2520%2528The%2520complete%2520code%2520repository%2520can%2520be%2520accessed%2520via%2520GitHub%2520at%2520the%2520following%2520URL%253A%2520https%253A//github.com/sysau/DWFF-Net%2529%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DWFF-Net%20%3A%20A%20Multi-Scale%20Farmland%20System%20Habitat%20Identification%20Method%20with%20Adaptive%20Dynamic%20Weight&entry.906535625=Kesong%20Zheng%20and%20Zhi%20Song%20and%20Peizhou%20Li%20and%20Shuyi%20Yao%20and%20Zhenxing%20Bian&entry.1292438233=Addressing%20the%20current%20lack%20of%20a%20standardized%20habitat%20classification%20system%20for%20cultivated%20land%20ecosystems%2C%20incomplete%20coverage%20of%20the%20habitat%20types%2C%20and%20the%20inability%20of%20existing%20models%20to%20effectively%20integrate%20semantic%20and%20texture%20features-resulting%20in%20insufficient%20segmentation%20accuracy%20and%20blurred%20boundaries%20for%20multi-scale%20habitats%20%28e.g.%2C%20large-scale%20field%20plots%20and%20micro-habitats%29-this%20study%20developed%20a%20comprehensively%20annotated%20ultra-high-resolution%20remote%20sensing%20image%20dataset%20encompassing%2015%20categories%20of%20cultivated%20land%20system%20habitats.%20Furthermore%2C%20we%20propose%20a%20Dynamic-Weighted%20Feature%20Fusion%20Network%20%28DWFF-Net%29.%20The%20encoder%20of%20this%20model%20utilizes%20a%20frozen-parameter%20DINOv3%20to%20extract%20foundational%20features.%20By%20analyzing%20the%20relationships%20between%20different%20category%20images%20and%20feature%20maps%2C%20we%20introduce%20a%20data-level%20adaptive%20dynamic%20weighting%20strategy%20for%20feature%20fusion.%20The%20decoder%20incorporates%20a%20dynamic%20weight%20computation%20network%20to%20achieve%20thorough%20integration%20of%20multi-layer%20features%2C%20and%20a%20hybrid%20loss%20function%20is%20adopted%20to%20optimize%20model%20training.%20Experimental%20results%20on%20the%20constructed%20dataset%20demonstrate%20that%20the%20proposed%20model%20achieves%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2069.79%25%20and%20an%20F1-score%20of%2080.49%25%2C%20outperforming%20the%20baseline%20network%20by%202.1%25%20and%201.61%25%2C%20respectively.%20Ablation%20studies%20further%20confirm%20the%20complementary%20nature%20of%20multi-layer%20feature%20fusion%2C%20which%20effectively%20improves%20the%20IoU%20for%20micro-habitat%20categories%20such%20as%20field%20ridges.%20This%20study%20establishes%20a%20habitat%20identification%20framework%20for%20cultivated%20land%20systems%20based%20on%20adaptive%20multi-layer%20feature%20fusion%2C%20enabling%20sub-meter%20precision%20habitat%20mapping%20at%20a%20low%20cost%20and%20providing%20robust%20technical%20support%20for%20fine-grained%20habitat%20monitoring%20in%20cultivated%20landscapes.%20%28The%20complete%20code%20repository%20can%20be%20accessed%20via%20GitHub%20at%20the%20following%20URL%3A%20https%3A//github.com/sysau/DWFF-Net%29&entry.1838667208=http%3A//arxiv.org/abs/2511.11659v2&entry.124074799=Read"},
{"title": "Self-Paced Learning for Images of Antinuclear Antibodies", "author": "Yiyang Jiang and Guangwu Qian and Jiaxin Wu and Qi Huang and Qing Li and Yongkang Wu and Xiao-Yong Wei", "abstract": "Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sj\u00f6gren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.", "link": "http://arxiv.org/abs/2511.21519v1", "date": "2025-11-26", "relevancy": 2.112, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5759}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Paced%20Learning%20for%20Images%20of%20Antinuclear%20Antibodies&body=Title%3A%20Self-Paced%20Learning%20for%20Images%20of%20Antinuclear%20Antibodies%0AAuthor%3A%20Yiyang%20Jiang%20and%20Guangwu%20Qian%20and%20Jiaxin%20Wu%20and%20Qi%20Huang%20and%20Qing%20Li%20and%20Yongkang%20Wu%20and%20Xiao-Yong%20Wei%0AAbstract%3A%20Antinuclear%20antibody%20%28ANA%29%20testing%20is%20a%20crucial%20method%20for%20diagnosing%20autoimmune%20disorders%2C%20including%20lupus%2C%20Sj%C3%B6gren%27s%20syndrome%2C%20and%20scleroderma.%20Despite%20its%20importance%2C%20manual%20ANA%20detection%20is%20slow%2C%20labor-intensive%2C%20and%20demands%20years%20of%20training.%20ANA%20detection%20is%20complicated%20by%20over%20100%20coexisting%20antibody%20types%2C%20resulting%20in%20vast%20fluorescent%20pattern%20combinations.%20Although%20machine%20learning%20and%20deep%20learning%20have%20enabled%20automation%2C%20ANA%20detection%20in%20real-world%20clinical%20settings%20presents%20unique%20challenges%20as%20it%20involves%20multi-instance%2C%20multi-label%20%28MIML%29%20learning.%20In%20this%20paper%2C%20a%20novel%20framework%20for%20ANA%20detection%20is%20proposed%20that%20handles%20the%20complexities%20of%20MIML%20tasks%20using%20unaltered%20microscope%20images%20without%20manual%20preprocessing.%20Inspired%20by%20human%20labeling%20logic%2C%20it%20identifies%20consistent%20ANA%20sub-regions%20and%20assigns%20aggregated%20labels%20accordingly.%20These%20steps%20are%20implemented%20using%20three%20task-specific%20components%3A%20an%20instance%20sampler%2C%20a%20probabilistic%20pseudo-label%20dispatcher%2C%20and%20self-paced%20weight%20learning%20rate%20coefficients.%20The%20instance%20sampler%20suppresses%20low-confidence%20instances%20by%20modeling%20pattern%20confidence%2C%20while%20the%20dispatcher%20adaptively%20assigns%20labels%20based%20on%20instance%20distinguishability.%20Self-paced%20learning%20adjusts%20training%20according%20to%20empirical%20label%20observations.%20Our%20framework%20overcomes%20limitations%20of%20traditional%20MIML%20methods%20and%20supports%20end-to-end%20optimization.%20Extensive%20experiments%20on%20one%20ANA%20dataset%20and%20three%20public%20medical%20MIML%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20framework.%20On%20the%20ANA%20dataset%2C%20our%20model%20achieves%20up%20to%20%2B7.0%25%20F1-Macro%20and%20%2B12.6%25%20mAP%20gains%20over%20the%20best%20prior%20method%2C%20setting%20new%20state-of-the-art%20results.%20It%20also%20ranks%20top-2%20across%20all%20key%20metrics%20on%20public%20datasets%2C%20reducing%20Hamming%20loss%20and%20one-error%20by%20up%20to%2018.2%25%20and%2026.9%25%2C%20respectively.%20The%20source%20code%20can%20be%20accessed%20at%20https%3A//github.com/fletcherjiang/ANA-SelfPacedLearning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Paced%2520Learning%2520for%2520Images%2520of%2520Antinuclear%2520Antibodies%26entry.906535625%3DYiyang%2520Jiang%2520and%2520Guangwu%2520Qian%2520and%2520Jiaxin%2520Wu%2520and%2520Qi%2520Huang%2520and%2520Qing%2520Li%2520and%2520Yongkang%2520Wu%2520and%2520Xiao-Yong%2520Wei%26entry.1292438233%3DAntinuclear%2520antibody%2520%2528ANA%2529%2520testing%2520is%2520a%2520crucial%2520method%2520for%2520diagnosing%2520autoimmune%2520disorders%252C%2520including%2520lupus%252C%2520Sj%25C3%25B6gren%2527s%2520syndrome%252C%2520and%2520scleroderma.%2520Despite%2520its%2520importance%252C%2520manual%2520ANA%2520detection%2520is%2520slow%252C%2520labor-intensive%252C%2520and%2520demands%2520years%2520of%2520training.%2520ANA%2520detection%2520is%2520complicated%2520by%2520over%2520100%2520coexisting%2520antibody%2520types%252C%2520resulting%2520in%2520vast%2520fluorescent%2520pattern%2520combinations.%2520Although%2520machine%2520learning%2520and%2520deep%2520learning%2520have%2520enabled%2520automation%252C%2520ANA%2520detection%2520in%2520real-world%2520clinical%2520settings%2520presents%2520unique%2520challenges%2520as%2520it%2520involves%2520multi-instance%252C%2520multi-label%2520%2528MIML%2529%2520learning.%2520In%2520this%2520paper%252C%2520a%2520novel%2520framework%2520for%2520ANA%2520detection%2520is%2520proposed%2520that%2520handles%2520the%2520complexities%2520of%2520MIML%2520tasks%2520using%2520unaltered%2520microscope%2520images%2520without%2520manual%2520preprocessing.%2520Inspired%2520by%2520human%2520labeling%2520logic%252C%2520it%2520identifies%2520consistent%2520ANA%2520sub-regions%2520and%2520assigns%2520aggregated%2520labels%2520accordingly.%2520These%2520steps%2520are%2520implemented%2520using%2520three%2520task-specific%2520components%253A%2520an%2520instance%2520sampler%252C%2520a%2520probabilistic%2520pseudo-label%2520dispatcher%252C%2520and%2520self-paced%2520weight%2520learning%2520rate%2520coefficients.%2520The%2520instance%2520sampler%2520suppresses%2520low-confidence%2520instances%2520by%2520modeling%2520pattern%2520confidence%252C%2520while%2520the%2520dispatcher%2520adaptively%2520assigns%2520labels%2520based%2520on%2520instance%2520distinguishability.%2520Self-paced%2520learning%2520adjusts%2520training%2520according%2520to%2520empirical%2520label%2520observations.%2520Our%2520framework%2520overcomes%2520limitations%2520of%2520traditional%2520MIML%2520methods%2520and%2520supports%2520end-to-end%2520optimization.%2520Extensive%2520experiments%2520on%2520one%2520ANA%2520dataset%2520and%2520three%2520public%2520medical%2520MIML%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520framework.%2520On%2520the%2520ANA%2520dataset%252C%2520our%2520model%2520achieves%2520up%2520to%2520%252B7.0%2525%2520F1-Macro%2520and%2520%252B12.6%2525%2520mAP%2520gains%2520over%2520the%2520best%2520prior%2520method%252C%2520setting%2520new%2520state-of-the-art%2520results.%2520It%2520also%2520ranks%2520top-2%2520across%2520all%2520key%2520metrics%2520on%2520public%2520datasets%252C%2520reducing%2520Hamming%2520loss%2520and%2520one-error%2520by%2520up%2520to%252018.2%2525%2520and%252026.9%2525%252C%2520respectively.%2520The%2520source%2520code%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/fletcherjiang/ANA-SelfPacedLearning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Paced%20Learning%20for%20Images%20of%20Antinuclear%20Antibodies&entry.906535625=Yiyang%20Jiang%20and%20Guangwu%20Qian%20and%20Jiaxin%20Wu%20and%20Qi%20Huang%20and%20Qing%20Li%20and%20Yongkang%20Wu%20and%20Xiao-Yong%20Wei&entry.1292438233=Antinuclear%20antibody%20%28ANA%29%20testing%20is%20a%20crucial%20method%20for%20diagnosing%20autoimmune%20disorders%2C%20including%20lupus%2C%20Sj%C3%B6gren%27s%20syndrome%2C%20and%20scleroderma.%20Despite%20its%20importance%2C%20manual%20ANA%20detection%20is%20slow%2C%20labor-intensive%2C%20and%20demands%20years%20of%20training.%20ANA%20detection%20is%20complicated%20by%20over%20100%20coexisting%20antibody%20types%2C%20resulting%20in%20vast%20fluorescent%20pattern%20combinations.%20Although%20machine%20learning%20and%20deep%20learning%20have%20enabled%20automation%2C%20ANA%20detection%20in%20real-world%20clinical%20settings%20presents%20unique%20challenges%20as%20it%20involves%20multi-instance%2C%20multi-label%20%28MIML%29%20learning.%20In%20this%20paper%2C%20a%20novel%20framework%20for%20ANA%20detection%20is%20proposed%20that%20handles%20the%20complexities%20of%20MIML%20tasks%20using%20unaltered%20microscope%20images%20without%20manual%20preprocessing.%20Inspired%20by%20human%20labeling%20logic%2C%20it%20identifies%20consistent%20ANA%20sub-regions%20and%20assigns%20aggregated%20labels%20accordingly.%20These%20steps%20are%20implemented%20using%20three%20task-specific%20components%3A%20an%20instance%20sampler%2C%20a%20probabilistic%20pseudo-label%20dispatcher%2C%20and%20self-paced%20weight%20learning%20rate%20coefficients.%20The%20instance%20sampler%20suppresses%20low-confidence%20instances%20by%20modeling%20pattern%20confidence%2C%20while%20the%20dispatcher%20adaptively%20assigns%20labels%20based%20on%20instance%20distinguishability.%20Self-paced%20learning%20adjusts%20training%20according%20to%20empirical%20label%20observations.%20Our%20framework%20overcomes%20limitations%20of%20traditional%20MIML%20methods%20and%20supports%20end-to-end%20optimization.%20Extensive%20experiments%20on%20one%20ANA%20dataset%20and%20three%20public%20medical%20MIML%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20framework.%20On%20the%20ANA%20dataset%2C%20our%20model%20achieves%20up%20to%20%2B7.0%25%20F1-Macro%20and%20%2B12.6%25%20mAP%20gains%20over%20the%20best%20prior%20method%2C%20setting%20new%20state-of-the-art%20results.%20It%20also%20ranks%20top-2%20across%20all%20key%20metrics%20on%20public%20datasets%2C%20reducing%20Hamming%20loss%20and%20one-error%20by%20up%20to%2018.2%25%20and%2026.9%25%2C%20respectively.%20The%20source%20code%20can%20be%20accessed%20at%20https%3A//github.com/fletcherjiang/ANA-SelfPacedLearning.&entry.1838667208=http%3A//arxiv.org/abs/2511.21519v1&entry.124074799=Read"},
{"title": "Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation", "author": "Hana Satou and Alan Mitkiy and Emma Collins and Finn Kingston", "abstract": "Transfer learning under domain shift remains a fundamental challenge due to the divergence between source and target data manifolds. In this paper, we propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework that decomposes adversarial perturbations into on-manifold and off-manifold components to simultaneously capture semantic variation and model brittleness. We theoretically demonstrate that enforcing on-manifold consistency reduces hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions. Moreover, we introduce a geometry-aware alignment loss that minimizes geodesic discrepancy between source and target manifolds. Experiments on DomainNet, VisDA, and Office-Home show that MAADA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, demonstrating superior structural robustness and cross-domain generalization.", "link": "http://arxiv.org/abs/2505.15191v2", "date": "2025-11-26", "relevancy": 2.1092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5183}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometrically%20Regularized%20Transfer%20Learning%20with%20On-Manifold%20and%20Off-Manifold%20Perturbation&body=Title%3A%20Geometrically%20Regularized%20Transfer%20Learning%20with%20On-Manifold%20and%20Off-Manifold%20Perturbation%0AAuthor%3A%20Hana%20Satou%20and%20Alan%20Mitkiy%20and%20Emma%20Collins%20and%20Finn%20Kingston%0AAbstract%3A%20Transfer%20learning%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20due%20to%20the%20divergence%20between%20source%20and%20target%20data%20manifolds.%20In%20this%20paper%2C%20we%20propose%20MAADA%20%28Manifold-Aware%20Adversarial%20Data%20Augmentation%29%2C%20a%20novel%20framework%20that%20decomposes%20adversarial%20perturbations%20into%20on-manifold%20and%20off-manifold%20components%20to%20simultaneously%20capture%20semantic%20variation%20and%20model%20brittleness.%20We%20theoretically%20demonstrate%20that%20enforcing%20on-manifold%20consistency%20reduces%20hypothesis%20complexity%20and%20improves%20generalization%2C%20while%20off-manifold%20regularization%20smooths%20decision%20boundaries%20in%20low-density%20regions.%20Moreover%2C%20we%20introduce%20a%20geometry-aware%20alignment%20loss%20that%20minimizes%20geodesic%20discrepancy%20between%20source%20and%20target%20manifolds.%20Experiments%20on%20DomainNet%2C%20VisDA%2C%20and%20Office-Home%20show%20that%20MAADA%20consistently%20outperforms%20existing%20adversarial%20and%20adaptation%20methods%20in%20both%20unsupervised%20and%20few-shot%20settings%2C%20demonstrating%20superior%20structural%20robustness%20and%20cross-domain%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometrically%2520Regularized%2520Transfer%2520Learning%2520with%2520On-Manifold%2520and%2520Off-Manifold%2520Perturbation%26entry.906535625%3DHana%2520Satou%2520and%2520Alan%2520Mitkiy%2520and%2520Emma%2520Collins%2520and%2520Finn%2520Kingston%26entry.1292438233%3DTransfer%2520learning%2520under%2520domain%2520shift%2520remains%2520a%2520fundamental%2520challenge%2520due%2520to%2520the%2520divergence%2520between%2520source%2520and%2520target%2520data%2520manifolds.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MAADA%2520%2528Manifold-Aware%2520Adversarial%2520Data%2520Augmentation%2529%252C%2520a%2520novel%2520framework%2520that%2520decomposes%2520adversarial%2520perturbations%2520into%2520on-manifold%2520and%2520off-manifold%2520components%2520to%2520simultaneously%2520capture%2520semantic%2520variation%2520and%2520model%2520brittleness.%2520We%2520theoretically%2520demonstrate%2520that%2520enforcing%2520on-manifold%2520consistency%2520reduces%2520hypothesis%2520complexity%2520and%2520improves%2520generalization%252C%2520while%2520off-manifold%2520regularization%2520smooths%2520decision%2520boundaries%2520in%2520low-density%2520regions.%2520Moreover%252C%2520we%2520introduce%2520a%2520geometry-aware%2520alignment%2520loss%2520that%2520minimizes%2520geodesic%2520discrepancy%2520between%2520source%2520and%2520target%2520manifolds.%2520Experiments%2520on%2520DomainNet%252C%2520VisDA%252C%2520and%2520Office-Home%2520show%2520that%2520MAADA%2520consistently%2520outperforms%2520existing%2520adversarial%2520and%2520adaptation%2520methods%2520in%2520both%2520unsupervised%2520and%2520few-shot%2520settings%252C%2520demonstrating%2520superior%2520structural%2520robustness%2520and%2520cross-domain%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometrically%20Regularized%20Transfer%20Learning%20with%20On-Manifold%20and%20Off-Manifold%20Perturbation&entry.906535625=Hana%20Satou%20and%20Alan%20Mitkiy%20and%20Emma%20Collins%20and%20Finn%20Kingston&entry.1292438233=Transfer%20learning%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20due%20to%20the%20divergence%20between%20source%20and%20target%20data%20manifolds.%20In%20this%20paper%2C%20we%20propose%20MAADA%20%28Manifold-Aware%20Adversarial%20Data%20Augmentation%29%2C%20a%20novel%20framework%20that%20decomposes%20adversarial%20perturbations%20into%20on-manifold%20and%20off-manifold%20components%20to%20simultaneously%20capture%20semantic%20variation%20and%20model%20brittleness.%20We%20theoretically%20demonstrate%20that%20enforcing%20on-manifold%20consistency%20reduces%20hypothesis%20complexity%20and%20improves%20generalization%2C%20while%20off-manifold%20regularization%20smooths%20decision%20boundaries%20in%20low-density%20regions.%20Moreover%2C%20we%20introduce%20a%20geometry-aware%20alignment%20loss%20that%20minimizes%20geodesic%20discrepancy%20between%20source%20and%20target%20manifolds.%20Experiments%20on%20DomainNet%2C%20VisDA%2C%20and%20Office-Home%20show%20that%20MAADA%20consistently%20outperforms%20existing%20adversarial%20and%20adaptation%20methods%20in%20both%20unsupervised%20and%20few-shot%20settings%2C%20demonstrating%20superior%20structural%20robustness%20and%20cross-domain%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2505.15191v2&entry.124074799=Read"},
{"title": "Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning", "author": "Alex Ning and Yen-Ling Kuo and Gabe Gomes", "abstract": "Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.", "link": "http://arxiv.org/abs/2511.21581v1", "date": "2025-11-26", "relevancy": 1.5414, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5366}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20When%20to%20Stop%3A%20Adaptive%20Latent%20Reasoning%20via%20Reinforcement%20Learning&body=Title%3A%20Learning%20When%20to%20Stop%3A%20Adaptive%20Latent%20Reasoning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Alex%20Ning%20and%20Yen-Ling%20Kuo%20and%20Gabe%20Gomes%0AAbstract%3A%20Latent%20reasoning%20represents%20a%20new%20development%20in%20Transformer%20language%20models%20that%20has%20shown%20potential%20in%20compressing%20reasoning%20lengths%20compared%20to%20chain-of-thought%20reasoning.%20By%20directly%20passing%20the%20information-rich%20previous%20final%20latent%20state%20into%20the%20next%20sequence%2C%20latent%20reasoning%20removes%20the%20restriction%20to%20human%20language%20tokens%20as%20the%20medium%20for%20reasoning.%20We%20develop%20adaptive-length%20latent%20reasoning%20models%20and%20introduce%20a%20post-SFT%20reinforcement-learning%20methodology%20to%20optimize%20latent%20reasoning%20length%20by%20minimizing%20reasoning%20length%20while%20maintaining%20accuracy.%20This%2C%20in%20turn%2C%20further%20reduces%20compute%20usage%20and%20raises%20the%20bar%20on%20the%20compressive%20capabilities%20of%20latent%20reasoning%20models.%20Experiments%20on%20the%20Llama%203.2%201B%20model%20and%20the%20GSM8K-Aug%20dataset%20show%20a%20%2452%5C%25%24%20drop%20in%20total%20reasoning%20length%20with%20no%20penalty%20to%20accuracy.%20In%20future%20work%2C%20we%20plan%20to%20extend%20to%20additional%20models%20and%20datasets%2C%20analyze%20relationships%20between%20training%20coefficients%2C%20experiment%20with%20architecture%20variations%2C%20and%20continue%20our%20knowledge%20distillation%20for%20latent%20reasoning%20SFT%20efforts.%20We%20make%20our%20code%20and%20pretrained%20weights%20available%20at%20https%3A//github.com/apning/adaptive-latent-reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520When%2520to%2520Stop%253A%2520Adaptive%2520Latent%2520Reasoning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DAlex%2520Ning%2520and%2520Yen-Ling%2520Kuo%2520and%2520Gabe%2520Gomes%26entry.1292438233%3DLatent%2520reasoning%2520represents%2520a%2520new%2520development%2520in%2520Transformer%2520language%2520models%2520that%2520has%2520shown%2520potential%2520in%2520compressing%2520reasoning%2520lengths%2520compared%2520to%2520chain-of-thought%2520reasoning.%2520By%2520directly%2520passing%2520the%2520information-rich%2520previous%2520final%2520latent%2520state%2520into%2520the%2520next%2520sequence%252C%2520latent%2520reasoning%2520removes%2520the%2520restriction%2520to%2520human%2520language%2520tokens%2520as%2520the%2520medium%2520for%2520reasoning.%2520We%2520develop%2520adaptive-length%2520latent%2520reasoning%2520models%2520and%2520introduce%2520a%2520post-SFT%2520reinforcement-learning%2520methodology%2520to%2520optimize%2520latent%2520reasoning%2520length%2520by%2520minimizing%2520reasoning%2520length%2520while%2520maintaining%2520accuracy.%2520This%252C%2520in%2520turn%252C%2520further%2520reduces%2520compute%2520usage%2520and%2520raises%2520the%2520bar%2520on%2520the%2520compressive%2520capabilities%2520of%2520latent%2520reasoning%2520models.%2520Experiments%2520on%2520the%2520Llama%25203.2%25201B%2520model%2520and%2520the%2520GSM8K-Aug%2520dataset%2520show%2520a%2520%252452%255C%2525%2524%2520drop%2520in%2520total%2520reasoning%2520length%2520with%2520no%2520penalty%2520to%2520accuracy.%2520In%2520future%2520work%252C%2520we%2520plan%2520to%2520extend%2520to%2520additional%2520models%2520and%2520datasets%252C%2520analyze%2520relationships%2520between%2520training%2520coefficients%252C%2520experiment%2520with%2520architecture%2520variations%252C%2520and%2520continue%2520our%2520knowledge%2520distillation%2520for%2520latent%2520reasoning%2520SFT%2520efforts.%2520We%2520make%2520our%2520code%2520and%2520pretrained%2520weights%2520available%2520at%2520https%253A//github.com/apning/adaptive-latent-reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20When%20to%20Stop%3A%20Adaptive%20Latent%20Reasoning%20via%20Reinforcement%20Learning&entry.906535625=Alex%20Ning%20and%20Yen-Ling%20Kuo%20and%20Gabe%20Gomes&entry.1292438233=Latent%20reasoning%20represents%20a%20new%20development%20in%20Transformer%20language%20models%20that%20has%20shown%20potential%20in%20compressing%20reasoning%20lengths%20compared%20to%20chain-of-thought%20reasoning.%20By%20directly%20passing%20the%20information-rich%20previous%20final%20latent%20state%20into%20the%20next%20sequence%2C%20latent%20reasoning%20removes%20the%20restriction%20to%20human%20language%20tokens%20as%20the%20medium%20for%20reasoning.%20We%20develop%20adaptive-length%20latent%20reasoning%20models%20and%20introduce%20a%20post-SFT%20reinforcement-learning%20methodology%20to%20optimize%20latent%20reasoning%20length%20by%20minimizing%20reasoning%20length%20while%20maintaining%20accuracy.%20This%2C%20in%20turn%2C%20further%20reduces%20compute%20usage%20and%20raises%20the%20bar%20on%20the%20compressive%20capabilities%20of%20latent%20reasoning%20models.%20Experiments%20on%20the%20Llama%203.2%201B%20model%20and%20the%20GSM8K-Aug%20dataset%20show%20a%20%2452%5C%25%24%20drop%20in%20total%20reasoning%20length%20with%20no%20penalty%20to%20accuracy.%20In%20future%20work%2C%20we%20plan%20to%20extend%20to%20additional%20models%20and%20datasets%2C%20analyze%20relationships%20between%20training%20coefficients%2C%20experiment%20with%20architecture%20variations%2C%20and%20continue%20our%20knowledge%20distillation%20for%20latent%20reasoning%20SFT%20efforts.%20We%20make%20our%20code%20and%20pretrained%20weights%20available%20at%20https%3A//github.com/apning/adaptive-latent-reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.21581v1&entry.124074799=Read"},
{"title": "AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs", "author": "Shuhan Xia and Peipei Li and Xuannan Liu and Dongsen Zhang and Xinyu Guo and Zekun Li", "abstract": "The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.", "link": "http://arxiv.org/abs/2511.21251v1", "date": "2025-11-26", "relevancy": 2.0135, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5324}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AVFakeBench%3A%20A%20Comprehensive%20Audio-Video%20Forgery%20Detection%20Benchmark%20for%20AV-LMMs&body=Title%3A%20AVFakeBench%3A%20A%20Comprehensive%20Audio-Video%20Forgery%20Detection%20Benchmark%20for%20AV-LMMs%0AAuthor%3A%20Shuhan%20Xia%20and%20Peipei%20Li%20and%20Xuannan%20Liu%20and%20Dongsen%20Zhang%20and%20Xinyu%20Guo%20and%20Zekun%20Li%0AAbstract%3A%20The%20threat%20of%20Audio-Video%20%28AV%29%20forgery%20is%20rapidly%20evolving%20beyond%20human-centric%20deepfakes%20to%20include%20more%20diverse%20manipulations%20across%20complex%20natural%20scenes.%20However%2C%20existing%20benchmarks%20are%20still%20confined%20to%20DeepFake-based%20forgeries%20and%20single-granularity%20annotations%2C%20thus%20failing%20to%20capture%20the%20diversity%20and%20complexity%20of%20real-world%20forgery%20scenarios.%20To%20address%20this%2C%20we%20introduce%20AVFakeBench%2C%20the%20first%20comprehensive%20audio-video%20forgery%20detection%20benchmark%20that%20spans%20rich%20forgery%20semantics%20across%20both%20human%20subject%20and%20general%20subject.%20AVFakeBench%20comprises%2012K%20carefully%20curated%20audio-video%20questions%2C%20covering%20seven%20forgery%20types%20and%20four%20levels%20of%20annotations.%20To%20ensure%20high-quality%20and%20diverse%20forgeries%2C%20we%20propose%20a%20multi-stage%20hybrid%20forgery%20framework%20that%20integrates%20proprietary%20models%20for%20task%20planning%20with%20expert%20generative%20models%20for%20precise%20manipulation.%20The%20benchmark%20establishes%20a%20multi-task%20evaluation%20framework%20covering%20binary%20judgment%2C%20forgery%20types%20classification%2C%20forgery%20detail%20selection%2C%20and%20explanatory%20reasoning.%20We%20evaluate%2011%20Audio-Video%20Large%20Language%20Models%20%28AV-LMMs%29%20and%202%20prevalent%20detection%20methods%20on%20AVFakeBench%2C%20demonstrating%20the%20potential%20of%20AV-LMMs%20as%20emerging%20forgery%20detectors%20while%20revealing%20their%20notable%20weaknesses%20in%20fine-grained%20perception%20and%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAVFakeBench%253A%2520A%2520Comprehensive%2520Audio-Video%2520Forgery%2520Detection%2520Benchmark%2520for%2520AV-LMMs%26entry.906535625%3DShuhan%2520Xia%2520and%2520Peipei%2520Li%2520and%2520Xuannan%2520Liu%2520and%2520Dongsen%2520Zhang%2520and%2520Xinyu%2520Guo%2520and%2520Zekun%2520Li%26entry.1292438233%3DThe%2520threat%2520of%2520Audio-Video%2520%2528AV%2529%2520forgery%2520is%2520rapidly%2520evolving%2520beyond%2520human-centric%2520deepfakes%2520to%2520include%2520more%2520diverse%2520manipulations%2520across%2520complex%2520natural%2520scenes.%2520However%252C%2520existing%2520benchmarks%2520are%2520still%2520confined%2520to%2520DeepFake-based%2520forgeries%2520and%2520single-granularity%2520annotations%252C%2520thus%2520failing%2520to%2520capture%2520the%2520diversity%2520and%2520complexity%2520of%2520real-world%2520forgery%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520introduce%2520AVFakeBench%252C%2520the%2520first%2520comprehensive%2520audio-video%2520forgery%2520detection%2520benchmark%2520that%2520spans%2520rich%2520forgery%2520semantics%2520across%2520both%2520human%2520subject%2520and%2520general%2520subject.%2520AVFakeBench%2520comprises%252012K%2520carefully%2520curated%2520audio-video%2520questions%252C%2520covering%2520seven%2520forgery%2520types%2520and%2520four%2520levels%2520of%2520annotations.%2520To%2520ensure%2520high-quality%2520and%2520diverse%2520forgeries%252C%2520we%2520propose%2520a%2520multi-stage%2520hybrid%2520forgery%2520framework%2520that%2520integrates%2520proprietary%2520models%2520for%2520task%2520planning%2520with%2520expert%2520generative%2520models%2520for%2520precise%2520manipulation.%2520The%2520benchmark%2520establishes%2520a%2520multi-task%2520evaluation%2520framework%2520covering%2520binary%2520judgment%252C%2520forgery%2520types%2520classification%252C%2520forgery%2520detail%2520selection%252C%2520and%2520explanatory%2520reasoning.%2520We%2520evaluate%252011%2520Audio-Video%2520Large%2520Language%2520Models%2520%2528AV-LMMs%2529%2520and%25202%2520prevalent%2520detection%2520methods%2520on%2520AVFakeBench%252C%2520demonstrating%2520the%2520potential%2520of%2520AV-LMMs%2520as%2520emerging%2520forgery%2520detectors%2520while%2520revealing%2520their%2520notable%2520weaknesses%2520in%2520fine-grained%2520perception%2520and%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AVFakeBench%3A%20A%20Comprehensive%20Audio-Video%20Forgery%20Detection%20Benchmark%20for%20AV-LMMs&entry.906535625=Shuhan%20Xia%20and%20Peipei%20Li%20and%20Xuannan%20Liu%20and%20Dongsen%20Zhang%20and%20Xinyu%20Guo%20and%20Zekun%20Li&entry.1292438233=The%20threat%20of%20Audio-Video%20%28AV%29%20forgery%20is%20rapidly%20evolving%20beyond%20human-centric%20deepfakes%20to%20include%20more%20diverse%20manipulations%20across%20complex%20natural%20scenes.%20However%2C%20existing%20benchmarks%20are%20still%20confined%20to%20DeepFake-based%20forgeries%20and%20single-granularity%20annotations%2C%20thus%20failing%20to%20capture%20the%20diversity%20and%20complexity%20of%20real-world%20forgery%20scenarios.%20To%20address%20this%2C%20we%20introduce%20AVFakeBench%2C%20the%20first%20comprehensive%20audio-video%20forgery%20detection%20benchmark%20that%20spans%20rich%20forgery%20semantics%20across%20both%20human%20subject%20and%20general%20subject.%20AVFakeBench%20comprises%2012K%20carefully%20curated%20audio-video%20questions%2C%20covering%20seven%20forgery%20types%20and%20four%20levels%20of%20annotations.%20To%20ensure%20high-quality%20and%20diverse%20forgeries%2C%20we%20propose%20a%20multi-stage%20hybrid%20forgery%20framework%20that%20integrates%20proprietary%20models%20for%20task%20planning%20with%20expert%20generative%20models%20for%20precise%20manipulation.%20The%20benchmark%20establishes%20a%20multi-task%20evaluation%20framework%20covering%20binary%20judgment%2C%20forgery%20types%20classification%2C%20forgery%20detail%20selection%2C%20and%20explanatory%20reasoning.%20We%20evaluate%2011%20Audio-Video%20Large%20Language%20Models%20%28AV-LMMs%29%20and%202%20prevalent%20detection%20methods%20on%20AVFakeBench%2C%20demonstrating%20the%20potential%20of%20AV-LMMs%20as%20emerging%20forgery%20detectors%20while%20revealing%20their%20notable%20weaknesses%20in%20fine-grained%20perception%20and%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.21251v1&entry.124074799=Read"},
{"title": "Prune4Web: DOM Tree Pruning Programming for Web Agent", "author": "Jiayuan Zhang and Kaiquan Chen and Zhihao Lu and Enshen Zhou and Qian Yu and Jing Zhang", "abstract": "Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.", "link": "http://arxiv.org/abs/2511.21398v1", "date": "2025-11-26", "relevancy": 1.8898, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prune4Web%3A%20DOM%20Tree%20Pruning%20Programming%20for%20Web%20Agent&body=Title%3A%20Prune4Web%3A%20DOM%20Tree%20Pruning%20Programming%20for%20Web%20Agent%0AAuthor%3A%20Jiayuan%20Zhang%20and%20Kaiquan%20Chen%20and%20Zhihao%20Lu%20and%20Enshen%20Zhou%20and%20Qian%20Yu%20and%20Jing%20Zhang%0AAbstract%3A%20Web%20automation%20employs%20intelligent%20agents%20to%20execute%20high-level%20tasks%20by%20mimicking%20human%20interactions%20with%20web%20interfaces.%20Despite%20the%20capabilities%20of%20recent%20Large%20Language%20Model%20%28LLM%29-based%20web%20agents%2C%20navigating%20complex%2C%20real-world%20webpages%20efficiently%20remains%20a%20significant%20hurdle%20due%20to%20the%20prohibitively%20large%20size%20of%20Document%20Object%20Model%20%28DOM%29%20structures%2C%20often%20ranging%20from%2010%2C000%20to%20100%2C000%20tokens.%20Existing%20strategies%20typically%20rely%20on%20crude%20DOM%20truncation%20--%20risking%20the%20loss%20of%20critical%20information%20--%20or%20employ%20inefficient%20heuristics%20and%20separate%20ranking%20models%2C%20failing%20to%20achieve%20an%20optimal%20balance%20between%20precision%20and%20scalability.%20To%20address%20these%20challenges%2C%20we%20introduce%20Prune4Web%2C%20a%20novel%20paradigm%20that%20shifts%20DOM%20processing%20from%20resource-intensive%20LLM%20reading%20to%20efficient%20programmatic%20pruning.%20Central%20to%20our%20approach%20is%20DOM%20Tree%20Pruning%20Programming%2C%20where%20an%20LLM%20generates%20executable%20Python%20scoring%20scripts%20to%20dynamically%20filter%20DOM%20elements%20based%20on%20semantic%20cues%20from%20decomposed%20sub-tasks.%20This%20mechanism%20eliminates%20the%20need%20for%20LLMs%20to%20ingest%20raw%2C%20massive%20DOMs%2C%20instead%20delegating%20traversal%20and%20scoring%20to%20lightweight%2C%20interpretable%20programs.%20This%20methodology%20achieves%20a%2025x%20to%2050x%20reduction%20in%20candidate%20elements%20for%20grounding%2C%20thereby%20facilitating%20precise%20action%20localization%20while%20mitigating%20attention%20dilution.%20Furthermore%2C%20we%20propose%20a%20specialized%20data%20annotation%20pipeline%20and%20a%20two-turn%20dialogue%20training%20strategy%20that%20jointly%20optimizes%20the%20Planner%2C%20Programmatic%20Filter%2C%20and%20Grounder%20within%20a%20unified%20framework.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance.%20Notably%2C%20on%20our%20low-level%20grounding%20task%2C%20Prune4Web%20dramatically%20improves%20accuracy%20from%2046.8%25%20to%2088.28%25%2C%20underscoring%20its%20efficacy%20in%20real-world%20web%20automation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrune4Web%253A%2520DOM%2520Tree%2520Pruning%2520Programming%2520for%2520Web%2520Agent%26entry.906535625%3DJiayuan%2520Zhang%2520and%2520Kaiquan%2520Chen%2520and%2520Zhihao%2520Lu%2520and%2520Enshen%2520Zhou%2520and%2520Qian%2520Yu%2520and%2520Jing%2520Zhang%26entry.1292438233%3DWeb%2520automation%2520employs%2520intelligent%2520agents%2520to%2520execute%2520high-level%2520tasks%2520by%2520mimicking%2520human%2520interactions%2520with%2520web%2520interfaces.%2520Despite%2520the%2520capabilities%2520of%2520recent%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520web%2520agents%252C%2520navigating%2520complex%252C%2520real-world%2520webpages%2520efficiently%2520remains%2520a%2520significant%2520hurdle%2520due%2520to%2520the%2520prohibitively%2520large%2520size%2520of%2520Document%2520Object%2520Model%2520%2528DOM%2529%2520structures%252C%2520often%2520ranging%2520from%252010%252C000%2520to%2520100%252C000%2520tokens.%2520Existing%2520strategies%2520typically%2520rely%2520on%2520crude%2520DOM%2520truncation%2520--%2520risking%2520the%2520loss%2520of%2520critical%2520information%2520--%2520or%2520employ%2520inefficient%2520heuristics%2520and%2520separate%2520ranking%2520models%252C%2520failing%2520to%2520achieve%2520an%2520optimal%2520balance%2520between%2520precision%2520and%2520scalability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Prune4Web%252C%2520a%2520novel%2520paradigm%2520that%2520shifts%2520DOM%2520processing%2520from%2520resource-intensive%2520LLM%2520reading%2520to%2520efficient%2520programmatic%2520pruning.%2520Central%2520to%2520our%2520approach%2520is%2520DOM%2520Tree%2520Pruning%2520Programming%252C%2520where%2520an%2520LLM%2520generates%2520executable%2520Python%2520scoring%2520scripts%2520to%2520dynamically%2520filter%2520DOM%2520elements%2520based%2520on%2520semantic%2520cues%2520from%2520decomposed%2520sub-tasks.%2520This%2520mechanism%2520eliminates%2520the%2520need%2520for%2520LLMs%2520to%2520ingest%2520raw%252C%2520massive%2520DOMs%252C%2520instead%2520delegating%2520traversal%2520and%2520scoring%2520to%2520lightweight%252C%2520interpretable%2520programs.%2520This%2520methodology%2520achieves%2520a%252025x%2520to%252050x%2520reduction%2520in%2520candidate%2520elements%2520for%2520grounding%252C%2520thereby%2520facilitating%2520precise%2520action%2520localization%2520while%2520mitigating%2520attention%2520dilution.%2520Furthermore%252C%2520we%2520propose%2520a%2520specialized%2520data%2520annotation%2520pipeline%2520and%2520a%2520two-turn%2520dialogue%2520training%2520strategy%2520that%2520jointly%2520optimizes%2520the%2520Planner%252C%2520Programmatic%2520Filter%252C%2520and%2520Grounder%2520within%2520a%2520unified%2520framework.%2520Extensive%2520experiments%2520demonstrate%2520state-of-the-art%2520performance.%2520Notably%252C%2520on%2520our%2520low-level%2520grounding%2520task%252C%2520Prune4Web%2520dramatically%2520improves%2520accuracy%2520from%252046.8%2525%2520to%252088.28%2525%252C%2520underscoring%2520its%2520efficacy%2520in%2520real-world%2520web%2520automation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prune4Web%3A%20DOM%20Tree%20Pruning%20Programming%20for%20Web%20Agent&entry.906535625=Jiayuan%20Zhang%20and%20Kaiquan%20Chen%20and%20Zhihao%20Lu%20and%20Enshen%20Zhou%20and%20Qian%20Yu%20and%20Jing%20Zhang&entry.1292438233=Web%20automation%20employs%20intelligent%20agents%20to%20execute%20high-level%20tasks%20by%20mimicking%20human%20interactions%20with%20web%20interfaces.%20Despite%20the%20capabilities%20of%20recent%20Large%20Language%20Model%20%28LLM%29-based%20web%20agents%2C%20navigating%20complex%2C%20real-world%20webpages%20efficiently%20remains%20a%20significant%20hurdle%20due%20to%20the%20prohibitively%20large%20size%20of%20Document%20Object%20Model%20%28DOM%29%20structures%2C%20often%20ranging%20from%2010%2C000%20to%20100%2C000%20tokens.%20Existing%20strategies%20typically%20rely%20on%20crude%20DOM%20truncation%20--%20risking%20the%20loss%20of%20critical%20information%20--%20or%20employ%20inefficient%20heuristics%20and%20separate%20ranking%20models%2C%20failing%20to%20achieve%20an%20optimal%20balance%20between%20precision%20and%20scalability.%20To%20address%20these%20challenges%2C%20we%20introduce%20Prune4Web%2C%20a%20novel%20paradigm%20that%20shifts%20DOM%20processing%20from%20resource-intensive%20LLM%20reading%20to%20efficient%20programmatic%20pruning.%20Central%20to%20our%20approach%20is%20DOM%20Tree%20Pruning%20Programming%2C%20where%20an%20LLM%20generates%20executable%20Python%20scoring%20scripts%20to%20dynamically%20filter%20DOM%20elements%20based%20on%20semantic%20cues%20from%20decomposed%20sub-tasks.%20This%20mechanism%20eliminates%20the%20need%20for%20LLMs%20to%20ingest%20raw%2C%20massive%20DOMs%2C%20instead%20delegating%20traversal%20and%20scoring%20to%20lightweight%2C%20interpretable%20programs.%20This%20methodology%20achieves%20a%2025x%20to%2050x%20reduction%20in%20candidate%20elements%20for%20grounding%2C%20thereby%20facilitating%20precise%20action%20localization%20while%20mitigating%20attention%20dilution.%20Furthermore%2C%20we%20propose%20a%20specialized%20data%20annotation%20pipeline%20and%20a%20two-turn%20dialogue%20training%20strategy%20that%20jointly%20optimizes%20the%20Planner%2C%20Programmatic%20Filter%2C%20and%20Grounder%20within%20a%20unified%20framework.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance.%20Notably%2C%20on%20our%20low-level%20grounding%20task%2C%20Prune4Web%20dramatically%20improves%20accuracy%20from%2046.8%25%20to%2088.28%25%2C%20underscoring%20its%20efficacy%20in%20real-world%20web%20automation.&entry.1838667208=http%3A//arxiv.org/abs/2511.21398v1&entry.124074799=Read"},
{"title": "Alignment of large language models with constrained learning", "author": "Botong Zhang and Shuo Li and Ignacio Hounie and Osbert Bastani and Dongsheng Ding and Alejandro Ribeiro", "abstract": "We study the problem of computing an optimal large language model (LLM) policy for the constrained alignment problem, where the goal is to maximize a primary reward objective while satisfying constraints on secondary utilities. Despite the popularity of Lagrangian-based LLM policy search in constrained alignment, iterative primal-dual methods often fail to converge, and non-iterative dual-based methods do not achieve optimality in the LLM parameter space. To address these challenges, we employ Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent. In theory, we characterize the primal-dual gap between the primal value in the distribution space and the dual value in the LLM parameter space. We further quantify the optimality gap of the learned LLM policies at near-optimal dual variables with respect to both the objective and the constraint functions. These results prove that dual-based alignment methods can find an optimal constrained LLM policy, up to an LLM parametrization gap. We demonstrate the effectiveness and merits of our approach through extensive experiments conducted on the PKU-SafeRLHF and Anthropic HH-RLHF datasets.", "link": "http://arxiv.org/abs/2505.19387v2", "date": "2025-11-26", "relevancy": 1.8948, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4864}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20of%20large%20language%20models%20with%20constrained%20learning&body=Title%3A%20Alignment%20of%20large%20language%20models%20with%20constrained%20learning%0AAuthor%3A%20Botong%20Zhang%20and%20Shuo%20Li%20and%20Ignacio%20Hounie%20and%20Osbert%20Bastani%20and%20Dongsheng%20Ding%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20We%20study%20the%20problem%20of%20computing%20an%20optimal%20large%20language%20model%20%28LLM%29%20policy%20for%20the%20constrained%20alignment%20problem%2C%20where%20the%20goal%20is%20to%20maximize%20a%20primary%20reward%20objective%20while%20satisfying%20constraints%20on%20secondary%20utilities.%20Despite%20the%20popularity%20of%20Lagrangian-based%20LLM%20policy%20search%20in%20constrained%20alignment%2C%20iterative%20primal-dual%20methods%20often%20fail%20to%20converge%2C%20and%20non-iterative%20dual-based%20methods%20do%20not%20achieve%20optimality%20in%20the%20LLM%20parameter%20space.%20To%20address%20these%20challenges%2C%20we%20employ%20Lagrangian%20duality%20to%20develop%20an%20iterative%20dual-based%20alignment%20method%20that%20alternates%20between%20updating%20the%20LLM%20policy%20via%20Lagrangian%20maximization%20and%20updating%20the%20dual%20variable%20via%20dual%20descent.%20In%20theory%2C%20we%20characterize%20the%20primal-dual%20gap%20between%20the%20primal%20value%20in%20the%20distribution%20space%20and%20the%20dual%20value%20in%20the%20LLM%20parameter%20space.%20We%20further%20quantify%20the%20optimality%20gap%20of%20the%20learned%20LLM%20policies%20at%20near-optimal%20dual%20variables%20with%20respect%20to%20both%20the%20objective%20and%20the%20constraint%20functions.%20These%20results%20prove%20that%20dual-based%20alignment%20methods%20can%20find%20an%20optimal%20constrained%20LLM%20policy%2C%20up%20to%20an%20LLM%20parametrization%20gap.%20We%20demonstrate%20the%20effectiveness%20and%20merits%20of%20our%20approach%20through%20extensive%20experiments%20conducted%20on%20the%20PKU-SafeRLHF%20and%20Anthropic%20HH-RLHF%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520of%2520large%2520language%2520models%2520with%2520constrained%2520learning%26entry.906535625%3DBotong%2520Zhang%2520and%2520Shuo%2520Li%2520and%2520Ignacio%2520Hounie%2520and%2520Osbert%2520Bastani%2520and%2520Dongsheng%2520Ding%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3DWe%2520study%2520the%2520problem%2520of%2520computing%2520an%2520optimal%2520large%2520language%2520model%2520%2528LLM%2529%2520policy%2520for%2520the%2520constrained%2520alignment%2520problem%252C%2520where%2520the%2520goal%2520is%2520to%2520maximize%2520a%2520primary%2520reward%2520objective%2520while%2520satisfying%2520constraints%2520on%2520secondary%2520utilities.%2520Despite%2520the%2520popularity%2520of%2520Lagrangian-based%2520LLM%2520policy%2520search%2520in%2520constrained%2520alignment%252C%2520iterative%2520primal-dual%2520methods%2520often%2520fail%2520to%2520converge%252C%2520and%2520non-iterative%2520dual-based%2520methods%2520do%2520not%2520achieve%2520optimality%2520in%2520the%2520LLM%2520parameter%2520space.%2520To%2520address%2520these%2520challenges%252C%2520we%2520employ%2520Lagrangian%2520duality%2520to%2520develop%2520an%2520iterative%2520dual-based%2520alignment%2520method%2520that%2520alternates%2520between%2520updating%2520the%2520LLM%2520policy%2520via%2520Lagrangian%2520maximization%2520and%2520updating%2520the%2520dual%2520variable%2520via%2520dual%2520descent.%2520In%2520theory%252C%2520we%2520characterize%2520the%2520primal-dual%2520gap%2520between%2520the%2520primal%2520value%2520in%2520the%2520distribution%2520space%2520and%2520the%2520dual%2520value%2520in%2520the%2520LLM%2520parameter%2520space.%2520We%2520further%2520quantify%2520the%2520optimality%2520gap%2520of%2520the%2520learned%2520LLM%2520policies%2520at%2520near-optimal%2520dual%2520variables%2520with%2520respect%2520to%2520both%2520the%2520objective%2520and%2520the%2520constraint%2520functions.%2520These%2520results%2520prove%2520that%2520dual-based%2520alignment%2520methods%2520can%2520find%2520an%2520optimal%2520constrained%2520LLM%2520policy%252C%2520up%2520to%2520an%2520LLM%2520parametrization%2520gap.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520merits%2520of%2520our%2520approach%2520through%2520extensive%2520experiments%2520conducted%2520on%2520the%2520PKU-SafeRLHF%2520and%2520Anthropic%2520HH-RLHF%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20of%20large%20language%20models%20with%20constrained%20learning&entry.906535625=Botong%20Zhang%20and%20Shuo%20Li%20and%20Ignacio%20Hounie%20and%20Osbert%20Bastani%20and%20Dongsheng%20Ding%20and%20Alejandro%20Ribeiro&entry.1292438233=We%20study%20the%20problem%20of%20computing%20an%20optimal%20large%20language%20model%20%28LLM%29%20policy%20for%20the%20constrained%20alignment%20problem%2C%20where%20the%20goal%20is%20to%20maximize%20a%20primary%20reward%20objective%20while%20satisfying%20constraints%20on%20secondary%20utilities.%20Despite%20the%20popularity%20of%20Lagrangian-based%20LLM%20policy%20search%20in%20constrained%20alignment%2C%20iterative%20primal-dual%20methods%20often%20fail%20to%20converge%2C%20and%20non-iterative%20dual-based%20methods%20do%20not%20achieve%20optimality%20in%20the%20LLM%20parameter%20space.%20To%20address%20these%20challenges%2C%20we%20employ%20Lagrangian%20duality%20to%20develop%20an%20iterative%20dual-based%20alignment%20method%20that%20alternates%20between%20updating%20the%20LLM%20policy%20via%20Lagrangian%20maximization%20and%20updating%20the%20dual%20variable%20via%20dual%20descent.%20In%20theory%2C%20we%20characterize%20the%20primal-dual%20gap%20between%20the%20primal%20value%20in%20the%20distribution%20space%20and%20the%20dual%20value%20in%20the%20LLM%20parameter%20space.%20We%20further%20quantify%20the%20optimality%20gap%20of%20the%20learned%20LLM%20policies%20at%20near-optimal%20dual%20variables%20with%20respect%20to%20both%20the%20objective%20and%20the%20constraint%20functions.%20These%20results%20prove%20that%20dual-based%20alignment%20methods%20can%20find%20an%20optimal%20constrained%20LLM%20policy%2C%20up%20to%20an%20LLM%20parametrization%20gap.%20We%20demonstrate%20the%20effectiveness%20and%20merits%20of%20our%20approach%20through%20extensive%20experiments%20conducted%20on%20the%20PKU-SafeRLHF%20and%20Anthropic%20HH-RLHF%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2505.19387v2&entry.124074799=Read"},
{"title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction", "author": "Guoxiao Zhang and Tan Qu and Ao Li and DongLin Ni and Qianlong Xie and Xingxing Wang", "abstract": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.", "link": "http://arxiv.org/abs/2511.21394v1", "date": "2025-11-26", "relevancy": 1.8353, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4782}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4711}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIA%3A%20A%20Ranking-Infused%20Approach%20for%20Optimized%20listwise%20CTR%20Prediction&body=Title%3A%20RIA%3A%20A%20Ranking-Infused%20Approach%20for%20Optimized%20listwise%20CTR%20Prediction%0AAuthor%3A%20Guoxiao%20Zhang%20and%20Tan%20Qu%20and%20Ao%20Li%20and%20DongLin%20Ni%20and%20Qianlong%20Xie%20and%20Xingxing%20Wang%0AAbstract%3A%20Reranking%20improves%20recommendation%20quality%20by%20modeling%20item%20interactions.%20However%2C%20existing%20methods%20often%20decouple%20ranking%20and%20reranking%2C%20leading%20to%20weak%20listwise%20evaluation%20models%20that%20suffer%20from%20combinatorial%20sparsity%20and%20limited%20representational%20power%20under%20strict%20latency%20constraints.%20In%20this%20paper%2C%20we%20propose%20RIA%20%28Ranking-Infused%20Architecture%29%2C%20a%20unified%2C%20end-to-end%20framework%20that%20seamlessly%20integrates%20pointwise%20and%20listwise%20evaluation.%20RIA%20introduces%20four%20key%20components%3A%20%281%29%20the%20User%20and%20Candidate%20DualTransformer%20%28UCDT%29%20for%20fine-grained%20user-item-context%20modeling%3B%20%282%29%20the%20Context-aware%20User%20History%20and%20Target%20%28CUHT%29%20module%20for%20position-sensitive%20preference%20learning%3B%20%283%29%20the%20Listwise%20Multi-HSTU%20%28LMH%29%20module%20to%20capture%20hierarchical%20item%20dependencies%3B%20and%20%284%29%20the%20Embedding%20Cache%20%28EC%29%20module%20to%20bridge%20efficiency%20and%20effectiveness%20during%20inference.%20By%20sharing%20representations%20across%20ranking%20and%20reranking%2C%20RIA%20enables%20rich%20contextual%20knowledge%20transfer%20while%20maintaining%20low%20latency.%20Extensive%20experiments%20show%20that%20RIA%20outperforms%20state-of-the-art%20models%20on%20both%20public%20and%20industrial%20datasets%2C%20achieving%20significant%20gains%20in%20AUC%20and%20LogLoss.%20Deployed%20in%20Meituan%20advertising%20system%2C%20RIA%20yields%20a%20%2B1.69%25%20improvement%20in%20Click-Through%20Rate%20%28CTR%29%20and%20a%20%2B4.54%25%20increase%20in%20Cost%20Per%20Mille%20%28CPM%29%20in%20online%20A/B%20tests.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIA%253A%2520A%2520Ranking-Infused%2520Approach%2520for%2520Optimized%2520listwise%2520CTR%2520Prediction%26entry.906535625%3DGuoxiao%2520Zhang%2520and%2520Tan%2520Qu%2520and%2520Ao%2520Li%2520and%2520DongLin%2520Ni%2520and%2520Qianlong%2520Xie%2520and%2520Xingxing%2520Wang%26entry.1292438233%3DReranking%2520improves%2520recommendation%2520quality%2520by%2520modeling%2520item%2520interactions.%2520However%252C%2520existing%2520methods%2520often%2520decouple%2520ranking%2520and%2520reranking%252C%2520leading%2520to%2520weak%2520listwise%2520evaluation%2520models%2520that%2520suffer%2520from%2520combinatorial%2520sparsity%2520and%2520limited%2520representational%2520power%2520under%2520strict%2520latency%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RIA%2520%2528Ranking-Infused%2520Architecture%2529%252C%2520a%2520unified%252C%2520end-to-end%2520framework%2520that%2520seamlessly%2520integrates%2520pointwise%2520and%2520listwise%2520evaluation.%2520RIA%2520introduces%2520four%2520key%2520components%253A%2520%25281%2529%2520the%2520User%2520and%2520Candidate%2520DualTransformer%2520%2528UCDT%2529%2520for%2520fine-grained%2520user-item-context%2520modeling%253B%2520%25282%2529%2520the%2520Context-aware%2520User%2520History%2520and%2520Target%2520%2528CUHT%2529%2520module%2520for%2520position-sensitive%2520preference%2520learning%253B%2520%25283%2529%2520the%2520Listwise%2520Multi-HSTU%2520%2528LMH%2529%2520module%2520to%2520capture%2520hierarchical%2520item%2520dependencies%253B%2520and%2520%25284%2529%2520the%2520Embedding%2520Cache%2520%2528EC%2529%2520module%2520to%2520bridge%2520efficiency%2520and%2520effectiveness%2520during%2520inference.%2520By%2520sharing%2520representations%2520across%2520ranking%2520and%2520reranking%252C%2520RIA%2520enables%2520rich%2520contextual%2520knowledge%2520transfer%2520while%2520maintaining%2520low%2520latency.%2520Extensive%2520experiments%2520show%2520that%2520RIA%2520outperforms%2520state-of-the-art%2520models%2520on%2520both%2520public%2520and%2520industrial%2520datasets%252C%2520achieving%2520significant%2520gains%2520in%2520AUC%2520and%2520LogLoss.%2520Deployed%2520in%2520Meituan%2520advertising%2520system%252C%2520RIA%2520yields%2520a%2520%252B1.69%2525%2520improvement%2520in%2520Click-Through%2520Rate%2520%2528CTR%2529%2520and%2520a%2520%252B4.54%2525%2520increase%2520in%2520Cost%2520Per%2520Mille%2520%2528CPM%2529%2520in%2520online%2520A/B%2520tests.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIA%3A%20A%20Ranking-Infused%20Approach%20for%20Optimized%20listwise%20CTR%20Prediction&entry.906535625=Guoxiao%20Zhang%20and%20Tan%20Qu%20and%20Ao%20Li%20and%20DongLin%20Ni%20and%20Qianlong%20Xie%20and%20Xingxing%20Wang&entry.1292438233=Reranking%20improves%20recommendation%20quality%20by%20modeling%20item%20interactions.%20However%2C%20existing%20methods%20often%20decouple%20ranking%20and%20reranking%2C%20leading%20to%20weak%20listwise%20evaluation%20models%20that%20suffer%20from%20combinatorial%20sparsity%20and%20limited%20representational%20power%20under%20strict%20latency%20constraints.%20In%20this%20paper%2C%20we%20propose%20RIA%20%28Ranking-Infused%20Architecture%29%2C%20a%20unified%2C%20end-to-end%20framework%20that%20seamlessly%20integrates%20pointwise%20and%20listwise%20evaluation.%20RIA%20introduces%20four%20key%20components%3A%20%281%29%20the%20User%20and%20Candidate%20DualTransformer%20%28UCDT%29%20for%20fine-grained%20user-item-context%20modeling%3B%20%282%29%20the%20Context-aware%20User%20History%20and%20Target%20%28CUHT%29%20module%20for%20position-sensitive%20preference%20learning%3B%20%283%29%20the%20Listwise%20Multi-HSTU%20%28LMH%29%20module%20to%20capture%20hierarchical%20item%20dependencies%3B%20and%20%284%29%20the%20Embedding%20Cache%20%28EC%29%20module%20to%20bridge%20efficiency%20and%20effectiveness%20during%20inference.%20By%20sharing%20representations%20across%20ranking%20and%20reranking%2C%20RIA%20enables%20rich%20contextual%20knowledge%20transfer%20while%20maintaining%20low%20latency.%20Extensive%20experiments%20show%20that%20RIA%20outperforms%20state-of-the-art%20models%20on%20both%20public%20and%20industrial%20datasets%2C%20achieving%20significant%20gains%20in%20AUC%20and%20LogLoss.%20Deployed%20in%20Meituan%20advertising%20system%2C%20RIA%20yields%20a%20%2B1.69%25%20improvement%20in%20Click-Through%20Rate%20%28CTR%29%20and%20a%20%2B4.54%25%20increase%20in%20Cost%20Per%20Mille%20%28CPM%29%20in%20online%20A/B%20tests.&entry.1838667208=http%3A//arxiv.org/abs/2511.21394v1&entry.124074799=Read"},
{"title": "F-INR: Functional Tensor Decomposition for Implicit Neural Representations", "author": "Sai Karthikeya Vemuri and Tim B\u00fcchner and Joachim Denzler", "abstract": "Implicit Neural Representations (INRs) model signals as continuous, differentiable functions. However, monolithic INRs scale poorly with data dimensionality, leading to excessive training costs. We propose F-INR, a framework that addresses this limitation by factorizing a high-dimensional INR into a set of compact, axis-specific sub-networks based on functional tensor decomposition. These sub-networks learn low-dimensional functional components that are then combined via tensor operations. This factorization reduces computational complexity while additionally improving representational capacity. F-INR is both architecture- and decomposition-agnostic. It integrates with various existing INR backbones (e.g., SIREN, WIRE, FINER, Factor Fields) and tensor formats (e.g., CP, TT, Tucker), offering fine-grained control over the speed-accuracy trade-off via the tensor rank and mode. Our experiments show F-INR accelerates training by up to $20\\times$ and improves fidelity by over \\num{6.0} dB PSNR compared to state-of-the-art INRs. We validate these gains on diverse tasks, including image representation, 3D geometry reconstruction, and neural radiance fields. We further show F-INR's applicability to scientific computing by modeling complex physics simulations. Thus, F-INR provides a scalable, flexible, and efficient framework for high-dimensional signal modeling. Project page: https://f-inr.github.io", "link": "http://arxiv.org/abs/2503.21507v2", "date": "2025-11-26", "relevancy": 2.0372, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5123}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5077}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-INR%3A%20Functional%20Tensor%20Decomposition%20for%20Implicit%20Neural%20Representations&body=Title%3A%20F-INR%3A%20Functional%20Tensor%20Decomposition%20for%20Implicit%20Neural%20Representations%0AAuthor%3A%20Sai%20Karthikeya%20Vemuri%20and%20Tim%20B%C3%BCchner%20and%20Joachim%20Denzler%0AAbstract%3A%20Implicit%20Neural%20Representations%20%28INRs%29%20model%20signals%20as%20continuous%2C%20differentiable%20functions.%20However%2C%20monolithic%20INRs%20scale%20poorly%20with%20data%20dimensionality%2C%20leading%20to%20excessive%20training%20costs.%20We%20propose%20F-INR%2C%20a%20framework%20that%20addresses%20this%20limitation%20by%20factorizing%20a%20high-dimensional%20INR%20into%20a%20set%20of%20compact%2C%20axis-specific%20sub-networks%20based%20on%20functional%20tensor%20decomposition.%20These%20sub-networks%20learn%20low-dimensional%20functional%20components%20that%20are%20then%20combined%20via%20tensor%20operations.%20This%20factorization%20reduces%20computational%20complexity%20while%20additionally%20improving%20representational%20capacity.%20F-INR%20is%20both%20architecture-%20and%20decomposition-agnostic.%20It%20integrates%20with%20various%20existing%20INR%20backbones%20%28e.g.%2C%20SIREN%2C%20WIRE%2C%20FINER%2C%20Factor%20Fields%29%20and%20tensor%20formats%20%28e.g.%2C%20CP%2C%20TT%2C%20Tucker%29%2C%20offering%20fine-grained%20control%20over%20the%20speed-accuracy%20trade-off%20via%20the%20tensor%20rank%20and%20mode.%20Our%20experiments%20show%20F-INR%20accelerates%20training%20by%20up%20to%20%2420%5Ctimes%24%20and%20improves%20fidelity%20by%20over%20%5Cnum%7B6.0%7D%20dB%20PSNR%20compared%20to%20state-of-the-art%20INRs.%20We%20validate%20these%20gains%20on%20diverse%20tasks%2C%20including%20image%20representation%2C%203D%20geometry%20reconstruction%2C%20and%20neural%20radiance%20fields.%20We%20further%20show%20F-INR%27s%20applicability%20to%20scientific%20computing%20by%20modeling%20complex%20physics%20simulations.%20Thus%2C%20F-INR%20provides%20a%20scalable%2C%20flexible%2C%20and%20efficient%20framework%20for%20high-dimensional%20signal%20modeling.%20Project%20page%3A%20https%3A//f-inr.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2503.21507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-INR%253A%2520Functional%2520Tensor%2520Decomposition%2520for%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DSai%2520Karthikeya%2520Vemuri%2520and%2520Tim%2520B%25C3%25BCchner%2520and%2520Joachim%2520Denzler%26entry.1292438233%3DImplicit%2520Neural%2520Representations%2520%2528INRs%2529%2520model%2520signals%2520as%2520continuous%252C%2520differentiable%2520functions.%2520However%252C%2520monolithic%2520INRs%2520scale%2520poorly%2520with%2520data%2520dimensionality%252C%2520leading%2520to%2520excessive%2520training%2520costs.%2520We%2520propose%2520F-INR%252C%2520a%2520framework%2520that%2520addresses%2520this%2520limitation%2520by%2520factorizing%2520a%2520high-dimensional%2520INR%2520into%2520a%2520set%2520of%2520compact%252C%2520axis-specific%2520sub-networks%2520based%2520on%2520functional%2520tensor%2520decomposition.%2520These%2520sub-networks%2520learn%2520low-dimensional%2520functional%2520components%2520that%2520are%2520then%2520combined%2520via%2520tensor%2520operations.%2520This%2520factorization%2520reduces%2520computational%2520complexity%2520while%2520additionally%2520improving%2520representational%2520capacity.%2520F-INR%2520is%2520both%2520architecture-%2520and%2520decomposition-agnostic.%2520It%2520integrates%2520with%2520various%2520existing%2520INR%2520backbones%2520%2528e.g.%252C%2520SIREN%252C%2520WIRE%252C%2520FINER%252C%2520Factor%2520Fields%2529%2520and%2520tensor%2520formats%2520%2528e.g.%252C%2520CP%252C%2520TT%252C%2520Tucker%2529%252C%2520offering%2520fine-grained%2520control%2520over%2520the%2520speed-accuracy%2520trade-off%2520via%2520the%2520tensor%2520rank%2520and%2520mode.%2520Our%2520experiments%2520show%2520F-INR%2520accelerates%2520training%2520by%2520up%2520to%2520%252420%255Ctimes%2524%2520and%2520improves%2520fidelity%2520by%2520over%2520%255Cnum%257B6.0%257D%2520dB%2520PSNR%2520compared%2520to%2520state-of-the-art%2520INRs.%2520We%2520validate%2520these%2520gains%2520on%2520diverse%2520tasks%252C%2520including%2520image%2520representation%252C%25203D%2520geometry%2520reconstruction%252C%2520and%2520neural%2520radiance%2520fields.%2520We%2520further%2520show%2520F-INR%2527s%2520applicability%2520to%2520scientific%2520computing%2520by%2520modeling%2520complex%2520physics%2520simulations.%2520Thus%252C%2520F-INR%2520provides%2520a%2520scalable%252C%2520flexible%252C%2520and%2520efficient%2520framework%2520for%2520high-dimensional%2520signal%2520modeling.%2520Project%2520page%253A%2520https%253A//f-inr.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-INR%3A%20Functional%20Tensor%20Decomposition%20for%20Implicit%20Neural%20Representations&entry.906535625=Sai%20Karthikeya%20Vemuri%20and%20Tim%20B%C3%BCchner%20and%20Joachim%20Denzler&entry.1292438233=Implicit%20Neural%20Representations%20%28INRs%29%20model%20signals%20as%20continuous%2C%20differentiable%20functions.%20However%2C%20monolithic%20INRs%20scale%20poorly%20with%20data%20dimensionality%2C%20leading%20to%20excessive%20training%20costs.%20We%20propose%20F-INR%2C%20a%20framework%20that%20addresses%20this%20limitation%20by%20factorizing%20a%20high-dimensional%20INR%20into%20a%20set%20of%20compact%2C%20axis-specific%20sub-networks%20based%20on%20functional%20tensor%20decomposition.%20These%20sub-networks%20learn%20low-dimensional%20functional%20components%20that%20are%20then%20combined%20via%20tensor%20operations.%20This%20factorization%20reduces%20computational%20complexity%20while%20additionally%20improving%20representational%20capacity.%20F-INR%20is%20both%20architecture-%20and%20decomposition-agnostic.%20It%20integrates%20with%20various%20existing%20INR%20backbones%20%28e.g.%2C%20SIREN%2C%20WIRE%2C%20FINER%2C%20Factor%20Fields%29%20and%20tensor%20formats%20%28e.g.%2C%20CP%2C%20TT%2C%20Tucker%29%2C%20offering%20fine-grained%20control%20over%20the%20speed-accuracy%20trade-off%20via%20the%20tensor%20rank%20and%20mode.%20Our%20experiments%20show%20F-INR%20accelerates%20training%20by%20up%20to%20%2420%5Ctimes%24%20and%20improves%20fidelity%20by%20over%20%5Cnum%7B6.0%7D%20dB%20PSNR%20compared%20to%20state-of-the-art%20INRs.%20We%20validate%20these%20gains%20on%20diverse%20tasks%2C%20including%20image%20representation%2C%203D%20geometry%20reconstruction%2C%20and%20neural%20radiance%20fields.%20We%20further%20show%20F-INR%27s%20applicability%20to%20scientific%20computing%20by%20modeling%20complex%20physics%20simulations.%20Thus%2C%20F-INR%20provides%20a%20scalable%2C%20flexible%2C%20and%20efficient%20framework%20for%20high-dimensional%20signal%20modeling.%20Project%20page%3A%20https%3A//f-inr.github.io&entry.1838667208=http%3A//arxiv.org/abs/2503.21507v2&entry.124074799=Read"},
{"title": "Through the telecom lens: Are all training samples important?", "author": "Shruti Bothe and Illyyne Saffar and Aurelie Boisbunon and Hasan Farooq and Julien Forgeat and Md Moin Uddin Chowdhury", "abstract": "The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.", "link": "http://arxiv.org/abs/2511.21668v1", "date": "2025-11-26", "relevancy": 1.8723, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.49}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through%20the%20telecom%20lens%3A%20Are%20all%20training%20samples%20important%3F&body=Title%3A%20Through%20the%20telecom%20lens%3A%20Are%20all%20training%20samples%20important%3F%0AAuthor%3A%20Shruti%20Bothe%20and%20Illyyne%20Saffar%20and%20Aurelie%20Boisbunon%20and%20Hasan%20Farooq%20and%20Julien%20Forgeat%20and%20Md%20Moin%20Uddin%20Chowdhury%0AAbstract%3A%20The%20rise%20of%20AI%20in%20telecommunications%2C%20from%20optimizing%20Radio%20Access%20Networks%20to%20managing%20user%20experience%2C%20has%20sharply%20increased%20data%20volumes%20and%20training%20demands.%20Telecom%20data%20is%20often%20noisy%2C%20high-dimensional%2C%20costly%20to%20store%2C%20process%2C%20and%20label.%20Despite%20Ai%27s%20critical%20role%2C%20standard%20workflows%20still%20assume%20all%20training%20samples%20contribute%20equally.%20On%20the%20other%20hand%2C%20next%20generation%20systems%20require%20AI%20models%20that%20are%20accurate%2C%20efficient%2C%20and%20sustainable.The%20paper%20questions%20the%20assumptions%20of%20equal%20importance%20by%20focusing%20on%20applying%20and%20analyzing%20the%20roles%20of%20individual%20samples%20in%20telecom%20training%20and%20assessing%20whether%20the%20proposed%20model%20optimizes%20computation%20and%20energy%20use.%20we%20perform%20sample-level%20gradient%20analysis%20across%20epochs%20to%20identify%20patterns%20of%20influence%20and%20redundancy%20in%20model%20learning.%20Based%20on%20this%2C%20we%20propose%20a%20sample%20importance%20framework%20thats%20electively%20prioritizes%20impactful%20data%20and%20reduces%20computation%20without%20compromising%20accuracy.%20Experiments%20on%20three%20real-world%20telecom%20datasets%20show%20that%20our%20method%20%5Breserves%20performance%20while%20reducing%20data%20needs%20and%20computational%20overhead%20while%20advancing%20the%20goals%20of%20sustainable%20AI%20in%20telecommunications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough%2520the%2520telecom%2520lens%253A%2520Are%2520all%2520training%2520samples%2520important%253F%26entry.906535625%3DShruti%2520Bothe%2520and%2520Illyyne%2520Saffar%2520and%2520Aurelie%2520Boisbunon%2520and%2520Hasan%2520Farooq%2520and%2520Julien%2520Forgeat%2520and%2520Md%2520Moin%2520Uddin%2520Chowdhury%26entry.1292438233%3DThe%2520rise%2520of%2520AI%2520in%2520telecommunications%252C%2520from%2520optimizing%2520Radio%2520Access%2520Networks%2520to%2520managing%2520user%2520experience%252C%2520has%2520sharply%2520increased%2520data%2520volumes%2520and%2520training%2520demands.%2520Telecom%2520data%2520is%2520often%2520noisy%252C%2520high-dimensional%252C%2520costly%2520to%2520store%252C%2520process%252C%2520and%2520label.%2520Despite%2520Ai%2527s%2520critical%2520role%252C%2520standard%2520workflows%2520still%2520assume%2520all%2520training%2520samples%2520contribute%2520equally.%2520On%2520the%2520other%2520hand%252C%2520next%2520generation%2520systems%2520require%2520AI%2520models%2520that%2520are%2520accurate%252C%2520efficient%252C%2520and%2520sustainable.The%2520paper%2520questions%2520the%2520assumptions%2520of%2520equal%2520importance%2520by%2520focusing%2520on%2520applying%2520and%2520analyzing%2520the%2520roles%2520of%2520individual%2520samples%2520in%2520telecom%2520training%2520and%2520assessing%2520whether%2520the%2520proposed%2520model%2520optimizes%2520computation%2520and%2520energy%2520use.%2520we%2520perform%2520sample-level%2520gradient%2520analysis%2520across%2520epochs%2520to%2520identify%2520patterns%2520of%2520influence%2520and%2520redundancy%2520in%2520model%2520learning.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520sample%2520importance%2520framework%2520thats%2520electively%2520prioritizes%2520impactful%2520data%2520and%2520reduces%2520computation%2520without%2520compromising%2520accuracy.%2520Experiments%2520on%2520three%2520real-world%2520telecom%2520datasets%2520show%2520that%2520our%2520method%2520%255Breserves%2520performance%2520while%2520reducing%2520data%2520needs%2520and%2520computational%2520overhead%2520while%2520advancing%2520the%2520goals%2520of%2520sustainable%2520AI%2520in%2520telecommunications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through%20the%20telecom%20lens%3A%20Are%20all%20training%20samples%20important%3F&entry.906535625=Shruti%20Bothe%20and%20Illyyne%20Saffar%20and%20Aurelie%20Boisbunon%20and%20Hasan%20Farooq%20and%20Julien%20Forgeat%20and%20Md%20Moin%20Uddin%20Chowdhury&entry.1292438233=The%20rise%20of%20AI%20in%20telecommunications%2C%20from%20optimizing%20Radio%20Access%20Networks%20to%20managing%20user%20experience%2C%20has%20sharply%20increased%20data%20volumes%20and%20training%20demands.%20Telecom%20data%20is%20often%20noisy%2C%20high-dimensional%2C%20costly%20to%20store%2C%20process%2C%20and%20label.%20Despite%20Ai%27s%20critical%20role%2C%20standard%20workflows%20still%20assume%20all%20training%20samples%20contribute%20equally.%20On%20the%20other%20hand%2C%20next%20generation%20systems%20require%20AI%20models%20that%20are%20accurate%2C%20efficient%2C%20and%20sustainable.The%20paper%20questions%20the%20assumptions%20of%20equal%20importance%20by%20focusing%20on%20applying%20and%20analyzing%20the%20roles%20of%20individual%20samples%20in%20telecom%20training%20and%20assessing%20whether%20the%20proposed%20model%20optimizes%20computation%20and%20energy%20use.%20we%20perform%20sample-level%20gradient%20analysis%20across%20epochs%20to%20identify%20patterns%20of%20influence%20and%20redundancy%20in%20model%20learning.%20Based%20on%20this%2C%20we%20propose%20a%20sample%20importance%20framework%20thats%20electively%20prioritizes%20impactful%20data%20and%20reduces%20computation%20without%20compromising%20accuracy.%20Experiments%20on%20three%20real-world%20telecom%20datasets%20show%20that%20our%20method%20%5Breserves%20performance%20while%20reducing%20data%20needs%20and%20computational%20overhead%20while%20advancing%20the%20goals%20of%20sustainable%20AI%20in%20telecommunications.&entry.1838667208=http%3A//arxiv.org/abs/2511.21668v1&entry.124074799=Read"},
{"title": "Data Valuation by Fusing Global and Local Statistical Information", "author": "Xiaoling Zhou and Ou Wu and Michael K. Ng and Hao Jiang", "abstract": "Data valuation has garnered increasing attention in recent years, given the critical role of high-quality data in various applications. Among diverse data valuation approaches, Shapley value-based methods are predominant due to their strong theoretical grounding. However, the exact computation of Shapley values is often computationally prohibitive, prompting the development of numerous approximation techniques. Despite notable advancements, existing methods generally neglect the incorporation of value distribution information and fail to account for dynamic data conditions, thereby compromising their performance and application potential. In this paper, we highlight the crucial role of both global and local statistical properties of value distributions in the context of data valuation for machine learning. First, we conduct a comprehensive analysis of these distributions across various simulated and real-world datasets, uncovering valuable insights and key patterns. Second, we propose an enhanced data valuation method that fuses the explored distribution characteristics into two regularization terms to refine Shapley value estimation. The proposed regularizers can be seamlessly incorporated into various existing data valuation methods. Third, we introduce a novel approach for dynamic data valuation that infers updated data values without recomputing Shapley values, thereby significantly improving computational efficiency. Extensive experiments have been conducted across a range of tasks, including Shapley value estimation, value-based data addition and removal, mislabeled data detection, and dynamic data valuation. The results showcase the consistent effectiveness and efficiency of our proposed methodologies, affirming the significant potential of global and local value distributions in data valuation.", "link": "http://arxiv.org/abs/2405.17464v2", "date": "2025-11-26", "relevancy": 1.3106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Valuation%20by%20Fusing%20Global%20and%20Local%20Statistical%20Information&body=Title%3A%20Data%20Valuation%20by%20Fusing%20Global%20and%20Local%20Statistical%20Information%0AAuthor%3A%20Xiaoling%20Zhou%20and%20Ou%20Wu%20and%20Michael%20K.%20Ng%20and%20Hao%20Jiang%0AAbstract%3A%20Data%20valuation%20has%20garnered%20increasing%20attention%20in%20recent%20years%2C%20given%20the%20critical%20role%20of%20high-quality%20data%20in%20various%20applications.%20Among%20diverse%20data%20valuation%20approaches%2C%20Shapley%20value-based%20methods%20are%20predominant%20due%20to%20their%20strong%20theoretical%20grounding.%20However%2C%20the%20exact%20computation%20of%20Shapley%20values%20is%20often%20computationally%20prohibitive%2C%20prompting%20the%20development%20of%20numerous%20approximation%20techniques.%20Despite%20notable%20advancements%2C%20existing%20methods%20generally%20neglect%20the%20incorporation%20of%20value%20distribution%20information%20and%20fail%20to%20account%20for%20dynamic%20data%20conditions%2C%20thereby%20compromising%20their%20performance%20and%20application%20potential.%20In%20this%20paper%2C%20we%20highlight%20the%20crucial%20role%20of%20both%20global%20and%20local%20statistical%20properties%20of%20value%20distributions%20in%20the%20context%20of%20data%20valuation%20for%20machine%20learning.%20First%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20these%20distributions%20across%20various%20simulated%20and%20real-world%20datasets%2C%20uncovering%20valuable%20insights%20and%20key%20patterns.%20Second%2C%20we%20propose%20an%20enhanced%20data%20valuation%20method%20that%20fuses%20the%20explored%20distribution%20characteristics%20into%20two%20regularization%20terms%20to%20refine%20Shapley%20value%20estimation.%20The%20proposed%20regularizers%20can%20be%20seamlessly%20incorporated%20into%20various%20existing%20data%20valuation%20methods.%20Third%2C%20we%20introduce%20a%20novel%20approach%20for%20dynamic%20data%20valuation%20that%20infers%20updated%20data%20values%20without%20recomputing%20Shapley%20values%2C%20thereby%20significantly%20improving%20computational%20efficiency.%20Extensive%20experiments%20have%20been%20conducted%20across%20a%20range%20of%20tasks%2C%20including%20Shapley%20value%20estimation%2C%20value-based%20data%20addition%20and%20removal%2C%20mislabeled%20data%20detection%2C%20and%20dynamic%20data%20valuation.%20The%20results%20showcase%20the%20consistent%20effectiveness%20and%20efficiency%20of%20our%20proposed%20methodologies%2C%20affirming%20the%20significant%20potential%20of%20global%20and%20local%20value%20distributions%20in%20data%20valuation.%0ALink%3A%20http%3A//arxiv.org/abs/2405.17464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Valuation%2520by%2520Fusing%2520Global%2520and%2520Local%2520Statistical%2520Information%26entry.906535625%3DXiaoling%2520Zhou%2520and%2520Ou%2520Wu%2520and%2520Michael%2520K.%2520Ng%2520and%2520Hao%2520Jiang%26entry.1292438233%3DData%2520valuation%2520has%2520garnered%2520increasing%2520attention%2520in%2520recent%2520years%252C%2520given%2520the%2520critical%2520role%2520of%2520high-quality%2520data%2520in%2520various%2520applications.%2520Among%2520diverse%2520data%2520valuation%2520approaches%252C%2520Shapley%2520value-based%2520methods%2520are%2520predominant%2520due%2520to%2520their%2520strong%2520theoretical%2520grounding.%2520However%252C%2520the%2520exact%2520computation%2520of%2520Shapley%2520values%2520is%2520often%2520computationally%2520prohibitive%252C%2520prompting%2520the%2520development%2520of%2520numerous%2520approximation%2520techniques.%2520Despite%2520notable%2520advancements%252C%2520existing%2520methods%2520generally%2520neglect%2520the%2520incorporation%2520of%2520value%2520distribution%2520information%2520and%2520fail%2520to%2520account%2520for%2520dynamic%2520data%2520conditions%252C%2520thereby%2520compromising%2520their%2520performance%2520and%2520application%2520potential.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520the%2520crucial%2520role%2520of%2520both%2520global%2520and%2520local%2520statistical%2520properties%2520of%2520value%2520distributions%2520in%2520the%2520context%2520of%2520data%2520valuation%2520for%2520machine%2520learning.%2520First%252C%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520these%2520distributions%2520across%2520various%2520simulated%2520and%2520real-world%2520datasets%252C%2520uncovering%2520valuable%2520insights%2520and%2520key%2520patterns.%2520Second%252C%2520we%2520propose%2520an%2520enhanced%2520data%2520valuation%2520method%2520that%2520fuses%2520the%2520explored%2520distribution%2520characteristics%2520into%2520two%2520regularization%2520terms%2520to%2520refine%2520Shapley%2520value%2520estimation.%2520The%2520proposed%2520regularizers%2520can%2520be%2520seamlessly%2520incorporated%2520into%2520various%2520existing%2520data%2520valuation%2520methods.%2520Third%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520dynamic%2520data%2520valuation%2520that%2520infers%2520updated%2520data%2520values%2520without%2520recomputing%2520Shapley%2520values%252C%2520thereby%2520significantly%2520improving%2520computational%2520efficiency.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520across%2520a%2520range%2520of%2520tasks%252C%2520including%2520Shapley%2520value%2520estimation%252C%2520value-based%2520data%2520addition%2520and%2520removal%252C%2520mislabeled%2520data%2520detection%252C%2520and%2520dynamic%2520data%2520valuation.%2520The%2520results%2520showcase%2520the%2520consistent%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520proposed%2520methodologies%252C%2520affirming%2520the%2520significant%2520potential%2520of%2520global%2520and%2520local%2520value%2520distributions%2520in%2520data%2520valuation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Valuation%20by%20Fusing%20Global%20and%20Local%20Statistical%20Information&entry.906535625=Xiaoling%20Zhou%20and%20Ou%20Wu%20and%20Michael%20K.%20Ng%20and%20Hao%20Jiang&entry.1292438233=Data%20valuation%20has%20garnered%20increasing%20attention%20in%20recent%20years%2C%20given%20the%20critical%20role%20of%20high-quality%20data%20in%20various%20applications.%20Among%20diverse%20data%20valuation%20approaches%2C%20Shapley%20value-based%20methods%20are%20predominant%20due%20to%20their%20strong%20theoretical%20grounding.%20However%2C%20the%20exact%20computation%20of%20Shapley%20values%20is%20often%20computationally%20prohibitive%2C%20prompting%20the%20development%20of%20numerous%20approximation%20techniques.%20Despite%20notable%20advancements%2C%20existing%20methods%20generally%20neglect%20the%20incorporation%20of%20value%20distribution%20information%20and%20fail%20to%20account%20for%20dynamic%20data%20conditions%2C%20thereby%20compromising%20their%20performance%20and%20application%20potential.%20In%20this%20paper%2C%20we%20highlight%20the%20crucial%20role%20of%20both%20global%20and%20local%20statistical%20properties%20of%20value%20distributions%20in%20the%20context%20of%20data%20valuation%20for%20machine%20learning.%20First%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20these%20distributions%20across%20various%20simulated%20and%20real-world%20datasets%2C%20uncovering%20valuable%20insights%20and%20key%20patterns.%20Second%2C%20we%20propose%20an%20enhanced%20data%20valuation%20method%20that%20fuses%20the%20explored%20distribution%20characteristics%20into%20two%20regularization%20terms%20to%20refine%20Shapley%20value%20estimation.%20The%20proposed%20regularizers%20can%20be%20seamlessly%20incorporated%20into%20various%20existing%20data%20valuation%20methods.%20Third%2C%20we%20introduce%20a%20novel%20approach%20for%20dynamic%20data%20valuation%20that%20infers%20updated%20data%20values%20without%20recomputing%20Shapley%20values%2C%20thereby%20significantly%20improving%20computational%20efficiency.%20Extensive%20experiments%20have%20been%20conducted%20across%20a%20range%20of%20tasks%2C%20including%20Shapley%20value%20estimation%2C%20value-based%20data%20addition%20and%20removal%2C%20mislabeled%20data%20detection%2C%20and%20dynamic%20data%20valuation.%20The%20results%20showcase%20the%20consistent%20effectiveness%20and%20efficiency%20of%20our%20proposed%20methodologies%2C%20affirming%20the%20significant%20potential%20of%20global%20and%20local%20value%20distributions%20in%20data%20valuation.&entry.1838667208=http%3A//arxiv.org/abs/2405.17464v2&entry.124074799=Read"},
{"title": "Stream and Query-guided Feature Aggregation for Efficient and Effective 3D Occupancy Prediction", "author": "Seokha Moon and Janghyun Baek and Giseop Kim and Jinkyu Kim and Sunwook Choi", "abstract": "3D occupancy prediction has become a key perception task in autonomous driving, as it enables comprehensive scene understanding. Recent methods enhance this understanding by incorporating spatiotemporal information through multi-frame fusion, but they suffer from a trade-off: dense voxel-based representations provide high accuracy at significant computational cost, whereas sparse representations improve efficiency but lose spatial detail. To mitigate this trade-off, we introduce DuOcc, which employs a dual aggregation strategy that retains dense voxel representations to preserve spatial fidelity while maintaining high efficiency. DuOcc consists of two key components: (i) Stream-based Voxel Aggregation, which recurrently accumulates voxel features over time and refines them to suppress warping-induced distortions, preserving a clear separation between occupied and free space. (ii) Query-guided Aggregation, which complements the limitations of voxel accumulation by selectively injecting instance-level query features into the voxel regions occupied by dynamic objects. Experiments on the widely used Occ3D-nuScenes and SurroundOcc datasets demonstrate that DuOcc achieves state-of-the-art performance in real-time settings, while reducing memory usage by over 40% compared to prior methods.", "link": "http://arxiv.org/abs/2503.22087v2", "date": "2025-11-26", "relevancy": 1.5779, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5283}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.525}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream%20and%20Query-guided%20Feature%20Aggregation%20for%20Efficient%20and%20Effective%203D%20Occupancy%20Prediction&body=Title%3A%20Stream%20and%20Query-guided%20Feature%20Aggregation%20for%20Efficient%20and%20Effective%203D%20Occupancy%20Prediction%0AAuthor%3A%20Seokha%20Moon%20and%20Janghyun%20Baek%20and%20Giseop%20Kim%20and%20Jinkyu%20Kim%20and%20Sunwook%20Choi%0AAbstract%3A%203D%20occupancy%20prediction%20has%20become%20a%20key%20perception%20task%20in%20autonomous%20driving%2C%20as%20it%20enables%20comprehensive%20scene%20understanding.%20Recent%20methods%20enhance%20this%20understanding%20by%20incorporating%20spatiotemporal%20information%20through%20multi-frame%20fusion%2C%20but%20they%20suffer%20from%20a%20trade-off%3A%20dense%20voxel-based%20representations%20provide%20high%20accuracy%20at%20significant%20computational%20cost%2C%20whereas%20sparse%20representations%20improve%20efficiency%20but%20lose%20spatial%20detail.%20To%20mitigate%20this%20trade-off%2C%20we%20introduce%20DuOcc%2C%20which%20employs%20a%20dual%20aggregation%20strategy%20that%20retains%20dense%20voxel%20representations%20to%20preserve%20spatial%20fidelity%20while%20maintaining%20high%20efficiency.%20DuOcc%20consists%20of%20two%20key%20components%3A%20%28i%29%20Stream-based%20Voxel%20Aggregation%2C%20which%20recurrently%20accumulates%20voxel%20features%20over%20time%20and%20refines%20them%20to%20suppress%20warping-induced%20distortions%2C%20preserving%20a%20clear%20separation%20between%20occupied%20and%20free%20space.%20%28ii%29%20Query-guided%20Aggregation%2C%20which%20complements%20the%20limitations%20of%20voxel%20accumulation%20by%20selectively%20injecting%20instance-level%20query%20features%20into%20the%20voxel%20regions%20occupied%20by%20dynamic%20objects.%20Experiments%20on%20the%20widely%20used%20Occ3D-nuScenes%20and%20SurroundOcc%20datasets%20demonstrate%20that%20DuOcc%20achieves%20state-of-the-art%20performance%20in%20real-time%20settings%2C%20while%20reducing%20memory%20usage%20by%20over%2040%25%20compared%20to%20prior%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream%2520and%2520Query-guided%2520Feature%2520Aggregation%2520for%2520Efficient%2520and%2520Effective%25203D%2520Occupancy%2520Prediction%26entry.906535625%3DSeokha%2520Moon%2520and%2520Janghyun%2520Baek%2520and%2520Giseop%2520Kim%2520and%2520Jinkyu%2520Kim%2520and%2520Sunwook%2520Choi%26entry.1292438233%3D3D%2520occupancy%2520prediction%2520has%2520become%2520a%2520key%2520perception%2520task%2520in%2520autonomous%2520driving%252C%2520as%2520it%2520enables%2520comprehensive%2520scene%2520understanding.%2520Recent%2520methods%2520enhance%2520this%2520understanding%2520by%2520incorporating%2520spatiotemporal%2520information%2520through%2520multi-frame%2520fusion%252C%2520but%2520they%2520suffer%2520from%2520a%2520trade-off%253A%2520dense%2520voxel-based%2520representations%2520provide%2520high%2520accuracy%2520at%2520significant%2520computational%2520cost%252C%2520whereas%2520sparse%2520representations%2520improve%2520efficiency%2520but%2520lose%2520spatial%2520detail.%2520To%2520mitigate%2520this%2520trade-off%252C%2520we%2520introduce%2520DuOcc%252C%2520which%2520employs%2520a%2520dual%2520aggregation%2520strategy%2520that%2520retains%2520dense%2520voxel%2520representations%2520to%2520preserve%2520spatial%2520fidelity%2520while%2520maintaining%2520high%2520efficiency.%2520DuOcc%2520consists%2520of%2520two%2520key%2520components%253A%2520%2528i%2529%2520Stream-based%2520Voxel%2520Aggregation%252C%2520which%2520recurrently%2520accumulates%2520voxel%2520features%2520over%2520time%2520and%2520refines%2520them%2520to%2520suppress%2520warping-induced%2520distortions%252C%2520preserving%2520a%2520clear%2520separation%2520between%2520occupied%2520and%2520free%2520space.%2520%2528ii%2529%2520Query-guided%2520Aggregation%252C%2520which%2520complements%2520the%2520limitations%2520of%2520voxel%2520accumulation%2520by%2520selectively%2520injecting%2520instance-level%2520query%2520features%2520into%2520the%2520voxel%2520regions%2520occupied%2520by%2520dynamic%2520objects.%2520Experiments%2520on%2520the%2520widely%2520used%2520Occ3D-nuScenes%2520and%2520SurroundOcc%2520datasets%2520demonstrate%2520that%2520DuOcc%2520achieves%2520state-of-the-art%2520performance%2520in%2520real-time%2520settings%252C%2520while%2520reducing%2520memory%2520usage%2520by%2520over%252040%2525%2520compared%2520to%2520prior%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream%20and%20Query-guided%20Feature%20Aggregation%20for%20Efficient%20and%20Effective%203D%20Occupancy%20Prediction&entry.906535625=Seokha%20Moon%20and%20Janghyun%20Baek%20and%20Giseop%20Kim%20and%20Jinkyu%20Kim%20and%20Sunwook%20Choi&entry.1292438233=3D%20occupancy%20prediction%20has%20become%20a%20key%20perception%20task%20in%20autonomous%20driving%2C%20as%20it%20enables%20comprehensive%20scene%20understanding.%20Recent%20methods%20enhance%20this%20understanding%20by%20incorporating%20spatiotemporal%20information%20through%20multi-frame%20fusion%2C%20but%20they%20suffer%20from%20a%20trade-off%3A%20dense%20voxel-based%20representations%20provide%20high%20accuracy%20at%20significant%20computational%20cost%2C%20whereas%20sparse%20representations%20improve%20efficiency%20but%20lose%20spatial%20detail.%20To%20mitigate%20this%20trade-off%2C%20we%20introduce%20DuOcc%2C%20which%20employs%20a%20dual%20aggregation%20strategy%20that%20retains%20dense%20voxel%20representations%20to%20preserve%20spatial%20fidelity%20while%20maintaining%20high%20efficiency.%20DuOcc%20consists%20of%20two%20key%20components%3A%20%28i%29%20Stream-based%20Voxel%20Aggregation%2C%20which%20recurrently%20accumulates%20voxel%20features%20over%20time%20and%20refines%20them%20to%20suppress%20warping-induced%20distortions%2C%20preserving%20a%20clear%20separation%20between%20occupied%20and%20free%20space.%20%28ii%29%20Query-guided%20Aggregation%2C%20which%20complements%20the%20limitations%20of%20voxel%20accumulation%20by%20selectively%20injecting%20instance-level%20query%20features%20into%20the%20voxel%20regions%20occupied%20by%20dynamic%20objects.%20Experiments%20on%20the%20widely%20used%20Occ3D-nuScenes%20and%20SurroundOcc%20datasets%20demonstrate%20that%20DuOcc%20achieves%20state-of-the-art%20performance%20in%20real-time%20settings%2C%20while%20reducing%20memory%20usage%20by%20over%2040%25%20compared%20to%20prior%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2503.22087v2&entry.124074799=Read"},
{"title": "Hierarchical Ranking Neural Network for Long Document Readability Assessment", "author": "Yurui Zheng and Yijun Chen and Shaohong Zhang", "abstract": "Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.", "link": "http://arxiv.org/abs/2511.21473v1", "date": "2025-11-26", "relevancy": 1.8066, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4689}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Ranking%20Neural%20Network%20for%20Long%20Document%20Readability%20Assessment&body=Title%3A%20Hierarchical%20Ranking%20Neural%20Network%20for%20Long%20Document%20Readability%20Assessment%0AAuthor%3A%20Yurui%20Zheng%20and%20Yijun%20Chen%20and%20Shaohong%20Zhang%0AAbstract%3A%20Readability%20assessment%20aims%20to%20evaluate%20the%20reading%20difficulty%20of%20a%20text.%20In%20recent%20years%2C%20while%20deep%20learning%20technology%20has%20been%20gradually%20applied%20to%20readability%20assessment%2C%20most%20approaches%20fail%20to%20consider%20either%20the%20length%20of%20the%20text%20or%20the%20ordinal%20relationship%20of%20readability%20labels.%20This%20paper%20proposes%20a%20bidirectional%20readability%20assessment%20mechanism%20that%20captures%20contextual%20information%20to%20identify%20regions%20with%20rich%20semantic%20information%20in%20the%20text%2C%20thereby%20predicting%20the%20readability%20level%20of%20individual%20sentences.%20These%20sentence-level%20labels%20are%20then%20used%20to%20assist%20in%20predicting%20the%20overall%20readability%20level%20of%20the%20document.%20Additionally%2C%20a%20pairwise%20sorting%20algorithm%20is%20introduced%20to%20model%20the%20ordinal%20relationship%20between%20readability%20levels%20through%20label%20subtraction.%20Experimental%20results%20on%20Chinese%20and%20English%20datasets%20demonstrate%20that%20the%20proposed%20model%20achieves%20competitive%20performance%20and%20outperforms%20other%20baseline%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Ranking%2520Neural%2520Network%2520for%2520Long%2520Document%2520Readability%2520Assessment%26entry.906535625%3DYurui%2520Zheng%2520and%2520Yijun%2520Chen%2520and%2520Shaohong%2520Zhang%26entry.1292438233%3DReadability%2520assessment%2520aims%2520to%2520evaluate%2520the%2520reading%2520difficulty%2520of%2520a%2520text.%2520In%2520recent%2520years%252C%2520while%2520deep%2520learning%2520technology%2520has%2520been%2520gradually%2520applied%2520to%2520readability%2520assessment%252C%2520most%2520approaches%2520fail%2520to%2520consider%2520either%2520the%2520length%2520of%2520the%2520text%2520or%2520the%2520ordinal%2520relationship%2520of%2520readability%2520labels.%2520This%2520paper%2520proposes%2520a%2520bidirectional%2520readability%2520assessment%2520mechanism%2520that%2520captures%2520contextual%2520information%2520to%2520identify%2520regions%2520with%2520rich%2520semantic%2520information%2520in%2520the%2520text%252C%2520thereby%2520predicting%2520the%2520readability%2520level%2520of%2520individual%2520sentences.%2520These%2520sentence-level%2520labels%2520are%2520then%2520used%2520to%2520assist%2520in%2520predicting%2520the%2520overall%2520readability%2520level%2520of%2520the%2520document.%2520Additionally%252C%2520a%2520pairwise%2520sorting%2520algorithm%2520is%2520introduced%2520to%2520model%2520the%2520ordinal%2520relationship%2520between%2520readability%2520levels%2520through%2520label%2520subtraction.%2520Experimental%2520results%2520on%2520Chinese%2520and%2520English%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520model%2520achieves%2520competitive%2520performance%2520and%2520outperforms%2520other%2520baseline%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Ranking%20Neural%20Network%20for%20Long%20Document%20Readability%20Assessment&entry.906535625=Yurui%20Zheng%20and%20Yijun%20Chen%20and%20Shaohong%20Zhang&entry.1292438233=Readability%20assessment%20aims%20to%20evaluate%20the%20reading%20difficulty%20of%20a%20text.%20In%20recent%20years%2C%20while%20deep%20learning%20technology%20has%20been%20gradually%20applied%20to%20readability%20assessment%2C%20most%20approaches%20fail%20to%20consider%20either%20the%20length%20of%20the%20text%20or%20the%20ordinal%20relationship%20of%20readability%20labels.%20This%20paper%20proposes%20a%20bidirectional%20readability%20assessment%20mechanism%20that%20captures%20contextual%20information%20to%20identify%20regions%20with%20rich%20semantic%20information%20in%20the%20text%2C%20thereby%20predicting%20the%20readability%20level%20of%20individual%20sentences.%20These%20sentence-level%20labels%20are%20then%20used%20to%20assist%20in%20predicting%20the%20overall%20readability%20level%20of%20the%20document.%20Additionally%2C%20a%20pairwise%20sorting%20algorithm%20is%20introduced%20to%20model%20the%20ordinal%20relationship%20between%20readability%20levels%20through%20label%20subtraction.%20Experimental%20results%20on%20Chinese%20and%20English%20datasets%20demonstrate%20that%20the%20proposed%20model%20achieves%20competitive%20performance%20and%20outperforms%20other%20baseline%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.21473v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


