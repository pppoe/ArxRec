<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250705.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars", "author": "Gent Serifi and Marcel C. B\u00fchler", "abstract": "  We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.\n", "link": "http://arxiv.org/abs/2507.02803v1", "date": "2025-07-03", "relevancy": 3.7185, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&body=Title%3A%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars%0AAuthor%3A%20Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler%0AAbstract%3A%20%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperGaussians%253A%2520High-Dimensional%2520Gaussian%2520Splatting%2520for%2520High-Fidelity%250A%2520%2520Animatable%2520Face%2520Avatars%26entry.906535625%3DGent%2520Serifi%2520and%2520Marcel%2520C.%2520B%25C3%25BChler%26entry.1292438233%3D%2520%2520We%2520introduce%2520HyperGaussians%252C%2520a%2520novel%2520extension%2520of%25203D%2520Gaussian%2520Splatting%2520for%250Ahigh-quality%2520animatable%2520face%2520avatars.%2520Creating%2520such%2520detailed%2520face%2520avatars%2520from%250Avideos%2520is%2520a%2520challenging%2520problem%2520and%2520has%2520numerous%2520applications%2520in%2520augmented%2520and%250Avirtual%2520reality.%2520While%2520tremendous%2520successes%2520have%2520been%2520achieved%2520for%2520static%250Afaces%252C%2520animatable%2520avatars%2520from%2520monocular%2520videos%2520still%2520fall%2520in%2520the%2520uncanny%250Avalley.%2520The%2520de%2520facto%2520standard%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520represents%2520a%2520face%250Athrough%2520a%2520collection%2520of%25203D%2520Gaussian%2520primitives.%25203DGS%2520excels%2520at%2520rendering%2520static%250Afaces%252C%2520but%2520the%2520state-of-the-art%2520still%2520struggles%2520with%2520nonlinear%2520deformations%252C%250Acomplex%2520lighting%2520effects%252C%2520and%2520fine%2520details.%2520While%2520most%2520related%2520works%2520focus%2520on%250Apredicting%2520better%2520Gaussian%2520parameters%2520from%2520expression%2520codes%252C%2520we%2520rethink%2520the%25203D%250AGaussian%2520representation%2520itself%2520and%2520how%2520to%2520make%2520it%2520more%2520expressive.%2520Our%2520insights%250Alead%2520to%2520a%2520novel%2520extension%2520of%25203D%2520Gaussians%2520to%2520high-dimensional%2520multivariate%250AGaussians%252C%2520dubbed%2520%2527HyperGaussians%2527.%2520The%2520higher%2520dimensionality%2520increases%250Aexpressivity%2520through%2520conditioning%2520on%2520a%2520learnable%2520local%2520embedding.%2520However%252C%250Asplatting%2520HyperGaussians%2520is%2520computationally%2520expensive%2520because%2520it%2520requires%250Ainverting%2520a%2520high-dimensional%2520covariance%2520matrix.%2520We%2520solve%2520this%2520by%250Areparameterizing%2520the%2520covariance%2520matrix%252C%2520dubbed%2520the%2520%2527inverse%2520covariance%2520trick%2527.%250AThis%2520trick%2520boosts%2520the%2520efficiency%2520so%2520that%2520HyperGaussians%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520existing%2520models.%2520To%2520demonstrate%2520this%252C%2520we%2520plug%2520in%2520HyperGaussians%250Ainto%2520the%2520state-of-the-art%2520in%2520fast%2520monocular%2520face%2520avatars%253A%2520FlashAvatar.%2520Our%250Aevaluation%2520on%252019%2520subjects%2520from%25204%2520face%2520datasets%2520shows%2520that%2520HyperGaussians%250Aoutperform%25203DGS%2520numerically%2520and%2520visually%252C%2520particularly%2520for%2520high-frequency%250Adetails%2520like%2520eyeglass%2520frames%252C%2520teeth%252C%2520complex%2520facial%2520movements%252C%2520and%2520specular%250Areflections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&entry.906535625=Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler&entry.1292438233=%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02803v1&entry.124074799=Read"},
{"title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and\n  Manipulation of Articulated Objects", "author": "Qiaojun Yu and Xibin Yuan and Yu jiang and Junting Chen and Dongzhe Zheng and Ce Hao and Yang You and Yixing Chen and Yao Mu and Liu Liu and Cewu Lu", "abstract": "  Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home\n", "link": "http://arxiv.org/abs/2507.02600v1", "date": "2025-07-03", "relevancy": 3.3891, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.705}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6692}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects&body=Title%3A%20ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects%0AAuthor%3A%20Qiaojun%20Yu%20and%20Xibin%20Yuan%20and%20Yu%20jiang%20and%20Junting%20Chen%20and%20Dongzhe%20Zheng%20and%20Ce%20Hao%20and%20Yang%20You%20and%20Yixing%20Chen%20and%20Yao%20Mu%20and%20Liu%20Liu%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Articulated%20object%20manipulation%20remains%20a%20critical%20challenge%20in%20robotics%20due%0Ato%20the%20complex%20kinematic%20constraints%20and%20the%20limited%20physical%20reasoning%20of%0Aexisting%20methods.%20In%20this%20work%2C%20we%20introduce%20ArtGS%2C%20a%20novel%20framework%20that%0Aextends%203D%20Gaussian%20Splatting%20%283DGS%29%20by%20integrating%20visual-physical%20modeling%0Afor%20articulated%20object%20understanding%20and%20interaction.%20ArtGS%20begins%20with%0Amulti-view%20RGB-D%20reconstruction%2C%20followed%20by%20reasoning%20with%20a%20vision-language%0Amodel%20%28VLM%29%20to%20extract%20semantic%20and%20structural%20information%2C%20particularly%20the%0Aarticulated%20bones.%20Through%20dynamic%2C%20differentiable%203DGS-based%20rendering%2C%20ArtGS%0Aoptimizes%20the%20parameters%20of%20the%20articulated%20bones%2C%20ensuring%20physically%0Aconsistent%20motion%20constraints%20and%20enhancing%20the%20manipulation%20policy.%20By%0Aleveraging%20dynamic%20Gaussian%20splatting%2C%20cross-embodiment%20adaptability%2C%20and%0Aclosed-loop%20optimization%2C%20ArtGS%20establishes%20a%20new%20framework%20for%20efficient%2C%0Ascalable%2C%20and%20generalizable%20articulated%20object%20modeling%20and%20manipulation.%0AExperiments%20conducted%20in%20both%20simulation%20and%20real-world%20environments%0Ademonstrate%20that%20ArtGS%20significantly%20outperforms%20previous%20methods%20in%20joint%0Aestimation%20accuracy%20and%20manipulation%20success%20rates%20across%20a%20variety%20of%0Aarticulated%20objects.%20Additional%20images%20and%20videos%20are%20available%20on%20the%20project%0Awebsite%3A%20https%3A//sites.google.com/view/artgs/home%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtGS%253A3D%2520Gaussian%2520Splatting%2520for%2520Interactive%2520Visual-Physical%2520Modeling%2520and%250A%2520%2520Manipulation%2520of%2520Articulated%2520Objects%26entry.906535625%3DQiaojun%2520Yu%2520and%2520Xibin%2520Yuan%2520and%2520Yu%2520jiang%2520and%2520Junting%2520Chen%2520and%2520Dongzhe%2520Zheng%2520and%2520Ce%2520Hao%2520and%2520Yang%2520You%2520and%2520Yixing%2520Chen%2520and%2520Yao%2520Mu%2520and%2520Liu%2520Liu%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Articulated%2520object%2520manipulation%2520remains%2520a%2520critical%2520challenge%2520in%2520robotics%2520due%250Ato%2520the%2520complex%2520kinematic%2520constraints%2520and%2520the%2520limited%2520physical%2520reasoning%2520of%250Aexisting%2520methods.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ArtGS%252C%2520a%2520novel%2520framework%2520that%250Aextends%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520by%2520integrating%2520visual-physical%2520modeling%250Afor%2520articulated%2520object%2520understanding%2520and%2520interaction.%2520ArtGS%2520begins%2520with%250Amulti-view%2520RGB-D%2520reconstruction%252C%2520followed%2520by%2520reasoning%2520with%2520a%2520vision-language%250Amodel%2520%2528VLM%2529%2520to%2520extract%2520semantic%2520and%2520structural%2520information%252C%2520particularly%2520the%250Aarticulated%2520bones.%2520Through%2520dynamic%252C%2520differentiable%25203DGS-based%2520rendering%252C%2520ArtGS%250Aoptimizes%2520the%2520parameters%2520of%2520the%2520articulated%2520bones%252C%2520ensuring%2520physically%250Aconsistent%2520motion%2520constraints%2520and%2520enhancing%2520the%2520manipulation%2520policy.%2520By%250Aleveraging%2520dynamic%2520Gaussian%2520splatting%252C%2520cross-embodiment%2520adaptability%252C%2520and%250Aclosed-loop%2520optimization%252C%2520ArtGS%2520establishes%2520a%2520new%2520framework%2520for%2520efficient%252C%250Ascalable%252C%2520and%2520generalizable%2520articulated%2520object%2520modeling%2520and%2520manipulation.%250AExperiments%2520conducted%2520in%2520both%2520simulation%2520and%2520real-world%2520environments%250Ademonstrate%2520that%2520ArtGS%2520significantly%2520outperforms%2520previous%2520methods%2520in%2520joint%250Aestimation%2520accuracy%2520and%2520manipulation%2520success%2520rates%2520across%2520a%2520variety%2520of%250Aarticulated%2520objects.%2520Additional%2520images%2520and%2520videos%2520are%2520available%2520on%2520the%2520project%250Awebsite%253A%2520https%253A//sites.google.com/view/artgs/home%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects&entry.906535625=Qiaojun%20Yu%20and%20Xibin%20Yuan%20and%20Yu%20jiang%20and%20Junting%20Chen%20and%20Dongzhe%20Zheng%20and%20Ce%20Hao%20and%20Yang%20You%20and%20Yixing%20Chen%20and%20Yao%20Mu%20and%20Liu%20Liu%20and%20Cewu%20Lu&entry.1292438233=%20%20Articulated%20object%20manipulation%20remains%20a%20critical%20challenge%20in%20robotics%20due%0Ato%20the%20complex%20kinematic%20constraints%20and%20the%20limited%20physical%20reasoning%20of%0Aexisting%20methods.%20In%20this%20work%2C%20we%20introduce%20ArtGS%2C%20a%20novel%20framework%20that%0Aextends%203D%20Gaussian%20Splatting%20%283DGS%29%20by%20integrating%20visual-physical%20modeling%0Afor%20articulated%20object%20understanding%20and%20interaction.%20ArtGS%20begins%20with%0Amulti-view%20RGB-D%20reconstruction%2C%20followed%20by%20reasoning%20with%20a%20vision-language%0Amodel%20%28VLM%29%20to%20extract%20semantic%20and%20structural%20information%2C%20particularly%20the%0Aarticulated%20bones.%20Through%20dynamic%2C%20differentiable%203DGS-based%20rendering%2C%20ArtGS%0Aoptimizes%20the%20parameters%20of%20the%20articulated%20bones%2C%20ensuring%20physically%0Aconsistent%20motion%20constraints%20and%20enhancing%20the%20manipulation%20policy.%20By%0Aleveraging%20dynamic%20Gaussian%20splatting%2C%20cross-embodiment%20adaptability%2C%20and%0Aclosed-loop%20optimization%2C%20ArtGS%20establishes%20a%20new%20framework%20for%20efficient%2C%0Ascalable%2C%20and%20generalizable%20articulated%20object%20modeling%20and%20manipulation.%0AExperiments%20conducted%20in%20both%20simulation%20and%20real-world%20environments%0Ademonstrate%20that%20ArtGS%20significantly%20outperforms%20previous%20methods%20in%20joint%0Aestimation%20accuracy%20and%20manipulation%20success%20rates%20across%20a%20variety%20of%0Aarticulated%20objects.%20Additional%20images%20and%20videos%20are%20available%20on%20the%20project%0Awebsite%3A%20https%3A//sites.google.com/view/artgs/home%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02600v1&entry.124074799=Read"},
{"title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "author": "Zhening Huang and Xiaoyang Wu and Fangcheng Zhong and Hengshuang Zhao and Matthias Nie\u00dfner and Joan Lasenby", "abstract": "  We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c\n", "link": "http://arxiv.org/abs/2507.02861v1", "date": "2025-07-03", "relevancy": 3.2138, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6448}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6448}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans&body=Title%3A%20LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans%0AAuthor%3A%20Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Fangcheng%20Zhong%20and%20Hengshuang%20Zhao%20and%20Matthias%20Nie%C3%9Fner%20and%20Joan%20Lasenby%0AAbstract%3A%20%20%20We%20propose%20LiteReality%2C%20a%20novel%20pipeline%20that%20converts%20RGB-D%20scans%20of%20indoor%0Aenvironments%20into%20compact%2C%20realistic%2C%20and%20interactive%203D%20virtual%20replicas.%0ALiteReality%20not%20only%20reconstructs%20scenes%20that%20visually%20resemble%20reality%20but%0Aalso%20supports%20key%20features%20essential%20for%20graphics%20pipelines%20--%20such%20as%20object%0Aindividuality%2C%20articulation%2C%20high-quality%20physically%20based%20rendering%20materials%2C%0Aand%20physically%20based%20interaction.%20At%20its%20core%2C%20LiteReality%20first%20performs%20scene%0Aunderstanding%20and%20parses%20the%20results%20into%20a%20coherent%203D%20layout%20and%20objects%20with%0Athe%20help%20of%20a%20structured%20scene%20graph.%20It%20then%20reconstructs%20the%20scene%20by%0Aretrieving%20the%20most%20visually%20similar%203D%20artist-crafted%20models%20from%20a%20curated%0Aasset%20database.%20Next%2C%20the%20Material%20Painting%20module%20enhances%20realism%20by%0Arecovering%20high-quality%2C%20spatially%20varying%20materials.%20Finally%2C%20the%0Areconstructed%20scene%20is%20integrated%20into%20a%20simulation%20engine%20with%20basic%20physical%0Aproperties%20to%20enable%20interactive%20behavior.%20The%20resulting%20scenes%20are%20compact%2C%0Aeditable%2C%20and%20fully%20compatible%20with%20standard%20graphics%20pipelines%2C%20making%20them%0Asuitable%20for%20applications%20in%20AR/VR%2C%20gaming%2C%20robotics%2C%20and%20digital%20twins.%20In%0Aaddition%2C%20LiteReality%20introduces%20a%20training-free%20object%20retrieval%20module%20that%0Aachieves%20state-of-the-art%20similarity%20performance%20on%20the%20Scan2CAD%20benchmark%2C%0Aalong%20with%20a%20robust%20material%20painting%20module%20capable%20of%20transferring%0Aappearances%20from%20images%20of%20any%20style%20to%203D%20assets%20--%20even%20under%20severe%0Amisalignment%2C%20occlusion%2C%20and%20poor%20lighting.%20We%20demonstrate%20the%20effectiveness%20of%0ALiteReality%20on%20both%20real-life%20scans%20and%20public%20datasets.%20Project%20page%3A%0Ahttps%3A//litereality.github.io%3B%20Video%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DecK9m3LXg2c%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteReality%253A%2520Graphics-Ready%25203D%2520Scene%2520Reconstruction%2520from%2520RGB-D%2520Scans%26entry.906535625%3DZhening%2520Huang%2520and%2520Xiaoyang%2520Wu%2520and%2520Fangcheng%2520Zhong%2520and%2520Hengshuang%2520Zhao%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Joan%2520Lasenby%26entry.1292438233%3D%2520%2520We%2520propose%2520LiteReality%252C%2520a%2520novel%2520pipeline%2520that%2520converts%2520RGB-D%2520scans%2520of%2520indoor%250Aenvironments%2520into%2520compact%252C%2520realistic%252C%2520and%2520interactive%25203D%2520virtual%2520replicas.%250ALiteReality%2520not%2520only%2520reconstructs%2520scenes%2520that%2520visually%2520resemble%2520reality%2520but%250Aalso%2520supports%2520key%2520features%2520essential%2520for%2520graphics%2520pipelines%2520--%2520such%2520as%2520object%250Aindividuality%252C%2520articulation%252C%2520high-quality%2520physically%2520based%2520rendering%2520materials%252C%250Aand%2520physically%2520based%2520interaction.%2520At%2520its%2520core%252C%2520LiteReality%2520first%2520performs%2520scene%250Aunderstanding%2520and%2520parses%2520the%2520results%2520into%2520a%2520coherent%25203D%2520layout%2520and%2520objects%2520with%250Athe%2520help%2520of%2520a%2520structured%2520scene%2520graph.%2520It%2520then%2520reconstructs%2520the%2520scene%2520by%250Aretrieving%2520the%2520most%2520visually%2520similar%25203D%2520artist-crafted%2520models%2520from%2520a%2520curated%250Aasset%2520database.%2520Next%252C%2520the%2520Material%2520Painting%2520module%2520enhances%2520realism%2520by%250Arecovering%2520high-quality%252C%2520spatially%2520varying%2520materials.%2520Finally%252C%2520the%250Areconstructed%2520scene%2520is%2520integrated%2520into%2520a%2520simulation%2520engine%2520with%2520basic%2520physical%250Aproperties%2520to%2520enable%2520interactive%2520behavior.%2520The%2520resulting%2520scenes%2520are%2520compact%252C%250Aeditable%252C%2520and%2520fully%2520compatible%2520with%2520standard%2520graphics%2520pipelines%252C%2520making%2520them%250Asuitable%2520for%2520applications%2520in%2520AR/VR%252C%2520gaming%252C%2520robotics%252C%2520and%2520digital%2520twins.%2520In%250Aaddition%252C%2520LiteReality%2520introduces%2520a%2520training-free%2520object%2520retrieval%2520module%2520that%250Aachieves%2520state-of-the-art%2520similarity%2520performance%2520on%2520the%2520Scan2CAD%2520benchmark%252C%250Aalong%2520with%2520a%2520robust%2520material%2520painting%2520module%2520capable%2520of%2520transferring%250Aappearances%2520from%2520images%2520of%2520any%2520style%2520to%25203D%2520assets%2520--%2520even%2520under%2520severe%250Amisalignment%252C%2520occlusion%252C%2520and%2520poor%2520lighting.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250ALiteReality%2520on%2520both%2520real-life%2520scans%2520and%2520public%2520datasets.%2520Project%2520page%253A%250Ahttps%253A//litereality.github.io%253B%2520Video%253A%250Ahttps%253A//www.youtube.com/watch%253Fv%253DecK9m3LXg2c%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans&entry.906535625=Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Fangcheng%20Zhong%20and%20Hengshuang%20Zhao%20and%20Matthias%20Nie%C3%9Fner%20and%20Joan%20Lasenby&entry.1292438233=%20%20We%20propose%20LiteReality%2C%20a%20novel%20pipeline%20that%20converts%20RGB-D%20scans%20of%20indoor%0Aenvironments%20into%20compact%2C%20realistic%2C%20and%20interactive%203D%20virtual%20replicas.%0ALiteReality%20not%20only%20reconstructs%20scenes%20that%20visually%20resemble%20reality%20but%0Aalso%20supports%20key%20features%20essential%20for%20graphics%20pipelines%20--%20such%20as%20object%0Aindividuality%2C%20articulation%2C%20high-quality%20physically%20based%20rendering%20materials%2C%0Aand%20physically%20based%20interaction.%20At%20its%20core%2C%20LiteReality%20first%20performs%20scene%0Aunderstanding%20and%20parses%20the%20results%20into%20a%20coherent%203D%20layout%20and%20objects%20with%0Athe%20help%20of%20a%20structured%20scene%20graph.%20It%20then%20reconstructs%20the%20scene%20by%0Aretrieving%20the%20most%20visually%20similar%203D%20artist-crafted%20models%20from%20a%20curated%0Aasset%20database.%20Next%2C%20the%20Material%20Painting%20module%20enhances%20realism%20by%0Arecovering%20high-quality%2C%20spatially%20varying%20materials.%20Finally%2C%20the%0Areconstructed%20scene%20is%20integrated%20into%20a%20simulation%20engine%20with%20basic%20physical%0Aproperties%20to%20enable%20interactive%20behavior.%20The%20resulting%20scenes%20are%20compact%2C%0Aeditable%2C%20and%20fully%20compatible%20with%20standard%20graphics%20pipelines%2C%20making%20them%0Asuitable%20for%20applications%20in%20AR/VR%2C%20gaming%2C%20robotics%2C%20and%20digital%20twins.%20In%0Aaddition%2C%20LiteReality%20introduces%20a%20training-free%20object%20retrieval%20module%20that%0Aachieves%20state-of-the-art%20similarity%20performance%20on%20the%20Scan2CAD%20benchmark%2C%0Aalong%20with%20a%20robust%20material%20painting%20module%20capable%20of%20transferring%0Aappearances%20from%20images%20of%20any%20style%20to%203D%20assets%20--%20even%20under%20severe%0Amisalignment%2C%20occlusion%2C%20and%20poor%20lighting.%20We%20demonstrate%20the%20effectiveness%20of%0ALiteReality%20on%20both%20real-life%20scans%20and%20public%20datasets.%20Project%20page%3A%0Ahttps%3A//litereality.github.io%3B%20Video%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DecK9m3LXg2c%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02861v1&entry.124074799=Read"},
{"title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "author": "Jiawei He and Danshi Li and Xinqiang Yu and Zekun Qi and Wenyao Zhang and Jiayi Chen and Zhaoxiang Zhang and Zhizheng Zhang and Li Yi and He Wang", "abstract": "  As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.\n", "link": "http://arxiv.org/abs/2507.02747v1", "date": "2025-07-03", "relevancy": 3.156, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale&body=Title%3A%20DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale%0AAuthor%3A%20Jiawei%20He%20and%20Danshi%20Li%20and%20Xinqiang%20Yu%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Jiayi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhizheng%20Zhang%20and%20Li%20Yi%20and%20He%20Wang%0AAbstract%3A%20%20%20As%20large%20models%20gain%20traction%2C%20vision-language-action%20%28VLA%29%20systems%20are%0Aenabling%20robots%20to%20tackle%20increasingly%20complex%20tasks.%20However%2C%20limited%20by%20the%0Adifficulty%20of%20data%20collection%2C%20progress%20has%20mainly%20focused%20on%20controlling%0Asimple%20gripper%20end-effectors.%20There%20is%20little%20research%20on%20functional%20grasping%0Awith%20large%20models%20for%20human-like%20dexterous%20hands.%20In%20this%20paper%2C%20we%20introduce%0ADexVLG%2C%20a%20large%20Vision-Language-Grasp%20model%20for%20Dexterous%20grasp%20pose%20prediction%0Aaligned%20with%20language%20instructions%20using%20single-view%20RGBD%20input.%20To%20accomplish%0Athis%2C%20we%20generate%20a%20dataset%20of%20170%20million%20dexterous%20grasp%20poses%20mapped%20to%0Asemantic%20parts%20across%20174%2C000%20objects%20in%20simulation%2C%20paired%20with%20detailed%0Apart-level%20captions.%20This%20large-scale%20dataset%2C%20named%20DexGraspNet%203.0%2C%20is%20used%0Ato%20train%20a%20VLM%20and%20flow-matching-based%20pose%20head%20capable%20of%20producing%0Ainstruction-aligned%20grasp%20poses%20for%20tabletop%20objects.%20To%20assess%20DexVLG%27s%0Aperformance%2C%20we%20create%20benchmarks%20in%20physics-based%20simulations%20and%20conduct%0Areal-world%20experiments.%20Extensive%20testing%20demonstrates%20DexVLG%27s%20strong%0Azero-shot%20generalization%20capabilities-achieving%20over%2076%25%20zero-shot%20execution%0Asuccess%20rate%20and%20state-of-the-art%20part-grasp%20accuracy%20in%20simulation-and%0Asuccessful%20part-aligned%20grasps%20on%20physical%20objects%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexVLG%253A%2520Dexterous%2520Vision-Language-Grasp%2520Model%2520at%2520Scale%26entry.906535625%3DJiawei%2520He%2520and%2520Danshi%2520Li%2520and%2520Xinqiang%2520Yu%2520and%2520Zekun%2520Qi%2520and%2520Wenyao%2520Zhang%2520and%2520Jiayi%2520Chen%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Zhizheng%2520Zhang%2520and%2520Li%2520Yi%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520As%2520large%2520models%2520gain%2520traction%252C%2520vision-language-action%2520%2528VLA%2529%2520systems%2520are%250Aenabling%2520robots%2520to%2520tackle%2520increasingly%2520complex%2520tasks.%2520However%252C%2520limited%2520by%2520the%250Adifficulty%2520of%2520data%2520collection%252C%2520progress%2520has%2520mainly%2520focused%2520on%2520controlling%250Asimple%2520gripper%2520end-effectors.%2520There%2520is%2520little%2520research%2520on%2520functional%2520grasping%250Awith%2520large%2520models%2520for%2520human-like%2520dexterous%2520hands.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADexVLG%252C%2520a%2520large%2520Vision-Language-Grasp%2520model%2520for%2520Dexterous%2520grasp%2520pose%2520prediction%250Aaligned%2520with%2520language%2520instructions%2520using%2520single-view%2520RGBD%2520input.%2520To%2520accomplish%250Athis%252C%2520we%2520generate%2520a%2520dataset%2520of%2520170%2520million%2520dexterous%2520grasp%2520poses%2520mapped%2520to%250Asemantic%2520parts%2520across%2520174%252C000%2520objects%2520in%2520simulation%252C%2520paired%2520with%2520detailed%250Apart-level%2520captions.%2520This%2520large-scale%2520dataset%252C%2520named%2520DexGraspNet%25203.0%252C%2520is%2520used%250Ato%2520train%2520a%2520VLM%2520and%2520flow-matching-based%2520pose%2520head%2520capable%2520of%2520producing%250Ainstruction-aligned%2520grasp%2520poses%2520for%2520tabletop%2520objects.%2520To%2520assess%2520DexVLG%2527s%250Aperformance%252C%2520we%2520create%2520benchmarks%2520in%2520physics-based%2520simulations%2520and%2520conduct%250Areal-world%2520experiments.%2520Extensive%2520testing%2520demonstrates%2520DexVLG%2527s%2520strong%250Azero-shot%2520generalization%2520capabilities-achieving%2520over%252076%2525%2520zero-shot%2520execution%250Asuccess%2520rate%2520and%2520state-of-the-art%2520part-grasp%2520accuracy%2520in%2520simulation-and%250Asuccessful%2520part-aligned%2520grasps%2520on%2520physical%2520objects%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale&entry.906535625=Jiawei%20He%20and%20Danshi%20Li%20and%20Xinqiang%20Yu%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Jiayi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhizheng%20Zhang%20and%20Li%20Yi%20and%20He%20Wang&entry.1292438233=%20%20As%20large%20models%20gain%20traction%2C%20vision-language-action%20%28VLA%29%20systems%20are%0Aenabling%20robots%20to%20tackle%20increasingly%20complex%20tasks.%20However%2C%20limited%20by%20the%0Adifficulty%20of%20data%20collection%2C%20progress%20has%20mainly%20focused%20on%20controlling%0Asimple%20gripper%20end-effectors.%20There%20is%20little%20research%20on%20functional%20grasping%0Awith%20large%20models%20for%20human-like%20dexterous%20hands.%20In%20this%20paper%2C%20we%20introduce%0ADexVLG%2C%20a%20large%20Vision-Language-Grasp%20model%20for%20Dexterous%20grasp%20pose%20prediction%0Aaligned%20with%20language%20instructions%20using%20single-view%20RGBD%20input.%20To%20accomplish%0Athis%2C%20we%20generate%20a%20dataset%20of%20170%20million%20dexterous%20grasp%20poses%20mapped%20to%0Asemantic%20parts%20across%20174%2C000%20objects%20in%20simulation%2C%20paired%20with%20detailed%0Apart-level%20captions.%20This%20large-scale%20dataset%2C%20named%20DexGraspNet%203.0%2C%20is%20used%0Ato%20train%20a%20VLM%20and%20flow-matching-based%20pose%20head%20capable%20of%20producing%0Ainstruction-aligned%20grasp%20poses%20for%20tabletop%20objects.%20To%20assess%20DexVLG%27s%0Aperformance%2C%20we%20create%20benchmarks%20in%20physics-based%20simulations%20and%20conduct%0Areal-world%20experiments.%20Extensive%20testing%20demonstrates%20DexVLG%27s%20strong%0Azero-shot%20generalization%20capabilities-achieving%20over%2076%25%20zero-shot%20execution%0Asuccess%20rate%20and%20state-of-the-art%20part-grasp%20accuracy%20in%20simulation-and%0Asuccessful%20part-aligned%20grasps%20on%20physical%20objects%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02747v1&entry.124074799=Read"},
{"title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond\n  Feature Alignment", "author": "Qi Xu and Dongxu Wei and Lingzhe Zhao and Wenpu Li and Zhangchi Huang and Shunping Ji and Peidong Liu", "abstract": "  Simultaneous understanding and 3D reconstruction plays an important role in\ndeveloping end-to-end embodied intelligent systems. To achieve this, recent\napproaches resort to 2D-to-3D feature alignment paradigm, which leads to\nlimited 3D understanding capability and potential semantic information loss. In\nlight of this, we propose SIU3R, the first alignment-free framework for\ngeneralizable simultaneous understanding and 3D reconstruction from unposed\nimages. Specifically, SIU3R bridges reconstruction and understanding tasks via\npixel-aligned 3D representation, and unifies multiple understanding tasks into\na set of unified learnable queries, enabling native 3D understanding without\nthe need of alignment with 2D models. To encourage collaboration between the\ntwo tasks with shared representation, we further conduct in-depth analyses of\ntheir mutual benefits, and propose two lightweight modules to facilitate their\ninteraction. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance not only on the individual tasks of 3D\nreconstruction and understanding, but also on the task of simultaneous\nunderstanding and 3D reconstruction, highlighting the advantages of our\nalignment-free framework and the effectiveness of the mutual benefit designs.\n", "link": "http://arxiv.org/abs/2507.02705v1", "date": "2025-07-03", "relevancy": 3.1275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment&body=Title%3A%20SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment%0AAuthor%3A%20Qi%20Xu%20and%20Dongxu%20Wei%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Zhangchi%20Huang%20and%20Shunping%20Ji%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Simultaneous%20understanding%20and%203D%20reconstruction%20plays%20an%20important%20role%20in%0Adeveloping%20end-to-end%20embodied%20intelligent%20systems.%20To%20achieve%20this%2C%20recent%0Aapproaches%20resort%20to%202D-to-3D%20feature%20alignment%20paradigm%2C%20which%20leads%20to%0Alimited%203D%20understanding%20capability%20and%20potential%20semantic%20information%20loss.%20In%0Alight%20of%20this%2C%20we%20propose%20SIU3R%2C%20the%20first%20alignment-free%20framework%20for%0Ageneralizable%20simultaneous%20understanding%20and%203D%20reconstruction%20from%20unposed%0Aimages.%20Specifically%2C%20SIU3R%20bridges%20reconstruction%20and%20understanding%20tasks%20via%0Apixel-aligned%203D%20representation%2C%20and%20unifies%20multiple%20understanding%20tasks%20into%0Aa%20set%20of%20unified%20learnable%20queries%2C%20enabling%20native%203D%20understanding%20without%0Athe%20need%20of%20alignment%20with%202D%20models.%20To%20encourage%20collaboration%20between%20the%0Atwo%20tasks%20with%20shared%20representation%2C%20we%20further%20conduct%20in-depth%20analyses%20of%0Atheir%20mutual%20benefits%2C%20and%20propose%20two%20lightweight%20modules%20to%20facilitate%20their%0Ainteraction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20not%20only%20on%20the%20individual%20tasks%20of%203D%0Areconstruction%20and%20understanding%2C%20but%20also%20on%20the%20task%20of%20simultaneous%0Aunderstanding%20and%203D%20reconstruction%2C%20highlighting%20the%20advantages%20of%20our%0Aalignment-free%20framework%20and%20the%20effectiveness%20of%20the%20mutual%20benefit%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIU3R%253A%2520Simultaneous%2520Scene%2520Understanding%2520and%25203D%2520Reconstruction%2520Beyond%250A%2520%2520Feature%2520Alignment%26entry.906535625%3DQi%2520Xu%2520and%2520Dongxu%2520Wei%2520and%2520Lingzhe%2520Zhao%2520and%2520Wenpu%2520Li%2520and%2520Zhangchi%2520Huang%2520and%2520Shunping%2520Ji%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Simultaneous%2520understanding%2520and%25203D%2520reconstruction%2520plays%2520an%2520important%2520role%2520in%250Adeveloping%2520end-to-end%2520embodied%2520intelligent%2520systems.%2520To%2520achieve%2520this%252C%2520recent%250Aapproaches%2520resort%2520to%25202D-to-3D%2520feature%2520alignment%2520paradigm%252C%2520which%2520leads%2520to%250Alimited%25203D%2520understanding%2520capability%2520and%2520potential%2520semantic%2520information%2520loss.%2520In%250Alight%2520of%2520this%252C%2520we%2520propose%2520SIU3R%252C%2520the%2520first%2520alignment-free%2520framework%2520for%250Ageneralizable%2520simultaneous%2520understanding%2520and%25203D%2520reconstruction%2520from%2520unposed%250Aimages.%2520Specifically%252C%2520SIU3R%2520bridges%2520reconstruction%2520and%2520understanding%2520tasks%2520via%250Apixel-aligned%25203D%2520representation%252C%2520and%2520unifies%2520multiple%2520understanding%2520tasks%2520into%250Aa%2520set%2520of%2520unified%2520learnable%2520queries%252C%2520enabling%2520native%25203D%2520understanding%2520without%250Athe%2520need%2520of%2520alignment%2520with%25202D%2520models.%2520To%2520encourage%2520collaboration%2520between%2520the%250Atwo%2520tasks%2520with%2520shared%2520representation%252C%2520we%2520further%2520conduct%2520in-depth%2520analyses%2520of%250Atheir%2520mutual%2520benefits%252C%2520and%2520propose%2520two%2520lightweight%2520modules%2520to%2520facilitate%2520their%250Ainteraction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520not%2520only%2520on%2520the%2520individual%2520tasks%2520of%25203D%250Areconstruction%2520and%2520understanding%252C%2520but%2520also%2520on%2520the%2520task%2520of%2520simultaneous%250Aunderstanding%2520and%25203D%2520reconstruction%252C%2520highlighting%2520the%2520advantages%2520of%2520our%250Aalignment-free%2520framework%2520and%2520the%2520effectiveness%2520of%2520the%2520mutual%2520benefit%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment&entry.906535625=Qi%20Xu%20and%20Dongxu%20Wei%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Zhangchi%20Huang%20and%20Shunping%20Ji%20and%20Peidong%20Liu&entry.1292438233=%20%20Simultaneous%20understanding%20and%203D%20reconstruction%20plays%20an%20important%20role%20in%0Adeveloping%20end-to-end%20embodied%20intelligent%20systems.%20To%20achieve%20this%2C%20recent%0Aapproaches%20resort%20to%202D-to-3D%20feature%20alignment%20paradigm%2C%20which%20leads%20to%0Alimited%203D%20understanding%20capability%20and%20potential%20semantic%20information%20loss.%20In%0Alight%20of%20this%2C%20we%20propose%20SIU3R%2C%20the%20first%20alignment-free%20framework%20for%0Ageneralizable%20simultaneous%20understanding%20and%203D%20reconstruction%20from%20unposed%0Aimages.%20Specifically%2C%20SIU3R%20bridges%20reconstruction%20and%20understanding%20tasks%20via%0Apixel-aligned%203D%20representation%2C%20and%20unifies%20multiple%20understanding%20tasks%20into%0Aa%20set%20of%20unified%20learnable%20queries%2C%20enabling%20native%203D%20understanding%20without%0Athe%20need%20of%20alignment%20with%202D%20models.%20To%20encourage%20collaboration%20between%20the%0Atwo%20tasks%20with%20shared%20representation%2C%20we%20further%20conduct%20in-depth%20analyses%20of%0Atheir%20mutual%20benefits%2C%20and%20propose%20two%20lightweight%20modules%20to%20facilitate%20their%0Ainteraction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20not%20only%20on%20the%20individual%20tasks%20of%203D%0Areconstruction%20and%20understanding%2C%20but%20also%20on%20the%20task%20of%20simultaneous%0Aunderstanding%20and%203D%20reconstruction%2C%20highlighting%20the%20advantages%20of%20our%0Aalignment-free%20framework%20and%20the%20effectiveness%20of%20the%20mutual%20benefit%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02705v1&entry.124074799=Read"},
{"title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer\n  Memory", "author": "Yuqi Wu and Wenzhao Zheng and Jie Zhou and Jiwen Lu", "abstract": "  Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.\n", "link": "http://arxiv.org/abs/2507.02863v1", "date": "2025-07-03", "relevancy": 3.0569, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6162}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6129}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory&body=Title%3A%20Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory%0AAuthor%3A%20Yuqi%20Wu%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Dense%203D%20scene%20reconstruction%20from%20an%20ordered%20sequence%20or%20unordered%20image%0Acollections%20is%20a%20critical%20step%20when%20bringing%20research%20in%20computer%20vision%20into%0Apractical%20scenarios.%20Following%20the%20paradigm%20introduced%20by%20DUSt3R%2C%20which%20unifies%0Aan%20image%20pair%20densely%20into%20a%20shared%20coordinate%20system%2C%20subsequent%20methods%0Amaintain%20an%20implicit%20memory%20to%20achieve%20dense%203D%20reconstruction%20from%20more%0Aimages.%20However%2C%20such%20implicit%20memory%20is%20limited%20in%20capacity%20and%20may%20suffer%0Afrom%20information%20loss%20of%20earlier%20frames.%20We%20propose%20Point3R%2C%20an%20online%0Aframework%20targeting%20dense%20streaming%203D%20reconstruction.%20To%20be%20specific%2C%20we%0Amaintain%20an%20explicit%20spatial%20pointer%20memory%20directly%20associated%20with%20the%203D%0Astructure%20of%20the%20current%20scene.%20Each%20pointer%20in%20this%20memory%20is%20assigned%20a%0Aspecific%203D%20position%20and%20aggregates%20scene%20information%20nearby%20in%20the%20global%0Acoordinate%20system%20into%20a%20changing%20spatial%20feature.%20Information%20extracted%20from%0Athe%20latest%20frame%20interacts%20explicitly%20with%20this%20pointer%20memory%2C%20enabling%20dense%0Aintegration%20of%20the%20current%20observation%20into%20the%20global%20coordinate%20system.%20We%0Adesign%20a%203D%20hierarchical%20position%20embedding%20to%20promote%20this%20interaction%20and%0Adesign%20a%20simple%20yet%20effective%20fusion%20mechanism%20to%20ensure%20that%20our%20pointer%0Amemory%20is%20uniform%20and%20efficient.%20Our%20method%20achieves%20competitive%20or%0Astate-of-the-art%20performance%20on%20various%20tasks%20with%20low%20training%20costs.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YkiWu/Point3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint3R%253A%2520Streaming%25203D%2520Reconstruction%2520with%2520Explicit%2520Spatial%2520Pointer%250A%2520%2520Memory%26entry.906535625%3DYuqi%2520Wu%2520and%2520Wenzhao%2520Zheng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Dense%25203D%2520scene%2520reconstruction%2520from%2520an%2520ordered%2520sequence%2520or%2520unordered%2520image%250Acollections%2520is%2520a%2520critical%2520step%2520when%2520bringing%2520research%2520in%2520computer%2520vision%2520into%250Apractical%2520scenarios.%2520Following%2520the%2520paradigm%2520introduced%2520by%2520DUSt3R%252C%2520which%2520unifies%250Aan%2520image%2520pair%2520densely%2520into%2520a%2520shared%2520coordinate%2520system%252C%2520subsequent%2520methods%250Amaintain%2520an%2520implicit%2520memory%2520to%2520achieve%2520dense%25203D%2520reconstruction%2520from%2520more%250Aimages.%2520However%252C%2520such%2520implicit%2520memory%2520is%2520limited%2520in%2520capacity%2520and%2520may%2520suffer%250Afrom%2520information%2520loss%2520of%2520earlier%2520frames.%2520We%2520propose%2520Point3R%252C%2520an%2520online%250Aframework%2520targeting%2520dense%2520streaming%25203D%2520reconstruction.%2520To%2520be%2520specific%252C%2520we%250Amaintain%2520an%2520explicit%2520spatial%2520pointer%2520memory%2520directly%2520associated%2520with%2520the%25203D%250Astructure%2520of%2520the%2520current%2520scene.%2520Each%2520pointer%2520in%2520this%2520memory%2520is%2520assigned%2520a%250Aspecific%25203D%2520position%2520and%2520aggregates%2520scene%2520information%2520nearby%2520in%2520the%2520global%250Acoordinate%2520system%2520into%2520a%2520changing%2520spatial%2520feature.%2520Information%2520extracted%2520from%250Athe%2520latest%2520frame%2520interacts%2520explicitly%2520with%2520this%2520pointer%2520memory%252C%2520enabling%2520dense%250Aintegration%2520of%2520the%2520current%2520observation%2520into%2520the%2520global%2520coordinate%2520system.%2520We%250Adesign%2520a%25203D%2520hierarchical%2520position%2520embedding%2520to%2520promote%2520this%2520interaction%2520and%250Adesign%2520a%2520simple%2520yet%2520effective%2520fusion%2520mechanism%2520to%2520ensure%2520that%2520our%2520pointer%250Amemory%2520is%2520uniform%2520and%2520efficient.%2520Our%2520method%2520achieves%2520competitive%2520or%250Astate-of-the-art%2520performance%2520on%2520various%2520tasks%2520with%2520low%2520training%2520costs.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/YkiWu/Point3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory&entry.906535625=Yuqi%20Wu%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Dense%203D%20scene%20reconstruction%20from%20an%20ordered%20sequence%20or%20unordered%20image%0Acollections%20is%20a%20critical%20step%20when%20bringing%20research%20in%20computer%20vision%20into%0Apractical%20scenarios.%20Following%20the%20paradigm%20introduced%20by%20DUSt3R%2C%20which%20unifies%0Aan%20image%20pair%20densely%20into%20a%20shared%20coordinate%20system%2C%20subsequent%20methods%0Amaintain%20an%20implicit%20memory%20to%20achieve%20dense%203D%20reconstruction%20from%20more%0Aimages.%20However%2C%20such%20implicit%20memory%20is%20limited%20in%20capacity%20and%20may%20suffer%0Afrom%20information%20loss%20of%20earlier%20frames.%20We%20propose%20Point3R%2C%20an%20online%0Aframework%20targeting%20dense%20streaming%203D%20reconstruction.%20To%20be%20specific%2C%20we%0Amaintain%20an%20explicit%20spatial%20pointer%20memory%20directly%20associated%20with%20the%203D%0Astructure%20of%20the%20current%20scene.%20Each%20pointer%20in%20this%20memory%20is%20assigned%20a%0Aspecific%203D%20position%20and%20aggregates%20scene%20information%20nearby%20in%20the%20global%0Acoordinate%20system%20into%20a%20changing%20spatial%20feature.%20Information%20extracted%20from%0Athe%20latest%20frame%20interacts%20explicitly%20with%20this%20pointer%20memory%2C%20enabling%20dense%0Aintegration%20of%20the%20current%20observation%20into%20the%20global%20coordinate%20system.%20We%0Adesign%20a%203D%20hierarchical%20position%20embedding%20to%20promote%20this%20interaction%20and%0Adesign%20a%20simple%20yet%20effective%20fusion%20mechanism%20to%20ensure%20that%20our%20pointer%0Amemory%20is%20uniform%20and%20efficient.%20Our%20method%20achieves%20competitive%20or%0Astate-of-the-art%20performance%20on%20various%20tasks%20with%20low%20training%20costs.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YkiWu/Point3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02863v1&entry.124074799=Read"},
{"title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers", "author": "Zhaochen Su and Peng Xia and Hangyu Guo and Zhenhua Liu and Yan Ma and Xiaoye Qu and Jiaqi Liu and Yanshu Li and Kaide Zeng and Zhengyuan Yang and Linjie Li and Yu Cheng and Heng Ji and Junxian He and Yi R. Fung", "abstract": "  Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.\n", "link": "http://arxiv.org/abs/2506.23918v3", "date": "2025-07-03", "relevancy": 2.9132, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5893}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5893}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers&body=Title%3A%20Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers%0AAuthor%3A%20Zhaochen%20Su%20and%20Peng%20Xia%20and%20Hangyu%20Guo%20and%20Zhenhua%20Liu%20and%20Yan%20Ma%20and%20Xiaoye%20Qu%20and%20Jiaqi%20Liu%20and%20Yanshu%20Li%20and%20Kaide%20Zeng%20and%20Zhengyuan%20Yang%20and%20Linjie%20Li%20and%20Yu%20Cheng%20and%20Heng%20Ji%20and%20Junxian%20He%20and%20Yi%20R.%20Fung%0AAbstract%3A%20%20%20Recent%20progress%20in%20multimodal%20reasoning%20has%20been%20significantly%20advanced%20by%0Atextual%20Chain-of-Thought%20%28CoT%29%2C%20a%20paradigm%20where%20models%20conduct%20reasoning%0Awithin%20language.%20This%20text-centric%20approach%2C%20however%2C%20treats%20vision%20as%20a%0Astatic%2C%20initial%20context%2C%20creating%20a%20fundamental%20%22semantic%20gap%22%20between%20rich%0Aperceptual%20data%20and%20discrete%20symbolic%20thought.%20Human%20cognition%20often%20transcends%0Alanguage%2C%20utilizing%20vision%20as%20a%20dynamic%20mental%20sketchpad.%20A%20similar%20evolution%0Ais%20now%20unfolding%20in%20AI%2C%20marking%20a%20fundamental%20paradigm%20shift%20from%20models%20that%0Amerely%20think%20about%20images%20to%20those%20that%20can%20truly%20think%20with%20images.%20This%0Aemerging%20paradigm%20is%20characterized%20by%20models%20leveraging%20visual%20information%20as%0Aintermediate%20steps%20in%20their%20thought%20process%2C%20transforming%20vision%20from%20a%20passive%0Ainput%20into%20a%20dynamic%2C%20manipulable%20cognitive%20workspace.%20In%20this%20survey%2C%20we%20chart%0Athis%20evolution%20of%20intelligence%20along%20a%20trajectory%20of%20increasing%20cognitive%0Aautonomy%2C%20which%20unfolds%20across%20three%20key%20stages%3A%20from%20external%20tool%0Aexploration%2C%20through%20programmatic%20manipulation%2C%20to%20intrinsic%20imagination.%20To%0Astructure%20this%20rapidly%20evolving%20field%2C%20our%20survey%20makes%20four%20key%20contributions.%0A%281%29%20We%20establish%20the%20foundational%20principles%20of%20the%20think%20with%20image%20paradigm%0Aand%20its%20three-stage%20framework.%20%282%29%20We%20provide%20a%20comprehensive%20review%20of%20the%0Acore%20methods%20that%20characterize%20each%20stage%20of%20this%20roadmap.%20%283%29%20We%20analyze%20the%0Acritical%20landscape%20of%20evaluation%20benchmarks%20and%20transformative%20applications.%0A%284%29%20We%20identify%20significant%20challenges%20and%20outline%20promising%20future%20directions.%0ABy%20providing%20this%20structured%20overview%2C%20we%20aim%20to%20offer%20a%20clear%20roadmap%20for%0Afuture%20research%20towards%20more%20powerful%20and%20human-aligned%20multimodal%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23918v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Images%2520for%2520Multimodal%2520Reasoning%253A%2520Foundations%252C%2520Methods%252C%2520and%250A%2520%2520Future%2520Frontiers%26entry.906535625%3DZhaochen%2520Su%2520and%2520Peng%2520Xia%2520and%2520Hangyu%2520Guo%2520and%2520Zhenhua%2520Liu%2520and%2520Yan%2520Ma%2520and%2520Xiaoye%2520Qu%2520and%2520Jiaqi%2520Liu%2520and%2520Yanshu%2520Li%2520and%2520Kaide%2520Zeng%2520and%2520Zhengyuan%2520Yang%2520and%2520Linjie%2520Li%2520and%2520Yu%2520Cheng%2520and%2520Heng%2520Ji%2520and%2520Junxian%2520He%2520and%2520Yi%2520R.%2520Fung%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520multimodal%2520reasoning%2520has%2520been%2520significantly%2520advanced%2520by%250Atextual%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520a%2520paradigm%2520where%2520models%2520conduct%2520reasoning%250Awithin%2520language.%2520This%2520text-centric%2520approach%252C%2520however%252C%2520treats%2520vision%2520as%2520a%250Astatic%252C%2520initial%2520context%252C%2520creating%2520a%2520fundamental%2520%2522semantic%2520gap%2522%2520between%2520rich%250Aperceptual%2520data%2520and%2520discrete%2520symbolic%2520thought.%2520Human%2520cognition%2520often%2520transcends%250Alanguage%252C%2520utilizing%2520vision%2520as%2520a%2520dynamic%2520mental%2520sketchpad.%2520A%2520similar%2520evolution%250Ais%2520now%2520unfolding%2520in%2520AI%252C%2520marking%2520a%2520fundamental%2520paradigm%2520shift%2520from%2520models%2520that%250Amerely%2520think%2520about%2520images%2520to%2520those%2520that%2520can%2520truly%2520think%2520with%2520images.%2520This%250Aemerging%2520paradigm%2520is%2520characterized%2520by%2520models%2520leveraging%2520visual%2520information%2520as%250Aintermediate%2520steps%2520in%2520their%2520thought%2520process%252C%2520transforming%2520vision%2520from%2520a%2520passive%250Ainput%2520into%2520a%2520dynamic%252C%2520manipulable%2520cognitive%2520workspace.%2520In%2520this%2520survey%252C%2520we%2520chart%250Athis%2520evolution%2520of%2520intelligence%2520along%2520a%2520trajectory%2520of%2520increasing%2520cognitive%250Aautonomy%252C%2520which%2520unfolds%2520across%2520three%2520key%2520stages%253A%2520from%2520external%2520tool%250Aexploration%252C%2520through%2520programmatic%2520manipulation%252C%2520to%2520intrinsic%2520imagination.%2520To%250Astructure%2520this%2520rapidly%2520evolving%2520field%252C%2520our%2520survey%2520makes%2520four%2520key%2520contributions.%250A%25281%2529%2520We%2520establish%2520the%2520foundational%2520principles%2520of%2520the%2520think%2520with%2520image%2520paradigm%250Aand%2520its%2520three-stage%2520framework.%2520%25282%2529%2520We%2520provide%2520a%2520comprehensive%2520review%2520of%2520the%250Acore%2520methods%2520that%2520characterize%2520each%2520stage%2520of%2520this%2520roadmap.%2520%25283%2529%2520We%2520analyze%2520the%250Acritical%2520landscape%2520of%2520evaluation%2520benchmarks%2520and%2520transformative%2520applications.%250A%25284%2529%2520We%2520identify%2520significant%2520challenges%2520and%2520outline%2520promising%2520future%2520directions.%250ABy%2520providing%2520this%2520structured%2520overview%252C%2520we%2520aim%2520to%2520offer%2520a%2520clear%2520roadmap%2520for%250Afuture%2520research%2520towards%2520more%2520powerful%2520and%2520human-aligned%2520multimodal%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23918v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers&entry.906535625=Zhaochen%20Su%20and%20Peng%20Xia%20and%20Hangyu%20Guo%20and%20Zhenhua%20Liu%20and%20Yan%20Ma%20and%20Xiaoye%20Qu%20and%20Jiaqi%20Liu%20and%20Yanshu%20Li%20and%20Kaide%20Zeng%20and%20Zhengyuan%20Yang%20and%20Linjie%20Li%20and%20Yu%20Cheng%20and%20Heng%20Ji%20and%20Junxian%20He%20and%20Yi%20R.%20Fung&entry.1292438233=%20%20Recent%20progress%20in%20multimodal%20reasoning%20has%20been%20significantly%20advanced%20by%0Atextual%20Chain-of-Thought%20%28CoT%29%2C%20a%20paradigm%20where%20models%20conduct%20reasoning%0Awithin%20language.%20This%20text-centric%20approach%2C%20however%2C%20treats%20vision%20as%20a%0Astatic%2C%20initial%20context%2C%20creating%20a%20fundamental%20%22semantic%20gap%22%20between%20rich%0Aperceptual%20data%20and%20discrete%20symbolic%20thought.%20Human%20cognition%20often%20transcends%0Alanguage%2C%20utilizing%20vision%20as%20a%20dynamic%20mental%20sketchpad.%20A%20similar%20evolution%0Ais%20now%20unfolding%20in%20AI%2C%20marking%20a%20fundamental%20paradigm%20shift%20from%20models%20that%0Amerely%20think%20about%20images%20to%20those%20that%20can%20truly%20think%20with%20images.%20This%0Aemerging%20paradigm%20is%20characterized%20by%20models%20leveraging%20visual%20information%20as%0Aintermediate%20steps%20in%20their%20thought%20process%2C%20transforming%20vision%20from%20a%20passive%0Ainput%20into%20a%20dynamic%2C%20manipulable%20cognitive%20workspace.%20In%20this%20survey%2C%20we%20chart%0Athis%20evolution%20of%20intelligence%20along%20a%20trajectory%20of%20increasing%20cognitive%0Aautonomy%2C%20which%20unfolds%20across%20three%20key%20stages%3A%20from%20external%20tool%0Aexploration%2C%20through%20programmatic%20manipulation%2C%20to%20intrinsic%20imagination.%20To%0Astructure%20this%20rapidly%20evolving%20field%2C%20our%20survey%20makes%20four%20key%20contributions.%0A%281%29%20We%20establish%20the%20foundational%20principles%20of%20the%20think%20with%20image%20paradigm%0Aand%20its%20three-stage%20framework.%20%282%29%20We%20provide%20a%20comprehensive%20review%20of%20the%0Acore%20methods%20that%20characterize%20each%20stage%20of%20this%20roadmap.%20%283%29%20We%20analyze%20the%0Acritical%20landscape%20of%20evaluation%20benchmarks%20and%20transformative%20applications.%0A%284%29%20We%20identify%20significant%20challenges%20and%20outline%20promising%20future%20directions.%0ABy%20providing%20this%20structured%20overview%2C%20we%20aim%20to%20offer%20a%20clear%20roadmap%20for%0Afuture%20research%20towards%20more%20powerful%20and%20human-aligned%20multimodal%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23918v3&entry.124074799=Read"},
{"title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion", "author": "Fangfu Liu and Hao Li and Jiawei Chi and Hanyang Wang and Minghui Yang and Fudong Wang and Yueqi Duan", "abstract": "  Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.\n", "link": "http://arxiv.org/abs/2507.02813v1", "date": "2025-07-03", "relevancy": 2.8211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7138}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion&body=Title%3A%20LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion%0AAuthor%3A%20Fangfu%20Liu%20and%20Hao%20Li%20and%20Jiawei%20Chi%20and%20Hanyang%20Wang%20and%20Minghui%20Yang%20and%20Fudong%20Wang%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Recovering%203D%20structures%20with%20open-vocabulary%20scene%20understanding%20from%202D%0Aimages%20is%20a%20fundamental%20but%20daunting%20task.%20Recent%20developments%20have%20achieved%0Athis%20by%20performing%20per-scene%20optimization%20with%20embedded%20language%20information.%0AHowever%2C%20they%20heavily%20rely%20on%20the%20calibrated%20dense-view%20reconstruction%0Aparadigm%2C%20thereby%20suffering%20from%20severe%20rendering%20artifacts%20and%20implausible%0Asemantic%20synthesis%20when%20limited%20views%20are%20available.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20generative%20framework%2C%20coined%20LangScene-X%2C%20to%20unify%20and%0Agenerate%203D%20consistent%20multi-modality%20information%20for%20reconstruction%20and%0Aunderstanding.%20Powered%20by%20the%20generative%20capability%20of%20creating%20more%20consistent%0Anovel%20observations%2C%20we%20can%20build%20generalizable%203D%20language-embedded%20scenes%20from%0Aonly%20sparse%20views.%20Specifically%2C%20we%20first%20train%20a%20TriMap%20video%20diffusion%20model%0Athat%20can%20generate%20appearance%20%28RGBs%29%2C%20geometry%20%28normals%29%2C%20and%20semantics%0A%28segmentation%20maps%29%20from%20sparse%20inputs%20through%20progressive%20knowledge%0Aintegration.%20Furthermore%2C%20we%20propose%20a%20Language%20Quantized%20Compressor%20%28LQC%29%2C%0Atrained%20on%20large-scale%20image%20datasets%2C%20to%20efficiently%20encode%20language%0Aembeddings%2C%20enabling%20cross-scene%20generalization%20without%20per-scene%20retraining.%0AFinally%2C%20we%20reconstruct%20the%20language%20surface%20fields%20by%20aligning%20language%0Ainformation%20onto%20the%20surface%20of%203D%20scenes%2C%20enabling%20open-ended%20language%0Aqueries.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20superiority%0Aof%20our%20LangScene-X%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%20and%0Ageneralizability.%20Project%20Page%3A%20https%3A//liuff19.github.io/LangScene-X.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangScene-X%253A%2520Reconstruct%2520Generalizable%25203D%2520Language-Embedded%2520Scenes%2520with%250A%2520%2520TriMap%2520Video%2520Diffusion%26entry.906535625%3DFangfu%2520Liu%2520and%2520Hao%2520Li%2520and%2520Jiawei%2520Chi%2520and%2520Hanyang%2520Wang%2520and%2520Minghui%2520Yang%2520and%2520Fudong%2520Wang%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Recovering%25203D%2520structures%2520with%2520open-vocabulary%2520scene%2520understanding%2520from%25202D%250Aimages%2520is%2520a%2520fundamental%2520but%2520daunting%2520task.%2520Recent%2520developments%2520have%2520achieved%250Athis%2520by%2520performing%2520per-scene%2520optimization%2520with%2520embedded%2520language%2520information.%250AHowever%252C%2520they%2520heavily%2520rely%2520on%2520the%2520calibrated%2520dense-view%2520reconstruction%250Aparadigm%252C%2520thereby%2520suffering%2520from%2520severe%2520rendering%2520artifacts%2520and%2520implausible%250Asemantic%2520synthesis%2520when%2520limited%2520views%2520are%2520available.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520generative%2520framework%252C%2520coined%2520LangScene-X%252C%2520to%2520unify%2520and%250Agenerate%25203D%2520consistent%2520multi-modality%2520information%2520for%2520reconstruction%2520and%250Aunderstanding.%2520Powered%2520by%2520the%2520generative%2520capability%2520of%2520creating%2520more%2520consistent%250Anovel%2520observations%252C%2520we%2520can%2520build%2520generalizable%25203D%2520language-embedded%2520scenes%2520from%250Aonly%2520sparse%2520views.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520TriMap%2520video%2520diffusion%2520model%250Athat%2520can%2520generate%2520appearance%2520%2528RGBs%2529%252C%2520geometry%2520%2528normals%2529%252C%2520and%2520semantics%250A%2528segmentation%2520maps%2529%2520from%2520sparse%2520inputs%2520through%2520progressive%2520knowledge%250Aintegration.%2520Furthermore%252C%2520we%2520propose%2520a%2520Language%2520Quantized%2520Compressor%2520%2528LQC%2529%252C%250Atrained%2520on%2520large-scale%2520image%2520datasets%252C%2520to%2520efficiently%2520encode%2520language%250Aembeddings%252C%2520enabling%2520cross-scene%2520generalization%2520without%2520per-scene%2520retraining.%250AFinally%252C%2520we%2520reconstruct%2520the%2520language%2520surface%2520fields%2520by%2520aligning%2520language%250Ainformation%2520onto%2520the%2520surface%2520of%25203D%2520scenes%252C%2520enabling%2520open-ended%2520language%250Aqueries.%2520Extensive%2520experiments%2520on%2520real-world%2520data%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520LangScene-X%2520over%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520quality%2520and%250Ageneralizability.%2520Project%2520Page%253A%2520https%253A//liuff19.github.io/LangScene-X.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion&entry.906535625=Fangfu%20Liu%20and%20Hao%20Li%20and%20Jiawei%20Chi%20and%20Hanyang%20Wang%20and%20Minghui%20Yang%20and%20Fudong%20Wang%20and%20Yueqi%20Duan&entry.1292438233=%20%20Recovering%203D%20structures%20with%20open-vocabulary%20scene%20understanding%20from%202D%0Aimages%20is%20a%20fundamental%20but%20daunting%20task.%20Recent%20developments%20have%20achieved%0Athis%20by%20performing%20per-scene%20optimization%20with%20embedded%20language%20information.%0AHowever%2C%20they%20heavily%20rely%20on%20the%20calibrated%20dense-view%20reconstruction%0Aparadigm%2C%20thereby%20suffering%20from%20severe%20rendering%20artifacts%20and%20implausible%0Asemantic%20synthesis%20when%20limited%20views%20are%20available.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20generative%20framework%2C%20coined%20LangScene-X%2C%20to%20unify%20and%0Agenerate%203D%20consistent%20multi-modality%20information%20for%20reconstruction%20and%0Aunderstanding.%20Powered%20by%20the%20generative%20capability%20of%20creating%20more%20consistent%0Anovel%20observations%2C%20we%20can%20build%20generalizable%203D%20language-embedded%20scenes%20from%0Aonly%20sparse%20views.%20Specifically%2C%20we%20first%20train%20a%20TriMap%20video%20diffusion%20model%0Athat%20can%20generate%20appearance%20%28RGBs%29%2C%20geometry%20%28normals%29%2C%20and%20semantics%0A%28segmentation%20maps%29%20from%20sparse%20inputs%20through%20progressive%20knowledge%0Aintegration.%20Furthermore%2C%20we%20propose%20a%20Language%20Quantized%20Compressor%20%28LQC%29%2C%0Atrained%20on%20large-scale%20image%20datasets%2C%20to%20efficiently%20encode%20language%0Aembeddings%2C%20enabling%20cross-scene%20generalization%20without%20per-scene%20retraining.%0AFinally%2C%20we%20reconstruct%20the%20language%20surface%20fields%20by%20aligning%20language%0Ainformation%20onto%20the%20surface%20of%203D%20scenes%2C%20enabling%20open-ended%20language%0Aqueries.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20superiority%0Aof%20our%20LangScene-X%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%20and%0Ageneralizability.%20Project%20Page%3A%20https%3A//liuff19.github.io/LangScene-X.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02813v1&entry.124074799=Read"},
{"title": "Modality-agnostic, patient-specific digital twins modeling temporally\n  varying digestive motion", "author": "Jorge Tapias Gomez and Nishant Nadkarni and Lando S. Bosma and Jue Jiang and Ergys D. Subashi and William P. Segars and James M. Balter and Mert R Sabuncu and Neelam Tyagi and Harini Veeraraghavan", "abstract": "  Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.\n", "link": "http://arxiv.org/abs/2507.01909v2", "date": "2025-07-03", "relevancy": 2.7788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&body=Title%3A%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion%0AAuthor%3A%20Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-agnostic%252C%2520patient-specific%2520digital%2520twins%2520modeling%2520temporally%250A%2520%2520varying%2520digestive%2520motion%26entry.906535625%3DJorge%2520Tapias%2520Gomez%2520and%2520Nishant%2520Nadkarni%2520and%2520Lando%2520S.%2520Bosma%2520and%2520Jue%2520Jiang%2520and%2520Ergys%2520D.%2520Subashi%2520and%2520William%2520P.%2520Segars%2520and%2520James%2520M.%2520Balter%2520and%2520Mert%2520R%2520Sabuncu%2520and%2520Neelam%2520Tyagi%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Objective%253A%2520Clinical%2520implementation%2520of%2520deformable%2520image%2520registration%2520%2528DIR%2529%250Arequires%2520voxel-based%2520spatial%2520accuracy%2520metrics%2520such%2520as%2520manually%2520identified%250Alandmarks%252C%2520which%2520are%2520challenging%2520to%2520implement%2520for%2520highly%2520mobile%250Agastrointestinal%2520%2528GI%2529%2520organs.%2520To%2520address%2520this%252C%2520patient-specific%2520digital%2520twins%250A%2528DT%2529%2520modeling%2520temporally%2520varying%2520motion%2520were%2520created%2520to%2520assess%2520the%2520accuracy%2520of%250ADIR%2520methods.%2520Approach%253A%252021%2520motion%2520phases%2520simulating%2520digestive%2520GI%2520motion%2520as%25204D%250Asequences%2520were%2520generated%2520from%2520static%25203D%2520patient%2520scans%2520using%2520published%250Aanalytical%2520GI%2520motion%2520models%2520through%2520a%2520semi-automated%2520pipeline.%2520Eleven%2520datasets%252C%250Aincluding%2520six%2520T2w%2520FSE%2520MRI%2520%2528T2w%2520MRI%2529%252C%2520two%2520T1w%25204D%2520golden-angle%2520stack-of-stars%252C%250Aand%2520three%2520contrast-enhanced%2520CT%2520scans.%2520The%2520motion%2520amplitudes%2520of%2520the%2520DTs%2520were%250Aassessed%2520against%2520real%2520patient%2520stomach%2520motion%2520amplitudes%2520extracted%2520from%250Aindependent%25204D%2520MRI%2520datasets.%2520The%2520generated%2520DTs%2520were%2520then%2520used%2520to%2520assess%2520six%250Adifferent%2520DIR%2520methods%2520using%2520target%2520registration%2520error%252C%2520Dice%2520similarity%250Acoefficient%252C%2520and%2520the%252095th%2520percentile%2520Hausdorff%2520distance%2520using%2520summary%2520metrics%250Aand%2520voxel-level%2520granular%2520visualizations.%2520Finally%252C%2520for%2520a%2520subset%2520of%2520T2w%2520MRI%2520scans%250Afrom%2520patients%2520treated%2520with%2520MR-guided%2520radiation%2520therapy%252C%2520dose%2520distributions%2520were%250Awarped%2520and%2520accumulated%2520to%2520assess%2520dose%2520warping%2520errors%252C%2520including%2520evaluations%2520of%250ADIR%2520performance%2520in%2520both%2520low-%2520and%2520high-dose%2520regions%2520for%2520patient-specific%2520error%250Aestimation.%2520Main%2520results%253A%2520Our%2520proposed%2520pipeline%2520synthesized%2520DTs%2520modeling%250Arealistic%2520GI%2520motion%252C%2520achieving%2520mean%2520and%2520maximum%2520motion%2520amplitudes%2520and%2520a%2520mean%250Alog%2520Jacobian%2520determinant%2520within%25200.8%2520mm%2520and%25200.01%252C%2520respectively%252C%2520similar%2520to%250Apublished%2520real-patient%2520gastric%2520motion%2520data.%2520It%2520also%2520enables%2520the%2520extraction%2520of%250Adetailed%2520quantitative%2520DIR%2520performance%2520metrics%2520and%2520rigorous%2520validation%2520of%2520dose%250Amapping%2520accuracy.%2520Significance%253A%2520The%2520pipeline%2520enables%2520rigorously%2520testing%2520DIR%250Atools%2520for%2520dynamic%252C%2520anatomically%2520complex%2520regions%2520enabling%2520granular%2520spatial%2520and%250Adosimetric%2520accuracies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&entry.906535625=Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01909v2&entry.124074799=Read"},
{"title": "MaizeField3D: A Curated 3D Point Cloud and Procedural Model Dataset of\n  Field-Grown Maize from a Diversity Panel", "author": "Elvis Kimara and Mozhgan Hadadi and Jackson Godbersen and Aditya Balu and Talukder Jubery and Yawei Li and Adarsh Krishnamurthy and Patrick S. Schnable and Baskar Ganapathysubramanian", "abstract": "  The development of artificial intelligence (AI) and machine learning (ML)\nbased tools for 3D phenotyping, especially for maize, has been limited due to\nthe lack of large and diverse 3D datasets. 2D image datasets fail to capture\nessential structural details such as leaf architecture, plant volume, and\nspatial arrangements that 3D data provide. To address this limitation, we\npresent MaizeField3D (https://baskargroup.github.io/MaizeField3D/), a curated\ndataset of 3D point clouds of field-grown maize plants from a diverse genetic\npanel, designed to be AI-ready for advancing agricultural research. Our dataset\nincludes 1,045 high-quality point clouds of field-grown maize collected using a\nterrestrial laser scanner (TLS). Point clouds of 520 plants from this dataset\nwere segmented and annotated using a graph-based segmentation method to isolate\nindividual leaves and stalks, ensuring consistent labeling across all samples.\nThis labeled data was then used for fitting procedural models that provide a\nstructured parametric representation of the maize plants. The leaves of the\nmaize plants in the procedural models are represented using Non-Uniform\nRational B-Spline (NURBS) surfaces that were generated using a two-step\noptimization process combining gradient-free and gradient-based methods. We\nconducted rigorous manual quality control on all datasets, correcting errors in\nsegmentation, ensuring accurate leaf ordering, and validating metadata\nannotations. The dataset also includes metadata detailing plant morphology and\nquality, alongside multi-resolution subsampled point cloud data (100k, 50k, 10k\npoints), which can be readily used for different downstream computational\ntasks. MaizeField3D will serve as a comprehensive foundational dataset for\nAI-driven phenotyping, plant structural analysis, and 3D applications in\nagricultural research.\n", "link": "http://arxiv.org/abs/2503.07813v3", "date": "2025-07-03", "relevancy": 2.7306, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel&body=Title%3A%20MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel%0AAuthor%3A%20Elvis%20Kimara%20and%20Mozhgan%20Hadadi%20and%20Jackson%20Godbersen%20and%20Aditya%20Balu%20and%20Talukder%20Jubery%20and%20Yawei%20Li%20and%20Adarsh%20Krishnamurthy%20and%20Patrick%20S.%20Schnable%20and%20Baskar%20Ganapathysubramanian%0AAbstract%3A%20%20%20The%20development%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%0Abased%20tools%20for%203D%20phenotyping%2C%20especially%20for%20maize%2C%20has%20been%20limited%20due%20to%0Athe%20lack%20of%20large%20and%20diverse%203D%20datasets.%202D%20image%20datasets%20fail%20to%20capture%0Aessential%20structural%20details%20such%20as%20leaf%20architecture%2C%20plant%20volume%2C%20and%0Aspatial%20arrangements%20that%203D%20data%20provide.%20To%20address%20this%20limitation%2C%20we%0Apresent%20MaizeField3D%20%28https%3A//baskargroup.github.io/MaizeField3D/%29%2C%20a%20curated%0Adataset%20of%203D%20point%20clouds%20of%20field-grown%20maize%20plants%20from%20a%20diverse%20genetic%0Apanel%2C%20designed%20to%20be%20AI-ready%20for%20advancing%20agricultural%20research.%20Our%20dataset%0Aincludes%201%2C045%20high-quality%20point%20clouds%20of%20field-grown%20maize%20collected%20using%20a%0Aterrestrial%20laser%20scanner%20%28TLS%29.%20Point%20clouds%20of%20520%20plants%20from%20this%20dataset%0Awere%20segmented%20and%20annotated%20using%20a%20graph-based%20segmentation%20method%20to%20isolate%0Aindividual%20leaves%20and%20stalks%2C%20ensuring%20consistent%20labeling%20across%20all%20samples.%0AThis%20labeled%20data%20was%20then%20used%20for%20fitting%20procedural%20models%20that%20provide%20a%0Astructured%20parametric%20representation%20of%20the%20maize%20plants.%20The%20leaves%20of%20the%0Amaize%20plants%20in%20the%20procedural%20models%20are%20represented%20using%20Non-Uniform%0ARational%20B-Spline%20%28NURBS%29%20surfaces%20that%20were%20generated%20using%20a%20two-step%0Aoptimization%20process%20combining%20gradient-free%20and%20gradient-based%20methods.%20We%0Aconducted%20rigorous%20manual%20quality%20control%20on%20all%20datasets%2C%20correcting%20errors%20in%0Asegmentation%2C%20ensuring%20accurate%20leaf%20ordering%2C%20and%20validating%20metadata%0Aannotations.%20The%20dataset%20also%20includes%20metadata%20detailing%20plant%20morphology%20and%0Aquality%2C%20alongside%20multi-resolution%20subsampled%20point%20cloud%20data%20%28100k%2C%2050k%2C%2010k%0Apoints%29%2C%20which%20can%20be%20readily%20used%20for%20different%20downstream%20computational%0Atasks.%20MaizeField3D%20will%20serve%20as%20a%20comprehensive%20foundational%20dataset%20for%0AAI-driven%20phenotyping%2C%20plant%20structural%20analysis%2C%20and%203D%20applications%20in%0Aagricultural%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaizeField3D%253A%2520A%2520Curated%25203D%2520Point%2520Cloud%2520and%2520Procedural%2520Model%2520Dataset%2520of%250A%2520%2520Field-Grown%2520Maize%2520from%2520a%2520Diversity%2520Panel%26entry.906535625%3DElvis%2520Kimara%2520and%2520Mozhgan%2520Hadadi%2520and%2520Jackson%2520Godbersen%2520and%2520Aditya%2520Balu%2520and%2520Talukder%2520Jubery%2520and%2520Yawei%2520Li%2520and%2520Adarsh%2520Krishnamurthy%2520and%2520Patrick%2520S.%2520Schnable%2520and%2520Baskar%2520Ganapathysubramanian%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520machine%2520learning%2520%2528ML%2529%250Abased%2520tools%2520for%25203D%2520phenotyping%252C%2520especially%2520for%2520maize%252C%2520has%2520been%2520limited%2520due%2520to%250Athe%2520lack%2520of%2520large%2520and%2520diverse%25203D%2520datasets.%25202D%2520image%2520datasets%2520fail%2520to%2520capture%250Aessential%2520structural%2520details%2520such%2520as%2520leaf%2520architecture%252C%2520plant%2520volume%252C%2520and%250Aspatial%2520arrangements%2520that%25203D%2520data%2520provide.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apresent%2520MaizeField3D%2520%2528https%253A//baskargroup.github.io/MaizeField3D/%2529%252C%2520a%2520curated%250Adataset%2520of%25203D%2520point%2520clouds%2520of%2520field-grown%2520maize%2520plants%2520from%2520a%2520diverse%2520genetic%250Apanel%252C%2520designed%2520to%2520be%2520AI-ready%2520for%2520advancing%2520agricultural%2520research.%2520Our%2520dataset%250Aincludes%25201%252C045%2520high-quality%2520point%2520clouds%2520of%2520field-grown%2520maize%2520collected%2520using%2520a%250Aterrestrial%2520laser%2520scanner%2520%2528TLS%2529.%2520Point%2520clouds%2520of%2520520%2520plants%2520from%2520this%2520dataset%250Awere%2520segmented%2520and%2520annotated%2520using%2520a%2520graph-based%2520segmentation%2520method%2520to%2520isolate%250Aindividual%2520leaves%2520and%2520stalks%252C%2520ensuring%2520consistent%2520labeling%2520across%2520all%2520samples.%250AThis%2520labeled%2520data%2520was%2520then%2520used%2520for%2520fitting%2520procedural%2520models%2520that%2520provide%2520a%250Astructured%2520parametric%2520representation%2520of%2520the%2520maize%2520plants.%2520The%2520leaves%2520of%2520the%250Amaize%2520plants%2520in%2520the%2520procedural%2520models%2520are%2520represented%2520using%2520Non-Uniform%250ARational%2520B-Spline%2520%2528NURBS%2529%2520surfaces%2520that%2520were%2520generated%2520using%2520a%2520two-step%250Aoptimization%2520process%2520combining%2520gradient-free%2520and%2520gradient-based%2520methods.%2520We%250Aconducted%2520rigorous%2520manual%2520quality%2520control%2520on%2520all%2520datasets%252C%2520correcting%2520errors%2520in%250Asegmentation%252C%2520ensuring%2520accurate%2520leaf%2520ordering%252C%2520and%2520validating%2520metadata%250Aannotations.%2520The%2520dataset%2520also%2520includes%2520metadata%2520detailing%2520plant%2520morphology%2520and%250Aquality%252C%2520alongside%2520multi-resolution%2520subsampled%2520point%2520cloud%2520data%2520%2528100k%252C%252050k%252C%252010k%250Apoints%2529%252C%2520which%2520can%2520be%2520readily%2520used%2520for%2520different%2520downstream%2520computational%250Atasks.%2520MaizeField3D%2520will%2520serve%2520as%2520a%2520comprehensive%2520foundational%2520dataset%2520for%250AAI-driven%2520phenotyping%252C%2520plant%2520structural%2520analysis%252C%2520and%25203D%2520applications%2520in%250Aagricultural%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel&entry.906535625=Elvis%20Kimara%20and%20Mozhgan%20Hadadi%20and%20Jackson%20Godbersen%20and%20Aditya%20Balu%20and%20Talukder%20Jubery%20and%20Yawei%20Li%20and%20Adarsh%20Krishnamurthy%20and%20Patrick%20S.%20Schnable%20and%20Baskar%20Ganapathysubramanian&entry.1292438233=%20%20The%20development%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%0Abased%20tools%20for%203D%20phenotyping%2C%20especially%20for%20maize%2C%20has%20been%20limited%20due%20to%0Athe%20lack%20of%20large%20and%20diverse%203D%20datasets.%202D%20image%20datasets%20fail%20to%20capture%0Aessential%20structural%20details%20such%20as%20leaf%20architecture%2C%20plant%20volume%2C%20and%0Aspatial%20arrangements%20that%203D%20data%20provide.%20To%20address%20this%20limitation%2C%20we%0Apresent%20MaizeField3D%20%28https%3A//baskargroup.github.io/MaizeField3D/%29%2C%20a%20curated%0Adataset%20of%203D%20point%20clouds%20of%20field-grown%20maize%20plants%20from%20a%20diverse%20genetic%0Apanel%2C%20designed%20to%20be%20AI-ready%20for%20advancing%20agricultural%20research.%20Our%20dataset%0Aincludes%201%2C045%20high-quality%20point%20clouds%20of%20field-grown%20maize%20collected%20using%20a%0Aterrestrial%20laser%20scanner%20%28TLS%29.%20Point%20clouds%20of%20520%20plants%20from%20this%20dataset%0Awere%20segmented%20and%20annotated%20using%20a%20graph-based%20segmentation%20method%20to%20isolate%0Aindividual%20leaves%20and%20stalks%2C%20ensuring%20consistent%20labeling%20across%20all%20samples.%0AThis%20labeled%20data%20was%20then%20used%20for%20fitting%20procedural%20models%20that%20provide%20a%0Astructured%20parametric%20representation%20of%20the%20maize%20plants.%20The%20leaves%20of%20the%0Amaize%20plants%20in%20the%20procedural%20models%20are%20represented%20using%20Non-Uniform%0ARational%20B-Spline%20%28NURBS%29%20surfaces%20that%20were%20generated%20using%20a%20two-step%0Aoptimization%20process%20combining%20gradient-free%20and%20gradient-based%20methods.%20We%0Aconducted%20rigorous%20manual%20quality%20control%20on%20all%20datasets%2C%20correcting%20errors%20in%0Asegmentation%2C%20ensuring%20accurate%20leaf%20ordering%2C%20and%20validating%20metadata%0Aannotations.%20The%20dataset%20also%20includes%20metadata%20detailing%20plant%20morphology%20and%0Aquality%2C%20alongside%20multi-resolution%20subsampled%20point%20cloud%20data%20%28100k%2C%2050k%2C%2010k%0Apoints%29%2C%20which%20can%20be%20readily%20used%20for%20different%20downstream%20computational%0Atasks.%20MaizeField3D%20will%20serve%20as%20a%20comprehensive%20foundational%20dataset%20for%0AAI-driven%20phenotyping%2C%20plant%20structural%20analysis%2C%20and%203D%20applications%20in%0Aagricultural%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07813v3&entry.124074799=Read"},
{"title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "author": "Shaocheng Yan and Pengcheng Shi and Zhenjun Zhao and Kaixin Wang and Kuang Cao and Ji Wu and Jiayuan Li", "abstract": "  Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.\n", "link": "http://arxiv.org/abs/2507.01439v2", "date": "2025-07-03", "relevancy": 2.5972, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5417}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5109}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration&body=Title%3A%20TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration%0AAuthor%3A%20Shaocheng%20Yan%20and%20Pengcheng%20Shi%20and%20Zhenjun%20Zhao%20and%20Kaixin%20Wang%20and%20Kuang%20Cao%20and%20Ji%20Wu%20and%20Jiayuan%20Li%0AAbstract%3A%20%20%20Robust%20estimation%20is%20essential%20in%20correspondence-based%20Point%20Cloud%0ARegistration%20%28PCR%29.%20Existing%20methods%20using%20maximal%20clique%20search%20in%0Acompatibility%20graphs%20achieve%20high%20recall%20but%20suffer%20from%20exponential%20time%0Acomplexity%2C%20limiting%20their%20use%20in%20time-sensitive%20applications.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20fast%20and%20robust%20estimator%2C%20TurboReg%2C%20built%20upon%20a%20novel%0Alightweight%20clique%2C%20TurboClique%2C%20and%20a%20highly%20parallelizable%20Pivot-Guided%0ASearch%20%28PGS%29%20algorithm.%20First%2C%20we%20define%20the%20TurboClique%20as%20a%203-clique%20within%20a%0Ahighly-constrained%20compatibility%20graph.%20The%20lightweight%20nature%20of%20the%203-clique%0Aallows%20for%20efficient%20parallel%20searching%2C%20and%20the%20highly-constrained%0Acompatibility%20graph%20ensures%20robust%20spatial%20consistency%20for%20stable%0Atransformation%20estimation.%20Next%2C%20PGS%20selects%20matching%20pairs%20with%20high%20SC%24%5E2%24%0Ascores%20as%20pivots%2C%20effectively%20guiding%20the%20search%20toward%20TurboCliques%20with%0Ahigher%20inlier%20ratios.%20Moreover%2C%20the%20PGS%20algorithm%20has%20linear%20time%20complexity%0Aand%20is%20significantly%20more%20efficient%20than%20the%20maximal%20clique%20search%20with%0Aexponential%20time%20complexity.%20Extensive%20experiments%20show%20that%20TurboReg%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20real-world%20datasets%2C%20with%0Asubstantial%20speed%20improvements.%20For%20example%2C%20on%20the%203DMatch%2BFCGF%20dataset%2C%0ATurboReg%20%281K%29%20operates%20%24208.22%5Ctimes%24%20faster%20than%203DMAC%20while%20also%20achieving%0Ahigher%20recall.%20Our%20code%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/Laka-3DV/TurboReg%7D%7B%5Ctexttt%7BTurboReg%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboReg%253A%2520TurboClique%2520for%2520Robust%2520and%2520Efficient%2520Point%2520Cloud%2520Registration%26entry.906535625%3DShaocheng%2520Yan%2520and%2520Pengcheng%2520Shi%2520and%2520Zhenjun%2520Zhao%2520and%2520Kaixin%2520Wang%2520and%2520Kuang%2520Cao%2520and%2520Ji%2520Wu%2520and%2520Jiayuan%2520Li%26entry.1292438233%3D%2520%2520Robust%2520estimation%2520is%2520essential%2520in%2520correspondence-based%2520Point%2520Cloud%250ARegistration%2520%2528PCR%2529.%2520Existing%2520methods%2520using%2520maximal%2520clique%2520search%2520in%250Acompatibility%2520graphs%2520achieve%2520high%2520recall%2520but%2520suffer%2520from%2520exponential%2520time%250Acomplexity%252C%2520limiting%2520their%2520use%2520in%2520time-sensitive%2520applications.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520fast%2520and%2520robust%2520estimator%252C%2520TurboReg%252C%2520built%2520upon%2520a%2520novel%250Alightweight%2520clique%252C%2520TurboClique%252C%2520and%2520a%2520highly%2520parallelizable%2520Pivot-Guided%250ASearch%2520%2528PGS%2529%2520algorithm.%2520First%252C%2520we%2520define%2520the%2520TurboClique%2520as%2520a%25203-clique%2520within%2520a%250Ahighly-constrained%2520compatibility%2520graph.%2520The%2520lightweight%2520nature%2520of%2520the%25203-clique%250Aallows%2520for%2520efficient%2520parallel%2520searching%252C%2520and%2520the%2520highly-constrained%250Acompatibility%2520graph%2520ensures%2520robust%2520spatial%2520consistency%2520for%2520stable%250Atransformation%2520estimation.%2520Next%252C%2520PGS%2520selects%2520matching%2520pairs%2520with%2520high%2520SC%2524%255E2%2524%250Ascores%2520as%2520pivots%252C%2520effectively%2520guiding%2520the%2520search%2520toward%2520TurboCliques%2520with%250Ahigher%2520inlier%2520ratios.%2520Moreover%252C%2520the%2520PGS%2520algorithm%2520has%2520linear%2520time%2520complexity%250Aand%2520is%2520significantly%2520more%2520efficient%2520than%2520the%2520maximal%2520clique%2520search%2520with%250Aexponential%2520time%2520complexity.%2520Extensive%2520experiments%2520show%2520that%2520TurboReg%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520real-world%2520datasets%252C%2520with%250Asubstantial%2520speed%2520improvements.%2520For%2520example%252C%2520on%2520the%25203DMatch%252BFCGF%2520dataset%252C%250ATurboReg%2520%25281K%2529%2520operates%2520%2524208.22%255Ctimes%2524%2520faster%2520than%25203DMAC%2520while%2520also%2520achieving%250Ahigher%2520recall.%2520Our%2520code%2520is%2520accessible%2520at%250A%255Chref%257Bhttps%253A//github.com/Laka-3DV/TurboReg%257D%257B%255Ctexttt%257BTurboReg%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration&entry.906535625=Shaocheng%20Yan%20and%20Pengcheng%20Shi%20and%20Zhenjun%20Zhao%20and%20Kaixin%20Wang%20and%20Kuang%20Cao%20and%20Ji%20Wu%20and%20Jiayuan%20Li&entry.1292438233=%20%20Robust%20estimation%20is%20essential%20in%20correspondence-based%20Point%20Cloud%0ARegistration%20%28PCR%29.%20Existing%20methods%20using%20maximal%20clique%20search%20in%0Acompatibility%20graphs%20achieve%20high%20recall%20but%20suffer%20from%20exponential%20time%0Acomplexity%2C%20limiting%20their%20use%20in%20time-sensitive%20applications.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20fast%20and%20robust%20estimator%2C%20TurboReg%2C%20built%20upon%20a%20novel%0Alightweight%20clique%2C%20TurboClique%2C%20and%20a%20highly%20parallelizable%20Pivot-Guided%0ASearch%20%28PGS%29%20algorithm.%20First%2C%20we%20define%20the%20TurboClique%20as%20a%203-clique%20within%20a%0Ahighly-constrained%20compatibility%20graph.%20The%20lightweight%20nature%20of%20the%203-clique%0Aallows%20for%20efficient%20parallel%20searching%2C%20and%20the%20highly-constrained%0Acompatibility%20graph%20ensures%20robust%20spatial%20consistency%20for%20stable%0Atransformation%20estimation.%20Next%2C%20PGS%20selects%20matching%20pairs%20with%20high%20SC%24%5E2%24%0Ascores%20as%20pivots%2C%20effectively%20guiding%20the%20search%20toward%20TurboCliques%20with%0Ahigher%20inlier%20ratios.%20Moreover%2C%20the%20PGS%20algorithm%20has%20linear%20time%20complexity%0Aand%20is%20significantly%20more%20efficient%20than%20the%20maximal%20clique%20search%20with%0Aexponential%20time%20complexity.%20Extensive%20experiments%20show%20that%20TurboReg%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20real-world%20datasets%2C%20with%0Asubstantial%20speed%20improvements.%20For%20example%2C%20on%20the%203DMatch%2BFCGF%20dataset%2C%0ATurboReg%20%281K%29%20operates%20%24208.22%5Ctimes%24%20faster%20than%203DMAC%20while%20also%20achieving%0Ahigher%20recall.%20Our%20code%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/Laka-3DV/TurboReg%7D%7B%5Ctexttt%7BTurboReg%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01439v2&entry.124074799=Read"},
{"title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "author": "Qingyu Yin and Chak Tou Leong and Hongbo Zhang and Minjun Zhu and Hanqi Yan and Qiang Zhang and Yulan He and Wenjie Li and Jun Wang and Yue Zhang and Linyi Yang", "abstract": "  The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.\n", "link": "http://arxiv.org/abs/2411.07618v2", "date": "2025-07-03", "relevancy": 2.5929, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints&body=Title%3A%20Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints%0AAuthor%3A%20Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Hongbo%20Zhang%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Preference%2520Optimization%2520Using%2520Sparse%2520Feature-Level%2520Constraints%26entry.906535625%3DQingyu%2520Yin%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Hongbo%2520Zhang%2520and%2520Minjun%2520Zhu%2520and%2520Hanqi%2520Yan%2520and%2520Qiang%2520Zhang%2520and%2520Yulan%2520He%2520and%2520Wenjie%2520Li%2520and%2520Jun%2520Wang%2520and%2520Yue%2520Zhang%2520and%2520Linyi%2520Yang%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520preferences%2520remains%250Aa%2520key%2520challenge.%2520While%2520post-training%2520techniques%2520like%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520have%250Aachieved%2520notable%2520success%252C%2520they%2520often%2520introduce%2520computational%2520inefficiencies%2520and%250Atraining%2520instability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Feature-level%2520constrained%250APreference%2520Optimization%2520%2528FPO%2529%252C%2520a%2520novel%2520method%2520designed%2520to%2520simplify%2520the%250Aalignment%2520process%2520while%2520ensuring%2520stability.%2520FPO%2520leverages%2520pre-trained%2520Sparse%250AAutoencoders%2520%2528SAEs%2529%2520and%2520introduces%2520feature-level%2520constraints%252C%2520allowing%2520for%250Aefficient%252C%2520sparsity-enforced%2520alignment.%2520Our%2520approach%2520enjoys%2520efficiency%2520by%2520using%250Asparse%2520features%2520activated%2520in%2520a%2520well-trained%2520sparse%2520autoencoder%2520and%2520the%2520quality%250Aof%2520sequential%2520KL%2520divergence%2520by%2520using%2520the%2520feature-level%2520offline%2520reference.%250AExperimental%2520results%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520FPO%2520achieves%2520a%250A5.08%2525%2520absolute%2520improvement%2520in%2520win%2520rate%2520with%2520much%2520lower%2520computational%2520cost%250Acompared%2520to%2520state-of-the-art%2520baselines%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%250Aefficient%2520and%2520controllable%2520LLM%2520alignments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints&entry.906535625=Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Hongbo%20Zhang%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07618v2&entry.124074799=Read"},
{"title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "author": "Lin Wu and Zhixiang Chen and Jianglin Lan", "abstract": "  Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.\n", "link": "http://arxiv.org/abs/2507.01737v2", "date": "2025-07-03", "relevancy": 2.5525, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6926}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6198}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion&body=Title%3A%20HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion%0AAuthor%3A%20Lin%20Wu%20and%20Zhixiang%20Chen%20and%20Jianglin%20Lan%0AAbstract%3A%20%20%20Generating%20realistic%203D%20human-object%20interactions%20%28HOIs%29%20remains%20a%0Achallenging%20task%20due%20to%20the%20difficulty%20of%20modeling%20detailed%20interaction%0Adynamics.%20Existing%20methods%20treat%20human%20and%20object%20motions%20independently%2C%0Aresulting%20in%20physically%20implausible%20and%20causally%20inconsistent%20behaviors.%20In%0Athis%20work%2C%20we%20present%20HOI-Dyn%2C%20a%20novel%20framework%20that%20formulates%20HOI%20generation%0Aas%20a%20driver-responder%20system%2C%20where%20human%20actions%20drive%20object%20responses.%20At%0Athe%20core%20of%20our%20method%20is%20a%20lightweight%20transformer-based%20interaction%20dynamics%0Amodel%20that%20explicitly%20predicts%20how%20objects%20should%20react%20to%20human%20motion.%20To%0Afurther%20enforce%20consistency%2C%20we%20introduce%20a%20residual-based%20dynamics%20loss%20that%0Amitigates%20the%20impact%20of%20dynamics%20prediction%20errors%20and%20prevents%20misleading%0Aoptimization%20signals.%20The%20dynamics%20model%20is%20used%20only%20during%20training%2C%0Apreserving%20inference%20efficiency.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20the%20quality%20of%0AHOI%20generation%20but%20also%20establishes%20a%20feasible%20metric%20for%20evaluating%20the%0Aquality%20of%20generated%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOI-Dyn%253A%2520Learning%2520Interaction%2520Dynamics%2520for%2520Human-Object%2520Motion%2520Diffusion%26entry.906535625%3DLin%2520Wu%2520and%2520Zhixiang%2520Chen%2520and%2520Jianglin%2520Lan%26entry.1292438233%3D%2520%2520Generating%2520realistic%25203D%2520human-object%2520interactions%2520%2528HOIs%2529%2520remains%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520difficulty%2520of%2520modeling%2520detailed%2520interaction%250Adynamics.%2520Existing%2520methods%2520treat%2520human%2520and%2520object%2520motions%2520independently%252C%250Aresulting%2520in%2520physically%2520implausible%2520and%2520causally%2520inconsistent%2520behaviors.%2520In%250Athis%2520work%252C%2520we%2520present%2520HOI-Dyn%252C%2520a%2520novel%2520framework%2520that%2520formulates%2520HOI%2520generation%250Aas%2520a%2520driver-responder%2520system%252C%2520where%2520human%2520actions%2520drive%2520object%2520responses.%2520At%250Athe%2520core%2520of%2520our%2520method%2520is%2520a%2520lightweight%2520transformer-based%2520interaction%2520dynamics%250Amodel%2520that%2520explicitly%2520predicts%2520how%2520objects%2520should%2520react%2520to%2520human%2520motion.%2520To%250Afurther%2520enforce%2520consistency%252C%2520we%2520introduce%2520a%2520residual-based%2520dynamics%2520loss%2520that%250Amitigates%2520the%2520impact%2520of%2520dynamics%2520prediction%2520errors%2520and%2520prevents%2520misleading%250Aoptimization%2520signals.%2520The%2520dynamics%2520model%2520is%2520used%2520only%2520during%2520training%252C%250Apreserving%2520inference%2520efficiency.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520enhances%2520the%2520quality%2520of%250AHOI%2520generation%2520but%2520also%2520establishes%2520a%2520feasible%2520metric%2520for%2520evaluating%2520the%250Aquality%2520of%2520generated%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion&entry.906535625=Lin%20Wu%20and%20Zhixiang%20Chen%20and%20Jianglin%20Lan&entry.1292438233=%20%20Generating%20realistic%203D%20human-object%20interactions%20%28HOIs%29%20remains%20a%0Achallenging%20task%20due%20to%20the%20difficulty%20of%20modeling%20detailed%20interaction%0Adynamics.%20Existing%20methods%20treat%20human%20and%20object%20motions%20independently%2C%0Aresulting%20in%20physically%20implausible%20and%20causally%20inconsistent%20behaviors.%20In%0Athis%20work%2C%20we%20present%20HOI-Dyn%2C%20a%20novel%20framework%20that%20formulates%20HOI%20generation%0Aas%20a%20driver-responder%20system%2C%20where%20human%20actions%20drive%20object%20responses.%20At%0Athe%20core%20of%20our%20method%20is%20a%20lightweight%20transformer-based%20interaction%20dynamics%0Amodel%20that%20explicitly%20predicts%20how%20objects%20should%20react%20to%20human%20motion.%20To%0Afurther%20enforce%20consistency%2C%20we%20introduce%20a%20residual-based%20dynamics%20loss%20that%0Amitigates%20the%20impact%20of%20dynamics%20prediction%20errors%20and%20prevents%20misleading%0Aoptimization%20signals.%20The%20dynamics%20model%20is%20used%20only%20during%20training%2C%0Apreserving%20inference%20efficiency.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20the%20quality%20of%0AHOI%20generation%20but%20also%20establishes%20a%20feasible%20metric%20for%20evaluating%20the%0Aquality%20of%20generated%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01737v2&entry.124074799=Read"},
{"title": "Parametric shape models for vessels learned from segmentations via\n  differentiable voxelization", "author": "Alina F. Dima and Suprosanna Shit and Huaqi Qiu and Robbie Holland and Tamara T. Mueller and Fabio Antonio Musio and Kaiyuan Yang and Bjoern Menze and Rickmer Braren and Marcus Makowski and Daniel Rueckert", "abstract": "  Vessels are complex structures in the body that have been studied extensively\nin multiple representations. While voxelization is the most common of them,\nmeshes and parametric models are critical in various applications due to their\ndesirable properties. However, these representations are typically extracted\nthrough segmentations and used disjointly from each other. We propose a\nframework that joins the three representations under differentiable\ntransformations. By leveraging differentiable voxelization, we automatically\nextract a parametric shape model of the vessels through shape-to-segmentation\nfitting, where we learn shape parameters from segmentations without the\nexplicit need for ground-truth shape parameters. The vessel is parametrized as\ncenterlines and radii using cubic B-splines, ensuring smoothness and continuity\nby construction. Meshes are differentiably extracted from the learned shape\nparameters, resulting in high-fidelity meshes that can be manipulated post-fit.\nOur method can accurately capture the geometry of complex vessels, as\ndemonstrated by the volumetric fits in experiments on aortas, aneurysms, and\nbrain vessels.\n", "link": "http://arxiv.org/abs/2507.02576v1", "date": "2025-07-03", "relevancy": 2.5469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4988}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization&body=Title%3A%20Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization%0AAuthor%3A%20Alina%20F.%20Dima%20and%20Suprosanna%20Shit%20and%20Huaqi%20Qiu%20and%20Robbie%20Holland%20and%20Tamara%20T.%20Mueller%20and%20Fabio%20Antonio%20Musio%20and%20Kaiyuan%20Yang%20and%20Bjoern%20Menze%20and%20Rickmer%20Braren%20and%20Marcus%20Makowski%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Vessels%20are%20complex%20structures%20in%20the%20body%20that%20have%20been%20studied%20extensively%0Ain%20multiple%20representations.%20While%20voxelization%20is%20the%20most%20common%20of%20them%2C%0Ameshes%20and%20parametric%20models%20are%20critical%20in%20various%20applications%20due%20to%20their%0Adesirable%20properties.%20However%2C%20these%20representations%20are%20typically%20extracted%0Athrough%20segmentations%20and%20used%20disjointly%20from%20each%20other.%20We%20propose%20a%0Aframework%20that%20joins%20the%20three%20representations%20under%20differentiable%0Atransformations.%20By%20leveraging%20differentiable%20voxelization%2C%20we%20automatically%0Aextract%20a%20parametric%20shape%20model%20of%20the%20vessels%20through%20shape-to-segmentation%0Afitting%2C%20where%20we%20learn%20shape%20parameters%20from%20segmentations%20without%20the%0Aexplicit%20need%20for%20ground-truth%20shape%20parameters.%20The%20vessel%20is%20parametrized%20as%0Acenterlines%20and%20radii%20using%20cubic%20B-splines%2C%20ensuring%20smoothness%20and%20continuity%0Aby%20construction.%20Meshes%20are%20differentiably%20extracted%20from%20the%20learned%20shape%0Aparameters%2C%20resulting%20in%20high-fidelity%20meshes%20that%20can%20be%20manipulated%20post-fit.%0AOur%20method%20can%20accurately%20capture%20the%20geometry%20of%20complex%20vessels%2C%20as%0Ademonstrated%20by%20the%20volumetric%20fits%20in%20experiments%20on%20aortas%2C%20aneurysms%2C%20and%0Abrain%20vessels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParametric%2520shape%2520models%2520for%2520vessels%2520learned%2520from%2520segmentations%2520via%250A%2520%2520differentiable%2520voxelization%26entry.906535625%3DAlina%2520F.%2520Dima%2520and%2520Suprosanna%2520Shit%2520and%2520Huaqi%2520Qiu%2520and%2520Robbie%2520Holland%2520and%2520Tamara%2520T.%2520Mueller%2520and%2520Fabio%2520Antonio%2520Musio%2520and%2520Kaiyuan%2520Yang%2520and%2520Bjoern%2520Menze%2520and%2520Rickmer%2520Braren%2520and%2520Marcus%2520Makowski%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Vessels%2520are%2520complex%2520structures%2520in%2520the%2520body%2520that%2520have%2520been%2520studied%2520extensively%250Ain%2520multiple%2520representations.%2520While%2520voxelization%2520is%2520the%2520most%2520common%2520of%2520them%252C%250Ameshes%2520and%2520parametric%2520models%2520are%2520critical%2520in%2520various%2520applications%2520due%2520to%2520their%250Adesirable%2520properties.%2520However%252C%2520these%2520representations%2520are%2520typically%2520extracted%250Athrough%2520segmentations%2520and%2520used%2520disjointly%2520from%2520each%2520other.%2520We%2520propose%2520a%250Aframework%2520that%2520joins%2520the%2520three%2520representations%2520under%2520differentiable%250Atransformations.%2520By%2520leveraging%2520differentiable%2520voxelization%252C%2520we%2520automatically%250Aextract%2520a%2520parametric%2520shape%2520model%2520of%2520the%2520vessels%2520through%2520shape-to-segmentation%250Afitting%252C%2520where%2520we%2520learn%2520shape%2520parameters%2520from%2520segmentations%2520without%2520the%250Aexplicit%2520need%2520for%2520ground-truth%2520shape%2520parameters.%2520The%2520vessel%2520is%2520parametrized%2520as%250Acenterlines%2520and%2520radii%2520using%2520cubic%2520B-splines%252C%2520ensuring%2520smoothness%2520and%2520continuity%250Aby%2520construction.%2520Meshes%2520are%2520differentiably%2520extracted%2520from%2520the%2520learned%2520shape%250Aparameters%252C%2520resulting%2520in%2520high-fidelity%2520meshes%2520that%2520can%2520be%2520manipulated%2520post-fit.%250AOur%2520method%2520can%2520accurately%2520capture%2520the%2520geometry%2520of%2520complex%2520vessels%252C%2520as%250Ademonstrated%2520by%2520the%2520volumetric%2520fits%2520in%2520experiments%2520on%2520aortas%252C%2520aneurysms%252C%2520and%250Abrain%2520vessels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization&entry.906535625=Alina%20F.%20Dima%20and%20Suprosanna%20Shit%20and%20Huaqi%20Qiu%20and%20Robbie%20Holland%20and%20Tamara%20T.%20Mueller%20and%20Fabio%20Antonio%20Musio%20and%20Kaiyuan%20Yang%20and%20Bjoern%20Menze%20and%20Rickmer%20Braren%20and%20Marcus%20Makowski%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Vessels%20are%20complex%20structures%20in%20the%20body%20that%20have%20been%20studied%20extensively%0Ain%20multiple%20representations.%20While%20voxelization%20is%20the%20most%20common%20of%20them%2C%0Ameshes%20and%20parametric%20models%20are%20critical%20in%20various%20applications%20due%20to%20their%0Adesirable%20properties.%20However%2C%20these%20representations%20are%20typically%20extracted%0Athrough%20segmentations%20and%20used%20disjointly%20from%20each%20other.%20We%20propose%20a%0Aframework%20that%20joins%20the%20three%20representations%20under%20differentiable%0Atransformations.%20By%20leveraging%20differentiable%20voxelization%2C%20we%20automatically%0Aextract%20a%20parametric%20shape%20model%20of%20the%20vessels%20through%20shape-to-segmentation%0Afitting%2C%20where%20we%20learn%20shape%20parameters%20from%20segmentations%20without%20the%0Aexplicit%20need%20for%20ground-truth%20shape%20parameters.%20The%20vessel%20is%20parametrized%20as%0Acenterlines%20and%20radii%20using%20cubic%20B-splines%2C%20ensuring%20smoothness%20and%20continuity%0Aby%20construction.%20Meshes%20are%20differentiably%20extracted%20from%20the%20learned%20shape%0Aparameters%2C%20resulting%20in%20high-fidelity%20meshes%20that%20can%20be%20manipulated%20post-fit.%0AOur%20method%20can%20accurately%20capture%20the%20geometry%20of%20complex%20vessels%2C%20as%0Ademonstrated%20by%20the%20volumetric%20fits%20in%20experiments%20on%20aortas%2C%20aneurysms%2C%20and%0Abrain%20vessels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02576v1&entry.124074799=Read"},
{"title": "ASDA: Audio Spectrogram Differential Attention Mechanism for\n  Self-Supervised Representation Learning", "author": "Junyu Wang and Tianrui Wang and Meng Ge and Longbiao Wang and Jianwu Dang", "abstract": "  In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.\n", "link": "http://arxiv.org/abs/2507.02666v1", "date": "2025-07-03", "relevancy": 2.5398, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5129}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5056}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning&body=Title%3A%20ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning%0AAuthor%3A%20Junyu%20Wang%20and%20Tianrui%20Wang%20and%20Meng%20Ge%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang%0AAbstract%3A%20%20%20In%20recent%20advancements%20in%20audio%20self-supervised%20representation%20learning%2C%20the%0Astandard%20Transformer%20architecture%20has%20emerged%20as%20the%20predominant%20approach%2C%20yet%0Aits%20attention%20mechanism%20often%20allocates%20a%20portion%20of%20attention%20weights%20to%0Airrelevant%20information%2C%20potentially%20impairing%20the%20model%27s%20discriminative%0Aability.%20To%20address%20this%2C%20we%20introduce%20a%20differential%20attention%20mechanism%2C%0Awhich%20effectively%20mitigates%20ineffective%20attention%20allocation%20through%20the%0Aintegration%20of%20dual-softmax%20operations%20and%20appropriately%20tuned%20differential%0Acoefficients.%20Experimental%20results%20demonstrate%20that%20our%20ASDA%20model%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20multiple%20benchmarks%2C%20including%20audio%0Aclassification%20%2849.0%25%20mAP%20on%20AS-2M%2C%2041.5%25%20mAP%20on%20AS20K%29%2C%20keyword%20spotting%0A%2898.3%25%20accuracy%20on%20SPC-2%29%2C%20and%20environmental%20sound%20classification%20%2896.1%25%0Aaccuracy%20on%20ESC-50%29.%20These%20results%20highlight%20ASDA%27s%20effectiveness%20in%20audio%0Atasks%2C%20paving%20the%20way%20for%20broader%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASDA%253A%2520Audio%2520Spectrogram%2520Differential%2520Attention%2520Mechanism%2520for%250A%2520%2520Self-Supervised%2520Representation%2520Learning%26entry.906535625%3DJunyu%2520Wang%2520and%2520Tianrui%2520Wang%2520and%2520Meng%2520Ge%2520and%2520Longbiao%2520Wang%2520and%2520Jianwu%2520Dang%26entry.1292438233%3D%2520%2520In%2520recent%2520advancements%2520in%2520audio%2520self-supervised%2520representation%2520learning%252C%2520the%250Astandard%2520Transformer%2520architecture%2520has%2520emerged%2520as%2520the%2520predominant%2520approach%252C%2520yet%250Aits%2520attention%2520mechanism%2520often%2520allocates%2520a%2520portion%2520of%2520attention%2520weights%2520to%250Airrelevant%2520information%252C%2520potentially%2520impairing%2520the%2520model%2527s%2520discriminative%250Aability.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520differential%2520attention%2520mechanism%252C%250Awhich%2520effectively%2520mitigates%2520ineffective%2520attention%2520allocation%2520through%2520the%250Aintegration%2520of%2520dual-softmax%2520operations%2520and%2520appropriately%2520tuned%2520differential%250Acoefficients.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520ASDA%2520model%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520multiple%2520benchmarks%252C%2520including%2520audio%250Aclassification%2520%252849.0%2525%2520mAP%2520on%2520AS-2M%252C%252041.5%2525%2520mAP%2520on%2520AS20K%2529%252C%2520keyword%2520spotting%250A%252898.3%2525%2520accuracy%2520on%2520SPC-2%2529%252C%2520and%2520environmental%2520sound%2520classification%2520%252896.1%2525%250Aaccuracy%2520on%2520ESC-50%2529.%2520These%2520results%2520highlight%2520ASDA%2527s%2520effectiveness%2520in%2520audio%250Atasks%252C%2520paving%2520the%2520way%2520for%2520broader%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning&entry.906535625=Junyu%20Wang%20and%20Tianrui%20Wang%20and%20Meng%20Ge%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang&entry.1292438233=%20%20In%20recent%20advancements%20in%20audio%20self-supervised%20representation%20learning%2C%20the%0Astandard%20Transformer%20architecture%20has%20emerged%20as%20the%20predominant%20approach%2C%20yet%0Aits%20attention%20mechanism%20often%20allocates%20a%20portion%20of%20attention%20weights%20to%0Airrelevant%20information%2C%20potentially%20impairing%20the%20model%27s%20discriminative%0Aability.%20To%20address%20this%2C%20we%20introduce%20a%20differential%20attention%20mechanism%2C%0Awhich%20effectively%20mitigates%20ineffective%20attention%20allocation%20through%20the%0Aintegration%20of%20dual-softmax%20operations%20and%20appropriately%20tuned%20differential%0Acoefficients.%20Experimental%20results%20demonstrate%20that%20our%20ASDA%20model%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20multiple%20benchmarks%2C%20including%20audio%0Aclassification%20%2849.0%25%20mAP%20on%20AS-2M%2C%2041.5%25%20mAP%20on%20AS20K%29%2C%20keyword%20spotting%0A%2898.3%25%20accuracy%20on%20SPC-2%29%2C%20and%20environmental%20sound%20classification%20%2896.1%25%0Aaccuracy%20on%20ESC-50%29.%20These%20results%20highlight%20ASDA%27s%20effectiveness%20in%20audio%0Atasks%2C%20paving%20the%20way%20for%20broader%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02666v1&entry.124074799=Read"},
{"title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching", "author": "Xin Zhou and Dingkang Liang and Kaijin Chen and Tianrui Feng and Xiwu Chen and Hongkai Lin and Yikang Ding and Feiyang Tan and Hengshuang Zhao and Xiang Bai", "abstract": "  Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.\n", "link": "http://arxiv.org/abs/2507.02860v1", "date": "2025-07-03", "relevancy": 2.5378, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.654}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6474}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching&body=Title%3A%20Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching%0AAuthor%3A%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Kaijin%20Chen%20and%20Tianrui%20Feng%20and%20Xiwu%20Chen%20and%20Hongkai%20Lin%20and%20Yikang%20Ding%20and%20Feiyang%20Tan%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Video%20generation%20models%20have%20demonstrated%20remarkable%20performance%2C%20yet%20their%0Abroader%20adoption%20remains%20constrained%20by%20slow%20inference%20speeds%20and%20substantial%0Acomputational%20costs%2C%20primarily%20due%20to%20the%20iterative%20nature%20of%20the%20denoising%0Aprocess.%20Addressing%20this%20bottleneck%20is%20essential%20for%20democratizing%20advanced%0Avideo%20synthesis%20technologies%20and%20enabling%20their%20integration%20into%20real-world%0Aapplications.%20This%20work%20proposes%20EasyCache%2C%20a%20training-free%20acceleration%0Aframework%20for%20video%20diffusion%20models.%20EasyCache%20introduces%20a%20lightweight%2C%0Aruntime-adaptive%20caching%20mechanism%20that%20dynamically%20reuses%20previously%20computed%0Atransformation%20vectors%2C%20avoiding%20redundant%20computations%20during%20inference.%0AUnlike%20prior%20approaches%2C%20EasyCache%20requires%20no%20offline%20profiling%2C%0Apre-computation%2C%20or%20extensive%20parameter%20tuning.%20We%20conduct%20comprehensive%0Astudies%20on%20various%20large-scale%20video%20generation%20models%2C%20including%20OpenSora%2C%0AWan2.1%2C%20and%20HunyuanVideo.%20Our%20method%20achieves%20leading%20acceleration%20performance%2C%0Areducing%20inference%20time%20by%20up%20to%202.1-3.3%24%5Ctimes%24%20compared%20to%20the%20original%0Abaselines%20while%20maintaining%20high%20visual%20fidelity%20with%20a%20significant%20up%20to%2036%25%0APSNR%20improvement%20compared%20to%20the%20previous%20SOTA%20method.%20This%20improvement%20makes%0Aour%20EasyCache%20a%20efficient%20and%20highly%20accessible%20solution%20for%20high-quality%20video%0Ageneration%20in%20both%20research%20and%20practical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/H-EmbodVis/EasyCache.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520Enough%253A%2520Training-Free%2520Video%2520Diffusion%2520Acceleration%2520via%250A%2520%2520Runtime-Adaptive%2520Caching%26entry.906535625%3DXin%2520Zhou%2520and%2520Dingkang%2520Liang%2520and%2520Kaijin%2520Chen%2520and%2520Tianrui%2520Feng%2520and%2520Xiwu%2520Chen%2520and%2520Hongkai%2520Lin%2520and%2520Yikang%2520Ding%2520and%2520Feiyang%2520Tan%2520and%2520Hengshuang%2520Zhao%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520have%2520demonstrated%2520remarkable%2520performance%252C%2520yet%2520their%250Abroader%2520adoption%2520remains%2520constrained%2520by%2520slow%2520inference%2520speeds%2520and%2520substantial%250Acomputational%2520costs%252C%2520primarily%2520due%2520to%2520the%2520iterative%2520nature%2520of%2520the%2520denoising%250Aprocess.%2520Addressing%2520this%2520bottleneck%2520is%2520essential%2520for%2520democratizing%2520advanced%250Avideo%2520synthesis%2520technologies%2520and%2520enabling%2520their%2520integration%2520into%2520real-world%250Aapplications.%2520This%2520work%2520proposes%2520EasyCache%252C%2520a%2520training-free%2520acceleration%250Aframework%2520for%2520video%2520diffusion%2520models.%2520EasyCache%2520introduces%2520a%2520lightweight%252C%250Aruntime-adaptive%2520caching%2520mechanism%2520that%2520dynamically%2520reuses%2520previously%2520computed%250Atransformation%2520vectors%252C%2520avoiding%2520redundant%2520computations%2520during%2520inference.%250AUnlike%2520prior%2520approaches%252C%2520EasyCache%2520requires%2520no%2520offline%2520profiling%252C%250Apre-computation%252C%2520or%2520extensive%2520parameter%2520tuning.%2520We%2520conduct%2520comprehensive%250Astudies%2520on%2520various%2520large-scale%2520video%2520generation%2520models%252C%2520including%2520OpenSora%252C%250AWan2.1%252C%2520and%2520HunyuanVideo.%2520Our%2520method%2520achieves%2520leading%2520acceleration%2520performance%252C%250Areducing%2520inference%2520time%2520by%2520up%2520to%25202.1-3.3%2524%255Ctimes%2524%2520compared%2520to%2520the%2520original%250Abaselines%2520while%2520maintaining%2520high%2520visual%2520fidelity%2520with%2520a%2520significant%2520up%2520to%252036%2525%250APSNR%2520improvement%2520compared%2520to%2520the%2520previous%2520SOTA%2520method.%2520This%2520improvement%2520makes%250Aour%2520EasyCache%2520a%2520efficient%2520and%2520highly%2520accessible%2520solution%2520for%2520high-quality%2520video%250Ageneration%2520in%2520both%2520research%2520and%2520practical%2520applications.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/H-EmbodVis/EasyCache.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching&entry.906535625=Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Kaijin%20Chen%20and%20Tianrui%20Feng%20and%20Xiwu%20Chen%20and%20Hongkai%20Lin%20and%20Yikang%20Ding%20and%20Feiyang%20Tan%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai&entry.1292438233=%20%20Video%20generation%20models%20have%20demonstrated%20remarkable%20performance%2C%20yet%20their%0Abroader%20adoption%20remains%20constrained%20by%20slow%20inference%20speeds%20and%20substantial%0Acomputational%20costs%2C%20primarily%20due%20to%20the%20iterative%20nature%20of%20the%20denoising%0Aprocess.%20Addressing%20this%20bottleneck%20is%20essential%20for%20democratizing%20advanced%0Avideo%20synthesis%20technologies%20and%20enabling%20their%20integration%20into%20real-world%0Aapplications.%20This%20work%20proposes%20EasyCache%2C%20a%20training-free%20acceleration%0Aframework%20for%20video%20diffusion%20models.%20EasyCache%20introduces%20a%20lightweight%2C%0Aruntime-adaptive%20caching%20mechanism%20that%20dynamically%20reuses%20previously%20computed%0Atransformation%20vectors%2C%20avoiding%20redundant%20computations%20during%20inference.%0AUnlike%20prior%20approaches%2C%20EasyCache%20requires%20no%20offline%20profiling%2C%0Apre-computation%2C%20or%20extensive%20parameter%20tuning.%20We%20conduct%20comprehensive%0Astudies%20on%20various%20large-scale%20video%20generation%20models%2C%20including%20OpenSora%2C%0AWan2.1%2C%20and%20HunyuanVideo.%20Our%20method%20achieves%20leading%20acceleration%20performance%2C%0Areducing%20inference%20time%20by%20up%20to%202.1-3.3%24%5Ctimes%24%20compared%20to%20the%20original%0Abaselines%20while%20maintaining%20high%20visual%20fidelity%20with%20a%20significant%20up%20to%2036%25%0APSNR%20improvement%20compared%20to%20the%20previous%20SOTA%20method.%20This%20improvement%20makes%0Aour%20EasyCache%20a%20efficient%20and%20highly%20accessible%20solution%20for%20high-quality%20video%0Ageneration%20in%20both%20research%20and%20practical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/H-EmbodVis/EasyCache.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02860v1&entry.124074799=Read"},
{"title": "AnyI2V: Animating Any Conditional Image with Motion Control", "author": "Ziye Li and Hao Luo and Xincheng Shuai and Henghui Ding", "abstract": "  Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.\n", "link": "http://arxiv.org/abs/2507.02857v1", "date": "2025-07-03", "relevancy": 2.5184, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6306}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6298}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control&body=Title%3A%20AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control%0AAuthor%3A%20Ziye%20Li%20and%20Hao%20Luo%20and%20Xincheng%20Shuai%20and%20Henghui%20Ding%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%2C%20particularly%20in%20diffusion%20models%2C%0Ahave%20driven%20notable%20progress%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%0Asynthesis.%20However%2C%20challenges%20remain%20in%20effectively%20integrating%20dynamic%20motion%0Asignals%20and%20flexible%20spatial%20constraints.%20Existing%20T2V%20methods%20typically%20rely%0Aon%20text%20prompts%2C%20which%20inherently%20lack%20precise%20control%20over%20the%20spatial%20layout%0Aof%20generated%20content.%20In%20contrast%2C%20I2V%20methods%20are%20limited%20by%20their%20dependence%0Aon%20real%20images%2C%20which%20restricts%20the%20editability%20of%20the%20synthesized%20content.%0AAlthough%20some%20methods%20incorporate%20ControlNet%20to%20introduce%20image-based%0Aconditioning%2C%20they%20often%20lack%20explicit%20motion%20control%20and%20require%0Acomputationally%20expensive%20training.%20To%20address%20these%20limitations%2C%20we%20propose%0AAnyI2V%2C%20a%20training-free%20framework%20that%20animates%20any%20conditional%20images%20with%0Auser-defined%20motion%20trajectories.%20AnyI2V%20supports%20a%20broader%20range%20of%20modalities%0Aas%20the%20conditional%20image%2C%20including%20data%20types%20such%20as%20meshes%20and%20point%20clouds%0Athat%20are%20not%20supported%20by%20ControlNet%2C%20enabling%20more%20flexible%20and%20versatile%0Avideo%20generation.%20Additionally%2C%20it%20supports%20mixed%20conditional%20inputs%20and%0Aenables%20style%20transfer%20and%20editing%20via%20LoRA%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20AnyI2V%20achieves%20superior%20performance%0Aand%20provides%20a%20new%20perspective%20in%20spatial-%20and%20motion-controlled%20video%0Ageneration.%20Code%20is%20available%20at%20https%3A//henghuiding.com/AnyI2V/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyI2V%253A%2520Animating%2520Any%2520Conditional%2520Image%2520with%2520Motion%2520Control%26entry.906535625%3DZiye%2520Li%2520and%2520Hao%2520Luo%2520and%2520Xincheng%2520Shuai%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%252C%2520particularly%2520in%2520diffusion%2520models%252C%250Ahave%2520driven%2520notable%2520progress%2520in%2520text-to-video%2520%2528T2V%2529%2520and%2520image-to-video%2520%2528I2V%2529%250Asynthesis.%2520However%252C%2520challenges%2520remain%2520in%2520effectively%2520integrating%2520dynamic%2520motion%250Asignals%2520and%2520flexible%2520spatial%2520constraints.%2520Existing%2520T2V%2520methods%2520typically%2520rely%250Aon%2520text%2520prompts%252C%2520which%2520inherently%2520lack%2520precise%2520control%2520over%2520the%2520spatial%2520layout%250Aof%2520generated%2520content.%2520In%2520contrast%252C%2520I2V%2520methods%2520are%2520limited%2520by%2520their%2520dependence%250Aon%2520real%2520images%252C%2520which%2520restricts%2520the%2520editability%2520of%2520the%2520synthesized%2520content.%250AAlthough%2520some%2520methods%2520incorporate%2520ControlNet%2520to%2520introduce%2520image-based%250Aconditioning%252C%2520they%2520often%2520lack%2520explicit%2520motion%2520control%2520and%2520require%250Acomputationally%2520expensive%2520training.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AAnyI2V%252C%2520a%2520training-free%2520framework%2520that%2520animates%2520any%2520conditional%2520images%2520with%250Auser-defined%2520motion%2520trajectories.%2520AnyI2V%2520supports%2520a%2520broader%2520range%2520of%2520modalities%250Aas%2520the%2520conditional%2520image%252C%2520including%2520data%2520types%2520such%2520as%2520meshes%2520and%2520point%2520clouds%250Athat%2520are%2520not%2520supported%2520by%2520ControlNet%252C%2520enabling%2520more%2520flexible%2520and%2520versatile%250Avideo%2520generation.%2520Additionally%252C%2520it%2520supports%2520mixed%2520conditional%2520inputs%2520and%250Aenables%2520style%2520transfer%2520and%2520editing%2520via%2520LoRA%2520and%2520text%2520prompts.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520AnyI2V%2520achieves%2520superior%2520performance%250Aand%2520provides%2520a%2520new%2520perspective%2520in%2520spatial-%2520and%2520motion-controlled%2520video%250Ageneration.%2520Code%2520is%2520available%2520at%2520https%253A//henghuiding.com/AnyI2V/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control&entry.906535625=Ziye%20Li%20and%20Hao%20Luo%20and%20Xincheng%20Shuai%20and%20Henghui%20Ding&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%2C%20particularly%20in%20diffusion%20models%2C%0Ahave%20driven%20notable%20progress%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%0Asynthesis.%20However%2C%20challenges%20remain%20in%20effectively%20integrating%20dynamic%20motion%0Asignals%20and%20flexible%20spatial%20constraints.%20Existing%20T2V%20methods%20typically%20rely%0Aon%20text%20prompts%2C%20which%20inherently%20lack%20precise%20control%20over%20the%20spatial%20layout%0Aof%20generated%20content.%20In%20contrast%2C%20I2V%20methods%20are%20limited%20by%20their%20dependence%0Aon%20real%20images%2C%20which%20restricts%20the%20editability%20of%20the%20synthesized%20content.%0AAlthough%20some%20methods%20incorporate%20ControlNet%20to%20introduce%20image-based%0Aconditioning%2C%20they%20often%20lack%20explicit%20motion%20control%20and%20require%0Acomputationally%20expensive%20training.%20To%20address%20these%20limitations%2C%20we%20propose%0AAnyI2V%2C%20a%20training-free%20framework%20that%20animates%20any%20conditional%20images%20with%0Auser-defined%20motion%20trajectories.%20AnyI2V%20supports%20a%20broader%20range%20of%20modalities%0Aas%20the%20conditional%20image%2C%20including%20data%20types%20such%20as%20meshes%20and%20point%20clouds%0Athat%20are%20not%20supported%20by%20ControlNet%2C%20enabling%20more%20flexible%20and%20versatile%0Avideo%20generation.%20Additionally%2C%20it%20supports%20mixed%20conditional%20inputs%20and%0Aenables%20style%20transfer%20and%20editing%20via%20LoRA%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20AnyI2V%20achieves%20superior%20performance%0Aand%20provides%20a%20new%20perspective%20in%20spatial-%20and%20motion-controlled%20video%0Ageneration.%20Code%20is%20available%20at%20https%3A//henghuiding.com/AnyI2V/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02857v1&entry.124074799=Read"},
{"title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context\n  Injection", "author": "Ziqi Miao and Yi Ding and Lijun Li and Jing Shao", "abstract": "  With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.\n", "link": "http://arxiv.org/abs/2507.02844v1", "date": "2025-07-03", "relevancy": 2.5162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection&body=Title%3A%20Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection%0AAuthor%3A%20Ziqi%20Miao%20and%20Yi%20Ding%20and%20Lijun%20Li%20and%20Jing%20Shao%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20strong%20visual-language%20capabilities%2C%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20demonstrated%20tremendous%20potential%20for%20real-world%0Aapplications.%20However%2C%20the%20security%20vulnerabilities%20exhibited%20by%20the%20visual%0Amodality%20pose%20significant%20challenges%20to%20deploying%20such%20models%20in%20open-world%0Aenvironments.%20Recent%20studies%20have%20successfully%20induced%20harmful%20responses%20from%0Atarget%20MLLMs%20by%20encoding%20harmful%20textual%20semantics%20directly%20into%20visual%20inputs.%0AHowever%2C%20in%20these%20approaches%2C%20the%20visual%20modality%20primarily%20serves%20as%20a%20trigger%0Afor%20unsafe%20behavior%2C%20often%20exhibiting%20semantic%20ambiguity%20and%20lacking%20grounding%0Ain%20realistic%20scenarios.%20In%20this%20work%2C%20we%20define%20a%20novel%20setting%3A%20visual-centric%0Ajailbreak%2C%20where%20visual%20information%20serves%20as%20a%20necessary%20component%20in%0Aconstructing%20a%20complete%20and%20realistic%20jailbreak%20context.%20Building%20on%20this%0Asetting%2C%20we%20propose%20the%20VisCo%20%28Visual%20Contextual%29%20Attack.%20VisCo%20fabricates%0Acontextual%20dialogue%20using%20four%20distinct%20visual-focused%20strategies%2C%20dynamically%0Agenerating%20auxiliary%20images%20when%20necessary%20to%20construct%20a%20visual-centric%0Ajailbreak%20scenario.%20To%20maximize%20attack%20effectiveness%2C%20it%20incorporates%20automatic%0Atoxicity%20obfuscation%20and%20semantic%20refinement%20to%20produce%20a%20final%20attack%20prompt%0Athat%20reliably%20triggers%20harmful%20responses%20from%20the%20target%20black-box%20MLLMs.%0ASpecifically%2C%20VisCo%20achieves%20a%20toxicity%20score%20of%204.78%20and%20an%20Attack%20Success%0ARate%20%28ASR%29%20of%2085%25%20on%20MM-SafetyBench%20against%20GPT-4o%2C%20significantly%20outperforming%0Athe%20baseline%2C%20which%20performs%20a%20toxicity%20score%20of%202.48%20and%20an%20ASR%20of%2022.2%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Dtc7w3PQ/Visco-Attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Contextual%2520Attack%253A%2520Jailbreaking%2520MLLMs%2520with%2520Image-Driven%2520Context%250A%2520%2520Injection%26entry.906535625%3DZiqi%2520Miao%2520and%2520Yi%2520Ding%2520and%2520Lijun%2520Li%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520strong%2520visual-language%2520capabilities%252C%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520tremendous%2520potential%2520for%2520real-world%250Aapplications.%2520However%252C%2520the%2520security%2520vulnerabilities%2520exhibited%2520by%2520the%2520visual%250Amodality%2520pose%2520significant%2520challenges%2520to%2520deploying%2520such%2520models%2520in%2520open-world%250Aenvironments.%2520Recent%2520studies%2520have%2520successfully%2520induced%2520harmful%2520responses%2520from%250Atarget%2520MLLMs%2520by%2520encoding%2520harmful%2520textual%2520semantics%2520directly%2520into%2520visual%2520inputs.%250AHowever%252C%2520in%2520these%2520approaches%252C%2520the%2520visual%2520modality%2520primarily%2520serves%2520as%2520a%2520trigger%250Afor%2520unsafe%2520behavior%252C%2520often%2520exhibiting%2520semantic%2520ambiguity%2520and%2520lacking%2520grounding%250Ain%2520realistic%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520define%2520a%2520novel%2520setting%253A%2520visual-centric%250Ajailbreak%252C%2520where%2520visual%2520information%2520serves%2520as%2520a%2520necessary%2520component%2520in%250Aconstructing%2520a%2520complete%2520and%2520realistic%2520jailbreak%2520context.%2520Building%2520on%2520this%250Asetting%252C%2520we%2520propose%2520the%2520VisCo%2520%2528Visual%2520Contextual%2529%2520Attack.%2520VisCo%2520fabricates%250Acontextual%2520dialogue%2520using%2520four%2520distinct%2520visual-focused%2520strategies%252C%2520dynamically%250Agenerating%2520auxiliary%2520images%2520when%2520necessary%2520to%2520construct%2520a%2520visual-centric%250Ajailbreak%2520scenario.%2520To%2520maximize%2520attack%2520effectiveness%252C%2520it%2520incorporates%2520automatic%250Atoxicity%2520obfuscation%2520and%2520semantic%2520refinement%2520to%2520produce%2520a%2520final%2520attack%2520prompt%250Athat%2520reliably%2520triggers%2520harmful%2520responses%2520from%2520the%2520target%2520black-box%2520MLLMs.%250ASpecifically%252C%2520VisCo%2520achieves%2520a%2520toxicity%2520score%2520of%25204.78%2520and%2520an%2520Attack%2520Success%250ARate%2520%2528ASR%2529%2520of%252085%2525%2520on%2520MM-SafetyBench%2520against%2520GPT-4o%252C%2520significantly%2520outperforming%250Athe%2520baseline%252C%2520which%2520performs%2520a%2520toxicity%2520score%2520of%25202.48%2520and%2520an%2520ASR%2520of%252022.2%2525.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Dtc7w3PQ/Visco-Attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection&entry.906535625=Ziqi%20Miao%20and%20Yi%20Ding%20and%20Lijun%20Li%20and%20Jing%20Shao&entry.1292438233=%20%20With%20the%20emergence%20of%20strong%20visual-language%20capabilities%2C%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20demonstrated%20tremendous%20potential%20for%20real-world%0Aapplications.%20However%2C%20the%20security%20vulnerabilities%20exhibited%20by%20the%20visual%0Amodality%20pose%20significant%20challenges%20to%20deploying%20such%20models%20in%20open-world%0Aenvironments.%20Recent%20studies%20have%20successfully%20induced%20harmful%20responses%20from%0Atarget%20MLLMs%20by%20encoding%20harmful%20textual%20semantics%20directly%20into%20visual%20inputs.%0AHowever%2C%20in%20these%20approaches%2C%20the%20visual%20modality%20primarily%20serves%20as%20a%20trigger%0Afor%20unsafe%20behavior%2C%20often%20exhibiting%20semantic%20ambiguity%20and%20lacking%20grounding%0Ain%20realistic%20scenarios.%20In%20this%20work%2C%20we%20define%20a%20novel%20setting%3A%20visual-centric%0Ajailbreak%2C%20where%20visual%20information%20serves%20as%20a%20necessary%20component%20in%0Aconstructing%20a%20complete%20and%20realistic%20jailbreak%20context.%20Building%20on%20this%0Asetting%2C%20we%20propose%20the%20VisCo%20%28Visual%20Contextual%29%20Attack.%20VisCo%20fabricates%0Acontextual%20dialogue%20using%20four%20distinct%20visual-focused%20strategies%2C%20dynamically%0Agenerating%20auxiliary%20images%20when%20necessary%20to%20construct%20a%20visual-centric%0Ajailbreak%20scenario.%20To%20maximize%20attack%20effectiveness%2C%20it%20incorporates%20automatic%0Atoxicity%20obfuscation%20and%20semantic%20refinement%20to%20produce%20a%20final%20attack%20prompt%0Athat%20reliably%20triggers%20harmful%20responses%20from%20the%20target%20black-box%20MLLMs.%0ASpecifically%2C%20VisCo%20achieves%20a%20toxicity%20score%20of%204.78%20and%20an%20Attack%20Success%0ARate%20%28ASR%29%20of%2085%25%20on%20MM-SafetyBench%20against%20GPT-4o%2C%20significantly%20outperforming%0Athe%20baseline%2C%20which%20performs%20a%20toxicity%20score%20of%202.48%20and%20an%20ASR%20of%2022.2%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Dtc7w3PQ/Visco-Attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02844v1&entry.124074799=Read"},
{"title": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment", "author": "Zhonghao Lyu and Yulan Gao and Junting Chen and Hongyang Du and Jie Xu and Kaibin Huang and Dong In Kim", "abstract": "  Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.\n", "link": "http://arxiv.org/abs/2505.22343v2", "date": "2025-07-03", "relevancy": 2.5066, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&body=Title%3A%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment%0AAuthor%3A%20Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim%0AAbstract%3A%20%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Intelligent%2520Low-altitude%2520Economy%2520with%2520Large%2520AI%2520Model%250A%2520%2520Deployment%26entry.906535625%3DZhonghao%2520Lyu%2520and%2520Yulan%2520Gao%2520and%2520Junting%2520Chen%2520and%2520Hongyang%2520Du%2520and%2520Jie%2520Xu%2520and%2520Kaibin%2520Huang%2520and%2520Dong%2520In%2520Kim%26entry.1292438233%3D%2520%2520Low-altitude%2520economy%2520%2528LAE%2529%2520represents%2520an%2520emerging%2520economic%2520paradigm%2520that%250Aredefines%2520commercial%2520and%2520social%2520aerial%2520activities.%2520Large%2520artificial%250Aintelligence%2520models%2520%2528LAIMs%2529%2520offer%2520transformative%2520potential%2520to%2520further%2520enhance%250Athe%2520intelligence%2520of%2520LAE%2520services.%2520However%252C%2520deploying%2520LAIMs%2520in%2520LAE%2520poses%2520several%250Achallenges%252C%2520including%2520the%2520significant%2520gap%2520between%2520their%2520computational/storage%250Ademands%2520and%2520the%2520limited%2520onboard%2520resources%2520of%2520LAE%2520entities%252C%2520the%2520mismatch%2520between%250Alab-trained%2520LAIMs%2520and%2520dynamic%2520physical%2520environments%252C%2520and%2520the%2520inefficiencies%2520of%250Atraditional%2520decoupled%2520designs%2520for%2520sensing%252C%2520communication%252C%2520and%2520computation.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520hierarchical%2520system%2520architecture%250Atailored%2520for%2520LAIM%2520deployment%2520and%2520present%2520representative%2520LAE%2520application%250Ascenarios.%2520Next%252C%2520we%2520explore%2520key%2520enabling%2520techniques%2520that%2520facilitate%2520the%2520mutual%250Aco-evolution%2520of%2520LAIMs%2520and%2520low-altitude%2520systems%252C%2520and%2520introduce%2520a%2520task-oriented%250Aexecution%2520pipeline%2520for%2520scalable%2520and%2520adaptive%2520service%2520delivery.%2520Then%252C%2520the%250Aproposed%2520framework%2520is%2520validated%2520through%2520real-world%2520case%2520studies.%2520Finally%252C%2520we%250Aoutline%2520open%2520challenges%2520to%2520inspire%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&entry.906535625=Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim&entry.1292438233=%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22343v2&entry.124074799=Read"},
{"title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided\n  Multi-Class Image Generation", "author": "Qin Guo and Ailing Zeng and Dongxu Yue and Ceyuan Yang and Yang Cao and Hanzhong Guo and Fei Shen and Wei Liu and Xihui Liu and Dan Xu", "abstract": "  Although significant advancements have been achieved in the progress of\nkeypoint-guided Text-to-Image diffusion models, existing mainstream\nkeypoint-guided models encounter challenges in controlling the generation of\nmore general non-rigid objects beyond humans (e.g., animals). Moreover, it is\ndifficult to generate multiple overlapping humans and animals based on keypoint\ncontrols solely. These challenges arise from two main aspects: the inherent\nlimitations of existing controllable methods and the lack of suitable datasets.\nFirst, we design a DiT-based framework, named UniMC, to explore unifying\ncontrollable multi-class image generation. UniMC integrates instance- and\nkeypoint-level conditions into compact tokens, incorporating attributes such as\nclass, bounding box, and keypoint coordinates. This approach overcomes the\nlimitations of previous methods that struggled to distinguish instances and\nclasses due to their reliance on skeleton images as conditions. Second, we\npropose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed\nfor keypoint-guided human and animal image generation. HAIG-2.9M includes 786K\nimages with 2.9M instances. This dataset features extensive annotations such as\nkeypoints, bounding boxes, and fine-grained captions for both humans and\nanimals, along with rigorous manual inspection to ensure annotation accuracy.\nExtensive experiments demonstrate the high quality of HAIG-2.9M and the\neffectiveness of UniMC, particularly in heavy occlusions and multi-class\nscenarios.\n", "link": "http://arxiv.org/abs/2507.02713v1", "date": "2025-07-03", "relevancy": 2.5012, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6311}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation&body=Title%3A%20UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation%0AAuthor%3A%20Qin%20Guo%20and%20Ailing%20Zeng%20and%20Dongxu%20Yue%20and%20Ceyuan%20Yang%20and%20Yang%20Cao%20and%20Hanzhong%20Guo%20and%20Fei%20Shen%20and%20Wei%20Liu%20and%20Xihui%20Liu%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Although%20significant%20advancements%20have%20been%20achieved%20in%20the%20progress%20of%0Akeypoint-guided%20Text-to-Image%20diffusion%20models%2C%20existing%20mainstream%0Akeypoint-guided%20models%20encounter%20challenges%20in%20controlling%20the%20generation%20of%0Amore%20general%20non-rigid%20objects%20beyond%20humans%20%28e.g.%2C%20animals%29.%20Moreover%2C%20it%20is%0Adifficult%20to%20generate%20multiple%20overlapping%20humans%20and%20animals%20based%20on%20keypoint%0Acontrols%20solely.%20These%20challenges%20arise%20from%20two%20main%20aspects%3A%20the%20inherent%0Alimitations%20of%20existing%20controllable%20methods%20and%20the%20lack%20of%20suitable%20datasets.%0AFirst%2C%20we%20design%20a%20DiT-based%20framework%2C%20named%20UniMC%2C%20to%20explore%20unifying%0Acontrollable%20multi-class%20image%20generation.%20UniMC%20integrates%20instance-%20and%0Akeypoint-level%20conditions%20into%20compact%20tokens%2C%20incorporating%20attributes%20such%20as%0Aclass%2C%20bounding%20box%2C%20and%20keypoint%20coordinates.%20This%20approach%20overcomes%20the%0Alimitations%20of%20previous%20methods%20that%20struggled%20to%20distinguish%20instances%20and%0Aclasses%20due%20to%20their%20reliance%20on%20skeleton%20images%20as%20conditions.%20Second%2C%20we%0Apropose%20HAIG-2.9M%2C%20a%20large-scale%2C%20high-quality%2C%20and%20diverse%20dataset%20designed%0Afor%20keypoint-guided%20human%20and%20animal%20image%20generation.%20HAIG-2.9M%20includes%20786K%0Aimages%20with%202.9M%20instances.%20This%20dataset%20features%20extensive%20annotations%20such%20as%0Akeypoints%2C%20bounding%20boxes%2C%20and%20fine-grained%20captions%20for%20both%20humans%20and%0Aanimals%2C%20along%20with%20rigorous%20manual%20inspection%20to%20ensure%20annotation%20accuracy.%0AExtensive%20experiments%20demonstrate%20the%20high%20quality%20of%20HAIG-2.9M%20and%20the%0Aeffectiveness%20of%20UniMC%2C%20particularly%20in%20heavy%20occlusions%20and%20multi-class%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMC%253A%2520Taming%2520Diffusion%2520Transformer%2520for%2520Unified%2520Keypoint-Guided%250A%2520%2520Multi-Class%2520Image%2520Generation%26entry.906535625%3DQin%2520Guo%2520and%2520Ailing%2520Zeng%2520and%2520Dongxu%2520Yue%2520and%2520Ceyuan%2520Yang%2520and%2520Yang%2520Cao%2520and%2520Hanzhong%2520Guo%2520and%2520Fei%2520Shen%2520and%2520Wei%2520Liu%2520and%2520Xihui%2520Liu%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Although%2520significant%2520advancements%2520have%2520been%2520achieved%2520in%2520the%2520progress%2520of%250Akeypoint-guided%2520Text-to-Image%2520diffusion%2520models%252C%2520existing%2520mainstream%250Akeypoint-guided%2520models%2520encounter%2520challenges%2520in%2520controlling%2520the%2520generation%2520of%250Amore%2520general%2520non-rigid%2520objects%2520beyond%2520humans%2520%2528e.g.%252C%2520animals%2529.%2520Moreover%252C%2520it%2520is%250Adifficult%2520to%2520generate%2520multiple%2520overlapping%2520humans%2520and%2520animals%2520based%2520on%2520keypoint%250Acontrols%2520solely.%2520These%2520challenges%2520arise%2520from%2520two%2520main%2520aspects%253A%2520the%2520inherent%250Alimitations%2520of%2520existing%2520controllable%2520methods%2520and%2520the%2520lack%2520of%2520suitable%2520datasets.%250AFirst%252C%2520we%2520design%2520a%2520DiT-based%2520framework%252C%2520named%2520UniMC%252C%2520to%2520explore%2520unifying%250Acontrollable%2520multi-class%2520image%2520generation.%2520UniMC%2520integrates%2520instance-%2520and%250Akeypoint-level%2520conditions%2520into%2520compact%2520tokens%252C%2520incorporating%2520attributes%2520such%2520as%250Aclass%252C%2520bounding%2520box%252C%2520and%2520keypoint%2520coordinates.%2520This%2520approach%2520overcomes%2520the%250Alimitations%2520of%2520previous%2520methods%2520that%2520struggled%2520to%2520distinguish%2520instances%2520and%250Aclasses%2520due%2520to%2520their%2520reliance%2520on%2520skeleton%2520images%2520as%2520conditions.%2520Second%252C%2520we%250Apropose%2520HAIG-2.9M%252C%2520a%2520large-scale%252C%2520high-quality%252C%2520and%2520diverse%2520dataset%2520designed%250Afor%2520keypoint-guided%2520human%2520and%2520animal%2520image%2520generation.%2520HAIG-2.9M%2520includes%2520786K%250Aimages%2520with%25202.9M%2520instances.%2520This%2520dataset%2520features%2520extensive%2520annotations%2520such%2520as%250Akeypoints%252C%2520bounding%2520boxes%252C%2520and%2520fine-grained%2520captions%2520for%2520both%2520humans%2520and%250Aanimals%252C%2520along%2520with%2520rigorous%2520manual%2520inspection%2520to%2520ensure%2520annotation%2520accuracy.%250AExtensive%2520experiments%2520demonstrate%2520the%2520high%2520quality%2520of%2520HAIG-2.9M%2520and%2520the%250Aeffectiveness%2520of%2520UniMC%252C%2520particularly%2520in%2520heavy%2520occlusions%2520and%2520multi-class%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation&entry.906535625=Qin%20Guo%20and%20Ailing%20Zeng%20and%20Dongxu%20Yue%20and%20Ceyuan%20Yang%20and%20Yang%20Cao%20and%20Hanzhong%20Guo%20and%20Fei%20Shen%20and%20Wei%20Liu%20and%20Xihui%20Liu%20and%20Dan%20Xu&entry.1292438233=%20%20Although%20significant%20advancements%20have%20been%20achieved%20in%20the%20progress%20of%0Akeypoint-guided%20Text-to-Image%20diffusion%20models%2C%20existing%20mainstream%0Akeypoint-guided%20models%20encounter%20challenges%20in%20controlling%20the%20generation%20of%0Amore%20general%20non-rigid%20objects%20beyond%20humans%20%28e.g.%2C%20animals%29.%20Moreover%2C%20it%20is%0Adifficult%20to%20generate%20multiple%20overlapping%20humans%20and%20animals%20based%20on%20keypoint%0Acontrols%20solely.%20These%20challenges%20arise%20from%20two%20main%20aspects%3A%20the%20inherent%0Alimitations%20of%20existing%20controllable%20methods%20and%20the%20lack%20of%20suitable%20datasets.%0AFirst%2C%20we%20design%20a%20DiT-based%20framework%2C%20named%20UniMC%2C%20to%20explore%20unifying%0Acontrollable%20multi-class%20image%20generation.%20UniMC%20integrates%20instance-%20and%0Akeypoint-level%20conditions%20into%20compact%20tokens%2C%20incorporating%20attributes%20such%20as%0Aclass%2C%20bounding%20box%2C%20and%20keypoint%20coordinates.%20This%20approach%20overcomes%20the%0Alimitations%20of%20previous%20methods%20that%20struggled%20to%20distinguish%20instances%20and%0Aclasses%20due%20to%20their%20reliance%20on%20skeleton%20images%20as%20conditions.%20Second%2C%20we%0Apropose%20HAIG-2.9M%2C%20a%20large-scale%2C%20high-quality%2C%20and%20diverse%20dataset%20designed%0Afor%20keypoint-guided%20human%20and%20animal%20image%20generation.%20HAIG-2.9M%20includes%20786K%0Aimages%20with%202.9M%20instances.%20This%20dataset%20features%20extensive%20annotations%20such%20as%0Akeypoints%2C%20bounding%20boxes%2C%20and%20fine-grained%20captions%20for%20both%20humans%20and%0Aanimals%2C%20along%20with%20rigorous%20manual%20inspection%20to%20ensure%20annotation%20accuracy.%0AExtensive%20experiments%20demonstrate%20the%20high%20quality%20of%20HAIG-2.9M%20and%20the%0Aeffectiveness%20of%20UniMC%2C%20particularly%20in%20heavy%20occlusions%20and%20multi-class%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02713v1&entry.124074799=Read"},
{"title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial\n  Control for Text-to-Image Generation", "author": "Liheng Zhang and Lexi Pang and Hang Ye and Xiaoxuan Ma and Yizhou Wang", "abstract": "  Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.\n", "link": "http://arxiv.org/abs/2507.02792v1", "date": "2025-07-03", "relevancy": 2.4783, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6363}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&body=Title%3A%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRichControl%253A%2520Structure-%2520and%2520Appearance-Rich%2520Training-Free%2520Spatial%250A%2520%2520Control%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DLiheng%2520Zhang%2520and%2520Lexi%2520Pang%2520and%2520Hang%2520Ye%2520and%2520Xiaoxuan%2520Ma%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520success%2520in%250Agenerating%2520high-quality%2520images%2520from%2520text%2520prompts.%2520Recent%2520efforts%2520extend%2520these%250Amodels%2520to%2520incorporate%2520conditional%2520images%2520%2528e.g.%252C%2520depth%2520or%2520pose%2520maps%2529%2520for%250Afine-grained%2520spatial%2520control.%2520Among%2520them%252C%2520feature%2520injection%2520methods%2520have%250Aemerged%2520as%2520a%2520training-free%2520alternative%2520to%2520traditional%2520fine-tuning%2520approaches.%250AHowever%252C%2520they%2520often%2520suffer%2520from%2520structural%2520misalignment%252C%2520condition%2520leakage%252C%2520and%250Avisual%2520artifacts%252C%2520especially%2520when%2520the%2520condition%2520image%2520diverges%2520significantly%250Afrom%2520natural%2520RGB%2520distributions.%2520By%2520revisiting%2520existing%2520methods%252C%2520we%2520identify%2520a%250Acore%2520limitation%253A%2520the%2520synchronous%2520injection%2520of%2520condition%2520features%2520fails%2520to%250Aaccount%2520for%2520the%2520trade-off%2520between%2520domain%2520alignment%2520and%2520structural%2520preservation%250Aduring%2520denoising.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520flexible%2520feature%250Ainjection%2520framework%2520that%2520decouples%2520the%2520injection%2520timestep%2520from%2520the%2520denoising%250Aprocess.%2520At%2520its%2520core%2520is%2520a%2520structure-rich%2520injection%2520module%252C%2520which%2520enables%2520the%250Amodel%2520to%2520better%2520adapt%2520to%2520the%2520evolving%2520interplay%2520between%2520alignment%2520and%2520structure%250Apreservation%2520throughout%2520the%2520diffusion%2520steps%252C%2520resulting%2520in%2520more%2520faithful%250Astructural%2520generation.%2520In%2520addition%252C%2520we%2520introduce%2520appearance-rich%2520prompting%2520and%250Aa%2520restart%2520refinement%2520strategy%2520to%2520further%2520enhance%2520appearance%2520control%2520and%2520visual%250Aquality.%2520Together%252C%2520these%2520designs%2520enable%2520training-free%2520generation%2520that%2520is%2520both%250Astructure-rich%2520and%2520appearance-rich.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%2520zero-shot%250Aconditioning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&entry.906535625=Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02792v1&entry.124074799=Read"},
{"title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal\n  Policies in Real", "author": "Renhao Wang and Haoran Geng and Tingle Li and Feishi Wang and Gopala Anumanchipalli and Philipp Wu and Trevor Darrell and Boyi Li and Pieter Abbeel and Jitendra Malik and Alexei A. Efros", "abstract": "  Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.\n", "link": "http://arxiv.org/abs/2507.02864v1", "date": "2025-07-03", "relevancy": 2.4668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6452}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6187}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real&body=Title%3A%20MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real%0AAuthor%3A%20Renhao%20Wang%20and%20Haoran%20Geng%20and%20Tingle%20Li%20and%20Feishi%20Wang%20and%20Gopala%20Anumanchipalli%20and%20Philipp%20Wu%20and%20Trevor%20Darrell%20and%20Boyi%20Li%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Alexei%20A.%20Efros%0AAbstract%3A%20%20%20Robots%20must%20integrate%20multiple%20sensory%20modalities%20to%20act%20effectively%20in%20the%0Areal%20world.%20Yet%2C%20learning%20such%20multimodal%20policies%20at%20scale%20remains%0Achallenging.%20Simulation%20offers%20a%20viable%20solution%2C%20but%20while%20vision%20has%0Abenefited%20from%20high-fidelity%20simulators%2C%20other%20modalities%20%28e.g.%20sound%29%20can%20be%0Anotoriously%20difficult%20to%20simulate.%20As%20a%20result%2C%20sim-to-real%20transfer%20has%0Asucceeded%20primarily%20in%20vision-based%20tasks%2C%20with%20multimodal%20transfer%20still%0Alargely%20unrealized.%20In%20this%20work%2C%20we%20tackle%20these%20challenges%20by%20introducing%0AMultiGen%2C%20a%20framework%20that%20integrates%20large-scale%20generative%20models%20into%0Atraditional%20physics%20simulators%2C%20enabling%20multisensory%20simulation.%20We%20showcase%0Aour%20framework%20on%20the%20dynamic%20task%20of%20robot%20pouring%2C%20which%20inherently%20relies%20on%0Amultimodal%20feedback.%20By%20synthesizing%20realistic%20audio%20conditioned%20on%20simulation%0Avideo%2C%20our%20method%20enables%20training%20on%20rich%20audiovisual%20trajectories%20--%20without%0Aany%20real%20robot%20data.%20We%20demonstrate%20effective%20zero-shot%20transfer%20to%20real-world%0Apouring%20with%20novel%20containers%20and%20liquids%2C%20highlighting%20the%20potential%20of%0Agenerative%20modeling%20to%20both%20simulate%20hard-to-model%20modalities%20and%20close%20the%0Amultimodal%20sim-to-real%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiGen%253A%2520Using%2520Multimodal%2520Generation%2520in%2520Simulation%2520to%2520Learn%2520Multimodal%250A%2520%2520Policies%2520in%2520Real%26entry.906535625%3DRenhao%2520Wang%2520and%2520Haoran%2520Geng%2520and%2520Tingle%2520Li%2520and%2520Feishi%2520Wang%2520and%2520Gopala%2520Anumanchipalli%2520and%2520Philipp%2520Wu%2520and%2520Trevor%2520Darrell%2520and%2520Boyi%2520Li%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%2520and%2520Alexei%2520A.%2520Efros%26entry.1292438233%3D%2520%2520Robots%2520must%2520integrate%2520multiple%2520sensory%2520modalities%2520to%2520act%2520effectively%2520in%2520the%250Areal%2520world.%2520Yet%252C%2520learning%2520such%2520multimodal%2520policies%2520at%2520scale%2520remains%250Achallenging.%2520Simulation%2520offers%2520a%2520viable%2520solution%252C%2520but%2520while%2520vision%2520has%250Abenefited%2520from%2520high-fidelity%2520simulators%252C%2520other%2520modalities%2520%2528e.g.%2520sound%2529%2520can%2520be%250Anotoriously%2520difficult%2520to%2520simulate.%2520As%2520a%2520result%252C%2520sim-to-real%2520transfer%2520has%250Asucceeded%2520primarily%2520in%2520vision-based%2520tasks%252C%2520with%2520multimodal%2520transfer%2520still%250Alargely%2520unrealized.%2520In%2520this%2520work%252C%2520we%2520tackle%2520these%2520challenges%2520by%2520introducing%250AMultiGen%252C%2520a%2520framework%2520that%2520integrates%2520large-scale%2520generative%2520models%2520into%250Atraditional%2520physics%2520simulators%252C%2520enabling%2520multisensory%2520simulation.%2520We%2520showcase%250Aour%2520framework%2520on%2520the%2520dynamic%2520task%2520of%2520robot%2520pouring%252C%2520which%2520inherently%2520relies%2520on%250Amultimodal%2520feedback.%2520By%2520synthesizing%2520realistic%2520audio%2520conditioned%2520on%2520simulation%250Avideo%252C%2520our%2520method%2520enables%2520training%2520on%2520rich%2520audiovisual%2520trajectories%2520--%2520without%250Aany%2520real%2520robot%2520data.%2520We%2520demonstrate%2520effective%2520zero-shot%2520transfer%2520to%2520real-world%250Apouring%2520with%2520novel%2520containers%2520and%2520liquids%252C%2520highlighting%2520the%2520potential%2520of%250Agenerative%2520modeling%2520to%2520both%2520simulate%2520hard-to-model%2520modalities%2520and%2520close%2520the%250Amultimodal%2520sim-to-real%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real&entry.906535625=Renhao%20Wang%20and%20Haoran%20Geng%20and%20Tingle%20Li%20and%20Feishi%20Wang%20and%20Gopala%20Anumanchipalli%20and%20Philipp%20Wu%20and%20Trevor%20Darrell%20and%20Boyi%20Li%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Alexei%20A.%20Efros&entry.1292438233=%20%20Robots%20must%20integrate%20multiple%20sensory%20modalities%20to%20act%20effectively%20in%20the%0Areal%20world.%20Yet%2C%20learning%20such%20multimodal%20policies%20at%20scale%20remains%0Achallenging.%20Simulation%20offers%20a%20viable%20solution%2C%20but%20while%20vision%20has%0Abenefited%20from%20high-fidelity%20simulators%2C%20other%20modalities%20%28e.g.%20sound%29%20can%20be%0Anotoriously%20difficult%20to%20simulate.%20As%20a%20result%2C%20sim-to-real%20transfer%20has%0Asucceeded%20primarily%20in%20vision-based%20tasks%2C%20with%20multimodal%20transfer%20still%0Alargely%20unrealized.%20In%20this%20work%2C%20we%20tackle%20these%20challenges%20by%20introducing%0AMultiGen%2C%20a%20framework%20that%20integrates%20large-scale%20generative%20models%20into%0Atraditional%20physics%20simulators%2C%20enabling%20multisensory%20simulation.%20We%20showcase%0Aour%20framework%20on%20the%20dynamic%20task%20of%20robot%20pouring%2C%20which%20inherently%20relies%20on%0Amultimodal%20feedback.%20By%20synthesizing%20realistic%20audio%20conditioned%20on%20simulation%0Avideo%2C%20our%20method%20enables%20training%20on%20rich%20audiovisual%20trajectories%20--%20without%0Aany%20real%20robot%20data.%20We%20demonstrate%20effective%20zero-shot%20transfer%20to%20real-world%0Apouring%20with%20novel%20containers%20and%20liquids%2C%20highlighting%20the%20potential%20of%0Agenerative%20modeling%20to%20both%20simulate%20hard-to-model%20modalities%20and%20close%20the%0Amultimodal%20sim-to-real%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02864v1&entry.124074799=Read"},
{"title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with\n  Minimum Potential Delay Fairness in Diffusion Models", "author": "Yuxuan Wang and Tianwei Cao and Huayu Zhang and Zhongjiang He and Kongming Liang and Zhanyu Ma", "abstract": "  Image generation has achieved remarkable progress with the development of\nlarge-scale text-to-image models, especially diffusion-based models. However,\ngenerating human images with plausible details, such as faces or hands, remains\nchallenging due to insufficient supervision of local regions during training.\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\napproach designed to enhance both global and local generation quality fairly.\nSpecifically, we first construct three learning objectives: a global objective\nderived from the default diffusion objective function and two local objectives\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\nderive the optimal parameter updating strategy under the guidance of the\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\noptimization for this multi-objective problem. Based on this, our proposed\nmethod can achieve significant improvements in generating challenging local\ndetails while maintaining overall quality. Extensive experiments showcase the\neffectiveness of our method in improving the performance of human image\ngeneration under different scenarios.\n", "link": "http://arxiv.org/abs/2507.02714v1", "date": "2025-07-03", "relevancy": 2.4539, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models&body=Title%3A%20FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models%0AAuthor%3A%20Yuxuan%20Wang%20and%20Tianwei%20Cao%20and%20Huayu%20Zhang%20and%20Zhongjiang%20He%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma%0AAbstract%3A%20%20%20Image%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20development%20of%0Alarge-scale%20text-to-image%20models%2C%20especially%20diffusion-based%20models.%20However%2C%0Agenerating%20human%20images%20with%20plausible%20details%2C%20such%20as%20faces%20or%20hands%2C%20remains%0Achallenging%20due%20to%20insufficient%20supervision%20of%20local%20regions%20during%20training.%0ATo%20address%20this%20issue%2C%20we%20propose%20FairHuman%2C%20a%20multi-objective%20fine-tuning%0Aapproach%20designed%20to%20enhance%20both%20global%20and%20local%20generation%20quality%20fairly.%0ASpecifically%2C%20we%20first%20construct%20three%20learning%20objectives%3A%20a%20global%20objective%0Aderived%20from%20the%20default%20diffusion%20objective%20function%20and%20two%20local%20objectives%0Afor%20hands%20and%20faces%20based%20on%20pre-annotated%20positional%20priors.%20Subsequently%2C%20we%0Aderive%20the%20optimal%20parameter%20updating%20strategy%20under%20the%20guidance%20of%20the%0AMinimum%20Potential%20Delay%20%28MPD%29%20criterion%2C%20thereby%20attaining%20fairness-ware%0Aoptimization%20for%20this%20multi-objective%20problem.%20Based%20on%20this%2C%20our%20proposed%0Amethod%20can%20achieve%20significant%20improvements%20in%20generating%20challenging%20local%0Adetails%20while%20maintaining%20overall%20quality.%20Extensive%20experiments%20showcase%20the%0Aeffectiveness%20of%20our%20method%20in%20improving%20the%20performance%20of%20human%20image%0Ageneration%20under%20different%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairHuman%253A%2520Boosting%2520Hand%2520and%2520Face%2520Quality%2520in%2520Human%2520Image%2520Generation%2520with%250A%2520%2520Minimum%2520Potential%2520Delay%2520Fairness%2520in%2520Diffusion%2520Models%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Tianwei%2520Cao%2520and%2520Huayu%2520Zhang%2520and%2520Zhongjiang%2520He%2520and%2520Kongming%2520Liang%2520and%2520Zhanyu%2520Ma%26entry.1292438233%3D%2520%2520Image%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520with%2520the%2520development%2520of%250Alarge-scale%2520text-to-image%2520models%252C%2520especially%2520diffusion-based%2520models.%2520However%252C%250Agenerating%2520human%2520images%2520with%2520plausible%2520details%252C%2520such%2520as%2520faces%2520or%2520hands%252C%2520remains%250Achallenging%2520due%2520to%2520insufficient%2520supervision%2520of%2520local%2520regions%2520during%2520training.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520FairHuman%252C%2520a%2520multi-objective%2520fine-tuning%250Aapproach%2520designed%2520to%2520enhance%2520both%2520global%2520and%2520local%2520generation%2520quality%2520fairly.%250ASpecifically%252C%2520we%2520first%2520construct%2520three%2520learning%2520objectives%253A%2520a%2520global%2520objective%250Aderived%2520from%2520the%2520default%2520diffusion%2520objective%2520function%2520and%2520two%2520local%2520objectives%250Afor%2520hands%2520and%2520faces%2520based%2520on%2520pre-annotated%2520positional%2520priors.%2520Subsequently%252C%2520we%250Aderive%2520the%2520optimal%2520parameter%2520updating%2520strategy%2520under%2520the%2520guidance%2520of%2520the%250AMinimum%2520Potential%2520Delay%2520%2528MPD%2529%2520criterion%252C%2520thereby%2520attaining%2520fairness-ware%250Aoptimization%2520for%2520this%2520multi-objective%2520problem.%2520Based%2520on%2520this%252C%2520our%2520proposed%250Amethod%2520can%2520achieve%2520significant%2520improvements%2520in%2520generating%2520challenging%2520local%250Adetails%2520while%2520maintaining%2520overall%2520quality.%2520Extensive%2520experiments%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520improving%2520the%2520performance%2520of%2520human%2520image%250Ageneration%2520under%2520different%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models&entry.906535625=Yuxuan%20Wang%20and%20Tianwei%20Cao%20and%20Huayu%20Zhang%20and%20Zhongjiang%20He%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma&entry.1292438233=%20%20Image%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20development%20of%0Alarge-scale%20text-to-image%20models%2C%20especially%20diffusion-based%20models.%20However%2C%0Agenerating%20human%20images%20with%20plausible%20details%2C%20such%20as%20faces%20or%20hands%2C%20remains%0Achallenging%20due%20to%20insufficient%20supervision%20of%20local%20regions%20during%20training.%0ATo%20address%20this%20issue%2C%20we%20propose%20FairHuman%2C%20a%20multi-objective%20fine-tuning%0Aapproach%20designed%20to%20enhance%20both%20global%20and%20local%20generation%20quality%20fairly.%0ASpecifically%2C%20we%20first%20construct%20three%20learning%20objectives%3A%20a%20global%20objective%0Aderived%20from%20the%20default%20diffusion%20objective%20function%20and%20two%20local%20objectives%0Afor%20hands%20and%20faces%20based%20on%20pre-annotated%20positional%20priors.%20Subsequently%2C%20we%0Aderive%20the%20optimal%20parameter%20updating%20strategy%20under%20the%20guidance%20of%20the%0AMinimum%20Potential%20Delay%20%28MPD%29%20criterion%2C%20thereby%20attaining%20fairness-ware%0Aoptimization%20for%20this%20multi-objective%20problem.%20Based%20on%20this%2C%20our%20proposed%0Amethod%20can%20achieve%20significant%20improvements%20in%20generating%20challenging%20local%0Adetails%20while%20maintaining%20overall%20quality.%20Extensive%20experiments%20showcase%20the%0Aeffectiveness%20of%20our%20method%20in%20improving%20the%20performance%20of%20human%20image%0Ageneration%20under%20different%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02714v1&entry.124074799=Read"},
{"title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in\n  Pharmacogenes", "author": "Antoine Honor\u00e9 and Borja Rodr\u00edguez G\u00e1lvez and Yoomi Park and Yitian Zhou and Volker M. Lauschke and Ming Xiao", "abstract": "  Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction.\n", "link": "http://arxiv.org/abs/2507.02624v1", "date": "2025-07-03", "relevancy": 2.444, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes&body=Title%3A%20A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes%0AAuthor%3A%20Antoine%20Honor%C3%A9%20and%20Borja%20Rodr%C3%ADguez%20G%C3%A1lvez%20and%20Yoomi%20Park%20and%20Yitian%20Zhou%20and%20Volker%20M.%20Lauschke%20and%20Ming%20Xiao%0AAbstract%3A%20%20%20Variant%20effect%20predictors%20%28VEPs%29%20aim%20to%20assess%20the%20functional%20impact%20of%0Aprotein%20variants%2C%20traditionally%20relying%20on%20multiple%20sequence%20alignments%20%28MSAs%29.%0AThis%20approach%20assumes%20that%20naturally%20occurring%20variants%20are%20fit%2C%20an%20assumption%0Achallenged%20by%20pharmacogenomics%2C%20where%20some%20pharmacogenes%20experience%20low%0Aevolutionary%20pressure.%20Deep%20mutational%20scanning%20%28DMS%29%20datasets%20provide%20an%0Aalternative%20by%20offering%20quantitative%20fitness%20scores%20for%20variants.%20In%20this%20work%2C%0Awe%20propose%20a%20transformer-based%20matrix%20variational%20auto-encoder%20%28matVAE%29%20with%20a%0Astructured%20prior%20and%20evaluate%20its%20performance%20on%2033%20DMS%20datasets%20corresponding%0Ato%2026%20drug%20target%20and%20ADME%20proteins%20from%20the%20ProteinGym%20benchmark.%20Our%20model%0Atrained%20on%20MSAs%20%28matVAE-MSA%29%20outperforms%20the%20state-of-the-art%20DeepSequence%0Amodel%20in%20zero-shot%20prediction%20on%20DMS%20datasets%2C%20despite%20using%20an%20order%20of%0Amagnitude%20fewer%20parameters%20and%20requiring%20less%20computation%20at%20inference%20time.%20We%0Aalso%20compare%20matVAE-MSA%20to%20matENC-DMS%2C%20a%20model%20of%20similar%20capacity%20trained%20on%0ADMS%20data%2C%20and%20find%20that%20the%20latter%20performs%20better%20on%20supervised%20prediction%0Atasks.%20Additionally%2C%20incorporating%20AlphaFold-generated%20structures%20into%20our%0Atransformer%20model%20further%20improves%20performance%2C%20achieving%20results%20comparable%20to%0ADeepSequence%20trained%20on%20MSAs%20and%20finetuned%20on%20DMS.%20These%20findings%20highlight%20the%0Apotential%20of%20DMS%20datasets%20to%20replace%20MSAs%20without%20significant%20loss%20in%0Apredictive%20performance%2C%20motivating%20further%20development%20of%20DMS%20datasets%20and%0Aexploration%20of%20their%20relationships%20to%20enhance%20variant%20effect%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Matrix%2520Variational%2520Auto-Encoder%2520for%2520Variant%2520Effect%2520Prediction%2520in%250A%2520%2520Pharmacogenes%26entry.906535625%3DAntoine%2520Honor%25C3%25A9%2520and%2520Borja%2520Rodr%25C3%25ADguez%2520G%25C3%25A1lvez%2520and%2520Yoomi%2520Park%2520and%2520Yitian%2520Zhou%2520and%2520Volker%2520M.%2520Lauschke%2520and%2520Ming%2520Xiao%26entry.1292438233%3D%2520%2520Variant%2520effect%2520predictors%2520%2528VEPs%2529%2520aim%2520to%2520assess%2520the%2520functional%2520impact%2520of%250Aprotein%2520variants%252C%2520traditionally%2520relying%2520on%2520multiple%2520sequence%2520alignments%2520%2528MSAs%2529.%250AThis%2520approach%2520assumes%2520that%2520naturally%2520occurring%2520variants%2520are%2520fit%252C%2520an%2520assumption%250Achallenged%2520by%2520pharmacogenomics%252C%2520where%2520some%2520pharmacogenes%2520experience%2520low%250Aevolutionary%2520pressure.%2520Deep%2520mutational%2520scanning%2520%2528DMS%2529%2520datasets%2520provide%2520an%250Aalternative%2520by%2520offering%2520quantitative%2520fitness%2520scores%2520for%2520variants.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520transformer-based%2520matrix%2520variational%2520auto-encoder%2520%2528matVAE%2529%2520with%2520a%250Astructured%2520prior%2520and%2520evaluate%2520its%2520performance%2520on%252033%2520DMS%2520datasets%2520corresponding%250Ato%252026%2520drug%2520target%2520and%2520ADME%2520proteins%2520from%2520the%2520ProteinGym%2520benchmark.%2520Our%2520model%250Atrained%2520on%2520MSAs%2520%2528matVAE-MSA%2529%2520outperforms%2520the%2520state-of-the-art%2520DeepSequence%250Amodel%2520in%2520zero-shot%2520prediction%2520on%2520DMS%2520datasets%252C%2520despite%2520using%2520an%2520order%2520of%250Amagnitude%2520fewer%2520parameters%2520and%2520requiring%2520less%2520computation%2520at%2520inference%2520time.%2520We%250Aalso%2520compare%2520matVAE-MSA%2520to%2520matENC-DMS%252C%2520a%2520model%2520of%2520similar%2520capacity%2520trained%2520on%250ADMS%2520data%252C%2520and%2520find%2520that%2520the%2520latter%2520performs%2520better%2520on%2520supervised%2520prediction%250Atasks.%2520Additionally%252C%2520incorporating%2520AlphaFold-generated%2520structures%2520into%2520our%250Atransformer%2520model%2520further%2520improves%2520performance%252C%2520achieving%2520results%2520comparable%2520to%250ADeepSequence%2520trained%2520on%2520MSAs%2520and%2520finetuned%2520on%2520DMS.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520DMS%2520datasets%2520to%2520replace%2520MSAs%2520without%2520significant%2520loss%2520in%250Apredictive%2520performance%252C%2520motivating%2520further%2520development%2520of%2520DMS%2520datasets%2520and%250Aexploration%2520of%2520their%2520relationships%2520to%2520enhance%2520variant%2520effect%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes&entry.906535625=Antoine%20Honor%C3%A9%20and%20Borja%20Rodr%C3%ADguez%20G%C3%A1lvez%20and%20Yoomi%20Park%20and%20Yitian%20Zhou%20and%20Volker%20M.%20Lauschke%20and%20Ming%20Xiao&entry.1292438233=%20%20Variant%20effect%20predictors%20%28VEPs%29%20aim%20to%20assess%20the%20functional%20impact%20of%0Aprotein%20variants%2C%20traditionally%20relying%20on%20multiple%20sequence%20alignments%20%28MSAs%29.%0AThis%20approach%20assumes%20that%20naturally%20occurring%20variants%20are%20fit%2C%20an%20assumption%0Achallenged%20by%20pharmacogenomics%2C%20where%20some%20pharmacogenes%20experience%20low%0Aevolutionary%20pressure.%20Deep%20mutational%20scanning%20%28DMS%29%20datasets%20provide%20an%0Aalternative%20by%20offering%20quantitative%20fitness%20scores%20for%20variants.%20In%20this%20work%2C%0Awe%20propose%20a%20transformer-based%20matrix%20variational%20auto-encoder%20%28matVAE%29%20with%20a%0Astructured%20prior%20and%20evaluate%20its%20performance%20on%2033%20DMS%20datasets%20corresponding%0Ato%2026%20drug%20target%20and%20ADME%20proteins%20from%20the%20ProteinGym%20benchmark.%20Our%20model%0Atrained%20on%20MSAs%20%28matVAE-MSA%29%20outperforms%20the%20state-of-the-art%20DeepSequence%0Amodel%20in%20zero-shot%20prediction%20on%20DMS%20datasets%2C%20despite%20using%20an%20order%20of%0Amagnitude%20fewer%20parameters%20and%20requiring%20less%20computation%20at%20inference%20time.%20We%0Aalso%20compare%20matVAE-MSA%20to%20matENC-DMS%2C%20a%20model%20of%20similar%20capacity%20trained%20on%0ADMS%20data%2C%20and%20find%20that%20the%20latter%20performs%20better%20on%20supervised%20prediction%0Atasks.%20Additionally%2C%20incorporating%20AlphaFold-generated%20structures%20into%20our%0Atransformer%20model%20further%20improves%20performance%2C%20achieving%20results%20comparable%20to%0ADeepSequence%20trained%20on%20MSAs%20and%20finetuned%20on%20DMS.%20These%20findings%20highlight%20the%0Apotential%20of%20DMS%20datasets%20to%20replace%20MSAs%20without%20significant%20loss%20in%0Apredictive%20performance%2C%20motivating%20further%20development%20of%20DMS%20datasets%20and%0Aexploration%20of%20their%20relationships%20to%20enhance%20variant%20effect%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02624v1&entry.124074799=Read"},
{"title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive\n  Learning for Enhanced Volumetric Grasping", "author": "Qingyu Fan and Yinghao Cai and Chao Li and Chunting Jiao and Xudong Zheng and Tao Lu and Bin Liang and Shuo Wang", "abstract": "  Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.\n", "link": "http://arxiv.org/abs/2507.02672v1", "date": "2025-07-03", "relevancy": 2.4084, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6545}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5698}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping&body=Title%3A%20MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping%0AAuthor%3A%20Qingyu%20Fan%20and%20Yinghao%20Cai%20and%20Chao%20Li%20and%20Chunting%20Jiao%20and%20Xudong%20Zheng%20and%20Tao%20Lu%20and%20Bin%20Liang%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20Robotic%20grasping%20faces%20challenges%20in%20adapting%20to%20objects%20with%20varying%20shapes%0Aand%20sizes.%20In%20this%20paper%2C%20we%20introduce%20MISCGrasp%2C%20a%20volumetric%20grasping%20method%0Athat%20integrates%20multi-scale%20feature%20extraction%20with%20contrastive%20feature%0Aenhancement%20for%20self-adaptive%20grasping.%20We%20propose%20a%20query-based%20interaction%0Abetween%20high-level%20and%20low-level%20features%20through%20the%20Insight%20Transformer%2C%0Awhile%20the%20Empower%20Transformer%20selectively%20attends%20to%20the%20highest-level%0Afeatures%2C%20which%20synergistically%20strikes%20a%20balance%20between%20focusing%20on%20fine%0Ageometric%20details%20and%20overall%20geometric%20structures.%20Furthermore%2C%20MISCGrasp%0Autilizes%20multi-scale%20contrastive%20learning%20to%20exploit%20similarities%20among%0Apositive%20grasp%20samples%2C%20ensuring%20consistency%20across%20multi-scale%20features.%0AExtensive%20experiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%0Athat%20MISCGrasp%20outperforms%20baseline%20and%20variant%20methods%20in%20tabletop%0Adecluttering%20tasks.%20More%20details%20are%20available%20at%20https%3A//miscgrasp.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMISCGrasp%253A%2520Leveraging%2520Multiple%2520Integrated%2520Scales%2520and%2520Contrastive%250A%2520%2520Learning%2520for%2520Enhanced%2520Volumetric%2520Grasping%26entry.906535625%3DQingyu%2520Fan%2520and%2520Yinghao%2520Cai%2520and%2520Chao%2520Li%2520and%2520Chunting%2520Jiao%2520and%2520Xudong%2520Zheng%2520and%2520Tao%2520Lu%2520and%2520Bin%2520Liang%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520Robotic%2520grasping%2520faces%2520challenges%2520in%2520adapting%2520to%2520objects%2520with%2520varying%2520shapes%250Aand%2520sizes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MISCGrasp%252C%2520a%2520volumetric%2520grasping%2520method%250Athat%2520integrates%2520multi-scale%2520feature%2520extraction%2520with%2520contrastive%2520feature%250Aenhancement%2520for%2520self-adaptive%2520grasping.%2520We%2520propose%2520a%2520query-based%2520interaction%250Abetween%2520high-level%2520and%2520low-level%2520features%2520through%2520the%2520Insight%2520Transformer%252C%250Awhile%2520the%2520Empower%2520Transformer%2520selectively%2520attends%2520to%2520the%2520highest-level%250Afeatures%252C%2520which%2520synergistically%2520strikes%2520a%2520balance%2520between%2520focusing%2520on%2520fine%250Ageometric%2520details%2520and%2520overall%2520geometric%2520structures.%2520Furthermore%252C%2520MISCGrasp%250Autilizes%2520multi-scale%2520contrastive%2520learning%2520to%2520exploit%2520similarities%2520among%250Apositive%2520grasp%2520samples%252C%2520ensuring%2520consistency%2520across%2520multi-scale%2520features.%250AExtensive%2520experiments%2520in%2520both%2520simulated%2520and%2520real-world%2520environments%2520demonstrate%250Athat%2520MISCGrasp%2520outperforms%2520baseline%2520and%2520variant%2520methods%2520in%2520tabletop%250Adecluttering%2520tasks.%2520More%2520details%2520are%2520available%2520at%2520https%253A//miscgrasp.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping&entry.906535625=Qingyu%20Fan%20and%20Yinghao%20Cai%20and%20Chao%20Li%20and%20Chunting%20Jiao%20and%20Xudong%20Zheng%20and%20Tao%20Lu%20and%20Bin%20Liang%20and%20Shuo%20Wang&entry.1292438233=%20%20Robotic%20grasping%20faces%20challenges%20in%20adapting%20to%20objects%20with%20varying%20shapes%0Aand%20sizes.%20In%20this%20paper%2C%20we%20introduce%20MISCGrasp%2C%20a%20volumetric%20grasping%20method%0Athat%20integrates%20multi-scale%20feature%20extraction%20with%20contrastive%20feature%0Aenhancement%20for%20self-adaptive%20grasping.%20We%20propose%20a%20query-based%20interaction%0Abetween%20high-level%20and%20low-level%20features%20through%20the%20Insight%20Transformer%2C%0Awhile%20the%20Empower%20Transformer%20selectively%20attends%20to%20the%20highest-level%0Afeatures%2C%20which%20synergistically%20strikes%20a%20balance%20between%20focusing%20on%20fine%0Ageometric%20details%20and%20overall%20geometric%20structures.%20Furthermore%2C%20MISCGrasp%0Autilizes%20multi-scale%20contrastive%20learning%20to%20exploit%20similarities%20among%0Apositive%20grasp%20samples%2C%20ensuring%20consistency%20across%20multi-scale%20features.%0AExtensive%20experiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%0Athat%20MISCGrasp%20outperforms%20baseline%20and%20variant%20methods%20in%20tabletop%0Adecluttering%20tasks.%20More%20details%20are%20available%20at%20https%3A//miscgrasp.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02672v1&entry.124074799=Read"},
{"title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation", "author": "Chenyang An and Shima Imani and Feng Yao and Chengyu Dong and Ali Abbasi and Harsh Shrivastava and Samuel Buss and Jingbo Shang and Gayathri Mahalingam and Pramod Sharma and Maurice Diesendruck", "abstract": "  In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.\n", "link": "http://arxiv.org/abs/2411.00863v2", "date": "2025-07-03", "relevancy": 2.4063, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation&body=Title%3A%20Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation%0AAuthor%3A%20Chenyang%20An%20and%20Shima%20Imani%20and%20Feng%20Yao%20and%20Chengyu%20Dong%20and%20Ali%20Abbasi%20and%20Harsh%20Shrivastava%20and%20Samuel%20Buss%20and%20Jingbo%20Shang%20and%20Gayathri%20Mahalingam%20and%20Pramod%20Sharma%20and%20Maurice%20Diesendruck%0AAbstract%3A%20%20%20In%20the%20field%20of%20large%20language%20model%20%28LLM%29-based%20proof%20generation%2C%20despite%0Aextensive%20training%20on%20large%20datasets%20such%20as%20ArXiv%2C%20LLMs%20still%20exhibit%20only%0Amodest%20performance%20on%20proving%20tasks%20of%20moderate%20difficulty.%20We%20believe%20that%0Athis%20is%20partly%20due%20to%20the%20widespread%20presence%20of%20suboptimal%20ordering%20within%20the%0Adata%20for%20each%20proof%20used%20in%20training.%20For%20example%2C%20published%20proofs%20often%0Afollow%20a%20purely%20logical%20order%2C%20where%20each%20step%20logically%20proceeds%20from%20the%0Aprevious%20steps%20based%20on%20the%20deductive%20rules.%20This%20order%20is%20designed%20to%0Afacilitate%20the%20verification%20of%20the%20proof%27s%20soundness%2C%20rather%20than%20to%20help%0Apeople%20and%20models%20learn%20the%20discovery%20process%20of%20the%20proof.%20In%20proof%0Ageneration%2C%20we%20argue%20that%20the%20optimal%20order%20for%20one%20training%20data%20sample%20occurs%0Awhen%20the%20relevant%20intermediate%20supervision%20for%20a%20particular%20proof%20step%20in%20the%0Aproof%20is%20always%20positioned%20to%20the%20left%20of%20that%20proof%20step.%20We%20call%20such%20order%0Athe%20intuitively%20sequential%20order.%20We%20validate%20our%20claims%20using%20two%20tasks%3A%0Aintuitionistic%20propositional%20logic%20theorem-proving%20and%20digit%20multiplication.%0AOur%20experiments%20verify%20the%20order%20effect%20and%20provide%20support%20for%20our%0Aexplanations.%20We%20demonstrate%20that%20training%20is%20most%20effective%20when%20the%20proof%20is%0Ain%20the%20intuitively%20sequential%20order.%20Moreover%2C%20the%20order%20effect%20and%20the%0Aperformance%20gap%20between%20models%20trained%20on%20different%20data%20orders%20can%20be%0Asubstantial%20--%20with%20an%2011%20percent%20improvement%20in%20proof%20success%20rate%20observed%20in%0Athe%20propositional%20logic%20theorem-proving%20task%2C%20between%20models%20trained%20on%20the%0Aoptimal%20order%20compared%20to%20the%20worst%20order.%20Lastly%2C%20we%20define%20a%20common%20type%20of%0Aorder%20issue%20in%20advanced%20math%20proofs%20and%20find%20that%2017.3%20percent%20of%20theorems%20with%0Anontrivial%20proofs%20in%20the%20first%20two%20chapters%20of%20a%20widely%20used%20graduate-level%0Amathematics%20textbook%20suffer%20from%20this%20issue.%20A%20detailed%20list%20of%20those%20proofs%20is%0Aprovided%20in%20the%20appendix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Token%2520Prediction%2520Task%2520Assumes%2520Optimal%2520Data%2520Ordering%2520for%2520LLM%250A%2520%2520Training%2520in%2520Proof%2520Generation%26entry.906535625%3DChenyang%2520An%2520and%2520Shima%2520Imani%2520and%2520Feng%2520Yao%2520and%2520Chengyu%2520Dong%2520and%2520Ali%2520Abbasi%2520and%2520Harsh%2520Shrivastava%2520and%2520Samuel%2520Buss%2520and%2520Jingbo%2520Shang%2520and%2520Gayathri%2520Mahalingam%2520and%2520Pramod%2520Sharma%2520and%2520Maurice%2520Diesendruck%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520large%2520language%2520model%2520%2528LLM%2529-based%2520proof%2520generation%252C%2520despite%250Aextensive%2520training%2520on%2520large%2520datasets%2520such%2520as%2520ArXiv%252C%2520LLMs%2520still%2520exhibit%2520only%250Amodest%2520performance%2520on%2520proving%2520tasks%2520of%2520moderate%2520difficulty.%2520We%2520believe%2520that%250Athis%2520is%2520partly%2520due%2520to%2520the%2520widespread%2520presence%2520of%2520suboptimal%2520ordering%2520within%2520the%250Adata%2520for%2520each%2520proof%2520used%2520in%2520training.%2520For%2520example%252C%2520published%2520proofs%2520often%250Afollow%2520a%2520purely%2520logical%2520order%252C%2520where%2520each%2520step%2520logically%2520proceeds%2520from%2520the%250Aprevious%2520steps%2520based%2520on%2520the%2520deductive%2520rules.%2520This%2520order%2520is%2520designed%2520to%250Afacilitate%2520the%2520verification%2520of%2520the%2520proof%2527s%2520soundness%252C%2520rather%2520than%2520to%2520help%250Apeople%2520and%2520models%2520learn%2520the%2520discovery%2520process%2520of%2520the%2520proof.%2520In%2520proof%250Ageneration%252C%2520we%2520argue%2520that%2520the%2520optimal%2520order%2520for%2520one%2520training%2520data%2520sample%2520occurs%250Awhen%2520the%2520relevant%2520intermediate%2520supervision%2520for%2520a%2520particular%2520proof%2520step%2520in%2520the%250Aproof%2520is%2520always%2520positioned%2520to%2520the%2520left%2520of%2520that%2520proof%2520step.%2520We%2520call%2520such%2520order%250Athe%2520intuitively%2520sequential%2520order.%2520We%2520validate%2520our%2520claims%2520using%2520two%2520tasks%253A%250Aintuitionistic%2520propositional%2520logic%2520theorem-proving%2520and%2520digit%2520multiplication.%250AOur%2520experiments%2520verify%2520the%2520order%2520effect%2520and%2520provide%2520support%2520for%2520our%250Aexplanations.%2520We%2520demonstrate%2520that%2520training%2520is%2520most%2520effective%2520when%2520the%2520proof%2520is%250Ain%2520the%2520intuitively%2520sequential%2520order.%2520Moreover%252C%2520the%2520order%2520effect%2520and%2520the%250Aperformance%2520gap%2520between%2520models%2520trained%2520on%2520different%2520data%2520orders%2520can%2520be%250Asubstantial%2520--%2520with%2520an%252011%2520percent%2520improvement%2520in%2520proof%2520success%2520rate%2520observed%2520in%250Athe%2520propositional%2520logic%2520theorem-proving%2520task%252C%2520between%2520models%2520trained%2520on%2520the%250Aoptimal%2520order%2520compared%2520to%2520the%2520worst%2520order.%2520Lastly%252C%2520we%2520define%2520a%2520common%2520type%2520of%250Aorder%2520issue%2520in%2520advanced%2520math%2520proofs%2520and%2520find%2520that%252017.3%2520percent%2520of%2520theorems%2520with%250Anontrivial%2520proofs%2520in%2520the%2520first%2520two%2520chapters%2520of%2520a%2520widely%2520used%2520graduate-level%250Amathematics%2520textbook%2520suffer%2520from%2520this%2520issue.%2520A%2520detailed%2520list%2520of%2520those%2520proofs%2520is%250Aprovided%2520in%2520the%2520appendix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation&entry.906535625=Chenyang%20An%20and%20Shima%20Imani%20and%20Feng%20Yao%20and%20Chengyu%20Dong%20and%20Ali%20Abbasi%20and%20Harsh%20Shrivastava%20and%20Samuel%20Buss%20and%20Jingbo%20Shang%20and%20Gayathri%20Mahalingam%20and%20Pramod%20Sharma%20and%20Maurice%20Diesendruck&entry.1292438233=%20%20In%20the%20field%20of%20large%20language%20model%20%28LLM%29-based%20proof%20generation%2C%20despite%0Aextensive%20training%20on%20large%20datasets%20such%20as%20ArXiv%2C%20LLMs%20still%20exhibit%20only%0Amodest%20performance%20on%20proving%20tasks%20of%20moderate%20difficulty.%20We%20believe%20that%0Athis%20is%20partly%20due%20to%20the%20widespread%20presence%20of%20suboptimal%20ordering%20within%20the%0Adata%20for%20each%20proof%20used%20in%20training.%20For%20example%2C%20published%20proofs%20often%0Afollow%20a%20purely%20logical%20order%2C%20where%20each%20step%20logically%20proceeds%20from%20the%0Aprevious%20steps%20based%20on%20the%20deductive%20rules.%20This%20order%20is%20designed%20to%0Afacilitate%20the%20verification%20of%20the%20proof%27s%20soundness%2C%20rather%20than%20to%20help%0Apeople%20and%20models%20learn%20the%20discovery%20process%20of%20the%20proof.%20In%20proof%0Ageneration%2C%20we%20argue%20that%20the%20optimal%20order%20for%20one%20training%20data%20sample%20occurs%0Awhen%20the%20relevant%20intermediate%20supervision%20for%20a%20particular%20proof%20step%20in%20the%0Aproof%20is%20always%20positioned%20to%20the%20left%20of%20that%20proof%20step.%20We%20call%20such%20order%0Athe%20intuitively%20sequential%20order.%20We%20validate%20our%20claims%20using%20two%20tasks%3A%0Aintuitionistic%20propositional%20logic%20theorem-proving%20and%20digit%20multiplication.%0AOur%20experiments%20verify%20the%20order%20effect%20and%20provide%20support%20for%20our%0Aexplanations.%20We%20demonstrate%20that%20training%20is%20most%20effective%20when%20the%20proof%20is%0Ain%20the%20intuitively%20sequential%20order.%20Moreover%2C%20the%20order%20effect%20and%20the%0Aperformance%20gap%20between%20models%20trained%20on%20different%20data%20orders%20can%20be%0Asubstantial%20--%20with%20an%2011%20percent%20improvement%20in%20proof%20success%20rate%20observed%20in%0Athe%20propositional%20logic%20theorem-proving%20task%2C%20between%20models%20trained%20on%20the%0Aoptimal%20order%20compared%20to%20the%20worst%20order.%20Lastly%2C%20we%20define%20a%20common%20type%20of%0Aorder%20issue%20in%20advanced%20math%20proofs%20and%20find%20that%2017.3%20percent%20of%20theorems%20with%0Anontrivial%20proofs%20in%20the%20first%20two%20chapters%20of%20a%20widely%20used%20graduate-level%0Amathematics%20textbook%20suffer%20from%20this%20issue.%20A%20detailed%20list%20of%20those%20proofs%20is%0Aprovided%20in%20the%20appendix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00863v2&entry.124074799=Read"},
{"title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network", "author": "Ying Yu and Hang Xiao and Siyao Li and Jiarui Li and Haotian Tang and Hanyu Liu and Chao Li", "abstract": "  The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.\n", "link": "http://arxiv.org/abs/2507.02827v1", "date": "2025-07-03", "relevancy": 2.3692, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6047}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5858}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network&body=Title%3A%20USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network%0AAuthor%3A%20Ying%20Yu%20and%20Hang%20Xiao%20and%20Siyao%20Li%20and%20Jiarui%20Li%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li%0AAbstract%3A%20%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSAD%253A%2520An%2520Unsupervised%2520Data%2520Augmentation%2520Spatio-Temporal%2520Attention%250A%2520%2520Diffusion%2520Network%26entry.906535625%3DYing%2520Yu%2520and%2520Hang%2520Xiao%2520and%2520Siyao%2520Li%2520and%2520Jiarui%2520Li%2520and%2520Haotian%2520Tang%2520and%2520Hanyu%2520Liu%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520The%2520primary%2520objective%2520of%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520to%2520infer%2520ongoing%250Ahuman%2520actions%2520from%2520sensor%2520data%252C%2520a%2520task%2520that%2520finds%2520broad%2520applications%2520in%2520health%250Amonitoring%252C%2520safety%2520protection%252C%2520and%2520sports%2520analysis.%2520Despite%2520proliferating%250Aresearch%252C%2520HAR%2520still%2520faces%2520key%2520challenges%252C%2520including%2520the%2520scarcity%2520of%2520labeled%250Asamples%2520for%2520rare%2520activities%252C%2520insufficient%2520extraction%2520of%2520high-level%2520features%252C%250Aand%2520suboptimal%2520model%2520performance%2520on%2520lightweight%2520devices.%2520To%2520address%2520these%250Aissues%252C%2520this%2520paper%2520proposes%2520a%2520comprehensive%2520optimization%2520approach%2520centered%2520on%250Amulti-attention%2520interaction%2520mechanisms.%2520First%252C%2520an%2520unsupervised%252C%250Astatistics-guided%2520diffusion%2520model%2520is%2520employed%2520to%2520perform%2520data%2520augmentation%252C%250Athereby%2520alleviating%2520the%2520problems%2520of%2520labeled%2520data%2520scarcity%2520and%2520severe%2520class%250Aimbalance.%2520Second%252C%2520a%2520multi-branch%2520spatio-temporal%2520interaction%2520network%2520is%250Adesigned%252C%2520which%2520captures%2520multi-scale%2520features%2520of%2520sequential%2520data%2520through%250Aparallel%2520residual%2520branches%2520with%25203%252A3%252C%25205%252A5%252C%2520and%25207%252A7%2520convolutional%2520kernels.%250ASimultaneously%252C%2520temporal%2520attention%2520mechanisms%2520are%2520incorporated%2520to%2520identify%250Acritical%2520time%2520points%252C%2520while%2520spatial%2520attention%2520enhances%2520inter-sensor%250Ainteractions.%2520A%2520cross-branch%2520feature%2520fusion%2520unit%2520is%2520further%2520introduced%2520to%250Aimprove%2520the%2520overall%2520feature%2520representation%2520capability.%2520Finally%252C%2520an%2520adaptive%250Amulti-loss%2520function%2520fusion%2520strategy%2520is%2520integrated%252C%2520allowing%2520for%2520dynamic%250Aadjustment%2520of%2520loss%2520weights%2520and%2520overall%2520model%2520optimization.%2520Experimental%2520results%250Aon%2520three%2520public%2520datasets%252C%2520WISDM%252C%2520PAMAP2%252C%2520and%2520OPPORTUNITY%252C%2520demonstrate%2520that%2520the%250Aproposed%2520unsupervised%2520data%2520augmentation%2520spatio-temporal%2520attention%2520diffusion%250Anetwork%2520%2528USAD%2529%2520achieves%2520accuracies%2520of%252098.84%2525%252C%252093.81%2525%252C%2520and%252080.92%2525%2520respectively%252C%250Asignificantly%2520outperforming%2520existing%2520approaches.%2520Furthermore%252C%2520practical%250Adeployment%2520on%2520embedded%2520devices%2520verifies%2520the%2520efficiency%2520and%2520feasibility%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network&entry.906535625=Ying%20Yu%20and%20Hang%20Xiao%20and%20Siyao%20Li%20and%20Jiarui%20Li%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li&entry.1292438233=%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02827v1&entry.124074799=Read"},
{"title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical\n  Image Self-supervised Learning", "author": "Tan Pan and Zhaorui Tan and Kaiyu Guo and Dongli Xu and Weidi Xu and Chen Jiang and Xin Guo and Yuan Qi and Yuan Cheng", "abstract": "  3D medical image self-supervised learning (mSSL) holds great promise for\nmedical analysis. Effectively supporting broader applications requires\nconsidering anatomical structure variations in location, scale, and morphology,\nwhich are crucial for capturing meaningful distinctions. However, previous mSSL\nmethods partition images with fixed-size patches, often ignoring the structure\nvariations. In this work, we introduce a novel perspective on 3D medical images\nwith the goal of learning structure-aware representations. We assume that\npatches within the same structure share the same semantics (semantic\nconsistency) while those from different structures exhibit distinct semantics\n(semantic discrepancy). Based on this assumption, we propose an mSSL framework\nnamed $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency\nin two steps. First, $S^2DC$ enforces distinct representations for different\npatches to increase semantic discrepancy by leveraging an optimal transport\nstrategy. Second, $S^2DC$ advances semantic consistency at the structural level\nbased on neighborhood similarity distribution. By bridging patch-level and\nstructure-level representations, $S^2DC$ achieves structure-aware\nrepresentations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3\nmodalities, our proposed method consistently outperforms the state-of-the-art\nmethods in mSSL.\n", "link": "http://arxiv.org/abs/2507.02581v1", "date": "2025-07-03", "relevancy": 2.3624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.61}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5772}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning&body=Title%3A%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning%0AAuthor%3A%20Tan%20Pan%20and%20Zhaorui%20Tan%20and%20Kaiyu%20Guo%20and%20Dongli%20Xu%20and%20Weidi%20Xu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Yuan%20Qi%20and%20Yuan%20Cheng%0AAbstract%3A%20%20%203D%20medical%20image%20self-supervised%20learning%20%28mSSL%29%20holds%20great%20promise%20for%0Amedical%20analysis.%20Effectively%20supporting%20broader%20applications%20requires%0Aconsidering%20anatomical%20structure%20variations%20in%20location%2C%20scale%2C%20and%20morphology%2C%0Awhich%20are%20crucial%20for%20capturing%20meaningful%20distinctions.%20However%2C%20previous%20mSSL%0Amethods%20partition%20images%20with%20fixed-size%20patches%2C%20often%20ignoring%20the%20structure%0Avariations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20perspective%20on%203D%20medical%20images%0Awith%20the%20goal%20of%20learning%20structure-aware%20representations.%20We%20assume%20that%0Apatches%20within%20the%20same%20structure%20share%20the%20same%20semantics%20%28semantic%0Aconsistency%29%20while%20those%20from%20different%20structures%20exhibit%20distinct%20semantics%0A%28semantic%20discrepancy%29.%20Based%20on%20this%20assumption%2C%20we%20propose%20an%20mSSL%20framework%0Anamed%20%24S%5E2DC%24%2C%20achieving%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%0Ain%20two%20steps.%20First%2C%20%24S%5E2DC%24%20enforces%20distinct%20representations%20for%20different%0Apatches%20to%20increase%20semantic%20discrepancy%20by%20leveraging%20an%20optimal%20transport%0Astrategy.%20Second%2C%20%24S%5E2DC%24%20advances%20semantic%20consistency%20at%20the%20structural%20level%0Abased%20on%20neighborhood%20similarity%20distribution.%20By%20bridging%20patch-level%20and%0Astructure-level%20representations%2C%20%24S%5E2DC%24%20achieves%20structure-aware%0Arepresentations.%20Thoroughly%20evaluated%20across%2010%20datasets%2C%204%20tasks%2C%20and%203%0Amodalities%2C%20our%20proposed%20method%20consistently%20outperforms%20the%20state-of-the-art%0Amethods%20in%20mSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-aware%2520Semantic%2520Discrepancy%2520and%2520Consistency%2520for%25203D%2520Medical%250A%2520%2520Image%2520Self-supervised%2520Learning%26entry.906535625%3DTan%2520Pan%2520and%2520Zhaorui%2520Tan%2520and%2520Kaiyu%2520Guo%2520and%2520Dongli%2520Xu%2520and%2520Weidi%2520Xu%2520and%2520Chen%2520Jiang%2520and%2520Xin%2520Guo%2520and%2520Yuan%2520Qi%2520and%2520Yuan%2520Cheng%26entry.1292438233%3D%2520%25203D%2520medical%2520image%2520self-supervised%2520learning%2520%2528mSSL%2529%2520holds%2520great%2520promise%2520for%250Amedical%2520analysis.%2520Effectively%2520supporting%2520broader%2520applications%2520requires%250Aconsidering%2520anatomical%2520structure%2520variations%2520in%2520location%252C%2520scale%252C%2520and%2520morphology%252C%250Awhich%2520are%2520crucial%2520for%2520capturing%2520meaningful%2520distinctions.%2520However%252C%2520previous%2520mSSL%250Amethods%2520partition%2520images%2520with%2520fixed-size%2520patches%252C%2520often%2520ignoring%2520the%2520structure%250Avariations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520perspective%2520on%25203D%2520medical%2520images%250Awith%2520the%2520goal%2520of%2520learning%2520structure-aware%2520representations.%2520We%2520assume%2520that%250Apatches%2520within%2520the%2520same%2520structure%2520share%2520the%2520same%2520semantics%2520%2528semantic%250Aconsistency%2529%2520while%2520those%2520from%2520different%2520structures%2520exhibit%2520distinct%2520semantics%250A%2528semantic%2520discrepancy%2529.%2520Based%2520on%2520this%2520assumption%252C%2520we%2520propose%2520an%2520mSSL%2520framework%250Anamed%2520%2524S%255E2DC%2524%252C%2520achieving%2520Structure-aware%2520Semantic%2520Discrepancy%2520and%2520Consistency%250Ain%2520two%2520steps.%2520First%252C%2520%2524S%255E2DC%2524%2520enforces%2520distinct%2520representations%2520for%2520different%250Apatches%2520to%2520increase%2520semantic%2520discrepancy%2520by%2520leveraging%2520an%2520optimal%2520transport%250Astrategy.%2520Second%252C%2520%2524S%255E2DC%2524%2520advances%2520semantic%2520consistency%2520at%2520the%2520structural%2520level%250Abased%2520on%2520neighborhood%2520similarity%2520distribution.%2520By%2520bridging%2520patch-level%2520and%250Astructure-level%2520representations%252C%2520%2524S%255E2DC%2524%2520achieves%2520structure-aware%250Arepresentations.%2520Thoroughly%2520evaluated%2520across%252010%2520datasets%252C%25204%2520tasks%252C%2520and%25203%250Amodalities%252C%2520our%2520proposed%2520method%2520consistently%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520in%2520mSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning&entry.906535625=Tan%20Pan%20and%20Zhaorui%20Tan%20and%20Kaiyu%20Guo%20and%20Dongli%20Xu%20and%20Weidi%20Xu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Yuan%20Qi%20and%20Yuan%20Cheng&entry.1292438233=%20%203D%20medical%20image%20self-supervised%20learning%20%28mSSL%29%20holds%20great%20promise%20for%0Amedical%20analysis.%20Effectively%20supporting%20broader%20applications%20requires%0Aconsidering%20anatomical%20structure%20variations%20in%20location%2C%20scale%2C%20and%20morphology%2C%0Awhich%20are%20crucial%20for%20capturing%20meaningful%20distinctions.%20However%2C%20previous%20mSSL%0Amethods%20partition%20images%20with%20fixed-size%20patches%2C%20often%20ignoring%20the%20structure%0Avariations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20perspective%20on%203D%20medical%20images%0Awith%20the%20goal%20of%20learning%20structure-aware%20representations.%20We%20assume%20that%0Apatches%20within%20the%20same%20structure%20share%20the%20same%20semantics%20%28semantic%0Aconsistency%29%20while%20those%20from%20different%20structures%20exhibit%20distinct%20semantics%0A%28semantic%20discrepancy%29.%20Based%20on%20this%20assumption%2C%20we%20propose%20an%20mSSL%20framework%0Anamed%20%24S%5E2DC%24%2C%20achieving%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%0Ain%20two%20steps.%20First%2C%20%24S%5E2DC%24%20enforces%20distinct%20representations%20for%20different%0Apatches%20to%20increase%20semantic%20discrepancy%20by%20leveraging%20an%20optimal%20transport%0Astrategy.%20Second%2C%20%24S%5E2DC%24%20advances%20semantic%20consistency%20at%20the%20structural%20level%0Abased%20on%20neighborhood%20similarity%20distribution.%20By%20bridging%20patch-level%20and%0Astructure-level%20representations%2C%20%24S%5E2DC%24%20achieves%20structure-aware%0Arepresentations.%20Thoroughly%20evaluated%20across%2010%20datasets%2C%204%20tasks%2C%20and%203%0Amodalities%2C%20our%20proposed%20method%20consistently%20outperforms%20the%20state-of-the-art%0Amethods%20in%20mSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02581v1&entry.124074799=Read"},
{"title": "RefTok: Reference-Based Tokenization for Video Generation", "author": "Xiang Fan and Xiaohang Sun and Kushan Thakkar and Zhu Liu and Vimal Bhat and Ranjay Krishna and Xiang Hao", "abstract": "  Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.\n", "link": "http://arxiv.org/abs/2507.02862v1", "date": "2025-07-03", "relevancy": 2.3621, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5977}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation&body=Title%3A%20RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation%0AAuthor%3A%20Xiang%20Fan%20and%20Xiaohang%20Sun%20and%20Kushan%20Thakkar%20and%20Zhu%20Liu%20and%20Vimal%20Bhat%20and%20Ranjay%20Krishna%20and%20Xiang%20Hao%0AAbstract%3A%20%20%20Effectively%20handling%20temporal%20redundancy%20remains%20a%20key%20challenge%20in%20learning%0Avideo%20models.%20Prevailing%20approaches%20often%20treat%20each%20set%20of%20frames%0Aindependently%2C%20failing%20to%20effectively%20capture%20the%20temporal%20dependencies%20and%0Aredundancies%20inherent%20in%20videos.%20To%20address%20this%20limitation%2C%20we%20introduce%0ARefTok%2C%20a%20novel%20reference-based%20tokenization%20method%20capable%20of%20capturing%0Acomplex%20temporal%20dynamics%20and%20contextual%20information.%20Our%20method%20encodes%20and%0Adecodes%20sets%20of%20frames%20conditioned%20on%20an%20unquantized%20reference%20frame.%20When%0Adecoded%2C%20RefTok%20preserves%20the%20continuity%20of%20motion%20and%20the%20appearance%20of%0Aobjects%20across%20frames.%20For%20example%2C%20RefTok%20retains%20facial%20details%20despite%20head%0Amotion%2C%20reconstructs%20text%20correctly%2C%20preserves%20small%20patterns%2C%20and%20maintains%0Athe%20legibility%20of%20handwriting%20from%20the%20context.%20Across%204%20video%20datasets%20%28K600%2C%0AUCF-101%2C%20BAIR%20Robot%20Pushing%2C%20and%20DAVIS%29%2C%20RefTok%20significantly%20outperforms%0Acurrent%20state-of-the-art%20tokenizers%20%28Cosmos%20and%20MAGVIT%29%20and%20improves%20all%0Aevaluated%20metrics%20%28PSNR%2C%20SSIM%2C%20LPIPS%29%20by%20an%20average%20of%2036.7%25%20at%20the%20same%20or%0Ahigher%20compression%20ratios.%20When%20a%20video%20generation%20model%20is%20trained%20using%0ARefTok%27s%20latents%20on%20the%20BAIR%20Robot%20Pushing%20task%2C%20the%20generations%20not%20only%0Aoutperform%20MAGVIT-B%20but%20the%20larger%20MAGVIT-L%2C%20which%20has%204x%20more%20parameters%2C%0Aacross%20all%20generation%20metrics%20by%20an%20average%20of%2027.9%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefTok%253A%2520Reference-Based%2520Tokenization%2520for%2520Video%2520Generation%26entry.906535625%3DXiang%2520Fan%2520and%2520Xiaohang%2520Sun%2520and%2520Kushan%2520Thakkar%2520and%2520Zhu%2520Liu%2520and%2520Vimal%2520Bhat%2520and%2520Ranjay%2520Krishna%2520and%2520Xiang%2520Hao%26entry.1292438233%3D%2520%2520Effectively%2520handling%2520temporal%2520redundancy%2520remains%2520a%2520key%2520challenge%2520in%2520learning%250Avideo%2520models.%2520Prevailing%2520approaches%2520often%2520treat%2520each%2520set%2520of%2520frames%250Aindependently%252C%2520failing%2520to%2520effectively%2520capture%2520the%2520temporal%2520dependencies%2520and%250Aredundancies%2520inherent%2520in%2520videos.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250ARefTok%252C%2520a%2520novel%2520reference-based%2520tokenization%2520method%2520capable%2520of%2520capturing%250Acomplex%2520temporal%2520dynamics%2520and%2520contextual%2520information.%2520Our%2520method%2520encodes%2520and%250Adecodes%2520sets%2520of%2520frames%2520conditioned%2520on%2520an%2520unquantized%2520reference%2520frame.%2520When%250Adecoded%252C%2520RefTok%2520preserves%2520the%2520continuity%2520of%2520motion%2520and%2520the%2520appearance%2520of%250Aobjects%2520across%2520frames.%2520For%2520example%252C%2520RefTok%2520retains%2520facial%2520details%2520despite%2520head%250Amotion%252C%2520reconstructs%2520text%2520correctly%252C%2520preserves%2520small%2520patterns%252C%2520and%2520maintains%250Athe%2520legibility%2520of%2520handwriting%2520from%2520the%2520context.%2520Across%25204%2520video%2520datasets%2520%2528K600%252C%250AUCF-101%252C%2520BAIR%2520Robot%2520Pushing%252C%2520and%2520DAVIS%2529%252C%2520RefTok%2520significantly%2520outperforms%250Acurrent%2520state-of-the-art%2520tokenizers%2520%2528Cosmos%2520and%2520MAGVIT%2529%2520and%2520improves%2520all%250Aevaluated%2520metrics%2520%2528PSNR%252C%2520SSIM%252C%2520LPIPS%2529%2520by%2520an%2520average%2520of%252036.7%2525%2520at%2520the%2520same%2520or%250Ahigher%2520compression%2520ratios.%2520When%2520a%2520video%2520generation%2520model%2520is%2520trained%2520using%250ARefTok%2527s%2520latents%2520on%2520the%2520BAIR%2520Robot%2520Pushing%2520task%252C%2520the%2520generations%2520not%2520only%250Aoutperform%2520MAGVIT-B%2520but%2520the%2520larger%2520MAGVIT-L%252C%2520which%2520has%25204x%2520more%2520parameters%252C%250Aacross%2520all%2520generation%2520metrics%2520by%2520an%2520average%2520of%252027.9%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation&entry.906535625=Xiang%20Fan%20and%20Xiaohang%20Sun%20and%20Kushan%20Thakkar%20and%20Zhu%20Liu%20and%20Vimal%20Bhat%20and%20Ranjay%20Krishna%20and%20Xiang%20Hao&entry.1292438233=%20%20Effectively%20handling%20temporal%20redundancy%20remains%20a%20key%20challenge%20in%20learning%0Avideo%20models.%20Prevailing%20approaches%20often%20treat%20each%20set%20of%20frames%0Aindependently%2C%20failing%20to%20effectively%20capture%20the%20temporal%20dependencies%20and%0Aredundancies%20inherent%20in%20videos.%20To%20address%20this%20limitation%2C%20we%20introduce%0ARefTok%2C%20a%20novel%20reference-based%20tokenization%20method%20capable%20of%20capturing%0Acomplex%20temporal%20dynamics%20and%20contextual%20information.%20Our%20method%20encodes%20and%0Adecodes%20sets%20of%20frames%20conditioned%20on%20an%20unquantized%20reference%20frame.%20When%0Adecoded%2C%20RefTok%20preserves%20the%20continuity%20of%20motion%20and%20the%20appearance%20of%0Aobjects%20across%20frames.%20For%20example%2C%20RefTok%20retains%20facial%20details%20despite%20head%0Amotion%2C%20reconstructs%20text%20correctly%2C%20preserves%20small%20patterns%2C%20and%20maintains%0Athe%20legibility%20of%20handwriting%20from%20the%20context.%20Across%204%20video%20datasets%20%28K600%2C%0AUCF-101%2C%20BAIR%20Robot%20Pushing%2C%20and%20DAVIS%29%2C%20RefTok%20significantly%20outperforms%0Acurrent%20state-of-the-art%20tokenizers%20%28Cosmos%20and%20MAGVIT%29%20and%20improves%20all%0Aevaluated%20metrics%20%28PSNR%2C%20SSIM%2C%20LPIPS%29%20by%20an%20average%20of%2036.7%25%20at%20the%20same%20or%0Ahigher%20compression%20ratios.%20When%20a%20video%20generation%20model%20is%20trained%20using%0ARefTok%27s%20latents%20on%20the%20BAIR%20Robot%20Pushing%20task%2C%20the%20generations%20not%20only%0Aoutperform%20MAGVIT-B%20but%20the%20larger%20MAGVIT-L%2C%20which%20has%204x%20more%20parameters%2C%0Aacross%20all%20generation%20metrics%20by%20an%20average%20of%2027.9%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02862v1&entry.124074799=Read"},
{"title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification", "author": "Deepak Narayan Gadde and Keerthan Kopparam Radhakrishna and Vaisakh Naduvodi Viswambharan and Aman Kumar and Djones Lettnin and Wolfgang Kunz and Sebastian Simon", "abstract": "  Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.\n", "link": "http://arxiv.org/abs/2507.02660v1", "date": "2025-07-03", "relevancy": 2.3584, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4886}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4751}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification&body=Title%3A%20Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification%0AAuthor%3A%20Deepak%20Narayan%20Gadde%20and%20Keerthan%20Kopparam%20Radhakrishna%20and%20Vaisakh%20Naduvodi%20Viswambharan%20and%20Aman%20Kumar%20and%20Djones%20Lettnin%20and%20Wolfgang%20Kunz%20and%20Sebastian%20Simon%0AAbstract%3A%20%20%20Modern%20Integrated%20Circuits%20%28ICs%29%20are%20becoming%20increasingly%20complex%2C%20and%20so%20is%0Atheir%20development%20process.%20Hardware%20design%20verification%20entails%20a%20methodical%0Aand%20disciplined%20approach%20to%20the%20planning%2C%20development%2C%20execution%2C%20and%20sign-off%0Aof%20functionally%20correct%20hardware%20designs.%20This%20tedious%20process%20requires%0Asignificant%20effort%20and%20time%20to%20ensure%20a%20bug-free%20tape-out.%20The%20field%20of%20Natural%0ALanguage%20Processing%20has%20undergone%20a%20significant%20transformation%20with%20the%20advent%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20These%20powerful%20models%2C%20often%20referred%20to%20as%0AGenerative%20AI%20%28GenAI%29%2C%20have%20revolutionized%20how%20machines%20understand%20and%20generate%0Ahuman%20language%2C%20enabling%20unprecedented%20advancements%20in%20a%20wide%20array%20of%0Aapplications%2C%20including%20hardware%20design%20verification.%20This%20paper%20presents%20an%0Aagentic%20AI-based%20approach%20to%20hardware%20design%20verification%2C%20which%20empowers%20AI%0Aagents%2C%20in%20collaboration%20with%20Humain-in-the-Loop%20%28HITL%29%20intervention%2C%20to%20engage%0Ain%20a%20more%20dynamic%2C%20iterative%2C%20and%20self-reflective%20process%2C%20ultimately%0Aperforming%20end-to-end%20hardware%20design%20and%20verification.%20This%20methodology%20is%0Aevaluated%20on%20five%20open-source%20designs%2C%20achieving%20over%2095%25%20coverage%20with%20reduced%0Averification%20time%20while%20demonstrating%20superior%20performance%2C%20adaptability%2C%20and%0Aconfigurability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHey%2520AI%252C%2520Generate%2520Me%2520a%2520Hardware%2520Code%2521%2520Agentic%2520AI-based%2520Hardware%2520Design%2520%2526%250A%2520%2520Verification%26entry.906535625%3DDeepak%2520Narayan%2520Gadde%2520and%2520Keerthan%2520Kopparam%2520Radhakrishna%2520and%2520Vaisakh%2520Naduvodi%2520Viswambharan%2520and%2520Aman%2520Kumar%2520and%2520Djones%2520Lettnin%2520and%2520Wolfgang%2520Kunz%2520and%2520Sebastian%2520Simon%26entry.1292438233%3D%2520%2520Modern%2520Integrated%2520Circuits%2520%2528ICs%2529%2520are%2520becoming%2520increasingly%2520complex%252C%2520and%2520so%2520is%250Atheir%2520development%2520process.%2520Hardware%2520design%2520verification%2520entails%2520a%2520methodical%250Aand%2520disciplined%2520approach%2520to%2520the%2520planning%252C%2520development%252C%2520execution%252C%2520and%2520sign-off%250Aof%2520functionally%2520correct%2520hardware%2520designs.%2520This%2520tedious%2520process%2520requires%250Asignificant%2520effort%2520and%2520time%2520to%2520ensure%2520a%2520bug-free%2520tape-out.%2520The%2520field%2520of%2520Natural%250ALanguage%2520Processing%2520has%2520undergone%2520a%2520significant%2520transformation%2520with%2520the%2520advent%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520These%2520powerful%2520models%252C%2520often%2520referred%2520to%2520as%250AGenerative%2520AI%2520%2528GenAI%2529%252C%2520have%2520revolutionized%2520how%2520machines%2520understand%2520and%2520generate%250Ahuman%2520language%252C%2520enabling%2520unprecedented%2520advancements%2520in%2520a%2520wide%2520array%2520of%250Aapplications%252C%2520including%2520hardware%2520design%2520verification.%2520This%2520paper%2520presents%2520an%250Aagentic%2520AI-based%2520approach%2520to%2520hardware%2520design%2520verification%252C%2520which%2520empowers%2520AI%250Aagents%252C%2520in%2520collaboration%2520with%2520Humain-in-the-Loop%2520%2528HITL%2529%2520intervention%252C%2520to%2520engage%250Ain%2520a%2520more%2520dynamic%252C%2520iterative%252C%2520and%2520self-reflective%2520process%252C%2520ultimately%250Aperforming%2520end-to-end%2520hardware%2520design%2520and%2520verification.%2520This%2520methodology%2520is%250Aevaluated%2520on%2520five%2520open-source%2520designs%252C%2520achieving%2520over%252095%2525%2520coverage%2520with%2520reduced%250Averification%2520time%2520while%2520demonstrating%2520superior%2520performance%252C%2520adaptability%252C%2520and%250Aconfigurability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification&entry.906535625=Deepak%20Narayan%20Gadde%20and%20Keerthan%20Kopparam%20Radhakrishna%20and%20Vaisakh%20Naduvodi%20Viswambharan%20and%20Aman%20Kumar%20and%20Djones%20Lettnin%20and%20Wolfgang%20Kunz%20and%20Sebastian%20Simon&entry.1292438233=%20%20Modern%20Integrated%20Circuits%20%28ICs%29%20are%20becoming%20increasingly%20complex%2C%20and%20so%20is%0Atheir%20development%20process.%20Hardware%20design%20verification%20entails%20a%20methodical%0Aand%20disciplined%20approach%20to%20the%20planning%2C%20development%2C%20execution%2C%20and%20sign-off%0Aof%20functionally%20correct%20hardware%20designs.%20This%20tedious%20process%20requires%0Asignificant%20effort%20and%20time%20to%20ensure%20a%20bug-free%20tape-out.%20The%20field%20of%20Natural%0ALanguage%20Processing%20has%20undergone%20a%20significant%20transformation%20with%20the%20advent%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20These%20powerful%20models%2C%20often%20referred%20to%20as%0AGenerative%20AI%20%28GenAI%29%2C%20have%20revolutionized%20how%20machines%20understand%20and%20generate%0Ahuman%20language%2C%20enabling%20unprecedented%20advancements%20in%20a%20wide%20array%20of%0Aapplications%2C%20including%20hardware%20design%20verification.%20This%20paper%20presents%20an%0Aagentic%20AI-based%20approach%20to%20hardware%20design%20verification%2C%20which%20empowers%20AI%0Aagents%2C%20in%20collaboration%20with%20Humain-in-the-Loop%20%28HITL%29%20intervention%2C%20to%20engage%0Ain%20a%20more%20dynamic%2C%20iterative%2C%20and%20self-reflective%20process%2C%20ultimately%0Aperforming%20end-to-end%20hardware%20design%20and%20verification.%20This%20methodology%20is%0Aevaluated%20on%20five%20open-source%20designs%2C%20achieving%20over%2095%25%20coverage%20with%20reduced%0Averification%20time%20while%20demonstrating%20superior%20performance%2C%20adaptability%2C%20and%0Aconfigurability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02660v1&entry.124074799=Read"},
{"title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models", "author": "Shehel Yoosuf and Temoor Ali and Ahmed Lekssays and Mashael AlSabah and Issa Khalil", "abstract": "  In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.\n", "link": "http://arxiv.org/abs/2502.11853v2", "date": "2025-07-03", "relevancy": 2.3342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&body=Title%3A%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%2C%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%20a%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%20content%0Atransformations%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructTransform%253A%2520A%2520Scalable%2520Attack%2520Surface%2520for%2520Safety-Aligned%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DShehel%2520Yoosuf%2520and%2520Temoor%2520Ali%2520and%2520Ahmed%2520Lekssays%2520and%2520Mashael%2520AlSabah%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520series%2520of%2520structure%2520transformation%2520attacks%2520on%2520LLM%250Aalignment%252C%2520where%2520we%2520encode%2520natural%2520language%2520intent%2520using%2520diverse%2520syntax%2520spaces%252C%250Aranging%2520from%2520simple%2520structure%2520formats%2520and%2520basic%2520query%2520languages%2520%2528e.g.%252C%2520SQL%2529%2520to%250Anew%2520novel%2520spaces%2520and%2520syntaxes%2520created%2520entirely%2520by%2520LLMs.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520our%2520simplest%2520attacks%2520can%2520achieve%2520close%2520to%2520a%252090%2525%2520success%250Arate%252C%2520even%2520on%2520strict%2520LLMs%2520%2528such%2520as%2520Claude%25203.5%2520Sonnet%2529%2520using%2520SOTA%2520alignment%250Amechanisms.%2520We%2520improve%2520the%2520attack%2520performance%2520further%2520by%2520using%2520an%2520adaptive%250Ascheme%2520that%2520combines%2520structure%2520transformations%2520along%2520with%2520existing%2520content%250Atransformations%252C%2520resulting%2520in%2520over%252096%2525%2520ASR%2520with%25200%2525%2520refusals.%250A%2520%2520To%2520generalize%2520our%2520attacks%252C%2520we%2520explore%2520numerous%2520structure%2520formats%252C%2520including%250Asyntaxes%2520purely%2520generated%2520by%2520LLMs.%2520Our%2520results%2520indicate%2520that%2520such%2520novel%250Asyntaxes%2520are%2520easy%2520to%2520generate%2520and%2520result%2520in%2520a%2520high%2520ASR%252C%2520suggesting%2520that%250Adefending%2520against%2520our%2520attacks%2520is%2520not%2520a%2520straightforward%2520process.%2520Finally%252C%2520we%250Adevelop%2520a%2520benchmark%2520and%2520evaluate%2520existing%2520safety-alignment%2520defenses%2520against%2520it%252C%250Ashowing%2520that%2520most%2520of%2520them%2520fail%2520with%2520100%2525%2520ASR.%2520Our%2520results%2520show%2520that%2520existing%250Asafety%2520alignment%2520mostly%2520relies%2520on%2520token-level%2520patterns%2520without%2520recognizing%250Aharmful%2520concepts%252C%2520highlighting%2520and%2520motivating%2520the%2520need%2520for%2520serious%2520research%250Aefforts%2520in%2520this%2520direction.%2520As%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520how%2520attackers%2520can%250Ause%2520our%2520attack%2520to%2520easily%2520generate%2520a%2520sample%2520malware%2520and%2520a%2520corpus%2520of%2520fraudulent%250ASMS%2520messages%252C%2520which%2520perform%2520well%2520in%2520bypassing%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&entry.906535625=Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%2C%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%20a%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%20content%0Atransformations%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11853v2&entry.124074799=Read"},
{"title": "Interpreting Graph Inference with Skyline Explanations", "author": "Dazhuo Qiu and Haolai Che and Arijit Khan and Yinghui Wu", "abstract": "  Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNNs outputs are often hard to interpret comprehensively.\nExisting methods typically compromise to individual pre-defined explainability\nmeasures (such as fidelity), which often leads to biased, ``one-sided''\ninterpretations. This paper introduces skyline explanation, a new paradigm that\ninterprets GNN output by simultaneously optimizing multiple explainability\nmeasures of users' interests. (1) We propose skyline explanations as a Pareto\nset of explanatory subgraphs that dominate others over multiple explanatory\nmeasures. We formulate skyline explanation as a multi-criteria optimization\nproblem, and establish its hardness results. (2) We design efficient algorithms\nwith an onion-peeling approach, which strategically prioritizes nodes and\nremoves unpromising edges to incrementally assemble skyline explanations. (3)\nWe also develop an algorithm to diversify the skyline explanations to enrich\nthe comprehensive interpretation. (4) We introduce efficient parallel\nalgorithms with load-balancing strategies to scale skyline explanation for\nlarge-scale GNN-based inference. Using real-world and synthetic graphs, we\nexperimentally verify our algorithms' effectiveness and scalability.\n", "link": "http://arxiv.org/abs/2505.07635v2", "date": "2025-07-03", "relevancy": 2.3318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&body=Title%3A%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations%0AAuthor%3A%20Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu%0AAbstract%3A%20%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNNs%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%0AExisting%20methods%20typically%20compromise%20to%20individual%20pre-defined%20explainability%0Ameasures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-sided%27%27%0Ainterpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%0Ainterprets%20GNN%20output%20by%20simultaneously%20optimizing%20multiple%20explainability%0Ameasures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%0Aset%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%0Ameasures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%0Aproblem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%0Awith%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%0Aremoves%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%0AWe%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%0Athe%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%0Aalgorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%0Alarge-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%0Aexperimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Graph%2520Inference%2520with%2520Skyline%2520Explanations%26entry.906535625%3DDazhuo%2520Qiu%2520and%2520Haolai%2520Che%2520and%2520Arijit%2520Khan%2520and%2520Yinghui%2520Wu%26entry.1292438233%3D%2520%2520Inference%2520queries%2520have%2520been%2520routinely%2520issued%2520to%2520graph%2520machine%2520learning%2520models%250Asuch%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520various%2520network%2520analytical%2520tasks.%250ANevertheless%252C%2520GNNs%2520outputs%2520are%2520often%2520hard%2520to%2520interpret%2520comprehensively.%250AExisting%2520methods%2520typically%2520compromise%2520to%2520individual%2520pre-defined%2520explainability%250Ameasures%2520%2528such%2520as%2520fidelity%2529%252C%2520which%2520often%2520leads%2520to%2520biased%252C%2520%2560%2560one-sided%2527%2527%250Ainterpretations.%2520This%2520paper%2520introduces%2520skyline%2520explanation%252C%2520a%2520new%2520paradigm%2520that%250Ainterprets%2520GNN%2520output%2520by%2520simultaneously%2520optimizing%2520multiple%2520explainability%250Ameasures%2520of%2520users%2527%2520interests.%2520%25281%2529%2520We%2520propose%2520skyline%2520explanations%2520as%2520a%2520Pareto%250Aset%2520of%2520explanatory%2520subgraphs%2520that%2520dominate%2520others%2520over%2520multiple%2520explanatory%250Ameasures.%2520We%2520formulate%2520skyline%2520explanation%2520as%2520a%2520multi-criteria%2520optimization%250Aproblem%252C%2520and%2520establish%2520its%2520hardness%2520results.%2520%25282%2529%2520We%2520design%2520efficient%2520algorithms%250Awith%2520an%2520onion-peeling%2520approach%252C%2520which%2520strategically%2520prioritizes%2520nodes%2520and%250Aremoves%2520unpromising%2520edges%2520to%2520incrementally%2520assemble%2520skyline%2520explanations.%2520%25283%2529%250AWe%2520also%2520develop%2520an%2520algorithm%2520to%2520diversify%2520the%2520skyline%2520explanations%2520to%2520enrich%250Athe%2520comprehensive%2520interpretation.%2520%25284%2529%2520We%2520introduce%2520efficient%2520parallel%250Aalgorithms%2520with%2520load-balancing%2520strategies%2520to%2520scale%2520skyline%2520explanation%2520for%250Alarge-scale%2520GNN-based%2520inference.%2520Using%2520real-world%2520and%2520synthetic%2520graphs%252C%2520we%250Aexperimentally%2520verify%2520our%2520algorithms%2527%2520effectiveness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&entry.906535625=Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu&entry.1292438233=%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNNs%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%0AExisting%20methods%20typically%20compromise%20to%20individual%20pre-defined%20explainability%0Ameasures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-sided%27%27%0Ainterpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%0Ainterprets%20GNN%20output%20by%20simultaneously%20optimizing%20multiple%20explainability%0Ameasures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%0Aset%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%0Ameasures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%0Aproblem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%0Awith%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%0Aremoves%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%0AWe%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%0Athe%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%0Aalgorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%0Alarge-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%0Aexperimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07635v2&entry.124074799=Read"},
{"title": "Learning few-step posterior samplers by unfolding and distillation of\n  diffusion models", "author": "Charlesquin Kemajou Mbakam and Jonathan Spence and Marcelo Pereyra", "abstract": "  Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.\n", "link": "http://arxiv.org/abs/2507.02686v1", "date": "2025-07-03", "relevancy": 2.3244, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6703}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5661}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models&body=Title%3A%20Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models%0AAuthor%3A%20Charlesquin%20Kemajou%20Mbakam%20and%20Jonathan%20Spence%20and%20Marcelo%20Pereyra%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20emerged%20as%20powerful%20image%20priors%20in%20Bayesian%0Acomputational%20imaging.%20Two%20primary%20strategies%20have%20been%20proposed%20for%20leveraging%0ADMs%20in%20this%20context%3A%20Plug-and-Play%20methods%2C%20which%20are%20zero-shot%20and%20highly%0Aflexible%20but%20rely%20on%20approximations%3B%20and%20specialized%20conditional%20DMs%2C%20which%0Aachieve%20higher%20accuracy%20and%20faster%20inference%20for%20specific%20tasks%20through%0Asupervised%20training.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20that%0Aintegrates%20deep%20unfolding%20and%20model%20distillation%20to%20transform%20a%20DM%20image%20prior%0Ainto%20a%20few-step%20conditional%20model%20for%20posterior%20sampling.%20A%20central%20innovation%0Aof%20our%20approach%20is%20the%20unfolding%20of%20a%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithm%0A-%20specifically%2C%20the%20recently%20proposed%20LATINO%20Langevin%20sampler%20%28Spagnoletti%20et%0Aal.%2C%202025%29%20-%20representing%20the%20first%20known%20instance%20of%20deep%20unfolding%20applied%20to%0Aa%20Monte%20Carlo%20sampling%20scheme.%20We%20demonstrate%20our%20proposed%20unfolded%20and%0Adistilled%20samplers%20through%20extensive%20experiments%20and%20comparisons%20with%20the%20state%0Aof%20the%20art%2C%20where%20they%20achieve%20excellent%20accuracy%20and%20computational%20efficiency%2C%0Awhile%20retaining%20the%20flexibility%20to%20adapt%20to%20variations%20in%20the%20forward%20model%20at%0Ainference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520few-step%2520posterior%2520samplers%2520by%2520unfolding%2520and%2520distillation%2520of%250A%2520%2520diffusion%2520models%26entry.906535625%3DCharlesquin%2520Kemajou%2520Mbakam%2520and%2520Jonathan%2520Spence%2520and%2520Marcelo%2520Pereyra%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520emerged%2520as%2520powerful%2520image%2520priors%2520in%2520Bayesian%250Acomputational%2520imaging.%2520Two%2520primary%2520strategies%2520have%2520been%2520proposed%2520for%2520leveraging%250ADMs%2520in%2520this%2520context%253A%2520Plug-and-Play%2520methods%252C%2520which%2520are%2520zero-shot%2520and%2520highly%250Aflexible%2520but%2520rely%2520on%2520approximations%253B%2520and%2520specialized%2520conditional%2520DMs%252C%2520which%250Aachieve%2520higher%2520accuracy%2520and%2520faster%2520inference%2520for%2520specific%2520tasks%2520through%250Asupervised%2520training.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%250Aintegrates%2520deep%2520unfolding%2520and%2520model%2520distillation%2520to%2520transform%2520a%2520DM%2520image%2520prior%250Ainto%2520a%2520few-step%2520conditional%2520model%2520for%2520posterior%2520sampling.%2520A%2520central%2520innovation%250Aof%2520our%2520approach%2520is%2520the%2520unfolding%2520of%2520a%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520algorithm%250A-%2520specifically%252C%2520the%2520recently%2520proposed%2520LATINO%2520Langevin%2520sampler%2520%2528Spagnoletti%2520et%250Aal.%252C%25202025%2529%2520-%2520representing%2520the%2520first%2520known%2520instance%2520of%2520deep%2520unfolding%2520applied%2520to%250Aa%2520Monte%2520Carlo%2520sampling%2520scheme.%2520We%2520demonstrate%2520our%2520proposed%2520unfolded%2520and%250Adistilled%2520samplers%2520through%2520extensive%2520experiments%2520and%2520comparisons%2520with%2520the%2520state%250Aof%2520the%2520art%252C%2520where%2520they%2520achieve%2520excellent%2520accuracy%2520and%2520computational%2520efficiency%252C%250Awhile%2520retaining%2520the%2520flexibility%2520to%2520adapt%2520to%2520variations%2520in%2520the%2520forward%2520model%2520at%250Ainference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models&entry.906535625=Charlesquin%20Kemajou%20Mbakam%20and%20Jonathan%20Spence%20and%20Marcelo%20Pereyra&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20emerged%20as%20powerful%20image%20priors%20in%20Bayesian%0Acomputational%20imaging.%20Two%20primary%20strategies%20have%20been%20proposed%20for%20leveraging%0ADMs%20in%20this%20context%3A%20Plug-and-Play%20methods%2C%20which%20are%20zero-shot%20and%20highly%0Aflexible%20but%20rely%20on%20approximations%3B%20and%20specialized%20conditional%20DMs%2C%20which%0Aachieve%20higher%20accuracy%20and%20faster%20inference%20for%20specific%20tasks%20through%0Asupervised%20training.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20that%0Aintegrates%20deep%20unfolding%20and%20model%20distillation%20to%20transform%20a%20DM%20image%20prior%0Ainto%20a%20few-step%20conditional%20model%20for%20posterior%20sampling.%20A%20central%20innovation%0Aof%20our%20approach%20is%20the%20unfolding%20of%20a%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithm%0A-%20specifically%2C%20the%20recently%20proposed%20LATINO%20Langevin%20sampler%20%28Spagnoletti%20et%0Aal.%2C%202025%29%20-%20representing%20the%20first%20known%20instance%20of%20deep%20unfolding%20applied%20to%0Aa%20Monte%20Carlo%20sampling%20scheme.%20We%20demonstrate%20our%20proposed%20unfolded%20and%0Adistilled%20samplers%20through%20extensive%20experiments%20and%20comparisons%20with%20the%20state%0Aof%20the%20art%2C%20where%20they%20achieve%20excellent%20accuracy%20and%20computational%20efficiency%2C%0Awhile%20retaining%20the%20flexibility%20to%20adapt%20to%20variations%20in%20the%20forward%20model%20at%0Ainference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02686v1&entry.124074799=Read"},
{"title": "Down with the Hierarchy: The 'H' in HNSW Stands for \"Hubs\"", "author": "Blaise Munyampirwa and Vihan Lakshman and Benjamin Coleman", "abstract": "  Driven by recent breakthrough advances in neural representation learning,\napproximate near-neighbor (ANN) search over vector embeddings has emerged as a\ncritical computational workload. With the introduction of the seminal\nHierarchical Navigable Small World (HNSW) algorithm, graph-based indexes have\nestablished themselves as the overwhelmingly dominant paradigm for efficient\nand scalable ANN search. As the name suggests, HNSW searches a layered\nhierarchical graph to quickly identify neighborhoods of similar points to a\ngiven query vector. But is this hierarchy even necessary? A rigorous\nexperimental analysis to answer this question would provide valuable insights\ninto the nature of algorithm design for ANN search and motivate directions for\nfuture work in this increasingly crucial domain. We conduct an extensive\nbenchmarking study covering more large-scale datasets than prior investigations\nof this question. We ultimately find that a flat navigable small world graph\ngraph retains all of the benefits of HNSW on high-dimensional datasets, with\nlatency and recall performance essentially \\emph{identical} to the original\nalgorithm but with less memory overhead. Furthermore, we go a step further and\nstudy \\emph{why} the hierarchy of HNSW provides no benefit in high dimensions,\nhypothesizing that navigable small world graphs contain a well-connected,\nfrequently traversed ``highway\" of hub nodes that maintain the same purported\nfunction as the hierarchical layers. We present compelling empirical evidence\nthat the \\emph{Hub Highway Hypothesis} holds for real datasets and investigate\nthe mechanisms by which the highway forms. The implications of this hypothesis\nmay also provide future research directions in developing enhancements to\ngraph-based ANN search.\n", "link": "http://arxiv.org/abs/2412.01940v3", "date": "2025-07-03", "relevancy": 2.2914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22&body=Title%3A%20Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22%0AAuthor%3A%20Blaise%20Munyampirwa%20and%20Vihan%20Lakshman%20and%20Benjamin%20Coleman%0AAbstract%3A%20%20%20Driven%20by%20recent%20breakthrough%20advances%20in%20neural%20representation%20learning%2C%0Aapproximate%20near-neighbor%20%28ANN%29%20search%20over%20vector%20embeddings%20has%20emerged%20as%20a%0Acritical%20computational%20workload.%20With%20the%20introduction%20of%20the%20seminal%0AHierarchical%20Navigable%20Small%20World%20%28HNSW%29%20algorithm%2C%20graph-based%20indexes%20have%0Aestablished%20themselves%20as%20the%20overwhelmingly%20dominant%20paradigm%20for%20efficient%0Aand%20scalable%20ANN%20search.%20As%20the%20name%20suggests%2C%20HNSW%20searches%20a%20layered%0Ahierarchical%20graph%20to%20quickly%20identify%20neighborhoods%20of%20similar%20points%20to%20a%0Agiven%20query%20vector.%20But%20is%20this%20hierarchy%20even%20necessary%3F%20A%20rigorous%0Aexperimental%20analysis%20to%20answer%20this%20question%20would%20provide%20valuable%20insights%0Ainto%20the%20nature%20of%20algorithm%20design%20for%20ANN%20search%20and%20motivate%20directions%20for%0Afuture%20work%20in%20this%20increasingly%20crucial%20domain.%20We%20conduct%20an%20extensive%0Abenchmarking%20study%20covering%20more%20large-scale%20datasets%20than%20prior%20investigations%0Aof%20this%20question.%20We%20ultimately%20find%20that%20a%20flat%20navigable%20small%20world%20graph%0Agraph%20retains%20all%20of%20the%20benefits%20of%20HNSW%20on%20high-dimensional%20datasets%2C%20with%0Alatency%20and%20recall%20performance%20essentially%20%5Cemph%7Bidentical%7D%20to%20the%20original%0Aalgorithm%20but%20with%20less%20memory%20overhead.%20Furthermore%2C%20we%20go%20a%20step%20further%20and%0Astudy%20%5Cemph%7Bwhy%7D%20the%20hierarchy%20of%20HNSW%20provides%20no%20benefit%20in%20high%20dimensions%2C%0Ahypothesizing%20that%20navigable%20small%20world%20graphs%20contain%20a%20well-connected%2C%0Afrequently%20traversed%20%60%60highway%22%20of%20hub%20nodes%20that%20maintain%20the%20same%20purported%0Afunction%20as%20the%20hierarchical%20layers.%20We%20present%20compelling%20empirical%20evidence%0Athat%20the%20%5Cemph%7BHub%20Highway%20Hypothesis%7D%20holds%20for%20real%20datasets%20and%20investigate%0Athe%20mechanisms%20by%20which%20the%20highway%20forms.%20The%20implications%20of%20this%20hypothesis%0Amay%20also%20provide%20future%20research%20directions%20in%20developing%20enhancements%20to%0Agraph-based%20ANN%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDown%2520with%2520the%2520Hierarchy%253A%2520The%2520%2527H%2527%2520in%2520HNSW%2520Stands%2520for%2520%2522Hubs%2522%26entry.906535625%3DBlaise%2520Munyampirwa%2520and%2520Vihan%2520Lakshman%2520and%2520Benjamin%2520Coleman%26entry.1292438233%3D%2520%2520Driven%2520by%2520recent%2520breakthrough%2520advances%2520in%2520neural%2520representation%2520learning%252C%250Aapproximate%2520near-neighbor%2520%2528ANN%2529%2520search%2520over%2520vector%2520embeddings%2520has%2520emerged%2520as%2520a%250Acritical%2520computational%2520workload.%2520With%2520the%2520introduction%2520of%2520the%2520seminal%250AHierarchical%2520Navigable%2520Small%2520World%2520%2528HNSW%2529%2520algorithm%252C%2520graph-based%2520indexes%2520have%250Aestablished%2520themselves%2520as%2520the%2520overwhelmingly%2520dominant%2520paradigm%2520for%2520efficient%250Aand%2520scalable%2520ANN%2520search.%2520As%2520the%2520name%2520suggests%252C%2520HNSW%2520searches%2520a%2520layered%250Ahierarchical%2520graph%2520to%2520quickly%2520identify%2520neighborhoods%2520of%2520similar%2520points%2520to%2520a%250Agiven%2520query%2520vector.%2520But%2520is%2520this%2520hierarchy%2520even%2520necessary%253F%2520A%2520rigorous%250Aexperimental%2520analysis%2520to%2520answer%2520this%2520question%2520would%2520provide%2520valuable%2520insights%250Ainto%2520the%2520nature%2520of%2520algorithm%2520design%2520for%2520ANN%2520search%2520and%2520motivate%2520directions%2520for%250Afuture%2520work%2520in%2520this%2520increasingly%2520crucial%2520domain.%2520We%2520conduct%2520an%2520extensive%250Abenchmarking%2520study%2520covering%2520more%2520large-scale%2520datasets%2520than%2520prior%2520investigations%250Aof%2520this%2520question.%2520We%2520ultimately%2520find%2520that%2520a%2520flat%2520navigable%2520small%2520world%2520graph%250Agraph%2520retains%2520all%2520of%2520the%2520benefits%2520of%2520HNSW%2520on%2520high-dimensional%2520datasets%252C%2520with%250Alatency%2520and%2520recall%2520performance%2520essentially%2520%255Cemph%257Bidentical%257D%2520to%2520the%2520original%250Aalgorithm%2520but%2520with%2520less%2520memory%2520overhead.%2520Furthermore%252C%2520we%2520go%2520a%2520step%2520further%2520and%250Astudy%2520%255Cemph%257Bwhy%257D%2520the%2520hierarchy%2520of%2520HNSW%2520provides%2520no%2520benefit%2520in%2520high%2520dimensions%252C%250Ahypothesizing%2520that%2520navigable%2520small%2520world%2520graphs%2520contain%2520a%2520well-connected%252C%250Afrequently%2520traversed%2520%2560%2560highway%2522%2520of%2520hub%2520nodes%2520that%2520maintain%2520the%2520same%2520purported%250Afunction%2520as%2520the%2520hierarchical%2520layers.%2520We%2520present%2520compelling%2520empirical%2520evidence%250Athat%2520the%2520%255Cemph%257BHub%2520Highway%2520Hypothesis%257D%2520holds%2520for%2520real%2520datasets%2520and%2520investigate%250Athe%2520mechanisms%2520by%2520which%2520the%2520highway%2520forms.%2520The%2520implications%2520of%2520this%2520hypothesis%250Amay%2520also%2520provide%2520future%2520research%2520directions%2520in%2520developing%2520enhancements%2520to%250Agraph-based%2520ANN%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22&entry.906535625=Blaise%20Munyampirwa%20and%20Vihan%20Lakshman%20and%20Benjamin%20Coleman&entry.1292438233=%20%20Driven%20by%20recent%20breakthrough%20advances%20in%20neural%20representation%20learning%2C%0Aapproximate%20near-neighbor%20%28ANN%29%20search%20over%20vector%20embeddings%20has%20emerged%20as%20a%0Acritical%20computational%20workload.%20With%20the%20introduction%20of%20the%20seminal%0AHierarchical%20Navigable%20Small%20World%20%28HNSW%29%20algorithm%2C%20graph-based%20indexes%20have%0Aestablished%20themselves%20as%20the%20overwhelmingly%20dominant%20paradigm%20for%20efficient%0Aand%20scalable%20ANN%20search.%20As%20the%20name%20suggests%2C%20HNSW%20searches%20a%20layered%0Ahierarchical%20graph%20to%20quickly%20identify%20neighborhoods%20of%20similar%20points%20to%20a%0Agiven%20query%20vector.%20But%20is%20this%20hierarchy%20even%20necessary%3F%20A%20rigorous%0Aexperimental%20analysis%20to%20answer%20this%20question%20would%20provide%20valuable%20insights%0Ainto%20the%20nature%20of%20algorithm%20design%20for%20ANN%20search%20and%20motivate%20directions%20for%0Afuture%20work%20in%20this%20increasingly%20crucial%20domain.%20We%20conduct%20an%20extensive%0Abenchmarking%20study%20covering%20more%20large-scale%20datasets%20than%20prior%20investigations%0Aof%20this%20question.%20We%20ultimately%20find%20that%20a%20flat%20navigable%20small%20world%20graph%0Agraph%20retains%20all%20of%20the%20benefits%20of%20HNSW%20on%20high-dimensional%20datasets%2C%20with%0Alatency%20and%20recall%20performance%20essentially%20%5Cemph%7Bidentical%7D%20to%20the%20original%0Aalgorithm%20but%20with%20less%20memory%20overhead.%20Furthermore%2C%20we%20go%20a%20step%20further%20and%0Astudy%20%5Cemph%7Bwhy%7D%20the%20hierarchy%20of%20HNSW%20provides%20no%20benefit%20in%20high%20dimensions%2C%0Ahypothesizing%20that%20navigable%20small%20world%20graphs%20contain%20a%20well-connected%2C%0Afrequently%20traversed%20%60%60highway%22%20of%20hub%20nodes%20that%20maintain%20the%20same%20purported%0Afunction%20as%20the%20hierarchical%20layers.%20We%20present%20compelling%20empirical%20evidence%0Athat%20the%20%5Cemph%7BHub%20Highway%20Hypothesis%7D%20holds%20for%20real%20datasets%20and%20investigate%0Athe%20mechanisms%20by%20which%20the%20highway%20forms.%20The%20implications%20of%20this%20hypothesis%0Amay%20also%20provide%20future%20research%20directions%20in%20developing%20enhancements%20to%0Agraph-based%20ANN%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01940v3&entry.124074799=Read"},
{"title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks", "author": "Kim-Celine Kahl and Selen Erkan and Jeremias Traub and Carsten T. L\u00fcth and Klaus Maier-Hein and Lena Maier-Hein and Paul F. Jaeger", "abstract": "  Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.\n", "link": "http://arxiv.org/abs/2411.19688v3", "date": "2025-07-03", "relevancy": 2.2899, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&body=Title%3A%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks%0AAuthor%3A%20Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20key%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%0Arequirements%20to%20overcome%20current%20pitfalls%20and%20systematically%20analyze%20VLM%0Arobustness%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%0Atranslate%20to%20real-world%20shifts%2C%20it%20should%20be%20measured%20on%20real-world%20shifts%20that%0Aare%20inherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%0Ato%20capture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20Fine-Tuning%20%28FT%29%20methods%20across%20three%20medical%20datasets%0Awith%20four%20types%20of%20distribution%20shifts.%20Our%20study%20highlights%20key%20insights%20into%0Arobustness%3A%201%29%20No%20FT%20method%20consistently%20outperforms%20others%20in%20robustness%2C%20and%0A2%29%20robustness%20trends%20are%20more%20stable%20across%20FT%20methods%20than%20across%20distribution%0Ashifts.%20Additionally%2C%20we%20find%20that%20simple%20sanity%20baselines%20that%20do%20not%20use%20the%0Aimage%20data%20can%20perform%20surprisingly%20well%20and%20confirm%20LoRA%20as%20the%0Abest-performing%20FT%20method%20on%20in-distribution%20data.%20Code%20is%20provided%20at%0Ahttps%3A//github.com/IML-DKFZ/sure-vqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19688v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSURE-VQA%253A%2520Systematic%2520Understanding%2520of%2520Robustness%2520Evaluation%2520in%2520Medical%250A%2520%2520VQA%2520Tasks%26entry.906535625%3DKim-Celine%2520Kahl%2520and%2520Selen%2520Erkan%2520and%2520Jeremias%2520Traub%2520and%2520Carsten%2520T.%2520L%25C3%25BCth%2520and%2520Klaus%2520Maier-Hein%2520and%2520Lena%2520Maier-Hein%2520and%2520Paul%2520F.%2520Jaeger%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520great%2520potential%2520in%2520medical%2520tasks%252C%2520like%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520where%2520they%2520could%2520act%2520as%2520interactive%2520assistants%250Afor%2520both%2520patients%2520and%2520clinicians.%2520Yet%2520their%2520robustness%2520to%2520distribution%2520shifts%250Aon%2520unseen%2520data%2520remains%2520a%2520key%2520concern%2520for%2520safe%2520deployment.%2520Evaluating%2520such%250Arobustness%2520requires%2520a%2520controlled%2520experimental%2520setup%2520that%2520allows%2520for%2520systematic%250Ainsights%2520into%2520the%2520model%2527s%2520behavior.%2520However%252C%2520we%2520demonstrate%2520that%2520current%2520setups%250Afail%2520to%2520offer%2520sufficiently%2520thorough%2520evaluations.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%252C%2520called%2520SURE-VQA%252C%2520centered%2520around%2520three%2520key%250Arequirements%2520to%2520overcome%2520current%2520pitfalls%2520and%2520systematically%2520analyze%2520VLM%250Arobustness%253A%25201%2529%2520Since%2520robustness%2520on%2520synthetic%2520shifts%2520does%2520not%2520necessarily%250Atranslate%2520to%2520real-world%2520shifts%252C%2520it%2520should%2520be%2520measured%2520on%2520real-world%2520shifts%2520that%250Aare%2520inherent%2520to%2520the%2520VQA%2520data%253B%25202%2529%2520Traditional%2520token-matching%2520metrics%2520often%2520fail%250Ato%2520capture%2520underlying%2520semantics%252C%2520necessitating%2520the%2520use%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520for%2520more%2520accurate%2520semantic%2520evaluation%253B%25203%2529%2520Model%2520performance%2520often%2520lacks%250Ainterpretability%2520due%2520to%2520missing%2520sanity%2520baselines%252C%2520thus%2520meaningful%2520baselines%250Ashould%2520be%2520reported%2520that%2520allow%2520assessing%2520the%2520multimodal%2520impact%2520on%2520the%2520VLM.%2520To%250Ademonstrate%2520the%2520relevance%2520of%2520this%2520framework%252C%2520we%2520conduct%2520a%2520study%2520on%2520the%250Arobustness%2520of%2520various%2520Fine-Tuning%2520%2528FT%2529%2520methods%2520across%2520three%2520medical%2520datasets%250Awith%2520four%2520types%2520of%2520distribution%2520shifts.%2520Our%2520study%2520highlights%2520key%2520insights%2520into%250Arobustness%253A%25201%2529%2520No%2520FT%2520method%2520consistently%2520outperforms%2520others%2520in%2520robustness%252C%2520and%250A2%2529%2520robustness%2520trends%2520are%2520more%2520stable%2520across%2520FT%2520methods%2520than%2520across%2520distribution%250Ashifts.%2520Additionally%252C%2520we%2520find%2520that%2520simple%2520sanity%2520baselines%2520that%2520do%2520not%2520use%2520the%250Aimage%2520data%2520can%2520perform%2520surprisingly%2520well%2520and%2520confirm%2520LoRA%2520as%2520the%250Abest-performing%2520FT%2520method%2520on%2520in-distribution%2520data.%2520Code%2520is%2520provided%2520at%250Ahttps%253A//github.com/IML-DKFZ/sure-vqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19688v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&entry.906535625=Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20key%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%0Arequirements%20to%20overcome%20current%20pitfalls%20and%20systematically%20analyze%20VLM%0Arobustness%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%0Atranslate%20to%20real-world%20shifts%2C%20it%20should%20be%20measured%20on%20real-world%20shifts%20that%0Aare%20inherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%0Ato%20capture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20Fine-Tuning%20%28FT%29%20methods%20across%20three%20medical%20datasets%0Awith%20four%20types%20of%20distribution%20shifts.%20Our%20study%20highlights%20key%20insights%20into%0Arobustness%3A%201%29%20No%20FT%20method%20consistently%20outperforms%20others%20in%20robustness%2C%20and%0A2%29%20robustness%20trends%20are%20more%20stable%20across%20FT%20methods%20than%20across%20distribution%0Ashifts.%20Additionally%2C%20we%20find%20that%20simple%20sanity%20baselines%20that%20do%20not%20use%20the%0Aimage%20data%20can%20perform%20surprisingly%20well%20and%20confirm%20LoRA%20as%20the%0Abest-performing%20FT%20method%20on%20in-distribution%20data.%20Code%20is%20provided%20at%0Ahttps%3A//github.com/IML-DKFZ/sure-vqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19688v3&entry.124074799=Read"},
{"title": "Orientation-Aware Sparse Tensor PCA for Efficient Unsupervised Feature\n  Selection", "author": "Junjing Zheng and Xinyu Zhang and Weidong Jiang and Xiangfeng Qiu and Mingjian Ren", "abstract": "  Recently, introducing Tensor Decomposition (TD) techniques into unsupervised\nfeature selection (UFS) has been an emerging research topic. A tensor structure\nis beneficial for mining the relations between different modes and helps\nrelieve the computation burden. However, while existing methods exploit TD to\npreserve the data tensor structure, they do not consider the influence of data\norientation and thus have difficulty in handling orientation-specific data such\nas time series. To solve the above problem, we utilize the\norientation-dependent tensor-tensor product from Tensor Singular Value\nDecomposition based on *M-product (T-SVDM) and extend the one-dimensional\nSparse Principal Component Analysis (SPCA) to a tensor form. The proposed\nsparse tensor PCA model can constrain sparsity at the specified mode and yield\nsparse tensor principal components, enhancing flexibility and accuracy in\nlearning feature relations. To ensure fast convergence and a flexible\ndescription of feature correlation, we develop a convex version specially\ndesigned for general UFS tasks and propose an efficient slice-by-slice\nalgorithm that performs dual optimization in the transform domain. Experimental\nresults on real-world datasets demonstrate the effectiveness and remarkable\ncomputational efficiency of the proposed method for tensor data of diverse\nstructures over the state-of-the-art. When transform axes align with feature\ndistribution patterns, our method is promising for various applications. The\ncodes related to our proposed methods and the experiments are available at\nhttps://github.com/zjj20212035/STPCA.git.\n", "link": "http://arxiv.org/abs/2407.16985v3", "date": "2025-07-03", "relevancy": 2.2879, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4638}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4604}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection&body=Title%3A%20Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection%0AAuthor%3A%20Junjing%20Zheng%20and%20Xinyu%20Zhang%20and%20Weidong%20Jiang%20and%20Xiangfeng%20Qiu%20and%20Mingjian%20Ren%0AAbstract%3A%20%20%20Recently%2C%20introducing%20Tensor%20Decomposition%20%28TD%29%20techniques%20into%20unsupervised%0Afeature%20selection%20%28UFS%29%20has%20been%20an%20emerging%20research%20topic.%20A%20tensor%20structure%0Ais%20beneficial%20for%20mining%20the%20relations%20between%20different%20modes%20and%20helps%0Arelieve%20the%20computation%20burden.%20However%2C%20while%20existing%20methods%20exploit%20TD%20to%0Apreserve%20the%20data%20tensor%20structure%2C%20they%20do%20not%20consider%20the%20influence%20of%20data%0Aorientation%20and%20thus%20have%20difficulty%20in%20handling%20orientation-specific%20data%20such%0Aas%20time%20series.%20To%20solve%20the%20above%20problem%2C%20we%20utilize%20the%0Aorientation-dependent%20tensor-tensor%20product%20from%20Tensor%20Singular%20Value%0ADecomposition%20based%20on%20%2AM-product%20%28T-SVDM%29%20and%20extend%20the%20one-dimensional%0ASparse%20Principal%20Component%20Analysis%20%28SPCA%29%20to%20a%20tensor%20form.%20The%20proposed%0Asparse%20tensor%20PCA%20model%20can%20constrain%20sparsity%20at%20the%20specified%20mode%20and%20yield%0Asparse%20tensor%20principal%20components%2C%20enhancing%20flexibility%20and%20accuracy%20in%0Alearning%20feature%20relations.%20To%20ensure%20fast%20convergence%20and%20a%20flexible%0Adescription%20of%20feature%20correlation%2C%20we%20develop%20a%20convex%20version%20specially%0Adesigned%20for%20general%20UFS%20tasks%20and%20propose%20an%20efficient%20slice-by-slice%0Aalgorithm%20that%20performs%20dual%20optimization%20in%20the%20transform%20domain.%20Experimental%0Aresults%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20and%20remarkable%0Acomputational%20efficiency%20of%20the%20proposed%20method%20for%20tensor%20data%20of%20diverse%0Astructures%20over%20the%20state-of-the-art.%20When%20transform%20axes%20align%20with%20feature%0Adistribution%20patterns%2C%20our%20method%20is%20promising%20for%20various%20applications.%20The%0Acodes%20related%20to%20our%20proposed%20methods%20and%20the%20experiments%20are%20available%20at%0Ahttps%3A//github.com/zjj20212035/STPCA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16985v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrientation-Aware%2520Sparse%2520Tensor%2520PCA%2520for%2520Efficient%2520Unsupervised%2520Feature%250A%2520%2520Selection%26entry.906535625%3DJunjing%2520Zheng%2520and%2520Xinyu%2520Zhang%2520and%2520Weidong%2520Jiang%2520and%2520Xiangfeng%2520Qiu%2520and%2520Mingjian%2520Ren%26entry.1292438233%3D%2520%2520Recently%252C%2520introducing%2520Tensor%2520Decomposition%2520%2528TD%2529%2520techniques%2520into%2520unsupervised%250Afeature%2520selection%2520%2528UFS%2529%2520has%2520been%2520an%2520emerging%2520research%2520topic.%2520A%2520tensor%2520structure%250Ais%2520beneficial%2520for%2520mining%2520the%2520relations%2520between%2520different%2520modes%2520and%2520helps%250Arelieve%2520the%2520computation%2520burden.%2520However%252C%2520while%2520existing%2520methods%2520exploit%2520TD%2520to%250Apreserve%2520the%2520data%2520tensor%2520structure%252C%2520they%2520do%2520not%2520consider%2520the%2520influence%2520of%2520data%250Aorientation%2520and%2520thus%2520have%2520difficulty%2520in%2520handling%2520orientation-specific%2520data%2520such%250Aas%2520time%2520series.%2520To%2520solve%2520the%2520above%2520problem%252C%2520we%2520utilize%2520the%250Aorientation-dependent%2520tensor-tensor%2520product%2520from%2520Tensor%2520Singular%2520Value%250ADecomposition%2520based%2520on%2520%252AM-product%2520%2528T-SVDM%2529%2520and%2520extend%2520the%2520one-dimensional%250ASparse%2520Principal%2520Component%2520Analysis%2520%2528SPCA%2529%2520to%2520a%2520tensor%2520form.%2520The%2520proposed%250Asparse%2520tensor%2520PCA%2520model%2520can%2520constrain%2520sparsity%2520at%2520the%2520specified%2520mode%2520and%2520yield%250Asparse%2520tensor%2520principal%2520components%252C%2520enhancing%2520flexibility%2520and%2520accuracy%2520in%250Alearning%2520feature%2520relations.%2520To%2520ensure%2520fast%2520convergence%2520and%2520a%2520flexible%250Adescription%2520of%2520feature%2520correlation%252C%2520we%2520develop%2520a%2520convex%2520version%2520specially%250Adesigned%2520for%2520general%2520UFS%2520tasks%2520and%2520propose%2520an%2520efficient%2520slice-by-slice%250Aalgorithm%2520that%2520performs%2520dual%2520optimization%2520in%2520the%2520transform%2520domain.%2520Experimental%250Aresults%2520on%2520real-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520remarkable%250Acomputational%2520efficiency%2520of%2520the%2520proposed%2520method%2520for%2520tensor%2520data%2520of%2520diverse%250Astructures%2520over%2520the%2520state-of-the-art.%2520When%2520transform%2520axes%2520align%2520with%2520feature%250Adistribution%2520patterns%252C%2520our%2520method%2520is%2520promising%2520for%2520various%2520applications.%2520The%250Acodes%2520related%2520to%2520our%2520proposed%2520methods%2520and%2520the%2520experiments%2520are%2520available%2520at%250Ahttps%253A//github.com/zjj20212035/STPCA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16985v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection&entry.906535625=Junjing%20Zheng%20and%20Xinyu%20Zhang%20and%20Weidong%20Jiang%20and%20Xiangfeng%20Qiu%20and%20Mingjian%20Ren&entry.1292438233=%20%20Recently%2C%20introducing%20Tensor%20Decomposition%20%28TD%29%20techniques%20into%20unsupervised%0Afeature%20selection%20%28UFS%29%20has%20been%20an%20emerging%20research%20topic.%20A%20tensor%20structure%0Ais%20beneficial%20for%20mining%20the%20relations%20between%20different%20modes%20and%20helps%0Arelieve%20the%20computation%20burden.%20However%2C%20while%20existing%20methods%20exploit%20TD%20to%0Apreserve%20the%20data%20tensor%20structure%2C%20they%20do%20not%20consider%20the%20influence%20of%20data%0Aorientation%20and%20thus%20have%20difficulty%20in%20handling%20orientation-specific%20data%20such%0Aas%20time%20series.%20To%20solve%20the%20above%20problem%2C%20we%20utilize%20the%0Aorientation-dependent%20tensor-tensor%20product%20from%20Tensor%20Singular%20Value%0ADecomposition%20based%20on%20%2AM-product%20%28T-SVDM%29%20and%20extend%20the%20one-dimensional%0ASparse%20Principal%20Component%20Analysis%20%28SPCA%29%20to%20a%20tensor%20form.%20The%20proposed%0Asparse%20tensor%20PCA%20model%20can%20constrain%20sparsity%20at%20the%20specified%20mode%20and%20yield%0Asparse%20tensor%20principal%20components%2C%20enhancing%20flexibility%20and%20accuracy%20in%0Alearning%20feature%20relations.%20To%20ensure%20fast%20convergence%20and%20a%20flexible%0Adescription%20of%20feature%20correlation%2C%20we%20develop%20a%20convex%20version%20specially%0Adesigned%20for%20general%20UFS%20tasks%20and%20propose%20an%20efficient%20slice-by-slice%0Aalgorithm%20that%20performs%20dual%20optimization%20in%20the%20transform%20domain.%20Experimental%0Aresults%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20and%20remarkable%0Acomputational%20efficiency%20of%20the%20proposed%20method%20for%20tensor%20data%20of%20diverse%0Astructures%20over%20the%20state-of-the-art.%20When%20transform%20axes%20align%20with%20feature%0Adistribution%20patterns%2C%20our%20method%20is%20promising%20for%20various%20applications.%20The%0Acodes%20related%20to%20our%20proposed%20methods%20and%20the%20experiments%20are%20available%20at%0Ahttps%3A//github.com/zjj20212035/STPCA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16985v3&entry.124074799=Read"},
{"title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via\n  Canonical Space Modulation", "author": "Xiangyang Luo and Ye Zhu and Yunfei Liu and Lijian Lin and Cong Wan and Zijian Cai and Shao-Lun Huang and Yu Li", "abstract": "  Video face swapping aims to address two primary challenges: effectively\ntransferring the source identity to the target video and accurately preserving\nthe dynamic attributes of the target face, such as head poses, facial\nexpressions, lip-sync, \\etc. Existing methods mainly focus on achieving\nhigh-quality identity transfer but often fall short in maintaining the dynamic\nattributes of the target face, leading to inconsistent results. We attribute\nthis issue to the inherent coupling of facial appearance and motion in videos.\nTo address this, we propose CanonSwap, a novel video face-swapping framework\nthat decouples motion information from appearance information. Specifically,\nCanonSwap first eliminates motion-related information, enabling identity\nmodification within a unified canonical space. Subsequently, the swapped\nfeature is reintegrated into the original video space, ensuring the\npreservation of the target face's dynamic attributes. To further achieve\nprecise identity transfer with minimal artifacts and enhanced realism, we\ndesign a Partial Identity Modulation module that adaptively integrates source\nidentity features using a spatial mask to restrict modifications to facial\nregions. Additionally, we introduce several fine-grained synchronization\nmetrics to comprehensively evaluate the performance of video face swapping\nmethods. Extensive experiments demonstrate that our method significantly\noutperforms existing approaches in terms of visual quality, temporal\nconsistency, and identity preservation. Our project page are publicly available\nat https://luoxyhappy.github.io/CanonSwap/.\n", "link": "http://arxiv.org/abs/2507.02691v1", "date": "2025-07-03", "relevancy": 2.2594, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5898}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5742}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation&body=Title%3A%20CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation%0AAuthor%3A%20Xiangyang%20Luo%20and%20Ye%20Zhu%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Cong%20Wan%20and%20Zijian%20Cai%20and%20Shao-Lun%20Huang%20and%20Yu%20Li%0AAbstract%3A%20%20%20Video%20face%20swapping%20aims%20to%20address%20two%20primary%20challenges%3A%20effectively%0Atransferring%20the%20source%20identity%20to%20the%20target%20video%20and%20accurately%20preserving%0Athe%20dynamic%20attributes%20of%20the%20target%20face%2C%20such%20as%20head%20poses%2C%20facial%0Aexpressions%2C%20lip-sync%2C%20%5Cetc.%20Existing%20methods%20mainly%20focus%20on%20achieving%0Ahigh-quality%20identity%20transfer%20but%20often%20fall%20short%20in%20maintaining%20the%20dynamic%0Aattributes%20of%20the%20target%20face%2C%20leading%20to%20inconsistent%20results.%20We%20attribute%0Athis%20issue%20to%20the%20inherent%20coupling%20of%20facial%20appearance%20and%20motion%20in%20videos.%0ATo%20address%20this%2C%20we%20propose%20CanonSwap%2C%20a%20novel%20video%20face-swapping%20framework%0Athat%20decouples%20motion%20information%20from%20appearance%20information.%20Specifically%2C%0ACanonSwap%20first%20eliminates%20motion-related%20information%2C%20enabling%20identity%0Amodification%20within%20a%20unified%20canonical%20space.%20Subsequently%2C%20the%20swapped%0Afeature%20is%20reintegrated%20into%20the%20original%20video%20space%2C%20ensuring%20the%0Apreservation%20of%20the%20target%20face%27s%20dynamic%20attributes.%20To%20further%20achieve%0Aprecise%20identity%20transfer%20with%20minimal%20artifacts%20and%20enhanced%20realism%2C%20we%0Adesign%20a%20Partial%20Identity%20Modulation%20module%20that%20adaptively%20integrates%20source%0Aidentity%20features%20using%20a%20spatial%20mask%20to%20restrict%20modifications%20to%20facial%0Aregions.%20Additionally%2C%20we%20introduce%20several%20fine-grained%20synchronization%0Ametrics%20to%20comprehensively%20evaluate%20the%20performance%20of%20video%20face%20swapping%0Amethods.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%20in%20terms%20of%20visual%20quality%2C%20temporal%0Aconsistency%2C%20and%20identity%20preservation.%20Our%20project%20page%20are%20publicly%20available%0Aat%20https%3A//luoxyhappy.github.io/CanonSwap/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonSwap%253A%2520High-Fidelity%2520and%2520Consistent%2520Video%2520Face%2520Swapping%2520via%250A%2520%2520Canonical%2520Space%2520Modulation%26entry.906535625%3DXiangyang%2520Luo%2520and%2520Ye%2520Zhu%2520and%2520Yunfei%2520Liu%2520and%2520Lijian%2520Lin%2520and%2520Cong%2520Wan%2520and%2520Zijian%2520Cai%2520and%2520Shao-Lun%2520Huang%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520Video%2520face%2520swapping%2520aims%2520to%2520address%2520two%2520primary%2520challenges%253A%2520effectively%250Atransferring%2520the%2520source%2520identity%2520to%2520the%2520target%2520video%2520and%2520accurately%2520preserving%250Athe%2520dynamic%2520attributes%2520of%2520the%2520target%2520face%252C%2520such%2520as%2520head%2520poses%252C%2520facial%250Aexpressions%252C%2520lip-sync%252C%2520%255Cetc.%2520Existing%2520methods%2520mainly%2520focus%2520on%2520achieving%250Ahigh-quality%2520identity%2520transfer%2520but%2520often%2520fall%2520short%2520in%2520maintaining%2520the%2520dynamic%250Aattributes%2520of%2520the%2520target%2520face%252C%2520leading%2520to%2520inconsistent%2520results.%2520We%2520attribute%250Athis%2520issue%2520to%2520the%2520inherent%2520coupling%2520of%2520facial%2520appearance%2520and%2520motion%2520in%2520videos.%250ATo%2520address%2520this%252C%2520we%2520propose%2520CanonSwap%252C%2520a%2520novel%2520video%2520face-swapping%2520framework%250Athat%2520decouples%2520motion%2520information%2520from%2520appearance%2520information.%2520Specifically%252C%250ACanonSwap%2520first%2520eliminates%2520motion-related%2520information%252C%2520enabling%2520identity%250Amodification%2520within%2520a%2520unified%2520canonical%2520space.%2520Subsequently%252C%2520the%2520swapped%250Afeature%2520is%2520reintegrated%2520into%2520the%2520original%2520video%2520space%252C%2520ensuring%2520the%250Apreservation%2520of%2520the%2520target%2520face%2527s%2520dynamic%2520attributes.%2520To%2520further%2520achieve%250Aprecise%2520identity%2520transfer%2520with%2520minimal%2520artifacts%2520and%2520enhanced%2520realism%252C%2520we%250Adesign%2520a%2520Partial%2520Identity%2520Modulation%2520module%2520that%2520adaptively%2520integrates%2520source%250Aidentity%2520features%2520using%2520a%2520spatial%2520mask%2520to%2520restrict%2520modifications%2520to%2520facial%250Aregions.%2520Additionally%252C%2520we%2520introduce%2520several%2520fine-grained%2520synchronization%250Ametrics%2520to%2520comprehensively%2520evaluate%2520the%2520performance%2520of%2520video%2520face%2520swapping%250Amethods.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520existing%2520approaches%2520in%2520terms%2520of%2520visual%2520quality%252C%2520temporal%250Aconsistency%252C%2520and%2520identity%2520preservation.%2520Our%2520project%2520page%2520are%2520publicly%2520available%250Aat%2520https%253A//luoxyhappy.github.io/CanonSwap/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation&entry.906535625=Xiangyang%20Luo%20and%20Ye%20Zhu%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Cong%20Wan%20and%20Zijian%20Cai%20and%20Shao-Lun%20Huang%20and%20Yu%20Li&entry.1292438233=%20%20Video%20face%20swapping%20aims%20to%20address%20two%20primary%20challenges%3A%20effectively%0Atransferring%20the%20source%20identity%20to%20the%20target%20video%20and%20accurately%20preserving%0Athe%20dynamic%20attributes%20of%20the%20target%20face%2C%20such%20as%20head%20poses%2C%20facial%0Aexpressions%2C%20lip-sync%2C%20%5Cetc.%20Existing%20methods%20mainly%20focus%20on%20achieving%0Ahigh-quality%20identity%20transfer%20but%20often%20fall%20short%20in%20maintaining%20the%20dynamic%0Aattributes%20of%20the%20target%20face%2C%20leading%20to%20inconsistent%20results.%20We%20attribute%0Athis%20issue%20to%20the%20inherent%20coupling%20of%20facial%20appearance%20and%20motion%20in%20videos.%0ATo%20address%20this%2C%20we%20propose%20CanonSwap%2C%20a%20novel%20video%20face-swapping%20framework%0Athat%20decouples%20motion%20information%20from%20appearance%20information.%20Specifically%2C%0ACanonSwap%20first%20eliminates%20motion-related%20information%2C%20enabling%20identity%0Amodification%20within%20a%20unified%20canonical%20space.%20Subsequently%2C%20the%20swapped%0Afeature%20is%20reintegrated%20into%20the%20original%20video%20space%2C%20ensuring%20the%0Apreservation%20of%20the%20target%20face%27s%20dynamic%20attributes.%20To%20further%20achieve%0Aprecise%20identity%20transfer%20with%20minimal%20artifacts%20and%20enhanced%20realism%2C%20we%0Adesign%20a%20Partial%20Identity%20Modulation%20module%20that%20adaptively%20integrates%20source%0Aidentity%20features%20using%20a%20spatial%20mask%20to%20restrict%20modifications%20to%20facial%0Aregions.%20Additionally%2C%20we%20introduce%20several%20fine-grained%20synchronization%0Ametrics%20to%20comprehensively%20evaluate%20the%20performance%20of%20video%20face%20swapping%0Amethods.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%20in%20terms%20of%20visual%20quality%2C%20temporal%0Aconsistency%2C%20and%20identity%20preservation.%20Our%20project%20page%20are%20publicly%20available%0Aat%20https%3A//luoxyhappy.github.io/CanonSwap/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02691v1&entry.124074799=Read"},
{"title": "Fast and Simplex: 2-Simplicial Attention in Triton", "author": "Aurko Roy and Timothy Chou and Sai Surya Duvvuri and Sijia Chen and Jiecao Yu and Xiaodong Wang and Manzil Zaheer and Rohan Anil", "abstract": "  Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.\n", "link": "http://arxiv.org/abs/2507.02754v1", "date": "2025-07-03", "relevancy": 2.2566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5967}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5415}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton&body=Title%3A%20Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton%0AAuthor%3A%20Aurko%20Roy%20and%20Timothy%20Chou%20and%20Sai%20Surya%20Duvvuri%20and%20Sijia%20Chen%20and%20Jiecao%20Yu%20and%20Xiaodong%20Wang%20and%20Manzil%20Zaheer%20and%20Rohan%20Anil%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20training%20loss%20scales%20as%20a%20power%20law%20with%20both%0Amodel%20size%20and%20the%20number%20of%20tokens%2C%20and%20that%20achieving%20compute-optimal%20models%0Arequires%20scaling%20model%20size%20and%20token%20count%20together.%20However%2C%20these%20scaling%0Alaws%20assume%20an%20infinite%20supply%20of%20data%20and%20apply%20primarily%20in%20compute-bound%0Asettings.%20As%20modern%20large%20language%20models%20increasingly%20rely%20on%20massive%0Ainternet-scale%20datasets%2C%20the%20assumption%20that%20they%20are%20compute-bound%20is%20becoming%0Aless%20valid.%20This%20shift%20highlights%20the%20need%20for%20architectures%20that%20prioritize%0Atoken%20efficiency.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20the%202-simplicial%20Transformer%2C%20an%0Aarchitecture%20that%20generalizes%20standard%20dot-product%20attention%20to%20trilinear%0Afunctions%20through%20an%20efficient%20Triton%20kernel%20implementation.%20We%20demonstrate%0Athat%20the%202-simplicial%20Transformer%20achieves%20better%20token%20efficiency%20than%0Astandard%20Transformers%3A%20for%20a%20fixed%20token%20budget%2C%20similarly%20sized%20models%0Aoutperform%20their%20dot-product%20counterparts%20on%20tasks%20involving%20mathematics%2C%0Acoding%2C%20reasoning%2C%20and%20logic.%20We%20quantify%20these%20gains%20by%20demonstrating%20that%0A%242%24-simplicial%20attention%20changes%20the%20exponent%20in%20the%20scaling%20laws%20for%20knowledge%0Aand%20reasoning%20tasks%20compared%20to%20dot%20product%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Simplex%253A%25202-Simplicial%2520Attention%2520in%2520Triton%26entry.906535625%3DAurko%2520Roy%2520and%2520Timothy%2520Chou%2520and%2520Sai%2520Surya%2520Duvvuri%2520and%2520Sijia%2520Chen%2520and%2520Jiecao%2520Yu%2520and%2520Xiaodong%2520Wang%2520and%2520Manzil%2520Zaheer%2520and%2520Rohan%2520Anil%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520training%2520loss%2520scales%2520as%2520a%2520power%2520law%2520with%2520both%250Amodel%2520size%2520and%2520the%2520number%2520of%2520tokens%252C%2520and%2520that%2520achieving%2520compute-optimal%2520models%250Arequires%2520scaling%2520model%2520size%2520and%2520token%2520count%2520together.%2520However%252C%2520these%2520scaling%250Alaws%2520assume%2520an%2520infinite%2520supply%2520of%2520data%2520and%2520apply%2520primarily%2520in%2520compute-bound%250Asettings.%2520As%2520modern%2520large%2520language%2520models%2520increasingly%2520rely%2520on%2520massive%250Ainternet-scale%2520datasets%252C%2520the%2520assumption%2520that%2520they%2520are%2520compute-bound%2520is%2520becoming%250Aless%2520valid.%2520This%2520shift%2520highlights%2520the%2520need%2520for%2520architectures%2520that%2520prioritize%250Atoken%2520efficiency.%250A%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520use%2520of%2520the%25202-simplicial%2520Transformer%252C%2520an%250Aarchitecture%2520that%2520generalizes%2520standard%2520dot-product%2520attention%2520to%2520trilinear%250Afunctions%2520through%2520an%2520efficient%2520Triton%2520kernel%2520implementation.%2520We%2520demonstrate%250Athat%2520the%25202-simplicial%2520Transformer%2520achieves%2520better%2520token%2520efficiency%2520than%250Astandard%2520Transformers%253A%2520for%2520a%2520fixed%2520token%2520budget%252C%2520similarly%2520sized%2520models%250Aoutperform%2520their%2520dot-product%2520counterparts%2520on%2520tasks%2520involving%2520mathematics%252C%250Acoding%252C%2520reasoning%252C%2520and%2520logic.%2520We%2520quantify%2520these%2520gains%2520by%2520demonstrating%2520that%250A%25242%2524-simplicial%2520attention%2520changes%2520the%2520exponent%2520in%2520the%2520scaling%2520laws%2520for%2520knowledge%250Aand%2520reasoning%2520tasks%2520compared%2520to%2520dot%2520product%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton&entry.906535625=Aurko%20Roy%20and%20Timothy%20Chou%20and%20Sai%20Surya%20Duvvuri%20and%20Sijia%20Chen%20and%20Jiecao%20Yu%20and%20Xiaodong%20Wang%20and%20Manzil%20Zaheer%20and%20Rohan%20Anil&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20training%20loss%20scales%20as%20a%20power%20law%20with%20both%0Amodel%20size%20and%20the%20number%20of%20tokens%2C%20and%20that%20achieving%20compute-optimal%20models%0Arequires%20scaling%20model%20size%20and%20token%20count%20together.%20However%2C%20these%20scaling%0Alaws%20assume%20an%20infinite%20supply%20of%20data%20and%20apply%20primarily%20in%20compute-bound%0Asettings.%20As%20modern%20large%20language%20models%20increasingly%20rely%20on%20massive%0Ainternet-scale%20datasets%2C%20the%20assumption%20that%20they%20are%20compute-bound%20is%20becoming%0Aless%20valid.%20This%20shift%20highlights%20the%20need%20for%20architectures%20that%20prioritize%0Atoken%20efficiency.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20the%202-simplicial%20Transformer%2C%20an%0Aarchitecture%20that%20generalizes%20standard%20dot-product%20attention%20to%20trilinear%0Afunctions%20through%20an%20efficient%20Triton%20kernel%20implementation.%20We%20demonstrate%0Athat%20the%202-simplicial%20Transformer%20achieves%20better%20token%20efficiency%20than%0Astandard%20Transformers%3A%20for%20a%20fixed%20token%20budget%2C%20similarly%20sized%20models%0Aoutperform%20their%20dot-product%20counterparts%20on%20tasks%20involving%20mathematics%2C%0Acoding%2C%20reasoning%2C%20and%20logic.%20We%20quantify%20these%20gains%20by%20demonstrating%20that%0A%242%24-simplicial%20attention%20changes%20the%20exponent%20in%20the%20scaling%20laws%20for%20knowledge%0Aand%20reasoning%20tasks%20compared%20to%20dot%20product%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02754v1&entry.124074799=Read"},
{"title": "No time to train! Training-Free Reference-Based Instance Segmentation", "author": "Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley", "abstract": "  The performance of image segmentation models has historically been\nconstrained by the high cost of collecting large-scale annotated data. The\nSegment Anything Model (SAM) alleviates this original problem through a\npromptable, semantics-agnostic, segmentation paradigm and yet still requires\nmanual visual-prompts or complex domain-dependent prompt-generation rules to\nprocess a new image. Towards reducing this new burden, our work investigates\nthe task of object segmentation when provided with, alternatively, only a small\nset of reference images. Our key insight is to leverage strong semantic priors,\nas learned by foundation models, to identify corresponding regions between a\nreference and a target image. We find that correspondences enable automatic\ngeneration of instance-level segmentation masks for downstream tasks and\ninstantiate our ideas via a multi-stage, training-free method incorporating (1)\nmemory bank construction; (2) representation aggregation and (3) semantic-aware\nfeature matching. Our experiments show significant improvements on segmentation\nmetrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),\nPASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free\napproaches on the Cross-Domain FSOD benchmark (22.4% nAP).\n", "link": "http://arxiv.org/abs/2507.02798v1", "date": "2025-07-03", "relevancy": 2.2548, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation&body=Title%3A%20No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation%0AAuthor%3A%20Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Linus%20Ericsson%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20The%20performance%20of%20image%20segmentation%20models%20has%20historically%20been%0Aconstrained%20by%20the%20high%20cost%20of%20collecting%20large-scale%20annotated%20data.%20The%0ASegment%20Anything%20Model%20%28SAM%29%20alleviates%20this%20original%20problem%20through%20a%0Apromptable%2C%20semantics-agnostic%2C%20segmentation%20paradigm%20and%20yet%20still%20requires%0Amanual%20visual-prompts%20or%20complex%20domain-dependent%20prompt-generation%20rules%20to%0Aprocess%20a%20new%20image.%20Towards%20reducing%20this%20new%20burden%2C%20our%20work%20investigates%0Athe%20task%20of%20object%20segmentation%20when%20provided%20with%2C%20alternatively%2C%20only%20a%20small%0Aset%20of%20reference%20images.%20Our%20key%20insight%20is%20to%20leverage%20strong%20semantic%20priors%2C%0Aas%20learned%20by%20foundation%20models%2C%20to%20identify%20corresponding%20regions%20between%20a%0Areference%20and%20a%20target%20image.%20We%20find%20that%20correspondences%20enable%20automatic%0Ageneration%20of%20instance-level%20segmentation%20masks%20for%20downstream%20tasks%20and%0Ainstantiate%20our%20ideas%20via%20a%20multi-stage%2C%20training-free%20method%20incorporating%20%281%29%0Amemory%20bank%20construction%3B%20%282%29%20representation%20aggregation%20and%20%283%29%20semantic-aware%0Afeature%20matching.%20Our%20experiments%20show%20significant%20improvements%20on%20segmentation%0Ametrics%2C%20leading%20to%20state-of-the-art%20performance%20on%20COCO%20FSOD%20%2836.8%25%20nAP%29%2C%0APASCAL%20VOC%20Few-Shot%20%2871.2%25%20nAP50%29%20and%20outperforming%20existing%20training-free%0Aapproaches%20on%20the%20Cross-Domain%20FSOD%20benchmark%20%2822.4%25%20nAP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520time%2520to%2520train%2521%2520Training-Free%2520Reference-Based%2520Instance%2520Segmentation%26entry.906535625%3DMiguel%2520Espinosa%2520and%2520Chenhongyi%2520Yang%2520and%2520Linus%2520Ericsson%2520and%2520Steven%2520McDonagh%2520and%2520Elliot%2520J.%2520Crowley%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520image%2520segmentation%2520models%2520has%2520historically%2520been%250Aconstrained%2520by%2520the%2520high%2520cost%2520of%2520collecting%2520large-scale%2520annotated%2520data.%2520The%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%2520alleviates%2520this%2520original%2520problem%2520through%2520a%250Apromptable%252C%2520semantics-agnostic%252C%2520segmentation%2520paradigm%2520and%2520yet%2520still%2520requires%250Amanual%2520visual-prompts%2520or%2520complex%2520domain-dependent%2520prompt-generation%2520rules%2520to%250Aprocess%2520a%2520new%2520image.%2520Towards%2520reducing%2520this%2520new%2520burden%252C%2520our%2520work%2520investigates%250Athe%2520task%2520of%2520object%2520segmentation%2520when%2520provided%2520with%252C%2520alternatively%252C%2520only%2520a%2520small%250Aset%2520of%2520reference%2520images.%2520Our%2520key%2520insight%2520is%2520to%2520leverage%2520strong%2520semantic%2520priors%252C%250Aas%2520learned%2520by%2520foundation%2520models%252C%2520to%2520identify%2520corresponding%2520regions%2520between%2520a%250Areference%2520and%2520a%2520target%2520image.%2520We%2520find%2520that%2520correspondences%2520enable%2520automatic%250Ageneration%2520of%2520instance-level%2520segmentation%2520masks%2520for%2520downstream%2520tasks%2520and%250Ainstantiate%2520our%2520ideas%2520via%2520a%2520multi-stage%252C%2520training-free%2520method%2520incorporating%2520%25281%2529%250Amemory%2520bank%2520construction%253B%2520%25282%2529%2520representation%2520aggregation%2520and%2520%25283%2529%2520semantic-aware%250Afeature%2520matching.%2520Our%2520experiments%2520show%2520significant%2520improvements%2520on%2520segmentation%250Ametrics%252C%2520leading%2520to%2520state-of-the-art%2520performance%2520on%2520COCO%2520FSOD%2520%252836.8%2525%2520nAP%2529%252C%250APASCAL%2520VOC%2520Few-Shot%2520%252871.2%2525%2520nAP50%2529%2520and%2520outperforming%2520existing%2520training-free%250Aapproaches%2520on%2520the%2520Cross-Domain%2520FSOD%2520benchmark%2520%252822.4%2525%2520nAP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation&entry.906535625=Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Linus%20Ericsson%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20The%20performance%20of%20image%20segmentation%20models%20has%20historically%20been%0Aconstrained%20by%20the%20high%20cost%20of%20collecting%20large-scale%20annotated%20data.%20The%0ASegment%20Anything%20Model%20%28SAM%29%20alleviates%20this%20original%20problem%20through%20a%0Apromptable%2C%20semantics-agnostic%2C%20segmentation%20paradigm%20and%20yet%20still%20requires%0Amanual%20visual-prompts%20or%20complex%20domain-dependent%20prompt-generation%20rules%20to%0Aprocess%20a%20new%20image.%20Towards%20reducing%20this%20new%20burden%2C%20our%20work%20investigates%0Athe%20task%20of%20object%20segmentation%20when%20provided%20with%2C%20alternatively%2C%20only%20a%20small%0Aset%20of%20reference%20images.%20Our%20key%20insight%20is%20to%20leverage%20strong%20semantic%20priors%2C%0Aas%20learned%20by%20foundation%20models%2C%20to%20identify%20corresponding%20regions%20between%20a%0Areference%20and%20a%20target%20image.%20We%20find%20that%20correspondences%20enable%20automatic%0Ageneration%20of%20instance-level%20segmentation%20masks%20for%20downstream%20tasks%20and%0Ainstantiate%20our%20ideas%20via%20a%20multi-stage%2C%20training-free%20method%20incorporating%20%281%29%0Amemory%20bank%20construction%3B%20%282%29%20representation%20aggregation%20and%20%283%29%20semantic-aware%0Afeature%20matching.%20Our%20experiments%20show%20significant%20improvements%20on%20segmentation%0Ametrics%2C%20leading%20to%20state-of-the-art%20performance%20on%20COCO%20FSOD%20%2836.8%25%20nAP%29%2C%0APASCAL%20VOC%20Few-Shot%20%2871.2%25%20nAP50%29%20and%20outperforming%20existing%20training-free%0Aapproaches%20on%20the%20Cross-Domain%20FSOD%20benchmark%20%2822.4%25%20nAP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02798v1&entry.124074799=Read"},
{"title": "HAPI: A Model for Learning Robot Facial Expressions from Human\n  Preferences", "author": "Dongsheng Yang and Qianying Liu and Wataru Sato and Takashi Minato and Chaoran Liu and Shin'ya Nishida", "abstract": "  Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.\n", "link": "http://arxiv.org/abs/2503.17046v2", "date": "2025-07-03", "relevancy": 2.2456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5765}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5598}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences&body=Title%3A%20HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences%0AAuthor%3A%20Dongsheng%20Yang%20and%20Qianying%20Liu%20and%20Wataru%20Sato%20and%20Takashi%20Minato%20and%20Chaoran%20Liu%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20%20%20Automatic%20robotic%20facial%20expression%20generation%20is%20crucial%20for%20human-robot%0Ainteraction%2C%20as%20handcrafted%20methods%20based%20on%20fixed%20joint%20configurations%20often%0Ayield%20rigid%20and%20unnatural%20behaviors.%20Although%20recent%20automated%20techniques%0Areduce%20the%20need%20for%20manual%20tuning%2C%20they%20tend%20to%20fall%20short%20by%20not%20adequately%0Abridging%20the%20gap%20between%20human%20preferences%20and%20model%20predictions-resulting%20in%20a%0Adeficiency%20of%20nuanced%20and%20realistic%20expressions%20due%20to%20limited%20degrees%20of%0Afreedom%20and%20insufficient%20perceptual%20integration.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20learning-to-rank%20framework%20that%20leverages%20human%20feedback%20to%20address%20this%0Adiscrepancy%20and%20enhanced%20the%20expressiveness%20of%20robotic%20faces.%20Specifically%2C%20we%0Aconduct%20pairwise%20comparison%20annotations%20to%20collect%20human%20preference%20data%20and%0Adevelop%20the%20Human%20Affective%20Pairwise%20Impressions%20%28HAPI%29%20model%2C%20a%20Siamese%0ARankNet-based%20approach%20that%20refines%20expression%20evaluation.%20Results%20obtained%20via%0ABayesian%20Optimization%20and%20online%20expression%20survey%20on%20a%2035-DOF%20android%20platform%0Ademonstrate%20that%20our%20approach%20produces%20significantly%20more%20realistic%20and%0Asocially%20resonant%20expressions%20of%20Anger%2C%20Happiness%2C%20and%20Surprise%20than%20those%0Agenerated%20by%20baseline%20and%20expert-designed%20methods.%20This%20confirms%20that%20our%0Aframework%20effectively%20bridges%20the%20gap%20between%20human%20preferences%20and%20model%0Apredictions%20while%20robustly%20aligning%20robotic%20expression%20generation%20with%20human%0Aaffective%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAPI%253A%2520A%2520Model%2520for%2520Learning%2520Robot%2520Facial%2520Expressions%2520from%2520Human%250A%2520%2520Preferences%26entry.906535625%3DDongsheng%2520Yang%2520and%2520Qianying%2520Liu%2520and%2520Wataru%2520Sato%2520and%2520Takashi%2520Minato%2520and%2520Chaoran%2520Liu%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3D%2520%2520Automatic%2520robotic%2520facial%2520expression%2520generation%2520is%2520crucial%2520for%2520human-robot%250Ainteraction%252C%2520as%2520handcrafted%2520methods%2520based%2520on%2520fixed%2520joint%2520configurations%2520often%250Ayield%2520rigid%2520and%2520unnatural%2520behaviors.%2520Although%2520recent%2520automated%2520techniques%250Areduce%2520the%2520need%2520for%2520manual%2520tuning%252C%2520they%2520tend%2520to%2520fall%2520short%2520by%2520not%2520adequately%250Abridging%2520the%2520gap%2520between%2520human%2520preferences%2520and%2520model%2520predictions-resulting%2520in%2520a%250Adeficiency%2520of%2520nuanced%2520and%2520realistic%2520expressions%2520due%2520to%2520limited%2520degrees%2520of%250Afreedom%2520and%2520insufficient%2520perceptual%2520integration.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520learning-to-rank%2520framework%2520that%2520leverages%2520human%2520feedback%2520to%2520address%2520this%250Adiscrepancy%2520and%2520enhanced%2520the%2520expressiveness%2520of%2520robotic%2520faces.%2520Specifically%252C%2520we%250Aconduct%2520pairwise%2520comparison%2520annotations%2520to%2520collect%2520human%2520preference%2520data%2520and%250Adevelop%2520the%2520Human%2520Affective%2520Pairwise%2520Impressions%2520%2528HAPI%2529%2520model%252C%2520a%2520Siamese%250ARankNet-based%2520approach%2520that%2520refines%2520expression%2520evaluation.%2520Results%2520obtained%2520via%250ABayesian%2520Optimization%2520and%2520online%2520expression%2520survey%2520on%2520a%252035-DOF%2520android%2520platform%250Ademonstrate%2520that%2520our%2520approach%2520produces%2520significantly%2520more%2520realistic%2520and%250Asocially%2520resonant%2520expressions%2520of%2520Anger%252C%2520Happiness%252C%2520and%2520Surprise%2520than%2520those%250Agenerated%2520by%2520baseline%2520and%2520expert-designed%2520methods.%2520This%2520confirms%2520that%2520our%250Aframework%2520effectively%2520bridges%2520the%2520gap%2520between%2520human%2520preferences%2520and%2520model%250Apredictions%2520while%2520robustly%2520aligning%2520robotic%2520expression%2520generation%2520with%2520human%250Aaffective%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences&entry.906535625=Dongsheng%20Yang%20and%20Qianying%20Liu%20and%20Wataru%20Sato%20and%20Takashi%20Minato%20and%20Chaoran%20Liu%20and%20Shin%27ya%20Nishida&entry.1292438233=%20%20Automatic%20robotic%20facial%20expression%20generation%20is%20crucial%20for%20human-robot%0Ainteraction%2C%20as%20handcrafted%20methods%20based%20on%20fixed%20joint%20configurations%20often%0Ayield%20rigid%20and%20unnatural%20behaviors.%20Although%20recent%20automated%20techniques%0Areduce%20the%20need%20for%20manual%20tuning%2C%20they%20tend%20to%20fall%20short%20by%20not%20adequately%0Abridging%20the%20gap%20between%20human%20preferences%20and%20model%20predictions-resulting%20in%20a%0Adeficiency%20of%20nuanced%20and%20realistic%20expressions%20due%20to%20limited%20degrees%20of%0Afreedom%20and%20insufficient%20perceptual%20integration.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20learning-to-rank%20framework%20that%20leverages%20human%20feedback%20to%20address%20this%0Adiscrepancy%20and%20enhanced%20the%20expressiveness%20of%20robotic%20faces.%20Specifically%2C%20we%0Aconduct%20pairwise%20comparison%20annotations%20to%20collect%20human%20preference%20data%20and%0Adevelop%20the%20Human%20Affective%20Pairwise%20Impressions%20%28HAPI%29%20model%2C%20a%20Siamese%0ARankNet-based%20approach%20that%20refines%20expression%20evaluation.%20Results%20obtained%20via%0ABayesian%20Optimization%20and%20online%20expression%20survey%20on%20a%2035-DOF%20android%20platform%0Ademonstrate%20that%20our%20approach%20produces%20significantly%20more%20realistic%20and%0Asocially%20resonant%20expressions%20of%20Anger%2C%20Happiness%2C%20and%20Surprise%20than%20those%0Agenerated%20by%20baseline%20and%20expert-designed%20methods.%20This%20confirms%20that%20our%0Aframework%20effectively%20bridges%20the%20gap%20between%20human%20preferences%20and%20model%0Apredictions%20while%20robustly%20aligning%20robotic%20expression%20generation%20with%20human%0Aaffective%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17046v2&entry.124074799=Read"},
{"title": "Real-time Image-based Lighting of Glints", "author": "Tom Kneiphof and Reinhard Klein", "abstract": "  Image-based lighting is a widely used technique to reproduce shading under\nreal-world lighting conditions, especially in real-time rendering applications.\nA particularly challenging scenario involves materials exhibiting a sparkling\nor glittering appearance, caused by discrete microfacets scattered across their\nsurface. In this paper, we propose an efficient approximation for image-based\nlighting of glints, enabling fully dynamic material properties and environment\nmaps. Our novel approach is grounded in real-time glint rendering under area\nlight illumination and employs standard environment map filtering techniques.\nCrucially, our environment map filtering process is sufficiently fast to be\nexecuted on a per-frame basis. Our method assumes that the environment map is\npartitioned into few homogeneous regions of constant radiance. By filtering the\ncorresponding indicator functions with the normal distribution function, we\nobtain the probabilities for individual microfacets to reflect light from each\nregion. During shading, these probabilities are utilized to hierarchically\nsample a multinomial distribution, facilitated by our novel dual-gated Gaussian\napproximation of binomial distributions. We validate that our real-time\napproximation is close to ground-truth renderings for a range of material\nproperties and lighting conditions, and demonstrate robust and stable\nperformance, with little overhead over rendering glints from a single\ndirectional light. Compared to rendering smooth materials without glints, our\napproach requires twice as much memory to store the prefiltered environment\nmap.\n", "link": "http://arxiv.org/abs/2507.02674v1", "date": "2025-07-03", "relevancy": 2.2438, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5872}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Image-based%20Lighting%20of%20Glints&body=Title%3A%20Real-time%20Image-based%20Lighting%20of%20Glints%0AAuthor%3A%20Tom%20Kneiphof%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%20Image-based%20lighting%20is%20a%20widely%20used%20technique%20to%20reproduce%20shading%20under%0Areal-world%20lighting%20conditions%2C%20especially%20in%20real-time%20rendering%20applications.%0AA%20particularly%20challenging%20scenario%20involves%20materials%20exhibiting%20a%20sparkling%0Aor%20glittering%20appearance%2C%20caused%20by%20discrete%20microfacets%20scattered%20across%20their%0Asurface.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20approximation%20for%20image-based%0Alighting%20of%20glints%2C%20enabling%20fully%20dynamic%20material%20properties%20and%20environment%0Amaps.%20Our%20novel%20approach%20is%20grounded%20in%20real-time%20glint%20rendering%20under%20area%0Alight%20illumination%20and%20employs%20standard%20environment%20map%20filtering%20techniques.%0ACrucially%2C%20our%20environment%20map%20filtering%20process%20is%20sufficiently%20fast%20to%20be%0Aexecuted%20on%20a%20per-frame%20basis.%20Our%20method%20assumes%20that%20the%20environment%20map%20is%0Apartitioned%20into%20few%20homogeneous%20regions%20of%20constant%20radiance.%20By%20filtering%20the%0Acorresponding%20indicator%20functions%20with%20the%20normal%20distribution%20function%2C%20we%0Aobtain%20the%20probabilities%20for%20individual%20microfacets%20to%20reflect%20light%20from%20each%0Aregion.%20During%20shading%2C%20these%20probabilities%20are%20utilized%20to%20hierarchically%0Asample%20a%20multinomial%20distribution%2C%20facilitated%20by%20our%20novel%20dual-gated%20Gaussian%0Aapproximation%20of%20binomial%20distributions.%20We%20validate%20that%20our%20real-time%0Aapproximation%20is%20close%20to%20ground-truth%20renderings%20for%20a%20range%20of%20material%0Aproperties%20and%20lighting%20conditions%2C%20and%20demonstrate%20robust%20and%20stable%0Aperformance%2C%20with%20little%20overhead%20over%20rendering%20glints%20from%20a%20single%0Adirectional%20light.%20Compared%20to%20rendering%20smooth%20materials%20without%20glints%2C%20our%0Aapproach%20requires%20twice%20as%20much%20memory%20to%20store%20the%20prefiltered%20environment%0Amap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Image-based%2520Lighting%2520of%2520Glints%26entry.906535625%3DTom%2520Kneiphof%2520and%2520Reinhard%2520Klein%26entry.1292438233%3D%2520%2520Image-based%2520lighting%2520is%2520a%2520widely%2520used%2520technique%2520to%2520reproduce%2520shading%2520under%250Areal-world%2520lighting%2520conditions%252C%2520especially%2520in%2520real-time%2520rendering%2520applications.%250AA%2520particularly%2520challenging%2520scenario%2520involves%2520materials%2520exhibiting%2520a%2520sparkling%250Aor%2520glittering%2520appearance%252C%2520caused%2520by%2520discrete%2520microfacets%2520scattered%2520across%2520their%250Asurface.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520approximation%2520for%2520image-based%250Alighting%2520of%2520glints%252C%2520enabling%2520fully%2520dynamic%2520material%2520properties%2520and%2520environment%250Amaps.%2520Our%2520novel%2520approach%2520is%2520grounded%2520in%2520real-time%2520glint%2520rendering%2520under%2520area%250Alight%2520illumination%2520and%2520employs%2520standard%2520environment%2520map%2520filtering%2520techniques.%250ACrucially%252C%2520our%2520environment%2520map%2520filtering%2520process%2520is%2520sufficiently%2520fast%2520to%2520be%250Aexecuted%2520on%2520a%2520per-frame%2520basis.%2520Our%2520method%2520assumes%2520that%2520the%2520environment%2520map%2520is%250Apartitioned%2520into%2520few%2520homogeneous%2520regions%2520of%2520constant%2520radiance.%2520By%2520filtering%2520the%250Acorresponding%2520indicator%2520functions%2520with%2520the%2520normal%2520distribution%2520function%252C%2520we%250Aobtain%2520the%2520probabilities%2520for%2520individual%2520microfacets%2520to%2520reflect%2520light%2520from%2520each%250Aregion.%2520During%2520shading%252C%2520these%2520probabilities%2520are%2520utilized%2520to%2520hierarchically%250Asample%2520a%2520multinomial%2520distribution%252C%2520facilitated%2520by%2520our%2520novel%2520dual-gated%2520Gaussian%250Aapproximation%2520of%2520binomial%2520distributions.%2520We%2520validate%2520that%2520our%2520real-time%250Aapproximation%2520is%2520close%2520to%2520ground-truth%2520renderings%2520for%2520a%2520range%2520of%2520material%250Aproperties%2520and%2520lighting%2520conditions%252C%2520and%2520demonstrate%2520robust%2520and%2520stable%250Aperformance%252C%2520with%2520little%2520overhead%2520over%2520rendering%2520glints%2520from%2520a%2520single%250Adirectional%2520light.%2520Compared%2520to%2520rendering%2520smooth%2520materials%2520without%2520glints%252C%2520our%250Aapproach%2520requires%2520twice%2520as%2520much%2520memory%2520to%2520store%2520the%2520prefiltered%2520environment%250Amap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Image-based%20Lighting%20of%20Glints&entry.906535625=Tom%20Kneiphof%20and%20Reinhard%20Klein&entry.1292438233=%20%20Image-based%20lighting%20is%20a%20widely%20used%20technique%20to%20reproduce%20shading%20under%0Areal-world%20lighting%20conditions%2C%20especially%20in%20real-time%20rendering%20applications.%0AA%20particularly%20challenging%20scenario%20involves%20materials%20exhibiting%20a%20sparkling%0Aor%20glittering%20appearance%2C%20caused%20by%20discrete%20microfacets%20scattered%20across%20their%0Asurface.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20approximation%20for%20image-based%0Alighting%20of%20glints%2C%20enabling%20fully%20dynamic%20material%20properties%20and%20environment%0Amaps.%20Our%20novel%20approach%20is%20grounded%20in%20real-time%20glint%20rendering%20under%20area%0Alight%20illumination%20and%20employs%20standard%20environment%20map%20filtering%20techniques.%0ACrucially%2C%20our%20environment%20map%20filtering%20process%20is%20sufficiently%20fast%20to%20be%0Aexecuted%20on%20a%20per-frame%20basis.%20Our%20method%20assumes%20that%20the%20environment%20map%20is%0Apartitioned%20into%20few%20homogeneous%20regions%20of%20constant%20radiance.%20By%20filtering%20the%0Acorresponding%20indicator%20functions%20with%20the%20normal%20distribution%20function%2C%20we%0Aobtain%20the%20probabilities%20for%20individual%20microfacets%20to%20reflect%20light%20from%20each%0Aregion.%20During%20shading%2C%20these%20probabilities%20are%20utilized%20to%20hierarchically%0Asample%20a%20multinomial%20distribution%2C%20facilitated%20by%20our%20novel%20dual-gated%20Gaussian%0Aapproximation%20of%20binomial%20distributions.%20We%20validate%20that%20our%20real-time%0Aapproximation%20is%20close%20to%20ground-truth%20renderings%20for%20a%20range%20of%20material%0Aproperties%20and%20lighting%20conditions%2C%20and%20demonstrate%20robust%20and%20stable%0Aperformance%2C%20with%20little%20overhead%20over%20rendering%20glints%20from%20a%20single%0Adirectional%20light.%20Compared%20to%20rendering%20smooth%20materials%20without%20glints%2C%20our%0Aapproach%20requires%20twice%20as%20much%20memory%20to%20store%20the%20prefiltered%20environment%0Amap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02674v1&entry.124074799=Read"},
{"title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing\n  Framework with Multimodal Narrative Understanding", "author": "Xiangfeng Wang and Xiao Li and Yadong Wei and Xueyu Song and Yang Song and Xiaoqiang Xia and Fangrui Zeng and Zaiyi Chen and Liu Liu and Gu Xu and Tong Xu", "abstract": "  The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.\n", "link": "http://arxiv.org/abs/2507.02790v1", "date": "2025-07-03", "relevancy": 2.2407, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&body=Title%3A%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding%0AAuthor%3A%20Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Long%2520Videos%2520to%2520Engaging%2520Clips%253A%2520A%2520Human-Inspired%2520Video%2520Editing%250A%2520%2520Framework%2520with%2520Multimodal%2520Narrative%2520Understanding%26entry.906535625%3DXiangfeng%2520Wang%2520and%2520Xiao%2520Li%2520and%2520Yadong%2520Wei%2520and%2520Xueyu%2520Song%2520and%2520Yang%2520Song%2520and%2520Xiaoqiang%2520Xia%2520and%2520Fangrui%2520Zeng%2520and%2520Zaiyi%2520Chen%2520and%2520Liu%2520Liu%2520and%2520Gu%2520Xu%2520and%2520Tong%2520Xu%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520online%2520video%2520content%252C%2520especially%2520on%2520short%2520video%250Aplatforms%252C%2520has%2520created%2520a%2520growing%2520demand%2520for%2520efficient%2520video%2520editing%2520techniques%250Athat%2520can%2520condense%2520long-form%2520videos%2520into%2520concise%2520and%2520engaging%2520clips.%2520Existing%250Aautomatic%2520editing%2520methods%2520predominantly%2520rely%2520on%2520textual%2520cues%2520from%2520ASR%250Atranscripts%2520and%2520end-to-end%2520segment%2520selection%252C%2520often%2520neglecting%2520the%2520rich%2520visual%250Acontext%2520and%2520leading%2520to%2520incoherent%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ahuman-inspired%2520automatic%2520video%2520editing%2520framework%2520%2528HIVE%2529%2520that%2520leverages%250Amultimodal%2520narrative%2520understanding%2520to%2520address%2520these%2520limitations.%2520Our%2520approach%250Aincorporates%2520character%2520extraction%252C%2520dialogue%2520analysis%252C%2520and%2520narrative%250Asummarization%2520through%2520multimodal%2520large%2520language%2520models%252C%2520enabling%2520a%2520holistic%250Aunderstanding%2520of%2520the%2520video%2520content.%2520To%2520further%2520enhance%2520coherence%252C%2520we%2520apply%250Ascene-level%2520segmentation%2520and%2520decompose%2520the%2520editing%2520process%2520into%2520three%2520subtasks%253A%250Ahighlight%2520detection%252C%2520opening/ending%2520selection%252C%2520and%2520pruning%2520of%2520irrelevant%250Acontent.%2520To%2520facilitate%2520research%2520in%2520this%2520area%252C%2520we%2520introduce%2520DramaAD%252C%2520a%2520novel%250Abenchmark%2520dataset%2520comprising%2520over%2520800%2520short%2520drama%2520episodes%2520and%2520500%250Aprofessionally%2520edited%2520advertisement%2520clips.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520framework%2520consistently%2520outperforms%2520existing%2520baselines%2520across%2520both%250Ageneral%2520and%2520advertisement-oriented%2520editing%2520tasks%252C%2520significantly%2520narrowing%2520the%250Aquality%2520gap%2520between%2520automatic%2520and%2520human-edited%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&entry.906535625=Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu&entry.1292438233=%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02790v1&entry.124074799=Read"},
{"title": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science", "author": "Erle Zhu and Yadi Liu and Zhe Zhang and Xujun Li and Jin Zhou and Xinjie Yu and Minlie Huang and Hongning Wang", "abstract": "  Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.\n", "link": "http://arxiv.org/abs/2501.10768v2", "date": "2025-07-03", "relevancy": 2.235, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science&body=Title%3A%20MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science%0AAuthor%3A%20Erle%20Zhu%20and%20Yadi%20Liu%20and%20Zhe%20Zhang%20and%20Xujun%20Li%20and%20Jin%20Zhou%20and%20Xinjie%20Yu%20and%20Minlie%20Huang%20and%20Hongning%20Wang%0AAbstract%3A%20%20%20Pre-trained%20on%20extensive%20text%20and%20image%20corpora%2C%20current%20Multi-Modal%20Large%0ALanguage%20Models%20%28MLLM%29%20have%20shown%20strong%20capabilities%20in%20general%20visual%0Areasoning%20tasks.%20However%2C%20their%20performance%20is%20still%20lacking%20in%20physical%0Adomains%20that%20require%20understanding%20diagrams%20with%20complex%20physical%20structures%0Aand%20quantitative%20analysis%20based%20on%20multi-modal%20information.%20To%20address%20this%2C%20we%0Adevelop%20a%20new%20framework%2C%20named%20Multi-Modal%20Scientific%20Reasoning%20with%20Physics%0APerception%20and%20Simulation%20%28MAPS%29%20based%20on%20an%20MLLM.%20MAPS%20decomposes%20expert-level%0Amulti-modal%20reasoning%20task%20into%20physical%20diagram%20understanding%20via%20a%20Physical%0APerception%20Model%20%28PPM%29%20and%20reasoning%20with%20physical%20knowledge%20via%20a%20simulator.%0AThe%20PPM%20module%20is%20obtained%20by%20fine-tuning%20a%20visual%20language%20model%20using%0Acarefully%20designed%20synthetic%20data%20with%20paired%20physical%20diagrams%20and%0Acorresponding%20simulation%20language%20descriptions.%20At%20the%20inference%20stage%2C%20MAPS%0Aintegrates%20the%20simulation%20language%20description%20of%20the%20input%20diagram%20provided%20by%0APPM%20and%20results%20obtained%20through%20a%20Chain-of-Simulation%20process%20with%20MLLM%20to%0Aderive%20the%20underlying%20rationale%20and%20the%20final%20answer.%20Validated%20using%20our%0Acollected%20college-level%20circuit%20analysis%20problems%2C%20MAPS%20significantly%20improves%0Areasoning%20accuracy%20of%20MLLM%20and%20outperforms%20all%20existing%20models.%20The%20results%0Aconfirm%20MAPS%20offers%20a%20promising%20direction%20for%20enhancing%20multi-modal%20scientific%0Areasoning%20ability%20of%20MLLMs.%20We%20will%20release%20our%20code%2C%20model%20and%20dataset%20used%0Afor%20our%20experiments%20upon%20publishing%20of%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPS%253A%2520Advancing%2520Multi-Modal%2520Reasoning%2520in%2520Expert-Level%2520Physical%2520Science%26entry.906535625%3DErle%2520Zhu%2520and%2520Yadi%2520Liu%2520and%2520Zhe%2520Zhang%2520and%2520Xujun%2520Li%2520and%2520Jin%2520Zhou%2520and%2520Xinjie%2520Yu%2520and%2520Minlie%2520Huang%2520and%2520Hongning%2520Wang%26entry.1292438233%3D%2520%2520Pre-trained%2520on%2520extensive%2520text%2520and%2520image%2520corpora%252C%2520current%2520Multi-Modal%2520Large%250ALanguage%2520Models%2520%2528MLLM%2529%2520have%2520shown%2520strong%2520capabilities%2520in%2520general%2520visual%250Areasoning%2520tasks.%2520However%252C%2520their%2520performance%2520is%2520still%2520lacking%2520in%2520physical%250Adomains%2520that%2520require%2520understanding%2520diagrams%2520with%2520complex%2520physical%2520structures%250Aand%2520quantitative%2520analysis%2520based%2520on%2520multi-modal%2520information.%2520To%2520address%2520this%252C%2520we%250Adevelop%2520a%2520new%2520framework%252C%2520named%2520Multi-Modal%2520Scientific%2520Reasoning%2520with%2520Physics%250APerception%2520and%2520Simulation%2520%2528MAPS%2529%2520based%2520on%2520an%2520MLLM.%2520MAPS%2520decomposes%2520expert-level%250Amulti-modal%2520reasoning%2520task%2520into%2520physical%2520diagram%2520understanding%2520via%2520a%2520Physical%250APerception%2520Model%2520%2528PPM%2529%2520and%2520reasoning%2520with%2520physical%2520knowledge%2520via%2520a%2520simulator.%250AThe%2520PPM%2520module%2520is%2520obtained%2520by%2520fine-tuning%2520a%2520visual%2520language%2520model%2520using%250Acarefully%2520designed%2520synthetic%2520data%2520with%2520paired%2520physical%2520diagrams%2520and%250Acorresponding%2520simulation%2520language%2520descriptions.%2520At%2520the%2520inference%2520stage%252C%2520MAPS%250Aintegrates%2520the%2520simulation%2520language%2520description%2520of%2520the%2520input%2520diagram%2520provided%2520by%250APPM%2520and%2520results%2520obtained%2520through%2520a%2520Chain-of-Simulation%2520process%2520with%2520MLLM%2520to%250Aderive%2520the%2520underlying%2520rationale%2520and%2520the%2520final%2520answer.%2520Validated%2520using%2520our%250Acollected%2520college-level%2520circuit%2520analysis%2520problems%252C%2520MAPS%2520significantly%2520improves%250Areasoning%2520accuracy%2520of%2520MLLM%2520and%2520outperforms%2520all%2520existing%2520models.%2520The%2520results%250Aconfirm%2520MAPS%2520offers%2520a%2520promising%2520direction%2520for%2520enhancing%2520multi-modal%2520scientific%250Areasoning%2520ability%2520of%2520MLLMs.%2520We%2520will%2520release%2520our%2520code%252C%2520model%2520and%2520dataset%2520used%250Afor%2520our%2520experiments%2520upon%2520publishing%2520of%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science&entry.906535625=Erle%20Zhu%20and%20Yadi%20Liu%20and%20Zhe%20Zhang%20and%20Xujun%20Li%20and%20Jin%20Zhou%20and%20Xinjie%20Yu%20and%20Minlie%20Huang%20and%20Hongning%20Wang&entry.1292438233=%20%20Pre-trained%20on%20extensive%20text%20and%20image%20corpora%2C%20current%20Multi-Modal%20Large%0ALanguage%20Models%20%28MLLM%29%20have%20shown%20strong%20capabilities%20in%20general%20visual%0Areasoning%20tasks.%20However%2C%20their%20performance%20is%20still%20lacking%20in%20physical%0Adomains%20that%20require%20understanding%20diagrams%20with%20complex%20physical%20structures%0Aand%20quantitative%20analysis%20based%20on%20multi-modal%20information.%20To%20address%20this%2C%20we%0Adevelop%20a%20new%20framework%2C%20named%20Multi-Modal%20Scientific%20Reasoning%20with%20Physics%0APerception%20and%20Simulation%20%28MAPS%29%20based%20on%20an%20MLLM.%20MAPS%20decomposes%20expert-level%0Amulti-modal%20reasoning%20task%20into%20physical%20diagram%20understanding%20via%20a%20Physical%0APerception%20Model%20%28PPM%29%20and%20reasoning%20with%20physical%20knowledge%20via%20a%20simulator.%0AThe%20PPM%20module%20is%20obtained%20by%20fine-tuning%20a%20visual%20language%20model%20using%0Acarefully%20designed%20synthetic%20data%20with%20paired%20physical%20diagrams%20and%0Acorresponding%20simulation%20language%20descriptions.%20At%20the%20inference%20stage%2C%20MAPS%0Aintegrates%20the%20simulation%20language%20description%20of%20the%20input%20diagram%20provided%20by%0APPM%20and%20results%20obtained%20through%20a%20Chain-of-Simulation%20process%20with%20MLLM%20to%0Aderive%20the%20underlying%20rationale%20and%20the%20final%20answer.%20Validated%20using%20our%0Acollected%20college-level%20circuit%20analysis%20problems%2C%20MAPS%20significantly%20improves%0Areasoning%20accuracy%20of%20MLLM%20and%20outperforms%20all%20existing%20models.%20The%20results%0Aconfirm%20MAPS%20offers%20a%20promising%20direction%20for%20enhancing%20multi-modal%20scientific%0Areasoning%20ability%20of%20MLLMs.%20We%20will%20release%20our%20code%2C%20model%20and%20dataset%20used%0Afor%20our%20experiments%20upon%20publishing%20of%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10768v2&entry.124074799=Read"},
{"title": "Partial Weakly-Supervised Oriented Object Detection", "author": "Mingxin Liu and Peiyuan Zhang and Yuan Liu and Wei Zhang and Yue Zhou and Ning Liao and Ziyang Gong and Junwei Luo and Zhirui Wang and Yi Yu and Xue Yang", "abstract": "  The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.\n", "link": "http://arxiv.org/abs/2507.02751v1", "date": "2025-07-03", "relevancy": 2.2348, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5688}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5569}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Weakly-Supervised%20Oriented%20Object%20Detection&body=Title%3A%20Partial%20Weakly-Supervised%20Oriented%20Object%20Detection%0AAuthor%3A%20Mingxin%20Liu%20and%20Peiyuan%20Zhang%20and%20Yuan%20Liu%20and%20Wei%20Zhang%20and%20Yue%20Zhou%20and%20Ning%20Liao%20and%20Ziyang%20Gong%20and%20Junwei%20Luo%20and%20Zhirui%20Wang%20and%20Yi%20Yu%20and%20Xue%20Yang%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%20across%20various%20domains%0Ahas%20driven%20significant%20research%20in%20this%20area.%20However%2C%20the%20high%20cost%20of%20dataset%0Aannotation%20remains%20a%20major%20concern.%20Current%20mainstream%20OOD%20algorithms%20can%20be%0Amainly%20categorized%20into%20three%20types%3A%20%281%29%20fully%20supervised%20methods%20using%0Acomplete%20oriented%20bounding%20box%20%28OBB%29%20annotations%2C%20%282%29%20semi-supervised%20methods%0Ausing%20partial%20OBB%20annotations%2C%20and%20%283%29%20weakly%20supervised%20methods%20using%20weak%0Aannotations%20such%20as%20horizontal%20boxes%20or%20points.%20However%2C%20these%20algorithms%0Ainevitably%20increase%20the%20cost%20of%20models%20in%20terms%20of%20annotation%20speed%20or%0Aannotation%20cost.%20To%20address%20this%20issue%2C%20we%20propose%3A%281%29%20the%20first%20Partial%0AWeakly-Supervised%20Oriented%20Object%20Detection%20%28PWOOD%29%20framework%20based%20on%0Apartially%20weak%20annotations%20%28horizontal%20boxes%20or%20single%20points%29%2C%20which%20can%0Aefficiently%20leverage%20large%20amounts%20of%20unlabeled%20data%2C%20significantly%0Aoutperforming%20weakly%20supervised%20algorithms%20trained%20with%20partially%20weak%0Aannotations%2C%20also%20offers%20a%20lower%20cost%20solution%3B%20%282%29%20Orientation-and-Scale-aware%0AStudent%20%28OS-Student%29%20model%20capable%20of%20learning%20orientation%20and%20scale%0Ainformation%20with%20only%20a%20small%20amount%20of%20orientation-agnostic%20or%20scale-agnostic%0Aweak%20annotations%3B%20and%20%283%29%20Class-Agnostic%20Pseudo-Label%20Filtering%20strategy%20%28CPF%29%0Ato%20reduce%20the%20model%27s%20sensitivity%20to%20static%20filtering%20thresholds.%20Comprehensive%0Aexperiments%20on%20DOTA-v1.0/v1.5/v2.0%20and%20DIOR%20datasets%20demonstrate%20that%20our%20PWOOD%0Aframework%20performs%20comparably%20to%2C%20or%20even%20surpasses%2C%20traditional%0Asemi-supervised%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Weakly-Supervised%2520Oriented%2520Object%2520Detection%26entry.906535625%3DMingxin%2520Liu%2520and%2520Peiyuan%2520Zhang%2520and%2520Yuan%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Yue%2520Zhou%2520and%2520Ning%2520Liao%2520and%2520Ziyang%2520Gong%2520and%2520Junwei%2520Luo%2520and%2520Zhirui%2520Wang%2520and%2520Yi%2520Yu%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520oriented%2520object%2520detection%2520%2528OOD%2529%2520across%2520various%2520domains%250Ahas%2520driven%2520significant%2520research%2520in%2520this%2520area.%2520However%252C%2520the%2520high%2520cost%2520of%2520dataset%250Aannotation%2520remains%2520a%2520major%2520concern.%2520Current%2520mainstream%2520OOD%2520algorithms%2520can%2520be%250Amainly%2520categorized%2520into%2520three%2520types%253A%2520%25281%2529%2520fully%2520supervised%2520methods%2520using%250Acomplete%2520oriented%2520bounding%2520box%2520%2528OBB%2529%2520annotations%252C%2520%25282%2529%2520semi-supervised%2520methods%250Ausing%2520partial%2520OBB%2520annotations%252C%2520and%2520%25283%2529%2520weakly%2520supervised%2520methods%2520using%2520weak%250Aannotations%2520such%2520as%2520horizontal%2520boxes%2520or%2520points.%2520However%252C%2520these%2520algorithms%250Ainevitably%2520increase%2520the%2520cost%2520of%2520models%2520in%2520terms%2520of%2520annotation%2520speed%2520or%250Aannotation%2520cost.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%253A%25281%2529%2520the%2520first%2520Partial%250AWeakly-Supervised%2520Oriented%2520Object%2520Detection%2520%2528PWOOD%2529%2520framework%2520based%2520on%250Apartially%2520weak%2520annotations%2520%2528horizontal%2520boxes%2520or%2520single%2520points%2529%252C%2520which%2520can%250Aefficiently%2520leverage%2520large%2520amounts%2520of%2520unlabeled%2520data%252C%2520significantly%250Aoutperforming%2520weakly%2520supervised%2520algorithms%2520trained%2520with%2520partially%2520weak%250Aannotations%252C%2520also%2520offers%2520a%2520lower%2520cost%2520solution%253B%2520%25282%2529%2520Orientation-and-Scale-aware%250AStudent%2520%2528OS-Student%2529%2520model%2520capable%2520of%2520learning%2520orientation%2520and%2520scale%250Ainformation%2520with%2520only%2520a%2520small%2520amount%2520of%2520orientation-agnostic%2520or%2520scale-agnostic%250Aweak%2520annotations%253B%2520and%2520%25283%2529%2520Class-Agnostic%2520Pseudo-Label%2520Filtering%2520strategy%2520%2528CPF%2529%250Ato%2520reduce%2520the%2520model%2527s%2520sensitivity%2520to%2520static%2520filtering%2520thresholds.%2520Comprehensive%250Aexperiments%2520on%2520DOTA-v1.0/v1.5/v2.0%2520and%2520DIOR%2520datasets%2520demonstrate%2520that%2520our%2520PWOOD%250Aframework%2520performs%2520comparably%2520to%252C%2520or%2520even%2520surpasses%252C%2520traditional%250Asemi-supervised%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Weakly-Supervised%20Oriented%20Object%20Detection&entry.906535625=Mingxin%20Liu%20and%20Peiyuan%20Zhang%20and%20Yuan%20Liu%20and%20Wei%20Zhang%20and%20Yue%20Zhou%20and%20Ning%20Liao%20and%20Ziyang%20Gong%20and%20Junwei%20Luo%20and%20Zhirui%20Wang%20and%20Yi%20Yu%20and%20Xue%20Yang&entry.1292438233=%20%20The%20growing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%20across%20various%20domains%0Ahas%20driven%20significant%20research%20in%20this%20area.%20However%2C%20the%20high%20cost%20of%20dataset%0Aannotation%20remains%20a%20major%20concern.%20Current%20mainstream%20OOD%20algorithms%20can%20be%0Amainly%20categorized%20into%20three%20types%3A%20%281%29%20fully%20supervised%20methods%20using%0Acomplete%20oriented%20bounding%20box%20%28OBB%29%20annotations%2C%20%282%29%20semi-supervised%20methods%0Ausing%20partial%20OBB%20annotations%2C%20and%20%283%29%20weakly%20supervised%20methods%20using%20weak%0Aannotations%20such%20as%20horizontal%20boxes%20or%20points.%20However%2C%20these%20algorithms%0Ainevitably%20increase%20the%20cost%20of%20models%20in%20terms%20of%20annotation%20speed%20or%0Aannotation%20cost.%20To%20address%20this%20issue%2C%20we%20propose%3A%281%29%20the%20first%20Partial%0AWeakly-Supervised%20Oriented%20Object%20Detection%20%28PWOOD%29%20framework%20based%20on%0Apartially%20weak%20annotations%20%28horizontal%20boxes%20or%20single%20points%29%2C%20which%20can%0Aefficiently%20leverage%20large%20amounts%20of%20unlabeled%20data%2C%20significantly%0Aoutperforming%20weakly%20supervised%20algorithms%20trained%20with%20partially%20weak%0Aannotations%2C%20also%20offers%20a%20lower%20cost%20solution%3B%20%282%29%20Orientation-and-Scale-aware%0AStudent%20%28OS-Student%29%20model%20capable%20of%20learning%20orientation%20and%20scale%0Ainformation%20with%20only%20a%20small%20amount%20of%20orientation-agnostic%20or%20scale-agnostic%0Aweak%20annotations%3B%20and%20%283%29%20Class-Agnostic%20Pseudo-Label%20Filtering%20strategy%20%28CPF%29%0Ato%20reduce%20the%20model%27s%20sensitivity%20to%20static%20filtering%20thresholds.%20Comprehensive%0Aexperiments%20on%20DOTA-v1.0/v1.5/v2.0%20and%20DIOR%20datasets%20demonstrate%20that%20our%20PWOOD%0Aframework%20performs%20comparably%20to%2C%20or%20even%20surpasses%2C%20traditional%0Asemi-supervised%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02751v1&entry.124074799=Read"},
{"title": "LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud\n  Occupancy Functions", "author": "Pit Henrich and Franziska Mathis-Ullrich and Paul Maria Scheikl", "abstract": "  Accurately determining the shape of deformable objects and the location of\ntheir internal structures is crucial for medical tasks that require precise\ntargeting, such as robotic biopsies. We introduce LUDO, a method for accurate\nlow-latency understanding of deformable objects. LUDO reconstructs objects in\ntheir deformed state, including their internal structures, from a single-view\npoint cloud observation in under 30 ms using occupancy networks. LUDO provides\nuncertainty estimates for its predictions. Additionally, it provides\nexplainability by highlighting key features in its input observations. Both\nuncertainty and explainability are important for safety-critical applications\nsuch as surgery. We evaluate LUDO in real-world robotic experiments, achieving\na success rate of 98.9% for puncturing various regions of interest (ROIs)\ninside deformable objects. We compare LUDO to a popular baseline and show its\nsuperior ROI localization accuracy, training time, and memory requirements.\nLUDO demonstrates the potential to interact with deformable objects without the\nneed for deformable registration methods.\n", "link": "http://arxiv.org/abs/2411.08777v5", "date": "2025-07-03", "relevancy": 2.229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5915}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions&body=Title%3A%20LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions%0AAuthor%3A%20Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl%0AAbstract%3A%20%20%20Accurately%20determining%20the%20shape%20of%20deformable%20objects%20and%20the%20location%20of%0Atheir%20internal%20structures%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%0Atargeting%2C%20such%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%0Alow-latency%20understanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%0Atheir%20deformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%0Apoint%20cloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20LUDO%20provides%0Auncertainty%20estimates%20for%20its%20predictions.%20Additionally%2C%20it%20provides%0Aexplainability%20by%20highlighting%20key%20features%20in%20its%20input%20observations.%20Both%0Auncertainty%20and%20explainability%20are%20important%20for%20safety-critical%20applications%0Asuch%20as%20surgery.%20We%20evaluate%20LUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%0Aa%20success%20rate%20of%2098.9%25%20for%20puncturing%20various%20regions%20of%20interest%20%28ROIs%29%0Ainside%20deformable%20objects.%20We%20compare%20LUDO%20to%20a%20popular%20baseline%20and%20show%20its%0Asuperior%20ROI%20localization%20accuracy%2C%20training%20time%2C%20and%20memory%20requirements.%0ALUDO%20demonstrates%20the%20potential%20to%20interact%20with%20deformable%20objects%20without%20the%0Aneed%20for%20deformable%20registration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08777v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUDO%253A%2520Low-Latency%2520Understanding%2520of%2520Deformable%2520Objects%2520using%2520Point%2520Cloud%250A%2520%2520Occupancy%2520Functions%26entry.906535625%3DPit%2520Henrich%2520and%2520Franziska%2520Mathis-Ullrich%2520and%2520Paul%2520Maria%2520Scheikl%26entry.1292438233%3D%2520%2520Accurately%2520determining%2520the%2520shape%2520of%2520deformable%2520objects%2520and%2520the%2520location%2520of%250Atheir%2520internal%2520structures%2520is%2520crucial%2520for%2520medical%2520tasks%2520that%2520require%2520precise%250Atargeting%252C%2520such%2520as%2520robotic%2520biopsies.%2520We%2520introduce%2520LUDO%252C%2520a%2520method%2520for%2520accurate%250Alow-latency%2520understanding%2520of%2520deformable%2520objects.%2520LUDO%2520reconstructs%2520objects%2520in%250Atheir%2520deformed%2520state%252C%2520including%2520their%2520internal%2520structures%252C%2520from%2520a%2520single-view%250Apoint%2520cloud%2520observation%2520in%2520under%252030%2520ms%2520using%2520occupancy%2520networks.%2520LUDO%2520provides%250Auncertainty%2520estimates%2520for%2520its%2520predictions.%2520Additionally%252C%2520it%2520provides%250Aexplainability%2520by%2520highlighting%2520key%2520features%2520in%2520its%2520input%2520observations.%2520Both%250Auncertainty%2520and%2520explainability%2520are%2520important%2520for%2520safety-critical%2520applications%250Asuch%2520as%2520surgery.%2520We%2520evaluate%2520LUDO%2520in%2520real-world%2520robotic%2520experiments%252C%2520achieving%250Aa%2520success%2520rate%2520of%252098.9%2525%2520for%2520puncturing%2520various%2520regions%2520of%2520interest%2520%2528ROIs%2529%250Ainside%2520deformable%2520objects.%2520We%2520compare%2520LUDO%2520to%2520a%2520popular%2520baseline%2520and%2520show%2520its%250Asuperior%2520ROI%2520localization%2520accuracy%252C%2520training%2520time%252C%2520and%2520memory%2520requirements.%250ALUDO%2520demonstrates%2520the%2520potential%2520to%2520interact%2520with%2520deformable%2520objects%2520without%2520the%250Aneed%2520for%2520deformable%2520registration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08777v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions&entry.906535625=Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl&entry.1292438233=%20%20Accurately%20determining%20the%20shape%20of%20deformable%20objects%20and%20the%20location%20of%0Atheir%20internal%20structures%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%0Atargeting%2C%20such%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%0Alow-latency%20understanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%0Atheir%20deformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%0Apoint%20cloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20LUDO%20provides%0Auncertainty%20estimates%20for%20its%20predictions.%20Additionally%2C%20it%20provides%0Aexplainability%20by%20highlighting%20key%20features%20in%20its%20input%20observations.%20Both%0Auncertainty%20and%20explainability%20are%20important%20for%20safety-critical%20applications%0Asuch%20as%20surgery.%20We%20evaluate%20LUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%0Aa%20success%20rate%20of%2098.9%25%20for%20puncturing%20various%20regions%20of%20interest%20%28ROIs%29%0Ainside%20deformable%20objects.%20We%20compare%20LUDO%20to%20a%20popular%20baseline%20and%20show%20its%0Asuperior%20ROI%20localization%20accuracy%2C%20training%20time%2C%20and%20memory%20requirements.%0ALUDO%20demonstrates%20the%20potential%20to%20interact%20with%20deformable%20objects%20without%20the%0Aneed%20for%20deformable%20registration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08777v5&entry.124074799=Read"},
{"title": "Grounding Intelligence in Movement", "author": "Melanie Segado and Felipe Parodi and Jordan K. Matelsky and Michael L. Platt and Eva B. Dyer and Konrad P. Kording", "abstract": "  Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.\n", "link": "http://arxiv.org/abs/2507.02771v1", "date": "2025-07-03", "relevancy": 2.2037, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5581}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Intelligence%20in%20Movement&body=Title%3A%20Grounding%20Intelligence%20in%20Movement%0AAuthor%3A%20Melanie%20Segado%20and%20Felipe%20Parodi%20and%20Jordan%20K.%20Matelsky%20and%20Michael%20L.%20Platt%20and%20Eva%20B.%20Dyer%20and%20Konrad%20P.%20Kording%0AAbstract%3A%20%20%20Recent%20advances%20in%20machine%20learning%20have%20dramatically%20improved%20our%20ability%20to%0Amodel%20language%2C%20vision%2C%20and%20other%20high-dimensional%20data%2C%20yet%20they%20continue%20to%0Astruggle%20with%20one%20of%20the%20most%20fundamental%20aspects%20of%20biological%20systems%3A%0Amovement.%20Across%20neuroscience%2C%20medicine%2C%20robotics%2C%20and%20ethology%2C%20movement%20is%0Aessential%20for%20interpreting%20behavior%2C%20predicting%20intent%2C%20and%20enabling%0Ainteraction.%20Despite%20its%20core%20significance%20in%20our%20intelligence%2C%20movement%20is%0Aoften%20treated%20as%20an%20afterthought%20rather%20than%20as%20a%20rich%20and%20structured%20modality%0Ain%20its%20own%20right.%20This%20reflects%20a%20deeper%20fragmentation%20in%20how%20movement%20data%20is%0Acollected%20and%20modeled%2C%20often%20constrained%20by%20task-specific%20goals%20and%0Adomain-specific%20assumptions.%20But%20movement%20is%20not%20domain-bound.%20It%20reflects%0Ashared%20physical%20constraints%2C%20conserved%20morphological%20structures%2C%20and%20purposeful%0Adynamics%20that%20cut%20across%20species%20and%20settings.%20We%20argue%20that%20movement%20should%20be%0Atreated%20as%20a%20primary%20modeling%20target%20for%20AI.%20It%20is%20inherently%20structured%20and%0Agrounded%20in%20embodiment%20and%20physics.%20This%20structure%2C%20often%20allowing%20for%20compact%2C%0Alower-dimensional%20representations%20%28e.g.%2C%20pose%29%2C%20makes%20it%20more%20interpretable%20and%0Acomputationally%20tractable%20to%20model%20than%20raw%2C%20high-dimensional%20sensory%20inputs.%0ADeveloping%20models%20that%20can%20learn%20from%20and%20generalize%20across%20diverse%20movement%0Adata%20will%20not%20only%20advance%20core%20capabilities%20in%20generative%20modeling%20and%0Acontrol%2C%20but%20also%20create%20a%20shared%20foundation%20for%20understanding%20behavior%20across%0Abiological%20and%20artificial%20systems.%20Movement%20is%20not%20just%20an%20outcome%2C%20it%20is%20a%0Awindow%20into%20how%20intelligent%20systems%20engage%20with%20the%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Intelligence%2520in%2520Movement%26entry.906535625%3DMelanie%2520Segado%2520and%2520Felipe%2520Parodi%2520and%2520Jordan%2520K.%2520Matelsky%2520and%2520Michael%2520L.%2520Platt%2520and%2520Eva%2520B.%2520Dyer%2520and%2520Konrad%2520P.%2520Kording%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520machine%2520learning%2520have%2520dramatically%2520improved%2520our%2520ability%2520to%250Amodel%2520language%252C%2520vision%252C%2520and%2520other%2520high-dimensional%2520data%252C%2520yet%2520they%2520continue%2520to%250Astruggle%2520with%2520one%2520of%2520the%2520most%2520fundamental%2520aspects%2520of%2520biological%2520systems%253A%250Amovement.%2520Across%2520neuroscience%252C%2520medicine%252C%2520robotics%252C%2520and%2520ethology%252C%2520movement%2520is%250Aessential%2520for%2520interpreting%2520behavior%252C%2520predicting%2520intent%252C%2520and%2520enabling%250Ainteraction.%2520Despite%2520its%2520core%2520significance%2520in%2520our%2520intelligence%252C%2520movement%2520is%250Aoften%2520treated%2520as%2520an%2520afterthought%2520rather%2520than%2520as%2520a%2520rich%2520and%2520structured%2520modality%250Ain%2520its%2520own%2520right.%2520This%2520reflects%2520a%2520deeper%2520fragmentation%2520in%2520how%2520movement%2520data%2520is%250Acollected%2520and%2520modeled%252C%2520often%2520constrained%2520by%2520task-specific%2520goals%2520and%250Adomain-specific%2520assumptions.%2520But%2520movement%2520is%2520not%2520domain-bound.%2520It%2520reflects%250Ashared%2520physical%2520constraints%252C%2520conserved%2520morphological%2520structures%252C%2520and%2520purposeful%250Adynamics%2520that%2520cut%2520across%2520species%2520and%2520settings.%2520We%2520argue%2520that%2520movement%2520should%2520be%250Atreated%2520as%2520a%2520primary%2520modeling%2520target%2520for%2520AI.%2520It%2520is%2520inherently%2520structured%2520and%250Agrounded%2520in%2520embodiment%2520and%2520physics.%2520This%2520structure%252C%2520often%2520allowing%2520for%2520compact%252C%250Alower-dimensional%2520representations%2520%2528e.g.%252C%2520pose%2529%252C%2520makes%2520it%2520more%2520interpretable%2520and%250Acomputationally%2520tractable%2520to%2520model%2520than%2520raw%252C%2520high-dimensional%2520sensory%2520inputs.%250ADeveloping%2520models%2520that%2520can%2520learn%2520from%2520and%2520generalize%2520across%2520diverse%2520movement%250Adata%2520will%2520not%2520only%2520advance%2520core%2520capabilities%2520in%2520generative%2520modeling%2520and%250Acontrol%252C%2520but%2520also%2520create%2520a%2520shared%2520foundation%2520for%2520understanding%2520behavior%2520across%250Abiological%2520and%2520artificial%2520systems.%2520Movement%2520is%2520not%2520just%2520an%2520outcome%252C%2520it%2520is%2520a%250Awindow%2520into%2520how%2520intelligent%2520systems%2520engage%2520with%2520the%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Intelligence%20in%20Movement&entry.906535625=Melanie%20Segado%20and%20Felipe%20Parodi%20and%20Jordan%20K.%20Matelsky%20and%20Michael%20L.%20Platt%20and%20Eva%20B.%20Dyer%20and%20Konrad%20P.%20Kording&entry.1292438233=%20%20Recent%20advances%20in%20machine%20learning%20have%20dramatically%20improved%20our%20ability%20to%0Amodel%20language%2C%20vision%2C%20and%20other%20high-dimensional%20data%2C%20yet%20they%20continue%20to%0Astruggle%20with%20one%20of%20the%20most%20fundamental%20aspects%20of%20biological%20systems%3A%0Amovement.%20Across%20neuroscience%2C%20medicine%2C%20robotics%2C%20and%20ethology%2C%20movement%20is%0Aessential%20for%20interpreting%20behavior%2C%20predicting%20intent%2C%20and%20enabling%0Ainteraction.%20Despite%20its%20core%20significance%20in%20our%20intelligence%2C%20movement%20is%0Aoften%20treated%20as%20an%20afterthought%20rather%20than%20as%20a%20rich%20and%20structured%20modality%0Ain%20its%20own%20right.%20This%20reflects%20a%20deeper%20fragmentation%20in%20how%20movement%20data%20is%0Acollected%20and%20modeled%2C%20often%20constrained%20by%20task-specific%20goals%20and%0Adomain-specific%20assumptions.%20But%20movement%20is%20not%20domain-bound.%20It%20reflects%0Ashared%20physical%20constraints%2C%20conserved%20morphological%20structures%2C%20and%20purposeful%0Adynamics%20that%20cut%20across%20species%20and%20settings.%20We%20argue%20that%20movement%20should%20be%0Atreated%20as%20a%20primary%20modeling%20target%20for%20AI.%20It%20is%20inherently%20structured%20and%0Agrounded%20in%20embodiment%20and%20physics.%20This%20structure%2C%20often%20allowing%20for%20compact%2C%0Alower-dimensional%20representations%20%28e.g.%2C%20pose%29%2C%20makes%20it%20more%20interpretable%20and%0Acomputationally%20tractable%20to%20model%20than%20raw%2C%20high-dimensional%20sensory%20inputs.%0ADeveloping%20models%20that%20can%20learn%20from%20and%20generalize%20across%20diverse%20movement%0Adata%20will%20not%20only%20advance%20core%20capabilities%20in%20generative%20modeling%20and%0Acontrol%2C%20but%20also%20create%20a%20shared%20foundation%20for%20understanding%20behavior%20across%0Abiological%20and%20artificial%20systems.%20Movement%20is%20not%20just%20an%20outcome%2C%20it%20is%20a%0Awindow%20into%20how%20intelligent%20systems%20engage%20with%20the%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02771v1&entry.124074799=Read"},
{"title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing", "author": "Yu Yuan and Shizhao Sun and Qi Liu and Jiang Bian", "abstract": "  Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively. The code is\navailable at \\url {https://github.com/microsoft/CAD-Editor}.\n", "link": "http://arxiv.org/abs/2502.03997v2", "date": "2025-07-03", "relevancy": 2.2015, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5856}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Editor%3A%20A%20Locate-then-Infill%20Framework%20with%20Automated%20Training%20Data%0A%20%20Synthesis%20for%20Text-Based%20CAD%20Editing&body=Title%3A%20CAD-Editor%3A%20A%20Locate-then-Infill%20Framework%20with%20Automated%20Training%20Data%0A%20%20Synthesis%20for%20Text-Based%20CAD%20Editing%0AAuthor%3A%20Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Qi%20Liu%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Computer%20Aided%20Design%20%28CAD%29%20is%20indispensable%20across%20various%20industries.%0A%5Cemph%7BText-based%20CAD%20editing%7D%2C%20which%20automates%20the%20modification%20of%20CAD%20models%0Abased%20on%20textual%20instructions%2C%20holds%20great%20potential%20but%20remains%20underexplored.%0AExisting%20methods%20primarily%20focus%20on%20design%20variation%20generation%20or%20text-based%0ACAD%20generation%2C%20either%20lacking%20support%20for%20text-based%20control%20or%20neglecting%0Aexisting%20CAD%20models%20as%20constraints.%20We%20introduce%20%5Cemph%7BCAD-Editor%7D%2C%20the%20first%0Aframework%20for%20text-based%20CAD%20editing.%20To%20address%20the%20challenge%20of%20demanding%0Atriplet%20data%20with%20accurate%20correspondence%20for%20training%2C%20we%20propose%20an%20automated%0Adata%20synthesis%20pipeline.%20This%20pipeline%20utilizes%20design%20variation%20models%20to%0Agenerate%20pairs%20of%20original%20and%20edited%20CAD%20models%20and%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20summarize%20their%20differences%20into%20editing%0Ainstructions.%20To%20tackle%20the%20composite%20nature%20of%20text-based%20CAD%20editing%2C%20we%0Apropose%20a%20locate-then-infill%20framework%20that%20decomposes%20the%20task%20into%20two%0Afocused%20sub-tasks%3A%20locating%20regions%20requiring%20modification%20and%20infilling%20these%0Aregions%20with%20appropriate%20edits.%20Large%20Language%20Models%20%28LLMs%29%20serve%20as%20the%0Abackbone%20for%20both%20sub-tasks%2C%20leveraging%20their%20capabilities%20in%20natural%20language%0Aunderstanding%20and%20CAD%20knowledge.%20Experiments%20show%20that%20CAD-Editor%20achieves%0Asuperior%20performance%20both%20quantitatively%20and%20qualitatively.%20The%20code%20is%0Aavailable%20at%20%5Curl%20%7Bhttps%3A//github.com/microsoft/CAD-Editor%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Editor%253A%2520A%2520Locate-then-Infill%2520Framework%2520with%2520Automated%2520Training%2520Data%250A%2520%2520Synthesis%2520for%2520Text-Based%2520CAD%2520Editing%26entry.906535625%3DYu%2520Yuan%2520and%2520Shizhao%2520Sun%2520and%2520Qi%2520Liu%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Computer%2520Aided%2520Design%2520%2528CAD%2529%2520is%2520indispensable%2520across%2520various%2520industries.%250A%255Cemph%257BText-based%2520CAD%2520editing%257D%252C%2520which%2520automates%2520the%2520modification%2520of%2520CAD%2520models%250Abased%2520on%2520textual%2520instructions%252C%2520holds%2520great%2520potential%2520but%2520remains%2520underexplored.%250AExisting%2520methods%2520primarily%2520focus%2520on%2520design%2520variation%2520generation%2520or%2520text-based%250ACAD%2520generation%252C%2520either%2520lacking%2520support%2520for%2520text-based%2520control%2520or%2520neglecting%250Aexisting%2520CAD%2520models%2520as%2520constraints.%2520We%2520introduce%2520%255Cemph%257BCAD-Editor%257D%252C%2520the%2520first%250Aframework%2520for%2520text-based%2520CAD%2520editing.%2520To%2520address%2520the%2520challenge%2520of%2520demanding%250Atriplet%2520data%2520with%2520accurate%2520correspondence%2520for%2520training%252C%2520we%2520propose%2520an%2520automated%250Adata%2520synthesis%2520pipeline.%2520This%2520pipeline%2520utilizes%2520design%2520variation%2520models%2520to%250Agenerate%2520pairs%2520of%2520original%2520and%2520edited%2520CAD%2520models%2520and%2520employs%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520summarize%2520their%2520differences%2520into%2520editing%250Ainstructions.%2520To%2520tackle%2520the%2520composite%2520nature%2520of%2520text-based%2520CAD%2520editing%252C%2520we%250Apropose%2520a%2520locate-then-infill%2520framework%2520that%2520decomposes%2520the%2520task%2520into%2520two%250Afocused%2520sub-tasks%253A%2520locating%2520regions%2520requiring%2520modification%2520and%2520infilling%2520these%250Aregions%2520with%2520appropriate%2520edits.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520serve%2520as%2520the%250Abackbone%2520for%2520both%2520sub-tasks%252C%2520leveraging%2520their%2520capabilities%2520in%2520natural%2520language%250Aunderstanding%2520and%2520CAD%2520knowledge.%2520Experiments%2520show%2520that%2520CAD-Editor%2520achieves%250Asuperior%2520performance%2520both%2520quantitatively%2520and%2520qualitatively.%2520The%2520code%2520is%250Aavailable%2520at%2520%255Curl%2520%257Bhttps%253A//github.com/microsoft/CAD-Editor%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Editor%3A%20A%20Locate-then-Infill%20Framework%20with%20Automated%20Training%20Data%0A%20%20Synthesis%20for%20Text-Based%20CAD%20Editing&entry.906535625=Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Qi%20Liu%20and%20Jiang%20Bian&entry.1292438233=%20%20Computer%20Aided%20Design%20%28CAD%29%20is%20indispensable%20across%20various%20industries.%0A%5Cemph%7BText-based%20CAD%20editing%7D%2C%20which%20automates%20the%20modification%20of%20CAD%20models%0Abased%20on%20textual%20instructions%2C%20holds%20great%20potential%20but%20remains%20underexplored.%0AExisting%20methods%20primarily%20focus%20on%20design%20variation%20generation%20or%20text-based%0ACAD%20generation%2C%20either%20lacking%20support%20for%20text-based%20control%20or%20neglecting%0Aexisting%20CAD%20models%20as%20constraints.%20We%20introduce%20%5Cemph%7BCAD-Editor%7D%2C%20the%20first%0Aframework%20for%20text-based%20CAD%20editing.%20To%20address%20the%20challenge%20of%20demanding%0Atriplet%20data%20with%20accurate%20correspondence%20for%20training%2C%20we%20propose%20an%20automated%0Adata%20synthesis%20pipeline.%20This%20pipeline%20utilizes%20design%20variation%20models%20to%0Agenerate%20pairs%20of%20original%20and%20edited%20CAD%20models%20and%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20summarize%20their%20differences%20into%20editing%0Ainstructions.%20To%20tackle%20the%20composite%20nature%20of%20text-based%20CAD%20editing%2C%20we%0Apropose%20a%20locate-then-infill%20framework%20that%20decomposes%20the%20task%20into%20two%0Afocused%20sub-tasks%3A%20locating%20regions%20requiring%20modification%20and%20infilling%20these%0Aregions%20with%20appropriate%20edits.%20Large%20Language%20Models%20%28LLMs%29%20serve%20as%20the%0Abackbone%20for%20both%20sub-tasks%2C%20leveraging%20their%20capabilities%20in%20natural%20language%0Aunderstanding%20and%20CAD%20knowledge.%20Experiments%20show%20that%20CAD-Editor%20achieves%0Asuperior%20performance%20both%20quantitatively%20and%20qualitatively.%20The%20code%20is%0Aavailable%20at%20%5Curl%20%7Bhttps%3A//github.com/microsoft/CAD-Editor%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03997v2&entry.124074799=Read"},
{"title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled\n  Representation", "author": "Hazal Mogultay Ozcan and Sinan Kalkan and Fatos T. Yarman-Vural", "abstract": "  In this paper, we propose a novel model called Learnable VAE (L-VAE), which\nlearns a disentangled representation together with the hyperparameters of the\ncost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein\nthe hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the\nlimitations of \\b{eta}-VAE by learning the relative weights of the terms in the\nloss function to control the dynamic trade-off between disentanglement and\nreconstruction losses. In the proposed model, the weight of the loss terms and\nthe parameters of the model architecture are learned concurrently. An\nadditional regularization term is added to the loss function to prevent bias\ntowards either reconstruction or disentanglement losses. Experimental analyses\nshow that the proposed L-VAE finds an effective balance between reconstruction\nfidelity and disentangling the latent dimensions. Comparisons of the proposed\nL-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on\ndatasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that\nL-VAE consistently provides the best or the second best performances measured\nby a set of disentanglement metrics. Moreover, qualitative experiments on\nCelebA dataset, confirm the success of the L-VAE model for disentangling the\nfacial attributes.\n", "link": "http://arxiv.org/abs/2507.02619v1", "date": "2025-07-03", "relevancy": 2.1798, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5676}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5291}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-VAE%3A%20Variational%20Auto-Encoder%20with%20Learnable%20Beta%20for%20Disentangled%0A%20%20Representation&body=Title%3A%20L-VAE%3A%20Variational%20Auto-Encoder%20with%20Learnable%20Beta%20for%20Disentangled%0A%20%20Representation%0AAuthor%3A%20Hazal%20Mogultay%20Ozcan%20and%20Sinan%20Kalkan%20and%20Fatos%20T.%20Yarman-Vural%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%20called%20Learnable%20VAE%20%28L-VAE%29%2C%20which%0Alearns%20a%20disentangled%20representation%20together%20with%20the%20hyperparameters%20of%20the%0Acost%20function.%20L-VAE%20can%20be%20considered%20as%20an%20extension%20of%20%5Cb%7Beta%7D-VAE%2C%20wherein%0Athe%20hyperparameter%2C%20%5Cb%7Beta%7D%2C%20is%20empirically%20adjusted.%20L-VAE%20mitigates%20the%0Alimitations%20of%20%5Cb%7Beta%7D-VAE%20by%20learning%20the%20relative%20weights%20of%20the%20terms%20in%20the%0Aloss%20function%20to%20control%20the%20dynamic%20trade-off%20between%20disentanglement%20and%0Areconstruction%20losses.%20In%20the%20proposed%20model%2C%20the%20weight%20of%20the%20loss%20terms%20and%0Athe%20parameters%20of%20the%20model%20architecture%20are%20learned%20concurrently.%20An%0Aadditional%20regularization%20term%20is%20added%20to%20the%20loss%20function%20to%20prevent%20bias%0Atowards%20either%20reconstruction%20or%20disentanglement%20losses.%20Experimental%20analyses%0Ashow%20that%20the%20proposed%20L-VAE%20finds%20an%20effective%20balance%20between%20reconstruction%0Afidelity%20and%20disentangling%20the%20latent%20dimensions.%20Comparisons%20of%20the%20proposed%0AL-VAE%20against%20%5Cb%7Beta%7D-VAE%2C%20VAE%2C%20ControlVAE%2C%20DynamicVAE%2C%20and%20%7B%5Csigma%7D-VAE%20on%0Adatasets%2C%20such%20as%20dSprites%2C%20MPI3D-complex%2C%20Falcor3D%2C%20and%20Isaac3D%20reveals%20that%0AL-VAE%20consistently%20provides%20the%20best%20or%20the%20second%20best%20performances%20measured%0Aby%20a%20set%20of%20disentanglement%20metrics.%20Moreover%2C%20qualitative%20experiments%20on%0ACelebA%20dataset%2C%20confirm%20the%20success%20of%20the%20L-VAE%20model%20for%20disentangling%20the%0Afacial%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-VAE%253A%2520Variational%2520Auto-Encoder%2520with%2520Learnable%2520Beta%2520for%2520Disentangled%250A%2520%2520Representation%26entry.906535625%3DHazal%2520Mogultay%2520Ozcan%2520and%2520Sinan%2520Kalkan%2520and%2520Fatos%2520T.%2520Yarman-Vural%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520model%2520called%2520Learnable%2520VAE%2520%2528L-VAE%2529%252C%2520which%250Alearns%2520a%2520disentangled%2520representation%2520together%2520with%2520the%2520hyperparameters%2520of%2520the%250Acost%2520function.%2520L-VAE%2520can%2520be%2520considered%2520as%2520an%2520extension%2520of%2520%255Cb%257Beta%257D-VAE%252C%2520wherein%250Athe%2520hyperparameter%252C%2520%255Cb%257Beta%257D%252C%2520is%2520empirically%2520adjusted.%2520L-VAE%2520mitigates%2520the%250Alimitations%2520of%2520%255Cb%257Beta%257D-VAE%2520by%2520learning%2520the%2520relative%2520weights%2520of%2520the%2520terms%2520in%2520the%250Aloss%2520function%2520to%2520control%2520the%2520dynamic%2520trade-off%2520between%2520disentanglement%2520and%250Areconstruction%2520losses.%2520In%2520the%2520proposed%2520model%252C%2520the%2520weight%2520of%2520the%2520loss%2520terms%2520and%250Athe%2520parameters%2520of%2520the%2520model%2520architecture%2520are%2520learned%2520concurrently.%2520An%250Aadditional%2520regularization%2520term%2520is%2520added%2520to%2520the%2520loss%2520function%2520to%2520prevent%2520bias%250Atowards%2520either%2520reconstruction%2520or%2520disentanglement%2520losses.%2520Experimental%2520analyses%250Ashow%2520that%2520the%2520proposed%2520L-VAE%2520finds%2520an%2520effective%2520balance%2520between%2520reconstruction%250Afidelity%2520and%2520disentangling%2520the%2520latent%2520dimensions.%2520Comparisons%2520of%2520the%2520proposed%250AL-VAE%2520against%2520%255Cb%257Beta%257D-VAE%252C%2520VAE%252C%2520ControlVAE%252C%2520DynamicVAE%252C%2520and%2520%257B%255Csigma%257D-VAE%2520on%250Adatasets%252C%2520such%2520as%2520dSprites%252C%2520MPI3D-complex%252C%2520Falcor3D%252C%2520and%2520Isaac3D%2520reveals%2520that%250AL-VAE%2520consistently%2520provides%2520the%2520best%2520or%2520the%2520second%2520best%2520performances%2520measured%250Aby%2520a%2520set%2520of%2520disentanglement%2520metrics.%2520Moreover%252C%2520qualitative%2520experiments%2520on%250ACelebA%2520dataset%252C%2520confirm%2520the%2520success%2520of%2520the%2520L-VAE%2520model%2520for%2520disentangling%2520the%250Afacial%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-VAE%3A%20Variational%20Auto-Encoder%20with%20Learnable%20Beta%20for%20Disentangled%0A%20%20Representation&entry.906535625=Hazal%20Mogultay%20Ozcan%20and%20Sinan%20Kalkan%20and%20Fatos%20T.%20Yarman-Vural&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%20called%20Learnable%20VAE%20%28L-VAE%29%2C%20which%0Alearns%20a%20disentangled%20representation%20together%20with%20the%20hyperparameters%20of%20the%0Acost%20function.%20L-VAE%20can%20be%20considered%20as%20an%20extension%20of%20%5Cb%7Beta%7D-VAE%2C%20wherein%0Athe%20hyperparameter%2C%20%5Cb%7Beta%7D%2C%20is%20empirically%20adjusted.%20L-VAE%20mitigates%20the%0Alimitations%20of%20%5Cb%7Beta%7D-VAE%20by%20learning%20the%20relative%20weights%20of%20the%20terms%20in%20the%0Aloss%20function%20to%20control%20the%20dynamic%20trade-off%20between%20disentanglement%20and%0Areconstruction%20losses.%20In%20the%20proposed%20model%2C%20the%20weight%20of%20the%20loss%20terms%20and%0Athe%20parameters%20of%20the%20model%20architecture%20are%20learned%20concurrently.%20An%0Aadditional%20regularization%20term%20is%20added%20to%20the%20loss%20function%20to%20prevent%20bias%0Atowards%20either%20reconstruction%20or%20disentanglement%20losses.%20Experimental%20analyses%0Ashow%20that%20the%20proposed%20L-VAE%20finds%20an%20effective%20balance%20between%20reconstruction%0Afidelity%20and%20disentangling%20the%20latent%20dimensions.%20Comparisons%20of%20the%20proposed%0AL-VAE%20against%20%5Cb%7Beta%7D-VAE%2C%20VAE%2C%20ControlVAE%2C%20DynamicVAE%2C%20and%20%7B%5Csigma%7D-VAE%20on%0Adatasets%2C%20such%20as%20dSprites%2C%20MPI3D-complex%2C%20Falcor3D%2C%20and%20Isaac3D%20reveals%20that%0AL-VAE%20consistently%20provides%20the%20best%20or%20the%20second%20best%20performances%20measured%0Aby%20a%20set%20of%20disentanglement%20metrics.%20Moreover%2C%20qualitative%20experiments%20on%0ACelebA%20dataset%2C%20confirm%20the%20success%20of%20the%20L-VAE%20model%20for%20disentangling%20the%0Afacial%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02619v1&entry.124074799=Read"},
{"title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image\n  Detection via Multimodal Large Language Models", "author": "Ziyin Zhou and Yunpeng Luo and Yuanchen Wu and Ke Sun and Jiayi Ji and Ke Yan and Shouhong Ding and Xiaoshuai Sun and Yunsheng Wu and Rongrong Ji", "abstract": "  The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.\n", "link": "http://arxiv.org/abs/2507.02664v1", "date": "2025-07-03", "relevancy": 2.166, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5722}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5388}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models&body=Title%3A%20AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Ziyin%20Zhou%20and%20Yunpeng%20Luo%20and%20Yuanchen%20Wu%20and%20Ke%20Sun%20and%20Jiayi%20Ji%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Xiaoshuai%20Sun%20and%20Yunsheng%20Wu%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%20has%20led%20to%0Athe%20misuse%20of%20highly%20realistic%20AI-generated%20images%20%28AIGI%29%20in%20spreading%0Amisinformation%2C%20posing%20a%20threat%20to%20public%20information%20security.%20Although%0Aexisting%20AIGI%20detection%20techniques%20are%20generally%20effective%2C%20they%20face%20two%0Aissues%3A%201%29%20a%20lack%20of%20human-verifiable%20explanations%2C%20and%202%29%20a%20lack%20of%0Ageneralization%20in%20the%20latest%20generation%20technology.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20large-scale%20and%20comprehensive%20dataset%2C%20Holmes-Set%2C%20which%20includes%0Athe%20Holmes-SFTSet%2C%20an%20instruction-tuning%20dataset%20with%20explanations%20on%20whether%0Aimages%20are%20AI-generated%2C%20and%20the%20Holmes-DPOSet%2C%20a%20human-aligned%20preference%0Adataset.%20Our%20work%20introduces%20an%20efficient%20data%20annotation%20method%20called%20the%0AMulti-Expert%20Jury%2C%20enhancing%20data%20generation%20through%20structured%20MLLM%0Aexplanations%20and%20quality%20control%20via%20cross-model%20evaluation%2C%20expert%20defect%0Afiltering%2C%20and%20human%20preference%20modification.%20In%20addition%2C%20we%20propose%20Holmes%0APipeline%2C%20a%20meticulously%20designed%20three-stage%20training%20framework%20comprising%0Avisual%20expert%20pre-training%2C%20supervised%20fine-tuning%2C%20and%20direct%20preference%0Aoptimization.%20Holmes%20Pipeline%20adapts%20multimodal%20large%20language%20models%20%28MLLMs%29%0Afor%20AIGI%20detection%20while%20generating%20human-verifiable%20and%20human-aligned%0Aexplanations%2C%20ultimately%20yielding%20our%20model%20AIGI-Holmes.%20During%20the%20inference%0Astage%2C%20we%20introduce%20a%20collaborative%20decoding%20strategy%20that%20integrates%20the%20model%0Aperception%20of%20the%20visual%20expert%20with%20the%20semantic%20reasoning%20of%20MLLMs%2C%20further%0Aenhancing%20the%20generalization%20capabilities.%20Extensive%20experiments%20on%20three%0Abenchmarks%20validate%20the%20effectiveness%20of%20our%20AIGI-Holmes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIGI-Holmes%253A%2520Towards%2520Explainable%2520and%2520Generalizable%2520AI-Generated%2520Image%250A%2520%2520Detection%2520via%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DZiyin%2520Zhou%2520and%2520Yunpeng%2520Luo%2520and%2520Yuanchen%2520Wu%2520and%2520Ke%2520Sun%2520and%2520Jiayi%2520Ji%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%2520and%2520Xiaoshuai%2520Sun%2520and%2520Yunsheng%2520Wu%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520AI-generated%2520content%2520%2528AIGC%2529%2520technology%2520has%2520led%2520to%250Athe%2520misuse%2520of%2520highly%2520realistic%2520AI-generated%2520images%2520%2528AIGI%2529%2520in%2520spreading%250Amisinformation%252C%2520posing%2520a%2520threat%2520to%2520public%2520information%2520security.%2520Although%250Aexisting%2520AIGI%2520detection%2520techniques%2520are%2520generally%2520effective%252C%2520they%2520face%2520two%250Aissues%253A%25201%2529%2520a%2520lack%2520of%2520human-verifiable%2520explanations%252C%2520and%25202%2529%2520a%2520lack%2520of%250Ageneralization%2520in%2520the%2520latest%2520generation%2520technology.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520large-scale%2520and%2520comprehensive%2520dataset%252C%2520Holmes-Set%252C%2520which%2520includes%250Athe%2520Holmes-SFTSet%252C%2520an%2520instruction-tuning%2520dataset%2520with%2520explanations%2520on%2520whether%250Aimages%2520are%2520AI-generated%252C%2520and%2520the%2520Holmes-DPOSet%252C%2520a%2520human-aligned%2520preference%250Adataset.%2520Our%2520work%2520introduces%2520an%2520efficient%2520data%2520annotation%2520method%2520called%2520the%250AMulti-Expert%2520Jury%252C%2520enhancing%2520data%2520generation%2520through%2520structured%2520MLLM%250Aexplanations%2520and%2520quality%2520control%2520via%2520cross-model%2520evaluation%252C%2520expert%2520defect%250Afiltering%252C%2520and%2520human%2520preference%2520modification.%2520In%2520addition%252C%2520we%2520propose%2520Holmes%250APipeline%252C%2520a%2520meticulously%2520designed%2520three-stage%2520training%2520framework%2520comprising%250Avisual%2520expert%2520pre-training%252C%2520supervised%2520fine-tuning%252C%2520and%2520direct%2520preference%250Aoptimization.%2520Holmes%2520Pipeline%2520adapts%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Afor%2520AIGI%2520detection%2520while%2520generating%2520human-verifiable%2520and%2520human-aligned%250Aexplanations%252C%2520ultimately%2520yielding%2520our%2520model%2520AIGI-Holmes.%2520During%2520the%2520inference%250Astage%252C%2520we%2520introduce%2520a%2520collaborative%2520decoding%2520strategy%2520that%2520integrates%2520the%2520model%250Aperception%2520of%2520the%2520visual%2520expert%2520with%2520the%2520semantic%2520reasoning%2520of%2520MLLMs%252C%2520further%250Aenhancing%2520the%2520generalization%2520capabilities.%2520Extensive%2520experiments%2520on%2520three%250Abenchmarks%2520validate%2520the%2520effectiveness%2520of%2520our%2520AIGI-Holmes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models&entry.906535625=Ziyin%20Zhou%20and%20Yunpeng%20Luo%20and%20Yuanchen%20Wu%20and%20Ke%20Sun%20and%20Jiayi%20Ji%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Xiaoshuai%20Sun%20and%20Yunsheng%20Wu%20and%20Rongrong%20Ji&entry.1292438233=%20%20The%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%20has%20led%20to%0Athe%20misuse%20of%20highly%20realistic%20AI-generated%20images%20%28AIGI%29%20in%20spreading%0Amisinformation%2C%20posing%20a%20threat%20to%20public%20information%20security.%20Although%0Aexisting%20AIGI%20detection%20techniques%20are%20generally%20effective%2C%20they%20face%20two%0Aissues%3A%201%29%20a%20lack%20of%20human-verifiable%20explanations%2C%20and%202%29%20a%20lack%20of%0Ageneralization%20in%20the%20latest%20generation%20technology.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20large-scale%20and%20comprehensive%20dataset%2C%20Holmes-Set%2C%20which%20includes%0Athe%20Holmes-SFTSet%2C%20an%20instruction-tuning%20dataset%20with%20explanations%20on%20whether%0Aimages%20are%20AI-generated%2C%20and%20the%20Holmes-DPOSet%2C%20a%20human-aligned%20preference%0Adataset.%20Our%20work%20introduces%20an%20efficient%20data%20annotation%20method%20called%20the%0AMulti-Expert%20Jury%2C%20enhancing%20data%20generation%20through%20structured%20MLLM%0Aexplanations%20and%20quality%20control%20via%20cross-model%20evaluation%2C%20expert%20defect%0Afiltering%2C%20and%20human%20preference%20modification.%20In%20addition%2C%20we%20propose%20Holmes%0APipeline%2C%20a%20meticulously%20designed%20three-stage%20training%20framework%20comprising%0Avisual%20expert%20pre-training%2C%20supervised%20fine-tuning%2C%20and%20direct%20preference%0Aoptimization.%20Holmes%20Pipeline%20adapts%20multimodal%20large%20language%20models%20%28MLLMs%29%0Afor%20AIGI%20detection%20while%20generating%20human-verifiable%20and%20human-aligned%0Aexplanations%2C%20ultimately%20yielding%20our%20model%20AIGI-Holmes.%20During%20the%20inference%0Astage%2C%20we%20introduce%20a%20collaborative%20decoding%20strategy%20that%20integrates%20the%20model%0Aperception%20of%20the%20visual%20expert%20with%20the%20semantic%20reasoning%20of%20MLLMs%2C%20further%0Aenhancing%20the%20generalization%20capabilities.%20Extensive%20experiments%20on%20three%0Abenchmarks%20validate%20the%20effectiveness%20of%20our%20AIGI-Holmes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02664v1&entry.124074799=Read"},
{"title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding", "author": "Ramchalam Kinattinkara Ramakrishnan and Zhaocong Yuan and Shaojie Zhuo and Chen Feng and Yicheng Lin and Chenzheng Su and Xiaopeng Zhang", "abstract": "  Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.\n", "link": "http://arxiv.org/abs/2507.02659v1", "date": "2025-07-03", "relevancy": 2.157, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding&body=Title%3A%20OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding%0AAuthor%3A%20Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Shaojie%20Zhuo%20and%20Chen%20Feng%20and%20Yicheng%20Lin%20and%20Chenzheng%20Su%20and%20Xiaopeng%20Zhang%0AAbstract%3A%20%20%20Speculative%20decoding%20generally%20dictates%20having%20a%20small%2C%20efficient%20draft%20model%0Athat%20is%20either%20pretrained%20or%20distilled%20offline%20to%20a%20particular%20target%20model%0Aseries%2C%20for%20instance%2C%20Llama%20or%20Qwen%20models.%20However%2C%20within%20online%20deployment%0Asettings%2C%20there%20are%20two%20major%20challenges%3A%201%29%20usage%20of%20a%20target%20model%20that%20is%0Aincompatible%20with%20the%20draft%20model%3B%202%29%20expectation%20of%20latency%20improvements%20over%0Ausage%20and%20time.%20In%20this%20work%2C%20we%20propose%20OmniDraft%2C%20a%20unified%20framework%20that%0Aenables%20a%20single%20draft%20model%20to%20operate%20with%20any%20target%20model%20and%20adapt%0Adynamically%20to%20user%20data.%20We%20introduce%20an%20online%20n-gram%20cache%20with%20hybrid%0Adistillation%20fine-tuning%20to%20address%20the%20cross-vocabulary%20mismatch%20across%20draft%0Aand%20target%20models%3B%20and%20further%20improve%20decoding%20speed%20by%20leveraging%20adaptive%0Adrafting%20techniques.%20OmniDraft%20is%20particularly%20suitable%20for%20on-device%20LLM%0Aapplications%20where%20model%20cost%2C%20efficiency%20and%20user%20customization%20are%20the%20major%0Apoints%20of%20contention.%20This%20further%20highlights%20the%20need%20to%20tackle%20the%20above%0Achallenges%20and%20motivates%20the%20%5Ctextit%7B%60%60one%20drafter%20for%20all%27%27%7D%20paradigm.%20We%0Ashowcase%20the%20proficiency%20of%20the%20OmniDraft%20framework%20by%20performing%20online%0Alearning%20on%20math%20reasoning%2C%20coding%20and%20text%20generation%20tasks.%20Notably%2C%0AOmniDraft%20enables%20a%20single%20Llama-68M%20model%20to%20pair%20with%20various%20target%20models%0Aincluding%20Vicuna-7B%2C%20Qwen2-7B%20and%20Llama3-8B%20models%20for%20speculative%20decoding%3B%0Aand%20additionally%20provides%20up%20to%201.5-2x%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDraft%253A%2520A%2520Cross-vocabulary%252C%2520Online%2520Adaptive%2520Drafter%2520for%2520On-device%250A%2520%2520Speculative%2520Decoding%26entry.906535625%3DRamchalam%2520Kinattinkara%2520Ramakrishnan%2520and%2520Zhaocong%2520Yuan%2520and%2520Shaojie%2520Zhuo%2520and%2520Chen%2520Feng%2520and%2520Yicheng%2520Lin%2520and%2520Chenzheng%2520Su%2520and%2520Xiaopeng%2520Zhang%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520generally%2520dictates%2520having%2520a%2520small%252C%2520efficient%2520draft%2520model%250Athat%2520is%2520either%2520pretrained%2520or%2520distilled%2520offline%2520to%2520a%2520particular%2520target%2520model%250Aseries%252C%2520for%2520instance%252C%2520Llama%2520or%2520Qwen%2520models.%2520However%252C%2520within%2520online%2520deployment%250Asettings%252C%2520there%2520are%2520two%2520major%2520challenges%253A%25201%2529%2520usage%2520of%2520a%2520target%2520model%2520that%2520is%250Aincompatible%2520with%2520the%2520draft%2520model%253B%25202%2529%2520expectation%2520of%2520latency%2520improvements%2520over%250Ausage%2520and%2520time.%2520In%2520this%2520work%252C%2520we%2520propose%2520OmniDraft%252C%2520a%2520unified%2520framework%2520that%250Aenables%2520a%2520single%2520draft%2520model%2520to%2520operate%2520with%2520any%2520target%2520model%2520and%2520adapt%250Adynamically%2520to%2520user%2520data.%2520We%2520introduce%2520an%2520online%2520n-gram%2520cache%2520with%2520hybrid%250Adistillation%2520fine-tuning%2520to%2520address%2520the%2520cross-vocabulary%2520mismatch%2520across%2520draft%250Aand%2520target%2520models%253B%2520and%2520further%2520improve%2520decoding%2520speed%2520by%2520leveraging%2520adaptive%250Adrafting%2520techniques.%2520OmniDraft%2520is%2520particularly%2520suitable%2520for%2520on-device%2520LLM%250Aapplications%2520where%2520model%2520cost%252C%2520efficiency%2520and%2520user%2520customization%2520are%2520the%2520major%250Apoints%2520of%2520contention.%2520This%2520further%2520highlights%2520the%2520need%2520to%2520tackle%2520the%2520above%250Achallenges%2520and%2520motivates%2520the%2520%255Ctextit%257B%2560%2560one%2520drafter%2520for%2520all%2527%2527%257D%2520paradigm.%2520We%250Ashowcase%2520the%2520proficiency%2520of%2520the%2520OmniDraft%2520framework%2520by%2520performing%2520online%250Alearning%2520on%2520math%2520reasoning%252C%2520coding%2520and%2520text%2520generation%2520tasks.%2520Notably%252C%250AOmniDraft%2520enables%2520a%2520single%2520Llama-68M%2520model%2520to%2520pair%2520with%2520various%2520target%2520models%250Aincluding%2520Vicuna-7B%252C%2520Qwen2-7B%2520and%2520Llama3-8B%2520models%2520for%2520speculative%2520decoding%253B%250Aand%2520additionally%2520provides%2520up%2520to%25201.5-2x%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding&entry.906535625=Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Shaojie%20Zhuo%20and%20Chen%20Feng%20and%20Yicheng%20Lin%20and%20Chenzheng%20Su%20and%20Xiaopeng%20Zhang&entry.1292438233=%20%20Speculative%20decoding%20generally%20dictates%20having%20a%20small%2C%20efficient%20draft%20model%0Athat%20is%20either%20pretrained%20or%20distilled%20offline%20to%20a%20particular%20target%20model%0Aseries%2C%20for%20instance%2C%20Llama%20or%20Qwen%20models.%20However%2C%20within%20online%20deployment%0Asettings%2C%20there%20are%20two%20major%20challenges%3A%201%29%20usage%20of%20a%20target%20model%20that%20is%0Aincompatible%20with%20the%20draft%20model%3B%202%29%20expectation%20of%20latency%20improvements%20over%0Ausage%20and%20time.%20In%20this%20work%2C%20we%20propose%20OmniDraft%2C%20a%20unified%20framework%20that%0Aenables%20a%20single%20draft%20model%20to%20operate%20with%20any%20target%20model%20and%20adapt%0Adynamically%20to%20user%20data.%20We%20introduce%20an%20online%20n-gram%20cache%20with%20hybrid%0Adistillation%20fine-tuning%20to%20address%20the%20cross-vocabulary%20mismatch%20across%20draft%0Aand%20target%20models%3B%20and%20further%20improve%20decoding%20speed%20by%20leveraging%20adaptive%0Adrafting%20techniques.%20OmniDraft%20is%20particularly%20suitable%20for%20on-device%20LLM%0Aapplications%20where%20model%20cost%2C%20efficiency%20and%20user%20customization%20are%20the%20major%0Apoints%20of%20contention.%20This%20further%20highlights%20the%20need%20to%20tackle%20the%20above%0Achallenges%20and%20motivates%20the%20%5Ctextit%7B%60%60one%20drafter%20for%20all%27%27%7D%20paradigm.%20We%0Ashowcase%20the%20proficiency%20of%20the%20OmniDraft%20framework%20by%20performing%20online%0Alearning%20on%20math%20reasoning%2C%20coding%20and%20text%20generation%20tasks.%20Notably%2C%0AOmniDraft%20enables%20a%20single%20Llama-68M%20model%20to%20pair%20with%20various%20target%20models%0Aincluding%20Vicuna-7B%2C%20Qwen2-7B%20and%20Llama3-8B%20models%20for%20speculative%20decoding%3B%0Aand%20additionally%20provides%20up%20to%201.5-2x%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02659v1&entry.124074799=Read"},
{"title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding", "author": "Weili Xu and Enxin Song and Wenhao Chai and Xuexiang Wen and Tian Ye and Gaoang Wang", "abstract": "  The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.\n", "link": "http://arxiv.org/abs/2507.02591v1", "date": "2025-07-03", "relevancy": 2.1482, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuroraLong%3A%20Bringing%20RNNs%20Back%20to%20Efficient%20Open-Ended%20Video%0A%20%20Understanding&body=Title%3A%20AuroraLong%3A%20Bringing%20RNNs%20Back%20to%20Efficient%20Open-Ended%20Video%0A%20%20Understanding%0AAuthor%3A%20Weili%20Xu%20and%20Enxin%20Song%20and%20Wenhao%20Chai%20and%20Xuexiang%20Wen%20and%20Tian%20Ye%20and%20Gaoang%20Wang%0AAbstract%3A%20%20%20The%20challenge%20of%20long%20video%20understanding%20lies%20in%20its%20high%20computational%0Acomplexity%20and%20prohibitive%20memory%20cost%2C%20since%20the%20memory%20and%20computation%0Arequired%20by%20transformer-based%20LLMs%20scale%20quadratically%20with%20input%20sequence%0Alength.%20We%20propose%20AuroraLong%20to%20address%20this%20challenge%20by%20replacing%20the%20LLM%0Acomponent%20in%20MLLMs%20with%20a%20linear%20RNN%20language%20model%20that%20handles%20input%20sequence%0Aof%20arbitrary%20length%20with%20constant-size%20hidden%20states.%20To%20further%20increase%0Athroughput%20and%20efficiency%2C%20we%20combine%20visual%20token%20merge%20with%20linear%20RNN%20models%0Aby%20reordering%20the%20visual%20tokens%20by%20their%20sizes%20in%20ascending%20order.%20Despite%0Ahaving%20only%202B%20parameters%20and%20being%20trained%20exclusively%20on%20public%20data%2C%0AAuroraLong%20achieves%20performance%20comparable%20to%20Transformer-based%20models%20of%0Asimilar%20size%20trained%20on%20private%20datasets%20across%20multiple%20video%20benchmarks.%20This%0Ademonstrates%20the%20potential%20of%20efficient%2C%20linear%20RNNs%20to%20democratize%20long%20video%0Aunderstanding%20by%20lowering%20its%20computational%20entry%20barrier.%20To%20our%20best%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20a%20linear%20RNN%20based%20LLM%20backbone%20in%20a%0ALLaVA-like%20model%20for%20open-ended%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuroraLong%253A%2520Bringing%2520RNNs%2520Back%2520to%2520Efficient%2520Open-Ended%2520Video%250A%2520%2520Understanding%26entry.906535625%3DWeili%2520Xu%2520and%2520Enxin%2520Song%2520and%2520Wenhao%2520Chai%2520and%2520Xuexiang%2520Wen%2520and%2520Tian%2520Ye%2520and%2520Gaoang%2520Wang%26entry.1292438233%3D%2520%2520The%2520challenge%2520of%2520long%2520video%2520understanding%2520lies%2520in%2520its%2520high%2520computational%250Acomplexity%2520and%2520prohibitive%2520memory%2520cost%252C%2520since%2520the%2520memory%2520and%2520computation%250Arequired%2520by%2520transformer-based%2520LLMs%2520scale%2520quadratically%2520with%2520input%2520sequence%250Alength.%2520We%2520propose%2520AuroraLong%2520to%2520address%2520this%2520challenge%2520by%2520replacing%2520the%2520LLM%250Acomponent%2520in%2520MLLMs%2520with%2520a%2520linear%2520RNN%2520language%2520model%2520that%2520handles%2520input%2520sequence%250Aof%2520arbitrary%2520length%2520with%2520constant-size%2520hidden%2520states.%2520To%2520further%2520increase%250Athroughput%2520and%2520efficiency%252C%2520we%2520combine%2520visual%2520token%2520merge%2520with%2520linear%2520RNN%2520models%250Aby%2520reordering%2520the%2520visual%2520tokens%2520by%2520their%2520sizes%2520in%2520ascending%2520order.%2520Despite%250Ahaving%2520only%25202B%2520parameters%2520and%2520being%2520trained%2520exclusively%2520on%2520public%2520data%252C%250AAuroraLong%2520achieves%2520performance%2520comparable%2520to%2520Transformer-based%2520models%2520of%250Asimilar%2520size%2520trained%2520on%2520private%2520datasets%2520across%2520multiple%2520video%2520benchmarks.%2520This%250Ademonstrates%2520the%2520potential%2520of%2520efficient%252C%2520linear%2520RNNs%2520to%2520democratize%2520long%2520video%250Aunderstanding%2520by%2520lowering%2520its%2520computational%2520entry%2520barrier.%2520To%2520our%2520best%250Aknowledge%252C%2520we%2520are%2520the%2520first%2520to%2520use%2520a%2520linear%2520RNN%2520based%2520LLM%2520backbone%2520in%2520a%250ALLaVA-like%2520model%2520for%2520open-ended%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuroraLong%3A%20Bringing%20RNNs%20Back%20to%20Efficient%20Open-Ended%20Video%0A%20%20Understanding&entry.906535625=Weili%20Xu%20and%20Enxin%20Song%20and%20Wenhao%20Chai%20and%20Xuexiang%20Wen%20and%20Tian%20Ye%20and%20Gaoang%20Wang&entry.1292438233=%20%20The%20challenge%20of%20long%20video%20understanding%20lies%20in%20its%20high%20computational%0Acomplexity%20and%20prohibitive%20memory%20cost%2C%20since%20the%20memory%20and%20computation%0Arequired%20by%20transformer-based%20LLMs%20scale%20quadratically%20with%20input%20sequence%0Alength.%20We%20propose%20AuroraLong%20to%20address%20this%20challenge%20by%20replacing%20the%20LLM%0Acomponent%20in%20MLLMs%20with%20a%20linear%20RNN%20language%20model%20that%20handles%20input%20sequence%0Aof%20arbitrary%20length%20with%20constant-size%20hidden%20states.%20To%20further%20increase%0Athroughput%20and%20efficiency%2C%20we%20combine%20visual%20token%20merge%20with%20linear%20RNN%20models%0Aby%20reordering%20the%20visual%20tokens%20by%20their%20sizes%20in%20ascending%20order.%20Despite%0Ahaving%20only%202B%20parameters%20and%20being%20trained%20exclusively%20on%20public%20data%2C%0AAuroraLong%20achieves%20performance%20comparable%20to%20Transformer-based%20models%20of%0Asimilar%20size%20trained%20on%20private%20datasets%20across%20multiple%20video%20benchmarks.%20This%0Ademonstrates%20the%20potential%20of%20efficient%2C%20linear%20RNNs%20to%20democratize%20long%20video%0Aunderstanding%20by%20lowering%20its%20computational%20entry%20barrier.%20To%20our%20best%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20a%20linear%20RNN%20based%20LLM%20backbone%20in%20a%0ALLaVA-like%20model%20for%20open-ended%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02591v1&entry.124074799=Read"},
{"title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity\n  Recognition", "author": "Hanyu Liu and Siyao Li and Ying Yu and Yixuan Jiang and Hang Xiao and Jingxi Long and Haotian Tang and Chao Li", "abstract": "  Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.\n", "link": "http://arxiv.org/abs/2503.21843v2", "date": "2025-07-03", "relevancy": 2.1399, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5678}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMD-HAR%3A%20Cross-Modal%20Disentanglement%20for%20Wearable%20Human%20Activity%0A%20%20Recognition&body=Title%3A%20CMD-HAR%3A%20Cross-Modal%20Disentanglement%20for%20Wearable%20Human%20Activity%0A%20%20Recognition%0AAuthor%3A%20Hanyu%20Liu%20and%20Siyao%20Li%20and%20Ying%20Yu%20and%20Yixuan%20Jiang%20and%20Hang%20Xiao%20and%20Jingxi%20Long%20and%20Haotian%20Tang%20and%20Chao%20Li%0AAbstract%3A%20%20%20Human%20Activity%20Recognition%20%28HAR%29%20is%20a%20fundamental%20technology%20for%20numerous%0Ahuman%20-%20centered%20intelligent%20applications.%20Although%20deep%20learning%20methods%20have%0Abeen%20utilized%20to%20accelerate%20feature%20extraction%2C%20issues%20such%20as%20multimodal%20data%0Amixing%2C%20activity%20heterogeneity%2C%20and%20complex%20model%20deployment%20remain%20largely%0Aunresolved.%20The%20aim%20of%20this%20paper%20is%20to%20address%20issues%20such%20as%20multimodal%20data%0Amixing%2C%20activity%20heterogeneity%2C%20and%20complex%20model%20deployment%20in%20sensor-based%0Ahuman%20activity%20recognition.%20We%20propose%20a%20spatiotemporal%20attention%20modal%0Adecomposition%20alignment%20fusion%20strategy%20to%20tackle%20the%20problem%20of%20the%20mixed%0Adistribution%20of%20sensor%20data.%20Key%20discriminative%20features%20of%20activities%20are%0Acaptured%20through%20cross-modal%20spatio-temporal%20disentangled%20representation%2C%20and%0Agradient%20modulation%20is%20combined%20to%20alleviate%20data%20heterogeneity.%20In%20addition%2C%20a%0Awearable%20deployment%20simulation%20system%20is%20constructed.%20We%20conducted%20experiments%0Aon%20a%20large%20number%20of%20public%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20the%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMD-HAR%253A%2520Cross-Modal%2520Disentanglement%2520for%2520Wearable%2520Human%2520Activity%250A%2520%2520Recognition%26entry.906535625%3DHanyu%2520Liu%2520and%2520Siyao%2520Li%2520and%2520Ying%2520Yu%2520and%2520Yixuan%2520Jiang%2520and%2520Hang%2520Xiao%2520and%2520Jingxi%2520Long%2520and%2520Haotian%2520Tang%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520is%2520a%2520fundamental%2520technology%2520for%2520numerous%250Ahuman%2520-%2520centered%2520intelligent%2520applications.%2520Although%2520deep%2520learning%2520methods%2520have%250Abeen%2520utilized%2520to%2520accelerate%2520feature%2520extraction%252C%2520issues%2520such%2520as%2520multimodal%2520data%250Amixing%252C%2520activity%2520heterogeneity%252C%2520and%2520complex%2520model%2520deployment%2520remain%2520largely%250Aunresolved.%2520The%2520aim%2520of%2520this%2520paper%2520is%2520to%2520address%2520issues%2520such%2520as%2520multimodal%2520data%250Amixing%252C%2520activity%2520heterogeneity%252C%2520and%2520complex%2520model%2520deployment%2520in%2520sensor-based%250Ahuman%2520activity%2520recognition.%2520We%2520propose%2520a%2520spatiotemporal%2520attention%2520modal%250Adecomposition%2520alignment%2520fusion%2520strategy%2520to%2520tackle%2520the%2520problem%2520of%2520the%2520mixed%250Adistribution%2520of%2520sensor%2520data.%2520Key%2520discriminative%2520features%2520of%2520activities%2520are%250Acaptured%2520through%2520cross-modal%2520spatio-temporal%2520disentangled%2520representation%252C%2520and%250Agradient%2520modulation%2520is%2520combined%2520to%2520alleviate%2520data%2520heterogeneity.%2520In%2520addition%252C%2520a%250Awearable%2520deployment%2520simulation%2520system%2520is%2520constructed.%2520We%2520conducted%2520experiments%250Aon%2520a%2520large%2520number%2520of%2520public%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMD-HAR%3A%20Cross-Modal%20Disentanglement%20for%20Wearable%20Human%20Activity%0A%20%20Recognition&entry.906535625=Hanyu%20Liu%20and%20Siyao%20Li%20and%20Ying%20Yu%20and%20Yixuan%20Jiang%20and%20Hang%20Xiao%20and%20Jingxi%20Long%20and%20Haotian%20Tang%20and%20Chao%20Li&entry.1292438233=%20%20Human%20Activity%20Recognition%20%28HAR%29%20is%20a%20fundamental%20technology%20for%20numerous%0Ahuman%20-%20centered%20intelligent%20applications.%20Although%20deep%20learning%20methods%20have%0Abeen%20utilized%20to%20accelerate%20feature%20extraction%2C%20issues%20such%20as%20multimodal%20data%0Amixing%2C%20activity%20heterogeneity%2C%20and%20complex%20model%20deployment%20remain%20largely%0Aunresolved.%20The%20aim%20of%20this%20paper%20is%20to%20address%20issues%20such%20as%20multimodal%20data%0Amixing%2C%20activity%20heterogeneity%2C%20and%20complex%20model%20deployment%20in%20sensor-based%0Ahuman%20activity%20recognition.%20We%20propose%20a%20spatiotemporal%20attention%20modal%0Adecomposition%20alignment%20fusion%20strategy%20to%20tackle%20the%20problem%20of%20the%20mixed%0Adistribution%20of%20sensor%20data.%20Key%20discriminative%20features%20of%20activities%20are%0Acaptured%20through%20cross-modal%20spatio-temporal%20disentangled%20representation%2C%20and%0Agradient%20modulation%20is%20combined%20to%20alleviate%20data%20heterogeneity.%20In%20addition%2C%20a%0Awearable%20deployment%20simulation%20system%20is%20constructed.%20We%20conducted%20experiments%0Aon%20a%20large%20number%20of%20public%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20the%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21843v2&entry.124074799=Read"},
{"title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain\n  Disorder Diagnosis", "author": "Kunyu Zhang and Qiang Li and Shujian Yu", "abstract": "  Recent evidence suggests that modeling higher-order interactions (HOIs) in\nfunctional magnetic resonance imaging (fMRI) data can enhance the diagnostic\naccuracy of machine learning systems. However, effectively extracting and\nutilizing HOIs remains a significant challenge. In this work, we propose\nMvHo-IB, a novel multi-view learning framework that integrates both pairwise\ninteractions and HOIs for diagnostic decision-making, while automatically\ncompressing task-irrelevant redundant information. MvHo-IB introduces several\nkey innovations: (1) a principled method that combines O-information from\ninformation theory with a matrix-based Renyi alpha-order entropy estimator to\nquantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to\neffectively utilize these interactions, and (3) a new multi-view learning\ninformation bottleneck objective to enhance representation learning.\nExperiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves\nstate-of-the-art performance, significantly outperforming previous methods,\nincluding recent hypergraph-based techniques. The implementation of MvHo-IB is\navailable at https://github.com/zky04/MvHo-IB.\n", "link": "http://arxiv.org/abs/2507.02847v1", "date": "2025-07-03", "relevancy": 2.1125, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MvHo-IB%3A%20Multi-View%20Higher-Order%20Information%20Bottleneck%20for%20Brain%0A%20%20Disorder%20Diagnosis&body=Title%3A%20MvHo-IB%3A%20Multi-View%20Higher-Order%20Information%20Bottleneck%20for%20Brain%0A%20%20Disorder%20Diagnosis%0AAuthor%3A%20Kunyu%20Zhang%20and%20Qiang%20Li%20and%20Shujian%20Yu%0AAbstract%3A%20%20%20Recent%20evidence%20suggests%20that%20modeling%20higher-order%20interactions%20%28HOIs%29%20in%0Afunctional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%20can%20enhance%20the%20diagnostic%0Aaccuracy%20of%20machine%20learning%20systems.%20However%2C%20effectively%20extracting%20and%0Autilizing%20HOIs%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20propose%0AMvHo-IB%2C%20a%20novel%20multi-view%20learning%20framework%20that%20integrates%20both%20pairwise%0Ainteractions%20and%20HOIs%20for%20diagnostic%20decision-making%2C%20while%20automatically%0Acompressing%20task-irrelevant%20redundant%20information.%20MvHo-IB%20introduces%20several%0Akey%20innovations%3A%20%281%29%20a%20principled%20method%20that%20combines%20O-information%20from%0Ainformation%20theory%20with%20a%20matrix-based%20Renyi%20alpha-order%20entropy%20estimator%20to%0Aquantify%20and%20extract%20HOIs%2C%20%282%29%20a%20purpose-built%20Brain3DCNN%20encoder%20to%0Aeffectively%20utilize%20these%20interactions%2C%20and%20%283%29%20a%20new%20multi-view%20learning%0Ainformation%20bottleneck%20objective%20to%20enhance%20representation%20learning.%0AExperiments%20on%20three%20benchmark%20fMRI%20datasets%20demonstrate%20that%20MvHo-IB%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20previous%20methods%2C%0Aincluding%20recent%20hypergraph-based%20techniques.%20The%20implementation%20of%20MvHo-IB%20is%0Aavailable%20at%20https%3A//github.com/zky04/MvHo-IB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMvHo-IB%253A%2520Multi-View%2520Higher-Order%2520Information%2520Bottleneck%2520for%2520Brain%250A%2520%2520Disorder%2520Diagnosis%26entry.906535625%3DKunyu%2520Zhang%2520and%2520Qiang%2520Li%2520and%2520Shujian%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520evidence%2520suggests%2520that%2520modeling%2520higher-order%2520interactions%2520%2528HOIs%2529%2520in%250Afunctional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520data%2520can%2520enhance%2520the%2520diagnostic%250Aaccuracy%2520of%2520machine%2520learning%2520systems.%2520However%252C%2520effectively%2520extracting%2520and%250Autilizing%2520HOIs%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%250AMvHo-IB%252C%2520a%2520novel%2520multi-view%2520learning%2520framework%2520that%2520integrates%2520both%2520pairwise%250Ainteractions%2520and%2520HOIs%2520for%2520diagnostic%2520decision-making%252C%2520while%2520automatically%250Acompressing%2520task-irrelevant%2520redundant%2520information.%2520MvHo-IB%2520introduces%2520several%250Akey%2520innovations%253A%2520%25281%2529%2520a%2520principled%2520method%2520that%2520combines%2520O-information%2520from%250Ainformation%2520theory%2520with%2520a%2520matrix-based%2520Renyi%2520alpha-order%2520entropy%2520estimator%2520to%250Aquantify%2520and%2520extract%2520HOIs%252C%2520%25282%2529%2520a%2520purpose-built%2520Brain3DCNN%2520encoder%2520to%250Aeffectively%2520utilize%2520these%2520interactions%252C%2520and%2520%25283%2529%2520a%2520new%2520multi-view%2520learning%250Ainformation%2520bottleneck%2520objective%2520to%2520enhance%2520representation%2520learning.%250AExperiments%2520on%2520three%2520benchmark%2520fMRI%2520datasets%2520demonstrate%2520that%2520MvHo-IB%2520achieves%250Astate-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520previous%2520methods%252C%250Aincluding%2520recent%2520hypergraph-based%2520techniques.%2520The%2520implementation%2520of%2520MvHo-IB%2520is%250Aavailable%2520at%2520https%253A//github.com/zky04/MvHo-IB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MvHo-IB%3A%20Multi-View%20Higher-Order%20Information%20Bottleneck%20for%20Brain%0A%20%20Disorder%20Diagnosis&entry.906535625=Kunyu%20Zhang%20and%20Qiang%20Li%20and%20Shujian%20Yu&entry.1292438233=%20%20Recent%20evidence%20suggests%20that%20modeling%20higher-order%20interactions%20%28HOIs%29%20in%0Afunctional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%20can%20enhance%20the%20diagnostic%0Aaccuracy%20of%20machine%20learning%20systems.%20However%2C%20effectively%20extracting%20and%0Autilizing%20HOIs%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20propose%0AMvHo-IB%2C%20a%20novel%20multi-view%20learning%20framework%20that%20integrates%20both%20pairwise%0Ainteractions%20and%20HOIs%20for%20diagnostic%20decision-making%2C%20while%20automatically%0Acompressing%20task-irrelevant%20redundant%20information.%20MvHo-IB%20introduces%20several%0Akey%20innovations%3A%20%281%29%20a%20principled%20method%20that%20combines%20O-information%20from%0Ainformation%20theory%20with%20a%20matrix-based%20Renyi%20alpha-order%20entropy%20estimator%20to%0Aquantify%20and%20extract%20HOIs%2C%20%282%29%20a%20purpose-built%20Brain3DCNN%20encoder%20to%0Aeffectively%20utilize%20these%20interactions%2C%20and%20%283%29%20a%20new%20multi-view%20learning%0Ainformation%20bottleneck%20objective%20to%20enhance%20representation%20learning.%0AExperiments%20on%20three%20benchmark%20fMRI%20datasets%20demonstrate%20that%20MvHo-IB%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20previous%20methods%2C%0Aincluding%20recent%20hypergraph-based%20techniques.%20The%20implementation%20of%20MvHo-IB%20is%0Aavailable%20at%20https%3A//github.com/zky04/MvHo-IB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02847v1&entry.124074799=Read"},
{"title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover\n  Classification", "author": "Huiling Zheng and Xian Zhong and Bin Liu and Yi Xiao and Bihan Wen and Xiaofeng Li", "abstract": "  The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and\nunderutilized spectral complementarity. Existing methods often fail to decouple\nshared structural features from modality-complementary radiometric attributes,\ncausing feature conflicts and information loss. To address this, we propose\nPhase-Amplitude Decoupling (PAD), a frequency-aware framework that separates\nphase (modality-shared) and amplitude (modality-complementary) components in\nthe Fourier domain, thus reinforcing shared structures while preserving\ncomplementary characteristics to improve fusion quality. Unlike prior\napproaches that overlook the distinct physical properties encoded in frequency\nspectra, PAD is the first to introduce explicit amplitude-phase decoupling for\nmulti-modal fusion. Specifically, PAD comprises two key components: 1) Phase\nSpectrum Correction (PSC), which aligns cross-modal phase features via\nconvolution-guided scaling to enhance geometric consistency; and 2) Amplitude\nSpectrum Fusion (ASF), which dynamically integrates high-frequency and\nlow-frequency patterns using frequency-adaptive multilayer perceptrons,\nleveraging SAR's morphological sensitivity and RGB's spectral richness.\nExtensive experiments on WHU-OPT-SAR and DDHR-SK datasets demonstrate\nstate-of-the-art performance. Our work establishes a new paradigm for\nphysics-aware multi-modal fusion in remote sensing. The code will be available\nat https://github.com/RanFeng2/PAD.\n", "link": "http://arxiv.org/abs/2504.19136v2", "date": "2025-07-03", "relevancy": 2.1125, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAD%3A%20Phase-Amplitude%20Decoupling%20Fusion%20for%20Multi-Modal%20Land%20Cover%0A%20%20Classification&body=Title%3A%20PAD%3A%20Phase-Amplitude%20Decoupling%20Fusion%20for%20Multi-Modal%20Land%20Cover%0A%20%20Classification%0AAuthor%3A%20Huiling%20Zheng%20and%20Xian%20Zhong%20and%20Bin%20Liu%20and%20Yi%20Xiao%20and%20Bihan%20Wen%20and%20Xiaofeng%20Li%0AAbstract%3A%20%20%20The%20fusion%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20and%20RGB%20imagery%20for%20land%20cover%0Aclassification%20remains%20challenging%20due%20to%20modality%20heterogeneity%20and%0Aunderutilized%20spectral%20complementarity.%20Existing%20methods%20often%20fail%20to%20decouple%0Ashared%20structural%20features%20from%20modality-complementary%20radiometric%20attributes%2C%0Acausing%20feature%20conflicts%20and%20information%20loss.%20To%20address%20this%2C%20we%20propose%0APhase-Amplitude%20Decoupling%20%28PAD%29%2C%20a%20frequency-aware%20framework%20that%20separates%0Aphase%20%28modality-shared%29%20and%20amplitude%20%28modality-complementary%29%20components%20in%0Athe%20Fourier%20domain%2C%20thus%20reinforcing%20shared%20structures%20while%20preserving%0Acomplementary%20characteristics%20to%20improve%20fusion%20quality.%20Unlike%20prior%0Aapproaches%20that%20overlook%20the%20distinct%20physical%20properties%20encoded%20in%20frequency%0Aspectra%2C%20PAD%20is%20the%20first%20to%20introduce%20explicit%20amplitude-phase%20decoupling%20for%0Amulti-modal%20fusion.%20Specifically%2C%20PAD%20comprises%20two%20key%20components%3A%201%29%20Phase%0ASpectrum%20Correction%20%28PSC%29%2C%20which%20aligns%20cross-modal%20phase%20features%20via%0Aconvolution-guided%20scaling%20to%20enhance%20geometric%20consistency%3B%20and%202%29%20Amplitude%0ASpectrum%20Fusion%20%28ASF%29%2C%20which%20dynamically%20integrates%20high-frequency%20and%0Alow-frequency%20patterns%20using%20frequency-adaptive%20multilayer%20perceptrons%2C%0Aleveraging%20SAR%27s%20morphological%20sensitivity%20and%20RGB%27s%20spectral%20richness.%0AExtensive%20experiments%20on%20WHU-OPT-SAR%20and%20DDHR-SK%20datasets%20demonstrate%0Astate-of-the-art%20performance.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Aphysics-aware%20multi-modal%20fusion%20in%20remote%20sensing.%20The%20code%20will%20be%20available%0Aat%20https%3A//github.com/RanFeng2/PAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAD%253A%2520Phase-Amplitude%2520Decoupling%2520Fusion%2520for%2520Multi-Modal%2520Land%2520Cover%250A%2520%2520Classification%26entry.906535625%3DHuiling%2520Zheng%2520and%2520Xian%2520Zhong%2520and%2520Bin%2520Liu%2520and%2520Yi%2520Xiao%2520and%2520Bihan%2520Wen%2520and%2520Xiaofeng%2520Li%26entry.1292438233%3D%2520%2520The%2520fusion%2520of%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520and%2520RGB%2520imagery%2520for%2520land%2520cover%250Aclassification%2520remains%2520challenging%2520due%2520to%2520modality%2520heterogeneity%2520and%250Aunderutilized%2520spectral%2520complementarity.%2520Existing%2520methods%2520often%2520fail%2520to%2520decouple%250Ashared%2520structural%2520features%2520from%2520modality-complementary%2520radiometric%2520attributes%252C%250Acausing%2520feature%2520conflicts%2520and%2520information%2520loss.%2520To%2520address%2520this%252C%2520we%2520propose%250APhase-Amplitude%2520Decoupling%2520%2528PAD%2529%252C%2520a%2520frequency-aware%2520framework%2520that%2520separates%250Aphase%2520%2528modality-shared%2529%2520and%2520amplitude%2520%2528modality-complementary%2529%2520components%2520in%250Athe%2520Fourier%2520domain%252C%2520thus%2520reinforcing%2520shared%2520structures%2520while%2520preserving%250Acomplementary%2520characteristics%2520to%2520improve%2520fusion%2520quality.%2520Unlike%2520prior%250Aapproaches%2520that%2520overlook%2520the%2520distinct%2520physical%2520properties%2520encoded%2520in%2520frequency%250Aspectra%252C%2520PAD%2520is%2520the%2520first%2520to%2520introduce%2520explicit%2520amplitude-phase%2520decoupling%2520for%250Amulti-modal%2520fusion.%2520Specifically%252C%2520PAD%2520comprises%2520two%2520key%2520components%253A%25201%2529%2520Phase%250ASpectrum%2520Correction%2520%2528PSC%2529%252C%2520which%2520aligns%2520cross-modal%2520phase%2520features%2520via%250Aconvolution-guided%2520scaling%2520to%2520enhance%2520geometric%2520consistency%253B%2520and%25202%2529%2520Amplitude%250ASpectrum%2520Fusion%2520%2528ASF%2529%252C%2520which%2520dynamically%2520integrates%2520high-frequency%2520and%250Alow-frequency%2520patterns%2520using%2520frequency-adaptive%2520multilayer%2520perceptrons%252C%250Aleveraging%2520SAR%2527s%2520morphological%2520sensitivity%2520and%2520RGB%2527s%2520spectral%2520richness.%250AExtensive%2520experiments%2520on%2520WHU-OPT-SAR%2520and%2520DDHR-SK%2520datasets%2520demonstrate%250Astate-of-the-art%2520performance.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%250Aphysics-aware%2520multi-modal%2520fusion%2520in%2520remote%2520sensing.%2520The%2520code%2520will%2520be%2520available%250Aat%2520https%253A//github.com/RanFeng2/PAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAD%3A%20Phase-Amplitude%20Decoupling%20Fusion%20for%20Multi-Modal%20Land%20Cover%0A%20%20Classification&entry.906535625=Huiling%20Zheng%20and%20Xian%20Zhong%20and%20Bin%20Liu%20and%20Yi%20Xiao%20and%20Bihan%20Wen%20and%20Xiaofeng%20Li&entry.1292438233=%20%20The%20fusion%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20and%20RGB%20imagery%20for%20land%20cover%0Aclassification%20remains%20challenging%20due%20to%20modality%20heterogeneity%20and%0Aunderutilized%20spectral%20complementarity.%20Existing%20methods%20often%20fail%20to%20decouple%0Ashared%20structural%20features%20from%20modality-complementary%20radiometric%20attributes%2C%0Acausing%20feature%20conflicts%20and%20information%20loss.%20To%20address%20this%2C%20we%20propose%0APhase-Amplitude%20Decoupling%20%28PAD%29%2C%20a%20frequency-aware%20framework%20that%20separates%0Aphase%20%28modality-shared%29%20and%20amplitude%20%28modality-complementary%29%20components%20in%0Athe%20Fourier%20domain%2C%20thus%20reinforcing%20shared%20structures%20while%20preserving%0Acomplementary%20characteristics%20to%20improve%20fusion%20quality.%20Unlike%20prior%0Aapproaches%20that%20overlook%20the%20distinct%20physical%20properties%20encoded%20in%20frequency%0Aspectra%2C%20PAD%20is%20the%20first%20to%20introduce%20explicit%20amplitude-phase%20decoupling%20for%0Amulti-modal%20fusion.%20Specifically%2C%20PAD%20comprises%20two%20key%20components%3A%201%29%20Phase%0ASpectrum%20Correction%20%28PSC%29%2C%20which%20aligns%20cross-modal%20phase%20features%20via%0Aconvolution-guided%20scaling%20to%20enhance%20geometric%20consistency%3B%20and%202%29%20Amplitude%0ASpectrum%20Fusion%20%28ASF%29%2C%20which%20dynamically%20integrates%20high-frequency%20and%0Alow-frequency%20patterns%20using%20frequency-adaptive%20multilayer%20perceptrons%2C%0Aleveraging%20SAR%27s%20morphological%20sensitivity%20and%20RGB%27s%20spectral%20richness.%0AExtensive%20experiments%20on%20WHU-OPT-SAR%20and%20DDHR-SK%20datasets%20demonstrate%0Astate-of-the-art%20performance.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Aphysics-aware%20multi-modal%20fusion%20in%20remote%20sensing.%20The%20code%20will%20be%20available%0Aat%20https%3A//github.com/RanFeng2/PAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19136v2&entry.124074799=Read"},
{"title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in\n  Continuous Control", "author": "Zilin Kang and Chenyuan Hu and Yu Luo and Zhecheng Yuan and Ruijie Zheng and Huazhe Xu", "abstract": "  Deep reinforcement learning for continuous control has recently achieved\nimpressive progress. However, existing methods often suffer from primacy bias,\na tendency to overfit early experiences stored in the replay buffer, which\nlimits an RL agent's sample efficiency and generalizability. In contrast,\nhumans are less susceptible to such bias, partly due to infantile amnesia,\nwhere the formation of new neurons disrupts early memory traces, leading to the\nforgetting of initial experiences. Inspired by this dual processes of\nforgetting and growing in neuroscience, in this paper, we propose Forget and\nGrow (FoG), a new deep RL algorithm with two mechanisms introduced. First,\nExperience Replay Decay (ER Decay) \"forgetting early experience\", which\nbalances memory by gradually reducing the influence of early experiences.\nSecond, Network Expansion, \"growing neural capacity\", which enhances agents'\ncapability to exploit the patterns of existing data by dynamically adding new\nparameters during training. Empirical results on four major continuous control\nbenchmarks with more than 40 tasks demonstrate the superior performance of FoG\nagainst SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.\n", "link": "http://arxiv.org/abs/2507.02712v1", "date": "2025-07-03", "relevancy": 2.0847, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5424}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5284}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Forget-and-Grow%20Strategy%20for%20Deep%20Reinforcement%20Learning%20Scaling%20in%0A%20%20Continuous%20Control&body=Title%3A%20A%20Forget-and-Grow%20Strategy%20for%20Deep%20Reinforcement%20Learning%20Scaling%20in%0A%20%20Continuous%20Control%0AAuthor%3A%20Zilin%20Kang%20and%20Chenyuan%20Hu%20and%20Yu%20Luo%20and%20Zhecheng%20Yuan%20and%20Ruijie%20Zheng%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20for%20continuous%20control%20has%20recently%20achieved%0Aimpressive%20progress.%20However%2C%20existing%20methods%20often%20suffer%20from%20primacy%20bias%2C%0Aa%20tendency%20to%20overfit%20early%20experiences%20stored%20in%20the%20replay%20buffer%2C%20which%0Alimits%20an%20RL%20agent%27s%20sample%20efficiency%20and%20generalizability.%20In%20contrast%2C%0Ahumans%20are%20less%20susceptible%20to%20such%20bias%2C%20partly%20due%20to%20infantile%20amnesia%2C%0Awhere%20the%20formation%20of%20new%20neurons%20disrupts%20early%20memory%20traces%2C%20leading%20to%20the%0Aforgetting%20of%20initial%20experiences.%20Inspired%20by%20this%20dual%20processes%20of%0Aforgetting%20and%20growing%20in%20neuroscience%2C%20in%20this%20paper%2C%20we%20propose%20Forget%20and%0AGrow%20%28FoG%29%2C%20a%20new%20deep%20RL%20algorithm%20with%20two%20mechanisms%20introduced.%20First%2C%0AExperience%20Replay%20Decay%20%28ER%20Decay%29%20%22forgetting%20early%20experience%22%2C%20which%0Abalances%20memory%20by%20gradually%20reducing%20the%20influence%20of%20early%20experiences.%0ASecond%2C%20Network%20Expansion%2C%20%22growing%20neural%20capacity%22%2C%20which%20enhances%20agents%27%0Acapability%20to%20exploit%20the%20patterns%20of%20existing%20data%20by%20dynamically%20adding%20new%0Aparameters%20during%20training.%20Empirical%20results%20on%20four%20major%20continuous%20control%0Abenchmarks%20with%20more%20than%2040%20tasks%20demonstrate%20the%20superior%20performance%20of%20FoG%0Aagainst%20SoTA%20existing%20deep%20RL%20algorithms%2C%20including%20BRO%2C%20SimBa%2C%20and%20TD-MPC2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Forget-and-Grow%2520Strategy%2520for%2520Deep%2520Reinforcement%2520Learning%2520Scaling%2520in%250A%2520%2520Continuous%2520Control%26entry.906535625%3DZilin%2520Kang%2520and%2520Chenyuan%2520Hu%2520and%2520Yu%2520Luo%2520and%2520Zhecheng%2520Yuan%2520and%2520Ruijie%2520Zheng%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Deep%2520reinforcement%2520learning%2520for%2520continuous%2520control%2520has%2520recently%2520achieved%250Aimpressive%2520progress.%2520However%252C%2520existing%2520methods%2520often%2520suffer%2520from%2520primacy%2520bias%252C%250Aa%2520tendency%2520to%2520overfit%2520early%2520experiences%2520stored%2520in%2520the%2520replay%2520buffer%252C%2520which%250Alimits%2520an%2520RL%2520agent%2527s%2520sample%2520efficiency%2520and%2520generalizability.%2520In%2520contrast%252C%250Ahumans%2520are%2520less%2520susceptible%2520to%2520such%2520bias%252C%2520partly%2520due%2520to%2520infantile%2520amnesia%252C%250Awhere%2520the%2520formation%2520of%2520new%2520neurons%2520disrupts%2520early%2520memory%2520traces%252C%2520leading%2520to%2520the%250Aforgetting%2520of%2520initial%2520experiences.%2520Inspired%2520by%2520this%2520dual%2520processes%2520of%250Aforgetting%2520and%2520growing%2520in%2520neuroscience%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520Forget%2520and%250AGrow%2520%2528FoG%2529%252C%2520a%2520new%2520deep%2520RL%2520algorithm%2520with%2520two%2520mechanisms%2520introduced.%2520First%252C%250AExperience%2520Replay%2520Decay%2520%2528ER%2520Decay%2529%2520%2522forgetting%2520early%2520experience%2522%252C%2520which%250Abalances%2520memory%2520by%2520gradually%2520reducing%2520the%2520influence%2520of%2520early%2520experiences.%250ASecond%252C%2520Network%2520Expansion%252C%2520%2522growing%2520neural%2520capacity%2522%252C%2520which%2520enhances%2520agents%2527%250Acapability%2520to%2520exploit%2520the%2520patterns%2520of%2520existing%2520data%2520by%2520dynamically%2520adding%2520new%250Aparameters%2520during%2520training.%2520Empirical%2520results%2520on%2520four%2520major%2520continuous%2520control%250Abenchmarks%2520with%2520more%2520than%252040%2520tasks%2520demonstrate%2520the%2520superior%2520performance%2520of%2520FoG%250Aagainst%2520SoTA%2520existing%2520deep%2520RL%2520algorithms%252C%2520including%2520BRO%252C%2520SimBa%252C%2520and%2520TD-MPC2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Forget-and-Grow%20Strategy%20for%20Deep%20Reinforcement%20Learning%20Scaling%20in%0A%20%20Continuous%20Control&entry.906535625=Zilin%20Kang%20and%20Chenyuan%20Hu%20and%20Yu%20Luo%20and%20Zhecheng%20Yuan%20and%20Ruijie%20Zheng%20and%20Huazhe%20Xu&entry.1292438233=%20%20Deep%20reinforcement%20learning%20for%20continuous%20control%20has%20recently%20achieved%0Aimpressive%20progress.%20However%2C%20existing%20methods%20often%20suffer%20from%20primacy%20bias%2C%0Aa%20tendency%20to%20overfit%20early%20experiences%20stored%20in%20the%20replay%20buffer%2C%20which%0Alimits%20an%20RL%20agent%27s%20sample%20efficiency%20and%20generalizability.%20In%20contrast%2C%0Ahumans%20are%20less%20susceptible%20to%20such%20bias%2C%20partly%20due%20to%20infantile%20amnesia%2C%0Awhere%20the%20formation%20of%20new%20neurons%20disrupts%20early%20memory%20traces%2C%20leading%20to%20the%0Aforgetting%20of%20initial%20experiences.%20Inspired%20by%20this%20dual%20processes%20of%0Aforgetting%20and%20growing%20in%20neuroscience%2C%20in%20this%20paper%2C%20we%20propose%20Forget%20and%0AGrow%20%28FoG%29%2C%20a%20new%20deep%20RL%20algorithm%20with%20two%20mechanisms%20introduced.%20First%2C%0AExperience%20Replay%20Decay%20%28ER%20Decay%29%20%22forgetting%20early%20experience%22%2C%20which%0Abalances%20memory%20by%20gradually%20reducing%20the%20influence%20of%20early%20experiences.%0ASecond%2C%20Network%20Expansion%2C%20%22growing%20neural%20capacity%22%2C%20which%20enhances%20agents%27%0Acapability%20to%20exploit%20the%20patterns%20of%20existing%20data%20by%20dynamically%20adding%20new%0Aparameters%20during%20training.%20Empirical%20results%20on%20four%20major%20continuous%20control%0Abenchmarks%20with%20more%20than%2040%20tasks%20demonstrate%20the%20superior%20performance%20of%20FoG%0Aagainst%20SoTA%20existing%20deep%20RL%20algorithms%2C%20including%20BRO%2C%20SimBa%2C%20and%20TD-MPC2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02712v1&entry.124074799=Read"},
{"title": "Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient\n  Extraction of Moving Speakers under Weak Guidance", "author": "Jakob Kienegger and Alina Mannanova and Huajian Fang and Timo Gerkmann", "abstract": "  Recent works on deep non-linear spatially selective filters demonstrate\nexceptional enhancement performance with computationally lightweight\narchitectures for stationary speakers of known directions. However, to maintain\nthis performance in dynamic scenarios, resource-intensive data-driven tracking\nalgorithms become necessary to provide precise spatial guidance conditioned on\nthe initial direction of a target speaker. As this additional computational\noverhead hinders application in resource-constrained scenarios such as\nreal-time speech enhancement, we present a novel strategy utilizing a\nlow-complexity tracking algorithm in the form of a particle filter instead.\nAssuming a causal, sequential processing style, we introduce temporal feedback\nto leverage the enhanced speech signal of the spatially selective filter to\ncompensate for the limited modeling capabilities of the particle filter.\nEvaluation on a synthetic dataset illustrates how the autoregressive interplay\nbetween both algorithms drastically improves tracking accuracy and leads to\nstrong enhancement performance. A listening test with real-world recordings\ncomplements these findings by indicating a clear trend towards our proposed\nself-steering pipeline as preferred choice over comparable methods.\n", "link": "http://arxiv.org/abs/2507.02791v1", "date": "2025-07-03", "relevancy": 2.0838, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Efficient%0A%20%20Extraction%20of%20Moving%20Speakers%20under%20Weak%20Guidance&body=Title%3A%20Self-Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Efficient%0A%20%20Extraction%20of%20Moving%20Speakers%20under%20Weak%20Guidance%0AAuthor%3A%20Jakob%20Kienegger%20and%20Alina%20Mannanova%20and%20Huajian%20Fang%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20Recent%20works%20on%20deep%20non-linear%20spatially%20selective%20filters%20demonstrate%0Aexceptional%20enhancement%20performance%20with%20computationally%20lightweight%0Aarchitectures%20for%20stationary%20speakers%20of%20known%20directions.%20However%2C%20to%20maintain%0Athis%20performance%20in%20dynamic%20scenarios%2C%20resource-intensive%20data-driven%20tracking%0Aalgorithms%20become%20necessary%20to%20provide%20precise%20spatial%20guidance%20conditioned%20on%0Athe%20initial%20direction%20of%20a%20target%20speaker.%20As%20this%20additional%20computational%0Aoverhead%20hinders%20application%20in%20resource-constrained%20scenarios%20such%20as%0Areal-time%20speech%20enhancement%2C%20we%20present%20a%20novel%20strategy%20utilizing%20a%0Alow-complexity%20tracking%20algorithm%20in%20the%20form%20of%20a%20particle%20filter%20instead.%0AAssuming%20a%20causal%2C%20sequential%20processing%20style%2C%20we%20introduce%20temporal%20feedback%0Ato%20leverage%20the%20enhanced%20speech%20signal%20of%20the%20spatially%20selective%20filter%20to%0Acompensate%20for%20the%20limited%20modeling%20capabilities%20of%20the%20particle%20filter.%0AEvaluation%20on%20a%20synthetic%20dataset%20illustrates%20how%20the%20autoregressive%20interplay%0Abetween%20both%20algorithms%20drastically%20improves%20tracking%20accuracy%20and%20leads%20to%0Astrong%20enhancement%20performance.%20A%20listening%20test%20with%20real-world%20recordings%0Acomplements%20these%20findings%20by%20indicating%20a%20clear%20trend%20towards%20our%20proposed%0Aself-steering%20pipeline%20as%20preferred%20choice%20over%20comparable%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Steering%2520Deep%2520Non-Linear%2520Spatially%2520Selective%2520Filters%2520for%2520Efficient%250A%2520%2520Extraction%2520of%2520Moving%2520Speakers%2520under%2520Weak%2520Guidance%26entry.906535625%3DJakob%2520Kienegger%2520and%2520Alina%2520Mannanova%2520and%2520Huajian%2520Fang%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520deep%2520non-linear%2520spatially%2520selective%2520filters%2520demonstrate%250Aexceptional%2520enhancement%2520performance%2520with%2520computationally%2520lightweight%250Aarchitectures%2520for%2520stationary%2520speakers%2520of%2520known%2520directions.%2520However%252C%2520to%2520maintain%250Athis%2520performance%2520in%2520dynamic%2520scenarios%252C%2520resource-intensive%2520data-driven%2520tracking%250Aalgorithms%2520become%2520necessary%2520to%2520provide%2520precise%2520spatial%2520guidance%2520conditioned%2520on%250Athe%2520initial%2520direction%2520of%2520a%2520target%2520speaker.%2520As%2520this%2520additional%2520computational%250Aoverhead%2520hinders%2520application%2520in%2520resource-constrained%2520scenarios%2520such%2520as%250Areal-time%2520speech%2520enhancement%252C%2520we%2520present%2520a%2520novel%2520strategy%2520utilizing%2520a%250Alow-complexity%2520tracking%2520algorithm%2520in%2520the%2520form%2520of%2520a%2520particle%2520filter%2520instead.%250AAssuming%2520a%2520causal%252C%2520sequential%2520processing%2520style%252C%2520we%2520introduce%2520temporal%2520feedback%250Ato%2520leverage%2520the%2520enhanced%2520speech%2520signal%2520of%2520the%2520spatially%2520selective%2520filter%2520to%250Acompensate%2520for%2520the%2520limited%2520modeling%2520capabilities%2520of%2520the%2520particle%2520filter.%250AEvaluation%2520on%2520a%2520synthetic%2520dataset%2520illustrates%2520how%2520the%2520autoregressive%2520interplay%250Abetween%2520both%2520algorithms%2520drastically%2520improves%2520tracking%2520accuracy%2520and%2520leads%2520to%250Astrong%2520enhancement%2520performance.%2520A%2520listening%2520test%2520with%2520real-world%2520recordings%250Acomplements%2520these%2520findings%2520by%2520indicating%2520a%2520clear%2520trend%2520towards%2520our%2520proposed%250Aself-steering%2520pipeline%2520as%2520preferred%2520choice%2520over%2520comparable%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Efficient%0A%20%20Extraction%20of%20Moving%20Speakers%20under%20Weak%20Guidance&entry.906535625=Jakob%20Kienegger%20and%20Alina%20Mannanova%20and%20Huajian%20Fang%20and%20Timo%20Gerkmann&entry.1292438233=%20%20Recent%20works%20on%20deep%20non-linear%20spatially%20selective%20filters%20demonstrate%0Aexceptional%20enhancement%20performance%20with%20computationally%20lightweight%0Aarchitectures%20for%20stationary%20speakers%20of%20known%20directions.%20However%2C%20to%20maintain%0Athis%20performance%20in%20dynamic%20scenarios%2C%20resource-intensive%20data-driven%20tracking%0Aalgorithms%20become%20necessary%20to%20provide%20precise%20spatial%20guidance%20conditioned%20on%0Athe%20initial%20direction%20of%20a%20target%20speaker.%20As%20this%20additional%20computational%0Aoverhead%20hinders%20application%20in%20resource-constrained%20scenarios%20such%20as%0Areal-time%20speech%20enhancement%2C%20we%20present%20a%20novel%20strategy%20utilizing%20a%0Alow-complexity%20tracking%20algorithm%20in%20the%20form%20of%20a%20particle%20filter%20instead.%0AAssuming%20a%20causal%2C%20sequential%20processing%20style%2C%20we%20introduce%20temporal%20feedback%0Ato%20leverage%20the%20enhanced%20speech%20signal%20of%20the%20spatially%20selective%20filter%20to%0Acompensate%20for%20the%20limited%20modeling%20capabilities%20of%20the%20particle%20filter.%0AEvaluation%20on%20a%20synthetic%20dataset%20illustrates%20how%20the%20autoregressive%20interplay%0Abetween%20both%20algorithms%20drastically%20improves%20tracking%20accuracy%20and%20leads%20to%0Astrong%20enhancement%20performance.%20A%20listening%20test%20with%20real-world%20recordings%0Acomplements%20these%20findings%20by%20indicating%20a%20clear%20trend%20towards%20our%20proposed%0Aself-steering%20pipeline%20as%20preferred%20choice%20over%20comparable%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02791v1&entry.124074799=Read"},
{"title": "Weakly Supervised Segmentation Framework for Thyroid Nodule Based on\n  High-confidence Labels and High-rationality Losses", "author": "Jianning Chi and Zelan Li and Geng Lin and MingYang Sun and Xiaosheng Yu", "abstract": "  Weakly supervised segmentation methods can delineate thyroid nodules in\nultrasound images efficiently using training data with coarse labels, but\nsuffer from: 1) low-confidence pseudo-labels that follow topological priors,\nintroducing significant label noise, and 2) low-rationality loss functions that\nrigidly compare segmentation with labels, ignoring discriminative information\nfor nodules with diverse and complex shapes. To solve these issues, we clarify\nthe objective and references for weakly supervised ultrasound image\nsegmentation, presenting a framework with high-confidence pseudo-labels to\nrepresent topological and anatomical information and high-rationality losses to\ncapture multi-level discriminative features. Specifically, we fuse geometric\ntransformations of four-point annotations and MedSAM model results prompted by\nspecific annotations to generate high-confidence box, foreground, and\nbackground labels. Our high-rationality learning strategy includes: 1)\nAlignment loss measuring spatial consistency between segmentation and box\nlabel, and topological continuity within the foreground label, guiding the\nnetwork to perceive nodule location; 2) Contrastive loss pulling features from\nlabeled foreground regions while pushing features from labeled foreground and\nbackground regions, guiding the network to learn nodule and background feature\ndistribution; 3) Prototype correlation loss measuring consistency between\ncorrelation maps derived by comparing features with foreground and background\nprototypes, refining uncertain regions to accurate nodule edges. Experimental\nresults show that our method achieves state-of-the-art performance on the TN3K\nand DDTI datasets. The code is available at\nhttps://github.com/bluehenglee/MLI-MSC.\n", "link": "http://arxiv.org/abs/2502.19707v2", "date": "2025-07-03", "relevancy": 2.0569, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5645}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5104}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Segmentation%20Framework%20for%20Thyroid%20Nodule%20Based%20on%0A%20%20High-confidence%20Labels%20and%20High-rationality%20Losses&body=Title%3A%20Weakly%20Supervised%20Segmentation%20Framework%20for%20Thyroid%20Nodule%20Based%20on%0A%20%20High-confidence%20Labels%20and%20High-rationality%20Losses%0AAuthor%3A%20Jianning%20Chi%20and%20Zelan%20Li%20and%20Geng%20Lin%20and%20MingYang%20Sun%20and%20Xiaosheng%20Yu%0AAbstract%3A%20%20%20Weakly%20supervised%20segmentation%20methods%20can%20delineate%20thyroid%20nodules%20in%0Aultrasound%20images%20efficiently%20using%20training%20data%20with%20coarse%20labels%2C%20but%0Asuffer%20from%3A%201%29%20low-confidence%20pseudo-labels%20that%20follow%20topological%20priors%2C%0Aintroducing%20significant%20label%20noise%2C%20and%202%29%20low-rationality%20loss%20functions%20that%0Arigidly%20compare%20segmentation%20with%20labels%2C%20ignoring%20discriminative%20information%0Afor%20nodules%20with%20diverse%20and%20complex%20shapes.%20To%20solve%20these%20issues%2C%20we%20clarify%0Athe%20objective%20and%20references%20for%20weakly%20supervised%20ultrasound%20image%0Asegmentation%2C%20presenting%20a%20framework%20with%20high-confidence%20pseudo-labels%20to%0Arepresent%20topological%20and%20anatomical%20information%20and%20high-rationality%20losses%20to%0Acapture%20multi-level%20discriminative%20features.%20Specifically%2C%20we%20fuse%20geometric%0Atransformations%20of%20four-point%20annotations%20and%20MedSAM%20model%20results%20prompted%20by%0Aspecific%20annotations%20to%20generate%20high-confidence%20box%2C%20foreground%2C%20and%0Abackground%20labels.%20Our%20high-rationality%20learning%20strategy%20includes%3A%201%29%0AAlignment%20loss%20measuring%20spatial%20consistency%20between%20segmentation%20and%20box%0Alabel%2C%20and%20topological%20continuity%20within%20the%20foreground%20label%2C%20guiding%20the%0Anetwork%20to%20perceive%20nodule%20location%3B%202%29%20Contrastive%20loss%20pulling%20features%20from%0Alabeled%20foreground%20regions%20while%20pushing%20features%20from%20labeled%20foreground%20and%0Abackground%20regions%2C%20guiding%20the%20network%20to%20learn%20nodule%20and%20background%20feature%0Adistribution%3B%203%29%20Prototype%20correlation%20loss%20measuring%20consistency%20between%0Acorrelation%20maps%20derived%20by%20comparing%20features%20with%20foreground%20and%20background%0Aprototypes%2C%20refining%20uncertain%20regions%20to%20accurate%20nodule%20edges.%20Experimental%0Aresults%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20the%20TN3K%0Aand%20DDTI%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bluehenglee/MLI-MSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Segmentation%2520Framework%2520for%2520Thyroid%2520Nodule%2520Based%2520on%250A%2520%2520High-confidence%2520Labels%2520and%2520High-rationality%2520Losses%26entry.906535625%3DJianning%2520Chi%2520and%2520Zelan%2520Li%2520and%2520Geng%2520Lin%2520and%2520MingYang%2520Sun%2520and%2520Xiaosheng%2520Yu%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520segmentation%2520methods%2520can%2520delineate%2520thyroid%2520nodules%2520in%250Aultrasound%2520images%2520efficiently%2520using%2520training%2520data%2520with%2520coarse%2520labels%252C%2520but%250Asuffer%2520from%253A%25201%2529%2520low-confidence%2520pseudo-labels%2520that%2520follow%2520topological%2520priors%252C%250Aintroducing%2520significant%2520label%2520noise%252C%2520and%25202%2529%2520low-rationality%2520loss%2520functions%2520that%250Arigidly%2520compare%2520segmentation%2520with%2520labels%252C%2520ignoring%2520discriminative%2520information%250Afor%2520nodules%2520with%2520diverse%2520and%2520complex%2520shapes.%2520To%2520solve%2520these%2520issues%252C%2520we%2520clarify%250Athe%2520objective%2520and%2520references%2520for%2520weakly%2520supervised%2520ultrasound%2520image%250Asegmentation%252C%2520presenting%2520a%2520framework%2520with%2520high-confidence%2520pseudo-labels%2520to%250Arepresent%2520topological%2520and%2520anatomical%2520information%2520and%2520high-rationality%2520losses%2520to%250Acapture%2520multi-level%2520discriminative%2520features.%2520Specifically%252C%2520we%2520fuse%2520geometric%250Atransformations%2520of%2520four-point%2520annotations%2520and%2520MedSAM%2520model%2520results%2520prompted%2520by%250Aspecific%2520annotations%2520to%2520generate%2520high-confidence%2520box%252C%2520foreground%252C%2520and%250Abackground%2520labels.%2520Our%2520high-rationality%2520learning%2520strategy%2520includes%253A%25201%2529%250AAlignment%2520loss%2520measuring%2520spatial%2520consistency%2520between%2520segmentation%2520and%2520box%250Alabel%252C%2520and%2520topological%2520continuity%2520within%2520the%2520foreground%2520label%252C%2520guiding%2520the%250Anetwork%2520to%2520perceive%2520nodule%2520location%253B%25202%2529%2520Contrastive%2520loss%2520pulling%2520features%2520from%250Alabeled%2520foreground%2520regions%2520while%2520pushing%2520features%2520from%2520labeled%2520foreground%2520and%250Abackground%2520regions%252C%2520guiding%2520the%2520network%2520to%2520learn%2520nodule%2520and%2520background%2520feature%250Adistribution%253B%25203%2529%2520Prototype%2520correlation%2520loss%2520measuring%2520consistency%2520between%250Acorrelation%2520maps%2520derived%2520by%2520comparing%2520features%2520with%2520foreground%2520and%2520background%250Aprototypes%252C%2520refining%2520uncertain%2520regions%2520to%2520accurate%2520nodule%2520edges.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520TN3K%250Aand%2520DDTI%2520datasets.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/bluehenglee/MLI-MSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Segmentation%20Framework%20for%20Thyroid%20Nodule%20Based%20on%0A%20%20High-confidence%20Labels%20and%20High-rationality%20Losses&entry.906535625=Jianning%20Chi%20and%20Zelan%20Li%20and%20Geng%20Lin%20and%20MingYang%20Sun%20and%20Xiaosheng%20Yu&entry.1292438233=%20%20Weakly%20supervised%20segmentation%20methods%20can%20delineate%20thyroid%20nodules%20in%0Aultrasound%20images%20efficiently%20using%20training%20data%20with%20coarse%20labels%2C%20but%0Asuffer%20from%3A%201%29%20low-confidence%20pseudo-labels%20that%20follow%20topological%20priors%2C%0Aintroducing%20significant%20label%20noise%2C%20and%202%29%20low-rationality%20loss%20functions%20that%0Arigidly%20compare%20segmentation%20with%20labels%2C%20ignoring%20discriminative%20information%0Afor%20nodules%20with%20diverse%20and%20complex%20shapes.%20To%20solve%20these%20issues%2C%20we%20clarify%0Athe%20objective%20and%20references%20for%20weakly%20supervised%20ultrasound%20image%0Asegmentation%2C%20presenting%20a%20framework%20with%20high-confidence%20pseudo-labels%20to%0Arepresent%20topological%20and%20anatomical%20information%20and%20high-rationality%20losses%20to%0Acapture%20multi-level%20discriminative%20features.%20Specifically%2C%20we%20fuse%20geometric%0Atransformations%20of%20four-point%20annotations%20and%20MedSAM%20model%20results%20prompted%20by%0Aspecific%20annotations%20to%20generate%20high-confidence%20box%2C%20foreground%2C%20and%0Abackground%20labels.%20Our%20high-rationality%20learning%20strategy%20includes%3A%201%29%0AAlignment%20loss%20measuring%20spatial%20consistency%20between%20segmentation%20and%20box%0Alabel%2C%20and%20topological%20continuity%20within%20the%20foreground%20label%2C%20guiding%20the%0Anetwork%20to%20perceive%20nodule%20location%3B%202%29%20Contrastive%20loss%20pulling%20features%20from%0Alabeled%20foreground%20regions%20while%20pushing%20features%20from%20labeled%20foreground%20and%0Abackground%20regions%2C%20guiding%20the%20network%20to%20learn%20nodule%20and%20background%20feature%0Adistribution%3B%203%29%20Prototype%20correlation%20loss%20measuring%20consistency%20between%0Acorrelation%20maps%20derived%20by%20comparing%20features%20with%20foreground%20and%20background%0Aprototypes%2C%20refining%20uncertain%20regions%20to%20accurate%20nodule%20edges.%20Experimental%0Aresults%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20the%20TN3K%0Aand%20DDTI%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bluehenglee/MLI-MSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19707v2&entry.124074799=Read"},
{"title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided\n  Reinforcement Learning", "author": "Ruiyang Zhou and Shuozhe Li and Amy Zhang and Liu Leqi", "abstract": "  Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most.\n", "link": "http://arxiv.org/abs/2507.02834v1", "date": "2025-07-03", "relevancy": 2.0525, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExPO%3A%20Unlocking%20Hard%20Reasoning%20with%20Self-Explanation-Guided%0A%20%20Reinforcement%20Learning&body=Title%3A%20ExPO%3A%20Unlocking%20Hard%20Reasoning%20with%20Self-Explanation-Guided%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ruiyang%20Zhou%20and%20Shuozhe%20Li%20and%20Amy%20Zhang%20and%20Liu%20Leqi%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20have%20been%20driven%20by%20reinforcement%0Alearning%20%28RL%29-style%20post-training%2C%20which%20improves%20reasoning%20by%20optimizing%20model%0Aoutputs%20based%20on%20reward%20or%20preference%20signals.%20GRPO-style%20approaches%20implement%0Athis%20by%20using%20self-generated%20samples%20labeled%20by%20an%20outcome-based%20verifier.%0AHowever%2C%20these%20methods%20depend%20heavily%20on%20the%20model%27s%20initial%20ability%20to%20produce%0Apositive%20samples.%20They%20primarily%20refine%20what%20the%20model%20already%20knows%0A%28distribution%20sharpening%29%20rather%20than%20enabling%20the%20model%20to%20solve%20problems%0Awhere%20it%20initially%20fails.%20This%20limitation%20is%20especially%20problematic%20in%0Aearly-stage%20RL%20training%20and%20on%20challenging%20reasoning%20tasks%2C%20where%20positive%0Asamples%20are%20unlikely%20to%20be%20generated.%20To%20unlock%20reasoning%20ability%20in%20such%0Asettings%2C%20the%20model%20must%20explore%20new%20reasoning%20trajectories%20beyond%20its%20current%0Aoutput%20distribution.%20Such%20exploration%20requires%20access%20to%20sufficiently%20good%0Apositive%20samples%20to%20guide%20the%20learning.%20While%20expert%20demonstrations%20seem%20like%20a%0Anatural%20solution%2C%20we%20find%20that%20they%20are%20often%20ineffective%20in%20RL%20post-training.%0AInstead%2C%20we%20identify%20two%20key%20properties%20of%20effective%20positive%20samples%3A%20they%0Ashould%20%281%29%20be%20likely%20under%20the%20current%20policy%2C%20and%20%282%29%20increase%20the%20model%27s%0Alikelihood%20of%20predicting%20the%20correct%20answer.%20Based%20on%20these%20insights%2C%20we%0Apropose%20%24%5Ctextbf%7BSelf-Explanation%20Policy%20Optimization%20%28ExPO%29%7D%24-a%20simple%20and%0Amodular%20framework%20that%20generates%20such%20samples%20by%20conditioning%20on%20the%0Aground-truth%20answer.%20ExPO%20enables%20efficient%20exploration%20and%20guides%20the%20model%20to%0Aproduce%20reasoning%20trajectories%20more%20aligned%20with%20its%20policy%20than%20expert-written%0ACoTs%2C%20while%20ensuring%20higher%20quality%20than%20its%20own%20%28incorrect%29%20samples.%0AExperiments%20show%20that%20ExPO%20improves%20both%20learning%20efficiency%20and%20final%0Aperformance%20on%20reasoning%20benchmarks%2C%20surpassing%20expert-demonstration-based%0Amethods%20in%20challenging%20settings%20such%20as%20MATH%20level-5%2C%20where%20the%20model%20initially%0Astruggles%20the%20most.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExPO%253A%2520Unlocking%2520Hard%2520Reasoning%2520with%2520Self-Explanation-Guided%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRuiyang%2520Zhou%2520and%2520Shuozhe%2520Li%2520and%2520Amy%2520Zhang%2520and%2520Liu%2520Leqi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520have%2520been%2520driven%2520by%2520reinforcement%250Alearning%2520%2528RL%2529-style%2520post-training%252C%2520which%2520improves%2520reasoning%2520by%2520optimizing%2520model%250Aoutputs%2520based%2520on%2520reward%2520or%2520preference%2520signals.%2520GRPO-style%2520approaches%2520implement%250Athis%2520by%2520using%2520self-generated%2520samples%2520labeled%2520by%2520an%2520outcome-based%2520verifier.%250AHowever%252C%2520these%2520methods%2520depend%2520heavily%2520on%2520the%2520model%2527s%2520initial%2520ability%2520to%2520produce%250Apositive%2520samples.%2520They%2520primarily%2520refine%2520what%2520the%2520model%2520already%2520knows%250A%2528distribution%2520sharpening%2529%2520rather%2520than%2520enabling%2520the%2520model%2520to%2520solve%2520problems%250Awhere%2520it%2520initially%2520fails.%2520This%2520limitation%2520is%2520especially%2520problematic%2520in%250Aearly-stage%2520RL%2520training%2520and%2520on%2520challenging%2520reasoning%2520tasks%252C%2520where%2520positive%250Asamples%2520are%2520unlikely%2520to%2520be%2520generated.%2520To%2520unlock%2520reasoning%2520ability%2520in%2520such%250Asettings%252C%2520the%2520model%2520must%2520explore%2520new%2520reasoning%2520trajectories%2520beyond%2520its%2520current%250Aoutput%2520distribution.%2520Such%2520exploration%2520requires%2520access%2520to%2520sufficiently%2520good%250Apositive%2520samples%2520to%2520guide%2520the%2520learning.%2520While%2520expert%2520demonstrations%2520seem%2520like%2520a%250Anatural%2520solution%252C%2520we%2520find%2520that%2520they%2520are%2520often%2520ineffective%2520in%2520RL%2520post-training.%250AInstead%252C%2520we%2520identify%2520two%2520key%2520properties%2520of%2520effective%2520positive%2520samples%253A%2520they%250Ashould%2520%25281%2529%2520be%2520likely%2520under%2520the%2520current%2520policy%252C%2520and%2520%25282%2529%2520increase%2520the%2520model%2527s%250Alikelihood%2520of%2520predicting%2520the%2520correct%2520answer.%2520Based%2520on%2520these%2520insights%252C%2520we%250Apropose%2520%2524%255Ctextbf%257BSelf-Explanation%2520Policy%2520Optimization%2520%2528ExPO%2529%257D%2524-a%2520simple%2520and%250Amodular%2520framework%2520that%2520generates%2520such%2520samples%2520by%2520conditioning%2520on%2520the%250Aground-truth%2520answer.%2520ExPO%2520enables%2520efficient%2520exploration%2520and%2520guides%2520the%2520model%2520to%250Aproduce%2520reasoning%2520trajectories%2520more%2520aligned%2520with%2520its%2520policy%2520than%2520expert-written%250ACoTs%252C%2520while%2520ensuring%2520higher%2520quality%2520than%2520its%2520own%2520%2528incorrect%2529%2520samples.%250AExperiments%2520show%2520that%2520ExPO%2520improves%2520both%2520learning%2520efficiency%2520and%2520final%250Aperformance%2520on%2520reasoning%2520benchmarks%252C%2520surpassing%2520expert-demonstration-based%250Amethods%2520in%2520challenging%2520settings%2520such%2520as%2520MATH%2520level-5%252C%2520where%2520the%2520model%2520initially%250Astruggles%2520the%2520most.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExPO%3A%20Unlocking%20Hard%20Reasoning%20with%20Self-Explanation-Guided%0A%20%20Reinforcement%20Learning&entry.906535625=Ruiyang%20Zhou%20and%20Shuozhe%20Li%20and%20Amy%20Zhang%20and%20Liu%20Leqi&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20have%20been%20driven%20by%20reinforcement%0Alearning%20%28RL%29-style%20post-training%2C%20which%20improves%20reasoning%20by%20optimizing%20model%0Aoutputs%20based%20on%20reward%20or%20preference%20signals.%20GRPO-style%20approaches%20implement%0Athis%20by%20using%20self-generated%20samples%20labeled%20by%20an%20outcome-based%20verifier.%0AHowever%2C%20these%20methods%20depend%20heavily%20on%20the%20model%27s%20initial%20ability%20to%20produce%0Apositive%20samples.%20They%20primarily%20refine%20what%20the%20model%20already%20knows%0A%28distribution%20sharpening%29%20rather%20than%20enabling%20the%20model%20to%20solve%20problems%0Awhere%20it%20initially%20fails.%20This%20limitation%20is%20especially%20problematic%20in%0Aearly-stage%20RL%20training%20and%20on%20challenging%20reasoning%20tasks%2C%20where%20positive%0Asamples%20are%20unlikely%20to%20be%20generated.%20To%20unlock%20reasoning%20ability%20in%20such%0Asettings%2C%20the%20model%20must%20explore%20new%20reasoning%20trajectories%20beyond%20its%20current%0Aoutput%20distribution.%20Such%20exploration%20requires%20access%20to%20sufficiently%20good%0Apositive%20samples%20to%20guide%20the%20learning.%20While%20expert%20demonstrations%20seem%20like%20a%0Anatural%20solution%2C%20we%20find%20that%20they%20are%20often%20ineffective%20in%20RL%20post-training.%0AInstead%2C%20we%20identify%20two%20key%20properties%20of%20effective%20positive%20samples%3A%20they%0Ashould%20%281%29%20be%20likely%20under%20the%20current%20policy%2C%20and%20%282%29%20increase%20the%20model%27s%0Alikelihood%20of%20predicting%20the%20correct%20answer.%20Based%20on%20these%20insights%2C%20we%0Apropose%20%24%5Ctextbf%7BSelf-Explanation%20Policy%20Optimization%20%28ExPO%29%7D%24-a%20simple%20and%0Amodular%20framework%20that%20generates%20such%20samples%20by%20conditioning%20on%20the%0Aground-truth%20answer.%20ExPO%20enables%20efficient%20exploration%20and%20guides%20the%20model%20to%0Aproduce%20reasoning%20trajectories%20more%20aligned%20with%20its%20policy%20than%20expert-written%0ACoTs%2C%20while%20ensuring%20higher%20quality%20than%20its%20own%20%28incorrect%29%20samples.%0AExperiments%20show%20that%20ExPO%20improves%20both%20learning%20efficiency%20and%20final%0Aperformance%20on%20reasoning%20benchmarks%2C%20surpassing%20expert-demonstration-based%0Amethods%20in%20challenging%20settings%20such%20as%20MATH%20level-5%2C%20where%20the%20model%20initially%0Astruggles%20the%20most.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02834v1&entry.124074799=Read"},
{"title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason", "author": "Kaiyi Zhang and Ang Lv and Jinpeng Li and Yongbo Wang and Feng Wang and Haoyuan Hu and Rui Yan", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.\n", "link": "http://arxiv.org/abs/2507.02841v1", "date": "2025-07-03", "relevancy": 2.05, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepHint%3A%20Multi-level%20Stepwise%20Hints%20Enhance%20Reinforcement%20Learning%20to%0A%20%20Reason&body=Title%3A%20StepHint%3A%20Multi-level%20Stepwise%20Hints%20Enhance%20Reinforcement%20Learning%20to%0A%20%20Reason%0AAuthor%3A%20Kaiyi%20Zhang%20and%20Ang%20Lv%20and%20Jinpeng%20Li%20and%20Yongbo%20Wang%20and%20Feng%20Wang%20and%20Haoyuan%20Hu%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20is%20a%20promising%20approach%0Afor%20improving%20the%20complex%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20current%20RLVR%20methods%20face%20two%20significant%20challenges%3A%20the%20near-miss%0Areward%20problem%2C%20where%20a%20small%20mistake%20can%20invalidate%20an%20otherwise%20correct%0Areasoning%20process%2C%20greatly%20hindering%20training%20efficiency%3B%20and%20exploration%0Astagnation%2C%20where%20models%20tend%20to%20focus%20on%20solutions%20within%20their%20%60%60comfort%0Azone%2C%27%27%20lacking%20the%20motivation%20to%20explore%20potentially%20more%20effective%0Aalternatives.%20To%20address%20these%20challenges%2C%20we%20propose%20StepHint%2C%20a%20novel%20RLVR%0Aalgorithm%20that%20utilizes%20multi-level%20stepwise%20hints%20to%20help%20models%20explore%20the%0Asolution%20space%20more%20effectively.%20StepHint%20generates%20valid%20reasoning%20chains%20from%0Astronger%20models%20and%20partitions%20these%20chains%20into%20reasoning%20steps%20using%20our%0Aproposed%20adaptive%20partitioning%20method.%20The%20initial%20few%20steps%20are%20used%20as%20hints%2C%0Aand%20simultaneously%2C%20multiple-level%20hints%20%28each%20comprising%20a%20different%20number%20of%0Asteps%29%20are%20provided%20to%20the%20model.%20This%20approach%20directs%20the%20model%27s%20exploration%0Atoward%20a%20promising%20solution%20subspace%20while%20preserving%20its%20flexibility%20for%0Aindependent%20exploration.%20By%20providing%20hints%2C%20StepHint%20mitigates%20the%20near-miss%0Areward%20problem%2C%20thereby%20improving%20training%20efficiency.%20Additionally%2C%20the%0Aexternal%20reasoning%20pathways%20help%20the%20model%20develop%20better%20reasoning%20abilities%2C%0Aenabling%20it%20to%20move%20beyond%20its%20%60%60comfort%20zone%27%27%20and%20mitigate%20exploration%0Astagnation.%20StepHint%20outperforms%20competitive%20RLVR%20enhancement%20methods%20across%0Asix%20mathematical%20benchmarks%2C%20while%20also%20demonstrating%20superior%20generalization%0Aand%20excelling%20over%20baselines%20on%20out-of-domain%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepHint%253A%2520Multi-level%2520Stepwise%2520Hints%2520Enhance%2520Reinforcement%2520Learning%2520to%250A%2520%2520Reason%26entry.906535625%3DKaiyi%2520Zhang%2520and%2520Ang%2520Lv%2520and%2520Jinpeng%2520Li%2520and%2520Yongbo%2520Wang%2520and%2520Feng%2520Wang%2520and%2520Haoyuan%2520Hu%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520is%2520a%2520promising%2520approach%250Afor%2520improving%2520the%2520complex%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520current%2520RLVR%2520methods%2520face%2520two%2520significant%2520challenges%253A%2520the%2520near-miss%250Areward%2520problem%252C%2520where%2520a%2520small%2520mistake%2520can%2520invalidate%2520an%2520otherwise%2520correct%250Areasoning%2520process%252C%2520greatly%2520hindering%2520training%2520efficiency%253B%2520and%2520exploration%250Astagnation%252C%2520where%2520models%2520tend%2520to%2520focus%2520on%2520solutions%2520within%2520their%2520%2560%2560comfort%250Azone%252C%2527%2527%2520lacking%2520the%2520motivation%2520to%2520explore%2520potentially%2520more%2520effective%250Aalternatives.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520StepHint%252C%2520a%2520novel%2520RLVR%250Aalgorithm%2520that%2520utilizes%2520multi-level%2520stepwise%2520hints%2520to%2520help%2520models%2520explore%2520the%250Asolution%2520space%2520more%2520effectively.%2520StepHint%2520generates%2520valid%2520reasoning%2520chains%2520from%250Astronger%2520models%2520and%2520partitions%2520these%2520chains%2520into%2520reasoning%2520steps%2520using%2520our%250Aproposed%2520adaptive%2520partitioning%2520method.%2520The%2520initial%2520few%2520steps%2520are%2520used%2520as%2520hints%252C%250Aand%2520simultaneously%252C%2520multiple-level%2520hints%2520%2528each%2520comprising%2520a%2520different%2520number%2520of%250Asteps%2529%2520are%2520provided%2520to%2520the%2520model.%2520This%2520approach%2520directs%2520the%2520model%2527s%2520exploration%250Atoward%2520a%2520promising%2520solution%2520subspace%2520while%2520preserving%2520its%2520flexibility%2520for%250Aindependent%2520exploration.%2520By%2520providing%2520hints%252C%2520StepHint%2520mitigates%2520the%2520near-miss%250Areward%2520problem%252C%2520thereby%2520improving%2520training%2520efficiency.%2520Additionally%252C%2520the%250Aexternal%2520reasoning%2520pathways%2520help%2520the%2520model%2520develop%2520better%2520reasoning%2520abilities%252C%250Aenabling%2520it%2520to%2520move%2520beyond%2520its%2520%2560%2560comfort%2520zone%2527%2527%2520and%2520mitigate%2520exploration%250Astagnation.%2520StepHint%2520outperforms%2520competitive%2520RLVR%2520enhancement%2520methods%2520across%250Asix%2520mathematical%2520benchmarks%252C%2520while%2520also%2520demonstrating%2520superior%2520generalization%250Aand%2520excelling%2520over%2520baselines%2520on%2520out-of-domain%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepHint%3A%20Multi-level%20Stepwise%20Hints%20Enhance%20Reinforcement%20Learning%20to%0A%20%20Reason&entry.906535625=Kaiyi%20Zhang%20and%20Ang%20Lv%20and%20Jinpeng%20Li%20and%20Yongbo%20Wang%20and%20Feng%20Wang%20and%20Haoyuan%20Hu%20and%20Rui%20Yan&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20is%20a%20promising%20approach%0Afor%20improving%20the%20complex%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20current%20RLVR%20methods%20face%20two%20significant%20challenges%3A%20the%20near-miss%0Areward%20problem%2C%20where%20a%20small%20mistake%20can%20invalidate%20an%20otherwise%20correct%0Areasoning%20process%2C%20greatly%20hindering%20training%20efficiency%3B%20and%20exploration%0Astagnation%2C%20where%20models%20tend%20to%20focus%20on%20solutions%20within%20their%20%60%60comfort%0Azone%2C%27%27%20lacking%20the%20motivation%20to%20explore%20potentially%20more%20effective%0Aalternatives.%20To%20address%20these%20challenges%2C%20we%20propose%20StepHint%2C%20a%20novel%20RLVR%0Aalgorithm%20that%20utilizes%20multi-level%20stepwise%20hints%20to%20help%20models%20explore%20the%0Asolution%20space%20more%20effectively.%20StepHint%20generates%20valid%20reasoning%20chains%20from%0Astronger%20models%20and%20partitions%20these%20chains%20into%20reasoning%20steps%20using%20our%0Aproposed%20adaptive%20partitioning%20method.%20The%20initial%20few%20steps%20are%20used%20as%20hints%2C%0Aand%20simultaneously%2C%20multiple-level%20hints%20%28each%20comprising%20a%20different%20number%20of%0Asteps%29%20are%20provided%20to%20the%20model.%20This%20approach%20directs%20the%20model%27s%20exploration%0Atoward%20a%20promising%20solution%20subspace%20while%20preserving%20its%20flexibility%20for%0Aindependent%20exploration.%20By%20providing%20hints%2C%20StepHint%20mitigates%20the%20near-miss%0Areward%20problem%2C%20thereby%20improving%20training%20efficiency.%20Additionally%2C%20the%0Aexternal%20reasoning%20pathways%20help%20the%20model%20develop%20better%20reasoning%20abilities%2C%0Aenabling%20it%20to%20move%20beyond%20its%20%60%60comfort%20zone%27%27%20and%20mitigate%20exploration%0Astagnation.%20StepHint%20outperforms%20competitive%20RLVR%20enhancement%20methods%20across%0Asix%20mathematical%20benchmarks%2C%20while%20also%20demonstrating%20superior%20generalization%0Aand%20excelling%20over%20baselines%20on%20out-of-domain%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02841v1&entry.124074799=Read"},
{"title": "Similarity Memory Prior is All You Need for Medical Image Segmentation", "author": "Tang Hao and Guo ZhiQing and Wang LieJun and Liu Chao", "abstract": "  In recent years, it has been found that \"grandmother cells\" in the primary\nvisual cortex (V1) of macaques can directly recognize visual input with complex\nshapes. This inspires us to examine the value of these cells in promoting the\nresearch of medical image segmentation. In this paper, we design a Similarity\nMemory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,\nwe propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and\nremembers the category features of specific lesions or organs in medical images\nthrough the similarity memory prior in the prototype memory bank, thus helping\nthe network to learn subtle texture changes between categories. DMW-LA also\ndynamically updates the similarity memory prior in reverse through Weight-Loss\nDynamic (W-LD) update strategy, effectively assisting the network directly\nextract category features. In addition, we propose the Double-Similarity Global\nInternal Enhancement Module (DS-GIM) to deeply explore the internal differences\nin the feature distribution of input data through cosine similarity and\neuclidean distance. Extensive experiments on four public datasets show that\nSim-MPNet has better segmentation performance than other state-of-the-art\nmethods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.\n", "link": "http://arxiv.org/abs/2507.00585v2", "date": "2025-07-03", "relevancy": 2.0443, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5351}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4952}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity%20Memory%20Prior%20is%20All%20You%20Need%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Similarity%20Memory%20Prior%20is%20All%20You%20Need%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Tang%20Hao%20and%20Guo%20ZhiQing%20and%20Wang%20LieJun%20and%20Liu%20Chao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20it%20has%20been%20found%20that%20%22grandmother%20cells%22%20in%20the%20primary%0Avisual%20cortex%20%28V1%29%20of%20macaques%20can%20directly%20recognize%20visual%20input%20with%20complex%0Ashapes.%20This%20inspires%20us%20to%20examine%20the%20value%20of%20these%20cells%20in%20promoting%20the%0Aresearch%20of%20medical%20image%20segmentation.%20In%20this%20paper%2C%20we%20design%20a%20Similarity%0AMemory%20Prior%20Network%20%28Sim-MPNet%29%20for%20medical%20image%20segmentation.%20Specifically%2C%0Awe%20propose%20a%20Dynamic%20Memory%20Weights-Loss%20Attention%20%28DMW-LA%29%2C%20which%20matches%20and%0Aremembers%20the%20category%20features%20of%20specific%20lesions%20or%20organs%20in%20medical%20images%0Athrough%20the%20similarity%20memory%20prior%20in%20the%20prototype%20memory%20bank%2C%20thus%20helping%0Athe%20network%20to%20learn%20subtle%20texture%20changes%20between%20categories.%20DMW-LA%20also%0Adynamically%20updates%20the%20similarity%20memory%20prior%20in%20reverse%20through%20Weight-Loss%0ADynamic%20%28W-LD%29%20update%20strategy%2C%20effectively%20assisting%20the%20network%20directly%0Aextract%20category%20features.%20In%20addition%2C%20we%20propose%20the%20Double-Similarity%20Global%0AInternal%20Enhancement%20Module%20%28DS-GIM%29%20to%20deeply%20explore%20the%20internal%20differences%0Ain%20the%20feature%20distribution%20of%20input%20data%20through%20cosine%20similarity%20and%0Aeuclidean%20distance.%20Extensive%20experiments%20on%20four%20public%20datasets%20show%20that%0ASim-MPNet%20has%20better%20segmentation%20performance%20than%20other%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20on%20https%3A//github.com/vpsg-research/Sim-MPNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity%2520Memory%2520Prior%2520is%2520All%2520You%2520Need%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DTang%2520Hao%2520and%2520Guo%2520ZhiQing%2520and%2520Wang%2520LieJun%2520and%2520Liu%2520Chao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520it%2520has%2520been%2520found%2520that%2520%2522grandmother%2520cells%2522%2520in%2520the%2520primary%250Avisual%2520cortex%2520%2528V1%2529%2520of%2520macaques%2520can%2520directly%2520recognize%2520visual%2520input%2520with%2520complex%250Ashapes.%2520This%2520inspires%2520us%2520to%2520examine%2520the%2520value%2520of%2520these%2520cells%2520in%2520promoting%2520the%250Aresearch%2520of%2520medical%2520image%2520segmentation.%2520In%2520this%2520paper%252C%2520we%2520design%2520a%2520Similarity%250AMemory%2520Prior%2520Network%2520%2528Sim-MPNet%2529%2520for%2520medical%2520image%2520segmentation.%2520Specifically%252C%250Awe%2520propose%2520a%2520Dynamic%2520Memory%2520Weights-Loss%2520Attention%2520%2528DMW-LA%2529%252C%2520which%2520matches%2520and%250Aremembers%2520the%2520category%2520features%2520of%2520specific%2520lesions%2520or%2520organs%2520in%2520medical%2520images%250Athrough%2520the%2520similarity%2520memory%2520prior%2520in%2520the%2520prototype%2520memory%2520bank%252C%2520thus%2520helping%250Athe%2520network%2520to%2520learn%2520subtle%2520texture%2520changes%2520between%2520categories.%2520DMW-LA%2520also%250Adynamically%2520updates%2520the%2520similarity%2520memory%2520prior%2520in%2520reverse%2520through%2520Weight-Loss%250ADynamic%2520%2528W-LD%2529%2520update%2520strategy%252C%2520effectively%2520assisting%2520the%2520network%2520directly%250Aextract%2520category%2520features.%2520In%2520addition%252C%2520we%2520propose%2520the%2520Double-Similarity%2520Global%250AInternal%2520Enhancement%2520Module%2520%2528DS-GIM%2529%2520to%2520deeply%2520explore%2520the%2520internal%2520differences%250Ain%2520the%2520feature%2520distribution%2520of%2520input%2520data%2520through%2520cosine%2520similarity%2520and%250Aeuclidean%2520distance.%2520Extensive%2520experiments%2520on%2520four%2520public%2520datasets%2520show%2520that%250ASim-MPNet%2520has%2520better%2520segmentation%2520performance%2520than%2520other%2520state-of-the-art%250Amethods.%2520Our%2520code%2520is%2520available%2520on%2520https%253A//github.com/vpsg-research/Sim-MPNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity%20Memory%20Prior%20is%20All%20You%20Need%20for%20Medical%20Image%20Segmentation&entry.906535625=Tang%20Hao%20and%20Guo%20ZhiQing%20and%20Wang%20LieJun%20and%20Liu%20Chao&entry.1292438233=%20%20In%20recent%20years%2C%20it%20has%20been%20found%20that%20%22grandmother%20cells%22%20in%20the%20primary%0Avisual%20cortex%20%28V1%29%20of%20macaques%20can%20directly%20recognize%20visual%20input%20with%20complex%0Ashapes.%20This%20inspires%20us%20to%20examine%20the%20value%20of%20these%20cells%20in%20promoting%20the%0Aresearch%20of%20medical%20image%20segmentation.%20In%20this%20paper%2C%20we%20design%20a%20Similarity%0AMemory%20Prior%20Network%20%28Sim-MPNet%29%20for%20medical%20image%20segmentation.%20Specifically%2C%0Awe%20propose%20a%20Dynamic%20Memory%20Weights-Loss%20Attention%20%28DMW-LA%29%2C%20which%20matches%20and%0Aremembers%20the%20category%20features%20of%20specific%20lesions%20or%20organs%20in%20medical%20images%0Athrough%20the%20similarity%20memory%20prior%20in%20the%20prototype%20memory%20bank%2C%20thus%20helping%0Athe%20network%20to%20learn%20subtle%20texture%20changes%20between%20categories.%20DMW-LA%20also%0Adynamically%20updates%20the%20similarity%20memory%20prior%20in%20reverse%20through%20Weight-Loss%0ADynamic%20%28W-LD%29%20update%20strategy%2C%20effectively%20assisting%20the%20network%20directly%0Aextract%20category%20features.%20In%20addition%2C%20we%20propose%20the%20Double-Similarity%20Global%0AInternal%20Enhancement%20Module%20%28DS-GIM%29%20to%20deeply%20explore%20the%20internal%20differences%0Ain%20the%20feature%20distribution%20of%20input%20data%20through%20cosine%20similarity%20and%0Aeuclidean%20distance.%20Extensive%20experiments%20on%20four%20public%20datasets%20show%20that%0ASim-MPNet%20has%20better%20segmentation%20performance%20than%20other%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20on%20https%3A//github.com/vpsg-research/Sim-MPNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00585v2&entry.124074799=Read"},
{"title": "Learning Traffic Anomalies from Generative Models on Real-Time\n  Observations", "author": "Fotis I. Giasemis and Alexandros Sopasakis", "abstract": "  Accurate detection of traffic anomalies is crucial for effective urban\ntraffic management and congestion mitigation. We use the Spatiotemporal\nGenerative Adversarial Network (STGAN) framework combining Graph Neural\nNetworks and Long Short-Term Memory networks to capture complex spatial and\ntemporal dependencies in traffic data. We apply STGAN to real-time,\nminute-by-minute observations from 42 traffic cameras across Gothenburg,\nSweden, collected over several months in 2020. The images are processed to\ncompute a flow metric representing vehicle density, which serves as input for\nthe model. Training is conducted on data from April to November 2020, and\nvalidation is performed on a separate dataset from November 14 to 23, 2020. Our\nresults demonstrate that the model effectively detects traffic anomalies with\nhigh precision and low false positive rates. The detected anomalies include\ncamera signal interruptions, visual artifacts, and extreme weather conditions\naffecting traffic flow.\n", "link": "http://arxiv.org/abs/2502.01391v3", "date": "2025-07-03", "relevancy": 2.0421, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5115}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5111}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Traffic%20Anomalies%20from%20Generative%20Models%20on%20Real-Time%0A%20%20Observations&body=Title%3A%20Learning%20Traffic%20Anomalies%20from%20Generative%20Models%20on%20Real-Time%0A%20%20Observations%0AAuthor%3A%20Fotis%20I.%20Giasemis%20and%20Alexandros%20Sopasakis%0AAbstract%3A%20%20%20Accurate%20detection%20of%20traffic%20anomalies%20is%20crucial%20for%20effective%20urban%0Atraffic%20management%20and%20congestion%20mitigation.%20We%20use%20the%20Spatiotemporal%0AGenerative%20Adversarial%20Network%20%28STGAN%29%20framework%20combining%20Graph%20Neural%0ANetworks%20and%20Long%20Short-Term%20Memory%20networks%20to%20capture%20complex%20spatial%20and%0Atemporal%20dependencies%20in%20traffic%20data.%20We%20apply%20STGAN%20to%20real-time%2C%0Aminute-by-minute%20observations%20from%2042%20traffic%20cameras%20across%20Gothenburg%2C%0ASweden%2C%20collected%20over%20several%20months%20in%202020.%20The%20images%20are%20processed%20to%0Acompute%20a%20flow%20metric%20representing%20vehicle%20density%2C%20which%20serves%20as%20input%20for%0Athe%20model.%20Training%20is%20conducted%20on%20data%20from%20April%20to%20November%202020%2C%20and%0Avalidation%20is%20performed%20on%20a%20separate%20dataset%20from%20November%2014%20to%2023%2C%202020.%20Our%0Aresults%20demonstrate%20that%20the%20model%20effectively%20detects%20traffic%20anomalies%20with%0Ahigh%20precision%20and%20low%20false%20positive%20rates.%20The%20detected%20anomalies%20include%0Acamera%20signal%20interruptions%2C%20visual%20artifacts%2C%20and%20extreme%20weather%20conditions%0Aaffecting%20traffic%20flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01391v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Traffic%2520Anomalies%2520from%2520Generative%2520Models%2520on%2520Real-Time%250A%2520%2520Observations%26entry.906535625%3DFotis%2520I.%2520Giasemis%2520and%2520Alexandros%2520Sopasakis%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520of%2520traffic%2520anomalies%2520is%2520crucial%2520for%2520effective%2520urban%250Atraffic%2520management%2520and%2520congestion%2520mitigation.%2520We%2520use%2520the%2520Spatiotemporal%250AGenerative%2520Adversarial%2520Network%2520%2528STGAN%2529%2520framework%2520combining%2520Graph%2520Neural%250ANetworks%2520and%2520Long%2520Short-Term%2520Memory%2520networks%2520to%2520capture%2520complex%2520spatial%2520and%250Atemporal%2520dependencies%2520in%2520traffic%2520data.%2520We%2520apply%2520STGAN%2520to%2520real-time%252C%250Aminute-by-minute%2520observations%2520from%252042%2520traffic%2520cameras%2520across%2520Gothenburg%252C%250ASweden%252C%2520collected%2520over%2520several%2520months%2520in%25202020.%2520The%2520images%2520are%2520processed%2520to%250Acompute%2520a%2520flow%2520metric%2520representing%2520vehicle%2520density%252C%2520which%2520serves%2520as%2520input%2520for%250Athe%2520model.%2520Training%2520is%2520conducted%2520on%2520data%2520from%2520April%2520to%2520November%25202020%252C%2520and%250Avalidation%2520is%2520performed%2520on%2520a%2520separate%2520dataset%2520from%2520November%252014%2520to%252023%252C%25202020.%2520Our%250Aresults%2520demonstrate%2520that%2520the%2520model%2520effectively%2520detects%2520traffic%2520anomalies%2520with%250Ahigh%2520precision%2520and%2520low%2520false%2520positive%2520rates.%2520The%2520detected%2520anomalies%2520include%250Acamera%2520signal%2520interruptions%252C%2520visual%2520artifacts%252C%2520and%2520extreme%2520weather%2520conditions%250Aaffecting%2520traffic%2520flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01391v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Traffic%20Anomalies%20from%20Generative%20Models%20on%20Real-Time%0A%20%20Observations&entry.906535625=Fotis%20I.%20Giasemis%20and%20Alexandros%20Sopasakis&entry.1292438233=%20%20Accurate%20detection%20of%20traffic%20anomalies%20is%20crucial%20for%20effective%20urban%0Atraffic%20management%20and%20congestion%20mitigation.%20We%20use%20the%20Spatiotemporal%0AGenerative%20Adversarial%20Network%20%28STGAN%29%20framework%20combining%20Graph%20Neural%0ANetworks%20and%20Long%20Short-Term%20Memory%20networks%20to%20capture%20complex%20spatial%20and%0Atemporal%20dependencies%20in%20traffic%20data.%20We%20apply%20STGAN%20to%20real-time%2C%0Aminute-by-minute%20observations%20from%2042%20traffic%20cameras%20across%20Gothenburg%2C%0ASweden%2C%20collected%20over%20several%20months%20in%202020.%20The%20images%20are%20processed%20to%0Acompute%20a%20flow%20metric%20representing%20vehicle%20density%2C%20which%20serves%20as%20input%20for%0Athe%20model.%20Training%20is%20conducted%20on%20data%20from%20April%20to%20November%202020%2C%20and%0Avalidation%20is%20performed%20on%20a%20separate%20dataset%20from%20November%2014%20to%2023%2C%202020.%20Our%0Aresults%20demonstrate%20that%20the%20model%20effectively%20detects%20traffic%20anomalies%20with%0Ahigh%20precision%20and%20low%20false%20positive%20rates.%20The%20detected%20anomalies%20include%0Acamera%20signal%20interruptions%2C%20visual%20artifacts%2C%20and%20extreme%20weather%20conditions%0Aaffecting%20traffic%20flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01391v3&entry.124074799=Read"},
{"title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling", "author": "Tianhao Chen and Xin Xu and Zijing Liu and Pengxiang Li and Xinyuan Song and Ajay Kumar Jaiswal and Fan Zhang and Jishan Hu and Yang Wang and Hao Chen and Shizhe Diao and Shiwei Liu and Yu Li and Lu Yin and Can Yang", "abstract": "  Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS.\n", "link": "http://arxiv.org/abs/2506.22049v2", "date": "2025-07-03", "relevancy": 2.0389, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPAS%3A%20Accelerating%20Convergence%20of%20LLM%20Pretraining%20via%0A%20%20Gradient-Preserving%20Activation%20Scaling&body=Title%3A%20GPAS%3A%20Accelerating%20Convergence%20of%20LLM%20Pretraining%20via%0A%20%20Gradient-Preserving%20Activation%20Scaling%0AAuthor%3A%20Tianhao%20Chen%20and%20Xin%20Xu%20and%20Zijing%20Liu%20and%20Pengxiang%20Li%20and%20Xinyuan%20Song%20and%20Ajay%20Kumar%20Jaiswal%20and%20Fan%20Zhang%20and%20Jishan%20Hu%20and%20Yang%20Wang%20and%20Hao%20Chen%20and%20Shizhe%20Diao%20and%20Shiwei%20Liu%20and%20Yu%20Li%20and%20Lu%20Yin%20and%20Can%20Yang%0AAbstract%3A%20%20%20Modern%20Large%20Language%20Models%2C%20such%20as%20the%20LLaMA%2C%20Qwen%20and%20DeepSeek%20series%2C%0Apredominantly%20adopt%20the%20Pre-LayerNorm%20%28Pre-LN%29%20Transformer%20architecture.%20While%0Abeing%20stable%20during%20pretraining%20and%20scalable%20to%20large%20model%20sizes%2C%20Pre-LN%0Asuffers%20from%20an%20exponential%20growth%20in%20activation%20variance%20across%20layers%2C%0Acausing%20the%20shortcut%20to%20dominate%20over%20sub-layer%20outputs%20in%20the%20residual%0Aconnection%20and%20limiting%20the%20learning%20capacity%20of%20deeper%20layers.%20To%20mitigate%0Athis%20issue%2C%20we%20propose%20Gradient-Preserving%20Activation%20Scaling%20%28GPAS%29%2C%20a%20simple%0Atechnique%20that%20can%20be%20used%20in%20combination%20with%20existing%20approaches.%20GPAS%20works%0Aby%20scaling%20down%20the%20intermediate%20activations%20while%20keeping%20their%20gradients%0Aunchanged.%20This%20leaves%20information%20in%20the%20activations%20intact%2C%20and%20avoids%20the%0Agradient%20vanishing%20problem%20associated%20with%20gradient%20downscaling.%20Extensive%0Aexperiments%20across%20various%20model%20sizes%20from%2071M%20to%201B%20show%20that%20GPAS%20achieves%0Aconsistent%20performance%20gains.%20Beyond%20enhancing%20Pre-LN%20Transformers%2C%20GPAS%20also%0Ashows%20promise%20in%20improving%20alternative%20architectures%20such%20as%20Sandwich-LN%20and%0ADeepNorm%2C%20demonstrating%20its%20versatility%20and%20potential%20for%20improving%20training%0Adynamics%20in%20a%20wide%20range%20of%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/dandingsky/GPAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPAS%253A%2520Accelerating%2520Convergence%2520of%2520LLM%2520Pretraining%2520via%250A%2520%2520Gradient-Preserving%2520Activation%2520Scaling%26entry.906535625%3DTianhao%2520Chen%2520and%2520Xin%2520Xu%2520and%2520Zijing%2520Liu%2520and%2520Pengxiang%2520Li%2520and%2520Xinyuan%2520Song%2520and%2520Ajay%2520Kumar%2520Jaiswal%2520and%2520Fan%2520Zhang%2520and%2520Jishan%2520Hu%2520and%2520Yang%2520Wang%2520and%2520Hao%2520Chen%2520and%2520Shizhe%2520Diao%2520and%2520Shiwei%2520Liu%2520and%2520Yu%2520Li%2520and%2520Lu%2520Yin%2520and%2520Can%2520Yang%26entry.1292438233%3D%2520%2520Modern%2520Large%2520Language%2520Models%252C%2520such%2520as%2520the%2520LLaMA%252C%2520Qwen%2520and%2520DeepSeek%2520series%252C%250Apredominantly%2520adopt%2520the%2520Pre-LayerNorm%2520%2528Pre-LN%2529%2520Transformer%2520architecture.%2520While%250Abeing%2520stable%2520during%2520pretraining%2520and%2520scalable%2520to%2520large%2520model%2520sizes%252C%2520Pre-LN%250Asuffers%2520from%2520an%2520exponential%2520growth%2520in%2520activation%2520variance%2520across%2520layers%252C%250Acausing%2520the%2520shortcut%2520to%2520dominate%2520over%2520sub-layer%2520outputs%2520in%2520the%2520residual%250Aconnection%2520and%2520limiting%2520the%2520learning%2520capacity%2520of%2520deeper%2520layers.%2520To%2520mitigate%250Athis%2520issue%252C%2520we%2520propose%2520Gradient-Preserving%2520Activation%2520Scaling%2520%2528GPAS%2529%252C%2520a%2520simple%250Atechnique%2520that%2520can%2520be%2520used%2520in%2520combination%2520with%2520existing%2520approaches.%2520GPAS%2520works%250Aby%2520scaling%2520down%2520the%2520intermediate%2520activations%2520while%2520keeping%2520their%2520gradients%250Aunchanged.%2520This%2520leaves%2520information%2520in%2520the%2520activations%2520intact%252C%2520and%2520avoids%2520the%250Agradient%2520vanishing%2520problem%2520associated%2520with%2520gradient%2520downscaling.%2520Extensive%250Aexperiments%2520across%2520various%2520model%2520sizes%2520from%252071M%2520to%25201B%2520show%2520that%2520GPAS%2520achieves%250Aconsistent%2520performance%2520gains.%2520Beyond%2520enhancing%2520Pre-LN%2520Transformers%252C%2520GPAS%2520also%250Ashows%2520promise%2520in%2520improving%2520alternative%2520architectures%2520such%2520as%2520Sandwich-LN%2520and%250ADeepNorm%252C%2520demonstrating%2520its%2520versatility%2520and%2520potential%2520for%2520improving%2520training%250Adynamics%2520in%2520a%2520wide%2520range%2520of%2520settings.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dandingsky/GPAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPAS%3A%20Accelerating%20Convergence%20of%20LLM%20Pretraining%20via%0A%20%20Gradient-Preserving%20Activation%20Scaling&entry.906535625=Tianhao%20Chen%20and%20Xin%20Xu%20and%20Zijing%20Liu%20and%20Pengxiang%20Li%20and%20Xinyuan%20Song%20and%20Ajay%20Kumar%20Jaiswal%20and%20Fan%20Zhang%20and%20Jishan%20Hu%20and%20Yang%20Wang%20and%20Hao%20Chen%20and%20Shizhe%20Diao%20and%20Shiwei%20Liu%20and%20Yu%20Li%20and%20Lu%20Yin%20and%20Can%20Yang&entry.1292438233=%20%20Modern%20Large%20Language%20Models%2C%20such%20as%20the%20LLaMA%2C%20Qwen%20and%20DeepSeek%20series%2C%0Apredominantly%20adopt%20the%20Pre-LayerNorm%20%28Pre-LN%29%20Transformer%20architecture.%20While%0Abeing%20stable%20during%20pretraining%20and%20scalable%20to%20large%20model%20sizes%2C%20Pre-LN%0Asuffers%20from%20an%20exponential%20growth%20in%20activation%20variance%20across%20layers%2C%0Acausing%20the%20shortcut%20to%20dominate%20over%20sub-layer%20outputs%20in%20the%20residual%0Aconnection%20and%20limiting%20the%20learning%20capacity%20of%20deeper%20layers.%20To%20mitigate%0Athis%20issue%2C%20we%20propose%20Gradient-Preserving%20Activation%20Scaling%20%28GPAS%29%2C%20a%20simple%0Atechnique%20that%20can%20be%20used%20in%20combination%20with%20existing%20approaches.%20GPAS%20works%0Aby%20scaling%20down%20the%20intermediate%20activations%20while%20keeping%20their%20gradients%0Aunchanged.%20This%20leaves%20information%20in%20the%20activations%20intact%2C%20and%20avoids%20the%0Agradient%20vanishing%20problem%20associated%20with%20gradient%20downscaling.%20Extensive%0Aexperiments%20across%20various%20model%20sizes%20from%2071M%20to%201B%20show%20that%20GPAS%20achieves%0Aconsistent%20performance%20gains.%20Beyond%20enhancing%20Pre-LN%20Transformers%2C%20GPAS%20also%0Ashows%20promise%20in%20improving%20alternative%20architectures%20such%20as%20Sandwich-LN%20and%0ADeepNorm%2C%20demonstrating%20its%20versatility%20and%20potential%20for%20improving%20training%0Adynamics%20in%20a%20wide%20range%20of%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/dandingsky/GPAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22049v2&entry.124074799=Read"},
{"title": "RFWNet: A Lightweight Remote Sensing Object Detector Integrating\n  Multiscale Receptive Fields and Foreground Focus Mechanism", "author": "Yujie Lei and Wenjie Sun and Sen Jia and Qingquan Li and Jie Zhang", "abstract": "  Challenges in remote sensing object detection(RSOD), such as high interclass\nsimilarity, imbalanced foreground-background distribution, and the small size\nof objects in remote sensing images, significantly hinder detection accuracy.\nMoreover, the tradeoff between model accuracy and computational complexity\nposes additional constraints on the application of RSOD algorithms. To address\nthese issues, this study proposes an efficient and lightweight RSOD algorithm\nintegrating multiscale receptive fields and foreground focus mechanism, named\nrobust foreground weighted network(RFWNet). Specifically, we proposed a\nlightweight backbone network receptive field adaptive selection network\n(RFASNet), leveraging the rich context information of remote sensing images to\nenhance class separability. Additionally, we developed a foreground-background\nseparation module(FBSM)consisting of a background redundant information\nfiltering module (BRIFM) and a foreground information enhancement module (FIEM)\nto emphasize critical regions within images while filtering redundant\nbackground information. Finally, we designed a loss function, the weighted\nCIoU-Wasserstein loss (LWCW),which weights the IoU-based loss by using the\nnormalized Wasserstein distance to mitigate model sensitivity to small object\nposition deviations. The comprehensive experimental results demonstrate that\nRFWNet achieved 95.3% and 73.2% mean average precision (mAP) with 6.0 M\nparameters on the DOTA V1.0 and NWPU VHR-10 datasets, respectively, with an\ninference speed of 52 FPS.\n", "link": "http://arxiv.org/abs/2503.00545v2", "date": "2025-07-03", "relevancy": 2.0355, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5136}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RFWNet%3A%20A%20Lightweight%20Remote%20Sensing%20Object%20Detector%20Integrating%0A%20%20Multiscale%20Receptive%20Fields%20and%20Foreground%20Focus%20Mechanism&body=Title%3A%20RFWNet%3A%20A%20Lightweight%20Remote%20Sensing%20Object%20Detector%20Integrating%0A%20%20Multiscale%20Receptive%20Fields%20and%20Foreground%20Focus%20Mechanism%0AAuthor%3A%20Yujie%20Lei%20and%20Wenjie%20Sun%20and%20Sen%20Jia%20and%20Qingquan%20Li%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20Challenges%20in%20remote%20sensing%20object%20detection%28RSOD%29%2C%20such%20as%20high%20interclass%0Asimilarity%2C%20imbalanced%20foreground-background%20distribution%2C%20and%20the%20small%20size%0Aof%20objects%20in%20remote%20sensing%20images%2C%20significantly%20hinder%20detection%20accuracy.%0AMoreover%2C%20the%20tradeoff%20between%20model%20accuracy%20and%20computational%20complexity%0Aposes%20additional%20constraints%20on%20the%20application%20of%20RSOD%20algorithms.%20To%20address%0Athese%20issues%2C%20this%20study%20proposes%20an%20efficient%20and%20lightweight%20RSOD%20algorithm%0Aintegrating%20multiscale%20receptive%20fields%20and%20foreground%20focus%20mechanism%2C%20named%0Arobust%20foreground%20weighted%20network%28RFWNet%29.%20Specifically%2C%20we%20proposed%20a%0Alightweight%20backbone%20network%20receptive%20field%20adaptive%20selection%20network%0A%28RFASNet%29%2C%20leveraging%20the%20rich%20context%20information%20of%20remote%20sensing%20images%20to%0Aenhance%20class%20separability.%20Additionally%2C%20we%20developed%20a%20foreground-background%0Aseparation%20module%28FBSM%29consisting%20of%20a%20background%20redundant%20information%0Afiltering%20module%20%28BRIFM%29%20and%20a%20foreground%20information%20enhancement%20module%20%28FIEM%29%0Ato%20emphasize%20critical%20regions%20within%20images%20while%20filtering%20redundant%0Abackground%20information.%20Finally%2C%20we%20designed%20a%20loss%20function%2C%20the%20weighted%0ACIoU-Wasserstein%20loss%20%28LWCW%29%2Cwhich%20weights%20the%20IoU-based%20loss%20by%20using%20the%0Anormalized%20Wasserstein%20distance%20to%20mitigate%20model%20sensitivity%20to%20small%20object%0Aposition%20deviations.%20The%20comprehensive%20experimental%20results%20demonstrate%20that%0ARFWNet%20achieved%2095.3%25%20and%2073.2%25%20mean%20average%20precision%20%28mAP%29%20with%206.0%20M%0Aparameters%20on%20the%20DOTA%20V1.0%20and%20NWPU%20VHR-10%20datasets%2C%20respectively%2C%20with%20an%0Ainference%20speed%20of%2052%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRFWNet%253A%2520A%2520Lightweight%2520Remote%2520Sensing%2520Object%2520Detector%2520Integrating%250A%2520%2520Multiscale%2520Receptive%2520Fields%2520and%2520Foreground%2520Focus%2520Mechanism%26entry.906535625%3DYujie%2520Lei%2520and%2520Wenjie%2520Sun%2520and%2520Sen%2520Jia%2520and%2520Qingquan%2520Li%2520and%2520Jie%2520Zhang%26entry.1292438233%3D%2520%2520Challenges%2520in%2520remote%2520sensing%2520object%2520detection%2528RSOD%2529%252C%2520such%2520as%2520high%2520interclass%250Asimilarity%252C%2520imbalanced%2520foreground-background%2520distribution%252C%2520and%2520the%2520small%2520size%250Aof%2520objects%2520in%2520remote%2520sensing%2520images%252C%2520significantly%2520hinder%2520detection%2520accuracy.%250AMoreover%252C%2520the%2520tradeoff%2520between%2520model%2520accuracy%2520and%2520computational%2520complexity%250Aposes%2520additional%2520constraints%2520on%2520the%2520application%2520of%2520RSOD%2520algorithms.%2520To%2520address%250Athese%2520issues%252C%2520this%2520study%2520proposes%2520an%2520efficient%2520and%2520lightweight%2520RSOD%2520algorithm%250Aintegrating%2520multiscale%2520receptive%2520fields%2520and%2520foreground%2520focus%2520mechanism%252C%2520named%250Arobust%2520foreground%2520weighted%2520network%2528RFWNet%2529.%2520Specifically%252C%2520we%2520proposed%2520a%250Alightweight%2520backbone%2520network%2520receptive%2520field%2520adaptive%2520selection%2520network%250A%2528RFASNet%2529%252C%2520leveraging%2520the%2520rich%2520context%2520information%2520of%2520remote%2520sensing%2520images%2520to%250Aenhance%2520class%2520separability.%2520Additionally%252C%2520we%2520developed%2520a%2520foreground-background%250Aseparation%2520module%2528FBSM%2529consisting%2520of%2520a%2520background%2520redundant%2520information%250Afiltering%2520module%2520%2528BRIFM%2529%2520and%2520a%2520foreground%2520information%2520enhancement%2520module%2520%2528FIEM%2529%250Ato%2520emphasize%2520critical%2520regions%2520within%2520images%2520while%2520filtering%2520redundant%250Abackground%2520information.%2520Finally%252C%2520we%2520designed%2520a%2520loss%2520function%252C%2520the%2520weighted%250ACIoU-Wasserstein%2520loss%2520%2528LWCW%2529%252Cwhich%2520weights%2520the%2520IoU-based%2520loss%2520by%2520using%2520the%250Anormalized%2520Wasserstein%2520distance%2520to%2520mitigate%2520model%2520sensitivity%2520to%2520small%2520object%250Aposition%2520deviations.%2520The%2520comprehensive%2520experimental%2520results%2520demonstrate%2520that%250ARFWNet%2520achieved%252095.3%2525%2520and%252073.2%2525%2520mean%2520average%2520precision%2520%2528mAP%2529%2520with%25206.0%2520M%250Aparameters%2520on%2520the%2520DOTA%2520V1.0%2520and%2520NWPU%2520VHR-10%2520datasets%252C%2520respectively%252C%2520with%2520an%250Ainference%2520speed%2520of%252052%2520FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RFWNet%3A%20A%20Lightweight%20Remote%20Sensing%20Object%20Detector%20Integrating%0A%20%20Multiscale%20Receptive%20Fields%20and%20Foreground%20Focus%20Mechanism&entry.906535625=Yujie%20Lei%20and%20Wenjie%20Sun%20and%20Sen%20Jia%20and%20Qingquan%20Li%20and%20Jie%20Zhang&entry.1292438233=%20%20Challenges%20in%20remote%20sensing%20object%20detection%28RSOD%29%2C%20such%20as%20high%20interclass%0Asimilarity%2C%20imbalanced%20foreground-background%20distribution%2C%20and%20the%20small%20size%0Aof%20objects%20in%20remote%20sensing%20images%2C%20significantly%20hinder%20detection%20accuracy.%0AMoreover%2C%20the%20tradeoff%20between%20model%20accuracy%20and%20computational%20complexity%0Aposes%20additional%20constraints%20on%20the%20application%20of%20RSOD%20algorithms.%20To%20address%0Athese%20issues%2C%20this%20study%20proposes%20an%20efficient%20and%20lightweight%20RSOD%20algorithm%0Aintegrating%20multiscale%20receptive%20fields%20and%20foreground%20focus%20mechanism%2C%20named%0Arobust%20foreground%20weighted%20network%28RFWNet%29.%20Specifically%2C%20we%20proposed%20a%0Alightweight%20backbone%20network%20receptive%20field%20adaptive%20selection%20network%0A%28RFASNet%29%2C%20leveraging%20the%20rich%20context%20information%20of%20remote%20sensing%20images%20to%0Aenhance%20class%20separability.%20Additionally%2C%20we%20developed%20a%20foreground-background%0Aseparation%20module%28FBSM%29consisting%20of%20a%20background%20redundant%20information%0Afiltering%20module%20%28BRIFM%29%20and%20a%20foreground%20information%20enhancement%20module%20%28FIEM%29%0Ato%20emphasize%20critical%20regions%20within%20images%20while%20filtering%20redundant%0Abackground%20information.%20Finally%2C%20we%20designed%20a%20loss%20function%2C%20the%20weighted%0ACIoU-Wasserstein%20loss%20%28LWCW%29%2Cwhich%20weights%20the%20IoU-based%20loss%20by%20using%20the%0Anormalized%20Wasserstein%20distance%20to%20mitigate%20model%20sensitivity%20to%20small%20object%0Aposition%20deviations.%20The%20comprehensive%20experimental%20results%20demonstrate%20that%0ARFWNet%20achieved%2095.3%25%20and%2073.2%25%20mean%20average%20precision%20%28mAP%29%20with%206.0%20M%0Aparameters%20on%20the%20DOTA%20V1.0%20and%20NWPU%20VHR-10%20datasets%2C%20respectively%2C%20with%20an%0Ainference%20speed%20of%2052%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00545v2&entry.124074799=Read"},
{"title": "Towards autonomous photogrammetric forest inventory using a lightweight\n  under-canopy robotic drone", "author": "V\u00e4in\u00f6 Karjalainen and Niko Koivum\u00e4ki and Teemu Hakala and Jesse Muhojoki and Eric Hyypp\u00e4 and Anand George and Juha Suomalainen and Eija Honkavaara", "abstract": "  Drones are increasingly used in forestry to capture high-resolution remote\nsensing data, supporting enhanced monitoring, assessment, and decision-making\nprocesses. While operations above the forest canopy are already highly\nautomated, flying inside forests remains challenging, primarily relying on\nmanual piloting. Inside dense forests, reliance on the Global Navigation\nSatellite System (GNSS) for localization is not feasible. Additionally, the\ndrone must autonomously adjust its flight path to avoid collisions. Recently,\nadvancements in robotics have enabled autonomous drone flights in GNSS-denied\nobstacle-rich areas. In this article, a step towards autonomous forest data\ncollection is taken by building a prototype of a robotic under-canopy drone\nutilizing state-of-the-art open-source methods and validating its performance\nfor data collection inside forests. Specifically, the study focused on\ncamera-based autonomous flight under the forest canopy and photogrammetric\npost-processing of the data collected with the low-cost onboard stereo camera.\nThe autonomous flight capability of the prototype was evaluated through\nmultiple test flights at boreal forests. The tree parameter estimation\ncapability was studied by performing diameter at breast height (DBH)\nestimation. The prototype successfully carried out flights in selected\nchallenging forest environments, and the experiments showed excellent\nperformance in forest 3D modeling with a miniaturized stereoscopic\nphotogrammetric system. The DBH estimation achieved a root mean square error\n(RMSE) of 3.33 cm (12.79 \\%) across all trees. For trees with a DBH less than\n30 cm, the RMSE was 1.16 cm (5.74 \\%). The results provide valuable insights\ninto autonomous under-canopy forest mapping and highlight the critical next\nsteps for advancing lightweight robotic drone systems for mapping complex\nforest environments.\n", "link": "http://arxiv.org/abs/2501.12073v2", "date": "2025-07-03", "relevancy": 2.0299, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5282}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5136}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone&body=Title%3A%20Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone%0AAuthor%3A%20V%C3%A4in%C3%B6%20Karjalainen%20and%20Niko%20Koivum%C3%A4ki%20and%20Teemu%20Hakala%20and%20Jesse%20Muhojoki%20and%20Eric%20Hyypp%C3%A4%20and%20Anand%20George%20and%20Juha%20Suomalainen%20and%20Eija%20Honkavaara%0AAbstract%3A%20%20%20Drones%20are%20increasingly%20used%20in%20forestry%20to%20capture%20high-resolution%20remote%0Asensing%20data%2C%20supporting%20enhanced%20monitoring%2C%20assessment%2C%20and%20decision-making%0Aprocesses.%20While%20operations%20above%20the%20forest%20canopy%20are%20already%20highly%0Aautomated%2C%20flying%20inside%20forests%20remains%20challenging%2C%20primarily%20relying%20on%0Amanual%20piloting.%20Inside%20dense%20forests%2C%20reliance%20on%20the%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29%20for%20localization%20is%20not%20feasible.%20Additionally%2C%20the%0Adrone%20must%20autonomously%20adjust%20its%20flight%20path%20to%20avoid%20collisions.%20Recently%2C%0Aadvancements%20in%20robotics%20have%20enabled%20autonomous%20drone%20flights%20in%20GNSS-denied%0Aobstacle-rich%20areas.%20In%20this%20article%2C%20a%20step%20towards%20autonomous%20forest%20data%0Acollection%20is%20taken%20by%20building%20a%20prototype%20of%20a%20robotic%20under-canopy%20drone%0Autilizing%20state-of-the-art%20open-source%20methods%20and%20validating%20its%20performance%0Afor%20data%20collection%20inside%20forests.%20Specifically%2C%20the%20study%20focused%20on%0Acamera-based%20autonomous%20flight%20under%20the%20forest%20canopy%20and%20photogrammetric%0Apost-processing%20of%20the%20data%20collected%20with%20the%20low-cost%20onboard%20stereo%20camera.%0AThe%20autonomous%20flight%20capability%20of%20the%20prototype%20was%20evaluated%20through%0Amultiple%20test%20flights%20at%20boreal%20forests.%20The%20tree%20parameter%20estimation%0Acapability%20was%20studied%20by%20performing%20diameter%20at%20breast%20height%20%28DBH%29%0Aestimation.%20The%20prototype%20successfully%20carried%20out%20flights%20in%20selected%0Achallenging%20forest%20environments%2C%20and%20the%20experiments%20showed%20excellent%0Aperformance%20in%20forest%203D%20modeling%20with%20a%20miniaturized%20stereoscopic%0Aphotogrammetric%20system.%20The%20DBH%20estimation%20achieved%20a%20root%20mean%20square%20error%0A%28RMSE%29%20of%203.33%20cm%20%2812.79%20%5C%25%29%20across%20all%20trees.%20For%20trees%20with%20a%20DBH%20less%20than%0A30%20cm%2C%20the%20RMSE%20was%201.16%20cm%20%285.74%20%5C%25%29.%20The%20results%20provide%20valuable%20insights%0Ainto%20autonomous%20under-canopy%20forest%20mapping%20and%20highlight%20the%20critical%20next%0Asteps%20for%20advancing%20lightweight%20robotic%20drone%20systems%20for%20mapping%20complex%0Aforest%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520autonomous%2520photogrammetric%2520forest%2520inventory%2520using%2520a%2520lightweight%250A%2520%2520under-canopy%2520robotic%2520drone%26entry.906535625%3DV%25C3%25A4in%25C3%25B6%2520Karjalainen%2520and%2520Niko%2520Koivum%25C3%25A4ki%2520and%2520Teemu%2520Hakala%2520and%2520Jesse%2520Muhojoki%2520and%2520Eric%2520Hyypp%25C3%25A4%2520and%2520Anand%2520George%2520and%2520Juha%2520Suomalainen%2520and%2520Eija%2520Honkavaara%26entry.1292438233%3D%2520%2520Drones%2520are%2520increasingly%2520used%2520in%2520forestry%2520to%2520capture%2520high-resolution%2520remote%250Asensing%2520data%252C%2520supporting%2520enhanced%2520monitoring%252C%2520assessment%252C%2520and%2520decision-making%250Aprocesses.%2520While%2520operations%2520above%2520the%2520forest%2520canopy%2520are%2520already%2520highly%250Aautomated%252C%2520flying%2520inside%2520forests%2520remains%2520challenging%252C%2520primarily%2520relying%2520on%250Amanual%2520piloting.%2520Inside%2520dense%2520forests%252C%2520reliance%2520on%2520the%2520Global%2520Navigation%250ASatellite%2520System%2520%2528GNSS%2529%2520for%2520localization%2520is%2520not%2520feasible.%2520Additionally%252C%2520the%250Adrone%2520must%2520autonomously%2520adjust%2520its%2520flight%2520path%2520to%2520avoid%2520collisions.%2520Recently%252C%250Aadvancements%2520in%2520robotics%2520have%2520enabled%2520autonomous%2520drone%2520flights%2520in%2520GNSS-denied%250Aobstacle-rich%2520areas.%2520In%2520this%2520article%252C%2520a%2520step%2520towards%2520autonomous%2520forest%2520data%250Acollection%2520is%2520taken%2520by%2520building%2520a%2520prototype%2520of%2520a%2520robotic%2520under-canopy%2520drone%250Autilizing%2520state-of-the-art%2520open-source%2520methods%2520and%2520validating%2520its%2520performance%250Afor%2520data%2520collection%2520inside%2520forests.%2520Specifically%252C%2520the%2520study%2520focused%2520on%250Acamera-based%2520autonomous%2520flight%2520under%2520the%2520forest%2520canopy%2520and%2520photogrammetric%250Apost-processing%2520of%2520the%2520data%2520collected%2520with%2520the%2520low-cost%2520onboard%2520stereo%2520camera.%250AThe%2520autonomous%2520flight%2520capability%2520of%2520the%2520prototype%2520was%2520evaluated%2520through%250Amultiple%2520test%2520flights%2520at%2520boreal%2520forests.%2520The%2520tree%2520parameter%2520estimation%250Acapability%2520was%2520studied%2520by%2520performing%2520diameter%2520at%2520breast%2520height%2520%2528DBH%2529%250Aestimation.%2520The%2520prototype%2520successfully%2520carried%2520out%2520flights%2520in%2520selected%250Achallenging%2520forest%2520environments%252C%2520and%2520the%2520experiments%2520showed%2520excellent%250Aperformance%2520in%2520forest%25203D%2520modeling%2520with%2520a%2520miniaturized%2520stereoscopic%250Aphotogrammetric%2520system.%2520The%2520DBH%2520estimation%2520achieved%2520a%2520root%2520mean%2520square%2520error%250A%2528RMSE%2529%2520of%25203.33%2520cm%2520%252812.79%2520%255C%2525%2529%2520across%2520all%2520trees.%2520For%2520trees%2520with%2520a%2520DBH%2520less%2520than%250A30%2520cm%252C%2520the%2520RMSE%2520was%25201.16%2520cm%2520%25285.74%2520%255C%2525%2529.%2520The%2520results%2520provide%2520valuable%2520insights%250Ainto%2520autonomous%2520under-canopy%2520forest%2520mapping%2520and%2520highlight%2520the%2520critical%2520next%250Asteps%2520for%2520advancing%2520lightweight%2520robotic%2520drone%2520systems%2520for%2520mapping%2520complex%250Aforest%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone&entry.906535625=V%C3%A4in%C3%B6%20Karjalainen%20and%20Niko%20Koivum%C3%A4ki%20and%20Teemu%20Hakala%20and%20Jesse%20Muhojoki%20and%20Eric%20Hyypp%C3%A4%20and%20Anand%20George%20and%20Juha%20Suomalainen%20and%20Eija%20Honkavaara&entry.1292438233=%20%20Drones%20are%20increasingly%20used%20in%20forestry%20to%20capture%20high-resolution%20remote%0Asensing%20data%2C%20supporting%20enhanced%20monitoring%2C%20assessment%2C%20and%20decision-making%0Aprocesses.%20While%20operations%20above%20the%20forest%20canopy%20are%20already%20highly%0Aautomated%2C%20flying%20inside%20forests%20remains%20challenging%2C%20primarily%20relying%20on%0Amanual%20piloting.%20Inside%20dense%20forests%2C%20reliance%20on%20the%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29%20for%20localization%20is%20not%20feasible.%20Additionally%2C%20the%0Adrone%20must%20autonomously%20adjust%20its%20flight%20path%20to%20avoid%20collisions.%20Recently%2C%0Aadvancements%20in%20robotics%20have%20enabled%20autonomous%20drone%20flights%20in%20GNSS-denied%0Aobstacle-rich%20areas.%20In%20this%20article%2C%20a%20step%20towards%20autonomous%20forest%20data%0Acollection%20is%20taken%20by%20building%20a%20prototype%20of%20a%20robotic%20under-canopy%20drone%0Autilizing%20state-of-the-art%20open-source%20methods%20and%20validating%20its%20performance%0Afor%20data%20collection%20inside%20forests.%20Specifically%2C%20the%20study%20focused%20on%0Acamera-based%20autonomous%20flight%20under%20the%20forest%20canopy%20and%20photogrammetric%0Apost-processing%20of%20the%20data%20collected%20with%20the%20low-cost%20onboard%20stereo%20camera.%0AThe%20autonomous%20flight%20capability%20of%20the%20prototype%20was%20evaluated%20through%0Amultiple%20test%20flights%20at%20boreal%20forests.%20The%20tree%20parameter%20estimation%0Acapability%20was%20studied%20by%20performing%20diameter%20at%20breast%20height%20%28DBH%29%0Aestimation.%20The%20prototype%20successfully%20carried%20out%20flights%20in%20selected%0Achallenging%20forest%20environments%2C%20and%20the%20experiments%20showed%20excellent%0Aperformance%20in%20forest%203D%20modeling%20with%20a%20miniaturized%20stereoscopic%0Aphotogrammetric%20system.%20The%20DBH%20estimation%20achieved%20a%20root%20mean%20square%20error%0A%28RMSE%29%20of%203.33%20cm%20%2812.79%20%5C%25%29%20across%20all%20trees.%20For%20trees%20with%20a%20DBH%20less%20than%0A30%20cm%2C%20the%20RMSE%20was%201.16%20cm%20%285.74%20%5C%25%29.%20The%20results%20provide%20valuable%20insights%0Ainto%20autonomous%20under-canopy%20forest%20mapping%20and%20highlight%20the%20critical%20next%0Asteps%20for%20advancing%20lightweight%20robotic%20drone%20systems%20for%20mapping%20complex%0Aforest%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12073v2&entry.124074799=Read"},
{"title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak\n  Boundary Polyp Detection", "author": "Zhe Yee Tan", "abstract": "  Colorectal polyp segmentation is critical for early detection of colorectal\ncancer, yet weak and low contrast boundaries significantly limit automated\naccuracy. Existing deep models either blur fine edge details or rely on\nhandcrafted filters that perform poorly under variable imaging conditions. We\npropose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects\ndirectional, parameter free Haar wavelet edge maps into each decoder stage to\nrecalibrate semantic features. Our two main contributions are: (1) a two-level\nHaar wavelet head for multi orientation edge extraction; and (2) Wavelet Edge\nGuided Attention (WEGA) modules that fuse wavelet cues with reverse and input\nbranches. On five public polyp datasets, MEGANetW consistently outperforms\nexisting methods, improving mIoU by up to 2.3% and mDice by 1.2%, while\nintroducing no additional learnable parameters.\n", "link": "http://arxiv.org/abs/2507.02668v1", "date": "2025-07-03", "relevancy": 2.0173, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5253}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection&body=Title%3A%20MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection%0AAuthor%3A%20Zhe%20Yee%20Tan%0AAbstract%3A%20%20%20Colorectal%20polyp%20segmentation%20is%20critical%20for%20early%20detection%20of%20colorectal%0Acancer%2C%20yet%20weak%20and%20low%20contrast%20boundaries%20significantly%20limit%20automated%0Aaccuracy.%20Existing%20deep%20models%20either%20blur%20fine%20edge%20details%20or%20rely%20on%0Ahandcrafted%20filters%20that%20perform%20poorly%20under%20variable%20imaging%20conditions.%20We%0Apropose%20MEGANet-W%2C%20a%20Wavelet%20Driven%20Edge%20Guided%20Attention%20Network%20that%20injects%0Adirectional%2C%20parameter%20free%20Haar%20wavelet%20edge%20maps%20into%20each%20decoder%20stage%20to%0Arecalibrate%20semantic%20features.%20Our%20two%20main%20contributions%20are%3A%20%281%29%20a%20two-level%0AHaar%20wavelet%20head%20for%20multi%20orientation%20edge%20extraction%3B%20and%20%282%29%20Wavelet%20Edge%0AGuided%20Attention%20%28WEGA%29%20modules%20that%20fuse%20wavelet%20cues%20with%20reverse%20and%20input%0Abranches.%20On%20five%20public%20polyp%20datasets%2C%20MEGANetW%20consistently%20outperforms%0Aexisting%20methods%2C%20improving%20mIoU%20by%20up%20to%202.3%25%20and%20mDice%20by%201.2%25%2C%20while%0Aintroducing%20no%20additional%20learnable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGANet-W%253A%2520A%2520Wavelet-Driven%2520Edge-Guided%2520Attention%2520Framework%2520for%2520Weak%250A%2520%2520Boundary%2520Polyp%2520Detection%26entry.906535625%3DZhe%2520Yee%2520Tan%26entry.1292438233%3D%2520%2520Colorectal%2520polyp%2520segmentation%2520is%2520critical%2520for%2520early%2520detection%2520of%2520colorectal%250Acancer%252C%2520yet%2520weak%2520and%2520low%2520contrast%2520boundaries%2520significantly%2520limit%2520automated%250Aaccuracy.%2520Existing%2520deep%2520models%2520either%2520blur%2520fine%2520edge%2520details%2520or%2520rely%2520on%250Ahandcrafted%2520filters%2520that%2520perform%2520poorly%2520under%2520variable%2520imaging%2520conditions.%2520We%250Apropose%2520MEGANet-W%252C%2520a%2520Wavelet%2520Driven%2520Edge%2520Guided%2520Attention%2520Network%2520that%2520injects%250Adirectional%252C%2520parameter%2520free%2520Haar%2520wavelet%2520edge%2520maps%2520into%2520each%2520decoder%2520stage%2520to%250Arecalibrate%2520semantic%2520features.%2520Our%2520two%2520main%2520contributions%2520are%253A%2520%25281%2529%2520a%2520two-level%250AHaar%2520wavelet%2520head%2520for%2520multi%2520orientation%2520edge%2520extraction%253B%2520and%2520%25282%2529%2520Wavelet%2520Edge%250AGuided%2520Attention%2520%2528WEGA%2529%2520modules%2520that%2520fuse%2520wavelet%2520cues%2520with%2520reverse%2520and%2520input%250Abranches.%2520On%2520five%2520public%2520polyp%2520datasets%252C%2520MEGANetW%2520consistently%2520outperforms%250Aexisting%2520methods%252C%2520improving%2520mIoU%2520by%2520up%2520to%25202.3%2525%2520and%2520mDice%2520by%25201.2%2525%252C%2520while%250Aintroducing%2520no%2520additional%2520learnable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection&entry.906535625=Zhe%20Yee%20Tan&entry.1292438233=%20%20Colorectal%20polyp%20segmentation%20is%20critical%20for%20early%20detection%20of%20colorectal%0Acancer%2C%20yet%20weak%20and%20low%20contrast%20boundaries%20significantly%20limit%20automated%0Aaccuracy.%20Existing%20deep%20models%20either%20blur%20fine%20edge%20details%20or%20rely%20on%0Ahandcrafted%20filters%20that%20perform%20poorly%20under%20variable%20imaging%20conditions.%20We%0Apropose%20MEGANet-W%2C%20a%20Wavelet%20Driven%20Edge%20Guided%20Attention%20Network%20that%20injects%0Adirectional%2C%20parameter%20free%20Haar%20wavelet%20edge%20maps%20into%20each%20decoder%20stage%20to%0Arecalibrate%20semantic%20features.%20Our%20two%20main%20contributions%20are%3A%20%281%29%20a%20two-level%0AHaar%20wavelet%20head%20for%20multi%20orientation%20edge%20extraction%3B%20and%20%282%29%20Wavelet%20Edge%0AGuided%20Attention%20%28WEGA%29%20modules%20that%20fuse%20wavelet%20cues%20with%20reverse%20and%20input%0Abranches.%20On%20five%20public%20polyp%20datasets%2C%20MEGANetW%20consistently%20outperforms%0Aexisting%20methods%2C%20improving%20mIoU%20by%20up%20to%202.3%25%20and%20mDice%20by%201.2%25%2C%20while%0Aintroducing%20no%20additional%20learnable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02668v1&entry.124074799=Read"},
{"title": "Trajectory Optimization for Differential Drive Mobile Manipulators via\n  Topological Paths Search and Arc Length-Yaw Parameterization", "author": "Long Xu and Choilam Wong and Mengke Zhang and Junxiao Lin and Fei Gao", "abstract": "  We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.\n", "link": "http://arxiv.org/abs/2507.02761v1", "date": "2025-07-03", "relevancy": 2.0166, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5178}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4983}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Optimization%20for%20Differential%20Drive%20Mobile%20Manipulators%20via%0A%20%20Topological%20Paths%20Search%20and%20Arc%20Length-Yaw%20Parameterization&body=Title%3A%20Trajectory%20Optimization%20for%20Differential%20Drive%20Mobile%20Manipulators%20via%0A%20%20Topological%20Paths%20Search%20and%20Arc%20Length-Yaw%20Parameterization%0AAuthor%3A%20Long%20Xu%20and%20Choilam%20Wong%20and%20Mengke%20Zhang%20and%20Junxiao%20Lin%20and%20Fei%20Gao%0AAbstract%3A%20%20%20We%20present%20an%20efficient%20hierarchical%20motion%20planning%20pipeline%20for%0Adifferential%20drive%20mobile%20manipulators.%20Our%20approach%20first%20searches%20for%0Amultiple%20collisionfree%20and%20topologically%20distinct%20paths%20for%20the%20mobile%20base%20to%0Aextract%20the%20space%20in%20which%20optimal%20solutions%20may%20exist.%20Further%20sampling%20and%0Aoptimization%20are%20then%20conducted%20in%20parallel%20to%20explore%20feasible%20whole-body%0Atrajectories.%20For%20trajectory%20optimization%2C%20we%20employ%20polynomial%20trajectories%0Aand%20arc%20length-yaw%20parameterization%2C%20enabling%20efficient%20handling%20of%20the%0Anonholonomic%20dynamics%20while%20ensuring%20optimality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Optimization%2520for%2520Differential%2520Drive%2520Mobile%2520Manipulators%2520via%250A%2520%2520Topological%2520Paths%2520Search%2520and%2520Arc%2520Length-Yaw%2520Parameterization%26entry.906535625%3DLong%2520Xu%2520and%2520Choilam%2520Wong%2520and%2520Mengke%2520Zhang%2520and%2520Junxiao%2520Lin%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520efficient%2520hierarchical%2520motion%2520planning%2520pipeline%2520for%250Adifferential%2520drive%2520mobile%2520manipulators.%2520Our%2520approach%2520first%2520searches%2520for%250Amultiple%2520collisionfree%2520and%2520topologically%2520distinct%2520paths%2520for%2520the%2520mobile%2520base%2520to%250Aextract%2520the%2520space%2520in%2520which%2520optimal%2520solutions%2520may%2520exist.%2520Further%2520sampling%2520and%250Aoptimization%2520are%2520then%2520conducted%2520in%2520parallel%2520to%2520explore%2520feasible%2520whole-body%250Atrajectories.%2520For%2520trajectory%2520optimization%252C%2520we%2520employ%2520polynomial%2520trajectories%250Aand%2520arc%2520length-yaw%2520parameterization%252C%2520enabling%2520efficient%2520handling%2520of%2520the%250Anonholonomic%2520dynamics%2520while%2520ensuring%2520optimality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Optimization%20for%20Differential%20Drive%20Mobile%20Manipulators%20via%0A%20%20Topological%20Paths%20Search%20and%20Arc%20Length-Yaw%20Parameterization&entry.906535625=Long%20Xu%20and%20Choilam%20Wong%20and%20Mengke%20Zhang%20and%20Junxiao%20Lin%20and%20Fei%20Gao&entry.1292438233=%20%20We%20present%20an%20efficient%20hierarchical%20motion%20planning%20pipeline%20for%0Adifferential%20drive%20mobile%20manipulators.%20Our%20approach%20first%20searches%20for%0Amultiple%20collisionfree%20and%20topologically%20distinct%20paths%20for%20the%20mobile%20base%20to%0Aextract%20the%20space%20in%20which%20optimal%20solutions%20may%20exist.%20Further%20sampling%20and%0Aoptimization%20are%20then%20conducted%20in%20parallel%20to%20explore%20feasible%20whole-body%0Atrajectories.%20For%20trajectory%20optimization%2C%20we%20employ%20polynomial%20trajectories%0Aand%20arc%20length-yaw%20parameterization%2C%20enabling%20efficient%20handling%20of%20the%0Anonholonomic%20dynamics%20while%20ensuring%20optimality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02761v1&entry.124074799=Read"},
{"title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and\n  Family Classification", "author": "Zehao Wu and Yanjie Zhao and Haoyu Wang", "abstract": "  As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering.\n", "link": "http://arxiv.org/abs/2506.01631v2", "date": "2025-07-03", "relevancy": 2.0157, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Based%20Model%20Fingerprinting%20for%20LLM%20Similarity%20Detection%20and%0A%20%20Family%20Classification&body=Title%3A%20Gradient-Based%20Model%20Fingerprinting%20for%20LLM%20Similarity%20Detection%20and%0A%20%20Family%20Classification%0AAuthor%3A%20Zehao%20Wu%20and%20Yanjie%20Zhao%20and%20Haoyu%20Wang%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20integral%20software%20components%20in%20modern%0Aapplications%2C%20unauthorized%20model%20derivations%20through%20fine-tuning%2C%20merging%2C%20and%0Aredistribution%20have%20emerged%20as%20critical%20software%20engineering%20challenges.%20Unlike%0Atraditional%20software%20where%20clone%20detection%20and%20license%20compliance%20are%0Awell-established%2C%20the%20LLM%20ecosystem%20lacks%20effective%20mechanisms%20to%20detect%20model%0Alineage%20and%20enforce%20licensing%20agreements.%20This%20gap%20is%20particularly%20problematic%0Awhen%20open-source%20model%20creators%2C%20such%20as%20Meta%27s%20LLaMA%2C%20require%20derivative%20works%0Ato%20maintain%20naming%20conventions%20for%20attribution%2C%20yet%20no%20technical%20means%20exist%20to%0Averify%20compliance.%0A%20%20To%20fill%20this%20gap%2C%20treating%20LLMs%20as%20software%20artifacts%20requiring%20provenance%0Atracking%2C%20we%20present%20TensorGuard%2C%20a%20gradient-based%20fingerprinting%20framework%20for%0ALLM%20similarity%20detection%20and%20family%20classification.%20Our%20approach%20extracts%0Amodel-intrinsic%20behavioral%20signatures%20by%20analyzing%20gradient%20responses%20to%20random%0Ainput%20perturbations%20across%20tensor%20layers%2C%20operating%20independently%20of%20training%0Adata%2C%20watermarks%2C%20or%20specific%20model%20formats.%20TensorGuard%20supports%20the%0Awidely-adopted%20safetensors%20format%20and%20constructs%20high-dimensional%20fingerprints%0Athrough%20statistical%20analysis%20of%20gradient%20features.%20These%20fingerprints%20enable%0Atwo%20complementary%20capabilities%3A%20direct%20pairwise%20similarity%20assessment%20between%0Aarbitrary%20models%20through%20distance%20computation%2C%20and%20systematic%20family%0Aclassification%20of%20unknown%20models%20via%20the%20K-Means%20clustering%20algorithm%20with%0Adomain-informed%20centroid%20initialization%20using%20known%20base%20models.%20Experimental%0Aevaluation%20on%2058%20models%20comprising%208%20base%20models%20and%2050%20derivatives%20across%20five%0Amodel%20families%20%28Llama%2C%20Qwen%2C%20Gemma%2C%20Phi%2C%20Mistral%29%20demonstrates%2094%25%0Aclassification%20accuracy%20under%20our%20centroid-initialized%20K-Means%20clustering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Based%2520Model%2520Fingerprinting%2520for%2520LLM%2520Similarity%2520Detection%2520and%250A%2520%2520Family%2520Classification%26entry.906535625%3DZehao%2520Wu%2520and%2520Yanjie%2520Zhao%2520and%2520Haoyu%2520Wang%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520integral%2520software%2520components%2520in%2520modern%250Aapplications%252C%2520unauthorized%2520model%2520derivations%2520through%2520fine-tuning%252C%2520merging%252C%2520and%250Aredistribution%2520have%2520emerged%2520as%2520critical%2520software%2520engineering%2520challenges.%2520Unlike%250Atraditional%2520software%2520where%2520clone%2520detection%2520and%2520license%2520compliance%2520are%250Awell-established%252C%2520the%2520LLM%2520ecosystem%2520lacks%2520effective%2520mechanisms%2520to%2520detect%2520model%250Alineage%2520and%2520enforce%2520licensing%2520agreements.%2520This%2520gap%2520is%2520particularly%2520problematic%250Awhen%2520open-source%2520model%2520creators%252C%2520such%2520as%2520Meta%2527s%2520LLaMA%252C%2520require%2520derivative%2520works%250Ato%2520maintain%2520naming%2520conventions%2520for%2520attribution%252C%2520yet%2520no%2520technical%2520means%2520exist%2520to%250Averify%2520compliance.%250A%2520%2520To%2520fill%2520this%2520gap%252C%2520treating%2520LLMs%2520as%2520software%2520artifacts%2520requiring%2520provenance%250Atracking%252C%2520we%2520present%2520TensorGuard%252C%2520a%2520gradient-based%2520fingerprinting%2520framework%2520for%250ALLM%2520similarity%2520detection%2520and%2520family%2520classification.%2520Our%2520approach%2520extracts%250Amodel-intrinsic%2520behavioral%2520signatures%2520by%2520analyzing%2520gradient%2520responses%2520to%2520random%250Ainput%2520perturbations%2520across%2520tensor%2520layers%252C%2520operating%2520independently%2520of%2520training%250Adata%252C%2520watermarks%252C%2520or%2520specific%2520model%2520formats.%2520TensorGuard%2520supports%2520the%250Awidely-adopted%2520safetensors%2520format%2520and%2520constructs%2520high-dimensional%2520fingerprints%250Athrough%2520statistical%2520analysis%2520of%2520gradient%2520features.%2520These%2520fingerprints%2520enable%250Atwo%2520complementary%2520capabilities%253A%2520direct%2520pairwise%2520similarity%2520assessment%2520between%250Aarbitrary%2520models%2520through%2520distance%2520computation%252C%2520and%2520systematic%2520family%250Aclassification%2520of%2520unknown%2520models%2520via%2520the%2520K-Means%2520clustering%2520algorithm%2520with%250Adomain-informed%2520centroid%2520initialization%2520using%2520known%2520base%2520models.%2520Experimental%250Aevaluation%2520on%252058%2520models%2520comprising%25208%2520base%2520models%2520and%252050%2520derivatives%2520across%2520five%250Amodel%2520families%2520%2528Llama%252C%2520Qwen%252C%2520Gemma%252C%2520Phi%252C%2520Mistral%2529%2520demonstrates%252094%2525%250Aclassification%2520accuracy%2520under%2520our%2520centroid-initialized%2520K-Means%2520clustering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Based%20Model%20Fingerprinting%20for%20LLM%20Similarity%20Detection%20and%0A%20%20Family%20Classification&entry.906535625=Zehao%20Wu%20and%20Yanjie%20Zhao%20and%20Haoyu%20Wang&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20integral%20software%20components%20in%20modern%0Aapplications%2C%20unauthorized%20model%20derivations%20through%20fine-tuning%2C%20merging%2C%20and%0Aredistribution%20have%20emerged%20as%20critical%20software%20engineering%20challenges.%20Unlike%0Atraditional%20software%20where%20clone%20detection%20and%20license%20compliance%20are%0Awell-established%2C%20the%20LLM%20ecosystem%20lacks%20effective%20mechanisms%20to%20detect%20model%0Alineage%20and%20enforce%20licensing%20agreements.%20This%20gap%20is%20particularly%20problematic%0Awhen%20open-source%20model%20creators%2C%20such%20as%20Meta%27s%20LLaMA%2C%20require%20derivative%20works%0Ato%20maintain%20naming%20conventions%20for%20attribution%2C%20yet%20no%20technical%20means%20exist%20to%0Averify%20compliance.%0A%20%20To%20fill%20this%20gap%2C%20treating%20LLMs%20as%20software%20artifacts%20requiring%20provenance%0Atracking%2C%20we%20present%20TensorGuard%2C%20a%20gradient-based%20fingerprinting%20framework%20for%0ALLM%20similarity%20detection%20and%20family%20classification.%20Our%20approach%20extracts%0Amodel-intrinsic%20behavioral%20signatures%20by%20analyzing%20gradient%20responses%20to%20random%0Ainput%20perturbations%20across%20tensor%20layers%2C%20operating%20independently%20of%20training%0Adata%2C%20watermarks%2C%20or%20specific%20model%20formats.%20TensorGuard%20supports%20the%0Awidely-adopted%20safetensors%20format%20and%20constructs%20high-dimensional%20fingerprints%0Athrough%20statistical%20analysis%20of%20gradient%20features.%20These%20fingerprints%20enable%0Atwo%20complementary%20capabilities%3A%20direct%20pairwise%20similarity%20assessment%20between%0Aarbitrary%20models%20through%20distance%20computation%2C%20and%20systematic%20family%0Aclassification%20of%20unknown%20models%20via%20the%20K-Means%20clustering%20algorithm%20with%0Adomain-informed%20centroid%20initialization%20using%20known%20base%20models.%20Experimental%0Aevaluation%20on%2058%20models%20comprising%208%20base%20models%20and%2050%20derivatives%20across%20five%0Amodel%20families%20%28Llama%2C%20Qwen%2C%20Gemma%2C%20Phi%2C%20Mistral%29%20demonstrates%2094%25%0Aclassification%20accuracy%20under%20our%20centroid-initialized%20K-Means%20clustering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01631v2&entry.124074799=Read"},
{"title": "Fair Deepfake Detectors Can Generalize", "author": "Harry Cheng and Ming-Hui Liu and Yangyang Guo and Tianyi Wang and Liqiang Nie and Mohan Kankanhalli", "abstract": "  Deepfake detection models face two critical challenges: generalization to\nunseen manipulations and demographic fairness among population groups. However,\nexisting approaches often demonstrate that these two objectives are inherently\nconflicting, revealing a trade-off between them. In this paper, we, for the\nfirst time, uncover and formally define a causal relationship between fairness\nand generalization. Building on the back-door adjustment, we show that\ncontrolling for confounders (data distribution and model capacity) enables\nimproved generalization via fairness interventions. Motivated by this insight,\nwe propose Demographic Attribute-insensitive Intervention Detection (DAID), a\nplug-and-play framework composed of: i) Demographic-aware data rebalancing,\nwhich employs inverse-propensity weighting and subgroup-wise feature\nnormalization to neutralize distributional biases; and ii) Demographic-agnostic\nfeature aggregation, which uses a novel alignment loss to suppress\nsensitive-attribute signals. Across three cross-domain benchmarks, DAID\nconsistently achieves superior performance in both fairness and generalization\ncompared to several state-of-the-art detectors, validating both its theoretical\nfoundation and practical effectiveness.\n", "link": "http://arxiv.org/abs/2507.02645v1", "date": "2025-07-03", "relevancy": 2.0082, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5168}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4931}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Deepfake%20Detectors%20Can%20Generalize&body=Title%3A%20Fair%20Deepfake%20Detectors%20Can%20Generalize%0AAuthor%3A%20Harry%20Cheng%20and%20Ming-Hui%20Liu%20and%20Yangyang%20Guo%20and%20Tianyi%20Wang%20and%20Liqiang%20Nie%20and%20Mohan%20Kankanhalli%0AAbstract%3A%20%20%20Deepfake%20detection%20models%20face%20two%20critical%20challenges%3A%20generalization%20to%0Aunseen%20manipulations%20and%20demographic%20fairness%20among%20population%20groups.%20However%2C%0Aexisting%20approaches%20often%20demonstrate%20that%20these%20two%20objectives%20are%20inherently%0Aconflicting%2C%20revealing%20a%20trade-off%20between%20them.%20In%20this%20paper%2C%20we%2C%20for%20the%0Afirst%20time%2C%20uncover%20and%20formally%20define%20a%20causal%20relationship%20between%20fairness%0Aand%20generalization.%20Building%20on%20the%20back-door%20adjustment%2C%20we%20show%20that%0Acontrolling%20for%20confounders%20%28data%20distribution%20and%20model%20capacity%29%20enables%0Aimproved%20generalization%20via%20fairness%20interventions.%20Motivated%20by%20this%20insight%2C%0Awe%20propose%20Demographic%20Attribute-insensitive%20Intervention%20Detection%20%28DAID%29%2C%20a%0Aplug-and-play%20framework%20composed%20of%3A%20i%29%20Demographic-aware%20data%20rebalancing%2C%0Awhich%20employs%20inverse-propensity%20weighting%20and%20subgroup-wise%20feature%0Anormalization%20to%20neutralize%20distributional%20biases%3B%20and%20ii%29%20Demographic-agnostic%0Afeature%20aggregation%2C%20which%20uses%20a%20novel%20alignment%20loss%20to%20suppress%0Asensitive-attribute%20signals.%20Across%20three%20cross-domain%20benchmarks%2C%20DAID%0Aconsistently%20achieves%20superior%20performance%20in%20both%20fairness%20and%20generalization%0Acompared%20to%20several%20state-of-the-art%20detectors%2C%20validating%20both%20its%20theoretical%0Afoundation%20and%20practical%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Deepfake%2520Detectors%2520Can%2520Generalize%26entry.906535625%3DHarry%2520Cheng%2520and%2520Ming-Hui%2520Liu%2520and%2520Yangyang%2520Guo%2520and%2520Tianyi%2520Wang%2520and%2520Liqiang%2520Nie%2520and%2520Mohan%2520Kankanhalli%26entry.1292438233%3D%2520%2520Deepfake%2520detection%2520models%2520face%2520two%2520critical%2520challenges%253A%2520generalization%2520to%250Aunseen%2520manipulations%2520and%2520demographic%2520fairness%2520among%2520population%2520groups.%2520However%252C%250Aexisting%2520approaches%2520often%2520demonstrate%2520that%2520these%2520two%2520objectives%2520are%2520inherently%250Aconflicting%252C%2520revealing%2520a%2520trade-off%2520between%2520them.%2520In%2520this%2520paper%252C%2520we%252C%2520for%2520the%250Afirst%2520time%252C%2520uncover%2520and%2520formally%2520define%2520a%2520causal%2520relationship%2520between%2520fairness%250Aand%2520generalization.%2520Building%2520on%2520the%2520back-door%2520adjustment%252C%2520we%2520show%2520that%250Acontrolling%2520for%2520confounders%2520%2528data%2520distribution%2520and%2520model%2520capacity%2529%2520enables%250Aimproved%2520generalization%2520via%2520fairness%2520interventions.%2520Motivated%2520by%2520this%2520insight%252C%250Awe%2520propose%2520Demographic%2520Attribute-insensitive%2520Intervention%2520Detection%2520%2528DAID%2529%252C%2520a%250Aplug-and-play%2520framework%2520composed%2520of%253A%2520i%2529%2520Demographic-aware%2520data%2520rebalancing%252C%250Awhich%2520employs%2520inverse-propensity%2520weighting%2520and%2520subgroup-wise%2520feature%250Anormalization%2520to%2520neutralize%2520distributional%2520biases%253B%2520and%2520ii%2529%2520Demographic-agnostic%250Afeature%2520aggregation%252C%2520which%2520uses%2520a%2520novel%2520alignment%2520loss%2520to%2520suppress%250Asensitive-attribute%2520signals.%2520Across%2520three%2520cross-domain%2520benchmarks%252C%2520DAID%250Aconsistently%2520achieves%2520superior%2520performance%2520in%2520both%2520fairness%2520and%2520generalization%250Acompared%2520to%2520several%2520state-of-the-art%2520detectors%252C%2520validating%2520both%2520its%2520theoretical%250Afoundation%2520and%2520practical%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Deepfake%20Detectors%20Can%20Generalize&entry.906535625=Harry%20Cheng%20and%20Ming-Hui%20Liu%20and%20Yangyang%20Guo%20and%20Tianyi%20Wang%20and%20Liqiang%20Nie%20and%20Mohan%20Kankanhalli&entry.1292438233=%20%20Deepfake%20detection%20models%20face%20two%20critical%20challenges%3A%20generalization%20to%0Aunseen%20manipulations%20and%20demographic%20fairness%20among%20population%20groups.%20However%2C%0Aexisting%20approaches%20often%20demonstrate%20that%20these%20two%20objectives%20are%20inherently%0Aconflicting%2C%20revealing%20a%20trade-off%20between%20them.%20In%20this%20paper%2C%20we%2C%20for%20the%0Afirst%20time%2C%20uncover%20and%20formally%20define%20a%20causal%20relationship%20between%20fairness%0Aand%20generalization.%20Building%20on%20the%20back-door%20adjustment%2C%20we%20show%20that%0Acontrolling%20for%20confounders%20%28data%20distribution%20and%20model%20capacity%29%20enables%0Aimproved%20generalization%20via%20fairness%20interventions.%20Motivated%20by%20this%20insight%2C%0Awe%20propose%20Demographic%20Attribute-insensitive%20Intervention%20Detection%20%28DAID%29%2C%20a%0Aplug-and-play%20framework%20composed%20of%3A%20i%29%20Demographic-aware%20data%20rebalancing%2C%0Awhich%20employs%20inverse-propensity%20weighting%20and%20subgroup-wise%20feature%0Anormalization%20to%20neutralize%20distributional%20biases%3B%20and%20ii%29%20Demographic-agnostic%0Afeature%20aggregation%2C%20which%20uses%20a%20novel%20alignment%20loss%20to%20suppress%0Asensitive-attribute%20signals.%20Across%20three%20cross-domain%20benchmarks%2C%20DAID%0Aconsistently%20achieves%20superior%20performance%20in%20both%20fairness%20and%20generalization%0Acompared%20to%20several%20state-of-the-art%20detectors%2C%20validating%20both%20its%20theoretical%0Afoundation%20and%20practical%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02645v1&entry.124074799=Read"},
{"title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "author": "Artur Zolkowski and Kei Nishimura-Gasparian and Robert McCarthy and Roland S. Zimmermann and David Lindner", "abstract": "  Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.\n", "link": "http://arxiv.org/abs/2507.02737v1", "date": "2025-07-03", "relevancy": 1.9974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Signs%20of%20Steganographic%20Capabilities%20in%20Frontier%20LLMs&body=Title%3A%20Early%20Signs%20of%20Steganographic%20Capabilities%20in%20Frontier%20LLMs%0AAuthor%3A%20Artur%20Zolkowski%20and%20Kei%20Nishimura-Gasparian%20and%20Robert%20McCarthy%20and%20Roland%20S.%20Zimmermann%20and%20David%20Lindner%0AAbstract%3A%20%20%20Monitoring%20Large%20Language%20Model%20%28LLM%29%20outputs%20is%20crucial%20for%20mitigating%20risks%0Afrom%20misuse%20and%20misalignment.%20However%2C%20LLMs%20could%20evade%20monitoring%20through%0Asteganography%3A%20Encoding%20hidden%20information%20within%20seemingly%20benign%20generations.%0AIn%20this%20paper%2C%20we%20evaluate%20the%20steganography%20capabilities%20in%20frontier%20LLMs%20to%0Abetter%20understand%20the%20risk%20they%20pose.%20We%20focus%20on%20two%20types%20of%20steganography%3A%0Apassing%20encoded%20messages%20and%20performing%20encoded%20reasoning.%20We%20find%20that%20current%0Amodels%20are%20unable%20to%20encode%20short%20messages%20in%20their%20outputs%20without%20a%20monitor%0Anoticing%20under%20standard%20affordances.%20They%20can%20succeed%2C%20however%2C%20if%20given%0Aadditional%20affordances%20such%20as%20using%20an%20unmonitored%20scratchpad%20and%20coordinating%0Aon%20what%20encoding%20scheme%20to%20use.%20We%20additionally%20find%20early%20signs%20that%20models%0Acan%20perform%20basic%20encoded%20reasoning%20in%20a%20simple%20state-tracking%20problem.%20This%0Aincludes%20some%20ability%20to%20reason%20with%20their%20own%20and%20pre-defined%20schemes%2C%0Aincluding%20encoding%20schemes%20such%20as%20Hexadecimal.%20Despite%20this%2C%20they%20can%20rarely%0Ahide%20reasoning%20subtly%20within%20a%20cover%20task%20to%20fool%20a%20monitor.%20Overall%2C%20our%0Aresults%20indicate%20that%20current%20LLMs%20exhibit%20nascent%20steganographic%20capabilities.%0AWhile%20these%20capabilities%20are%20likely%20insufficient%20to%20bypass%20well-designed%0Amonitors%20at%20present%2C%20this%20could%20change%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Signs%2520of%2520Steganographic%2520Capabilities%2520in%2520Frontier%2520LLMs%26entry.906535625%3DArtur%2520Zolkowski%2520and%2520Kei%2520Nishimura-Gasparian%2520and%2520Robert%2520McCarthy%2520and%2520Roland%2520S.%2520Zimmermann%2520and%2520David%2520Lindner%26entry.1292438233%3D%2520%2520Monitoring%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520outputs%2520is%2520crucial%2520for%2520mitigating%2520risks%250Afrom%2520misuse%2520and%2520misalignment.%2520However%252C%2520LLMs%2520could%2520evade%2520monitoring%2520through%250Asteganography%253A%2520Encoding%2520hidden%2520information%2520within%2520seemingly%2520benign%2520generations.%250AIn%2520this%2520paper%252C%2520we%2520evaluate%2520the%2520steganography%2520capabilities%2520in%2520frontier%2520LLMs%2520to%250Abetter%2520understand%2520the%2520risk%2520they%2520pose.%2520We%2520focus%2520on%2520two%2520types%2520of%2520steganography%253A%250Apassing%2520encoded%2520messages%2520and%2520performing%2520encoded%2520reasoning.%2520We%2520find%2520that%2520current%250Amodels%2520are%2520unable%2520to%2520encode%2520short%2520messages%2520in%2520their%2520outputs%2520without%2520a%2520monitor%250Anoticing%2520under%2520standard%2520affordances.%2520They%2520can%2520succeed%252C%2520however%252C%2520if%2520given%250Aadditional%2520affordances%2520such%2520as%2520using%2520an%2520unmonitored%2520scratchpad%2520and%2520coordinating%250Aon%2520what%2520encoding%2520scheme%2520to%2520use.%2520We%2520additionally%2520find%2520early%2520signs%2520that%2520models%250Acan%2520perform%2520basic%2520encoded%2520reasoning%2520in%2520a%2520simple%2520state-tracking%2520problem.%2520This%250Aincludes%2520some%2520ability%2520to%2520reason%2520with%2520their%2520own%2520and%2520pre-defined%2520schemes%252C%250Aincluding%2520encoding%2520schemes%2520such%2520as%2520Hexadecimal.%2520Despite%2520this%252C%2520they%2520can%2520rarely%250Ahide%2520reasoning%2520subtly%2520within%2520a%2520cover%2520task%2520to%2520fool%2520a%2520monitor.%2520Overall%252C%2520our%250Aresults%2520indicate%2520that%2520current%2520LLMs%2520exhibit%2520nascent%2520steganographic%2520capabilities.%250AWhile%2520these%2520capabilities%2520are%2520likely%2520insufficient%2520to%2520bypass%2520well-designed%250Amonitors%2520at%2520present%252C%2520this%2520could%2520change%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Signs%20of%20Steganographic%20Capabilities%20in%20Frontier%20LLMs&entry.906535625=Artur%20Zolkowski%20and%20Kei%20Nishimura-Gasparian%20and%20Robert%20McCarthy%20and%20Roland%20S.%20Zimmermann%20and%20David%20Lindner&entry.1292438233=%20%20Monitoring%20Large%20Language%20Model%20%28LLM%29%20outputs%20is%20crucial%20for%20mitigating%20risks%0Afrom%20misuse%20and%20misalignment.%20However%2C%20LLMs%20could%20evade%20monitoring%20through%0Asteganography%3A%20Encoding%20hidden%20information%20within%20seemingly%20benign%20generations.%0AIn%20this%20paper%2C%20we%20evaluate%20the%20steganography%20capabilities%20in%20frontier%20LLMs%20to%0Abetter%20understand%20the%20risk%20they%20pose.%20We%20focus%20on%20two%20types%20of%20steganography%3A%0Apassing%20encoded%20messages%20and%20performing%20encoded%20reasoning.%20We%20find%20that%20current%0Amodels%20are%20unable%20to%20encode%20short%20messages%20in%20their%20outputs%20without%20a%20monitor%0Anoticing%20under%20standard%20affordances.%20They%20can%20succeed%2C%20however%2C%20if%20given%0Aadditional%20affordances%20such%20as%20using%20an%20unmonitored%20scratchpad%20and%20coordinating%0Aon%20what%20encoding%20scheme%20to%20use.%20We%20additionally%20find%20early%20signs%20that%20models%0Acan%20perform%20basic%20encoded%20reasoning%20in%20a%20simple%20state-tracking%20problem.%20This%0Aincludes%20some%20ability%20to%20reason%20with%20their%20own%20and%20pre-defined%20schemes%2C%0Aincluding%20encoding%20schemes%20such%20as%20Hexadecimal.%20Despite%20this%2C%20they%20can%20rarely%0Ahide%20reasoning%20subtly%20within%20a%20cover%20task%20to%20fool%20a%20monitor.%20Overall%2C%20our%0Aresults%20indicate%20that%20current%20LLMs%20exhibit%20nascent%20steganographic%20capabilities.%0AWhile%20these%20capabilities%20are%20likely%20insufficient%20to%20bypass%20well-designed%0Amonitors%20at%20present%2C%20this%20could%20change%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02737v1&entry.124074799=Read"},
{"title": "Pad\u00e9 Approximant Neural Networks for Enhanced Electric Motor Fault\n  Diagnosis Using Vibration and Acoustic Data", "author": "Sertac Kilickaya and Levent Eren", "abstract": "  Purpose: The primary aim of this study is to enhance fault diagnosis in\ninduction machines by leveraging the Pad\\'e Approximant Neuron (PAON) model.\nWhile accelerometers and microphones are standard in motor condition\nmonitoring, deep learning models with nonlinear neuron architectures offer\npromising improvements in diagnostic performance. This research addresses the\nquestion: Can Pad\\'e Approximant Neural Networks (Pad\\'eNets) outperform\nconventional Convolutional Neural Networks (CNNs) and Self-Organized\nOperational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical\nfaults using vibration and acoustic data?\n  Methods: We evaluate and compare the diagnostic capabilities of three deep\nlearning architectures: one-dimensional CNNs, Self-ONNs, and Pad\\'eNets. These\nmodels are tested on the University of Ottawa's publicly available\nconstant-speed induction motor datasets, which include both vibration and\nacoustic sensor data. The Pad\\'eNet model is designed to introduce enhanced\nnonlinearity and is compatible with unbounded activation functions such as\nLeaky ReLU.\n  Results and Conclusion: Pad\\'eNets consistently outperformed the baseline\nmodels, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%\nfor accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced\nnonlinearity of Pad\\'eNets, together with their compatibility with unbounded\nactivation functions, significantly improves fault diagnosis performance in\ninduction motor condition monitoring.\n", "link": "http://arxiv.org/abs/2507.02599v1", "date": "2025-07-03", "relevancy": 1.9807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5062}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4952}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pad%C3%A9%20Approximant%20Neural%20Networks%20for%20Enhanced%20Electric%20Motor%20Fault%0A%20%20Diagnosis%20Using%20Vibration%20and%20Acoustic%20Data&body=Title%3A%20Pad%C3%A9%20Approximant%20Neural%20Networks%20for%20Enhanced%20Electric%20Motor%20Fault%0A%20%20Diagnosis%20Using%20Vibration%20and%20Acoustic%20Data%0AAuthor%3A%20Sertac%20Kilickaya%20and%20Levent%20Eren%0AAbstract%3A%20%20%20Purpose%3A%20The%20primary%20aim%20of%20this%20study%20is%20to%20enhance%20fault%20diagnosis%20in%0Ainduction%20machines%20by%20leveraging%20the%20Pad%5C%27e%20Approximant%20Neuron%20%28PAON%29%20model.%0AWhile%20accelerometers%20and%20microphones%20are%20standard%20in%20motor%20condition%0Amonitoring%2C%20deep%20learning%20models%20with%20nonlinear%20neuron%20architectures%20offer%0Apromising%20improvements%20in%20diagnostic%20performance.%20This%20research%20addresses%20the%0Aquestion%3A%20Can%20Pad%5C%27e%20Approximant%20Neural%20Networks%20%28Pad%5C%27eNets%29%20outperform%0Aconventional%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Self-Organized%0AOperational%20Neural%20Networks%20%28Self-ONNs%29%20in%20diagnosing%20electrical%20and%20mechanical%0Afaults%20using%20vibration%20and%20acoustic%20data%3F%0A%20%20Methods%3A%20We%20evaluate%20and%20compare%20the%20diagnostic%20capabilities%20of%20three%20deep%0Alearning%20architectures%3A%20one-dimensional%20CNNs%2C%20Self-ONNs%2C%20and%20Pad%5C%27eNets.%20These%0Amodels%20are%20tested%20on%20the%20University%20of%20Ottawa%27s%20publicly%20available%0Aconstant-speed%20induction%20motor%20datasets%2C%20which%20include%20both%20vibration%20and%0Aacoustic%20sensor%20data.%20The%20Pad%5C%27eNet%20model%20is%20designed%20to%20introduce%20enhanced%0Anonlinearity%20and%20is%20compatible%20with%20unbounded%20activation%20functions%20such%20as%0ALeaky%20ReLU.%0A%20%20Results%20and%20Conclusion%3A%20Pad%5C%27eNets%20consistently%20outperformed%20the%20baseline%0Amodels%2C%20achieving%20diagnostic%20accuracies%20of%2099.96%25%2C%2098.26%25%2C%2097.61%25%2C%20and%2098.33%25%0Afor%20accelerometers%201%2C%202%2C%203%2C%20and%20the%20acoustic%20sensor%2C%20respectively.%20The%20enhanced%0Anonlinearity%20of%20Pad%5C%27eNets%2C%20together%20with%20their%20compatibility%20with%20unbounded%0Aactivation%20functions%2C%20significantly%20improves%20fault%20diagnosis%20performance%20in%0Ainduction%20motor%20condition%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPad%25C3%25A9%2520Approximant%2520Neural%2520Networks%2520for%2520Enhanced%2520Electric%2520Motor%2520Fault%250A%2520%2520Diagnosis%2520Using%2520Vibration%2520and%2520Acoustic%2520Data%26entry.906535625%3DSertac%2520Kilickaya%2520and%2520Levent%2520Eren%26entry.1292438233%3D%2520%2520Purpose%253A%2520The%2520primary%2520aim%2520of%2520this%2520study%2520is%2520to%2520enhance%2520fault%2520diagnosis%2520in%250Ainduction%2520machines%2520by%2520leveraging%2520the%2520Pad%255C%2527e%2520Approximant%2520Neuron%2520%2528PAON%2529%2520model.%250AWhile%2520accelerometers%2520and%2520microphones%2520are%2520standard%2520in%2520motor%2520condition%250Amonitoring%252C%2520deep%2520learning%2520models%2520with%2520nonlinear%2520neuron%2520architectures%2520offer%250Apromising%2520improvements%2520in%2520diagnostic%2520performance.%2520This%2520research%2520addresses%2520the%250Aquestion%253A%2520Can%2520Pad%255C%2527e%2520Approximant%2520Neural%2520Networks%2520%2528Pad%255C%2527eNets%2529%2520outperform%250Aconventional%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Self-Organized%250AOperational%2520Neural%2520Networks%2520%2528Self-ONNs%2529%2520in%2520diagnosing%2520electrical%2520and%2520mechanical%250Afaults%2520using%2520vibration%2520and%2520acoustic%2520data%253F%250A%2520%2520Methods%253A%2520We%2520evaluate%2520and%2520compare%2520the%2520diagnostic%2520capabilities%2520of%2520three%2520deep%250Alearning%2520architectures%253A%2520one-dimensional%2520CNNs%252C%2520Self-ONNs%252C%2520and%2520Pad%255C%2527eNets.%2520These%250Amodels%2520are%2520tested%2520on%2520the%2520University%2520of%2520Ottawa%2527s%2520publicly%2520available%250Aconstant-speed%2520induction%2520motor%2520datasets%252C%2520which%2520include%2520both%2520vibration%2520and%250Aacoustic%2520sensor%2520data.%2520The%2520Pad%255C%2527eNet%2520model%2520is%2520designed%2520to%2520introduce%2520enhanced%250Anonlinearity%2520and%2520is%2520compatible%2520with%2520unbounded%2520activation%2520functions%2520such%2520as%250ALeaky%2520ReLU.%250A%2520%2520Results%2520and%2520Conclusion%253A%2520Pad%255C%2527eNets%2520consistently%2520outperformed%2520the%2520baseline%250Amodels%252C%2520achieving%2520diagnostic%2520accuracies%2520of%252099.96%2525%252C%252098.26%2525%252C%252097.61%2525%252C%2520and%252098.33%2525%250Afor%2520accelerometers%25201%252C%25202%252C%25203%252C%2520and%2520the%2520acoustic%2520sensor%252C%2520respectively.%2520The%2520enhanced%250Anonlinearity%2520of%2520Pad%255C%2527eNets%252C%2520together%2520with%2520their%2520compatibility%2520with%2520unbounded%250Aactivation%2520functions%252C%2520significantly%2520improves%2520fault%2520diagnosis%2520performance%2520in%250Ainduction%2520motor%2520condition%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pad%C3%A9%20Approximant%20Neural%20Networks%20for%20Enhanced%20Electric%20Motor%20Fault%0A%20%20Diagnosis%20Using%20Vibration%20and%20Acoustic%20Data&entry.906535625=Sertac%20Kilickaya%20and%20Levent%20Eren&entry.1292438233=%20%20Purpose%3A%20The%20primary%20aim%20of%20this%20study%20is%20to%20enhance%20fault%20diagnosis%20in%0Ainduction%20machines%20by%20leveraging%20the%20Pad%5C%27e%20Approximant%20Neuron%20%28PAON%29%20model.%0AWhile%20accelerometers%20and%20microphones%20are%20standard%20in%20motor%20condition%0Amonitoring%2C%20deep%20learning%20models%20with%20nonlinear%20neuron%20architectures%20offer%0Apromising%20improvements%20in%20diagnostic%20performance.%20This%20research%20addresses%20the%0Aquestion%3A%20Can%20Pad%5C%27e%20Approximant%20Neural%20Networks%20%28Pad%5C%27eNets%29%20outperform%0Aconventional%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Self-Organized%0AOperational%20Neural%20Networks%20%28Self-ONNs%29%20in%20diagnosing%20electrical%20and%20mechanical%0Afaults%20using%20vibration%20and%20acoustic%20data%3F%0A%20%20Methods%3A%20We%20evaluate%20and%20compare%20the%20diagnostic%20capabilities%20of%20three%20deep%0Alearning%20architectures%3A%20one-dimensional%20CNNs%2C%20Self-ONNs%2C%20and%20Pad%5C%27eNets.%20These%0Amodels%20are%20tested%20on%20the%20University%20of%20Ottawa%27s%20publicly%20available%0Aconstant-speed%20induction%20motor%20datasets%2C%20which%20include%20both%20vibration%20and%0Aacoustic%20sensor%20data.%20The%20Pad%5C%27eNet%20model%20is%20designed%20to%20introduce%20enhanced%0Anonlinearity%20and%20is%20compatible%20with%20unbounded%20activation%20functions%20such%20as%0ALeaky%20ReLU.%0A%20%20Results%20and%20Conclusion%3A%20Pad%5C%27eNets%20consistently%20outperformed%20the%20baseline%0Amodels%2C%20achieving%20diagnostic%20accuracies%20of%2099.96%25%2C%2098.26%25%2C%2097.61%25%2C%20and%2098.33%25%0Afor%20accelerometers%201%2C%202%2C%203%2C%20and%20the%20acoustic%20sensor%2C%20respectively.%20The%20enhanced%0Anonlinearity%20of%20Pad%5C%27eNets%2C%20together%20with%20their%20compatibility%20with%20unbounded%0Aactivation%20functions%2C%20significantly%20improves%20fault%20diagnosis%20performance%20in%0Ainduction%20motor%20condition%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02599v1&entry.124074799=Read"},
{"title": "High-Order Deep Meta-Learning with Category-Theoretic Interpretation", "author": "David H. Mguni", "abstract": "  We introduce a new hierarchical deep learning framework for recursive\nhigher-order meta-learning that enables neural networks (NNs) to construct,\nsolve, and generalise across hierarchies of tasks. Central to this approach is\na generative mechanism that creates \\emph{virtual tasks} -- synthetic problem\ninstances designed to enable the meta-learner to learn \\emph{soft constraints}\nand unknown generalisable rules across related tasks. Crucially, this enables\nthe framework to generate its own informative, task-grounded datasets thereby\nfreeing machine learning (ML) training from the limitations of relying entirely\non human-generated data. By actively exploring the virtual point landscape and\nseeking out tasks lower-level learners find difficult, the meta-learner\niteratively refines constraint regions. This enhances inductive biases,\nregularises the adaptation process, and produces novel, unanticipated tasks and\nconstraints required for generalisation. Each meta-level of the hierarchy\ncorresponds to a progressively abstracted generalisation of problems solved at\nlower levels, enabling a structured and interpretable learning progression. By\ninterpreting meta-learners as category-theoretic \\emph{functors} that generate\nand condition a hierarchy of subordinate learners, we establish a compositional\nstructure that supports abstraction and knowledge transfer across progressively\ngeneralised tasks. The category-theoretic perspective unifies existing\nmeta-learning models and reveals how learning processes can be transformed and\ncompared through functorial relationships, while offering practical design\nprinciples for structuring meta-learning. We speculate this architecture may\nunderpin the next generation of NNs capable of autonomously generating novel,\ninstructive tasks and their solutions, thereby advancing ML towards general\nartificial intelligence.\n", "link": "http://arxiv.org/abs/2507.02634v1", "date": "2025-07-03", "relevancy": 1.9796, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4982}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Order%20Deep%20Meta-Learning%20with%20Category-Theoretic%20Interpretation&body=Title%3A%20High-Order%20Deep%20Meta-Learning%20with%20Category-Theoretic%20Interpretation%0AAuthor%3A%20David%20H.%20Mguni%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20hierarchical%20deep%20learning%20framework%20for%20recursive%0Ahigher-order%20meta-learning%20that%20enables%20neural%20networks%20%28NNs%29%20to%20construct%2C%0Asolve%2C%20and%20generalise%20across%20hierarchies%20of%20tasks.%20Central%20to%20this%20approach%20is%0Aa%20generative%20mechanism%20that%20creates%20%5Cemph%7Bvirtual%20tasks%7D%20--%20synthetic%20problem%0Ainstances%20designed%20to%20enable%20the%20meta-learner%20to%20learn%20%5Cemph%7Bsoft%20constraints%7D%0Aand%20unknown%20generalisable%20rules%20across%20related%20tasks.%20Crucially%2C%20this%20enables%0Athe%20framework%20to%20generate%20its%20own%20informative%2C%20task-grounded%20datasets%20thereby%0Afreeing%20machine%20learning%20%28ML%29%20training%20from%20the%20limitations%20of%20relying%20entirely%0Aon%20human-generated%20data.%20By%20actively%20exploring%20the%20virtual%20point%20landscape%20and%0Aseeking%20out%20tasks%20lower-level%20learners%20find%20difficult%2C%20the%20meta-learner%0Aiteratively%20refines%20constraint%20regions.%20This%20enhances%20inductive%20biases%2C%0Aregularises%20the%20adaptation%20process%2C%20and%20produces%20novel%2C%20unanticipated%20tasks%20and%0Aconstraints%20required%20for%20generalisation.%20Each%20meta-level%20of%20the%20hierarchy%0Acorresponds%20to%20a%20progressively%20abstracted%20generalisation%20of%20problems%20solved%20at%0Alower%20levels%2C%20enabling%20a%20structured%20and%20interpretable%20learning%20progression.%20By%0Ainterpreting%20meta-learners%20as%20category-theoretic%20%5Cemph%7Bfunctors%7D%20that%20generate%0Aand%20condition%20a%20hierarchy%20of%20subordinate%20learners%2C%20we%20establish%20a%20compositional%0Astructure%20that%20supports%20abstraction%20and%20knowledge%20transfer%20across%20progressively%0Ageneralised%20tasks.%20The%20category-theoretic%20perspective%20unifies%20existing%0Ameta-learning%20models%20and%20reveals%20how%20learning%20processes%20can%20be%20transformed%20and%0Acompared%20through%20functorial%20relationships%2C%20while%20offering%20practical%20design%0Aprinciples%20for%20structuring%20meta-learning.%20We%20speculate%20this%20architecture%20may%0Aunderpin%20the%20next%20generation%20of%20NNs%20capable%20of%20autonomously%20generating%20novel%2C%0Ainstructive%20tasks%20and%20their%20solutions%2C%20thereby%20advancing%20ML%20towards%20general%0Aartificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Order%2520Deep%2520Meta-Learning%2520with%2520Category-Theoretic%2520Interpretation%26entry.906535625%3DDavid%2520H.%2520Mguni%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520hierarchical%2520deep%2520learning%2520framework%2520for%2520recursive%250Ahigher-order%2520meta-learning%2520that%2520enables%2520neural%2520networks%2520%2528NNs%2529%2520to%2520construct%252C%250Asolve%252C%2520and%2520generalise%2520across%2520hierarchies%2520of%2520tasks.%2520Central%2520to%2520this%2520approach%2520is%250Aa%2520generative%2520mechanism%2520that%2520creates%2520%255Cemph%257Bvirtual%2520tasks%257D%2520--%2520synthetic%2520problem%250Ainstances%2520designed%2520to%2520enable%2520the%2520meta-learner%2520to%2520learn%2520%255Cemph%257Bsoft%2520constraints%257D%250Aand%2520unknown%2520generalisable%2520rules%2520across%2520related%2520tasks.%2520Crucially%252C%2520this%2520enables%250Athe%2520framework%2520to%2520generate%2520its%2520own%2520informative%252C%2520task-grounded%2520datasets%2520thereby%250Afreeing%2520machine%2520learning%2520%2528ML%2529%2520training%2520from%2520the%2520limitations%2520of%2520relying%2520entirely%250Aon%2520human-generated%2520data.%2520By%2520actively%2520exploring%2520the%2520virtual%2520point%2520landscape%2520and%250Aseeking%2520out%2520tasks%2520lower-level%2520learners%2520find%2520difficult%252C%2520the%2520meta-learner%250Aiteratively%2520refines%2520constraint%2520regions.%2520This%2520enhances%2520inductive%2520biases%252C%250Aregularises%2520the%2520adaptation%2520process%252C%2520and%2520produces%2520novel%252C%2520unanticipated%2520tasks%2520and%250Aconstraints%2520required%2520for%2520generalisation.%2520Each%2520meta-level%2520of%2520the%2520hierarchy%250Acorresponds%2520to%2520a%2520progressively%2520abstracted%2520generalisation%2520of%2520problems%2520solved%2520at%250Alower%2520levels%252C%2520enabling%2520a%2520structured%2520and%2520interpretable%2520learning%2520progression.%2520By%250Ainterpreting%2520meta-learners%2520as%2520category-theoretic%2520%255Cemph%257Bfunctors%257D%2520that%2520generate%250Aand%2520condition%2520a%2520hierarchy%2520of%2520subordinate%2520learners%252C%2520we%2520establish%2520a%2520compositional%250Astructure%2520that%2520supports%2520abstraction%2520and%2520knowledge%2520transfer%2520across%2520progressively%250Ageneralised%2520tasks.%2520The%2520category-theoretic%2520perspective%2520unifies%2520existing%250Ameta-learning%2520models%2520and%2520reveals%2520how%2520learning%2520processes%2520can%2520be%2520transformed%2520and%250Acompared%2520through%2520functorial%2520relationships%252C%2520while%2520offering%2520practical%2520design%250Aprinciples%2520for%2520structuring%2520meta-learning.%2520We%2520speculate%2520this%2520architecture%2520may%250Aunderpin%2520the%2520next%2520generation%2520of%2520NNs%2520capable%2520of%2520autonomously%2520generating%2520novel%252C%250Ainstructive%2520tasks%2520and%2520their%2520solutions%252C%2520thereby%2520advancing%2520ML%2520towards%2520general%250Aartificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Order%20Deep%20Meta-Learning%20with%20Category-Theoretic%20Interpretation&entry.906535625=David%20H.%20Mguni&entry.1292438233=%20%20We%20introduce%20a%20new%20hierarchical%20deep%20learning%20framework%20for%20recursive%0Ahigher-order%20meta-learning%20that%20enables%20neural%20networks%20%28NNs%29%20to%20construct%2C%0Asolve%2C%20and%20generalise%20across%20hierarchies%20of%20tasks.%20Central%20to%20this%20approach%20is%0Aa%20generative%20mechanism%20that%20creates%20%5Cemph%7Bvirtual%20tasks%7D%20--%20synthetic%20problem%0Ainstances%20designed%20to%20enable%20the%20meta-learner%20to%20learn%20%5Cemph%7Bsoft%20constraints%7D%0Aand%20unknown%20generalisable%20rules%20across%20related%20tasks.%20Crucially%2C%20this%20enables%0Athe%20framework%20to%20generate%20its%20own%20informative%2C%20task-grounded%20datasets%20thereby%0Afreeing%20machine%20learning%20%28ML%29%20training%20from%20the%20limitations%20of%20relying%20entirely%0Aon%20human-generated%20data.%20By%20actively%20exploring%20the%20virtual%20point%20landscape%20and%0Aseeking%20out%20tasks%20lower-level%20learners%20find%20difficult%2C%20the%20meta-learner%0Aiteratively%20refines%20constraint%20regions.%20This%20enhances%20inductive%20biases%2C%0Aregularises%20the%20adaptation%20process%2C%20and%20produces%20novel%2C%20unanticipated%20tasks%20and%0Aconstraints%20required%20for%20generalisation.%20Each%20meta-level%20of%20the%20hierarchy%0Acorresponds%20to%20a%20progressively%20abstracted%20generalisation%20of%20problems%20solved%20at%0Alower%20levels%2C%20enabling%20a%20structured%20and%20interpretable%20learning%20progression.%20By%0Ainterpreting%20meta-learners%20as%20category-theoretic%20%5Cemph%7Bfunctors%7D%20that%20generate%0Aand%20condition%20a%20hierarchy%20of%20subordinate%20learners%2C%20we%20establish%20a%20compositional%0Astructure%20that%20supports%20abstraction%20and%20knowledge%20transfer%20across%20progressively%0Ageneralised%20tasks.%20The%20category-theoretic%20perspective%20unifies%20existing%0Ameta-learning%20models%20and%20reveals%20how%20learning%20processes%20can%20be%20transformed%20and%0Acompared%20through%20functorial%20relationships%2C%20while%20offering%20practical%20design%0Aprinciples%20for%20structuring%20meta-learning.%20We%20speculate%20this%20architecture%20may%0Aunderpin%20the%20next%20generation%20of%20NNs%20capable%20of%20autonomously%20generating%20novel%2C%0Ainstructive%20tasks%20and%20their%20solutions%2C%20thereby%20advancing%20ML%20towards%20general%0Aartificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02634v1&entry.124074799=Read"},
{"title": "Urban Region Pre-training and Prompting: A Graph-based Approach", "author": "Jiahui Jin and Yifan Song and Dong Kan and Haojia Zhu and Xiangguo Sun and Zhicheng Li and Xigang Sun and Jinghui Zhang", "abstract": "  Urban region representation is crucial for various urban downstream tasks.\nHowever, despite the proliferation of methods and their success, acquiring\ngeneral urban region knowledge and adapting to different tasks remains\nchallenging. Existing work pays limited attention to the fine-grained\nfunctional layout semantics in urban regions, limiting their ability to capture\ntransferable knowledge across regions. Further, inadequate handling of the\nunique features and relationships required for different downstream tasks may\nalso hinder effective task adaptation. In this paper, we propose a\n$\\textbf{G}$raph-based $\\textbf{U}$rban $\\textbf{R}$egion\n$\\textbf{P}$re-training and $\\textbf{P}$rompting framework ($\\textbf{GURPP}$)\nfor region representation learning. Specifically, we first construct an urban\nregion graph and develop a subgraph-centric urban region pre-training model to\ncapture the heterogeneous and transferable patterns of entity interactions.\nThis model pre-trains knowledge-rich region embeddings using contrastive\nlearning and multi-view learning methods. To further refine these\nrepresentations, we design two graph-based prompting methods: a\nmanually-defined prompt to incorporate explicit task knowledge and a\ntask-learnable prompt to discover hidden knowledge, which enhances the\nadaptability of these embeddings to different tasks. Extensive experiments on\nvarious urban region prediction tasks and different cities demonstrate the\nsuperior performance of our framework.\n", "link": "http://arxiv.org/abs/2408.05920v4", "date": "2025-07-03", "relevancy": 1.9793, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4907}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&body=Title%3A%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach%0AAuthor%3A%20Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang%0AAbstract%3A%20%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Existing%20work%20pays%20limited%20attention%20to%20the%20fine-grained%0Afunctional%20layout%20semantics%20in%20urban%20regions%2C%20limiting%20their%20ability%20to%20capture%0Atransferable%20knowledge%20across%20regions.%20Further%2C%20inadequate%20handling%20of%20the%0Aunique%20features%20and%20relationships%20required%20for%20different%20downstream%20tasks%20may%0Aalso%20hinder%20effective%20task%20adaptation.%20In%20this%20paper%2C%20we%20propose%20a%0A%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20and%20develop%20a%20subgraph-centric%20urban%20region%20pre-training%20model%20to%0Acapture%20the%20heterogeneous%20and%20transferable%20patterns%20of%20entity%20interactions.%0AThis%20model%20pre-trains%20knowledge-rich%20region%20embeddings%20using%20contrastive%0Alearning%20and%20multi-view%20learning%20methods.%20To%20further%20refine%20these%0Arepresentations%2C%20we%20design%20two%20graph-based%20prompting%20methods%3A%20a%0Amanually-defined%20prompt%20to%20incorporate%20explicit%20task%20knowledge%20and%20a%0Atask-learnable%20prompt%20to%20discover%20hidden%20knowledge%2C%20which%20enhances%20the%0Aadaptability%20of%20these%20embeddings%20to%20different%20tasks.%20Extensive%20experiments%20on%0Avarious%20urban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05920v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban%2520Region%2520Pre-training%2520and%2520Prompting%253A%2520A%2520Graph-based%2520Approach%26entry.906535625%3DJiahui%2520Jin%2520and%2520Yifan%2520Song%2520and%2520Dong%2520Kan%2520and%2520Haojia%2520Zhu%2520and%2520Xiangguo%2520Sun%2520and%2520Zhicheng%2520Li%2520and%2520Xigang%2520Sun%2520and%2520Jinghui%2520Zhang%26entry.1292438233%3D%2520%2520Urban%2520region%2520representation%2520is%2520crucial%2520for%2520various%2520urban%2520downstream%2520tasks.%250AHowever%252C%2520despite%2520the%2520proliferation%2520of%2520methods%2520and%2520their%2520success%252C%2520acquiring%250Ageneral%2520urban%2520region%2520knowledge%2520and%2520adapting%2520to%2520different%2520tasks%2520remains%250Achallenging.%2520Existing%2520work%2520pays%2520limited%2520attention%2520to%2520the%2520fine-grained%250Afunctional%2520layout%2520semantics%2520in%2520urban%2520regions%252C%2520limiting%2520their%2520ability%2520to%2520capture%250Atransferable%2520knowledge%2520across%2520regions.%2520Further%252C%2520inadequate%2520handling%2520of%2520the%250Aunique%2520features%2520and%2520relationships%2520required%2520for%2520different%2520downstream%2520tasks%2520may%250Aalso%2520hinder%2520effective%2520task%2520adaptation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250A%2524%255Ctextbf%257BG%257D%2524raph-based%2520%2524%255Ctextbf%257BU%257D%2524rban%2520%2524%255Ctextbf%257BR%257D%2524egion%250A%2524%255Ctextbf%257BP%257D%2524re-training%2520and%2520%2524%255Ctextbf%257BP%257D%2524rompting%2520framework%2520%2528%2524%255Ctextbf%257BGURPP%257D%2524%2529%250Afor%2520region%2520representation%2520learning.%2520Specifically%252C%2520we%2520first%2520construct%2520an%2520urban%250Aregion%2520graph%2520and%2520develop%2520a%2520subgraph-centric%2520urban%2520region%2520pre-training%2520model%2520to%250Acapture%2520the%2520heterogeneous%2520and%2520transferable%2520patterns%2520of%2520entity%2520interactions.%250AThis%2520model%2520pre-trains%2520knowledge-rich%2520region%2520embeddings%2520using%2520contrastive%250Alearning%2520and%2520multi-view%2520learning%2520methods.%2520To%2520further%2520refine%2520these%250Arepresentations%252C%2520we%2520design%2520two%2520graph-based%2520prompting%2520methods%253A%2520a%250Amanually-defined%2520prompt%2520to%2520incorporate%2520explicit%2520task%2520knowledge%2520and%2520a%250Atask-learnable%2520prompt%2520to%2520discover%2520hidden%2520knowledge%252C%2520which%2520enhances%2520the%250Aadaptability%2520of%2520these%2520embeddings%2520to%2520different%2520tasks.%2520Extensive%2520experiments%2520on%250Avarious%2520urban%2520region%2520prediction%2520tasks%2520and%2520different%2520cities%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05920v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&entry.906535625=Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang&entry.1292438233=%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Existing%20work%20pays%20limited%20attention%20to%20the%20fine-grained%0Afunctional%20layout%20semantics%20in%20urban%20regions%2C%20limiting%20their%20ability%20to%20capture%0Atransferable%20knowledge%20across%20regions.%20Further%2C%20inadequate%20handling%20of%20the%0Aunique%20features%20and%20relationships%20required%20for%20different%20downstream%20tasks%20may%0Aalso%20hinder%20effective%20task%20adaptation.%20In%20this%20paper%2C%20we%20propose%20a%0A%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20and%20develop%20a%20subgraph-centric%20urban%20region%20pre-training%20model%20to%0Acapture%20the%20heterogeneous%20and%20transferable%20patterns%20of%20entity%20interactions.%0AThis%20model%20pre-trains%20knowledge-rich%20region%20embeddings%20using%20contrastive%0Alearning%20and%20multi-view%20learning%20methods.%20To%20further%20refine%20these%0Arepresentations%2C%20we%20design%20two%20graph-based%20prompting%20methods%3A%20a%0Amanually-defined%20prompt%20to%20incorporate%20explicit%20task%20knowledge%20and%20a%0Atask-learnable%20prompt%20to%20discover%20hidden%20knowledge%2C%20which%20enhances%20the%0Aadaptability%20of%20these%20embeddings%20to%20different%20tasks.%20Extensive%20experiments%20on%0Avarious%20urban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05920v4&entry.124074799=Read"},
{"title": "Revisiting Active Learning under (Human) Label Variation", "author": "Cornelia Gruber and Helen Alber and Bernd Bischl and G\u00f6ran Kauermann and Barbara Plank and Matthias A\u00dfenmacher", "abstract": "  Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.\n", "link": "http://arxiv.org/abs/2507.02593v1", "date": "2025-07-03", "relevancy": 1.9787, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5284}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Active%20Learning%20under%20%28Human%29%20Label%20Variation&body=Title%3A%20Revisiting%20Active%20Learning%20under%20%28Human%29%20Label%20Variation%0AAuthor%3A%20Cornelia%20Gruber%20and%20Helen%20Alber%20and%20Bernd%20Bischl%20and%20G%C3%B6ran%20Kauermann%20and%20Barbara%20Plank%20and%20Matthias%20A%C3%9Fenmacher%0AAbstract%3A%20%20%20Access%20to%20high-quality%20labeled%20data%20remains%20a%20limiting%20factor%20in%20applied%0Asupervised%20learning.%20While%20label%20variation%20%28LV%29%2C%20i.e.%2C%20differing%20labels%20for%20the%0Asame%20instance%2C%20is%20common%2C%20especially%20in%20natural%20language%20processing%2C%20annotation%0Aframeworks%20often%20still%20rest%20on%20the%20assumption%20of%20a%20single%20ground%20truth.%20This%0Aoverlooks%20human%20label%20variation%20%28HLV%29%2C%20the%20occurrence%20of%20plausible%20differences%0Ain%20annotations%2C%20as%20an%20informative%20signal.%20Similarly%2C%20active%20learning%20%28AL%29%2C%20a%0Apopular%20approach%20to%20optimizing%20the%20use%20of%20limited%20annotation%20budgets%20in%0Atraining%20ML%20models%2C%20often%20relies%20on%20at%20least%20one%20of%20several%20simplifying%0Aassumptions%2C%20which%20rarely%20hold%20in%20practice%20when%20acknowledging%20HLV.%20In%20this%0Apaper%2C%20we%20examine%20foundational%20assumptions%20about%20truth%20and%20label%20nature%2C%0Ahighlighting%20the%20need%20to%20decompose%20observed%20LV%20into%20signal%20%28e.g.%2C%20HLV%29%20and%0Anoise%20%28e.g.%2C%20annotation%20error%29.%20We%20survey%20how%20the%20AL%20and%20%28H%29LV%20communities%20have%0Aaddressed%20--%20or%20neglected%20--%20these%20distinctions%20and%20propose%20a%20conceptual%0Aframework%20for%20incorporating%20HLV%20throughout%20the%20AL%20loop%2C%20including%20instance%0Aselection%2C%20annotator%20choice%2C%20and%20label%20representation.%20We%20further%20discuss%20the%0Aintegration%20of%20large%20language%20models%20%28LLM%29%20as%20annotators.%20Our%20work%20aims%20to%20lay%0Aa%20conceptual%20foundation%20for%20HLV-aware%20active%20learning%2C%20better%20reflecting%20the%0Acomplexities%20of%20real-world%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Active%2520Learning%2520under%2520%2528Human%2529%2520Label%2520Variation%26entry.906535625%3DCornelia%2520Gruber%2520and%2520Helen%2520Alber%2520and%2520Bernd%2520Bischl%2520and%2520G%25C3%25B6ran%2520Kauermann%2520and%2520Barbara%2520Plank%2520and%2520Matthias%2520A%25C3%259Fenmacher%26entry.1292438233%3D%2520%2520Access%2520to%2520high-quality%2520labeled%2520data%2520remains%2520a%2520limiting%2520factor%2520in%2520applied%250Asupervised%2520learning.%2520While%2520label%2520variation%2520%2528LV%2529%252C%2520i.e.%252C%2520differing%2520labels%2520for%2520the%250Asame%2520instance%252C%2520is%2520common%252C%2520especially%2520in%2520natural%2520language%2520processing%252C%2520annotation%250Aframeworks%2520often%2520still%2520rest%2520on%2520the%2520assumption%2520of%2520a%2520single%2520ground%2520truth.%2520This%250Aoverlooks%2520human%2520label%2520variation%2520%2528HLV%2529%252C%2520the%2520occurrence%2520of%2520plausible%2520differences%250Ain%2520annotations%252C%2520as%2520an%2520informative%2520signal.%2520Similarly%252C%2520active%2520learning%2520%2528AL%2529%252C%2520a%250Apopular%2520approach%2520to%2520optimizing%2520the%2520use%2520of%2520limited%2520annotation%2520budgets%2520in%250Atraining%2520ML%2520models%252C%2520often%2520relies%2520on%2520at%2520least%2520one%2520of%2520several%2520simplifying%250Aassumptions%252C%2520which%2520rarely%2520hold%2520in%2520practice%2520when%2520acknowledging%2520HLV.%2520In%2520this%250Apaper%252C%2520we%2520examine%2520foundational%2520assumptions%2520about%2520truth%2520and%2520label%2520nature%252C%250Ahighlighting%2520the%2520need%2520to%2520decompose%2520observed%2520LV%2520into%2520signal%2520%2528e.g.%252C%2520HLV%2529%2520and%250Anoise%2520%2528e.g.%252C%2520annotation%2520error%2529.%2520We%2520survey%2520how%2520the%2520AL%2520and%2520%2528H%2529LV%2520communities%2520have%250Aaddressed%2520--%2520or%2520neglected%2520--%2520these%2520distinctions%2520and%2520propose%2520a%2520conceptual%250Aframework%2520for%2520incorporating%2520HLV%2520throughout%2520the%2520AL%2520loop%252C%2520including%2520instance%250Aselection%252C%2520annotator%2520choice%252C%2520and%2520label%2520representation.%2520We%2520further%2520discuss%2520the%250Aintegration%2520of%2520large%2520language%2520models%2520%2528LLM%2529%2520as%2520annotators.%2520Our%2520work%2520aims%2520to%2520lay%250Aa%2520conceptual%2520foundation%2520for%2520HLV-aware%2520active%2520learning%252C%2520better%2520reflecting%2520the%250Acomplexities%2520of%2520real-world%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Active%20Learning%20under%20%28Human%29%20Label%20Variation&entry.906535625=Cornelia%20Gruber%20and%20Helen%20Alber%20and%20Bernd%20Bischl%20and%20G%C3%B6ran%20Kauermann%20and%20Barbara%20Plank%20and%20Matthias%20A%C3%9Fenmacher&entry.1292438233=%20%20Access%20to%20high-quality%20labeled%20data%20remains%20a%20limiting%20factor%20in%20applied%0Asupervised%20learning.%20While%20label%20variation%20%28LV%29%2C%20i.e.%2C%20differing%20labels%20for%20the%0Asame%20instance%2C%20is%20common%2C%20especially%20in%20natural%20language%20processing%2C%20annotation%0Aframeworks%20often%20still%20rest%20on%20the%20assumption%20of%20a%20single%20ground%20truth.%20This%0Aoverlooks%20human%20label%20variation%20%28HLV%29%2C%20the%20occurrence%20of%20plausible%20differences%0Ain%20annotations%2C%20as%20an%20informative%20signal.%20Similarly%2C%20active%20learning%20%28AL%29%2C%20a%0Apopular%20approach%20to%20optimizing%20the%20use%20of%20limited%20annotation%20budgets%20in%0Atraining%20ML%20models%2C%20often%20relies%20on%20at%20least%20one%20of%20several%20simplifying%0Aassumptions%2C%20which%20rarely%20hold%20in%20practice%20when%20acknowledging%20HLV.%20In%20this%0Apaper%2C%20we%20examine%20foundational%20assumptions%20about%20truth%20and%20label%20nature%2C%0Ahighlighting%20the%20need%20to%20decompose%20observed%20LV%20into%20signal%20%28e.g.%2C%20HLV%29%20and%0Anoise%20%28e.g.%2C%20annotation%20error%29.%20We%20survey%20how%20the%20AL%20and%20%28H%29LV%20communities%20have%0Aaddressed%20--%20or%20neglected%20--%20these%20distinctions%20and%20propose%20a%20conceptual%0Aframework%20for%20incorporating%20HLV%20throughout%20the%20AL%20loop%2C%20including%20instance%0Aselection%2C%20annotator%20choice%2C%20and%20label%20representation.%20We%20further%20discuss%20the%0Aintegration%20of%20large%20language%20models%20%28LLM%29%20as%20annotators.%20Our%20work%20aims%20to%20lay%0Aa%20conceptual%20foundation%20for%20HLV-aware%20active%20learning%2C%20better%20reflecting%20the%0Acomplexities%20of%20real-world%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02593v1&entry.124074799=Read"},
{"title": "Integrating path-planning and control for robotic unicycles", "author": "M\u00e1t\u00e9 B. Vizi and D\u00e9nes T\u00e1k\u00e1cs and G\u00e1bor St\u00e9p\u00e1n and G\u00e1bor Orosz", "abstract": "  This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.\n", "link": "http://arxiv.org/abs/2507.02700v1", "date": "2025-07-03", "relevancy": 1.974, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20path-planning%20and%20control%20for%20robotic%20unicycles&body=Title%3A%20Integrating%20path-planning%20and%20control%20for%20robotic%20unicycles%0AAuthor%3A%20M%C3%A1t%C3%A9%20B.%20Vizi%20and%20D%C3%A9nes%20T%C3%A1k%C3%A1cs%20and%20G%C3%A1bor%20St%C3%A9p%C3%A1n%20and%20G%C3%A1bor%20Orosz%0AAbstract%3A%20%20%20This%20article%20focuses%20on%20integrating%20path-planning%20and%20control%20with%0Aspecializing%20on%20the%20unique%20needs%20of%20robotic%20unicycles.%20A%20unicycle%20design%20is%0Apresented%20which%20is%20capable%20of%20accelerating/breaking%20and%20carrying%20out%20a%20variety%0Aof%20maneuvers.%20The%20proposed%20path-planning%20method%20segments%20the%20path%20into%20straight%0Aand%20curved%20path%20sections%20dedicated%20for%20accelerating/breaking%20and%20turning%0Amaneuvers%2C%20respectively.%20The%20curvature%20profiles%20of%20the%20curved%20sections%20are%0Aoptimized%20while%20considering%20the%20control%20performance%20and%20the%20slipping%20limits%20of%0Athe%20wheel.%20The%20performance%20of%20the%20proposed%20integrated%20approach%20is%20demonstrated%0Avia%20numerical%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520path-planning%2520and%2520control%2520for%2520robotic%2520unicycles%26entry.906535625%3DM%25C3%25A1t%25C3%25A9%2520B.%2520Vizi%2520and%2520D%25C3%25A9nes%2520T%25C3%25A1k%25C3%25A1cs%2520and%2520G%25C3%25A1bor%2520St%25C3%25A9p%25C3%25A1n%2520and%2520G%25C3%25A1bor%2520Orosz%26entry.1292438233%3D%2520%2520This%2520article%2520focuses%2520on%2520integrating%2520path-planning%2520and%2520control%2520with%250Aspecializing%2520on%2520the%2520unique%2520needs%2520of%2520robotic%2520unicycles.%2520A%2520unicycle%2520design%2520is%250Apresented%2520which%2520is%2520capable%2520of%2520accelerating/breaking%2520and%2520carrying%2520out%2520a%2520variety%250Aof%2520maneuvers.%2520The%2520proposed%2520path-planning%2520method%2520segments%2520the%2520path%2520into%2520straight%250Aand%2520curved%2520path%2520sections%2520dedicated%2520for%2520accelerating/breaking%2520and%2520turning%250Amaneuvers%252C%2520respectively.%2520The%2520curvature%2520profiles%2520of%2520the%2520curved%2520sections%2520are%250Aoptimized%2520while%2520considering%2520the%2520control%2520performance%2520and%2520the%2520slipping%2520limits%2520of%250Athe%2520wheel.%2520The%2520performance%2520of%2520the%2520proposed%2520integrated%2520approach%2520is%2520demonstrated%250Avia%2520numerical%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20path-planning%20and%20control%20for%20robotic%20unicycles&entry.906535625=M%C3%A1t%C3%A9%20B.%20Vizi%20and%20D%C3%A9nes%20T%C3%A1k%C3%A1cs%20and%20G%C3%A1bor%20St%C3%A9p%C3%A1n%20and%20G%C3%A1bor%20Orosz&entry.1292438233=%20%20This%20article%20focuses%20on%20integrating%20path-planning%20and%20control%20with%0Aspecializing%20on%20the%20unique%20needs%20of%20robotic%20unicycles.%20A%20unicycle%20design%20is%0Apresented%20which%20is%20capable%20of%20accelerating/breaking%20and%20carrying%20out%20a%20variety%0Aof%20maneuvers.%20The%20proposed%20path-planning%20method%20segments%20the%20path%20into%20straight%0Aand%20curved%20path%20sections%20dedicated%20for%20accelerating/breaking%20and%20turning%0Amaneuvers%2C%20respectively.%20The%20curvature%20profiles%20of%20the%20curved%20sections%20are%0Aoptimized%20while%20considering%20the%20control%20performance%20and%20the%20slipping%20limits%20of%0Athe%20wheel.%20The%20performance%20of%20the%20proposed%20integrated%20approach%20is%20demonstrated%0Avia%20numerical%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02700v1&entry.124074799=Read"},
{"title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "author": "Purbesh Mitra and Sennur Ulukus", "abstract": "  Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.\n", "link": "http://arxiv.org/abs/2507.02851v1", "date": "2025-07-03", "relevancy": 1.9738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOTIF%3A%20Modular%20Thinking%20via%20Reinforcement%20Fine-tuning%20in%20LLMs&body=Title%3A%20MOTIF%3A%20Modular%20Thinking%20via%20Reinforcement%20Fine-tuning%20in%20LLMs%0AAuthor%3A%20Purbesh%20Mitra%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20Recent%20advancements%20in%20the%20reasoning%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20show%20that%20employing%20group%20relative%20policy%20optimization%20%28GRPO%29%20algorithm%0Afor%20reinforcement%20learning%20%28RL%29%20training%20allows%20the%20models%20to%20use%20more%0Athinking/reasoning%20tokens%20for%20generating%20better%20responses.%20However%2C%20LLMs%20can%0Agenerate%20only%20a%20finite%20amount%20of%20tokens%20while%20maintaining%20attention%20to%20the%0Apreviously%20generated%20tokens.%20This%20limit%2C%20also%20known%20as%20the%20context%20size%20of%20an%0ALLM%2C%20is%20a%20bottleneck%20in%20LLM%20reasoning%20with%20arbitrarily%20large%20number%20of%20tokens.%0ATo%20think%20beyond%20the%20limit%20of%20context%20size%2C%20an%20LLM%20must%20employ%20a%20modular%0Athinking%20strategy%20to%20reason%20over%20multiple%20rounds.%20In%20this%20work%2C%20we%20propose%0A%24%5Ctextbf%7BMOTIF%3A%20Modular%20Thinking%20via%20Reinforcement%20Finetuning%7D%24%20--%20an%20RL%0Atraining%20method%20for%20generating%20thinking%20tokens%20in%20multiple%20rounds%2C%20effectively%0Aallowing%20the%20model%20to%20think%20with%20additional%20context%20size.%20We%20trained%20the%0Aopen-source%20model%20Qwen2.5-3B-Instruct%20on%20GSM8K%20dataset%20via%20parameter%20efficient%0Afine-tuning%20and%20tested%20its%20accuracy%20on%20MATH500%20and%20AIME2024%20benchmarks.%20Our%0Aexperiments%20show%203.8%5C%25%20and%203.3%5C%25%20improvements%20over%20vanilla%20GRPO%20based%20training%0Ain%20the%20respective%20benchmarks.%20Furthermore%2C%20this%20improvement%20was%20achieved%20with%0Aonly%2015%5C%25%20of%20samples%2C%20thus%20demonstrating%20sample%20efficiency%20of%20MOTIF.%20Our%20code%0Aand%20models%20are%20available%20at%20https%3A//github.com/purbeshmitra/MOTIF%20and%0Ahttps%3A//huggingface.co/purbeshmitra/MOTIF%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOTIF%253A%2520Modular%2520Thinking%2520via%2520Reinforcement%2520Fine-tuning%2520in%2520LLMs%26entry.906535625%3DPurbesh%2520Mitra%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520show%2520that%2520employing%2520group%2520relative%2520policy%2520optimization%2520%2528GRPO%2529%2520algorithm%250Afor%2520reinforcement%2520learning%2520%2528RL%2529%2520training%2520allows%2520the%2520models%2520to%2520use%2520more%250Athinking/reasoning%2520tokens%2520for%2520generating%2520better%2520responses.%2520However%252C%2520LLMs%2520can%250Agenerate%2520only%2520a%2520finite%2520amount%2520of%2520tokens%2520while%2520maintaining%2520attention%2520to%2520the%250Apreviously%2520generated%2520tokens.%2520This%2520limit%252C%2520also%2520known%2520as%2520the%2520context%2520size%2520of%2520an%250ALLM%252C%2520is%2520a%2520bottleneck%2520in%2520LLM%2520reasoning%2520with%2520arbitrarily%2520large%2520number%2520of%2520tokens.%250ATo%2520think%2520beyond%2520the%2520limit%2520of%2520context%2520size%252C%2520an%2520LLM%2520must%2520employ%2520a%2520modular%250Athinking%2520strategy%2520to%2520reason%2520over%2520multiple%2520rounds.%2520In%2520this%2520work%252C%2520we%2520propose%250A%2524%255Ctextbf%257BMOTIF%253A%2520Modular%2520Thinking%2520via%2520Reinforcement%2520Finetuning%257D%2524%2520--%2520an%2520RL%250Atraining%2520method%2520for%2520generating%2520thinking%2520tokens%2520in%2520multiple%2520rounds%252C%2520effectively%250Aallowing%2520the%2520model%2520to%2520think%2520with%2520additional%2520context%2520size.%2520We%2520trained%2520the%250Aopen-source%2520model%2520Qwen2.5-3B-Instruct%2520on%2520GSM8K%2520dataset%2520via%2520parameter%2520efficient%250Afine-tuning%2520and%2520tested%2520its%2520accuracy%2520on%2520MATH500%2520and%2520AIME2024%2520benchmarks.%2520Our%250Aexperiments%2520show%25203.8%255C%2525%2520and%25203.3%255C%2525%2520improvements%2520over%2520vanilla%2520GRPO%2520based%2520training%250Ain%2520the%2520respective%2520benchmarks.%2520Furthermore%252C%2520this%2520improvement%2520was%2520achieved%2520with%250Aonly%252015%255C%2525%2520of%2520samples%252C%2520thus%2520demonstrating%2520sample%2520efficiency%2520of%2520MOTIF.%2520Our%2520code%250Aand%2520models%2520are%2520available%2520at%2520https%253A//github.com/purbeshmitra/MOTIF%2520and%250Ahttps%253A//huggingface.co/purbeshmitra/MOTIF%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOTIF%3A%20Modular%20Thinking%20via%20Reinforcement%20Fine-tuning%20in%20LLMs&entry.906535625=Purbesh%20Mitra%20and%20Sennur%20Ulukus&entry.1292438233=%20%20Recent%20advancements%20in%20the%20reasoning%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20show%20that%20employing%20group%20relative%20policy%20optimization%20%28GRPO%29%20algorithm%0Afor%20reinforcement%20learning%20%28RL%29%20training%20allows%20the%20models%20to%20use%20more%0Athinking/reasoning%20tokens%20for%20generating%20better%20responses.%20However%2C%20LLMs%20can%0Agenerate%20only%20a%20finite%20amount%20of%20tokens%20while%20maintaining%20attention%20to%20the%0Apreviously%20generated%20tokens.%20This%20limit%2C%20also%20known%20as%20the%20context%20size%20of%20an%0ALLM%2C%20is%20a%20bottleneck%20in%20LLM%20reasoning%20with%20arbitrarily%20large%20number%20of%20tokens.%0ATo%20think%20beyond%20the%20limit%20of%20context%20size%2C%20an%20LLM%20must%20employ%20a%20modular%0Athinking%20strategy%20to%20reason%20over%20multiple%20rounds.%20In%20this%20work%2C%20we%20propose%0A%24%5Ctextbf%7BMOTIF%3A%20Modular%20Thinking%20via%20Reinforcement%20Finetuning%7D%24%20--%20an%20RL%0Atraining%20method%20for%20generating%20thinking%20tokens%20in%20multiple%20rounds%2C%20effectively%0Aallowing%20the%20model%20to%20think%20with%20additional%20context%20size.%20We%20trained%20the%0Aopen-source%20model%20Qwen2.5-3B-Instruct%20on%20GSM8K%20dataset%20via%20parameter%20efficient%0Afine-tuning%20and%20tested%20its%20accuracy%20on%20MATH500%20and%20AIME2024%20benchmarks.%20Our%0Aexperiments%20show%203.8%5C%25%20and%203.3%5C%25%20improvements%20over%20vanilla%20GRPO%20based%20training%0Ain%20the%20respective%20benchmarks.%20Furthermore%2C%20this%20improvement%20was%20achieved%20with%0Aonly%2015%5C%25%20of%20samples%2C%20thus%20demonstrating%20sample%20efficiency%20of%20MOTIF.%20Our%20code%0Aand%20models%20are%20available%20at%20https%3A//github.com/purbeshmitra/MOTIF%20and%0Ahttps%3A//huggingface.co/purbeshmitra/MOTIF%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02851v1&entry.124074799=Read"},
{"title": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory", "author": "Kenneth Payne and Baptiste Alloui-Cros", "abstract": "  Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.\n", "link": "http://arxiv.org/abs/2507.02618v1", "date": "2025-07-03", "relevancy": 1.9616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strategic%20Intelligence%20in%20Large%20Language%20Models%3A%20Evidence%20from%0A%20%20evolutionary%20Game%20Theory&body=Title%3A%20Strategic%20Intelligence%20in%20Large%20Language%20Models%3A%20Evidence%20from%0A%20%20evolutionary%20Game%20Theory%0AAuthor%3A%20Kenneth%20Payne%20and%20Baptiste%20Alloui-Cros%0AAbstract%3A%20%20%20Are%20Large%20Language%20Models%20%28LLMs%29%20a%20new%20form%20of%20strategic%20intelligence%2C%20able%0Ato%20reason%20about%20goals%20in%20competitive%20settings%3F%20We%20present%20compelling%20supporting%0Aevidence.%20The%20Iterated%20Prisoner%27s%20Dilemma%20%28IPD%29%20has%20long%20served%20as%20a%20model%20for%0Astudying%20decision-making.%20We%20conduct%20the%20first%20ever%20series%20of%20evolutionary%20IPD%0Atournaments%2C%20pitting%20canonical%20strategies%20%28e.g.%2C%20Tit-for-Tat%2C%20Grim%20Trigger%29%0Aagainst%20agents%20from%20the%20leading%20frontier%20AI%20companies%20OpenAI%2C%20Google%2C%20and%0AAnthropic.%20By%20varying%20the%20termination%20probability%20in%20each%20tournament%20%28the%0A%22shadow%20of%20the%20future%22%29%2C%20we%20introduce%20complexity%20and%20chance%2C%20confounding%0Amemorisation.%0A%20%20Our%20results%20show%20that%20LLMs%20are%20highly%20competitive%2C%20consistently%20surviving%20and%0Asometimes%20even%20proliferating%20in%20these%20complex%20ecosystems.%20Furthermore%2C%20they%0Aexhibit%20distinctive%20and%20persistent%20%22strategic%20fingerprints%22%3A%20Google%27s%20Gemini%0Amodels%20proved%20strategically%20ruthless%2C%20exploiting%20cooperative%20opponents%20and%0Aretaliating%20against%20defectors%2C%20while%20OpenAI%27s%20models%20remained%20highly%0Acooperative%2C%20a%20trait%20that%20proved%20catastrophic%20in%20hostile%20environments.%0AAnthropic%27s%20Claude%20emerged%20as%20the%20most%20forgiving%20reciprocator%2C%20showing%0Aremarkable%20willingness%20to%20restore%20cooperation%20even%20after%20being%20exploited%20or%0Asuccessfully%20defecting.%20Analysis%20of%20nearly%2032%2C000%20prose%20rationales%20provided%20by%0Athe%20models%20reveals%20that%20they%20actively%20reason%20about%20both%20the%20time%20horizon%20and%0Atheir%20opponent%27s%20likely%20strategy%2C%20and%20we%20demonstrate%20that%20this%20reasoning%20is%0Ainstrumental%20to%20their%20decisions.%20This%20work%20connects%20classic%20game%20theory%20with%0Amachine%20psychology%2C%20offering%20a%20rich%20and%20granular%20view%20of%20algorithmic%0Adecision-making%20under%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrategic%2520Intelligence%2520in%2520Large%2520Language%2520Models%253A%2520Evidence%2520from%250A%2520%2520evolutionary%2520Game%2520Theory%26entry.906535625%3DKenneth%2520Payne%2520and%2520Baptiste%2520Alloui-Cros%26entry.1292438233%3D%2520%2520Are%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520a%2520new%2520form%2520of%2520strategic%2520intelligence%252C%2520able%250Ato%2520reason%2520about%2520goals%2520in%2520competitive%2520settings%253F%2520We%2520present%2520compelling%2520supporting%250Aevidence.%2520The%2520Iterated%2520Prisoner%2527s%2520Dilemma%2520%2528IPD%2529%2520has%2520long%2520served%2520as%2520a%2520model%2520for%250Astudying%2520decision-making.%2520We%2520conduct%2520the%2520first%2520ever%2520series%2520of%2520evolutionary%2520IPD%250Atournaments%252C%2520pitting%2520canonical%2520strategies%2520%2528e.g.%252C%2520Tit-for-Tat%252C%2520Grim%2520Trigger%2529%250Aagainst%2520agents%2520from%2520the%2520leading%2520frontier%2520AI%2520companies%2520OpenAI%252C%2520Google%252C%2520and%250AAnthropic.%2520By%2520varying%2520the%2520termination%2520probability%2520in%2520each%2520tournament%2520%2528the%250A%2522shadow%2520of%2520the%2520future%2522%2529%252C%2520we%2520introduce%2520complexity%2520and%2520chance%252C%2520confounding%250Amemorisation.%250A%2520%2520Our%2520results%2520show%2520that%2520LLMs%2520are%2520highly%2520competitive%252C%2520consistently%2520surviving%2520and%250Asometimes%2520even%2520proliferating%2520in%2520these%2520complex%2520ecosystems.%2520Furthermore%252C%2520they%250Aexhibit%2520distinctive%2520and%2520persistent%2520%2522strategic%2520fingerprints%2522%253A%2520Google%2527s%2520Gemini%250Amodels%2520proved%2520strategically%2520ruthless%252C%2520exploiting%2520cooperative%2520opponents%2520and%250Aretaliating%2520against%2520defectors%252C%2520while%2520OpenAI%2527s%2520models%2520remained%2520highly%250Acooperative%252C%2520a%2520trait%2520that%2520proved%2520catastrophic%2520in%2520hostile%2520environments.%250AAnthropic%2527s%2520Claude%2520emerged%2520as%2520the%2520most%2520forgiving%2520reciprocator%252C%2520showing%250Aremarkable%2520willingness%2520to%2520restore%2520cooperation%2520even%2520after%2520being%2520exploited%2520or%250Asuccessfully%2520defecting.%2520Analysis%2520of%2520nearly%252032%252C000%2520prose%2520rationales%2520provided%2520by%250Athe%2520models%2520reveals%2520that%2520they%2520actively%2520reason%2520about%2520both%2520the%2520time%2520horizon%2520and%250Atheir%2520opponent%2527s%2520likely%2520strategy%252C%2520and%2520we%2520demonstrate%2520that%2520this%2520reasoning%2520is%250Ainstrumental%2520to%2520their%2520decisions.%2520This%2520work%2520connects%2520classic%2520game%2520theory%2520with%250Amachine%2520psychology%252C%2520offering%2520a%2520rich%2520and%2520granular%2520view%2520of%2520algorithmic%250Adecision-making%2520under%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strategic%20Intelligence%20in%20Large%20Language%20Models%3A%20Evidence%20from%0A%20%20evolutionary%20Game%20Theory&entry.906535625=Kenneth%20Payne%20and%20Baptiste%20Alloui-Cros&entry.1292438233=%20%20Are%20Large%20Language%20Models%20%28LLMs%29%20a%20new%20form%20of%20strategic%20intelligence%2C%20able%0Ato%20reason%20about%20goals%20in%20competitive%20settings%3F%20We%20present%20compelling%20supporting%0Aevidence.%20The%20Iterated%20Prisoner%27s%20Dilemma%20%28IPD%29%20has%20long%20served%20as%20a%20model%20for%0Astudying%20decision-making.%20We%20conduct%20the%20first%20ever%20series%20of%20evolutionary%20IPD%0Atournaments%2C%20pitting%20canonical%20strategies%20%28e.g.%2C%20Tit-for-Tat%2C%20Grim%20Trigger%29%0Aagainst%20agents%20from%20the%20leading%20frontier%20AI%20companies%20OpenAI%2C%20Google%2C%20and%0AAnthropic.%20By%20varying%20the%20termination%20probability%20in%20each%20tournament%20%28the%0A%22shadow%20of%20the%20future%22%29%2C%20we%20introduce%20complexity%20and%20chance%2C%20confounding%0Amemorisation.%0A%20%20Our%20results%20show%20that%20LLMs%20are%20highly%20competitive%2C%20consistently%20surviving%20and%0Asometimes%20even%20proliferating%20in%20these%20complex%20ecosystems.%20Furthermore%2C%20they%0Aexhibit%20distinctive%20and%20persistent%20%22strategic%20fingerprints%22%3A%20Google%27s%20Gemini%0Amodels%20proved%20strategically%20ruthless%2C%20exploiting%20cooperative%20opponents%20and%0Aretaliating%20against%20defectors%2C%20while%20OpenAI%27s%20models%20remained%20highly%0Acooperative%2C%20a%20trait%20that%20proved%20catastrophic%20in%20hostile%20environments.%0AAnthropic%27s%20Claude%20emerged%20as%20the%20most%20forgiving%20reciprocator%2C%20showing%0Aremarkable%20willingness%20to%20restore%20cooperation%20even%20after%20being%20exploited%20or%0Asuccessfully%20defecting.%20Analysis%20of%20nearly%2032%2C000%20prose%20rationales%20provided%20by%0Athe%20models%20reveals%20that%20they%20actively%20reason%20about%20both%20the%20time%20horizon%20and%0Atheir%20opponent%27s%20likely%20strategy%2C%20and%20we%20demonstrate%20that%20this%20reasoning%20is%0Ainstrumental%20to%20their%20decisions.%20This%20work%20connects%20classic%20game%20theory%20with%0Amachine%20psychology%2C%20offering%20a%20rich%20and%20granular%20view%20of%20algorithmic%0Adecision-making%20under%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02618v1&entry.124074799=Read"},
{"title": "Understanding and Improving Length Generalization in Recurrent Models", "author": "Ricardo Buitrago Ruiz and Albert Gu", "abstract": "  Recently, recurrent models such as state space models and linear attention\nhave become popular due to their linear complexity in the sequence length.\nThanks to their recurrent nature, in principle they can process arbitrarily\nlong sequences, but their performance sometimes drops considerably beyond their\ntraining context lengths-i.e. they fail to length generalize. In this work, we\nprovide comprehensive empirical and theoretical analysis to support the\nunexplored states hypothesis, which posits that models fail to length\ngeneralize when during training they are only exposed to a limited subset of\nthe distribution of all attainable states (i.e. states that would be attained\nif the recurrence was applied to long sequences). Furthermore, we investigate\nsimple training interventions that aim to increase the coverage of the states\nthat the model is trained on, e.g. by initializing the state with Gaussian\nnoise or with the final state of a different input sequence. With only 500\npost-training steps ($\\sim 0.1\\%$ of the pre-training budget), these\ninterventions enable length generalization for sequences that are orders of\nmagnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and\nshow improved performance in long context tasks, thus presenting a simple and\nefficient way to enable robust length generalization in general recurrent\nmodels.\n", "link": "http://arxiv.org/abs/2507.02782v1", "date": "2025-07-03", "relevancy": 1.9454, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4933}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4909}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models&body=Title%3A%20Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models%0AAuthor%3A%20Ricardo%20Buitrago%20Ruiz%20and%20Albert%20Gu%0AAbstract%3A%20%20%20Recently%2C%20recurrent%20models%20such%20as%20state%20space%20models%20and%20linear%20attention%0Ahave%20become%20popular%20due%20to%20their%20linear%20complexity%20in%20the%20sequence%20length.%0AThanks%20to%20their%20recurrent%20nature%2C%20in%20principle%20they%20can%20process%20arbitrarily%0Along%20sequences%2C%20but%20their%20performance%20sometimes%20drops%20considerably%20beyond%20their%0Atraining%20context%20lengths-i.e.%20they%20fail%20to%20length%20generalize.%20In%20this%20work%2C%20we%0Aprovide%20comprehensive%20empirical%20and%20theoretical%20analysis%20to%20support%20the%0Aunexplored%20states%20hypothesis%2C%20which%20posits%20that%20models%20fail%20to%20length%0Ageneralize%20when%20during%20training%20they%20are%20only%20exposed%20to%20a%20limited%20subset%20of%0Athe%20distribution%20of%20all%20attainable%20states%20%28i.e.%20states%20that%20would%20be%20attained%0Aif%20the%20recurrence%20was%20applied%20to%20long%20sequences%29.%20Furthermore%2C%20we%20investigate%0Asimple%20training%20interventions%20that%20aim%20to%20increase%20the%20coverage%20of%20the%20states%0Athat%20the%20model%20is%20trained%20on%2C%20e.g.%20by%20initializing%20the%20state%20with%20Gaussian%0Anoise%20or%20with%20the%20final%20state%20of%20a%20different%20input%20sequence.%20With%20only%20500%0Apost-training%20steps%20%28%24%5Csim%200.1%5C%25%24%20of%20the%20pre-training%20budget%29%2C%20these%0Ainterventions%20enable%20length%20generalization%20for%20sequences%20that%20are%20orders%20of%0Amagnitude%20longer%20than%20the%20training%20context%20%28e.g.%20%242k%5Clongrightarrow%20128k%24%29%20and%0Ashow%20improved%20performance%20in%20long%20context%20tasks%2C%20thus%20presenting%20a%20simple%20and%0Aefficient%20way%20to%20enable%20robust%20length%20generalization%20in%20general%20recurrent%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Improving%2520Length%2520Generalization%2520in%2520Recurrent%2520Models%26entry.906535625%3DRicardo%2520Buitrago%2520Ruiz%2520and%2520Albert%2520Gu%26entry.1292438233%3D%2520%2520Recently%252C%2520recurrent%2520models%2520such%2520as%2520state%2520space%2520models%2520and%2520linear%2520attention%250Ahave%2520become%2520popular%2520due%2520to%2520their%2520linear%2520complexity%2520in%2520the%2520sequence%2520length.%250AThanks%2520to%2520their%2520recurrent%2520nature%252C%2520in%2520principle%2520they%2520can%2520process%2520arbitrarily%250Along%2520sequences%252C%2520but%2520their%2520performance%2520sometimes%2520drops%2520considerably%2520beyond%2520their%250Atraining%2520context%2520lengths-i.e.%2520they%2520fail%2520to%2520length%2520generalize.%2520In%2520this%2520work%252C%2520we%250Aprovide%2520comprehensive%2520empirical%2520and%2520theoretical%2520analysis%2520to%2520support%2520the%250Aunexplored%2520states%2520hypothesis%252C%2520which%2520posits%2520that%2520models%2520fail%2520to%2520length%250Ageneralize%2520when%2520during%2520training%2520they%2520are%2520only%2520exposed%2520to%2520a%2520limited%2520subset%2520of%250Athe%2520distribution%2520of%2520all%2520attainable%2520states%2520%2528i.e.%2520states%2520that%2520would%2520be%2520attained%250Aif%2520the%2520recurrence%2520was%2520applied%2520to%2520long%2520sequences%2529.%2520Furthermore%252C%2520we%2520investigate%250Asimple%2520training%2520interventions%2520that%2520aim%2520to%2520increase%2520the%2520coverage%2520of%2520the%2520states%250Athat%2520the%2520model%2520is%2520trained%2520on%252C%2520e.g.%2520by%2520initializing%2520the%2520state%2520with%2520Gaussian%250Anoise%2520or%2520with%2520the%2520final%2520state%2520of%2520a%2520different%2520input%2520sequence.%2520With%2520only%2520500%250Apost-training%2520steps%2520%2528%2524%255Csim%25200.1%255C%2525%2524%2520of%2520the%2520pre-training%2520budget%2529%252C%2520these%250Ainterventions%2520enable%2520length%2520generalization%2520for%2520sequences%2520that%2520are%2520orders%2520of%250Amagnitude%2520longer%2520than%2520the%2520training%2520context%2520%2528e.g.%2520%25242k%255Clongrightarrow%2520128k%2524%2529%2520and%250Ashow%2520improved%2520performance%2520in%2520long%2520context%2520tasks%252C%2520thus%2520presenting%2520a%2520simple%2520and%250Aefficient%2520way%2520to%2520enable%2520robust%2520length%2520generalization%2520in%2520general%2520recurrent%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models&entry.906535625=Ricardo%20Buitrago%20Ruiz%20and%20Albert%20Gu&entry.1292438233=%20%20Recently%2C%20recurrent%20models%20such%20as%20state%20space%20models%20and%20linear%20attention%0Ahave%20become%20popular%20due%20to%20their%20linear%20complexity%20in%20the%20sequence%20length.%0AThanks%20to%20their%20recurrent%20nature%2C%20in%20principle%20they%20can%20process%20arbitrarily%0Along%20sequences%2C%20but%20their%20performance%20sometimes%20drops%20considerably%20beyond%20their%0Atraining%20context%20lengths-i.e.%20they%20fail%20to%20length%20generalize.%20In%20this%20work%2C%20we%0Aprovide%20comprehensive%20empirical%20and%20theoretical%20analysis%20to%20support%20the%0Aunexplored%20states%20hypothesis%2C%20which%20posits%20that%20models%20fail%20to%20length%0Ageneralize%20when%20during%20training%20they%20are%20only%20exposed%20to%20a%20limited%20subset%20of%0Athe%20distribution%20of%20all%20attainable%20states%20%28i.e.%20states%20that%20would%20be%20attained%0Aif%20the%20recurrence%20was%20applied%20to%20long%20sequences%29.%20Furthermore%2C%20we%20investigate%0Asimple%20training%20interventions%20that%20aim%20to%20increase%20the%20coverage%20of%20the%20states%0Athat%20the%20model%20is%20trained%20on%2C%20e.g.%20by%20initializing%20the%20state%20with%20Gaussian%0Anoise%20or%20with%20the%20final%20state%20of%20a%20different%20input%20sequence.%20With%20only%20500%0Apost-training%20steps%20%28%24%5Csim%200.1%5C%25%24%20of%20the%20pre-training%20budget%29%2C%20these%0Ainterventions%20enable%20length%20generalization%20for%20sequences%20that%20are%20orders%20of%0Amagnitude%20longer%20than%20the%20training%20context%20%28e.g.%20%242k%5Clongrightarrow%20128k%24%29%20and%0Ashow%20improved%20performance%20in%20long%20context%20tasks%2C%20thus%20presenting%20a%20simple%20and%0Aefficient%20way%20to%20enable%20robust%20length%20generalization%20in%20general%20recurrent%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02782v1&entry.124074799=Read"},
{"title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice\n  Cloning Attacks", "author": "Wei Fan and Kejiang Chen and Chang Liu and Weiming Zhang and Nenghai Yu", "abstract": "  The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io.\n", "link": "http://arxiv.org/abs/2507.02606v1", "date": "2025-07-03", "relevancy": 1.9337, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4993}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4772}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20De-AntiFake%3A%20Rethinking%20the%20Protective%20Perturbations%20Against%20Voice%0A%20%20Cloning%20Attacks&body=Title%3A%20De-AntiFake%3A%20Rethinking%20the%20Protective%20Perturbations%20Against%20Voice%0A%20%20Cloning%20Attacks%0AAuthor%3A%20Wei%20Fan%20and%20Kejiang%20Chen%20and%20Chang%20Liu%20and%20Weiming%20Zhang%20and%20Nenghai%20Yu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20speech%20generation%20models%20has%20heightened%20privacy%20and%0Asecurity%20concerns%20related%20to%20voice%20cloning%20%28VC%29.%20Recent%20studies%20have%0Ainvestigated%20disrupting%20unauthorized%20voice%20cloning%20by%20introducing%20adversarial%0Aperturbations.%20However%2C%20determined%20attackers%20can%20mitigate%20these%20protective%0Aperturbations%20and%20successfully%20execute%20VC.%20In%20this%20study%2C%20we%20conduct%20the%20first%0Asystematic%20evaluation%20of%20these%20protective%20perturbations%20against%20VC%20under%0Arealistic%20threat%20models%20that%20include%20perturbation%20purification.%20Our%20findings%0Areveal%20that%20while%20existing%20purification%20methods%20can%20neutralize%20a%20considerable%0Aportion%20of%20the%20protective%20perturbations%2C%20they%20still%20lead%20to%20distortions%20in%20the%0Afeature%20space%20of%20VC%20models%2C%20which%20degrades%20the%20performance%20of%20VC.%20From%20this%0Aperspective%2C%20we%20propose%20a%20novel%20two-stage%20purification%20method%3A%20%281%29%20Purify%20the%0Aperturbed%20speech%3B%20%282%29%20Refine%20it%20using%20phoneme%20guidance%20to%20align%20it%20with%20the%0Aclean%20speech%20distribution.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20purification%20methods%20in%20disrupting%20VC%20defenses.%0AOur%20study%20reveals%20the%20limitations%20of%20adversarial%20perturbation-based%20VC%20defenses%0Aand%20underscores%20the%20urgent%20need%20for%20more%20robust%20solutions%20to%20mitigate%20the%0Asecurity%20and%20privacy%20risks%20posed%20by%20VC.%20The%20code%20and%20audio%20samples%20are%0Aavailable%20at%20https%3A//de-antifake.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDe-AntiFake%253A%2520Rethinking%2520the%2520Protective%2520Perturbations%2520Against%2520Voice%250A%2520%2520Cloning%2520Attacks%26entry.906535625%3DWei%2520Fan%2520and%2520Kejiang%2520Chen%2520and%2520Chang%2520Liu%2520and%2520Weiming%2520Zhang%2520and%2520Nenghai%2520Yu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520speech%2520generation%2520models%2520has%2520heightened%2520privacy%2520and%250Asecurity%2520concerns%2520related%2520to%2520voice%2520cloning%2520%2528VC%2529.%2520Recent%2520studies%2520have%250Ainvestigated%2520disrupting%2520unauthorized%2520voice%2520cloning%2520by%2520introducing%2520adversarial%250Aperturbations.%2520However%252C%2520determined%2520attackers%2520can%2520mitigate%2520these%2520protective%250Aperturbations%2520and%2520successfully%2520execute%2520VC.%2520In%2520this%2520study%252C%2520we%2520conduct%2520the%2520first%250Asystematic%2520evaluation%2520of%2520these%2520protective%2520perturbations%2520against%2520VC%2520under%250Arealistic%2520threat%2520models%2520that%2520include%2520perturbation%2520purification.%2520Our%2520findings%250Areveal%2520that%2520while%2520existing%2520purification%2520methods%2520can%2520neutralize%2520a%2520considerable%250Aportion%2520of%2520the%2520protective%2520perturbations%252C%2520they%2520still%2520lead%2520to%2520distortions%2520in%2520the%250Afeature%2520space%2520of%2520VC%2520models%252C%2520which%2520degrades%2520the%2520performance%2520of%2520VC.%2520From%2520this%250Aperspective%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520purification%2520method%253A%2520%25281%2529%2520Purify%2520the%250Aperturbed%2520speech%253B%2520%25282%2529%2520Refine%2520it%2520using%2520phoneme%2520guidance%2520to%2520align%2520it%2520with%2520the%250Aclean%2520speech%2520distribution.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520purification%2520methods%2520in%2520disrupting%2520VC%2520defenses.%250AOur%2520study%2520reveals%2520the%2520limitations%2520of%2520adversarial%2520perturbation-based%2520VC%2520defenses%250Aand%2520underscores%2520the%2520urgent%2520need%2520for%2520more%2520robust%2520solutions%2520to%2520mitigate%2520the%250Asecurity%2520and%2520privacy%2520risks%2520posed%2520by%2520VC.%2520The%2520code%2520and%2520audio%2520samples%2520are%250Aavailable%2520at%2520https%253A//de-antifake.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=De-AntiFake%3A%20Rethinking%20the%20Protective%20Perturbations%20Against%20Voice%0A%20%20Cloning%20Attacks&entry.906535625=Wei%20Fan%20and%20Kejiang%20Chen%20and%20Chang%20Liu%20and%20Weiming%20Zhang%20and%20Nenghai%20Yu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20speech%20generation%20models%20has%20heightened%20privacy%20and%0Asecurity%20concerns%20related%20to%20voice%20cloning%20%28VC%29.%20Recent%20studies%20have%0Ainvestigated%20disrupting%20unauthorized%20voice%20cloning%20by%20introducing%20adversarial%0Aperturbations.%20However%2C%20determined%20attackers%20can%20mitigate%20these%20protective%0Aperturbations%20and%20successfully%20execute%20VC.%20In%20this%20study%2C%20we%20conduct%20the%20first%0Asystematic%20evaluation%20of%20these%20protective%20perturbations%20against%20VC%20under%0Arealistic%20threat%20models%20that%20include%20perturbation%20purification.%20Our%20findings%0Areveal%20that%20while%20existing%20purification%20methods%20can%20neutralize%20a%20considerable%0Aportion%20of%20the%20protective%20perturbations%2C%20they%20still%20lead%20to%20distortions%20in%20the%0Afeature%20space%20of%20VC%20models%2C%20which%20degrades%20the%20performance%20of%20VC.%20From%20this%0Aperspective%2C%20we%20propose%20a%20novel%20two-stage%20purification%20method%3A%20%281%29%20Purify%20the%0Aperturbed%20speech%3B%20%282%29%20Refine%20it%20using%20phoneme%20guidance%20to%20align%20it%20with%20the%0Aclean%20speech%20distribution.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20purification%20methods%20in%20disrupting%20VC%20defenses.%0AOur%20study%20reveals%20the%20limitations%20of%20adversarial%20perturbation-based%20VC%20defenses%0Aand%20underscores%20the%20urgent%20need%20for%20more%20robust%20solutions%20to%20mitigate%20the%0Asecurity%20and%20privacy%20risks%20posed%20by%20VC.%20The%20code%20and%20audio%20samples%20are%0Aavailable%20at%20https%3A//de-antifake.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02606v1&entry.124074799=Read"},
{"title": "On Characterizations for Language Generation: Interplay of\n  Hallucinations, Breadth, and Stability", "author": "Alkis Kalavasis and Anay Mehrotra and Grigoris Velegkas", "abstract": "  We study language generation in the limit - introduced by Kleinberg and\nMullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin\n[Ang79]. [KM24]'s main result is an algorithm for generating from any countable\nlanguage collection in the limit. While their algorithm eventually generates\nunseen strings from the target language $K$, it sacrifices coverage or breadth,\ni.e., its ability to generate a rich set of strings. Recent work introduces\ndifferent notions of breadth and explores when generation with breadth is\npossible, leaving a full characterization of these notions open. Our first set\nof results settles this by characterizing generation for existing notions of\nbreadth and their natural extensions. Interestingly, our lower bounds are very\nflexible and hold for many performance metrics beyond breadth - for instance,\nshowing that, in general, it is impossible to train generators which achieve a\nhigher perplexity or lower hallucination rate for $K$ compared to other\nlanguages. Next, we study language generation with breadth and stable\ngenerators - algorithms that eventually stop changing after seeing an arbitrary\nbut finite number of strings - and prove unconditional lower bounds for such\ngenerators, strengthening the results of [KMV25] and demonstrating that\ngeneration with many existing notions of breadth becomes equally hard, when\nstability is required. This gives a separation for generation with approximate\nbreadth, between stable and unstable generators, highlighting the rich\ninterplay between breadth, stability, and consistency in language generation.\n", "link": "http://arxiv.org/abs/2412.18530v2", "date": "2025-07-03", "relevancy": 1.9328, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5044}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Characterizations%20for%20Language%20Generation%3A%20Interplay%20of%0A%20%20Hallucinations%2C%20Breadth%2C%20and%20Stability&body=Title%3A%20On%20Characterizations%20for%20Language%20Generation%3A%20Interplay%20of%0A%20%20Hallucinations%2C%20Breadth%2C%20and%20Stability%0AAuthor%3A%20Alkis%20Kalavasis%20and%20Anay%20Mehrotra%20and%20Grigoris%20Velegkas%0AAbstract%3A%20%20%20We%20study%20language%20generation%20in%20the%20limit%20-%20introduced%20by%20Kleinberg%20and%0AMullainathan%20%5BKM24%5D%20-%20building%20on%20classical%20works%20of%20Gold%20%5BGol67%5D%20and%20Angluin%0A%5BAng79%5D.%20%5BKM24%5D%27s%20main%20result%20is%20an%20algorithm%20for%20generating%20from%20any%20countable%0Alanguage%20collection%20in%20the%20limit.%20While%20their%20algorithm%20eventually%20generates%0Aunseen%20strings%20from%20the%20target%20language%20%24K%24%2C%20it%20sacrifices%20coverage%20or%20breadth%2C%0Ai.e.%2C%20its%20ability%20to%20generate%20a%20rich%20set%20of%20strings.%20Recent%20work%20introduces%0Adifferent%20notions%20of%20breadth%20and%20explores%20when%20generation%20with%20breadth%20is%0Apossible%2C%20leaving%20a%20full%20characterization%20of%20these%20notions%20open.%20Our%20first%20set%0Aof%20results%20settles%20this%20by%20characterizing%20generation%20for%20existing%20notions%20of%0Abreadth%20and%20their%20natural%20extensions.%20Interestingly%2C%20our%20lower%20bounds%20are%20very%0Aflexible%20and%20hold%20for%20many%20performance%20metrics%20beyond%20breadth%20-%20for%20instance%2C%0Ashowing%20that%2C%20in%20general%2C%20it%20is%20impossible%20to%20train%20generators%20which%20achieve%20a%0Ahigher%20perplexity%20or%20lower%20hallucination%20rate%20for%20%24K%24%20compared%20to%20other%0Alanguages.%20Next%2C%20we%20study%20language%20generation%20with%20breadth%20and%20stable%0Agenerators%20-%20algorithms%20that%20eventually%20stop%20changing%20after%20seeing%20an%20arbitrary%0Abut%20finite%20number%20of%20strings%20-%20and%20prove%20unconditional%20lower%20bounds%20for%20such%0Agenerators%2C%20strengthening%20the%20results%20of%20%5BKMV25%5D%20and%20demonstrating%20that%0Ageneration%20with%20many%20existing%20notions%20of%20breadth%20becomes%20equally%20hard%2C%20when%0Astability%20is%20required.%20This%20gives%20a%20separation%20for%20generation%20with%20approximate%0Abreadth%2C%20between%20stable%20and%20unstable%20generators%2C%20highlighting%20the%20rich%0Ainterplay%20between%20breadth%2C%20stability%2C%20and%20consistency%20in%20language%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Characterizations%2520for%2520Language%2520Generation%253A%2520Interplay%2520of%250A%2520%2520Hallucinations%252C%2520Breadth%252C%2520and%2520Stability%26entry.906535625%3DAlkis%2520Kalavasis%2520and%2520Anay%2520Mehrotra%2520and%2520Grigoris%2520Velegkas%26entry.1292438233%3D%2520%2520We%2520study%2520language%2520generation%2520in%2520the%2520limit%2520-%2520introduced%2520by%2520Kleinberg%2520and%250AMullainathan%2520%255BKM24%255D%2520-%2520building%2520on%2520classical%2520works%2520of%2520Gold%2520%255BGol67%255D%2520and%2520Angluin%250A%255BAng79%255D.%2520%255BKM24%255D%2527s%2520main%2520result%2520is%2520an%2520algorithm%2520for%2520generating%2520from%2520any%2520countable%250Alanguage%2520collection%2520in%2520the%2520limit.%2520While%2520their%2520algorithm%2520eventually%2520generates%250Aunseen%2520strings%2520from%2520the%2520target%2520language%2520%2524K%2524%252C%2520it%2520sacrifices%2520coverage%2520or%2520breadth%252C%250Ai.e.%252C%2520its%2520ability%2520to%2520generate%2520a%2520rich%2520set%2520of%2520strings.%2520Recent%2520work%2520introduces%250Adifferent%2520notions%2520of%2520breadth%2520and%2520explores%2520when%2520generation%2520with%2520breadth%2520is%250Apossible%252C%2520leaving%2520a%2520full%2520characterization%2520of%2520these%2520notions%2520open.%2520Our%2520first%2520set%250Aof%2520results%2520settles%2520this%2520by%2520characterizing%2520generation%2520for%2520existing%2520notions%2520of%250Abreadth%2520and%2520their%2520natural%2520extensions.%2520Interestingly%252C%2520our%2520lower%2520bounds%2520are%2520very%250Aflexible%2520and%2520hold%2520for%2520many%2520performance%2520metrics%2520beyond%2520breadth%2520-%2520for%2520instance%252C%250Ashowing%2520that%252C%2520in%2520general%252C%2520it%2520is%2520impossible%2520to%2520train%2520generators%2520which%2520achieve%2520a%250Ahigher%2520perplexity%2520or%2520lower%2520hallucination%2520rate%2520for%2520%2524K%2524%2520compared%2520to%2520other%250Alanguages.%2520Next%252C%2520we%2520study%2520language%2520generation%2520with%2520breadth%2520and%2520stable%250Agenerators%2520-%2520algorithms%2520that%2520eventually%2520stop%2520changing%2520after%2520seeing%2520an%2520arbitrary%250Abut%2520finite%2520number%2520of%2520strings%2520-%2520and%2520prove%2520unconditional%2520lower%2520bounds%2520for%2520such%250Agenerators%252C%2520strengthening%2520the%2520results%2520of%2520%255BKMV25%255D%2520and%2520demonstrating%2520that%250Ageneration%2520with%2520many%2520existing%2520notions%2520of%2520breadth%2520becomes%2520equally%2520hard%252C%2520when%250Astability%2520is%2520required.%2520This%2520gives%2520a%2520separation%2520for%2520generation%2520with%2520approximate%250Abreadth%252C%2520between%2520stable%2520and%2520unstable%2520generators%252C%2520highlighting%2520the%2520rich%250Ainterplay%2520between%2520breadth%252C%2520stability%252C%2520and%2520consistency%2520in%2520language%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Characterizations%20for%20Language%20Generation%3A%20Interplay%20of%0A%20%20Hallucinations%2C%20Breadth%2C%20and%20Stability&entry.906535625=Alkis%20Kalavasis%20and%20Anay%20Mehrotra%20and%20Grigoris%20Velegkas&entry.1292438233=%20%20We%20study%20language%20generation%20in%20the%20limit%20-%20introduced%20by%20Kleinberg%20and%0AMullainathan%20%5BKM24%5D%20-%20building%20on%20classical%20works%20of%20Gold%20%5BGol67%5D%20and%20Angluin%0A%5BAng79%5D.%20%5BKM24%5D%27s%20main%20result%20is%20an%20algorithm%20for%20generating%20from%20any%20countable%0Alanguage%20collection%20in%20the%20limit.%20While%20their%20algorithm%20eventually%20generates%0Aunseen%20strings%20from%20the%20target%20language%20%24K%24%2C%20it%20sacrifices%20coverage%20or%20breadth%2C%0Ai.e.%2C%20its%20ability%20to%20generate%20a%20rich%20set%20of%20strings.%20Recent%20work%20introduces%0Adifferent%20notions%20of%20breadth%20and%20explores%20when%20generation%20with%20breadth%20is%0Apossible%2C%20leaving%20a%20full%20characterization%20of%20these%20notions%20open.%20Our%20first%20set%0Aof%20results%20settles%20this%20by%20characterizing%20generation%20for%20existing%20notions%20of%0Abreadth%20and%20their%20natural%20extensions.%20Interestingly%2C%20our%20lower%20bounds%20are%20very%0Aflexible%20and%20hold%20for%20many%20performance%20metrics%20beyond%20breadth%20-%20for%20instance%2C%0Ashowing%20that%2C%20in%20general%2C%20it%20is%20impossible%20to%20train%20generators%20which%20achieve%20a%0Ahigher%20perplexity%20or%20lower%20hallucination%20rate%20for%20%24K%24%20compared%20to%20other%0Alanguages.%20Next%2C%20we%20study%20language%20generation%20with%20breadth%20and%20stable%0Agenerators%20-%20algorithms%20that%20eventually%20stop%20changing%20after%20seeing%20an%20arbitrary%0Abut%20finite%20number%20of%20strings%20-%20and%20prove%20unconditional%20lower%20bounds%20for%20such%0Agenerators%2C%20strengthening%20the%20results%20of%20%5BKMV25%5D%20and%20demonstrating%20that%0Ageneration%20with%20many%20existing%20notions%20of%20breadth%20becomes%20equally%20hard%2C%20when%0Astability%20is%20required.%20This%20gives%20a%20separation%20for%20generation%20with%20approximate%0Abreadth%2C%20between%20stable%20and%20unstable%20generators%2C%20highlighting%20the%20rich%0Ainterplay%20between%20breadth%2C%20stability%2C%20and%20consistency%20in%20language%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18530v2&entry.124074799=Read"},
{"title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs", "author": "Ken Tsui", "abstract": "  Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.\n", "link": "http://arxiv.org/abs/2507.02778v1", "date": "2025-07-03", "relevancy": 1.9302, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4862}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Correction%20Bench%3A%20Revealing%20and%20Addressing%20the%20Self-Correction%0A%20%20Blind%20Spot%20in%20LLMs&body=Title%3A%20Self-Correction%20Bench%3A%20Revealing%20and%20Addressing%20the%20Self-Correction%0A%20%20Blind%20Spot%20in%20LLMs%0AAuthor%3A%20Ken%20Tsui%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20become%20transformative%2C%20they%20still%0Amake%20mistakes%20and%20can%20explore%20unproductive%20reasoning%20paths.%20Self-correction%20is%0Aan%20important%20capability%20for%20a%20trustworthy%20LLM%2C%20particularly%20an%20autoregressive%0ALLM.%20While%20LLMs%20can%20identify%20error%20in%20user%20input%2C%20they%20exhibit%20a%20systematic%0A%27Self-Correction%20Blind%20Spot%27%20-%20failing%20to%20correct%20identical%20error%20in%20their%20own%0Aoutputs.%20To%20systematically%20study%20this%20phenomenon%2C%20we%20introduce%20Self-Correction%0ABench%2C%20a%20systematic%20framework%20to%20measure%20this%20phenomenon%20through%20controlled%0Aerror%20injection%20at%20three%20complexity%20levels.%20Testing%2014%20models%2C%20we%20find%20an%0Aaverage%2064.5%25%20blind%20spot%20rate.%20We%20find%20multiple%20evidences%20that%20this%20limitation%0Arelates%20to%20training%20data%20composition%3A%20human%20training%20demonstrations%0Apredominantly%20show%20error-free%20responses%20rather%20than%20error-correction%20sequences%2C%0Aunlike%20RL-trained%20models%20that%20learn%20error%20correction%20through%20outcome%20feedback.%0ARemarkably%2C%20simply%20appending%20%22Wait%22%20reduces%20blind%20spots%20by%2089.3%25%2C%20suggesting%0Athat%20the%20capability%20exists%20but%20requires%20activation.%20Our%20work%20highlights%20a%0Acritical%20limitation%20in%20current%20LLMs%20and%20offers%20potential%20avenues%20for%20improving%0Atheir%20reliability%20and%20trustworthiness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Correction%2520Bench%253A%2520Revealing%2520and%2520Addressing%2520the%2520Self-Correction%250A%2520%2520Blind%2520Spot%2520in%2520LLMs%26entry.906535625%3DKen%2520Tsui%26entry.1292438233%3D%2520%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520transformative%252C%2520they%2520still%250Amake%2520mistakes%2520and%2520can%2520explore%2520unproductive%2520reasoning%2520paths.%2520Self-correction%2520is%250Aan%2520important%2520capability%2520for%2520a%2520trustworthy%2520LLM%252C%2520particularly%2520an%2520autoregressive%250ALLM.%2520While%2520LLMs%2520can%2520identify%2520error%2520in%2520user%2520input%252C%2520they%2520exhibit%2520a%2520systematic%250A%2527Self-Correction%2520Blind%2520Spot%2527%2520-%2520failing%2520to%2520correct%2520identical%2520error%2520in%2520their%2520own%250Aoutputs.%2520To%2520systematically%2520study%2520this%2520phenomenon%252C%2520we%2520introduce%2520Self-Correction%250ABench%252C%2520a%2520systematic%2520framework%2520to%2520measure%2520this%2520phenomenon%2520through%2520controlled%250Aerror%2520injection%2520at%2520three%2520complexity%2520levels.%2520Testing%252014%2520models%252C%2520we%2520find%2520an%250Aaverage%252064.5%2525%2520blind%2520spot%2520rate.%2520We%2520find%2520multiple%2520evidences%2520that%2520this%2520limitation%250Arelates%2520to%2520training%2520data%2520composition%253A%2520human%2520training%2520demonstrations%250Apredominantly%2520show%2520error-free%2520responses%2520rather%2520than%2520error-correction%2520sequences%252C%250Aunlike%2520RL-trained%2520models%2520that%2520learn%2520error%2520correction%2520through%2520outcome%2520feedback.%250ARemarkably%252C%2520simply%2520appending%2520%2522Wait%2522%2520reduces%2520blind%2520spots%2520by%252089.3%2525%252C%2520suggesting%250Athat%2520the%2520capability%2520exists%2520but%2520requires%2520activation.%2520Our%2520work%2520highlights%2520a%250Acritical%2520limitation%2520in%2520current%2520LLMs%2520and%2520offers%2520potential%2520avenues%2520for%2520improving%250Atheir%2520reliability%2520and%2520trustworthiness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Correction%20Bench%3A%20Revealing%20and%20Addressing%20the%20Self-Correction%0A%20%20Blind%20Spot%20in%20LLMs&entry.906535625=Ken%20Tsui&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20become%20transformative%2C%20they%20still%0Amake%20mistakes%20and%20can%20explore%20unproductive%20reasoning%20paths.%20Self-correction%20is%0Aan%20important%20capability%20for%20a%20trustworthy%20LLM%2C%20particularly%20an%20autoregressive%0ALLM.%20While%20LLMs%20can%20identify%20error%20in%20user%20input%2C%20they%20exhibit%20a%20systematic%0A%27Self-Correction%20Blind%20Spot%27%20-%20failing%20to%20correct%20identical%20error%20in%20their%20own%0Aoutputs.%20To%20systematically%20study%20this%20phenomenon%2C%20we%20introduce%20Self-Correction%0ABench%2C%20a%20systematic%20framework%20to%20measure%20this%20phenomenon%20through%20controlled%0Aerror%20injection%20at%20three%20complexity%20levels.%20Testing%2014%20models%2C%20we%20find%20an%0Aaverage%2064.5%25%20blind%20spot%20rate.%20We%20find%20multiple%20evidences%20that%20this%20limitation%0Arelates%20to%20training%20data%20composition%3A%20human%20training%20demonstrations%0Apredominantly%20show%20error-free%20responses%20rather%20than%20error-correction%20sequences%2C%0Aunlike%20RL-trained%20models%20that%20learn%20error%20correction%20through%20outcome%20feedback.%0ARemarkably%2C%20simply%20appending%20%22Wait%22%20reduces%20blind%20spots%20by%2089.3%25%2C%20suggesting%0Athat%20the%20capability%20exists%20but%20requires%20activation.%20Our%20work%20highlights%20a%0Acritical%20limitation%20in%20current%20LLMs%20and%20offers%20potential%20avenues%20for%20improving%0Atheir%20reliability%20and%20trustworthiness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02778v1&entry.124074799=Read"},
{"title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms", "author": "Shiyi Liu and Buwen Liang and Yuetong Fang and Zixuan Jiang and Renjing Xu", "abstract": "  Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.\n", "link": "http://arxiv.org/abs/2507.02724v1", "date": "2025-07-03", "relevancy": 1.9297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4982}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms&body=Title%3A%20Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms%0AAuthor%3A%20Shiyi%20Liu%20and%20Buwen%20Liang%20and%20Yuetong%20Fang%20and%20Zixuan%20Jiang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI%20for%20science%20have%20highlighted%20the%20power%20of%20contrastive%0Alearning%20in%20bridging%20heterogeneous%20biological%20data%20modalities.%20Building%20on%20this%0Aparadigm%2C%20we%20propose%20HIPPO%20%28HIerarchical%20Protein-Protein%20interaction%20prediction%0Aacross%20Organisms%29%2C%20a%20hierarchical%20contrastive%20framework%20for%20protein-protein%0Ainteraction%28PPI%29%20prediction%2C%20where%20protein%20sequences%20and%20their%20hierarchical%0Aattributes%20are%20aligned%20through%20multi-tiered%20biological%20representation%20matching.%0AThe%20proposed%20approach%20incorporates%20hierarchical%20contrastive%20loss%20functions%20that%0Aemulate%20the%20structured%20relationship%20among%20functional%20classes%20of%20proteins.%20The%0Aframework%20adaptively%20incorporates%20domain%20and%20family%20knowledge%20through%20a%0Adata-driven%20penalty%20mechanism%2C%20enforcing%20consistency%20between%20the%20learned%0Aembedding%20space%20and%20the%20intrinsic%20hierarchy%20of%20protein%20functions.%20Experiments%0Aon%20benchmark%20datasets%20demonstrate%20that%20HIPPO%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20existing%20methods%20and%20showing%20robustness%20in%20low-data%0Aregimes.%20Notably%2C%20the%20model%20demonstrates%20strong%20zero-shot%20transferability%20to%0Aother%20species%20without%20retraining%2C%20enabling%20reliable%20PPI%20prediction%20and%0Afunctional%20inference%20even%20in%20less%20characterized%20or%20rare%20organisms%20where%0Aexperimental%20data%20are%20limited.%20Further%20analysis%20reveals%20that%20hierarchical%0Afeature%20fusion%20is%20critical%20for%20capturing%20conserved%20interaction%20determinants%2C%0Asuch%20as%20binding%20motifs%20and%20functional%20annotations.%20This%20work%20advances%0Across-species%20PPI%20prediction%20and%20provides%20a%20unified%20framework%20for%20interaction%0Aprediction%20in%20scenarios%20with%20sparse%20or%20imbalanced%20multi-species%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multi-Label%2520Contrastive%2520Learning%2520for%2520Protein-Protein%250A%2520%2520Interaction%2520Prediction%2520Across%2520Organisms%26entry.906535625%3DShiyi%2520Liu%2520and%2520Buwen%2520Liang%2520and%2520Yuetong%2520Fang%2520and%2520Zixuan%2520Jiang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI%2520for%2520science%2520have%2520highlighted%2520the%2520power%2520of%2520contrastive%250Alearning%2520in%2520bridging%2520heterogeneous%2520biological%2520data%2520modalities.%2520Building%2520on%2520this%250Aparadigm%252C%2520we%2520propose%2520HIPPO%2520%2528HIerarchical%2520Protein-Protein%2520interaction%2520prediction%250Aacross%2520Organisms%2529%252C%2520a%2520hierarchical%2520contrastive%2520framework%2520for%2520protein-protein%250Ainteraction%2528PPI%2529%2520prediction%252C%2520where%2520protein%2520sequences%2520and%2520their%2520hierarchical%250Aattributes%2520are%2520aligned%2520through%2520multi-tiered%2520biological%2520representation%2520matching.%250AThe%2520proposed%2520approach%2520incorporates%2520hierarchical%2520contrastive%2520loss%2520functions%2520that%250Aemulate%2520the%2520structured%2520relationship%2520among%2520functional%2520classes%2520of%2520proteins.%2520The%250Aframework%2520adaptively%2520incorporates%2520domain%2520and%2520family%2520knowledge%2520through%2520a%250Adata-driven%2520penalty%2520mechanism%252C%2520enforcing%2520consistency%2520between%2520the%2520learned%250Aembedding%2520space%2520and%2520the%2520intrinsic%2520hierarchy%2520of%2520protein%2520functions.%2520Experiments%250Aon%2520benchmark%2520datasets%2520demonstrate%2520that%2520HIPPO%2520achieves%2520state-of-the-art%250Aperformance%252C%2520outperforming%2520existing%2520methods%2520and%2520showing%2520robustness%2520in%2520low-data%250Aregimes.%2520Notably%252C%2520the%2520model%2520demonstrates%2520strong%2520zero-shot%2520transferability%2520to%250Aother%2520species%2520without%2520retraining%252C%2520enabling%2520reliable%2520PPI%2520prediction%2520and%250Afunctional%2520inference%2520even%2520in%2520less%2520characterized%2520or%2520rare%2520organisms%2520where%250Aexperimental%2520data%2520are%2520limited.%2520Further%2520analysis%2520reveals%2520that%2520hierarchical%250Afeature%2520fusion%2520is%2520critical%2520for%2520capturing%2520conserved%2520interaction%2520determinants%252C%250Asuch%2520as%2520binding%2520motifs%2520and%2520functional%2520annotations.%2520This%2520work%2520advances%250Across-species%2520PPI%2520prediction%2520and%2520provides%2520a%2520unified%2520framework%2520for%2520interaction%250Aprediction%2520in%2520scenarios%2520with%2520sparse%2520or%2520imbalanced%2520multi-species%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms&entry.906535625=Shiyi%20Liu%20and%20Buwen%20Liang%20and%20Yuetong%20Fang%20and%20Zixuan%20Jiang%20and%20Renjing%20Xu&entry.1292438233=%20%20Recent%20advances%20in%20AI%20for%20science%20have%20highlighted%20the%20power%20of%20contrastive%0Alearning%20in%20bridging%20heterogeneous%20biological%20data%20modalities.%20Building%20on%20this%0Aparadigm%2C%20we%20propose%20HIPPO%20%28HIerarchical%20Protein-Protein%20interaction%20prediction%0Aacross%20Organisms%29%2C%20a%20hierarchical%20contrastive%20framework%20for%20protein-protein%0Ainteraction%28PPI%29%20prediction%2C%20where%20protein%20sequences%20and%20their%20hierarchical%0Aattributes%20are%20aligned%20through%20multi-tiered%20biological%20representation%20matching.%0AThe%20proposed%20approach%20incorporates%20hierarchical%20contrastive%20loss%20functions%20that%0Aemulate%20the%20structured%20relationship%20among%20functional%20classes%20of%20proteins.%20The%0Aframework%20adaptively%20incorporates%20domain%20and%20family%20knowledge%20through%20a%0Adata-driven%20penalty%20mechanism%2C%20enforcing%20consistency%20between%20the%20learned%0Aembedding%20space%20and%20the%20intrinsic%20hierarchy%20of%20protein%20functions.%20Experiments%0Aon%20benchmark%20datasets%20demonstrate%20that%20HIPPO%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20existing%20methods%20and%20showing%20robustness%20in%20low-data%0Aregimes.%20Notably%2C%20the%20model%20demonstrates%20strong%20zero-shot%20transferability%20to%0Aother%20species%20without%20retraining%2C%20enabling%20reliable%20PPI%20prediction%20and%0Afunctional%20inference%20even%20in%20less%20characterized%20or%20rare%20organisms%20where%0Aexperimental%20data%20are%20limited.%20Further%20analysis%20reveals%20that%20hierarchical%0Afeature%20fusion%20is%20critical%20for%20capturing%20conserved%20interaction%20determinants%2C%0Asuch%20as%20binding%20motifs%20and%20functional%20annotations.%20This%20work%20advances%0Across-species%20PPI%20prediction%20and%20provides%20a%20unified%20framework%20for%20interaction%0Aprediction%20in%20scenarios%20with%20sparse%20or%20imbalanced%20multi-species%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02724v1&entry.124074799=Read"},
{"title": "Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular\n  Analog Generation", "author": "Shuan Chen and Gunwook Nam and Yousung Jung", "abstract": "  The disconnect between AI-generated molecules with desirable properties and\ntheir synthetic feasibility remains a critical bottleneck in computational drug\nand material discovery. While generative AI has accelerated the proposal of\ncandidate molecules, many of these structures prove challenging or impossible\nto synthesize using established chemical reactions. Here, we introduce\nSynTwins, a novel retrosynthesis-guided molecular analog design framework that\ndesigns synthetically accessible molecular analogs by emulating expert chemist\nstrategies through a three-step process: retrosynthesis, similar building block\nsearching, and virtual synthesis. In comparative evaluations, SynTwins\ndemonstrates superior performance in generating synthetically accessible\nanalogs compared to state-of-the-art machine learning models while maintaining\nhigh structural similarity to original target molecules. Furthermore, when\nintegrated with existing molecule optimization frameworks, our hybrid approach\nproduces synthetically feasible molecules with property profiles comparable to\nunconstrained molecule generators, yet its synthesizability ensured. Our\ncomprehensive benchmarking across diverse molecular datasets demonstrates that\nSynTwins effectively bridges the gap between computational design and\nexperimental synthesis, providing a practical solution for accelerating the\ndiscovery of synthesizable molecules with desired properties for a wide range\nof applications.\n", "link": "http://arxiv.org/abs/2507.02752v1", "date": "2025-07-03", "relevancy": 1.9225, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.495}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4884}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizable%20by%20Design%3A%20A%20Retrosynthesis-Guided%20Framework%20for%20Molecular%0A%20%20Analog%20Generation&body=Title%3A%20Synthesizable%20by%20Design%3A%20A%20Retrosynthesis-Guided%20Framework%20for%20Molecular%0A%20%20Analog%20Generation%0AAuthor%3A%20Shuan%20Chen%20and%20Gunwook%20Nam%20and%20Yousung%20Jung%0AAbstract%3A%20%20%20The%20disconnect%20between%20AI-generated%20molecules%20with%20desirable%20properties%20and%0Atheir%20synthetic%20feasibility%20remains%20a%20critical%20bottleneck%20in%20computational%20drug%0Aand%20material%20discovery.%20While%20generative%20AI%20has%20accelerated%20the%20proposal%20of%0Acandidate%20molecules%2C%20many%20of%20these%20structures%20prove%20challenging%20or%20impossible%0Ato%20synthesize%20using%20established%20chemical%20reactions.%20Here%2C%20we%20introduce%0ASynTwins%2C%20a%20novel%20retrosynthesis-guided%20molecular%20analog%20design%20framework%20that%0Adesigns%20synthetically%20accessible%20molecular%20analogs%20by%20emulating%20expert%20chemist%0Astrategies%20through%20a%20three-step%20process%3A%20retrosynthesis%2C%20similar%20building%20block%0Asearching%2C%20and%20virtual%20synthesis.%20In%20comparative%20evaluations%2C%20SynTwins%0Ademonstrates%20superior%20performance%20in%20generating%20synthetically%20accessible%0Aanalogs%20compared%20to%20state-of-the-art%20machine%20learning%20models%20while%20maintaining%0Ahigh%20structural%20similarity%20to%20original%20target%20molecules.%20Furthermore%2C%20when%0Aintegrated%20with%20existing%20molecule%20optimization%20frameworks%2C%20our%20hybrid%20approach%0Aproduces%20synthetically%20feasible%20molecules%20with%20property%20profiles%20comparable%20to%0Aunconstrained%20molecule%20generators%2C%20yet%20its%20synthesizability%20ensured.%20Our%0Acomprehensive%20benchmarking%20across%20diverse%20molecular%20datasets%20demonstrates%20that%0ASynTwins%20effectively%20bridges%20the%20gap%20between%20computational%20design%20and%0Aexperimental%20synthesis%2C%20providing%20a%20practical%20solution%20for%20accelerating%20the%0Adiscovery%20of%20synthesizable%20molecules%20with%20desired%20properties%20for%20a%20wide%20range%0Aof%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizable%2520by%2520Design%253A%2520A%2520Retrosynthesis-Guided%2520Framework%2520for%2520Molecular%250A%2520%2520Analog%2520Generation%26entry.906535625%3DShuan%2520Chen%2520and%2520Gunwook%2520Nam%2520and%2520Yousung%2520Jung%26entry.1292438233%3D%2520%2520The%2520disconnect%2520between%2520AI-generated%2520molecules%2520with%2520desirable%2520properties%2520and%250Atheir%2520synthetic%2520feasibility%2520remains%2520a%2520critical%2520bottleneck%2520in%2520computational%2520drug%250Aand%2520material%2520discovery.%2520While%2520generative%2520AI%2520has%2520accelerated%2520the%2520proposal%2520of%250Acandidate%2520molecules%252C%2520many%2520of%2520these%2520structures%2520prove%2520challenging%2520or%2520impossible%250Ato%2520synthesize%2520using%2520established%2520chemical%2520reactions.%2520Here%252C%2520we%2520introduce%250ASynTwins%252C%2520a%2520novel%2520retrosynthesis-guided%2520molecular%2520analog%2520design%2520framework%2520that%250Adesigns%2520synthetically%2520accessible%2520molecular%2520analogs%2520by%2520emulating%2520expert%2520chemist%250Astrategies%2520through%2520a%2520three-step%2520process%253A%2520retrosynthesis%252C%2520similar%2520building%2520block%250Asearching%252C%2520and%2520virtual%2520synthesis.%2520In%2520comparative%2520evaluations%252C%2520SynTwins%250Ademonstrates%2520superior%2520performance%2520in%2520generating%2520synthetically%2520accessible%250Aanalogs%2520compared%2520to%2520state-of-the-art%2520machine%2520learning%2520models%2520while%2520maintaining%250Ahigh%2520structural%2520similarity%2520to%2520original%2520target%2520molecules.%2520Furthermore%252C%2520when%250Aintegrated%2520with%2520existing%2520molecule%2520optimization%2520frameworks%252C%2520our%2520hybrid%2520approach%250Aproduces%2520synthetically%2520feasible%2520molecules%2520with%2520property%2520profiles%2520comparable%2520to%250Aunconstrained%2520molecule%2520generators%252C%2520yet%2520its%2520synthesizability%2520ensured.%2520Our%250Acomprehensive%2520benchmarking%2520across%2520diverse%2520molecular%2520datasets%2520demonstrates%2520that%250ASynTwins%2520effectively%2520bridges%2520the%2520gap%2520between%2520computational%2520design%2520and%250Aexperimental%2520synthesis%252C%2520providing%2520a%2520practical%2520solution%2520for%2520accelerating%2520the%250Adiscovery%2520of%2520synthesizable%2520molecules%2520with%2520desired%2520properties%2520for%2520a%2520wide%2520range%250Aof%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizable%20by%20Design%3A%20A%20Retrosynthesis-Guided%20Framework%20for%20Molecular%0A%20%20Analog%20Generation&entry.906535625=Shuan%20Chen%20and%20Gunwook%20Nam%20and%20Yousung%20Jung&entry.1292438233=%20%20The%20disconnect%20between%20AI-generated%20molecules%20with%20desirable%20properties%20and%0Atheir%20synthetic%20feasibility%20remains%20a%20critical%20bottleneck%20in%20computational%20drug%0Aand%20material%20discovery.%20While%20generative%20AI%20has%20accelerated%20the%20proposal%20of%0Acandidate%20molecules%2C%20many%20of%20these%20structures%20prove%20challenging%20or%20impossible%0Ato%20synthesize%20using%20established%20chemical%20reactions.%20Here%2C%20we%20introduce%0ASynTwins%2C%20a%20novel%20retrosynthesis-guided%20molecular%20analog%20design%20framework%20that%0Adesigns%20synthetically%20accessible%20molecular%20analogs%20by%20emulating%20expert%20chemist%0Astrategies%20through%20a%20three-step%20process%3A%20retrosynthesis%2C%20similar%20building%20block%0Asearching%2C%20and%20virtual%20synthesis.%20In%20comparative%20evaluations%2C%20SynTwins%0Ademonstrates%20superior%20performance%20in%20generating%20synthetically%20accessible%0Aanalogs%20compared%20to%20state-of-the-art%20machine%20learning%20models%20while%20maintaining%0Ahigh%20structural%20similarity%20to%20original%20target%20molecules.%20Furthermore%2C%20when%0Aintegrated%20with%20existing%20molecule%20optimization%20frameworks%2C%20our%20hybrid%20approach%0Aproduces%20synthetically%20feasible%20molecules%20with%20property%20profiles%20comparable%20to%0Aunconstrained%20molecule%20generators%2C%20yet%20its%20synthesizability%20ensured.%20Our%0Acomprehensive%20benchmarking%20across%20diverse%20molecular%20datasets%20demonstrates%20that%0ASynTwins%20effectively%20bridges%20the%20gap%20between%20computational%20design%20and%0Aexperimental%20synthesis%2C%20providing%20a%20practical%20solution%20for%20accelerating%20the%0Adiscovery%20of%20synthesizable%20molecules%20with%20desired%20properties%20for%20a%20wide%20range%0Aof%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02752v1&entry.124074799=Read"},
{"title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation", "author": "Nikhil Chandak and Shashwat Goel and Ameya Prabhu and Moritz Hardt and Jonas Geiping", "abstract": "  Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.\n", "link": "http://arxiv.org/abs/2507.02856v1", "date": "2025-07-03", "relevancy": 1.9066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Answer%20Matching%20Outperforms%20Multiple%20Choice%20for%20Language%20Model%0A%20%20Evaluation&body=Title%3A%20Answer%20Matching%20Outperforms%20Multiple%20Choice%20for%20Language%20Model%0A%20%20Evaluation%0AAuthor%3A%20Nikhil%20Chandak%20and%20Shashwat%20Goel%20and%20Ameya%20Prabhu%20and%20Moritz%20Hardt%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20Multiple%20choice%20benchmarks%20have%20long%20been%20the%20workhorse%20of%20language%20model%0Aevaluation%20because%20grading%20multiple%20choice%20is%20objective%20and%20easy%20to%20automate.%0AHowever%2C%20we%20show%20multiple%20choice%20questions%20from%20popular%20benchmarks%20can%20often%20be%0Aanswered%20without%20even%20seeing%20the%20question.%20These%20shortcuts%20arise%20from%20a%0Afundamental%20limitation%20of%20discriminative%20evaluation%20not%20shared%20by%20evaluations%0Aof%20the%20model%27s%20free-form%2C%20generative%20answers.%20Until%20recently%2C%20there%20appeared%20to%0Abe%20no%20viable%2C%20scalable%20alternative%20to%20multiple%20choice--but%2C%20we%20show%20that%20this%0Ahas%20changed.%20We%20consider%20generative%20evaluation%20via%20what%20we%20call%20answer%0Amatching%3A%20Give%20the%20candidate%20model%20the%20question%20without%20the%20options%2C%20have%20it%0Agenerate%20a%20free-form%20response%2C%20then%20use%20a%20modern%20language%20model%20with%20the%0Areference%20answer%20to%20determine%20if%20the%20response%20matches%20the%20reference.%20To%20compare%0Athe%20validity%20of%20different%20evaluation%20strategies%2C%20we%20annotate%20MMLU-Pro%20and%0AGPQA-Diamond%20to%20obtain%20human%20grading%20data%2C%20and%20measure%20the%20agreement%20of%20each%0Aevaluation%20approach.%20We%20find%20answer%20matching%20using%20recent%20models--even%20small%0Aones--achieves%20near-perfect%20agreement%2C%20in%20the%20range%20of%20inter-annotator%0Aagreement.%20In%20contrast%2C%20both%20multiple%20choice%20evaluation%20and%20using%0ALLM-as-a-judge%20without%20reference%20answers%20aligns%20poorly%20with%20human%20grading.%0AImproving%20evaluations%20via%20answer%20matching%20is%20not%20merely%20a%20conceptual%20concern%3A%0Athe%20rankings%20of%20several%20models%20change%20significantly%20when%20evaluating%20their%0Afree-form%20responses%20with%20answer%20matching.%20In%20light%20of%20these%20findings%2C%20we%0Adiscuss%20how%20to%20move%20the%20evaluation%20ecosystem%20from%20multiple%20choice%20to%20answer%0Amatching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnswer%2520Matching%2520Outperforms%2520Multiple%2520Choice%2520for%2520Language%2520Model%250A%2520%2520Evaluation%26entry.906535625%3DNikhil%2520Chandak%2520and%2520Shashwat%2520Goel%2520and%2520Ameya%2520Prabhu%2520and%2520Moritz%2520Hardt%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520Multiple%2520choice%2520benchmarks%2520have%2520long%2520been%2520the%2520workhorse%2520of%2520language%2520model%250Aevaluation%2520because%2520grading%2520multiple%2520choice%2520is%2520objective%2520and%2520easy%2520to%2520automate.%250AHowever%252C%2520we%2520show%2520multiple%2520choice%2520questions%2520from%2520popular%2520benchmarks%2520can%2520often%2520be%250Aanswered%2520without%2520even%2520seeing%2520the%2520question.%2520These%2520shortcuts%2520arise%2520from%2520a%250Afundamental%2520limitation%2520of%2520discriminative%2520evaluation%2520not%2520shared%2520by%2520evaluations%250Aof%2520the%2520model%2527s%2520free-form%252C%2520generative%2520answers.%2520Until%2520recently%252C%2520there%2520appeared%2520to%250Abe%2520no%2520viable%252C%2520scalable%2520alternative%2520to%2520multiple%2520choice--but%252C%2520we%2520show%2520that%2520this%250Ahas%2520changed.%2520We%2520consider%2520generative%2520evaluation%2520via%2520what%2520we%2520call%2520answer%250Amatching%253A%2520Give%2520the%2520candidate%2520model%2520the%2520question%2520without%2520the%2520options%252C%2520have%2520it%250Agenerate%2520a%2520free-form%2520response%252C%2520then%2520use%2520a%2520modern%2520language%2520model%2520with%2520the%250Areference%2520answer%2520to%2520determine%2520if%2520the%2520response%2520matches%2520the%2520reference.%2520To%2520compare%250Athe%2520validity%2520of%2520different%2520evaluation%2520strategies%252C%2520we%2520annotate%2520MMLU-Pro%2520and%250AGPQA-Diamond%2520to%2520obtain%2520human%2520grading%2520data%252C%2520and%2520measure%2520the%2520agreement%2520of%2520each%250Aevaluation%2520approach.%2520We%2520find%2520answer%2520matching%2520using%2520recent%2520models--even%2520small%250Aones--achieves%2520near-perfect%2520agreement%252C%2520in%2520the%2520range%2520of%2520inter-annotator%250Aagreement.%2520In%2520contrast%252C%2520both%2520multiple%2520choice%2520evaluation%2520and%2520using%250ALLM-as-a-judge%2520without%2520reference%2520answers%2520aligns%2520poorly%2520with%2520human%2520grading.%250AImproving%2520evaluations%2520via%2520answer%2520matching%2520is%2520not%2520merely%2520a%2520conceptual%2520concern%253A%250Athe%2520rankings%2520of%2520several%2520models%2520change%2520significantly%2520when%2520evaluating%2520their%250Afree-form%2520responses%2520with%2520answer%2520matching.%2520In%2520light%2520of%2520these%2520findings%252C%2520we%250Adiscuss%2520how%2520to%2520move%2520the%2520evaluation%2520ecosystem%2520from%2520multiple%2520choice%2520to%2520answer%250Amatching.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Answer%20Matching%20Outperforms%20Multiple%20Choice%20for%20Language%20Model%0A%20%20Evaluation&entry.906535625=Nikhil%20Chandak%20and%20Shashwat%20Goel%20and%20Ameya%20Prabhu%20and%20Moritz%20Hardt%20and%20Jonas%20Geiping&entry.1292438233=%20%20Multiple%20choice%20benchmarks%20have%20long%20been%20the%20workhorse%20of%20language%20model%0Aevaluation%20because%20grading%20multiple%20choice%20is%20objective%20and%20easy%20to%20automate.%0AHowever%2C%20we%20show%20multiple%20choice%20questions%20from%20popular%20benchmarks%20can%20often%20be%0Aanswered%20without%20even%20seeing%20the%20question.%20These%20shortcuts%20arise%20from%20a%0Afundamental%20limitation%20of%20discriminative%20evaluation%20not%20shared%20by%20evaluations%0Aof%20the%20model%27s%20free-form%2C%20generative%20answers.%20Until%20recently%2C%20there%20appeared%20to%0Abe%20no%20viable%2C%20scalable%20alternative%20to%20multiple%20choice--but%2C%20we%20show%20that%20this%0Ahas%20changed.%20We%20consider%20generative%20evaluation%20via%20what%20we%20call%20answer%0Amatching%3A%20Give%20the%20candidate%20model%20the%20question%20without%20the%20options%2C%20have%20it%0Agenerate%20a%20free-form%20response%2C%20then%20use%20a%20modern%20language%20model%20with%20the%0Areference%20answer%20to%20determine%20if%20the%20response%20matches%20the%20reference.%20To%20compare%0Athe%20validity%20of%20different%20evaluation%20strategies%2C%20we%20annotate%20MMLU-Pro%20and%0AGPQA-Diamond%20to%20obtain%20human%20grading%20data%2C%20and%20measure%20the%20agreement%20of%20each%0Aevaluation%20approach.%20We%20find%20answer%20matching%20using%20recent%20models--even%20small%0Aones--achieves%20near-perfect%20agreement%2C%20in%20the%20range%20of%20inter-annotator%0Aagreement.%20In%20contrast%2C%20both%20multiple%20choice%20evaluation%20and%20using%0ALLM-as-a-judge%20without%20reference%20answers%20aligns%20poorly%20with%20human%20grading.%0AImproving%20evaluations%20via%20answer%20matching%20is%20not%20merely%20a%20conceptual%20concern%3A%0Athe%20rankings%20of%20several%20models%20change%20significantly%20when%20evaluating%20their%0Afree-form%20responses%20with%20answer%20matching.%20In%20light%20of%20these%20findings%2C%20we%0Adiscuss%20how%20to%20move%20the%20evaluation%20ecosystem%20from%20multiple%20choice%20to%20answer%0Amatching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02856v1&entry.124074799=Read"},
{"title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data", "author": "Irena Girshovitz and Atai Ambus and Moni Shahar and Ran Gilad-Bachrach", "abstract": "  Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.\n", "link": "http://arxiv.org/abs/2507.02628v1", "date": "2025-07-03", "relevancy": 1.8895, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20Data%20Pecking%3A%20A%20Context-Aware%20Approach%20for%20Automated%20Quality%0A%20%20Evaluation%20of%20Structured%20Medical%20Data&body=Title%3A%20Medical%20Data%20Pecking%3A%20A%20Context-Aware%20Approach%20for%20Automated%20Quality%0A%20%20Evaluation%20of%20Structured%20Medical%20Data%0AAuthor%3A%20Irena%20Girshovitz%20and%20Atai%20Ambus%20and%20Moni%20Shahar%20and%20Ran%20Gilad-Bachrach%0AAbstract%3A%20%20%20Background%3A%20The%20use%20of%20Electronic%20Health%20Records%20%28EHRs%29%20for%20epidemiological%0Astudies%20and%20artificial%20intelligence%20%28AI%29%20training%20is%20increasing%20rapidly.%20The%0Areliability%20of%20the%20results%20depends%20on%20the%20accuracy%20and%20completeness%20of%20EHR%0Adata.%20However%2C%20EHR%20data%20often%20contain%20significant%20quality%20issues%2C%20including%0Amisrepresentations%20of%20subpopulations%2C%20biases%2C%20and%20systematic%20errors%2C%20as%20they%0Aare%20primarily%20collected%20for%20clinical%20and%20billing%20purposes.%20Existing%20quality%0Aassessment%20methods%20remain%20insufficient%2C%20lacking%20systematic%20procedures%20to%20assess%0Adata%20fitness%20for%20research.%0A%20%20Methods%3A%20We%20present%20the%20Medical%20Data%20Pecking%20approach%2C%20which%20adapts%20unit%0Atesting%20and%20coverage%20concepts%20from%20software%20engineering%20to%20identify%20data%0Aquality%20concerns.%20We%20demonstrate%20our%20approach%20using%20the%20Medical%20Data%20Pecking%0ATool%20%28MDPT%29%2C%20which%20consists%20of%20two%20main%20components%3A%20%281%29%20an%20automated%20test%0Agenerator%20that%20uses%20large%20language%20models%20and%20grounding%20techniques%20to%20create%20a%0Atest%20suite%20from%20data%20and%20study%20descriptions%2C%20and%20%282%29%20a%20data%20testing%20framework%0Athat%20executes%20these%20tests%2C%20reporting%20potential%20errors%20and%20coverage.%0A%20%20Results%3A%20We%20evaluated%20MDPT%20on%20three%20datasets%3A%20All%20of%20Us%20%28AoU%29%2C%20MIMIC-III%2C%20and%0ASyntheticMass%2C%20generating%2055-73%20tests%20per%20cohort%20across%20four%20conditions.%20These%0Atests%20correctly%20identified%2020-43%20non-aligned%20or%20non-conforming%20data%20issues.%20We%0Apresent%20a%20detailed%20analysis%20of%20the%20LLM-generated%20test%20suites%20in%20terms%20of%0Areference%20grounding%20and%20value%20accuracy.%0A%20%20Conclusion%3A%20Our%20approach%20incorporates%20external%20medical%20knowledge%20to%20enable%0Acontext-sensitive%20data%20quality%20testing%20as%20part%20of%20the%20data%20analysis%20workflow%20to%0Aimprove%20the%20validity%20of%20its%20outcomes.%20Our%20approach%20tackles%20these%20challenges%0Afrom%20a%20quality%20assurance%20perspective%2C%20laying%20the%20foundation%20for%20further%0Adevelopment%20such%20as%20additional%20data%20modalities%20and%20improved%20grounding%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520Data%2520Pecking%253A%2520A%2520Context-Aware%2520Approach%2520for%2520Automated%2520Quality%250A%2520%2520Evaluation%2520of%2520Structured%2520Medical%2520Data%26entry.906535625%3DIrena%2520Girshovitz%2520and%2520Atai%2520Ambus%2520and%2520Moni%2520Shahar%2520and%2520Ran%2520Gilad-Bachrach%26entry.1292438233%3D%2520%2520Background%253A%2520The%2520use%2520of%2520Electronic%2520Health%2520Records%2520%2528EHRs%2529%2520for%2520epidemiological%250Astudies%2520and%2520artificial%2520intelligence%2520%2528AI%2529%2520training%2520is%2520increasing%2520rapidly.%2520The%250Areliability%2520of%2520the%2520results%2520depends%2520on%2520the%2520accuracy%2520and%2520completeness%2520of%2520EHR%250Adata.%2520However%252C%2520EHR%2520data%2520often%2520contain%2520significant%2520quality%2520issues%252C%2520including%250Amisrepresentations%2520of%2520subpopulations%252C%2520biases%252C%2520and%2520systematic%2520errors%252C%2520as%2520they%250Aare%2520primarily%2520collected%2520for%2520clinical%2520and%2520billing%2520purposes.%2520Existing%2520quality%250Aassessment%2520methods%2520remain%2520insufficient%252C%2520lacking%2520systematic%2520procedures%2520to%2520assess%250Adata%2520fitness%2520for%2520research.%250A%2520%2520Methods%253A%2520We%2520present%2520the%2520Medical%2520Data%2520Pecking%2520approach%252C%2520which%2520adapts%2520unit%250Atesting%2520and%2520coverage%2520concepts%2520from%2520software%2520engineering%2520to%2520identify%2520data%250Aquality%2520concerns.%2520We%2520demonstrate%2520our%2520approach%2520using%2520the%2520Medical%2520Data%2520Pecking%250ATool%2520%2528MDPT%2529%252C%2520which%2520consists%2520of%2520two%2520main%2520components%253A%2520%25281%2529%2520an%2520automated%2520test%250Agenerator%2520that%2520uses%2520large%2520language%2520models%2520and%2520grounding%2520techniques%2520to%2520create%2520a%250Atest%2520suite%2520from%2520data%2520and%2520study%2520descriptions%252C%2520and%2520%25282%2529%2520a%2520data%2520testing%2520framework%250Athat%2520executes%2520these%2520tests%252C%2520reporting%2520potential%2520errors%2520and%2520coverage.%250A%2520%2520Results%253A%2520We%2520evaluated%2520MDPT%2520on%2520three%2520datasets%253A%2520All%2520of%2520Us%2520%2528AoU%2529%252C%2520MIMIC-III%252C%2520and%250ASyntheticMass%252C%2520generating%252055-73%2520tests%2520per%2520cohort%2520across%2520four%2520conditions.%2520These%250Atests%2520correctly%2520identified%252020-43%2520non-aligned%2520or%2520non-conforming%2520data%2520issues.%2520We%250Apresent%2520a%2520detailed%2520analysis%2520of%2520the%2520LLM-generated%2520test%2520suites%2520in%2520terms%2520of%250Areference%2520grounding%2520and%2520value%2520accuracy.%250A%2520%2520Conclusion%253A%2520Our%2520approach%2520incorporates%2520external%2520medical%2520knowledge%2520to%2520enable%250Acontext-sensitive%2520data%2520quality%2520testing%2520as%2520part%2520of%2520the%2520data%2520analysis%2520workflow%2520to%250Aimprove%2520the%2520validity%2520of%2520its%2520outcomes.%2520Our%2520approach%2520tackles%2520these%2520challenges%250Afrom%2520a%2520quality%2520assurance%2520perspective%252C%2520laying%2520the%2520foundation%2520for%2520further%250Adevelopment%2520such%2520as%2520additional%2520data%2520modalities%2520and%2520improved%2520grounding%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Data%20Pecking%3A%20A%20Context-Aware%20Approach%20for%20Automated%20Quality%0A%20%20Evaluation%20of%20Structured%20Medical%20Data&entry.906535625=Irena%20Girshovitz%20and%20Atai%20Ambus%20and%20Moni%20Shahar%20and%20Ran%20Gilad-Bachrach&entry.1292438233=%20%20Background%3A%20The%20use%20of%20Electronic%20Health%20Records%20%28EHRs%29%20for%20epidemiological%0Astudies%20and%20artificial%20intelligence%20%28AI%29%20training%20is%20increasing%20rapidly.%20The%0Areliability%20of%20the%20results%20depends%20on%20the%20accuracy%20and%20completeness%20of%20EHR%0Adata.%20However%2C%20EHR%20data%20often%20contain%20significant%20quality%20issues%2C%20including%0Amisrepresentations%20of%20subpopulations%2C%20biases%2C%20and%20systematic%20errors%2C%20as%20they%0Aare%20primarily%20collected%20for%20clinical%20and%20billing%20purposes.%20Existing%20quality%0Aassessment%20methods%20remain%20insufficient%2C%20lacking%20systematic%20procedures%20to%20assess%0Adata%20fitness%20for%20research.%0A%20%20Methods%3A%20We%20present%20the%20Medical%20Data%20Pecking%20approach%2C%20which%20adapts%20unit%0Atesting%20and%20coverage%20concepts%20from%20software%20engineering%20to%20identify%20data%0Aquality%20concerns.%20We%20demonstrate%20our%20approach%20using%20the%20Medical%20Data%20Pecking%0ATool%20%28MDPT%29%2C%20which%20consists%20of%20two%20main%20components%3A%20%281%29%20an%20automated%20test%0Agenerator%20that%20uses%20large%20language%20models%20and%20grounding%20techniques%20to%20create%20a%0Atest%20suite%20from%20data%20and%20study%20descriptions%2C%20and%20%282%29%20a%20data%20testing%20framework%0Athat%20executes%20these%20tests%2C%20reporting%20potential%20errors%20and%20coverage.%0A%20%20Results%3A%20We%20evaluated%20MDPT%20on%20three%20datasets%3A%20All%20of%20Us%20%28AoU%29%2C%20MIMIC-III%2C%20and%0ASyntheticMass%2C%20generating%2055-73%20tests%20per%20cohort%20across%20four%20conditions.%20These%0Atests%20correctly%20identified%2020-43%20non-aligned%20or%20non-conforming%20data%20issues.%20We%0Apresent%20a%20detailed%20analysis%20of%20the%20LLM-generated%20test%20suites%20in%20terms%20of%0Areference%20grounding%20and%20value%20accuracy.%0A%20%20Conclusion%3A%20Our%20approach%20incorporates%20external%20medical%20knowledge%20to%20enable%0Acontext-sensitive%20data%20quality%20testing%20as%20part%20of%20the%20data%20analysis%20workflow%20to%0Aimprove%20the%20validity%20of%20its%20outcomes.%20Our%20approach%20tackles%20these%20challenges%0Afrom%20a%20quality%20assurance%20perspective%2C%20laying%20the%20foundation%20for%20further%0Adevelopment%20such%20as%20additional%20data%20modalities%20and%20improved%20grounding%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02628v1&entry.124074799=Read"},
{"title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks", "author": "Sizhe Chen and Arman Zharmagambetov and David Wagner and Chuan Guo", "abstract": "  Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense.\n", "link": "http://arxiv.org/abs/2507.02735v1", "date": "2025-07-03", "relevancy": 1.8827, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%20SecAlign%3A%20A%20Secure%20Foundation%20LLM%20Against%20Prompt%20Injection%20Attacks&body=Title%3A%20Meta%20SecAlign%3A%20A%20Secure%20Foundation%20LLM%20Against%20Prompt%20Injection%20Attacks%0AAuthor%3A%20Sizhe%20Chen%20and%20Arman%20Zharmagambetov%20and%20David%20Wagner%20and%20Chuan%20Guo%0AAbstract%3A%20%20%20Prompt%20injection%20attacks%20pose%20a%20significant%20security%20threat%20to%20LLM-integrated%0Aapplications.%20Model-level%20defenses%20have%20shown%20strong%20effectiveness%2C%20but%20are%0Acurrently%20deployed%20into%20commercial-grade%20models%20in%20a%20closed-source%20manner.%20We%0Abelieve%20open-source%20models%20are%20needed%20by%20the%20AI%20security%20community%2C%20where%0Aco-development%20of%20attacks%20and%20defenses%20through%20open%20research%20drives%20scientific%0Aprogress%20in%20mitigation%20against%20prompt%20injection%20attacks.%20To%20this%20end%2C%20we%0Adevelop%20Meta%20SecAlign%2C%20the%20first%20open-source%20and%20open-weight%20LLM%20with%20built-in%0Amodel-level%20defense%20that%20achieves%20commercial-grade%20model%20performance.%20We%0Aprovide%20complete%20details%20of%20our%20training%20recipe%2C%20which%20utilizes%20an%20improved%0Aversion%20of%20the%20SOTA%20SecAlign%20defense.%20Evaluations%20on%209%20utility%20benchmarks%20and%207%0Asecurity%20benchmarks%20show%20that%20Meta%20SecAlign%2C%20despite%20being%20trained%20on%20a%20generic%0Ainstruction-tuning%20dataset%2C%20confers%20security%20in%20unseen%20downstream%20tasks%2C%0Aincluding%20tool-calling%20and%20agentic%20web%20navigation%2C%20in%20addition%20general%0Ainstruction-following.%20Our%20best%20model%20--%20Meta-SecAlign-70B%20--%20achieves%0Astate-of-the-art%20robustness%20against%20prompt%20injection%20attacks%20and%20comparable%0Autility%20to%20closed-source%20commercial%20LLM%20with%20model-level%20defense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%2520SecAlign%253A%2520A%2520Secure%2520Foundation%2520LLM%2520Against%2520Prompt%2520Injection%2520Attacks%26entry.906535625%3DSizhe%2520Chen%2520and%2520Arman%2520Zharmagambetov%2520and%2520David%2520Wagner%2520and%2520Chuan%2520Guo%26entry.1292438233%3D%2520%2520Prompt%2520injection%2520attacks%2520pose%2520a%2520significant%2520security%2520threat%2520to%2520LLM-integrated%250Aapplications.%2520Model-level%2520defenses%2520have%2520shown%2520strong%2520effectiveness%252C%2520but%2520are%250Acurrently%2520deployed%2520into%2520commercial-grade%2520models%2520in%2520a%2520closed-source%2520manner.%2520We%250Abelieve%2520open-source%2520models%2520are%2520needed%2520by%2520the%2520AI%2520security%2520community%252C%2520where%250Aco-development%2520of%2520attacks%2520and%2520defenses%2520through%2520open%2520research%2520drives%2520scientific%250Aprogress%2520in%2520mitigation%2520against%2520prompt%2520injection%2520attacks.%2520To%2520this%2520end%252C%2520we%250Adevelop%2520Meta%2520SecAlign%252C%2520the%2520first%2520open-source%2520and%2520open-weight%2520LLM%2520with%2520built-in%250Amodel-level%2520defense%2520that%2520achieves%2520commercial-grade%2520model%2520performance.%2520We%250Aprovide%2520complete%2520details%2520of%2520our%2520training%2520recipe%252C%2520which%2520utilizes%2520an%2520improved%250Aversion%2520of%2520the%2520SOTA%2520SecAlign%2520defense.%2520Evaluations%2520on%25209%2520utility%2520benchmarks%2520and%25207%250Asecurity%2520benchmarks%2520show%2520that%2520Meta%2520SecAlign%252C%2520despite%2520being%2520trained%2520on%2520a%2520generic%250Ainstruction-tuning%2520dataset%252C%2520confers%2520security%2520in%2520unseen%2520downstream%2520tasks%252C%250Aincluding%2520tool-calling%2520and%2520agentic%2520web%2520navigation%252C%2520in%2520addition%2520general%250Ainstruction-following.%2520Our%2520best%2520model%2520--%2520Meta-SecAlign-70B%2520--%2520achieves%250Astate-of-the-art%2520robustness%2520against%2520prompt%2520injection%2520attacks%2520and%2520comparable%250Autility%2520to%2520closed-source%2520commercial%2520LLM%2520with%2520model-level%2520defense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20SecAlign%3A%20A%20Secure%20Foundation%20LLM%20Against%20Prompt%20Injection%20Attacks&entry.906535625=Sizhe%20Chen%20and%20Arman%20Zharmagambetov%20and%20David%20Wagner%20and%20Chuan%20Guo&entry.1292438233=%20%20Prompt%20injection%20attacks%20pose%20a%20significant%20security%20threat%20to%20LLM-integrated%0Aapplications.%20Model-level%20defenses%20have%20shown%20strong%20effectiveness%2C%20but%20are%0Acurrently%20deployed%20into%20commercial-grade%20models%20in%20a%20closed-source%20manner.%20We%0Abelieve%20open-source%20models%20are%20needed%20by%20the%20AI%20security%20community%2C%20where%0Aco-development%20of%20attacks%20and%20defenses%20through%20open%20research%20drives%20scientific%0Aprogress%20in%20mitigation%20against%20prompt%20injection%20attacks.%20To%20this%20end%2C%20we%0Adevelop%20Meta%20SecAlign%2C%20the%20first%20open-source%20and%20open-weight%20LLM%20with%20built-in%0Amodel-level%20defense%20that%20achieves%20commercial-grade%20model%20performance.%20We%0Aprovide%20complete%20details%20of%20our%20training%20recipe%2C%20which%20utilizes%20an%20improved%0Aversion%20of%20the%20SOTA%20SecAlign%20defense.%20Evaluations%20on%209%20utility%20benchmarks%20and%207%0Asecurity%20benchmarks%20show%20that%20Meta%20SecAlign%2C%20despite%20being%20trained%20on%20a%20generic%0Ainstruction-tuning%20dataset%2C%20confers%20security%20in%20unseen%20downstream%20tasks%2C%0Aincluding%20tool-calling%20and%20agentic%20web%20navigation%2C%20in%20addition%20general%0Ainstruction-following.%20Our%20best%20model%20--%20Meta-SecAlign-70B%20--%20achieves%0Astate-of-the-art%20robustness%20against%20prompt%20injection%20attacks%20and%20comparable%0Autility%20to%20closed-source%20commercial%20LLM%20with%20model-level%20defense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02735v1&entry.124074799=Read"},
{"title": "Scalable Interconnect Learning in Boolean Networks", "author": "Fabian Kresse and Emily Yu and Christoph H. Lampert", "abstract": "  Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off.\n", "link": "http://arxiv.org/abs/2507.02585v1", "date": "2025-07-03", "relevancy": 1.8657, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.454}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Interconnect%20Learning%20in%20Boolean%20Networks&body=Title%3A%20Scalable%20Interconnect%20Learning%20in%20Boolean%20Networks%0AAuthor%3A%20Fabian%20Kresse%20and%20Emily%20Yu%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20Learned%20Differentiable%20Boolean%20Logic%20Networks%20%28DBNs%29%20already%20deliver%0Aefficient%20inference%20on%20resource-constrained%20hardware.%20We%20extend%20them%20with%20a%0Atrainable%2C%20differentiable%20interconnect%20whose%20parameter%20count%20remains%20constant%0Aas%20input%20width%20grows%2C%20allowing%20DBNs%20to%20scale%20to%20far%20wider%20layers%20than%20earlier%0Alearnable-interconnect%20designs%20while%20preserving%20their%20advantageous%20accuracy.%20To%0Afurther%20reduce%20model%20size%2C%20we%20propose%20two%20complementary%20pruning%20stages%3A%20an%0ASAT-based%20logic%20equivalence%20pass%20that%20removes%20redundant%20gates%20without%20affecting%0Aperformance%2C%20and%20a%20similarity-based%2C%20data-driven%20pass%20that%20outperforms%20a%0Amagnitude-style%20greedy%20baseline%20and%20offers%20a%20superior%20compression-accuracy%0Atrade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Interconnect%2520Learning%2520in%2520Boolean%2520Networks%26entry.906535625%3DFabian%2520Kresse%2520and%2520Emily%2520Yu%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520Learned%2520Differentiable%2520Boolean%2520Logic%2520Networks%2520%2528DBNs%2529%2520already%2520deliver%250Aefficient%2520inference%2520on%2520resource-constrained%2520hardware.%2520We%2520extend%2520them%2520with%2520a%250Atrainable%252C%2520differentiable%2520interconnect%2520whose%2520parameter%2520count%2520remains%2520constant%250Aas%2520input%2520width%2520grows%252C%2520allowing%2520DBNs%2520to%2520scale%2520to%2520far%2520wider%2520layers%2520than%2520earlier%250Alearnable-interconnect%2520designs%2520while%2520preserving%2520their%2520advantageous%2520accuracy.%2520To%250Afurther%2520reduce%2520model%2520size%252C%2520we%2520propose%2520two%2520complementary%2520pruning%2520stages%253A%2520an%250ASAT-based%2520logic%2520equivalence%2520pass%2520that%2520removes%2520redundant%2520gates%2520without%2520affecting%250Aperformance%252C%2520and%2520a%2520similarity-based%252C%2520data-driven%2520pass%2520that%2520outperforms%2520a%250Amagnitude-style%2520greedy%2520baseline%2520and%2520offers%2520a%2520superior%2520compression-accuracy%250Atrade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Interconnect%20Learning%20in%20Boolean%20Networks&entry.906535625=Fabian%20Kresse%20and%20Emily%20Yu%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20Learned%20Differentiable%20Boolean%20Logic%20Networks%20%28DBNs%29%20already%20deliver%0Aefficient%20inference%20on%20resource-constrained%20hardware.%20We%20extend%20them%20with%20a%0Atrainable%2C%20differentiable%20interconnect%20whose%20parameter%20count%20remains%20constant%0Aas%20input%20width%20grows%2C%20allowing%20DBNs%20to%20scale%20to%20far%20wider%20layers%20than%20earlier%0Alearnable-interconnect%20designs%20while%20preserving%20their%20advantageous%20accuracy.%20To%0Afurther%20reduce%20model%20size%2C%20we%20propose%20two%20complementary%20pruning%20stages%3A%20an%0ASAT-based%20logic%20equivalence%20pass%20that%20removes%20redundant%20gates%20without%20affecting%0Aperformance%2C%20and%20a%20similarity-based%2C%20data-driven%20pass%20that%20outperforms%20a%0Amagnitude-style%20greedy%20baseline%20and%20offers%20a%20superior%20compression-accuracy%0Atrade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02585v1&entry.124074799=Read"},
{"title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile\n  Manipulation", "author": "Sixiang Chen and Jiaming Liu and Siyuan Qian and Han Jiang and Lily Li and Renrui Zhang and Zhuoyang Liu and Chenyang Gu and Chengkai Hou and Pengwei Wang and Zhongyuan Wang and Shanghang Zhang", "abstract": "  Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.\n", "link": "http://arxiv.org/abs/2507.01961v2", "date": "2025-07-03", "relevancy": 1.8647, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6356}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6154}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC-DiT%3A%20Adaptive%20Coordination%20Diffusion%20Transformer%20for%20Mobile%0A%20%20Manipulation&body=Title%3A%20AC-DiT%3A%20Adaptive%20Coordination%20Diffusion%20Transformer%20for%20Mobile%0A%20%20Manipulation%0AAuthor%3A%20Sixiang%20Chen%20and%20Jiaming%20Liu%20and%20Siyuan%20Qian%20and%20Han%20Jiang%20and%20Lily%20Li%20and%20Renrui%20Zhang%20and%20Zhuoyang%20Liu%20and%20Chenyang%20Gu%20and%20Chengkai%20Hou%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20mobile%20manipulation%20has%20attracted%20increasing%20attention%20for%20enabling%0Alanguage-conditioned%20robotic%20control%20in%20household%20tasks.%20However%2C%20existing%0Amethods%20still%20face%20challenges%20in%20coordinating%20mobile%20base%20and%20manipulator%2C%0Aprimarily%20due%20to%20two%20limitations.%20On%20the%20one%20hand%2C%20they%20fail%20to%20explicitly%0Amodel%20the%20influence%20of%20the%20mobile%20base%20on%20manipulator%20control%2C%20which%20easily%0Aleads%20to%20error%20accumulation%20under%20high%20degrees%20of%20freedom.%20On%20the%20other%20hand%2C%0Athey%20treat%20the%20entire%20mobile%20manipulation%20process%20with%20the%20same%20visual%0Aobservation%20modality%20%28e.g.%2C%20either%20all%202D%20or%20all%203D%29%2C%20overlooking%20the%20distinct%0Amultimodal%20perception%20requirements%20at%20different%20stages%20during%20mobile%0Amanipulation.%20To%20address%20this%2C%20we%20propose%20the%20Adaptive%20Coordination%20Diffusion%0ATransformer%20%28AC-DiT%29%2C%20which%20enhances%20mobile%20base%20and%20manipulator%20coordination%0Afor%20end-to-end%20mobile%20manipulation.%20First%2C%20since%20the%20motion%20of%20the%20mobile%20base%0Adirectly%20influences%20the%20manipulator%27s%20actions%2C%20we%20introduce%20a%20mobility-to-body%0Aconditioning%20mechanism%20that%20guides%20the%20model%20to%20first%20extract%20base%20motion%0Arepresentations%2C%20which%20are%20then%20used%20as%20context%20prior%20for%20predicting%20whole-body%0Aactions.%20This%20enables%20whole-body%20control%20that%20accounts%20for%20the%20potential%20impact%0Aof%20the%20mobile%20base%27s%20motion.%20Second%2C%20to%20meet%20the%20perception%20requirements%20at%0Adifferent%20stages%20of%20mobile%20manipulation%2C%20we%20design%20a%20perception-aware%0Amultimodal%20conditioning%20strategy%20that%20dynamically%20adjusts%20the%20fusion%20weights%0Abetween%20various%202D%20visual%20images%20and%203D%20point%20clouds%2C%20yielding%20visual%20features%0Atailored%20to%20the%20current%20perceptual%20needs.%20This%20allows%20the%20model%20to%2C%20for%0Aexample%2C%20adaptively%20rely%20more%20on%202D%20inputs%20when%20semantic%20information%20is%20crucial%0Afor%20action%20prediction%2C%20while%20placing%20greater%20emphasis%20on%203D%20geometric%0Ainformation%20when%20precise%20spatial%20understanding%20is%20required.%20We%20validate%20AC-DiT%0Athrough%20extensive%20experiments%20on%20both%20simulated%20and%20real-world%20mobile%0Amanipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC-DiT%253A%2520Adaptive%2520Coordination%2520Diffusion%2520Transformer%2520for%2520Mobile%250A%2520%2520Manipulation%26entry.906535625%3DSixiang%2520Chen%2520and%2520Jiaming%2520Liu%2520and%2520Siyuan%2520Qian%2520and%2520Han%2520Jiang%2520and%2520Lily%2520Li%2520and%2520Renrui%2520Zhang%2520and%2520Zhuoyang%2520Liu%2520and%2520Chenyang%2520Gu%2520and%2520Chengkai%2520Hou%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520mobile%2520manipulation%2520has%2520attracted%2520increasing%2520attention%2520for%2520enabling%250Alanguage-conditioned%2520robotic%2520control%2520in%2520household%2520tasks.%2520However%252C%2520existing%250Amethods%2520still%2520face%2520challenges%2520in%2520coordinating%2520mobile%2520base%2520and%2520manipulator%252C%250Aprimarily%2520due%2520to%2520two%2520limitations.%2520On%2520the%2520one%2520hand%252C%2520they%2520fail%2520to%2520explicitly%250Amodel%2520the%2520influence%2520of%2520the%2520mobile%2520base%2520on%2520manipulator%2520control%252C%2520which%2520easily%250Aleads%2520to%2520error%2520accumulation%2520under%2520high%2520degrees%2520of%2520freedom.%2520On%2520the%2520other%2520hand%252C%250Athey%2520treat%2520the%2520entire%2520mobile%2520manipulation%2520process%2520with%2520the%2520same%2520visual%250Aobservation%2520modality%2520%2528e.g.%252C%2520either%2520all%25202D%2520or%2520all%25203D%2529%252C%2520overlooking%2520the%2520distinct%250Amultimodal%2520perception%2520requirements%2520at%2520different%2520stages%2520during%2520mobile%250Amanipulation.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Adaptive%2520Coordination%2520Diffusion%250ATransformer%2520%2528AC-DiT%2529%252C%2520which%2520enhances%2520mobile%2520base%2520and%2520manipulator%2520coordination%250Afor%2520end-to-end%2520mobile%2520manipulation.%2520First%252C%2520since%2520the%2520motion%2520of%2520the%2520mobile%2520base%250Adirectly%2520influences%2520the%2520manipulator%2527s%2520actions%252C%2520we%2520introduce%2520a%2520mobility-to-body%250Aconditioning%2520mechanism%2520that%2520guides%2520the%2520model%2520to%2520first%2520extract%2520base%2520motion%250Arepresentations%252C%2520which%2520are%2520then%2520used%2520as%2520context%2520prior%2520for%2520predicting%2520whole-body%250Aactions.%2520This%2520enables%2520whole-body%2520control%2520that%2520accounts%2520for%2520the%2520potential%2520impact%250Aof%2520the%2520mobile%2520base%2527s%2520motion.%2520Second%252C%2520to%2520meet%2520the%2520perception%2520requirements%2520at%250Adifferent%2520stages%2520of%2520mobile%2520manipulation%252C%2520we%2520design%2520a%2520perception-aware%250Amultimodal%2520conditioning%2520strategy%2520that%2520dynamically%2520adjusts%2520the%2520fusion%2520weights%250Abetween%2520various%25202D%2520visual%2520images%2520and%25203D%2520point%2520clouds%252C%2520yielding%2520visual%2520features%250Atailored%2520to%2520the%2520current%2520perceptual%2520needs.%2520This%2520allows%2520the%2520model%2520to%252C%2520for%250Aexample%252C%2520adaptively%2520rely%2520more%2520on%25202D%2520inputs%2520when%2520semantic%2520information%2520is%2520crucial%250Afor%2520action%2520prediction%252C%2520while%2520placing%2520greater%2520emphasis%2520on%25203D%2520geometric%250Ainformation%2520when%2520precise%2520spatial%2520understanding%2520is%2520required.%2520We%2520validate%2520AC-DiT%250Athrough%2520extensive%2520experiments%2520on%2520both%2520simulated%2520and%2520real-world%2520mobile%250Amanipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC-DiT%3A%20Adaptive%20Coordination%20Diffusion%20Transformer%20for%20Mobile%0A%20%20Manipulation&entry.906535625=Sixiang%20Chen%20and%20Jiaming%20Liu%20and%20Siyuan%20Qian%20and%20Han%20Jiang%20and%20Lily%20Li%20and%20Renrui%20Zhang%20and%20Zhuoyang%20Liu%20and%20Chenyang%20Gu%20and%20Chengkai%20Hou%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recently%2C%20mobile%20manipulation%20has%20attracted%20increasing%20attention%20for%20enabling%0Alanguage-conditioned%20robotic%20control%20in%20household%20tasks.%20However%2C%20existing%0Amethods%20still%20face%20challenges%20in%20coordinating%20mobile%20base%20and%20manipulator%2C%0Aprimarily%20due%20to%20two%20limitations.%20On%20the%20one%20hand%2C%20they%20fail%20to%20explicitly%0Amodel%20the%20influence%20of%20the%20mobile%20base%20on%20manipulator%20control%2C%20which%20easily%0Aleads%20to%20error%20accumulation%20under%20high%20degrees%20of%20freedom.%20On%20the%20other%20hand%2C%0Athey%20treat%20the%20entire%20mobile%20manipulation%20process%20with%20the%20same%20visual%0Aobservation%20modality%20%28e.g.%2C%20either%20all%202D%20or%20all%203D%29%2C%20overlooking%20the%20distinct%0Amultimodal%20perception%20requirements%20at%20different%20stages%20during%20mobile%0Amanipulation.%20To%20address%20this%2C%20we%20propose%20the%20Adaptive%20Coordination%20Diffusion%0ATransformer%20%28AC-DiT%29%2C%20which%20enhances%20mobile%20base%20and%20manipulator%20coordination%0Afor%20end-to-end%20mobile%20manipulation.%20First%2C%20since%20the%20motion%20of%20the%20mobile%20base%0Adirectly%20influences%20the%20manipulator%27s%20actions%2C%20we%20introduce%20a%20mobility-to-body%0Aconditioning%20mechanism%20that%20guides%20the%20model%20to%20first%20extract%20base%20motion%0Arepresentations%2C%20which%20are%20then%20used%20as%20context%20prior%20for%20predicting%20whole-body%0Aactions.%20This%20enables%20whole-body%20control%20that%20accounts%20for%20the%20potential%20impact%0Aof%20the%20mobile%20base%27s%20motion.%20Second%2C%20to%20meet%20the%20perception%20requirements%20at%0Adifferent%20stages%20of%20mobile%20manipulation%2C%20we%20design%20a%20perception-aware%0Amultimodal%20conditioning%20strategy%20that%20dynamically%20adjusts%20the%20fusion%20weights%0Abetween%20various%202D%20visual%20images%20and%203D%20point%20clouds%2C%20yielding%20visual%20features%0Atailored%20to%20the%20current%20perceptual%20needs.%20This%20allows%20the%20model%20to%2C%20for%0Aexample%2C%20adaptively%20rely%20more%20on%202D%20inputs%20when%20semantic%20information%20is%20crucial%0Afor%20action%20prediction%2C%20while%20placing%20greater%20emphasis%20on%203D%20geometric%0Ainformation%20when%20precise%20spatial%20understanding%20is%20required.%20We%20validate%20AC-DiT%0Athrough%20extensive%20experiments%20on%20both%20simulated%20and%20real-world%20mobile%0Amanipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01961v2&entry.124074799=Read"},
{"title": "Enabling Population-Level Parallelism in Tree-Based Genetic Programming\n  for Comprehensive GPU Acceleration", "author": "Zhihong Wu and Lishuang Wang and Kebin Sun and Zhuozhao Li and Ran Cheng", "abstract": "  Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm\nfor tasks such as symbolic regression, classification, and robotic control. Due\nto the intensive computational demands of running TGP, GPU acceleration is\ncrucial for achieving scalable performance. However, efficient GPU-based\nexecution of TGP still remains challenging, primarily due to three core issues:\n(1) the structural heterogeneity of program individuals, (2) the complexity of\nintegrating multiple levels of parallelism, and (3) the incompatibility between\nhigh-performance CUDA execution and flexible Python-based environments. To\naddress these issues, we propose EvoGP, a high-performance framework tailored\nfor comprehensive GPU acceleration of TGP via population-level parallel\nexecution. First, EvoGP introduces a tensorized representation that encodes\nvariable-sized trees into fixed-shape, memory-aligned arrays, enabling uniform\nmemory access and parallel computation across diverse individuals. Second,\nEvoGP adopts an adaptive parallelism strategy that dynamically combines intra-\nand inter-individual parallelism based on dataset size, ensuring high GPU\nutilization across a broad spectrum of tasks. Third, EvoGP embeds custom CUDA\nkernels into the PyTorch runtime, achieving seamless integration with\nPython-based environments such as Gym, MuJoCo, Brax, and Genesis. Comprehensive\nexperiments show that EvoGP achieves up to 140x speedup over state-of-the-art\nGPU-based TGP implementations, while maintaining competitive accuracy and\nsignificantly improving scalability under large population sizes. EvoGP is open\nsource and accessible at: https://github.com/EMI-Group/evogp.\n", "link": "http://arxiv.org/abs/2501.17168v4", "date": "2025-07-03", "relevancy": 1.8528, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4684}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4651}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%0A%20%20for%20Comprehensive%20GPU%20Acceleration&body=Title%3A%20Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%0A%20%20for%20Comprehensive%20GPU%20Acceleration%0AAuthor%3A%20Zhihong%20Wu%20and%20Lishuang%20Wang%20and%20Kebin%20Sun%20and%20Zhuozhao%20Li%20and%20Ran%20Cheng%0AAbstract%3A%20%20%20Tree-based%20Genetic%20Programming%20%28TGP%29%20is%20a%20widely%20used%20evolutionary%20algorithm%0Afor%20tasks%20such%20as%20symbolic%20regression%2C%20classification%2C%20and%20robotic%20control.%20Due%0Ato%20the%20intensive%20computational%20demands%20of%20running%20TGP%2C%20GPU%20acceleration%20is%0Acrucial%20for%20achieving%20scalable%20performance.%20However%2C%20efficient%20GPU-based%0Aexecution%20of%20TGP%20still%20remains%20challenging%2C%20primarily%20due%20to%20three%20core%20issues%3A%0A%281%29%20the%20structural%20heterogeneity%20of%20program%20individuals%2C%20%282%29%20the%20complexity%20of%0Aintegrating%20multiple%20levels%20of%20parallelism%2C%20and%20%283%29%20the%20incompatibility%20between%0Ahigh-performance%20CUDA%20execution%20and%20flexible%20Python-based%20environments.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20EvoGP%2C%20a%20high-performance%20framework%20tailored%0Afor%20comprehensive%20GPU%20acceleration%20of%20TGP%20via%20population-level%20parallel%0Aexecution.%20First%2C%20EvoGP%20introduces%20a%20tensorized%20representation%20that%20encodes%0Avariable-sized%20trees%20into%20fixed-shape%2C%20memory-aligned%20arrays%2C%20enabling%20uniform%0Amemory%20access%20and%20parallel%20computation%20across%20diverse%20individuals.%20Second%2C%0AEvoGP%20adopts%20an%20adaptive%20parallelism%20strategy%20that%20dynamically%20combines%20intra-%0Aand%20inter-individual%20parallelism%20based%20on%20dataset%20size%2C%20ensuring%20high%20GPU%0Autilization%20across%20a%20broad%20spectrum%20of%20tasks.%20Third%2C%20EvoGP%20embeds%20custom%20CUDA%0Akernels%20into%20the%20PyTorch%20runtime%2C%20achieving%20seamless%20integration%20with%0APython-based%20environments%20such%20as%20Gym%2C%20MuJoCo%2C%20Brax%2C%20and%20Genesis.%20Comprehensive%0Aexperiments%20show%20that%20EvoGP%20achieves%20up%20to%20140x%20speedup%20over%20state-of-the-art%0AGPU-based%20TGP%20implementations%2C%20while%20maintaining%20competitive%20accuracy%20and%0Asignificantly%20improving%20scalability%20under%20large%20population%20sizes.%20EvoGP%20is%20open%0Asource%20and%20accessible%20at%3A%20https%3A//github.com/EMI-Group/evogp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17168v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Population-Level%2520Parallelism%2520in%2520Tree-Based%2520Genetic%2520Programming%250A%2520%2520for%2520Comprehensive%2520GPU%2520Acceleration%26entry.906535625%3DZhihong%2520Wu%2520and%2520Lishuang%2520Wang%2520and%2520Kebin%2520Sun%2520and%2520Zhuozhao%2520Li%2520and%2520Ran%2520Cheng%26entry.1292438233%3D%2520%2520Tree-based%2520Genetic%2520Programming%2520%2528TGP%2529%2520is%2520a%2520widely%2520used%2520evolutionary%2520algorithm%250Afor%2520tasks%2520such%2520as%2520symbolic%2520regression%252C%2520classification%252C%2520and%2520robotic%2520control.%2520Due%250Ato%2520the%2520intensive%2520computational%2520demands%2520of%2520running%2520TGP%252C%2520GPU%2520acceleration%2520is%250Acrucial%2520for%2520achieving%2520scalable%2520performance.%2520However%252C%2520efficient%2520GPU-based%250Aexecution%2520of%2520TGP%2520still%2520remains%2520challenging%252C%2520primarily%2520due%2520to%2520three%2520core%2520issues%253A%250A%25281%2529%2520the%2520structural%2520heterogeneity%2520of%2520program%2520individuals%252C%2520%25282%2529%2520the%2520complexity%2520of%250Aintegrating%2520multiple%2520levels%2520of%2520parallelism%252C%2520and%2520%25283%2529%2520the%2520incompatibility%2520between%250Ahigh-performance%2520CUDA%2520execution%2520and%2520flexible%2520Python-based%2520environments.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520EvoGP%252C%2520a%2520high-performance%2520framework%2520tailored%250Afor%2520comprehensive%2520GPU%2520acceleration%2520of%2520TGP%2520via%2520population-level%2520parallel%250Aexecution.%2520First%252C%2520EvoGP%2520introduces%2520a%2520tensorized%2520representation%2520that%2520encodes%250Avariable-sized%2520trees%2520into%2520fixed-shape%252C%2520memory-aligned%2520arrays%252C%2520enabling%2520uniform%250Amemory%2520access%2520and%2520parallel%2520computation%2520across%2520diverse%2520individuals.%2520Second%252C%250AEvoGP%2520adopts%2520an%2520adaptive%2520parallelism%2520strategy%2520that%2520dynamically%2520combines%2520intra-%250Aand%2520inter-individual%2520parallelism%2520based%2520on%2520dataset%2520size%252C%2520ensuring%2520high%2520GPU%250Autilization%2520across%2520a%2520broad%2520spectrum%2520of%2520tasks.%2520Third%252C%2520EvoGP%2520embeds%2520custom%2520CUDA%250Akernels%2520into%2520the%2520PyTorch%2520runtime%252C%2520achieving%2520seamless%2520integration%2520with%250APython-based%2520environments%2520such%2520as%2520Gym%252C%2520MuJoCo%252C%2520Brax%252C%2520and%2520Genesis.%2520Comprehensive%250Aexperiments%2520show%2520that%2520EvoGP%2520achieves%2520up%2520to%2520140x%2520speedup%2520over%2520state-of-the-art%250AGPU-based%2520TGP%2520implementations%252C%2520while%2520maintaining%2520competitive%2520accuracy%2520and%250Asignificantly%2520improving%2520scalability%2520under%2520large%2520population%2520sizes.%2520EvoGP%2520is%2520open%250Asource%2520and%2520accessible%2520at%253A%2520https%253A//github.com/EMI-Group/evogp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17168v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Population-Level%20Parallelism%20in%20Tree-Based%20Genetic%20Programming%0A%20%20for%20Comprehensive%20GPU%20Acceleration&entry.906535625=Zhihong%20Wu%20and%20Lishuang%20Wang%20and%20Kebin%20Sun%20and%20Zhuozhao%20Li%20and%20Ran%20Cheng&entry.1292438233=%20%20Tree-based%20Genetic%20Programming%20%28TGP%29%20is%20a%20widely%20used%20evolutionary%20algorithm%0Afor%20tasks%20such%20as%20symbolic%20regression%2C%20classification%2C%20and%20robotic%20control.%20Due%0Ato%20the%20intensive%20computational%20demands%20of%20running%20TGP%2C%20GPU%20acceleration%20is%0Acrucial%20for%20achieving%20scalable%20performance.%20However%2C%20efficient%20GPU-based%0Aexecution%20of%20TGP%20still%20remains%20challenging%2C%20primarily%20due%20to%20three%20core%20issues%3A%0A%281%29%20the%20structural%20heterogeneity%20of%20program%20individuals%2C%20%282%29%20the%20complexity%20of%0Aintegrating%20multiple%20levels%20of%20parallelism%2C%20and%20%283%29%20the%20incompatibility%20between%0Ahigh-performance%20CUDA%20execution%20and%20flexible%20Python-based%20environments.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20EvoGP%2C%20a%20high-performance%20framework%20tailored%0Afor%20comprehensive%20GPU%20acceleration%20of%20TGP%20via%20population-level%20parallel%0Aexecution.%20First%2C%20EvoGP%20introduces%20a%20tensorized%20representation%20that%20encodes%0Avariable-sized%20trees%20into%20fixed-shape%2C%20memory-aligned%20arrays%2C%20enabling%20uniform%0Amemory%20access%20and%20parallel%20computation%20across%20diverse%20individuals.%20Second%2C%0AEvoGP%20adopts%20an%20adaptive%20parallelism%20strategy%20that%20dynamically%20combines%20intra-%0Aand%20inter-individual%20parallelism%20based%20on%20dataset%20size%2C%20ensuring%20high%20GPU%0Autilization%20across%20a%20broad%20spectrum%20of%20tasks.%20Third%2C%20EvoGP%20embeds%20custom%20CUDA%0Akernels%20into%20the%20PyTorch%20runtime%2C%20achieving%20seamless%20integration%20with%0APython-based%20environments%20such%20as%20Gym%2C%20MuJoCo%2C%20Brax%2C%20and%20Genesis.%20Comprehensive%0Aexperiments%20show%20that%20EvoGP%20achieves%20up%20to%20140x%20speedup%20over%20state-of-the-art%0AGPU-based%20TGP%20implementations%2C%20while%20maintaining%20competitive%20accuracy%20and%0Asignificantly%20improving%20scalability%20under%20large%20population%20sizes.%20EvoGP%20is%20open%0Asource%20and%20accessible%20at%3A%20https%3A//github.com/EMI-Group/evogp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17168v4&entry.124074799=Read"},
{"title": "A Comprehensive Machine Learning Framework for Micromobility Demand\n  Prediction", "author": "Omri Porat and Michael Fire and Eran Ben-Elia", "abstract": "  Dockless e-scooters, a key micromobility service, have emerged as\neco-friendly and flexible urban transport alternatives. These services improve\nfirst and last-mile connectivity, reduce congestion and emissions, and\ncomplement public transport for short-distance travel. However, effective\nmanagement of these services depends on accurate demand prediction, which is\ncrucial for optimal fleet distribution and infrastructure planning. While\nprevious studies have focused on analyzing spatial or temporal factors in\nisolation, this study introduces a framework that integrates spatial, temporal,\nand network dependencies for improved micromobility demand forecasting. This\nintegration enhances accuracy while providing deeper insights into urban\nmicromobility usage patterns. Our framework improves demand prediction accuracy\nby 27 to 49% over baseline models, demonstrating its effectiveness in capturing\nmicromobility demand patterns. These findings support data-driven micromobility\nmanagement, enabling optimized fleet distribution, cost reduction, and\nsustainable urban planning.\n", "link": "http://arxiv.org/abs/2507.02715v1", "date": "2025-07-03", "relevancy": 1.8519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.465}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Machine%20Learning%20Framework%20for%20Micromobility%20Demand%0A%20%20Prediction&body=Title%3A%20A%20Comprehensive%20Machine%20Learning%20Framework%20for%20Micromobility%20Demand%0A%20%20Prediction%0AAuthor%3A%20Omri%20Porat%20and%20Michael%20Fire%20and%20Eran%20Ben-Elia%0AAbstract%3A%20%20%20Dockless%20e-scooters%2C%20a%20key%20micromobility%20service%2C%20have%20emerged%20as%0Aeco-friendly%20and%20flexible%20urban%20transport%20alternatives.%20These%20services%20improve%0Afirst%20and%20last-mile%20connectivity%2C%20reduce%20congestion%20and%20emissions%2C%20and%0Acomplement%20public%20transport%20for%20short-distance%20travel.%20However%2C%20effective%0Amanagement%20of%20these%20services%20depends%20on%20accurate%20demand%20prediction%2C%20which%20is%0Acrucial%20for%20optimal%20fleet%20distribution%20and%20infrastructure%20planning.%20While%0Aprevious%20studies%20have%20focused%20on%20analyzing%20spatial%20or%20temporal%20factors%20in%0Aisolation%2C%20this%20study%20introduces%20a%20framework%20that%20integrates%20spatial%2C%20temporal%2C%0Aand%20network%20dependencies%20for%20improved%20micromobility%20demand%20forecasting.%20This%0Aintegration%20enhances%20accuracy%20while%20providing%20deeper%20insights%20into%20urban%0Amicromobility%20usage%20patterns.%20Our%20framework%20improves%20demand%20prediction%20accuracy%0Aby%2027%20to%2049%25%20over%20baseline%20models%2C%20demonstrating%20its%20effectiveness%20in%20capturing%0Amicromobility%20demand%20patterns.%20These%20findings%20support%20data-driven%20micromobility%0Amanagement%2C%20enabling%20optimized%20fleet%20distribution%2C%20cost%20reduction%2C%20and%0Asustainable%20urban%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Machine%2520Learning%2520Framework%2520for%2520Micromobility%2520Demand%250A%2520%2520Prediction%26entry.906535625%3DOmri%2520Porat%2520and%2520Michael%2520Fire%2520and%2520Eran%2520Ben-Elia%26entry.1292438233%3D%2520%2520Dockless%2520e-scooters%252C%2520a%2520key%2520micromobility%2520service%252C%2520have%2520emerged%2520as%250Aeco-friendly%2520and%2520flexible%2520urban%2520transport%2520alternatives.%2520These%2520services%2520improve%250Afirst%2520and%2520last-mile%2520connectivity%252C%2520reduce%2520congestion%2520and%2520emissions%252C%2520and%250Acomplement%2520public%2520transport%2520for%2520short-distance%2520travel.%2520However%252C%2520effective%250Amanagement%2520of%2520these%2520services%2520depends%2520on%2520accurate%2520demand%2520prediction%252C%2520which%2520is%250Acrucial%2520for%2520optimal%2520fleet%2520distribution%2520and%2520infrastructure%2520planning.%2520While%250Aprevious%2520studies%2520have%2520focused%2520on%2520analyzing%2520spatial%2520or%2520temporal%2520factors%2520in%250Aisolation%252C%2520this%2520study%2520introduces%2520a%2520framework%2520that%2520integrates%2520spatial%252C%2520temporal%252C%250Aand%2520network%2520dependencies%2520for%2520improved%2520micromobility%2520demand%2520forecasting.%2520This%250Aintegration%2520enhances%2520accuracy%2520while%2520providing%2520deeper%2520insights%2520into%2520urban%250Amicromobility%2520usage%2520patterns.%2520Our%2520framework%2520improves%2520demand%2520prediction%2520accuracy%250Aby%252027%2520to%252049%2525%2520over%2520baseline%2520models%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520capturing%250Amicromobility%2520demand%2520patterns.%2520These%2520findings%2520support%2520data-driven%2520micromobility%250Amanagement%252C%2520enabling%2520optimized%2520fleet%2520distribution%252C%2520cost%2520reduction%252C%2520and%250Asustainable%2520urban%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Machine%20Learning%20Framework%20for%20Micromobility%20Demand%0A%20%20Prediction&entry.906535625=Omri%20Porat%20and%20Michael%20Fire%20and%20Eran%20Ben-Elia&entry.1292438233=%20%20Dockless%20e-scooters%2C%20a%20key%20micromobility%20service%2C%20have%20emerged%20as%0Aeco-friendly%20and%20flexible%20urban%20transport%20alternatives.%20These%20services%20improve%0Afirst%20and%20last-mile%20connectivity%2C%20reduce%20congestion%20and%20emissions%2C%20and%0Acomplement%20public%20transport%20for%20short-distance%20travel.%20However%2C%20effective%0Amanagement%20of%20these%20services%20depends%20on%20accurate%20demand%20prediction%2C%20which%20is%0Acrucial%20for%20optimal%20fleet%20distribution%20and%20infrastructure%20planning.%20While%0Aprevious%20studies%20have%20focused%20on%20analyzing%20spatial%20or%20temporal%20factors%20in%0Aisolation%2C%20this%20study%20introduces%20a%20framework%20that%20integrates%20spatial%2C%20temporal%2C%0Aand%20network%20dependencies%20for%20improved%20micromobility%20demand%20forecasting.%20This%0Aintegration%20enhances%20accuracy%20while%20providing%20deeper%20insights%20into%20urban%0Amicromobility%20usage%20patterns.%20Our%20framework%20improves%20demand%20prediction%20accuracy%0Aby%2027%20to%2049%25%20over%20baseline%20models%2C%20demonstrating%20its%20effectiveness%20in%20capturing%0Amicromobility%20demand%20patterns.%20These%20findings%20support%20data-driven%20micromobility%0Amanagement%2C%20enabling%20optimized%20fleet%20distribution%2C%20cost%20reduction%2C%20and%0Asustainable%20urban%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02715v1&entry.124074799=Read"},
{"title": "Guided Generation for Developable Antibodies", "author": "Siqi Zhao and Joshua Moller and Porfi Quintero-Cadena and Lood van Niekerk", "abstract": "  Therapeutic antibodies require not only high-affinity target engagement, but\nalso favorable manufacturability, stability, and safety profiles for clinical\neffectiveness. These properties are collectively called `developability'. To\nenable a computational framework for optimizing antibody sequences for\nfavorable developability, we introduce a guided discrete diffusion model\ntrained on natural paired heavy- and light-chain sequences from the Observed\nAntibody Space (OAS) and quantitative developability measurements for 246\nclinical-stage antibodies. To steer generation toward biophysically viable\ncandidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module\nthat biases sampling without compromising naturalness. In unconstrained\nsampling, our model reproduces global features of both the natural repertoire\nand approved therapeutics, and under SVDD guidance we achieve significant\nenrichment in predicted developability scores over unguided baselines. When\ncombined with high-throughput developability assays, this framework enables an\niterative, ML-driven pipeline for designing antibodies that satisfy binding and\nbiophysical criteria in tandem.\n", "link": "http://arxiv.org/abs/2507.02670v1", "date": "2025-07-03", "relevancy": 1.8517, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4751}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4562}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Generation%20for%20Developable%20Antibodies&body=Title%3A%20Guided%20Generation%20for%20Developable%20Antibodies%0AAuthor%3A%20Siqi%20Zhao%20and%20Joshua%20Moller%20and%20Porfi%20Quintero-Cadena%20and%20Lood%20van%20Niekerk%0AAbstract%3A%20%20%20Therapeutic%20antibodies%20require%20not%20only%20high-affinity%20target%20engagement%2C%20but%0Aalso%20favorable%20manufacturability%2C%20stability%2C%20and%20safety%20profiles%20for%20clinical%0Aeffectiveness.%20These%20properties%20are%20collectively%20called%20%60developability%27.%20To%0Aenable%20a%20computational%20framework%20for%20optimizing%20antibody%20sequences%20for%0Afavorable%20developability%2C%20we%20introduce%20a%20guided%20discrete%20diffusion%20model%0Atrained%20on%20natural%20paired%20heavy-%20and%20light-chain%20sequences%20from%20the%20Observed%0AAntibody%20Space%20%28OAS%29%20and%20quantitative%20developability%20measurements%20for%20246%0Aclinical-stage%20antibodies.%20To%20steer%20generation%20toward%20biophysically%20viable%0Acandidates%2C%20we%20integrate%20a%20Soft%20Value-based%20Decoding%20in%20Diffusion%20%28SVDD%29%20Module%0Athat%20biases%20sampling%20without%20compromising%20naturalness.%20In%20unconstrained%0Asampling%2C%20our%20model%20reproduces%20global%20features%20of%20both%20the%20natural%20repertoire%0Aand%20approved%20therapeutics%2C%20and%20under%20SVDD%20guidance%20we%20achieve%20significant%0Aenrichment%20in%20predicted%20developability%20scores%20over%20unguided%20baselines.%20When%0Acombined%20with%20high-throughput%20developability%20assays%2C%20this%20framework%20enables%20an%0Aiterative%2C%20ML-driven%20pipeline%20for%20designing%20antibodies%20that%20satisfy%20binding%20and%0Abiophysical%20criteria%20in%20tandem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Generation%2520for%2520Developable%2520Antibodies%26entry.906535625%3DSiqi%2520Zhao%2520and%2520Joshua%2520Moller%2520and%2520Porfi%2520Quintero-Cadena%2520and%2520Lood%2520van%2520Niekerk%26entry.1292438233%3D%2520%2520Therapeutic%2520antibodies%2520require%2520not%2520only%2520high-affinity%2520target%2520engagement%252C%2520but%250Aalso%2520favorable%2520manufacturability%252C%2520stability%252C%2520and%2520safety%2520profiles%2520for%2520clinical%250Aeffectiveness.%2520These%2520properties%2520are%2520collectively%2520called%2520%2560developability%2527.%2520To%250Aenable%2520a%2520computational%2520framework%2520for%2520optimizing%2520antibody%2520sequences%2520for%250Afavorable%2520developability%252C%2520we%2520introduce%2520a%2520guided%2520discrete%2520diffusion%2520model%250Atrained%2520on%2520natural%2520paired%2520heavy-%2520and%2520light-chain%2520sequences%2520from%2520the%2520Observed%250AAntibody%2520Space%2520%2528OAS%2529%2520and%2520quantitative%2520developability%2520measurements%2520for%2520246%250Aclinical-stage%2520antibodies.%2520To%2520steer%2520generation%2520toward%2520biophysically%2520viable%250Acandidates%252C%2520we%2520integrate%2520a%2520Soft%2520Value-based%2520Decoding%2520in%2520Diffusion%2520%2528SVDD%2529%2520Module%250Athat%2520biases%2520sampling%2520without%2520compromising%2520naturalness.%2520In%2520unconstrained%250Asampling%252C%2520our%2520model%2520reproduces%2520global%2520features%2520of%2520both%2520the%2520natural%2520repertoire%250Aand%2520approved%2520therapeutics%252C%2520and%2520under%2520SVDD%2520guidance%2520we%2520achieve%2520significant%250Aenrichment%2520in%2520predicted%2520developability%2520scores%2520over%2520unguided%2520baselines.%2520When%250Acombined%2520with%2520high-throughput%2520developability%2520assays%252C%2520this%2520framework%2520enables%2520an%250Aiterative%252C%2520ML-driven%2520pipeline%2520for%2520designing%2520antibodies%2520that%2520satisfy%2520binding%2520and%250Abiophysical%2520criteria%2520in%2520tandem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Generation%20for%20Developable%20Antibodies&entry.906535625=Siqi%20Zhao%20and%20Joshua%20Moller%20and%20Porfi%20Quintero-Cadena%20and%20Lood%20van%20Niekerk&entry.1292438233=%20%20Therapeutic%20antibodies%20require%20not%20only%20high-affinity%20target%20engagement%2C%20but%0Aalso%20favorable%20manufacturability%2C%20stability%2C%20and%20safety%20profiles%20for%20clinical%0Aeffectiveness.%20These%20properties%20are%20collectively%20called%20%60developability%27.%20To%0Aenable%20a%20computational%20framework%20for%20optimizing%20antibody%20sequences%20for%0Afavorable%20developability%2C%20we%20introduce%20a%20guided%20discrete%20diffusion%20model%0Atrained%20on%20natural%20paired%20heavy-%20and%20light-chain%20sequences%20from%20the%20Observed%0AAntibody%20Space%20%28OAS%29%20and%20quantitative%20developability%20measurements%20for%20246%0Aclinical-stage%20antibodies.%20To%20steer%20generation%20toward%20biophysically%20viable%0Acandidates%2C%20we%20integrate%20a%20Soft%20Value-based%20Decoding%20in%20Diffusion%20%28SVDD%29%20Module%0Athat%20biases%20sampling%20without%20compromising%20naturalness.%20In%20unconstrained%0Asampling%2C%20our%20model%20reproduces%20global%20features%20of%20both%20the%20natural%20repertoire%0Aand%20approved%20therapeutics%2C%20and%20under%20SVDD%20guidance%20we%20achieve%20significant%0Aenrichment%20in%20predicted%20developability%20scores%20over%20unguided%20baselines.%20When%0Acombined%20with%20high-throughput%20developability%20assays%2C%20this%20framework%20enables%20an%0Aiterative%2C%20ML-driven%20pipeline%20for%20designing%20antibodies%20that%20satisfy%20binding%20and%0Abiophysical%20criteria%20in%20tandem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02670v1&entry.124074799=Read"},
{"title": "Learning to Coordinate Bidders in Non-Truthful Auctions", "author": "Hu Fu and Tao Lin", "abstract": "  In non-truthful auctions such as first-price and all-pay auctions, the\nindependent strategic behaviors of bidders, with the corresponding equilibrium\nnotion -- Bayes Nash equilibria -- are notoriously difficult to characterize\nand can cause undesirable outcomes. An alternative approach to designing better\nauction systems is to coordinate the bidders: let a mediator make\nincentive-compatible recommendations of correlated bidding strategies to the\nbidders, namely, implementing a Bayes correlated equilibrium (BCE). The\nimplementation of BCE, however, requires knowledge of the distribution of\nbidders' private valuations, which is often unavailable. We initiate the study\nof the sample complexity of learning Bayes correlated equilibria in\nnon-truthful auctions. We prove that the BCEs in a large class of non-truthful\nauctions, including first-price and all-pay auctions, can be learned with a\npolynomial number $\\tilde O(\\frac{n}{\\varepsilon^2})$ of samples from the\nbidders' value distributions. Our technique is a reduction to the problem of\nestimating bidders' expected utility from samples, combined with an analysis of\nthe pseudo-dimension of the class of all monotone bidding strategies of\nbidders.\n", "link": "http://arxiv.org/abs/2507.02801v1", "date": "2025-07-03", "relevancy": 1.6288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4231}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4061}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions&body=Title%3A%20Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions%0AAuthor%3A%20Hu%20Fu%20and%20Tao%20Lin%0AAbstract%3A%20%20%20In%20non-truthful%20auctions%20such%20as%20first-price%20and%20all-pay%20auctions%2C%20the%0Aindependent%20strategic%20behaviors%20of%20bidders%2C%20with%20the%20corresponding%20equilibrium%0Anotion%20--%20Bayes%20Nash%20equilibria%20--%20are%20notoriously%20difficult%20to%20characterize%0Aand%20can%20cause%20undesirable%20outcomes.%20An%20alternative%20approach%20to%20designing%20better%0Aauction%20systems%20is%20to%20coordinate%20the%20bidders%3A%20let%20a%20mediator%20make%0Aincentive-compatible%20recommendations%20of%20correlated%20bidding%20strategies%20to%20the%0Abidders%2C%20namely%2C%20implementing%20a%20Bayes%20correlated%20equilibrium%20%28BCE%29.%20The%0Aimplementation%20of%20BCE%2C%20however%2C%20requires%20knowledge%20of%20the%20distribution%20of%0Abidders%27%20private%20valuations%2C%20which%20is%20often%20unavailable.%20We%20initiate%20the%20study%0Aof%20the%20sample%20complexity%20of%20learning%20Bayes%20correlated%20equilibria%20in%0Anon-truthful%20auctions.%20We%20prove%20that%20the%20BCEs%20in%20a%20large%20class%20of%20non-truthful%0Aauctions%2C%20including%20first-price%20and%20all-pay%20auctions%2C%20can%20be%20learned%20with%20a%0Apolynomial%20number%20%24%5Ctilde%20O%28%5Cfrac%7Bn%7D%7B%5Cvarepsilon%5E2%7D%29%24%20of%20samples%20from%20the%0Abidders%27%20value%20distributions.%20Our%20technique%20is%20a%20reduction%20to%20the%20problem%20of%0Aestimating%20bidders%27%20expected%20utility%20from%20samples%2C%20combined%20with%20an%20analysis%20of%0Athe%20pseudo-dimension%20of%20the%20class%20of%20all%20monotone%20bidding%20strategies%20of%0Abidders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Coordinate%2520Bidders%2520in%2520Non-Truthful%2520Auctions%26entry.906535625%3DHu%2520Fu%2520and%2520Tao%2520Lin%26entry.1292438233%3D%2520%2520In%2520non-truthful%2520auctions%2520such%2520as%2520first-price%2520and%2520all-pay%2520auctions%252C%2520the%250Aindependent%2520strategic%2520behaviors%2520of%2520bidders%252C%2520with%2520the%2520corresponding%2520equilibrium%250Anotion%2520--%2520Bayes%2520Nash%2520equilibria%2520--%2520are%2520notoriously%2520difficult%2520to%2520characterize%250Aand%2520can%2520cause%2520undesirable%2520outcomes.%2520An%2520alternative%2520approach%2520to%2520designing%2520better%250Aauction%2520systems%2520is%2520to%2520coordinate%2520the%2520bidders%253A%2520let%2520a%2520mediator%2520make%250Aincentive-compatible%2520recommendations%2520of%2520correlated%2520bidding%2520strategies%2520to%2520the%250Abidders%252C%2520namely%252C%2520implementing%2520a%2520Bayes%2520correlated%2520equilibrium%2520%2528BCE%2529.%2520The%250Aimplementation%2520of%2520BCE%252C%2520however%252C%2520requires%2520knowledge%2520of%2520the%2520distribution%2520of%250Abidders%2527%2520private%2520valuations%252C%2520which%2520is%2520often%2520unavailable.%2520We%2520initiate%2520the%2520study%250Aof%2520the%2520sample%2520complexity%2520of%2520learning%2520Bayes%2520correlated%2520equilibria%2520in%250Anon-truthful%2520auctions.%2520We%2520prove%2520that%2520the%2520BCEs%2520in%2520a%2520large%2520class%2520of%2520non-truthful%250Aauctions%252C%2520including%2520first-price%2520and%2520all-pay%2520auctions%252C%2520can%2520be%2520learned%2520with%2520a%250Apolynomial%2520number%2520%2524%255Ctilde%2520O%2528%255Cfrac%257Bn%257D%257B%255Cvarepsilon%255E2%257D%2529%2524%2520of%2520samples%2520from%2520the%250Abidders%2527%2520value%2520distributions.%2520Our%2520technique%2520is%2520a%2520reduction%2520to%2520the%2520problem%2520of%250Aestimating%2520bidders%2527%2520expected%2520utility%2520from%2520samples%252C%2520combined%2520with%2520an%2520analysis%2520of%250Athe%2520pseudo-dimension%2520of%2520the%2520class%2520of%2520all%2520monotone%2520bidding%2520strategies%2520of%250Abidders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions&entry.906535625=Hu%20Fu%20and%20Tao%20Lin&entry.1292438233=%20%20In%20non-truthful%20auctions%20such%20as%20first-price%20and%20all-pay%20auctions%2C%20the%0Aindependent%20strategic%20behaviors%20of%20bidders%2C%20with%20the%20corresponding%20equilibrium%0Anotion%20--%20Bayes%20Nash%20equilibria%20--%20are%20notoriously%20difficult%20to%20characterize%0Aand%20can%20cause%20undesirable%20outcomes.%20An%20alternative%20approach%20to%20designing%20better%0Aauction%20systems%20is%20to%20coordinate%20the%20bidders%3A%20let%20a%20mediator%20make%0Aincentive-compatible%20recommendations%20of%20correlated%20bidding%20strategies%20to%20the%0Abidders%2C%20namely%2C%20implementing%20a%20Bayes%20correlated%20equilibrium%20%28BCE%29.%20The%0Aimplementation%20of%20BCE%2C%20however%2C%20requires%20knowledge%20of%20the%20distribution%20of%0Abidders%27%20private%20valuations%2C%20which%20is%20often%20unavailable.%20We%20initiate%20the%20study%0Aof%20the%20sample%20complexity%20of%20learning%20Bayes%20correlated%20equilibria%20in%0Anon-truthful%20auctions.%20We%20prove%20that%20the%20BCEs%20in%20a%20large%20class%20of%20non-truthful%0Aauctions%2C%20including%20first-price%20and%20all-pay%20auctions%2C%20can%20be%20learned%20with%20a%0Apolynomial%20number%20%24%5Ctilde%20O%28%5Cfrac%7Bn%7D%7B%5Cvarepsilon%5E2%7D%29%24%20of%20samples%20from%20the%0Abidders%27%20value%20distributions.%20Our%20technique%20is%20a%20reduction%20to%20the%20problem%20of%0Aestimating%20bidders%27%20expected%20utility%20from%20samples%2C%20combined%20with%20an%20analysis%20of%0Athe%20pseudo-dimension%20of%20the%20class%20of%20all%20monotone%20bidding%20strategies%20of%0Abidders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02801v1&entry.124074799=Read"},
{"title": "Optimizing Start Locations in Ergodic Search for Disaster Response", "author": "Ananya Rao and Alyssa Hargis and David Wettergreen and Howie Choset", "abstract": "  In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.\n", "link": "http://arxiv.org/abs/2507.02708v1", "date": "2025-07-03", "relevancy": 1.5201, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Start%20Locations%20in%20Ergodic%20Search%20for%20Disaster%20Response&body=Title%3A%20Optimizing%20Start%20Locations%20in%20Ergodic%20Search%20for%20Disaster%20Response%0AAuthor%3A%20Ananya%20Rao%20and%20Alyssa%20Hargis%20and%20David%20Wettergreen%20and%20Howie%20Choset%0AAbstract%3A%20%20%20In%20disaster%20response%20scenarios%2C%20deploying%20robotic%20teams%20effectively%20is%0Acrucial%20for%20improving%20situational%20awareness%20and%20enhancing%20search%20and%20rescue%0Aoperations.%20The%20use%20of%20robots%20in%20search%20and%20rescue%20has%20been%20studied%20but%20the%0Aquestion%20of%20where%20to%20start%20robot%20deployments%20has%20not%20been%20addressed.%20This%20work%0Aaddresses%20the%20problem%20of%20optimally%20selecting%20starting%20locations%20for%20robots%20with%0Aheterogeneous%20capabilities%20by%20formulating%20a%20joint%20optimization%20problem.%20To%0Adetermine%20start%20locations%2C%20this%20work%20adds%20a%20constraint%20to%20the%20ergodic%0Aoptimization%20framework%20whose%20minimum%20assigns%20robots%20to%20start%20locations.%20This%0Abecomes%20a%20little%20more%20challenging%20when%20the%20robots%20are%20heterogeneous%20%28equipped%0Awith%20different%20sensing%20and%20motion%20modalities%29%20because%20not%20all%20robots%20start%20at%0Athe%20same%20location%2C%20and%20a%20more%20complex%20adaptation%20of%20the%20aforementioned%0Aconstraint%20is%20applied.%20Our%20method%20assumes%20access%20to%20potential%20starting%0Alocations%2C%20which%20can%20be%20obtained%20from%20expert%20knowledge%20or%20aerial%20imagery.%20We%0Aexperimentally%20evaluate%20the%20efficacy%20of%20our%20joint%20optimization%20approach%20by%0Acomparing%20it%20to%20baseline%20methods%20that%20use%20fixed%20starting%20locations%20for%20all%0Arobots.%20Our%20experimental%20results%20show%20significant%20gains%20in%20coverage%0Aperformance%2C%20with%20average%20improvements%20of%2035.98%25%20on%20synthetic%20data%20and%2031.91%25%0Aon%20real-world%20data%20for%20homogeneous%20and%20heterogeneous%20teams%2C%20in%20terms%20of%20the%0Aergodic%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Start%2520Locations%2520in%2520Ergodic%2520Search%2520for%2520Disaster%2520Response%26entry.906535625%3DAnanya%2520Rao%2520and%2520Alyssa%2520Hargis%2520and%2520David%2520Wettergreen%2520and%2520Howie%2520Choset%26entry.1292438233%3D%2520%2520In%2520disaster%2520response%2520scenarios%252C%2520deploying%2520robotic%2520teams%2520effectively%2520is%250Acrucial%2520for%2520improving%2520situational%2520awareness%2520and%2520enhancing%2520search%2520and%2520rescue%250Aoperations.%2520The%2520use%2520of%2520robots%2520in%2520search%2520and%2520rescue%2520has%2520been%2520studied%2520but%2520the%250Aquestion%2520of%2520where%2520to%2520start%2520robot%2520deployments%2520has%2520not%2520been%2520addressed.%2520This%2520work%250Aaddresses%2520the%2520problem%2520of%2520optimally%2520selecting%2520starting%2520locations%2520for%2520robots%2520with%250Aheterogeneous%2520capabilities%2520by%2520formulating%2520a%2520joint%2520optimization%2520problem.%2520To%250Adetermine%2520start%2520locations%252C%2520this%2520work%2520adds%2520a%2520constraint%2520to%2520the%2520ergodic%250Aoptimization%2520framework%2520whose%2520minimum%2520assigns%2520robots%2520to%2520start%2520locations.%2520This%250Abecomes%2520a%2520little%2520more%2520challenging%2520when%2520the%2520robots%2520are%2520heterogeneous%2520%2528equipped%250Awith%2520different%2520sensing%2520and%2520motion%2520modalities%2529%2520because%2520not%2520all%2520robots%2520start%2520at%250Athe%2520same%2520location%252C%2520and%2520a%2520more%2520complex%2520adaptation%2520of%2520the%2520aforementioned%250Aconstraint%2520is%2520applied.%2520Our%2520method%2520assumes%2520access%2520to%2520potential%2520starting%250Alocations%252C%2520which%2520can%2520be%2520obtained%2520from%2520expert%2520knowledge%2520or%2520aerial%2520imagery.%2520We%250Aexperimentally%2520evaluate%2520the%2520efficacy%2520of%2520our%2520joint%2520optimization%2520approach%2520by%250Acomparing%2520it%2520to%2520baseline%2520methods%2520that%2520use%2520fixed%2520starting%2520locations%2520for%2520all%250Arobots.%2520Our%2520experimental%2520results%2520show%2520significant%2520gains%2520in%2520coverage%250Aperformance%252C%2520with%2520average%2520improvements%2520of%252035.98%2525%2520on%2520synthetic%2520data%2520and%252031.91%2525%250Aon%2520real-world%2520data%2520for%2520homogeneous%2520and%2520heterogeneous%2520teams%252C%2520in%2520terms%2520of%2520the%250Aergodic%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Start%20Locations%20in%20Ergodic%20Search%20for%20Disaster%20Response&entry.906535625=Ananya%20Rao%20and%20Alyssa%20Hargis%20and%20David%20Wettergreen%20and%20Howie%20Choset&entry.1292438233=%20%20In%20disaster%20response%20scenarios%2C%20deploying%20robotic%20teams%20effectively%20is%0Acrucial%20for%20improving%20situational%20awareness%20and%20enhancing%20search%20and%20rescue%0Aoperations.%20The%20use%20of%20robots%20in%20search%20and%20rescue%20has%20been%20studied%20but%20the%0Aquestion%20of%20where%20to%20start%20robot%20deployments%20has%20not%20been%20addressed.%20This%20work%0Aaddresses%20the%20problem%20of%20optimally%20selecting%20starting%20locations%20for%20robots%20with%0Aheterogeneous%20capabilities%20by%20formulating%20a%20joint%20optimization%20problem.%20To%0Adetermine%20start%20locations%2C%20this%20work%20adds%20a%20constraint%20to%20the%20ergodic%0Aoptimization%20framework%20whose%20minimum%20assigns%20robots%20to%20start%20locations.%20This%0Abecomes%20a%20little%20more%20challenging%20when%20the%20robots%20are%20heterogeneous%20%28equipped%0Awith%20different%20sensing%20and%20motion%20modalities%29%20because%20not%20all%20robots%20start%20at%0Athe%20same%20location%2C%20and%20a%20more%20complex%20adaptation%20of%20the%20aforementioned%0Aconstraint%20is%20applied.%20Our%20method%20assumes%20access%20to%20potential%20starting%0Alocations%2C%20which%20can%20be%20obtained%20from%20expert%20knowledge%20or%20aerial%20imagery.%20We%0Aexperimentally%20evaluate%20the%20efficacy%20of%20our%20joint%20optimization%20approach%20by%0Acomparing%20it%20to%20baseline%20methods%20that%20use%20fixed%20starting%20locations%20for%20all%0Arobots.%20Our%20experimental%20results%20show%20significant%20gains%20in%20coverage%0Aperformance%2C%20with%20average%20improvements%20of%2035.98%25%20on%20synthetic%20data%20and%2031.91%25%0Aon%20real-world%20data%20for%20homogeneous%20and%20heterogeneous%20teams%2C%20in%20terms%20of%20the%0Aergodic%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02708v1&entry.124074799=Read"},
{"title": "In-Training Multicalibrated Survival Analysis for Healthcare via\n  Constrained Optimization", "author": "Thiti Suttaket and Stanley Kok", "abstract": "  Survival analysis is an important problem in healthcare because it models the\nrelationship between an individual's covariates and the onset time of an event\nof interest (e.g., death). It is important for survival models to be\nwell-calibrated (i.e., for their predicted probabilities to be close to\nground-truth probabilities) because badly calibrated systems can result in\nerroneous clinical decisions. Existing survival models are typically calibrated\nat the population level only, and thus run the risk of being poorly calibrated\nfor one or more minority subpopulations. We propose a model called GRADUATE\nthat achieves multicalibration by ensuring that all subpopulations are\nwell-calibrated too. GRADUATE frames multicalibration as a constrained\noptimization problem, and optimizes both calibration and discrimination\nin-training to achieve a good balance between them. We mathematically prove\nthat the optimization method used yields a solution that is both near-optimal\nand feasible with high probability. Empirical comparisons against\nstate-of-the-art baselines on real-world clinical datasets demonstrate\nGRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of\nthe baselines vis-a-vis GRADUATE's strengths.\n", "link": "http://arxiv.org/abs/2507.02807v1", "date": "2025-07-03", "relevancy": 1.3017, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4274}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Training%20Multicalibrated%20Survival%20Analysis%20for%20Healthcare%20via%0A%20%20Constrained%20Optimization&body=Title%3A%20In-Training%20Multicalibrated%20Survival%20Analysis%20for%20Healthcare%20via%0A%20%20Constrained%20Optimization%0AAuthor%3A%20Thiti%20Suttaket%20and%20Stanley%20Kok%0AAbstract%3A%20%20%20Survival%20analysis%20is%20an%20important%20problem%20in%20healthcare%20because%20it%20models%20the%0Arelationship%20between%20an%20individual%27s%20covariates%20and%20the%20onset%20time%20of%20an%20event%0Aof%20interest%20%28e.g.%2C%20death%29.%20It%20is%20important%20for%20survival%20models%20to%20be%0Awell-calibrated%20%28i.e.%2C%20for%20their%20predicted%20probabilities%20to%20be%20close%20to%0Aground-truth%20probabilities%29%20because%20badly%20calibrated%20systems%20can%20result%20in%0Aerroneous%20clinical%20decisions.%20Existing%20survival%20models%20are%20typically%20calibrated%0Aat%20the%20population%20level%20only%2C%20and%20thus%20run%20the%20risk%20of%20being%20poorly%20calibrated%0Afor%20one%20or%20more%20minority%20subpopulations.%20We%20propose%20a%20model%20called%20GRADUATE%0Athat%20achieves%20multicalibration%20by%20ensuring%20that%20all%20subpopulations%20are%0Awell-calibrated%20too.%20GRADUATE%20frames%20multicalibration%20as%20a%20constrained%0Aoptimization%20problem%2C%20and%20optimizes%20both%20calibration%20and%20discrimination%0Ain-training%20to%20achieve%20a%20good%20balance%20between%20them.%20We%20mathematically%20prove%0Athat%20the%20optimization%20method%20used%20yields%20a%20solution%20that%20is%20both%20near-optimal%0Aand%20feasible%20with%20high%20probability.%20Empirical%20comparisons%20against%0Astate-of-the-art%20baselines%20on%20real-world%20clinical%20datasets%20demonstrate%0AGRADUATE%27s%20efficacy.%20In%20a%20detailed%20analysis%2C%20we%20elucidate%20the%20shortcomings%20of%0Athe%20baselines%20vis-a-vis%20GRADUATE%27s%20strengths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Training%2520Multicalibrated%2520Survival%2520Analysis%2520for%2520Healthcare%2520via%250A%2520%2520Constrained%2520Optimization%26entry.906535625%3DThiti%2520Suttaket%2520and%2520Stanley%2520Kok%26entry.1292438233%3D%2520%2520Survival%2520analysis%2520is%2520an%2520important%2520problem%2520in%2520healthcare%2520because%2520it%2520models%2520the%250Arelationship%2520between%2520an%2520individual%2527s%2520covariates%2520and%2520the%2520onset%2520time%2520of%2520an%2520event%250Aof%2520interest%2520%2528e.g.%252C%2520death%2529.%2520It%2520is%2520important%2520for%2520survival%2520models%2520to%2520be%250Awell-calibrated%2520%2528i.e.%252C%2520for%2520their%2520predicted%2520probabilities%2520to%2520be%2520close%2520to%250Aground-truth%2520probabilities%2529%2520because%2520badly%2520calibrated%2520systems%2520can%2520result%2520in%250Aerroneous%2520clinical%2520decisions.%2520Existing%2520survival%2520models%2520are%2520typically%2520calibrated%250Aat%2520the%2520population%2520level%2520only%252C%2520and%2520thus%2520run%2520the%2520risk%2520of%2520being%2520poorly%2520calibrated%250Afor%2520one%2520or%2520more%2520minority%2520subpopulations.%2520We%2520propose%2520a%2520model%2520called%2520GRADUATE%250Athat%2520achieves%2520multicalibration%2520by%2520ensuring%2520that%2520all%2520subpopulations%2520are%250Awell-calibrated%2520too.%2520GRADUATE%2520frames%2520multicalibration%2520as%2520a%2520constrained%250Aoptimization%2520problem%252C%2520and%2520optimizes%2520both%2520calibration%2520and%2520discrimination%250Ain-training%2520to%2520achieve%2520a%2520good%2520balance%2520between%2520them.%2520We%2520mathematically%2520prove%250Athat%2520the%2520optimization%2520method%2520used%2520yields%2520a%2520solution%2520that%2520is%2520both%2520near-optimal%250Aand%2520feasible%2520with%2520high%2520probability.%2520Empirical%2520comparisons%2520against%250Astate-of-the-art%2520baselines%2520on%2520real-world%2520clinical%2520datasets%2520demonstrate%250AGRADUATE%2527s%2520efficacy.%2520In%2520a%2520detailed%2520analysis%252C%2520we%2520elucidate%2520the%2520shortcomings%2520of%250Athe%2520baselines%2520vis-a-vis%2520GRADUATE%2527s%2520strengths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Training%20Multicalibrated%20Survival%20Analysis%20for%20Healthcare%20via%0A%20%20Constrained%20Optimization&entry.906535625=Thiti%20Suttaket%20and%20Stanley%20Kok&entry.1292438233=%20%20Survival%20analysis%20is%20an%20important%20problem%20in%20healthcare%20because%20it%20models%20the%0Arelationship%20between%20an%20individual%27s%20covariates%20and%20the%20onset%20time%20of%20an%20event%0Aof%20interest%20%28e.g.%2C%20death%29.%20It%20is%20important%20for%20survival%20models%20to%20be%0Awell-calibrated%20%28i.e.%2C%20for%20their%20predicted%20probabilities%20to%20be%20close%20to%0Aground-truth%20probabilities%29%20because%20badly%20calibrated%20systems%20can%20result%20in%0Aerroneous%20clinical%20decisions.%20Existing%20survival%20models%20are%20typically%20calibrated%0Aat%20the%20population%20level%20only%2C%20and%20thus%20run%20the%20risk%20of%20being%20poorly%20calibrated%0Afor%20one%20or%20more%20minority%20subpopulations.%20We%20propose%20a%20model%20called%20GRADUATE%0Athat%20achieves%20multicalibration%20by%20ensuring%20that%20all%20subpopulations%20are%0Awell-calibrated%20too.%20GRADUATE%20frames%20multicalibration%20as%20a%20constrained%0Aoptimization%20problem%2C%20and%20optimizes%20both%20calibration%20and%20discrimination%0Ain-training%20to%20achieve%20a%20good%20balance%20between%20them.%20We%20mathematically%20prove%0Athat%20the%20optimization%20method%20used%20yields%20a%20solution%20that%20is%20both%20near-optimal%0Aand%20feasible%20with%20high%20probability.%20Empirical%20comparisons%20against%0Astate-of-the-art%20baselines%20on%20real-world%20clinical%20datasets%20demonstrate%0AGRADUATE%27s%20efficacy.%20In%20a%20detailed%20analysis%2C%20we%20elucidate%20the%20shortcomings%20of%0Athe%20baselines%20vis-a-vis%20GRADUATE%27s%20strengths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02807v1&entry.124074799=Read"},
{"title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving", "author": "Matthieu Zimmer and Xiaotong Ji and Rasul Tutunov and Anthony Bordg and Jun Wang and Haitham Bou Ammar", "abstract": "  Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.\n", "link": "http://arxiv.org/abs/2507.02726v1", "date": "2025-07-03", "relevancy": 0.9961, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving&body=Title%3A%20Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving%0AAuthor%3A%20Matthieu%20Zimmer%20and%20Xiaotong%20Ji%20and%20Rasul%20Tutunov%20and%20Anthony%20Bordg%20and%20Jun%20Wang%20and%20Haitham%20Bou%20Ammar%0AAbstract%3A%20%20%20Reasoning%20remains%20a%20challenging%20task%20for%20large%20language%20models%20%28LLMs%29%2C%0Aespecially%20within%20the%20logically%20constrained%20environment%20of%20automated%20theorem%0Aproving%20%28ATP%29%2C%20due%20to%20sparse%20rewards%20and%20the%20vast%20scale%20of%20proofs.%20These%0Achallenges%20are%20amplified%20in%20benchmarks%20like%20PutnamBench%2C%20which%20contains%0Auniversity-level%20problems%20requiring%20complex%2C%20multi-step%20reasoning.%20To%20address%0Athis%2C%20we%20introduce%20self-generated%20goal-conditioned%20MDPs%20%28sG-MDPs%29%2C%20a%20new%0Aframework%20in%20which%20agents%20generate%20and%20pursue%20their%20subgoals%20based%20on%20the%0Aevolving%20proof%20state.%20Given%20this%20more%20structured%20generation%20of%20goals%2C%20the%0Aresulting%20problem%20becomes%20more%20amenable%20to%20search.%20We%20then%20apply%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29-like%20algorithms%20to%20solve%20the%20sG-MDP%2C%20instantiating%20our%0Aapproach%20in%20Bourbaki%20%287B%29%2C%20a%20modular%20system%20that%20can%20ensemble%20multiple%207B%20LLMs%0Afor%20subgoal%20generation%20and%20tactic%20synthesis.%20On%20PutnamBench%2C%20Bourbaki%20%287B%29%0Asolves%2026%20problems%2C%20achieving%20new%20state-of-the-art%20results%20with%20models%20at%20this%0Ascale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBourbaki%253A%2520Self-Generated%2520and%2520Goal-Conditioned%2520MDPs%2520for%2520Theorem%2520Proving%26entry.906535625%3DMatthieu%2520Zimmer%2520and%2520Xiaotong%2520Ji%2520and%2520Rasul%2520Tutunov%2520and%2520Anthony%2520Bordg%2520and%2520Jun%2520Wang%2520and%2520Haitham%2520Bou%2520Ammar%26entry.1292438233%3D%2520%2520Reasoning%2520remains%2520a%2520challenging%2520task%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Aespecially%2520within%2520the%2520logically%2520constrained%2520environment%2520of%2520automated%2520theorem%250Aproving%2520%2528ATP%2529%252C%2520due%2520to%2520sparse%2520rewards%2520and%2520the%2520vast%2520scale%2520of%2520proofs.%2520These%250Achallenges%2520are%2520amplified%2520in%2520benchmarks%2520like%2520PutnamBench%252C%2520which%2520contains%250Auniversity-level%2520problems%2520requiring%2520complex%252C%2520multi-step%2520reasoning.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520self-generated%2520goal-conditioned%2520MDPs%2520%2528sG-MDPs%2529%252C%2520a%2520new%250Aframework%2520in%2520which%2520agents%2520generate%2520and%2520pursue%2520their%2520subgoals%2520based%2520on%2520the%250Aevolving%2520proof%2520state.%2520Given%2520this%2520more%2520structured%2520generation%2520of%2520goals%252C%2520the%250Aresulting%2520problem%2520becomes%2520more%2520amenable%2520to%2520search.%2520We%2520then%2520apply%2520Monte%2520Carlo%250ATree%2520Search%2520%2528MCTS%2529-like%2520algorithms%2520to%2520solve%2520the%2520sG-MDP%252C%2520instantiating%2520our%250Aapproach%2520in%2520Bourbaki%2520%25287B%2529%252C%2520a%2520modular%2520system%2520that%2520can%2520ensemble%2520multiple%25207B%2520LLMs%250Afor%2520subgoal%2520generation%2520and%2520tactic%2520synthesis.%2520On%2520PutnamBench%252C%2520Bourbaki%2520%25287B%2529%250Asolves%252026%2520problems%252C%2520achieving%2520new%2520state-of-the-art%2520results%2520with%2520models%2520at%2520this%250Ascale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving&entry.906535625=Matthieu%20Zimmer%20and%20Xiaotong%20Ji%20and%20Rasul%20Tutunov%20and%20Anthony%20Bordg%20and%20Jun%20Wang%20and%20Haitham%20Bou%20Ammar&entry.1292438233=%20%20Reasoning%20remains%20a%20challenging%20task%20for%20large%20language%20models%20%28LLMs%29%2C%0Aespecially%20within%20the%20logically%20constrained%20environment%20of%20automated%20theorem%0Aproving%20%28ATP%29%2C%20due%20to%20sparse%20rewards%20and%20the%20vast%20scale%20of%20proofs.%20These%0Achallenges%20are%20amplified%20in%20benchmarks%20like%20PutnamBench%2C%20which%20contains%0Auniversity-level%20problems%20requiring%20complex%2C%20multi-step%20reasoning.%20To%20address%0Athis%2C%20we%20introduce%20self-generated%20goal-conditioned%20MDPs%20%28sG-MDPs%29%2C%20a%20new%0Aframework%20in%20which%20agents%20generate%20and%20pursue%20their%20subgoals%20based%20on%20the%0Aevolving%20proof%20state.%20Given%20this%20more%20structured%20generation%20of%20goals%2C%20the%0Aresulting%20problem%20becomes%20more%20amenable%20to%20search.%20We%20then%20apply%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29-like%20algorithms%20to%20solve%20the%20sG-MDP%2C%20instantiating%20our%0Aapproach%20in%20Bourbaki%20%287B%29%2C%20a%20modular%20system%20that%20can%20ensemble%20multiple%207B%20LLMs%0Afor%20subgoal%20generation%20and%20tactic%20synthesis.%20On%20PutnamBench%2C%20Bourbaki%20%287B%29%0Asolves%2026%20problems%2C%20achieving%20new%20state-of-the-art%20results%20with%20models%20at%20this%0Ascale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02726v1&entry.124074799=Read"},
{"title": "TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for\n  General Robot Control", "author": "Zhenyang Liu and Yongchong Gu and Sixiao Zheng and Xiangyang Xue and Yanwei Fu", "abstract": "  Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods design a specific architecture like dual-system to\nleverage large-scale pretrained knowledge, they tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks. To\nthis end, we propose TriVLA, a unified Vision-Language-Action model with a\ntriple-system architecture for general robot control. The vision-language\nmodule (System 2) interprets the environment through vision and language\ninstructions. The dynamics perception module (System 3) inherently produces\nvisual representations that encompass both current static information and\npredicted future dynamics, thereby providing valuable guidance for policy\nlearning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained\nvideo foundation model on robot datasets along with internet human manipulation\ndata. The subsequent policy learning module (System 1) generates fluid motor\nactions in real time. Experimental evaluation demonstrates that TriVLA operates\nat approximately 36 Hz and surpasses state-of-the-art imitation learning\nbaselines on standard simulation benchmarks as well as challenging real-world\nmanipulation tasks.\n", "link": "http://arxiv.org/abs/2507.01424v2", "date": "2025-07-03", "relevancy": 1.7361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5836}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5744}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriVLA%3A%20A%20Triple-System-Based%20Unified%20Vision-Language-Action%20Model%20for%0A%20%20General%20Robot%20Control&body=Title%3A%20TriVLA%3A%20A%20Triple-System-Based%20Unified%20Vision-Language-Action%20Model%20for%0A%20%20General%20Robot%20Control%0AAuthor%3A%20Zhenyang%20Liu%20and%20Yongchong%20Gu%20and%20Sixiao%20Zheng%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20for%20common-sense%0Areasoning%20have%20led%20to%20the%20development%20of%20vision-language-action%20%28VLA%29%20models%2C%0Aenabling%20robots%20to%20perform%20generalized%20manipulation.%20Although%20existing%0Aautoregressive%20VLA%20methods%20design%20a%20specific%20architecture%20like%20dual-system%20to%0Aleverage%20large-scale%20pretrained%20knowledge%2C%20they%20tend%20to%20capture%20static%0Ainformation%2C%20often%20neglecting%20the%20dynamic%20aspects%20vital%20for%20embodied%20tasks.%20To%0Athis%20end%2C%20we%20propose%20TriVLA%2C%20a%20unified%20Vision-Language-Action%20model%20with%20a%0Atriple-system%20architecture%20for%20general%20robot%20control.%20The%20vision-language%0Amodule%20%28System%202%29%20interprets%20the%20environment%20through%20vision%20and%20language%0Ainstructions.%20The%20dynamics%20perception%20module%20%28System%203%29%20inherently%20produces%0Avisual%20representations%20that%20encompass%20both%20current%20static%20information%20and%0Apredicted%20future%20dynamics%2C%20thereby%20providing%20valuable%20guidance%20for%20policy%0Alearning.%20TriVLA%20utilizes%20pre-trained%20VLM%20model%20and%20fine-tunes%20pre-trained%0Avideo%20foundation%20model%20on%20robot%20datasets%20along%20with%20internet%20human%20manipulation%0Adata.%20The%20subsequent%20policy%20learning%20module%20%28System%201%29%20generates%20fluid%20motor%0Aactions%20in%20real%20time.%20Experimental%20evaluation%20demonstrates%20that%20TriVLA%20operates%0Aat%20approximately%2036%20Hz%20and%20surpasses%20state-of-the-art%20imitation%20learning%0Abaselines%20on%20standard%20simulation%20benchmarks%20as%20well%20as%20challenging%20real-world%0Amanipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01424v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriVLA%253A%2520A%2520Triple-System-Based%2520Unified%2520Vision-Language-Action%2520Model%2520for%250A%2520%2520General%2520Robot%2520Control%26entry.906535625%3DZhenyang%2520Liu%2520and%2520Yongchong%2520Gu%2520and%2520Sixiao%2520Zheng%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520common-sense%250Areasoning%2520have%2520led%2520to%2520the%2520development%2520of%2520vision-language-action%2520%2528VLA%2529%2520models%252C%250Aenabling%2520robots%2520to%2520perform%2520generalized%2520manipulation.%2520Although%2520existing%250Aautoregressive%2520VLA%2520methods%2520design%2520a%2520specific%2520architecture%2520like%2520dual-system%2520to%250Aleverage%2520large-scale%2520pretrained%2520knowledge%252C%2520they%2520tend%2520to%2520capture%2520static%250Ainformation%252C%2520often%2520neglecting%2520the%2520dynamic%2520aspects%2520vital%2520for%2520embodied%2520tasks.%2520To%250Athis%2520end%252C%2520we%2520propose%2520TriVLA%252C%2520a%2520unified%2520Vision-Language-Action%2520model%2520with%2520a%250Atriple-system%2520architecture%2520for%2520general%2520robot%2520control.%2520The%2520vision-language%250Amodule%2520%2528System%25202%2529%2520interprets%2520the%2520environment%2520through%2520vision%2520and%2520language%250Ainstructions.%2520The%2520dynamics%2520perception%2520module%2520%2528System%25203%2529%2520inherently%2520produces%250Avisual%2520representations%2520that%2520encompass%2520both%2520current%2520static%2520information%2520and%250Apredicted%2520future%2520dynamics%252C%2520thereby%2520providing%2520valuable%2520guidance%2520for%2520policy%250Alearning.%2520TriVLA%2520utilizes%2520pre-trained%2520VLM%2520model%2520and%2520fine-tunes%2520pre-trained%250Avideo%2520foundation%2520model%2520on%2520robot%2520datasets%2520along%2520with%2520internet%2520human%2520manipulation%250Adata.%2520The%2520subsequent%2520policy%2520learning%2520module%2520%2528System%25201%2529%2520generates%2520fluid%2520motor%250Aactions%2520in%2520real%2520time.%2520Experimental%2520evaluation%2520demonstrates%2520that%2520TriVLA%2520operates%250Aat%2520approximately%252036%2520Hz%2520and%2520surpasses%2520state-of-the-art%2520imitation%2520learning%250Abaselines%2520on%2520standard%2520simulation%2520benchmarks%2520as%2520well%2520as%2520challenging%2520real-world%250Amanipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01424v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriVLA%3A%20A%20Triple-System-Based%20Unified%20Vision-Language-Action%20Model%20for%0A%20%20General%20Robot%20Control&entry.906535625=Zhenyang%20Liu%20and%20Yongchong%20Gu%20and%20Sixiao%20Zheng%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu&entry.1292438233=%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20for%20common-sense%0Areasoning%20have%20led%20to%20the%20development%20of%20vision-language-action%20%28VLA%29%20models%2C%0Aenabling%20robots%20to%20perform%20generalized%20manipulation.%20Although%20existing%0Aautoregressive%20VLA%20methods%20design%20a%20specific%20architecture%20like%20dual-system%20to%0Aleverage%20large-scale%20pretrained%20knowledge%2C%20they%20tend%20to%20capture%20static%0Ainformation%2C%20often%20neglecting%20the%20dynamic%20aspects%20vital%20for%20embodied%20tasks.%20To%0Athis%20end%2C%20we%20propose%20TriVLA%2C%20a%20unified%20Vision-Language-Action%20model%20with%20a%0Atriple-system%20architecture%20for%20general%20robot%20control.%20The%20vision-language%0Amodule%20%28System%202%29%20interprets%20the%20environment%20through%20vision%20and%20language%0Ainstructions.%20The%20dynamics%20perception%20module%20%28System%203%29%20inherently%20produces%0Avisual%20representations%20that%20encompass%20both%20current%20static%20information%20and%0Apredicted%20future%20dynamics%2C%20thereby%20providing%20valuable%20guidance%20for%20policy%0Alearning.%20TriVLA%20utilizes%20pre-trained%20VLM%20model%20and%20fine-tunes%20pre-trained%0Avideo%20foundation%20model%20on%20robot%20datasets%20along%20with%20internet%20human%20manipulation%0Adata.%20The%20subsequent%20policy%20learning%20module%20%28System%201%29%20generates%20fluid%20motor%0Aactions%20in%20real%20time.%20Experimental%20evaluation%20demonstrates%20that%20TriVLA%20operates%0Aat%20approximately%2036%20Hz%20and%20surpasses%20state-of-the-art%20imitation%20learning%0Abaselines%20on%20standard%20simulation%20benchmarks%20as%20well%20as%20challenging%20real-world%0Amanipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01424v2&entry.124074799=Read"},
{"title": "Prompt learning with bounding box constraints for medical image\n  segmentation", "author": "M\u00e9lanie Gaillochet and Mehrdad Noori and Sahar Dastani and Christian Desrosiers and Herv\u00e9 Lombaert", "abstract": "  Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git\n", "link": "http://arxiv.org/abs/2507.02743v1", "date": "2025-07-03", "relevancy": 1.564, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20learning%20with%20bounding%20box%20constraints%20for%20medical%20image%0A%20%20segmentation&body=Title%3A%20Prompt%20learning%20with%20bounding%20box%20constraints%20for%20medical%20image%0A%20%20segmentation%0AAuthor%3A%20M%C3%A9lanie%20Gaillochet%20and%20Mehrdad%20Noori%20and%20Sahar%20Dastani%20and%20Christian%20Desrosiers%20and%20Herv%C3%A9%20Lombaert%0AAbstract%3A%20%20%20Pixel-wise%20annotations%20are%20notoriously%20labourious%20and%20costly%20to%20obtain%20in%20the%0Amedical%20domain.%20To%20mitigate%20this%20burden%2C%20weakly%20supervised%20approaches%20based%20on%0Abounding%20box%20annotations-much%20easier%20to%20acquire-offer%20a%20practical%20alternative.%0AVision%20foundation%20models%20have%20recently%20shown%20noteworthy%20segmentation%0Aperformance%20when%20provided%20with%20prompts%20such%20as%20points%20or%20bounding%20boxes.%20Prompt%0Alearning%20exploits%20these%20models%20by%20adapting%20them%20to%20downstream%20tasks%20and%0Aautomating%20segmentation%2C%20thereby%20reducing%20user%20intervention.%20However%2C%20existing%0Aprompt%20learning%20approaches%20depend%20on%20fully%20annotated%20segmentation%20masks.%20This%0Apaper%20proposes%20a%20novel%20framework%20that%20combines%20the%20representational%20power%20of%0Afoundation%20models%20with%20the%20annotation%20efficiency%20of%20weakly%20supervised%0Asegmentation.%20More%20specifically%2C%20our%20approach%20automates%20prompt%20generation%20for%0Afoundation%20models%20using%20only%20bounding%20box%20annotations.%20Our%20proposed%0Aoptimization%20scheme%20integrates%20multiple%20constraints%20derived%20from%20box%0Aannotations%20with%20pseudo-labels%20generated%20by%20the%20prompted%20foundation%20model.%0AExtensive%20experiments%20across%20multimodal%20datasets%20reveal%20that%20our%20weakly%0Asupervised%20method%20achieves%20an%20average%20Dice%20score%20of%2084.90%25%20in%20a%20limited%20data%0Asetting%2C%20outperforming%20existing%20fully-supervised%20and%20weakly-supervised%0Aapproaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Minimel/box-prompt-learning-VFM.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520learning%2520with%2520bounding%2520box%2520constraints%2520for%2520medical%2520image%250A%2520%2520segmentation%26entry.906535625%3DM%25C3%25A9lanie%2520Gaillochet%2520and%2520Mehrdad%2520Noori%2520and%2520Sahar%2520Dastani%2520and%2520Christian%2520Desrosiers%2520and%2520Herv%25C3%25A9%2520Lombaert%26entry.1292438233%3D%2520%2520Pixel-wise%2520annotations%2520are%2520notoriously%2520labourious%2520and%2520costly%2520to%2520obtain%2520in%2520the%250Amedical%2520domain.%2520To%2520mitigate%2520this%2520burden%252C%2520weakly%2520supervised%2520approaches%2520based%2520on%250Abounding%2520box%2520annotations-much%2520easier%2520to%2520acquire-offer%2520a%2520practical%2520alternative.%250AVision%2520foundation%2520models%2520have%2520recently%2520shown%2520noteworthy%2520segmentation%250Aperformance%2520when%2520provided%2520with%2520prompts%2520such%2520as%2520points%2520or%2520bounding%2520boxes.%2520Prompt%250Alearning%2520exploits%2520these%2520models%2520by%2520adapting%2520them%2520to%2520downstream%2520tasks%2520and%250Aautomating%2520segmentation%252C%2520thereby%2520reducing%2520user%2520intervention.%2520However%252C%2520existing%250Aprompt%2520learning%2520approaches%2520depend%2520on%2520fully%2520annotated%2520segmentation%2520masks.%2520This%250Apaper%2520proposes%2520a%2520novel%2520framework%2520that%2520combines%2520the%2520representational%2520power%2520of%250Afoundation%2520models%2520with%2520the%2520annotation%2520efficiency%2520of%2520weakly%2520supervised%250Asegmentation.%2520More%2520specifically%252C%2520our%2520approach%2520automates%2520prompt%2520generation%2520for%250Afoundation%2520models%2520using%2520only%2520bounding%2520box%2520annotations.%2520Our%2520proposed%250Aoptimization%2520scheme%2520integrates%2520multiple%2520constraints%2520derived%2520from%2520box%250Aannotations%2520with%2520pseudo-labels%2520generated%2520by%2520the%2520prompted%2520foundation%2520model.%250AExtensive%2520experiments%2520across%2520multimodal%2520datasets%2520reveal%2520that%2520our%2520weakly%250Asupervised%2520method%2520achieves%2520an%2520average%2520Dice%2520score%2520of%252084.90%2525%2520in%2520a%2520limited%2520data%250Asetting%252C%2520outperforming%2520existing%2520fully-supervised%2520and%2520weakly-supervised%250Aapproaches.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Minimel/box-prompt-learning-VFM.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20learning%20with%20bounding%20box%20constraints%20for%20medical%20image%0A%20%20segmentation&entry.906535625=M%C3%A9lanie%20Gaillochet%20and%20Mehrdad%20Noori%20and%20Sahar%20Dastani%20and%20Christian%20Desrosiers%20and%20Herv%C3%A9%20Lombaert&entry.1292438233=%20%20Pixel-wise%20annotations%20are%20notoriously%20labourious%20and%20costly%20to%20obtain%20in%20the%0Amedical%20domain.%20To%20mitigate%20this%20burden%2C%20weakly%20supervised%20approaches%20based%20on%0Abounding%20box%20annotations-much%20easier%20to%20acquire-offer%20a%20practical%20alternative.%0AVision%20foundation%20models%20have%20recently%20shown%20noteworthy%20segmentation%0Aperformance%20when%20provided%20with%20prompts%20such%20as%20points%20or%20bounding%20boxes.%20Prompt%0Alearning%20exploits%20these%20models%20by%20adapting%20them%20to%20downstream%20tasks%20and%0Aautomating%20segmentation%2C%20thereby%20reducing%20user%20intervention.%20However%2C%20existing%0Aprompt%20learning%20approaches%20depend%20on%20fully%20annotated%20segmentation%20masks.%20This%0Apaper%20proposes%20a%20novel%20framework%20that%20combines%20the%20representational%20power%20of%0Afoundation%20models%20with%20the%20annotation%20efficiency%20of%20weakly%20supervised%0Asegmentation.%20More%20specifically%2C%20our%20approach%20automates%20prompt%20generation%20for%0Afoundation%20models%20using%20only%20bounding%20box%20annotations.%20Our%20proposed%0Aoptimization%20scheme%20integrates%20multiple%20constraints%20derived%20from%20box%0Aannotations%20with%20pseudo-labels%20generated%20by%20the%20prompted%20foundation%20model.%0AExtensive%20experiments%20across%20multimodal%20datasets%20reveal%20that%20our%20weakly%0Asupervised%20method%20achieves%20an%20average%20Dice%20score%20of%2084.90%25%20in%20a%20limited%20data%0Asetting%2C%20outperforming%20existing%20fully-supervised%20and%20weakly-supervised%0Aapproaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Minimel/box-prompt-learning-VFM.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02743v1&entry.124074799=Read"},
{"title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models", "author": "Yongjiang Liu and Haoxi Li and Xiaosong Ma and Jie Zhang and Song Guo", "abstract": "  Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\nin handling complex reasoning tasks, but are hindered by excessive\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\nof model outputs to intervene in the internal reasoning trajectory. Combined\nwith a heterogeneous short and long reasoning dataset, the trained model\nenhances its sensitivity to task difficulty, enabling native, differentiated\nreasoning strategies across various tasks. Second, we further extend\nredundancy-hypnosis to the internal reasoning process, guiding the model to\nidentify redundant structures within the reasoning steps and generate more\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\non hard tasks) while maintaining performance stability. The resulting outputs\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\nreflection).\n", "link": "http://arxiv.org/abs/2507.02663v1", "date": "2025-07-03", "relevancy": 1.8428, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20How%20to%20Think%3A%20Mitigating%20Overthinking%20with%20Autonomous%20Difficulty%0A%20%20Cognition%20in%20Large%20Reasoning%20Models&body=Title%3A%20Think%20How%20to%20Think%3A%20Mitigating%20Overthinking%20with%20Autonomous%20Difficulty%0A%20%20Cognition%20in%20Large%20Reasoning%20Models%0AAuthor%3A%20Yongjiang%20Liu%20and%20Haoxi%20Li%20and%20Xiaosong%20Ma%20and%20Jie%20Zhang%20and%20Song%20Guo%0AAbstract%3A%20%20%20Recent%20Long%20Reasoning%20Models%28LRMs%29%20have%20demonstrated%20remarkable%20capabilities%0Ain%20handling%20complex%20reasoning%20tasks%2C%20but%20are%20hindered%20by%20excessive%0Aoverthinking.%20To%20explore%20its%20essence%2C%20our%20empirical%20analysis%20reveals%20that%20LRMs%0Aare%20primarily%20limited%20to%20recognizing%20task%20properties%20%28i.e.%2C%20difficulty%20levels%29%0Alike%20humans%20before%20solving%20the%20problem%2C%20leading%20to%20a%20one-size-fits-all%0Areasoning%20process.%20Inspired%20by%20this%2C%20a%20pressing%20and%20natural%20question%20emerges%3A%0ACan%20we%20bootstrap%20such%20ability%20to%20further%20alleviate%20the%20overthinking%20phenomenon%0Ain%20LRMs%3F%20In%20this%20paper%2C%20we%20propose%20Think-How-to-Think%20%28TH2T%29%2C%20a%20novel%20two-stage%0Afine-tuning%20strategy%20that%20progressively%20inspires%20LRMs%27%20difficulty%20cognition%20and%0Aredundancy%20cognition.%20First%2C%20we%20introduce%20difficulty-hypnosis%20in%20the%20prefixes%0Aof%20model%20outputs%20to%20intervene%20in%20the%20internal%20reasoning%20trajectory.%20Combined%0Awith%20a%20heterogeneous%20short%20and%20long%20reasoning%20dataset%2C%20the%20trained%20model%0Aenhances%20its%20sensitivity%20to%20task%20difficulty%2C%20enabling%20native%2C%20differentiated%0Areasoning%20strategies%20across%20various%20tasks.%20Second%2C%20we%20further%20extend%0Aredundancy-hypnosis%20to%20the%20internal%20reasoning%20process%2C%20guiding%20the%20model%20to%0Aidentify%20redundant%20structures%20within%20the%20reasoning%20steps%20and%20generate%20more%0Aconcise%20reasoning%20outputs.%20Experiments%20on%207B/14B/32B%20models%20demonstrate%20that%0ATH2T%20significantly%20reduces%20inference%20costs%20%28more%20than%2070%25%20on%20easy%20tasks%20and%2040%25%0Aon%20hard%20tasks%29%20while%20maintaining%20performance%20stability.%20The%20resulting%20outputs%0Aexhibit%20clear%20difficulty-aware%20capabilities%20and%20reduced%20redundancy%20%28e.g.%2C%0Areflection%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520How%2520to%2520Think%253A%2520Mitigating%2520Overthinking%2520with%2520Autonomous%2520Difficulty%250A%2520%2520Cognition%2520in%2520Large%2520Reasoning%2520Models%26entry.906535625%3DYongjiang%2520Liu%2520and%2520Haoxi%2520Li%2520and%2520Xiaosong%2520Ma%2520and%2520Jie%2520Zhang%2520and%2520Song%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520Long%2520Reasoning%2520Models%2528LRMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%250Ain%2520handling%2520complex%2520reasoning%2520tasks%252C%2520but%2520are%2520hindered%2520by%2520excessive%250Aoverthinking.%2520To%2520explore%2520its%2520essence%252C%2520our%2520empirical%2520analysis%2520reveals%2520that%2520LRMs%250Aare%2520primarily%2520limited%2520to%2520recognizing%2520task%2520properties%2520%2528i.e.%252C%2520difficulty%2520levels%2529%250Alike%2520humans%2520before%2520solving%2520the%2520problem%252C%2520leading%2520to%2520a%2520one-size-fits-all%250Areasoning%2520process.%2520Inspired%2520by%2520this%252C%2520a%2520pressing%2520and%2520natural%2520question%2520emerges%253A%250ACan%2520we%2520bootstrap%2520such%2520ability%2520to%2520further%2520alleviate%2520the%2520overthinking%2520phenomenon%250Ain%2520LRMs%253F%2520In%2520this%2520paper%252C%2520we%2520propose%2520Think-How-to-Think%2520%2528TH2T%2529%252C%2520a%2520novel%2520two-stage%250Afine-tuning%2520strategy%2520that%2520progressively%2520inspires%2520LRMs%2527%2520difficulty%2520cognition%2520and%250Aredundancy%2520cognition.%2520First%252C%2520we%2520introduce%2520difficulty-hypnosis%2520in%2520the%2520prefixes%250Aof%2520model%2520outputs%2520to%2520intervene%2520in%2520the%2520internal%2520reasoning%2520trajectory.%2520Combined%250Awith%2520a%2520heterogeneous%2520short%2520and%2520long%2520reasoning%2520dataset%252C%2520the%2520trained%2520model%250Aenhances%2520its%2520sensitivity%2520to%2520task%2520difficulty%252C%2520enabling%2520native%252C%2520differentiated%250Areasoning%2520strategies%2520across%2520various%2520tasks.%2520Second%252C%2520we%2520further%2520extend%250Aredundancy-hypnosis%2520to%2520the%2520internal%2520reasoning%2520process%252C%2520guiding%2520the%2520model%2520to%250Aidentify%2520redundant%2520structures%2520within%2520the%2520reasoning%2520steps%2520and%2520generate%2520more%250Aconcise%2520reasoning%2520outputs.%2520Experiments%2520on%25207B/14B/32B%2520models%2520demonstrate%2520that%250ATH2T%2520significantly%2520reduces%2520inference%2520costs%2520%2528more%2520than%252070%2525%2520on%2520easy%2520tasks%2520and%252040%2525%250Aon%2520hard%2520tasks%2529%2520while%2520maintaining%2520performance%2520stability.%2520The%2520resulting%2520outputs%250Aexhibit%2520clear%2520difficulty-aware%2520capabilities%2520and%2520reduced%2520redundancy%2520%2528e.g.%252C%250Areflection%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20How%20to%20Think%3A%20Mitigating%20Overthinking%20with%20Autonomous%20Difficulty%0A%20%20Cognition%20in%20Large%20Reasoning%20Models&entry.906535625=Yongjiang%20Liu%20and%20Haoxi%20Li%20and%20Xiaosong%20Ma%20and%20Jie%20Zhang%20and%20Song%20Guo&entry.1292438233=%20%20Recent%20Long%20Reasoning%20Models%28LRMs%29%20have%20demonstrated%20remarkable%20capabilities%0Ain%20handling%20complex%20reasoning%20tasks%2C%20but%20are%20hindered%20by%20excessive%0Aoverthinking.%20To%20explore%20its%20essence%2C%20our%20empirical%20analysis%20reveals%20that%20LRMs%0Aare%20primarily%20limited%20to%20recognizing%20task%20properties%20%28i.e.%2C%20difficulty%20levels%29%0Alike%20humans%20before%20solving%20the%20problem%2C%20leading%20to%20a%20one-size-fits-all%0Areasoning%20process.%20Inspired%20by%20this%2C%20a%20pressing%20and%20natural%20question%20emerges%3A%0ACan%20we%20bootstrap%20such%20ability%20to%20further%20alleviate%20the%20overthinking%20phenomenon%0Ain%20LRMs%3F%20In%20this%20paper%2C%20we%20propose%20Think-How-to-Think%20%28TH2T%29%2C%20a%20novel%20two-stage%0Afine-tuning%20strategy%20that%20progressively%20inspires%20LRMs%27%20difficulty%20cognition%20and%0Aredundancy%20cognition.%20First%2C%20we%20introduce%20difficulty-hypnosis%20in%20the%20prefixes%0Aof%20model%20outputs%20to%20intervene%20in%20the%20internal%20reasoning%20trajectory.%20Combined%0Awith%20a%20heterogeneous%20short%20and%20long%20reasoning%20dataset%2C%20the%20trained%20model%0Aenhances%20its%20sensitivity%20to%20task%20difficulty%2C%20enabling%20native%2C%20differentiated%0Areasoning%20strategies%20across%20various%20tasks.%20Second%2C%20we%20further%20extend%0Aredundancy-hypnosis%20to%20the%20internal%20reasoning%20process%2C%20guiding%20the%20model%20to%0Aidentify%20redundant%20structures%20within%20the%20reasoning%20steps%20and%20generate%20more%0Aconcise%20reasoning%20outputs.%20Experiments%20on%207B/14B/32B%20models%20demonstrate%20that%0ATH2T%20significantly%20reduces%20inference%20costs%20%28more%20than%2070%25%20on%20easy%20tasks%20and%2040%25%0Aon%20hard%20tasks%29%20while%20maintaining%20performance%20stability.%20The%20resulting%20outputs%0Aexhibit%20clear%20difficulty-aware%20capabilities%20and%20reduced%20redundancy%20%28e.g.%2C%0Areflection%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02663v1&entry.124074799=Read"},
{"title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "author": "Boyu Gou and Zanming Huang and Yuting Ning and Yu Gu and Michael Lin and Weijian Qi and Andrei Kopanev and Botao Yu and Bernal Jim\u00e9nez Guti\u00e9rrez and Yiheng Shu and Chan Hee Song and Jiaman Wu and Shijie Chen and Hanane Nour Moussa and Tianshu Zhang and Jian Xie and Yifei Li and Tianci Xue and Zeyi Liao and Kai Zhang and Boyuan Zheng and Zhaowei Cai and Viktor Rozgic and Morteza Ziyadi and Huan Sun and Yu Su", "abstract": "  Agentic search such as Deep Research systems-where agents autonomously browse\nthe web, synthesize information, and return comprehensive citation-backed\nanswers-represents a major shift in how users interact with web-scale\ninformation. While promising greater efficiency and cognitive offloading, the\ngrowing complexity and open-endedness of agentic search have outpaced existing\nevaluation benchmarks and methodologies, which largely assume short search\nhorizons and static answers. In this paper, we introduce Mind2Web 2, a\nbenchmark of 130 realistic, high-quality, and long-horizon tasks that require\nreal-time web browsing and extensive information synthesis, constructed with\nover 1000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of ten frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, highlighting its great potential. Altogether, Mind2Web\n2 provides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.\n", "link": "http://arxiv.org/abs/2506.21506v2", "date": "2025-07-03", "relevancy": 1.4417, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind2Web%202%3A%20Evaluating%20Agentic%20Search%20with%20Agent-as-a-Judge&body=Title%3A%20Mind2Web%202%3A%20Evaluating%20Agentic%20Search%20with%20Agent-as-a-Judge%0AAuthor%3A%20Boyu%20Gou%20and%20Zanming%20Huang%20and%20Yuting%20Ning%20and%20Yu%20Gu%20and%20Michael%20Lin%20and%20Weijian%20Qi%20and%20Andrei%20Kopanev%20and%20Botao%20Yu%20and%20Bernal%20Jim%C3%A9nez%20Guti%C3%A9rrez%20and%20Yiheng%20Shu%20and%20Chan%20Hee%20Song%20and%20Jiaman%20Wu%20and%20Shijie%20Chen%20and%20Hanane%20Nour%20Moussa%20and%20Tianshu%20Zhang%20and%20Jian%20Xie%20and%20Yifei%20Li%20and%20Tianci%20Xue%20and%20Zeyi%20Liao%20and%20Kai%20Zhang%20and%20Boyuan%20Zheng%20and%20Zhaowei%20Cai%20and%20Viktor%20Rozgic%20and%20Morteza%20Ziyadi%20and%20Huan%20Sun%20and%20Yu%20Su%0AAbstract%3A%20%20%20Agentic%20search%20such%20as%20Deep%20Research%20systems-where%20agents%20autonomously%20browse%0Athe%20web%2C%20synthesize%20information%2C%20and%20return%20comprehensive%20citation-backed%0Aanswers-represents%20a%20major%20shift%20in%20how%20users%20interact%20with%20web-scale%0Ainformation.%20While%20promising%20greater%20efficiency%20and%20cognitive%20offloading%2C%20the%0Agrowing%20complexity%20and%20open-endedness%20of%20agentic%20search%20have%20outpaced%20existing%0Aevaluation%20benchmarks%20and%20methodologies%2C%20which%20largely%20assume%20short%20search%0Ahorizons%20and%20static%20answers.%20In%20this%20paper%2C%20we%20introduce%20Mind2Web%202%2C%20a%0Abenchmark%20of%20130%20realistic%2C%20high-quality%2C%20and%20long-horizon%20tasks%20that%20require%0Areal-time%20web%20browsing%20and%20extensive%20information%20synthesis%2C%20constructed%20with%0Aover%201000%20hours%20of%20human%20labor.%20To%20address%20the%20challenge%20of%20evaluating%0Atime-varying%20and%20complex%20answers%2C%20we%20propose%20a%20novel%20Agent-as-a-Judge%0Aframework.%20Our%20method%20constructs%20task-specific%20judge%20agents%20based%20on%20a%0Atree-structured%20rubric%20design%20to%20automatically%20assess%20both%20answer%20correctness%0Aand%20source%20attribution.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20ten%20frontier%0Aagentic%20search%20systems%20and%20human%20performance%2C%20along%20with%20a%20detailed%20error%0Aanalysis%20to%20draw%20insights%20for%20future%20development.%20The%20best-performing%20system%2C%0AOpenAI%20Deep%20Research%2C%20can%20already%20achieve%2050-70%25%20of%20human%20performance%20while%0Aspending%20half%20the%20time%2C%20highlighting%20its%20great%20potential.%20Altogether%2C%20Mind2Web%0A2%20provides%20a%20rigorous%20foundation%20for%20developing%20and%20benchmarking%20the%20next%0Ageneration%20of%20agentic%20search%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind2Web%25202%253A%2520Evaluating%2520Agentic%2520Search%2520with%2520Agent-as-a-Judge%26entry.906535625%3DBoyu%2520Gou%2520and%2520Zanming%2520Huang%2520and%2520Yuting%2520Ning%2520and%2520Yu%2520Gu%2520and%2520Michael%2520Lin%2520and%2520Weijian%2520Qi%2520and%2520Andrei%2520Kopanev%2520and%2520Botao%2520Yu%2520and%2520Bernal%2520Jim%25C3%25A9nez%2520Guti%25C3%25A9rrez%2520and%2520Yiheng%2520Shu%2520and%2520Chan%2520Hee%2520Song%2520and%2520Jiaman%2520Wu%2520and%2520Shijie%2520Chen%2520and%2520Hanane%2520Nour%2520Moussa%2520and%2520Tianshu%2520Zhang%2520and%2520Jian%2520Xie%2520and%2520Yifei%2520Li%2520and%2520Tianci%2520Xue%2520and%2520Zeyi%2520Liao%2520and%2520Kai%2520Zhang%2520and%2520Boyuan%2520Zheng%2520and%2520Zhaowei%2520Cai%2520and%2520Viktor%2520Rozgic%2520and%2520Morteza%2520Ziyadi%2520and%2520Huan%2520Sun%2520and%2520Yu%2520Su%26entry.1292438233%3D%2520%2520Agentic%2520search%2520such%2520as%2520Deep%2520Research%2520systems-where%2520agents%2520autonomously%2520browse%250Athe%2520web%252C%2520synthesize%2520information%252C%2520and%2520return%2520comprehensive%2520citation-backed%250Aanswers-represents%2520a%2520major%2520shift%2520in%2520how%2520users%2520interact%2520with%2520web-scale%250Ainformation.%2520While%2520promising%2520greater%2520efficiency%2520and%2520cognitive%2520offloading%252C%2520the%250Agrowing%2520complexity%2520and%2520open-endedness%2520of%2520agentic%2520search%2520have%2520outpaced%2520existing%250Aevaluation%2520benchmarks%2520and%2520methodologies%252C%2520which%2520largely%2520assume%2520short%2520search%250Ahorizons%2520and%2520static%2520answers.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Mind2Web%25202%252C%2520a%250Abenchmark%2520of%2520130%2520realistic%252C%2520high-quality%252C%2520and%2520long-horizon%2520tasks%2520that%2520require%250Areal-time%2520web%2520browsing%2520and%2520extensive%2520information%2520synthesis%252C%2520constructed%2520with%250Aover%25201000%2520hours%2520of%2520human%2520labor.%2520To%2520address%2520the%2520challenge%2520of%2520evaluating%250Atime-varying%2520and%2520complex%2520answers%252C%2520we%2520propose%2520a%2520novel%2520Agent-as-a-Judge%250Aframework.%2520Our%2520method%2520constructs%2520task-specific%2520judge%2520agents%2520based%2520on%2520a%250Atree-structured%2520rubric%2520design%2520to%2520automatically%2520assess%2520both%2520answer%2520correctness%250Aand%2520source%2520attribution.%2520We%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520ten%2520frontier%250Aagentic%2520search%2520systems%2520and%2520human%2520performance%252C%2520along%2520with%2520a%2520detailed%2520error%250Aanalysis%2520to%2520draw%2520insights%2520for%2520future%2520development.%2520The%2520best-performing%2520system%252C%250AOpenAI%2520Deep%2520Research%252C%2520can%2520already%2520achieve%252050-70%2525%2520of%2520human%2520performance%2520while%250Aspending%2520half%2520the%2520time%252C%2520highlighting%2520its%2520great%2520potential.%2520Altogether%252C%2520Mind2Web%250A2%2520provides%2520a%2520rigorous%2520foundation%2520for%2520developing%2520and%2520benchmarking%2520the%2520next%250Ageneration%2520of%2520agentic%2520search%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind2Web%202%3A%20Evaluating%20Agentic%20Search%20with%20Agent-as-a-Judge&entry.906535625=Boyu%20Gou%20and%20Zanming%20Huang%20and%20Yuting%20Ning%20and%20Yu%20Gu%20and%20Michael%20Lin%20and%20Weijian%20Qi%20and%20Andrei%20Kopanev%20and%20Botao%20Yu%20and%20Bernal%20Jim%C3%A9nez%20Guti%C3%A9rrez%20and%20Yiheng%20Shu%20and%20Chan%20Hee%20Song%20and%20Jiaman%20Wu%20and%20Shijie%20Chen%20and%20Hanane%20Nour%20Moussa%20and%20Tianshu%20Zhang%20and%20Jian%20Xie%20and%20Yifei%20Li%20and%20Tianci%20Xue%20and%20Zeyi%20Liao%20and%20Kai%20Zhang%20and%20Boyuan%20Zheng%20and%20Zhaowei%20Cai%20and%20Viktor%20Rozgic%20and%20Morteza%20Ziyadi%20and%20Huan%20Sun%20and%20Yu%20Su&entry.1292438233=%20%20Agentic%20search%20such%20as%20Deep%20Research%20systems-where%20agents%20autonomously%20browse%0Athe%20web%2C%20synthesize%20information%2C%20and%20return%20comprehensive%20citation-backed%0Aanswers-represents%20a%20major%20shift%20in%20how%20users%20interact%20with%20web-scale%0Ainformation.%20While%20promising%20greater%20efficiency%20and%20cognitive%20offloading%2C%20the%0Agrowing%20complexity%20and%20open-endedness%20of%20agentic%20search%20have%20outpaced%20existing%0Aevaluation%20benchmarks%20and%20methodologies%2C%20which%20largely%20assume%20short%20search%0Ahorizons%20and%20static%20answers.%20In%20this%20paper%2C%20we%20introduce%20Mind2Web%202%2C%20a%0Abenchmark%20of%20130%20realistic%2C%20high-quality%2C%20and%20long-horizon%20tasks%20that%20require%0Areal-time%20web%20browsing%20and%20extensive%20information%20synthesis%2C%20constructed%20with%0Aover%201000%20hours%20of%20human%20labor.%20To%20address%20the%20challenge%20of%20evaluating%0Atime-varying%20and%20complex%20answers%2C%20we%20propose%20a%20novel%20Agent-as-a-Judge%0Aframework.%20Our%20method%20constructs%20task-specific%20judge%20agents%20based%20on%20a%0Atree-structured%20rubric%20design%20to%20automatically%20assess%20both%20answer%20correctness%0Aand%20source%20attribution.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20ten%20frontier%0Aagentic%20search%20systems%20and%20human%20performance%2C%20along%20with%20a%20detailed%20error%0Aanalysis%20to%20draw%20insights%20for%20future%20development.%20The%20best-performing%20system%2C%0AOpenAI%20Deep%20Research%2C%20can%20already%20achieve%2050-70%25%20of%20human%20performance%20while%0Aspending%20half%20the%20time%2C%20highlighting%20its%20great%20potential.%20Altogether%2C%20Mind2Web%0A2%20provides%20a%20rigorous%20foundation%20for%20developing%20and%20benchmarking%20the%20next%0Ageneration%20of%20agentic%20search%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21506v2&entry.124074799=Read"},
{"title": "Classification by Separating Hypersurfaces: An Entropic Approach", "author": "Argimiro Arratia and Mahmoud El Daou and Henryk Gzyl", "abstract": "  We consider the following classification problem: Given a population of\nindividuals characterized by a set of attributes represented as a vector in\n${\\mathbb R}^N$, the goal is to find a hyperplane in ${\\mathbb R}^N$ that\nseparates two sets of points corresponding to two distinct classes. This\nproblem, with a history dating back to the perceptron model, remains central to\nmachine learning. In this paper we propose a novel approach by searching for a\nvector of parameters in a bounded $N$-dimensional hypercube centered at the\norigin and a positive vector in ${\\mathbb R}^M$, obtained through the\nminimization of an entropy-based function defined over the space of unknown\nvariables. The method extends to polynomial surfaces, allowing the separation\nof data points by more complex decision boundaries. This provides a robust\nalternative to traditional linear or quadratic optimization techniques, such as\nsupport vector machines and gradient descent. Numerical experiments demonstrate\nthe efficiency and versatility of the method in handling diverse classification\ntasks, including linear and non-linear separability.\n", "link": "http://arxiv.org/abs/2507.02732v1", "date": "2025-07-03", "relevancy": 1.7504, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4583}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20by%20Separating%20Hypersurfaces%3A%20An%20Entropic%20Approach&body=Title%3A%20Classification%20by%20Separating%20Hypersurfaces%3A%20An%20Entropic%20Approach%0AAuthor%3A%20Argimiro%20Arratia%20and%20Mahmoud%20El%20Daou%20and%20Henryk%20Gzyl%0AAbstract%3A%20%20%20We%20consider%20the%20following%20classification%20problem%3A%20Given%20a%20population%20of%0Aindividuals%20characterized%20by%20a%20set%20of%20attributes%20represented%20as%20a%20vector%20in%0A%24%7B%5Cmathbb%20R%7D%5EN%24%2C%20the%20goal%20is%20to%20find%20a%20hyperplane%20in%20%24%7B%5Cmathbb%20R%7D%5EN%24%20that%0Aseparates%20two%20sets%20of%20points%20corresponding%20to%20two%20distinct%20classes.%20This%0Aproblem%2C%20with%20a%20history%20dating%20back%20to%20the%20perceptron%20model%2C%20remains%20central%20to%0Amachine%20learning.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%20by%20searching%20for%20a%0Avector%20of%20parameters%20in%20a%20bounded%20%24N%24-dimensional%20hypercube%20centered%20at%20the%0Aorigin%20and%20a%20positive%20vector%20in%20%24%7B%5Cmathbb%20R%7D%5EM%24%2C%20obtained%20through%20the%0Aminimization%20of%20an%20entropy-based%20function%20defined%20over%20the%20space%20of%20unknown%0Avariables.%20The%20method%20extends%20to%20polynomial%20surfaces%2C%20allowing%20the%20separation%0Aof%20data%20points%20by%20more%20complex%20decision%20boundaries.%20This%20provides%20a%20robust%0Aalternative%20to%20traditional%20linear%20or%20quadratic%20optimization%20techniques%2C%20such%20as%0Asupport%20vector%20machines%20and%20gradient%20descent.%20Numerical%20experiments%20demonstrate%0Athe%20efficiency%20and%20versatility%20of%20the%20method%20in%20handling%20diverse%20classification%0Atasks%2C%20including%20linear%20and%20non-linear%20separability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520by%2520Separating%2520Hypersurfaces%253A%2520An%2520Entropic%2520Approach%26entry.906535625%3DArgimiro%2520Arratia%2520and%2520Mahmoud%2520El%2520Daou%2520and%2520Henryk%2520Gzyl%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520following%2520classification%2520problem%253A%2520Given%2520a%2520population%2520of%250Aindividuals%2520characterized%2520by%2520a%2520set%2520of%2520attributes%2520represented%2520as%2520a%2520vector%2520in%250A%2524%257B%255Cmathbb%2520R%257D%255EN%2524%252C%2520the%2520goal%2520is%2520to%2520find%2520a%2520hyperplane%2520in%2520%2524%257B%255Cmathbb%2520R%257D%255EN%2524%2520that%250Aseparates%2520two%2520sets%2520of%2520points%2520corresponding%2520to%2520two%2520distinct%2520classes.%2520This%250Aproblem%252C%2520with%2520a%2520history%2520dating%2520back%2520to%2520the%2520perceptron%2520model%252C%2520remains%2520central%2520to%250Amachine%2520learning.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520approach%2520by%2520searching%2520for%2520a%250Avector%2520of%2520parameters%2520in%2520a%2520bounded%2520%2524N%2524-dimensional%2520hypercube%2520centered%2520at%2520the%250Aorigin%2520and%2520a%2520positive%2520vector%2520in%2520%2524%257B%255Cmathbb%2520R%257D%255EM%2524%252C%2520obtained%2520through%2520the%250Aminimization%2520of%2520an%2520entropy-based%2520function%2520defined%2520over%2520the%2520space%2520of%2520unknown%250Avariables.%2520The%2520method%2520extends%2520to%2520polynomial%2520surfaces%252C%2520allowing%2520the%2520separation%250Aof%2520data%2520points%2520by%2520more%2520complex%2520decision%2520boundaries.%2520This%2520provides%2520a%2520robust%250Aalternative%2520to%2520traditional%2520linear%2520or%2520quadratic%2520optimization%2520techniques%252C%2520such%2520as%250Asupport%2520vector%2520machines%2520and%2520gradient%2520descent.%2520Numerical%2520experiments%2520demonstrate%250Athe%2520efficiency%2520and%2520versatility%2520of%2520the%2520method%2520in%2520handling%2520diverse%2520classification%250Atasks%252C%2520including%2520linear%2520and%2520non-linear%2520separability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20by%20Separating%20Hypersurfaces%3A%20An%20Entropic%20Approach&entry.906535625=Argimiro%20Arratia%20and%20Mahmoud%20El%20Daou%20and%20Henryk%20Gzyl&entry.1292438233=%20%20We%20consider%20the%20following%20classification%20problem%3A%20Given%20a%20population%20of%0Aindividuals%20characterized%20by%20a%20set%20of%20attributes%20represented%20as%20a%20vector%20in%0A%24%7B%5Cmathbb%20R%7D%5EN%24%2C%20the%20goal%20is%20to%20find%20a%20hyperplane%20in%20%24%7B%5Cmathbb%20R%7D%5EN%24%20that%0Aseparates%20two%20sets%20of%20points%20corresponding%20to%20two%20distinct%20classes.%20This%0Aproblem%2C%20with%20a%20history%20dating%20back%20to%20the%20perceptron%20model%2C%20remains%20central%20to%0Amachine%20learning.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%20by%20searching%20for%20a%0Avector%20of%20parameters%20in%20a%20bounded%20%24N%24-dimensional%20hypercube%20centered%20at%20the%0Aorigin%20and%20a%20positive%20vector%20in%20%24%7B%5Cmathbb%20R%7D%5EM%24%2C%20obtained%20through%20the%0Aminimization%20of%20an%20entropy-based%20function%20defined%20over%20the%20space%20of%20unknown%0Avariables.%20The%20method%20extends%20to%20polynomial%20surfaces%2C%20allowing%20the%20separation%0Aof%20data%20points%20by%20more%20complex%20decision%20boundaries.%20This%20provides%20a%20robust%0Aalternative%20to%20traditional%20linear%20or%20quadratic%20optimization%20techniques%2C%20such%20as%0Asupport%20vector%20machines%20and%20gradient%20descent.%20Numerical%20experiments%20demonstrate%0Athe%20efficiency%20and%20versatility%20of%20the%20method%20in%20handling%20diverse%20classification%0Atasks%2C%20including%20linear%20and%20non-linear%20separability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02732v1&entry.124074799=Read"},
{"title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network\n  for Next Activity Prediction in Business Processes", "author": "Jiaxing Wang and Yifeng Yu and Jiahan Song and Bin Cao and Jing Fan and Ji Zhang", "abstract": "  Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.\n", "link": "http://arxiv.org/abs/2507.02690v1", "date": "2025-07-03", "relevancy": 1.4378, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4927}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLHGNN%3A%20Reinforcement%20Learning-driven%20Heterogeneous%20Graph%20Neural%20Network%0A%20%20for%20Next%20Activity%20Prediction%20in%20Business%20Processes&body=Title%3A%20RLHGNN%3A%20Reinforcement%20Learning-driven%20Heterogeneous%20Graph%20Neural%20Network%0A%20%20for%20Next%20Activity%20Prediction%20in%20Business%20Processes%0AAuthor%3A%20Jiaxing%20Wang%20and%20Yifeng%20Yu%20and%20Jiahan%20Song%20and%20Bin%20Cao%20and%20Jing%20Fan%20and%20Ji%20Zhang%0AAbstract%3A%20%20%20Next%20activity%20prediction%20represents%20a%20fundamental%20challenge%20for%20optimizing%0Abusiness%20processes%20in%20service-oriented%20architectures%20such%20as%20microservices%0Aenvironments%2C%20distributed%20enterprise%20systems%2C%20and%20cloud-native%20platforms%2C%20which%0Aenables%20proactive%20resource%20allocation%20and%20dynamic%20service%20composition.%20Despite%0Athe%20prevalence%20of%20sequence-based%20methods%2C%20these%20approaches%20fail%20to%20capture%0Anon-sequential%20relationships%20that%20arise%20from%20parallel%20executions%20and%0Aconditional%20dependencies.%20Even%20though%20graph-based%20approaches%20address%20structural%0Apreservation%2C%20they%20suffer%20from%20homogeneous%20representations%20and%20static%0Astructures%20that%20apply%20uniform%20modeling%20strategies%20regardless%20of%20individual%0Aprocess%20complexity%20characteristics.%20To%20address%20these%20limitations%2C%20we%20introduce%0ARLHGNN%2C%20a%20novel%20framework%20that%20transforms%20event%20logs%20into%20heterogeneous%20process%0Agraphs%20with%20three%20distinct%20edge%20types%20grounded%20in%20established%20process%20mining%0Atheory.%20Our%20approach%20creates%20four%20flexible%20graph%20structures%20by%20selectively%0Acombining%20these%20edges%20to%20accommodate%20different%20process%20complexities%2C%20and%0Aemploys%20reinforcement%20learning%20formulated%20as%20a%20Markov%20Decision%20Process%20to%0Aautomatically%20determine%20the%20optimal%20graph%20structure%20for%20each%20specific%20process%0Ainstance.%20RLHGNN%20then%20applies%20heterogeneous%20graph%20convolution%20with%0Arelation-specific%20aggregation%20strategies%20to%20effectively%20predict%20the%20next%0Aactivity.%20This%20adaptive%20methodology%20enables%20precise%20modeling%20of%20both%20sequential%0Aand%20non-sequential%20relationships%20in%20service%20interactions.%20Comprehensive%0Aevaluation%20on%20six%20real-world%20datasets%20demonstrates%20that%20RLHGNN%20consistently%0Aoutperforms%20state-of-the-art%20approaches.%20Furthermore%2C%20it%20maintains%20an%20inference%0Alatency%20of%20approximately%201%20ms%20per%20prediction%2C%20representing%20a%20highly%20practical%0Asolution%20suitable%20for%20real-time%20business%20process%20monitoring%20applications.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/Joker3993/RLHGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLHGNN%253A%2520Reinforcement%2520Learning-driven%2520Heterogeneous%2520Graph%2520Neural%2520Network%250A%2520%2520for%2520Next%2520Activity%2520Prediction%2520in%2520Business%2520Processes%26entry.906535625%3DJiaxing%2520Wang%2520and%2520Yifeng%2520Yu%2520and%2520Jiahan%2520Song%2520and%2520Bin%2520Cao%2520and%2520Jing%2520Fan%2520and%2520Ji%2520Zhang%26entry.1292438233%3D%2520%2520Next%2520activity%2520prediction%2520represents%2520a%2520fundamental%2520challenge%2520for%2520optimizing%250Abusiness%2520processes%2520in%2520service-oriented%2520architectures%2520such%2520as%2520microservices%250Aenvironments%252C%2520distributed%2520enterprise%2520systems%252C%2520and%2520cloud-native%2520platforms%252C%2520which%250Aenables%2520proactive%2520resource%2520allocation%2520and%2520dynamic%2520service%2520composition.%2520Despite%250Athe%2520prevalence%2520of%2520sequence-based%2520methods%252C%2520these%2520approaches%2520fail%2520to%2520capture%250Anon-sequential%2520relationships%2520that%2520arise%2520from%2520parallel%2520executions%2520and%250Aconditional%2520dependencies.%2520Even%2520though%2520graph-based%2520approaches%2520address%2520structural%250Apreservation%252C%2520they%2520suffer%2520from%2520homogeneous%2520representations%2520and%2520static%250Astructures%2520that%2520apply%2520uniform%2520modeling%2520strategies%2520regardless%2520of%2520individual%250Aprocess%2520complexity%2520characteristics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250ARLHGNN%252C%2520a%2520novel%2520framework%2520that%2520transforms%2520event%2520logs%2520into%2520heterogeneous%2520process%250Agraphs%2520with%2520three%2520distinct%2520edge%2520types%2520grounded%2520in%2520established%2520process%2520mining%250Atheory.%2520Our%2520approach%2520creates%2520four%2520flexible%2520graph%2520structures%2520by%2520selectively%250Acombining%2520these%2520edges%2520to%2520accommodate%2520different%2520process%2520complexities%252C%2520and%250Aemploys%2520reinforcement%2520learning%2520formulated%2520as%2520a%2520Markov%2520Decision%2520Process%2520to%250Aautomatically%2520determine%2520the%2520optimal%2520graph%2520structure%2520for%2520each%2520specific%2520process%250Ainstance.%2520RLHGNN%2520then%2520applies%2520heterogeneous%2520graph%2520convolution%2520with%250Arelation-specific%2520aggregation%2520strategies%2520to%2520effectively%2520predict%2520the%2520next%250Aactivity.%2520This%2520adaptive%2520methodology%2520enables%2520precise%2520modeling%2520of%2520both%2520sequential%250Aand%2520non-sequential%2520relationships%2520in%2520service%2520interactions.%2520Comprehensive%250Aevaluation%2520on%2520six%2520real-world%2520datasets%2520demonstrates%2520that%2520RLHGNN%2520consistently%250Aoutperforms%2520state-of-the-art%2520approaches.%2520Furthermore%252C%2520it%2520maintains%2520an%2520inference%250Alatency%2520of%2520approximately%25201%2520ms%2520per%2520prediction%252C%2520representing%2520a%2520highly%2520practical%250Asolution%2520suitable%2520for%2520real-time%2520business%2520process%2520monitoring%2520applications.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/Joker3993/RLHGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLHGNN%3A%20Reinforcement%20Learning-driven%20Heterogeneous%20Graph%20Neural%20Network%0A%20%20for%20Next%20Activity%20Prediction%20in%20Business%20Processes&entry.906535625=Jiaxing%20Wang%20and%20Yifeng%20Yu%20and%20Jiahan%20Song%20and%20Bin%20Cao%20and%20Jing%20Fan%20and%20Ji%20Zhang&entry.1292438233=%20%20Next%20activity%20prediction%20represents%20a%20fundamental%20challenge%20for%20optimizing%0Abusiness%20processes%20in%20service-oriented%20architectures%20such%20as%20microservices%0Aenvironments%2C%20distributed%20enterprise%20systems%2C%20and%20cloud-native%20platforms%2C%20which%0Aenables%20proactive%20resource%20allocation%20and%20dynamic%20service%20composition.%20Despite%0Athe%20prevalence%20of%20sequence-based%20methods%2C%20these%20approaches%20fail%20to%20capture%0Anon-sequential%20relationships%20that%20arise%20from%20parallel%20executions%20and%0Aconditional%20dependencies.%20Even%20though%20graph-based%20approaches%20address%20structural%0Apreservation%2C%20they%20suffer%20from%20homogeneous%20representations%20and%20static%0Astructures%20that%20apply%20uniform%20modeling%20strategies%20regardless%20of%20individual%0Aprocess%20complexity%20characteristics.%20To%20address%20these%20limitations%2C%20we%20introduce%0ARLHGNN%2C%20a%20novel%20framework%20that%20transforms%20event%20logs%20into%20heterogeneous%20process%0Agraphs%20with%20three%20distinct%20edge%20types%20grounded%20in%20established%20process%20mining%0Atheory.%20Our%20approach%20creates%20four%20flexible%20graph%20structures%20by%20selectively%0Acombining%20these%20edges%20to%20accommodate%20different%20process%20complexities%2C%20and%0Aemploys%20reinforcement%20learning%20formulated%20as%20a%20Markov%20Decision%20Process%20to%0Aautomatically%20determine%20the%20optimal%20graph%20structure%20for%20each%20specific%20process%0Ainstance.%20RLHGNN%20then%20applies%20heterogeneous%20graph%20convolution%20with%0Arelation-specific%20aggregation%20strategies%20to%20effectively%20predict%20the%20next%0Aactivity.%20This%20adaptive%20methodology%20enables%20precise%20modeling%20of%20both%20sequential%0Aand%20non-sequential%20relationships%20in%20service%20interactions.%20Comprehensive%0Aevaluation%20on%20six%20real-world%20datasets%20demonstrates%20that%20RLHGNN%20consistently%0Aoutperforms%20state-of-the-art%20approaches.%20Furthermore%2C%20it%20maintains%20an%20inference%0Alatency%20of%20approximately%201%20ms%20per%20prediction%2C%20representing%20a%20highly%20practical%0Asolution%20suitable%20for%20real-time%20business%20process%20monitoring%20applications.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/Joker3993/RLHGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02690v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


