<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250129.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs", "author": "Artem Sevastopolsky and Philip-William Grassal and Simon Giebenhain and ShahRukh Athar and Luisa Verdoliva and Matthias Niessner", "abstract": "  Current advances in human head modeling allow the generation of\nplausible-looking 3D head models via neural representations, such as NeRFs and\nSDFs. Nevertheless, constructing complete high-fidelity head models with\nexplicitly controlled animation remains an issue. Furthermore, completing the\nhead geometry based on a partial observation, e.g., coming from a depth sensor,\nwhile preserving a high level of detail is often problematic for the existing\nmethods. We introduce a generative model for detailed 3D head meshes on top of\nan articulated 3DMM, simultaneously allowing explicit animation and high-detail\npreservation. Our method is trained in two stages. First, we register a\nparametric head model with vertex displacements to each mesh of the recently\nintroduced NPHM dataset of accurate 3D head scans. The estimated displacements\nare baked into a hand-crafted UV layout. Second, we train a StyleGAN model to\ngeneralize over the UV maps of displacements, which we later refer to as\nHeadCraft. The decomposition of the parametric model and high-quality vertex\ndisplacements allows us to animate the model and modify the regions\nsemantically. We demonstrate the results of unconditional sampling, fitting to\na scan and editing. The project page is available at\nhttps://seva100.github.io/headcraft.\n", "link": "http://arxiv.org/abs/2312.14140v2", "date": "2025-01-31", "relevancy": 3.406, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6849}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6849}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadCraft%3A%20Modeling%20High-Detail%20Shape%20Variations%20for%20Animated%203DMMs&body=Title%3A%20HeadCraft%3A%20Modeling%20High-Detail%20Shape%20Variations%20for%20Animated%203DMMs%0AAuthor%3A%20Artem%20Sevastopolsky%20and%20Philip-William%20Grassal%20and%20Simon%20Giebenhain%20and%20ShahRukh%20Athar%20and%20Luisa%20Verdoliva%20and%20Matthias%20Niessner%0AAbstract%3A%20%20%20Current%20advances%20in%20human%20head%20modeling%20allow%20the%20generation%20of%0Aplausible-looking%203D%20head%20models%20via%20neural%20representations%2C%20such%20as%20NeRFs%20and%0ASDFs.%20Nevertheless%2C%20constructing%20complete%20high-fidelity%20head%20models%20with%0Aexplicitly%20controlled%20animation%20remains%20an%20issue.%20Furthermore%2C%20completing%20the%0Ahead%20geometry%20based%20on%20a%20partial%20observation%2C%20e.g.%2C%20coming%20from%20a%20depth%20sensor%2C%0Awhile%20preserving%20a%20high%20level%20of%20detail%20is%20often%20problematic%20for%20the%20existing%0Amethods.%20We%20introduce%20a%20generative%20model%20for%20detailed%203D%20head%20meshes%20on%20top%20of%0Aan%20articulated%203DMM%2C%20simultaneously%20allowing%20explicit%20animation%20and%20high-detail%0Apreservation.%20Our%20method%20is%20trained%20in%20two%20stages.%20First%2C%20we%20register%20a%0Aparametric%20head%20model%20with%20vertex%20displacements%20to%20each%20mesh%20of%20the%20recently%0Aintroduced%20NPHM%20dataset%20of%20accurate%203D%20head%20scans.%20The%20estimated%20displacements%0Aare%20baked%20into%20a%20hand-crafted%20UV%20layout.%20Second%2C%20we%20train%20a%20StyleGAN%20model%20to%0Ageneralize%20over%20the%20UV%20maps%20of%20displacements%2C%20which%20we%20later%20refer%20to%20as%0AHeadCraft.%20The%20decomposition%20of%20the%20parametric%20model%20and%20high-quality%20vertex%0Adisplacements%20allows%20us%20to%20animate%20the%20model%20and%20modify%20the%20regions%0Asemantically.%20We%20demonstrate%20the%20results%20of%20unconditional%20sampling%2C%20fitting%20to%0Aa%20scan%20and%20editing.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//seva100.github.io/headcraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadCraft%253A%2520Modeling%2520High-Detail%2520Shape%2520Variations%2520for%2520Animated%25203DMMs%26entry.906535625%3DArtem%2520Sevastopolsky%2520and%2520Philip-William%2520Grassal%2520and%2520Simon%2520Giebenhain%2520and%2520ShahRukh%2520Athar%2520and%2520Luisa%2520Verdoliva%2520and%2520Matthias%2520Niessner%26entry.1292438233%3D%2520%2520Current%2520advances%2520in%2520human%2520head%2520modeling%2520allow%2520the%2520generation%2520of%250Aplausible-looking%25203D%2520head%2520models%2520via%2520neural%2520representations%252C%2520such%2520as%2520NeRFs%2520and%250ASDFs.%2520Nevertheless%252C%2520constructing%2520complete%2520high-fidelity%2520head%2520models%2520with%250Aexplicitly%2520controlled%2520animation%2520remains%2520an%2520issue.%2520Furthermore%252C%2520completing%2520the%250Ahead%2520geometry%2520based%2520on%2520a%2520partial%2520observation%252C%2520e.g.%252C%2520coming%2520from%2520a%2520depth%2520sensor%252C%250Awhile%2520preserving%2520a%2520high%2520level%2520of%2520detail%2520is%2520often%2520problematic%2520for%2520the%2520existing%250Amethods.%2520We%2520introduce%2520a%2520generative%2520model%2520for%2520detailed%25203D%2520head%2520meshes%2520on%2520top%2520of%250Aan%2520articulated%25203DMM%252C%2520simultaneously%2520allowing%2520explicit%2520animation%2520and%2520high-detail%250Apreservation.%2520Our%2520method%2520is%2520trained%2520in%2520two%2520stages.%2520First%252C%2520we%2520register%2520a%250Aparametric%2520head%2520model%2520with%2520vertex%2520displacements%2520to%2520each%2520mesh%2520of%2520the%2520recently%250Aintroduced%2520NPHM%2520dataset%2520of%2520accurate%25203D%2520head%2520scans.%2520The%2520estimated%2520displacements%250Aare%2520baked%2520into%2520a%2520hand-crafted%2520UV%2520layout.%2520Second%252C%2520we%2520train%2520a%2520StyleGAN%2520model%2520to%250Ageneralize%2520over%2520the%2520UV%2520maps%2520of%2520displacements%252C%2520which%2520we%2520later%2520refer%2520to%2520as%250AHeadCraft.%2520The%2520decomposition%2520of%2520the%2520parametric%2520model%2520and%2520high-quality%2520vertex%250Adisplacements%2520allows%2520us%2520to%2520animate%2520the%2520model%2520and%2520modify%2520the%2520regions%250Asemantically.%2520We%2520demonstrate%2520the%2520results%2520of%2520unconditional%2520sampling%252C%2520fitting%2520to%250Aa%2520scan%2520and%2520editing.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//seva100.github.io/headcraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadCraft%3A%20Modeling%20High-Detail%20Shape%20Variations%20for%20Animated%203DMMs&entry.906535625=Artem%20Sevastopolsky%20and%20Philip-William%20Grassal%20and%20Simon%20Giebenhain%20and%20ShahRukh%20Athar%20and%20Luisa%20Verdoliva%20and%20Matthias%20Niessner&entry.1292438233=%20%20Current%20advances%20in%20human%20head%20modeling%20allow%20the%20generation%20of%0Aplausible-looking%203D%20head%20models%20via%20neural%20representations%2C%20such%20as%20NeRFs%20and%0ASDFs.%20Nevertheless%2C%20constructing%20complete%20high-fidelity%20head%20models%20with%0Aexplicitly%20controlled%20animation%20remains%20an%20issue.%20Furthermore%2C%20completing%20the%0Ahead%20geometry%20based%20on%20a%20partial%20observation%2C%20e.g.%2C%20coming%20from%20a%20depth%20sensor%2C%0Awhile%20preserving%20a%20high%20level%20of%20detail%20is%20often%20problematic%20for%20the%20existing%0Amethods.%20We%20introduce%20a%20generative%20model%20for%20detailed%203D%20head%20meshes%20on%20top%20of%0Aan%20articulated%203DMM%2C%20simultaneously%20allowing%20explicit%20animation%20and%20high-detail%0Apreservation.%20Our%20method%20is%20trained%20in%20two%20stages.%20First%2C%20we%20register%20a%0Aparametric%20head%20model%20with%20vertex%20displacements%20to%20each%20mesh%20of%20the%20recently%0Aintroduced%20NPHM%20dataset%20of%20accurate%203D%20head%20scans.%20The%20estimated%20displacements%0Aare%20baked%20into%20a%20hand-crafted%20UV%20layout.%20Second%2C%20we%20train%20a%20StyleGAN%20model%20to%0Ageneralize%20over%20the%20UV%20maps%20of%20displacements%2C%20which%20we%20later%20refer%20to%20as%0AHeadCraft.%20The%20decomposition%20of%20the%20parametric%20model%20and%20high-quality%20vertex%0Adisplacements%20allows%20us%20to%20animate%20the%20model%20and%20modify%20the%20regions%0Asemantically.%20We%20demonstrate%20the%20results%20of%20unconditional%20sampling%2C%20fitting%20to%0Aa%20scan%20and%20editing.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//seva100.github.io/headcraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14140v2&entry.124074799=Read"},
{"title": "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping", "author": "Yiming Huang and Beilei Cui and Long Bai and Zhen Chen and Jinlin Wu and Zhen Li and Hongbin Liu and Hongliang Ren", "abstract": "  Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com/lastbasket/Endo-2DTAM.\n", "link": "http://arxiv.org/abs/2501.19319v1", "date": "2025-01-31", "relevancy": 3.362, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7472}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6389}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Dense%20Endoscopic%20Reconstruction%20with%20Gaussian%20Splatting-driven%0A%20%20Surface%20Normal-aware%20Tracking%20and%20Mapping&body=Title%3A%20Advancing%20Dense%20Endoscopic%20Reconstruction%20with%20Gaussian%20Splatting-driven%0A%20%20Surface%20Normal-aware%20Tracking%20and%20Mapping%0AAuthor%3A%20Yiming%20Huang%20and%20Beilei%20Cui%20and%20Long%20Bai%20and%20Zhen%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Li%20and%20Hongbin%20Liu%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20essential%20for%20precise%0Asurgical%20interventions%20and%20robotic%20tasks%20in%20minimally%20invasive%20procedures.%0AWhile%20recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20improved%20SLAM%0Awith%20high-quality%20novel%20view%20synthesis%20and%20fast%20rendering%2C%20these%20systems%0Astruggle%20with%20accurate%20depth%20and%20surface%20reconstruction%20due%20to%20multi-view%0Ainconsistencies.%20Simply%20incorporating%20SLAM%20and%203DGS%20leads%20to%20mismatches%20between%0Athe%20reconstructed%20frames.%20In%20this%20work%2C%20we%20present%20Endo-2DTAM%2C%20a%20real-time%0Aendoscopic%20SLAM%20system%20with%202D%20Gaussian%20Splatting%20%282DGS%29%20to%20address%20these%0Achallenges.%20Endo-2DTAM%20incorporates%20a%20surface%20normal-aware%20pipeline%2C%20which%0Aconsists%20of%20tracking%2C%20mapping%2C%20and%20bundle%20adjustment%20modules%20for%20geometrically%0Aaccurate%20reconstruction.%20Our%20robust%20tracking%20module%20combines%20point-to-point%20and%0Apoint-to-plane%20distance%20metrics%2C%20while%20the%20mapping%20module%20utilizes%20normal%0Aconsistency%20and%20depth%20distortion%20to%20enhance%20surface%20reconstruction%20quality.%20We%0Aalso%20introduce%20a%20pose-consistent%20strategy%20for%20efficient%20and%20geometrically%0Acoherent%20keyframe%20sampling.%20Extensive%20experiments%20on%20public%20endoscopic%20datasets%0Ademonstrate%20that%20Endo-2DTAM%20achieves%20an%20RMSE%20of%20%241.87%5Cpm%200.63%24%20mm%20for%20depth%0Areconstruction%20of%20surgical%20scenes%20while%20maintaining%20computationally%20efficient%0Atracking%2C%20high-quality%20visual%20appearance%2C%20and%20real-time%20rendering.%20Our%20code%0Awill%20be%20released%20at%20github.com/lastbasket/Endo-2DTAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Dense%2520Endoscopic%2520Reconstruction%2520with%2520Gaussian%2520Splatting-driven%250A%2520%2520Surface%2520Normal-aware%2520Tracking%2520and%2520Mapping%26entry.906535625%3DYiming%2520Huang%2520and%2520Beilei%2520Cui%2520and%2520Long%2520Bai%2520and%2520Zhen%2520Chen%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Li%2520and%2520Hongbin%2520Liu%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520essential%2520for%2520precise%250Asurgical%2520interventions%2520and%2520robotic%2520tasks%2520in%2520minimally%2520invasive%2520procedures.%250AWhile%2520recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520improved%2520SLAM%250Awith%2520high-quality%2520novel%2520view%2520synthesis%2520and%2520fast%2520rendering%252C%2520these%2520systems%250Astruggle%2520with%2520accurate%2520depth%2520and%2520surface%2520reconstruction%2520due%2520to%2520multi-view%250Ainconsistencies.%2520Simply%2520incorporating%2520SLAM%2520and%25203DGS%2520leads%2520to%2520mismatches%2520between%250Athe%2520reconstructed%2520frames.%2520In%2520this%2520work%252C%2520we%2520present%2520Endo-2DTAM%252C%2520a%2520real-time%250Aendoscopic%2520SLAM%2520system%2520with%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520to%2520address%2520these%250Achallenges.%2520Endo-2DTAM%2520incorporates%2520a%2520surface%2520normal-aware%2520pipeline%252C%2520which%250Aconsists%2520of%2520tracking%252C%2520mapping%252C%2520and%2520bundle%2520adjustment%2520modules%2520for%2520geometrically%250Aaccurate%2520reconstruction.%2520Our%2520robust%2520tracking%2520module%2520combines%2520point-to-point%2520and%250Apoint-to-plane%2520distance%2520metrics%252C%2520while%2520the%2520mapping%2520module%2520utilizes%2520normal%250Aconsistency%2520and%2520depth%2520distortion%2520to%2520enhance%2520surface%2520reconstruction%2520quality.%2520We%250Aalso%2520introduce%2520a%2520pose-consistent%2520strategy%2520for%2520efficient%2520and%2520geometrically%250Acoherent%2520keyframe%2520sampling.%2520Extensive%2520experiments%2520on%2520public%2520endoscopic%2520datasets%250Ademonstrate%2520that%2520Endo-2DTAM%2520achieves%2520an%2520RMSE%2520of%2520%25241.87%255Cpm%25200.63%2524%2520mm%2520for%2520depth%250Areconstruction%2520of%2520surgical%2520scenes%2520while%2520maintaining%2520computationally%2520efficient%250Atracking%252C%2520high-quality%2520visual%2520appearance%252C%2520and%2520real-time%2520rendering.%2520Our%2520code%250Awill%2520be%2520released%2520at%2520github.com/lastbasket/Endo-2DTAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Dense%20Endoscopic%20Reconstruction%20with%20Gaussian%20Splatting-driven%0A%20%20Surface%20Normal-aware%20Tracking%20and%20Mapping&entry.906535625=Yiming%20Huang%20and%20Beilei%20Cui%20and%20Long%20Bai%20and%20Zhen%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Li%20and%20Hongbin%20Liu%20and%20Hongliang%20Ren&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20essential%20for%20precise%0Asurgical%20interventions%20and%20robotic%20tasks%20in%20minimally%20invasive%20procedures.%0AWhile%20recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20improved%20SLAM%0Awith%20high-quality%20novel%20view%20synthesis%20and%20fast%20rendering%2C%20these%20systems%0Astruggle%20with%20accurate%20depth%20and%20surface%20reconstruction%20due%20to%20multi-view%0Ainconsistencies.%20Simply%20incorporating%20SLAM%20and%203DGS%20leads%20to%20mismatches%20between%0Athe%20reconstructed%20frames.%20In%20this%20work%2C%20we%20present%20Endo-2DTAM%2C%20a%20real-time%0Aendoscopic%20SLAM%20system%20with%202D%20Gaussian%20Splatting%20%282DGS%29%20to%20address%20these%0Achallenges.%20Endo-2DTAM%20incorporates%20a%20surface%20normal-aware%20pipeline%2C%20which%0Aconsists%20of%20tracking%2C%20mapping%2C%20and%20bundle%20adjustment%20modules%20for%20geometrically%0Aaccurate%20reconstruction.%20Our%20robust%20tracking%20module%20combines%20point-to-point%20and%0Apoint-to-plane%20distance%20metrics%2C%20while%20the%20mapping%20module%20utilizes%20normal%0Aconsistency%20and%20depth%20distortion%20to%20enhance%20surface%20reconstruction%20quality.%20We%0Aalso%20introduce%20a%20pose-consistent%20strategy%20for%20efficient%20and%20geometrically%0Acoherent%20keyframe%20sampling.%20Extensive%20experiments%20on%20public%20endoscopic%20datasets%0Ademonstrate%20that%20Endo-2DTAM%20achieves%20an%20RMSE%20of%20%241.87%5Cpm%200.63%24%20mm%20for%20depth%0Areconstruction%20of%20surgical%20scenes%20while%20maintaining%20computationally%20efficient%0Atracking%2C%20high-quality%20visual%20appearance%2C%20and%20real-time%20rendering.%20Our%20code%0Awill%20be%20released%20at%20github.com/lastbasket/Endo-2DTAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19319v1&entry.124074799=Read"},
{"title": "RaySplats: Ray Tracing based Gaussian Splatting", "author": "Krzysztof Byrski and Marcin Mazur and Jacek Tabor and Tadeusz Dziarmaga and Marcin K\u0105dzio\u0142ka and Dawid Baran and Przemys\u0142aw Spurek", "abstract": "  3D Gaussian Splatting (3DGS) is a process that enables the direct creation of\n3D objects from 2D images. This representation offers numerous advantages,\nincluding rapid training and rendering. However, a significant limitation of\n3DGS is the challenge of incorporating light and shadow reflections, primarily\ndue to the utilization of rasterization rather than ray tracing for rendering.\nThis paper introduces RaySplats, a model that employs ray-tracing based\nGaussian Splatting. Rather than utilizing the projection of Gaussians, our\nmethod employs a ray-tracing mechanism, operating directly on Gaussian\nprimitives represented by confidence ellipses with RGB colors. In practice, we\ncompute the intersection between ellipses and rays to construct ray-tracing\nalgorithms, facilitating the incorporation of meshes with Gaussian Splatting\nmodels and the addition of lights, shadows, and other related effects.\n", "link": "http://arxiv.org/abs/2501.19196v1", "date": "2025-01-31", "relevancy": 3.3485, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7185}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6461}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaySplats%3A%20Ray%20Tracing%20based%20Gaussian%20Splatting&body=Title%3A%20RaySplats%3A%20Ray%20Tracing%20based%20Gaussian%20Splatting%0AAuthor%3A%20Krzysztof%20Byrski%20and%20Marcin%20Mazur%20and%20Jacek%20Tabor%20and%20Tadeusz%20Dziarmaga%20and%20Marcin%20K%C4%85dzio%C5%82ka%20and%20Dawid%20Baran%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20process%20that%20enables%20the%20direct%20creation%20of%0A3D%20objects%20from%202D%20images.%20This%20representation%20offers%20numerous%20advantages%2C%0Aincluding%20rapid%20training%20and%20rendering.%20However%2C%20a%20significant%20limitation%20of%0A3DGS%20is%20the%20challenge%20of%20incorporating%20light%20and%20shadow%20reflections%2C%20primarily%0Adue%20to%20the%20utilization%20of%20rasterization%20rather%20than%20ray%20tracing%20for%20rendering.%0AThis%20paper%20introduces%20RaySplats%2C%20a%20model%20that%20employs%20ray-tracing%20based%0AGaussian%20Splatting.%20Rather%20than%20utilizing%20the%20projection%20of%20Gaussians%2C%20our%0Amethod%20employs%20a%20ray-tracing%20mechanism%2C%20operating%20directly%20on%20Gaussian%0Aprimitives%20represented%20by%20confidence%20ellipses%20with%20RGB%20colors.%20In%20practice%2C%20we%0Acompute%20the%20intersection%20between%20ellipses%20and%20rays%20to%20construct%20ray-tracing%0Aalgorithms%2C%20facilitating%20the%20incorporation%20of%20meshes%20with%20Gaussian%20Splatting%0Amodels%20and%20the%20addition%20of%20lights%2C%20shadows%2C%20and%20other%20related%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaySplats%253A%2520Ray%2520Tracing%2520based%2520Gaussian%2520Splatting%26entry.906535625%3DKrzysztof%2520Byrski%2520and%2520Marcin%2520Mazur%2520and%2520Jacek%2520Tabor%2520and%2520Tadeusz%2520Dziarmaga%2520and%2520Marcin%2520K%25C4%2585dzio%25C5%2582ka%2520and%2520Dawid%2520Baran%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520a%2520process%2520that%2520enables%2520the%2520direct%2520creation%2520of%250A3D%2520objects%2520from%25202D%2520images.%2520This%2520representation%2520offers%2520numerous%2520advantages%252C%250Aincluding%2520rapid%2520training%2520and%2520rendering.%2520However%252C%2520a%2520significant%2520limitation%2520of%250A3DGS%2520is%2520the%2520challenge%2520of%2520incorporating%2520light%2520and%2520shadow%2520reflections%252C%2520primarily%250Adue%2520to%2520the%2520utilization%2520of%2520rasterization%2520rather%2520than%2520ray%2520tracing%2520for%2520rendering.%250AThis%2520paper%2520introduces%2520RaySplats%252C%2520a%2520model%2520that%2520employs%2520ray-tracing%2520based%250AGaussian%2520Splatting.%2520Rather%2520than%2520utilizing%2520the%2520projection%2520of%2520Gaussians%252C%2520our%250Amethod%2520employs%2520a%2520ray-tracing%2520mechanism%252C%2520operating%2520directly%2520on%2520Gaussian%250Aprimitives%2520represented%2520by%2520confidence%2520ellipses%2520with%2520RGB%2520colors.%2520In%2520practice%252C%2520we%250Acompute%2520the%2520intersection%2520between%2520ellipses%2520and%2520rays%2520to%2520construct%2520ray-tracing%250Aalgorithms%252C%2520facilitating%2520the%2520incorporation%2520of%2520meshes%2520with%2520Gaussian%2520Splatting%250Amodels%2520and%2520the%2520addition%2520of%2520lights%252C%2520shadows%252C%2520and%2520other%2520related%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaySplats%3A%20Ray%20Tracing%20based%20Gaussian%20Splatting&entry.906535625=Krzysztof%20Byrski%20and%20Marcin%20Mazur%20and%20Jacek%20Tabor%20and%20Tadeusz%20Dziarmaga%20and%20Marcin%20K%C4%85dzio%C5%82ka%20and%20Dawid%20Baran%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20process%20that%20enables%20the%20direct%20creation%20of%0A3D%20objects%20from%202D%20images.%20This%20representation%20offers%20numerous%20advantages%2C%0Aincluding%20rapid%20training%20and%20rendering.%20However%2C%20a%20significant%20limitation%20of%0A3DGS%20is%20the%20challenge%20of%20incorporating%20light%20and%20shadow%20reflections%2C%20primarily%0Adue%20to%20the%20utilization%20of%20rasterization%20rather%20than%20ray%20tracing%20for%20rendering.%0AThis%20paper%20introduces%20RaySplats%2C%20a%20model%20that%20employs%20ray-tracing%20based%0AGaussian%20Splatting.%20Rather%20than%20utilizing%20the%20projection%20of%20Gaussians%2C%20our%0Amethod%20employs%20a%20ray-tracing%20mechanism%2C%20operating%20directly%20on%20Gaussian%0Aprimitives%20represented%20by%20confidence%20ellipses%20with%20RGB%20colors.%20In%20practice%2C%20we%0Acompute%20the%20intersection%20between%20ellipses%20and%20rays%20to%20construct%20ray-tracing%0Aalgorithms%2C%20facilitating%20the%20incorporation%20of%20meshes%20with%20Gaussian%20Splatting%0Amodels%20and%20the%20addition%20of%20lights%2C%20shadows%2C%20and%20other%20related%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19196v1&entry.124074799=Read"},
{"title": "JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting", "author": "Zhoutao Sun and Xukun Shen and Yong Hu and Yuyou Zhong and Xueyang Zhou", "abstract": "  Since hands are the primary interface in daily interactions, modeling\nhigh-quality digital human hands and rendering realistic images is a critical\nresearch problem. Furthermore, considering the requirements of interactive and\nrendering applications, it is essential to achieve real-time rendering and\ndriveability of the digital model without compromising rendering quality. Thus,\nwe propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian\nSplatting (3DGS)-based hand representation that renders high-fidelity hand\nimages in real-time for various poses and characters. Distinct from existing\narticulated neural rendering techniques, we introduce a differentiable process\nfor spatial transformations based on 3D key points. This process supports\ndeformations from the canonical template to a mesh with arbitrary bone lengths\nand poses. Additionally, we propose a real-time shadow simulation method based\non per-pixel depth to simulate self-occlusion shadows caused by finger\nmovements. Finally, we embed the hand prior and propose an animatable 3DGS\nrepresentation of the hand driven solely by 3D key points. We validate the\neffectiveness of each component of our approach through comprehensive ablation\nstudies. Experimental results on public datasets demonstrate that JGHand\nachieves real-time rendering speeds with enhanced quality, surpassing\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2501.19088v1", "date": "2025-01-31", "relevancy": 3.3049, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6628}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6628}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JGHand%3A%20Joint-Driven%20Animatable%20Hand%20Avater%20via%203D%20Gaussian%20Splatting&body=Title%3A%20JGHand%3A%20Joint-Driven%20Animatable%20Hand%20Avater%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zhoutao%20Sun%20and%20Xukun%20Shen%20and%20Yong%20Hu%20and%20Yuyou%20Zhong%20and%20Xueyang%20Zhou%0AAbstract%3A%20%20%20Since%20hands%20are%20the%20primary%20interface%20in%20daily%20interactions%2C%20modeling%0Ahigh-quality%20digital%20human%20hands%20and%20rendering%20realistic%20images%20is%20a%20critical%0Aresearch%20problem.%20Furthermore%2C%20considering%20the%20requirements%20of%20interactive%20and%0Arendering%20applications%2C%20it%20is%20essential%20to%20achieve%20real-time%20rendering%20and%0Adriveability%20of%20the%20digital%20model%20without%20compromising%20rendering%20quality.%20Thus%2C%0Awe%20propose%20Jointly%203D%20Gaussian%20Hand%20%28JGHand%29%2C%20a%20novel%20joint-driven%203D%20Gaussian%0ASplatting%20%283DGS%29-based%20hand%20representation%20that%20renders%20high-fidelity%20hand%0Aimages%20in%20real-time%20for%20various%20poses%20and%20characters.%20Distinct%20from%20existing%0Aarticulated%20neural%20rendering%20techniques%2C%20we%20introduce%20a%20differentiable%20process%0Afor%20spatial%20transformations%20based%20on%203D%20key%20points.%20This%20process%20supports%0Adeformations%20from%20the%20canonical%20template%20to%20a%20mesh%20with%20arbitrary%20bone%20lengths%0Aand%20poses.%20Additionally%2C%20we%20propose%20a%20real-time%20shadow%20simulation%20method%20based%0Aon%20per-pixel%20depth%20to%20simulate%20self-occlusion%20shadows%20caused%20by%20finger%0Amovements.%20Finally%2C%20we%20embed%20the%20hand%20prior%20and%20propose%20an%20animatable%203DGS%0Arepresentation%20of%20the%20hand%20driven%20solely%20by%203D%20key%20points.%20We%20validate%20the%0Aeffectiveness%20of%20each%20component%20of%20our%20approach%20through%20comprehensive%20ablation%0Astudies.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%20JGHand%0Aachieves%20real-time%20rendering%20speeds%20with%20enhanced%20quality%2C%20surpassing%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJGHand%253A%2520Joint-Driven%2520Animatable%2520Hand%2520Avater%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZhoutao%2520Sun%2520and%2520Xukun%2520Shen%2520and%2520Yong%2520Hu%2520and%2520Yuyou%2520Zhong%2520and%2520Xueyang%2520Zhou%26entry.1292438233%3D%2520%2520Since%2520hands%2520are%2520the%2520primary%2520interface%2520in%2520daily%2520interactions%252C%2520modeling%250Ahigh-quality%2520digital%2520human%2520hands%2520and%2520rendering%2520realistic%2520images%2520is%2520a%2520critical%250Aresearch%2520problem.%2520Furthermore%252C%2520considering%2520the%2520requirements%2520of%2520interactive%2520and%250Arendering%2520applications%252C%2520it%2520is%2520essential%2520to%2520achieve%2520real-time%2520rendering%2520and%250Adriveability%2520of%2520the%2520digital%2520model%2520without%2520compromising%2520rendering%2520quality.%2520Thus%252C%250Awe%2520propose%2520Jointly%25203D%2520Gaussian%2520Hand%2520%2528JGHand%2529%252C%2520a%2520novel%2520joint-driven%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529-based%2520hand%2520representation%2520that%2520renders%2520high-fidelity%2520hand%250Aimages%2520in%2520real-time%2520for%2520various%2520poses%2520and%2520characters.%2520Distinct%2520from%2520existing%250Aarticulated%2520neural%2520rendering%2520techniques%252C%2520we%2520introduce%2520a%2520differentiable%2520process%250Afor%2520spatial%2520transformations%2520based%2520on%25203D%2520key%2520points.%2520This%2520process%2520supports%250Adeformations%2520from%2520the%2520canonical%2520template%2520to%2520a%2520mesh%2520with%2520arbitrary%2520bone%2520lengths%250Aand%2520poses.%2520Additionally%252C%2520we%2520propose%2520a%2520real-time%2520shadow%2520simulation%2520method%2520based%250Aon%2520per-pixel%2520depth%2520to%2520simulate%2520self-occlusion%2520shadows%2520caused%2520by%2520finger%250Amovements.%2520Finally%252C%2520we%2520embed%2520the%2520hand%2520prior%2520and%2520propose%2520an%2520animatable%25203DGS%250Arepresentation%2520of%2520the%2520hand%2520driven%2520solely%2520by%25203D%2520key%2520points.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520each%2520component%2520of%2520our%2520approach%2520through%2520comprehensive%2520ablation%250Astudies.%2520Experimental%2520results%2520on%2520public%2520datasets%2520demonstrate%2520that%2520JGHand%250Aachieves%2520real-time%2520rendering%2520speeds%2520with%2520enhanced%2520quality%252C%2520surpassing%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JGHand%3A%20Joint-Driven%20Animatable%20Hand%20Avater%20via%203D%20Gaussian%20Splatting&entry.906535625=Zhoutao%20Sun%20and%20Xukun%20Shen%20and%20Yong%20Hu%20and%20Yuyou%20Zhong%20and%20Xueyang%20Zhou&entry.1292438233=%20%20Since%20hands%20are%20the%20primary%20interface%20in%20daily%20interactions%2C%20modeling%0Ahigh-quality%20digital%20human%20hands%20and%20rendering%20realistic%20images%20is%20a%20critical%0Aresearch%20problem.%20Furthermore%2C%20considering%20the%20requirements%20of%20interactive%20and%0Arendering%20applications%2C%20it%20is%20essential%20to%20achieve%20real-time%20rendering%20and%0Adriveability%20of%20the%20digital%20model%20without%20compromising%20rendering%20quality.%20Thus%2C%0Awe%20propose%20Jointly%203D%20Gaussian%20Hand%20%28JGHand%29%2C%20a%20novel%20joint-driven%203D%20Gaussian%0ASplatting%20%283DGS%29-based%20hand%20representation%20that%20renders%20high-fidelity%20hand%0Aimages%20in%20real-time%20for%20various%20poses%20and%20characters.%20Distinct%20from%20existing%0Aarticulated%20neural%20rendering%20techniques%2C%20we%20introduce%20a%20differentiable%20process%0Afor%20spatial%20transformations%20based%20on%203D%20key%20points.%20This%20process%20supports%0Adeformations%20from%20the%20canonical%20template%20to%20a%20mesh%20with%20arbitrary%20bone%20lengths%0Aand%20poses.%20Additionally%2C%20we%20propose%20a%20real-time%20shadow%20simulation%20method%20based%0Aon%20per-pixel%20depth%20to%20simulate%20self-occlusion%20shadows%20caused%20by%20finger%0Amovements.%20Finally%2C%20we%20embed%20the%20hand%20prior%20and%20propose%20an%20animatable%203DGS%0Arepresentation%20of%20the%20hand%20driven%20solely%20by%203D%20key%20points.%20We%20validate%20the%0Aeffectiveness%20of%20each%20component%20of%20our%20approach%20through%20comprehensive%20ablation%0Astudies.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%20JGHand%0Aachieves%20real-time%20rendering%20speeds%20with%20enhanced%20quality%2C%20surpassing%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19088v1&entry.124074799=Read"},
{"title": "VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting", "author": "Mateusz Nowak and Wojciech Jarosz and Peter Chin", "abstract": "  Reconstructing a 3D scene from images is challenging due to the different\nways light interacts with surfaces depending on the viewer's position and the\nsurface's material. In classical computer graphics, materials can be classified\nas diffuse or specular, interacting with light differently. The standard 3D\nGaussian Splatting model struggles to represent view-dependent content, since\nit cannot differentiate an object within the scene from the light interacting\nwith its specular surfaces, which produce highlights or reflections. In this\npaper, we propose to extend the 3D Gaussian Splatting model by introducing an\nadditional symmetric matrix to enhance the opacity representation of each 3D\nGaussian. This improvement allows certain Gaussians to be suppressed based on\nthe viewer's perspective, resulting in a more accurate representation of\nview-dependent reflections and specular highlights without compromising the\nscene's integrity. By allowing the opacity to be view dependent, our enhanced\nmodel achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep\nBlending, and Nerf-Synthetic datasets without a significant loss in rendering\nspeed, achieving >60FPS, and only incurring a minimal increase in memory used.\n", "link": "http://arxiv.org/abs/2501.17978v2", "date": "2025-01-31", "relevancy": 3.2868, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6762}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.661}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoD-3DGS%3A%20View-opacity-Dependent%203D%20Gaussian%20Splatting&body=Title%3A%20VoD-3DGS%3A%20View-opacity-Dependent%203D%20Gaussian%20Splatting%0AAuthor%3A%20Mateusz%20Nowak%20and%20Wojciech%20Jarosz%20and%20Peter%20Chin%0AAbstract%3A%20%20%20Reconstructing%20a%203D%20scene%20from%20images%20is%20challenging%20due%20to%20the%20different%0Aways%20light%20interacts%20with%20surfaces%20depending%20on%20the%20viewer%27s%20position%20and%20the%0Asurface%27s%20material.%20In%20classical%20computer%20graphics%2C%20materials%20can%20be%20classified%0Aas%20diffuse%20or%20specular%2C%20interacting%20with%20light%20differently.%20The%20standard%203D%0AGaussian%20Splatting%20model%20struggles%20to%20represent%20view-dependent%20content%2C%20since%0Ait%20cannot%20differentiate%20an%20object%20within%20the%20scene%20from%20the%20light%20interacting%0Awith%20its%20specular%20surfaces%2C%20which%20produce%20highlights%20or%20reflections.%20In%20this%0Apaper%2C%20we%20propose%20to%20extend%20the%203D%20Gaussian%20Splatting%20model%20by%20introducing%20an%0Aadditional%20symmetric%20matrix%20to%20enhance%20the%20opacity%20representation%20of%20each%203D%0AGaussian.%20This%20improvement%20allows%20certain%20Gaussians%20to%20be%20suppressed%20based%20on%0Athe%20viewer%27s%20perspective%2C%20resulting%20in%20a%20more%20accurate%20representation%20of%0Aview-dependent%20reflections%20and%20specular%20highlights%20without%20compromising%20the%0Ascene%27s%20integrity.%20By%20allowing%20the%20opacity%20to%20be%20view%20dependent%2C%20our%20enhanced%0Amodel%20achieves%20state-of-the-art%20performance%20on%20Mip-Nerf%2C%20Tanks%26Temples%2C%20Deep%0ABlending%2C%20and%20Nerf-Synthetic%20datasets%20without%20a%20significant%20loss%20in%20rendering%0Aspeed%2C%20achieving%20%3E60FPS%2C%20and%20only%20incurring%20a%20minimal%20increase%20in%20memory%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoD-3DGS%253A%2520View-opacity-Dependent%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DMateusz%2520Nowak%2520and%2520Wojciech%2520Jarosz%2520and%2520Peter%2520Chin%26entry.1292438233%3D%2520%2520Reconstructing%2520a%25203D%2520scene%2520from%2520images%2520is%2520challenging%2520due%2520to%2520the%2520different%250Aways%2520light%2520interacts%2520with%2520surfaces%2520depending%2520on%2520the%2520viewer%2527s%2520position%2520and%2520the%250Asurface%2527s%2520material.%2520In%2520classical%2520computer%2520graphics%252C%2520materials%2520can%2520be%2520classified%250Aas%2520diffuse%2520or%2520specular%252C%2520interacting%2520with%2520light%2520differently.%2520The%2520standard%25203D%250AGaussian%2520Splatting%2520model%2520struggles%2520to%2520represent%2520view-dependent%2520content%252C%2520since%250Ait%2520cannot%2520differentiate%2520an%2520object%2520within%2520the%2520scene%2520from%2520the%2520light%2520interacting%250Awith%2520its%2520specular%2520surfaces%252C%2520which%2520produce%2520highlights%2520or%2520reflections.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520to%2520extend%2520the%25203D%2520Gaussian%2520Splatting%2520model%2520by%2520introducing%2520an%250Aadditional%2520symmetric%2520matrix%2520to%2520enhance%2520the%2520opacity%2520representation%2520of%2520each%25203D%250AGaussian.%2520This%2520improvement%2520allows%2520certain%2520Gaussians%2520to%2520be%2520suppressed%2520based%2520on%250Athe%2520viewer%2527s%2520perspective%252C%2520resulting%2520in%2520a%2520more%2520accurate%2520representation%2520of%250Aview-dependent%2520reflections%2520and%2520specular%2520highlights%2520without%2520compromising%2520the%250Ascene%2527s%2520integrity.%2520By%2520allowing%2520the%2520opacity%2520to%2520be%2520view%2520dependent%252C%2520our%2520enhanced%250Amodel%2520achieves%2520state-of-the-art%2520performance%2520on%2520Mip-Nerf%252C%2520Tanks%2526Temples%252C%2520Deep%250ABlending%252C%2520and%2520Nerf-Synthetic%2520datasets%2520without%2520a%2520significant%2520loss%2520in%2520rendering%250Aspeed%252C%2520achieving%2520%253E60FPS%252C%2520and%2520only%2520incurring%2520a%2520minimal%2520increase%2520in%2520memory%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoD-3DGS%3A%20View-opacity-Dependent%203D%20Gaussian%20Splatting&entry.906535625=Mateusz%20Nowak%20and%20Wojciech%20Jarosz%20and%20Peter%20Chin&entry.1292438233=%20%20Reconstructing%20a%203D%20scene%20from%20images%20is%20challenging%20due%20to%20the%20different%0Aways%20light%20interacts%20with%20surfaces%20depending%20on%20the%20viewer%27s%20position%20and%20the%0Asurface%27s%20material.%20In%20classical%20computer%20graphics%2C%20materials%20can%20be%20classified%0Aas%20diffuse%20or%20specular%2C%20interacting%20with%20light%20differently.%20The%20standard%203D%0AGaussian%20Splatting%20model%20struggles%20to%20represent%20view-dependent%20content%2C%20since%0Ait%20cannot%20differentiate%20an%20object%20within%20the%20scene%20from%20the%20light%20interacting%0Awith%20its%20specular%20surfaces%2C%20which%20produce%20highlights%20or%20reflections.%20In%20this%0Apaper%2C%20we%20propose%20to%20extend%20the%203D%20Gaussian%20Splatting%20model%20by%20introducing%20an%0Aadditional%20symmetric%20matrix%20to%20enhance%20the%20opacity%20representation%20of%20each%203D%0AGaussian.%20This%20improvement%20allows%20certain%20Gaussians%20to%20be%20suppressed%20based%20on%0Athe%20viewer%27s%20perspective%2C%20resulting%20in%20a%20more%20accurate%20representation%20of%0Aview-dependent%20reflections%20and%20specular%20highlights%20without%20compromising%20the%0Ascene%27s%20integrity.%20By%20allowing%20the%20opacity%20to%20be%20view%20dependent%2C%20our%20enhanced%0Amodel%20achieves%20state-of-the-art%20performance%20on%20Mip-Nerf%2C%20Tanks%26Temples%2C%20Deep%0ABlending%2C%20and%20Nerf-Synthetic%20datasets%20without%20a%20significant%20loss%20in%20rendering%0Aspeed%2C%20achieving%20%3E60FPS%2C%20and%20only%20incurring%20a%20minimal%20increase%20in%20memory%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17978v2&entry.124074799=Read"},
{"title": "Consistency Diffusion Models for Single-Image 3D Reconstruction with\n  Priors", "author": "Chenru Jiang and Chengrui Zhang and Xi Yang and Jie Sun and Yifei Zhang and Bin Dong and Kaizhu Huang", "abstract": "  This paper delves into the study of 3D point cloud reconstruction from a\nsingle image. Our objective is to develop the Consistency Diffusion Model,\nexploring synergistic 2D and 3D priors in the Bayesian framework to ensure\nsuperior consistency in the reconstruction process, a challenging yet critical\nrequirement in this field. Specifically, we introduce a pioneering training\nframework under diffusion models that brings two key innovations. First, we\nconvert 3D structural priors derived from the initial 3D point cloud as a bound\nterm to increase evidence in the variational Bayesian framework, leveraging\nthese robust intrinsic priors to tightly govern the diffusion training process\nand bolster consistency in reconstruction. Second, we extract and incorporate\n2D priors from the single input image, projecting them onto the 3D point cloud\nto enrich the guidance for diffusion training. Our framework not only sidesteps\npotential model learning shifts that may arise from directly imposing\nadditional constraints during training but also precisely transposes the 2D\npriors into the 3D domain. Extensive experimental evaluations reveal that our\napproach sets new benchmarks in both synthetic and real-world datasets. The\ncode is included with the submission.\n", "link": "http://arxiv.org/abs/2501.16737v2", "date": "2025-01-31", "relevancy": 3.1439, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6351}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6351}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20Diffusion%20Models%20for%20Single-Image%203D%20Reconstruction%20with%0A%20%20Priors&body=Title%3A%20Consistency%20Diffusion%20Models%20for%20Single-Image%203D%20Reconstruction%20with%0A%20%20Priors%0AAuthor%3A%20Chenru%20Jiang%20and%20Chengrui%20Zhang%20and%20Xi%20Yang%20and%20Jie%20Sun%20and%20Yifei%20Zhang%20and%20Bin%20Dong%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20This%20paper%20delves%20into%20the%20study%20of%203D%20point%20cloud%20reconstruction%20from%20a%0Asingle%20image.%20Our%20objective%20is%20to%20develop%20the%20Consistency%20Diffusion%20Model%2C%0Aexploring%20synergistic%202D%20and%203D%20priors%20in%20the%20Bayesian%20framework%20to%20ensure%0Asuperior%20consistency%20in%20the%20reconstruction%20process%2C%20a%20challenging%20yet%20critical%0Arequirement%20in%20this%20field.%20Specifically%2C%20we%20introduce%20a%20pioneering%20training%0Aframework%20under%20diffusion%20models%20that%20brings%20two%20key%20innovations.%20First%2C%20we%0Aconvert%203D%20structural%20priors%20derived%20from%20the%20initial%203D%20point%20cloud%20as%20a%20bound%0Aterm%20to%20increase%20evidence%20in%20the%20variational%20Bayesian%20framework%2C%20leveraging%0Athese%20robust%20intrinsic%20priors%20to%20tightly%20govern%20the%20diffusion%20training%20process%0Aand%20bolster%20consistency%20in%20reconstruction.%20Second%2C%20we%20extract%20and%20incorporate%0A2D%20priors%20from%20the%20single%20input%20image%2C%20projecting%20them%20onto%20the%203D%20point%20cloud%0Ato%20enrich%20the%20guidance%20for%20diffusion%20training.%20Our%20framework%20not%20only%20sidesteps%0Apotential%20model%20learning%20shifts%20that%20may%20arise%20from%20directly%20imposing%0Aadditional%20constraints%20during%20training%20but%20also%20precisely%20transposes%20the%202D%0Apriors%20into%20the%203D%20domain.%20Extensive%20experimental%20evaluations%20reveal%20that%20our%0Aapproach%20sets%20new%20benchmarks%20in%20both%20synthetic%20and%20real-world%20datasets.%20The%0Acode%20is%20included%20with%20the%20submission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520Diffusion%2520Models%2520for%2520Single-Image%25203D%2520Reconstruction%2520with%250A%2520%2520Priors%26entry.906535625%3DChenru%2520Jiang%2520and%2520Chengrui%2520Zhang%2520and%2520Xi%2520Yang%2520and%2520Jie%2520Sun%2520and%2520Yifei%2520Zhang%2520and%2520Bin%2520Dong%2520and%2520Kaizhu%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520delves%2520into%2520the%2520study%2520of%25203D%2520point%2520cloud%2520reconstruction%2520from%2520a%250Asingle%2520image.%2520Our%2520objective%2520is%2520to%2520develop%2520the%2520Consistency%2520Diffusion%2520Model%252C%250Aexploring%2520synergistic%25202D%2520and%25203D%2520priors%2520in%2520the%2520Bayesian%2520framework%2520to%2520ensure%250Asuperior%2520consistency%2520in%2520the%2520reconstruction%2520process%252C%2520a%2520challenging%2520yet%2520critical%250Arequirement%2520in%2520this%2520field.%2520Specifically%252C%2520we%2520introduce%2520a%2520pioneering%2520training%250Aframework%2520under%2520diffusion%2520models%2520that%2520brings%2520two%2520key%2520innovations.%2520First%252C%2520we%250Aconvert%25203D%2520structural%2520priors%2520derived%2520from%2520the%2520initial%25203D%2520point%2520cloud%2520as%2520a%2520bound%250Aterm%2520to%2520increase%2520evidence%2520in%2520the%2520variational%2520Bayesian%2520framework%252C%2520leveraging%250Athese%2520robust%2520intrinsic%2520priors%2520to%2520tightly%2520govern%2520the%2520diffusion%2520training%2520process%250Aand%2520bolster%2520consistency%2520in%2520reconstruction.%2520Second%252C%2520we%2520extract%2520and%2520incorporate%250A2D%2520priors%2520from%2520the%2520single%2520input%2520image%252C%2520projecting%2520them%2520onto%2520the%25203D%2520point%2520cloud%250Ato%2520enrich%2520the%2520guidance%2520for%2520diffusion%2520training.%2520Our%2520framework%2520not%2520only%2520sidesteps%250Apotential%2520model%2520learning%2520shifts%2520that%2520may%2520arise%2520from%2520directly%2520imposing%250Aadditional%2520constraints%2520during%2520training%2520but%2520also%2520precisely%2520transposes%2520the%25202D%250Apriors%2520into%2520the%25203D%2520domain.%2520Extensive%2520experimental%2520evaluations%2520reveal%2520that%2520our%250Aapproach%2520sets%2520new%2520benchmarks%2520in%2520both%2520synthetic%2520and%2520real-world%2520datasets.%2520The%250Acode%2520is%2520included%2520with%2520the%2520submission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20Diffusion%20Models%20for%20Single-Image%203D%20Reconstruction%20with%0A%20%20Priors&entry.906535625=Chenru%20Jiang%20and%20Chengrui%20Zhang%20and%20Xi%20Yang%20and%20Jie%20Sun%20and%20Yifei%20Zhang%20and%20Bin%20Dong%20and%20Kaizhu%20Huang&entry.1292438233=%20%20This%20paper%20delves%20into%20the%20study%20of%203D%20point%20cloud%20reconstruction%20from%20a%0Asingle%20image.%20Our%20objective%20is%20to%20develop%20the%20Consistency%20Diffusion%20Model%2C%0Aexploring%20synergistic%202D%20and%203D%20priors%20in%20the%20Bayesian%20framework%20to%20ensure%0Asuperior%20consistency%20in%20the%20reconstruction%20process%2C%20a%20challenging%20yet%20critical%0Arequirement%20in%20this%20field.%20Specifically%2C%20we%20introduce%20a%20pioneering%20training%0Aframework%20under%20diffusion%20models%20that%20brings%20two%20key%20innovations.%20First%2C%20we%0Aconvert%203D%20structural%20priors%20derived%20from%20the%20initial%203D%20point%20cloud%20as%20a%20bound%0Aterm%20to%20increase%20evidence%20in%20the%20variational%20Bayesian%20framework%2C%20leveraging%0Athese%20robust%20intrinsic%20priors%20to%20tightly%20govern%20the%20diffusion%20training%20process%0Aand%20bolster%20consistency%20in%20reconstruction.%20Second%2C%20we%20extract%20and%20incorporate%0A2D%20priors%20from%20the%20single%20input%20image%2C%20projecting%20them%20onto%20the%203D%20point%20cloud%0Ato%20enrich%20the%20guidance%20for%20diffusion%20training.%20Our%20framework%20not%20only%20sidesteps%0Apotential%20model%20learning%20shifts%20that%20may%20arise%20from%20directly%20imposing%0Aadditional%20constraints%20during%20training%20but%20also%20precisely%20transposes%20the%202D%0Apriors%20into%20the%203D%20domain.%20Extensive%20experimental%20evaluations%20reveal%20that%20our%0Aapproach%20sets%20new%20benchmarks%20in%20both%20synthetic%20and%20real-world%20datasets.%20The%0Acode%20is%20included%20with%20the%20submission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16737v2&entry.124074799=Read"},
{"title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution", "author": "Zuyan Liu and Yuhao Dong and Ziwei Liu and Winston Hu and Jiwen Lu and Yongming Rao", "abstract": "  Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.\n", "link": "http://arxiv.org/abs/2409.12961v3", "date": "2025-01-31", "relevancy": 3.0425, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution&body=Title%3A%20Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution%0AAuthor%3A%20Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao%0AAbstract%3A%20%20%20Visual%20data%20comes%20in%20various%20forms%2C%20ranging%20from%20small%20icons%20of%20just%20a%20few%0Apixels%20to%20long%20videos%20spanning%20hours.%20Existing%20multi-modal%20LLMs%20usually%0Astandardize%20these%20diverse%20visual%20inputs%20to%20a%20fixed%20resolution%20for%20visual%0Aencoders%20and%20yield%20similar%20numbers%20of%20tokens%20for%20LLMs.%20This%20approach%20is%0Anon-optimal%20for%20multimodal%20understanding%20and%20inefficient%20for%20processing%20inputs%0Awith%20long%20and%20short%20visual%20contents.%20To%20solve%20the%20problem%2C%20we%20propose%20Oryx%2C%20a%0Aunified%20multimodal%20architecture%20for%20the%20spatial-temporal%20understanding%20of%0Aimages%2C%20videos%2C%20and%20multi-view%203D%20scenes.%20Oryx%20offers%20an%20on-demand%20solution%20to%0Aseamlessly%20and%20efficiently%20process%20visual%20inputs%20with%20arbitrary%20spatial%20sizes%0Aand%20temporal%20lengths%20through%20two%20core%20innovations%3A%201%29%20a%20pre-trained%20OryxViT%0Amodel%20that%20can%20encode%20images%20at%20any%20resolution%20into%20LLM-friendly%20visual%0Arepresentations%3B%202%29%20a%20dynamic%20compressor%20module%20that%20supports%201x%20to%2016x%0Acompression%20on%20visual%20tokens%20by%20request.%20These%20design%20features%20enable%20Oryx%20to%0Aaccommodate%20extremely%20long%20visual%20contexts%2C%20such%20as%20videos%2C%20with%20lower%0Aresolution%20and%20high%20compression%20while%20maintaining%20high%20recognition%20precision%0Afor%20tasks%20like%20document%20understanding%20with%20native%20resolution%20and%20no%0Acompression.%20Beyond%20the%20architectural%20improvements%2C%20enhanced%20data%20curation%20and%0Aspecialized%20training%20on%20long-context%20retrieval%20and%20spatial-aware%20data%20help%20Oryx%0Aachieve%20strong%20capabilities%20in%20image%2C%20video%2C%20and%203D%20multimodal%20understanding%0Asimultaneously.%20Our%20work%20is%20open-sourced%20at%20https%3A//github.com/Oryx-mllm/Oryx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12961v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOryx%2520MLLM%253A%2520On-Demand%2520Spatial-Temporal%2520Understanding%2520at%2520Arbitrary%250A%2520%2520Resolution%26entry.906535625%3DZuyan%2520Liu%2520and%2520Yuhao%2520Dong%2520and%2520Ziwei%2520Liu%2520and%2520Winston%2520Hu%2520and%2520Jiwen%2520Lu%2520and%2520Yongming%2520Rao%26entry.1292438233%3D%2520%2520Visual%2520data%2520comes%2520in%2520various%2520forms%252C%2520ranging%2520from%2520small%2520icons%2520of%2520just%2520a%2520few%250Apixels%2520to%2520long%2520videos%2520spanning%2520hours.%2520Existing%2520multi-modal%2520LLMs%2520usually%250Astandardize%2520these%2520diverse%2520visual%2520inputs%2520to%2520a%2520fixed%2520resolution%2520for%2520visual%250Aencoders%2520and%2520yield%2520similar%2520numbers%2520of%2520tokens%2520for%2520LLMs.%2520This%2520approach%2520is%250Anon-optimal%2520for%2520multimodal%2520understanding%2520and%2520inefficient%2520for%2520processing%2520inputs%250Awith%2520long%2520and%2520short%2520visual%2520contents.%2520To%2520solve%2520the%2520problem%252C%2520we%2520propose%2520Oryx%252C%2520a%250Aunified%2520multimodal%2520architecture%2520for%2520the%2520spatial-temporal%2520understanding%2520of%250Aimages%252C%2520videos%252C%2520and%2520multi-view%25203D%2520scenes.%2520Oryx%2520offers%2520an%2520on-demand%2520solution%2520to%250Aseamlessly%2520and%2520efficiently%2520process%2520visual%2520inputs%2520with%2520arbitrary%2520spatial%2520sizes%250Aand%2520temporal%2520lengths%2520through%2520two%2520core%2520innovations%253A%25201%2529%2520a%2520pre-trained%2520OryxViT%250Amodel%2520that%2520can%2520encode%2520images%2520at%2520any%2520resolution%2520into%2520LLM-friendly%2520visual%250Arepresentations%253B%25202%2529%2520a%2520dynamic%2520compressor%2520module%2520that%2520supports%25201x%2520to%252016x%250Acompression%2520on%2520visual%2520tokens%2520by%2520request.%2520These%2520design%2520features%2520enable%2520Oryx%2520to%250Aaccommodate%2520extremely%2520long%2520visual%2520contexts%252C%2520such%2520as%2520videos%252C%2520with%2520lower%250Aresolution%2520and%2520high%2520compression%2520while%2520maintaining%2520high%2520recognition%2520precision%250Afor%2520tasks%2520like%2520document%2520understanding%2520with%2520native%2520resolution%2520and%2520no%250Acompression.%2520Beyond%2520the%2520architectural%2520improvements%252C%2520enhanced%2520data%2520curation%2520and%250Aspecialized%2520training%2520on%2520long-context%2520retrieval%2520and%2520spatial-aware%2520data%2520help%2520Oryx%250Aachieve%2520strong%2520capabilities%2520in%2520image%252C%2520video%252C%2520and%25203D%2520multimodal%2520understanding%250Asimultaneously.%2520Our%2520work%2520is%2520open-sourced%2520at%2520https%253A//github.com/Oryx-mllm/Oryx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12961v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution&entry.906535625=Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao&entry.1292438233=%20%20Visual%20data%20comes%20in%20various%20forms%2C%20ranging%20from%20small%20icons%20of%20just%20a%20few%0Apixels%20to%20long%20videos%20spanning%20hours.%20Existing%20multi-modal%20LLMs%20usually%0Astandardize%20these%20diverse%20visual%20inputs%20to%20a%20fixed%20resolution%20for%20visual%0Aencoders%20and%20yield%20similar%20numbers%20of%20tokens%20for%20LLMs.%20This%20approach%20is%0Anon-optimal%20for%20multimodal%20understanding%20and%20inefficient%20for%20processing%20inputs%0Awith%20long%20and%20short%20visual%20contents.%20To%20solve%20the%20problem%2C%20we%20propose%20Oryx%2C%20a%0Aunified%20multimodal%20architecture%20for%20the%20spatial-temporal%20understanding%20of%0Aimages%2C%20videos%2C%20and%20multi-view%203D%20scenes.%20Oryx%20offers%20an%20on-demand%20solution%20to%0Aseamlessly%20and%20efficiently%20process%20visual%20inputs%20with%20arbitrary%20spatial%20sizes%0Aand%20temporal%20lengths%20through%20two%20core%20innovations%3A%201%29%20a%20pre-trained%20OryxViT%0Amodel%20that%20can%20encode%20images%20at%20any%20resolution%20into%20LLM-friendly%20visual%0Arepresentations%3B%202%29%20a%20dynamic%20compressor%20module%20that%20supports%201x%20to%2016x%0Acompression%20on%20visual%20tokens%20by%20request.%20These%20design%20features%20enable%20Oryx%20to%0Aaccommodate%20extremely%20long%20visual%20contexts%2C%20such%20as%20videos%2C%20with%20lower%0Aresolution%20and%20high%20compression%20while%20maintaining%20high%20recognition%20precision%0Afor%20tasks%20like%20document%20understanding%20with%20native%20resolution%20and%20no%0Acompression.%20Beyond%20the%20architectural%20improvements%2C%20enhanced%20data%20curation%20and%0Aspecialized%20training%20on%20long-context%20retrieval%20and%20spatial-aware%20data%20help%20Oryx%0Aachieve%20strong%20capabilities%20in%20image%2C%20video%2C%20and%203D%20multimodal%20understanding%0Asimultaneously.%20Our%20work%20is%20open-sourced%20at%20https%3A//github.com/Oryx-mllm/Oryx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12961v3&entry.124074799=Read"},
{"title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs", "author": "Yikang Zhou and Tao Zhang and Shilin Xu and Shihao Chen and Qianyu Zhou and Yunhai Tong and Shunping Ji and Jiangning Zhang and Xiangtai Li and Lu Qi", "abstract": "  Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.\n", "link": "http://arxiv.org/abs/2501.04670v2", "date": "2025-01-31", "relevancy": 2.9489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs&body=Title%3A%20Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Shilin%20Xu%20and%20Shihao%20Chen%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong%20and%20Shunping%20Ji%20and%20Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20models%20have%20shown%20a%20strong%20ability%20in%0Avisual%20perception%2C%20reasoning%20abilities%2C%20and%20vision-language%20understanding.%0AHowever%2C%20studies%20on%20visual%20matching%20ability%20are%20missing%2C%20where%20finding%20the%0Avisual%20correspondence%20of%20objects%20is%20essential%20in%20vision%20research.%20Our%20research%0Areveals%20that%20the%20matching%20capabilities%20in%20recent%20multimodal%20LLMs%20%28MLLMs%29%20still%0Aexhibit%20systematic%20shortcomings%2C%20even%20with%20current%20strong%20MLLMs%20models%2C%20GPT-4o.%0AIn%20particular%2C%20we%20construct%20a%20Multimodal%20Visual%20Matching%20%28MMVM%29%20benchmark%20to%0Afairly%20benchmark%20over%2030%20different%20MLLMs.%20The%20MMVM%20benchmark%20is%20built%20from%2015%0Aopen-source%20datasets%20and%20Internet%20videos%20with%20manual%20annotation.%20We%20categorize%0Athe%20data%20samples%20of%20MMVM%20benchmark%20into%20eight%20aspects%20based%20on%20the%20required%0Acues%20and%20capabilities%20to%20more%20comprehensively%20evaluate%20and%20analyze%20current%0AMLLMs.%20In%20addition%2C%20we%20have%20designed%20an%20automatic%20annotation%20pipeline%20to%0Agenerate%20the%20MMVM%20SFT%20dataset%2C%20including%20220K%20visual%20matching%20data%20with%0Areasoning%20annotation.%20Finally%2C%20we%20present%20CoLVA%2C%20a%20novel%20contrastive%20MLLM%20with%0Atwo%20novel%20technical%20designs%3A%20fine-grained%20vision%20expert%20with%20object-level%0Acontrastive%20learning%20and%20instruction%20augmentation%20strategy.%20CoLVA%20achieves%0A51.06%5C%25%20overall%20accuracy%20%28OA%29%20on%20the%20MMVM%20benchmark%2C%20surpassing%20GPT-4o%20and%0Abaseline%20by%208.41%5C%25%20and%2023.58%5C%25%20OA%2C%20respectively.%20The%20results%20show%20the%0Aeffectiveness%20of%20our%20MMVM%20SFT%20dataset%20and%20our%20novel%20technical%20designs.%20Code%2C%0Abenchmark%2C%20dataset%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhouyiks/CoLVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520They%2520the%2520Same%253F%2520Exploring%2520Visual%2520Correspondence%2520Shortcomings%2520of%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DYikang%2520Zhou%2520and%2520Tao%2520Zhang%2520and%2520Shilin%2520Xu%2520and%2520Shihao%2520Chen%2520and%2520Qianyu%2520Zhou%2520and%2520Yunhai%2520Tong%2520and%2520Shunping%2520Ji%2520and%2520Jiangning%2520Zhang%2520and%2520Xiangtai%2520Li%2520and%2520Lu%2520Qi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520models%2520have%2520shown%2520a%2520strong%2520ability%2520in%250Avisual%2520perception%252C%2520reasoning%2520abilities%252C%2520and%2520vision-language%2520understanding.%250AHowever%252C%2520studies%2520on%2520visual%2520matching%2520ability%2520are%2520missing%252C%2520where%2520finding%2520the%250Avisual%2520correspondence%2520of%2520objects%2520is%2520essential%2520in%2520vision%2520research.%2520Our%2520research%250Areveals%2520that%2520the%2520matching%2520capabilities%2520in%2520recent%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520still%250Aexhibit%2520systematic%2520shortcomings%252C%2520even%2520with%2520current%2520strong%2520MLLMs%2520models%252C%2520GPT-4o.%250AIn%2520particular%252C%2520we%2520construct%2520a%2520Multimodal%2520Visual%2520Matching%2520%2528MMVM%2529%2520benchmark%2520to%250Afairly%2520benchmark%2520over%252030%2520different%2520MLLMs.%2520The%2520MMVM%2520benchmark%2520is%2520built%2520from%252015%250Aopen-source%2520datasets%2520and%2520Internet%2520videos%2520with%2520manual%2520annotation.%2520We%2520categorize%250Athe%2520data%2520samples%2520of%2520MMVM%2520benchmark%2520into%2520eight%2520aspects%2520based%2520on%2520the%2520required%250Acues%2520and%2520capabilities%2520to%2520more%2520comprehensively%2520evaluate%2520and%2520analyze%2520current%250AMLLMs.%2520In%2520addition%252C%2520we%2520have%2520designed%2520an%2520automatic%2520annotation%2520pipeline%2520to%250Agenerate%2520the%2520MMVM%2520SFT%2520dataset%252C%2520including%2520220K%2520visual%2520matching%2520data%2520with%250Areasoning%2520annotation.%2520Finally%252C%2520we%2520present%2520CoLVA%252C%2520a%2520novel%2520contrastive%2520MLLM%2520with%250Atwo%2520novel%2520technical%2520designs%253A%2520fine-grained%2520vision%2520expert%2520with%2520object-level%250Acontrastive%2520learning%2520and%2520instruction%2520augmentation%2520strategy.%2520CoLVA%2520achieves%250A51.06%255C%2525%2520overall%2520accuracy%2520%2528OA%2529%2520on%2520the%2520MMVM%2520benchmark%252C%2520surpassing%2520GPT-4o%2520and%250Abaseline%2520by%25208.41%255C%2525%2520and%252023.58%255C%2525%2520OA%252C%2520respectively.%2520The%2520results%2520show%2520the%250Aeffectiveness%2520of%2520our%2520MMVM%2520SFT%2520dataset%2520and%2520our%2520novel%2520technical%2520designs.%2520Code%252C%250Abenchmark%252C%2520dataset%252C%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/zhouyiks/CoLVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs&entry.906535625=Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Shilin%20Xu%20and%20Shihao%20Chen%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong%20and%20Shunping%20Ji%20and%20Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Lu%20Qi&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20models%20have%20shown%20a%20strong%20ability%20in%0Avisual%20perception%2C%20reasoning%20abilities%2C%20and%20vision-language%20understanding.%0AHowever%2C%20studies%20on%20visual%20matching%20ability%20are%20missing%2C%20where%20finding%20the%0Avisual%20correspondence%20of%20objects%20is%20essential%20in%20vision%20research.%20Our%20research%0Areveals%20that%20the%20matching%20capabilities%20in%20recent%20multimodal%20LLMs%20%28MLLMs%29%20still%0Aexhibit%20systematic%20shortcomings%2C%20even%20with%20current%20strong%20MLLMs%20models%2C%20GPT-4o.%0AIn%20particular%2C%20we%20construct%20a%20Multimodal%20Visual%20Matching%20%28MMVM%29%20benchmark%20to%0Afairly%20benchmark%20over%2030%20different%20MLLMs.%20The%20MMVM%20benchmark%20is%20built%20from%2015%0Aopen-source%20datasets%20and%20Internet%20videos%20with%20manual%20annotation.%20We%20categorize%0Athe%20data%20samples%20of%20MMVM%20benchmark%20into%20eight%20aspects%20based%20on%20the%20required%0Acues%20and%20capabilities%20to%20more%20comprehensively%20evaluate%20and%20analyze%20current%0AMLLMs.%20In%20addition%2C%20we%20have%20designed%20an%20automatic%20annotation%20pipeline%20to%0Agenerate%20the%20MMVM%20SFT%20dataset%2C%20including%20220K%20visual%20matching%20data%20with%0Areasoning%20annotation.%20Finally%2C%20we%20present%20CoLVA%2C%20a%20novel%20contrastive%20MLLM%20with%0Atwo%20novel%20technical%20designs%3A%20fine-grained%20vision%20expert%20with%20object-level%0Acontrastive%20learning%20and%20instruction%20augmentation%20strategy.%20CoLVA%20achieves%0A51.06%5C%25%20overall%20accuracy%20%28OA%29%20on%20the%20MMVM%20benchmark%2C%20surpassing%20GPT-4o%20and%0Abaseline%20by%208.41%5C%25%20and%2023.58%5C%25%20OA%2C%20respectively.%20The%20results%20show%20the%0Aeffectiveness%20of%20our%20MMVM%20SFT%20dataset%20and%20our%20novel%20technical%20designs.%20Code%2C%0Abenchmark%2C%20dataset%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhouyiks/CoLVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04670v2&entry.124074799=Read"},
{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "author": "Stefan Popov and Amit Raj and Michael Krainin and Yuanzhen Li and William T. Freeman and Michael Rubinstein", "abstract": "  We propose a method for generating fly-through videos of a scene, from a\nsingle image and a given camera trajectory. We build upon an image-to-video\nlatent diffusion model. We condition its UNet denoiser on the camera\ntrajectory, using four techniques. (1) We condition the UNet's temporal blocks\non raw camera extrinsics, similar to MotionCtrl. (2) We use images containing\ncamera rays and directions, similar to CameraCtrl. (3) We reproject the initial\nimage to subsequent frames and use the resulting video as a condition. (4) We\nuse 2D<=>3D transformers to introduce a global 3D representation, which\nimplicitly conditions on the camera poses. We combine all conditions in a\nContolNet-style architecture. We then propose a metric that evaluates overall\nvideo quality and the ability to preserve details with view changes, which we\nuse to analyze the trade-offs of individual and combined conditions. Finally,\nwe identify an optimal combination of conditions. We calibrate camera positions\nin our datasets for scale consistency across scenes, and we train our scene\nexploration model, CamCtrl3D, demonstrating state-of-theart results.\n", "link": "http://arxiv.org/abs/2501.06006v2", "date": "2025-01-31", "relevancy": 2.9444, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 1.0}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6833}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control&body=Title%3A%20CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control%0AAuthor%3A%20Stefan%20Popov%20and%20Amit%20Raj%20and%20Michael%20Krainin%20and%20Yuanzhen%20Li%20and%20William%20T.%20Freeman%20and%20Michael%20Rubinstein%0AAbstract%3A%20%20%20We%20propose%20a%20method%20for%20generating%20fly-through%20videos%20of%20a%20scene%2C%20from%20a%0Asingle%20image%20and%20a%20given%20camera%20trajectory.%20We%20build%20upon%20an%20image-to-video%0Alatent%20diffusion%20model.%20We%20condition%20its%20UNet%20denoiser%20on%20the%20camera%0Atrajectory%2C%20using%20four%20techniques.%20%281%29%20We%20condition%20the%20UNet%27s%20temporal%20blocks%0Aon%20raw%20camera%20extrinsics%2C%20similar%20to%20MotionCtrl.%20%282%29%20We%20use%20images%20containing%0Acamera%20rays%20and%20directions%2C%20similar%20to%20CameraCtrl.%20%283%29%20We%20reproject%20the%20initial%0Aimage%20to%20subsequent%20frames%20and%20use%20the%20resulting%20video%20as%20a%20condition.%20%284%29%20We%0Ause%202D%3C%3D%3E3D%20transformers%20to%20introduce%20a%20global%203D%20representation%2C%20which%0Aimplicitly%20conditions%20on%20the%20camera%20poses.%20We%20combine%20all%20conditions%20in%20a%0AContolNet-style%20architecture.%20We%20then%20propose%20a%20metric%20that%20evaluates%20overall%0Avideo%20quality%20and%20the%20ability%20to%20preserve%20details%20with%20view%20changes%2C%20which%20we%0Ause%20to%20analyze%20the%20trade-offs%20of%20individual%20and%20combined%20conditions.%20Finally%2C%0Awe%20identify%20an%20optimal%20combination%20of%20conditions.%20We%20calibrate%20camera%20positions%0Ain%20our%20datasets%20for%20scale%20consistency%20across%20scenes%2C%20and%20we%20train%20our%20scene%0Aexploration%20model%2C%20CamCtrl3D%2C%20demonstrating%20state-of-theart%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamCtrl3D%253A%2520Single-Image%2520Scene%2520Exploration%2520with%2520Precise%25203D%2520Camera%2520Control%26entry.906535625%3DStefan%2520Popov%2520and%2520Amit%2520Raj%2520and%2520Michael%2520Krainin%2520and%2520Yuanzhen%2520Li%2520and%2520William%2520T.%2520Freeman%2520and%2520Michael%2520Rubinstein%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520for%2520generating%2520fly-through%2520videos%2520of%2520a%2520scene%252C%2520from%2520a%250Asingle%2520image%2520and%2520a%2520given%2520camera%2520trajectory.%2520We%2520build%2520upon%2520an%2520image-to-video%250Alatent%2520diffusion%2520model.%2520We%2520condition%2520its%2520UNet%2520denoiser%2520on%2520the%2520camera%250Atrajectory%252C%2520using%2520four%2520techniques.%2520%25281%2529%2520We%2520condition%2520the%2520UNet%2527s%2520temporal%2520blocks%250Aon%2520raw%2520camera%2520extrinsics%252C%2520similar%2520to%2520MotionCtrl.%2520%25282%2529%2520We%2520use%2520images%2520containing%250Acamera%2520rays%2520and%2520directions%252C%2520similar%2520to%2520CameraCtrl.%2520%25283%2529%2520We%2520reproject%2520the%2520initial%250Aimage%2520to%2520subsequent%2520frames%2520and%2520use%2520the%2520resulting%2520video%2520as%2520a%2520condition.%2520%25284%2529%2520We%250Ause%25202D%253C%253D%253E3D%2520transformers%2520to%2520introduce%2520a%2520global%25203D%2520representation%252C%2520which%250Aimplicitly%2520conditions%2520on%2520the%2520camera%2520poses.%2520We%2520combine%2520all%2520conditions%2520in%2520a%250AContolNet-style%2520architecture.%2520We%2520then%2520propose%2520a%2520metric%2520that%2520evaluates%2520overall%250Avideo%2520quality%2520and%2520the%2520ability%2520to%2520preserve%2520details%2520with%2520view%2520changes%252C%2520which%2520we%250Ause%2520to%2520analyze%2520the%2520trade-offs%2520of%2520individual%2520and%2520combined%2520conditions.%2520Finally%252C%250Awe%2520identify%2520an%2520optimal%2520combination%2520of%2520conditions.%2520We%2520calibrate%2520camera%2520positions%250Ain%2520our%2520datasets%2520for%2520scale%2520consistency%2520across%2520scenes%252C%2520and%2520we%2520train%2520our%2520scene%250Aexploration%2520model%252C%2520CamCtrl3D%252C%2520demonstrating%2520state-of-theart%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control&entry.906535625=Stefan%20Popov%20and%20Amit%20Raj%20and%20Michael%20Krainin%20and%20Yuanzhen%20Li%20and%20William%20T.%20Freeman%20and%20Michael%20Rubinstein&entry.1292438233=%20%20We%20propose%20a%20method%20for%20generating%20fly-through%20videos%20of%20a%20scene%2C%20from%20a%0Asingle%20image%20and%20a%20given%20camera%20trajectory.%20We%20build%20upon%20an%20image-to-video%0Alatent%20diffusion%20model.%20We%20condition%20its%20UNet%20denoiser%20on%20the%20camera%0Atrajectory%2C%20using%20four%20techniques.%20%281%29%20We%20condition%20the%20UNet%27s%20temporal%20blocks%0Aon%20raw%20camera%20extrinsics%2C%20similar%20to%20MotionCtrl.%20%282%29%20We%20use%20images%20containing%0Acamera%20rays%20and%20directions%2C%20similar%20to%20CameraCtrl.%20%283%29%20We%20reproject%20the%20initial%0Aimage%20to%20subsequent%20frames%20and%20use%20the%20resulting%20video%20as%20a%20condition.%20%284%29%20We%0Ause%202D%3C%3D%3E3D%20transformers%20to%20introduce%20a%20global%203D%20representation%2C%20which%0Aimplicitly%20conditions%20on%20the%20camera%20poses.%20We%20combine%20all%20conditions%20in%20a%0AContolNet-style%20architecture.%20We%20then%20propose%20a%20metric%20that%20evaluates%20overall%0Avideo%20quality%20and%20the%20ability%20to%20preserve%20details%20with%20view%20changes%2C%20which%20we%0Ause%20to%20analyze%20the%20trade-offs%20of%20individual%20and%20combined%20conditions.%20Finally%2C%0Awe%20identify%20an%20optimal%20combination%20of%20conditions.%20We%20calibrate%20camera%20positions%0Ain%20our%20datasets%20for%20scale%20consistency%20across%20scenes%2C%20and%20we%20train%20our%20scene%0Aexploration%20model%2C%20CamCtrl3D%2C%20demonstrating%20state-of-theart%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06006v2&entry.124074799=Read"},
{"title": "Contextual Emotion Recognition using Large Vision Language Models", "author": "Yasaman Etesam and \u00d6zge Nilay Yal\u00e7\u0131n and Chuxuan Zhang and Angelica Lim", "abstract": "  \"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future.\n", "link": "http://arxiv.org/abs/2405.08992v2", "date": "2025-01-31", "relevancy": 2.9307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.607}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Emotion%20Recognition%20using%20Large%20Vision%20Language%20Models&body=Title%3A%20Contextual%20Emotion%20Recognition%20using%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Yasaman%20Etesam%20and%20%C3%96zge%20Nilay%20Yal%C3%A7%C4%B1n%20and%20Chuxuan%20Zhang%20and%20Angelica%20Lim%0AAbstract%3A%20%20%20%22How%20does%20the%20person%20in%20the%20bounding%20box%20feel%3F%22%20Achieving%20human-level%0Arecognition%20of%20the%20apparent%20emotion%20of%20a%20person%20in%20real%20world%20situations%0Aremains%20an%20unsolved%20task%20in%20computer%20vision.%20Facial%20expressions%20are%20not%20enough%3A%0Abody%20pose%2C%20contextual%20knowledge%2C%20and%20commonsense%20reasoning%20all%20contribute%20to%0Ahow%20humans%20perform%20this%20emotional%20theory%20of%20mind%20task.%20In%20this%20paper%2C%20we%0Aexamine%20two%20major%20approaches%20enabled%20by%20recent%20large%20vision%20language%20models%3A%201%29%0Aimage%20captioning%20followed%20by%20a%20language-only%20LLM%2C%20and%202%29%20vision%20language%0Amodels%2C%20under%20zero-shot%20and%20fine-tuned%20setups.%20We%20evaluate%20the%20methods%20on%20the%0AEmotions%20in%20Context%20%28EMOTIC%29%20dataset%20and%20demonstrate%20that%20a%20vision%20language%0Amodel%2C%20fine-tuned%20even%20on%20a%20small%20dataset%2C%20can%20significantly%20outperform%0Atraditional%20baselines.%20The%20results%20of%20this%20work%20aim%20to%20help%20robots%20and%20agents%0Aperform%20emotionally%20sensitive%20decision-making%20and%20interaction%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Emotion%2520Recognition%2520using%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DYasaman%2520Etesam%2520and%2520%25C3%2596zge%2520Nilay%2520Yal%25C3%25A7%25C4%25B1n%2520and%2520Chuxuan%2520Zhang%2520and%2520Angelica%2520Lim%26entry.1292438233%3D%2520%2520%2522How%2520does%2520the%2520person%2520in%2520the%2520bounding%2520box%2520feel%253F%2522%2520Achieving%2520human-level%250Arecognition%2520of%2520the%2520apparent%2520emotion%2520of%2520a%2520person%2520in%2520real%2520world%2520situations%250Aremains%2520an%2520unsolved%2520task%2520in%2520computer%2520vision.%2520Facial%2520expressions%2520are%2520not%2520enough%253A%250Abody%2520pose%252C%2520contextual%2520knowledge%252C%2520and%2520commonsense%2520reasoning%2520all%2520contribute%2520to%250Ahow%2520humans%2520perform%2520this%2520emotional%2520theory%2520of%2520mind%2520task.%2520In%2520this%2520paper%252C%2520we%250Aexamine%2520two%2520major%2520approaches%2520enabled%2520by%2520recent%2520large%2520vision%2520language%2520models%253A%25201%2529%250Aimage%2520captioning%2520followed%2520by%2520a%2520language-only%2520LLM%252C%2520and%25202%2529%2520vision%2520language%250Amodels%252C%2520under%2520zero-shot%2520and%2520fine-tuned%2520setups.%2520We%2520evaluate%2520the%2520methods%2520on%2520the%250AEmotions%2520in%2520Context%2520%2528EMOTIC%2529%2520dataset%2520and%2520demonstrate%2520that%2520a%2520vision%2520language%250Amodel%252C%2520fine-tuned%2520even%2520on%2520a%2520small%2520dataset%252C%2520can%2520significantly%2520outperform%250Atraditional%2520baselines.%2520The%2520results%2520of%2520this%2520work%2520aim%2520to%2520help%2520robots%2520and%2520agents%250Aperform%2520emotionally%2520sensitive%2520decision-making%2520and%2520interaction%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Emotion%20Recognition%20using%20Large%20Vision%20Language%20Models&entry.906535625=Yasaman%20Etesam%20and%20%C3%96zge%20Nilay%20Yal%C3%A7%C4%B1n%20and%20Chuxuan%20Zhang%20and%20Angelica%20Lim&entry.1292438233=%20%20%22How%20does%20the%20person%20in%20the%20bounding%20box%20feel%3F%22%20Achieving%20human-level%0Arecognition%20of%20the%20apparent%20emotion%20of%20a%20person%20in%20real%20world%20situations%0Aremains%20an%20unsolved%20task%20in%20computer%20vision.%20Facial%20expressions%20are%20not%20enough%3A%0Abody%20pose%2C%20contextual%20knowledge%2C%20and%20commonsense%20reasoning%20all%20contribute%20to%0Ahow%20humans%20perform%20this%20emotional%20theory%20of%20mind%20task.%20In%20this%20paper%2C%20we%0Aexamine%20two%20major%20approaches%20enabled%20by%20recent%20large%20vision%20language%20models%3A%201%29%0Aimage%20captioning%20followed%20by%20a%20language-only%20LLM%2C%20and%202%29%20vision%20language%0Amodels%2C%20under%20zero-shot%20and%20fine-tuned%20setups.%20We%20evaluate%20the%20methods%20on%20the%0AEmotions%20in%20Context%20%28EMOTIC%29%20dataset%20and%20demonstrate%20that%20a%20vision%20language%0Amodel%2C%20fine-tuned%20even%20on%20a%20small%20dataset%2C%20can%20significantly%20outperform%0Atraditional%20baselines.%20The%20results%20of%20this%20work%20aim%20to%20help%20robots%20and%20agents%0Aperform%20emotionally%20sensitive%20decision-making%20and%20interaction%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08992v2&entry.124074799=Read"},
{"title": "RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual\n  Preference Data", "author": "Chenglong Wang and Yang Gan and Yifu Huo and Yongyu Mu and Murun Yang and Qiaozhi He and Tong Xiao and Chunliang Zhang and Tongran Liu and Quan Du and Di Yang and Jingbo Zhu", "abstract": "  Large vision-language models (LVLMs) often fail to align with human\npreferences, leading to issues like generating misleading content without\nproper visual context (also known as hallucination). A promising solution to\nthis problem is using human-preference alignment techniques, such as best-of-n\nsampling and reinforcement learning. However, these techniques face the\ndifficulty arising from the scarcity of visual preference data, which is\nrequired to train a visual reward model (VRM). In this work, we continue the\nline of research. We present a Robust Visual Reward Model (RoVRM) which\nimproves human-preference alignment for LVLMs. RoVRM leverages auxiliary\ntextual preference data through a three-phase progressive training and optimal\ntransport-based preference data selection to effectively mitigate the scarcity\nof visual preference data. We experiment with RoVRM on the commonly used\nvision-language tasks based on the LLaVA-1.5-7B and -13B models. Experimental\nresults demonstrate that RoVRM consistently outperforms traditional VRMs.\nFurthermore, our three-phase progressive training and preference data selection\napproaches can yield consistent performance gains over ranking-based alignment\ntechniques, such as direct preference optimization.\n", "link": "http://arxiv.org/abs/2408.12109v2", "date": "2025-01-31", "relevancy": 2.9136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoVRM%3A%20A%20Robust%20Visual%20Reward%20Model%20Optimized%20via%20Auxiliary%20Textual%0A%20%20Preference%20Data&body=Title%3A%20RoVRM%3A%20A%20Robust%20Visual%20Reward%20Model%20Optimized%20via%20Auxiliary%20Textual%0A%20%20Preference%20Data%0AAuthor%3A%20Chenglong%20Wang%20and%20Yang%20Gan%20and%20Yifu%20Huo%20and%20Yongyu%20Mu%20and%20Murun%20Yang%20and%20Qiaozhi%20He%20and%20Tong%20Xiao%20and%20Chunliang%20Zhang%20and%20Tongran%20Liu%20and%20Quan%20Du%20and%20Di%20Yang%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20often%20fail%20to%20align%20with%20human%0Apreferences%2C%20leading%20to%20issues%20like%20generating%20misleading%20content%20without%0Aproper%20visual%20context%20%28also%20known%20as%20hallucination%29.%20A%20promising%20solution%20to%0Athis%20problem%20is%20using%20human-preference%20alignment%20techniques%2C%20such%20as%20best-of-n%0Asampling%20and%20reinforcement%20learning.%20However%2C%20these%20techniques%20face%20the%0Adifficulty%20arising%20from%20the%20scarcity%20of%20visual%20preference%20data%2C%20which%20is%0Arequired%20to%20train%20a%20visual%20reward%20model%20%28VRM%29.%20In%20this%20work%2C%20we%20continue%20the%0Aline%20of%20research.%20We%20present%20a%20Robust%20Visual%20Reward%20Model%20%28RoVRM%29%20which%0Aimproves%20human-preference%20alignment%20for%20LVLMs.%20RoVRM%20leverages%20auxiliary%0Atextual%20preference%20data%20through%20a%20three-phase%20progressive%20training%20and%20optimal%0Atransport-based%20preference%20data%20selection%20to%20effectively%20mitigate%20the%20scarcity%0Aof%20visual%20preference%20data.%20We%20experiment%20with%20RoVRM%20on%20the%20commonly%20used%0Avision-language%20tasks%20based%20on%20the%20LLaVA-1.5-7B%20and%20-13B%20models.%20Experimental%0Aresults%20demonstrate%20that%20RoVRM%20consistently%20outperforms%20traditional%20VRMs.%0AFurthermore%2C%20our%20three-phase%20progressive%20training%20and%20preference%20data%20selection%0Aapproaches%20can%20yield%20consistent%20performance%20gains%20over%20ranking-based%20alignment%0Atechniques%2C%20such%20as%20direct%20preference%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoVRM%253A%2520A%2520Robust%2520Visual%2520Reward%2520Model%2520Optimized%2520via%2520Auxiliary%2520Textual%250A%2520%2520Preference%2520Data%26entry.906535625%3DChenglong%2520Wang%2520and%2520Yang%2520Gan%2520and%2520Yifu%2520Huo%2520and%2520Yongyu%2520Mu%2520and%2520Murun%2520Yang%2520and%2520Qiaozhi%2520He%2520and%2520Tong%2520Xiao%2520and%2520Chunliang%2520Zhang%2520and%2520Tongran%2520Liu%2520and%2520Quan%2520Du%2520and%2520Di%2520Yang%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520often%2520fail%2520to%2520align%2520with%2520human%250Apreferences%252C%2520leading%2520to%2520issues%2520like%2520generating%2520misleading%2520content%2520without%250Aproper%2520visual%2520context%2520%2528also%2520known%2520as%2520hallucination%2529.%2520A%2520promising%2520solution%2520to%250Athis%2520problem%2520is%2520using%2520human-preference%2520alignment%2520techniques%252C%2520such%2520as%2520best-of-n%250Asampling%2520and%2520reinforcement%2520learning.%2520However%252C%2520these%2520techniques%2520face%2520the%250Adifficulty%2520arising%2520from%2520the%2520scarcity%2520of%2520visual%2520preference%2520data%252C%2520which%2520is%250Arequired%2520to%2520train%2520a%2520visual%2520reward%2520model%2520%2528VRM%2529.%2520In%2520this%2520work%252C%2520we%2520continue%2520the%250Aline%2520of%2520research.%2520We%2520present%2520a%2520Robust%2520Visual%2520Reward%2520Model%2520%2528RoVRM%2529%2520which%250Aimproves%2520human-preference%2520alignment%2520for%2520LVLMs.%2520RoVRM%2520leverages%2520auxiliary%250Atextual%2520preference%2520data%2520through%2520a%2520three-phase%2520progressive%2520training%2520and%2520optimal%250Atransport-based%2520preference%2520data%2520selection%2520to%2520effectively%2520mitigate%2520the%2520scarcity%250Aof%2520visual%2520preference%2520data.%2520We%2520experiment%2520with%2520RoVRM%2520on%2520the%2520commonly%2520used%250Avision-language%2520tasks%2520based%2520on%2520the%2520LLaVA-1.5-7B%2520and%2520-13B%2520models.%2520Experimental%250Aresults%2520demonstrate%2520that%2520RoVRM%2520consistently%2520outperforms%2520traditional%2520VRMs.%250AFurthermore%252C%2520our%2520three-phase%2520progressive%2520training%2520and%2520preference%2520data%2520selection%250Aapproaches%2520can%2520yield%2520consistent%2520performance%2520gains%2520over%2520ranking-based%2520alignment%250Atechniques%252C%2520such%2520as%2520direct%2520preference%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoVRM%3A%20A%20Robust%20Visual%20Reward%20Model%20Optimized%20via%20Auxiliary%20Textual%0A%20%20Preference%20Data&entry.906535625=Chenglong%20Wang%20and%20Yang%20Gan%20and%20Yifu%20Huo%20and%20Yongyu%20Mu%20and%20Murun%20Yang%20and%20Qiaozhi%20He%20and%20Tong%20Xiao%20and%20Chunliang%20Zhang%20and%20Tongran%20Liu%20and%20Quan%20Du%20and%20Di%20Yang%20and%20Jingbo%20Zhu&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20often%20fail%20to%20align%20with%20human%0Apreferences%2C%20leading%20to%20issues%20like%20generating%20misleading%20content%20without%0Aproper%20visual%20context%20%28also%20known%20as%20hallucination%29.%20A%20promising%20solution%20to%0Athis%20problem%20is%20using%20human-preference%20alignment%20techniques%2C%20such%20as%20best-of-n%0Asampling%20and%20reinforcement%20learning.%20However%2C%20these%20techniques%20face%20the%0Adifficulty%20arising%20from%20the%20scarcity%20of%20visual%20preference%20data%2C%20which%20is%0Arequired%20to%20train%20a%20visual%20reward%20model%20%28VRM%29.%20In%20this%20work%2C%20we%20continue%20the%0Aline%20of%20research.%20We%20present%20a%20Robust%20Visual%20Reward%20Model%20%28RoVRM%29%20which%0Aimproves%20human-preference%20alignment%20for%20LVLMs.%20RoVRM%20leverages%20auxiliary%0Atextual%20preference%20data%20through%20a%20three-phase%20progressive%20training%20and%20optimal%0Atransport-based%20preference%20data%20selection%20to%20effectively%20mitigate%20the%20scarcity%0Aof%20visual%20preference%20data.%20We%20experiment%20with%20RoVRM%20on%20the%20commonly%20used%0Avision-language%20tasks%20based%20on%20the%20LLaVA-1.5-7B%20and%20-13B%20models.%20Experimental%0Aresults%20demonstrate%20that%20RoVRM%20consistently%20outperforms%20traditional%20VRMs.%0AFurthermore%2C%20our%20three-phase%20progressive%20training%20and%20preference%20data%20selection%0Aapproaches%20can%20yield%20consistent%20performance%20gains%20over%20ranking-based%20alignment%0Atechniques%2C%20such%20as%20direct%20preference%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12109v2&entry.124074799=Read"},
{"title": "Improving vision-language alignment with graph spiking hybrid Networks", "author": "Siyu Zhang and Heming Zheng and Yiming Wu and Yeming Chen", "abstract": "  To bridge the semantic gap between vision and language (VL), it is necessary\nto develop a good alignment strategy, which includes handling semantic\ndiversity, abstract representation of visual information, and generalization\nability of models. Recent works use detector-based bounding boxes or patches\nwith regular partitions to represent visual semantics. While current paradigms\nhave made strides, they are still insufficient for fully capturing the nuanced\ncontextual relations among various objects. This paper proposes a comprehensive\nvisual semantic representation module, necessitating the utilization of\npanoptic segmentation to generate coherent fine-grained semantic features.\nFurthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that\nintegrates the complementary advantages of Spiking Neural Networks (SNNs) and\nGraph Attention Networks (GATs) to encode visual semantic information.\nIntriguingly, the model not only encodes the discrete and continuous latent\nvariables of instances but also adeptly captures both local and global\ncontextual features, thereby significantly enhancing the richness and diversity\nof semantic representations. Leveraging the spatiotemporal properties inherent\nin SNNs, we employ contrastive learning (CL) to enhance the similarity-based\nrepresentation of embeddings. This strategy alleviates the computational\noverhead of the model and enriches meaningful visual representations by\nconstructing positive and negative sample pairs. We design an innovative\npre-training method, Spiked Text Learning (STL), which uses text features to\nimprove the encoding ability of discrete semantics. Experiments show that the\nproposed GSHN exhibits promising results on multiple VL downstream tasks.\n", "link": "http://arxiv.org/abs/2501.19069v1", "date": "2025-01-31", "relevancy": 2.9085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20vision-language%20alignment%20with%20graph%20spiking%20hybrid%20Networks&body=Title%3A%20Improving%20vision-language%20alignment%20with%20graph%20spiking%20hybrid%20Networks%0AAuthor%3A%20Siyu%20Zhang%20and%20Heming%20Zheng%20and%20Yiming%20Wu%20and%20Yeming%20Chen%0AAbstract%3A%20%20%20To%20bridge%20the%20semantic%20gap%20between%20vision%20and%20language%20%28VL%29%2C%20it%20is%20necessary%0Ato%20develop%20a%20good%20alignment%20strategy%2C%20which%20includes%20handling%20semantic%0Adiversity%2C%20abstract%20representation%20of%20visual%20information%2C%20and%20generalization%0Aability%20of%20models.%20Recent%20works%20use%20detector-based%20bounding%20boxes%20or%20patches%0Awith%20regular%20partitions%20to%20represent%20visual%20semantics.%20While%20current%20paradigms%0Ahave%20made%20strides%2C%20they%20are%20still%20insufficient%20for%20fully%20capturing%20the%20nuanced%0Acontextual%20relations%20among%20various%20objects.%20This%20paper%20proposes%20a%20comprehensive%0Avisual%20semantic%20representation%20module%2C%20necessitating%20the%20utilization%20of%0Apanoptic%20segmentation%20to%20generate%20coherent%20fine-grained%20semantic%20features.%0AFurthermore%2C%20we%20propose%20a%20novel%20Graph%20Spiking%20Hybrid%20Network%20%28GSHN%29%20that%0Aintegrates%20the%20complementary%20advantages%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%20and%0AGraph%20Attention%20Networks%20%28GATs%29%20to%20encode%20visual%20semantic%20information.%0AIntriguingly%2C%20the%20model%20not%20only%20encodes%20the%20discrete%20and%20continuous%20latent%0Avariables%20of%20instances%20but%20also%20adeptly%20captures%20both%20local%20and%20global%0Acontextual%20features%2C%20thereby%20significantly%20enhancing%20the%20richness%20and%20diversity%0Aof%20semantic%20representations.%20Leveraging%20the%20spatiotemporal%20properties%20inherent%0Ain%20SNNs%2C%20we%20employ%20contrastive%20learning%20%28CL%29%20to%20enhance%20the%20similarity-based%0Arepresentation%20of%20embeddings.%20This%20strategy%20alleviates%20the%20computational%0Aoverhead%20of%20the%20model%20and%20enriches%20meaningful%20visual%20representations%20by%0Aconstructing%20positive%20and%20negative%20sample%20pairs.%20We%20design%20an%20innovative%0Apre-training%20method%2C%20Spiked%20Text%20Learning%20%28STL%29%2C%20which%20uses%20text%20features%20to%0Aimprove%20the%20encoding%20ability%20of%20discrete%20semantics.%20Experiments%20show%20that%20the%0Aproposed%20GSHN%20exhibits%20promising%20results%20on%20multiple%20VL%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520vision-language%2520alignment%2520with%2520graph%2520spiking%2520hybrid%2520Networks%26entry.906535625%3DSiyu%2520Zhang%2520and%2520Heming%2520Zheng%2520and%2520Yiming%2520Wu%2520and%2520Yeming%2520Chen%26entry.1292438233%3D%2520%2520To%2520bridge%2520the%2520semantic%2520gap%2520between%2520vision%2520and%2520language%2520%2528VL%2529%252C%2520it%2520is%2520necessary%250Ato%2520develop%2520a%2520good%2520alignment%2520strategy%252C%2520which%2520includes%2520handling%2520semantic%250Adiversity%252C%2520abstract%2520representation%2520of%2520visual%2520information%252C%2520and%2520generalization%250Aability%2520of%2520models.%2520Recent%2520works%2520use%2520detector-based%2520bounding%2520boxes%2520or%2520patches%250Awith%2520regular%2520partitions%2520to%2520represent%2520visual%2520semantics.%2520While%2520current%2520paradigms%250Ahave%2520made%2520strides%252C%2520they%2520are%2520still%2520insufficient%2520for%2520fully%2520capturing%2520the%2520nuanced%250Acontextual%2520relations%2520among%2520various%2520objects.%2520This%2520paper%2520proposes%2520a%2520comprehensive%250Avisual%2520semantic%2520representation%2520module%252C%2520necessitating%2520the%2520utilization%2520of%250Apanoptic%2520segmentation%2520to%2520generate%2520coherent%2520fine-grained%2520semantic%2520features.%250AFurthermore%252C%2520we%2520propose%2520a%2520novel%2520Graph%2520Spiking%2520Hybrid%2520Network%2520%2528GSHN%2529%2520that%250Aintegrates%2520the%2520complementary%2520advantages%2520of%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520and%250AGraph%2520Attention%2520Networks%2520%2528GATs%2529%2520to%2520encode%2520visual%2520semantic%2520information.%250AIntriguingly%252C%2520the%2520model%2520not%2520only%2520encodes%2520the%2520discrete%2520and%2520continuous%2520latent%250Avariables%2520of%2520instances%2520but%2520also%2520adeptly%2520captures%2520both%2520local%2520and%2520global%250Acontextual%2520features%252C%2520thereby%2520significantly%2520enhancing%2520the%2520richness%2520and%2520diversity%250Aof%2520semantic%2520representations.%2520Leveraging%2520the%2520spatiotemporal%2520properties%2520inherent%250Ain%2520SNNs%252C%2520we%2520employ%2520contrastive%2520learning%2520%2528CL%2529%2520to%2520enhance%2520the%2520similarity-based%250Arepresentation%2520of%2520embeddings.%2520This%2520strategy%2520alleviates%2520the%2520computational%250Aoverhead%2520of%2520the%2520model%2520and%2520enriches%2520meaningful%2520visual%2520representations%2520by%250Aconstructing%2520positive%2520and%2520negative%2520sample%2520pairs.%2520We%2520design%2520an%2520innovative%250Apre-training%2520method%252C%2520Spiked%2520Text%2520Learning%2520%2528STL%2529%252C%2520which%2520uses%2520text%2520features%2520to%250Aimprove%2520the%2520encoding%2520ability%2520of%2520discrete%2520semantics.%2520Experiments%2520show%2520that%2520the%250Aproposed%2520GSHN%2520exhibits%2520promising%2520results%2520on%2520multiple%2520VL%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20vision-language%20alignment%20with%20graph%20spiking%20hybrid%20Networks&entry.906535625=Siyu%20Zhang%20and%20Heming%20Zheng%20and%20Yiming%20Wu%20and%20Yeming%20Chen&entry.1292438233=%20%20To%20bridge%20the%20semantic%20gap%20between%20vision%20and%20language%20%28VL%29%2C%20it%20is%20necessary%0Ato%20develop%20a%20good%20alignment%20strategy%2C%20which%20includes%20handling%20semantic%0Adiversity%2C%20abstract%20representation%20of%20visual%20information%2C%20and%20generalization%0Aability%20of%20models.%20Recent%20works%20use%20detector-based%20bounding%20boxes%20or%20patches%0Awith%20regular%20partitions%20to%20represent%20visual%20semantics.%20While%20current%20paradigms%0Ahave%20made%20strides%2C%20they%20are%20still%20insufficient%20for%20fully%20capturing%20the%20nuanced%0Acontextual%20relations%20among%20various%20objects.%20This%20paper%20proposes%20a%20comprehensive%0Avisual%20semantic%20representation%20module%2C%20necessitating%20the%20utilization%20of%0Apanoptic%20segmentation%20to%20generate%20coherent%20fine-grained%20semantic%20features.%0AFurthermore%2C%20we%20propose%20a%20novel%20Graph%20Spiking%20Hybrid%20Network%20%28GSHN%29%20that%0Aintegrates%20the%20complementary%20advantages%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%20and%0AGraph%20Attention%20Networks%20%28GATs%29%20to%20encode%20visual%20semantic%20information.%0AIntriguingly%2C%20the%20model%20not%20only%20encodes%20the%20discrete%20and%20continuous%20latent%0Avariables%20of%20instances%20but%20also%20adeptly%20captures%20both%20local%20and%20global%0Acontextual%20features%2C%20thereby%20significantly%20enhancing%20the%20richness%20and%20diversity%0Aof%20semantic%20representations.%20Leveraging%20the%20spatiotemporal%20properties%20inherent%0Ain%20SNNs%2C%20we%20employ%20contrastive%20learning%20%28CL%29%20to%20enhance%20the%20similarity-based%0Arepresentation%20of%20embeddings.%20This%20strategy%20alleviates%20the%20computational%0Aoverhead%20of%20the%20model%20and%20enriches%20meaningful%20visual%20representations%20by%0Aconstructing%20positive%20and%20negative%20sample%20pairs.%20We%20design%20an%20innovative%0Apre-training%20method%2C%20Spiked%20Text%20Learning%20%28STL%29%2C%20which%20uses%20text%20features%20to%0Aimprove%20the%20encoding%20ability%20of%20discrete%20semantics.%20Experiments%20show%20that%20the%0Aproposed%20GSHN%20exhibits%20promising%20results%20on%20multiple%20VL%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19069v1&entry.124074799=Read"},
{"title": "Beyond Token Compression: A Training-Free Reduction Framework for\n  Efficient Visual Processing in MLLMs", "author": "Hongliang Li and Jiaxin Zhang and Wenhui Liao and Dezhi Peng and Kai Ding and Lianwen Jin", "abstract": "  Multimodal Large Language Models (MLLMs) are typically based on decoder-only\nor cross-attention architectures. While decoder-only MLLMs outperform their\ncross-attention counterparts, they require significantly higher computational\nresources due to extensive self-attention and FFN operations on visual tokens.\nThis raises the question: can we eliminate these expensive operations while\nmaintaining the performance? To this end, we present a novel analysis framework\nto investigate the necessity of these costly operations in decoder-only MLLMs.\nOur framework introduces two key innovations: (1) Hollow Attention, which\nlimits visual token interactions to local attention while maintaining\nvisual-text associations, and (2) Probe-Activated Dynamic FFN, which\nselectively activates FFN parameters for visual tokens. Both methods do not\nrequire fine-tuning, which significantly enhances analysis efficiency. To\nassess the impact of applying these reductions across different proportions of\nlayers, we developed a greedy search method that significantly narrows the\nsearch space. Experiments on state-of-the-art MLLMs reveal that applying our\nreductions to approximately half of the layers not only maintains but sometimes\nimproves model performance, indicating significant computational redundancy in\ncurrent architectures. Additionally, our method is orthogonal to existing token\ncompression techniques, allowing for further combination to achieve greater\ncomputational reduction. Our findings may provide valuable insights for the\ndesign of more efficient future MLLMs. Our code will be publicly available at\nhttps://github.com/L-Hugh/Beyond-Token-Compression.\n", "link": "http://arxiv.org/abs/2501.19036v1", "date": "2025-01-31", "relevancy": 2.8897, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Token%20Compression%3A%20A%20Training-Free%20Reduction%20Framework%20for%0A%20%20Efficient%20Visual%20Processing%20in%20MLLMs&body=Title%3A%20Beyond%20Token%20Compression%3A%20A%20Training-Free%20Reduction%20Framework%20for%0A%20%20Efficient%20Visual%20Processing%20in%20MLLMs%0AAuthor%3A%20Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20typically%20based%20on%20decoder-only%0Aor%20cross-attention%20architectures.%20While%20decoder-only%20MLLMs%20outperform%20their%0Across-attention%20counterparts%2C%20they%20require%20significantly%20higher%20computational%0Aresources%20due%20to%20extensive%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens.%0AThis%20raises%20the%20question%3A%20can%20we%20eliminate%20these%20expensive%20operations%20while%0Amaintaining%20the%20performance%3F%20To%20this%20end%2C%20we%20present%20a%20novel%20analysis%20framework%0Ato%20investigate%20the%20necessity%20of%20these%20costly%20operations%20in%20decoder-only%20MLLMs.%0AOur%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20Hollow%20Attention%2C%20which%0Alimits%20visual%20token%20interactions%20to%20local%20attention%20while%20maintaining%0Avisual-text%20associations%2C%20and%20%282%29%20Probe-Activated%20Dynamic%20FFN%2C%20which%0Aselectively%20activates%20FFN%20parameters%20for%20visual%20tokens.%20Both%20methods%20do%20not%0Arequire%20fine-tuning%2C%20which%20significantly%20enhances%20analysis%20efficiency.%20To%0Aassess%20the%20impact%20of%20applying%20these%20reductions%20across%20different%20proportions%20of%0Alayers%2C%20we%20developed%20a%20greedy%20search%20method%20that%20significantly%20narrows%20the%0Asearch%20space.%20Experiments%20on%20state-of-the-art%20MLLMs%20reveal%20that%20applying%20our%0Areductions%20to%20approximately%20half%20of%20the%20layers%20not%20only%20maintains%20but%20sometimes%0Aimproves%20model%20performance%2C%20indicating%20significant%20computational%20redundancy%20in%0Acurrent%20architectures.%20Additionally%2C%20our%20method%20is%20orthogonal%20to%20existing%20token%0Acompression%20techniques%2C%20allowing%20for%20further%20combination%20to%20achieve%20greater%0Acomputational%20reduction.%20Our%20findings%20may%20provide%20valuable%20insights%20for%20the%0Adesign%20of%20more%20efficient%20future%20MLLMs.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/Beyond-Token-Compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Token%2520Compression%253A%2520A%2520Training-Free%2520Reduction%2520Framework%2520for%250A%2520%2520Efficient%2520Visual%2520Processing%2520in%2520MLLMs%26entry.906535625%3DHongliang%2520Li%2520and%2520Jiaxin%2520Zhang%2520and%2520Wenhui%2520Liao%2520and%2520Dezhi%2520Peng%2520and%2520Kai%2520Ding%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520typically%2520based%2520on%2520decoder-only%250Aor%2520cross-attention%2520architectures.%2520While%2520decoder-only%2520MLLMs%2520outperform%2520their%250Across-attention%2520counterparts%252C%2520they%2520require%2520significantly%2520higher%2520computational%250Aresources%2520due%2520to%2520extensive%2520self-attention%2520and%2520FFN%2520operations%2520on%2520visual%2520tokens.%250AThis%2520raises%2520the%2520question%253A%2520can%2520we%2520eliminate%2520these%2520expensive%2520operations%2520while%250Amaintaining%2520the%2520performance%253F%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520novel%2520analysis%2520framework%250Ato%2520investigate%2520the%2520necessity%2520of%2520these%2520costly%2520operations%2520in%2520decoder-only%2520MLLMs.%250AOur%2520framework%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520Hollow%2520Attention%252C%2520which%250Alimits%2520visual%2520token%2520interactions%2520to%2520local%2520attention%2520while%2520maintaining%250Avisual-text%2520associations%252C%2520and%2520%25282%2529%2520Probe-Activated%2520Dynamic%2520FFN%252C%2520which%250Aselectively%2520activates%2520FFN%2520parameters%2520for%2520visual%2520tokens.%2520Both%2520methods%2520do%2520not%250Arequire%2520fine-tuning%252C%2520which%2520significantly%2520enhances%2520analysis%2520efficiency.%2520To%250Aassess%2520the%2520impact%2520of%2520applying%2520these%2520reductions%2520across%2520different%2520proportions%2520of%250Alayers%252C%2520we%2520developed%2520a%2520greedy%2520search%2520method%2520that%2520significantly%2520narrows%2520the%250Asearch%2520space.%2520Experiments%2520on%2520state-of-the-art%2520MLLMs%2520reveal%2520that%2520applying%2520our%250Areductions%2520to%2520approximately%2520half%2520of%2520the%2520layers%2520not%2520only%2520maintains%2520but%2520sometimes%250Aimproves%2520model%2520performance%252C%2520indicating%2520significant%2520computational%2520redundancy%2520in%250Acurrent%2520architectures.%2520Additionally%252C%2520our%2520method%2520is%2520orthogonal%2520to%2520existing%2520token%250Acompression%2520techniques%252C%2520allowing%2520for%2520further%2520combination%2520to%2520achieve%2520greater%250Acomputational%2520reduction.%2520Our%2520findings%2520may%2520provide%2520valuable%2520insights%2520for%2520the%250Adesign%2520of%2520more%2520efficient%2520future%2520MLLMs.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/L-Hugh/Beyond-Token-Compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Token%20Compression%3A%20A%20Training-Free%20Reduction%20Framework%20for%0A%20%20Efficient%20Visual%20Processing%20in%20MLLMs&entry.906535625=Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20typically%20based%20on%20decoder-only%0Aor%20cross-attention%20architectures.%20While%20decoder-only%20MLLMs%20outperform%20their%0Across-attention%20counterparts%2C%20they%20require%20significantly%20higher%20computational%0Aresources%20due%20to%20extensive%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens.%0AThis%20raises%20the%20question%3A%20can%20we%20eliminate%20these%20expensive%20operations%20while%0Amaintaining%20the%20performance%3F%20To%20this%20end%2C%20we%20present%20a%20novel%20analysis%20framework%0Ato%20investigate%20the%20necessity%20of%20these%20costly%20operations%20in%20decoder-only%20MLLMs.%0AOur%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20Hollow%20Attention%2C%20which%0Alimits%20visual%20token%20interactions%20to%20local%20attention%20while%20maintaining%0Avisual-text%20associations%2C%20and%20%282%29%20Probe-Activated%20Dynamic%20FFN%2C%20which%0Aselectively%20activates%20FFN%20parameters%20for%20visual%20tokens.%20Both%20methods%20do%20not%0Arequire%20fine-tuning%2C%20which%20significantly%20enhances%20analysis%20efficiency.%20To%0Aassess%20the%20impact%20of%20applying%20these%20reductions%20across%20different%20proportions%20of%0Alayers%2C%20we%20developed%20a%20greedy%20search%20method%20that%20significantly%20narrows%20the%0Asearch%20space.%20Experiments%20on%20state-of-the-art%20MLLMs%20reveal%20that%20applying%20our%0Areductions%20to%20approximately%20half%20of%20the%20layers%20not%20only%20maintains%20but%20sometimes%0Aimproves%20model%20performance%2C%20indicating%20significant%20computational%20redundancy%20in%0Acurrent%20architectures.%20Additionally%2C%20our%20method%20is%20orthogonal%20to%20existing%20token%0Acompression%20techniques%2C%20allowing%20for%20further%20combination%20to%20achieve%20greater%0Acomputational%20reduction.%20Our%20findings%20may%20provide%20valuable%20insights%20for%20the%0Adesign%20of%20more%20efficient%20future%20MLLMs.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/Beyond-Token-Compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19036v1&entry.124074799=Read"},
{"title": "Laser: Efficient Language-Guided Segmentation in Neural Radiance Fields", "author": "Xingyu Miao and Haoran Duan and Yang Bai and Tejal Shah and Jun Song and Yang Long and Rajiv Ranjan and Ling Shao", "abstract": "  In this work, we propose a method that leverages CLIP feature distillation,\nachieving efficient 3D segmentation through language guidance. Unlike previous\nmethods that rely on multi-scale CLIP features and are limited by processing\nspeed and storage requirements, our approach aims to streamline the workflow by\ndirectly and effectively distilling dense CLIP features, thereby achieving\nprecise segmentation of 3D scenes using text. To achieve this, we introduce an\nadapter module and mitigate the noise issue in the dense CLIP feature\ndistillation process through a self-cross-training strategy. Moreover, to\nenhance the accuracy of segmentation edges, this work presents a low-rank\ntransient query attention mechanism. To ensure the consistency of segmentation\nfor similar colors under different viewpoints, we convert the segmentation task\ninto a classification task through label volume, which significantly improves\nthe consistency of segmentation in color-similar areas. We also propose a\nsimplified text augmentation strategy to alleviate the issue of ambiguity in\nthe correspondence between CLIP features and text. Extensive experimental\nresults show that our method surpasses current state-of-the-art technologies in\nboth training speed and performance. Our code is available on:\nhttps://github.com/xingy038/Laser.git.\n", "link": "http://arxiv.org/abs/2501.19084v1", "date": "2025-01-31", "relevancy": 2.8522, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Laser%3A%20Efficient%20Language-Guided%20Segmentation%20in%20Neural%20Radiance%20Fields&body=Title%3A%20Laser%3A%20Efficient%20Language-Guided%20Segmentation%20in%20Neural%20Radiance%20Fields%0AAuthor%3A%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Bai%20and%20Tejal%20Shah%20and%20Jun%20Song%20and%20Yang%20Long%20and%20Rajiv%20Ranjan%20and%20Ling%20Shao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20leverages%20CLIP%20feature%20distillation%2C%0Aachieving%20efficient%203D%20segmentation%20through%20language%20guidance.%20Unlike%20previous%0Amethods%20that%20rely%20on%20multi-scale%20CLIP%20features%20and%20are%20limited%20by%20processing%0Aspeed%20and%20storage%20requirements%2C%20our%20approach%20aims%20to%20streamline%20the%20workflow%20by%0Adirectly%20and%20effectively%20distilling%20dense%20CLIP%20features%2C%20thereby%20achieving%0Aprecise%20segmentation%20of%203D%20scenes%20using%20text.%20To%20achieve%20this%2C%20we%20introduce%20an%0Aadapter%20module%20and%20mitigate%20the%20noise%20issue%20in%20the%20dense%20CLIP%20feature%0Adistillation%20process%20through%20a%20self-cross-training%20strategy.%20Moreover%2C%20to%0Aenhance%20the%20accuracy%20of%20segmentation%20edges%2C%20this%20work%20presents%20a%20low-rank%0Atransient%20query%20attention%20mechanism.%20To%20ensure%20the%20consistency%20of%20segmentation%0Afor%20similar%20colors%20under%20different%20viewpoints%2C%20we%20convert%20the%20segmentation%20task%0Ainto%20a%20classification%20task%20through%20label%20volume%2C%20which%20significantly%20improves%0Athe%20consistency%20of%20segmentation%20in%20color-similar%20areas.%20We%20also%20propose%20a%0Asimplified%20text%20augmentation%20strategy%20to%20alleviate%20the%20issue%20of%20ambiguity%20in%0Athe%20correspondence%20between%20CLIP%20features%20and%20text.%20Extensive%20experimental%0Aresults%20show%20that%20our%20method%20surpasses%20current%20state-of-the-art%20technologies%20in%0Aboth%20training%20speed%20and%20performance.%20Our%20code%20is%20available%20on%3A%0Ahttps%3A//github.com/xingy038/Laser.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaser%253A%2520Efficient%2520Language-Guided%2520Segmentation%2520in%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DXingyu%2520Miao%2520and%2520Haoran%2520Duan%2520and%2520Yang%2520Bai%2520and%2520Tejal%2520Shah%2520and%2520Jun%2520Song%2520and%2520Yang%2520Long%2520and%2520Rajiv%2520Ranjan%2520and%2520Ling%2520Shao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520that%2520leverages%2520CLIP%2520feature%2520distillation%252C%250Aachieving%2520efficient%25203D%2520segmentation%2520through%2520language%2520guidance.%2520Unlike%2520previous%250Amethods%2520that%2520rely%2520on%2520multi-scale%2520CLIP%2520features%2520and%2520are%2520limited%2520by%2520processing%250Aspeed%2520and%2520storage%2520requirements%252C%2520our%2520approach%2520aims%2520to%2520streamline%2520the%2520workflow%2520by%250Adirectly%2520and%2520effectively%2520distilling%2520dense%2520CLIP%2520features%252C%2520thereby%2520achieving%250Aprecise%2520segmentation%2520of%25203D%2520scenes%2520using%2520text.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520an%250Aadapter%2520module%2520and%2520mitigate%2520the%2520noise%2520issue%2520in%2520the%2520dense%2520CLIP%2520feature%250Adistillation%2520process%2520through%2520a%2520self-cross-training%2520strategy.%2520Moreover%252C%2520to%250Aenhance%2520the%2520accuracy%2520of%2520segmentation%2520edges%252C%2520this%2520work%2520presents%2520a%2520low-rank%250Atransient%2520query%2520attention%2520mechanism.%2520To%2520ensure%2520the%2520consistency%2520of%2520segmentation%250Afor%2520similar%2520colors%2520under%2520different%2520viewpoints%252C%2520we%2520convert%2520the%2520segmentation%2520task%250Ainto%2520a%2520classification%2520task%2520through%2520label%2520volume%252C%2520which%2520significantly%2520improves%250Athe%2520consistency%2520of%2520segmentation%2520in%2520color-similar%2520areas.%2520We%2520also%2520propose%2520a%250Asimplified%2520text%2520augmentation%2520strategy%2520to%2520alleviate%2520the%2520issue%2520of%2520ambiguity%2520in%250Athe%2520correspondence%2520between%2520CLIP%2520features%2520and%2520text.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520our%2520method%2520surpasses%2520current%2520state-of-the-art%2520technologies%2520in%250Aboth%2520training%2520speed%2520and%2520performance.%2520Our%2520code%2520is%2520available%2520on%253A%250Ahttps%253A//github.com/xingy038/Laser.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laser%3A%20Efficient%20Language-Guided%20Segmentation%20in%20Neural%20Radiance%20Fields&entry.906535625=Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Bai%20and%20Tejal%20Shah%20and%20Jun%20Song%20and%20Yang%20Long%20and%20Rajiv%20Ranjan%20and%20Ling%20Shao&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20leverages%20CLIP%20feature%20distillation%2C%0Aachieving%20efficient%203D%20segmentation%20through%20language%20guidance.%20Unlike%20previous%0Amethods%20that%20rely%20on%20multi-scale%20CLIP%20features%20and%20are%20limited%20by%20processing%0Aspeed%20and%20storage%20requirements%2C%20our%20approach%20aims%20to%20streamline%20the%20workflow%20by%0Adirectly%20and%20effectively%20distilling%20dense%20CLIP%20features%2C%20thereby%20achieving%0Aprecise%20segmentation%20of%203D%20scenes%20using%20text.%20To%20achieve%20this%2C%20we%20introduce%20an%0Aadapter%20module%20and%20mitigate%20the%20noise%20issue%20in%20the%20dense%20CLIP%20feature%0Adistillation%20process%20through%20a%20self-cross-training%20strategy.%20Moreover%2C%20to%0Aenhance%20the%20accuracy%20of%20segmentation%20edges%2C%20this%20work%20presents%20a%20low-rank%0Atransient%20query%20attention%20mechanism.%20To%20ensure%20the%20consistency%20of%20segmentation%0Afor%20similar%20colors%20under%20different%20viewpoints%2C%20we%20convert%20the%20segmentation%20task%0Ainto%20a%20classification%20task%20through%20label%20volume%2C%20which%20significantly%20improves%0Athe%20consistency%20of%20segmentation%20in%20color-similar%20areas.%20We%20also%20propose%20a%0Asimplified%20text%20augmentation%20strategy%20to%20alleviate%20the%20issue%20of%20ambiguity%20in%0Athe%20correspondence%20between%20CLIP%20features%20and%20text.%20Extensive%20experimental%0Aresults%20show%20that%20our%20method%20surpasses%20current%20state-of-the-art%20technologies%20in%0Aboth%20training%20speed%20and%20performance.%20Our%20code%20is%20available%20on%3A%0Ahttps%3A//github.com/xingy038/Laser.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19084v1&entry.124074799=Read"},
{"title": "LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention\n  Networks", "author": "Liudi Yang and Ruben Mascaro and Ignacio Alzugaray and Sai Manoj Prakhya and Marco Karrer and Ziyuan Liu and Margarita Chli", "abstract": "  In this paper, we propose a novel loop closure detection algorithm that uses\ngraph attention neural networks to encode semantic graphs to perform place\nrecognition and then use semantic registration to estimate the 6 DoF relative\npose constraint. Our place recognition algorithm has two key modules, namely, a\nsemantic graph encoder module and a graph comparison module. The semantic graph\nencoder employs graph attention networks to efficiently encode spatial,\nsemantic and geometric information from the semantic graph of the input point\ncloud. We then use self-attention mechanism in both node-embedding and\ngraph-embedding steps to create distinctive graph vectors. The graph vectors of\nthe current scan and a keyframe scan are then compared in the graph comparison\nmodule to identify a possible loop closure. Specifically, employing the\ndifference of the two graph vectors showed a significant improvement in\nperformance, as shown in ablation studies. Lastly, we implemented a semantic\nregistration algorithm that takes in loop closure candidate scans and estimates\nthe relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive\nevaluation on public datasets shows that our model is more accurate and robust,\nachieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,\nwhen compared to the baseline semantic graph algorithm. For the benefit of the\ncommunity, we open-source the complete implementation of our proposed algorithm\nand custom implementation of semantic registration at\nhttps://github.com/crepuscularlight/SemanticLoopClosure\n", "link": "http://arxiv.org/abs/2501.19382v1", "date": "2025-01-31", "relevancy": 2.7623, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR%20Loop%20Closure%20Detection%20using%20Semantic%20Graphs%20with%20Graph%20Attention%0A%20%20Networks&body=Title%3A%20LiDAR%20Loop%20Closure%20Detection%20using%20Semantic%20Graphs%20with%20Graph%20Attention%0A%20%20Networks%0AAuthor%3A%20Liudi%20Yang%20and%20Ruben%20Mascaro%20and%20Ignacio%20Alzugaray%20and%20Sai%20Manoj%20Prakhya%20and%20Marco%20Karrer%20and%20Ziyuan%20Liu%20and%20Margarita%20Chli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20loop%20closure%20detection%20algorithm%20that%20uses%0Agraph%20attention%20neural%20networks%20to%20encode%20semantic%20graphs%20to%20perform%20place%0Arecognition%20and%20then%20use%20semantic%20registration%20to%20estimate%20the%206%20DoF%20relative%0Apose%20constraint.%20Our%20place%20recognition%20algorithm%20has%20two%20key%20modules%2C%20namely%2C%20a%0Asemantic%20graph%20encoder%20module%20and%20a%20graph%20comparison%20module.%20The%20semantic%20graph%0Aencoder%20employs%20graph%20attention%20networks%20to%20efficiently%20encode%20spatial%2C%0Asemantic%20and%20geometric%20information%20from%20the%20semantic%20graph%20of%20the%20input%20point%0Acloud.%20We%20then%20use%20self-attention%20mechanism%20in%20both%20node-embedding%20and%0Agraph-embedding%20steps%20to%20create%20distinctive%20graph%20vectors.%20The%20graph%20vectors%20of%0Athe%20current%20scan%20and%20a%20keyframe%20scan%20are%20then%20compared%20in%20the%20graph%20comparison%0Amodule%20to%20identify%20a%20possible%20loop%20closure.%20Specifically%2C%20employing%20the%0Adifference%20of%20the%20two%20graph%20vectors%20showed%20a%20significant%20improvement%20in%0Aperformance%2C%20as%20shown%20in%20ablation%20studies.%20Lastly%2C%20we%20implemented%20a%20semantic%0Aregistration%20algorithm%20that%20takes%20in%20loop%20closure%20candidate%20scans%20and%20estimates%0Athe%20relative%206%20DoF%20pose%20constraint%20for%20the%20LiDAR%20SLAM%20system.%20Extensive%0Aevaluation%20on%20public%20datasets%20shows%20that%20our%20model%20is%20more%20accurate%20and%20robust%2C%0Aachieving%2013%25%20improvement%20in%20maximum%20F1%20score%20on%20the%20SemanticKITTI%20dataset%2C%0Awhen%20compared%20to%20the%20baseline%20semantic%20graph%20algorithm.%20For%20the%20benefit%20of%20the%0Acommunity%2C%20we%20open-source%20the%20complete%20implementation%20of%20our%20proposed%20algorithm%0Aand%20custom%20implementation%20of%20semantic%20registration%20at%0Ahttps%3A//github.com/crepuscularlight/SemanticLoopClosure%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR%2520Loop%2520Closure%2520Detection%2520using%2520Semantic%2520Graphs%2520with%2520Graph%2520Attention%250A%2520%2520Networks%26entry.906535625%3DLiudi%2520Yang%2520and%2520Ruben%2520Mascaro%2520and%2520Ignacio%2520Alzugaray%2520and%2520Sai%2520Manoj%2520Prakhya%2520and%2520Marco%2520Karrer%2520and%2520Ziyuan%2520Liu%2520and%2520Margarita%2520Chli%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520loop%2520closure%2520detection%2520algorithm%2520that%2520uses%250Agraph%2520attention%2520neural%2520networks%2520to%2520encode%2520semantic%2520graphs%2520to%2520perform%2520place%250Arecognition%2520and%2520then%2520use%2520semantic%2520registration%2520to%2520estimate%2520the%25206%2520DoF%2520relative%250Apose%2520constraint.%2520Our%2520place%2520recognition%2520algorithm%2520has%2520two%2520key%2520modules%252C%2520namely%252C%2520a%250Asemantic%2520graph%2520encoder%2520module%2520and%2520a%2520graph%2520comparison%2520module.%2520The%2520semantic%2520graph%250Aencoder%2520employs%2520graph%2520attention%2520networks%2520to%2520efficiently%2520encode%2520spatial%252C%250Asemantic%2520and%2520geometric%2520information%2520from%2520the%2520semantic%2520graph%2520of%2520the%2520input%2520point%250Acloud.%2520We%2520then%2520use%2520self-attention%2520mechanism%2520in%2520both%2520node-embedding%2520and%250Agraph-embedding%2520steps%2520to%2520create%2520distinctive%2520graph%2520vectors.%2520The%2520graph%2520vectors%2520of%250Athe%2520current%2520scan%2520and%2520a%2520keyframe%2520scan%2520are%2520then%2520compared%2520in%2520the%2520graph%2520comparison%250Amodule%2520to%2520identify%2520a%2520possible%2520loop%2520closure.%2520Specifically%252C%2520employing%2520the%250Adifference%2520of%2520the%2520two%2520graph%2520vectors%2520showed%2520a%2520significant%2520improvement%2520in%250Aperformance%252C%2520as%2520shown%2520in%2520ablation%2520studies.%2520Lastly%252C%2520we%2520implemented%2520a%2520semantic%250Aregistration%2520algorithm%2520that%2520takes%2520in%2520loop%2520closure%2520candidate%2520scans%2520and%2520estimates%250Athe%2520relative%25206%2520DoF%2520pose%2520constraint%2520for%2520the%2520LiDAR%2520SLAM%2520system.%2520Extensive%250Aevaluation%2520on%2520public%2520datasets%2520shows%2520that%2520our%2520model%2520is%2520more%2520accurate%2520and%2520robust%252C%250Aachieving%252013%2525%2520improvement%2520in%2520maximum%2520F1%2520score%2520on%2520the%2520SemanticKITTI%2520dataset%252C%250Awhen%2520compared%2520to%2520the%2520baseline%2520semantic%2520graph%2520algorithm.%2520For%2520the%2520benefit%2520of%2520the%250Acommunity%252C%2520we%2520open-source%2520the%2520complete%2520implementation%2520of%2520our%2520proposed%2520algorithm%250Aand%2520custom%2520implementation%2520of%2520semantic%2520registration%2520at%250Ahttps%253A//github.com/crepuscularlight/SemanticLoopClosure%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR%20Loop%20Closure%20Detection%20using%20Semantic%20Graphs%20with%20Graph%20Attention%0A%20%20Networks&entry.906535625=Liudi%20Yang%20and%20Ruben%20Mascaro%20and%20Ignacio%20Alzugaray%20and%20Sai%20Manoj%20Prakhya%20and%20Marco%20Karrer%20and%20Ziyuan%20Liu%20and%20Margarita%20Chli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20loop%20closure%20detection%20algorithm%20that%20uses%0Agraph%20attention%20neural%20networks%20to%20encode%20semantic%20graphs%20to%20perform%20place%0Arecognition%20and%20then%20use%20semantic%20registration%20to%20estimate%20the%206%20DoF%20relative%0Apose%20constraint.%20Our%20place%20recognition%20algorithm%20has%20two%20key%20modules%2C%20namely%2C%20a%0Asemantic%20graph%20encoder%20module%20and%20a%20graph%20comparison%20module.%20The%20semantic%20graph%0Aencoder%20employs%20graph%20attention%20networks%20to%20efficiently%20encode%20spatial%2C%0Asemantic%20and%20geometric%20information%20from%20the%20semantic%20graph%20of%20the%20input%20point%0Acloud.%20We%20then%20use%20self-attention%20mechanism%20in%20both%20node-embedding%20and%0Agraph-embedding%20steps%20to%20create%20distinctive%20graph%20vectors.%20The%20graph%20vectors%20of%0Athe%20current%20scan%20and%20a%20keyframe%20scan%20are%20then%20compared%20in%20the%20graph%20comparison%0Amodule%20to%20identify%20a%20possible%20loop%20closure.%20Specifically%2C%20employing%20the%0Adifference%20of%20the%20two%20graph%20vectors%20showed%20a%20significant%20improvement%20in%0Aperformance%2C%20as%20shown%20in%20ablation%20studies.%20Lastly%2C%20we%20implemented%20a%20semantic%0Aregistration%20algorithm%20that%20takes%20in%20loop%20closure%20candidate%20scans%20and%20estimates%0Athe%20relative%206%20DoF%20pose%20constraint%20for%20the%20LiDAR%20SLAM%20system.%20Extensive%0Aevaluation%20on%20public%20datasets%20shows%20that%20our%20model%20is%20more%20accurate%20and%20robust%2C%0Aachieving%2013%25%20improvement%20in%20maximum%20F1%20score%20on%20the%20SemanticKITTI%20dataset%2C%0Awhen%20compared%20to%20the%20baseline%20semantic%20graph%20algorithm.%20For%20the%20benefit%20of%20the%0Acommunity%2C%20we%20open-source%20the%20complete%20implementation%20of%20our%20proposed%20algorithm%0Aand%20custom%20implementation%20of%20semantic%20registration%20at%0Ahttps%3A//github.com/crepuscularlight/SemanticLoopClosure%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19382v1&entry.124074799=Read"},
{"title": "A Generic Hybrid Framework for 2D Visual Reconstruction", "author": "Daniel Rika and Dror Sholomon and Eli David and Alexandre Pais and Nathan S. Netanyahu", "abstract": "  This paper presents a versatile hybrid framework for addressing 2D real-world\nreconstruction tasks formulated as jigsaw puzzle problems (JPPs) with square,\nnon-overlapping pieces. Our approach integrates a deep learning (DL)-based\ncompatibility measure (CM) model that evaluates pairs of puzzle pieces\nholistically, rather than focusing solely on their adjacent edges as\ntraditionally done. This DL-based CM is paired with an optimized genetic\nalgorithm (GA)-based solver, which iteratively searches for a global optimal\narrangement using the pairwise CM scores of the puzzle pieces. Extensive\nexperimental results highlight the framework's adaptability and robustness\nacross multiple real-world domains. Notably, our unique hybrid methodology\nachieves state-of-the-art (SOTA) results in reconstructing Portuguese tile\npanels and large degraded puzzles with eroded boundaries.\n", "link": "http://arxiv.org/abs/2501.19325v1", "date": "2025-01-31", "relevancy": 2.7608, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5636}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5501}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generic%20Hybrid%20Framework%20for%202D%20Visual%20Reconstruction&body=Title%3A%20A%20Generic%20Hybrid%20Framework%20for%202D%20Visual%20Reconstruction%0AAuthor%3A%20Daniel%20Rika%20and%20Dror%20Sholomon%20and%20Eli%20David%20and%20Alexandre%20Pais%20and%20Nathan%20S.%20Netanyahu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20versatile%20hybrid%20framework%20for%20addressing%202D%20real-world%0Areconstruction%20tasks%20formulated%20as%20jigsaw%20puzzle%20problems%20%28JPPs%29%20with%20square%2C%0Anon-overlapping%20pieces.%20Our%20approach%20integrates%20a%20deep%20learning%20%28DL%29-based%0Acompatibility%20measure%20%28CM%29%20model%20that%20evaluates%20pairs%20of%20puzzle%20pieces%0Aholistically%2C%20rather%20than%20focusing%20solely%20on%20their%20adjacent%20edges%20as%0Atraditionally%20done.%20This%20DL-based%20CM%20is%20paired%20with%20an%20optimized%20genetic%0Aalgorithm%20%28GA%29-based%20solver%2C%20which%20iteratively%20searches%20for%20a%20global%20optimal%0Aarrangement%20using%20the%20pairwise%20CM%20scores%20of%20the%20puzzle%20pieces.%20Extensive%0Aexperimental%20results%20highlight%20the%20framework%27s%20adaptability%20and%20robustness%0Aacross%20multiple%20real-world%20domains.%20Notably%2C%20our%20unique%20hybrid%20methodology%0Aachieves%20state-of-the-art%20%28SOTA%29%20results%20in%20reconstructing%20Portuguese%20tile%0Apanels%20and%20large%20degraded%20puzzles%20with%20eroded%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generic%2520Hybrid%2520Framework%2520for%25202D%2520Visual%2520Reconstruction%26entry.906535625%3DDaniel%2520Rika%2520and%2520Dror%2520Sholomon%2520and%2520Eli%2520David%2520and%2520Alexandre%2520Pais%2520and%2520Nathan%2520S.%2520Netanyahu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520versatile%2520hybrid%2520framework%2520for%2520addressing%25202D%2520real-world%250Areconstruction%2520tasks%2520formulated%2520as%2520jigsaw%2520puzzle%2520problems%2520%2528JPPs%2529%2520with%2520square%252C%250Anon-overlapping%2520pieces.%2520Our%2520approach%2520integrates%2520a%2520deep%2520learning%2520%2528DL%2529-based%250Acompatibility%2520measure%2520%2528CM%2529%2520model%2520that%2520evaluates%2520pairs%2520of%2520puzzle%2520pieces%250Aholistically%252C%2520rather%2520than%2520focusing%2520solely%2520on%2520their%2520adjacent%2520edges%2520as%250Atraditionally%2520done.%2520This%2520DL-based%2520CM%2520is%2520paired%2520with%2520an%2520optimized%2520genetic%250Aalgorithm%2520%2528GA%2529-based%2520solver%252C%2520which%2520iteratively%2520searches%2520for%2520a%2520global%2520optimal%250Aarrangement%2520using%2520the%2520pairwise%2520CM%2520scores%2520of%2520the%2520puzzle%2520pieces.%2520Extensive%250Aexperimental%2520results%2520highlight%2520the%2520framework%2527s%2520adaptability%2520and%2520robustness%250Aacross%2520multiple%2520real-world%2520domains.%2520Notably%252C%2520our%2520unique%2520hybrid%2520methodology%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520in%2520reconstructing%2520Portuguese%2520tile%250Apanels%2520and%2520large%2520degraded%2520puzzles%2520with%2520eroded%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generic%20Hybrid%20Framework%20for%202D%20Visual%20Reconstruction&entry.906535625=Daniel%20Rika%20and%20Dror%20Sholomon%20and%20Eli%20David%20and%20Alexandre%20Pais%20and%20Nathan%20S.%20Netanyahu&entry.1292438233=%20%20This%20paper%20presents%20a%20versatile%20hybrid%20framework%20for%20addressing%202D%20real-world%0Areconstruction%20tasks%20formulated%20as%20jigsaw%20puzzle%20problems%20%28JPPs%29%20with%20square%2C%0Anon-overlapping%20pieces.%20Our%20approach%20integrates%20a%20deep%20learning%20%28DL%29-based%0Acompatibility%20measure%20%28CM%29%20model%20that%20evaluates%20pairs%20of%20puzzle%20pieces%0Aholistically%2C%20rather%20than%20focusing%20solely%20on%20their%20adjacent%20edges%20as%0Atraditionally%20done.%20This%20DL-based%20CM%20is%20paired%20with%20an%20optimized%20genetic%0Aalgorithm%20%28GA%29-based%20solver%2C%20which%20iteratively%20searches%20for%20a%20global%20optimal%0Aarrangement%20using%20the%20pairwise%20CM%20scores%20of%20the%20puzzle%20pieces.%20Extensive%0Aexperimental%20results%20highlight%20the%20framework%27s%20adaptability%20and%20robustness%0Aacross%20multiple%20real-world%20domains.%20Notably%2C%20our%20unique%20hybrid%20methodology%0Aachieves%20state-of-the-art%20%28SOTA%29%20results%20in%20reconstructing%20Portuguese%20tile%0Apanels%20and%20large%20degraded%20puzzles%20with%20eroded%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19325v1&entry.124074799=Read"},
{"title": "Scalable Multi-phase Word Embedding Using Conjunctive Propositional\n  Clauses", "author": "Ahmed K. Kadhim and Lei Jiao and Rishad Shafik and Ole-Christoffer Granmo and Bimal Bhattarai", "abstract": "  The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness\nin Machine Learning (ML), particularly within Natural Language Processing\n(NLP). It has been utilized to construct word embedding using conjunctive\npropositional clauses, thereby significantly enhancing our understanding and\ninterpretation of machine-derived decisions. The previous approach performed\nthe word embedding over a sequence of input words to consolidate the\ninformation into a cohesive and unified representation. However, that approach\nencounters scalability challenges as the input size increases. In this study,\nwe introduce a novel approach incorporating two-phase training to discover\ncontextual embeddings of input sequences. Specifically, this method\nencapsulates the knowledge for each input word within the dataset's vocabulary,\nsubsequently constructing embeddings for a sequence of input words utilizing\nthe extracted knowledge. This technique not only facilitates the design of a\nscalable model but also preserves interpretability. Our experimental findings\nrevealed that the proposed method yields competitive performance compared to\nthe previous approaches, demonstrating promising results in contrast to\nhuman-generated benchmarks. Furthermore, we applied the proposed approach to\nsentiment analysis on the IMDB dataset, where the TM embedding and the TM\nclassifier, along with other interpretable classifiers, offered a transparent\nend-to-end solution with competitive performance.\n", "link": "http://arxiv.org/abs/2501.19018v1", "date": "2025-01-31", "relevancy": 2.7212, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses&body=Title%3A%20Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses%0AAuthor%3A%20Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo%20and%20Bimal%20Bhattarai%0AAbstract%3A%20%20%20The%20Tsetlin%20Machine%20%28TM%29%20architecture%20has%20recently%20demonstrated%20effectiveness%0Ain%20Machine%20Learning%20%28ML%29%2C%20particularly%20within%20Natural%20Language%20Processing%0A%28NLP%29.%20It%20has%20been%20utilized%20to%20construct%20word%20embedding%20using%20conjunctive%0Apropositional%20clauses%2C%20thereby%20significantly%20enhancing%20our%20understanding%20and%0Ainterpretation%20of%20machine-derived%20decisions.%20The%20previous%20approach%20performed%0Athe%20word%20embedding%20over%20a%20sequence%20of%20input%20words%20to%20consolidate%20the%0Ainformation%20into%20a%20cohesive%20and%20unified%20representation.%20However%2C%20that%20approach%0Aencounters%20scalability%20challenges%20as%20the%20input%20size%20increases.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20approach%20incorporating%20two-phase%20training%20to%20discover%0Acontextual%20embeddings%20of%20input%20sequences.%20Specifically%2C%20this%20method%0Aencapsulates%20the%20knowledge%20for%20each%20input%20word%20within%20the%20dataset%27s%20vocabulary%2C%0Asubsequently%20constructing%20embeddings%20for%20a%20sequence%20of%20input%20words%20utilizing%0Athe%20extracted%20knowledge.%20This%20technique%20not%20only%20facilitates%20the%20design%20of%20a%0Ascalable%20model%20but%20also%20preserves%20interpretability.%20Our%20experimental%20findings%0Arevealed%20that%20the%20proposed%20method%20yields%20competitive%20performance%20compared%20to%0Athe%20previous%20approaches%2C%20demonstrating%20promising%20results%20in%20contrast%20to%0Ahuman-generated%20benchmarks.%20Furthermore%2C%20we%20applied%20the%20proposed%20approach%20to%0Asentiment%20analysis%20on%20the%20IMDB%20dataset%2C%20where%20the%20TM%20embedding%20and%20the%20TM%0Aclassifier%2C%20along%20with%20other%20interpretable%20classifiers%2C%20offered%20a%20transparent%0Aend-to-end%20solution%20with%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-phase%2520Word%2520Embedding%2520Using%2520Conjunctive%2520Propositional%250A%2520%2520Clauses%26entry.906535625%3DAhmed%2520K.%2520Kadhim%2520and%2520Lei%2520Jiao%2520and%2520Rishad%2520Shafik%2520and%2520Ole-Christoffer%2520Granmo%2520and%2520Bimal%2520Bhattarai%26entry.1292438233%3D%2520%2520The%2520Tsetlin%2520Machine%2520%2528TM%2529%2520architecture%2520has%2520recently%2520demonstrated%2520effectiveness%250Ain%2520Machine%2520Learning%2520%2528ML%2529%252C%2520particularly%2520within%2520Natural%2520Language%2520Processing%250A%2528NLP%2529.%2520It%2520has%2520been%2520utilized%2520to%2520construct%2520word%2520embedding%2520using%2520conjunctive%250Apropositional%2520clauses%252C%2520thereby%2520significantly%2520enhancing%2520our%2520understanding%2520and%250Ainterpretation%2520of%2520machine-derived%2520decisions.%2520The%2520previous%2520approach%2520performed%250Athe%2520word%2520embedding%2520over%2520a%2520sequence%2520of%2520input%2520words%2520to%2520consolidate%2520the%250Ainformation%2520into%2520a%2520cohesive%2520and%2520unified%2520representation.%2520However%252C%2520that%2520approach%250Aencounters%2520scalability%2520challenges%2520as%2520the%2520input%2520size%2520increases.%2520In%2520this%2520study%252C%250Awe%2520introduce%2520a%2520novel%2520approach%2520incorporating%2520two-phase%2520training%2520to%2520discover%250Acontextual%2520embeddings%2520of%2520input%2520sequences.%2520Specifically%252C%2520this%2520method%250Aencapsulates%2520the%2520knowledge%2520for%2520each%2520input%2520word%2520within%2520the%2520dataset%2527s%2520vocabulary%252C%250Asubsequently%2520constructing%2520embeddings%2520for%2520a%2520sequence%2520of%2520input%2520words%2520utilizing%250Athe%2520extracted%2520knowledge.%2520This%2520technique%2520not%2520only%2520facilitates%2520the%2520design%2520of%2520a%250Ascalable%2520model%2520but%2520also%2520preserves%2520interpretability.%2520Our%2520experimental%2520findings%250Arevealed%2520that%2520the%2520proposed%2520method%2520yields%2520competitive%2520performance%2520compared%2520to%250Athe%2520previous%2520approaches%252C%2520demonstrating%2520promising%2520results%2520in%2520contrast%2520to%250Ahuman-generated%2520benchmarks.%2520Furthermore%252C%2520we%2520applied%2520the%2520proposed%2520approach%2520to%250Asentiment%2520analysis%2520on%2520the%2520IMDB%2520dataset%252C%2520where%2520the%2520TM%2520embedding%2520and%2520the%2520TM%250Aclassifier%252C%2520along%2520with%2520other%2520interpretable%2520classifiers%252C%2520offered%2520a%2520transparent%250Aend-to-end%2520solution%2520with%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses&entry.906535625=Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo%20and%20Bimal%20Bhattarai&entry.1292438233=%20%20The%20Tsetlin%20Machine%20%28TM%29%20architecture%20has%20recently%20demonstrated%20effectiveness%0Ain%20Machine%20Learning%20%28ML%29%2C%20particularly%20within%20Natural%20Language%20Processing%0A%28NLP%29.%20It%20has%20been%20utilized%20to%20construct%20word%20embedding%20using%20conjunctive%0Apropositional%20clauses%2C%20thereby%20significantly%20enhancing%20our%20understanding%20and%0Ainterpretation%20of%20machine-derived%20decisions.%20The%20previous%20approach%20performed%0Athe%20word%20embedding%20over%20a%20sequence%20of%20input%20words%20to%20consolidate%20the%0Ainformation%20into%20a%20cohesive%20and%20unified%20representation.%20However%2C%20that%20approach%0Aencounters%20scalability%20challenges%20as%20the%20input%20size%20increases.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20approach%20incorporating%20two-phase%20training%20to%20discover%0Acontextual%20embeddings%20of%20input%20sequences.%20Specifically%2C%20this%20method%0Aencapsulates%20the%20knowledge%20for%20each%20input%20word%20within%20the%20dataset%27s%20vocabulary%2C%0Asubsequently%20constructing%20embeddings%20for%20a%20sequence%20of%20input%20words%20utilizing%0Athe%20extracted%20knowledge.%20This%20technique%20not%20only%20facilitates%20the%20design%20of%20a%0Ascalable%20model%20but%20also%20preserves%20interpretability.%20Our%20experimental%20findings%0Arevealed%20that%20the%20proposed%20method%20yields%20competitive%20performance%20compared%20to%0Athe%20previous%20approaches%2C%20demonstrating%20promising%20results%20in%20contrast%20to%0Ahuman-generated%20benchmarks.%20Furthermore%2C%20we%20applied%20the%20proposed%20approach%20to%0Asentiment%20analysis%20on%20the%20IMDB%20dataset%2C%20where%20the%20TM%20embedding%20and%20the%20TM%0Aclassifier%2C%20along%20with%20other%20interpretable%20classifiers%2C%20offered%20a%20transparent%0Aend-to-end%20solution%20with%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19018v1&entry.124074799=Read"},
{"title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment", "author": "Song-Lin Lv and Yu-Yang Chen and Zhi Zhou and Yu-Feng Li and Lan-Zhe Guo", "abstract": "  Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.\n", "link": "http://arxiv.org/abs/2501.19060v1", "date": "2025-01-31", "relevancy": 2.7026, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment&body=Title%3A%20Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment%0AAuthor%3A%20Song-Lin%20Lv%20and%20Yu-Yang%20Chen%20and%20Zhi%20Zhou%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20exceptional%0Ageneralization%20capabilities%20and%20can%20quickly%20adapt%20to%20downstream%20tasks%20through%0Aprompt%20fine-tuning.%20Unfortunately%2C%20in%20classification%20tasks%20involving%0Anon-training%20classes%2C%20known%20as%20open-vocabulary%20setting%2C%20fine-tuned%20VLMs%20often%0Aoverfit%20to%20train%20classes%2C%20resulting%20in%20a%20misalignment%20between%20confidence%20scores%0Aand%20actual%20accuracy%20on%20unseen%20classes%2C%20which%20significantly%20undermines%20their%0Areliability%20in%20real-world%20deployments.%20Existing%20confidence%20calibration%20methods%0Atypically%20require%20training%20parameters%20or%20analyzing%20features%20from%20the%20training%0Adataset%2C%20restricting%20their%20ability%20to%20generalize%20unseen%20classes%20without%0Acorresponding%20train%20data.%20Moreover%2C%20VLM-specific%20calibration%20methods%20rely%0Asolely%20on%20text%20features%20from%20train%20classes%20as%20calibration%20indicators%2C%20which%0Ainherently%20limits%20their%20ability%20to%20calibrate%20train%20classes.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20effective%20multimodal%20calibration%20method%0AContrast-Aware%20Calibration%20%28CAC%29.%20Building%20on%20the%20original%20CLIP%27s%20zero-shot%0Aadaptability%20and%20the%20conclusion%20from%20empirical%20analysis%20that%20poor%20intra-class%0Aand%20inter-class%20discriminative%20ability%20on%20unseen%20classes%20is%20the%20root%20cause%2C%20we%0Acalculate%20calibration%20weights%20based%20on%20the%20contrastive%20difference%20between%20the%0Aoriginal%20and%20fine-tuned%20CLIP.%20This%20method%20not%20only%20adapts%20to%20calibrating%20unseen%0Aclasses%20but%20also%20overcomes%20the%20limitations%20of%20previous%20VLM%20calibration%20methods%0Athat%20could%20not%20calibrate%20train%20classes.%20In%20experiments%20involving%2011%20datasets%0Awith%205%20fine-tuning%20methods%2C%20CAC%20consistently%20achieved%20the%20best%20calibration%0Aeffect%20on%20both%20train%20and%20unseen%20classes%20without%20sacrificing%20accuracy%20and%0Ainference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast-Aware%2520Calibration%2520for%2520Fine-Tuned%2520CLIP%253A%2520Leveraging%2520Image-Text%250A%2520%2520Alignment%26entry.906535625%3DSong-Lin%2520Lv%2520and%2520Yu-Yang%2520Chen%2520and%2520Zhi%2520Zhou%2520and%2520Yu-Feng%2520Li%2520and%2520Lan-Zhe%2520Guo%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%2520exceptional%250Ageneralization%2520capabilities%2520and%2520can%2520quickly%2520adapt%2520to%2520downstream%2520tasks%2520through%250Aprompt%2520fine-tuning.%2520Unfortunately%252C%2520in%2520classification%2520tasks%2520involving%250Anon-training%2520classes%252C%2520known%2520as%2520open-vocabulary%2520setting%252C%2520fine-tuned%2520VLMs%2520often%250Aoverfit%2520to%2520train%2520classes%252C%2520resulting%2520in%2520a%2520misalignment%2520between%2520confidence%2520scores%250Aand%2520actual%2520accuracy%2520on%2520unseen%2520classes%252C%2520which%2520significantly%2520undermines%2520their%250Areliability%2520in%2520real-world%2520deployments.%2520Existing%2520confidence%2520calibration%2520methods%250Atypically%2520require%2520training%2520parameters%2520or%2520analyzing%2520features%2520from%2520the%2520training%250Adataset%252C%2520restricting%2520their%2520ability%2520to%2520generalize%2520unseen%2520classes%2520without%250Acorresponding%2520train%2520data.%2520Moreover%252C%2520VLM-specific%2520calibration%2520methods%2520rely%250Asolely%2520on%2520text%2520features%2520from%2520train%2520classes%2520as%2520calibration%2520indicators%252C%2520which%250Ainherently%2520limits%2520their%2520ability%2520to%2520calibrate%2520train%2520classes.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520an%2520effective%2520multimodal%2520calibration%2520method%250AContrast-Aware%2520Calibration%2520%2528CAC%2529.%2520Building%2520on%2520the%2520original%2520CLIP%2527s%2520zero-shot%250Aadaptability%2520and%2520the%2520conclusion%2520from%2520empirical%2520analysis%2520that%2520poor%2520intra-class%250Aand%2520inter-class%2520discriminative%2520ability%2520on%2520unseen%2520classes%2520is%2520the%2520root%2520cause%252C%2520we%250Acalculate%2520calibration%2520weights%2520based%2520on%2520the%2520contrastive%2520difference%2520between%2520the%250Aoriginal%2520and%2520fine-tuned%2520CLIP.%2520This%2520method%2520not%2520only%2520adapts%2520to%2520calibrating%2520unseen%250Aclasses%2520but%2520also%2520overcomes%2520the%2520limitations%2520of%2520previous%2520VLM%2520calibration%2520methods%250Athat%2520could%2520not%2520calibrate%2520train%2520classes.%2520In%2520experiments%2520involving%252011%2520datasets%250Awith%25205%2520fine-tuning%2520methods%252C%2520CAC%2520consistently%2520achieved%2520the%2520best%2520calibration%250Aeffect%2520on%2520both%2520train%2520and%2520unseen%2520classes%2520without%2520sacrificing%2520accuracy%2520and%250Ainference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment&entry.906535625=Song-Lin%20Lv%20and%20Yu-Yang%20Chen%20and%20Zhi%20Zhou%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20exceptional%0Ageneralization%20capabilities%20and%20can%20quickly%20adapt%20to%20downstream%20tasks%20through%0Aprompt%20fine-tuning.%20Unfortunately%2C%20in%20classification%20tasks%20involving%0Anon-training%20classes%2C%20known%20as%20open-vocabulary%20setting%2C%20fine-tuned%20VLMs%20often%0Aoverfit%20to%20train%20classes%2C%20resulting%20in%20a%20misalignment%20between%20confidence%20scores%0Aand%20actual%20accuracy%20on%20unseen%20classes%2C%20which%20significantly%20undermines%20their%0Areliability%20in%20real-world%20deployments.%20Existing%20confidence%20calibration%20methods%0Atypically%20require%20training%20parameters%20or%20analyzing%20features%20from%20the%20training%0Adataset%2C%20restricting%20their%20ability%20to%20generalize%20unseen%20classes%20without%0Acorresponding%20train%20data.%20Moreover%2C%20VLM-specific%20calibration%20methods%20rely%0Asolely%20on%20text%20features%20from%20train%20classes%20as%20calibration%20indicators%2C%20which%0Ainherently%20limits%20their%20ability%20to%20calibrate%20train%20classes.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20effective%20multimodal%20calibration%20method%0AContrast-Aware%20Calibration%20%28CAC%29.%20Building%20on%20the%20original%20CLIP%27s%20zero-shot%0Aadaptability%20and%20the%20conclusion%20from%20empirical%20analysis%20that%20poor%20intra-class%0Aand%20inter-class%20discriminative%20ability%20on%20unseen%20classes%20is%20the%20root%20cause%2C%20we%0Acalculate%20calibration%20weights%20based%20on%20the%20contrastive%20difference%20between%20the%0Aoriginal%20and%20fine-tuned%20CLIP.%20This%20method%20not%20only%20adapts%20to%20calibrating%20unseen%0Aclasses%20but%20also%20overcomes%20the%20limitations%20of%20previous%20VLM%20calibration%20methods%0Athat%20could%20not%20calibrate%20train%20classes.%20In%20experiments%20involving%2011%20datasets%0Awith%205%20fine-tuning%20methods%2C%20CAC%20consistently%20achieved%20the%20best%20calibration%0Aeffect%20on%20both%20train%20and%20unseen%20classes%20without%20sacrificing%20accuracy%20and%0Ainference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19060v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models", "author": "Michael Lan and Philip Torr and Austin Meek and Ashkan Khakzar and David Krueger and Fazl Barez", "abstract": "  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.\n", "link": "http://arxiv.org/abs/2410.06981v2", "date": "2025-01-31", "relevancy": 2.6955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5544}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models&body=Title%3A%20Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Michael%20Lan%20and%20Philip%20Torr%20and%20Austin%20Meek%20and%20Ashkan%20Khakzar%20and%20David%20Krueger%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20We%20investigate%20feature%20universality%20in%20large%20language%20models%20%28LLMs%29%2C%20a%0Aresearch%20field%20that%20aims%20to%20understand%20how%20different%20models%20similarly%20represent%0Aconcepts%20in%20the%20latent%20spaces%20of%20their%20intermediate%20layers.%20Demonstrating%0Afeature%20universality%20allows%20discoveries%20about%20latent%20representations%20to%0Ageneralize%20across%20several%20models.%20However%2C%20comparing%20features%20across%20LLMs%20is%0Achallenging%20due%20to%20polysemanticity%2C%20in%20which%20individual%20neurons%20often%0Acorrespond%20to%20multiple%20features%20rather%20than%20distinct%20ones%2C%20making%20it%20difficult%0Ato%20disentangle%20and%20match%20features%20across%20different%20models.%20To%20address%20this%0Aissue%2C%20we%20employ%20a%20method%20known%20as%20dictionary%20learning%20by%20using%20sparse%0Aautoencoders%20%28SAEs%29%20to%20transform%20LLM%20activations%20into%20more%20interpretable%20spaces%0Aspanned%20by%20neurons%20corresponding%20to%20individual%20features.%20After%20matching%20feature%0Aneurons%20across%20models%20via%20activation%20correlation%2C%20we%20apply%20representational%0Aspace%20similarity%20metrics%20on%20SAE%20feature%20spaces%20across%20different%20LLMs.%20Our%0Aexperiments%20reveal%20significant%20similarities%20in%20SAE%20feature%20spaces%20across%0Avarious%20LLMs%2C%20providing%20new%20evidence%20for%20feature%20universality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Reveal%2520Universal%2520Feature%2520Spaces%2520Across%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMichael%2520Lan%2520and%2520Philip%2520Torr%2520and%2520Austin%2520Meek%2520and%2520Ashkan%2520Khakzar%2520and%2520David%2520Krueger%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520We%2520investigate%2520feature%2520universality%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%250Aresearch%2520field%2520that%2520aims%2520to%2520understand%2520how%2520different%2520models%2520similarly%2520represent%250Aconcepts%2520in%2520the%2520latent%2520spaces%2520of%2520their%2520intermediate%2520layers.%2520Demonstrating%250Afeature%2520universality%2520allows%2520discoveries%2520about%2520latent%2520representations%2520to%250Ageneralize%2520across%2520several%2520models.%2520However%252C%2520comparing%2520features%2520across%2520LLMs%2520is%250Achallenging%2520due%2520to%2520polysemanticity%252C%2520in%2520which%2520individual%2520neurons%2520often%250Acorrespond%2520to%2520multiple%2520features%2520rather%2520than%2520distinct%2520ones%252C%2520making%2520it%2520difficult%250Ato%2520disentangle%2520and%2520match%2520features%2520across%2520different%2520models.%2520To%2520address%2520this%250Aissue%252C%2520we%2520employ%2520a%2520method%2520known%2520as%2520dictionary%2520learning%2520by%2520using%2520sparse%250Aautoencoders%2520%2528SAEs%2529%2520to%2520transform%2520LLM%2520activations%2520into%2520more%2520interpretable%2520spaces%250Aspanned%2520by%2520neurons%2520corresponding%2520to%2520individual%2520features.%2520After%2520matching%2520feature%250Aneurons%2520across%2520models%2520via%2520activation%2520correlation%252C%2520we%2520apply%2520representational%250Aspace%2520similarity%2520metrics%2520on%2520SAE%2520feature%2520spaces%2520across%2520different%2520LLMs.%2520Our%250Aexperiments%2520reveal%2520significant%2520similarities%2520in%2520SAE%2520feature%2520spaces%2520across%250Avarious%2520LLMs%252C%2520providing%2520new%2520evidence%2520for%2520feature%2520universality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models&entry.906535625=Michael%20Lan%20and%20Philip%20Torr%20and%20Austin%20Meek%20and%20Ashkan%20Khakzar%20and%20David%20Krueger%20and%20Fazl%20Barez&entry.1292438233=%20%20We%20investigate%20feature%20universality%20in%20large%20language%20models%20%28LLMs%29%2C%20a%0Aresearch%20field%20that%20aims%20to%20understand%20how%20different%20models%20similarly%20represent%0Aconcepts%20in%20the%20latent%20spaces%20of%20their%20intermediate%20layers.%20Demonstrating%0Afeature%20universality%20allows%20discoveries%20about%20latent%20representations%20to%0Ageneralize%20across%20several%20models.%20However%2C%20comparing%20features%20across%20LLMs%20is%0Achallenging%20due%20to%20polysemanticity%2C%20in%20which%20individual%20neurons%20often%0Acorrespond%20to%20multiple%20features%20rather%20than%20distinct%20ones%2C%20making%20it%20difficult%0Ato%20disentangle%20and%20match%20features%20across%20different%20models.%20To%20address%20this%0Aissue%2C%20we%20employ%20a%20method%20known%20as%20dictionary%20learning%20by%20using%20sparse%0Aautoencoders%20%28SAEs%29%20to%20transform%20LLM%20activations%20into%20more%20interpretable%20spaces%0Aspanned%20by%20neurons%20corresponding%20to%20individual%20features.%20After%20matching%20feature%0Aneurons%20across%20models%20via%20activation%20correlation%2C%20we%20apply%20representational%0Aspace%20similarity%20metrics%20on%20SAE%20feature%20spaces%20across%20different%20LLMs.%20Our%0Aexperiments%20reveal%20significant%20similarities%20in%20SAE%20feature%20spaces%20across%0Avarious%20LLMs%2C%20providing%20new%20evidence%20for%20feature%20universality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06981v2&entry.124074799=Read"},
{"title": "Learning positional encodings in transformers depends on initialization", "author": "Takuya Ito and Luca Cocchi and Tim Klinger and Parikshit Ram and Murray Campbell and Luke Hearne", "abstract": "  The attention mechanism is central to the transformer's ability to capture\ncomplex dependencies between tokens of an input sequence. Key to the successful\napplication of the attention mechanism in transformers is its choice of\npositional encoding (PE). The PE provides essential information that\ndistinguishes the position and order amongst tokens in a sequence. Most prior\ninvestigations of PE effects on generalization were tailored to 1D input\nsequences, such as those presented in natural language, where adjacent tokens\n(e.g., words) are highly related. In contrast, many real world tasks involve\ndatasets with highly non-trivial positional arrangements, such as datasets\norganized in multiple spatial dimensions, or datasets for which ground truth\npositions are not known, such as in biological data. Here we study the\nimportance of learning accurate PE for problems which rely on a non-trivial\narrangement of input tokens. Critically, we find that the choice of\ninitialization of a learnable PE greatly influences its ability to learn\naccurate PEs that lead to enhanced generalization. We empirically demonstrate\nour findings in three experiments: 1) A 2D relational reasoning task; 2) A\nnonlinear stochastic network simulation; 3) A real world 3D neuroscience\ndataset, applying interpretability analyses to verify the learning of accurate\nPEs. Overall, we find that a learned PE initialized from a small-norm\ndistribution can 1) uncover interpretable PEs that mirror ground truth\npositions in multiple dimensions, and 2) lead to improved downstream\ngeneralization in empirical evaluations. Importantly, choosing an ill-suited PE\ncan be detrimental to both model interpretability and generalization. Together,\nour results illustrate the feasibility of learning identifiable and\ninterpretable PEs for enhanced generalization.\n", "link": "http://arxiv.org/abs/2406.08272v3", "date": "2025-01-31", "relevancy": 2.6812, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20positional%20encodings%20in%20transformers%20depends%20on%20initialization&body=Title%3A%20Learning%20positional%20encodings%20in%20transformers%20depends%20on%20initialization%0AAuthor%3A%20Takuya%20Ito%20and%20Luca%20Cocchi%20and%20Tim%20Klinger%20and%20Parikshit%20Ram%20and%20Murray%20Campbell%20and%20Luke%20Hearne%0AAbstract%3A%20%20%20The%20attention%20mechanism%20is%20central%20to%20the%20transformer%27s%20ability%20to%20capture%0Acomplex%20dependencies%20between%20tokens%20of%20an%20input%20sequence.%20Key%20to%20the%20successful%0Aapplication%20of%20the%20attention%20mechanism%20in%20transformers%20is%20its%20choice%20of%0Apositional%20encoding%20%28PE%29.%20The%20PE%20provides%20essential%20information%20that%0Adistinguishes%20the%20position%20and%20order%20amongst%20tokens%20in%20a%20sequence.%20Most%20prior%0Ainvestigations%20of%20PE%20effects%20on%20generalization%20were%20tailored%20to%201D%20input%0Asequences%2C%20such%20as%20those%20presented%20in%20natural%20language%2C%20where%20adjacent%20tokens%0A%28e.g.%2C%20words%29%20are%20highly%20related.%20In%20contrast%2C%20many%20real%20world%20tasks%20involve%0Adatasets%20with%20highly%20non-trivial%20positional%20arrangements%2C%20such%20as%20datasets%0Aorganized%20in%20multiple%20spatial%20dimensions%2C%20or%20datasets%20for%20which%20ground%20truth%0Apositions%20are%20not%20known%2C%20such%20as%20in%20biological%20data.%20Here%20we%20study%20the%0Aimportance%20of%20learning%20accurate%20PE%20for%20problems%20which%20rely%20on%20a%20non-trivial%0Aarrangement%20of%20input%20tokens.%20Critically%2C%20we%20find%20that%20the%20choice%20of%0Ainitialization%20of%20a%20learnable%20PE%20greatly%20influences%20its%20ability%20to%20learn%0Aaccurate%20PEs%20that%20lead%20to%20enhanced%20generalization.%20We%20empirically%20demonstrate%0Aour%20findings%20in%20three%20experiments%3A%201%29%20A%202D%20relational%20reasoning%20task%3B%202%29%20A%0Anonlinear%20stochastic%20network%20simulation%3B%203%29%20A%20real%20world%203D%20neuroscience%0Adataset%2C%20applying%20interpretability%20analyses%20to%20verify%20the%20learning%20of%20accurate%0APEs.%20Overall%2C%20we%20find%20that%20a%20learned%20PE%20initialized%20from%20a%20small-norm%0Adistribution%20can%201%29%20uncover%20interpretable%20PEs%20that%20mirror%20ground%20truth%0Apositions%20in%20multiple%20dimensions%2C%20and%202%29%20lead%20to%20improved%20downstream%0Ageneralization%20in%20empirical%20evaluations.%20Importantly%2C%20choosing%20an%20ill-suited%20PE%0Acan%20be%20detrimental%20to%20both%20model%20interpretability%20and%20generalization.%20Together%2C%0Aour%20results%20illustrate%20the%20feasibility%20of%20learning%20identifiable%20and%0Ainterpretable%20PEs%20for%20enhanced%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08272v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520positional%2520encodings%2520in%2520transformers%2520depends%2520on%2520initialization%26entry.906535625%3DTakuya%2520Ito%2520and%2520Luca%2520Cocchi%2520and%2520Tim%2520Klinger%2520and%2520Parikshit%2520Ram%2520and%2520Murray%2520Campbell%2520and%2520Luke%2520Hearne%26entry.1292438233%3D%2520%2520The%2520attention%2520mechanism%2520is%2520central%2520to%2520the%2520transformer%2527s%2520ability%2520to%2520capture%250Acomplex%2520dependencies%2520between%2520tokens%2520of%2520an%2520input%2520sequence.%2520Key%2520to%2520the%2520successful%250Aapplication%2520of%2520the%2520attention%2520mechanism%2520in%2520transformers%2520is%2520its%2520choice%2520of%250Apositional%2520encoding%2520%2528PE%2529.%2520The%2520PE%2520provides%2520essential%2520information%2520that%250Adistinguishes%2520the%2520position%2520and%2520order%2520amongst%2520tokens%2520in%2520a%2520sequence.%2520Most%2520prior%250Ainvestigations%2520of%2520PE%2520effects%2520on%2520generalization%2520were%2520tailored%2520to%25201D%2520input%250Asequences%252C%2520such%2520as%2520those%2520presented%2520in%2520natural%2520language%252C%2520where%2520adjacent%2520tokens%250A%2528e.g.%252C%2520words%2529%2520are%2520highly%2520related.%2520In%2520contrast%252C%2520many%2520real%2520world%2520tasks%2520involve%250Adatasets%2520with%2520highly%2520non-trivial%2520positional%2520arrangements%252C%2520such%2520as%2520datasets%250Aorganized%2520in%2520multiple%2520spatial%2520dimensions%252C%2520or%2520datasets%2520for%2520which%2520ground%2520truth%250Apositions%2520are%2520not%2520known%252C%2520such%2520as%2520in%2520biological%2520data.%2520Here%2520we%2520study%2520the%250Aimportance%2520of%2520learning%2520accurate%2520PE%2520for%2520problems%2520which%2520rely%2520on%2520a%2520non-trivial%250Aarrangement%2520of%2520input%2520tokens.%2520Critically%252C%2520we%2520find%2520that%2520the%2520choice%2520of%250Ainitialization%2520of%2520a%2520learnable%2520PE%2520greatly%2520influences%2520its%2520ability%2520to%2520learn%250Aaccurate%2520PEs%2520that%2520lead%2520to%2520enhanced%2520generalization.%2520We%2520empirically%2520demonstrate%250Aour%2520findings%2520in%2520three%2520experiments%253A%25201%2529%2520A%25202D%2520relational%2520reasoning%2520task%253B%25202%2529%2520A%250Anonlinear%2520stochastic%2520network%2520simulation%253B%25203%2529%2520A%2520real%2520world%25203D%2520neuroscience%250Adataset%252C%2520applying%2520interpretability%2520analyses%2520to%2520verify%2520the%2520learning%2520of%2520accurate%250APEs.%2520Overall%252C%2520we%2520find%2520that%2520a%2520learned%2520PE%2520initialized%2520from%2520a%2520small-norm%250Adistribution%2520can%25201%2529%2520uncover%2520interpretable%2520PEs%2520that%2520mirror%2520ground%2520truth%250Apositions%2520in%2520multiple%2520dimensions%252C%2520and%25202%2529%2520lead%2520to%2520improved%2520downstream%250Ageneralization%2520in%2520empirical%2520evaluations.%2520Importantly%252C%2520choosing%2520an%2520ill-suited%2520PE%250Acan%2520be%2520detrimental%2520to%2520both%2520model%2520interpretability%2520and%2520generalization.%2520Together%252C%250Aour%2520results%2520illustrate%2520the%2520feasibility%2520of%2520learning%2520identifiable%2520and%250Ainterpretable%2520PEs%2520for%2520enhanced%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08272v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20positional%20encodings%20in%20transformers%20depends%20on%20initialization&entry.906535625=Takuya%20Ito%20and%20Luca%20Cocchi%20and%20Tim%20Klinger%20and%20Parikshit%20Ram%20and%20Murray%20Campbell%20and%20Luke%20Hearne&entry.1292438233=%20%20The%20attention%20mechanism%20is%20central%20to%20the%20transformer%27s%20ability%20to%20capture%0Acomplex%20dependencies%20between%20tokens%20of%20an%20input%20sequence.%20Key%20to%20the%20successful%0Aapplication%20of%20the%20attention%20mechanism%20in%20transformers%20is%20its%20choice%20of%0Apositional%20encoding%20%28PE%29.%20The%20PE%20provides%20essential%20information%20that%0Adistinguishes%20the%20position%20and%20order%20amongst%20tokens%20in%20a%20sequence.%20Most%20prior%0Ainvestigations%20of%20PE%20effects%20on%20generalization%20were%20tailored%20to%201D%20input%0Asequences%2C%20such%20as%20those%20presented%20in%20natural%20language%2C%20where%20adjacent%20tokens%0A%28e.g.%2C%20words%29%20are%20highly%20related.%20In%20contrast%2C%20many%20real%20world%20tasks%20involve%0Adatasets%20with%20highly%20non-trivial%20positional%20arrangements%2C%20such%20as%20datasets%0Aorganized%20in%20multiple%20spatial%20dimensions%2C%20or%20datasets%20for%20which%20ground%20truth%0Apositions%20are%20not%20known%2C%20such%20as%20in%20biological%20data.%20Here%20we%20study%20the%0Aimportance%20of%20learning%20accurate%20PE%20for%20problems%20which%20rely%20on%20a%20non-trivial%0Aarrangement%20of%20input%20tokens.%20Critically%2C%20we%20find%20that%20the%20choice%20of%0Ainitialization%20of%20a%20learnable%20PE%20greatly%20influences%20its%20ability%20to%20learn%0Aaccurate%20PEs%20that%20lead%20to%20enhanced%20generalization.%20We%20empirically%20demonstrate%0Aour%20findings%20in%20three%20experiments%3A%201%29%20A%202D%20relational%20reasoning%20task%3B%202%29%20A%0Anonlinear%20stochastic%20network%20simulation%3B%203%29%20A%20real%20world%203D%20neuroscience%0Adataset%2C%20applying%20interpretability%20analyses%20to%20verify%20the%20learning%20of%20accurate%0APEs.%20Overall%2C%20we%20find%20that%20a%20learned%20PE%20initialized%20from%20a%20small-norm%0Adistribution%20can%201%29%20uncover%20interpretable%20PEs%20that%20mirror%20ground%20truth%0Apositions%20in%20multiple%20dimensions%2C%20and%202%29%20lead%20to%20improved%20downstream%0Ageneralization%20in%20empirical%20evaluations.%20Importantly%2C%20choosing%20an%20ill-suited%20PE%0Acan%20be%20detrimental%20to%20both%20model%20interpretability%20and%20generalization.%20Together%2C%0Aour%20results%20illustrate%20the%20feasibility%20of%20learning%20identifiable%20and%0Ainterpretable%20PEs%20for%20enhanced%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08272v3&entry.124074799=Read"},
{"title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning", "author": "Fanxu Meng and Pingzhi Tang and Fan jiang and Muhan Zhang", "abstract": "  Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.\n", "link": "http://arxiv.org/abs/2411.17426v3", "date": "2025-01-31", "relevancy": 2.6763, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLOVER%3A%20Cross-Layer%20Orthogonal%20Vectors%20Pruning%20and%20Fine-Tuning&body=Title%3A%20CLOVER%3A%20Cross-Layer%20Orthogonal%20Vectors%20Pruning%20and%20Fine-Tuning%0AAuthor%3A%20Fanxu%20Meng%20and%20Pingzhi%20Tang%20and%20Fan%20jiang%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Decoder-only%20models%20generate%20tokens%20autoregressively%20by%20caching%20key/value%0Avectors%2C%20but%20as%20the%20cache%20grows%2C%20inference%20becomes%20memory-bound.%20To%20address%0Athis%20issue%2C%20we%20introduce%20CLOVER%20%28Cross-Layer%20Orthogonal%20Vectors%29%2C%20a%20novel%0Aapproach%20that%20treats%20pairs%20of%20attention%20layers%20as%20a%20set%20of%20low-rank%0Adecompositions.%20CLOVER%20applies%20Singular%20Value%20Decomposition%20%28SVD%29%20to%20the%20%5C%28%20Q%0A%5C%29-%5C%28%20K%20%5C%29%20and%20%5C%28%20V%20%5C%29-%5C%28%20O%20%5C%29%20pairs%20within%20each%20attention%20head.%20The%20resulting%0Asingular%20values%20can%20either%20guide%20pruning%20or%20serve%20as%20trainable%20parameters%20for%0Aefficient%20fine-tuning%20of%20all%20orthogonal%20vectors.%20After%20pruning%20or%20fine-tuning%2C%0Athese%20values%20are%20reintegrated%20into%20the%20model%20without%20increasing%20its%20parameter%0Acount.%20We%20apply%20CLOVER%20to%20various%20models%2C%20including%20GPT-2%20XL%2C%20DeepSeek-V2-Lite%2C%0AWhisper-Large-v3%2C%20Stable%20Diffusion%20XL%2C%20and%20LLaMA-3.2-11B-Vision.%20Our%20results%0Ademonstrate%20that%20CLOVER%20significantly%20improves%20pruning%20efficiency.%20For%0Ainstance%2C%20the%20perplexity%20of%20pruning%2070%5C%25%20of%20the%20%5C%28%20Q%20%5C%29-%5C%28%20K%20%5C%29%20pairs%20in%20GPT-2%0AXL%20is%20similar%20to%20that%20of%20pruning%20just%208%5C%25%20with%20vanilla%20methods.%20Fine-tuning%20the%0Asingular%20values%20further%20results%20in%20a%20full-rank%20update%2C%20outperforming%0Astate-of-the-art%20methods%20%28LoRA%2C%20DoRA%2C%20HiRA%2C%20and%20PiSSA%29%20by%207.6%5C%25%2C%205.5%5C%25%2C%203.8%5C%25%2C%0Aand%200.7%5C%25%2C%20respectively%2C%20on%20eight%20commonsense%20tasks%20for%20LLaMA-2%207B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17426v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLOVER%253A%2520Cross-Layer%2520Orthogonal%2520Vectors%2520Pruning%2520and%2520Fine-Tuning%26entry.906535625%3DFanxu%2520Meng%2520and%2520Pingzhi%2520Tang%2520and%2520Fan%2520jiang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Decoder-only%2520models%2520generate%2520tokens%2520autoregressively%2520by%2520caching%2520key/value%250Avectors%252C%2520but%2520as%2520the%2520cache%2520grows%252C%2520inference%2520becomes%2520memory-bound.%2520To%2520address%250Athis%2520issue%252C%2520we%2520introduce%2520CLOVER%2520%2528Cross-Layer%2520Orthogonal%2520Vectors%2529%252C%2520a%2520novel%250Aapproach%2520that%2520treats%2520pairs%2520of%2520attention%2520layers%2520as%2520a%2520set%2520of%2520low-rank%250Adecompositions.%2520CLOVER%2520applies%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529%2520to%2520the%2520%255C%2528%2520Q%250A%255C%2529-%255C%2528%2520K%2520%255C%2529%2520and%2520%255C%2528%2520V%2520%255C%2529-%255C%2528%2520O%2520%255C%2529%2520pairs%2520within%2520each%2520attention%2520head.%2520The%2520resulting%250Asingular%2520values%2520can%2520either%2520guide%2520pruning%2520or%2520serve%2520as%2520trainable%2520parameters%2520for%250Aefficient%2520fine-tuning%2520of%2520all%2520orthogonal%2520vectors.%2520After%2520pruning%2520or%2520fine-tuning%252C%250Athese%2520values%2520are%2520reintegrated%2520into%2520the%2520model%2520without%2520increasing%2520its%2520parameter%250Acount.%2520We%2520apply%2520CLOVER%2520to%2520various%2520models%252C%2520including%2520GPT-2%2520XL%252C%2520DeepSeek-V2-Lite%252C%250AWhisper-Large-v3%252C%2520Stable%2520Diffusion%2520XL%252C%2520and%2520LLaMA-3.2-11B-Vision.%2520Our%2520results%250Ademonstrate%2520that%2520CLOVER%2520significantly%2520improves%2520pruning%2520efficiency.%2520For%250Ainstance%252C%2520the%2520perplexity%2520of%2520pruning%252070%255C%2525%2520of%2520the%2520%255C%2528%2520Q%2520%255C%2529-%255C%2528%2520K%2520%255C%2529%2520pairs%2520in%2520GPT-2%250AXL%2520is%2520similar%2520to%2520that%2520of%2520pruning%2520just%25208%255C%2525%2520with%2520vanilla%2520methods.%2520Fine-tuning%2520the%250Asingular%2520values%2520further%2520results%2520in%2520a%2520full-rank%2520update%252C%2520outperforming%250Astate-of-the-art%2520methods%2520%2528LoRA%252C%2520DoRA%252C%2520HiRA%252C%2520and%2520PiSSA%2529%2520by%25207.6%255C%2525%252C%25205.5%255C%2525%252C%25203.8%255C%2525%252C%250Aand%25200.7%255C%2525%252C%2520respectively%252C%2520on%2520eight%2520commonsense%2520tasks%2520for%2520LLaMA-2%25207B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17426v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOVER%3A%20Cross-Layer%20Orthogonal%20Vectors%20Pruning%20and%20Fine-Tuning&entry.906535625=Fanxu%20Meng%20and%20Pingzhi%20Tang%20and%20Fan%20jiang%20and%20Muhan%20Zhang&entry.1292438233=%20%20Decoder-only%20models%20generate%20tokens%20autoregressively%20by%20caching%20key/value%0Avectors%2C%20but%20as%20the%20cache%20grows%2C%20inference%20becomes%20memory-bound.%20To%20address%0Athis%20issue%2C%20we%20introduce%20CLOVER%20%28Cross-Layer%20Orthogonal%20Vectors%29%2C%20a%20novel%0Aapproach%20that%20treats%20pairs%20of%20attention%20layers%20as%20a%20set%20of%20low-rank%0Adecompositions.%20CLOVER%20applies%20Singular%20Value%20Decomposition%20%28SVD%29%20to%20the%20%5C%28%20Q%0A%5C%29-%5C%28%20K%20%5C%29%20and%20%5C%28%20V%20%5C%29-%5C%28%20O%20%5C%29%20pairs%20within%20each%20attention%20head.%20The%20resulting%0Asingular%20values%20can%20either%20guide%20pruning%20or%20serve%20as%20trainable%20parameters%20for%0Aefficient%20fine-tuning%20of%20all%20orthogonal%20vectors.%20After%20pruning%20or%20fine-tuning%2C%0Athese%20values%20are%20reintegrated%20into%20the%20model%20without%20increasing%20its%20parameter%0Acount.%20We%20apply%20CLOVER%20to%20various%20models%2C%20including%20GPT-2%20XL%2C%20DeepSeek-V2-Lite%2C%0AWhisper-Large-v3%2C%20Stable%20Diffusion%20XL%2C%20and%20LLaMA-3.2-11B-Vision.%20Our%20results%0Ademonstrate%20that%20CLOVER%20significantly%20improves%20pruning%20efficiency.%20For%0Ainstance%2C%20the%20perplexity%20of%20pruning%2070%5C%25%20of%20the%20%5C%28%20Q%20%5C%29-%5C%28%20K%20%5C%29%20pairs%20in%20GPT-2%0AXL%20is%20similar%20to%20that%20of%20pruning%20just%208%5C%25%20with%20vanilla%20methods.%20Fine-tuning%20the%0Asingular%20values%20further%20results%20in%20a%20full-rank%20update%2C%20outperforming%0Astate-of-the-art%20methods%20%28LoRA%2C%20DoRA%2C%20HiRA%2C%20and%20PiSSA%29%20by%207.6%5C%25%2C%205.5%5C%25%2C%203.8%5C%25%2C%0Aand%200.7%5C%25%2C%20respectively%2C%20on%20eight%20commonsense%20tasks%20for%20LLaMA-2%207B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17426v3&entry.124074799=Read"},
{"title": "Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023", "author": "Ting-Yao E. Hsu and Yi-Li Hsu and Shaurya Rohatgi and Chieh-Yang Huang and Ho Yin Sam Ng and Ryan Rossi and Sungchul Kim and Tong Yu and Lun-Wei Ku and C. Lee Giles and Ting-Hao K. Huang", "abstract": "  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n", "link": "http://arxiv.org/abs/2501.19353v1", "date": "2025-01-31", "relevancy": 2.6689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023&body=Title%3A%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023%0AAuthor%3A%20Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang%0AAbstract%3A%20%20%20Since%20the%20SCICAP%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SCICAP%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SCICAP%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SCICAP%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Multimodal%2520Models%2520Solve%2520Caption%2520Generation%2520for%2520Scientific%250A%2520%2520Figures%253F%2520Lessons%2520Learned%2520from%2520SCICAP%2520Challenge%25202023%26entry.906535625%3DTing-Yao%2520E.%2520Hsu%2520and%2520Yi-Li%2520Hsu%2520and%2520Shaurya%2520Rohatgi%2520and%2520Chieh-Yang%2520Huang%2520and%2520Ho%2520Yin%2520Sam%2520Ng%2520and%2520Ryan%2520Rossi%2520and%2520Sungchul%2520Kim%2520and%2520Tong%2520Yu%2520and%2520Lun-Wei%2520Ku%2520and%2520C.%2520Lee%2520Giles%2520and%2520Ting-Hao%2520K.%2520Huang%26entry.1292438233%3D%2520%2520Since%2520the%2520SCICAP%2520datasets%2520launch%2520in%25202021%252C%2520the%2520research%2520community%2520has%2520made%250Asignificant%2520progress%2520in%2520generating%2520captions%2520for%2520scientific%2520figures%2520in%2520scholarly%250Aarticles.%2520In%25202023%252C%2520the%2520first%2520SCICAP%2520Challenge%2520took%2520place%252C%2520inviting%2520global%2520teams%250Ato%2520use%2520an%2520expanded%2520SCICAP%2520dataset%2520to%2520develop%2520models%2520for%2520captioning%2520diverse%250Afigure%2520types%2520across%2520various%2520academic%2520fields.%2520At%2520the%2520same%2520time%252C%2520text%2520generation%250Amodels%2520advanced%2520quickly%252C%2520with%2520many%2520powerful%2520pre-trained%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520emerging%2520that%2520showed%2520impressive%2520capabilities%2520in%2520various%250Avision-and-language%2520tasks.%2520This%2520paper%2520presents%2520an%2520overview%2520of%2520the%2520first%2520SCICAP%250AChallenge%2520and%2520details%2520the%2520performance%2520of%2520various%2520models%2520on%2520its%2520data%252C%2520capturing%250Aa%2520snapshot%2520of%2520the%2520fields%2520state.%2520We%2520found%2520that%2520professional%2520editors%250Aoverwhelmingly%2520preferred%2520figure%2520captions%2520generated%2520by%2520GPT-4V%2520over%2520those%2520from%250Aall%2520other%2520models%2520and%2520even%2520the%2520original%2520captions%2520written%2520by%2520authors.%2520Following%250Athis%2520key%2520finding%252C%2520we%2520conducted%2520detailed%2520analyses%2520to%2520answer%2520this%2520question%253A%2520Have%250Aadvanced%2520LMMs%2520solved%2520the%2520task%2520of%2520generating%2520captions%2520for%2520scientific%2520figures%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023&entry.906535625=Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang&entry.1292438233=%20%20Since%20the%20SCICAP%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SCICAP%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SCICAP%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SCICAP%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19353v1&entry.124074799=Read"},
{"title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image\n  Classification", "author": "Xiangyu Sun and Xiaoguang Zou and Yuanquan Wu and Guotai Wang and Shaoting Zhang", "abstract": "  X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.\n", "link": "http://arxiv.org/abs/2501.19086v1", "date": "2025-01-31", "relevancy": 2.661, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Analysis%20of%20CLIP-Based%20Foundation%20Models%20for%20X-Ray%20Image%0A%20%20Classification&body=Title%3A%20Fairness%20Analysis%20of%20CLIP-Based%20Foundation%20Models%20for%20X-Ray%20Image%0A%20%20Classification%0AAuthor%3A%20Xiangyu%20Sun%20and%20Xiaoguang%20Zou%20and%20Yuanquan%20Wu%20and%20Guotai%20Wang%20and%20Shaoting%20Zhang%0AAbstract%3A%20%20%20X-ray%20imaging%20is%20pivotal%20in%20medical%20diagnostics%2C%20offering%20non-invasive%0Ainsights%20into%20a%20range%20of%20health%20conditions.%20Recently%2C%20vision-language%20models%2C%0Asuch%20as%20the%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20model%2C%20have%0Ademonstrated%20potential%20in%20improving%20diagnostic%20accuracy%20by%20leveraging%0Alarge-scale%20image-text%20datasets.%20However%2C%20since%20CLIP%20was%20not%20initially%20designed%0Afor%20medical%20images%2C%20several%20CLIP-like%20models%20trained%20specifically%20on%20medical%0Aimages%20have%20been%20developed.%20Despite%20their%20enhanced%20performance%2C%20issues%20of%0Afairness%20-%20particularly%20regarding%20demographic%20attributes%20-%20remain%20largely%0Aunaddressed.%20In%20this%20study%2C%20we%20perform%20a%20comprehensive%20fairness%20analysis%20of%0ACLIP-like%20models%20applied%20to%20X-ray%20image%20classification.%20We%20assess%20their%0Aperformance%20and%20fairness%20across%20diverse%20patient%20demographics%20and%20disease%0Acategories%20using%20zero-shot%20inference%20and%20various%20fine-tuning%20techniques%2C%0Aincluding%20Linear%20Probing%2C%20Multilayer%20Perceptron%20%28MLP%29%2C%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20and%20full%20fine-tuning.%20Our%20results%20indicate%20that%20while%20fine-tuning%0Aimproves%20model%20accuracy%2C%20fairness%20concerns%20persist%2C%20highlighting%20the%20need%20for%0Afurther%20fairness%20interventions%20in%20these%20foundational%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Analysis%2520of%2520CLIP-Based%2520Foundation%2520Models%2520for%2520X-Ray%2520Image%250A%2520%2520Classification%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Xiaoguang%2520Zou%2520and%2520Yuanquan%2520Wu%2520and%2520Guotai%2520Wang%2520and%2520Shaoting%2520Zhang%26entry.1292438233%3D%2520%2520X-ray%2520imaging%2520is%2520pivotal%2520in%2520medical%2520diagnostics%252C%2520offering%2520non-invasive%250Ainsights%2520into%2520a%2520range%2520of%2520health%2520conditions.%2520Recently%252C%2520vision-language%2520models%252C%250Asuch%2520as%2520the%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520model%252C%2520have%250Ademonstrated%2520potential%2520in%2520improving%2520diagnostic%2520accuracy%2520by%2520leveraging%250Alarge-scale%2520image-text%2520datasets.%2520However%252C%2520since%2520CLIP%2520was%2520not%2520initially%2520designed%250Afor%2520medical%2520images%252C%2520several%2520CLIP-like%2520models%2520trained%2520specifically%2520on%2520medical%250Aimages%2520have%2520been%2520developed.%2520Despite%2520their%2520enhanced%2520performance%252C%2520issues%2520of%250Afairness%2520-%2520particularly%2520regarding%2520demographic%2520attributes%2520-%2520remain%2520largely%250Aunaddressed.%2520In%2520this%2520study%252C%2520we%2520perform%2520a%2520comprehensive%2520fairness%2520analysis%2520of%250ACLIP-like%2520models%2520applied%2520to%2520X-ray%2520image%2520classification.%2520We%2520assess%2520their%250Aperformance%2520and%2520fairness%2520across%2520diverse%2520patient%2520demographics%2520and%2520disease%250Acategories%2520using%2520zero-shot%2520inference%2520and%2520various%2520fine-tuning%2520techniques%252C%250Aincluding%2520Linear%2520Probing%252C%2520Multilayer%2520Perceptron%2520%2528MLP%2529%252C%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%252C%2520and%2520full%2520fine-tuning.%2520Our%2520results%2520indicate%2520that%2520while%2520fine-tuning%250Aimproves%2520model%2520accuracy%252C%2520fairness%2520concerns%2520persist%252C%2520highlighting%2520the%2520need%2520for%250Afurther%2520fairness%2520interventions%2520in%2520these%2520foundational%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Analysis%20of%20CLIP-Based%20Foundation%20Models%20for%20X-Ray%20Image%0A%20%20Classification&entry.906535625=Xiangyu%20Sun%20and%20Xiaoguang%20Zou%20and%20Yuanquan%20Wu%20and%20Guotai%20Wang%20and%20Shaoting%20Zhang&entry.1292438233=%20%20X-ray%20imaging%20is%20pivotal%20in%20medical%20diagnostics%2C%20offering%20non-invasive%0Ainsights%20into%20a%20range%20of%20health%20conditions.%20Recently%2C%20vision-language%20models%2C%0Asuch%20as%20the%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20model%2C%20have%0Ademonstrated%20potential%20in%20improving%20diagnostic%20accuracy%20by%20leveraging%0Alarge-scale%20image-text%20datasets.%20However%2C%20since%20CLIP%20was%20not%20initially%20designed%0Afor%20medical%20images%2C%20several%20CLIP-like%20models%20trained%20specifically%20on%20medical%0Aimages%20have%20been%20developed.%20Despite%20their%20enhanced%20performance%2C%20issues%20of%0Afairness%20-%20particularly%20regarding%20demographic%20attributes%20-%20remain%20largely%0Aunaddressed.%20In%20this%20study%2C%20we%20perform%20a%20comprehensive%20fairness%20analysis%20of%0ACLIP-like%20models%20applied%20to%20X-ray%20image%20classification.%20We%20assess%20their%0Aperformance%20and%20fairness%20across%20diverse%20patient%20demographics%20and%20disease%0Acategories%20using%20zero-shot%20inference%20and%20various%20fine-tuning%20techniques%2C%0Aincluding%20Linear%20Probing%2C%20Multilayer%20Perceptron%20%28MLP%29%2C%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20and%20full%20fine-tuning.%20Our%20results%20indicate%20that%20while%20fine-tuning%0Aimproves%20model%20accuracy%2C%20fairness%20concerns%20persist%2C%20highlighting%20the%20need%20for%0Afurther%20fairness%20interventions%20in%20these%20foundational%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19086v1&entry.124074799=Read"},
{"title": "GDO: Gradual Domain Osmosis", "author": "Zixi Wang and Yubo Huang", "abstract": "  In this paper, we propose a new method called Gradual Domain Osmosis, which\naims to solve the problem of smooth knowledge migration from source domain to\ntarget domain in Gradual Domain Adaptation (GDA). Traditional Gradual Domain\nAdaptation methods mitigate domain bias by introducing intermediate domains and\nself-training strategies, but often face the challenges of inefficient\nknowledge migration or missing data in intermediate domains. In this paper, we\ndesign an optimisation framework based on the hyperparameter $\\lambda$ by\ndynamically balancing the loss weights of the source and target domains, which\nenables the model to progressively adjust the strength of knowledge migration\n($\\lambda$ incrementing from 0 to 1) during the training process, thus\nachieving cross-domain generalisation more efficiently. Specifically, the\nmethod incorporates self-training to generate pseudo-labels and iteratively\nupdates the model by minimising a weighted loss function to ensure stability\nand robustness during progressive adaptation in the intermediate domain. The\nexperimental part validates the effectiveness of the method on rotated MNIST,\ncolour-shifted MNIST, portrait dataset and forest cover type dataset, and the\nresults show that it outperforms existing baseline methods. The paper further\nanalyses the impact of the dynamic tuning strategy of the hyperparameter\n$\\lambda$ on the performance through ablation experiments, confirming the\nadvantages of progressive domain penetration in mitigating the domain bias and\nenhancing the model generalisation capability. The study provides a theoretical\nsupport and practical framework for asymptotic domain adaptation and expands\nits application potential in dynamic environments.\n", "link": "http://arxiv.org/abs/2501.19159v1", "date": "2025-01-31", "relevancy": 2.6389, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5504}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDO%3A%20Gradual%20Domain%20Osmosis&body=Title%3A%20GDO%3A%20Gradual%20Domain%20Osmosis%0AAuthor%3A%20Zixi%20Wang%20and%20Yubo%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%20Gradual%20Domain%20Osmosis%2C%20which%0Aaims%20to%20solve%20the%20problem%20of%20smooth%20knowledge%20migration%20from%20source%20domain%20to%0Atarget%20domain%20in%20Gradual%20Domain%20Adaptation%20%28GDA%29.%20Traditional%20Gradual%20Domain%0AAdaptation%20methods%20mitigate%20domain%20bias%20by%20introducing%20intermediate%20domains%20and%0Aself-training%20strategies%2C%20but%20often%20face%20the%20challenges%20of%20inefficient%0Aknowledge%20migration%20or%20missing%20data%20in%20intermediate%20domains.%20In%20this%20paper%2C%20we%0Adesign%20an%20optimisation%20framework%20based%20on%20the%20hyperparameter%20%24%5Clambda%24%20by%0Adynamically%20balancing%20the%20loss%20weights%20of%20the%20source%20and%20target%20domains%2C%20which%0Aenables%20the%20model%20to%20progressively%20adjust%20the%20strength%20of%20knowledge%20migration%0A%28%24%5Clambda%24%20incrementing%20from%200%20to%201%29%20during%20the%20training%20process%2C%20thus%0Aachieving%20cross-domain%20generalisation%20more%20efficiently.%20Specifically%2C%20the%0Amethod%20incorporates%20self-training%20to%20generate%20pseudo-labels%20and%20iteratively%0Aupdates%20the%20model%20by%20minimising%20a%20weighted%20loss%20function%20to%20ensure%20stability%0Aand%20robustness%20during%20progressive%20adaptation%20in%20the%20intermediate%20domain.%20The%0Aexperimental%20part%20validates%20the%20effectiveness%20of%20the%20method%20on%20rotated%20MNIST%2C%0Acolour-shifted%20MNIST%2C%20portrait%20dataset%20and%20forest%20cover%20type%20dataset%2C%20and%20the%0Aresults%20show%20that%20it%20outperforms%20existing%20baseline%20methods.%20The%20paper%20further%0Aanalyses%20the%20impact%20of%20the%20dynamic%20tuning%20strategy%20of%20the%20hyperparameter%0A%24%5Clambda%24%20on%20the%20performance%20through%20ablation%20experiments%2C%20confirming%20the%0Aadvantages%20of%20progressive%20domain%20penetration%20in%20mitigating%20the%20domain%20bias%20and%0Aenhancing%20the%20model%20generalisation%20capability.%20The%20study%20provides%20a%20theoretical%0Asupport%20and%20practical%20framework%20for%20asymptotic%20domain%20adaptation%20and%20expands%0Aits%20application%20potential%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDO%253A%2520Gradual%2520Domain%2520Osmosis%26entry.906535625%3DZixi%2520Wang%2520and%2520Yubo%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520method%2520called%2520Gradual%2520Domain%2520Osmosis%252C%2520which%250Aaims%2520to%2520solve%2520the%2520problem%2520of%2520smooth%2520knowledge%2520migration%2520from%2520source%2520domain%2520to%250Atarget%2520domain%2520in%2520Gradual%2520Domain%2520Adaptation%2520%2528GDA%2529.%2520Traditional%2520Gradual%2520Domain%250AAdaptation%2520methods%2520mitigate%2520domain%2520bias%2520by%2520introducing%2520intermediate%2520domains%2520and%250Aself-training%2520strategies%252C%2520but%2520often%2520face%2520the%2520challenges%2520of%2520inefficient%250Aknowledge%2520migration%2520or%2520missing%2520data%2520in%2520intermediate%2520domains.%2520In%2520this%2520paper%252C%2520we%250Adesign%2520an%2520optimisation%2520framework%2520based%2520on%2520the%2520hyperparameter%2520%2524%255Clambda%2524%2520by%250Adynamically%2520balancing%2520the%2520loss%2520weights%2520of%2520the%2520source%2520and%2520target%2520domains%252C%2520which%250Aenables%2520the%2520model%2520to%2520progressively%2520adjust%2520the%2520strength%2520of%2520knowledge%2520migration%250A%2528%2524%255Clambda%2524%2520incrementing%2520from%25200%2520to%25201%2529%2520during%2520the%2520training%2520process%252C%2520thus%250Aachieving%2520cross-domain%2520generalisation%2520more%2520efficiently.%2520Specifically%252C%2520the%250Amethod%2520incorporates%2520self-training%2520to%2520generate%2520pseudo-labels%2520and%2520iteratively%250Aupdates%2520the%2520model%2520by%2520minimising%2520a%2520weighted%2520loss%2520function%2520to%2520ensure%2520stability%250Aand%2520robustness%2520during%2520progressive%2520adaptation%2520in%2520the%2520intermediate%2520domain.%2520The%250Aexperimental%2520part%2520validates%2520the%2520effectiveness%2520of%2520the%2520method%2520on%2520rotated%2520MNIST%252C%250Acolour-shifted%2520MNIST%252C%2520portrait%2520dataset%2520and%2520forest%2520cover%2520type%2520dataset%252C%2520and%2520the%250Aresults%2520show%2520that%2520it%2520outperforms%2520existing%2520baseline%2520methods.%2520The%2520paper%2520further%250Aanalyses%2520the%2520impact%2520of%2520the%2520dynamic%2520tuning%2520strategy%2520of%2520the%2520hyperparameter%250A%2524%255Clambda%2524%2520on%2520the%2520performance%2520through%2520ablation%2520experiments%252C%2520confirming%2520the%250Aadvantages%2520of%2520progressive%2520domain%2520penetration%2520in%2520mitigating%2520the%2520domain%2520bias%2520and%250Aenhancing%2520the%2520model%2520generalisation%2520capability.%2520The%2520study%2520provides%2520a%2520theoretical%250Asupport%2520and%2520practical%2520framework%2520for%2520asymptotic%2520domain%2520adaptation%2520and%2520expands%250Aits%2520application%2520potential%2520in%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDO%3A%20Gradual%20Domain%20Osmosis&entry.906535625=Zixi%20Wang%20and%20Yubo%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%20Gradual%20Domain%20Osmosis%2C%20which%0Aaims%20to%20solve%20the%20problem%20of%20smooth%20knowledge%20migration%20from%20source%20domain%20to%0Atarget%20domain%20in%20Gradual%20Domain%20Adaptation%20%28GDA%29.%20Traditional%20Gradual%20Domain%0AAdaptation%20methods%20mitigate%20domain%20bias%20by%20introducing%20intermediate%20domains%20and%0Aself-training%20strategies%2C%20but%20often%20face%20the%20challenges%20of%20inefficient%0Aknowledge%20migration%20or%20missing%20data%20in%20intermediate%20domains.%20In%20this%20paper%2C%20we%0Adesign%20an%20optimisation%20framework%20based%20on%20the%20hyperparameter%20%24%5Clambda%24%20by%0Adynamically%20balancing%20the%20loss%20weights%20of%20the%20source%20and%20target%20domains%2C%20which%0Aenables%20the%20model%20to%20progressively%20adjust%20the%20strength%20of%20knowledge%20migration%0A%28%24%5Clambda%24%20incrementing%20from%200%20to%201%29%20during%20the%20training%20process%2C%20thus%0Aachieving%20cross-domain%20generalisation%20more%20efficiently.%20Specifically%2C%20the%0Amethod%20incorporates%20self-training%20to%20generate%20pseudo-labels%20and%20iteratively%0Aupdates%20the%20model%20by%20minimising%20a%20weighted%20loss%20function%20to%20ensure%20stability%0Aand%20robustness%20during%20progressive%20adaptation%20in%20the%20intermediate%20domain.%20The%0Aexperimental%20part%20validates%20the%20effectiveness%20of%20the%20method%20on%20rotated%20MNIST%2C%0Acolour-shifted%20MNIST%2C%20portrait%20dataset%20and%20forest%20cover%20type%20dataset%2C%20and%20the%0Aresults%20show%20that%20it%20outperforms%20existing%20baseline%20methods.%20The%20paper%20further%0Aanalyses%20the%20impact%20of%20the%20dynamic%20tuning%20strategy%20of%20the%20hyperparameter%0A%24%5Clambda%24%20on%20the%20performance%20through%20ablation%20experiments%2C%20confirming%20the%0Aadvantages%20of%20progressive%20domain%20penetration%20in%20mitigating%20the%20domain%20bias%20and%0Aenhancing%20the%20model%20generalisation%20capability.%20The%20study%20provides%20a%20theoretical%0Asupport%20and%20practical%20framework%20for%20asymptotic%20domain%20adaptation%20and%20expands%0Aits%20application%20potential%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19159v1&entry.124074799=Read"},
{"title": "Point-Level Topological Representation Learning on Point Clouds", "author": "Vincent P. Grande and Michael T. Schaub", "abstract": "  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise and heterogeneous sampling.\n", "link": "http://arxiv.org/abs/2406.02300v2", "date": "2025-01-31", "relevancy": 2.6354, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&body=Title%3A%20Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds%0AAuthor%3A%20Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub%0AAbstract%3A%20%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise%20and%20heterogeneous%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-Level%2520Topological%2520Representation%2520Learning%2520on%2520Point%2520Clouds%26entry.906535625%3DVincent%2520P.%2520Grande%2520and%2520Michael%2520T.%2520Schaub%26entry.1292438233%3D%2520%2520Topological%2520Data%2520Analysis%2520%2528TDA%2529%2520allows%2520us%2520to%2520extract%2520powerful%2520topological%2520and%250Ahigher-order%2520information%2520on%2520the%2520global%2520shape%2520of%2520a%2520data%2520set%2520or%2520point%2520cloud.%250ATools%2520like%2520Persistent%2520Homology%2520or%2520the%2520Euler%2520Transform%2520give%2520a%2520single%2520complex%250Adescription%2520of%2520the%2520global%2520structure%2520of%2520the%2520point%2520cloud.%2520However%252C%2520common%2520machine%250Alearning%2520applications%2520like%2520classification%2520require%2520point-level%2520information%2520and%250Afeatures%2520to%2520be%2520available.%2520In%2520this%2520paper%252C%2520we%2520bridge%2520this%2520gap%2520and%2520propose%2520a%2520novel%250Amethod%2520to%2520extract%2520node-level%2520topological%2520features%2520from%2520complex%2520point%2520clouds%250Ausing%2520discrete%2520variants%2520of%2520concepts%2520from%2520algebraic%2520topology%2520and%2520differential%250Ageometry.%2520We%2520verify%2520the%2520effectiveness%2520of%2520these%2520topological%2520point%2520features%250A%2528TOPF%2529%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520and%2520study%2520their%2520robustness%2520under%250Anoise%2520and%2520heterogeneous%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&entry.906535625=Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub&entry.1292438233=%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise%20and%20heterogeneous%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02300v2&entry.124074799=Read"},
{"title": "MTGA: Multi-View Temporal Granularity Aligned Aggregation for\n  Event-Based Lip-Reading", "author": "Wenhao Zhang and Jun Wang and Yong Luo and Lei Yu and Wei Yu and Zheng He and Jialie Shen", "abstract": "  Lip-reading is to utilize the visual information of the speaker's lip\nmovements to recognize words and sentences. Existing event-based lip-reading\nsolutions integrate different frame rate branches to learn spatio-temporal\nfeatures of varying granularities. However, aggregating events into event\nframes inevitably leads to the loss of fine-grained temporal information within\nframes. To remedy this drawback, we propose a novel framework termed Multi-view\nTemporal Granularity aligned Aggregation (MTGA). Specifically, we first present\na novel event representation method, namely time-segmented voxel graph list,\nwhere the most significant local voxels are temporally connected into a graph\nlist. Then we design a spatio-temporal fusion module based on temporal\ngranularity alignment, where the global spatial features extracted from event\nframes, together with the local relative spatial and temporal features\ncontained in voxel graph list are effectively aligned and integrated. Finally,\nwe design a temporal aggregation module that incorporates positional encoding,\nwhich enables the capture of local absolute spatial and global temporal\ninformation. Experiments demonstrate that our method outperforms both the\nevent-based and video-based lip-reading counterparts.\n", "link": "http://arxiv.org/abs/2404.11979v2", "date": "2025-01-31", "relevancy": 2.6067, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5272}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTGA%3A%20Multi-View%20Temporal%20Granularity%20Aligned%20Aggregation%20for%0A%20%20Event-Based%20Lip-Reading&body=Title%3A%20MTGA%3A%20Multi-View%20Temporal%20Granularity%20Aligned%20Aggregation%20for%0A%20%20Event-Based%20Lip-Reading%0AAuthor%3A%20Wenhao%20Zhang%20and%20Jun%20Wang%20and%20Yong%20Luo%20and%20Lei%20Yu%20and%20Wei%20Yu%20and%20Zheng%20He%20and%20Jialie%20Shen%0AAbstract%3A%20%20%20Lip-reading%20is%20to%20utilize%20the%20visual%20information%20of%20the%20speaker%27s%20lip%0Amovements%20to%20recognize%20words%20and%20sentences.%20Existing%20event-based%20lip-reading%0Asolutions%20integrate%20different%20frame%20rate%20branches%20to%20learn%20spatio-temporal%0Afeatures%20of%20varying%20granularities.%20However%2C%20aggregating%20events%20into%20event%0Aframes%20inevitably%20leads%20to%20the%20loss%20of%20fine-grained%20temporal%20information%20within%0Aframes.%20To%20remedy%20this%20drawback%2C%20we%20propose%20a%20novel%20framework%20termed%20Multi-view%0ATemporal%20Granularity%20aligned%20Aggregation%20%28MTGA%29.%20Specifically%2C%20we%20first%20present%0Aa%20novel%20event%20representation%20method%2C%20namely%20time-segmented%20voxel%20graph%20list%2C%0Awhere%20the%20most%20significant%20local%20voxels%20are%20temporally%20connected%20into%20a%20graph%0Alist.%20Then%20we%20design%20a%20spatio-temporal%20fusion%20module%20based%20on%20temporal%0Agranularity%20alignment%2C%20where%20the%20global%20spatial%20features%20extracted%20from%20event%0Aframes%2C%20together%20with%20the%20local%20relative%20spatial%20and%20temporal%20features%0Acontained%20in%20voxel%20graph%20list%20are%20effectively%20aligned%20and%20integrated.%20Finally%2C%0Awe%20design%20a%20temporal%20aggregation%20module%20that%20incorporates%20positional%20encoding%2C%0Awhich%20enables%20the%20capture%20of%20local%20absolute%20spatial%20and%20global%20temporal%0Ainformation.%20Experiments%20demonstrate%20that%20our%20method%20outperforms%20both%20the%0Aevent-based%20and%20video-based%20lip-reading%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTGA%253A%2520Multi-View%2520Temporal%2520Granularity%2520Aligned%2520Aggregation%2520for%250A%2520%2520Event-Based%2520Lip-Reading%26entry.906535625%3DWenhao%2520Zhang%2520and%2520Jun%2520Wang%2520and%2520Yong%2520Luo%2520and%2520Lei%2520Yu%2520and%2520Wei%2520Yu%2520and%2520Zheng%2520He%2520and%2520Jialie%2520Shen%26entry.1292438233%3D%2520%2520Lip-reading%2520is%2520to%2520utilize%2520the%2520visual%2520information%2520of%2520the%2520speaker%2527s%2520lip%250Amovements%2520to%2520recognize%2520words%2520and%2520sentences.%2520Existing%2520event-based%2520lip-reading%250Asolutions%2520integrate%2520different%2520frame%2520rate%2520branches%2520to%2520learn%2520spatio-temporal%250Afeatures%2520of%2520varying%2520granularities.%2520However%252C%2520aggregating%2520events%2520into%2520event%250Aframes%2520inevitably%2520leads%2520to%2520the%2520loss%2520of%2520fine-grained%2520temporal%2520information%2520within%250Aframes.%2520To%2520remedy%2520this%2520drawback%252C%2520we%2520propose%2520a%2520novel%2520framework%2520termed%2520Multi-view%250ATemporal%2520Granularity%2520aligned%2520Aggregation%2520%2528MTGA%2529.%2520Specifically%252C%2520we%2520first%2520present%250Aa%2520novel%2520event%2520representation%2520method%252C%2520namely%2520time-segmented%2520voxel%2520graph%2520list%252C%250Awhere%2520the%2520most%2520significant%2520local%2520voxels%2520are%2520temporally%2520connected%2520into%2520a%2520graph%250Alist.%2520Then%2520we%2520design%2520a%2520spatio-temporal%2520fusion%2520module%2520based%2520on%2520temporal%250Agranularity%2520alignment%252C%2520where%2520the%2520global%2520spatial%2520features%2520extracted%2520from%2520event%250Aframes%252C%2520together%2520with%2520the%2520local%2520relative%2520spatial%2520and%2520temporal%2520features%250Acontained%2520in%2520voxel%2520graph%2520list%2520are%2520effectively%2520aligned%2520and%2520integrated.%2520Finally%252C%250Awe%2520design%2520a%2520temporal%2520aggregation%2520module%2520that%2520incorporates%2520positional%2520encoding%252C%250Awhich%2520enables%2520the%2520capture%2520of%2520local%2520absolute%2520spatial%2520and%2520global%2520temporal%250Ainformation.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520both%2520the%250Aevent-based%2520and%2520video-based%2520lip-reading%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTGA%3A%20Multi-View%20Temporal%20Granularity%20Aligned%20Aggregation%20for%0A%20%20Event-Based%20Lip-Reading&entry.906535625=Wenhao%20Zhang%20and%20Jun%20Wang%20and%20Yong%20Luo%20and%20Lei%20Yu%20and%20Wei%20Yu%20and%20Zheng%20He%20and%20Jialie%20Shen&entry.1292438233=%20%20Lip-reading%20is%20to%20utilize%20the%20visual%20information%20of%20the%20speaker%27s%20lip%0Amovements%20to%20recognize%20words%20and%20sentences.%20Existing%20event-based%20lip-reading%0Asolutions%20integrate%20different%20frame%20rate%20branches%20to%20learn%20spatio-temporal%0Afeatures%20of%20varying%20granularities.%20However%2C%20aggregating%20events%20into%20event%0Aframes%20inevitably%20leads%20to%20the%20loss%20of%20fine-grained%20temporal%20information%20within%0Aframes.%20To%20remedy%20this%20drawback%2C%20we%20propose%20a%20novel%20framework%20termed%20Multi-view%0ATemporal%20Granularity%20aligned%20Aggregation%20%28MTGA%29.%20Specifically%2C%20we%20first%20present%0Aa%20novel%20event%20representation%20method%2C%20namely%20time-segmented%20voxel%20graph%20list%2C%0Awhere%20the%20most%20significant%20local%20voxels%20are%20temporally%20connected%20into%20a%20graph%0Alist.%20Then%20we%20design%20a%20spatio-temporal%20fusion%20module%20based%20on%20temporal%0Agranularity%20alignment%2C%20where%20the%20global%20spatial%20features%20extracted%20from%20event%0Aframes%2C%20together%20with%20the%20local%20relative%20spatial%20and%20temporal%20features%0Acontained%20in%20voxel%20graph%20list%20are%20effectively%20aligned%20and%20integrated.%20Finally%2C%0Awe%20design%20a%20temporal%20aggregation%20module%20that%20incorporates%20positional%20encoding%2C%0Awhich%20enables%20the%20capture%20of%20local%20absolute%20spatial%20and%20global%20temporal%0Ainformation.%20Experiments%20demonstrate%20that%20our%20method%20outperforms%20both%20the%0Aevent-based%20and%20video-based%20lip-reading%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11979v2&entry.124074799=Read"},
{"title": "BiSSL: A Bilevel Optimization Framework for Enhancing the Alignment\n  Between Self-Supervised Pre-Training and Downstream Fine-Tuning", "author": "Gustav Wagner Zakarias and Lars Kai Hansen and Zheng-Hua Tan", "abstract": "  This study presents BiSSL, a novel training framework that utilizes bilevel\noptimization to enhance the alignment between the pretext pre-training and\ndownstream fine-tuning stages in self-supervised learning. BiSSL formulates the\npretext and downstream task objectives as the lower- and upper-level objectives\nin a bilevel optimization problem and serves as an intermediate training stage\nwithin the self-supervised learning pipeline. By explicitly modeling the\ninterdependence of these training stages, BiSSL facilitates enhanced\ninformation sharing between them, ultimately leading to a backbone parameter\ninitialization that is better aligned for the downstream task. We propose a\nversatile training algorithm that alternates between optimizing the two\nobjectives defined in BiSSL, which is applicable to a broad range of pretext\nand downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pre-train\nResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed\nframework significantly outperforms the conventional self-supervised learning\npipeline on the vast majority of 12 downstream image classification datasets,\nas well as on object detection. Visualizations of the backbone features provide\nfurther evidence that BiSSL improves the downstream task alignment of the\nbackbone features prior to fine-tuning.\n", "link": "http://arxiv.org/abs/2410.02387v3", "date": "2025-01-31", "relevancy": 2.5977, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiSSL%3A%20A%20Bilevel%20Optimization%20Framework%20for%20Enhancing%20the%20Alignment%0A%20%20Between%20Self-Supervised%20Pre-Training%20and%20Downstream%20Fine-Tuning&body=Title%3A%20BiSSL%3A%20A%20Bilevel%20Optimization%20Framework%20for%20Enhancing%20the%20Alignment%0A%20%20Between%20Self-Supervised%20Pre-Training%20and%20Downstream%20Fine-Tuning%0AAuthor%3A%20Gustav%20Wagner%20Zakarias%20and%20Lars%20Kai%20Hansen%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20This%20study%20presents%20BiSSL%2C%20a%20novel%20training%20framework%20that%20utilizes%20bilevel%0Aoptimization%20to%20enhance%20the%20alignment%20between%20the%20pretext%20pre-training%20and%0Adownstream%20fine-tuning%20stages%20in%20self-supervised%20learning.%20BiSSL%20formulates%20the%0Apretext%20and%20downstream%20task%20objectives%20as%20the%20lower-%20and%20upper-level%20objectives%0Ain%20a%20bilevel%20optimization%20problem%20and%20serves%20as%20an%20intermediate%20training%20stage%0Awithin%20the%20self-supervised%20learning%20pipeline.%20By%20explicitly%20modeling%20the%0Ainterdependence%20of%20these%20training%20stages%2C%20BiSSL%20facilitates%20enhanced%0Ainformation%20sharing%20between%20them%2C%20ultimately%20leading%20to%20a%20backbone%20parameter%0Ainitialization%20that%20is%20better%20aligned%20for%20the%20downstream%20task.%20We%20propose%20a%0Aversatile%20training%20algorithm%20that%20alternates%20between%20optimizing%20the%20two%0Aobjectives%20defined%20in%20BiSSL%2C%20which%20is%20applicable%20to%20a%20broad%20range%20of%20pretext%0Aand%20downstream%20tasks.%20Using%20SimCLR%20and%20Bootstrap%20Your%20Own%20Latent%20to%20pre-train%0AResNet-50%20backbones%20on%20the%20ImageNet%20dataset%2C%20we%20demonstrate%20that%20our%20proposed%0Aframework%20significantly%20outperforms%20the%20conventional%20self-supervised%20learning%0Apipeline%20on%20the%20vast%20majority%20of%2012%20downstream%20image%20classification%20datasets%2C%0Aas%20well%20as%20on%20object%20detection.%20Visualizations%20of%20the%20backbone%20features%20provide%0Afurther%20evidence%20that%20BiSSL%20improves%20the%20downstream%20task%20alignment%20of%20the%0Abackbone%20features%20prior%20to%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiSSL%253A%2520A%2520Bilevel%2520Optimization%2520Framework%2520for%2520Enhancing%2520the%2520Alignment%250A%2520%2520Between%2520Self-Supervised%2520Pre-Training%2520and%2520Downstream%2520Fine-Tuning%26entry.906535625%3DGustav%2520Wagner%2520Zakarias%2520and%2520Lars%2520Kai%2520Hansen%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520BiSSL%252C%2520a%2520novel%2520training%2520framework%2520that%2520utilizes%2520bilevel%250Aoptimization%2520to%2520enhance%2520the%2520alignment%2520between%2520the%2520pretext%2520pre-training%2520and%250Adownstream%2520fine-tuning%2520stages%2520in%2520self-supervised%2520learning.%2520BiSSL%2520formulates%2520the%250Apretext%2520and%2520downstream%2520task%2520objectives%2520as%2520the%2520lower-%2520and%2520upper-level%2520objectives%250Ain%2520a%2520bilevel%2520optimization%2520problem%2520and%2520serves%2520as%2520an%2520intermediate%2520training%2520stage%250Awithin%2520the%2520self-supervised%2520learning%2520pipeline.%2520By%2520explicitly%2520modeling%2520the%250Ainterdependence%2520of%2520these%2520training%2520stages%252C%2520BiSSL%2520facilitates%2520enhanced%250Ainformation%2520sharing%2520between%2520them%252C%2520ultimately%2520leading%2520to%2520a%2520backbone%2520parameter%250Ainitialization%2520that%2520is%2520better%2520aligned%2520for%2520the%2520downstream%2520task.%2520We%2520propose%2520a%250Aversatile%2520training%2520algorithm%2520that%2520alternates%2520between%2520optimizing%2520the%2520two%250Aobjectives%2520defined%2520in%2520BiSSL%252C%2520which%2520is%2520applicable%2520to%2520a%2520broad%2520range%2520of%2520pretext%250Aand%2520downstream%2520tasks.%2520Using%2520SimCLR%2520and%2520Bootstrap%2520Your%2520Own%2520Latent%2520to%2520pre-train%250AResNet-50%2520backbones%2520on%2520the%2520ImageNet%2520dataset%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%250Aframework%2520significantly%2520outperforms%2520the%2520conventional%2520self-supervised%2520learning%250Apipeline%2520on%2520the%2520vast%2520majority%2520of%252012%2520downstream%2520image%2520classification%2520datasets%252C%250Aas%2520well%2520as%2520on%2520object%2520detection.%2520Visualizations%2520of%2520the%2520backbone%2520features%2520provide%250Afurther%2520evidence%2520that%2520BiSSL%2520improves%2520the%2520downstream%2520task%2520alignment%2520of%2520the%250Abackbone%2520features%2520prior%2520to%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiSSL%3A%20A%20Bilevel%20Optimization%20Framework%20for%20Enhancing%20the%20Alignment%0A%20%20Between%20Self-Supervised%20Pre-Training%20and%20Downstream%20Fine-Tuning&entry.906535625=Gustav%20Wagner%20Zakarias%20and%20Lars%20Kai%20Hansen%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20This%20study%20presents%20BiSSL%2C%20a%20novel%20training%20framework%20that%20utilizes%20bilevel%0Aoptimization%20to%20enhance%20the%20alignment%20between%20the%20pretext%20pre-training%20and%0Adownstream%20fine-tuning%20stages%20in%20self-supervised%20learning.%20BiSSL%20formulates%20the%0Apretext%20and%20downstream%20task%20objectives%20as%20the%20lower-%20and%20upper-level%20objectives%0Ain%20a%20bilevel%20optimization%20problem%20and%20serves%20as%20an%20intermediate%20training%20stage%0Awithin%20the%20self-supervised%20learning%20pipeline.%20By%20explicitly%20modeling%20the%0Ainterdependence%20of%20these%20training%20stages%2C%20BiSSL%20facilitates%20enhanced%0Ainformation%20sharing%20between%20them%2C%20ultimately%20leading%20to%20a%20backbone%20parameter%0Ainitialization%20that%20is%20better%20aligned%20for%20the%20downstream%20task.%20We%20propose%20a%0Aversatile%20training%20algorithm%20that%20alternates%20between%20optimizing%20the%20two%0Aobjectives%20defined%20in%20BiSSL%2C%20which%20is%20applicable%20to%20a%20broad%20range%20of%20pretext%0Aand%20downstream%20tasks.%20Using%20SimCLR%20and%20Bootstrap%20Your%20Own%20Latent%20to%20pre-train%0AResNet-50%20backbones%20on%20the%20ImageNet%20dataset%2C%20we%20demonstrate%20that%20our%20proposed%0Aframework%20significantly%20outperforms%20the%20conventional%20self-supervised%20learning%0Apipeline%20on%20the%20vast%20majority%20of%2012%20downstream%20image%20classification%20datasets%2C%0Aas%20well%20as%20on%20object%20detection.%20Visualizations%20of%20the%20backbone%20features%20provide%0Afurther%20evidence%20that%20BiSSL%20improves%20the%20downstream%20task%20alignment%20of%20the%0Abackbone%20features%20prior%20to%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02387v3&entry.124074799=Read"},
{"title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling", "author": "Hong Huang and Hai Yang and Yuan Chen and Jiaxun Ye and Dapeng Wu", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS\n", "link": "http://arxiv.org/abs/2501.19122v1", "date": "2025-01-31", "relevancy": 2.5752, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5103}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedRTS%3A%20Federated%20Robust%20Pruning%20via%20Combinatorial%20Thompson%20Sampling&body=Title%3A%20FedRTS%3A%20Federated%20Robust%20Pruning%20via%20Combinatorial%20Thompson%20Sampling%0AAuthor%3A%20Hong%20Huang%20and%20Hai%20Yang%20and%20Yuan%20Chen%20and%20Jiaxun%20Ye%20and%20Dapeng%20Wu%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20without%20data%20sharing%2C%20but%20its%20high%20computational%20and%0Acommunication%20demands%20strain%20resource-constrained%20devices.%20While%20existing%0Amethods%20use%20dynamic%20pruning%20to%20improve%20efficiency%20by%20periodically%20adjusting%0Asparse%20model%20topologies%20while%20maintaining%20sparsity%2C%20these%20approaches%20suffer%0Afrom%20issues%20such%20as%20greedy%20adjustments%2C%20unstable%20topologies%2C%20and%20communication%0Ainefficiency%2C%20resulting%20in%20less%20robust%20models%20and%20suboptimal%20performance%20under%0Adata%20heterogeneity%20and%20partial%20client%20availability.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Federated%20Robust%20pruning%20via%20combinatorial%20Thompson%0ASampling%20%28FedRTS%29%2C%20a%20novel%20framework%20designed%20to%20develop%20robust%20sparse%20models.%0AFedRTS%20enhances%20robustness%20and%20performance%20through%20its%20Thompson%20Sampling-based%0AAdjustment%20%28TSAdj%29%20mechanism%2C%20which%20uses%20probabilistic%20decisions%20informed%20by%0Astable%2C%20farsighted%20information%20instead%20of%20deterministic%20decisions%20reliant%20on%0Aunstable%20and%20myopic%20information%20in%20previous%20methods.%20Extensive%20experiments%0Ademonstrate%20that%20FedRTS%20achieves%20state-of-the-art%20performance%20in%20computer%0Avision%20and%20natural%20language%20processing%20tasks%20while%20reducing%20communication%0Acosts%2C%20particularly%20excelling%20in%20scenarios%20with%20heterogeneous%20data%0Adistributions%20and%20partial%20client%20participation.%20Our%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/Little0o0/FedRTS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedRTS%253A%2520Federated%2520Robust%2520Pruning%2520via%2520Combinatorial%2520Thompson%2520Sampling%26entry.906535625%3DHong%2520Huang%2520and%2520Hai%2520Yang%2520and%2520Yuan%2520Chen%2520and%2520Jiaxun%2520Ye%2520and%2520Dapeng%2520Wu%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520without%2520data%2520sharing%252C%2520but%2520its%2520high%2520computational%2520and%250Acommunication%2520demands%2520strain%2520resource-constrained%2520devices.%2520While%2520existing%250Amethods%2520use%2520dynamic%2520pruning%2520to%2520improve%2520efficiency%2520by%2520periodically%2520adjusting%250Asparse%2520model%2520topologies%2520while%2520maintaining%2520sparsity%252C%2520these%2520approaches%2520suffer%250Afrom%2520issues%2520such%2520as%2520greedy%2520adjustments%252C%2520unstable%2520topologies%252C%2520and%2520communication%250Ainefficiency%252C%2520resulting%2520in%2520less%2520robust%2520models%2520and%2520suboptimal%2520performance%2520under%250Adata%2520heterogeneity%2520and%2520partial%2520client%2520availability.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520Federated%2520Robust%2520pruning%2520via%2520combinatorial%2520Thompson%250ASampling%2520%2528FedRTS%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520develop%2520robust%2520sparse%2520models.%250AFedRTS%2520enhances%2520robustness%2520and%2520performance%2520through%2520its%2520Thompson%2520Sampling-based%250AAdjustment%2520%2528TSAdj%2529%2520mechanism%252C%2520which%2520uses%2520probabilistic%2520decisions%2520informed%2520by%250Astable%252C%2520farsighted%2520information%2520instead%2520of%2520deterministic%2520decisions%2520reliant%2520on%250Aunstable%2520and%2520myopic%2520information%2520in%2520previous%2520methods.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520FedRTS%2520achieves%2520state-of-the-art%2520performance%2520in%2520computer%250Avision%2520and%2520natural%2520language%2520processing%2520tasks%2520while%2520reducing%2520communication%250Acosts%252C%2520particularly%2520excelling%2520in%2520scenarios%2520with%2520heterogeneous%2520data%250Adistributions%2520and%2520partial%2520client%2520participation.%2520Our%2520codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Little0o0/FedRTS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedRTS%3A%20Federated%20Robust%20Pruning%20via%20Combinatorial%20Thompson%20Sampling&entry.906535625=Hong%20Huang%20and%20Hai%20Yang%20and%20Yuan%20Chen%20and%20Jiaxun%20Ye%20and%20Dapeng%20Wu&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20without%20data%20sharing%2C%20but%20its%20high%20computational%20and%0Acommunication%20demands%20strain%20resource-constrained%20devices.%20While%20existing%0Amethods%20use%20dynamic%20pruning%20to%20improve%20efficiency%20by%20periodically%20adjusting%0Asparse%20model%20topologies%20while%20maintaining%20sparsity%2C%20these%20approaches%20suffer%0Afrom%20issues%20such%20as%20greedy%20adjustments%2C%20unstable%20topologies%2C%20and%20communication%0Ainefficiency%2C%20resulting%20in%20less%20robust%20models%20and%20suboptimal%20performance%20under%0Adata%20heterogeneity%20and%20partial%20client%20availability.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Federated%20Robust%20pruning%20via%20combinatorial%20Thompson%0ASampling%20%28FedRTS%29%2C%20a%20novel%20framework%20designed%20to%20develop%20robust%20sparse%20models.%0AFedRTS%20enhances%20robustness%20and%20performance%20through%20its%20Thompson%20Sampling-based%0AAdjustment%20%28TSAdj%29%20mechanism%2C%20which%20uses%20probabilistic%20decisions%20informed%20by%0Astable%2C%20farsighted%20information%20instead%20of%20deterministic%20decisions%20reliant%20on%0Aunstable%20and%20myopic%20information%20in%20previous%20methods.%20Extensive%20experiments%0Ademonstrate%20that%20FedRTS%20achieves%20state-of-the-art%20performance%20in%20computer%0Avision%20and%20natural%20language%20processing%20tasks%20while%20reducing%20communication%0Acosts%2C%20particularly%20excelling%20in%20scenarios%20with%20heterogeneous%20data%0Adistributions%20and%20partial%20client%20participation.%20Our%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/Little0o0/FedRTS%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19122v1&entry.124074799=Read"},
{"title": "Restoring balance: principled under/oversampling of data for optimal\n  classification", "author": "Emanuele Loffredo and Mauro Pastore and Simona Cocco and R\u00e9mi Monasson", "abstract": "  Class imbalance in real-world data poses a common bottleneck for machine\nlearning tasks, since achieving good generalization on under-represented\nexamples is often challenging. Mitigation strategies, such as under or\noversampling the data depending on their abundances, are routinely proposed and\ntested empirically, but how they should adapt to the data statistics remains\npoorly understood. In this work, we determine exact analytical expressions of\nthe generalization curves in the high-dimensional regime for linear classifiers\n(Support Vector Machines). We also provide a sharp prediction of the effects of\nunder/oversampling strategies depending on class imbalance, first and second\nmoments of the data, and the metrics of performance considered. We show that\nmixed strategies involving under and oversampling of data lead to performance\nimprovement. Through numerical experiments, we show the relevance of our\ntheoretical predictions on real datasets, on deeper architectures and with\nsampling strategies based on unsupervised probabilistic models.\n", "link": "http://arxiv.org/abs/2405.09535v2", "date": "2025-01-31", "relevancy": 2.5426, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification&body=Title%3A%20Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification%0AAuthor%3A%20Emanuele%20Loffredo%20and%20Mauro%20Pastore%20and%20Simona%20Cocco%20and%20R%C3%A9mi%20Monasson%0AAbstract%3A%20%20%20Class%20imbalance%20in%20real-world%20data%20poses%20a%20common%20bottleneck%20for%20machine%0Alearning%20tasks%2C%20since%20achieving%20good%20generalization%20on%20under-represented%0Aexamples%20is%20often%20challenging.%20Mitigation%20strategies%2C%20such%20as%20under%20or%0Aoversampling%20the%20data%20depending%20on%20their%20abundances%2C%20are%20routinely%20proposed%20and%0Atested%20empirically%2C%20but%20how%20they%20should%20adapt%20to%20the%20data%20statistics%20remains%0Apoorly%20understood.%20In%20this%20work%2C%20we%20determine%20exact%20analytical%20expressions%20of%0Athe%20generalization%20curves%20in%20the%20high-dimensional%20regime%20for%20linear%20classifiers%0A%28Support%20Vector%20Machines%29.%20We%20also%20provide%20a%20sharp%20prediction%20of%20the%20effects%20of%0Aunder/oversampling%20strategies%20depending%20on%20class%20imbalance%2C%20first%20and%20second%0Amoments%20of%20the%20data%2C%20and%20the%20metrics%20of%20performance%20considered.%20We%20show%20that%0Amixed%20strategies%20involving%20under%20and%20oversampling%20of%20data%20lead%20to%20performance%0Aimprovement.%20Through%20numerical%20experiments%2C%20we%20show%20the%20relevance%20of%20our%0Atheoretical%20predictions%20on%20real%20datasets%2C%20on%20deeper%20architectures%20and%20with%0Asampling%20strategies%20based%20on%20unsupervised%20probabilistic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestoring%2520balance%253A%2520principled%2520under/oversampling%2520of%2520data%2520for%2520optimal%250A%2520%2520classification%26entry.906535625%3DEmanuele%2520Loffredo%2520and%2520Mauro%2520Pastore%2520and%2520Simona%2520Cocco%2520and%2520R%25C3%25A9mi%2520Monasson%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520in%2520real-world%2520data%2520poses%2520a%2520common%2520bottleneck%2520for%2520machine%250Alearning%2520tasks%252C%2520since%2520achieving%2520good%2520generalization%2520on%2520under-represented%250Aexamples%2520is%2520often%2520challenging.%2520Mitigation%2520strategies%252C%2520such%2520as%2520under%2520or%250Aoversampling%2520the%2520data%2520depending%2520on%2520their%2520abundances%252C%2520are%2520routinely%2520proposed%2520and%250Atested%2520empirically%252C%2520but%2520how%2520they%2520should%2520adapt%2520to%2520the%2520data%2520statistics%2520remains%250Apoorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520determine%2520exact%2520analytical%2520expressions%2520of%250Athe%2520generalization%2520curves%2520in%2520the%2520high-dimensional%2520regime%2520for%2520linear%2520classifiers%250A%2528Support%2520Vector%2520Machines%2529.%2520We%2520also%2520provide%2520a%2520sharp%2520prediction%2520of%2520the%2520effects%2520of%250Aunder/oversampling%2520strategies%2520depending%2520on%2520class%2520imbalance%252C%2520first%2520and%2520second%250Amoments%2520of%2520the%2520data%252C%2520and%2520the%2520metrics%2520of%2520performance%2520considered.%2520We%2520show%2520that%250Amixed%2520strategies%2520involving%2520under%2520and%2520oversampling%2520of%2520data%2520lead%2520to%2520performance%250Aimprovement.%2520Through%2520numerical%2520experiments%252C%2520we%2520show%2520the%2520relevance%2520of%2520our%250Atheoretical%2520predictions%2520on%2520real%2520datasets%252C%2520on%2520deeper%2520architectures%2520and%2520with%250Asampling%2520strategies%2520based%2520on%2520unsupervised%2520probabilistic%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification&entry.906535625=Emanuele%20Loffredo%20and%20Mauro%20Pastore%20and%20Simona%20Cocco%20and%20R%C3%A9mi%20Monasson&entry.1292438233=%20%20Class%20imbalance%20in%20real-world%20data%20poses%20a%20common%20bottleneck%20for%20machine%0Alearning%20tasks%2C%20since%20achieving%20good%20generalization%20on%20under-represented%0Aexamples%20is%20often%20challenging.%20Mitigation%20strategies%2C%20such%20as%20under%20or%0Aoversampling%20the%20data%20depending%20on%20their%20abundances%2C%20are%20routinely%20proposed%20and%0Atested%20empirically%2C%20but%20how%20they%20should%20adapt%20to%20the%20data%20statistics%20remains%0Apoorly%20understood.%20In%20this%20work%2C%20we%20determine%20exact%20analytical%20expressions%20of%0Athe%20generalization%20curves%20in%20the%20high-dimensional%20regime%20for%20linear%20classifiers%0A%28Support%20Vector%20Machines%29.%20We%20also%20provide%20a%20sharp%20prediction%20of%20the%20effects%20of%0Aunder/oversampling%20strategies%20depending%20on%20class%20imbalance%2C%20first%20and%20second%0Amoments%20of%20the%20data%2C%20and%20the%20metrics%20of%20performance%20considered.%20We%20show%20that%0Amixed%20strategies%20involving%20under%20and%20oversampling%20of%20data%20lead%20to%20performance%0Aimprovement.%20Through%20numerical%20experiments%2C%20we%20show%20the%20relevance%20of%20our%0Atheoretical%20predictions%20on%20real%20datasets%2C%20on%20deeper%20architectures%20and%20with%0Asampling%20strategies%20based%20on%20unsupervised%20probabilistic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09535v2&entry.124074799=Read"},
{"title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search", "author": "Yuta Oshima and Masahiro Suzuki and Yutaka Matsuo and Hiroki Furuta", "abstract": "  The remarkable progress in text-to-video diffusion models enables\nphotorealistic generations, although the contents of the generated video often\ninclude unnatural movement or deformation, reverse playback, and motionless\nscenes. Recently, an alignment problem has attracted huge attention, where we\nsteer the output of diffusion models based on some quantity on the goodness of\nthe content. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect better diffusion latent to maximize a given alignment reward, at\ninference time. We then point out that the improvement of perceptual video\nquality considering the alignment to prompts requires reward calibration by\nweighting existing metrics. When evaluating outputs by using vision language\nmodels as a proxy of humans, many previous metrics to quantify the naturalness\nof video do not always correlate with evaluation and also depend on the degree\nof dynamic descriptions in evaluation prompts. We demonstrate that our method\nimproves the perceptual quality based on the calibrated reward, without model\nparameter update, and outputs the best generation compared to greedy search and\nbest-of-N sampling. We provide practical guidelines on which axes, among search\nbudget, lookahead steps for reward estimate, and denoising steps, in the\nreverse diffusion process, we should allocate the inference-time computation.\n", "link": "http://arxiv.org/abs/2501.19252v1", "date": "2025-01-31", "relevancy": 2.536, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6675}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6364}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search&body=Title%3A%20Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search%0AAuthor%3A%20Yuta%20Oshima%20and%20Masahiro%20Suzuki%20and%20Yutaka%20Matsuo%20and%20Hiroki%20Furuta%0AAbstract%3A%20%20%20The%20remarkable%20progress%20in%20text-to-video%20diffusion%20models%20enables%0Aphotorealistic%20generations%2C%20although%20the%20contents%20of%20the%20generated%20video%20often%0Ainclude%20unnatural%20movement%20or%20deformation%2C%20reverse%20playback%2C%20and%20motionless%0Ascenes.%20Recently%2C%20an%20alignment%20problem%20has%20attracted%20huge%20attention%2C%20where%20we%0Asteer%20the%20output%20of%20diffusion%20models%20based%20on%20some%20quantity%20on%20the%20goodness%20of%0Athe%20content.%20Because%20there%20is%20a%20large%20room%20for%20improvement%20of%20perceptual%0Aquality%20along%20the%20frame%20direction%2C%20we%20should%20address%20which%20metrics%20we%20should%0Aoptimize%20and%20how%20we%20can%20optimize%20them%20in%20the%20video%20generation.%20In%20this%20paper%2C%0Awe%20propose%20diffusion%20latent%20beam%20search%20with%20lookahead%20estimator%2C%20which%20can%0Aselect%20better%20diffusion%20latent%20to%20maximize%20a%20given%20alignment%20reward%2C%20at%0Ainference%20time.%20We%20then%20point%20out%20that%20the%20improvement%20of%20perceptual%20video%0Aquality%20considering%20the%20alignment%20to%20prompts%20requires%20reward%20calibration%20by%0Aweighting%20existing%20metrics.%20When%20evaluating%20outputs%20by%20using%20vision%20language%0Amodels%20as%20a%20proxy%20of%20humans%2C%20many%20previous%20metrics%20to%20quantify%20the%20naturalness%0Aof%20video%20do%20not%20always%20correlate%20with%20evaluation%20and%20also%20depend%20on%20the%20degree%0Aof%20dynamic%20descriptions%20in%20evaluation%20prompts.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20perceptual%20quality%20based%20on%20the%20calibrated%20reward%2C%20without%20model%0Aparameter%20update%2C%20and%20outputs%20the%20best%20generation%20compared%20to%20greedy%20search%20and%0Abest-of-N%20sampling.%20We%20provide%20practical%20guidelines%20on%20which%20axes%2C%20among%20search%0Abudget%2C%20lookahead%20steps%20for%20reward%20estimate%2C%20and%20denoising%20steps%2C%20in%20the%0Areverse%20diffusion%20process%2C%20we%20should%20allocate%20the%20inference-time%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Text-to-Video%2520Alignment%2520with%2520Diffusion%2520Latent%2520Beam%2520Search%26entry.906535625%3DYuta%2520Oshima%2520and%2520Masahiro%2520Suzuki%2520and%2520Yutaka%2520Matsuo%2520and%2520Hiroki%2520Furuta%26entry.1292438233%3D%2520%2520The%2520remarkable%2520progress%2520in%2520text-to-video%2520diffusion%2520models%2520enables%250Aphotorealistic%2520generations%252C%2520although%2520the%2520contents%2520of%2520the%2520generated%2520video%2520often%250Ainclude%2520unnatural%2520movement%2520or%2520deformation%252C%2520reverse%2520playback%252C%2520and%2520motionless%250Ascenes.%2520Recently%252C%2520an%2520alignment%2520problem%2520has%2520attracted%2520huge%2520attention%252C%2520where%2520we%250Asteer%2520the%2520output%2520of%2520diffusion%2520models%2520based%2520on%2520some%2520quantity%2520on%2520the%2520goodness%2520of%250Athe%2520content.%2520Because%2520there%2520is%2520a%2520large%2520room%2520for%2520improvement%2520of%2520perceptual%250Aquality%2520along%2520the%2520frame%2520direction%252C%2520we%2520should%2520address%2520which%2520metrics%2520we%2520should%250Aoptimize%2520and%2520how%2520we%2520can%2520optimize%2520them%2520in%2520the%2520video%2520generation.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520diffusion%2520latent%2520beam%2520search%2520with%2520lookahead%2520estimator%252C%2520which%2520can%250Aselect%2520better%2520diffusion%2520latent%2520to%2520maximize%2520a%2520given%2520alignment%2520reward%252C%2520at%250Ainference%2520time.%2520We%2520then%2520point%2520out%2520that%2520the%2520improvement%2520of%2520perceptual%2520video%250Aquality%2520considering%2520the%2520alignment%2520to%2520prompts%2520requires%2520reward%2520calibration%2520by%250Aweighting%2520existing%2520metrics.%2520When%2520evaluating%2520outputs%2520by%2520using%2520vision%2520language%250Amodels%2520as%2520a%2520proxy%2520of%2520humans%252C%2520many%2520previous%2520metrics%2520to%2520quantify%2520the%2520naturalness%250Aof%2520video%2520do%2520not%2520always%2520correlate%2520with%2520evaluation%2520and%2520also%2520depend%2520on%2520the%2520degree%250Aof%2520dynamic%2520descriptions%2520in%2520evaluation%2520prompts.%2520We%2520demonstrate%2520that%2520our%2520method%250Aimproves%2520the%2520perceptual%2520quality%2520based%2520on%2520the%2520calibrated%2520reward%252C%2520without%2520model%250Aparameter%2520update%252C%2520and%2520outputs%2520the%2520best%2520generation%2520compared%2520to%2520greedy%2520search%2520and%250Abest-of-N%2520sampling.%2520We%2520provide%2520practical%2520guidelines%2520on%2520which%2520axes%252C%2520among%2520search%250Abudget%252C%2520lookahead%2520steps%2520for%2520reward%2520estimate%252C%2520and%2520denoising%2520steps%252C%2520in%2520the%250Areverse%2520diffusion%2520process%252C%2520we%2520should%2520allocate%2520the%2520inference-time%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search&entry.906535625=Yuta%20Oshima%20and%20Masahiro%20Suzuki%20and%20Yutaka%20Matsuo%20and%20Hiroki%20Furuta&entry.1292438233=%20%20The%20remarkable%20progress%20in%20text-to-video%20diffusion%20models%20enables%0Aphotorealistic%20generations%2C%20although%20the%20contents%20of%20the%20generated%20video%20often%0Ainclude%20unnatural%20movement%20or%20deformation%2C%20reverse%20playback%2C%20and%20motionless%0Ascenes.%20Recently%2C%20an%20alignment%20problem%20has%20attracted%20huge%20attention%2C%20where%20we%0Asteer%20the%20output%20of%20diffusion%20models%20based%20on%20some%20quantity%20on%20the%20goodness%20of%0Athe%20content.%20Because%20there%20is%20a%20large%20room%20for%20improvement%20of%20perceptual%0Aquality%20along%20the%20frame%20direction%2C%20we%20should%20address%20which%20metrics%20we%20should%0Aoptimize%20and%20how%20we%20can%20optimize%20them%20in%20the%20video%20generation.%20In%20this%20paper%2C%0Awe%20propose%20diffusion%20latent%20beam%20search%20with%20lookahead%20estimator%2C%20which%20can%0Aselect%20better%20diffusion%20latent%20to%20maximize%20a%20given%20alignment%20reward%2C%20at%0Ainference%20time.%20We%20then%20point%20out%20that%20the%20improvement%20of%20perceptual%20video%0Aquality%20considering%20the%20alignment%20to%20prompts%20requires%20reward%20calibration%20by%0Aweighting%20existing%20metrics.%20When%20evaluating%20outputs%20by%20using%20vision%20language%0Amodels%20as%20a%20proxy%20of%20humans%2C%20many%20previous%20metrics%20to%20quantify%20the%20naturalness%0Aof%20video%20do%20not%20always%20correlate%20with%20evaluation%20and%20also%20depend%20on%20the%20degree%0Aof%20dynamic%20descriptions%20in%20evaluation%20prompts.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20perceptual%20quality%20based%20on%20the%20calibrated%20reward%2C%20without%20model%0Aparameter%20update%2C%20and%20outputs%20the%20best%20generation%20compared%20to%20greedy%20search%20and%0Abest-of-N%20sampling.%20We%20provide%20practical%20guidelines%20on%20which%20axes%2C%20among%20search%0Abudget%2C%20lookahead%20steps%20for%20reward%20estimate%2C%20and%20denoising%20steps%2C%20in%20the%0Areverse%20diffusion%20process%2C%20we%20should%20allocate%20the%20inference-time%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19252v1&entry.124074799=Read"},
{"title": "EEG-Language Modeling for Pathology Detection", "author": "Sam Gijsen and Kerstin Ritter", "abstract": "  Multimodal language modeling has enabled breakthroughs for representation\nlearning, yet remains unexplored in the realm of functional brain data for\npathology detection. This paper pioneers EEG-language models (ELMs) trained on\nclinical reports and 15000 EEGs. We propose to combine multimodal alignment in\nthis novel domain with timeseries cropping and text segmentation, enabling an\nextension based on multiple instance learning to alleviate misalignment between\nirrelevant EEG or text segments. Our multimodal models significantly improve\npathology detection compared to EEG-only models across four evaluations and for\nthe first time enable zero-shot classification as well as retrieval of both\nneural signals and reports. In sum, these results highlight the potential of\nELMs, representing significant progress for clinical applications.\n", "link": "http://arxiv.org/abs/2409.07480v3", "date": "2025-01-31", "relevancy": 2.5282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-Language%20Modeling%20for%20Pathology%20Detection&body=Title%3A%20EEG-Language%20Modeling%20for%20Pathology%20Detection%0AAuthor%3A%20Sam%20Gijsen%20and%20Kerstin%20Ritter%0AAbstract%3A%20%20%20Multimodal%20language%20modeling%20has%20enabled%20breakthroughs%20for%20representation%0Alearning%2C%20yet%20remains%20unexplored%20in%20the%20realm%20of%20functional%20brain%20data%20for%0Apathology%20detection.%20This%20paper%20pioneers%20EEG-language%20models%20%28ELMs%29%20trained%20on%0Aclinical%20reports%20and%2015000%20EEGs.%20We%20propose%20to%20combine%20multimodal%20alignment%20in%0Athis%20novel%20domain%20with%20timeseries%20cropping%20and%20text%20segmentation%2C%20enabling%20an%0Aextension%20based%20on%20multiple%20instance%20learning%20to%20alleviate%20misalignment%20between%0Airrelevant%20EEG%20or%20text%20segments.%20Our%20multimodal%20models%20significantly%20improve%0Apathology%20detection%20compared%20to%20EEG-only%20models%20across%20four%20evaluations%20and%20for%0Athe%20first%20time%20enable%20zero-shot%20classification%20as%20well%20as%20retrieval%20of%20both%0Aneural%20signals%20and%20reports.%20In%20sum%2C%20these%20results%20highlight%20the%20potential%20of%0AELMs%2C%20representing%20significant%20progress%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07480v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-Language%2520Modeling%2520for%2520Pathology%2520Detection%26entry.906535625%3DSam%2520Gijsen%2520and%2520Kerstin%2520Ritter%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520modeling%2520has%2520enabled%2520breakthroughs%2520for%2520representation%250Alearning%252C%2520yet%2520remains%2520unexplored%2520in%2520the%2520realm%2520of%2520functional%2520brain%2520data%2520for%250Apathology%2520detection.%2520This%2520paper%2520pioneers%2520EEG-language%2520models%2520%2528ELMs%2529%2520trained%2520on%250Aclinical%2520reports%2520and%252015000%2520EEGs.%2520We%2520propose%2520to%2520combine%2520multimodal%2520alignment%2520in%250Athis%2520novel%2520domain%2520with%2520timeseries%2520cropping%2520and%2520text%2520segmentation%252C%2520enabling%2520an%250Aextension%2520based%2520on%2520multiple%2520instance%2520learning%2520to%2520alleviate%2520misalignment%2520between%250Airrelevant%2520EEG%2520or%2520text%2520segments.%2520Our%2520multimodal%2520models%2520significantly%2520improve%250Apathology%2520detection%2520compared%2520to%2520EEG-only%2520models%2520across%2520four%2520evaluations%2520and%2520for%250Athe%2520first%2520time%2520enable%2520zero-shot%2520classification%2520as%2520well%2520as%2520retrieval%2520of%2520both%250Aneural%2520signals%2520and%2520reports.%2520In%2520sum%252C%2520these%2520results%2520highlight%2520the%2520potential%2520of%250AELMs%252C%2520representing%2520significant%2520progress%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07480v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-Language%20Modeling%20for%20Pathology%20Detection&entry.906535625=Sam%20Gijsen%20and%20Kerstin%20Ritter&entry.1292438233=%20%20Multimodal%20language%20modeling%20has%20enabled%20breakthroughs%20for%20representation%0Alearning%2C%20yet%20remains%20unexplored%20in%20the%20realm%20of%20functional%20brain%20data%20for%0Apathology%20detection.%20This%20paper%20pioneers%20EEG-language%20models%20%28ELMs%29%20trained%20on%0Aclinical%20reports%20and%2015000%20EEGs.%20We%20propose%20to%20combine%20multimodal%20alignment%20in%0Athis%20novel%20domain%20with%20timeseries%20cropping%20and%20text%20segmentation%2C%20enabling%20an%0Aextension%20based%20on%20multiple%20instance%20learning%20to%20alleviate%20misalignment%20between%0Airrelevant%20EEG%20or%20text%20segments.%20Our%20multimodal%20models%20significantly%20improve%0Apathology%20detection%20compared%20to%20EEG-only%20models%20across%20four%20evaluations%20and%20for%0Athe%20first%20time%20enable%20zero-shot%20classification%20as%20well%20as%20retrieval%20of%20both%0Aneural%20signals%20and%20reports.%20In%20sum%2C%20these%20results%20highlight%20the%20potential%20of%0AELMs%2C%20representing%20significant%20progress%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07480v3&entry.124074799=Read"},
{"title": "Application of Generative Adversarial Network (GAN) for Synthetic\n  Training Data Creation to improve performance of ANN Classifier for\n  extracting Built-Up pixels from Landsat Satellite Imagery", "author": "Amritendu Mukherjee and Dipanwita Sinha Mukherjee and Parthasarathy Ramachandran", "abstract": "  Training a neural network for pixel based classification task using low\nresolution Landsat images is difficult as the size of the training data is\nusually small due to less number of available pixels that represent a single\nclass without any mixing with other classes. Due to this scarcity of training\ndata, neural network may not be able to attain expected level of accuracy. This\nlimitation could be overcome using a generative network that aims to generate\nsynthetic data having the same distribution as the sample data with which it is\ntrained. In this work, we have proposed a methodology for improving the\nperformance of ANN classifier to identify built-up pixels in the Landsat$7$\nimage with the help of developing a simple GAN architecture that could generate\nsynthetic training pixels when trained using original set of sample built-up\npixels. To ensure that the marginal and joint distributions of all the bands\ncorresponding to the generated and original set of pixels are\nindistinguishable, non-parametric Kolmogorov Smirnov Test and Ball Divergence\nbased Equality of Distributions Test have been performed respectively. It has\nbeen observed that the overall accuracy and kappa coefficient of the ANN model\nfor built-up classification have continuously improved from $0.9331$ to\n$0.9983$ and $0.8277$ to $0.9958$ respectively, with the inclusion of generated\nsets of built-up pixels to the original one.\n", "link": "http://arxiv.org/abs/2501.19283v1", "date": "2025-01-31", "relevancy": 2.5241, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5044}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Generative%20Adversarial%20Network%20%28GAN%29%20for%20Synthetic%0A%20%20Training%20Data%20Creation%20to%20improve%20performance%20of%20ANN%20Classifier%20for%0A%20%20extracting%20Built-Up%20pixels%20from%20Landsat%20Satellite%20Imagery&body=Title%3A%20Application%20of%20Generative%20Adversarial%20Network%20%28GAN%29%20for%20Synthetic%0A%20%20Training%20Data%20Creation%20to%20improve%20performance%20of%20ANN%20Classifier%20for%0A%20%20extracting%20Built-Up%20pixels%20from%20Landsat%20Satellite%20Imagery%0AAuthor%3A%20Amritendu%20Mukherjee%20and%20Dipanwita%20Sinha%20Mukherjee%20and%20Parthasarathy%20Ramachandran%0AAbstract%3A%20%20%20Training%20a%20neural%20network%20for%20pixel%20based%20classification%20task%20using%20low%0Aresolution%20Landsat%20images%20is%20difficult%20as%20the%20size%20of%20the%20training%20data%20is%0Ausually%20small%20due%20to%20less%20number%20of%20available%20pixels%20that%20represent%20a%20single%0Aclass%20without%20any%20mixing%20with%20other%20classes.%20Due%20to%20this%20scarcity%20of%20training%0Adata%2C%20neural%20network%20may%20not%20be%20able%20to%20attain%20expected%20level%20of%20accuracy.%20This%0Alimitation%20could%20be%20overcome%20using%20a%20generative%20network%20that%20aims%20to%20generate%0Asynthetic%20data%20having%20the%20same%20distribution%20as%20the%20sample%20data%20with%20which%20it%20is%0Atrained.%20In%20this%20work%2C%20we%20have%20proposed%20a%20methodology%20for%20improving%20the%0Aperformance%20of%20ANN%20classifier%20to%20identify%20built-up%20pixels%20in%20the%20Landsat%247%24%0Aimage%20with%20the%20help%20of%20developing%20a%20simple%20GAN%20architecture%20that%20could%20generate%0Asynthetic%20training%20pixels%20when%20trained%20using%20original%20set%20of%20sample%20built-up%0Apixels.%20To%20ensure%20that%20the%20marginal%20and%20joint%20distributions%20of%20all%20the%20bands%0Acorresponding%20to%20the%20generated%20and%20original%20set%20of%20pixels%20are%0Aindistinguishable%2C%20non-parametric%20Kolmogorov%20Smirnov%20Test%20and%20Ball%20Divergence%0Abased%20Equality%20of%20Distributions%20Test%20have%20been%20performed%20respectively.%20It%20has%0Abeen%20observed%20that%20the%20overall%20accuracy%20and%20kappa%20coefficient%20of%20the%20ANN%20model%0Afor%20built-up%20classification%20have%20continuously%20improved%20from%20%240.9331%24%20to%0A%240.9983%24%20and%20%240.8277%24%20to%20%240.9958%24%20respectively%2C%20with%20the%20inclusion%20of%20generated%0Asets%20of%20built-up%20pixels%20to%20the%20original%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520for%2520Synthetic%250A%2520%2520Training%2520Data%2520Creation%2520to%2520improve%2520performance%2520of%2520ANN%2520Classifier%2520for%250A%2520%2520extracting%2520Built-Up%2520pixels%2520from%2520Landsat%2520Satellite%2520Imagery%26entry.906535625%3DAmritendu%2520Mukherjee%2520and%2520Dipanwita%2520Sinha%2520Mukherjee%2520and%2520Parthasarathy%2520Ramachandran%26entry.1292438233%3D%2520%2520Training%2520a%2520neural%2520network%2520for%2520pixel%2520based%2520classification%2520task%2520using%2520low%250Aresolution%2520Landsat%2520images%2520is%2520difficult%2520as%2520the%2520size%2520of%2520the%2520training%2520data%2520is%250Ausually%2520small%2520due%2520to%2520less%2520number%2520of%2520available%2520pixels%2520that%2520represent%2520a%2520single%250Aclass%2520without%2520any%2520mixing%2520with%2520other%2520classes.%2520Due%2520to%2520this%2520scarcity%2520of%2520training%250Adata%252C%2520neural%2520network%2520may%2520not%2520be%2520able%2520to%2520attain%2520expected%2520level%2520of%2520accuracy.%2520This%250Alimitation%2520could%2520be%2520overcome%2520using%2520a%2520generative%2520network%2520that%2520aims%2520to%2520generate%250Asynthetic%2520data%2520having%2520the%2520same%2520distribution%2520as%2520the%2520sample%2520data%2520with%2520which%2520it%2520is%250Atrained.%2520In%2520this%2520work%252C%2520we%2520have%2520proposed%2520a%2520methodology%2520for%2520improving%2520the%250Aperformance%2520of%2520ANN%2520classifier%2520to%2520identify%2520built-up%2520pixels%2520in%2520the%2520Landsat%25247%2524%250Aimage%2520with%2520the%2520help%2520of%2520developing%2520a%2520simple%2520GAN%2520architecture%2520that%2520could%2520generate%250Asynthetic%2520training%2520pixels%2520when%2520trained%2520using%2520original%2520set%2520of%2520sample%2520built-up%250Apixels.%2520To%2520ensure%2520that%2520the%2520marginal%2520and%2520joint%2520distributions%2520of%2520all%2520the%2520bands%250Acorresponding%2520to%2520the%2520generated%2520and%2520original%2520set%2520of%2520pixels%2520are%250Aindistinguishable%252C%2520non-parametric%2520Kolmogorov%2520Smirnov%2520Test%2520and%2520Ball%2520Divergence%250Abased%2520Equality%2520of%2520Distributions%2520Test%2520have%2520been%2520performed%2520respectively.%2520It%2520has%250Abeen%2520observed%2520that%2520the%2520overall%2520accuracy%2520and%2520kappa%2520coefficient%2520of%2520the%2520ANN%2520model%250Afor%2520built-up%2520classification%2520have%2520continuously%2520improved%2520from%2520%25240.9331%2524%2520to%250A%25240.9983%2524%2520and%2520%25240.8277%2524%2520to%2520%25240.9958%2524%2520respectively%252C%2520with%2520the%2520inclusion%2520of%2520generated%250Asets%2520of%2520built-up%2520pixels%2520to%2520the%2520original%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Generative%20Adversarial%20Network%20%28GAN%29%20for%20Synthetic%0A%20%20Training%20Data%20Creation%20to%20improve%20performance%20of%20ANN%20Classifier%20for%0A%20%20extracting%20Built-Up%20pixels%20from%20Landsat%20Satellite%20Imagery&entry.906535625=Amritendu%20Mukherjee%20and%20Dipanwita%20Sinha%20Mukherjee%20and%20Parthasarathy%20Ramachandran&entry.1292438233=%20%20Training%20a%20neural%20network%20for%20pixel%20based%20classification%20task%20using%20low%0Aresolution%20Landsat%20images%20is%20difficult%20as%20the%20size%20of%20the%20training%20data%20is%0Ausually%20small%20due%20to%20less%20number%20of%20available%20pixels%20that%20represent%20a%20single%0Aclass%20without%20any%20mixing%20with%20other%20classes.%20Due%20to%20this%20scarcity%20of%20training%0Adata%2C%20neural%20network%20may%20not%20be%20able%20to%20attain%20expected%20level%20of%20accuracy.%20This%0Alimitation%20could%20be%20overcome%20using%20a%20generative%20network%20that%20aims%20to%20generate%0Asynthetic%20data%20having%20the%20same%20distribution%20as%20the%20sample%20data%20with%20which%20it%20is%0Atrained.%20In%20this%20work%2C%20we%20have%20proposed%20a%20methodology%20for%20improving%20the%0Aperformance%20of%20ANN%20classifier%20to%20identify%20built-up%20pixels%20in%20the%20Landsat%247%24%0Aimage%20with%20the%20help%20of%20developing%20a%20simple%20GAN%20architecture%20that%20could%20generate%0Asynthetic%20training%20pixels%20when%20trained%20using%20original%20set%20of%20sample%20built-up%0Apixels.%20To%20ensure%20that%20the%20marginal%20and%20joint%20distributions%20of%20all%20the%20bands%0Acorresponding%20to%20the%20generated%20and%20original%20set%20of%20pixels%20are%0Aindistinguishable%2C%20non-parametric%20Kolmogorov%20Smirnov%20Test%20and%20Ball%20Divergence%0Abased%20Equality%20of%20Distributions%20Test%20have%20been%20performed%20respectively.%20It%20has%0Abeen%20observed%20that%20the%20overall%20accuracy%20and%20kappa%20coefficient%20of%20the%20ANN%20model%0Afor%20built-up%20classification%20have%20continuously%20improved%20from%20%240.9331%24%20to%0A%240.9983%24%20and%20%240.8277%24%20to%20%240.9958%24%20respectively%2C%20with%20the%20inclusion%20of%20generated%0Asets%20of%20built-up%20pixels%20to%20the%20original%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19283v1&entry.124074799=Read"},
{"title": "BEAT: Balanced Frequency Adaptive Tuning for Long-Term Time-Series\n  Forecasting", "author": "Zhixuan Li and Naipeng Chen and Seonghwa Choi and Sanghoon Lee and Weisi Lin", "abstract": "  Time-series forecasting is crucial for numerous real-world applications\nincluding weather prediction and financial market modeling. While\ntemporal-domain methods remain prevalent, frequency-domain approaches can\neffectively capture multi-scale periodic patterns, reduce sequence\ndependencies, and naturally denoise signals. However, existing approaches\ntypically train model components for all frequencies under a unified training\nobjective, often leading to mismatched learning speeds: high-frequency\ncomponents converge faster and risk overfitting, while low-frequency components\nunderfit due to insufficient training time. To deal with this challenge, we\npropose BEAT (Balanced frEquency Adaptive Tuning), a novel framework that\ndynamically monitors the training status for each frequency and adaptively\nadjusts their gradient updates. By recognizing convergence, overfitting, or\nunderfitting for each frequency, BEAT dynamically reallocates learning\npriorities, moderating gradients for rapid learners and increasing those for\nslower ones, alleviating the tension between competing objectives across\nfrequencies and synchronizing the overall learning process. Extensive\nexperiments on seven real-world datasets demonstrate that BEAT consistently\noutperforms state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2501.19065v1", "date": "2025-01-31", "relevancy": 2.5239, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5245}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5053}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEAT%3A%20Balanced%20Frequency%20Adaptive%20Tuning%20for%20Long-Term%20Time-Series%0A%20%20Forecasting&body=Title%3A%20BEAT%3A%20Balanced%20Frequency%20Adaptive%20Tuning%20for%20Long-Term%20Time-Series%0A%20%20Forecasting%0AAuthor%3A%20Zhixuan%20Li%20and%20Naipeng%20Chen%20and%20Seonghwa%20Choi%20and%20Sanghoon%20Lee%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Time-series%20forecasting%20is%20crucial%20for%20numerous%20real-world%20applications%0Aincluding%20weather%20prediction%20and%20financial%20market%20modeling.%20While%0Atemporal-domain%20methods%20remain%20prevalent%2C%20frequency-domain%20approaches%20can%0Aeffectively%20capture%20multi-scale%20periodic%20patterns%2C%20reduce%20sequence%0Adependencies%2C%20and%20naturally%20denoise%20signals.%20However%2C%20existing%20approaches%0Atypically%20train%20model%20components%20for%20all%20frequencies%20under%20a%20unified%20training%0Aobjective%2C%20often%20leading%20to%20mismatched%20learning%20speeds%3A%20high-frequency%0Acomponents%20converge%20faster%20and%20risk%20overfitting%2C%20while%20low-frequency%20components%0Aunderfit%20due%20to%20insufficient%20training%20time.%20To%20deal%20with%20this%20challenge%2C%20we%0Apropose%20BEAT%20%28Balanced%20frEquency%20Adaptive%20Tuning%29%2C%20a%20novel%20framework%20that%0Adynamically%20monitors%20the%20training%20status%20for%20each%20frequency%20and%20adaptively%0Aadjusts%20their%20gradient%20updates.%20By%20recognizing%20convergence%2C%20overfitting%2C%20or%0Aunderfitting%20for%20each%20frequency%2C%20BEAT%20dynamically%20reallocates%20learning%0Apriorities%2C%20moderating%20gradients%20for%20rapid%20learners%20and%20increasing%20those%20for%0Aslower%20ones%2C%20alleviating%20the%20tension%20between%20competing%20objectives%20across%0Afrequencies%20and%20synchronizing%20the%20overall%20learning%20process.%20Extensive%0Aexperiments%20on%20seven%20real-world%20datasets%20demonstrate%20that%20BEAT%20consistently%0Aoutperforms%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEAT%253A%2520Balanced%2520Frequency%2520Adaptive%2520Tuning%2520for%2520Long-Term%2520Time-Series%250A%2520%2520Forecasting%26entry.906535625%3DZhixuan%2520Li%2520and%2520Naipeng%2520Chen%2520and%2520Seonghwa%2520Choi%2520and%2520Sanghoon%2520Lee%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Time-series%2520forecasting%2520is%2520crucial%2520for%2520numerous%2520real-world%2520applications%250Aincluding%2520weather%2520prediction%2520and%2520financial%2520market%2520modeling.%2520While%250Atemporal-domain%2520methods%2520remain%2520prevalent%252C%2520frequency-domain%2520approaches%2520can%250Aeffectively%2520capture%2520multi-scale%2520periodic%2520patterns%252C%2520reduce%2520sequence%250Adependencies%252C%2520and%2520naturally%2520denoise%2520signals.%2520However%252C%2520existing%2520approaches%250Atypically%2520train%2520model%2520components%2520for%2520all%2520frequencies%2520under%2520a%2520unified%2520training%250Aobjective%252C%2520often%2520leading%2520to%2520mismatched%2520learning%2520speeds%253A%2520high-frequency%250Acomponents%2520converge%2520faster%2520and%2520risk%2520overfitting%252C%2520while%2520low-frequency%2520components%250Aunderfit%2520due%2520to%2520insufficient%2520training%2520time.%2520To%2520deal%2520with%2520this%2520challenge%252C%2520we%250Apropose%2520BEAT%2520%2528Balanced%2520frEquency%2520Adaptive%2520Tuning%2529%252C%2520a%2520novel%2520framework%2520that%250Adynamically%2520monitors%2520the%2520training%2520status%2520for%2520each%2520frequency%2520and%2520adaptively%250Aadjusts%2520their%2520gradient%2520updates.%2520By%2520recognizing%2520convergence%252C%2520overfitting%252C%2520or%250Aunderfitting%2520for%2520each%2520frequency%252C%2520BEAT%2520dynamically%2520reallocates%2520learning%250Apriorities%252C%2520moderating%2520gradients%2520for%2520rapid%2520learners%2520and%2520increasing%2520those%2520for%250Aslower%2520ones%252C%2520alleviating%2520the%2520tension%2520between%2520competing%2520objectives%2520across%250Afrequencies%2520and%2520synchronizing%2520the%2520overall%2520learning%2520process.%2520Extensive%250Aexperiments%2520on%2520seven%2520real-world%2520datasets%2520demonstrate%2520that%2520BEAT%2520consistently%250Aoutperforms%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEAT%3A%20Balanced%20Frequency%20Adaptive%20Tuning%20for%20Long-Term%20Time-Series%0A%20%20Forecasting&entry.906535625=Zhixuan%20Li%20and%20Naipeng%20Chen%20and%20Seonghwa%20Choi%20and%20Sanghoon%20Lee%20and%20Weisi%20Lin&entry.1292438233=%20%20Time-series%20forecasting%20is%20crucial%20for%20numerous%20real-world%20applications%0Aincluding%20weather%20prediction%20and%20financial%20market%20modeling.%20While%0Atemporal-domain%20methods%20remain%20prevalent%2C%20frequency-domain%20approaches%20can%0Aeffectively%20capture%20multi-scale%20periodic%20patterns%2C%20reduce%20sequence%0Adependencies%2C%20and%20naturally%20denoise%20signals.%20However%2C%20existing%20approaches%0Atypically%20train%20model%20components%20for%20all%20frequencies%20under%20a%20unified%20training%0Aobjective%2C%20often%20leading%20to%20mismatched%20learning%20speeds%3A%20high-frequency%0Acomponents%20converge%20faster%20and%20risk%20overfitting%2C%20while%20low-frequency%20components%0Aunderfit%20due%20to%20insufficient%20training%20time.%20To%20deal%20with%20this%20challenge%2C%20we%0Apropose%20BEAT%20%28Balanced%20frEquency%20Adaptive%20Tuning%29%2C%20a%20novel%20framework%20that%0Adynamically%20monitors%20the%20training%20status%20for%20each%20frequency%20and%20adaptively%0Aadjusts%20their%20gradient%20updates.%20By%20recognizing%20convergence%2C%20overfitting%2C%20or%0Aunderfitting%20for%20each%20frequency%2C%20BEAT%20dynamically%20reallocates%20learning%0Apriorities%2C%20moderating%20gradients%20for%20rapid%20learners%20and%20increasing%20those%20for%0Aslower%20ones%2C%20alleviating%20the%20tension%20between%20competing%20objectives%20across%0Afrequencies%20and%20synchronizing%20the%20overall%20learning%20process.%20Extensive%0Aexperiments%20on%20seven%20real-world%20datasets%20demonstrate%20that%20BEAT%20consistently%0Aoutperforms%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19065v1&entry.124074799=Read"},
{"title": "Large Language Models are In-context Preference Learners", "author": "Chao Yu and Qixin Tan and Hong Lu and Jiaxuan Gao and Xinting Yang and Yu Wang and Yi Wu and Eugene Vinitsky", "abstract": "  Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.\n", "link": "http://arxiv.org/abs/2410.17233v2", "date": "2025-01-31", "relevancy": 2.5233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20are%20In-context%20Preference%20Learners&body=Title%3A%20Large%20Language%20Models%20are%20In-context%20Preference%20Learners%0AAuthor%3A%20Chao%20Yu%20and%20Qixin%20Tan%20and%20Hong%20Lu%20and%20Jiaxuan%20Gao%20and%20Xinting%20Yang%20and%20Yu%20Wang%20and%20Yi%20Wu%20and%20Eugene%20Vinitsky%0AAbstract%3A%20%20%20Preference-based%20reinforcement%20learning%20is%20an%20effective%20way%20to%20handle%20tasks%0Awhere%20rewards%20are%20hard%20to%20specify%20but%20can%20be%20exceedingly%20inefficient%20as%0Apreference%20learning%20is%20often%20tabula%20rasa.%20We%20demonstrate%20that%20Large%20Language%0AModels%20%28LLMs%29%20have%20native%20preference-learning%20capabilities%20that%20allow%20them%20to%0Aachieve%20sample-efficient%20preference%20learning%2C%20addressing%20this%20challenge.%20We%0Apropose%20In-Context%20Preference%20Learning%20%28ICPL%29%2C%20which%20uses%20in-context%20learning%0Acapabilities%20of%20LLMs%20to%20reduce%20human%20query%20inefficiency.%20ICPL%20uses%20the%20task%0Adescription%20and%20basic%20environment%20code%20to%20create%20sets%20of%20reward%20functions%20which%0Aare%20iteratively%20refined%20by%20placing%20human%20feedback%20over%20videos%20of%20the%20resultant%0Apolicies%20into%20the%20context%20of%20an%20LLM%20and%20then%20requesting%20better%20rewards.%20We%0Afirst%20demonstrate%20ICPL%27s%20effectiveness%20through%20a%20synthetic%20preference%20study%2C%0Aproviding%20quantitative%20evidence%20that%20it%20significantly%20outperforms%20baseline%0Apreference-based%20methods%20with%20much%20higher%20performance%20and%20orders%20of%20magnitude%0Agreater%20efficiency.%20We%20observe%20that%20these%20improvements%20are%20not%20solely%20coming%0Afrom%20LLM%20grounding%20in%20the%20task%20but%20that%20the%20quality%20of%20the%20rewards%20improves%0Aover%20time%2C%20indicating%20preference%20learning%20capabilities.%20Additionally%2C%20we%0Aperform%20a%20series%20of%20real%20human%20preference-learning%20trials%20and%20observe%20that%20ICPL%0Aextends%20beyond%20synthetic%20settings%20and%20can%20work%20effectively%20with%0Ahumans-in-the-loop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520are%2520In-context%2520Preference%2520Learners%26entry.906535625%3DChao%2520Yu%2520and%2520Qixin%2520Tan%2520and%2520Hong%2520Lu%2520and%2520Jiaxuan%2520Gao%2520and%2520Xinting%2520Yang%2520and%2520Yu%2520Wang%2520and%2520Yi%2520Wu%2520and%2520Eugene%2520Vinitsky%26entry.1292438233%3D%2520%2520Preference-based%2520reinforcement%2520learning%2520is%2520an%2520effective%2520way%2520to%2520handle%2520tasks%250Awhere%2520rewards%2520are%2520hard%2520to%2520specify%2520but%2520can%2520be%2520exceedingly%2520inefficient%2520as%250Apreference%2520learning%2520is%2520often%2520tabula%2520rasa.%2520We%2520demonstrate%2520that%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520native%2520preference-learning%2520capabilities%2520that%2520allow%2520them%2520to%250Aachieve%2520sample-efficient%2520preference%2520learning%252C%2520addressing%2520this%2520challenge.%2520We%250Apropose%2520In-Context%2520Preference%2520Learning%2520%2528ICPL%2529%252C%2520which%2520uses%2520in-context%2520learning%250Acapabilities%2520of%2520LLMs%2520to%2520reduce%2520human%2520query%2520inefficiency.%2520ICPL%2520uses%2520the%2520task%250Adescription%2520and%2520basic%2520environment%2520code%2520to%2520create%2520sets%2520of%2520reward%2520functions%2520which%250Aare%2520iteratively%2520refined%2520by%2520placing%2520human%2520feedback%2520over%2520videos%2520of%2520the%2520resultant%250Apolicies%2520into%2520the%2520context%2520of%2520an%2520LLM%2520and%2520then%2520requesting%2520better%2520rewards.%2520We%250Afirst%2520demonstrate%2520ICPL%2527s%2520effectiveness%2520through%2520a%2520synthetic%2520preference%2520study%252C%250Aproviding%2520quantitative%2520evidence%2520that%2520it%2520significantly%2520outperforms%2520baseline%250Apreference-based%2520methods%2520with%2520much%2520higher%2520performance%2520and%2520orders%2520of%2520magnitude%250Agreater%2520efficiency.%2520We%2520observe%2520that%2520these%2520improvements%2520are%2520not%2520solely%2520coming%250Afrom%2520LLM%2520grounding%2520in%2520the%2520task%2520but%2520that%2520the%2520quality%2520of%2520the%2520rewards%2520improves%250Aover%2520time%252C%2520indicating%2520preference%2520learning%2520capabilities.%2520Additionally%252C%2520we%250Aperform%2520a%2520series%2520of%2520real%2520human%2520preference-learning%2520trials%2520and%2520observe%2520that%2520ICPL%250Aextends%2520beyond%2520synthetic%2520settings%2520and%2520can%2520work%2520effectively%2520with%250Ahumans-in-the-loop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20are%20In-context%20Preference%20Learners&entry.906535625=Chao%20Yu%20and%20Qixin%20Tan%20and%20Hong%20Lu%20and%20Jiaxuan%20Gao%20and%20Xinting%20Yang%20and%20Yu%20Wang%20and%20Yi%20Wu%20and%20Eugene%20Vinitsky&entry.1292438233=%20%20Preference-based%20reinforcement%20learning%20is%20an%20effective%20way%20to%20handle%20tasks%0Awhere%20rewards%20are%20hard%20to%20specify%20but%20can%20be%20exceedingly%20inefficient%20as%0Apreference%20learning%20is%20often%20tabula%20rasa.%20We%20demonstrate%20that%20Large%20Language%0AModels%20%28LLMs%29%20have%20native%20preference-learning%20capabilities%20that%20allow%20them%20to%0Aachieve%20sample-efficient%20preference%20learning%2C%20addressing%20this%20challenge.%20We%0Apropose%20In-Context%20Preference%20Learning%20%28ICPL%29%2C%20which%20uses%20in-context%20learning%0Acapabilities%20of%20LLMs%20to%20reduce%20human%20query%20inefficiency.%20ICPL%20uses%20the%20task%0Adescription%20and%20basic%20environment%20code%20to%20create%20sets%20of%20reward%20functions%20which%0Aare%20iteratively%20refined%20by%20placing%20human%20feedback%20over%20videos%20of%20the%20resultant%0Apolicies%20into%20the%20context%20of%20an%20LLM%20and%20then%20requesting%20better%20rewards.%20We%0Afirst%20demonstrate%20ICPL%27s%20effectiveness%20through%20a%20synthetic%20preference%20study%2C%0Aproviding%20quantitative%20evidence%20that%20it%20significantly%20outperforms%20baseline%0Apreference-based%20methods%20with%20much%20higher%20performance%20and%20orders%20of%20magnitude%0Agreater%20efficiency.%20We%20observe%20that%20these%20improvements%20are%20not%20solely%20coming%0Afrom%20LLM%20grounding%20in%20the%20task%20but%20that%20the%20quality%20of%20the%20rewards%20improves%0Aover%20time%2C%20indicating%20preference%20learning%20capabilities.%20Additionally%2C%20we%0Aperform%20a%20series%20of%20real%20human%20preference-learning%20trials%20and%20observe%20that%20ICPL%0Aextends%20beyond%20synthetic%20settings%20and%20can%20work%20effectively%20with%0Ahumans-in-the-loop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17233v2&entry.124074799=Read"},
{"title": "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations", "author": "Sihwan Park and Jihun Yun and SungYub Kim and Souvik Kundu and Eunho Yang", "abstract": "  Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.\n", "link": "http://arxiv.org/abs/2501.19099v1", "date": "2025-01-31", "relevancy": 2.507, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Zeroth-Order%20Optimization%20through%20the%20Lens%20of%20Low-Dimensional%0A%20%20Structured%20Perturbations&body=Title%3A%20Unraveling%20Zeroth-Order%20Optimization%20through%20the%20Lens%20of%20Low-Dimensional%0A%20%20Structured%20Perturbations%0AAuthor%3A%20Sihwan%20Park%20and%20Jihun%20Yun%20and%20SungYub%20Kim%20and%20Souvik%20Kundu%20and%20Eunho%20Yang%0AAbstract%3A%20%20%20Zeroth-order%20%28ZO%29%20optimization%20has%20emerged%20as%20a%20promising%20alternative%20to%0Agradient-based%20backpropagation%20methods%2C%20particularly%20for%20black-box%20optimization%0Aand%20large%20language%20model%20%28LLM%29%20fine-tuning.%20However%2C%20ZO%20methods%20suffer%20from%0Aslow%20convergence%20due%20to%20high-variance%20stochastic%20gradient%20estimators.%20While%0Astructured%20perturbations%2C%20such%20as%20sparsity%20and%20low-rank%20constraints%2C%20have%20been%0Aexplored%20to%20mitigate%20these%20issues%2C%20their%20effectiveness%20remains%20highly%0Aunder-explored.%20In%20this%20work%2C%20we%20develop%20a%20unified%20theoretical%20framework%20that%0Aanalyzes%20both%20the%20convergence%20and%20generalization%20properties%20of%20ZO%20optimization%0Aunder%20structured%20perturbations.%20We%20show%20that%20high%20dimensionality%20is%20the%20primary%0Abottleneck%20and%20introduce%20the%20notions%20of%20%5Ctextit%7Bstable%20rank%7D%20and%0A%5Ctextit%7Beffective%20overlap%7D%20to%20explain%20how%20structured%20perturbations%20reduce%0Agradient%20noise%20and%20accelerate%20convergence.%20Using%20the%20uniform%20stability%20under%0Aour%20framework%2C%20we%20then%20provide%20the%20first%20theoretical%20justification%20for%20why%0Athese%20perturbations%20enhance%20generalization.%20Additionally%2C%20through%20empirical%0Aanalysis%2C%20we%20identify%20that%20%5Ctextbf%7Bblock%20coordinate%20descent%7D%20%28BCD%29%20to%20be%20an%0Aeffective%20structured%20perturbation%20method.%20Extensive%20experiments%20show%20that%2C%0Acompared%20to%20existing%20alternatives%2C%20memory-efficient%20ZO%20%28MeZO%29%20with%20BCD%0A%28%5Ctextit%7BMeZO-BCD%7D%29%20can%20provide%20improved%20converge%20with%20a%20faster%20wall-clock%0Atime/iteration%20by%20up%20to%20%24%5Ctimes%5Ctextbf%7B2.09%7D%24%20while%20yielding%20similar%20or%20better%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Zeroth-Order%2520Optimization%2520through%2520the%2520Lens%2520of%2520Low-Dimensional%250A%2520%2520Structured%2520Perturbations%26entry.906535625%3DSihwan%2520Park%2520and%2520Jihun%2520Yun%2520and%2520SungYub%2520Kim%2520and%2520Souvik%2520Kundu%2520and%2520Eunho%2520Yang%26entry.1292438233%3D%2520%2520Zeroth-order%2520%2528ZO%2529%2520optimization%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520to%250Agradient-based%2520backpropagation%2520methods%252C%2520particularly%2520for%2520black-box%2520optimization%250Aand%2520large%2520language%2520model%2520%2528LLM%2529%2520fine-tuning.%2520However%252C%2520ZO%2520methods%2520suffer%2520from%250Aslow%2520convergence%2520due%2520to%2520high-variance%2520stochastic%2520gradient%2520estimators.%2520While%250Astructured%2520perturbations%252C%2520such%2520as%2520sparsity%2520and%2520low-rank%2520constraints%252C%2520have%2520been%250Aexplored%2520to%2520mitigate%2520these%2520issues%252C%2520their%2520effectiveness%2520remains%2520highly%250Aunder-explored.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520unified%2520theoretical%2520framework%2520that%250Aanalyzes%2520both%2520the%2520convergence%2520and%2520generalization%2520properties%2520of%2520ZO%2520optimization%250Aunder%2520structured%2520perturbations.%2520We%2520show%2520that%2520high%2520dimensionality%2520is%2520the%2520primary%250Abottleneck%2520and%2520introduce%2520the%2520notions%2520of%2520%255Ctextit%257Bstable%2520rank%257D%2520and%250A%255Ctextit%257Beffective%2520overlap%257D%2520to%2520explain%2520how%2520structured%2520perturbations%2520reduce%250Agradient%2520noise%2520and%2520accelerate%2520convergence.%2520Using%2520the%2520uniform%2520stability%2520under%250Aour%2520framework%252C%2520we%2520then%2520provide%2520the%2520first%2520theoretical%2520justification%2520for%2520why%250Athese%2520perturbations%2520enhance%2520generalization.%2520Additionally%252C%2520through%2520empirical%250Aanalysis%252C%2520we%2520identify%2520that%2520%255Ctextbf%257Bblock%2520coordinate%2520descent%257D%2520%2528BCD%2529%2520to%2520be%2520an%250Aeffective%2520structured%2520perturbation%2520method.%2520Extensive%2520experiments%2520show%2520that%252C%250Acompared%2520to%2520existing%2520alternatives%252C%2520memory-efficient%2520ZO%2520%2528MeZO%2529%2520with%2520BCD%250A%2528%255Ctextit%257BMeZO-BCD%257D%2529%2520can%2520provide%2520improved%2520converge%2520with%2520a%2520faster%2520wall-clock%250Atime/iteration%2520by%2520up%2520to%2520%2524%255Ctimes%255Ctextbf%257B2.09%257D%2524%2520while%2520yielding%2520similar%2520or%2520better%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Zeroth-Order%20Optimization%20through%20the%20Lens%20of%20Low-Dimensional%0A%20%20Structured%20Perturbations&entry.906535625=Sihwan%20Park%20and%20Jihun%20Yun%20and%20SungYub%20Kim%20and%20Souvik%20Kundu%20and%20Eunho%20Yang&entry.1292438233=%20%20Zeroth-order%20%28ZO%29%20optimization%20has%20emerged%20as%20a%20promising%20alternative%20to%0Agradient-based%20backpropagation%20methods%2C%20particularly%20for%20black-box%20optimization%0Aand%20large%20language%20model%20%28LLM%29%20fine-tuning.%20However%2C%20ZO%20methods%20suffer%20from%0Aslow%20convergence%20due%20to%20high-variance%20stochastic%20gradient%20estimators.%20While%0Astructured%20perturbations%2C%20such%20as%20sparsity%20and%20low-rank%20constraints%2C%20have%20been%0Aexplored%20to%20mitigate%20these%20issues%2C%20their%20effectiveness%20remains%20highly%0Aunder-explored.%20In%20this%20work%2C%20we%20develop%20a%20unified%20theoretical%20framework%20that%0Aanalyzes%20both%20the%20convergence%20and%20generalization%20properties%20of%20ZO%20optimization%0Aunder%20structured%20perturbations.%20We%20show%20that%20high%20dimensionality%20is%20the%20primary%0Abottleneck%20and%20introduce%20the%20notions%20of%20%5Ctextit%7Bstable%20rank%7D%20and%0A%5Ctextit%7Beffective%20overlap%7D%20to%20explain%20how%20structured%20perturbations%20reduce%0Agradient%20noise%20and%20accelerate%20convergence.%20Using%20the%20uniform%20stability%20under%0Aour%20framework%2C%20we%20then%20provide%20the%20first%20theoretical%20justification%20for%20why%0Athese%20perturbations%20enhance%20generalization.%20Additionally%2C%20through%20empirical%0Aanalysis%2C%20we%20identify%20that%20%5Ctextbf%7Bblock%20coordinate%20descent%7D%20%28BCD%29%20to%20be%20an%0Aeffective%20structured%20perturbation%20method.%20Extensive%20experiments%20show%20that%2C%0Acompared%20to%20existing%20alternatives%2C%20memory-efficient%20ZO%20%28MeZO%29%20with%20BCD%0A%28%5Ctextit%7BMeZO-BCD%7D%29%20can%20provide%20improved%20converge%20with%20a%20faster%20wall-clock%0Atime/iteration%20by%20up%20to%20%24%5Ctimes%5Ctextbf%7B2.09%7D%24%20while%20yielding%20similar%20or%20better%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19099v1&entry.124074799=Read"},
{"title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation", "author": "Yunzhe Li and Junting Wang and Hari Sundaram and Zhining Liu", "abstract": "  Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.\n", "link": "http://arxiv.org/abs/2501.19232v1", "date": "2025-01-31", "relevancy": 2.5057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Zero-Shot%20Generalization%20Framework%20for%20LLM-Driven%20Cross-Domain%0A%20%20Sequential%20Recommendation&body=Title%3A%20A%20Zero-Shot%20Generalization%20Framework%20for%20LLM-Driven%20Cross-Domain%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Yunzhe%20Li%20and%20Junting%20Wang%20and%20Hari%20Sundaram%20and%20Zhining%20Liu%0AAbstract%3A%20%20%20Zero-shot%20cross-domain%20sequential%20recommendation%20%28ZCDSR%29%20enables%20predictions%0Ain%20unseen%20domains%20without%20the%20need%20for%20additional%20training%20or%20fine-tuning%2C%0Amaking%20it%20particularly%20valuable%20in%20data-sparse%20environments%20where%20traditional%0Amodels%20struggle.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%0Agreatly%20improved%20ZCDSR%20by%20leveraging%20rich%20pretrained%20representations%20to%0Afacilitate%20cross-domain%20knowledge%20transfer.%20However%2C%20a%20key%20challenge%20persists%3A%0Adomain%20semantic%20bias%2C%20which%20arises%20from%20variations%20in%20vocabulary%20and%20content%0Afocus%20across%20domains.%20This%20misalignment%20leads%20to%20inconsistencies%20in%20item%0Aembeddings%20and%20hinders%20generalization.%0A%20%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20designed%20to%20enhance%0ALLM-based%20ZCDSR%20by%20improving%20cross-domain%20alignment%20at%20both%20the%20item%20and%0Asequential%20levels.%20At%20the%20item%20level%2C%20we%20introduce%20a%20generalization%20loss%20that%0Apromotes%20inter-domain%20compactness%20by%20aligning%20embeddings%20of%20similar%20items%0Aacross%20domains%20while%20maintaining%20intra-domain%20diversity%20to%20preserve%20unique%20item%0Acharacteristics.%20This%20prevents%20embeddings%20from%20becoming%20overly%20generic%20while%0Aensuring%20effective%20transferability.%20At%20the%20sequential%20level%2C%20we%20develop%20a%0Amethod%20for%20transferring%20user%20behavioral%20patterns%20by%20clustering%20user%20sequences%0Ain%20the%20source%20domain%20and%20applying%20attention-based%20aggregation%20for%20target%20domain%0Ainference.%20This%20dynamic%20adaptation%20of%20user%20embeddings%20allows%20effective%0Azero-shot%20recommendations%20without%20requiring%20target-domain%20interactions.%0A%20%20Comprehensive%20experiments%20across%20multiple%20datasets%20and%20domains%20demonstrate%0Athat%20our%20framework%20significantly%20improves%20sequential%20recommendation%20performance%0Ain%20the%20ZCDSR%20setting.%20By%20mitigating%20domain%20bias%20and%20enhancing%20the%0Atransferability%20of%20sequential%20patterns%2C%20our%20method%20provides%20a%20scalable%20and%0Arobust%20approach%20for%20achieving%20more%20effective%20zero-shot%20recommendations%20across%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Zero-Shot%2520Generalization%2520Framework%2520for%2520LLM-Driven%2520Cross-Domain%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DYunzhe%2520Li%2520and%2520Junting%2520Wang%2520and%2520Hari%2520Sundaram%2520and%2520Zhining%2520Liu%26entry.1292438233%3D%2520%2520Zero-shot%2520cross-domain%2520sequential%2520recommendation%2520%2528ZCDSR%2529%2520enables%2520predictions%250Ain%2520unseen%2520domains%2520without%2520the%2520need%2520for%2520additional%2520training%2520or%2520fine-tuning%252C%250Amaking%2520it%2520particularly%2520valuable%2520in%2520data-sparse%2520environments%2520where%2520traditional%250Amodels%2520struggle.%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Agreatly%2520improved%2520ZCDSR%2520by%2520leveraging%2520rich%2520pretrained%2520representations%2520to%250Afacilitate%2520cross-domain%2520knowledge%2520transfer.%2520However%252C%2520a%2520key%2520challenge%2520persists%253A%250Adomain%2520semantic%2520bias%252C%2520which%2520arises%2520from%2520variations%2520in%2520vocabulary%2520and%2520content%250Afocus%2520across%2520domains.%2520This%2520misalignment%2520leads%2520to%2520inconsistencies%2520in%2520item%250Aembeddings%2520and%2520hinders%2520generalization.%250A%2520%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%250ALLM-based%2520ZCDSR%2520by%2520improving%2520cross-domain%2520alignment%2520at%2520both%2520the%2520item%2520and%250Asequential%2520levels.%2520At%2520the%2520item%2520level%252C%2520we%2520introduce%2520a%2520generalization%2520loss%2520that%250Apromotes%2520inter-domain%2520compactness%2520by%2520aligning%2520embeddings%2520of%2520similar%2520items%250Aacross%2520domains%2520while%2520maintaining%2520intra-domain%2520diversity%2520to%2520preserve%2520unique%2520item%250Acharacteristics.%2520This%2520prevents%2520embeddings%2520from%2520becoming%2520overly%2520generic%2520while%250Aensuring%2520effective%2520transferability.%2520At%2520the%2520sequential%2520level%252C%2520we%2520develop%2520a%250Amethod%2520for%2520transferring%2520user%2520behavioral%2520patterns%2520by%2520clustering%2520user%2520sequences%250Ain%2520the%2520source%2520domain%2520and%2520applying%2520attention-based%2520aggregation%2520for%2520target%2520domain%250Ainference.%2520This%2520dynamic%2520adaptation%2520of%2520user%2520embeddings%2520allows%2520effective%250Azero-shot%2520recommendations%2520without%2520requiring%2520target-domain%2520interactions.%250A%2520%2520Comprehensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520domains%2520demonstrate%250Athat%2520our%2520framework%2520significantly%2520improves%2520sequential%2520recommendation%2520performance%250Ain%2520the%2520ZCDSR%2520setting.%2520By%2520mitigating%2520domain%2520bias%2520and%2520enhancing%2520the%250Atransferability%2520of%2520sequential%2520patterns%252C%2520our%2520method%2520provides%2520a%2520scalable%2520and%250Arobust%2520approach%2520for%2520achieving%2520more%2520effective%2520zero-shot%2520recommendations%2520across%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Zero-Shot%20Generalization%20Framework%20for%20LLM-Driven%20Cross-Domain%0A%20%20Sequential%20Recommendation&entry.906535625=Yunzhe%20Li%20and%20Junting%20Wang%20and%20Hari%20Sundaram%20and%20Zhining%20Liu&entry.1292438233=%20%20Zero-shot%20cross-domain%20sequential%20recommendation%20%28ZCDSR%29%20enables%20predictions%0Ain%20unseen%20domains%20without%20the%20need%20for%20additional%20training%20or%20fine-tuning%2C%0Amaking%20it%20particularly%20valuable%20in%20data-sparse%20environments%20where%20traditional%0Amodels%20struggle.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%0Agreatly%20improved%20ZCDSR%20by%20leveraging%20rich%20pretrained%20representations%20to%0Afacilitate%20cross-domain%20knowledge%20transfer.%20However%2C%20a%20key%20challenge%20persists%3A%0Adomain%20semantic%20bias%2C%20which%20arises%20from%20variations%20in%20vocabulary%20and%20content%0Afocus%20across%20domains.%20This%20misalignment%20leads%20to%20inconsistencies%20in%20item%0Aembeddings%20and%20hinders%20generalization.%0A%20%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20designed%20to%20enhance%0ALLM-based%20ZCDSR%20by%20improving%20cross-domain%20alignment%20at%20both%20the%20item%20and%0Asequential%20levels.%20At%20the%20item%20level%2C%20we%20introduce%20a%20generalization%20loss%20that%0Apromotes%20inter-domain%20compactness%20by%20aligning%20embeddings%20of%20similar%20items%0Aacross%20domains%20while%20maintaining%20intra-domain%20diversity%20to%20preserve%20unique%20item%0Acharacteristics.%20This%20prevents%20embeddings%20from%20becoming%20overly%20generic%20while%0Aensuring%20effective%20transferability.%20At%20the%20sequential%20level%2C%20we%20develop%20a%0Amethod%20for%20transferring%20user%20behavioral%20patterns%20by%20clustering%20user%20sequences%0Ain%20the%20source%20domain%20and%20applying%20attention-based%20aggregation%20for%20target%20domain%0Ainference.%20This%20dynamic%20adaptation%20of%20user%20embeddings%20allows%20effective%0Azero-shot%20recommendations%20without%20requiring%20target-domain%20interactions.%0A%20%20Comprehensive%20experiments%20across%20multiple%20datasets%20and%20domains%20demonstrate%0Athat%20our%20framework%20significantly%20improves%20sequential%20recommendation%20performance%0Ain%20the%20ZCDSR%20setting.%20By%20mitigating%20domain%20bias%20and%20enhancing%20the%0Atransferability%20of%20sequential%20patterns%2C%20our%20method%20provides%20a%20scalable%20and%0Arobust%20approach%20for%20achieving%20more%20effective%20zero-shot%20recommendations%20across%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19232v1&entry.124074799=Read"},
{"title": "RIGNO: A Graph-based framework for robust and accurate operator learning\n  for PDEs on arbitrary domains", "author": "Sepehr Mousavi and Shizheng Wen and Levi Lingsch and Maximilian Herde and Bogdan Raoni\u0107 and Siddhartha Mishra", "abstract": "  Learning the solution operators of PDEs on arbitrary domains is challenging\ndue to the diversity of possible domain shapes, in addition to the often\nintricate underlying physics. We propose an end-to-end graph neural network\n(GNN) based neural operator to learn PDE solution operators from data on point\nclouds in arbitrary domains. Our multi-scale model maps data between\ninput/output point clouds by passing it through a downsampled regional mesh.\nMany novel elements are also incorporated to ensure resolution invariance and\ntemporal continuity. Our model, termed RIGNO, is tested on a challenging suite\nof benchmarks, composed of various time-dependent and steady PDEs defined on a\ndiverse set of domains. We demonstrate that RIGNO is significantly more\naccurate than neural operator baselines and robustly generalizes to unseen\nspatial resolutions and time instances.\n", "link": "http://arxiv.org/abs/2501.19205v1", "date": "2025-01-31", "relevancy": 2.5017, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5139}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4949}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIGNO%3A%20A%20Graph-based%20framework%20for%20robust%20and%20accurate%20operator%20learning%0A%20%20for%20PDEs%20on%20arbitrary%20domains&body=Title%3A%20RIGNO%3A%20A%20Graph-based%20framework%20for%20robust%20and%20accurate%20operator%20learning%0A%20%20for%20PDEs%20on%20arbitrary%20domains%0AAuthor%3A%20Sepehr%20Mousavi%20and%20Shizheng%20Wen%20and%20Levi%20Lingsch%20and%20Maximilian%20Herde%20and%20Bogdan%20Raoni%C4%87%20and%20Siddhartha%20Mishra%0AAbstract%3A%20%20%20Learning%20the%20solution%20operators%20of%20PDEs%20on%20arbitrary%20domains%20is%20challenging%0Adue%20to%20the%20diversity%20of%20possible%20domain%20shapes%2C%20in%20addition%20to%20the%20often%0Aintricate%20underlying%20physics.%20We%20propose%20an%20end-to-end%20graph%20neural%20network%0A%28GNN%29%20based%20neural%20operator%20to%20learn%20PDE%20solution%20operators%20from%20data%20on%20point%0Aclouds%20in%20arbitrary%20domains.%20Our%20multi-scale%20model%20maps%20data%20between%0Ainput/output%20point%20clouds%20by%20passing%20it%20through%20a%20downsampled%20regional%20mesh.%0AMany%20novel%20elements%20are%20also%20incorporated%20to%20ensure%20resolution%20invariance%20and%0Atemporal%20continuity.%20Our%20model%2C%20termed%20RIGNO%2C%20is%20tested%20on%20a%20challenging%20suite%0Aof%20benchmarks%2C%20composed%20of%20various%20time-dependent%20and%20steady%20PDEs%20defined%20on%20a%0Adiverse%20set%20of%20domains.%20We%20demonstrate%20that%20RIGNO%20is%20significantly%20more%0Aaccurate%20than%20neural%20operator%20baselines%20and%20robustly%20generalizes%20to%20unseen%0Aspatial%20resolutions%20and%20time%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIGNO%253A%2520A%2520Graph-based%2520framework%2520for%2520robust%2520and%2520accurate%2520operator%2520learning%250A%2520%2520for%2520PDEs%2520on%2520arbitrary%2520domains%26entry.906535625%3DSepehr%2520Mousavi%2520and%2520Shizheng%2520Wen%2520and%2520Levi%2520Lingsch%2520and%2520Maximilian%2520Herde%2520and%2520Bogdan%2520Raoni%25C4%2587%2520and%2520Siddhartha%2520Mishra%26entry.1292438233%3D%2520%2520Learning%2520the%2520solution%2520operators%2520of%2520PDEs%2520on%2520arbitrary%2520domains%2520is%2520challenging%250Adue%2520to%2520the%2520diversity%2520of%2520possible%2520domain%2520shapes%252C%2520in%2520addition%2520to%2520the%2520often%250Aintricate%2520underlying%2520physics.%2520We%2520propose%2520an%2520end-to-end%2520graph%2520neural%2520network%250A%2528GNN%2529%2520based%2520neural%2520operator%2520to%2520learn%2520PDE%2520solution%2520operators%2520from%2520data%2520on%2520point%250Aclouds%2520in%2520arbitrary%2520domains.%2520Our%2520multi-scale%2520model%2520maps%2520data%2520between%250Ainput/output%2520point%2520clouds%2520by%2520passing%2520it%2520through%2520a%2520downsampled%2520regional%2520mesh.%250AMany%2520novel%2520elements%2520are%2520also%2520incorporated%2520to%2520ensure%2520resolution%2520invariance%2520and%250Atemporal%2520continuity.%2520Our%2520model%252C%2520termed%2520RIGNO%252C%2520is%2520tested%2520on%2520a%2520challenging%2520suite%250Aof%2520benchmarks%252C%2520composed%2520of%2520various%2520time-dependent%2520and%2520steady%2520PDEs%2520defined%2520on%2520a%250Adiverse%2520set%2520of%2520domains.%2520We%2520demonstrate%2520that%2520RIGNO%2520is%2520significantly%2520more%250Aaccurate%2520than%2520neural%2520operator%2520baselines%2520and%2520robustly%2520generalizes%2520to%2520unseen%250Aspatial%2520resolutions%2520and%2520time%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIGNO%3A%20A%20Graph-based%20framework%20for%20robust%20and%20accurate%20operator%20learning%0A%20%20for%20PDEs%20on%20arbitrary%20domains&entry.906535625=Sepehr%20Mousavi%20and%20Shizheng%20Wen%20and%20Levi%20Lingsch%20and%20Maximilian%20Herde%20and%20Bogdan%20Raoni%C4%87%20and%20Siddhartha%20Mishra&entry.1292438233=%20%20Learning%20the%20solution%20operators%20of%20PDEs%20on%20arbitrary%20domains%20is%20challenging%0Adue%20to%20the%20diversity%20of%20possible%20domain%20shapes%2C%20in%20addition%20to%20the%20often%0Aintricate%20underlying%20physics.%20We%20propose%20an%20end-to-end%20graph%20neural%20network%0A%28GNN%29%20based%20neural%20operator%20to%20learn%20PDE%20solution%20operators%20from%20data%20on%20point%0Aclouds%20in%20arbitrary%20domains.%20Our%20multi-scale%20model%20maps%20data%20between%0Ainput/output%20point%20clouds%20by%20passing%20it%20through%20a%20downsampled%20regional%20mesh.%0AMany%20novel%20elements%20are%20also%20incorporated%20to%20ensure%20resolution%20invariance%20and%0Atemporal%20continuity.%20Our%20model%2C%20termed%20RIGNO%2C%20is%20tested%20on%20a%20challenging%20suite%0Aof%20benchmarks%2C%20composed%20of%20various%20time-dependent%20and%20steady%20PDEs%20defined%20on%20a%0Adiverse%20set%20of%20domains.%20We%20demonstrate%20that%20RIGNO%20is%20significantly%20more%0Aaccurate%20than%20neural%20operator%20baselines%20and%20robustly%20generalizes%20to%20unseen%0Aspatial%20resolutions%20and%20time%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19205v1&entry.124074799=Read"},
{"title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions", "author": "Dominik Wagner and Alexander Churchill and Siddarth Sigtia and Erik Marchi", "abstract": "  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.\n", "link": "http://arxiv.org/abs/2501.19377v1", "date": "2025-01-31", "relevancy": 2.4956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions&body=Title%3A%20SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions%0AAuthor%3A%20Dominik%20Wagner%20and%20Alexander%20Churchill%20and%20Siddarth%20Sigtia%20and%20Erik%20Marchi%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20and%20evaluate%20SELMA%2C%20a%20Speech-Enabled%20Language%20Model%0Afor%20virtual%20Assistant%20interactions%20that%20integrates%20audio%20and%20text%20as%20inputs%20to%0Aa%20Large%20Language%20Model%20%28LLM%29.%20SELMA%20is%20designed%20to%20handle%20three%20primary%20and%20two%0Aauxiliary%20tasks%20related%20to%20interactions%20with%20virtual%20assistants%20simultaneously%0Awithin%20a%20single%20end-to-end%20model.%20We%20employ%20low-rank%20adaptation%20modules%20for%0Aparameter-efficient%20training%20of%20both%20the%20audio%20encoder%20and%20the%20LLM.%0AAdditionally%2C%20we%20implement%20a%20feature%20pooling%20strategy%20enabling%20the%20system%20to%0Arecognize%20global%20patterns%20and%20improve%20accuracy%20on%20tasks%20less%20reliant%20on%0Aindividual%20sequence%20elements.%20Experimental%20results%20on%20Voice%20Trigger%20%28VT%29%0Adetection%2C%20Device-Directed%20Speech%20Detection%20%28DDSD%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%2C%20demonstrate%20that%20our%20approach%20both%20simplifies%20the%20typical%0Ainput%20processing%20pipeline%20of%20virtual%20assistants%20significantly%20and%20also%20improves%0Aperformance%20compared%20to%20dedicated%20models%20for%20each%20individual%20task.%20SELMA%20yields%0Arelative%20Equal-Error%20Rate%20improvements%20of%2064%25%20on%20the%20VT%20detection%20task%2C%20and%2022%25%0Aon%20DDSD%2C%20while%20also%20achieving%20word%20error%20rates%20close%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELMA%253A%2520A%2520Speech-Enabled%2520Language%2520Model%2520for%2520Virtual%2520Assistant%250A%2520%2520Interactions%26entry.906535625%3DDominik%2520Wagner%2520and%2520Alexander%2520Churchill%2520and%2520Siddarth%2520Sigtia%2520and%2520Erik%2520Marchi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520and%2520evaluate%2520SELMA%252C%2520a%2520Speech-Enabled%2520Language%2520Model%250Afor%2520virtual%2520Assistant%2520interactions%2520that%2520integrates%2520audio%2520and%2520text%2520as%2520inputs%2520to%250Aa%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520SELMA%2520is%2520designed%2520to%2520handle%2520three%2520primary%2520and%2520two%250Aauxiliary%2520tasks%2520related%2520to%2520interactions%2520with%2520virtual%2520assistants%2520simultaneously%250Awithin%2520a%2520single%2520end-to-end%2520model.%2520We%2520employ%2520low-rank%2520adaptation%2520modules%2520for%250Aparameter-efficient%2520training%2520of%2520both%2520the%2520audio%2520encoder%2520and%2520the%2520LLM.%250AAdditionally%252C%2520we%2520implement%2520a%2520feature%2520pooling%2520strategy%2520enabling%2520the%2520system%2520to%250Arecognize%2520global%2520patterns%2520and%2520improve%2520accuracy%2520on%2520tasks%2520less%2520reliant%2520on%250Aindividual%2520sequence%2520elements.%2520Experimental%2520results%2520on%2520Voice%2520Trigger%2520%2528VT%2529%250Adetection%252C%2520Device-Directed%2520Speech%2520Detection%2520%2528DDSD%2529%252C%2520and%2520Automatic%2520Speech%250ARecognition%2520%2528ASR%2529%252C%2520demonstrate%2520that%2520our%2520approach%2520both%2520simplifies%2520the%2520typical%250Ainput%2520processing%2520pipeline%2520of%2520virtual%2520assistants%2520significantly%2520and%2520also%2520improves%250Aperformance%2520compared%2520to%2520dedicated%2520models%2520for%2520each%2520individual%2520task.%2520SELMA%2520yields%250Arelative%2520Equal-Error%2520Rate%2520improvements%2520of%252064%2525%2520on%2520the%2520VT%2520detection%2520task%252C%2520and%252022%2525%250Aon%2520DDSD%252C%2520while%2520also%2520achieving%2520word%2520error%2520rates%2520close%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions&entry.906535625=Dominik%20Wagner%20and%20Alexander%20Churchill%20and%20Siddarth%20Sigtia%20and%20Erik%20Marchi&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20and%20evaluate%20SELMA%2C%20a%20Speech-Enabled%20Language%20Model%0Afor%20virtual%20Assistant%20interactions%20that%20integrates%20audio%20and%20text%20as%20inputs%20to%0Aa%20Large%20Language%20Model%20%28LLM%29.%20SELMA%20is%20designed%20to%20handle%20three%20primary%20and%20two%0Aauxiliary%20tasks%20related%20to%20interactions%20with%20virtual%20assistants%20simultaneously%0Awithin%20a%20single%20end-to-end%20model.%20We%20employ%20low-rank%20adaptation%20modules%20for%0Aparameter-efficient%20training%20of%20both%20the%20audio%20encoder%20and%20the%20LLM.%0AAdditionally%2C%20we%20implement%20a%20feature%20pooling%20strategy%20enabling%20the%20system%20to%0Arecognize%20global%20patterns%20and%20improve%20accuracy%20on%20tasks%20less%20reliant%20on%0Aindividual%20sequence%20elements.%20Experimental%20results%20on%20Voice%20Trigger%20%28VT%29%0Adetection%2C%20Device-Directed%20Speech%20Detection%20%28DDSD%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%2C%20demonstrate%20that%20our%20approach%20both%20simplifies%20the%20typical%0Ainput%20processing%20pipeline%20of%20virtual%20assistants%20significantly%20and%20also%20improves%0Aperformance%20compared%20to%20dedicated%20models%20for%20each%20individual%20task.%20SELMA%20yields%0Arelative%20Equal-Error%20Rate%20improvements%20of%2064%25%20on%20the%20VT%20detection%20task%2C%20and%2022%25%0Aon%20DDSD%2C%20while%20also%20achieving%20word%20error%20rates%20close%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19377v1&entry.124074799=Read"},
{"title": "Equivariant Neural Tangent Kernels", "author": "Philipp Misof and Pan Kessel and Jan E. Gerken", "abstract": "  Little is known about the training dynamics of equivariant neural networks,\nin particular how it compares to data augmented training of their\nnon-equivariant counterparts. Recently, neural tangent kernels (NTKs) have\nemerged as a powerful tool to analytically study the training dynamics of wide\nneural networks. In this work, we take an important step towards a theoretical\nunderstanding of training dynamics of equivariant models by deriving neural\ntangent kernels for a broad class of equivariant architectures based on group\nconvolutions. As a demonstration of the capabilities of our framework, we show\nan interesting relationship between data augmentation and group convolutional\nnetworks. Specifically, we prove that they share the same expected prediction\nat all training times and even off-manifold. In this sense, they have the same\ntraining dynamics. We demonstrate in numerical experiments that this still\nholds approximately for finite-width ensembles. By implementing equivariant\nNTKs for roto-translations in the plane ($G=C_{n}\\ltimes\\mathbb{R}^{2}$) and 3d\nrotations ($G=\\mathrm{SO}(3)$), we show that equivariant NTKs outperform their\nnon-equivariant counterparts as kernel predictors for histological image\nclassification and quantum mechanical property prediction.\n", "link": "http://arxiv.org/abs/2406.06504v2", "date": "2025-01-31", "relevancy": 2.4918, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5427}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Neural%20Tangent%20Kernels&body=Title%3A%20Equivariant%20Neural%20Tangent%20Kernels%0AAuthor%3A%20Philipp%20Misof%20and%20Pan%20Kessel%20and%20Jan%20E.%20Gerken%0AAbstract%3A%20%20%20Little%20is%20known%20about%20the%20training%20dynamics%20of%20equivariant%20neural%20networks%2C%0Ain%20particular%20how%20it%20compares%20to%20data%20augmented%20training%20of%20their%0Anon-equivariant%20counterparts.%20Recently%2C%20neural%20tangent%20kernels%20%28NTKs%29%20have%0Aemerged%20as%20a%20powerful%20tool%20to%20analytically%20study%20the%20training%20dynamics%20of%20wide%0Aneural%20networks.%20In%20this%20work%2C%20we%20take%20an%20important%20step%20towards%20a%20theoretical%0Aunderstanding%20of%20training%20dynamics%20of%20equivariant%20models%20by%20deriving%20neural%0Atangent%20kernels%20for%20a%20broad%20class%20of%20equivariant%20architectures%20based%20on%20group%0Aconvolutions.%20As%20a%20demonstration%20of%20the%20capabilities%20of%20our%20framework%2C%20we%20show%0Aan%20interesting%20relationship%20between%20data%20augmentation%20and%20group%20convolutional%0Anetworks.%20Specifically%2C%20we%20prove%20that%20they%20share%20the%20same%20expected%20prediction%0Aat%20all%20training%20times%20and%20even%20off-manifold.%20In%20this%20sense%2C%20they%20have%20the%20same%0Atraining%20dynamics.%20We%20demonstrate%20in%20numerical%20experiments%20that%20this%20still%0Aholds%20approximately%20for%20finite-width%20ensembles.%20By%20implementing%20equivariant%0ANTKs%20for%20roto-translations%20in%20the%20plane%20%28%24G%3DC_%7Bn%7D%5Cltimes%5Cmathbb%7BR%7D%5E%7B2%7D%24%29%20and%203d%0Arotations%20%28%24G%3D%5Cmathrm%7BSO%7D%283%29%24%29%2C%20we%20show%20that%20equivariant%20NTKs%20outperform%20their%0Anon-equivariant%20counterparts%20as%20kernel%20predictors%20for%20histological%20image%0Aclassification%20and%20quantum%20mechanical%20property%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Neural%2520Tangent%2520Kernels%26entry.906535625%3DPhilipp%2520Misof%2520and%2520Pan%2520Kessel%2520and%2520Jan%2520E.%2520Gerken%26entry.1292438233%3D%2520%2520Little%2520is%2520known%2520about%2520the%2520training%2520dynamics%2520of%2520equivariant%2520neural%2520networks%252C%250Ain%2520particular%2520how%2520it%2520compares%2520to%2520data%2520augmented%2520training%2520of%2520their%250Anon-equivariant%2520counterparts.%2520Recently%252C%2520neural%2520tangent%2520kernels%2520%2528NTKs%2529%2520have%250Aemerged%2520as%2520a%2520powerful%2520tool%2520to%2520analytically%2520study%2520the%2520training%2520dynamics%2520of%2520wide%250Aneural%2520networks.%2520In%2520this%2520work%252C%2520we%2520take%2520an%2520important%2520step%2520towards%2520a%2520theoretical%250Aunderstanding%2520of%2520training%2520dynamics%2520of%2520equivariant%2520models%2520by%2520deriving%2520neural%250Atangent%2520kernels%2520for%2520a%2520broad%2520class%2520of%2520equivariant%2520architectures%2520based%2520on%2520group%250Aconvolutions.%2520As%2520a%2520demonstration%2520of%2520the%2520capabilities%2520of%2520our%2520framework%252C%2520we%2520show%250Aan%2520interesting%2520relationship%2520between%2520data%2520augmentation%2520and%2520group%2520convolutional%250Anetworks.%2520Specifically%252C%2520we%2520prove%2520that%2520they%2520share%2520the%2520same%2520expected%2520prediction%250Aat%2520all%2520training%2520times%2520and%2520even%2520off-manifold.%2520In%2520this%2520sense%252C%2520they%2520have%2520the%2520same%250Atraining%2520dynamics.%2520We%2520demonstrate%2520in%2520numerical%2520experiments%2520that%2520this%2520still%250Aholds%2520approximately%2520for%2520finite-width%2520ensembles.%2520By%2520implementing%2520equivariant%250ANTKs%2520for%2520roto-translations%2520in%2520the%2520plane%2520%2528%2524G%253DC_%257Bn%257D%255Cltimes%255Cmathbb%257BR%257D%255E%257B2%257D%2524%2529%2520and%25203d%250Arotations%2520%2528%2524G%253D%255Cmathrm%257BSO%257D%25283%2529%2524%2529%252C%2520we%2520show%2520that%2520equivariant%2520NTKs%2520outperform%2520their%250Anon-equivariant%2520counterparts%2520as%2520kernel%2520predictors%2520for%2520histological%2520image%250Aclassification%2520and%2520quantum%2520mechanical%2520property%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Neural%20Tangent%20Kernels&entry.906535625=Philipp%20Misof%20and%20Pan%20Kessel%20and%20Jan%20E.%20Gerken&entry.1292438233=%20%20Little%20is%20known%20about%20the%20training%20dynamics%20of%20equivariant%20neural%20networks%2C%0Ain%20particular%20how%20it%20compares%20to%20data%20augmented%20training%20of%20their%0Anon-equivariant%20counterparts.%20Recently%2C%20neural%20tangent%20kernels%20%28NTKs%29%20have%0Aemerged%20as%20a%20powerful%20tool%20to%20analytically%20study%20the%20training%20dynamics%20of%20wide%0Aneural%20networks.%20In%20this%20work%2C%20we%20take%20an%20important%20step%20towards%20a%20theoretical%0Aunderstanding%20of%20training%20dynamics%20of%20equivariant%20models%20by%20deriving%20neural%0Atangent%20kernels%20for%20a%20broad%20class%20of%20equivariant%20architectures%20based%20on%20group%0Aconvolutions.%20As%20a%20demonstration%20of%20the%20capabilities%20of%20our%20framework%2C%20we%20show%0Aan%20interesting%20relationship%20between%20data%20augmentation%20and%20group%20convolutional%0Anetworks.%20Specifically%2C%20we%20prove%20that%20they%20share%20the%20same%20expected%20prediction%0Aat%20all%20training%20times%20and%20even%20off-manifold.%20In%20this%20sense%2C%20they%20have%20the%20same%0Atraining%20dynamics.%20We%20demonstrate%20in%20numerical%20experiments%20that%20this%20still%0Aholds%20approximately%20for%20finite-width%20ensembles.%20By%20implementing%20equivariant%0ANTKs%20for%20roto-translations%20in%20the%20plane%20%28%24G%3DC_%7Bn%7D%5Cltimes%5Cmathbb%7BR%7D%5E%7B2%7D%24%29%20and%203d%0Arotations%20%28%24G%3D%5Cmathrm%7BSO%7D%283%29%24%29%2C%20we%20show%20that%20equivariant%20NTKs%20outperform%20their%0Anon-equivariant%20counterparts%20as%20kernel%20predictors%20for%20histological%20image%0Aclassification%20and%20quantum%20mechanical%20property%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06504v2&entry.124074799=Read"},
{"title": "Learning Sheaf Laplacian Optimizing Restriction Maps", "author": "Leonardo Di Nino and Sergio Barbarossa and Paolo Di Lorenzo", "abstract": "  The aim of this paper is to propose a novel framework to infer the sheaf\nLaplacian, including the topology of a graph and the restriction maps, from a\nset of data observed over the nodes of a graph. The proposed method is based on\nsheaf theory, which represents an important generalization of graph signal\nprocessing. The learning problem aims to find the sheaf Laplacian that\nminimizes the total variation of the observed data, where the variation over\neach edge is also locally minimized by optimizing the associated restriction\nmaps. Compared to alternative methods based on semidefinite programming, our\nsolution is significantly more numerically efficient, as all its fundamental\nsteps are resolved in closed form. The method is numerically tested on data\nconsisting of vectors defined over subspaces of varying dimensions at each\nnode. We demonstrate how the resulting graph is influenced by two key factors:\nthe cross-correlation and the dimensionality difference of the data residing on\nthe graph's nodes.\n", "link": "http://arxiv.org/abs/2501.19207v1", "date": "2025-01-31", "relevancy": 2.4874, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5588}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4687}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Sheaf%20Laplacian%20Optimizing%20Restriction%20Maps&body=Title%3A%20Learning%20Sheaf%20Laplacian%20Optimizing%20Restriction%20Maps%0AAuthor%3A%20Leonardo%20Di%20Nino%20and%20Sergio%20Barbarossa%20and%20Paolo%20Di%20Lorenzo%0AAbstract%3A%20%20%20The%20aim%20of%20this%20paper%20is%20to%20propose%20a%20novel%20framework%20to%20infer%20the%20sheaf%0ALaplacian%2C%20including%20the%20topology%20of%20a%20graph%20and%20the%20restriction%20maps%2C%20from%20a%0Aset%20of%20data%20observed%20over%20the%20nodes%20of%20a%20graph.%20The%20proposed%20method%20is%20based%20on%0Asheaf%20theory%2C%20which%20represents%20an%20important%20generalization%20of%20graph%20signal%0Aprocessing.%20The%20learning%20problem%20aims%20to%20find%20the%20sheaf%20Laplacian%20that%0Aminimizes%20the%20total%20variation%20of%20the%20observed%20data%2C%20where%20the%20variation%20over%0Aeach%20edge%20is%20also%20locally%20minimized%20by%20optimizing%20the%20associated%20restriction%0Amaps.%20Compared%20to%20alternative%20methods%20based%20on%20semidefinite%20programming%2C%20our%0Asolution%20is%20significantly%20more%20numerically%20efficient%2C%20as%20all%20its%20fundamental%0Asteps%20are%20resolved%20in%20closed%20form.%20The%20method%20is%20numerically%20tested%20on%20data%0Aconsisting%20of%20vectors%20defined%20over%20subspaces%20of%20varying%20dimensions%20at%20each%0Anode.%20We%20demonstrate%20how%20the%20resulting%20graph%20is%20influenced%20by%20two%20key%20factors%3A%0Athe%20cross-correlation%20and%20the%20dimensionality%20difference%20of%20the%20data%20residing%20on%0Athe%20graph%27s%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Sheaf%2520Laplacian%2520Optimizing%2520Restriction%2520Maps%26entry.906535625%3DLeonardo%2520Di%2520Nino%2520and%2520Sergio%2520Barbarossa%2520and%2520Paolo%2520Di%2520Lorenzo%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520this%2520paper%2520is%2520to%2520propose%2520a%2520novel%2520framework%2520to%2520infer%2520the%2520sheaf%250ALaplacian%252C%2520including%2520the%2520topology%2520of%2520a%2520graph%2520and%2520the%2520restriction%2520maps%252C%2520from%2520a%250Aset%2520of%2520data%2520observed%2520over%2520the%2520nodes%2520of%2520a%2520graph.%2520The%2520proposed%2520method%2520is%2520based%2520on%250Asheaf%2520theory%252C%2520which%2520represents%2520an%2520important%2520generalization%2520of%2520graph%2520signal%250Aprocessing.%2520The%2520learning%2520problem%2520aims%2520to%2520find%2520the%2520sheaf%2520Laplacian%2520that%250Aminimizes%2520the%2520total%2520variation%2520of%2520the%2520observed%2520data%252C%2520where%2520the%2520variation%2520over%250Aeach%2520edge%2520is%2520also%2520locally%2520minimized%2520by%2520optimizing%2520the%2520associated%2520restriction%250Amaps.%2520Compared%2520to%2520alternative%2520methods%2520based%2520on%2520semidefinite%2520programming%252C%2520our%250Asolution%2520is%2520significantly%2520more%2520numerically%2520efficient%252C%2520as%2520all%2520its%2520fundamental%250Asteps%2520are%2520resolved%2520in%2520closed%2520form.%2520The%2520method%2520is%2520numerically%2520tested%2520on%2520data%250Aconsisting%2520of%2520vectors%2520defined%2520over%2520subspaces%2520of%2520varying%2520dimensions%2520at%2520each%250Anode.%2520We%2520demonstrate%2520how%2520the%2520resulting%2520graph%2520is%2520influenced%2520by%2520two%2520key%2520factors%253A%250Athe%2520cross-correlation%2520and%2520the%2520dimensionality%2520difference%2520of%2520the%2520data%2520residing%2520on%250Athe%2520graph%2527s%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sheaf%20Laplacian%20Optimizing%20Restriction%20Maps&entry.906535625=Leonardo%20Di%20Nino%20and%20Sergio%20Barbarossa%20and%20Paolo%20Di%20Lorenzo&entry.1292438233=%20%20The%20aim%20of%20this%20paper%20is%20to%20propose%20a%20novel%20framework%20to%20infer%20the%20sheaf%0ALaplacian%2C%20including%20the%20topology%20of%20a%20graph%20and%20the%20restriction%20maps%2C%20from%20a%0Aset%20of%20data%20observed%20over%20the%20nodes%20of%20a%20graph.%20The%20proposed%20method%20is%20based%20on%0Asheaf%20theory%2C%20which%20represents%20an%20important%20generalization%20of%20graph%20signal%0Aprocessing.%20The%20learning%20problem%20aims%20to%20find%20the%20sheaf%20Laplacian%20that%0Aminimizes%20the%20total%20variation%20of%20the%20observed%20data%2C%20where%20the%20variation%20over%0Aeach%20edge%20is%20also%20locally%20minimized%20by%20optimizing%20the%20associated%20restriction%0Amaps.%20Compared%20to%20alternative%20methods%20based%20on%20semidefinite%20programming%2C%20our%0Asolution%20is%20significantly%20more%20numerically%20efficient%2C%20as%20all%20its%20fundamental%0Asteps%20are%20resolved%20in%20closed%20form.%20The%20method%20is%20numerically%20tested%20on%20data%0Aconsisting%20of%20vectors%20defined%20over%20subspaces%20of%20varying%20dimensions%20at%20each%0Anode.%20We%20demonstrate%20how%20the%20resulting%20graph%20is%20influenced%20by%20two%20key%20factors%3A%0Athe%20cross-correlation%20and%20the%20dimensionality%20difference%20of%20the%20data%20residing%20on%0Athe%20graph%27s%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19207v1&entry.124074799=Read"},
{"title": "Towards Generalisable Time Series Understanding Across Domains", "author": "\u00d6zg\u00fcn Turgut and Philip M\u00fcller and Martin J. Menten and Daniel Rueckert", "abstract": "  Recent breakthroughs in natural language processing and computer vision,\ndriven by efficient pre-training on large datasets, have enabled foundation\nmodels to excel on a wide range of tasks. However, this potential has not yet\nbeen fully realised in time series analysis, as existing methods fail to\naddress the heterogeneity in large time series corpora. Prevalent in domains\nranging from medicine to finance, time series vary substantially in\ncharacteristics such as variate count, inter-variate relationships, temporal\npatterns, and sampling frequency. To address this, we introduce a novel\npre-training paradigm specifically designed to handle time series\nheterogeneity. We propose a tokeniser with learnable domain signatures, a dual\nmasking strategy, and a normalised cross-correlation loss, enabling our open\nmodel for general time series analysis (OTiS) to efficiently learn from large\ntime series corpora. Extensive benchmarking on diverse tasks, such as\nclassification, regression, and forecasting, demonstrates that OTiS outperforms\nstate-of-the-art baselines. Our code and pre-trained weights are available at\nhttps://github.com/oetu/otis.\n", "link": "http://arxiv.org/abs/2410.07299v2", "date": "2025-01-31", "relevancy": 2.4795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalisable%20Time%20Series%20Understanding%20Across%20Domains&body=Title%3A%20Towards%20Generalisable%20Time%20Series%20Understanding%20Across%20Domains%0AAuthor%3A%20%C3%96zg%C3%BCn%20Turgut%20and%20Philip%20M%C3%BCller%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20natural%20language%20processing%20and%20computer%20vision%2C%0Adriven%20by%20efficient%20pre-training%20on%20large%20datasets%2C%20have%20enabled%20foundation%0Amodels%20to%20excel%20on%20a%20wide%20range%20of%20tasks.%20However%2C%20this%20potential%20has%20not%20yet%0Abeen%20fully%20realised%20in%20time%20series%20analysis%2C%20as%20existing%20methods%20fail%20to%0Aaddress%20the%20heterogeneity%20in%20large%20time%20series%20corpora.%20Prevalent%20in%20domains%0Aranging%20from%20medicine%20to%20finance%2C%20time%20series%20vary%20substantially%20in%0Acharacteristics%20such%20as%20variate%20count%2C%20inter-variate%20relationships%2C%20temporal%0Apatterns%2C%20and%20sampling%20frequency.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Apre-training%20paradigm%20specifically%20designed%20to%20handle%20time%20series%0Aheterogeneity.%20We%20propose%20a%20tokeniser%20with%20learnable%20domain%20signatures%2C%20a%20dual%0Amasking%20strategy%2C%20and%20a%20normalised%20cross-correlation%20loss%2C%20enabling%20our%20open%0Amodel%20for%20general%20time%20series%20analysis%20%28OTiS%29%20to%20efficiently%20learn%20from%20large%0Atime%20series%20corpora.%20Extensive%20benchmarking%20on%20diverse%20tasks%2C%20such%20as%0Aclassification%2C%20regression%2C%20and%20forecasting%2C%20demonstrates%20that%20OTiS%20outperforms%0Astate-of-the-art%20baselines.%20Our%20code%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/oetu/otis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalisable%2520Time%2520Series%2520Understanding%2520Across%2520Domains%26entry.906535625%3D%25C3%2596zg%25C3%25BCn%2520Turgut%2520and%2520Philip%2520M%25C3%25BCller%2520and%2520Martin%2520J.%2520Menten%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%250Adriven%2520by%2520efficient%2520pre-training%2520on%2520large%2520datasets%252C%2520have%2520enabled%2520foundation%250Amodels%2520to%2520excel%2520on%2520a%2520wide%2520range%2520of%2520tasks.%2520However%252C%2520this%2520potential%2520has%2520not%2520yet%250Abeen%2520fully%2520realised%2520in%2520time%2520series%2520analysis%252C%2520as%2520existing%2520methods%2520fail%2520to%250Aaddress%2520the%2520heterogeneity%2520in%2520large%2520time%2520series%2520corpora.%2520Prevalent%2520in%2520domains%250Aranging%2520from%2520medicine%2520to%2520finance%252C%2520time%2520series%2520vary%2520substantially%2520in%250Acharacteristics%2520such%2520as%2520variate%2520count%252C%2520inter-variate%2520relationships%252C%2520temporal%250Apatterns%252C%2520and%2520sampling%2520frequency.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%250Apre-training%2520paradigm%2520specifically%2520designed%2520to%2520handle%2520time%2520series%250Aheterogeneity.%2520We%2520propose%2520a%2520tokeniser%2520with%2520learnable%2520domain%2520signatures%252C%2520a%2520dual%250Amasking%2520strategy%252C%2520and%2520a%2520normalised%2520cross-correlation%2520loss%252C%2520enabling%2520our%2520open%250Amodel%2520for%2520general%2520time%2520series%2520analysis%2520%2528OTiS%2529%2520to%2520efficiently%2520learn%2520from%2520large%250Atime%2520series%2520corpora.%2520Extensive%2520benchmarking%2520on%2520diverse%2520tasks%252C%2520such%2520as%250Aclassification%252C%2520regression%252C%2520and%2520forecasting%252C%2520demonstrates%2520that%2520OTiS%2520outperforms%250Astate-of-the-art%2520baselines.%2520Our%2520code%2520and%2520pre-trained%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/oetu/otis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalisable%20Time%20Series%20Understanding%20Across%20Domains&entry.906535625=%C3%96zg%C3%BCn%20Turgut%20and%20Philip%20M%C3%BCller%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Recent%20breakthroughs%20in%20natural%20language%20processing%20and%20computer%20vision%2C%0Adriven%20by%20efficient%20pre-training%20on%20large%20datasets%2C%20have%20enabled%20foundation%0Amodels%20to%20excel%20on%20a%20wide%20range%20of%20tasks.%20However%2C%20this%20potential%20has%20not%20yet%0Abeen%20fully%20realised%20in%20time%20series%20analysis%2C%20as%20existing%20methods%20fail%20to%0Aaddress%20the%20heterogeneity%20in%20large%20time%20series%20corpora.%20Prevalent%20in%20domains%0Aranging%20from%20medicine%20to%20finance%2C%20time%20series%20vary%20substantially%20in%0Acharacteristics%20such%20as%20variate%20count%2C%20inter-variate%20relationships%2C%20temporal%0Apatterns%2C%20and%20sampling%20frequency.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Apre-training%20paradigm%20specifically%20designed%20to%20handle%20time%20series%0Aheterogeneity.%20We%20propose%20a%20tokeniser%20with%20learnable%20domain%20signatures%2C%20a%20dual%0Amasking%20strategy%2C%20and%20a%20normalised%20cross-correlation%20loss%2C%20enabling%20our%20open%0Amodel%20for%20general%20time%20series%20analysis%20%28OTiS%29%20to%20efficiently%20learn%20from%20large%0Atime%20series%20corpora.%20Extensive%20benchmarking%20on%20diverse%20tasks%2C%20such%20as%0Aclassification%2C%20regression%2C%20and%20forecasting%2C%20demonstrates%20that%20OTiS%20outperforms%0Astate-of-the-art%20baselines.%20Our%20code%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/oetu/otis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07299v2&entry.124074799=Read"},
{"title": "Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector\n  Quantization-based Generative Models", "author": "Jiwan Seo and Joonhyuk Kang", "abstract": "  Learning discrete representations with vector quantization (VQ) has emerged\nas a powerful approach in various generative models. However, most VQ-based\nmodels rely on a single, fixed-rate codebook, requiring extensive retraining\nfor new bitrates or efficiency requirements. We introduce Rate-Adaptive\nQuantization (RAQ), a multi-rate codebook adaptation framework for VQ-based\ngenerative models. RAQ applies a data-driven approach to generate variable-rate\ncodebooks from a single baseline VQ model, enabling flexible tradeoffs between\ncompression and reconstruction fidelity. Additionally, we provide a simple\nclustering-based procedure for pre-trained VQ models, offering an alternative\nwhen retraining is infeasible. Our experiments show that RAQ performs\neffectively across multiple rates, often outperforming conventional fixed-rate\nVQ baselines. By enabling a single system to seamlessly handle diverse bitrate\nrequirements, RAQ extends the adaptability of VQ-based generative models and\nbroadens their applicability to data compression, reconstruction, and\ngeneration tasks.\n", "link": "http://arxiv.org/abs/2405.14222v2", "date": "2025-01-31", "relevancy": 2.4569, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4908}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rate-Adaptive%20Quantization%3A%20A%20Multi-Rate%20Codebook%20Adaptation%20for%20Vector%0A%20%20Quantization-based%20Generative%20Models&body=Title%3A%20Rate-Adaptive%20Quantization%3A%20A%20Multi-Rate%20Codebook%20Adaptation%20for%20Vector%0A%20%20Quantization-based%20Generative%20Models%0AAuthor%3A%20Jiwan%20Seo%20and%20Joonhyuk%20Kang%0AAbstract%3A%20%20%20Learning%20discrete%20representations%20with%20vector%20quantization%20%28VQ%29%20has%20emerged%0Aas%20a%20powerful%20approach%20in%20various%20generative%20models.%20However%2C%20most%20VQ-based%0Amodels%20rely%20on%20a%20single%2C%20fixed-rate%20codebook%2C%20requiring%20extensive%20retraining%0Afor%20new%20bitrates%20or%20efficiency%20requirements.%20We%20introduce%20Rate-Adaptive%0AQuantization%20%28RAQ%29%2C%20a%20multi-rate%20codebook%20adaptation%20framework%20for%20VQ-based%0Agenerative%20models.%20RAQ%20applies%20a%20data-driven%20approach%20to%20generate%20variable-rate%0Acodebooks%20from%20a%20single%20baseline%20VQ%20model%2C%20enabling%20flexible%20tradeoffs%20between%0Acompression%20and%20reconstruction%20fidelity.%20Additionally%2C%20we%20provide%20a%20simple%0Aclustering-based%20procedure%20for%20pre-trained%20VQ%20models%2C%20offering%20an%20alternative%0Awhen%20retraining%20is%20infeasible.%20Our%20experiments%20show%20that%20RAQ%20performs%0Aeffectively%20across%20multiple%20rates%2C%20often%20outperforming%20conventional%20fixed-rate%0AVQ%20baselines.%20By%20enabling%20a%20single%20system%20to%20seamlessly%20handle%20diverse%20bitrate%0Arequirements%2C%20RAQ%20extends%20the%20adaptability%20of%20VQ-based%20generative%20models%20and%0Abroadens%20their%20applicability%20to%20data%20compression%2C%20reconstruction%2C%20and%0Ageneration%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRate-Adaptive%2520Quantization%253A%2520A%2520Multi-Rate%2520Codebook%2520Adaptation%2520for%2520Vector%250A%2520%2520Quantization-based%2520Generative%2520Models%26entry.906535625%3DJiwan%2520Seo%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3D%2520%2520Learning%2520discrete%2520representations%2520with%2520vector%2520quantization%2520%2528VQ%2529%2520has%2520emerged%250Aas%2520a%2520powerful%2520approach%2520in%2520various%2520generative%2520models.%2520However%252C%2520most%2520VQ-based%250Amodels%2520rely%2520on%2520a%2520single%252C%2520fixed-rate%2520codebook%252C%2520requiring%2520extensive%2520retraining%250Afor%2520new%2520bitrates%2520or%2520efficiency%2520requirements.%2520We%2520introduce%2520Rate-Adaptive%250AQuantization%2520%2528RAQ%2529%252C%2520a%2520multi-rate%2520codebook%2520adaptation%2520framework%2520for%2520VQ-based%250Agenerative%2520models.%2520RAQ%2520applies%2520a%2520data-driven%2520approach%2520to%2520generate%2520variable-rate%250Acodebooks%2520from%2520a%2520single%2520baseline%2520VQ%2520model%252C%2520enabling%2520flexible%2520tradeoffs%2520between%250Acompression%2520and%2520reconstruction%2520fidelity.%2520Additionally%252C%2520we%2520provide%2520a%2520simple%250Aclustering-based%2520procedure%2520for%2520pre-trained%2520VQ%2520models%252C%2520offering%2520an%2520alternative%250Awhen%2520retraining%2520is%2520infeasible.%2520Our%2520experiments%2520show%2520that%2520RAQ%2520performs%250Aeffectively%2520across%2520multiple%2520rates%252C%2520often%2520outperforming%2520conventional%2520fixed-rate%250AVQ%2520baselines.%2520By%2520enabling%2520a%2520single%2520system%2520to%2520seamlessly%2520handle%2520diverse%2520bitrate%250Arequirements%252C%2520RAQ%2520extends%2520the%2520adaptability%2520of%2520VQ-based%2520generative%2520models%2520and%250Abroadens%2520their%2520applicability%2520to%2520data%2520compression%252C%2520reconstruction%252C%2520and%250Ageneration%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rate-Adaptive%20Quantization%3A%20A%20Multi-Rate%20Codebook%20Adaptation%20for%20Vector%0A%20%20Quantization-based%20Generative%20Models&entry.906535625=Jiwan%20Seo%20and%20Joonhyuk%20Kang&entry.1292438233=%20%20Learning%20discrete%20representations%20with%20vector%20quantization%20%28VQ%29%20has%20emerged%0Aas%20a%20powerful%20approach%20in%20various%20generative%20models.%20However%2C%20most%20VQ-based%0Amodels%20rely%20on%20a%20single%2C%20fixed-rate%20codebook%2C%20requiring%20extensive%20retraining%0Afor%20new%20bitrates%20or%20efficiency%20requirements.%20We%20introduce%20Rate-Adaptive%0AQuantization%20%28RAQ%29%2C%20a%20multi-rate%20codebook%20adaptation%20framework%20for%20VQ-based%0Agenerative%20models.%20RAQ%20applies%20a%20data-driven%20approach%20to%20generate%20variable-rate%0Acodebooks%20from%20a%20single%20baseline%20VQ%20model%2C%20enabling%20flexible%20tradeoffs%20between%0Acompression%20and%20reconstruction%20fidelity.%20Additionally%2C%20we%20provide%20a%20simple%0Aclustering-based%20procedure%20for%20pre-trained%20VQ%20models%2C%20offering%20an%20alternative%0Awhen%20retraining%20is%20infeasible.%20Our%20experiments%20show%20that%20RAQ%20performs%0Aeffectively%20across%20multiple%20rates%2C%20often%20outperforming%20conventional%20fixed-rate%0AVQ%20baselines.%20By%20enabling%20a%20single%20system%20to%20seamlessly%20handle%20diverse%20bitrate%0Arequirements%2C%20RAQ%20extends%20the%20adaptability%20of%20VQ-based%20generative%20models%20and%0Abroadens%20their%20applicability%20to%20data%20compression%2C%20reconstruction%2C%20and%0Ageneration%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14222v2&entry.124074799=Read"},
{"title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation", "author": "Zixi Wang and Yubo Huang and Wenwei Luo and Tonglan Xie and Mengmeng Jing and Lin Zuo", "abstract": "  Domain shifts are critical issues that harm the performance of machine\nlearning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers\nwhen the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA)\nalleviates this problem in a mild way by gradually adapting from the source to\nthe target domain using multiple intermediate domains. In this paper, we\npropose Sliding Window Adversarial Training (SWAT) for Gradual Domain\nAdaptation. SWAT uses the construction of adversarial streams to connect the\nfeature spaces of the source and target domains. In order to gradually narrow\nthe small gap between adjacent intermediate domains, a sliding window paradigm\nis designed that moves along the adversarial stream. When the window moves to\nthe end of the stream, i.e., the target domain, the domain shift is drastically\nreduced. Extensive experiments are conducted on public GDA benchmarks, and the\nresults demonstrate that the proposed SWAT significantly outperforms the\nstate-of-the-art approaches. The implementation is available at:\nhttps://anonymous.4open.science/r/SWAT-8677.\n", "link": "http://arxiv.org/abs/2501.19155v1", "date": "2025-01-31", "relevancy": 2.4523, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5006}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4875}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWAT%3A%20Sliding%20Window%20Adversarial%20Training%20for%20Gradual%20Domain%20Adaptation&body=Title%3A%20SWAT%3A%20Sliding%20Window%20Adversarial%20Training%20for%20Gradual%20Domain%20Adaptation%0AAuthor%3A%20Zixi%20Wang%20and%20Yubo%20Huang%20and%20Wenwei%20Luo%20and%20Tonglan%20Xie%20and%20Mengmeng%20Jing%20and%20Lin%20Zuo%0AAbstract%3A%20%20%20Domain%20shifts%20are%20critical%20issues%20that%20harm%20the%20performance%20of%20machine%0Alearning.%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20mitigates%20this%20issue%20but%20suffers%0Awhen%20the%20domain%20shifts%20are%20steep%20and%20drastic.%20Gradual%20Domain%20Adaptation%20%28GDA%29%0Aalleviates%20this%20problem%20in%20a%20mild%20way%20by%20gradually%20adapting%20from%20the%20source%20to%0Athe%20target%20domain%20using%20multiple%20intermediate%20domains.%20In%20this%20paper%2C%20we%0Apropose%20Sliding%20Window%20Adversarial%20Training%20%28SWAT%29%20for%20Gradual%20Domain%0AAdaptation.%20SWAT%20uses%20the%20construction%20of%20adversarial%20streams%20to%20connect%20the%0Afeature%20spaces%20of%20the%20source%20and%20target%20domains.%20In%20order%20to%20gradually%20narrow%0Athe%20small%20gap%20between%20adjacent%20intermediate%20domains%2C%20a%20sliding%20window%20paradigm%0Ais%20designed%20that%20moves%20along%20the%20adversarial%20stream.%20When%20the%20window%20moves%20to%0Athe%20end%20of%20the%20stream%2C%20i.e.%2C%20the%20target%20domain%2C%20the%20domain%20shift%20is%20drastically%0Areduced.%20Extensive%20experiments%20are%20conducted%20on%20public%20GDA%20benchmarks%2C%20and%20the%0Aresults%20demonstrate%20that%20the%20proposed%20SWAT%20significantly%20outperforms%20the%0Astate-of-the-art%20approaches.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/SWAT-8677.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWAT%253A%2520Sliding%2520Window%2520Adversarial%2520Training%2520for%2520Gradual%2520Domain%2520Adaptation%26entry.906535625%3DZixi%2520Wang%2520and%2520Yubo%2520Huang%2520and%2520Wenwei%2520Luo%2520and%2520Tonglan%2520Xie%2520and%2520Mengmeng%2520Jing%2520and%2520Lin%2520Zuo%26entry.1292438233%3D%2520%2520Domain%2520shifts%2520are%2520critical%2520issues%2520that%2520harm%2520the%2520performance%2520of%2520machine%250Alearning.%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520mitigates%2520this%2520issue%2520but%2520suffers%250Awhen%2520the%2520domain%2520shifts%2520are%2520steep%2520and%2520drastic.%2520Gradual%2520Domain%2520Adaptation%2520%2528GDA%2529%250Aalleviates%2520this%2520problem%2520in%2520a%2520mild%2520way%2520by%2520gradually%2520adapting%2520from%2520the%2520source%2520to%250Athe%2520target%2520domain%2520using%2520multiple%2520intermediate%2520domains.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Sliding%2520Window%2520Adversarial%2520Training%2520%2528SWAT%2529%2520for%2520Gradual%2520Domain%250AAdaptation.%2520SWAT%2520uses%2520the%2520construction%2520of%2520adversarial%2520streams%2520to%2520connect%2520the%250Afeature%2520spaces%2520of%2520the%2520source%2520and%2520target%2520domains.%2520In%2520order%2520to%2520gradually%2520narrow%250Athe%2520small%2520gap%2520between%2520adjacent%2520intermediate%2520domains%252C%2520a%2520sliding%2520window%2520paradigm%250Ais%2520designed%2520that%2520moves%2520along%2520the%2520adversarial%2520stream.%2520When%2520the%2520window%2520moves%2520to%250Athe%2520end%2520of%2520the%2520stream%252C%2520i.e.%252C%2520the%2520target%2520domain%252C%2520the%2520domain%2520shift%2520is%2520drastically%250Areduced.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520public%2520GDA%2520benchmarks%252C%2520and%2520the%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520SWAT%2520significantly%2520outperforms%2520the%250Astate-of-the-art%2520approaches.%2520The%2520implementation%2520is%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/SWAT-8677.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWAT%3A%20Sliding%20Window%20Adversarial%20Training%20for%20Gradual%20Domain%20Adaptation&entry.906535625=Zixi%20Wang%20and%20Yubo%20Huang%20and%20Wenwei%20Luo%20and%20Tonglan%20Xie%20and%20Mengmeng%20Jing%20and%20Lin%20Zuo&entry.1292438233=%20%20Domain%20shifts%20are%20critical%20issues%20that%20harm%20the%20performance%20of%20machine%0Alearning.%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20mitigates%20this%20issue%20but%20suffers%0Awhen%20the%20domain%20shifts%20are%20steep%20and%20drastic.%20Gradual%20Domain%20Adaptation%20%28GDA%29%0Aalleviates%20this%20problem%20in%20a%20mild%20way%20by%20gradually%20adapting%20from%20the%20source%20to%0Athe%20target%20domain%20using%20multiple%20intermediate%20domains.%20In%20this%20paper%2C%20we%0Apropose%20Sliding%20Window%20Adversarial%20Training%20%28SWAT%29%20for%20Gradual%20Domain%0AAdaptation.%20SWAT%20uses%20the%20construction%20of%20adversarial%20streams%20to%20connect%20the%0Afeature%20spaces%20of%20the%20source%20and%20target%20domains.%20In%20order%20to%20gradually%20narrow%0Athe%20small%20gap%20between%20adjacent%20intermediate%20domains%2C%20a%20sliding%20window%20paradigm%0Ais%20designed%20that%20moves%20along%20the%20adversarial%20stream.%20When%20the%20window%20moves%20to%0Athe%20end%20of%20the%20stream%2C%20i.e.%2C%20the%20target%20domain%2C%20the%20domain%20shift%20is%20drastically%0Areduced.%20Extensive%20experiments%20are%20conducted%20on%20public%20GDA%20benchmarks%2C%20and%20the%0Aresults%20demonstrate%20that%20the%20proposed%20SWAT%20significantly%20outperforms%20the%0Astate-of-the-art%20approaches.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/SWAT-8677.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19155v1&entry.124074799=Read"},
{"title": "Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising\n  Diffusions", "author": "S\u00f6ren Christensen and Claudia Strauch and Lukas Trottner", "abstract": "  We introduce a new class of generative diffusion models that, unlike\nconventional denoising diffusion models, achieve a time-homogeneous structure\nfor both the noising and denoising processes, allowing the number of steps to\nadaptively adjust based on the noise level. This is accomplished by\nconditioning the forward process using Doob's $h$-transform, which terminates\nthe process at a suitable sampling distribution at a random time. The model is\nparticularly well suited for generating data with lower intrinsic dimensions,\nas the termination criterion simplifies to a first-hitting rule. A key feature\nof the model is its adaptability to the target data, enabling a variety of\ndownstream tasks using a pre-trained unconditional generative model. These\ntasks include natural conditioning through appropriate initialization of the\ndenoising process and classification of noisy data.\n", "link": "http://arxiv.org/abs/2501.19373v1", "date": "2025-01-31", "relevancy": 2.4415, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6231}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6094}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Fixed%20Horizons%3A%20A%20Theoretical%20Framework%20for%20Adaptive%20Denoising%0A%20%20Diffusions&body=Title%3A%20Beyond%20Fixed%20Horizons%3A%20A%20Theoretical%20Framework%20for%20Adaptive%20Denoising%0A%20%20Diffusions%0AAuthor%3A%20S%C3%B6ren%20Christensen%20and%20Claudia%20Strauch%20and%20Lukas%20Trottner%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20class%20of%20generative%20diffusion%20models%20that%2C%20unlike%0Aconventional%20denoising%20diffusion%20models%2C%20achieve%20a%20time-homogeneous%20structure%0Afor%20both%20the%20noising%20and%20denoising%20processes%2C%20allowing%20the%20number%20of%20steps%20to%0Aadaptively%20adjust%20based%20on%20the%20noise%20level.%20This%20is%20accomplished%20by%0Aconditioning%20the%20forward%20process%20using%20Doob%27s%20%24h%24-transform%2C%20which%20terminates%0Athe%20process%20at%20a%20suitable%20sampling%20distribution%20at%20a%20random%20time.%20The%20model%20is%0Aparticularly%20well%20suited%20for%20generating%20data%20with%20lower%20intrinsic%20dimensions%2C%0Aas%20the%20termination%20criterion%20simplifies%20to%20a%20first-hitting%20rule.%20A%20key%20feature%0Aof%20the%20model%20is%20its%20adaptability%20to%20the%20target%20data%2C%20enabling%20a%20variety%20of%0Adownstream%20tasks%20using%20a%20pre-trained%20unconditional%20generative%20model.%20These%0Atasks%20include%20natural%20conditioning%20through%20appropriate%20initialization%20of%20the%0Adenoising%20process%20and%20classification%20of%20noisy%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Fixed%2520Horizons%253A%2520A%2520Theoretical%2520Framework%2520for%2520Adaptive%2520Denoising%250A%2520%2520Diffusions%26entry.906535625%3DS%25C3%25B6ren%2520Christensen%2520and%2520Claudia%2520Strauch%2520and%2520Lukas%2520Trottner%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520class%2520of%2520generative%2520diffusion%2520models%2520that%252C%2520unlike%250Aconventional%2520denoising%2520diffusion%2520models%252C%2520achieve%2520a%2520time-homogeneous%2520structure%250Afor%2520both%2520the%2520noising%2520and%2520denoising%2520processes%252C%2520allowing%2520the%2520number%2520of%2520steps%2520to%250Aadaptively%2520adjust%2520based%2520on%2520the%2520noise%2520level.%2520This%2520is%2520accomplished%2520by%250Aconditioning%2520the%2520forward%2520process%2520using%2520Doob%2527s%2520%2524h%2524-transform%252C%2520which%2520terminates%250Athe%2520process%2520at%2520a%2520suitable%2520sampling%2520distribution%2520at%2520a%2520random%2520time.%2520The%2520model%2520is%250Aparticularly%2520well%2520suited%2520for%2520generating%2520data%2520with%2520lower%2520intrinsic%2520dimensions%252C%250Aas%2520the%2520termination%2520criterion%2520simplifies%2520to%2520a%2520first-hitting%2520rule.%2520A%2520key%2520feature%250Aof%2520the%2520model%2520is%2520its%2520adaptability%2520to%2520the%2520target%2520data%252C%2520enabling%2520a%2520variety%2520of%250Adownstream%2520tasks%2520using%2520a%2520pre-trained%2520unconditional%2520generative%2520model.%2520These%250Atasks%2520include%2520natural%2520conditioning%2520through%2520appropriate%2520initialization%2520of%2520the%250Adenoising%2520process%2520and%2520classification%2520of%2520noisy%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Fixed%20Horizons%3A%20A%20Theoretical%20Framework%20for%20Adaptive%20Denoising%0A%20%20Diffusions&entry.906535625=S%C3%B6ren%20Christensen%20and%20Claudia%20Strauch%20and%20Lukas%20Trottner&entry.1292438233=%20%20We%20introduce%20a%20new%20class%20of%20generative%20diffusion%20models%20that%2C%20unlike%0Aconventional%20denoising%20diffusion%20models%2C%20achieve%20a%20time-homogeneous%20structure%0Afor%20both%20the%20noising%20and%20denoising%20processes%2C%20allowing%20the%20number%20of%20steps%20to%0Aadaptively%20adjust%20based%20on%20the%20noise%20level.%20This%20is%20accomplished%20by%0Aconditioning%20the%20forward%20process%20using%20Doob%27s%20%24h%24-transform%2C%20which%20terminates%0Athe%20process%20at%20a%20suitable%20sampling%20distribution%20at%20a%20random%20time.%20The%20model%20is%0Aparticularly%20well%20suited%20for%20generating%20data%20with%20lower%20intrinsic%20dimensions%2C%0Aas%20the%20termination%20criterion%20simplifies%20to%20a%20first-hitting%20rule.%20A%20key%20feature%0Aof%20the%20model%20is%20its%20adaptability%20to%20the%20target%20data%2C%20enabling%20a%20variety%20of%0Adownstream%20tasks%20using%20a%20pre-trained%20unconditional%20generative%20model.%20These%0Atasks%20include%20natural%20conditioning%20through%20appropriate%20initialization%20of%20the%0Adenoising%20process%20and%20classification%20of%20noisy%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19373v1&entry.124074799=Read"},
{"title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models", "author": "Wenzhi Fang and Dong-Jun Han and Liangqi Yuan and Seyyedali Hosseinalipour and Christopher G. Brinton", "abstract": "  Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines.\n", "link": "http://arxiv.org/abs/2501.19389v1", "date": "2025-01-31", "relevancy": 2.4324, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Sketching%20LoRA%3A%20On-Device%20Collaborative%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Federated%20Sketching%20LoRA%3A%20On-Device%20Collaborative%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Wenzhi%20Fang%20and%20Dong-Jun%20Han%20and%20Liangqi%20Yuan%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20devices%20is%20attracting%20increasing%0Ainterest.%20Recent%20works%20have%20fused%20low-rank%20adaptation%20%28LoRA%29%20techniques%20with%0Afederated%20fine-tuning%20to%20mitigate%20challenges%20associated%20with%20device%20model%20sizes%0Aand%20data%20scarcity.%20Still%2C%20the%20heterogeneity%20of%20computational%20resources%20remains%0Aa%20critical%20bottleneck%3A%20while%20higher-rank%20modules%20generally%20enhance%20performance%2C%0Avarying%20device%20capabilities%20constrain%20LoRA%27s%20feasible%20rank%20range.%20Existing%0Aapproaches%20attempting%20to%20resolve%20this%20issue%20either%20lack%20analytical%0Ajustification%20or%20impose%20additional%20computational%20overhead%2C%20leaving%20a%20wide%20gap%0Afor%20an%20efficient%20and%20theoretically-grounded%20solution.%20To%20address%20these%0Achallenges%2C%20we%20propose%20federated%20sketching%20LoRA%20%28FSLoRA%29%2C%20which%20leverages%20a%0Asketching%20mechanism%20to%20enable%20devices%20to%20selectively%20update%20submatrices%20of%0Aglobal%20LoRA%20modules%20maintained%20by%20the%20server.%20By%20adjusting%20the%20sketching%0Aratios%2C%20which%20determine%20the%20ranks%20of%20the%20submatrices%20on%20the%20devices%2C%20FSLoRA%0Aflexibly%20adapts%20to%20device-specific%20communication%20and%20computational%20constraints.%0AWe%20provide%20a%20rigorous%20convergence%20analysis%20of%20FSLoRA%20that%20characterizes%20how%20the%0Asketching%20ratios%20affect%20the%20convergence%20rate.%20Through%20comprehensive%20experiments%0Aon%20multiple%20datasets%20and%20LLM%20models%2C%20we%20demonstrate%20FSLoRA%27s%20superior%0Aperformance%20compared%20to%20various%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Sketching%2520LoRA%253A%2520On-Device%2520Collaborative%2520Fine-Tuning%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DWenzhi%2520Fang%2520and%2520Dong-Jun%2520Han%2520and%2520Liangqi%2520Yuan%2520and%2520Seyyedali%2520Hosseinalipour%2520and%2520Christopher%2520G.%2520Brinton%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520devices%2520is%2520attracting%2520increasing%250Ainterest.%2520Recent%2520works%2520have%2520fused%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520techniques%2520with%250Afederated%2520fine-tuning%2520to%2520mitigate%2520challenges%2520associated%2520with%2520device%2520model%2520sizes%250Aand%2520data%2520scarcity.%2520Still%252C%2520the%2520heterogeneity%2520of%2520computational%2520resources%2520remains%250Aa%2520critical%2520bottleneck%253A%2520while%2520higher-rank%2520modules%2520generally%2520enhance%2520performance%252C%250Avarying%2520device%2520capabilities%2520constrain%2520LoRA%2527s%2520feasible%2520rank%2520range.%2520Existing%250Aapproaches%2520attempting%2520to%2520resolve%2520this%2520issue%2520either%2520lack%2520analytical%250Ajustification%2520or%2520impose%2520additional%2520computational%2520overhead%252C%2520leaving%2520a%2520wide%2520gap%250Afor%2520an%2520efficient%2520and%2520theoretically-grounded%2520solution.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520federated%2520sketching%2520LoRA%2520%2528FSLoRA%2529%252C%2520which%2520leverages%2520a%250Asketching%2520mechanism%2520to%2520enable%2520devices%2520to%2520selectively%2520update%2520submatrices%2520of%250Aglobal%2520LoRA%2520modules%2520maintained%2520by%2520the%2520server.%2520By%2520adjusting%2520the%2520sketching%250Aratios%252C%2520which%2520determine%2520the%2520ranks%2520of%2520the%2520submatrices%2520on%2520the%2520devices%252C%2520FSLoRA%250Aflexibly%2520adapts%2520to%2520device-specific%2520communication%2520and%2520computational%2520constraints.%250AWe%2520provide%2520a%2520rigorous%2520convergence%2520analysis%2520of%2520FSLoRA%2520that%2520characterizes%2520how%2520the%250Asketching%2520ratios%2520affect%2520the%2520convergence%2520rate.%2520Through%2520comprehensive%2520experiments%250Aon%2520multiple%2520datasets%2520and%2520LLM%2520models%252C%2520we%2520demonstrate%2520FSLoRA%2527s%2520superior%250Aperformance%2520compared%2520to%2520various%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Sketching%20LoRA%3A%20On-Device%20Collaborative%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models&entry.906535625=Wenzhi%20Fang%20and%20Dong-Jun%20Han%20and%20Liangqi%20Yuan%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20G.%20Brinton&entry.1292438233=%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20devices%20is%20attracting%20increasing%0Ainterest.%20Recent%20works%20have%20fused%20low-rank%20adaptation%20%28LoRA%29%20techniques%20with%0Afederated%20fine-tuning%20to%20mitigate%20challenges%20associated%20with%20device%20model%20sizes%0Aand%20data%20scarcity.%20Still%2C%20the%20heterogeneity%20of%20computational%20resources%20remains%0Aa%20critical%20bottleneck%3A%20while%20higher-rank%20modules%20generally%20enhance%20performance%2C%0Avarying%20device%20capabilities%20constrain%20LoRA%27s%20feasible%20rank%20range.%20Existing%0Aapproaches%20attempting%20to%20resolve%20this%20issue%20either%20lack%20analytical%0Ajustification%20or%20impose%20additional%20computational%20overhead%2C%20leaving%20a%20wide%20gap%0Afor%20an%20efficient%20and%20theoretically-grounded%20solution.%20To%20address%20these%0Achallenges%2C%20we%20propose%20federated%20sketching%20LoRA%20%28FSLoRA%29%2C%20which%20leverages%20a%0Asketching%20mechanism%20to%20enable%20devices%20to%20selectively%20update%20submatrices%20of%0Aglobal%20LoRA%20modules%20maintained%20by%20the%20server.%20By%20adjusting%20the%20sketching%0Aratios%2C%20which%20determine%20the%20ranks%20of%20the%20submatrices%20on%20the%20devices%2C%20FSLoRA%0Aflexibly%20adapts%20to%20device-specific%20communication%20and%20computational%20constraints.%0AWe%20provide%20a%20rigorous%20convergence%20analysis%20of%20FSLoRA%20that%20characterizes%20how%20the%0Asketching%20ratios%20affect%20the%20convergence%20rate.%20Through%20comprehensive%20experiments%0Aon%20multiple%20datasets%20and%20LLM%20models%2C%20we%20demonstrate%20FSLoRA%27s%20superior%0Aperformance%20compared%20to%20various%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19389v1&entry.124074799=Read"},
{"title": "FL-APU: A Software Architecture to Ease Practical Implementation of\n  Cross-Silo Federated Learning", "author": "F. Stricker and J. A. Peregrina and D. Bermbach and C. Zirpins", "abstract": "  Federated Learning (FL) is an upcoming technology that is increasingly\napplied in real-world applications. Early applications focused on cross-device\nscenarios, where many participants with limited resources train machine\nlearning (ML) models together, e.g., in the case of Google's GBoard.\nContrarily, cross-silo scenarios have only few participants but with many\nresources, e.g., in the healthcare domain. Despite such early efforts, FL is\nstill rarely used in practice and best practices are, hence, missing. For new\napplications, in our case inter-organizational cross-silo applications,\novercoming this lack of role models is a significant challenge.\n  In order to ease the use of FL in real-world cross-silo applications, we here\npropose a scenario-based architecture for the practical use of FL in the\ncontext of multiple companies collaborating to improve the quality of their ML\nmodels. The architecture emphasizes the collaboration between the participants\nand the FL server and extends basic interactions with domain-specific features.\nFirst, it combines governance with authentication, creating an environment\nwhere only trusted participants can join. Second, it offers traceability of\ngovernance decisions and tracking of training processes, which are also crucial\nin a production environment. Beyond presenting the architectural design, we\nanalyze requirements for the real-world use of FL and evaluate the architecture\nwith a scenario-based analysis method.\n", "link": "http://arxiv.org/abs/2501.19091v1", "date": "2025-01-31", "relevancy": 2.432, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FL-APU%3A%20A%20Software%20Architecture%20to%20Ease%20Practical%20Implementation%20of%0A%20%20Cross-Silo%20Federated%20Learning&body=Title%3A%20FL-APU%3A%20A%20Software%20Architecture%20to%20Ease%20Practical%20Implementation%20of%0A%20%20Cross-Silo%20Federated%20Learning%0AAuthor%3A%20F.%20Stricker%20and%20J.%20A.%20Peregrina%20and%20D.%20Bermbach%20and%20C.%20Zirpins%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20an%20upcoming%20technology%20that%20is%20increasingly%0Aapplied%20in%20real-world%20applications.%20Early%20applications%20focused%20on%20cross-device%0Ascenarios%2C%20where%20many%20participants%20with%20limited%20resources%20train%20machine%0Alearning%20%28ML%29%20models%20together%2C%20e.g.%2C%20in%20the%20case%20of%20Google%27s%20GBoard.%0AContrarily%2C%20cross-silo%20scenarios%20have%20only%20few%20participants%20but%20with%20many%0Aresources%2C%20e.g.%2C%20in%20the%20healthcare%20domain.%20Despite%20such%20early%20efforts%2C%20FL%20is%0Astill%20rarely%20used%20in%20practice%20and%20best%20practices%20are%2C%20hence%2C%20missing.%20For%20new%0Aapplications%2C%20in%20our%20case%20inter-organizational%20cross-silo%20applications%2C%0Aovercoming%20this%20lack%20of%20role%20models%20is%20a%20significant%20challenge.%0A%20%20In%20order%20to%20ease%20the%20use%20of%20FL%20in%20real-world%20cross-silo%20applications%2C%20we%20here%0Apropose%20a%20scenario-based%20architecture%20for%20the%20practical%20use%20of%20FL%20in%20the%0Acontext%20of%20multiple%20companies%20collaborating%20to%20improve%20the%20quality%20of%20their%20ML%0Amodels.%20The%20architecture%20emphasizes%20the%20collaboration%20between%20the%20participants%0Aand%20the%20FL%20server%20and%20extends%20basic%20interactions%20with%20domain-specific%20features.%0AFirst%2C%20it%20combines%20governance%20with%20authentication%2C%20creating%20an%20environment%0Awhere%20only%20trusted%20participants%20can%20join.%20Second%2C%20it%20offers%20traceability%20of%0Agovernance%20decisions%20and%20tracking%20of%20training%20processes%2C%20which%20are%20also%20crucial%0Ain%20a%20production%20environment.%20Beyond%20presenting%20the%20architectural%20design%2C%20we%0Aanalyze%20requirements%20for%20the%20real-world%20use%20of%20FL%20and%20evaluate%20the%20architecture%0Awith%20a%20scenario-based%20analysis%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFL-APU%253A%2520A%2520Software%2520Architecture%2520to%2520Ease%2520Practical%2520Implementation%2520of%250A%2520%2520Cross-Silo%2520Federated%2520Learning%26entry.906535625%3DF.%2520Stricker%2520and%2520J.%2520A.%2520Peregrina%2520and%2520D.%2520Bermbach%2520and%2520C.%2520Zirpins%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520an%2520upcoming%2520technology%2520that%2520is%2520increasingly%250Aapplied%2520in%2520real-world%2520applications.%2520Early%2520applications%2520focused%2520on%2520cross-device%250Ascenarios%252C%2520where%2520many%2520participants%2520with%2520limited%2520resources%2520train%2520machine%250Alearning%2520%2528ML%2529%2520models%2520together%252C%2520e.g.%252C%2520in%2520the%2520case%2520of%2520Google%2527s%2520GBoard.%250AContrarily%252C%2520cross-silo%2520scenarios%2520have%2520only%2520few%2520participants%2520but%2520with%2520many%250Aresources%252C%2520e.g.%252C%2520in%2520the%2520healthcare%2520domain.%2520Despite%2520such%2520early%2520efforts%252C%2520FL%2520is%250Astill%2520rarely%2520used%2520in%2520practice%2520and%2520best%2520practices%2520are%252C%2520hence%252C%2520missing.%2520For%2520new%250Aapplications%252C%2520in%2520our%2520case%2520inter-organizational%2520cross-silo%2520applications%252C%250Aovercoming%2520this%2520lack%2520of%2520role%2520models%2520is%2520a%2520significant%2520challenge.%250A%2520%2520In%2520order%2520to%2520ease%2520the%2520use%2520of%2520FL%2520in%2520real-world%2520cross-silo%2520applications%252C%2520we%2520here%250Apropose%2520a%2520scenario-based%2520architecture%2520for%2520the%2520practical%2520use%2520of%2520FL%2520in%2520the%250Acontext%2520of%2520multiple%2520companies%2520collaborating%2520to%2520improve%2520the%2520quality%2520of%2520their%2520ML%250Amodels.%2520The%2520architecture%2520emphasizes%2520the%2520collaboration%2520between%2520the%2520participants%250Aand%2520the%2520FL%2520server%2520and%2520extends%2520basic%2520interactions%2520with%2520domain-specific%2520features.%250AFirst%252C%2520it%2520combines%2520governance%2520with%2520authentication%252C%2520creating%2520an%2520environment%250Awhere%2520only%2520trusted%2520participants%2520can%2520join.%2520Second%252C%2520it%2520offers%2520traceability%2520of%250Agovernance%2520decisions%2520and%2520tracking%2520of%2520training%2520processes%252C%2520which%2520are%2520also%2520crucial%250Ain%2520a%2520production%2520environment.%2520Beyond%2520presenting%2520the%2520architectural%2520design%252C%2520we%250Aanalyze%2520requirements%2520for%2520the%2520real-world%2520use%2520of%2520FL%2520and%2520evaluate%2520the%2520architecture%250Awith%2520a%2520scenario-based%2520analysis%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FL-APU%3A%20A%20Software%20Architecture%20to%20Ease%20Practical%20Implementation%20of%0A%20%20Cross-Silo%20Federated%20Learning&entry.906535625=F.%20Stricker%20and%20J.%20A.%20Peregrina%20and%20D.%20Bermbach%20and%20C.%20Zirpins&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20an%20upcoming%20technology%20that%20is%20increasingly%0Aapplied%20in%20real-world%20applications.%20Early%20applications%20focused%20on%20cross-device%0Ascenarios%2C%20where%20many%20participants%20with%20limited%20resources%20train%20machine%0Alearning%20%28ML%29%20models%20together%2C%20e.g.%2C%20in%20the%20case%20of%20Google%27s%20GBoard.%0AContrarily%2C%20cross-silo%20scenarios%20have%20only%20few%20participants%20but%20with%20many%0Aresources%2C%20e.g.%2C%20in%20the%20healthcare%20domain.%20Despite%20such%20early%20efforts%2C%20FL%20is%0Astill%20rarely%20used%20in%20practice%20and%20best%20practices%20are%2C%20hence%2C%20missing.%20For%20new%0Aapplications%2C%20in%20our%20case%20inter-organizational%20cross-silo%20applications%2C%0Aovercoming%20this%20lack%20of%20role%20models%20is%20a%20significant%20challenge.%0A%20%20In%20order%20to%20ease%20the%20use%20of%20FL%20in%20real-world%20cross-silo%20applications%2C%20we%20here%0Apropose%20a%20scenario-based%20architecture%20for%20the%20practical%20use%20of%20FL%20in%20the%0Acontext%20of%20multiple%20companies%20collaborating%20to%20improve%20the%20quality%20of%20their%20ML%0Amodels.%20The%20architecture%20emphasizes%20the%20collaboration%20between%20the%20participants%0Aand%20the%20FL%20server%20and%20extends%20basic%20interactions%20with%20domain-specific%20features.%0AFirst%2C%20it%20combines%20governance%20with%20authentication%2C%20creating%20an%20environment%0Awhere%20only%20trusted%20participants%20can%20join.%20Second%2C%20it%20offers%20traceability%20of%0Agovernance%20decisions%20and%20tracking%20of%20training%20processes%2C%20which%20are%20also%20crucial%0Ain%20a%20production%20environment.%20Beyond%20presenting%20the%20architectural%20design%2C%20we%0Aanalyze%20requirements%20for%20the%20real-world%20use%20of%20FL%20and%20evaluate%20the%20architecture%0Awith%20a%20scenario-based%20analysis%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19091v1&entry.124074799=Read"},
{"title": "Cautious Optimizers: Improving Training with One Line of Code", "author": "Kaizhao Liang and Lizhang Chen and Bo Liu and Qiang Liu", "abstract": "  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.\n", "link": "http://arxiv.org/abs/2411.16085v3", "date": "2025-01-31", "relevancy": 2.4265, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4945}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4891}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cautious%20Optimizers%3A%20Improving%20Training%20with%20One%20Line%20of%20Code&body=Title%3A%20Cautious%20Optimizers%3A%20Improving%20Training%20with%20One%20Line%20of%20Code%0AAuthor%3A%20Kaizhao%20Liang%20and%20Lizhang%20Chen%20and%20Bo%20Liu%20and%20Qiang%20Liu%0AAbstract%3A%20%20%20AdamW%20has%20been%20the%20default%20optimizer%20for%20transformer%20pretraining.%20For%20many%0Ayears%2C%20our%20community%20searched%20for%20faster%20and%20more%20stable%20optimizers%20with%20only%0Aconstrained%20positive%20outcomes.%20In%20this%20work%2C%20we%20propose%20a%20single-line%0Amodification%20in%20Pytorch%20to%20any%20momentum-based%20optimizer%2C%20which%20we%20rename%0Acautious%20optimizer%2C%20e.g.%20C-AdamW%20and%20C-Lion.%20Our%20theoretical%20result%20shows%20that%0Athis%20modification%20preserves%20Adam%27s%20Hamiltonian%20function%20and%20it%20does%20not%20break%0Athe%20convergence%20guarantee%20under%20the%20Lyapunov%20analysis.%20In%20addition%2C%20a%20whole%20new%0Afamily%20of%20optimizers%20is%20revealed%20by%20our%20theoretical%20insight.%20Among%20them%2C%20we%0Apick%20the%20simplest%20one%20for%20empirical%20experiments%2C%20showing%20not%20only%20speed-up%20on%0ALlama%20and%20MAE%20pretraining%20up%20to%20%241.47%24%20times%2C%20but%20also%20better%20results%20in%20LLM%0Apost-training%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/kyleliang919/C-Optim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCautious%2520Optimizers%253A%2520Improving%2520Training%2520with%2520One%2520Line%2520of%2520Code%26entry.906535625%3DKaizhao%2520Liang%2520and%2520Lizhang%2520Chen%2520and%2520Bo%2520Liu%2520and%2520Qiang%2520Liu%26entry.1292438233%3D%2520%2520AdamW%2520has%2520been%2520the%2520default%2520optimizer%2520for%2520transformer%2520pretraining.%2520For%2520many%250Ayears%252C%2520our%2520community%2520searched%2520for%2520faster%2520and%2520more%2520stable%2520optimizers%2520with%2520only%250Aconstrained%2520positive%2520outcomes.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520single-line%250Amodification%2520in%2520Pytorch%2520to%2520any%2520momentum-based%2520optimizer%252C%2520which%2520we%2520rename%250Acautious%2520optimizer%252C%2520e.g.%2520C-AdamW%2520and%2520C-Lion.%2520Our%2520theoretical%2520result%2520shows%2520that%250Athis%2520modification%2520preserves%2520Adam%2527s%2520Hamiltonian%2520function%2520and%2520it%2520does%2520not%2520break%250Athe%2520convergence%2520guarantee%2520under%2520the%2520Lyapunov%2520analysis.%2520In%2520addition%252C%2520a%2520whole%2520new%250Afamily%2520of%2520optimizers%2520is%2520revealed%2520by%2520our%2520theoretical%2520insight.%2520Among%2520them%252C%2520we%250Apick%2520the%2520simplest%2520one%2520for%2520empirical%2520experiments%252C%2520showing%2520not%2520only%2520speed-up%2520on%250ALlama%2520and%2520MAE%2520pretraining%2520up%2520to%2520%25241.47%2524%2520times%252C%2520but%2520also%2520better%2520results%2520in%2520LLM%250Apost-training%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/kyleliang919/C-Optim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cautious%20Optimizers%3A%20Improving%20Training%20with%20One%20Line%20of%20Code&entry.906535625=Kaizhao%20Liang%20and%20Lizhang%20Chen%20and%20Bo%20Liu%20and%20Qiang%20Liu&entry.1292438233=%20%20AdamW%20has%20been%20the%20default%20optimizer%20for%20transformer%20pretraining.%20For%20many%0Ayears%2C%20our%20community%20searched%20for%20faster%20and%20more%20stable%20optimizers%20with%20only%0Aconstrained%20positive%20outcomes.%20In%20this%20work%2C%20we%20propose%20a%20single-line%0Amodification%20in%20Pytorch%20to%20any%20momentum-based%20optimizer%2C%20which%20we%20rename%0Acautious%20optimizer%2C%20e.g.%20C-AdamW%20and%20C-Lion.%20Our%20theoretical%20result%20shows%20that%0Athis%20modification%20preserves%20Adam%27s%20Hamiltonian%20function%20and%20it%20does%20not%20break%0Athe%20convergence%20guarantee%20under%20the%20Lyapunov%20analysis.%20In%20addition%2C%20a%20whole%20new%0Afamily%20of%20optimizers%20is%20revealed%20by%20our%20theoretical%20insight.%20Among%20them%2C%20we%0Apick%20the%20simplest%20one%20for%20empirical%20experiments%2C%20showing%20not%20only%20speed-up%20on%0ALlama%20and%20MAE%20pretraining%20up%20to%20%241.47%24%20times%2C%20but%20also%20better%20results%20in%20LLM%0Apost-training%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/kyleliang919/C-Optim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16085v3&entry.124074799=Read"},
{"title": "RealCraft: Attention Control as A Tool for Zero-Shot Consistent Video\n  Editing", "author": "Shutong Jin and Ruiyu Wang and Florian T. Pokorny", "abstract": "  Even though large-scale text-to-image generative models show promising\nperformance in synthesizing high-quality images, applying these models directly\nto image editing remains a significant challenge. This challenge is further\namplified in video editing due to the additional dimension of time. This is\nespecially the case for editing real-world videos as it necessitates\nmaintaining a stable structural layout across frames while executing localized\nedits without disrupting the existing content. In this paper, we propose\nRealCraft, an attention-control-based method for zero-shot real-world video\nediting. By swapping cross-attention for new feature injection and relaxing\nspatial-temporal attention of the editing object, we achieve localized\nshape-wise edit along with enhanced temporal consistency. Our model directly\nuses Stable Diffusion and operates without the need for additional information.\nWe showcase the proposed zero-shot attention-control-based method across a\nrange of videos, demonstrating shape-wise, time-consistent and parameter-free\nediting in videos of up to 64 frames.\n", "link": "http://arxiv.org/abs/2312.12635v4", "date": "2025-01-31", "relevancy": 2.4198, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6457}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealCraft%3A%20Attention%20Control%20as%20A%20Tool%20for%20Zero-Shot%20Consistent%20Video%0A%20%20Editing&body=Title%3A%20RealCraft%3A%20Attention%20Control%20as%20A%20Tool%20for%20Zero-Shot%20Consistent%20Video%0A%20%20Editing%0AAuthor%3A%20Shutong%20Jin%20and%20Ruiyu%20Wang%20and%20Florian%20T.%20Pokorny%0AAbstract%3A%20%20%20Even%20though%20large-scale%20text-to-image%20generative%20models%20show%20promising%0Aperformance%20in%20synthesizing%20high-quality%20images%2C%20applying%20these%20models%20directly%0Ato%20image%20editing%20remains%20a%20significant%20challenge.%20This%20challenge%20is%20further%0Aamplified%20in%20video%20editing%20due%20to%20the%20additional%20dimension%20of%20time.%20This%20is%0Aespecially%20the%20case%20for%20editing%20real-world%20videos%20as%20it%20necessitates%0Amaintaining%20a%20stable%20structural%20layout%20across%20frames%20while%20executing%20localized%0Aedits%20without%20disrupting%20the%20existing%20content.%20In%20this%20paper%2C%20we%20propose%0ARealCraft%2C%20an%20attention-control-based%20method%20for%20zero-shot%20real-world%20video%0Aediting.%20By%20swapping%20cross-attention%20for%20new%20feature%20injection%20and%20relaxing%0Aspatial-temporal%20attention%20of%20the%20editing%20object%2C%20we%20achieve%20localized%0Ashape-wise%20edit%20along%20with%20enhanced%20temporal%20consistency.%20Our%20model%20directly%0Auses%20Stable%20Diffusion%20and%20operates%20without%20the%20need%20for%20additional%20information.%0AWe%20showcase%20the%20proposed%20zero-shot%20attention-control-based%20method%20across%20a%0Arange%20of%20videos%2C%20demonstrating%20shape-wise%2C%20time-consistent%20and%20parameter-free%0Aediting%20in%20videos%20of%20up%20to%2064%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12635v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealCraft%253A%2520Attention%2520Control%2520as%2520A%2520Tool%2520for%2520Zero-Shot%2520Consistent%2520Video%250A%2520%2520Editing%26entry.906535625%3DShutong%2520Jin%2520and%2520Ruiyu%2520Wang%2520and%2520Florian%2520T.%2520Pokorny%26entry.1292438233%3D%2520%2520Even%2520though%2520large-scale%2520text-to-image%2520generative%2520models%2520show%2520promising%250Aperformance%2520in%2520synthesizing%2520high-quality%2520images%252C%2520applying%2520these%2520models%2520directly%250Ato%2520image%2520editing%2520remains%2520a%2520significant%2520challenge.%2520This%2520challenge%2520is%2520further%250Aamplified%2520in%2520video%2520editing%2520due%2520to%2520the%2520additional%2520dimension%2520of%2520time.%2520This%2520is%250Aespecially%2520the%2520case%2520for%2520editing%2520real-world%2520videos%2520as%2520it%2520necessitates%250Amaintaining%2520a%2520stable%2520structural%2520layout%2520across%2520frames%2520while%2520executing%2520localized%250Aedits%2520without%2520disrupting%2520the%2520existing%2520content.%2520In%2520this%2520paper%252C%2520we%2520propose%250ARealCraft%252C%2520an%2520attention-control-based%2520method%2520for%2520zero-shot%2520real-world%2520video%250Aediting.%2520By%2520swapping%2520cross-attention%2520for%2520new%2520feature%2520injection%2520and%2520relaxing%250Aspatial-temporal%2520attention%2520of%2520the%2520editing%2520object%252C%2520we%2520achieve%2520localized%250Ashape-wise%2520edit%2520along%2520with%2520enhanced%2520temporal%2520consistency.%2520Our%2520model%2520directly%250Auses%2520Stable%2520Diffusion%2520and%2520operates%2520without%2520the%2520need%2520for%2520additional%2520information.%250AWe%2520showcase%2520the%2520proposed%2520zero-shot%2520attention-control-based%2520method%2520across%2520a%250Arange%2520of%2520videos%252C%2520demonstrating%2520shape-wise%252C%2520time-consistent%2520and%2520parameter-free%250Aediting%2520in%2520videos%2520of%2520up%2520to%252064%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12635v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealCraft%3A%20Attention%20Control%20as%20A%20Tool%20for%20Zero-Shot%20Consistent%20Video%0A%20%20Editing&entry.906535625=Shutong%20Jin%20and%20Ruiyu%20Wang%20and%20Florian%20T.%20Pokorny&entry.1292438233=%20%20Even%20though%20large-scale%20text-to-image%20generative%20models%20show%20promising%0Aperformance%20in%20synthesizing%20high-quality%20images%2C%20applying%20these%20models%20directly%0Ato%20image%20editing%20remains%20a%20significant%20challenge.%20This%20challenge%20is%20further%0Aamplified%20in%20video%20editing%20due%20to%20the%20additional%20dimension%20of%20time.%20This%20is%0Aespecially%20the%20case%20for%20editing%20real-world%20videos%20as%20it%20necessitates%0Amaintaining%20a%20stable%20structural%20layout%20across%20frames%20while%20executing%20localized%0Aedits%20without%20disrupting%20the%20existing%20content.%20In%20this%20paper%2C%20we%20propose%0ARealCraft%2C%20an%20attention-control-based%20method%20for%20zero-shot%20real-world%20video%0Aediting.%20By%20swapping%20cross-attention%20for%20new%20feature%20injection%20and%20relaxing%0Aspatial-temporal%20attention%20of%20the%20editing%20object%2C%20we%20achieve%20localized%0Ashape-wise%20edit%20along%20with%20enhanced%20temporal%20consistency.%20Our%20model%20directly%0Auses%20Stable%20Diffusion%20and%20operates%20without%20the%20need%20for%20additional%20information.%0AWe%20showcase%20the%20proposed%20zero-shot%20attention-control-based%20method%20across%20a%0Arange%20of%20videos%2C%20demonstrating%20shape-wise%2C%20time-consistent%20and%20parameter-free%0Aediting%20in%20videos%20of%20up%20to%2064%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12635v4&entry.124074799=Read"},
{"title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token\n  Probability-Based Approach Using Embeddings", "author": "Ahmed K. Kadhim and Lei Jiao and Rishad Shafik and Ole-Christoffer Granmo", "abstract": "  In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.\n", "link": "http://arxiv.org/abs/2501.18998v1", "date": "2025-01-31", "relevancy": 2.4142, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4901}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4793}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20on%20AI-Generated%20Text%20Detection%20Models%3A%20A%20Token%0A%20%20Probability-Based%20Approach%20Using%20Embeddings&body=Title%3A%20Adversarial%20Attacks%20on%20AI-Generated%20Text%20Detection%20Models%3A%20A%20Token%0A%20%20Probability-Based%20Approach%20Using%20Embeddings%0AAuthor%3A%20Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20text%20generation%20tools%20utilizing%20Artificial%20Intelligence%20%28AI%29%0Ahave%20occasionally%20been%20misused%20across%20various%20domains%2C%20such%20as%20generating%0Astudent%20reports%20or%20creative%20writings.%20This%20issue%20prompts%20plagiarism%20detection%0Aservices%20to%20enhance%20their%20capabilities%20in%20identifying%20AI-generated%20content.%0AAdversarial%20attacks%20are%20often%20used%20to%20test%20the%20robustness%20of%20AI-text%20generated%0Adetectors.%20This%20work%20proposes%20a%20novel%20textual%20adversarial%20attack%20on%20the%0Adetection%20models%20such%20as%20Fast-DetectGPT.%20The%20method%20employs%20embedding%20models%0Afor%20data%20perturbation%2C%20aiming%20at%20reconstructing%20the%20AI%20generated%20texts%20to%0Areduce%20the%20likelihood%20of%20detection%20of%20the%20true%20origin%20of%20the%20texts.%0ASpecifically%2C%20we%20employ%20different%20embedding%20techniques%2C%20including%20the%20Tsetlin%0AMachine%20%28TM%29%2C%20an%20interpretable%20approach%20in%20machine%20learning%20for%20this%20purpose.%0ABy%20combining%20synonyms%20and%20embedding%20similarity%20vectors%2C%20we%20demonstrates%20the%0Astate-of-the-art%20reduction%20in%20detection%20scores%20against%20Fast-DetectGPT.%0AParticularly%2C%20in%20the%20XSum%20dataset%2C%20the%20detection%20score%20decreased%20from%200.4431%20to%0A0.2744%20AUROC%2C%20and%20in%20the%20SQuAD%20dataset%2C%20it%20dropped%20from%200.5068%20to%200.3532%20AUROC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520on%2520AI-Generated%2520Text%2520Detection%2520Models%253A%2520A%2520Token%250A%2520%2520Probability-Based%2520Approach%2520Using%2520Embeddings%26entry.906535625%3DAhmed%2520K.%2520Kadhim%2520and%2520Lei%2520Jiao%2520and%2520Rishad%2520Shafik%2520and%2520Ole-Christoffer%2520Granmo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520text%2520generation%2520tools%2520utilizing%2520Artificial%2520Intelligence%2520%2528AI%2529%250Ahave%2520occasionally%2520been%2520misused%2520across%2520various%2520domains%252C%2520such%2520as%2520generating%250Astudent%2520reports%2520or%2520creative%2520writings.%2520This%2520issue%2520prompts%2520plagiarism%2520detection%250Aservices%2520to%2520enhance%2520their%2520capabilities%2520in%2520identifying%2520AI-generated%2520content.%250AAdversarial%2520attacks%2520are%2520often%2520used%2520to%2520test%2520the%2520robustness%2520of%2520AI-text%2520generated%250Adetectors.%2520This%2520work%2520proposes%2520a%2520novel%2520textual%2520adversarial%2520attack%2520on%2520the%250Adetection%2520models%2520such%2520as%2520Fast-DetectGPT.%2520The%2520method%2520employs%2520embedding%2520models%250Afor%2520data%2520perturbation%252C%2520aiming%2520at%2520reconstructing%2520the%2520AI%2520generated%2520texts%2520to%250Areduce%2520the%2520likelihood%2520of%2520detection%2520of%2520the%2520true%2520origin%2520of%2520the%2520texts.%250ASpecifically%252C%2520we%2520employ%2520different%2520embedding%2520techniques%252C%2520including%2520the%2520Tsetlin%250AMachine%2520%2528TM%2529%252C%2520an%2520interpretable%2520approach%2520in%2520machine%2520learning%2520for%2520this%2520purpose.%250ABy%2520combining%2520synonyms%2520and%2520embedding%2520similarity%2520vectors%252C%2520we%2520demonstrates%2520the%250Astate-of-the-art%2520reduction%2520in%2520detection%2520scores%2520against%2520Fast-DetectGPT.%250AParticularly%252C%2520in%2520the%2520XSum%2520dataset%252C%2520the%2520detection%2520score%2520decreased%2520from%25200.4431%2520to%250A0.2744%2520AUROC%252C%2520and%2520in%2520the%2520SQuAD%2520dataset%252C%2520it%2520dropped%2520from%25200.5068%2520to%25200.3532%2520AUROC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20on%20AI-Generated%20Text%20Detection%20Models%3A%20A%20Token%0A%20%20Probability-Based%20Approach%20Using%20Embeddings&entry.906535625=Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo&entry.1292438233=%20%20In%20recent%20years%2C%20text%20generation%20tools%20utilizing%20Artificial%20Intelligence%20%28AI%29%0Ahave%20occasionally%20been%20misused%20across%20various%20domains%2C%20such%20as%20generating%0Astudent%20reports%20or%20creative%20writings.%20This%20issue%20prompts%20plagiarism%20detection%0Aservices%20to%20enhance%20their%20capabilities%20in%20identifying%20AI-generated%20content.%0AAdversarial%20attacks%20are%20often%20used%20to%20test%20the%20robustness%20of%20AI-text%20generated%0Adetectors.%20This%20work%20proposes%20a%20novel%20textual%20adversarial%20attack%20on%20the%0Adetection%20models%20such%20as%20Fast-DetectGPT.%20The%20method%20employs%20embedding%20models%0Afor%20data%20perturbation%2C%20aiming%20at%20reconstructing%20the%20AI%20generated%20texts%20to%0Areduce%20the%20likelihood%20of%20detection%20of%20the%20true%20origin%20of%20the%20texts.%0ASpecifically%2C%20we%20employ%20different%20embedding%20techniques%2C%20including%20the%20Tsetlin%0AMachine%20%28TM%29%2C%20an%20interpretable%20approach%20in%20machine%20learning%20for%20this%20purpose.%0ABy%20combining%20synonyms%20and%20embedding%20similarity%20vectors%2C%20we%20demonstrates%20the%0Astate-of-the-art%20reduction%20in%20detection%20scores%20against%20Fast-DetectGPT.%0AParticularly%2C%20in%20the%20XSum%20dataset%2C%20the%20detection%20score%20decreased%20from%200.4431%20to%0A0.2744%20AUROC%2C%20and%20in%20the%20SQuAD%20dataset%2C%20it%20dropped%20from%200.5068%20to%200.3532%20AUROC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18998v1&entry.124074799=Read"},
{"title": "SituationalLLM: Proactive language models with scene awareness for\n  dynamic, contextual task guidance", "author": "Muhammad Saif Ullah Khan and Muhammad Zeshan Afzal and Didier Stricker", "abstract": "  Large language models (LLMs) have achieved remarkable success in text-based\ntasks but often struggle to provide actionable guidance in real-world physical\nenvironments. This is because of their inability to recognize their limited\nunderstanding of the user's physical context. We present SituationalLLM, a\nnovel approach that integrates structured scene information into an LLM to\ndeliver proactive, context-aware assistance. By encoding objects, attributes,\nand relationships in a custom Scene Graph Language, SituationalLLM actively\nidentifies gaps in environmental context and seeks clarifications during user\ninteractions. This behavior emerges from training on the Situational Awareness\nDatabase for Instruct-Tuning (SAD-Instruct), which combines diverse,\nscenario-specific scene graphs with iterative, dialogue-based refinements.\nExperimental results indicate that SituationalLLM outperforms generic LLM\nbaselines in task specificity, reliability, and adaptability, paving the way\nfor environment-aware AI assistants capable of delivering robust, user-centric\nguidance under real-world constraints.\n", "link": "http://arxiv.org/abs/2406.13302v3", "date": "2025-01-31", "relevancy": 2.4027, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SituationalLLM%3A%20Proactive%20language%20models%20with%20scene%20awareness%20for%0A%20%20dynamic%2C%20contextual%20task%20guidance&body=Title%3A%20SituationalLLM%3A%20Proactive%20language%20models%20with%20scene%20awareness%20for%0A%20%20dynamic%2C%20contextual%20task%20guidance%0AAuthor%3A%20Muhammad%20Saif%20Ullah%20Khan%20and%20Muhammad%20Zeshan%20Afzal%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20text-based%0Atasks%20but%20often%20struggle%20to%20provide%20actionable%20guidance%20in%20real-world%20physical%0Aenvironments.%20This%20is%20because%20of%20their%20inability%20to%20recognize%20their%20limited%0Aunderstanding%20of%20the%20user%27s%20physical%20context.%20We%20present%20SituationalLLM%2C%20a%0Anovel%20approach%20that%20integrates%20structured%20scene%20information%20into%20an%20LLM%20to%0Adeliver%20proactive%2C%20context-aware%20assistance.%20By%20encoding%20objects%2C%20attributes%2C%0Aand%20relationships%20in%20a%20custom%20Scene%20Graph%20Language%2C%20SituationalLLM%20actively%0Aidentifies%20gaps%20in%20environmental%20context%20and%20seeks%20clarifications%20during%20user%0Ainteractions.%20This%20behavior%20emerges%20from%20training%20on%20the%20Situational%20Awareness%0ADatabase%20for%20Instruct-Tuning%20%28SAD-Instruct%29%2C%20which%20combines%20diverse%2C%0Ascenario-specific%20scene%20graphs%20with%20iterative%2C%20dialogue-based%20refinements.%0AExperimental%20results%20indicate%20that%20SituationalLLM%20outperforms%20generic%20LLM%0Abaselines%20in%20task%20specificity%2C%20reliability%2C%20and%20adaptability%2C%20paving%20the%20way%0Afor%20environment-aware%20AI%20assistants%20capable%20of%20delivering%20robust%2C%20user-centric%0Aguidance%20under%20real-world%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13302v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSituationalLLM%253A%2520Proactive%2520language%2520models%2520with%2520scene%2520awareness%2520for%250A%2520%2520dynamic%252C%2520contextual%2520task%2520guidance%26entry.906535625%3DMuhammad%2520Saif%2520Ullah%2520Khan%2520and%2520Muhammad%2520Zeshan%2520Afzal%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520text-based%250Atasks%2520but%2520often%2520struggle%2520to%2520provide%2520actionable%2520guidance%2520in%2520real-world%2520physical%250Aenvironments.%2520This%2520is%2520because%2520of%2520their%2520inability%2520to%2520recognize%2520their%2520limited%250Aunderstanding%2520of%2520the%2520user%2527s%2520physical%2520context.%2520We%2520present%2520SituationalLLM%252C%2520a%250Anovel%2520approach%2520that%2520integrates%2520structured%2520scene%2520information%2520into%2520an%2520LLM%2520to%250Adeliver%2520proactive%252C%2520context-aware%2520assistance.%2520By%2520encoding%2520objects%252C%2520attributes%252C%250Aand%2520relationships%2520in%2520a%2520custom%2520Scene%2520Graph%2520Language%252C%2520SituationalLLM%2520actively%250Aidentifies%2520gaps%2520in%2520environmental%2520context%2520and%2520seeks%2520clarifications%2520during%2520user%250Ainteractions.%2520This%2520behavior%2520emerges%2520from%2520training%2520on%2520the%2520Situational%2520Awareness%250ADatabase%2520for%2520Instruct-Tuning%2520%2528SAD-Instruct%2529%252C%2520which%2520combines%2520diverse%252C%250Ascenario-specific%2520scene%2520graphs%2520with%2520iterative%252C%2520dialogue-based%2520refinements.%250AExperimental%2520results%2520indicate%2520that%2520SituationalLLM%2520outperforms%2520generic%2520LLM%250Abaselines%2520in%2520task%2520specificity%252C%2520reliability%252C%2520and%2520adaptability%252C%2520paving%2520the%2520way%250Afor%2520environment-aware%2520AI%2520assistants%2520capable%2520of%2520delivering%2520robust%252C%2520user-centric%250Aguidance%2520under%2520real-world%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13302v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SituationalLLM%3A%20Proactive%20language%20models%20with%20scene%20awareness%20for%0A%20%20dynamic%2C%20contextual%20task%20guidance&entry.906535625=Muhammad%20Saif%20Ullah%20Khan%20and%20Muhammad%20Zeshan%20Afzal%20and%20Didier%20Stricker&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20text-based%0Atasks%20but%20often%20struggle%20to%20provide%20actionable%20guidance%20in%20real-world%20physical%0Aenvironments.%20This%20is%20because%20of%20their%20inability%20to%20recognize%20their%20limited%0Aunderstanding%20of%20the%20user%27s%20physical%20context.%20We%20present%20SituationalLLM%2C%20a%0Anovel%20approach%20that%20integrates%20structured%20scene%20information%20into%20an%20LLM%20to%0Adeliver%20proactive%2C%20context-aware%20assistance.%20By%20encoding%20objects%2C%20attributes%2C%0Aand%20relationships%20in%20a%20custom%20Scene%20Graph%20Language%2C%20SituationalLLM%20actively%0Aidentifies%20gaps%20in%20environmental%20context%20and%20seeks%20clarifications%20during%20user%0Ainteractions.%20This%20behavior%20emerges%20from%20training%20on%20the%20Situational%20Awareness%0ADatabase%20for%20Instruct-Tuning%20%28SAD-Instruct%29%2C%20which%20combines%20diverse%2C%0Ascenario-specific%20scene%20graphs%20with%20iterative%2C%20dialogue-based%20refinements.%0AExperimental%20results%20indicate%20that%20SituationalLLM%20outperforms%20generic%20LLM%0Abaselines%20in%20task%20specificity%2C%20reliability%2C%20and%20adaptability%2C%20paving%20the%20way%0Afor%20environment-aware%20AI%20assistants%20capable%20of%20delivering%20robust%2C%20user-centric%0Aguidance%20under%20real-world%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13302v3&entry.124074799=Read"},
{"title": "Learning Non-Local Molecular Interactions via Equivariant Local\n  Representations and Charge Equilibration", "author": "Paul Fuchs and Micha\u0142 Sanocki and Julija Zavadlav", "abstract": "  Graph Neural Network (GNN) potentials relying on chemical locality offer\nnear-quantum mechanical accuracy at significantly reduced computational costs.\nBy propagating local information to distance particles, Message-passing neural\nnetworks (MPNNs) extend the locality concept to model interactions beyond their\nlocal neighborhood. Still, this locality precludes modeling long-range effects,\nsuch as charge transfer, electrostatic interactions, and dispersion effects,\nwhich are critical to adequately describe many real-world systems. In this\nwork, we propose the Charge Equilibration Layer for Long-range Interactions\n(CELLI) to address the challenging modeling of non-local interactions and the\nhigh computational cost of MPNNs. This novel architecture generalizes the\nfourth-generation high-dimensional neural network (4GHDNN) concept, integrating\nthe charge equilibration (Qeq) method into a model-agnostic building block for\nmodern equivariant GNN potentials. A series of benchmarks show that CELLI can\nextend the strictly local Allegro architecture to model highly non-local\ninteractions and charge transfer. Our architecture generalizes to diverse\ndatasets and large structures, achieving an accuracy comparable to MPNNs at\nabout twice the computational efficiency.\n", "link": "http://arxiv.org/abs/2501.19179v1", "date": "2025-01-31", "relevancy": 2.3945, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5043}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Non-Local%20Molecular%20Interactions%20via%20Equivariant%20Local%0A%20%20Representations%20and%20Charge%20Equilibration&body=Title%3A%20Learning%20Non-Local%20Molecular%20Interactions%20via%20Equivariant%20Local%0A%20%20Representations%20and%20Charge%20Equilibration%0AAuthor%3A%20Paul%20Fuchs%20and%20Micha%C5%82%20Sanocki%20and%20Julija%20Zavadlav%0AAbstract%3A%20%20%20Graph%20Neural%20Network%20%28GNN%29%20potentials%20relying%20on%20chemical%20locality%20offer%0Anear-quantum%20mechanical%20accuracy%20at%20significantly%20reduced%20computational%20costs.%0ABy%20propagating%20local%20information%20to%20distance%20particles%2C%20Message-passing%20neural%0Anetworks%20%28MPNNs%29%20extend%20the%20locality%20concept%20to%20model%20interactions%20beyond%20their%0Alocal%20neighborhood.%20Still%2C%20this%20locality%20precludes%20modeling%20long-range%20effects%2C%0Asuch%20as%20charge%20transfer%2C%20electrostatic%20interactions%2C%20and%20dispersion%20effects%2C%0Awhich%20are%20critical%20to%20adequately%20describe%20many%20real-world%20systems.%20In%20this%0Awork%2C%20we%20propose%20the%20Charge%20Equilibration%20Layer%20for%20Long-range%20Interactions%0A%28CELLI%29%20to%20address%20the%20challenging%20modeling%20of%20non-local%20interactions%20and%20the%0Ahigh%20computational%20cost%20of%20MPNNs.%20This%20novel%20architecture%20generalizes%20the%0Afourth-generation%20high-dimensional%20neural%20network%20%284GHDNN%29%20concept%2C%20integrating%0Athe%20charge%20equilibration%20%28Qeq%29%20method%20into%20a%20model-agnostic%20building%20block%20for%0Amodern%20equivariant%20GNN%20potentials.%20A%20series%20of%20benchmarks%20show%20that%20CELLI%20can%0Aextend%20the%20strictly%20local%20Allegro%20architecture%20to%20model%20highly%20non-local%0Ainteractions%20and%20charge%20transfer.%20Our%20architecture%20generalizes%20to%20diverse%0Adatasets%20and%20large%20structures%2C%20achieving%20an%20accuracy%20comparable%20to%20MPNNs%20at%0Aabout%20twice%20the%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Non-Local%2520Molecular%2520Interactions%2520via%2520Equivariant%2520Local%250A%2520%2520Representations%2520and%2520Charge%2520Equilibration%26entry.906535625%3DPaul%2520Fuchs%2520and%2520Micha%25C5%2582%2520Sanocki%2520and%2520Julija%2520Zavadlav%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520potentials%2520relying%2520on%2520chemical%2520locality%2520offer%250Anear-quantum%2520mechanical%2520accuracy%2520at%2520significantly%2520reduced%2520computational%2520costs.%250ABy%2520propagating%2520local%2520information%2520to%2520distance%2520particles%252C%2520Message-passing%2520neural%250Anetworks%2520%2528MPNNs%2529%2520extend%2520the%2520locality%2520concept%2520to%2520model%2520interactions%2520beyond%2520their%250Alocal%2520neighborhood.%2520Still%252C%2520this%2520locality%2520precludes%2520modeling%2520long-range%2520effects%252C%250Asuch%2520as%2520charge%2520transfer%252C%2520electrostatic%2520interactions%252C%2520and%2520dispersion%2520effects%252C%250Awhich%2520are%2520critical%2520to%2520adequately%2520describe%2520many%2520real-world%2520systems.%2520In%2520this%250Awork%252C%2520we%2520propose%2520the%2520Charge%2520Equilibration%2520Layer%2520for%2520Long-range%2520Interactions%250A%2528CELLI%2529%2520to%2520address%2520the%2520challenging%2520modeling%2520of%2520non-local%2520interactions%2520and%2520the%250Ahigh%2520computational%2520cost%2520of%2520MPNNs.%2520This%2520novel%2520architecture%2520generalizes%2520the%250Afourth-generation%2520high-dimensional%2520neural%2520network%2520%25284GHDNN%2529%2520concept%252C%2520integrating%250Athe%2520charge%2520equilibration%2520%2528Qeq%2529%2520method%2520into%2520a%2520model-agnostic%2520building%2520block%2520for%250Amodern%2520equivariant%2520GNN%2520potentials.%2520A%2520series%2520of%2520benchmarks%2520show%2520that%2520CELLI%2520can%250Aextend%2520the%2520strictly%2520local%2520Allegro%2520architecture%2520to%2520model%2520highly%2520non-local%250Ainteractions%2520and%2520charge%2520transfer.%2520Our%2520architecture%2520generalizes%2520to%2520diverse%250Adatasets%2520and%2520large%2520structures%252C%2520achieving%2520an%2520accuracy%2520comparable%2520to%2520MPNNs%2520at%250Aabout%2520twice%2520the%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Non-Local%20Molecular%20Interactions%20via%20Equivariant%20Local%0A%20%20Representations%20and%20Charge%20Equilibration&entry.906535625=Paul%20Fuchs%20and%20Micha%C5%82%20Sanocki%20and%20Julija%20Zavadlav&entry.1292438233=%20%20Graph%20Neural%20Network%20%28GNN%29%20potentials%20relying%20on%20chemical%20locality%20offer%0Anear-quantum%20mechanical%20accuracy%20at%20significantly%20reduced%20computational%20costs.%0ABy%20propagating%20local%20information%20to%20distance%20particles%2C%20Message-passing%20neural%0Anetworks%20%28MPNNs%29%20extend%20the%20locality%20concept%20to%20model%20interactions%20beyond%20their%0Alocal%20neighborhood.%20Still%2C%20this%20locality%20precludes%20modeling%20long-range%20effects%2C%0Asuch%20as%20charge%20transfer%2C%20electrostatic%20interactions%2C%20and%20dispersion%20effects%2C%0Awhich%20are%20critical%20to%20adequately%20describe%20many%20real-world%20systems.%20In%20this%0Awork%2C%20we%20propose%20the%20Charge%20Equilibration%20Layer%20for%20Long-range%20Interactions%0A%28CELLI%29%20to%20address%20the%20challenging%20modeling%20of%20non-local%20interactions%20and%20the%0Ahigh%20computational%20cost%20of%20MPNNs.%20This%20novel%20architecture%20generalizes%20the%0Afourth-generation%20high-dimensional%20neural%20network%20%284GHDNN%29%20concept%2C%20integrating%0Athe%20charge%20equilibration%20%28Qeq%29%20method%20into%20a%20model-agnostic%20building%20block%20for%0Amodern%20equivariant%20GNN%20potentials.%20A%20series%20of%20benchmarks%20show%20that%20CELLI%20can%0Aextend%20the%20strictly%20local%20Allegro%20architecture%20to%20model%20highly%20non-local%0Ainteractions%20and%20charge%20transfer.%20Our%20architecture%20generalizes%20to%20diverse%0Adatasets%20and%20large%20structures%2C%20achieving%20an%20accuracy%20comparable%20to%20MPNNs%20at%0Aabout%20twice%20the%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19179v1&entry.124074799=Read"},
{"title": "Low-Rank Adapting Models for Sparse Autoencoders", "author": "Matthew Chen and Joshua Engels and Max Tegmark", "abstract": "  Sparse autoencoders (SAEs) decompose language model representations into a\nsparse set of linear latent vectors. Recent works have improved SAEs using\nlanguage model gradients, but these techniques require many expensive backward\npasses during training and still cause a significant increase in cross entropy\nloss when SAE reconstructions are inserted into the model. In this work, we\nimprove on these limitations by taking a fundamentally different approach: we\nuse low-rank adaptation (LoRA) to finetune the language model itself around a\npreviously trained SAE. We analyze our method across SAE sparsity, SAE width,\nlanguage model size, LoRA rank, and model layer on the Gemma Scope family of\nSAEs. In these settings, our method reduces the cross entropy loss gap by 30%\nto 55% when SAEs are inserted during the forward pass. We also find that\ncompared to end-to-end (e2e) SAEs, our approach achieves the same downstream\ncross entropy loss 3$\\times$ to 20$\\times$ faster on Gemma-2-2B and 2$\\times$\nto 10$\\times$ faster on Llama-3.2-1B. We further show that our technique\nimproves downstream metrics and can adapt multiple SAEs at once. Our results\ndemonstrate that improving model interpretability is not limited to post-hoc\nSAE training; Pareto improvements can also be achieved by directly optimizing\nthe model itself.\n", "link": "http://arxiv.org/abs/2501.19406v1", "date": "2025-01-31", "relevancy": 2.3932, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4789}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Adapting%20Models%20for%20Sparse%20Autoencoders&body=Title%3A%20Low-Rank%20Adapting%20Models%20for%20Sparse%20Autoencoders%0AAuthor%3A%20Matthew%20Chen%20and%20Joshua%20Engels%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20decompose%20language%20model%20representations%20into%20a%0Asparse%20set%20of%20linear%20latent%20vectors.%20Recent%20works%20have%20improved%20SAEs%20using%0Alanguage%20model%20gradients%2C%20but%20these%20techniques%20require%20many%20expensive%20backward%0Apasses%20during%20training%20and%20still%20cause%20a%20significant%20increase%20in%20cross%20entropy%0Aloss%20when%20SAE%20reconstructions%20are%20inserted%20into%20the%20model.%20In%20this%20work%2C%20we%0Aimprove%20on%20these%20limitations%20by%20taking%20a%20fundamentally%20different%20approach%3A%20we%0Ause%20low-rank%20adaptation%20%28LoRA%29%20to%20finetune%20the%20language%20model%20itself%20around%20a%0Apreviously%20trained%20SAE.%20We%20analyze%20our%20method%20across%20SAE%20sparsity%2C%20SAE%20width%2C%0Alanguage%20model%20size%2C%20LoRA%20rank%2C%20and%20model%20layer%20on%20the%20Gemma%20Scope%20family%20of%0ASAEs.%20In%20these%20settings%2C%20our%20method%20reduces%20the%20cross%20entropy%20loss%20gap%20by%2030%25%0Ato%2055%25%20when%20SAEs%20are%20inserted%20during%20the%20forward%20pass.%20We%20also%20find%20that%0Acompared%20to%20end-to-end%20%28e2e%29%20SAEs%2C%20our%20approach%20achieves%20the%20same%20downstream%0Across%20entropy%20loss%203%24%5Ctimes%24%20to%2020%24%5Ctimes%24%20faster%20on%20Gemma-2-2B%20and%202%24%5Ctimes%24%0Ato%2010%24%5Ctimes%24%20faster%20on%20Llama-3.2-1B.%20We%20further%20show%20that%20our%20technique%0Aimproves%20downstream%20metrics%20and%20can%20adapt%20multiple%20SAEs%20at%20once.%20Our%20results%0Ademonstrate%20that%20improving%20model%20interpretability%20is%20not%20limited%20to%20post-hoc%0ASAE%20training%3B%20Pareto%20improvements%20can%20also%20be%20achieved%20by%20directly%20optimizing%0Athe%20model%20itself.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Adapting%2520Models%2520for%2520Sparse%2520Autoencoders%26entry.906535625%3DMatthew%2520Chen%2520and%2520Joshua%2520Engels%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520decompose%2520language%2520model%2520representations%2520into%2520a%250Asparse%2520set%2520of%2520linear%2520latent%2520vectors.%2520Recent%2520works%2520have%2520improved%2520SAEs%2520using%250Alanguage%2520model%2520gradients%252C%2520but%2520these%2520techniques%2520require%2520many%2520expensive%2520backward%250Apasses%2520during%2520training%2520and%2520still%2520cause%2520a%2520significant%2520increase%2520in%2520cross%2520entropy%250Aloss%2520when%2520SAE%2520reconstructions%2520are%2520inserted%2520into%2520the%2520model.%2520In%2520this%2520work%252C%2520we%250Aimprove%2520on%2520these%2520limitations%2520by%2520taking%2520a%2520fundamentally%2520different%2520approach%253A%2520we%250Ause%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520to%2520finetune%2520the%2520language%2520model%2520itself%2520around%2520a%250Apreviously%2520trained%2520SAE.%2520We%2520analyze%2520our%2520method%2520across%2520SAE%2520sparsity%252C%2520SAE%2520width%252C%250Alanguage%2520model%2520size%252C%2520LoRA%2520rank%252C%2520and%2520model%2520layer%2520on%2520the%2520Gemma%2520Scope%2520family%2520of%250ASAEs.%2520In%2520these%2520settings%252C%2520our%2520method%2520reduces%2520the%2520cross%2520entropy%2520loss%2520gap%2520by%252030%2525%250Ato%252055%2525%2520when%2520SAEs%2520are%2520inserted%2520during%2520the%2520forward%2520pass.%2520We%2520also%2520find%2520that%250Acompared%2520to%2520end-to-end%2520%2528e2e%2529%2520SAEs%252C%2520our%2520approach%2520achieves%2520the%2520same%2520downstream%250Across%2520entropy%2520loss%25203%2524%255Ctimes%2524%2520to%252020%2524%255Ctimes%2524%2520faster%2520on%2520Gemma-2-2B%2520and%25202%2524%255Ctimes%2524%250Ato%252010%2524%255Ctimes%2524%2520faster%2520on%2520Llama-3.2-1B.%2520We%2520further%2520show%2520that%2520our%2520technique%250Aimproves%2520downstream%2520metrics%2520and%2520can%2520adapt%2520multiple%2520SAEs%2520at%2520once.%2520Our%2520results%250Ademonstrate%2520that%2520improving%2520model%2520interpretability%2520is%2520not%2520limited%2520to%2520post-hoc%250ASAE%2520training%253B%2520Pareto%2520improvements%2520can%2520also%2520be%2520achieved%2520by%2520directly%2520optimizing%250Athe%2520model%2520itself.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Adapting%20Models%20for%20Sparse%20Autoencoders&entry.906535625=Matthew%20Chen%20and%20Joshua%20Engels%20and%20Max%20Tegmark&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20decompose%20language%20model%20representations%20into%20a%0Asparse%20set%20of%20linear%20latent%20vectors.%20Recent%20works%20have%20improved%20SAEs%20using%0Alanguage%20model%20gradients%2C%20but%20these%20techniques%20require%20many%20expensive%20backward%0Apasses%20during%20training%20and%20still%20cause%20a%20significant%20increase%20in%20cross%20entropy%0Aloss%20when%20SAE%20reconstructions%20are%20inserted%20into%20the%20model.%20In%20this%20work%2C%20we%0Aimprove%20on%20these%20limitations%20by%20taking%20a%20fundamentally%20different%20approach%3A%20we%0Ause%20low-rank%20adaptation%20%28LoRA%29%20to%20finetune%20the%20language%20model%20itself%20around%20a%0Apreviously%20trained%20SAE.%20We%20analyze%20our%20method%20across%20SAE%20sparsity%2C%20SAE%20width%2C%0Alanguage%20model%20size%2C%20LoRA%20rank%2C%20and%20model%20layer%20on%20the%20Gemma%20Scope%20family%20of%0ASAEs.%20In%20these%20settings%2C%20our%20method%20reduces%20the%20cross%20entropy%20loss%20gap%20by%2030%25%0Ato%2055%25%20when%20SAEs%20are%20inserted%20during%20the%20forward%20pass.%20We%20also%20find%20that%0Acompared%20to%20end-to-end%20%28e2e%29%20SAEs%2C%20our%20approach%20achieves%20the%20same%20downstream%0Across%20entropy%20loss%203%24%5Ctimes%24%20to%2020%24%5Ctimes%24%20faster%20on%20Gemma-2-2B%20and%202%24%5Ctimes%24%0Ato%2010%24%5Ctimes%24%20faster%20on%20Llama-3.2-1B.%20We%20further%20show%20that%20our%20technique%0Aimproves%20downstream%20metrics%20and%20can%20adapt%20multiple%20SAEs%20at%20once.%20Our%20results%0Ademonstrate%20that%20improving%20model%20interpretability%20is%20not%20limited%20to%20post-hoc%0ASAE%20training%3B%20Pareto%20improvements%20can%20also%20be%20achieved%20by%20directly%20optimizing%0Athe%20model%20itself.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19406v1&entry.124074799=Read"},
{"title": "LLMs Are In-Context Bandit Reinforcement Learners", "author": "Giovanni Monea and Antoine Bosselut and Kiant\u00e9 Brantley and Yoav Artzi", "abstract": "  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.\n", "link": "http://arxiv.org/abs/2410.05362v2", "date": "2025-01-31", "relevancy": 2.3745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners&body=Title%3A%20LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners%0AAuthor%3A%20Giovanni%20Monea%20and%20Antoine%20Bosselut%20and%20Kiant%C3%A9%20Brantley%20and%20Yoav%20Artzi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20in-context%20learning%20%28ICL%29%2C%20a%20supervised%0Alearning%20technique%20that%20relies%20on%20adding%20annotated%20examples%20to%20the%20model%0Acontext.%20We%20investigate%20a%20contextual%20bandit%20version%20of%20in-context%20reinforcement%0Alearning%20%28ICRL%29%2C%20where%20models%20learn%20in-context%2C%20online%2C%20from%20external%20reward%2C%0Ainstead%20of%20supervised%20data.%20We%20show%20that%20LLMs%20effectively%20demonstrate%20such%0Alearning%2C%20and%20provide%20a%20detailed%20study%20of%20the%20phenomena%2C%20experimenting%20with%0Achallenging%20classification%20tasks%20and%20models%20of%20sizes%20from%20500M%20to%2070B%0Aparameters.%20This%20includes%20identifying%20and%20addressing%20the%20instability%20of%20the%0Aprocess%2C%20demonstrating%20learning%20with%20both%20semantic%20and%20abstract%20labels%2C%20and%0Ashowing%20scaling%20trends.%20Our%20findings%20highlight%20ICRL%20capabilities%20in%20LLMs%2C%20while%0Aalso%20underscoring%20fundamental%20limitations%20in%20their%20implicit%20reasoning%20about%0Aerrors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Are%2520In-Context%2520Bandit%2520Reinforcement%2520Learners%26entry.906535625%3DGiovanni%2520Monea%2520and%2520Antoine%2520Bosselut%2520and%2520Kiant%25C3%25A9%2520Brantley%2520and%2520Yoav%2520Artzi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520in-context%2520learning%2520%2528ICL%2529%252C%2520a%2520supervised%250Alearning%2520technique%2520that%2520relies%2520on%2520adding%2520annotated%2520examples%2520to%2520the%2520model%250Acontext.%2520We%2520investigate%2520a%2520contextual%2520bandit%2520version%2520of%2520in-context%2520reinforcement%250Alearning%2520%2528ICRL%2529%252C%2520where%2520models%2520learn%2520in-context%252C%2520online%252C%2520from%2520external%2520reward%252C%250Ainstead%2520of%2520supervised%2520data.%2520We%2520show%2520that%2520LLMs%2520effectively%2520demonstrate%2520such%250Alearning%252C%2520and%2520provide%2520a%2520detailed%2520study%2520of%2520the%2520phenomena%252C%2520experimenting%2520with%250Achallenging%2520classification%2520tasks%2520and%2520models%2520of%2520sizes%2520from%2520500M%2520to%252070B%250Aparameters.%2520This%2520includes%2520identifying%2520and%2520addressing%2520the%2520instability%2520of%2520the%250Aprocess%252C%2520demonstrating%2520learning%2520with%2520both%2520semantic%2520and%2520abstract%2520labels%252C%2520and%250Ashowing%2520scaling%2520trends.%2520Our%2520findings%2520highlight%2520ICRL%2520capabilities%2520in%2520LLMs%252C%2520while%250Aalso%2520underscoring%2520fundamental%2520limitations%2520in%2520their%2520implicit%2520reasoning%2520about%250Aerrors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners&entry.906535625=Giovanni%20Monea%20and%20Antoine%20Bosselut%20and%20Kiant%C3%A9%20Brantley%20and%20Yoav%20Artzi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20in-context%20learning%20%28ICL%29%2C%20a%20supervised%0Alearning%20technique%20that%20relies%20on%20adding%20annotated%20examples%20to%20the%20model%0Acontext.%20We%20investigate%20a%20contextual%20bandit%20version%20of%20in-context%20reinforcement%0Alearning%20%28ICRL%29%2C%20where%20models%20learn%20in-context%2C%20online%2C%20from%20external%20reward%2C%0Ainstead%20of%20supervised%20data.%20We%20show%20that%20LLMs%20effectively%20demonstrate%20such%0Alearning%2C%20and%20provide%20a%20detailed%20study%20of%20the%20phenomena%2C%20experimenting%20with%0Achallenging%20classification%20tasks%20and%20models%20of%20sizes%20from%20500M%20to%2070B%0Aparameters.%20This%20includes%20identifying%20and%20addressing%20the%20instability%20of%20the%0Aprocess%2C%20demonstrating%20learning%20with%20both%20semantic%20and%20abstract%20labels%2C%20and%0Ashowing%20scaling%20trends.%20Our%20findings%20highlight%20ICRL%20capabilities%20in%20LLMs%2C%20while%0Aalso%20underscoring%20fundamental%20limitations%20in%20their%20implicit%20reasoning%20about%0Aerrors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05362v2&entry.124074799=Read"},
{"title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game", "author": "Mustafa O. Karabag and Ufuk Topcu", "abstract": "  Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.\n", "link": "http://arxiv.org/abs/2501.19398v1", "date": "2025-01-31", "relevancy": 2.366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Strategically%20Reveal%2C%20Conceal%2C%20and%20Infer%20Information%3F%20A%0A%20%20Theoretical%20and%20Empirical%20Analysis%20in%20The%20Chameleon%20Game&body=Title%3A%20Do%20LLMs%20Strategically%20Reveal%2C%20Conceal%2C%20and%20Infer%20Information%3F%20A%0A%20%20Theoretical%20and%20Empirical%20Analysis%20in%20The%20Chameleon%20Game%0AAuthor%3A%20Mustafa%20O.%20Karabag%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20Large%20language%20model-based%20%28LLM-based%29%20agents%20have%20become%20common%20in%20settings%0Athat%20include%20non-cooperative%20parties.%20In%20such%20settings%2C%20agents%27%20decision-making%0Aneeds%20to%20conceal%20information%20from%20their%20adversaries%2C%20reveal%20information%20to%0Atheir%20cooperators%2C%20and%20infer%20information%20to%20identify%20the%20other%20agents%27%0Acharacteristics.%20To%20investigate%20whether%20LLMs%20have%20these%20information%20control%20and%0Adecision-making%20capabilities%2C%20we%20make%20LLM%20agents%20play%20the%20language-based%0Ahidden-identity%20game%2C%20The%20Chameleon.%20In%20the%20game%2C%20a%20group%20of%20non-chameleon%0Aagents%20who%20do%20not%20know%20each%20other%20aim%20to%20identify%20the%20chameleon%20agent%20without%0Arevealing%20a%20secret.%20The%20game%20requires%20the%20aforementioned%20information%20control%0Acapabilities%20both%20as%20a%20chameleon%20and%20a%20non-chameleon.%20The%20empirical%20results%0Ashow%20that%20while%20non-chameleon%20LLM%20agents%20identify%20the%20chameleon%2C%20they%20fail%20to%0Aconceal%20the%20secret%20from%20the%20chameleon%2C%20and%20their%20winning%20probability%20is%20far%0Afrom%20the%20levels%20of%20even%20trivial%20strategies.%20To%20formally%20explain%20this%20behavior%2C%0Awe%20give%20a%20theoretical%20analysis%20for%20a%20spectrum%20of%20strategies%2C%20from%20concealing%20to%0Arevealing%2C%20and%20provide%20bounds%20on%20the%20non-chameleons%27%20winning%20probability.%20Based%0Aon%20the%20empirical%20results%20and%20theoretical%20analysis%20of%20different%20strategies%2C%20we%0Adeduce%20that%20LLM-based%20non-chameleon%20agents%20reveal%20excessive%20information%20to%0Aagents%20of%20unknown%20identities.%20Our%20results%20point%20to%20a%20weakness%20of%20contemporary%0ALLMs%2C%20including%20GPT-4%2C%20GPT-4o%2C%20Gemini%201.5%2C%20and%20Claude%203.5%20Sonnet%2C%20in%20strategic%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Strategically%2520Reveal%252C%2520Conceal%252C%2520and%2520Infer%2520Information%253F%2520A%250A%2520%2520Theoretical%2520and%2520Empirical%2520Analysis%2520in%2520The%2520Chameleon%2520Game%26entry.906535625%3DMustafa%2520O.%2520Karabag%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520Large%2520language%2520model-based%2520%2528LLM-based%2529%2520agents%2520have%2520become%2520common%2520in%2520settings%250Athat%2520include%2520non-cooperative%2520parties.%2520In%2520such%2520settings%252C%2520agents%2527%2520decision-making%250Aneeds%2520to%2520conceal%2520information%2520from%2520their%2520adversaries%252C%2520reveal%2520information%2520to%250Atheir%2520cooperators%252C%2520and%2520infer%2520information%2520to%2520identify%2520the%2520other%2520agents%2527%250Acharacteristics.%2520To%2520investigate%2520whether%2520LLMs%2520have%2520these%2520information%2520control%2520and%250Adecision-making%2520capabilities%252C%2520we%2520make%2520LLM%2520agents%2520play%2520the%2520language-based%250Ahidden-identity%2520game%252C%2520The%2520Chameleon.%2520In%2520the%2520game%252C%2520a%2520group%2520of%2520non-chameleon%250Aagents%2520who%2520do%2520not%2520know%2520each%2520other%2520aim%2520to%2520identify%2520the%2520chameleon%2520agent%2520without%250Arevealing%2520a%2520secret.%2520The%2520game%2520requires%2520the%2520aforementioned%2520information%2520control%250Acapabilities%2520both%2520as%2520a%2520chameleon%2520and%2520a%2520non-chameleon.%2520The%2520empirical%2520results%250Ashow%2520that%2520while%2520non-chameleon%2520LLM%2520agents%2520identify%2520the%2520chameleon%252C%2520they%2520fail%2520to%250Aconceal%2520the%2520secret%2520from%2520the%2520chameleon%252C%2520and%2520their%2520winning%2520probability%2520is%2520far%250Afrom%2520the%2520levels%2520of%2520even%2520trivial%2520strategies.%2520To%2520formally%2520explain%2520this%2520behavior%252C%250Awe%2520give%2520a%2520theoretical%2520analysis%2520for%2520a%2520spectrum%2520of%2520strategies%252C%2520from%2520concealing%2520to%250Arevealing%252C%2520and%2520provide%2520bounds%2520on%2520the%2520non-chameleons%2527%2520winning%2520probability.%2520Based%250Aon%2520the%2520empirical%2520results%2520and%2520theoretical%2520analysis%2520of%2520different%2520strategies%252C%2520we%250Adeduce%2520that%2520LLM-based%2520non-chameleon%2520agents%2520reveal%2520excessive%2520information%2520to%250Aagents%2520of%2520unknown%2520identities.%2520Our%2520results%2520point%2520to%2520a%2520weakness%2520of%2520contemporary%250ALLMs%252C%2520including%2520GPT-4%252C%2520GPT-4o%252C%2520Gemini%25201.5%252C%2520and%2520Claude%25203.5%2520Sonnet%252C%2520in%2520strategic%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Strategically%20Reveal%2C%20Conceal%2C%20and%20Infer%20Information%3F%20A%0A%20%20Theoretical%20and%20Empirical%20Analysis%20in%20The%20Chameleon%20Game&entry.906535625=Mustafa%20O.%20Karabag%20and%20Ufuk%20Topcu&entry.1292438233=%20%20Large%20language%20model-based%20%28LLM-based%29%20agents%20have%20become%20common%20in%20settings%0Athat%20include%20non-cooperative%20parties.%20In%20such%20settings%2C%20agents%27%20decision-making%0Aneeds%20to%20conceal%20information%20from%20their%20adversaries%2C%20reveal%20information%20to%0Atheir%20cooperators%2C%20and%20infer%20information%20to%20identify%20the%20other%20agents%27%0Acharacteristics.%20To%20investigate%20whether%20LLMs%20have%20these%20information%20control%20and%0Adecision-making%20capabilities%2C%20we%20make%20LLM%20agents%20play%20the%20language-based%0Ahidden-identity%20game%2C%20The%20Chameleon.%20In%20the%20game%2C%20a%20group%20of%20non-chameleon%0Aagents%20who%20do%20not%20know%20each%20other%20aim%20to%20identify%20the%20chameleon%20agent%20without%0Arevealing%20a%20secret.%20The%20game%20requires%20the%20aforementioned%20information%20control%0Acapabilities%20both%20as%20a%20chameleon%20and%20a%20non-chameleon.%20The%20empirical%20results%0Ashow%20that%20while%20non-chameleon%20LLM%20agents%20identify%20the%20chameleon%2C%20they%20fail%20to%0Aconceal%20the%20secret%20from%20the%20chameleon%2C%20and%20their%20winning%20probability%20is%20far%0Afrom%20the%20levels%20of%20even%20trivial%20strategies.%20To%20formally%20explain%20this%20behavior%2C%0Awe%20give%20a%20theoretical%20analysis%20for%20a%20spectrum%20of%20strategies%2C%20from%20concealing%20to%0Arevealing%2C%20and%20provide%20bounds%20on%20the%20non-chameleons%27%20winning%20probability.%20Based%0Aon%20the%20empirical%20results%20and%20theoretical%20analysis%20of%20different%20strategies%2C%20we%0Adeduce%20that%20LLM-based%20non-chameleon%20agents%20reveal%20excessive%20information%20to%0Aagents%20of%20unknown%20identities.%20Our%20results%20point%20to%20a%20weakness%20of%20contemporary%0ALLMs%2C%20including%20GPT-4%2C%20GPT-4o%2C%20Gemini%201.5%2C%20and%20Claude%203.5%20Sonnet%2C%20in%20strategic%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19398v1&entry.124074799=Read"},
{"title": "Rethinking Early Stopping: Refine, Then Calibrate", "author": "Eug\u00e8ne Berta and David Holzm\u00fcller and Michael I. Jordan and Francis Bach", "abstract": "  Machine learning classifiers often produce probabilistic predictions that are\ncritical for accurate and interpretable decision-making in various domains. The\nquality of these predictions is generally evaluated with proper losses like\ncross-entropy, which decompose into two components: calibration error assesses\ngeneral under/overconfidence, while refinement error measures the ability to\ndistinguish different classes. In this paper, we provide theoretical and\nempirical evidence that these two errors are not minimized simultaneously\nduring training. Selecting the best training epoch based on validation loss\nthus leads to a compromise point that is suboptimal for both calibration error\nand, most importantly, refinement error. To address this, we introduce a new\nmetric for early stopping and hyperparameter tuning that makes it possible to\nminimize refinement error during training. The calibration error is minimized\nafter training, using standard techniques. Our method integrates seamlessly\nwith any architecture and consistently improves performance across diverse\nclassification tasks.\n", "link": "http://arxiv.org/abs/2501.19195v1", "date": "2025-01-31", "relevancy": 2.3532, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4706}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Early%20Stopping%3A%20Refine%2C%20Then%20Calibrate&body=Title%3A%20Rethinking%20Early%20Stopping%3A%20Refine%2C%20Then%20Calibrate%0AAuthor%3A%20Eug%C3%A8ne%20Berta%20and%20David%20Holzm%C3%BCller%20and%20Michael%20I.%20Jordan%20and%20Francis%20Bach%0AAbstract%3A%20%20%20Machine%20learning%20classifiers%20often%20produce%20probabilistic%20predictions%20that%20are%0Acritical%20for%20accurate%20and%20interpretable%20decision-making%20in%20various%20domains.%20The%0Aquality%20of%20these%20predictions%20is%20generally%20evaluated%20with%20proper%20losses%20like%0Across-entropy%2C%20which%20decompose%20into%20two%20components%3A%20calibration%20error%20assesses%0Ageneral%20under/overconfidence%2C%20while%20refinement%20error%20measures%20the%20ability%20to%0Adistinguish%20different%20classes.%20In%20this%20paper%2C%20we%20provide%20theoretical%20and%0Aempirical%20evidence%20that%20these%20two%20errors%20are%20not%20minimized%20simultaneously%0Aduring%20training.%20Selecting%20the%20best%20training%20epoch%20based%20on%20validation%20loss%0Athus%20leads%20to%20a%20compromise%20point%20that%20is%20suboptimal%20for%20both%20calibration%20error%0Aand%2C%20most%20importantly%2C%20refinement%20error.%20To%20address%20this%2C%20we%20introduce%20a%20new%0Ametric%20for%20early%20stopping%20and%20hyperparameter%20tuning%20that%20makes%20it%20possible%20to%0Aminimize%20refinement%20error%20during%20training.%20The%20calibration%20error%20is%20minimized%0Aafter%20training%2C%20using%20standard%20techniques.%20Our%20method%20integrates%20seamlessly%0Awith%20any%20architecture%20and%20consistently%20improves%20performance%20across%20diverse%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Early%2520Stopping%253A%2520Refine%252C%2520Then%2520Calibrate%26entry.906535625%3DEug%25C3%25A8ne%2520Berta%2520and%2520David%2520Holzm%25C3%25BCller%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520Machine%2520learning%2520classifiers%2520often%2520produce%2520probabilistic%2520predictions%2520that%2520are%250Acritical%2520for%2520accurate%2520and%2520interpretable%2520decision-making%2520in%2520various%2520domains.%2520The%250Aquality%2520of%2520these%2520predictions%2520is%2520generally%2520evaluated%2520with%2520proper%2520losses%2520like%250Across-entropy%252C%2520which%2520decompose%2520into%2520two%2520components%253A%2520calibration%2520error%2520assesses%250Ageneral%2520under/overconfidence%252C%2520while%2520refinement%2520error%2520measures%2520the%2520ability%2520to%250Adistinguish%2520different%2520classes.%2520In%2520this%2520paper%252C%2520we%2520provide%2520theoretical%2520and%250Aempirical%2520evidence%2520that%2520these%2520two%2520errors%2520are%2520not%2520minimized%2520simultaneously%250Aduring%2520training.%2520Selecting%2520the%2520best%2520training%2520epoch%2520based%2520on%2520validation%2520loss%250Athus%2520leads%2520to%2520a%2520compromise%2520point%2520that%2520is%2520suboptimal%2520for%2520both%2520calibration%2520error%250Aand%252C%2520most%2520importantly%252C%2520refinement%2520error.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%250Ametric%2520for%2520early%2520stopping%2520and%2520hyperparameter%2520tuning%2520that%2520makes%2520it%2520possible%2520to%250Aminimize%2520refinement%2520error%2520during%2520training.%2520The%2520calibration%2520error%2520is%2520minimized%250Aafter%2520training%252C%2520using%2520standard%2520techniques.%2520Our%2520method%2520integrates%2520seamlessly%250Awith%2520any%2520architecture%2520and%2520consistently%2520improves%2520performance%2520across%2520diverse%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Early%20Stopping%3A%20Refine%2C%20Then%20Calibrate&entry.906535625=Eug%C3%A8ne%20Berta%20and%20David%20Holzm%C3%BCller%20and%20Michael%20I.%20Jordan%20and%20Francis%20Bach&entry.1292438233=%20%20Machine%20learning%20classifiers%20often%20produce%20probabilistic%20predictions%20that%20are%0Acritical%20for%20accurate%20and%20interpretable%20decision-making%20in%20various%20domains.%20The%0Aquality%20of%20these%20predictions%20is%20generally%20evaluated%20with%20proper%20losses%20like%0Across-entropy%2C%20which%20decompose%20into%20two%20components%3A%20calibration%20error%20assesses%0Ageneral%20under/overconfidence%2C%20while%20refinement%20error%20measures%20the%20ability%20to%0Adistinguish%20different%20classes.%20In%20this%20paper%2C%20we%20provide%20theoretical%20and%0Aempirical%20evidence%20that%20these%20two%20errors%20are%20not%20minimized%20simultaneously%0Aduring%20training.%20Selecting%20the%20best%20training%20epoch%20based%20on%20validation%20loss%0Athus%20leads%20to%20a%20compromise%20point%20that%20is%20suboptimal%20for%20both%20calibration%20error%0Aand%2C%20most%20importantly%2C%20refinement%20error.%20To%20address%20this%2C%20we%20introduce%20a%20new%0Ametric%20for%20early%20stopping%20and%20hyperparameter%20tuning%20that%20makes%20it%20possible%20to%0Aminimize%20refinement%20error%20during%20training.%20The%20calibration%20error%20is%20minimized%0Aafter%20training%2C%20using%20standard%20techniques.%20Our%20method%20integrates%20seamlessly%0Awith%20any%20architecture%20and%20consistently%20improves%20performance%20across%20diverse%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19195v1&entry.124074799=Read"},
{"title": "Efficient Reasoning with Hidden Thinking", "author": "Xuan Shen and Yizhou Wang and Xiangxi Shi and Yanzhi Wang and Pu Zhao and Jiuxiang Gu", "abstract": "  Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.\n", "link": "http://arxiv.org/abs/2501.19201v1", "date": "2025-01-31", "relevancy": 2.353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Reasoning%20with%20Hidden%20Thinking&body=Title%3A%20Efficient%20Reasoning%20with%20Hidden%20Thinking%0AAuthor%3A%20Xuan%20Shen%20and%20Yizhou%20Wang%20and%20Xiangxi%20Shi%20and%20Yanzhi%20Wang%20and%20Pu%20Zhao%20and%20Jiuxiang%20Gu%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20become%20a%20powerful%20framework%20for%0Aimproving%20complex%20problem-solving%20capabilities%20in%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20However%2C%20the%20verbose%20nature%20of%20textual%20reasoning%20introduces%0Asignificant%20inefficiencies.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextbf%7BHeima%7D%24%20%28as%0Ahidden%20llama%29%2C%20an%20efficient%20reasoning%20framework%20that%20leverages%20reasoning%20CoTs%0Aat%20hidden%20latent%20space.%20We%20design%20the%20Heima%20Encoder%20to%20condense%20each%0Aintermediate%20CoT%20into%20a%20compact%2C%20higher-level%20hidden%20representation%20using%20a%0Asingle%20thinking%20token%2C%20effectively%20minimizing%20verbosity%20and%20reducing%20the%0Aoverall%20number%20of%20tokens%20required%20during%20the%20reasoning%20process.%20Meanwhile%2C%20we%0Adesign%20corresponding%20Heima%20Decoder%20with%20traditional%20Large%20Language%20Models%0A%28LLMs%29%20to%20adaptively%20interpret%20the%20hidden%20representations%20into%20variable-length%0Atextual%20sequence%2C%20reconstructing%20reasoning%20processes%20that%20closely%20resemble%20the%0Aoriginal%20CoTs.%20Experimental%20results%20across%20diverse%20reasoning%20MLLM%20benchmarks%0Ademonstrate%20that%20Heima%20model%20achieves%20higher%20generation%20efficiency%20while%0Amaintaining%20or%20even%20better%20zero-shot%20task%20accuracy.%20Moreover%2C%20the%20effective%0Areconstruction%20of%20multimodal%20reasoning%20processes%20with%20Heima%20Decoder%20validates%0Aboth%20the%20robustness%20and%20interpretability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Reasoning%2520with%2520Hidden%2520Thinking%26entry.906535625%3DXuan%2520Shen%2520and%2520Yizhou%2520Wang%2520and%2520Xiangxi%2520Shi%2520and%2520Yanzhi%2520Wang%2520and%2520Pu%2520Zhao%2520and%2520Jiuxiang%2520Gu%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520has%2520become%2520a%2520powerful%2520framework%2520for%250Aimproving%2520complex%2520problem-solving%2520capabilities%2520in%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529.%2520However%252C%2520the%2520verbose%2520nature%2520of%2520textual%2520reasoning%2520introduces%250Asignificant%2520inefficiencies.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2524%255Ctextbf%257BHeima%257D%2524%2520%2528as%250Ahidden%2520llama%2529%252C%2520an%2520efficient%2520reasoning%2520framework%2520that%2520leverages%2520reasoning%2520CoTs%250Aat%2520hidden%2520latent%2520space.%2520We%2520design%2520the%2520Heima%2520Encoder%2520to%2520condense%2520each%250Aintermediate%2520CoT%2520into%2520a%2520compact%252C%2520higher-level%2520hidden%2520representation%2520using%2520a%250Asingle%2520thinking%2520token%252C%2520effectively%2520minimizing%2520verbosity%2520and%2520reducing%2520the%250Aoverall%2520number%2520of%2520tokens%2520required%2520during%2520the%2520reasoning%2520process.%2520Meanwhile%252C%2520we%250Adesign%2520corresponding%2520Heima%2520Decoder%2520with%2520traditional%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520adaptively%2520interpret%2520the%2520hidden%2520representations%2520into%2520variable-length%250Atextual%2520sequence%252C%2520reconstructing%2520reasoning%2520processes%2520that%2520closely%2520resemble%2520the%250Aoriginal%2520CoTs.%2520Experimental%2520results%2520across%2520diverse%2520reasoning%2520MLLM%2520benchmarks%250Ademonstrate%2520that%2520Heima%2520model%2520achieves%2520higher%2520generation%2520efficiency%2520while%250Amaintaining%2520or%2520even%2520better%2520zero-shot%2520task%2520accuracy.%2520Moreover%252C%2520the%2520effective%250Areconstruction%2520of%2520multimodal%2520reasoning%2520processes%2520with%2520Heima%2520Decoder%2520validates%250Aboth%2520the%2520robustness%2520and%2520interpretability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Reasoning%20with%20Hidden%20Thinking&entry.906535625=Xuan%20Shen%20and%20Yizhou%20Wang%20and%20Xiangxi%20Shi%20and%20Yanzhi%20Wang%20and%20Pu%20Zhao%20and%20Jiuxiang%20Gu&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20become%20a%20powerful%20framework%20for%0Aimproving%20complex%20problem-solving%20capabilities%20in%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20However%2C%20the%20verbose%20nature%20of%20textual%20reasoning%20introduces%0Asignificant%20inefficiencies.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextbf%7BHeima%7D%24%20%28as%0Ahidden%20llama%29%2C%20an%20efficient%20reasoning%20framework%20that%20leverages%20reasoning%20CoTs%0Aat%20hidden%20latent%20space.%20We%20design%20the%20Heima%20Encoder%20to%20condense%20each%0Aintermediate%20CoT%20into%20a%20compact%2C%20higher-level%20hidden%20representation%20using%20a%0Asingle%20thinking%20token%2C%20effectively%20minimizing%20verbosity%20and%20reducing%20the%0Aoverall%20number%20of%20tokens%20required%20during%20the%20reasoning%20process.%20Meanwhile%2C%20we%0Adesign%20corresponding%20Heima%20Decoder%20with%20traditional%20Large%20Language%20Models%0A%28LLMs%29%20to%20adaptively%20interpret%20the%20hidden%20representations%20into%20variable-length%0Atextual%20sequence%2C%20reconstructing%20reasoning%20processes%20that%20closely%20resemble%20the%0Aoriginal%20CoTs.%20Experimental%20results%20across%20diverse%20reasoning%20MLLM%20benchmarks%0Ademonstrate%20that%20Heima%20model%20achieves%20higher%20generation%20efficiency%20while%0Amaintaining%20or%20even%20better%20zero-shot%20task%20accuracy.%20Moreover%2C%20the%20effective%0Areconstruction%20of%20multimodal%20reasoning%20processes%20with%20Heima%20Decoder%20validates%0Aboth%20the%20robustness%20and%20interpretability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19201v1&entry.124074799=Read"},
{"title": "PixelWorld: Towards Perceiving Everything as Pixels", "author": "Zhiheng Lyu and Xueguang Ma and Wenhu Chen", "abstract": "  Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2501.19339v1", "date": "2025-01-31", "relevancy": 2.3464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelWorld%3A%20Towards%20Perceiving%20Everything%20as%20Pixels&body=Title%3A%20PixelWorld%3A%20Towards%20Perceiving%20Everything%20as%20Pixels%0AAuthor%3A%20Zhiheng%20Lyu%20and%20Xueguang%20Ma%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Existing%20foundation%20models%20typically%20process%20visual%20input%20as%20pixels%20and%0Atextual%20input%20as%20tokens%2C%20a%20paradigm%20that%20contrasts%20with%20human%20perception%2C%20where%0Aboth%20modalities%20are%20processed%20in%20a%20unified%20manner.%20With%20the%20rise%20of%20embodied%0Aand%20agentic%20AI%2C%20where%20inputs%20primarily%20come%20from%20camera%20pixels%2C%20the%20need%20for%20a%0Aunified%20perception%20framework%20becomes%20increasingly%20evident.%20In%20this%20paper%2C%20we%0Apropose%20to%20unify%20all%20modalities%20%28text%2C%20tables%2C%20code%2C%20diagrams%2C%20images%2C%20etc%29%20as%0Apixel%20inputs%2C%20i.e.%20%22Perceive%20Everything%20as%20Pixels%22%20%28PEAP%29.%20We%20introduce%0APixelWorld%2C%20a%20novel%20evaluation%20suite%20that%20unifies%20all%20the%20mentioned%20modalities%0Ainto%20pixel%20space%20to%20gauge%20the%20existing%20models%27%20performance.%20Our%20findings%20show%0Athat%20%281%29%20PEAP%20outperforms%20baseline%20with%20token-based%20input%20in%20multimodal%0Adatasets%2C%20benefiting%20from%20unified%20input%20for%20better%20disambiguation%2C%20%282%29%0Asignificant%20declines%20in%20reasoning%20and%20coding%20capabilities%20across%20all%20models%0Awhen%20processing%20pixel-based%20input%2C%20underscoring%20the%20need%20to%20enhance%20foundation%0Amodels%27%20perceptual%20abilities%2C%20%283%29%20larger%20models%20can%20maintain%20strong%20performance%0Aon%20non-reasoning%20tasks%20under%20PEAP%2C%20while%20smaller%20models%20like%20Phi-3.5-V%20suffer%0Asignificant%20performance%20degradation%2C%20%284%29%20the%20attention%20pattern%20of%20PEAP%20is%0Ahighly%20aligned%20with%20text%20token%20input%2C%20%285%29%20PEAP%20can%20be%20accelerated%20significantly%0Aby%20exploiting%20the%20spatial%20sparsity.%20We%20conclude%20that%20the%20existing%20frontier%0Amodels%20are%20competent%20in%20pixel%20perception%2C%20however%2C%20there%20is%20still%20headroom%20for%0Aimprovement.%20Our%20code%2C%20dataset%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelWorld%253A%2520Towards%2520Perceiving%2520Everything%2520as%2520Pixels%26entry.906535625%3DZhiheng%2520Lyu%2520and%2520Xueguang%2520Ma%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520foundation%2520models%2520typically%2520process%2520visual%2520input%2520as%2520pixels%2520and%250Atextual%2520input%2520as%2520tokens%252C%2520a%2520paradigm%2520that%2520contrasts%2520with%2520human%2520perception%252C%2520where%250Aboth%2520modalities%2520are%2520processed%2520in%2520a%2520unified%2520manner.%2520With%2520the%2520rise%2520of%2520embodied%250Aand%2520agentic%2520AI%252C%2520where%2520inputs%2520primarily%2520come%2520from%2520camera%2520pixels%252C%2520the%2520need%2520for%2520a%250Aunified%2520perception%2520framework%2520becomes%2520increasingly%2520evident.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520to%2520unify%2520all%2520modalities%2520%2528text%252C%2520tables%252C%2520code%252C%2520diagrams%252C%2520images%252C%2520etc%2529%2520as%250Apixel%2520inputs%252C%2520i.e.%2520%2522Perceive%2520Everything%2520as%2520Pixels%2522%2520%2528PEAP%2529.%2520We%2520introduce%250APixelWorld%252C%2520a%2520novel%2520evaluation%2520suite%2520that%2520unifies%2520all%2520the%2520mentioned%2520modalities%250Ainto%2520pixel%2520space%2520to%2520gauge%2520the%2520existing%2520models%2527%2520performance.%2520Our%2520findings%2520show%250Athat%2520%25281%2529%2520PEAP%2520outperforms%2520baseline%2520with%2520token-based%2520input%2520in%2520multimodal%250Adatasets%252C%2520benefiting%2520from%2520unified%2520input%2520for%2520better%2520disambiguation%252C%2520%25282%2529%250Asignificant%2520declines%2520in%2520reasoning%2520and%2520coding%2520capabilities%2520across%2520all%2520models%250Awhen%2520processing%2520pixel-based%2520input%252C%2520underscoring%2520the%2520need%2520to%2520enhance%2520foundation%250Amodels%2527%2520perceptual%2520abilities%252C%2520%25283%2529%2520larger%2520models%2520can%2520maintain%2520strong%2520performance%250Aon%2520non-reasoning%2520tasks%2520under%2520PEAP%252C%2520while%2520smaller%2520models%2520like%2520Phi-3.5-V%2520suffer%250Asignificant%2520performance%2520degradation%252C%2520%25284%2529%2520the%2520attention%2520pattern%2520of%2520PEAP%2520is%250Ahighly%2520aligned%2520with%2520text%2520token%2520input%252C%2520%25285%2529%2520PEAP%2520can%2520be%2520accelerated%2520significantly%250Aby%2520exploiting%2520the%2520spatial%2520sparsity.%2520We%2520conclude%2520that%2520the%2520existing%2520frontier%250Amodels%2520are%2520competent%2520in%2520pixel%2520perception%252C%2520however%252C%2520there%2520is%2520still%2520headroom%2520for%250Aimprovement.%2520Our%2520code%252C%2520dataset%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelWorld%3A%20Towards%20Perceiving%20Everything%20as%20Pixels&entry.906535625=Zhiheng%20Lyu%20and%20Xueguang%20Ma%20and%20Wenhu%20Chen&entry.1292438233=%20%20Existing%20foundation%20models%20typically%20process%20visual%20input%20as%20pixels%20and%0Atextual%20input%20as%20tokens%2C%20a%20paradigm%20that%20contrasts%20with%20human%20perception%2C%20where%0Aboth%20modalities%20are%20processed%20in%20a%20unified%20manner.%20With%20the%20rise%20of%20embodied%0Aand%20agentic%20AI%2C%20where%20inputs%20primarily%20come%20from%20camera%20pixels%2C%20the%20need%20for%20a%0Aunified%20perception%20framework%20becomes%20increasingly%20evident.%20In%20this%20paper%2C%20we%0Apropose%20to%20unify%20all%20modalities%20%28text%2C%20tables%2C%20code%2C%20diagrams%2C%20images%2C%20etc%29%20as%0Apixel%20inputs%2C%20i.e.%20%22Perceive%20Everything%20as%20Pixels%22%20%28PEAP%29.%20We%20introduce%0APixelWorld%2C%20a%20novel%20evaluation%20suite%20that%20unifies%20all%20the%20mentioned%20modalities%0Ainto%20pixel%20space%20to%20gauge%20the%20existing%20models%27%20performance.%20Our%20findings%20show%0Athat%20%281%29%20PEAP%20outperforms%20baseline%20with%20token-based%20input%20in%20multimodal%0Adatasets%2C%20benefiting%20from%20unified%20input%20for%20better%20disambiguation%2C%20%282%29%0Asignificant%20declines%20in%20reasoning%20and%20coding%20capabilities%20across%20all%20models%0Awhen%20processing%20pixel-based%20input%2C%20underscoring%20the%20need%20to%20enhance%20foundation%0Amodels%27%20perceptual%20abilities%2C%20%283%29%20larger%20models%20can%20maintain%20strong%20performance%0Aon%20non-reasoning%20tasks%20under%20PEAP%2C%20while%20smaller%20models%20like%20Phi-3.5-V%20suffer%0Asignificant%20performance%20degradation%2C%20%284%29%20the%20attention%20pattern%20of%20PEAP%20is%0Ahighly%20aligned%20with%20text%20token%20input%2C%20%285%29%20PEAP%20can%20be%20accelerated%20significantly%0Aby%20exploiting%20the%20spatial%20sparsity.%20We%20conclude%20that%20the%20existing%20frontier%0Amodels%20are%20competent%20in%20pixel%20perception%2C%20however%2C%20there%20is%20still%20headroom%20for%0Aimprovement.%20Our%20code%2C%20dataset%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19339v1&entry.124074799=Read"},
{"title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models", "author": "Ruiyu Wang and Yu Yuan and Shizhao Sun and Jiang Bian", "abstract": "  Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2501.19054v1", "date": "2025-01-31", "relevancy": 2.3439, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Ruiyu%20Wang%20and%20Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Creating%20Computer-Aided%20Design%20%28CAD%29%20models%20requires%20significant%20expertise%0Aand%20effort.%20Text-to-CAD%2C%20which%20converts%20textual%20descriptions%20into%20CAD%0Aparametric%20sequences%2C%20is%20crucial%20in%20streamlining%20this%20process.%20Recent%20studies%0Ahave%20utilized%20ground-truth%20parametric%20sequences%2C%20known%20as%20sequential%20signals%2C%0Aas%20supervision%20to%20achieve%20this%20goal.%20However%2C%20CAD%20models%20are%20inherently%0Amultimodal%2C%20comprising%20parametric%20sequences%20and%20corresponding%20rendered%20visual%0Aobjects.%20Besides%2Cthe%20rendering%20process%20from%20parametric%20sequences%20to%20visual%0Aobjects%20is%20many-to-one.%20Therefore%2C%20both%20sequential%20and%20visual%20signals%20are%0Acritical%20for%20effective%20training.%20In%20this%20work%2C%20we%20introduce%20CADFusion%2C%20a%0Aframework%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20as%20the%20backbone%20and%20alternates%0Abetween%20two%20training%20stages%3A%20the%20sequential%20learning%20%28SL%29%20stage%20and%20the%20visual%0Afeedback%20%28VF%29%20stage.%20In%20the%20SL%20stage%2C%20we%20train%20LLMs%20using%20ground-truth%0Aparametric%20sequences%2C%20enabling%20the%20generation%20of%20logically%20coherent%20parametric%0Asequences.%20In%20the%20VF%20stage%2C%20we%20reward%20parametric%20sequences%20that%20render%20into%0Avisually%20preferred%20objects%20and%20penalize%20those%20that%20do%20not%2C%20allowing%20LLMs%20to%0Alearn%20how%20rendered%20visual%20objects%20are%20perceived%20and%20evaluated.%20These%20two%20stages%0Aalternate%20throughout%20the%20training%2C%20ensuring%20balanced%20learning%20and%20preserving%0Abenefits%20of%20both%20signals.%20Experiments%20demonstrate%20that%20CADFusion%20significantly%0Aimproves%20performance%2C%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-CAD%2520Generation%2520Through%2520Infusing%2520Visual%2520Feedback%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DRuiyu%2520Wang%2520and%2520Yu%2520Yuan%2520and%2520Shizhao%2520Sun%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Creating%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520models%2520requires%2520significant%2520expertise%250Aand%2520effort.%2520Text-to-CAD%252C%2520which%2520converts%2520textual%2520descriptions%2520into%2520CAD%250Aparametric%2520sequences%252C%2520is%2520crucial%2520in%2520streamlining%2520this%2520process.%2520Recent%2520studies%250Ahave%2520utilized%2520ground-truth%2520parametric%2520sequences%252C%2520known%2520as%2520sequential%2520signals%252C%250Aas%2520supervision%2520to%2520achieve%2520this%2520goal.%2520However%252C%2520CAD%2520models%2520are%2520inherently%250Amultimodal%252C%2520comprising%2520parametric%2520sequences%2520and%2520corresponding%2520rendered%2520visual%250Aobjects.%2520Besides%252Cthe%2520rendering%2520process%2520from%2520parametric%2520sequences%2520to%2520visual%250Aobjects%2520is%2520many-to-one.%2520Therefore%252C%2520both%2520sequential%2520and%2520visual%2520signals%2520are%250Acritical%2520for%2520effective%2520training.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CADFusion%252C%2520a%250Aframework%2520that%2520uses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520the%2520backbone%2520and%2520alternates%250Abetween%2520two%2520training%2520stages%253A%2520the%2520sequential%2520learning%2520%2528SL%2529%2520stage%2520and%2520the%2520visual%250Afeedback%2520%2528VF%2529%2520stage.%2520In%2520the%2520SL%2520stage%252C%2520we%2520train%2520LLMs%2520using%2520ground-truth%250Aparametric%2520sequences%252C%2520enabling%2520the%2520generation%2520of%2520logically%2520coherent%2520parametric%250Asequences.%2520In%2520the%2520VF%2520stage%252C%2520we%2520reward%2520parametric%2520sequences%2520that%2520render%2520into%250Avisually%2520preferred%2520objects%2520and%2520penalize%2520those%2520that%2520do%2520not%252C%2520allowing%2520LLMs%2520to%250Alearn%2520how%2520rendered%2520visual%2520objects%2520are%2520perceived%2520and%2520evaluated.%2520These%2520two%2520stages%250Aalternate%2520throughout%2520the%2520training%252C%2520ensuring%2520balanced%2520learning%2520and%2520preserving%250Abenefits%2520of%2520both%2520signals.%2520Experiments%2520demonstrate%2520that%2520CADFusion%2520significantly%250Aimproves%2520performance%252C%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models&entry.906535625=Ruiyu%20Wang%20and%20Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Jiang%20Bian&entry.1292438233=%20%20Creating%20Computer-Aided%20Design%20%28CAD%29%20models%20requires%20significant%20expertise%0Aand%20effort.%20Text-to-CAD%2C%20which%20converts%20textual%20descriptions%20into%20CAD%0Aparametric%20sequences%2C%20is%20crucial%20in%20streamlining%20this%20process.%20Recent%20studies%0Ahave%20utilized%20ground-truth%20parametric%20sequences%2C%20known%20as%20sequential%20signals%2C%0Aas%20supervision%20to%20achieve%20this%20goal.%20However%2C%20CAD%20models%20are%20inherently%0Amultimodal%2C%20comprising%20parametric%20sequences%20and%20corresponding%20rendered%20visual%0Aobjects.%20Besides%2Cthe%20rendering%20process%20from%20parametric%20sequences%20to%20visual%0Aobjects%20is%20many-to-one.%20Therefore%2C%20both%20sequential%20and%20visual%20signals%20are%0Acritical%20for%20effective%20training.%20In%20this%20work%2C%20we%20introduce%20CADFusion%2C%20a%0Aframework%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20as%20the%20backbone%20and%20alternates%0Abetween%20two%20training%20stages%3A%20the%20sequential%20learning%20%28SL%29%20stage%20and%20the%20visual%0Afeedback%20%28VF%29%20stage.%20In%20the%20SL%20stage%2C%20we%20train%20LLMs%20using%20ground-truth%0Aparametric%20sequences%2C%20enabling%20the%20generation%20of%20logically%20coherent%20parametric%0Asequences.%20In%20the%20VF%20stage%2C%20we%20reward%20parametric%20sequences%20that%20render%20into%0Avisually%20preferred%20objects%20and%20penalize%20those%20that%20do%20not%2C%20allowing%20LLMs%20to%0Alearn%20how%20rendered%20visual%20objects%20are%20perceived%20and%20evaluated.%20These%20two%20stages%0Aalternate%20throughout%20the%20training%2C%20ensuring%20balanced%20learning%20and%20preserving%0Abenefits%20of%20both%20signals.%20Experiments%20demonstrate%20that%20CADFusion%20significantly%0Aimproves%20performance%2C%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19054v1&entry.124074799=Read"},
{"title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling", "author": "Jiefeng Chen and Jie Ren and Xinyun Chen and Chengrun Yang and Ruoxi Sun and Sercan \u00d6 Ar\u0131k", "abstract": "  Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.\n", "link": "http://arxiv.org/abs/2501.19306v1", "date": "2025-01-31", "relevancy": 2.336, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SETS%3A%20Leveraging%20Self-Verification%20and%20Self-Correction%20for%20Improved%0A%20%20Test-Time%20Scaling&body=Title%3A%20SETS%3A%20Leveraging%20Self-Verification%20and%20Self-Correction%20for%20Improved%0A%20%20Test-Time%20Scaling%0AAuthor%3A%20Jiefeng%20Chen%20and%20Jie%20Ren%20and%20Xinyun%20Chen%20and%20Chengrun%20Yang%20and%20Ruoxi%20Sun%20and%20Sercan%20%C3%96%20Ar%C4%B1k%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20created%20new%0Aopportunities%20to%20enhance%20performance%20on%20complex%20reasoning%20tasks%20by%20leveraging%0Atest-time%20computation.%20However%2C%20conventional%20approaches%20such%20as%20repeated%0Asampling%20with%20majority%20voting%20or%20reward%20model%20scoring%2C%20often%20face%20diminishing%0Areturns%20as%20test-time%20compute%20scales%2C%20in%20addition%20to%20requiring%20costly%0Atask-specific%20reward%20model%20training.%20In%20this%20paper%2C%20we%20present%20Self-Enhanced%0ATest-Time%20Scaling%20%28SETS%29%2C%20a%20novel%20method%20that%20leverages%20the%20self-verification%0Aand%20self-correction%20capabilities%20of%20recent%20advanced%20LLMs%20to%20overcome%20these%0Alimitations.%20SETS%20integrates%20sampling%2C%20self-verification%2C%20and%20self-correction%0Ainto%20a%20unified%20framework%2C%20enabling%20efficient%20and%20scalable%20test-time%20computation%0Afor%20improved%20capabilities%20at%20complex%20tasks.%20Through%20extensive%20experiments%20on%0Achallenging%20planning%20and%20reasoning%20benchmarks%2C%20compared%20to%20the%20alternatives%2C%20we%0Ademonstrate%20that%20SETS%20achieves%20significant%20performance%20improvements%20and%20more%0Afavorable%20test-time%20scaling%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSETS%253A%2520Leveraging%2520Self-Verification%2520and%2520Self-Correction%2520for%2520Improved%250A%2520%2520Test-Time%2520Scaling%26entry.906535625%3DJiefeng%2520Chen%2520and%2520Jie%2520Ren%2520and%2520Xinyun%2520Chen%2520and%2520Chengrun%2520Yang%2520and%2520Ruoxi%2520Sun%2520and%2520Sercan%2520%25C3%2596%2520Ar%25C4%25B1k%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520created%2520new%250Aopportunities%2520to%2520enhance%2520performance%2520on%2520complex%2520reasoning%2520tasks%2520by%2520leveraging%250Atest-time%2520computation.%2520However%252C%2520conventional%2520approaches%2520such%2520as%2520repeated%250Asampling%2520with%2520majority%2520voting%2520or%2520reward%2520model%2520scoring%252C%2520often%2520face%2520diminishing%250Areturns%2520as%2520test-time%2520compute%2520scales%252C%2520in%2520addition%2520to%2520requiring%2520costly%250Atask-specific%2520reward%2520model%2520training.%2520In%2520this%2520paper%252C%2520we%2520present%2520Self-Enhanced%250ATest-Time%2520Scaling%2520%2528SETS%2529%252C%2520a%2520novel%2520method%2520that%2520leverages%2520the%2520self-verification%250Aand%2520self-correction%2520capabilities%2520of%2520recent%2520advanced%2520LLMs%2520to%2520overcome%2520these%250Alimitations.%2520SETS%2520integrates%2520sampling%252C%2520self-verification%252C%2520and%2520self-correction%250Ainto%2520a%2520unified%2520framework%252C%2520enabling%2520efficient%2520and%2520scalable%2520test-time%2520computation%250Afor%2520improved%2520capabilities%2520at%2520complex%2520tasks.%2520Through%2520extensive%2520experiments%2520on%250Achallenging%2520planning%2520and%2520reasoning%2520benchmarks%252C%2520compared%2520to%2520the%2520alternatives%252C%2520we%250Ademonstrate%2520that%2520SETS%2520achieves%2520significant%2520performance%2520improvements%2520and%2520more%250Afavorable%2520test-time%2520scaling%2520laws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SETS%3A%20Leveraging%20Self-Verification%20and%20Self-Correction%20for%20Improved%0A%20%20Test-Time%20Scaling&entry.906535625=Jiefeng%20Chen%20and%20Jie%20Ren%20and%20Xinyun%20Chen%20and%20Chengrun%20Yang%20and%20Ruoxi%20Sun%20and%20Sercan%20%C3%96%20Ar%C4%B1k&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20created%20new%0Aopportunities%20to%20enhance%20performance%20on%20complex%20reasoning%20tasks%20by%20leveraging%0Atest-time%20computation.%20However%2C%20conventional%20approaches%20such%20as%20repeated%0Asampling%20with%20majority%20voting%20or%20reward%20model%20scoring%2C%20often%20face%20diminishing%0Areturns%20as%20test-time%20compute%20scales%2C%20in%20addition%20to%20requiring%20costly%0Atask-specific%20reward%20model%20training.%20In%20this%20paper%2C%20we%20present%20Self-Enhanced%0ATest-Time%20Scaling%20%28SETS%29%2C%20a%20novel%20method%20that%20leverages%20the%20self-verification%0Aand%20self-correction%20capabilities%20of%20recent%20advanced%20LLMs%20to%20overcome%20these%0Alimitations.%20SETS%20integrates%20sampling%2C%20self-verification%2C%20and%20self-correction%0Ainto%20a%20unified%20framework%2C%20enabling%20efficient%20and%20scalable%20test-time%20computation%0Afor%20improved%20capabilities%20at%20complex%20tasks.%20Through%20extensive%20experiments%20on%0Achallenging%20planning%20and%20reasoning%20benchmarks%2C%20compared%20to%20the%20alternatives%2C%20we%0Ademonstrate%20that%20SETS%20achieves%20significant%20performance%20improvements%20and%20more%0Afavorable%20test-time%20scaling%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19306v1&entry.124074799=Read"},
{"title": "Fused-Planes: Improving Planar Representations for Learning Large Sets\n  of 3D Scenes", "author": "Karim Kassab and Antoine Schnepf and Jean-Yves Franceschi and Laurent Caraffa and Flavian Vasile and Jeremie Mary and Andrew Comport and Val\u00e9rie Gouet-Brunet", "abstract": "  To learn large sets of scenes, Tri-Planes are commonly employed for their\nplanar structure that enables an interoperability with image models, and thus\ndiverse 3D applications. However, this advantage comes at the cost of resource\nefficiency, as Tri-Planes are not the most computationally efficient option. In\nthis paper, we introduce Fused-Planes, a new planar architecture that improves\nTri-Planes resource-efficiency in the framework of learning large sets of\nscenes, which we call \"multi-scene inverse graphics\". To learn a large set of\nscenes, our method divides it into two subsets and operates as follows: (i) we\ntrain the first subset of scenes jointly with a compression model, (ii) we use\nthat compression model to learn the remaining scenes. This compression model\nconsists of a 3D-aware latent space in which Fused-Planes are learned, enabling\na reduced rendering resolution, and shared structures across scenes that reduce\nscene representation complexity. Fused-Planes present competitive resource\ncosts in multi-scene inverse graphics, while preserving Tri-Planes rendering\nquality, and maintaining their widely favored planar structure. Our codebase is\npublicly available as open-source. Our project page can be found at\nhttps://fused-planes.github.io .\n", "link": "http://arxiv.org/abs/2410.23742v2", "date": "2025-01-31", "relevancy": 2.334, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fused-Planes%3A%20Improving%20Planar%20Representations%20for%20Learning%20Large%20Sets%0A%20%20of%203D%20Scenes&body=Title%3A%20Fused-Planes%3A%20Improving%20Planar%20Representations%20for%20Learning%20Large%20Sets%0A%20%20of%203D%20Scenes%0AAuthor%3A%20Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet%0AAbstract%3A%20%20%20To%20learn%20large%20sets%20of%20scenes%2C%20Tri-Planes%20are%20commonly%20employed%20for%20their%0Aplanar%20structure%20that%20enables%20an%20interoperability%20with%20image%20models%2C%20and%20thus%0Adiverse%203D%20applications.%20However%2C%20this%20advantage%20comes%20at%20the%20cost%20of%20resource%0Aefficiency%2C%20as%20Tri-Planes%20are%20not%20the%20most%20computationally%20efficient%20option.%20In%0Athis%20paper%2C%20we%20introduce%20Fused-Planes%2C%20a%20new%20planar%20architecture%20that%20improves%0ATri-Planes%20resource-efficiency%20in%20the%20framework%20of%20learning%20large%20sets%20of%0Ascenes%2C%20which%20we%20call%20%22multi-scene%20inverse%20graphics%22.%20To%20learn%20a%20large%20set%20of%0Ascenes%2C%20our%20method%20divides%20it%20into%20two%20subsets%20and%20operates%20as%20follows%3A%20%28i%29%20we%0Atrain%20the%20first%20subset%20of%20scenes%20jointly%20with%20a%20compression%20model%2C%20%28ii%29%20we%20use%0Athat%20compression%20model%20to%20learn%20the%20remaining%20scenes.%20This%20compression%20model%0Aconsists%20of%20a%203D-aware%20latent%20space%20in%20which%20Fused-Planes%20are%20learned%2C%20enabling%0Aa%20reduced%20rendering%20resolution%2C%20and%20shared%20structures%20across%20scenes%20that%20reduce%0Ascene%20representation%20complexity.%20Fused-Planes%20present%20competitive%20resource%0Acosts%20in%20multi-scene%20inverse%20graphics%2C%20while%20preserving%20Tri-Planes%20rendering%0Aquality%2C%20and%20maintaining%20their%20widely%20favored%20planar%20structure.%20Our%20codebase%20is%0Apublicly%20available%20as%20open-source.%20Our%20project%20page%20can%20be%20found%20at%0Ahttps%3A//fused-planes.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFused-Planes%253A%2520Improving%2520Planar%2520Representations%2520for%2520Learning%2520Large%2520Sets%250A%2520%2520of%25203D%2520Scenes%26entry.906535625%3DKarim%2520Kassab%2520and%2520Antoine%2520Schnepf%2520and%2520Jean-Yves%2520Franceschi%2520and%2520Laurent%2520Caraffa%2520and%2520Flavian%2520Vasile%2520and%2520Jeremie%2520Mary%2520and%2520Andrew%2520Comport%2520and%2520Val%25C3%25A9rie%2520Gouet-Brunet%26entry.1292438233%3D%2520%2520To%2520learn%2520large%2520sets%2520of%2520scenes%252C%2520Tri-Planes%2520are%2520commonly%2520employed%2520for%2520their%250Aplanar%2520structure%2520that%2520enables%2520an%2520interoperability%2520with%2520image%2520models%252C%2520and%2520thus%250Adiverse%25203D%2520applications.%2520However%252C%2520this%2520advantage%2520comes%2520at%2520the%2520cost%2520of%2520resource%250Aefficiency%252C%2520as%2520Tri-Planes%2520are%2520not%2520the%2520most%2520computationally%2520efficient%2520option.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Fused-Planes%252C%2520a%2520new%2520planar%2520architecture%2520that%2520improves%250ATri-Planes%2520resource-efficiency%2520in%2520the%2520framework%2520of%2520learning%2520large%2520sets%2520of%250Ascenes%252C%2520which%2520we%2520call%2520%2522multi-scene%2520inverse%2520graphics%2522.%2520To%2520learn%2520a%2520large%2520set%2520of%250Ascenes%252C%2520our%2520method%2520divides%2520it%2520into%2520two%2520subsets%2520and%2520operates%2520as%2520follows%253A%2520%2528i%2529%2520we%250Atrain%2520the%2520first%2520subset%2520of%2520scenes%2520jointly%2520with%2520a%2520compression%2520model%252C%2520%2528ii%2529%2520we%2520use%250Athat%2520compression%2520model%2520to%2520learn%2520the%2520remaining%2520scenes.%2520This%2520compression%2520model%250Aconsists%2520of%2520a%25203D-aware%2520latent%2520space%2520in%2520which%2520Fused-Planes%2520are%2520learned%252C%2520enabling%250Aa%2520reduced%2520rendering%2520resolution%252C%2520and%2520shared%2520structures%2520across%2520scenes%2520that%2520reduce%250Ascene%2520representation%2520complexity.%2520Fused-Planes%2520present%2520competitive%2520resource%250Acosts%2520in%2520multi-scene%2520inverse%2520graphics%252C%2520while%2520preserving%2520Tri-Planes%2520rendering%250Aquality%252C%2520and%2520maintaining%2520their%2520widely%2520favored%2520planar%2520structure.%2520Our%2520codebase%2520is%250Apublicly%2520available%2520as%2520open-source.%2520Our%2520project%2520page%2520can%2520be%2520found%2520at%250Ahttps%253A//fused-planes.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fused-Planes%3A%20Improving%20Planar%20Representations%20for%20Learning%20Large%20Sets%0A%20%20of%203D%20Scenes&entry.906535625=Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet&entry.1292438233=%20%20To%20learn%20large%20sets%20of%20scenes%2C%20Tri-Planes%20are%20commonly%20employed%20for%20their%0Aplanar%20structure%20that%20enables%20an%20interoperability%20with%20image%20models%2C%20and%20thus%0Adiverse%203D%20applications.%20However%2C%20this%20advantage%20comes%20at%20the%20cost%20of%20resource%0Aefficiency%2C%20as%20Tri-Planes%20are%20not%20the%20most%20computationally%20efficient%20option.%20In%0Athis%20paper%2C%20we%20introduce%20Fused-Planes%2C%20a%20new%20planar%20architecture%20that%20improves%0ATri-Planes%20resource-efficiency%20in%20the%20framework%20of%20learning%20large%20sets%20of%0Ascenes%2C%20which%20we%20call%20%22multi-scene%20inverse%20graphics%22.%20To%20learn%20a%20large%20set%20of%0Ascenes%2C%20our%20method%20divides%20it%20into%20two%20subsets%20and%20operates%20as%20follows%3A%20%28i%29%20we%0Atrain%20the%20first%20subset%20of%20scenes%20jointly%20with%20a%20compression%20model%2C%20%28ii%29%20we%20use%0Athat%20compression%20model%20to%20learn%20the%20remaining%20scenes.%20This%20compression%20model%0Aconsists%20of%20a%203D-aware%20latent%20space%20in%20which%20Fused-Planes%20are%20learned%2C%20enabling%0Aa%20reduced%20rendering%20resolution%2C%20and%20shared%20structures%20across%20scenes%20that%20reduce%0Ascene%20representation%20complexity.%20Fused-Planes%20present%20competitive%20resource%0Acosts%20in%20multi-scene%20inverse%20graphics%2C%20while%20preserving%20Tri-Planes%20rendering%0Aquality%2C%20and%20maintaining%20their%20widely%20favored%20planar%20structure.%20Our%20codebase%20is%0Apublicly%20available%20as%20open-source.%20Our%20project%20page%20can%20be%20found%20at%0Ahttps%3A//fused-planes.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23742v2&entry.124074799=Read"},
{"title": "Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge", "author": "Amogh Joshi and Sourav Sanyal and Kaushik Roy", "abstract": "  The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time.\n", "link": "http://arxiv.org/abs/2501.19259v1", "date": "2025-01-31", "relevancy": 2.3258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5858}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5845}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-LIFT%3A%20A%20Neuromorphic%2C%20LLM-based%20Interactive%20Framework%20for%0A%20%20Autonomous%20Drone%20FlighT%20at%20the%20Edge&body=Title%3A%20Neuro-LIFT%3A%20A%20Neuromorphic%2C%20LLM-based%20Interactive%20Framework%20for%0A%20%20Autonomous%20Drone%20FlighT%20at%20the%20Edge%0AAuthor%3A%20Amogh%20Joshi%20and%20Sourav%20Sanyal%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20The%20integration%20of%20human-intuitive%20interactions%20into%20autonomous%20systems%20has%0Abeen%20limited.%20Traditional%20Natural%20Language%20Processing%20%28NLP%29%20systems%20struggle%0Awith%20context%20and%20intent%20understanding%2C%20severely%20restricting%20human-robot%0Ainteraction.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Atransformed%20this%20dynamic%2C%20allowing%20for%20intuitive%20and%20high-level%20communication%0Athrough%20speech%20and%20text%2C%20and%20bridging%20the%20gap%20between%20human%20commands%20and%0Arobotic%20actions.%20Additionally%2C%20autonomous%20navigation%20has%20emerged%20as%20a%20central%0Afocus%20in%20robotics%20research%2C%20with%20artificial%20intelligence%20%28AI%29%20increasingly%0Abeing%20leveraged%20to%20enhance%20these%20systems.%20However%2C%20existing%20AI-based%20navigation%0Aalgorithms%20face%20significant%20challenges%20in%20latency-critical%20tasks%20where%20rapid%0Adecision-making%20is%20critical.%20Traditional%20frame-based%20vision%20systems%2C%20while%0Aeffective%20for%20high-level%20decision-making%2C%20suffer%20from%20high%20energy%20consumption%0Aand%20latency%2C%20limiting%20their%20applicability%20in%20real-time%20scenarios.%20Neuromorphic%0Avision%20systems%2C%20combining%20event-based%20cameras%20and%20spiking%20neural%20networks%0A%28SNNs%29%2C%20offer%20a%20promising%20alternative%20by%20enabling%20energy-efficient%2C%20low-latency%0Anavigation.%20Despite%20their%20potential%2C%20real-world%20implementations%20of%20these%0Asystems%2C%20particularly%20on%20physical%20platforms%20such%20as%20drones%2C%20remain%20scarce.%20In%0Athis%20work%2C%20we%20present%20Neuro-LIFT%2C%20a%20real-time%20neuromorphic%20navigation%20framework%0Aimplemented%20on%20a%20Parrot%20Bebop2%20quadrotor.%20Leveraging%20an%20LLM%20for%20natural%0Alanguage%20processing%2C%20Neuro-LIFT%20translates%20human%20speech%20into%20high-level%0Aplanning%20commands%20which%20are%20then%20autonomously%20executed%20using%20event-based%0Aneuromorphic%20vision%20and%20physics-driven%20planning.%20Our%20framework%20demonstrates%20its%0Acapabilities%20in%20navigating%20in%20a%20dynamic%20environment%2C%20avoiding%20obstacles%2C%20and%0Aadapting%20to%20human%20instructions%20in%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-LIFT%253A%2520A%2520Neuromorphic%252C%2520LLM-based%2520Interactive%2520Framework%2520for%250A%2520%2520Autonomous%2520Drone%2520FlighT%2520at%2520the%2520Edge%26entry.906535625%3DAmogh%2520Joshi%2520and%2520Sourav%2520Sanyal%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520human-intuitive%2520interactions%2520into%2520autonomous%2520systems%2520has%250Abeen%2520limited.%2520Traditional%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520systems%2520struggle%250Awith%2520context%2520and%2520intent%2520understanding%252C%2520severely%2520restricting%2520human-robot%250Ainteraction.%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Atransformed%2520this%2520dynamic%252C%2520allowing%2520for%2520intuitive%2520and%2520high-level%2520communication%250Athrough%2520speech%2520and%2520text%252C%2520and%2520bridging%2520the%2520gap%2520between%2520human%2520commands%2520and%250Arobotic%2520actions.%2520Additionally%252C%2520autonomous%2520navigation%2520has%2520emerged%2520as%2520a%2520central%250Afocus%2520in%2520robotics%2520research%252C%2520with%2520artificial%2520intelligence%2520%2528AI%2529%2520increasingly%250Abeing%2520leveraged%2520to%2520enhance%2520these%2520systems.%2520However%252C%2520existing%2520AI-based%2520navigation%250Aalgorithms%2520face%2520significant%2520challenges%2520in%2520latency-critical%2520tasks%2520where%2520rapid%250Adecision-making%2520is%2520critical.%2520Traditional%2520frame-based%2520vision%2520systems%252C%2520while%250Aeffective%2520for%2520high-level%2520decision-making%252C%2520suffer%2520from%2520high%2520energy%2520consumption%250Aand%2520latency%252C%2520limiting%2520their%2520applicability%2520in%2520real-time%2520scenarios.%2520Neuromorphic%250Avision%2520systems%252C%2520combining%2520event-based%2520cameras%2520and%2520spiking%2520neural%2520networks%250A%2528SNNs%2529%252C%2520offer%2520a%2520promising%2520alternative%2520by%2520enabling%2520energy-efficient%252C%2520low-latency%250Anavigation.%2520Despite%2520their%2520potential%252C%2520real-world%2520implementations%2520of%2520these%250Asystems%252C%2520particularly%2520on%2520physical%2520platforms%2520such%2520as%2520drones%252C%2520remain%2520scarce.%2520In%250Athis%2520work%252C%2520we%2520present%2520Neuro-LIFT%252C%2520a%2520real-time%2520neuromorphic%2520navigation%2520framework%250Aimplemented%2520on%2520a%2520Parrot%2520Bebop2%2520quadrotor.%2520Leveraging%2520an%2520LLM%2520for%2520natural%250Alanguage%2520processing%252C%2520Neuro-LIFT%2520translates%2520human%2520speech%2520into%2520high-level%250Aplanning%2520commands%2520which%2520are%2520then%2520autonomously%2520executed%2520using%2520event-based%250Aneuromorphic%2520vision%2520and%2520physics-driven%2520planning.%2520Our%2520framework%2520demonstrates%2520its%250Acapabilities%2520in%2520navigating%2520in%2520a%2520dynamic%2520environment%252C%2520avoiding%2520obstacles%252C%2520and%250Aadapting%2520to%2520human%2520instructions%2520in%2520real-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-LIFT%3A%20A%20Neuromorphic%2C%20LLM-based%20Interactive%20Framework%20for%0A%20%20Autonomous%20Drone%20FlighT%20at%20the%20Edge&entry.906535625=Amogh%20Joshi%20and%20Sourav%20Sanyal%20and%20Kaushik%20Roy&entry.1292438233=%20%20The%20integration%20of%20human-intuitive%20interactions%20into%20autonomous%20systems%20has%0Abeen%20limited.%20Traditional%20Natural%20Language%20Processing%20%28NLP%29%20systems%20struggle%0Awith%20context%20and%20intent%20understanding%2C%20severely%20restricting%20human-robot%0Ainteraction.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Atransformed%20this%20dynamic%2C%20allowing%20for%20intuitive%20and%20high-level%20communication%0Athrough%20speech%20and%20text%2C%20and%20bridging%20the%20gap%20between%20human%20commands%20and%0Arobotic%20actions.%20Additionally%2C%20autonomous%20navigation%20has%20emerged%20as%20a%20central%0Afocus%20in%20robotics%20research%2C%20with%20artificial%20intelligence%20%28AI%29%20increasingly%0Abeing%20leveraged%20to%20enhance%20these%20systems.%20However%2C%20existing%20AI-based%20navigation%0Aalgorithms%20face%20significant%20challenges%20in%20latency-critical%20tasks%20where%20rapid%0Adecision-making%20is%20critical.%20Traditional%20frame-based%20vision%20systems%2C%20while%0Aeffective%20for%20high-level%20decision-making%2C%20suffer%20from%20high%20energy%20consumption%0Aand%20latency%2C%20limiting%20their%20applicability%20in%20real-time%20scenarios.%20Neuromorphic%0Avision%20systems%2C%20combining%20event-based%20cameras%20and%20spiking%20neural%20networks%0A%28SNNs%29%2C%20offer%20a%20promising%20alternative%20by%20enabling%20energy-efficient%2C%20low-latency%0Anavigation.%20Despite%20their%20potential%2C%20real-world%20implementations%20of%20these%0Asystems%2C%20particularly%20on%20physical%20platforms%20such%20as%20drones%2C%20remain%20scarce.%20In%0Athis%20work%2C%20we%20present%20Neuro-LIFT%2C%20a%20real-time%20neuromorphic%20navigation%20framework%0Aimplemented%20on%20a%20Parrot%20Bebop2%20quadrotor.%20Leveraging%20an%20LLM%20for%20natural%0Alanguage%20processing%2C%20Neuro-LIFT%20translates%20human%20speech%20into%20high-level%0Aplanning%20commands%20which%20are%20then%20autonomously%20executed%20using%20event-based%0Aneuromorphic%20vision%20and%20physics-driven%20planning.%20Our%20framework%20demonstrates%20its%0Acapabilities%20in%20navigating%20in%20a%20dynamic%20environment%2C%20avoiding%20obstacles%2C%20and%0Aadapting%20to%20human%20instructions%20in%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19259v1&entry.124074799=Read"},
{"title": "ContextFormer: Redefining Efficiency in Semantic Segmentation", "author": "Mian Muhammad Naeem Abid and Nancy Mehta and Zongwei Wu and Fayaz Ali Dharejo and Radu Timofte", "abstract": "  Semantic segmentation assigns labels to pixels in images, a critical yet\nchallenging task in computer vision. Convolutional methods, although capturing\nlocal dependencies well, struggle with long-range relationships. Vision\nTransformers (ViTs) excel in global context capture but are hindered by high\ncomputational demands, especially for high-resolution inputs. Most research\noptimizes the encoder architecture, leaving the bottleneck underexplored - a\nkey area for enhancing performance and efficiency. We propose ContextFormer, a\nhybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to\nbalance efficiency, accuracy, and robustness for real-time semantic\nsegmentation. The framework's efficiency is driven by three synergistic\nmodules: the Token Pyramid Extraction Module (TPEM) for hierarchical\nmulti-scale representation, the Transformer and Modulating DepthwiseConv\n(Trans-MDC) block for dynamic scale-aware feature modeling, and the Feature\nMerging Module (FMM) for robust integration with enhanced spatial and\ncontextual consistency. Extensive experiments on ADE20K, Pascal Context,\nCityScapes, and COCO-Stuff datasets show ContextFormer significantly\noutperforms existing models, achieving state-of-the-art mIoU scores, setting a\nnew benchmark for efficiency and performance. The codes will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2501.19255v1", "date": "2025-01-31", "relevancy": 2.3219, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextFormer%3A%20Redefining%20Efficiency%20in%20Semantic%20Segmentation&body=Title%3A%20ContextFormer%3A%20Redefining%20Efficiency%20in%20Semantic%20Segmentation%0AAuthor%3A%20Mian%20Muhammad%20Naeem%20Abid%20and%20Nancy%20Mehta%20and%20Zongwei%20Wu%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20Semantic%20segmentation%20assigns%20labels%20to%20pixels%20in%20images%2C%20a%20critical%20yet%0Achallenging%20task%20in%20computer%20vision.%20Convolutional%20methods%2C%20although%20capturing%0Alocal%20dependencies%20well%2C%20struggle%20with%20long-range%20relationships.%20Vision%0ATransformers%20%28ViTs%29%20excel%20in%20global%20context%20capture%20but%20are%20hindered%20by%20high%0Acomputational%20demands%2C%20especially%20for%20high-resolution%20inputs.%20Most%20research%0Aoptimizes%20the%20encoder%20architecture%2C%20leaving%20the%20bottleneck%20underexplored%20-%20a%0Akey%20area%20for%20enhancing%20performance%20and%20efficiency.%20We%20propose%20ContextFormer%2C%20a%0Ahybrid%20framework%20leveraging%20the%20strengths%20of%20CNNs%20and%20ViTs%20in%20the%20bottleneck%20to%0Abalance%20efficiency%2C%20accuracy%2C%20and%20robustness%20for%20real-time%20semantic%0Asegmentation.%20The%20framework%27s%20efficiency%20is%20driven%20by%20three%20synergistic%0Amodules%3A%20the%20Token%20Pyramid%20Extraction%20Module%20%28TPEM%29%20for%20hierarchical%0Amulti-scale%20representation%2C%20the%20Transformer%20and%20Modulating%20DepthwiseConv%0A%28Trans-MDC%29%20block%20for%20dynamic%20scale-aware%20feature%20modeling%2C%20and%20the%20Feature%0AMerging%20Module%20%28FMM%29%20for%20robust%20integration%20with%20enhanced%20spatial%20and%0Acontextual%20consistency.%20Extensive%20experiments%20on%20ADE20K%2C%20Pascal%20Context%2C%0ACityScapes%2C%20and%20COCO-Stuff%20datasets%20show%20ContextFormer%20significantly%0Aoutperforms%20existing%20models%2C%20achieving%20state-of-the-art%20mIoU%20scores%2C%20setting%20a%0Anew%20benchmark%20for%20efficiency%20and%20performance.%20The%20codes%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextFormer%253A%2520Redefining%2520Efficiency%2520in%2520Semantic%2520Segmentation%26entry.906535625%3DMian%2520Muhammad%2520Naeem%2520Abid%2520and%2520Nancy%2520Mehta%2520and%2520Zongwei%2520Wu%2520and%2520Fayaz%2520Ali%2520Dharejo%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520assigns%2520labels%2520to%2520pixels%2520in%2520images%252C%2520a%2520critical%2520yet%250Achallenging%2520task%2520in%2520computer%2520vision.%2520Convolutional%2520methods%252C%2520although%2520capturing%250Alocal%2520dependencies%2520well%252C%2520struggle%2520with%2520long-range%2520relationships.%2520Vision%250ATransformers%2520%2528ViTs%2529%2520excel%2520in%2520global%2520context%2520capture%2520but%2520are%2520hindered%2520by%2520high%250Acomputational%2520demands%252C%2520especially%2520for%2520high-resolution%2520inputs.%2520Most%2520research%250Aoptimizes%2520the%2520encoder%2520architecture%252C%2520leaving%2520the%2520bottleneck%2520underexplored%2520-%2520a%250Akey%2520area%2520for%2520enhancing%2520performance%2520and%2520efficiency.%2520We%2520propose%2520ContextFormer%252C%2520a%250Ahybrid%2520framework%2520leveraging%2520the%2520strengths%2520of%2520CNNs%2520and%2520ViTs%2520in%2520the%2520bottleneck%2520to%250Abalance%2520efficiency%252C%2520accuracy%252C%2520and%2520robustness%2520for%2520real-time%2520semantic%250Asegmentation.%2520The%2520framework%2527s%2520efficiency%2520is%2520driven%2520by%2520three%2520synergistic%250Amodules%253A%2520the%2520Token%2520Pyramid%2520Extraction%2520Module%2520%2528TPEM%2529%2520for%2520hierarchical%250Amulti-scale%2520representation%252C%2520the%2520Transformer%2520and%2520Modulating%2520DepthwiseConv%250A%2528Trans-MDC%2529%2520block%2520for%2520dynamic%2520scale-aware%2520feature%2520modeling%252C%2520and%2520the%2520Feature%250AMerging%2520Module%2520%2528FMM%2529%2520for%2520robust%2520integration%2520with%2520enhanced%2520spatial%2520and%250Acontextual%2520consistency.%2520Extensive%2520experiments%2520on%2520ADE20K%252C%2520Pascal%2520Context%252C%250ACityScapes%252C%2520and%2520COCO-Stuff%2520datasets%2520show%2520ContextFormer%2520significantly%250Aoutperforms%2520existing%2520models%252C%2520achieving%2520state-of-the-art%2520mIoU%2520scores%252C%2520setting%2520a%250Anew%2520benchmark%2520for%2520efficiency%2520and%2520performance.%2520The%2520codes%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextFormer%3A%20Redefining%20Efficiency%20in%20Semantic%20Segmentation&entry.906535625=Mian%20Muhammad%20Naeem%20Abid%20and%20Nancy%20Mehta%20and%20Zongwei%20Wu%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte&entry.1292438233=%20%20Semantic%20segmentation%20assigns%20labels%20to%20pixels%20in%20images%2C%20a%20critical%20yet%0Achallenging%20task%20in%20computer%20vision.%20Convolutional%20methods%2C%20although%20capturing%0Alocal%20dependencies%20well%2C%20struggle%20with%20long-range%20relationships.%20Vision%0ATransformers%20%28ViTs%29%20excel%20in%20global%20context%20capture%20but%20are%20hindered%20by%20high%0Acomputational%20demands%2C%20especially%20for%20high-resolution%20inputs.%20Most%20research%0Aoptimizes%20the%20encoder%20architecture%2C%20leaving%20the%20bottleneck%20underexplored%20-%20a%0Akey%20area%20for%20enhancing%20performance%20and%20efficiency.%20We%20propose%20ContextFormer%2C%20a%0Ahybrid%20framework%20leveraging%20the%20strengths%20of%20CNNs%20and%20ViTs%20in%20the%20bottleneck%20to%0Abalance%20efficiency%2C%20accuracy%2C%20and%20robustness%20for%20real-time%20semantic%0Asegmentation.%20The%20framework%27s%20efficiency%20is%20driven%20by%20three%20synergistic%0Amodules%3A%20the%20Token%20Pyramid%20Extraction%20Module%20%28TPEM%29%20for%20hierarchical%0Amulti-scale%20representation%2C%20the%20Transformer%20and%20Modulating%20DepthwiseConv%0A%28Trans-MDC%29%20block%20for%20dynamic%20scale-aware%20feature%20modeling%2C%20and%20the%20Feature%0AMerging%20Module%20%28FMM%29%20for%20robust%20integration%20with%20enhanced%20spatial%20and%0Acontextual%20consistency.%20Extensive%20experiments%20on%20ADE20K%2C%20Pascal%20Context%2C%0ACityScapes%2C%20and%20COCO-Stuff%20datasets%20show%20ContextFormer%20significantly%0Aoutperforms%20existing%20models%2C%20achieving%20state-of-the-art%20mIoU%20scores%2C%20setting%20a%0Anew%20benchmark%20for%20efficiency%20and%20performance.%20The%20codes%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19255v1&entry.124074799=Read"},
{"title": "Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO*", "author": "Ayman Chaouki and Jesse Read and Albert Bifet", "abstract": "  Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine\nLearning, yet it poses a formidable optimisation challenge. Practical\nalgorithms have recently emerged, primarily leveraging Dynamic Programming and\nBranch & Bound. However, most of these approaches rely on a Depth-First-Search\nstrategy, which is inefficient when searching for DTs at high depths and\nrequires the definition of a maximum depth hyperparameter. Best-First-Search\nwas also employed by other methods to circumvent these issues. The downside of\nthis strategy is its higher memory consumption, as such, it has to be designed\nin a fully efficient manner that takes full advantage of the problem's\nstructure. We formulate the problem as an AND/OR graph search which we solve\nwith a novel AO*-type algorithm called Branches. We prove both optimality and\ncomplexity guarantees for Branches and we show that it is more efficient than\nthe state of the art theoretically and on a variety of experiments.\nFurthermore, Branches supports non-binary features unlike the other methods, we\nshow that this property can further induce larger gains in computational\nefficiency.\n", "link": "http://arxiv.org/abs/2406.02175v4", "date": "2025-01-31", "relevancy": 2.3091, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4826}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branches%3A%20Efficiently%20Seeking%20Optimal%20Sparse%20Decision%20Trees%20with%20AO%2A&body=Title%3A%20Branches%3A%20Efficiently%20Seeking%20Optimal%20Sparse%20Decision%20Trees%20with%20AO%2A%0AAuthor%3A%20Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet%0AAbstract%3A%20%20%20Decision%20Tree%20%28DT%29%20Learning%20is%20a%20fundamental%20problem%20in%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimisation%20challenge.%20Practical%0Aalgorithms%20have%20recently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20and%0ABranch%20%26%20Bound.%20However%2C%20most%20of%20these%20approaches%20rely%20on%20a%20Depth-First-Search%0Astrategy%2C%20which%20is%20inefficient%20when%20searching%20for%20DTs%20at%20high%20depths%20and%0Arequires%20the%20definition%20of%20a%20maximum%20depth%20hyperparameter.%20Best-First-Search%0Awas%20also%20employed%20by%20other%20methods%20to%20circumvent%20these%20issues.%20The%20downside%20of%0Athis%20strategy%20is%20its%20higher%20memory%20consumption%2C%20as%20such%2C%20it%20has%20to%20be%20designed%0Ain%20a%20fully%20efficient%20manner%20that%20takes%20full%20advantage%20of%20the%20problem%27s%0Astructure.%20We%20formulate%20the%20problem%20as%20an%20AND/OR%20graph%20search%20which%20we%20solve%0Awith%20a%20novel%20AO%2A-type%20algorithm%20called%20Branches.%20We%20prove%20both%20optimality%20and%0Acomplexity%20guarantees%20for%20Branches%20and%20we%20show%20that%20it%20is%20more%20efficient%20than%0Athe%20state%20of%20the%20art%20theoretically%20and%20on%20a%20variety%20of%20experiments.%0AFurthermore%2C%20Branches%20supports%20non-binary%20features%20unlike%20the%20other%20methods%2C%20we%0Ashow%20that%20this%20property%20can%20further%20induce%20larger%20gains%20in%20computational%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02175v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranches%253A%2520Efficiently%2520Seeking%2520Optimal%2520Sparse%2520Decision%2520Trees%2520with%2520AO%252A%26entry.906535625%3DAyman%2520Chaouki%2520and%2520Jesse%2520Read%2520and%2520Albert%2520Bifet%26entry.1292438233%3D%2520%2520Decision%2520Tree%2520%2528DT%2529%2520Learning%2520is%2520a%2520fundamental%2520problem%2520in%2520Interpretable%2520Machine%250ALearning%252C%2520yet%2520it%2520poses%2520a%2520formidable%2520optimisation%2520challenge.%2520Practical%250Aalgorithms%2520have%2520recently%2520emerged%252C%2520primarily%2520leveraging%2520Dynamic%2520Programming%2520and%250ABranch%2520%2526%2520Bound.%2520However%252C%2520most%2520of%2520these%2520approaches%2520rely%2520on%2520a%2520Depth-First-Search%250Astrategy%252C%2520which%2520is%2520inefficient%2520when%2520searching%2520for%2520DTs%2520at%2520high%2520depths%2520and%250Arequires%2520the%2520definition%2520of%2520a%2520maximum%2520depth%2520hyperparameter.%2520Best-First-Search%250Awas%2520also%2520employed%2520by%2520other%2520methods%2520to%2520circumvent%2520these%2520issues.%2520The%2520downside%2520of%250Athis%2520strategy%2520is%2520its%2520higher%2520memory%2520consumption%252C%2520as%2520such%252C%2520it%2520has%2520to%2520be%2520designed%250Ain%2520a%2520fully%2520efficient%2520manner%2520that%2520takes%2520full%2520advantage%2520of%2520the%2520problem%2527s%250Astructure.%2520We%2520formulate%2520the%2520problem%2520as%2520an%2520AND/OR%2520graph%2520search%2520which%2520we%2520solve%250Awith%2520a%2520novel%2520AO%252A-type%2520algorithm%2520called%2520Branches.%2520We%2520prove%2520both%2520optimality%2520and%250Acomplexity%2520guarantees%2520for%2520Branches%2520and%2520we%2520show%2520that%2520it%2520is%2520more%2520efficient%2520than%250Athe%2520state%2520of%2520the%2520art%2520theoretically%2520and%2520on%2520a%2520variety%2520of%2520experiments.%250AFurthermore%252C%2520Branches%2520supports%2520non-binary%2520features%2520unlike%2520the%2520other%2520methods%252C%2520we%250Ashow%2520that%2520this%2520property%2520can%2520further%2520induce%2520larger%2520gains%2520in%2520computational%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02175v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branches%3A%20Efficiently%20Seeking%20Optimal%20Sparse%20Decision%20Trees%20with%20AO%2A&entry.906535625=Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet&entry.1292438233=%20%20Decision%20Tree%20%28DT%29%20Learning%20is%20a%20fundamental%20problem%20in%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimisation%20challenge.%20Practical%0Aalgorithms%20have%20recently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20and%0ABranch%20%26%20Bound.%20However%2C%20most%20of%20these%20approaches%20rely%20on%20a%20Depth-First-Search%0Astrategy%2C%20which%20is%20inefficient%20when%20searching%20for%20DTs%20at%20high%20depths%20and%0Arequires%20the%20definition%20of%20a%20maximum%20depth%20hyperparameter.%20Best-First-Search%0Awas%20also%20employed%20by%20other%20methods%20to%20circumvent%20these%20issues.%20The%20downside%20of%0Athis%20strategy%20is%20its%20higher%20memory%20consumption%2C%20as%20such%2C%20it%20has%20to%20be%20designed%0Ain%20a%20fully%20efficient%20manner%20that%20takes%20full%20advantage%20of%20the%20problem%27s%0Astructure.%20We%20formulate%20the%20problem%20as%20an%20AND/OR%20graph%20search%20which%20we%20solve%0Awith%20a%20novel%20AO%2A-type%20algorithm%20called%20Branches.%20We%20prove%20both%20optimality%20and%0Acomplexity%20guarantees%20for%20Branches%20and%20we%20show%20that%20it%20is%20more%20efficient%20than%0Athe%20state%20of%20the%20art%20theoretically%20and%20on%20a%20variety%20of%20experiments.%0AFurthermore%2C%20Branches%20supports%20non-binary%20features%20unlike%20the%20other%20methods%2C%20we%0Ashow%20that%20this%20property%20can%20further%20induce%20larger%20gains%20in%20computational%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02175v4&entry.124074799=Read"},
{"title": "Consistent Video Colorization via Palette Guidance", "author": "Han Wang and Yuang Zhang and Yuhong Zhang and Lingxiao Lu and Li Song", "abstract": "  Colorization is a traditional computer vision task and it plays an important\nrole in many time-consuming tasks, such as old film restoration. Existing\nmethods suffer from unsaturated color and temporally inconsistency. In this\npaper, we propose a novel pipeline to overcome the challenges. We regard the\ncolorization task as a generative task and introduce Stable Video Diffusion\n(SVD) as our base model. We design a palette-based color guider to assist the\nmodel in generating vivid and consistent colors. The color context introduced\nby the palette not only provides guidance for color generation, but also\nenhances the stability of the generated colors through a unified color context\nacross multiple sequences. Experiments demonstrate that the proposed method can\nprovide vivid and stable colors for videos, surpassing previous methods.\n", "link": "http://arxiv.org/abs/2501.19331v1", "date": "2025-01-31", "relevancy": 2.3064, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5879}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5705}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Video%20Colorization%20via%20Palette%20Guidance&body=Title%3A%20Consistent%20Video%20Colorization%20via%20Palette%20Guidance%0AAuthor%3A%20Han%20Wang%20and%20Yuang%20Zhang%20and%20Yuhong%20Zhang%20and%20Lingxiao%20Lu%20and%20Li%20Song%0AAbstract%3A%20%20%20Colorization%20is%20a%20traditional%20computer%20vision%20task%20and%20it%20plays%20an%20important%0Arole%20in%20many%20time-consuming%20tasks%2C%20such%20as%20old%20film%20restoration.%20Existing%0Amethods%20suffer%20from%20unsaturated%20color%20and%20temporally%20inconsistency.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20pipeline%20to%20overcome%20the%20challenges.%20We%20regard%20the%0Acolorization%20task%20as%20a%20generative%20task%20and%20introduce%20Stable%20Video%20Diffusion%0A%28SVD%29%20as%20our%20base%20model.%20We%20design%20a%20palette-based%20color%20guider%20to%20assist%20the%0Amodel%20in%20generating%20vivid%20and%20consistent%20colors.%20The%20color%20context%20introduced%0Aby%20the%20palette%20not%20only%20provides%20guidance%20for%20color%20generation%2C%20but%20also%0Aenhances%20the%20stability%20of%20the%20generated%20colors%20through%20a%20unified%20color%20context%0Aacross%20multiple%20sequences.%20Experiments%20demonstrate%20that%20the%20proposed%20method%20can%0Aprovide%20vivid%20and%20stable%20colors%20for%20videos%2C%20surpassing%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Video%2520Colorization%2520via%2520Palette%2520Guidance%26entry.906535625%3DHan%2520Wang%2520and%2520Yuang%2520Zhang%2520and%2520Yuhong%2520Zhang%2520and%2520Lingxiao%2520Lu%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520Colorization%2520is%2520a%2520traditional%2520computer%2520vision%2520task%2520and%2520it%2520plays%2520an%2520important%250Arole%2520in%2520many%2520time-consuming%2520tasks%252C%2520such%2520as%2520old%2520film%2520restoration.%2520Existing%250Amethods%2520suffer%2520from%2520unsaturated%2520color%2520and%2520temporally%2520inconsistency.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520pipeline%2520to%2520overcome%2520the%2520challenges.%2520We%2520regard%2520the%250Acolorization%2520task%2520as%2520a%2520generative%2520task%2520and%2520introduce%2520Stable%2520Video%2520Diffusion%250A%2528SVD%2529%2520as%2520our%2520base%2520model.%2520We%2520design%2520a%2520palette-based%2520color%2520guider%2520to%2520assist%2520the%250Amodel%2520in%2520generating%2520vivid%2520and%2520consistent%2520colors.%2520The%2520color%2520context%2520introduced%250Aby%2520the%2520palette%2520not%2520only%2520provides%2520guidance%2520for%2520color%2520generation%252C%2520but%2520also%250Aenhances%2520the%2520stability%2520of%2520the%2520generated%2520colors%2520through%2520a%2520unified%2520color%2520context%250Aacross%2520multiple%2520sequences.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%250Aprovide%2520vivid%2520and%2520stable%2520colors%2520for%2520videos%252C%2520surpassing%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Video%20Colorization%20via%20Palette%20Guidance&entry.906535625=Han%20Wang%20and%20Yuang%20Zhang%20and%20Yuhong%20Zhang%20and%20Lingxiao%20Lu%20and%20Li%20Song&entry.1292438233=%20%20Colorization%20is%20a%20traditional%20computer%20vision%20task%20and%20it%20plays%20an%20important%0Arole%20in%20many%20time-consuming%20tasks%2C%20such%20as%20old%20film%20restoration.%20Existing%0Amethods%20suffer%20from%20unsaturated%20color%20and%20temporally%20inconsistency.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20pipeline%20to%20overcome%20the%20challenges.%20We%20regard%20the%0Acolorization%20task%20as%20a%20generative%20task%20and%20introduce%20Stable%20Video%20Diffusion%0A%28SVD%29%20as%20our%20base%20model.%20We%20design%20a%20palette-based%20color%20guider%20to%20assist%20the%0Amodel%20in%20generating%20vivid%20and%20consistent%20colors.%20The%20color%20context%20introduced%0Aby%20the%20palette%20not%20only%20provides%20guidance%20for%20color%20generation%2C%20but%20also%0Aenhances%20the%20stability%20of%20the%20generated%20colors%20through%20a%20unified%20color%20context%0Aacross%20multiple%20sequences.%20Experiments%20demonstrate%20that%20the%20proposed%20method%20can%0Aprovide%20vivid%20and%20stable%20colors%20for%20videos%2C%20surpassing%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19331v1&entry.124074799=Read"},
{"title": "Language Bias in Self-Supervised Learning For Automatic Speech\n  Recognition", "author": "Edward Storey and Naomi Harte and Peter Bell", "abstract": "  Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.\n", "link": "http://arxiv.org/abs/2501.19321v1", "date": "2025-01-31", "relevancy": 2.3006, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4786}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4564}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Bias%20in%20Self-Supervised%20Learning%20For%20Automatic%20Speech%0A%20%20Recognition&body=Title%3A%20Language%20Bias%20in%20Self-Supervised%20Learning%20For%20Automatic%20Speech%0A%20%20Recognition%0AAuthor%3A%20Edward%20Storey%20and%20Naomi%20Harte%20and%20Peter%20Bell%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20is%20used%20in%20deep%20learning%20to%20train%20on%20large%0Adatasets%20without%20the%20need%20for%20expensive%20labelling%20of%20the%20data.%20Recently%2C%20large%0AAutomatic%20Speech%20Recognition%20%28ASR%29%20models%20such%20as%20XLS-R%20have%20utilised%20SSL%20to%0Atrain%20on%20over%20one%20hundred%20different%20languages%20simultaneously.%20However%2C%20deeper%0Ainvestigation%20shows%20that%20the%20bulk%20of%20the%20training%20data%20for%20XLS-R%20comes%20from%20a%0Asmall%20number%20of%20languages.%20Biases%20learned%20through%20SSL%20have%20been%20shown%20to%20exist%0Ain%20multiple%20domains%2C%20but%20language%20bias%20in%20multilingual%20SSL%20ASR%20has%20not%20been%0Athoroughly%20examined.%20In%20this%20paper%2C%20we%20utilise%20the%20Lottery%20Ticket%20Hypothesis%0A%28LTH%29%20to%20identify%20language-specific%20subnetworks%20within%20XLS-R%20and%20test%20the%0Aperformance%20of%20these%20subnetworks%20on%20a%20variety%20of%20different%20languages.%20We%20are%0Aable%20to%20show%20that%20when%20fine-tuning%2C%20XLS-R%20bypasses%20traditional%20linguistic%0Aknowledge%20and%20builds%20only%20on%20weights%20learned%20from%20the%20languages%20with%20the%0Alargest%20data%20contribution%20to%20the%20pretraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Bias%2520in%2520Self-Supervised%2520Learning%2520For%2520Automatic%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DEdward%2520Storey%2520and%2520Naomi%2520Harte%2520and%2520Peter%2520Bell%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520is%2520used%2520in%2520deep%2520learning%2520to%2520train%2520on%2520large%250Adatasets%2520without%2520the%2520need%2520for%2520expensive%2520labelling%2520of%2520the%2520data.%2520Recently%252C%2520large%250AAutomatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520models%2520such%2520as%2520XLS-R%2520have%2520utilised%2520SSL%2520to%250Atrain%2520on%2520over%2520one%2520hundred%2520different%2520languages%2520simultaneously.%2520However%252C%2520deeper%250Ainvestigation%2520shows%2520that%2520the%2520bulk%2520of%2520the%2520training%2520data%2520for%2520XLS-R%2520comes%2520from%2520a%250Asmall%2520number%2520of%2520languages.%2520Biases%2520learned%2520through%2520SSL%2520have%2520been%2520shown%2520to%2520exist%250Ain%2520multiple%2520domains%252C%2520but%2520language%2520bias%2520in%2520multilingual%2520SSL%2520ASR%2520has%2520not%2520been%250Athoroughly%2520examined.%2520In%2520this%2520paper%252C%2520we%2520utilise%2520the%2520Lottery%2520Ticket%2520Hypothesis%250A%2528LTH%2529%2520to%2520identify%2520language-specific%2520subnetworks%2520within%2520XLS-R%2520and%2520test%2520the%250Aperformance%2520of%2520these%2520subnetworks%2520on%2520a%2520variety%2520of%2520different%2520languages.%2520We%2520are%250Aable%2520to%2520show%2520that%2520when%2520fine-tuning%252C%2520XLS-R%2520bypasses%2520traditional%2520linguistic%250Aknowledge%2520and%2520builds%2520only%2520on%2520weights%2520learned%2520from%2520the%2520languages%2520with%2520the%250Alargest%2520data%2520contribution%2520to%2520the%2520pretraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Bias%20in%20Self-Supervised%20Learning%20For%20Automatic%20Speech%0A%20%20Recognition&entry.906535625=Edward%20Storey%20and%20Naomi%20Harte%20and%20Peter%20Bell&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20is%20used%20in%20deep%20learning%20to%20train%20on%20large%0Adatasets%20without%20the%20need%20for%20expensive%20labelling%20of%20the%20data.%20Recently%2C%20large%0AAutomatic%20Speech%20Recognition%20%28ASR%29%20models%20such%20as%20XLS-R%20have%20utilised%20SSL%20to%0Atrain%20on%20over%20one%20hundred%20different%20languages%20simultaneously.%20However%2C%20deeper%0Ainvestigation%20shows%20that%20the%20bulk%20of%20the%20training%20data%20for%20XLS-R%20comes%20from%20a%0Asmall%20number%20of%20languages.%20Biases%20learned%20through%20SSL%20have%20been%20shown%20to%20exist%0Ain%20multiple%20domains%2C%20but%20language%20bias%20in%20multilingual%20SSL%20ASR%20has%20not%20been%0Athoroughly%20examined.%20In%20this%20paper%2C%20we%20utilise%20the%20Lottery%20Ticket%20Hypothesis%0A%28LTH%29%20to%20identify%20language-specific%20subnetworks%20within%20XLS-R%20and%20test%20the%0Aperformance%20of%20these%20subnetworks%20on%20a%20variety%20of%20different%20languages.%20We%20are%0Aable%20to%20show%20that%20when%20fine-tuning%2C%20XLS-R%20bypasses%20traditional%20linguistic%0Aknowledge%20and%20builds%20only%20on%20weights%20learned%20from%20the%20languages%20with%20the%0Alargest%20data%20contribution%20to%20the%20pretraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19321v1&entry.124074799=Read"},
{"title": "Transformation trees -- documentation of multimodal image registration", "author": "Agnieszka Anna Tomaka and Dariusz Pojda and Micha\u0142 Tarnawski and Leszek Luchowski", "abstract": "  The paper presents proposals for the application of a tree structure to the\ndocumentation of a set of transformations obtained as a result of various\nregistrations of multimodal images obtained in coordinate systems associated\nwith acquisition devices and being registered in one patient-specific\ncoordinate system. A special file format .dpw (digital patient workspace) is\nintroduced. Examples of different registrations yielded from orthodontic\nanalysis and showing main aspects of the usage of tree structure are\nillustrated in dpVision software.\n", "link": "http://arxiv.org/abs/2501.19140v1", "date": "2025-01-31", "relevancy": 2.3001, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4948}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformation%20trees%20--%20documentation%20of%20multimodal%20image%20registration&body=Title%3A%20Transformation%20trees%20--%20documentation%20of%20multimodal%20image%20registration%0AAuthor%3A%20Agnieszka%20Anna%20Tomaka%20and%20Dariusz%20Pojda%20and%20Micha%C5%82%20Tarnawski%20and%20Leszek%20Luchowski%0AAbstract%3A%20%20%20The%20paper%20presents%20proposals%20for%20the%20application%20of%20a%20tree%20structure%20to%20the%0Adocumentation%20of%20a%20set%20of%20transformations%20obtained%20as%20a%20result%20of%20various%0Aregistrations%20of%20multimodal%20images%20obtained%20in%20coordinate%20systems%20associated%0Awith%20acquisition%20devices%20and%20being%20registered%20in%20one%20patient-specific%0Acoordinate%20system.%20A%20special%20file%20format%20.dpw%20%28digital%20patient%20workspace%29%20is%0Aintroduced.%20Examples%20of%20different%20registrations%20yielded%20from%20orthodontic%0Aanalysis%20and%20showing%20main%20aspects%20of%20the%20usage%20of%20tree%20structure%20are%0Aillustrated%20in%20dpVision%20software.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformation%2520trees%2520--%2520documentation%2520of%2520multimodal%2520image%2520registration%26entry.906535625%3DAgnieszka%2520Anna%2520Tomaka%2520and%2520Dariusz%2520Pojda%2520and%2520Micha%25C5%2582%2520Tarnawski%2520and%2520Leszek%2520Luchowski%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520proposals%2520for%2520the%2520application%2520of%2520a%2520tree%2520structure%2520to%2520the%250Adocumentation%2520of%2520a%2520set%2520of%2520transformations%2520obtained%2520as%2520a%2520result%2520of%2520various%250Aregistrations%2520of%2520multimodal%2520images%2520obtained%2520in%2520coordinate%2520systems%2520associated%250Awith%2520acquisition%2520devices%2520and%2520being%2520registered%2520in%2520one%2520patient-specific%250Acoordinate%2520system.%2520A%2520special%2520file%2520format%2520.dpw%2520%2528digital%2520patient%2520workspace%2529%2520is%250Aintroduced.%2520Examples%2520of%2520different%2520registrations%2520yielded%2520from%2520orthodontic%250Aanalysis%2520and%2520showing%2520main%2520aspects%2520of%2520the%2520usage%2520of%2520tree%2520structure%2520are%250Aillustrated%2520in%2520dpVision%2520software.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformation%20trees%20--%20documentation%20of%20multimodal%20image%20registration&entry.906535625=Agnieszka%20Anna%20Tomaka%20and%20Dariusz%20Pojda%20and%20Micha%C5%82%20Tarnawski%20and%20Leszek%20Luchowski&entry.1292438233=%20%20The%20paper%20presents%20proposals%20for%20the%20application%20of%20a%20tree%20structure%20to%20the%0Adocumentation%20of%20a%20set%20of%20transformations%20obtained%20as%20a%20result%20of%20various%0Aregistrations%20of%20multimodal%20images%20obtained%20in%20coordinate%20systems%20associated%0Awith%20acquisition%20devices%20and%20being%20registered%20in%20one%20patient-specific%0Acoordinate%20system.%20A%20special%20file%20format%20.dpw%20%28digital%20patient%20workspace%29%20is%0Aintroduced.%20Examples%20of%20different%20registrations%20yielded%20from%20orthodontic%0Aanalysis%20and%20showing%20main%20aspects%20of%20the%20usage%20of%20tree%20structure%20are%0Aillustrated%20in%20dpVision%20software.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19140v1&entry.124074799=Read"},
{"title": "Learning multivariate Gaussians with imperfect advice", "author": "Arnab Bhattacharyya and Davin Choo and Philips George John and Themis Gouleakis", "abstract": "  We revisit the problem of distribution learning within the framework of\nlearning-augmented algorithms. In this setting, we explore the scenario where a\nprobability distribution is provided as potentially inaccurate advice on the\ntrue, unknown distribution. Our objective is to develop learning algorithms\nwhose sample complexity decreases as the quality of the advice improves,\nthereby surpassing standard learning lower bounds when the advice is\nsufficiently accurate.\n  Specifically, we demonstrate that this outcome is achievable for the problem\nof learning a multivariate Gaussian distribution $N(\\boldsymbol{\\mu},\n\\boldsymbol{\\Sigma})$ in the PAC learning setting. Classically, in the\nadvice-free setting, $\\tilde{\\Theta}(d^2/\\varepsilon^2)$ samples are sufficient\nand worst case necessary to learn $d$-dimensional Gaussians up to TV distance\n$\\varepsilon$ with constant probability. When we are additionally given a\nparameter $\\tilde{\\boldsymbol{\\Sigma}}$ as advice, we show that\n$\\tilde{O}(d^{2-\\beta}/\\varepsilon^2)$ samples suffices whenever $\\|\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} \\boldsymbol{\\Sigma}\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon\nd^{1-\\beta}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any\n$\\beta > 0$, yielding a polynomial improvement over the advice-free setting.\n", "link": "http://arxiv.org/abs/2411.12700v3", "date": "2025-01-31", "relevancy": 2.2879, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4719}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4627}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20multivariate%20Gaussians%20with%20imperfect%20advice&body=Title%3A%20Learning%20multivariate%20Gaussians%20with%20imperfect%20advice%0AAuthor%3A%20Arnab%20Bhattacharyya%20and%20Davin%20Choo%20and%20Philips%20George%20John%20and%20Themis%20Gouleakis%0AAbstract%3A%20%20%20We%20revisit%20the%20problem%20of%20distribution%20learning%20within%20the%20framework%20of%0Alearning-augmented%20algorithms.%20In%20this%20setting%2C%20we%20explore%20the%20scenario%20where%20a%0Aprobability%20distribution%20is%20provided%20as%20potentially%20inaccurate%20advice%20on%20the%0Atrue%2C%20unknown%20distribution.%20Our%20objective%20is%20to%20develop%20learning%20algorithms%0Awhose%20sample%20complexity%20decreases%20as%20the%20quality%20of%20the%20advice%20improves%2C%0Athereby%20surpassing%20standard%20learning%20lower%20bounds%20when%20the%20advice%20is%0Asufficiently%20accurate.%0A%20%20Specifically%2C%20we%20demonstrate%20that%20this%20outcome%20is%20achievable%20for%20the%20problem%0Aof%20learning%20a%20multivariate%20Gaussian%20distribution%20%24N%28%5Cboldsymbol%7B%5Cmu%7D%2C%0A%5Cboldsymbol%7B%5CSigma%7D%29%24%20in%20the%20PAC%20learning%20setting.%20Classically%2C%20in%20the%0Aadvice-free%20setting%2C%20%24%5Ctilde%7B%5CTheta%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20samples%20are%20sufficient%0Aand%20worst%20case%20necessary%20to%20learn%20%24d%24-dimensional%20Gaussians%20up%20to%20TV%20distance%0A%24%5Cvarepsilon%24%20with%20constant%20probability.%20When%20we%20are%20additionally%20given%20a%0Aparameter%20%24%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%24%20as%20advice%2C%20we%20show%20that%0A%24%5Ctilde%7BO%7D%28d%5E%7B2-%5Cbeta%7D/%5Cvarepsilon%5E2%29%24%20samples%20suffices%20whenever%20%24%5C%7C%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20%5Cboldsymbol%7B%5CSigma%7D%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20-%20%5Cboldsymbol%7BI_d%7D%20%5C%7C_1%20%5Cleq%20%5Cvarepsilon%0Ad%5E%7B1-%5Cbeta%7D%24%20%28where%20%24%5C%7C%5Ccdot%5C%7C_1%24%20denotes%20the%20entrywise%20%24%5Cell_1%24%20norm%29%20for%20any%0A%24%5Cbeta%20%3E%200%24%2C%20yielding%20a%20polynomial%20improvement%20over%20the%20advice-free%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12700v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520multivariate%2520Gaussians%2520with%2520imperfect%2520advice%26entry.906535625%3DArnab%2520Bhattacharyya%2520and%2520Davin%2520Choo%2520and%2520Philips%2520George%2520John%2520and%2520Themis%2520Gouleakis%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520problem%2520of%2520distribution%2520learning%2520within%2520the%2520framework%2520of%250Alearning-augmented%2520algorithms.%2520In%2520this%2520setting%252C%2520we%2520explore%2520the%2520scenario%2520where%2520a%250Aprobability%2520distribution%2520is%2520provided%2520as%2520potentially%2520inaccurate%2520advice%2520on%2520the%250Atrue%252C%2520unknown%2520distribution.%2520Our%2520objective%2520is%2520to%2520develop%2520learning%2520algorithms%250Awhose%2520sample%2520complexity%2520decreases%2520as%2520the%2520quality%2520of%2520the%2520advice%2520improves%252C%250Athereby%2520surpassing%2520standard%2520learning%2520lower%2520bounds%2520when%2520the%2520advice%2520is%250Asufficiently%2520accurate.%250A%2520%2520Specifically%252C%2520we%2520demonstrate%2520that%2520this%2520outcome%2520is%2520achievable%2520for%2520the%2520problem%250Aof%2520learning%2520a%2520multivariate%2520Gaussian%2520distribution%2520%2524N%2528%255Cboldsymbol%257B%255Cmu%257D%252C%250A%255Cboldsymbol%257B%255CSigma%257D%2529%2524%2520in%2520the%2520PAC%2520learning%2520setting.%2520Classically%252C%2520in%2520the%250Aadvice-free%2520setting%252C%2520%2524%255Ctilde%257B%255CTheta%257D%2528d%255E2/%255Cvarepsilon%255E2%2529%2524%2520samples%2520are%2520sufficient%250Aand%2520worst%2520case%2520necessary%2520to%2520learn%2520%2524d%2524-dimensional%2520Gaussians%2520up%2520to%2520TV%2520distance%250A%2524%255Cvarepsilon%2524%2520with%2520constant%2520probability.%2520When%2520we%2520are%2520additionally%2520given%2520a%250Aparameter%2520%2524%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%2524%2520as%2520advice%252C%2520we%2520show%2520that%250A%2524%255Ctilde%257BO%257D%2528d%255E%257B2-%255Cbeta%257D/%255Cvarepsilon%255E2%2529%2524%2520samples%2520suffices%2520whenever%2520%2524%255C%257C%250A%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%255E%257B-1/2%257D%2520%255Cboldsymbol%257B%255CSigma%257D%250A%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%255E%257B-1/2%257D%2520-%2520%255Cboldsymbol%257BI_d%257D%2520%255C%257C_1%2520%255Cleq%2520%255Cvarepsilon%250Ad%255E%257B1-%255Cbeta%257D%2524%2520%2528where%2520%2524%255C%257C%255Ccdot%255C%257C_1%2524%2520denotes%2520the%2520entrywise%2520%2524%255Cell_1%2524%2520norm%2529%2520for%2520any%250A%2524%255Cbeta%2520%253E%25200%2524%252C%2520yielding%2520a%2520polynomial%2520improvement%2520over%2520the%2520advice-free%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12700v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20multivariate%20Gaussians%20with%20imperfect%20advice&entry.906535625=Arnab%20Bhattacharyya%20and%20Davin%20Choo%20and%20Philips%20George%20John%20and%20Themis%20Gouleakis&entry.1292438233=%20%20We%20revisit%20the%20problem%20of%20distribution%20learning%20within%20the%20framework%20of%0Alearning-augmented%20algorithms.%20In%20this%20setting%2C%20we%20explore%20the%20scenario%20where%20a%0Aprobability%20distribution%20is%20provided%20as%20potentially%20inaccurate%20advice%20on%20the%0Atrue%2C%20unknown%20distribution.%20Our%20objective%20is%20to%20develop%20learning%20algorithms%0Awhose%20sample%20complexity%20decreases%20as%20the%20quality%20of%20the%20advice%20improves%2C%0Athereby%20surpassing%20standard%20learning%20lower%20bounds%20when%20the%20advice%20is%0Asufficiently%20accurate.%0A%20%20Specifically%2C%20we%20demonstrate%20that%20this%20outcome%20is%20achievable%20for%20the%20problem%0Aof%20learning%20a%20multivariate%20Gaussian%20distribution%20%24N%28%5Cboldsymbol%7B%5Cmu%7D%2C%0A%5Cboldsymbol%7B%5CSigma%7D%29%24%20in%20the%20PAC%20learning%20setting.%20Classically%2C%20in%20the%0Aadvice-free%20setting%2C%20%24%5Ctilde%7B%5CTheta%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20samples%20are%20sufficient%0Aand%20worst%20case%20necessary%20to%20learn%20%24d%24-dimensional%20Gaussians%20up%20to%20TV%20distance%0A%24%5Cvarepsilon%24%20with%20constant%20probability.%20When%20we%20are%20additionally%20given%20a%0Aparameter%20%24%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%24%20as%20advice%2C%20we%20show%20that%0A%24%5Ctilde%7BO%7D%28d%5E%7B2-%5Cbeta%7D/%5Cvarepsilon%5E2%29%24%20samples%20suffices%20whenever%20%24%5C%7C%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20%5Cboldsymbol%7B%5CSigma%7D%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20-%20%5Cboldsymbol%7BI_d%7D%20%5C%7C_1%20%5Cleq%20%5Cvarepsilon%0Ad%5E%7B1-%5Cbeta%7D%24%20%28where%20%24%5C%7C%5Ccdot%5C%7C_1%24%20denotes%20the%20entrywise%20%24%5Cell_1%24%20norm%29%20for%20any%0A%24%5Cbeta%20%3E%200%24%2C%20yielding%20a%20polynomial%20improvement%20over%20the%20advice-free%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12700v3&entry.124074799=Read"},
{"title": "Norm-Bounded Low-Rank Adaptation", "author": "Ruigang Wang and Krishnamurthy Dvijotham and Ian R. Manchester", "abstract": "  In this work, we propose norm-bounded low-rank adaptation (NB-LoRA) for\nparameter-efficient fine tuning. We introduce two parameterizations that allow\nexplicit bounds on each singular value of the weight adaptation matrix, which\ncan therefore satisfy any prescribed unitarily invariant norm bound, including\nthe Schatten norms (e.g., nuclear, Frobenius, spectral norm). The proposed\nparameterizations are unconstrained and complete, i.e. they cover all matrices\nsatisfying the prescribed rank and norm constraints. Experiments on vision\nfine-tuning benchmarks show that the proposed approach can achieve good\nadaptation performance while avoiding model catastrophic forgetting and also\nsubstantially improve robustness to a wide range of hyper-parameters, including\nadaptation rank, learning rate and number of training epochs. We also explore\napplications in privacy-preserving model merging and low-rank matrix\ncompletion.\n", "link": "http://arxiv.org/abs/2501.19050v1", "date": "2025-01-31", "relevancy": 2.2697, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4739}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4447}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Norm-Bounded%20Low-Rank%20Adaptation&body=Title%3A%20Norm-Bounded%20Low-Rank%20Adaptation%0AAuthor%3A%20Ruigang%20Wang%20and%20Krishnamurthy%20Dvijotham%20and%20Ian%20R.%20Manchester%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20norm-bounded%20low-rank%20adaptation%20%28NB-LoRA%29%20for%0Aparameter-efficient%20fine%20tuning.%20We%20introduce%20two%20parameterizations%20that%20allow%0Aexplicit%20bounds%20on%20each%20singular%20value%20of%20the%20weight%20adaptation%20matrix%2C%20which%0Acan%20therefore%20satisfy%20any%20prescribed%20unitarily%20invariant%20norm%20bound%2C%20including%0Athe%20Schatten%20norms%20%28e.g.%2C%20nuclear%2C%20Frobenius%2C%20spectral%20norm%29.%20The%20proposed%0Aparameterizations%20are%20unconstrained%20and%20complete%2C%20i.e.%20they%20cover%20all%20matrices%0Asatisfying%20the%20prescribed%20rank%20and%20norm%20constraints.%20Experiments%20on%20vision%0Afine-tuning%20benchmarks%20show%20that%20the%20proposed%20approach%20can%20achieve%20good%0Aadaptation%20performance%20while%20avoiding%20model%20catastrophic%20forgetting%20and%20also%0Asubstantially%20improve%20robustness%20to%20a%20wide%20range%20of%20hyper-parameters%2C%20including%0Aadaptation%20rank%2C%20learning%20rate%20and%20number%20of%20training%20epochs.%20We%20also%20explore%0Aapplications%20in%20privacy-preserving%20model%20merging%20and%20low-rank%20matrix%0Acompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNorm-Bounded%2520Low-Rank%2520Adaptation%26entry.906535625%3DRuigang%2520Wang%2520and%2520Krishnamurthy%2520Dvijotham%2520and%2520Ian%2520R.%2520Manchester%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520norm-bounded%2520low-rank%2520adaptation%2520%2528NB-LoRA%2529%2520for%250Aparameter-efficient%2520fine%2520tuning.%2520We%2520introduce%2520two%2520parameterizations%2520that%2520allow%250Aexplicit%2520bounds%2520on%2520each%2520singular%2520value%2520of%2520the%2520weight%2520adaptation%2520matrix%252C%2520which%250Acan%2520therefore%2520satisfy%2520any%2520prescribed%2520unitarily%2520invariant%2520norm%2520bound%252C%2520including%250Athe%2520Schatten%2520norms%2520%2528e.g.%252C%2520nuclear%252C%2520Frobenius%252C%2520spectral%2520norm%2529.%2520The%2520proposed%250Aparameterizations%2520are%2520unconstrained%2520and%2520complete%252C%2520i.e.%2520they%2520cover%2520all%2520matrices%250Asatisfying%2520the%2520prescribed%2520rank%2520and%2520norm%2520constraints.%2520Experiments%2520on%2520vision%250Afine-tuning%2520benchmarks%2520show%2520that%2520the%2520proposed%2520approach%2520can%2520achieve%2520good%250Aadaptation%2520performance%2520while%2520avoiding%2520model%2520catastrophic%2520forgetting%2520and%2520also%250Asubstantially%2520improve%2520robustness%2520to%2520a%2520wide%2520range%2520of%2520hyper-parameters%252C%2520including%250Aadaptation%2520rank%252C%2520learning%2520rate%2520and%2520number%2520of%2520training%2520epochs.%2520We%2520also%2520explore%250Aapplications%2520in%2520privacy-preserving%2520model%2520merging%2520and%2520low-rank%2520matrix%250Acompletion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Norm-Bounded%20Low-Rank%20Adaptation&entry.906535625=Ruigang%20Wang%20and%20Krishnamurthy%20Dvijotham%20and%20Ian%20R.%20Manchester&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20norm-bounded%20low-rank%20adaptation%20%28NB-LoRA%29%20for%0Aparameter-efficient%20fine%20tuning.%20We%20introduce%20two%20parameterizations%20that%20allow%0Aexplicit%20bounds%20on%20each%20singular%20value%20of%20the%20weight%20adaptation%20matrix%2C%20which%0Acan%20therefore%20satisfy%20any%20prescribed%20unitarily%20invariant%20norm%20bound%2C%20including%0Athe%20Schatten%20norms%20%28e.g.%2C%20nuclear%2C%20Frobenius%2C%20spectral%20norm%29.%20The%20proposed%0Aparameterizations%20are%20unconstrained%20and%20complete%2C%20i.e.%20they%20cover%20all%20matrices%0Asatisfying%20the%20prescribed%20rank%20and%20norm%20constraints.%20Experiments%20on%20vision%0Afine-tuning%20benchmarks%20show%20that%20the%20proposed%20approach%20can%20achieve%20good%0Aadaptation%20performance%20while%20avoiding%20model%20catastrophic%20forgetting%20and%20also%0Asubstantially%20improve%20robustness%20to%20a%20wide%20range%20of%20hyper-parameters%2C%20including%0Aadaptation%20rank%2C%20learning%20rate%20and%20number%20of%20training%20epochs.%20We%20also%20explore%0Aapplications%20in%20privacy-preserving%20model%20merging%20and%20low-rank%20matrix%0Acompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19050v1&entry.124074799=Read"},
{"title": "Learning Human-Aligned Representations with Contrastive Learning and\n  Generative Similarity", "author": "Raja Marjieh and Sreejan Kumar and Declan Campbell and Liyi Zhang and Gianluca Bencomo and Jake Snell and Thomas L. Griffiths", "abstract": "  Humans rely on effective representations to learn from few examples and\nabstract useful information from sensory data. Inducing such representations in\nmachine learning models has been shown to improve their performance on various\nbenchmarks such as few-shot learning and robustness. However, finding effective\ntraining procedures to achieve that goal can be challenging as psychologically\nrich training data such as human similarity judgments are expensive to scale,\nand Bayesian models of human inductive biases are often intractable for\ncomplex, realistic domains. Here, we address this challenge by leveraging a\nBayesian notion of generative similarity whereby two data points are considered\nsimilar if they are likely to have been sampled from the same distribution.\nThis measure can be applied to complex generative processes, including\nprobabilistic programs. We incorporate generative similarity into a contrastive\nlearning objective to enable learning of embeddings that express human\ncognitive representations. We demonstrate the utility of our approach by\nshowing that it can be used to capture human-like representations of shape\nregularity, abstract Euclidean geometric concepts, and semantic hierarchies for\nnatural images.\n", "link": "http://arxiv.org/abs/2405.19420v3", "date": "2025-01-31", "relevancy": 2.2623, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5769}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5692}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-Aligned%20Representations%20with%20Contrastive%20Learning%20and%0A%20%20Generative%20Similarity&body=Title%3A%20Learning%20Human-Aligned%20Representations%20with%20Contrastive%20Learning%20and%0A%20%20Generative%20Similarity%0AAuthor%3A%20Raja%20Marjieh%20and%20Sreejan%20Kumar%20and%20Declan%20Campbell%20and%20Liyi%20Zhang%20and%20Gianluca%20Bencomo%20and%20Jake%20Snell%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Humans%20rely%20on%20effective%20representations%20to%20learn%20from%20few%20examples%20and%0Aabstract%20useful%20information%20from%20sensory%20data.%20Inducing%20such%20representations%20in%0Amachine%20learning%20models%20has%20been%20shown%20to%20improve%20their%20performance%20on%20various%0Abenchmarks%20such%20as%20few-shot%20learning%20and%20robustness.%20However%2C%20finding%20effective%0Atraining%20procedures%20to%20achieve%20that%20goal%20can%20be%20challenging%20as%20psychologically%0Arich%20training%20data%20such%20as%20human%20similarity%20judgments%20are%20expensive%20to%20scale%2C%0Aand%20Bayesian%20models%20of%20human%20inductive%20biases%20are%20often%20intractable%20for%0Acomplex%2C%20realistic%20domains.%20Here%2C%20we%20address%20this%20challenge%20by%20leveraging%20a%0ABayesian%20notion%20of%20generative%20similarity%20whereby%20two%20data%20points%20are%20considered%0Asimilar%20if%20they%20are%20likely%20to%20have%20been%20sampled%20from%20the%20same%20distribution.%0AThis%20measure%20can%20be%20applied%20to%20complex%20generative%20processes%2C%20including%0Aprobabilistic%20programs.%20We%20incorporate%20generative%20similarity%20into%20a%20contrastive%0Alearning%20objective%20to%20enable%20learning%20of%20embeddings%20that%20express%20human%0Acognitive%20representations.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20by%0Ashowing%20that%20it%20can%20be%20used%20to%20capture%20human-like%20representations%20of%20shape%0Aregularity%2C%20abstract%20Euclidean%20geometric%20concepts%2C%20and%20semantic%20hierarchies%20for%0Anatural%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19420v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Human-Aligned%2520Representations%2520with%2520Contrastive%2520Learning%2520and%250A%2520%2520Generative%2520Similarity%26entry.906535625%3DRaja%2520Marjieh%2520and%2520Sreejan%2520Kumar%2520and%2520Declan%2520Campbell%2520and%2520Liyi%2520Zhang%2520and%2520Gianluca%2520Bencomo%2520and%2520Jake%2520Snell%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Humans%2520rely%2520on%2520effective%2520representations%2520to%2520learn%2520from%2520few%2520examples%2520and%250Aabstract%2520useful%2520information%2520from%2520sensory%2520data.%2520Inducing%2520such%2520representations%2520in%250Amachine%2520learning%2520models%2520has%2520been%2520shown%2520to%2520improve%2520their%2520performance%2520on%2520various%250Abenchmarks%2520such%2520as%2520few-shot%2520learning%2520and%2520robustness.%2520However%252C%2520finding%2520effective%250Atraining%2520procedures%2520to%2520achieve%2520that%2520goal%2520can%2520be%2520challenging%2520as%2520psychologically%250Arich%2520training%2520data%2520such%2520as%2520human%2520similarity%2520judgments%2520are%2520expensive%2520to%2520scale%252C%250Aand%2520Bayesian%2520models%2520of%2520human%2520inductive%2520biases%2520are%2520often%2520intractable%2520for%250Acomplex%252C%2520realistic%2520domains.%2520Here%252C%2520we%2520address%2520this%2520challenge%2520by%2520leveraging%2520a%250ABayesian%2520notion%2520of%2520generative%2520similarity%2520whereby%2520two%2520data%2520points%2520are%2520considered%250Asimilar%2520if%2520they%2520are%2520likely%2520to%2520have%2520been%2520sampled%2520from%2520the%2520same%2520distribution.%250AThis%2520measure%2520can%2520be%2520applied%2520to%2520complex%2520generative%2520processes%252C%2520including%250Aprobabilistic%2520programs.%2520We%2520incorporate%2520generative%2520similarity%2520into%2520a%2520contrastive%250Alearning%2520objective%2520to%2520enable%2520learning%2520of%2520embeddings%2520that%2520express%2520human%250Acognitive%2520representations.%2520We%2520demonstrate%2520the%2520utility%2520of%2520our%2520approach%2520by%250Ashowing%2520that%2520it%2520can%2520be%2520used%2520to%2520capture%2520human-like%2520representations%2520of%2520shape%250Aregularity%252C%2520abstract%2520Euclidean%2520geometric%2520concepts%252C%2520and%2520semantic%2520hierarchies%2520for%250Anatural%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19420v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-Aligned%20Representations%20with%20Contrastive%20Learning%20and%0A%20%20Generative%20Similarity&entry.906535625=Raja%20Marjieh%20and%20Sreejan%20Kumar%20and%20Declan%20Campbell%20and%20Liyi%20Zhang%20and%20Gianluca%20Bencomo%20and%20Jake%20Snell%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Humans%20rely%20on%20effective%20representations%20to%20learn%20from%20few%20examples%20and%0Aabstract%20useful%20information%20from%20sensory%20data.%20Inducing%20such%20representations%20in%0Amachine%20learning%20models%20has%20been%20shown%20to%20improve%20their%20performance%20on%20various%0Abenchmarks%20such%20as%20few-shot%20learning%20and%20robustness.%20However%2C%20finding%20effective%0Atraining%20procedures%20to%20achieve%20that%20goal%20can%20be%20challenging%20as%20psychologically%0Arich%20training%20data%20such%20as%20human%20similarity%20judgments%20are%20expensive%20to%20scale%2C%0Aand%20Bayesian%20models%20of%20human%20inductive%20biases%20are%20often%20intractable%20for%0Acomplex%2C%20realistic%20domains.%20Here%2C%20we%20address%20this%20challenge%20by%20leveraging%20a%0ABayesian%20notion%20of%20generative%20similarity%20whereby%20two%20data%20points%20are%20considered%0Asimilar%20if%20they%20are%20likely%20to%20have%20been%20sampled%20from%20the%20same%20distribution.%0AThis%20measure%20can%20be%20applied%20to%20complex%20generative%20processes%2C%20including%0Aprobabilistic%20programs.%20We%20incorporate%20generative%20similarity%20into%20a%20contrastive%0Alearning%20objective%20to%20enable%20learning%20of%20embeddings%20that%20express%20human%0Acognitive%20representations.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20by%0Ashowing%20that%20it%20can%20be%20used%20to%20capture%20human-like%20representations%20of%20shape%0Aregularity%2C%20abstract%20Euclidean%20geometric%20concepts%2C%20and%20semantic%20hierarchies%20for%0Anatural%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19420v3&entry.124074799=Read"},
{"title": "Imagine with the Teacher: Complete Shape in a Multi-View Distillation\n  Way", "author": "Zhanpeng Luo and Linna Wang and Guangwu Qian and Li Lu", "abstract": "  Point cloud completion aims to recover the completed 3D shape of an object\nfrom its partial observation caused by occlusion, sensor's limitation, noise,\netc. When some key semantic information is lost in the incomplete point cloud,\nthe neural network needs to infer the missing part based on the input\ninformation. Intuitively we would apply an autoencoder architecture to solve\nthis kind of problem, which take the incomplete point cloud as input and is\nsupervised by the ground truth. This process that develops model's imagination\nfrom incomplete shape to complete shape is done automatically in the latent\nspace. But the knowledge for mapping from incomplete to complete still remains\ndark and could be further explored. Motivated by the knowledge distillation's\nteacher-student learning strategy, we design a knowledge transfer way for\ncompleting 3d shape. In this work, we propose a novel View Distillation Point\nCompletion Network (VD-PCN), which solve the completion problem by a multi-view\ndistillation way. The design methodology fully leverages the orderliness of 2d\npixels, flexibleness of 2d processing and powerfulness of 2d network. Extensive\nevaluations on PCN, ShapeNet55/34, and MVP datasets confirm the effectiveness\nof our design and knowledge transfer strategy, both quantitatively and\nqualitatively. Committed to facilitate ongoing research, we will make our code\npublicly available.\n", "link": "http://arxiv.org/abs/2501.19270v1", "date": "2025-01-31", "relevancy": 2.2396, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5644}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5582}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine%20with%20the%20Teacher%3A%20Complete%20Shape%20in%20a%20Multi-View%20Distillation%0A%20%20Way&body=Title%3A%20Imagine%20with%20the%20Teacher%3A%20Complete%20Shape%20in%20a%20Multi-View%20Distillation%0A%20%20Way%0AAuthor%3A%20Zhanpeng%20Luo%20and%20Linna%20Wang%20and%20Guangwu%20Qian%20and%20Li%20Lu%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20recover%20the%20completed%203D%20shape%20of%20an%20object%0Afrom%20its%20partial%20observation%20caused%20by%20occlusion%2C%20sensor%27s%20limitation%2C%20noise%2C%0Aetc.%20When%20some%20key%20semantic%20information%20is%20lost%20in%20the%20incomplete%20point%20cloud%2C%0Athe%20neural%20network%20needs%20to%20infer%20the%20missing%20part%20based%20on%20the%20input%0Ainformation.%20Intuitively%20we%20would%20apply%20an%20autoencoder%20architecture%20to%20solve%0Athis%20kind%20of%20problem%2C%20which%20take%20the%20incomplete%20point%20cloud%20as%20input%20and%20is%0Asupervised%20by%20the%20ground%20truth.%20This%20process%20that%20develops%20model%27s%20imagination%0Afrom%20incomplete%20shape%20to%20complete%20shape%20is%20done%20automatically%20in%20the%20latent%0Aspace.%20But%20the%20knowledge%20for%20mapping%20from%20incomplete%20to%20complete%20still%20remains%0Adark%20and%20could%20be%20further%20explored.%20Motivated%20by%20the%20knowledge%20distillation%27s%0Ateacher-student%20learning%20strategy%2C%20we%20design%20a%20knowledge%20transfer%20way%20for%0Acompleting%203d%20shape.%20In%20this%20work%2C%20we%20propose%20a%20novel%20View%20Distillation%20Point%0ACompletion%20Network%20%28VD-PCN%29%2C%20which%20solve%20the%20completion%20problem%20by%20a%20multi-view%0Adistillation%20way.%20The%20design%20methodology%20fully%20leverages%20the%20orderliness%20of%202d%0Apixels%2C%20flexibleness%20of%202d%20processing%20and%20powerfulness%20of%202d%20network.%20Extensive%0Aevaluations%20on%20PCN%2C%20ShapeNet55/34%2C%20and%20MVP%20datasets%20confirm%20the%20effectiveness%0Aof%20our%20design%20and%20knowledge%20transfer%20strategy%2C%20both%20quantitatively%20and%0Aqualitatively.%20Committed%20to%20facilitate%20ongoing%20research%2C%20we%20will%20make%20our%20code%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine%2520with%2520the%2520Teacher%253A%2520Complete%2520Shape%2520in%2520a%2520Multi-View%2520Distillation%250A%2520%2520Way%26entry.906535625%3DZhanpeng%2520Luo%2520and%2520Linna%2520Wang%2520and%2520Guangwu%2520Qian%2520and%2520Li%2520Lu%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520aims%2520to%2520recover%2520the%2520completed%25203D%2520shape%2520of%2520an%2520object%250Afrom%2520its%2520partial%2520observation%2520caused%2520by%2520occlusion%252C%2520sensor%2527s%2520limitation%252C%2520noise%252C%250Aetc.%2520When%2520some%2520key%2520semantic%2520information%2520is%2520lost%2520in%2520the%2520incomplete%2520point%2520cloud%252C%250Athe%2520neural%2520network%2520needs%2520to%2520infer%2520the%2520missing%2520part%2520based%2520on%2520the%2520input%250Ainformation.%2520Intuitively%2520we%2520would%2520apply%2520an%2520autoencoder%2520architecture%2520to%2520solve%250Athis%2520kind%2520of%2520problem%252C%2520which%2520take%2520the%2520incomplete%2520point%2520cloud%2520as%2520input%2520and%2520is%250Asupervised%2520by%2520the%2520ground%2520truth.%2520This%2520process%2520that%2520develops%2520model%2527s%2520imagination%250Afrom%2520incomplete%2520shape%2520to%2520complete%2520shape%2520is%2520done%2520automatically%2520in%2520the%2520latent%250Aspace.%2520But%2520the%2520knowledge%2520for%2520mapping%2520from%2520incomplete%2520to%2520complete%2520still%2520remains%250Adark%2520and%2520could%2520be%2520further%2520explored.%2520Motivated%2520by%2520the%2520knowledge%2520distillation%2527s%250Ateacher-student%2520learning%2520strategy%252C%2520we%2520design%2520a%2520knowledge%2520transfer%2520way%2520for%250Acompleting%25203d%2520shape.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520View%2520Distillation%2520Point%250ACompletion%2520Network%2520%2528VD-PCN%2529%252C%2520which%2520solve%2520the%2520completion%2520problem%2520by%2520a%2520multi-view%250Adistillation%2520way.%2520The%2520design%2520methodology%2520fully%2520leverages%2520the%2520orderliness%2520of%25202d%250Apixels%252C%2520flexibleness%2520of%25202d%2520processing%2520and%2520powerfulness%2520of%25202d%2520network.%2520Extensive%250Aevaluations%2520on%2520PCN%252C%2520ShapeNet55/34%252C%2520and%2520MVP%2520datasets%2520confirm%2520the%2520effectiveness%250Aof%2520our%2520design%2520and%2520knowledge%2520transfer%2520strategy%252C%2520both%2520quantitatively%2520and%250Aqualitatively.%2520Committed%2520to%2520facilitate%2520ongoing%2520research%252C%2520we%2520will%2520make%2520our%2520code%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%20with%20the%20Teacher%3A%20Complete%20Shape%20in%20a%20Multi-View%20Distillation%0A%20%20Way&entry.906535625=Zhanpeng%20Luo%20and%20Linna%20Wang%20and%20Guangwu%20Qian%20and%20Li%20Lu&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20recover%20the%20completed%203D%20shape%20of%20an%20object%0Afrom%20its%20partial%20observation%20caused%20by%20occlusion%2C%20sensor%27s%20limitation%2C%20noise%2C%0Aetc.%20When%20some%20key%20semantic%20information%20is%20lost%20in%20the%20incomplete%20point%20cloud%2C%0Athe%20neural%20network%20needs%20to%20infer%20the%20missing%20part%20based%20on%20the%20input%0Ainformation.%20Intuitively%20we%20would%20apply%20an%20autoencoder%20architecture%20to%20solve%0Athis%20kind%20of%20problem%2C%20which%20take%20the%20incomplete%20point%20cloud%20as%20input%20and%20is%0Asupervised%20by%20the%20ground%20truth.%20This%20process%20that%20develops%20model%27s%20imagination%0Afrom%20incomplete%20shape%20to%20complete%20shape%20is%20done%20automatically%20in%20the%20latent%0Aspace.%20But%20the%20knowledge%20for%20mapping%20from%20incomplete%20to%20complete%20still%20remains%0Adark%20and%20could%20be%20further%20explored.%20Motivated%20by%20the%20knowledge%20distillation%27s%0Ateacher-student%20learning%20strategy%2C%20we%20design%20a%20knowledge%20transfer%20way%20for%0Acompleting%203d%20shape.%20In%20this%20work%2C%20we%20propose%20a%20novel%20View%20Distillation%20Point%0ACompletion%20Network%20%28VD-PCN%29%2C%20which%20solve%20the%20completion%20problem%20by%20a%20multi-view%0Adistillation%20way.%20The%20design%20methodology%20fully%20leverages%20the%20orderliness%20of%202d%0Apixels%2C%20flexibleness%20of%202d%20processing%20and%20powerfulness%20of%202d%20network.%20Extensive%0Aevaluations%20on%20PCN%2C%20ShapeNet55/34%2C%20and%20MVP%20datasets%20confirm%20the%20effectiveness%0Aof%20our%20design%20and%20knowledge%20transfer%20strategy%2C%20both%20quantitatively%20and%0Aqualitatively.%20Committed%20to%20facilitate%20ongoing%20research%2C%20we%20will%20make%20our%20code%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19270v1&entry.124074799=Read"},
{"title": "VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian\n  Extended Kalman Filter Integration", "author": "Jian-Yu Chen and Yi-Ru Chen and Yin-Qiao Chang and Che-Ming Li and Jann-Long Chern and Chih-Wei Huang", "abstract": "  This paper addresses the challenges in learning-based monocular positioning\nby proposing VKFPos, a novel approach that integrates Absolute Pose Regression\n(APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF)\nwithin a variational Bayesian inference framework. Our method shows that the\nessential posterior probability of the monocular positioning problem can be\ndecomposed into APR and RPR components. This decomposition is embedded in the\ndeep learning model by predicting covariances in both APR and RPR branches,\nallowing them to account for associated uncertainties. These covariances\nenhance the loss functions and facilitate EKF integration. Experimental\nevaluations on both indoor and outdoor datasets show that the single-shot APR\nbranch achieves accuracy on par with state-of-the-art methods. Furthermore, for\ntemporal positioning, where consecutive images allow for RPR and EKF\nintegration, VKFPos outperforms temporal APR and model-based integration\nmethods, achieving superior accuracy.\n", "link": "http://arxiv.org/abs/2501.18994v1", "date": "2025-01-31", "relevancy": 2.2329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5786}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5641}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VKFPos%3A%20A%20Learning-Based%20Monocular%20Positioning%20with%20Variational%20Bayesian%0A%20%20Extended%20Kalman%20Filter%20Integration&body=Title%3A%20VKFPos%3A%20A%20Learning-Based%20Monocular%20Positioning%20with%20Variational%20Bayesian%0A%20%20Extended%20Kalman%20Filter%20Integration%0AAuthor%3A%20Jian-Yu%20Chen%20and%20Yi-Ru%20Chen%20and%20Yin-Qiao%20Chang%20and%20Che-Ming%20Li%20and%20Jann-Long%20Chern%20and%20Chih-Wei%20Huang%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenges%20in%20learning-based%20monocular%20positioning%0Aby%20proposing%20VKFPos%2C%20a%20novel%20approach%20that%20integrates%20Absolute%20Pose%20Regression%0A%28APR%29%20and%20Relative%20Pose%20Regression%20%28RPR%29%20via%20an%20Extended%20Kalman%20Filter%20%28EKF%29%0Awithin%20a%20variational%20Bayesian%20inference%20framework.%20Our%20method%20shows%20that%20the%0Aessential%20posterior%20probability%20of%20the%20monocular%20positioning%20problem%20can%20be%0Adecomposed%20into%20APR%20and%20RPR%20components.%20This%20decomposition%20is%20embedded%20in%20the%0Adeep%20learning%20model%20by%20predicting%20covariances%20in%20both%20APR%20and%20RPR%20branches%2C%0Aallowing%20them%20to%20account%20for%20associated%20uncertainties.%20These%20covariances%0Aenhance%20the%20loss%20functions%20and%20facilitate%20EKF%20integration.%20Experimental%0Aevaluations%20on%20both%20indoor%20and%20outdoor%20datasets%20show%20that%20the%20single-shot%20APR%0Abranch%20achieves%20accuracy%20on%20par%20with%20state-of-the-art%20methods.%20Furthermore%2C%20for%0Atemporal%20positioning%2C%20where%20consecutive%20images%20allow%20for%20RPR%20and%20EKF%0Aintegration%2C%20VKFPos%20outperforms%20temporal%20APR%20and%20model-based%20integration%0Amethods%2C%20achieving%20superior%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVKFPos%253A%2520A%2520Learning-Based%2520Monocular%2520Positioning%2520with%2520Variational%2520Bayesian%250A%2520%2520Extended%2520Kalman%2520Filter%2520Integration%26entry.906535625%3DJian-Yu%2520Chen%2520and%2520Yi-Ru%2520Chen%2520and%2520Yin-Qiao%2520Chang%2520and%2520Che-Ming%2520Li%2520and%2520Jann-Long%2520Chern%2520and%2520Chih-Wei%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenges%2520in%2520learning-based%2520monocular%2520positioning%250Aby%2520proposing%2520VKFPos%252C%2520a%2520novel%2520approach%2520that%2520integrates%2520Absolute%2520Pose%2520Regression%250A%2528APR%2529%2520and%2520Relative%2520Pose%2520Regression%2520%2528RPR%2529%2520via%2520an%2520Extended%2520Kalman%2520Filter%2520%2528EKF%2529%250Awithin%2520a%2520variational%2520Bayesian%2520inference%2520framework.%2520Our%2520method%2520shows%2520that%2520the%250Aessential%2520posterior%2520probability%2520of%2520the%2520monocular%2520positioning%2520problem%2520can%2520be%250Adecomposed%2520into%2520APR%2520and%2520RPR%2520components.%2520This%2520decomposition%2520is%2520embedded%2520in%2520the%250Adeep%2520learning%2520model%2520by%2520predicting%2520covariances%2520in%2520both%2520APR%2520and%2520RPR%2520branches%252C%250Aallowing%2520them%2520to%2520account%2520for%2520associated%2520uncertainties.%2520These%2520covariances%250Aenhance%2520the%2520loss%2520functions%2520and%2520facilitate%2520EKF%2520integration.%2520Experimental%250Aevaluations%2520on%2520both%2520indoor%2520and%2520outdoor%2520datasets%2520show%2520that%2520the%2520single-shot%2520APR%250Abranch%2520achieves%2520accuracy%2520on%2520par%2520with%2520state-of-the-art%2520methods.%2520Furthermore%252C%2520for%250Atemporal%2520positioning%252C%2520where%2520consecutive%2520images%2520allow%2520for%2520RPR%2520and%2520EKF%250Aintegration%252C%2520VKFPos%2520outperforms%2520temporal%2520APR%2520and%2520model-based%2520integration%250Amethods%252C%2520achieving%2520superior%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VKFPos%3A%20A%20Learning-Based%20Monocular%20Positioning%20with%20Variational%20Bayesian%0A%20%20Extended%20Kalman%20Filter%20Integration&entry.906535625=Jian-Yu%20Chen%20and%20Yi-Ru%20Chen%20and%20Yin-Qiao%20Chang%20and%20Che-Ming%20Li%20and%20Jann-Long%20Chern%20and%20Chih-Wei%20Huang&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenges%20in%20learning-based%20monocular%20positioning%0Aby%20proposing%20VKFPos%2C%20a%20novel%20approach%20that%20integrates%20Absolute%20Pose%20Regression%0A%28APR%29%20and%20Relative%20Pose%20Regression%20%28RPR%29%20via%20an%20Extended%20Kalman%20Filter%20%28EKF%29%0Awithin%20a%20variational%20Bayesian%20inference%20framework.%20Our%20method%20shows%20that%20the%0Aessential%20posterior%20probability%20of%20the%20monocular%20positioning%20problem%20can%20be%0Adecomposed%20into%20APR%20and%20RPR%20components.%20This%20decomposition%20is%20embedded%20in%20the%0Adeep%20learning%20model%20by%20predicting%20covariances%20in%20both%20APR%20and%20RPR%20branches%2C%0Aallowing%20them%20to%20account%20for%20associated%20uncertainties.%20These%20covariances%0Aenhance%20the%20loss%20functions%20and%20facilitate%20EKF%20integration.%20Experimental%0Aevaluations%20on%20both%20indoor%20and%20outdoor%20datasets%20show%20that%20the%20single-shot%20APR%0Abranch%20achieves%20accuracy%20on%20par%20with%20state-of-the-art%20methods.%20Furthermore%2C%20for%0Atemporal%20positioning%2C%20where%20consecutive%20images%20allow%20for%20RPR%20and%20EKF%0Aintegration%2C%20VKFPos%20outperforms%20temporal%20APR%20and%20model-based%20integration%0Amethods%2C%20achieving%20superior%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18994v1&entry.124074799=Read"},
{"title": "Integrating Semi-Supervised and Active Learning for Semantic\n  Segmentation", "author": "Wanli Ma and Oktay Karakus and Paul L. Rosin", "abstract": "  In this paper, we propose a novel active learning approach integrated with an\nimproved semi-supervised learning framework to reduce the cost of manual\nannotation and enhance model performance. Our proposed approach effectively\nleverages both the labelled data selected through active learning and the\nunlabelled data excluded from the selection process. The proposed active\nlearning approach pinpoints areas where the pseudo-labels are likely to be\ninaccurate. Then, an automatic and efficient pseudo-label auto-refinement\n(PLAR) module is proposed to correct pixels with potentially erroneous\npseudo-labels by comparing their feature representations with those of labelled\nregions. This approach operates without increasing the labelling budget and is\nbased on the cluster assumption, which states that pixels belonging to the same\nclass should exhibit similar representations in feature space. Furthermore,\nmanual labelling is only applied to the most difficult and uncertain areas in\nunlabelled data, where insufficient information prevents the PLAR module from\nmaking a decision. We evaluated the proposed hybrid semi-supervised active\nlearning framework on two benchmark datasets, one from natural and the other\nfrom remote sensing imagery domains. In both cases, it outperformed\nstate-of-the-art methods in the semantic segmentation task.\n", "link": "http://arxiv.org/abs/2501.19227v1", "date": "2025-01-31", "relevancy": 2.2265, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Semi-Supervised%20and%20Active%20Learning%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Integrating%20Semi-Supervised%20and%20Active%20Learning%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Wanli%20Ma%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20active%20learning%20approach%20integrated%20with%20an%0Aimproved%20semi-supervised%20learning%20framework%20to%20reduce%20the%20cost%20of%20manual%0Aannotation%20and%20enhance%20model%20performance.%20Our%20proposed%20approach%20effectively%0Aleverages%20both%20the%20labelled%20data%20selected%20through%20active%20learning%20and%20the%0Aunlabelled%20data%20excluded%20from%20the%20selection%20process.%20The%20proposed%20active%0Alearning%20approach%20pinpoints%20areas%20where%20the%20pseudo-labels%20are%20likely%20to%20be%0Ainaccurate.%20Then%2C%20an%20automatic%20and%20efficient%20pseudo-label%20auto-refinement%0A%28PLAR%29%20module%20is%20proposed%20to%20correct%20pixels%20with%20potentially%20erroneous%0Apseudo-labels%20by%20comparing%20their%20feature%20representations%20with%20those%20of%20labelled%0Aregions.%20This%20approach%20operates%20without%20increasing%20the%20labelling%20budget%20and%20is%0Abased%20on%20the%20cluster%20assumption%2C%20which%20states%20that%20pixels%20belonging%20to%20the%20same%0Aclass%20should%20exhibit%20similar%20representations%20in%20feature%20space.%20Furthermore%2C%0Amanual%20labelling%20is%20only%20applied%20to%20the%20most%20difficult%20and%20uncertain%20areas%20in%0Aunlabelled%20data%2C%20where%20insufficient%20information%20prevents%20the%20PLAR%20module%20from%0Amaking%20a%20decision.%20We%20evaluated%20the%20proposed%20hybrid%20semi-supervised%20active%0Alearning%20framework%20on%20two%20benchmark%20datasets%2C%20one%20from%20natural%20and%20the%20other%0Afrom%20remote%20sensing%20imagery%20domains.%20In%20both%20cases%2C%20it%20outperformed%0Astate-of-the-art%20methods%20in%20the%20semantic%20segmentation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Semi-Supervised%2520and%2520Active%2520Learning%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DWanli%2520Ma%2520and%2520Oktay%2520Karakus%2520and%2520Paul%2520L.%2520Rosin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520active%2520learning%2520approach%2520integrated%2520with%2520an%250Aimproved%2520semi-supervised%2520learning%2520framework%2520to%2520reduce%2520the%2520cost%2520of%2520manual%250Aannotation%2520and%2520enhance%2520model%2520performance.%2520Our%2520proposed%2520approach%2520effectively%250Aleverages%2520both%2520the%2520labelled%2520data%2520selected%2520through%2520active%2520learning%2520and%2520the%250Aunlabelled%2520data%2520excluded%2520from%2520the%2520selection%2520process.%2520The%2520proposed%2520active%250Alearning%2520approach%2520pinpoints%2520areas%2520where%2520the%2520pseudo-labels%2520are%2520likely%2520to%2520be%250Ainaccurate.%2520Then%252C%2520an%2520automatic%2520and%2520efficient%2520pseudo-label%2520auto-refinement%250A%2528PLAR%2529%2520module%2520is%2520proposed%2520to%2520correct%2520pixels%2520with%2520potentially%2520erroneous%250Apseudo-labels%2520by%2520comparing%2520their%2520feature%2520representations%2520with%2520those%2520of%2520labelled%250Aregions.%2520This%2520approach%2520operates%2520without%2520increasing%2520the%2520labelling%2520budget%2520and%2520is%250Abased%2520on%2520the%2520cluster%2520assumption%252C%2520which%2520states%2520that%2520pixels%2520belonging%2520to%2520the%2520same%250Aclass%2520should%2520exhibit%2520similar%2520representations%2520in%2520feature%2520space.%2520Furthermore%252C%250Amanual%2520labelling%2520is%2520only%2520applied%2520to%2520the%2520most%2520difficult%2520and%2520uncertain%2520areas%2520in%250Aunlabelled%2520data%252C%2520where%2520insufficient%2520information%2520prevents%2520the%2520PLAR%2520module%2520from%250Amaking%2520a%2520decision.%2520We%2520evaluated%2520the%2520proposed%2520hybrid%2520semi-supervised%2520active%250Alearning%2520framework%2520on%2520two%2520benchmark%2520datasets%252C%2520one%2520from%2520natural%2520and%2520the%2520other%250Afrom%2520remote%2520sensing%2520imagery%2520domains.%2520In%2520both%2520cases%252C%2520it%2520outperformed%250Astate-of-the-art%2520methods%2520in%2520the%2520semantic%2520segmentation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Semi-Supervised%20and%20Active%20Learning%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Wanli%20Ma%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20active%20learning%20approach%20integrated%20with%20an%0Aimproved%20semi-supervised%20learning%20framework%20to%20reduce%20the%20cost%20of%20manual%0Aannotation%20and%20enhance%20model%20performance.%20Our%20proposed%20approach%20effectively%0Aleverages%20both%20the%20labelled%20data%20selected%20through%20active%20learning%20and%20the%0Aunlabelled%20data%20excluded%20from%20the%20selection%20process.%20The%20proposed%20active%0Alearning%20approach%20pinpoints%20areas%20where%20the%20pseudo-labels%20are%20likely%20to%20be%0Ainaccurate.%20Then%2C%20an%20automatic%20and%20efficient%20pseudo-label%20auto-refinement%0A%28PLAR%29%20module%20is%20proposed%20to%20correct%20pixels%20with%20potentially%20erroneous%0Apseudo-labels%20by%20comparing%20their%20feature%20representations%20with%20those%20of%20labelled%0Aregions.%20This%20approach%20operates%20without%20increasing%20the%20labelling%20budget%20and%20is%0Abased%20on%20the%20cluster%20assumption%2C%20which%20states%20that%20pixels%20belonging%20to%20the%20same%0Aclass%20should%20exhibit%20similar%20representations%20in%20feature%20space.%20Furthermore%2C%0Amanual%20labelling%20is%20only%20applied%20to%20the%20most%20difficult%20and%20uncertain%20areas%20in%0Aunlabelled%20data%2C%20where%20insufficient%20information%20prevents%20the%20PLAR%20module%20from%0Amaking%20a%20decision.%20We%20evaluated%20the%20proposed%20hybrid%20semi-supervised%20active%0Alearning%20framework%20on%20two%20benchmark%20datasets%2C%20one%20from%20natural%20and%20the%20other%0Afrom%20remote%20sensing%20imagery%20domains.%20In%20both%20cases%2C%20it%20outperformed%0Astate-of-the-art%20methods%20in%20the%20semantic%20segmentation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19227v1&entry.124074799=Read"},
{"title": "Referential communication in heterogeneous communities of pre-trained\n  visual deep networks", "author": "Mat\u00e9o Mahaut and Francesca Franzon and Roberto Dess\u00ec and Marco Baroni", "abstract": "  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of referential communication\nin a community of heterogeneous state-of-the-art pre-trained visual networks,\nshowing that they can develop, in a self-supervised way, a shared protocol to\nrefer to a target object among a set of candidates. This shared protocol can\nalso be used, to some extent, to communicate about previously unseen object\ncategories of different granularity. Moreover, a visual network that was not\ninitially part of an existing community can learn the community's protocol with\nremarkable ease. Finally, we study, both qualitatively and quantitatively, the\nproperties of the emergent protocol, providing some evidence that it is\ncapturing high-level semantic features of objects.\n", "link": "http://arxiv.org/abs/2302.08913v6", "date": "2025-01-31", "relevancy": 2.2225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&body=Title%3A%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks%0AAuthor%3A%20Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni%0AAbstract%3A%20%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20referential%20communication%0Ain%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%20visual%20networks%2C%0Ashowing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%20shared%20protocol%20to%0Arefer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%20shared%20protocol%20can%0Aalso%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%20previously%20unseen%20object%0Acategories%20of%20different%20granularity.%20Moreover%2C%20a%20visual%20network%20that%20was%20not%0Ainitially%20part%20of%20an%20existing%20community%20can%20learn%20the%20community%27s%20protocol%20with%0Aremarkable%20ease.%20Finally%2C%20we%20study%2C%20both%20qualitatively%20and%20quantitatively%2C%20the%0Aproperties%20of%20the%20emergent%20protocol%2C%20providing%20some%20evidence%20that%20it%20is%0Acapturing%20high-level%20semantic%20features%20of%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08913v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferential%2520communication%2520in%2520heterogeneous%2520communities%2520of%2520pre-trained%250A%2520%2520visual%2520deep%2520networks%26entry.906535625%3DMat%25C3%25A9o%2520Mahaut%2520and%2520Francesca%2520Franzon%2520and%2520Roberto%2520Dess%25C3%25AC%2520and%2520Marco%2520Baroni%26entry.1292438233%3D%2520%2520As%2520large%2520pre-trained%2520image-processing%2520neural%2520networks%2520are%2520being%2520embedded%2520in%250Aautonomous%2520agents%2520such%2520as%2520self-driving%2520cars%2520or%2520robots%252C%2520the%2520question%2520arises%2520of%250Ahow%2520such%2520systems%2520can%2520communicate%2520with%2520each%2520other%2520about%2520the%2520surrounding%2520world%252C%250Adespite%2520their%2520different%2520architectures%2520and%2520training%2520regimes.%2520As%2520a%2520first%2520step%2520in%250Athis%2520direction%252C%2520we%2520systematically%2520explore%2520the%2520task%2520of%2520referential%2520communication%250Ain%2520a%2520community%2520of%2520heterogeneous%2520state-of-the-art%2520pre-trained%2520visual%2520networks%252C%250Ashowing%2520that%2520they%2520can%2520develop%252C%2520in%2520a%2520self-supervised%2520way%252C%2520a%2520shared%2520protocol%2520to%250Arefer%2520to%2520a%2520target%2520object%2520among%2520a%2520set%2520of%2520candidates.%2520This%2520shared%2520protocol%2520can%250Aalso%2520be%2520used%252C%2520to%2520some%2520extent%252C%2520to%2520communicate%2520about%2520previously%2520unseen%2520object%250Acategories%2520of%2520different%2520granularity.%2520Moreover%252C%2520a%2520visual%2520network%2520that%2520was%2520not%250Ainitially%2520part%2520of%2520an%2520existing%2520community%2520can%2520learn%2520the%2520community%2527s%2520protocol%2520with%250Aremarkable%2520ease.%2520Finally%252C%2520we%2520study%252C%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520the%250Aproperties%2520of%2520the%2520emergent%2520protocol%252C%2520providing%2520some%2520evidence%2520that%2520it%2520is%250Acapturing%2520high-level%2520semantic%2520features%2520of%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08913v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&entry.906535625=Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni&entry.1292438233=%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20referential%20communication%0Ain%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%20visual%20networks%2C%0Ashowing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%20shared%20protocol%20to%0Arefer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%20shared%20protocol%20can%0Aalso%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%20previously%20unseen%20object%0Acategories%20of%20different%20granularity.%20Moreover%2C%20a%20visual%20network%20that%20was%20not%0Ainitially%20part%20of%20an%20existing%20community%20can%20learn%20the%20community%27s%20protocol%20with%0Aremarkable%20ease.%20Finally%2C%20we%20study%2C%20both%20qualitatively%20and%20quantitatively%2C%20the%0Aproperties%20of%20the%20emergent%20protocol%2C%20providing%20some%20evidence%20that%20it%20is%0Acapturing%20high-level%20semantic%20features%20of%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08913v6&entry.124074799=Read"},
{"title": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding", "author": "Doohyuk Jang and Sihwan Park and June Yong Yang and Yeonsung Jung and Jihun Yun and Souvik Kundu and Sung-Yub Kim and Eunho Yang", "abstract": "  Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model.\n", "link": "http://arxiv.org/abs/2410.03355v2", "date": "2025-01-31", "relevancy": 2.2224, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6034}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding&body=Title%3A%20LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding%0AAuthor%3A%20Doohyuk%20Jang%20and%20Sihwan%20Park%20and%20June%20Yong%20Yang%20and%20Yeonsung%20Jung%20and%20Jihun%20Yun%20and%20Souvik%20Kundu%20and%20Sung-Yub%20Kim%20and%20Eunho%20Yang%0AAbstract%3A%20%20%20Auto-Regressive%20%28AR%29%20models%20have%20recently%20gained%20prominence%20in%20image%0Ageneration%2C%20often%20matching%20or%20even%20surpassing%20the%20performance%20of%20diffusion%0Amodels.%20However%2C%20one%20major%20limitation%20of%20AR%20models%20is%20their%20sequential%20nature%2C%0Awhich%20processes%20tokens%20one%20at%20a%20time%2C%20slowing%20down%20generation%20compared%20to%0Amodels%20like%20GANs%20or%20diffusion-based%20methods%20that%20operate%20more%20efficiently.%0AWhile%20speculative%20decoding%20has%20proven%20effective%20for%20accelerating%20LLMs%20by%0Agenerating%20multiple%20tokens%20in%20a%20single%20forward%2C%20its%20application%20in%20visual%20AR%0Amodels%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20identify%20a%20challenge%20in%0Athis%20setting%2C%20which%20we%20term%20%5Ctextit%7Btoken%20selection%20ambiguity%7D%2C%20wherein%20visual%0AAR%20models%20frequently%20assign%20uniformly%20low%20probabilities%20to%20tokens%2C%20hampering%0Athe%20performance%20of%20speculative%20decoding.%20To%20overcome%20this%20challenge%2C%20we%20propose%0Aa%20relaxed%20acceptance%20condition%20referred%20to%20as%20LANTERN%20that%20leverages%20the%0Ainterchangeability%20of%20tokens%20in%20latent%20space.%20This%20relaxation%20restores%20the%0Aeffectiveness%20of%20speculative%20decoding%20in%20visual%20AR%20models%20by%20enabling%20more%0Aflexible%20use%20of%20candidate%20tokens%20that%20would%20otherwise%20be%20prematurely%20rejected.%0AFurthermore%2C%20by%20incorporating%20a%20total%20variation%20distance%20bound%2C%20we%20ensure%20that%0Athese%20speed%20gains%20are%20achieved%20without%20significantly%20compromising%20image%20quality%0Aor%20semantic%20coherence.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20providing%20a%20substantial%20speed-up%20over%20speculative%20decoding.%20In%0Aspecific%2C%20compared%20to%20a%20na%5C%22ive%20application%20of%20the%20state-of-the-art%20speculative%0Adecoding%2C%20LANTERN%20increases%20speed-ups%20by%20%24%5Cmathbf%7B1.75%7D%5Ctimes%24%20and%0A%24%5Cmathbf%7B1.82%7D%5Ctimes%24%2C%20as%20compared%20to%20greedy%20decoding%20and%20random%20sampling%2C%0Arespectively%2C%20when%20applied%20to%20LlamaGen%2C%20a%20contemporary%20visual%20AR%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLANTERN%253A%2520Accelerating%2520Visual%2520Autoregressive%2520Models%2520with%2520Relaxed%250A%2520%2520Speculative%2520Decoding%26entry.906535625%3DDoohyuk%2520Jang%2520and%2520Sihwan%2520Park%2520and%2520June%2520Yong%2520Yang%2520and%2520Yeonsung%2520Jung%2520and%2520Jihun%2520Yun%2520and%2520Souvik%2520Kundu%2520and%2520Sung-Yub%2520Kim%2520and%2520Eunho%2520Yang%26entry.1292438233%3D%2520%2520Auto-Regressive%2520%2528AR%2529%2520models%2520have%2520recently%2520gained%2520prominence%2520in%2520image%250Ageneration%252C%2520often%2520matching%2520or%2520even%2520surpassing%2520the%2520performance%2520of%2520diffusion%250Amodels.%2520However%252C%2520one%2520major%2520limitation%2520of%2520AR%2520models%2520is%2520their%2520sequential%2520nature%252C%250Awhich%2520processes%2520tokens%2520one%2520at%2520a%2520time%252C%2520slowing%2520down%2520generation%2520compared%2520to%250Amodels%2520like%2520GANs%2520or%2520diffusion-based%2520methods%2520that%2520operate%2520more%2520efficiently.%250AWhile%2520speculative%2520decoding%2520has%2520proven%2520effective%2520for%2520accelerating%2520LLMs%2520by%250Agenerating%2520multiple%2520tokens%2520in%2520a%2520single%2520forward%252C%2520its%2520application%2520in%2520visual%2520AR%250Amodels%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520identify%2520a%2520challenge%2520in%250Athis%2520setting%252C%2520which%2520we%2520term%2520%255Ctextit%257Btoken%2520selection%2520ambiguity%257D%252C%2520wherein%2520visual%250AAR%2520models%2520frequently%2520assign%2520uniformly%2520low%2520probabilities%2520to%2520tokens%252C%2520hampering%250Athe%2520performance%2520of%2520speculative%2520decoding.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%250Aa%2520relaxed%2520acceptance%2520condition%2520referred%2520to%2520as%2520LANTERN%2520that%2520leverages%2520the%250Ainterchangeability%2520of%2520tokens%2520in%2520latent%2520space.%2520This%2520relaxation%2520restores%2520the%250Aeffectiveness%2520of%2520speculative%2520decoding%2520in%2520visual%2520AR%2520models%2520by%2520enabling%2520more%250Aflexible%2520use%2520of%2520candidate%2520tokens%2520that%2520would%2520otherwise%2520be%2520prematurely%2520rejected.%250AFurthermore%252C%2520by%2520incorporating%2520a%2520total%2520variation%2520distance%2520bound%252C%2520we%2520ensure%2520that%250Athese%2520speed%2520gains%2520are%2520achieved%2520without%2520significantly%2520compromising%2520image%2520quality%250Aor%2520semantic%2520coherence.%2520Experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520our%250Amethod%2520in%2520providing%2520a%2520substantial%2520speed-up%2520over%2520speculative%2520decoding.%2520In%250Aspecific%252C%2520compared%2520to%2520a%2520na%255C%2522ive%2520application%2520of%2520the%2520state-of-the-art%2520speculative%250Adecoding%252C%2520LANTERN%2520increases%2520speed-ups%2520by%2520%2524%255Cmathbf%257B1.75%257D%255Ctimes%2524%2520and%250A%2524%255Cmathbf%257B1.82%257D%255Ctimes%2524%252C%2520as%2520compared%2520to%2520greedy%2520decoding%2520and%2520random%2520sampling%252C%250Arespectively%252C%2520when%2520applied%2520to%2520LlamaGen%252C%2520a%2520contemporary%2520visual%2520AR%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding&entry.906535625=Doohyuk%20Jang%20and%20Sihwan%20Park%20and%20June%20Yong%20Yang%20and%20Yeonsung%20Jung%20and%20Jihun%20Yun%20and%20Souvik%20Kundu%20and%20Sung-Yub%20Kim%20and%20Eunho%20Yang&entry.1292438233=%20%20Auto-Regressive%20%28AR%29%20models%20have%20recently%20gained%20prominence%20in%20image%0Ageneration%2C%20often%20matching%20or%20even%20surpassing%20the%20performance%20of%20diffusion%0Amodels.%20However%2C%20one%20major%20limitation%20of%20AR%20models%20is%20their%20sequential%20nature%2C%0Awhich%20processes%20tokens%20one%20at%20a%20time%2C%20slowing%20down%20generation%20compared%20to%0Amodels%20like%20GANs%20or%20diffusion-based%20methods%20that%20operate%20more%20efficiently.%0AWhile%20speculative%20decoding%20has%20proven%20effective%20for%20accelerating%20LLMs%20by%0Agenerating%20multiple%20tokens%20in%20a%20single%20forward%2C%20its%20application%20in%20visual%20AR%0Amodels%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20identify%20a%20challenge%20in%0Athis%20setting%2C%20which%20we%20term%20%5Ctextit%7Btoken%20selection%20ambiguity%7D%2C%20wherein%20visual%0AAR%20models%20frequently%20assign%20uniformly%20low%20probabilities%20to%20tokens%2C%20hampering%0Athe%20performance%20of%20speculative%20decoding.%20To%20overcome%20this%20challenge%2C%20we%20propose%0Aa%20relaxed%20acceptance%20condition%20referred%20to%20as%20LANTERN%20that%20leverages%20the%0Ainterchangeability%20of%20tokens%20in%20latent%20space.%20This%20relaxation%20restores%20the%0Aeffectiveness%20of%20speculative%20decoding%20in%20visual%20AR%20models%20by%20enabling%20more%0Aflexible%20use%20of%20candidate%20tokens%20that%20would%20otherwise%20be%20prematurely%20rejected.%0AFurthermore%2C%20by%20incorporating%20a%20total%20variation%20distance%20bound%2C%20we%20ensure%20that%0Athese%20speed%20gains%20are%20achieved%20without%20significantly%20compromising%20image%20quality%0Aor%20semantic%20coherence.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20providing%20a%20substantial%20speed-up%20over%20speculative%20decoding.%20In%0Aspecific%2C%20compared%20to%20a%20na%5C%22ive%20application%20of%20the%20state-of-the-art%20speculative%0Adecoding%2C%20LANTERN%20increases%20speed-ups%20by%20%24%5Cmathbf%7B1.75%7D%5Ctimes%24%20and%0A%24%5Cmathbf%7B1.82%7D%5Ctimes%24%2C%20as%20compared%20to%20greedy%20decoding%20and%20random%20sampling%2C%0Arespectively%2C%20when%20applied%20to%20LlamaGen%2C%20a%20contemporary%20visual%20AR%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03355v2&entry.124074799=Read"},
{"title": "Virtual airways heatmaps to optimize point of entry location in lung\n  biopsy planning systems", "author": "Debora Gil and Pere Lloret and Marta Diez-Ferrer and Carles Sanchez", "abstract": "  Purpose: We present a virtual model to optimize point of entry (POE) in lung\nbiopsy planning systems. Our model allows to compute the quality of a biopsy\nsample taken from potential POE, taking into account the margin of error that\narises from discrepancies between the orientation in the planning simulation\nand the actual orientation during the operation. Additionally, the study\nexamines the impact of the characteristics of the lesion. Methods: The quality\nof the biopsy is given by a heatmap projected onto the skeleton of a\npatient-specific model of airways. The skeleton provides a 3D representation of\nairways structure, while the heatmap intensity represents the potential amount\nof tissue that it could be extracted from each POE. This amount of tissue is\ndetermined by the intersection of the lesion with a cone that represents the\nuncertainty area in the introduction of biopsy instruments. The cone, lesion,\nand skeleton are modelled as graphical objects that define a 3D scene of the\nintervention. Results: We have simulated different settings of the intervention\nscene from a single anatomy extracted from a CT scan and two lesions with\nregular and irregular shapes. The different scenarios are simulated by\nsystematic rotation of each lesion placed at different distances from airways.\nAnalysis of the heatmaps for the different settings show a strong impact of\nlesion orientation for irregular shape and the distance for both shapes.\nConclusion: The proposed heatmaps help to visually assess the optimal POE and\nidentify whether multiple optimal POEs exist in different zones of the bronchi.\nThey also allow us to model the maximum allowable error in navigation systems\nand study which variables have the greatest influence on the success of the\noperation. Additionally, they help determine at what point this influence could\npotentially jeopardize the operation.\n", "link": "http://arxiv.org/abs/2501.19003v1", "date": "2025-01-31", "relevancy": 2.217, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.446}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20airways%20heatmaps%20to%20optimize%20point%20of%20entry%20location%20in%20lung%0A%20%20biopsy%20planning%20systems&body=Title%3A%20Virtual%20airways%20heatmaps%20to%20optimize%20point%20of%20entry%20location%20in%20lung%0A%20%20biopsy%20planning%20systems%0AAuthor%3A%20Debora%20Gil%20and%20Pere%20Lloret%20and%20Marta%20Diez-Ferrer%20and%20Carles%20Sanchez%0AAbstract%3A%20%20%20Purpose%3A%20We%20present%20a%20virtual%20model%20to%20optimize%20point%20of%20entry%20%28POE%29%20in%20lung%0Abiopsy%20planning%20systems.%20Our%20model%20allows%20to%20compute%20the%20quality%20of%20a%20biopsy%0Asample%20taken%20from%20potential%20POE%2C%20taking%20into%20account%20the%20margin%20of%20error%20that%0Aarises%20from%20discrepancies%20between%20the%20orientation%20in%20the%20planning%20simulation%0Aand%20the%20actual%20orientation%20during%20the%20operation.%20Additionally%2C%20the%20study%0Aexamines%20the%20impact%20of%20the%20characteristics%20of%20the%20lesion.%20Methods%3A%20The%20quality%0Aof%20the%20biopsy%20is%20given%20by%20a%20heatmap%20projected%20onto%20the%20skeleton%20of%20a%0Apatient-specific%20model%20of%20airways.%20The%20skeleton%20provides%20a%203D%20representation%20of%0Aairways%20structure%2C%20while%20the%20heatmap%20intensity%20represents%20the%20potential%20amount%0Aof%20tissue%20that%20it%20could%20be%20extracted%20from%20each%20POE.%20This%20amount%20of%20tissue%20is%0Adetermined%20by%20the%20intersection%20of%20the%20lesion%20with%20a%20cone%20that%20represents%20the%0Auncertainty%20area%20in%20the%20introduction%20of%20biopsy%20instruments.%20The%20cone%2C%20lesion%2C%0Aand%20skeleton%20are%20modelled%20as%20graphical%20objects%20that%20define%20a%203D%20scene%20of%20the%0Aintervention.%20Results%3A%20We%20have%20simulated%20different%20settings%20of%20the%20intervention%0Ascene%20from%20a%20single%20anatomy%20extracted%20from%20a%20CT%20scan%20and%20two%20lesions%20with%0Aregular%20and%20irregular%20shapes.%20The%20different%20scenarios%20are%20simulated%20by%0Asystematic%20rotation%20of%20each%20lesion%20placed%20at%20different%20distances%20from%20airways.%0AAnalysis%20of%20the%20heatmaps%20for%20the%20different%20settings%20show%20a%20strong%20impact%20of%0Alesion%20orientation%20for%20irregular%20shape%20and%20the%20distance%20for%20both%20shapes.%0AConclusion%3A%20The%20proposed%20heatmaps%20help%20to%20visually%20assess%20the%20optimal%20POE%20and%0Aidentify%20whether%20multiple%20optimal%20POEs%20exist%20in%20different%20zones%20of%20the%20bronchi.%0AThey%20also%20allow%20us%20to%20model%20the%20maximum%20allowable%20error%20in%20navigation%20systems%0Aand%20study%20which%20variables%20have%20the%20greatest%20influence%20on%20the%20success%20of%20the%0Aoperation.%20Additionally%2C%20they%20help%20determine%20at%20what%20point%20this%20influence%20could%0Apotentially%20jeopardize%20the%20operation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520airways%2520heatmaps%2520to%2520optimize%2520point%2520of%2520entry%2520location%2520in%2520lung%250A%2520%2520biopsy%2520planning%2520systems%26entry.906535625%3DDebora%2520Gil%2520and%2520Pere%2520Lloret%2520and%2520Marta%2520Diez-Ferrer%2520and%2520Carles%2520Sanchez%26entry.1292438233%3D%2520%2520Purpose%253A%2520We%2520present%2520a%2520virtual%2520model%2520to%2520optimize%2520point%2520of%2520entry%2520%2528POE%2529%2520in%2520lung%250Abiopsy%2520planning%2520systems.%2520Our%2520model%2520allows%2520to%2520compute%2520the%2520quality%2520of%2520a%2520biopsy%250Asample%2520taken%2520from%2520potential%2520POE%252C%2520taking%2520into%2520account%2520the%2520margin%2520of%2520error%2520that%250Aarises%2520from%2520discrepancies%2520between%2520the%2520orientation%2520in%2520the%2520planning%2520simulation%250Aand%2520the%2520actual%2520orientation%2520during%2520the%2520operation.%2520Additionally%252C%2520the%2520study%250Aexamines%2520the%2520impact%2520of%2520the%2520characteristics%2520of%2520the%2520lesion.%2520Methods%253A%2520The%2520quality%250Aof%2520the%2520biopsy%2520is%2520given%2520by%2520a%2520heatmap%2520projected%2520onto%2520the%2520skeleton%2520of%2520a%250Apatient-specific%2520model%2520of%2520airways.%2520The%2520skeleton%2520provides%2520a%25203D%2520representation%2520of%250Aairways%2520structure%252C%2520while%2520the%2520heatmap%2520intensity%2520represents%2520the%2520potential%2520amount%250Aof%2520tissue%2520that%2520it%2520could%2520be%2520extracted%2520from%2520each%2520POE.%2520This%2520amount%2520of%2520tissue%2520is%250Adetermined%2520by%2520the%2520intersection%2520of%2520the%2520lesion%2520with%2520a%2520cone%2520that%2520represents%2520the%250Auncertainty%2520area%2520in%2520the%2520introduction%2520of%2520biopsy%2520instruments.%2520The%2520cone%252C%2520lesion%252C%250Aand%2520skeleton%2520are%2520modelled%2520as%2520graphical%2520objects%2520that%2520define%2520a%25203D%2520scene%2520of%2520the%250Aintervention.%2520Results%253A%2520We%2520have%2520simulated%2520different%2520settings%2520of%2520the%2520intervention%250Ascene%2520from%2520a%2520single%2520anatomy%2520extracted%2520from%2520a%2520CT%2520scan%2520and%2520two%2520lesions%2520with%250Aregular%2520and%2520irregular%2520shapes.%2520The%2520different%2520scenarios%2520are%2520simulated%2520by%250Asystematic%2520rotation%2520of%2520each%2520lesion%2520placed%2520at%2520different%2520distances%2520from%2520airways.%250AAnalysis%2520of%2520the%2520heatmaps%2520for%2520the%2520different%2520settings%2520show%2520a%2520strong%2520impact%2520of%250Alesion%2520orientation%2520for%2520irregular%2520shape%2520and%2520the%2520distance%2520for%2520both%2520shapes.%250AConclusion%253A%2520The%2520proposed%2520heatmaps%2520help%2520to%2520visually%2520assess%2520the%2520optimal%2520POE%2520and%250Aidentify%2520whether%2520multiple%2520optimal%2520POEs%2520exist%2520in%2520different%2520zones%2520of%2520the%2520bronchi.%250AThey%2520also%2520allow%2520us%2520to%2520model%2520the%2520maximum%2520allowable%2520error%2520in%2520navigation%2520systems%250Aand%2520study%2520which%2520variables%2520have%2520the%2520greatest%2520influence%2520on%2520the%2520success%2520of%2520the%250Aoperation.%2520Additionally%252C%2520they%2520help%2520determine%2520at%2520what%2520point%2520this%2520influence%2520could%250Apotentially%2520jeopardize%2520the%2520operation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20airways%20heatmaps%20to%20optimize%20point%20of%20entry%20location%20in%20lung%0A%20%20biopsy%20planning%20systems&entry.906535625=Debora%20Gil%20and%20Pere%20Lloret%20and%20Marta%20Diez-Ferrer%20and%20Carles%20Sanchez&entry.1292438233=%20%20Purpose%3A%20We%20present%20a%20virtual%20model%20to%20optimize%20point%20of%20entry%20%28POE%29%20in%20lung%0Abiopsy%20planning%20systems.%20Our%20model%20allows%20to%20compute%20the%20quality%20of%20a%20biopsy%0Asample%20taken%20from%20potential%20POE%2C%20taking%20into%20account%20the%20margin%20of%20error%20that%0Aarises%20from%20discrepancies%20between%20the%20orientation%20in%20the%20planning%20simulation%0Aand%20the%20actual%20orientation%20during%20the%20operation.%20Additionally%2C%20the%20study%0Aexamines%20the%20impact%20of%20the%20characteristics%20of%20the%20lesion.%20Methods%3A%20The%20quality%0Aof%20the%20biopsy%20is%20given%20by%20a%20heatmap%20projected%20onto%20the%20skeleton%20of%20a%0Apatient-specific%20model%20of%20airways.%20The%20skeleton%20provides%20a%203D%20representation%20of%0Aairways%20structure%2C%20while%20the%20heatmap%20intensity%20represents%20the%20potential%20amount%0Aof%20tissue%20that%20it%20could%20be%20extracted%20from%20each%20POE.%20This%20amount%20of%20tissue%20is%0Adetermined%20by%20the%20intersection%20of%20the%20lesion%20with%20a%20cone%20that%20represents%20the%0Auncertainty%20area%20in%20the%20introduction%20of%20biopsy%20instruments.%20The%20cone%2C%20lesion%2C%0Aand%20skeleton%20are%20modelled%20as%20graphical%20objects%20that%20define%20a%203D%20scene%20of%20the%0Aintervention.%20Results%3A%20We%20have%20simulated%20different%20settings%20of%20the%20intervention%0Ascene%20from%20a%20single%20anatomy%20extracted%20from%20a%20CT%20scan%20and%20two%20lesions%20with%0Aregular%20and%20irregular%20shapes.%20The%20different%20scenarios%20are%20simulated%20by%0Asystematic%20rotation%20of%20each%20lesion%20placed%20at%20different%20distances%20from%20airways.%0AAnalysis%20of%20the%20heatmaps%20for%20the%20different%20settings%20show%20a%20strong%20impact%20of%0Alesion%20orientation%20for%20irregular%20shape%20and%20the%20distance%20for%20both%20shapes.%0AConclusion%3A%20The%20proposed%20heatmaps%20help%20to%20visually%20assess%20the%20optimal%20POE%20and%0Aidentify%20whether%20multiple%20optimal%20POEs%20exist%20in%20different%20zones%20of%20the%20bronchi.%0AThey%20also%20allow%20us%20to%20model%20the%20maximum%20allowable%20error%20in%20navigation%20systems%0Aand%20study%20which%20variables%20have%20the%20greatest%20influence%20on%20the%20success%20of%20the%0Aoperation.%20Additionally%2C%20they%20help%20determine%20at%20what%20point%20this%20influence%20could%0Apotentially%20jeopardize%20the%20operation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19003v1&entry.124074799=Read"},
{"title": "Imitation Game for Adversarial Disillusion with Multimodal Generative\n  Chain-of-Thought Role-Play", "author": "Ching-Chun Chang and Fan-Yun Chen and Shih-Hong Gu and Kai Gao and Hanrui Wang and Isao Echizen", "abstract": "  As the cornerstone of artificial intelligence, machine perception confronts a\nfundamental threat posed by adversarial illusions. These adversarial attacks\nmanifest in two primary forms: deductive illusion, where specific stimuli are\ncrafted based on the victim model's general decision logic, and inductive\nillusion, where the victim model's general decision logic is shaped by specific\nstimuli. The former exploits the model's decision boundaries to create a\nstimulus that, when applied, interferes with its decision-making process. The\nlatter reinforces a conditioned reflex in the model, embedding a backdoor\nduring its learning phase that, when triggered by a stimulus, causes aberrant\nbehaviours. The multifaceted nature of adversarial illusions calls for a\nunified defence framework, addressing vulnerabilities across various forms of\nattack. In this study, we propose a disillusion paradigm based on the concept\nof an imitation game. At the heart of the imitation game lies a multimodal\ngenerative agent, steered by chain-of-thought reasoning, which observes,\ninternalises and reconstructs the semantic essence of a sample, liberated from\nthe classic pursuit of reversing the sample to its original state. As a proof\nof concept, we conduct experimental simulations using a multimodal generative\ndialogue agent and evaluates the methodology under a variety of attack\nscenarios.\n", "link": "http://arxiv.org/abs/2501.19143v1", "date": "2025-01-31", "relevancy": 2.2052, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5636}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5465}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Game%20for%20Adversarial%20Disillusion%20with%20Multimodal%20Generative%0A%20%20Chain-of-Thought%20Role-Play&body=Title%3A%20Imitation%20Game%20for%20Adversarial%20Disillusion%20with%20Multimodal%20Generative%0A%20%20Chain-of-Thought%20Role-Play%0AAuthor%3A%20Ching-Chun%20Chang%20and%20Fan-Yun%20Chen%20and%20Shih-Hong%20Gu%20and%20Kai%20Gao%20and%20Hanrui%20Wang%20and%20Isao%20Echizen%0AAbstract%3A%20%20%20As%20the%20cornerstone%20of%20artificial%20intelligence%2C%20machine%20perception%20confronts%20a%0Afundamental%20threat%20posed%20by%20adversarial%20illusions.%20These%20adversarial%20attacks%0Amanifest%20in%20two%20primary%20forms%3A%20deductive%20illusion%2C%20where%20specific%20stimuli%20are%0Acrafted%20based%20on%20the%20victim%20model%27s%20general%20decision%20logic%2C%20and%20inductive%0Aillusion%2C%20where%20the%20victim%20model%27s%20general%20decision%20logic%20is%20shaped%20by%20specific%0Astimuli.%20The%20former%20exploits%20the%20model%27s%20decision%20boundaries%20to%20create%20a%0Astimulus%20that%2C%20when%20applied%2C%20interferes%20with%20its%20decision-making%20process.%20The%0Alatter%20reinforces%20a%20conditioned%20reflex%20in%20the%20model%2C%20embedding%20a%20backdoor%0Aduring%20its%20learning%20phase%20that%2C%20when%20triggered%20by%20a%20stimulus%2C%20causes%20aberrant%0Abehaviours.%20The%20multifaceted%20nature%20of%20adversarial%20illusions%20calls%20for%20a%0Aunified%20defence%20framework%2C%20addressing%20vulnerabilities%20across%20various%20forms%20of%0Aattack.%20In%20this%20study%2C%20we%20propose%20a%20disillusion%20paradigm%20based%20on%20the%20concept%0Aof%20an%20imitation%20game.%20At%20the%20heart%20of%20the%20imitation%20game%20lies%20a%20multimodal%0Agenerative%20agent%2C%20steered%20by%20chain-of-thought%20reasoning%2C%20which%20observes%2C%0Ainternalises%20and%20reconstructs%20the%20semantic%20essence%20of%20a%20sample%2C%20liberated%20from%0Athe%20classic%20pursuit%20of%20reversing%20the%20sample%20to%20its%20original%20state.%20As%20a%20proof%0Aof%20concept%2C%20we%20conduct%20experimental%20simulations%20using%20a%20multimodal%20generative%0Adialogue%20agent%20and%20evaluates%20the%20methodology%20under%20a%20variety%20of%20attack%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Game%2520for%2520Adversarial%2520Disillusion%2520with%2520Multimodal%2520Generative%250A%2520%2520Chain-of-Thought%2520Role-Play%26entry.906535625%3DChing-Chun%2520Chang%2520and%2520Fan-Yun%2520Chen%2520and%2520Shih-Hong%2520Gu%2520and%2520Kai%2520Gao%2520and%2520Hanrui%2520Wang%2520and%2520Isao%2520Echizen%26entry.1292438233%3D%2520%2520As%2520the%2520cornerstone%2520of%2520artificial%2520intelligence%252C%2520machine%2520perception%2520confronts%2520a%250Afundamental%2520threat%2520posed%2520by%2520adversarial%2520illusions.%2520These%2520adversarial%2520attacks%250Amanifest%2520in%2520two%2520primary%2520forms%253A%2520deductive%2520illusion%252C%2520where%2520specific%2520stimuli%2520are%250Acrafted%2520based%2520on%2520the%2520victim%2520model%2527s%2520general%2520decision%2520logic%252C%2520and%2520inductive%250Aillusion%252C%2520where%2520the%2520victim%2520model%2527s%2520general%2520decision%2520logic%2520is%2520shaped%2520by%2520specific%250Astimuli.%2520The%2520former%2520exploits%2520the%2520model%2527s%2520decision%2520boundaries%2520to%2520create%2520a%250Astimulus%2520that%252C%2520when%2520applied%252C%2520interferes%2520with%2520its%2520decision-making%2520process.%2520The%250Alatter%2520reinforces%2520a%2520conditioned%2520reflex%2520in%2520the%2520model%252C%2520embedding%2520a%2520backdoor%250Aduring%2520its%2520learning%2520phase%2520that%252C%2520when%2520triggered%2520by%2520a%2520stimulus%252C%2520causes%2520aberrant%250Abehaviours.%2520The%2520multifaceted%2520nature%2520of%2520adversarial%2520illusions%2520calls%2520for%2520a%250Aunified%2520defence%2520framework%252C%2520addressing%2520vulnerabilities%2520across%2520various%2520forms%2520of%250Aattack.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520disillusion%2520paradigm%2520based%2520on%2520the%2520concept%250Aof%2520an%2520imitation%2520game.%2520At%2520the%2520heart%2520of%2520the%2520imitation%2520game%2520lies%2520a%2520multimodal%250Agenerative%2520agent%252C%2520steered%2520by%2520chain-of-thought%2520reasoning%252C%2520which%2520observes%252C%250Ainternalises%2520and%2520reconstructs%2520the%2520semantic%2520essence%2520of%2520a%2520sample%252C%2520liberated%2520from%250Athe%2520classic%2520pursuit%2520of%2520reversing%2520the%2520sample%2520to%2520its%2520original%2520state.%2520As%2520a%2520proof%250Aof%2520concept%252C%2520we%2520conduct%2520experimental%2520simulations%2520using%2520a%2520multimodal%2520generative%250Adialogue%2520agent%2520and%2520evaluates%2520the%2520methodology%2520under%2520a%2520variety%2520of%2520attack%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Game%20for%20Adversarial%20Disillusion%20with%20Multimodal%20Generative%0A%20%20Chain-of-Thought%20Role-Play&entry.906535625=Ching-Chun%20Chang%20and%20Fan-Yun%20Chen%20and%20Shih-Hong%20Gu%20and%20Kai%20Gao%20and%20Hanrui%20Wang%20and%20Isao%20Echizen&entry.1292438233=%20%20As%20the%20cornerstone%20of%20artificial%20intelligence%2C%20machine%20perception%20confronts%20a%0Afundamental%20threat%20posed%20by%20adversarial%20illusions.%20These%20adversarial%20attacks%0Amanifest%20in%20two%20primary%20forms%3A%20deductive%20illusion%2C%20where%20specific%20stimuli%20are%0Acrafted%20based%20on%20the%20victim%20model%27s%20general%20decision%20logic%2C%20and%20inductive%0Aillusion%2C%20where%20the%20victim%20model%27s%20general%20decision%20logic%20is%20shaped%20by%20specific%0Astimuli.%20The%20former%20exploits%20the%20model%27s%20decision%20boundaries%20to%20create%20a%0Astimulus%20that%2C%20when%20applied%2C%20interferes%20with%20its%20decision-making%20process.%20The%0Alatter%20reinforces%20a%20conditioned%20reflex%20in%20the%20model%2C%20embedding%20a%20backdoor%0Aduring%20its%20learning%20phase%20that%2C%20when%20triggered%20by%20a%20stimulus%2C%20causes%20aberrant%0Abehaviours.%20The%20multifaceted%20nature%20of%20adversarial%20illusions%20calls%20for%20a%0Aunified%20defence%20framework%2C%20addressing%20vulnerabilities%20across%20various%20forms%20of%0Aattack.%20In%20this%20study%2C%20we%20propose%20a%20disillusion%20paradigm%20based%20on%20the%20concept%0Aof%20an%20imitation%20game.%20At%20the%20heart%20of%20the%20imitation%20game%20lies%20a%20multimodal%0Agenerative%20agent%2C%20steered%20by%20chain-of-thought%20reasoning%2C%20which%20observes%2C%0Ainternalises%20and%20reconstructs%20the%20semantic%20essence%20of%20a%20sample%2C%20liberated%20from%0Athe%20classic%20pursuit%20of%20reversing%20the%20sample%20to%20its%20original%20state.%20As%20a%20proof%0Aof%20concept%2C%20we%20conduct%20experimental%20simulations%20using%20a%20multimodal%20generative%0Adialogue%20agent%20and%20evaluates%20the%20methodology%20under%20a%20variety%20of%20attack%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19143v1&entry.124074799=Read"},
{"title": "Collaboratively Self-supervised Video Representation Learning for Action\n  Recognition", "author": "Jie Zhang and Zhifan Wan and Lanqing Hu and Stephen Lin and Shuzhe Wu and Shiguang Shan", "abstract": "  Considering the close connection between action recognition and human pose\nestimation, we design a Collaboratively Self-supervised Video Representation\n(CSVR) learning framework specific to action recognition by jointly factoring\nin generative pose prediction and discriminative context matching as pretext\ntasks. Specifically, our CSVR consists of three branches: a generative pose\nprediction branch, a discriminative context matching branch, and a video\ngenerating branch. Among them, the first one encodes dynamic motion feature by\nutilizing Conditional-GAN to predict the human poses of future frames, and the\nsecond branch extracts static context features by contrasting positive and\nnegative video feature and I-frame feature pairs. The third branch is designed\nto generate both current and future video frames, for the purpose of\ncollaboratively improving dynamic motion features and static context features.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance on multiple popular video datasets.\n", "link": "http://arxiv.org/abs/2401.07584v2", "date": "2025-01-31", "relevancy": 2.2049, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5541}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5507}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaboratively%20Self-supervised%20Video%20Representation%20Learning%20for%20Action%0A%20%20Recognition&body=Title%3A%20Collaboratively%20Self-supervised%20Video%20Representation%20Learning%20for%20Action%0A%20%20Recognition%0AAuthor%3A%20Jie%20Zhang%20and%20Zhifan%20Wan%20and%20Lanqing%20Hu%20and%20Stephen%20Lin%20and%20Shuzhe%20Wu%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20Considering%20the%20close%20connection%20between%20action%20recognition%20and%20human%20pose%0Aestimation%2C%20we%20design%20a%20Collaboratively%20Self-supervised%20Video%20Representation%0A%28CSVR%29%20learning%20framework%20specific%20to%20action%20recognition%20by%20jointly%20factoring%0Ain%20generative%20pose%20prediction%20and%20discriminative%20context%20matching%20as%20pretext%0Atasks.%20Specifically%2C%20our%20CSVR%20consists%20of%20three%20branches%3A%20a%20generative%20pose%0Aprediction%20branch%2C%20a%20discriminative%20context%20matching%20branch%2C%20and%20a%20video%0Agenerating%20branch.%20Among%20them%2C%20the%20first%20one%20encodes%20dynamic%20motion%20feature%20by%0Autilizing%20Conditional-GAN%20to%20predict%20the%20human%20poses%20of%20future%20frames%2C%20and%20the%0Asecond%20branch%20extracts%20static%20context%20features%20by%20contrasting%20positive%20and%0Anegative%20video%20feature%20and%20I-frame%20feature%20pairs.%20The%20third%20branch%20is%20designed%0Ato%20generate%20both%20current%20and%20future%20video%20frames%2C%20for%20the%20purpose%20of%0Acollaboratively%20improving%20dynamic%20motion%20features%20and%20static%20context%20features.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20popular%20video%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaboratively%2520Self-supervised%2520Video%2520Representation%2520Learning%2520for%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJie%2520Zhang%2520and%2520Zhifan%2520Wan%2520and%2520Lanqing%2520Hu%2520and%2520Stephen%2520Lin%2520and%2520Shuzhe%2520Wu%2520and%2520Shiguang%2520Shan%26entry.1292438233%3D%2520%2520Considering%2520the%2520close%2520connection%2520between%2520action%2520recognition%2520and%2520human%2520pose%250Aestimation%252C%2520we%2520design%2520a%2520Collaboratively%2520Self-supervised%2520Video%2520Representation%250A%2528CSVR%2529%2520learning%2520framework%2520specific%2520to%2520action%2520recognition%2520by%2520jointly%2520factoring%250Ain%2520generative%2520pose%2520prediction%2520and%2520discriminative%2520context%2520matching%2520as%2520pretext%250Atasks.%2520Specifically%252C%2520our%2520CSVR%2520consists%2520of%2520three%2520branches%253A%2520a%2520generative%2520pose%250Aprediction%2520branch%252C%2520a%2520discriminative%2520context%2520matching%2520branch%252C%2520and%2520a%2520video%250Agenerating%2520branch.%2520Among%2520them%252C%2520the%2520first%2520one%2520encodes%2520dynamic%2520motion%2520feature%2520by%250Autilizing%2520Conditional-GAN%2520to%2520predict%2520the%2520human%2520poses%2520of%2520future%2520frames%252C%2520and%2520the%250Asecond%2520branch%2520extracts%2520static%2520context%2520features%2520by%2520contrasting%2520positive%2520and%250Anegative%2520video%2520feature%2520and%2520I-frame%2520feature%2520pairs.%2520The%2520third%2520branch%2520is%2520designed%250Ato%2520generate%2520both%2520current%2520and%2520future%2520video%2520frames%252C%2520for%2520the%2520purpose%2520of%250Acollaboratively%2520improving%2520dynamic%2520motion%2520features%2520and%2520static%2520context%2520features.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520popular%2520video%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaboratively%20Self-supervised%20Video%20Representation%20Learning%20for%20Action%0A%20%20Recognition&entry.906535625=Jie%20Zhang%20and%20Zhifan%20Wan%20and%20Lanqing%20Hu%20and%20Stephen%20Lin%20and%20Shuzhe%20Wu%20and%20Shiguang%20Shan&entry.1292438233=%20%20Considering%20the%20close%20connection%20between%20action%20recognition%20and%20human%20pose%0Aestimation%2C%20we%20design%20a%20Collaboratively%20Self-supervised%20Video%20Representation%0A%28CSVR%29%20learning%20framework%20specific%20to%20action%20recognition%20by%20jointly%20factoring%0Ain%20generative%20pose%20prediction%20and%20discriminative%20context%20matching%20as%20pretext%0Atasks.%20Specifically%2C%20our%20CSVR%20consists%20of%20three%20branches%3A%20a%20generative%20pose%0Aprediction%20branch%2C%20a%20discriminative%20context%20matching%20branch%2C%20and%20a%20video%0Agenerating%20branch.%20Among%20them%2C%20the%20first%20one%20encodes%20dynamic%20motion%20feature%20by%0Autilizing%20Conditional-GAN%20to%20predict%20the%20human%20poses%20of%20future%20frames%2C%20and%20the%0Asecond%20branch%20extracts%20static%20context%20features%20by%20contrasting%20positive%20and%0Anegative%20video%20feature%20and%20I-frame%20feature%20pairs.%20The%20third%20branch%20is%20designed%0Ato%20generate%20both%20current%20and%20future%20video%20frames%2C%20for%20the%20purpose%20of%0Acollaboratively%20improving%20dynamic%20motion%20features%20and%20static%20context%20features.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20popular%20video%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07584v2&entry.124074799=Read"},
{"title": "Submodular Framework for Structured-Sparse Optimal Transport", "author": "Piyushi Manupriya and Pratik Jawanpuria and Karthik S. Gurumoorthy and SakethaNath Jagarlapudi and Bamdev Mishra", "abstract": "  Unbalanced optimal transport (UOT) has recently gained much attention due to\nits flexible framework for handling un-normalized measures and its robustness\nproperties. In this work, we explore learning (structured) sparse transport\nplans in the UOT setting, i.e., transport plans have an upper bound on the\nnumber of non-sparse entries in each column (structured sparse pattern) or in\nthe whole plan (general sparse pattern). We propose novel sparsity-constrained\nUOT formulations building on the recently explored maximum mean discrepancy\nbased UOT. We show that the proposed optimization problem is equivalent to the\nmaximization of a weakly submodular function over a uniform matroid or a\npartition matroid. We develop efficient gradient-based discrete greedy\nalgorithms and provide the corresponding theoretical guarantees. Empirically,\nwe observe that our proposed greedy algorithms select a diverse support set and\nwe illustrate the efficacy of the proposed approach in various applications.\n", "link": "http://arxiv.org/abs/2406.04914v2", "date": "2025-01-31", "relevancy": 2.1951, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4507}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport&body=Title%3A%20Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport%0AAuthor%3A%20Piyushi%20Manupriya%20and%20Pratik%20Jawanpuria%20and%20Karthik%20S.%20Gurumoorthy%20and%20SakethaNath%20Jagarlapudi%20and%20Bamdev%20Mishra%0AAbstract%3A%20%20%20Unbalanced%20optimal%20transport%20%28UOT%29%20has%20recently%20gained%20much%20attention%20due%20to%0Aits%20flexible%20framework%20for%20handling%20un-normalized%20measures%20and%20its%20robustness%0Aproperties.%20In%20this%20work%2C%20we%20explore%20learning%20%28structured%29%20sparse%20transport%0Aplans%20in%20the%20UOT%20setting%2C%20i.e.%2C%20transport%20plans%20have%20an%20upper%20bound%20on%20the%0Anumber%20of%20non-sparse%20entries%20in%20each%20column%20%28structured%20sparse%20pattern%29%20or%20in%0Athe%20whole%20plan%20%28general%20sparse%20pattern%29.%20We%20propose%20novel%20sparsity-constrained%0AUOT%20formulations%20building%20on%20the%20recently%20explored%20maximum%20mean%20discrepancy%0Abased%20UOT.%20We%20show%20that%20the%20proposed%20optimization%20problem%20is%20equivalent%20to%20the%0Amaximization%20of%20a%20weakly%20submodular%20function%20over%20a%20uniform%20matroid%20or%20a%0Apartition%20matroid.%20We%20develop%20efficient%20gradient-based%20discrete%20greedy%0Aalgorithms%20and%20provide%20the%20corresponding%20theoretical%20guarantees.%20Empirically%2C%0Awe%20observe%20that%20our%20proposed%20greedy%20algorithms%20select%20a%20diverse%20support%20set%20and%0Awe%20illustrate%20the%20efficacy%20of%20the%20proposed%20approach%20in%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubmodular%2520Framework%2520for%2520Structured-Sparse%2520Optimal%2520Transport%26entry.906535625%3DPiyushi%2520Manupriya%2520and%2520Pratik%2520Jawanpuria%2520and%2520Karthik%2520S.%2520Gurumoorthy%2520and%2520SakethaNath%2520Jagarlapudi%2520and%2520Bamdev%2520Mishra%26entry.1292438233%3D%2520%2520Unbalanced%2520optimal%2520transport%2520%2528UOT%2529%2520has%2520recently%2520gained%2520much%2520attention%2520due%2520to%250Aits%2520flexible%2520framework%2520for%2520handling%2520un-normalized%2520measures%2520and%2520its%2520robustness%250Aproperties.%2520In%2520this%2520work%252C%2520we%2520explore%2520learning%2520%2528structured%2529%2520sparse%2520transport%250Aplans%2520in%2520the%2520UOT%2520setting%252C%2520i.e.%252C%2520transport%2520plans%2520have%2520an%2520upper%2520bound%2520on%2520the%250Anumber%2520of%2520non-sparse%2520entries%2520in%2520each%2520column%2520%2528structured%2520sparse%2520pattern%2529%2520or%2520in%250Athe%2520whole%2520plan%2520%2528general%2520sparse%2520pattern%2529.%2520We%2520propose%2520novel%2520sparsity-constrained%250AUOT%2520formulations%2520building%2520on%2520the%2520recently%2520explored%2520maximum%2520mean%2520discrepancy%250Abased%2520UOT.%2520We%2520show%2520that%2520the%2520proposed%2520optimization%2520problem%2520is%2520equivalent%2520to%2520the%250Amaximization%2520of%2520a%2520weakly%2520submodular%2520function%2520over%2520a%2520uniform%2520matroid%2520or%2520a%250Apartition%2520matroid.%2520We%2520develop%2520efficient%2520gradient-based%2520discrete%2520greedy%250Aalgorithms%2520and%2520provide%2520the%2520corresponding%2520theoretical%2520guarantees.%2520Empirically%252C%250Awe%2520observe%2520that%2520our%2520proposed%2520greedy%2520algorithms%2520select%2520a%2520diverse%2520support%2520set%2520and%250Awe%2520illustrate%2520the%2520efficacy%2520of%2520the%2520proposed%2520approach%2520in%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport&entry.906535625=Piyushi%20Manupriya%20and%20Pratik%20Jawanpuria%20and%20Karthik%20S.%20Gurumoorthy%20and%20SakethaNath%20Jagarlapudi%20and%20Bamdev%20Mishra&entry.1292438233=%20%20Unbalanced%20optimal%20transport%20%28UOT%29%20has%20recently%20gained%20much%20attention%20due%20to%0Aits%20flexible%20framework%20for%20handling%20un-normalized%20measures%20and%20its%20robustness%0Aproperties.%20In%20this%20work%2C%20we%20explore%20learning%20%28structured%29%20sparse%20transport%0Aplans%20in%20the%20UOT%20setting%2C%20i.e.%2C%20transport%20plans%20have%20an%20upper%20bound%20on%20the%0Anumber%20of%20non-sparse%20entries%20in%20each%20column%20%28structured%20sparse%20pattern%29%20or%20in%0Athe%20whole%20plan%20%28general%20sparse%20pattern%29.%20We%20propose%20novel%20sparsity-constrained%0AUOT%20formulations%20building%20on%20the%20recently%20explored%20maximum%20mean%20discrepancy%0Abased%20UOT.%20We%20show%20that%20the%20proposed%20optimization%20problem%20is%20equivalent%20to%20the%0Amaximization%20of%20a%20weakly%20submodular%20function%20over%20a%20uniform%20matroid%20or%20a%0Apartition%20matroid.%20We%20develop%20efficient%20gradient-based%20discrete%20greedy%0Aalgorithms%20and%20provide%20the%20corresponding%20theoretical%20guarantees.%20Empirically%2C%0Awe%20observe%20that%20our%20proposed%20greedy%20algorithms%20select%20a%20diverse%20support%20set%20and%0Awe%20illustrate%20the%20efficacy%20of%20the%20proposed%20approach%20in%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04914v2&entry.124074799=Read"},
{"title": "OneBatchPAM: A Fast and Frugal K-Medoids Algorithm", "author": "Antoine de Mathelin and Nicolas Enrique Cecchi and Fran\u00e7ois Deheeger and Mathilde Mougeot and Nicolas Vayatis", "abstract": "  This paper proposes a novel k-medoids approximation algorithm to handle\nlarge-scale datasets with reasonable computational time and memory complexity.\nWe develop a local-search algorithm that iteratively improves the medoid\nselection based on the estimation of the k-medoids objective. A single batch of\nsize m << n provides the estimation, which reduces the required memory size and\nthe number of pairwise dissimilarities computations to O(mn), instead of O(n^2)\ncompared to most k-medoids baselines. We obtain theoretical results\nhighlighting that a batch of size m = O(log(n)) is sufficient to guarantee,\nwith strong probability, the same performance as the original local-search\nalgorithm. Multiple experiments conducted on real datasets of various sizes and\ndimensions show that our algorithm provides similar performances as\nstate-of-the-art methods such as FasterPAM and BanditPAM++ with a drastically\nreduced running time.\n", "link": "http://arxiv.org/abs/2501.19285v1", "date": "2025-01-31", "relevancy": 2.1774, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneBatchPAM%3A%20A%20Fast%20and%20Frugal%20K-Medoids%20Algorithm&body=Title%3A%20OneBatchPAM%3A%20A%20Fast%20and%20Frugal%20K-Medoids%20Algorithm%0AAuthor%3A%20Antoine%20de%20Mathelin%20and%20Nicolas%20Enrique%20Cecchi%20and%20Fran%C3%A7ois%20Deheeger%20and%20Mathilde%20Mougeot%20and%20Nicolas%20Vayatis%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20k-medoids%20approximation%20algorithm%20to%20handle%0Alarge-scale%20datasets%20with%20reasonable%20computational%20time%20and%20memory%20complexity.%0AWe%20develop%20a%20local-search%20algorithm%20that%20iteratively%20improves%20the%20medoid%0Aselection%20based%20on%20the%20estimation%20of%20the%20k-medoids%20objective.%20A%20single%20batch%20of%0Asize%20m%20%3C%3C%20n%20provides%20the%20estimation%2C%20which%20reduces%20the%20required%20memory%20size%20and%0Athe%20number%20of%20pairwise%20dissimilarities%20computations%20to%20O%28mn%29%2C%20instead%20of%20O%28n%5E2%29%0Acompared%20to%20most%20k-medoids%20baselines.%20We%20obtain%20theoretical%20results%0Ahighlighting%20that%20a%20batch%20of%20size%20m%20%3D%20O%28log%28n%29%29%20is%20sufficient%20to%20guarantee%2C%0Awith%20strong%20probability%2C%20the%20same%20performance%20as%20the%20original%20local-search%0Aalgorithm.%20Multiple%20experiments%20conducted%20on%20real%20datasets%20of%20various%20sizes%20and%0Adimensions%20show%20that%20our%20algorithm%20provides%20similar%20performances%20as%0Astate-of-the-art%20methods%20such%20as%20FasterPAM%20and%20BanditPAM%2B%2B%20with%20a%20drastically%0Areduced%20running%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneBatchPAM%253A%2520A%2520Fast%2520and%2520Frugal%2520K-Medoids%2520Algorithm%26entry.906535625%3DAntoine%2520de%2520Mathelin%2520and%2520Nicolas%2520Enrique%2520Cecchi%2520and%2520Fran%25C3%25A7ois%2520Deheeger%2520and%2520Mathilde%2520Mougeot%2520and%2520Nicolas%2520Vayatis%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520k-medoids%2520approximation%2520algorithm%2520to%2520handle%250Alarge-scale%2520datasets%2520with%2520reasonable%2520computational%2520time%2520and%2520memory%2520complexity.%250AWe%2520develop%2520a%2520local-search%2520algorithm%2520that%2520iteratively%2520improves%2520the%2520medoid%250Aselection%2520based%2520on%2520the%2520estimation%2520of%2520the%2520k-medoids%2520objective.%2520A%2520single%2520batch%2520of%250Asize%2520m%2520%253C%253C%2520n%2520provides%2520the%2520estimation%252C%2520which%2520reduces%2520the%2520required%2520memory%2520size%2520and%250Athe%2520number%2520of%2520pairwise%2520dissimilarities%2520computations%2520to%2520O%2528mn%2529%252C%2520instead%2520of%2520O%2528n%255E2%2529%250Acompared%2520to%2520most%2520k-medoids%2520baselines.%2520We%2520obtain%2520theoretical%2520results%250Ahighlighting%2520that%2520a%2520batch%2520of%2520size%2520m%2520%253D%2520O%2528log%2528n%2529%2529%2520is%2520sufficient%2520to%2520guarantee%252C%250Awith%2520strong%2520probability%252C%2520the%2520same%2520performance%2520as%2520the%2520original%2520local-search%250Aalgorithm.%2520Multiple%2520experiments%2520conducted%2520on%2520real%2520datasets%2520of%2520various%2520sizes%2520and%250Adimensions%2520show%2520that%2520our%2520algorithm%2520provides%2520similar%2520performances%2520as%250Astate-of-the-art%2520methods%2520such%2520as%2520FasterPAM%2520and%2520BanditPAM%252B%252B%2520with%2520a%2520drastically%250Areduced%2520running%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneBatchPAM%3A%20A%20Fast%20and%20Frugal%20K-Medoids%20Algorithm&entry.906535625=Antoine%20de%20Mathelin%20and%20Nicolas%20Enrique%20Cecchi%20and%20Fran%C3%A7ois%20Deheeger%20and%20Mathilde%20Mougeot%20and%20Nicolas%20Vayatis&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20k-medoids%20approximation%20algorithm%20to%20handle%0Alarge-scale%20datasets%20with%20reasonable%20computational%20time%20and%20memory%20complexity.%0AWe%20develop%20a%20local-search%20algorithm%20that%20iteratively%20improves%20the%20medoid%0Aselection%20based%20on%20the%20estimation%20of%20the%20k-medoids%20objective.%20A%20single%20batch%20of%0Asize%20m%20%3C%3C%20n%20provides%20the%20estimation%2C%20which%20reduces%20the%20required%20memory%20size%20and%0Athe%20number%20of%20pairwise%20dissimilarities%20computations%20to%20O%28mn%29%2C%20instead%20of%20O%28n%5E2%29%0Acompared%20to%20most%20k-medoids%20baselines.%20We%20obtain%20theoretical%20results%0Ahighlighting%20that%20a%20batch%20of%20size%20m%20%3D%20O%28log%28n%29%29%20is%20sufficient%20to%20guarantee%2C%0Awith%20strong%20probability%2C%20the%20same%20performance%20as%20the%20original%20local-search%0Aalgorithm.%20Multiple%20experiments%20conducted%20on%20real%20datasets%20of%20various%20sizes%20and%0Adimensions%20show%20that%20our%20algorithm%20provides%20similar%20performances%20as%0Astate-of-the-art%20methods%20such%20as%20FasterPAM%20and%20BanditPAM%2B%2B%20with%20a%20drastically%0Areduced%20running%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19285v1&entry.124074799=Read"},
{"title": "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected", "author": "Yingtao Zhang and Jialin Zhao and Wenjing Wu and Ziheng Liao and Umberto Michieli and Carlo Vittorio Cannistraci", "abstract": "  This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.\n", "link": "http://arxiv.org/abs/2501.19107v1", "date": "2025-01-31", "relevancy": 2.1749, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.534}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-inspired%20sparse%20training%20enables%20Transformers%20and%20LLMs%20to%20perform%0A%20%20as%20fully%20connected&body=Title%3A%20Brain-inspired%20sparse%20training%20enables%20Transformers%20and%20LLMs%20to%20perform%0A%20%20as%20fully%20connected%0AAuthor%3A%20Yingtao%20Zhang%20and%20Jialin%20Zhao%20and%20Wenjing%20Wu%20and%20Ziheng%20Liao%20and%20Umberto%20Michieli%20and%20Carlo%20Vittorio%20Cannistraci%0AAbstract%3A%20%20%20This%20study%20aims%20to%20enlarge%20our%20current%20knowledge%20on%20application%20of%0Abrain-inspired%20network%20science%20principles%20for%20training%20artificial%20neural%0Anetworks%20%28ANNs%29%20with%20sparse%20connectivity.%20Dynamic%20sparse%20training%20%28DST%29%20can%0Areduce%20the%20computational%20demands%20in%20ANNs%2C%20but%20faces%20difficulties%20to%20keep%20peak%0Aperformance%20at%20high%20sparsity%20levels.%20The%20Cannistraci-Hebb%20training%20%28CHT%29%20is%20a%0Abrain-inspired%20method%20for%20growing%20connectivity%20in%20DST.%20CHT%20leverages%20a%0Agradient-free%2C%20topology-driven%20link%20regrowth%2C%20which%20has%20shown%20ultra-sparse%20%281%25%0Aconnectivity%20or%20lower%29%20advantage%20across%20various%20tasks%20compared%20to%20fully%0Aconnected%20networks.%20Yet%2C%20CHT%20suffers%20two%20main%20drawbacks%3A%20%28i%29%20its%20time%0Acomplexity%20is%20O%28Nd%5E3%29%20-%20N%20node%20network%20size%2C%20d%20node%20degree%20-%20hence%20it%20can%20apply%0Aonly%20to%20ultra-sparse%20networks.%20%28ii%29%20it%20selects%20top%20link%20prediction%20scores%2C%0Awhich%20is%20inappropriate%20for%20the%20early%20training%20epochs%2C%20when%20the%20network%20presents%0Aunreliable%20connections.%20We%20propose%20a%20GPU-friendly%20approximation%20of%20the%20CH%20link%0Apredictor%2C%20which%20reduces%20the%20computational%20complexity%20to%20O%28N%5E3%29%2C%20enabling%20a%0Afast%20implementation%20of%20CHT%20in%20large-scale%20models.%20We%20introduce%20the%0ACannistraci-Hebb%20training%20soft%20rule%20%28CHTs%29%2C%20which%20adopts%20a%20strategy%20for%0Asampling%20connections%20in%20both%20link%20removal%20and%20regrowth%2C%20balancing%20the%0Aexploration%20and%20exploitation%20of%20network%20topology.%20To%20improve%20performance%2C%20we%0Aintegrate%20CHTs%20with%20a%20sigmoid%20gradual%20density%20decay%20%28CHTss%29.%20Empirical%20results%0Ashow%20that%2C%20using%201%25%20of%20connections%2C%20CHTs%20outperforms%20fully%20connected%20networks%0Ain%20MLP%20on%20visual%20classification%20tasks%2C%20compressing%20some%20networks%20to%20%3C%2030%25%0Anodes.%20Using%205%25%20of%20the%20connections%2C%20CHTss%20outperforms%20fully%20connected%20networks%0Ain%20two%20Transformer-based%20machine%20translation%20tasks.%20Using%2030%25%20of%20the%0Aconnections%2C%20CHTss%20achieves%20superior%20performance%20compared%20to%20other%20dynamic%0Asparse%20training%20methods%20in%20language%20modeling%2C%20and%20it%20surpasses%20the%20fully%0Aconnected%20counterpart%20in%20zero-shot%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-inspired%2520sparse%2520training%2520enables%2520Transformers%2520and%2520LLMs%2520to%2520perform%250A%2520%2520as%2520fully%2520connected%26entry.906535625%3DYingtao%2520Zhang%2520and%2520Jialin%2520Zhao%2520and%2520Wenjing%2520Wu%2520and%2520Ziheng%2520Liao%2520and%2520Umberto%2520Michieli%2520and%2520Carlo%2520Vittorio%2520Cannistraci%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520enlarge%2520our%2520current%2520knowledge%2520on%2520application%2520of%250Abrain-inspired%2520network%2520science%2520principles%2520for%2520training%2520artificial%2520neural%250Anetworks%2520%2528ANNs%2529%2520with%2520sparse%2520connectivity.%2520Dynamic%2520sparse%2520training%2520%2528DST%2529%2520can%250Areduce%2520the%2520computational%2520demands%2520in%2520ANNs%252C%2520but%2520faces%2520difficulties%2520to%2520keep%2520peak%250Aperformance%2520at%2520high%2520sparsity%2520levels.%2520The%2520Cannistraci-Hebb%2520training%2520%2528CHT%2529%2520is%2520a%250Abrain-inspired%2520method%2520for%2520growing%2520connectivity%2520in%2520DST.%2520CHT%2520leverages%2520a%250Agradient-free%252C%2520topology-driven%2520link%2520regrowth%252C%2520which%2520has%2520shown%2520ultra-sparse%2520%25281%2525%250Aconnectivity%2520or%2520lower%2529%2520advantage%2520across%2520various%2520tasks%2520compared%2520to%2520fully%250Aconnected%2520networks.%2520Yet%252C%2520CHT%2520suffers%2520two%2520main%2520drawbacks%253A%2520%2528i%2529%2520its%2520time%250Acomplexity%2520is%2520O%2528Nd%255E3%2529%2520-%2520N%2520node%2520network%2520size%252C%2520d%2520node%2520degree%2520-%2520hence%2520it%2520can%2520apply%250Aonly%2520to%2520ultra-sparse%2520networks.%2520%2528ii%2529%2520it%2520selects%2520top%2520link%2520prediction%2520scores%252C%250Awhich%2520is%2520inappropriate%2520for%2520the%2520early%2520training%2520epochs%252C%2520when%2520the%2520network%2520presents%250Aunreliable%2520connections.%2520We%2520propose%2520a%2520GPU-friendly%2520approximation%2520of%2520the%2520CH%2520link%250Apredictor%252C%2520which%2520reduces%2520the%2520computational%2520complexity%2520to%2520O%2528N%255E3%2529%252C%2520enabling%2520a%250Afast%2520implementation%2520of%2520CHT%2520in%2520large-scale%2520models.%2520We%2520introduce%2520the%250ACannistraci-Hebb%2520training%2520soft%2520rule%2520%2528CHTs%2529%252C%2520which%2520adopts%2520a%2520strategy%2520for%250Asampling%2520connections%2520in%2520both%2520link%2520removal%2520and%2520regrowth%252C%2520balancing%2520the%250Aexploration%2520and%2520exploitation%2520of%2520network%2520topology.%2520To%2520improve%2520performance%252C%2520we%250Aintegrate%2520CHTs%2520with%2520a%2520sigmoid%2520gradual%2520density%2520decay%2520%2528CHTss%2529.%2520Empirical%2520results%250Ashow%2520that%252C%2520using%25201%2525%2520of%2520connections%252C%2520CHTs%2520outperforms%2520fully%2520connected%2520networks%250Ain%2520MLP%2520on%2520visual%2520classification%2520tasks%252C%2520compressing%2520some%2520networks%2520to%2520%253C%252030%2525%250Anodes.%2520Using%25205%2525%2520of%2520the%2520connections%252C%2520CHTss%2520outperforms%2520fully%2520connected%2520networks%250Ain%2520two%2520Transformer-based%2520machine%2520translation%2520tasks.%2520Using%252030%2525%2520of%2520the%250Aconnections%252C%2520CHTss%2520achieves%2520superior%2520performance%2520compared%2520to%2520other%2520dynamic%250Asparse%2520training%2520methods%2520in%2520language%2520modeling%252C%2520and%2520it%2520surpasses%2520the%2520fully%250Aconnected%2520counterpart%2520in%2520zero-shot%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-inspired%20sparse%20training%20enables%20Transformers%20and%20LLMs%20to%20perform%0A%20%20as%20fully%20connected&entry.906535625=Yingtao%20Zhang%20and%20Jialin%20Zhao%20and%20Wenjing%20Wu%20and%20Ziheng%20Liao%20and%20Umberto%20Michieli%20and%20Carlo%20Vittorio%20Cannistraci&entry.1292438233=%20%20This%20study%20aims%20to%20enlarge%20our%20current%20knowledge%20on%20application%20of%0Abrain-inspired%20network%20science%20principles%20for%20training%20artificial%20neural%0Anetworks%20%28ANNs%29%20with%20sparse%20connectivity.%20Dynamic%20sparse%20training%20%28DST%29%20can%0Areduce%20the%20computational%20demands%20in%20ANNs%2C%20but%20faces%20difficulties%20to%20keep%20peak%0Aperformance%20at%20high%20sparsity%20levels.%20The%20Cannistraci-Hebb%20training%20%28CHT%29%20is%20a%0Abrain-inspired%20method%20for%20growing%20connectivity%20in%20DST.%20CHT%20leverages%20a%0Agradient-free%2C%20topology-driven%20link%20regrowth%2C%20which%20has%20shown%20ultra-sparse%20%281%25%0Aconnectivity%20or%20lower%29%20advantage%20across%20various%20tasks%20compared%20to%20fully%0Aconnected%20networks.%20Yet%2C%20CHT%20suffers%20two%20main%20drawbacks%3A%20%28i%29%20its%20time%0Acomplexity%20is%20O%28Nd%5E3%29%20-%20N%20node%20network%20size%2C%20d%20node%20degree%20-%20hence%20it%20can%20apply%0Aonly%20to%20ultra-sparse%20networks.%20%28ii%29%20it%20selects%20top%20link%20prediction%20scores%2C%0Awhich%20is%20inappropriate%20for%20the%20early%20training%20epochs%2C%20when%20the%20network%20presents%0Aunreliable%20connections.%20We%20propose%20a%20GPU-friendly%20approximation%20of%20the%20CH%20link%0Apredictor%2C%20which%20reduces%20the%20computational%20complexity%20to%20O%28N%5E3%29%2C%20enabling%20a%0Afast%20implementation%20of%20CHT%20in%20large-scale%20models.%20We%20introduce%20the%0ACannistraci-Hebb%20training%20soft%20rule%20%28CHTs%29%2C%20which%20adopts%20a%20strategy%20for%0Asampling%20connections%20in%20both%20link%20removal%20and%20regrowth%2C%20balancing%20the%0Aexploration%20and%20exploitation%20of%20network%20topology.%20To%20improve%20performance%2C%20we%0Aintegrate%20CHTs%20with%20a%20sigmoid%20gradual%20density%20decay%20%28CHTss%29.%20Empirical%20results%0Ashow%20that%2C%20using%201%25%20of%20connections%2C%20CHTs%20outperforms%20fully%20connected%20networks%0Ain%20MLP%20on%20visual%20classification%20tasks%2C%20compressing%20some%20networks%20to%20%3C%2030%25%0Anodes.%20Using%205%25%20of%20the%20connections%2C%20CHTss%20outperforms%20fully%20connected%20networks%0Ain%20two%20Transformer-based%20machine%20translation%20tasks.%20Using%2030%25%20of%20the%0Aconnections%2C%20CHTss%20achieves%20superior%20performance%20compared%20to%20other%20dynamic%0Asparse%20training%20methods%20in%20language%20modeling%2C%20and%20it%20surpasses%20the%20fully%0Aconnected%20counterpart%20in%20zero-shot%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19107v1&entry.124074799=Read"},
{"title": "$\\infty$-Video: A Training-Free Approach to Long Video Understanding via\n  Continuous-Time Memory Consolidation", "author": "Saul Santos and Ant\u00f3nio Farinhas and Daniel C. McNamee and Andr\u00e9 F. T. Martins", "abstract": "  Current video-language models struggle with long-video understanding due to\nlimited context lengths and reliance on sparse frame subsampling, often leading\nto information loss. This paper introduces $\\infty$-Video, which can process\narbitrarily long videos through a continuous-time long-term memory (LTM)\nconsolidation mechanism. Our framework augments video Q-formers by allowing\nthem to process unbounded video contexts efficiently and without requiring\nadditional training. Through continuous attention, our approach dynamically\nallocates higher granularity to the most relevant video segments, forming\n\"sticky\" memories that evolve over time. Experiments with Video-LLaMA and\nVideoChat2 demonstrate improved performance in video question-answering tasks,\nshowcasing the potential of continuous-time LTM mechanisms to enable scalable\nand training-free comprehension of long videos.\n", "link": "http://arxiv.org/abs/2501.19098v1", "date": "2025-01-31", "relevancy": 2.1734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation&body=Title%3A%20%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation%0AAuthor%3A%20Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Current%20video-language%20models%20struggle%20with%20long-video%20understanding%20due%20to%0Alimited%20context%20lengths%20and%20reliance%20on%20sparse%20frame%20subsampling%2C%20often%20leading%0Ato%20information%20loss.%20This%20paper%20introduces%20%24%5Cinfty%24-Video%2C%20which%20can%20process%0Aarbitrarily%20long%20videos%20through%20a%20continuous-time%20long-term%20memory%20%28LTM%29%0Aconsolidation%20mechanism.%20Our%20framework%20augments%20video%20Q-formers%20by%20allowing%0Athem%20to%20process%20unbounded%20video%20contexts%20efficiently%20and%20without%20requiring%0Aadditional%20training.%20Through%20continuous%20attention%2C%20our%20approach%20dynamically%0Aallocates%20higher%20granularity%20to%20the%20most%20relevant%20video%20segments%2C%20forming%0A%22sticky%22%20memories%20that%20evolve%20over%20time.%20Experiments%20with%20Video-LLaMA%20and%0AVideoChat2%20demonstrate%20improved%20performance%20in%20video%20question-answering%20tasks%2C%0Ashowcasing%20the%20potential%20of%20continuous-time%20LTM%20mechanisms%20to%20enable%20scalable%0Aand%20training-free%20comprehension%20of%20long%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cinfty%2524-Video%253A%2520A%2520Training-Free%2520Approach%2520to%2520Long%2520Video%2520Understanding%2520via%250A%2520%2520Continuous-Time%2520Memory%2520Consolidation%26entry.906535625%3DSaul%2520Santos%2520and%2520Ant%25C3%25B3nio%2520Farinhas%2520and%2520Daniel%2520C.%2520McNamee%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Current%2520video-language%2520models%2520struggle%2520with%2520long-video%2520understanding%2520due%2520to%250Alimited%2520context%2520lengths%2520and%2520reliance%2520on%2520sparse%2520frame%2520subsampling%252C%2520often%2520leading%250Ato%2520information%2520loss.%2520This%2520paper%2520introduces%2520%2524%255Cinfty%2524-Video%252C%2520which%2520can%2520process%250Aarbitrarily%2520long%2520videos%2520through%2520a%2520continuous-time%2520long-term%2520memory%2520%2528LTM%2529%250Aconsolidation%2520mechanism.%2520Our%2520framework%2520augments%2520video%2520Q-formers%2520by%2520allowing%250Athem%2520to%2520process%2520unbounded%2520video%2520contexts%2520efficiently%2520and%2520without%2520requiring%250Aadditional%2520training.%2520Through%2520continuous%2520attention%252C%2520our%2520approach%2520dynamically%250Aallocates%2520higher%2520granularity%2520to%2520the%2520most%2520relevant%2520video%2520segments%252C%2520forming%250A%2522sticky%2522%2520memories%2520that%2520evolve%2520over%2520time.%2520Experiments%2520with%2520Video-LLaMA%2520and%250AVideoChat2%2520demonstrate%2520improved%2520performance%2520in%2520video%2520question-answering%2520tasks%252C%250Ashowcasing%2520the%2520potential%2520of%2520continuous-time%2520LTM%2520mechanisms%2520to%2520enable%2520scalable%250Aand%2520training-free%2520comprehension%2520of%2520long%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation&entry.906535625=Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Current%20video-language%20models%20struggle%20with%20long-video%20understanding%20due%20to%0Alimited%20context%20lengths%20and%20reliance%20on%20sparse%20frame%20subsampling%2C%20often%20leading%0Ato%20information%20loss.%20This%20paper%20introduces%20%24%5Cinfty%24-Video%2C%20which%20can%20process%0Aarbitrarily%20long%20videos%20through%20a%20continuous-time%20long-term%20memory%20%28LTM%29%0Aconsolidation%20mechanism.%20Our%20framework%20augments%20video%20Q-formers%20by%20allowing%0Athem%20to%20process%20unbounded%20video%20contexts%20efficiently%20and%20without%20requiring%0Aadditional%20training.%20Through%20continuous%20attention%2C%20our%20approach%20dynamically%0Aallocates%20higher%20granularity%20to%20the%20most%20relevant%20video%20segments%2C%20forming%0A%22sticky%22%20memories%20that%20evolve%20over%20time.%20Experiments%20with%20Video-LLaMA%20and%0AVideoChat2%20demonstrate%20improved%20performance%20in%20video%20question-answering%20tasks%2C%0Ashowcasing%20the%20potential%20of%20continuous-time%20LTM%20mechanisms%20to%20enable%20scalable%0Aand%20training-free%20comprehension%20of%20long%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19098v1&entry.124074799=Read"},
{"title": "Let Human Sketches Help: Empowering Challenging Image Segmentation Task\n  with Freehand Sketches", "author": "Ying Zang and Runlong Cao and Jianqi Zhang and Yidong Han and Ziyue Cao and Wenjun Hu and Didi Zhu and Lanyun Zhu and Zejian Li and Deyi Ji and Tianrun Chen", "abstract": "  Sketches, with their expressive potential, allow humans to convey the essence\nof an object through even a rough contour. For the first time, we harness this\nexpressive potential to improve segmentation performance in challenging tasks\nlike camouflaged object detection (COD). Our approach introduces an innovative\nsketch-guided interactive segmentation framework, allowing users to intuitively\nannotate objects with freehand sketches (drawing a rough contour of the object)\ninstead of the traditional bounding boxes or points used in classic interactive\nsegmentation models like SAM. We demonstrate that sketch input can\nsignificantly improve performance in existing iterative segmentation methods,\noutperforming text or bounding box annotations. Additionally, we introduce key\nmodifications to network architectures and a novel sketch augmentation\ntechnique to fully harness the power of sketch input and further boost\nsegmentation accuracy. Remarkably, our model' s output can be directly used to\ntrain other neural networks, achieving results comparable to pixel-by-pixel\nannotations--while reducing annotation time by up to 120 times, which shows\ngreat potential in democratizing the annotation process and enabling model\ntraining with less reliance on resource-intensive, laborious pixel-level\nannotations. We also present KOSCamo+, the first freehand sketch dataset for\ncamouflaged object detection. The dataset, code, and the labeling tool will be\nopen sourced.\n", "link": "http://arxiv.org/abs/2501.19329v1", "date": "2025-01-31", "relevancy": 2.1708, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5646}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Human%20Sketches%20Help%3A%20Empowering%20Challenging%20Image%20Segmentation%20Task%0A%20%20with%20Freehand%20Sketches&body=Title%3A%20Let%20Human%20Sketches%20Help%3A%20Empowering%20Challenging%20Image%20Segmentation%20Task%0A%20%20with%20Freehand%20Sketches%0AAuthor%3A%20Ying%20Zang%20and%20Runlong%20Cao%20and%20Jianqi%20Zhang%20and%20Yidong%20Han%20and%20Ziyue%20Cao%20and%20Wenjun%20Hu%20and%20Didi%20Zhu%20and%20Lanyun%20Zhu%20and%20Zejian%20Li%20and%20Deyi%20Ji%20and%20Tianrun%20Chen%0AAbstract%3A%20%20%20Sketches%2C%20with%20their%20expressive%20potential%2C%20allow%20humans%20to%20convey%20the%20essence%0Aof%20an%20object%20through%20even%20a%20rough%20contour.%20For%20the%20first%20time%2C%20we%20harness%20this%0Aexpressive%20potential%20to%20improve%20segmentation%20performance%20in%20challenging%20tasks%0Alike%20camouflaged%20object%20detection%20%28COD%29.%20Our%20approach%20introduces%20an%20innovative%0Asketch-guided%20interactive%20segmentation%20framework%2C%20allowing%20users%20to%20intuitively%0Aannotate%20objects%20with%20freehand%20sketches%20%28drawing%20a%20rough%20contour%20of%20the%20object%29%0Ainstead%20of%20the%20traditional%20bounding%20boxes%20or%20points%20used%20in%20classic%20interactive%0Asegmentation%20models%20like%20SAM.%20We%20demonstrate%20that%20sketch%20input%20can%0Asignificantly%20improve%20performance%20in%20existing%20iterative%20segmentation%20methods%2C%0Aoutperforming%20text%20or%20bounding%20box%20annotations.%20Additionally%2C%20we%20introduce%20key%0Amodifications%20to%20network%20architectures%20and%20a%20novel%20sketch%20augmentation%0Atechnique%20to%20fully%20harness%20the%20power%20of%20sketch%20input%20and%20further%20boost%0Asegmentation%20accuracy.%20Remarkably%2C%20our%20model%27%20s%20output%20can%20be%20directly%20used%20to%0Atrain%20other%20neural%20networks%2C%20achieving%20results%20comparable%20to%20pixel-by-pixel%0Aannotations--while%20reducing%20annotation%20time%20by%20up%20to%20120%20times%2C%20which%20shows%0Agreat%20potential%20in%20democratizing%20the%20annotation%20process%20and%20enabling%20model%0Atraining%20with%20less%20reliance%20on%20resource-intensive%2C%20laborious%20pixel-level%0Aannotations.%20We%20also%20present%20KOSCamo%2B%2C%20the%20first%20freehand%20sketch%20dataset%20for%0Acamouflaged%20object%20detection.%20The%20dataset%2C%20code%2C%20and%20the%20labeling%20tool%20will%20be%0Aopen%20sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Human%2520Sketches%2520Help%253A%2520Empowering%2520Challenging%2520Image%2520Segmentation%2520Task%250A%2520%2520with%2520Freehand%2520Sketches%26entry.906535625%3DYing%2520Zang%2520and%2520Runlong%2520Cao%2520and%2520Jianqi%2520Zhang%2520and%2520Yidong%2520Han%2520and%2520Ziyue%2520Cao%2520and%2520Wenjun%2520Hu%2520and%2520Didi%2520Zhu%2520and%2520Lanyun%2520Zhu%2520and%2520Zejian%2520Li%2520and%2520Deyi%2520Ji%2520and%2520Tianrun%2520Chen%26entry.1292438233%3D%2520%2520Sketches%252C%2520with%2520their%2520expressive%2520potential%252C%2520allow%2520humans%2520to%2520convey%2520the%2520essence%250Aof%2520an%2520object%2520through%2520even%2520a%2520rough%2520contour.%2520For%2520the%2520first%2520time%252C%2520we%2520harness%2520this%250Aexpressive%2520potential%2520to%2520improve%2520segmentation%2520performance%2520in%2520challenging%2520tasks%250Alike%2520camouflaged%2520object%2520detection%2520%2528COD%2529.%2520Our%2520approach%2520introduces%2520an%2520innovative%250Asketch-guided%2520interactive%2520segmentation%2520framework%252C%2520allowing%2520users%2520to%2520intuitively%250Aannotate%2520objects%2520with%2520freehand%2520sketches%2520%2528drawing%2520a%2520rough%2520contour%2520of%2520the%2520object%2529%250Ainstead%2520of%2520the%2520traditional%2520bounding%2520boxes%2520or%2520points%2520used%2520in%2520classic%2520interactive%250Asegmentation%2520models%2520like%2520SAM.%2520We%2520demonstrate%2520that%2520sketch%2520input%2520can%250Asignificantly%2520improve%2520performance%2520in%2520existing%2520iterative%2520segmentation%2520methods%252C%250Aoutperforming%2520text%2520or%2520bounding%2520box%2520annotations.%2520Additionally%252C%2520we%2520introduce%2520key%250Amodifications%2520to%2520network%2520architectures%2520and%2520a%2520novel%2520sketch%2520augmentation%250Atechnique%2520to%2520fully%2520harness%2520the%2520power%2520of%2520sketch%2520input%2520and%2520further%2520boost%250Asegmentation%2520accuracy.%2520Remarkably%252C%2520our%2520model%2527%2520s%2520output%2520can%2520be%2520directly%2520used%2520to%250Atrain%2520other%2520neural%2520networks%252C%2520achieving%2520results%2520comparable%2520to%2520pixel-by-pixel%250Aannotations--while%2520reducing%2520annotation%2520time%2520by%2520up%2520to%2520120%2520times%252C%2520which%2520shows%250Agreat%2520potential%2520in%2520democratizing%2520the%2520annotation%2520process%2520and%2520enabling%2520model%250Atraining%2520with%2520less%2520reliance%2520on%2520resource-intensive%252C%2520laborious%2520pixel-level%250Aannotations.%2520We%2520also%2520present%2520KOSCamo%252B%252C%2520the%2520first%2520freehand%2520sketch%2520dataset%2520for%250Acamouflaged%2520object%2520detection.%2520The%2520dataset%252C%2520code%252C%2520and%2520the%2520labeling%2520tool%2520will%2520be%250Aopen%2520sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Human%20Sketches%20Help%3A%20Empowering%20Challenging%20Image%20Segmentation%20Task%0A%20%20with%20Freehand%20Sketches&entry.906535625=Ying%20Zang%20and%20Runlong%20Cao%20and%20Jianqi%20Zhang%20and%20Yidong%20Han%20and%20Ziyue%20Cao%20and%20Wenjun%20Hu%20and%20Didi%20Zhu%20and%20Lanyun%20Zhu%20and%20Zejian%20Li%20and%20Deyi%20Ji%20and%20Tianrun%20Chen&entry.1292438233=%20%20Sketches%2C%20with%20their%20expressive%20potential%2C%20allow%20humans%20to%20convey%20the%20essence%0Aof%20an%20object%20through%20even%20a%20rough%20contour.%20For%20the%20first%20time%2C%20we%20harness%20this%0Aexpressive%20potential%20to%20improve%20segmentation%20performance%20in%20challenging%20tasks%0Alike%20camouflaged%20object%20detection%20%28COD%29.%20Our%20approach%20introduces%20an%20innovative%0Asketch-guided%20interactive%20segmentation%20framework%2C%20allowing%20users%20to%20intuitively%0Aannotate%20objects%20with%20freehand%20sketches%20%28drawing%20a%20rough%20contour%20of%20the%20object%29%0Ainstead%20of%20the%20traditional%20bounding%20boxes%20or%20points%20used%20in%20classic%20interactive%0Asegmentation%20models%20like%20SAM.%20We%20demonstrate%20that%20sketch%20input%20can%0Asignificantly%20improve%20performance%20in%20existing%20iterative%20segmentation%20methods%2C%0Aoutperforming%20text%20or%20bounding%20box%20annotations.%20Additionally%2C%20we%20introduce%20key%0Amodifications%20to%20network%20architectures%20and%20a%20novel%20sketch%20augmentation%0Atechnique%20to%20fully%20harness%20the%20power%20of%20sketch%20input%20and%20further%20boost%0Asegmentation%20accuracy.%20Remarkably%2C%20our%20model%27%20s%20output%20can%20be%20directly%20used%20to%0Atrain%20other%20neural%20networks%2C%20achieving%20results%20comparable%20to%20pixel-by-pixel%0Aannotations--while%20reducing%20annotation%20time%20by%20up%20to%20120%20times%2C%20which%20shows%0Agreat%20potential%20in%20democratizing%20the%20annotation%20process%20and%20enabling%20model%0Atraining%20with%20less%20reliance%20on%20resource-intensive%2C%20laborious%20pixel-level%0Aannotations.%20We%20also%20present%20KOSCamo%2B%2C%20the%20first%20freehand%20sketch%20dataset%20for%0Acamouflaged%20object%20detection.%20The%20dataset%2C%20code%2C%20and%20the%20labeling%20tool%20will%20be%0Aopen%20sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19329v1&entry.124074799=Read"},
{"title": "Orthogonal Subspace Decomposition for Generalizable AI-Generated Image\n  Detection", "author": "Zhiyuan Yan and Jiangming Wang and Peng Jin and Ke-Yue Zhang and Chengchun Liu and Shen Chen and Taiping Yao and Shouhong Ding and Baoyuan Wu and Li Yuan", "abstract": "  AI-generated images (AIGIs), such as natural or face images, have become\nincreasingly realistic and indistinguishable, making their detection a critical\nand pressing challenge. In this paper, we start from a new perspective to\nexcavate the reason behind the failure generalization in AIGI detection, named\nthe \\textit{asymmetry phenomenon}, where a naively trained detector tends to\nfavor overfitting to the limited and monotonous fake patterns, causing the\nfeature space to become highly constrained and low-ranked, which is proved\nseriously limiting the expressivity and generalization. One potential remedy is\nincorporating the pre-trained knowledge within the vision foundation models\n(higher-ranked) to expand the feature space, alleviating the model's\noverfitting to fake. To this end, we employ Singular Value Decomposition (SVD)\nto decompose the original feature space into two orthogonal subspaces. By\nfreezing the principal components and adapting only the remained components, we\npreserve the pre-trained knowledge while learning forgery-related patterns.\nCompared to existing full-parameters and LoRA-based tuning methods, we\nexplicitly ensure orthogonality enabling the higher rank of the whole feature\nspace, effectively minimizing overfitting and enhancing generalization.\nExtensive experiments with our deep analysis on both deepfake and synthetic\nimage detection benchmarks demonstrate superior generalization performance in\ndetection.\n", "link": "http://arxiv.org/abs/2411.15633v2", "date": "2025-01-31", "relevancy": 2.1637, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.542}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orthogonal%20Subspace%20Decomposition%20for%20Generalizable%20AI-Generated%20Image%0A%20%20Detection&body=Title%3A%20Orthogonal%20Subspace%20Decomposition%20for%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%0AAuthor%3A%20Zhiyuan%20Yan%20and%20Jiangming%20Wang%20and%20Peng%20Jin%20and%20Ke-Yue%20Zhang%20and%20Chengchun%20Liu%20and%20Shen%20Chen%20and%20Taiping%20Yao%20and%20Shouhong%20Ding%20and%20Baoyuan%20Wu%20and%20Li%20Yuan%0AAbstract%3A%20%20%20AI-generated%20images%20%28AIGIs%29%2C%20such%20as%20natural%20or%20face%20images%2C%20have%20become%0Aincreasingly%20realistic%20and%20indistinguishable%2C%20making%20their%20detection%20a%20critical%0Aand%20pressing%20challenge.%20In%20this%20paper%2C%20we%20start%20from%20a%20new%20perspective%20to%0Aexcavate%20the%20reason%20behind%20the%20failure%20generalization%20in%20AIGI%20detection%2C%20named%0Athe%20%5Ctextit%7Basymmetry%20phenomenon%7D%2C%20where%20a%20naively%20trained%20detector%20tends%20to%0Afavor%20overfitting%20to%20the%20limited%20and%20monotonous%20fake%20patterns%2C%20causing%20the%0Afeature%20space%20to%20become%20highly%20constrained%20and%20low-ranked%2C%20which%20is%20proved%0Aseriously%20limiting%20the%20expressivity%20and%20generalization.%20One%20potential%20remedy%20is%0Aincorporating%20the%20pre-trained%20knowledge%20within%20the%20vision%20foundation%20models%0A%28higher-ranked%29%20to%20expand%20the%20feature%20space%2C%20alleviating%20the%20model%27s%0Aoverfitting%20to%20fake.%20To%20this%20end%2C%20we%20employ%20Singular%20Value%20Decomposition%20%28SVD%29%0Ato%20decompose%20the%20original%20feature%20space%20into%20two%20orthogonal%20subspaces.%20By%0Afreezing%20the%20principal%20components%20and%20adapting%20only%20the%20remained%20components%2C%20we%0Apreserve%20the%20pre-trained%20knowledge%20while%20learning%20forgery-related%20patterns.%0ACompared%20to%20existing%20full-parameters%20and%20LoRA-based%20tuning%20methods%2C%20we%0Aexplicitly%20ensure%20orthogonality%20enabling%20the%20higher%20rank%20of%20the%20whole%20feature%0Aspace%2C%20effectively%20minimizing%20overfitting%20and%20enhancing%20generalization.%0AExtensive%20experiments%20with%20our%20deep%20analysis%20on%20both%20deepfake%20and%20synthetic%0Aimage%20detection%20benchmarks%20demonstrate%20superior%20generalization%20performance%20in%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrthogonal%2520Subspace%2520Decomposition%2520for%2520Generalizable%2520AI-Generated%2520Image%250A%2520%2520Detection%26entry.906535625%3DZhiyuan%2520Yan%2520and%2520Jiangming%2520Wang%2520and%2520Peng%2520Jin%2520and%2520Ke-Yue%2520Zhang%2520and%2520Chengchun%2520Liu%2520and%2520Shen%2520Chen%2520and%2520Taiping%2520Yao%2520and%2520Shouhong%2520Ding%2520and%2520Baoyuan%2520Wu%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520AI-generated%2520images%2520%2528AIGIs%2529%252C%2520such%2520as%2520natural%2520or%2520face%2520images%252C%2520have%2520become%250Aincreasingly%2520realistic%2520and%2520indistinguishable%252C%2520making%2520their%2520detection%2520a%2520critical%250Aand%2520pressing%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520start%2520from%2520a%2520new%2520perspective%2520to%250Aexcavate%2520the%2520reason%2520behind%2520the%2520failure%2520generalization%2520in%2520AIGI%2520detection%252C%2520named%250Athe%2520%255Ctextit%257Basymmetry%2520phenomenon%257D%252C%2520where%2520a%2520naively%2520trained%2520detector%2520tends%2520to%250Afavor%2520overfitting%2520to%2520the%2520limited%2520and%2520monotonous%2520fake%2520patterns%252C%2520causing%2520the%250Afeature%2520space%2520to%2520become%2520highly%2520constrained%2520and%2520low-ranked%252C%2520which%2520is%2520proved%250Aseriously%2520limiting%2520the%2520expressivity%2520and%2520generalization.%2520One%2520potential%2520remedy%2520is%250Aincorporating%2520the%2520pre-trained%2520knowledge%2520within%2520the%2520vision%2520foundation%2520models%250A%2528higher-ranked%2529%2520to%2520expand%2520the%2520feature%2520space%252C%2520alleviating%2520the%2520model%2527s%250Aoverfitting%2520to%2520fake.%2520To%2520this%2520end%252C%2520we%2520employ%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529%250Ato%2520decompose%2520the%2520original%2520feature%2520space%2520into%2520two%2520orthogonal%2520subspaces.%2520By%250Afreezing%2520the%2520principal%2520components%2520and%2520adapting%2520only%2520the%2520remained%2520components%252C%2520we%250Apreserve%2520the%2520pre-trained%2520knowledge%2520while%2520learning%2520forgery-related%2520patterns.%250ACompared%2520to%2520existing%2520full-parameters%2520and%2520LoRA-based%2520tuning%2520methods%252C%2520we%250Aexplicitly%2520ensure%2520orthogonality%2520enabling%2520the%2520higher%2520rank%2520of%2520the%2520whole%2520feature%250Aspace%252C%2520effectively%2520minimizing%2520overfitting%2520and%2520enhancing%2520generalization.%250AExtensive%2520experiments%2520with%2520our%2520deep%2520analysis%2520on%2520both%2520deepfake%2520and%2520synthetic%250Aimage%2520detection%2520benchmarks%2520demonstrate%2520superior%2520generalization%2520performance%2520in%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orthogonal%20Subspace%20Decomposition%20for%20Generalizable%20AI-Generated%20Image%0A%20%20Detection&entry.906535625=Zhiyuan%20Yan%20and%20Jiangming%20Wang%20and%20Peng%20Jin%20and%20Ke-Yue%20Zhang%20and%20Chengchun%20Liu%20and%20Shen%20Chen%20and%20Taiping%20Yao%20and%20Shouhong%20Ding%20and%20Baoyuan%20Wu%20and%20Li%20Yuan&entry.1292438233=%20%20AI-generated%20images%20%28AIGIs%29%2C%20such%20as%20natural%20or%20face%20images%2C%20have%20become%0Aincreasingly%20realistic%20and%20indistinguishable%2C%20making%20their%20detection%20a%20critical%0Aand%20pressing%20challenge.%20In%20this%20paper%2C%20we%20start%20from%20a%20new%20perspective%20to%0Aexcavate%20the%20reason%20behind%20the%20failure%20generalization%20in%20AIGI%20detection%2C%20named%0Athe%20%5Ctextit%7Basymmetry%20phenomenon%7D%2C%20where%20a%20naively%20trained%20detector%20tends%20to%0Afavor%20overfitting%20to%20the%20limited%20and%20monotonous%20fake%20patterns%2C%20causing%20the%0Afeature%20space%20to%20become%20highly%20constrained%20and%20low-ranked%2C%20which%20is%20proved%0Aseriously%20limiting%20the%20expressivity%20and%20generalization.%20One%20potential%20remedy%20is%0Aincorporating%20the%20pre-trained%20knowledge%20within%20the%20vision%20foundation%20models%0A%28higher-ranked%29%20to%20expand%20the%20feature%20space%2C%20alleviating%20the%20model%27s%0Aoverfitting%20to%20fake.%20To%20this%20end%2C%20we%20employ%20Singular%20Value%20Decomposition%20%28SVD%29%0Ato%20decompose%20the%20original%20feature%20space%20into%20two%20orthogonal%20subspaces.%20By%0Afreezing%20the%20principal%20components%20and%20adapting%20only%20the%20remained%20components%2C%20we%0Apreserve%20the%20pre-trained%20knowledge%20while%20learning%20forgery-related%20patterns.%0ACompared%20to%20existing%20full-parameters%20and%20LoRA-based%20tuning%20methods%2C%20we%0Aexplicitly%20ensure%20orthogonality%20enabling%20the%20higher%20rank%20of%20the%20whole%20feature%0Aspace%2C%20effectively%20minimizing%20overfitting%20and%20enhancing%20generalization.%0AExtensive%20experiments%20with%20our%20deep%20analysis%20on%20both%20deepfake%20and%20synthetic%0Aimage%20detection%20benchmarks%20demonstrate%20superior%20generalization%20performance%20in%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15633v2&entry.124074799=Read"},
{"title": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach", "author": "Yingdan Shi and Ren Wang", "abstract": "  Machine unlearning seeks to systematically remove specified data from a\ntrained model, effectively achieving a state as though the data had never been\nencountered during training. While metrics such as Unlearning Accuracy (UA) and\nMembership Inference Attack (MIA) provide a baseline for assessing unlearning\nperformance, they fall short of evaluating the completeness and reliability of\nforgetting. This is because the ground truth labels remain potential candidates\nwithin the scope of uncertainty quantification, leaving gaps in the evaluation\nof true forgetting. In this paper, we identify critical limitations in existing\nunlearning metrics and propose enhanced evaluation metrics inspired by\nconformal prediction. Our metrics can effectively capture the extent to which\nground truth labels are excluded from the prediction set. Furthermore, we\nobserve that many existing machine unlearning methods do not achieve\nsatisfactory forgetting performance when evaluated with our new metrics. To\naddress this, we propose an unlearning framework that integrates conformal\nprediction insights into Carlini & Wagner adversarial attack loss. Extensive\nexperiments on the image classification task demonstrate that our enhanced\nmetrics offer deeper insights into unlearning effectiveness, and that our\nunlearning framework significantly improves the forgetting quality of\nunlearning methods.\n", "link": "http://arxiv.org/abs/2501.19403v1", "date": "2025-01-31", "relevancy": 2.1604, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5588}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redefining%20Machine%20Unlearning%3A%20A%20Conformal%20Prediction-Motivated%20Approach&body=Title%3A%20Redefining%20Machine%20Unlearning%3A%20A%20Conformal%20Prediction-Motivated%20Approach%0AAuthor%3A%20Yingdan%20Shi%20and%20Ren%20Wang%0AAbstract%3A%20%20%20Machine%20unlearning%20seeks%20to%20systematically%20remove%20specified%20data%20from%20a%0Atrained%20model%2C%20effectively%20achieving%20a%20state%20as%20though%20the%20data%20had%20never%20been%0Aencountered%20during%20training.%20While%20metrics%20such%20as%20Unlearning%20Accuracy%20%28UA%29%20and%0AMembership%20Inference%20Attack%20%28MIA%29%20provide%20a%20baseline%20for%20assessing%20unlearning%0Aperformance%2C%20they%20fall%20short%20of%20evaluating%20the%20completeness%20and%20reliability%20of%0Aforgetting.%20This%20is%20because%20the%20ground%20truth%20labels%20remain%20potential%20candidates%0Awithin%20the%20scope%20of%20uncertainty%20quantification%2C%20leaving%20gaps%20in%20the%20evaluation%0Aof%20true%20forgetting.%20In%20this%20paper%2C%20we%20identify%20critical%20limitations%20in%20existing%0Aunlearning%20metrics%20and%20propose%20enhanced%20evaluation%20metrics%20inspired%20by%0Aconformal%20prediction.%20Our%20metrics%20can%20effectively%20capture%20the%20extent%20to%20which%0Aground%20truth%20labels%20are%20excluded%20from%20the%20prediction%20set.%20Furthermore%2C%20we%0Aobserve%20that%20many%20existing%20machine%20unlearning%20methods%20do%20not%20achieve%0Asatisfactory%20forgetting%20performance%20when%20evaluated%20with%20our%20new%20metrics.%20To%0Aaddress%20this%2C%20we%20propose%20an%20unlearning%20framework%20that%20integrates%20conformal%0Aprediction%20insights%20into%20Carlini%20%26%20Wagner%20adversarial%20attack%20loss.%20Extensive%0Aexperiments%20on%20the%20image%20classification%20task%20demonstrate%20that%20our%20enhanced%0Ametrics%20offer%20deeper%20insights%20into%20unlearning%20effectiveness%2C%20and%20that%20our%0Aunlearning%20framework%20significantly%20improves%20the%20forgetting%20quality%20of%0Aunlearning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedefining%2520Machine%2520Unlearning%253A%2520A%2520Conformal%2520Prediction-Motivated%2520Approach%26entry.906535625%3DYingdan%2520Shi%2520and%2520Ren%2520Wang%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520seeks%2520to%2520systematically%2520remove%2520specified%2520data%2520from%2520a%250Atrained%2520model%252C%2520effectively%2520achieving%2520a%2520state%2520as%2520though%2520the%2520data%2520had%2520never%2520been%250Aencountered%2520during%2520training.%2520While%2520metrics%2520such%2520as%2520Unlearning%2520Accuracy%2520%2528UA%2529%2520and%250AMembership%2520Inference%2520Attack%2520%2528MIA%2529%2520provide%2520a%2520baseline%2520for%2520assessing%2520unlearning%250Aperformance%252C%2520they%2520fall%2520short%2520of%2520evaluating%2520the%2520completeness%2520and%2520reliability%2520of%250Aforgetting.%2520This%2520is%2520because%2520the%2520ground%2520truth%2520labels%2520remain%2520potential%2520candidates%250Awithin%2520the%2520scope%2520of%2520uncertainty%2520quantification%252C%2520leaving%2520gaps%2520in%2520the%2520evaluation%250Aof%2520true%2520forgetting.%2520In%2520this%2520paper%252C%2520we%2520identify%2520critical%2520limitations%2520in%2520existing%250Aunlearning%2520metrics%2520and%2520propose%2520enhanced%2520evaluation%2520metrics%2520inspired%2520by%250Aconformal%2520prediction.%2520Our%2520metrics%2520can%2520effectively%2520capture%2520the%2520extent%2520to%2520which%250Aground%2520truth%2520labels%2520are%2520excluded%2520from%2520the%2520prediction%2520set.%2520Furthermore%252C%2520we%250Aobserve%2520that%2520many%2520existing%2520machine%2520unlearning%2520methods%2520do%2520not%2520achieve%250Asatisfactory%2520forgetting%2520performance%2520when%2520evaluated%2520with%2520our%2520new%2520metrics.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520an%2520unlearning%2520framework%2520that%2520integrates%2520conformal%250Aprediction%2520insights%2520into%2520Carlini%2520%2526%2520Wagner%2520adversarial%2520attack%2520loss.%2520Extensive%250Aexperiments%2520on%2520the%2520image%2520classification%2520task%2520demonstrate%2520that%2520our%2520enhanced%250Ametrics%2520offer%2520deeper%2520insights%2520into%2520unlearning%2520effectiveness%252C%2520and%2520that%2520our%250Aunlearning%2520framework%2520significantly%2520improves%2520the%2520forgetting%2520quality%2520of%250Aunlearning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Machine%20Unlearning%3A%20A%20Conformal%20Prediction-Motivated%20Approach&entry.906535625=Yingdan%20Shi%20and%20Ren%20Wang&entry.1292438233=%20%20Machine%20unlearning%20seeks%20to%20systematically%20remove%20specified%20data%20from%20a%0Atrained%20model%2C%20effectively%20achieving%20a%20state%20as%20though%20the%20data%20had%20never%20been%0Aencountered%20during%20training.%20While%20metrics%20such%20as%20Unlearning%20Accuracy%20%28UA%29%20and%0AMembership%20Inference%20Attack%20%28MIA%29%20provide%20a%20baseline%20for%20assessing%20unlearning%0Aperformance%2C%20they%20fall%20short%20of%20evaluating%20the%20completeness%20and%20reliability%20of%0Aforgetting.%20This%20is%20because%20the%20ground%20truth%20labels%20remain%20potential%20candidates%0Awithin%20the%20scope%20of%20uncertainty%20quantification%2C%20leaving%20gaps%20in%20the%20evaluation%0Aof%20true%20forgetting.%20In%20this%20paper%2C%20we%20identify%20critical%20limitations%20in%20existing%0Aunlearning%20metrics%20and%20propose%20enhanced%20evaluation%20metrics%20inspired%20by%0Aconformal%20prediction.%20Our%20metrics%20can%20effectively%20capture%20the%20extent%20to%20which%0Aground%20truth%20labels%20are%20excluded%20from%20the%20prediction%20set.%20Furthermore%2C%20we%0Aobserve%20that%20many%20existing%20machine%20unlearning%20methods%20do%20not%20achieve%0Asatisfactory%20forgetting%20performance%20when%20evaluated%20with%20our%20new%20metrics.%20To%0Aaddress%20this%2C%20we%20propose%20an%20unlearning%20framework%20that%20integrates%20conformal%0Aprediction%20insights%20into%20Carlini%20%26%20Wagner%20adversarial%20attack%20loss.%20Extensive%0Aexperiments%20on%20the%20image%20classification%20task%20demonstrate%20that%20our%20enhanced%0Ametrics%20offer%20deeper%20insights%20into%20unlearning%20effectiveness%2C%20and%20that%20our%0Aunlearning%20framework%20significantly%20improves%20the%20forgetting%20quality%20of%0Aunlearning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19403v1&entry.124074799=Read"},
{"title": "Concept-Based Explainable Artificial Intelligence: Metrics and\n  Benchmarks", "author": "Halil Ibrahim Aysel and Xiaohao Cai and Adam Prugel-Bennett", "abstract": "  Concept-based explanation methods, such as concept bottleneck models (CBMs),\naim to improve the interpretability of machine learning models by linking their\ndecisions to human-understandable concepts, under the critical assumption that\nsuch concepts can be accurately attributed to the network's feature space.\nHowever, this foundational assumption has not been rigorously validated, mainly\nbecause the field lacks standardised metrics and benchmarks to assess the\nexistence and spatial alignment of such concepts. To address this, we propose\nthree metrics: the concept global importance metric, the concept existence\nmetric, and the concept location metric, including a technique for visualising\nconcept activations, i.e., concept activation mapping. We benchmark post-hoc\nCBMs to illustrate their capabilities and challenges. Through qualitative and\nquantitative experiments, we demonstrate that, in many cases, even the most\nimportant concepts determined by post-hoc CBMs are not present in input images;\nmoreover, when they are present, their saliency maps fail to align with the\nexpected regions by either activating across an entire object or misidentifying\nrelevant concept-specific regions. We analyse the root causes of these\nlimitations, such as the natural correlation of concepts. Our findings\nunderscore the need for more careful application of concept-based explanation\ntechniques especially in settings where spatial interpretability is critical.\n", "link": "http://arxiv.org/abs/2501.19271v1", "date": "2025-01-31", "relevancy": 2.1573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-Based%20Explainable%20Artificial%20Intelligence%3A%20Metrics%20and%0A%20%20Benchmarks&body=Title%3A%20Concept-Based%20Explainable%20Artificial%20Intelligence%3A%20Metrics%20and%0A%20%20Benchmarks%0AAuthor%3A%20Halil%20Ibrahim%20Aysel%20and%20Xiaohao%20Cai%20and%20Adam%20Prugel-Bennett%0AAbstract%3A%20%20%20Concept-based%20explanation%20methods%2C%20such%20as%20concept%20bottleneck%20models%20%28CBMs%29%2C%0Aaim%20to%20improve%20the%20interpretability%20of%20machine%20learning%20models%20by%20linking%20their%0Adecisions%20to%20human-understandable%20concepts%2C%20under%20the%20critical%20assumption%20that%0Asuch%20concepts%20can%20be%20accurately%20attributed%20to%20the%20network%27s%20feature%20space.%0AHowever%2C%20this%20foundational%20assumption%20has%20not%20been%20rigorously%20validated%2C%20mainly%0Abecause%20the%20field%20lacks%20standardised%20metrics%20and%20benchmarks%20to%20assess%20the%0Aexistence%20and%20spatial%20alignment%20of%20such%20concepts.%20To%20address%20this%2C%20we%20propose%0Athree%20metrics%3A%20the%20concept%20global%20importance%20metric%2C%20the%20concept%20existence%0Ametric%2C%20and%20the%20concept%20location%20metric%2C%20including%20a%20technique%20for%20visualising%0Aconcept%20activations%2C%20i.e.%2C%20concept%20activation%20mapping.%20We%20benchmark%20post-hoc%0ACBMs%20to%20illustrate%20their%20capabilities%20and%20challenges.%20Through%20qualitative%20and%0Aquantitative%20experiments%2C%20we%20demonstrate%20that%2C%20in%20many%20cases%2C%20even%20the%20most%0Aimportant%20concepts%20determined%20by%20post-hoc%20CBMs%20are%20not%20present%20in%20input%20images%3B%0Amoreover%2C%20when%20they%20are%20present%2C%20their%20saliency%20maps%20fail%20to%20align%20with%20the%0Aexpected%20regions%20by%20either%20activating%20across%20an%20entire%20object%20or%20misidentifying%0Arelevant%20concept-specific%20regions.%20We%20analyse%20the%20root%20causes%20of%20these%0Alimitations%2C%20such%20as%20the%20natural%20correlation%20of%20concepts.%20Our%20findings%0Aunderscore%20the%20need%20for%20more%20careful%20application%20of%20concept-based%20explanation%0Atechniques%20especially%20in%20settings%20where%20spatial%20interpretability%20is%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-Based%2520Explainable%2520Artificial%2520Intelligence%253A%2520Metrics%2520and%250A%2520%2520Benchmarks%26entry.906535625%3DHalil%2520Ibrahim%2520Aysel%2520and%2520Xiaohao%2520Cai%2520and%2520Adam%2520Prugel-Bennett%26entry.1292438233%3D%2520%2520Concept-based%2520explanation%2520methods%252C%2520such%2520as%2520concept%2520bottleneck%2520models%2520%2528CBMs%2529%252C%250Aaim%2520to%2520improve%2520the%2520interpretability%2520of%2520machine%2520learning%2520models%2520by%2520linking%2520their%250Adecisions%2520to%2520human-understandable%2520concepts%252C%2520under%2520the%2520critical%2520assumption%2520that%250Asuch%2520concepts%2520can%2520be%2520accurately%2520attributed%2520to%2520the%2520network%2527s%2520feature%2520space.%250AHowever%252C%2520this%2520foundational%2520assumption%2520has%2520not%2520been%2520rigorously%2520validated%252C%2520mainly%250Abecause%2520the%2520field%2520lacks%2520standardised%2520metrics%2520and%2520benchmarks%2520to%2520assess%2520the%250Aexistence%2520and%2520spatial%2520alignment%2520of%2520such%2520concepts.%2520To%2520address%2520this%252C%2520we%2520propose%250Athree%2520metrics%253A%2520the%2520concept%2520global%2520importance%2520metric%252C%2520the%2520concept%2520existence%250Ametric%252C%2520and%2520the%2520concept%2520location%2520metric%252C%2520including%2520a%2520technique%2520for%2520visualising%250Aconcept%2520activations%252C%2520i.e.%252C%2520concept%2520activation%2520mapping.%2520We%2520benchmark%2520post-hoc%250ACBMs%2520to%2520illustrate%2520their%2520capabilities%2520and%2520challenges.%2520Through%2520qualitative%2520and%250Aquantitative%2520experiments%252C%2520we%2520demonstrate%2520that%252C%2520in%2520many%2520cases%252C%2520even%2520the%2520most%250Aimportant%2520concepts%2520determined%2520by%2520post-hoc%2520CBMs%2520are%2520not%2520present%2520in%2520input%2520images%253B%250Amoreover%252C%2520when%2520they%2520are%2520present%252C%2520their%2520saliency%2520maps%2520fail%2520to%2520align%2520with%2520the%250Aexpected%2520regions%2520by%2520either%2520activating%2520across%2520an%2520entire%2520object%2520or%2520misidentifying%250Arelevant%2520concept-specific%2520regions.%2520We%2520analyse%2520the%2520root%2520causes%2520of%2520these%250Alimitations%252C%2520such%2520as%2520the%2520natural%2520correlation%2520of%2520concepts.%2520Our%2520findings%250Aunderscore%2520the%2520need%2520for%2520more%2520careful%2520application%2520of%2520concept-based%2520explanation%250Atechniques%2520especially%2520in%2520settings%2520where%2520spatial%2520interpretability%2520is%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-Based%20Explainable%20Artificial%20Intelligence%3A%20Metrics%20and%0A%20%20Benchmarks&entry.906535625=Halil%20Ibrahim%20Aysel%20and%20Xiaohao%20Cai%20and%20Adam%20Prugel-Bennett&entry.1292438233=%20%20Concept-based%20explanation%20methods%2C%20such%20as%20concept%20bottleneck%20models%20%28CBMs%29%2C%0Aaim%20to%20improve%20the%20interpretability%20of%20machine%20learning%20models%20by%20linking%20their%0Adecisions%20to%20human-understandable%20concepts%2C%20under%20the%20critical%20assumption%20that%0Asuch%20concepts%20can%20be%20accurately%20attributed%20to%20the%20network%27s%20feature%20space.%0AHowever%2C%20this%20foundational%20assumption%20has%20not%20been%20rigorously%20validated%2C%20mainly%0Abecause%20the%20field%20lacks%20standardised%20metrics%20and%20benchmarks%20to%20assess%20the%0Aexistence%20and%20spatial%20alignment%20of%20such%20concepts.%20To%20address%20this%2C%20we%20propose%0Athree%20metrics%3A%20the%20concept%20global%20importance%20metric%2C%20the%20concept%20existence%0Ametric%2C%20and%20the%20concept%20location%20metric%2C%20including%20a%20technique%20for%20visualising%0Aconcept%20activations%2C%20i.e.%2C%20concept%20activation%20mapping.%20We%20benchmark%20post-hoc%0ACBMs%20to%20illustrate%20their%20capabilities%20and%20challenges.%20Through%20qualitative%20and%0Aquantitative%20experiments%2C%20we%20demonstrate%20that%2C%20in%20many%20cases%2C%20even%20the%20most%0Aimportant%20concepts%20determined%20by%20post-hoc%20CBMs%20are%20not%20present%20in%20input%20images%3B%0Amoreover%2C%20when%20they%20are%20present%2C%20their%20saliency%20maps%20fail%20to%20align%20with%20the%0Aexpected%20regions%20by%20either%20activating%20across%20an%20entire%20object%20or%20misidentifying%0Arelevant%20concept-specific%20regions.%20We%20analyse%20the%20root%20causes%20of%20these%0Alimitations%2C%20such%20as%20the%20natural%20correlation%20of%20concepts.%20Our%20findings%0Aunderscore%20the%20need%20for%20more%20careful%20application%20of%20concept-based%20explanation%0Atechniques%20especially%20in%20settings%20where%20spatial%20interpretability%20is%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19271v1&entry.124074799=Read"},
{"title": "Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi\n  Equations", "author": "Yesom Park and Stanley Osher", "abstract": "  This paper presents an implicit solution formula for the Hamilton-Jacobi\npartial differential equation (HJ PDE). The formula is derived using the method\nof characteristics and is shown to coincide with the Hopf and Lax formulas in\nthe case where either the Hamiltonian or the initial function is convex. It\nprovides a simple and efficient numerical approach for computing the viscosity\nsolution of HJ PDEs, bypassing the need for the Legendre transform of the\nHamiltonian or the initial condition, and the explicit computation of\nindividual characteristic trajectories. A deep learning-based methodology is\nproposed to learn this implicit solution formula, leveraging the mesh-free\nnature of deep learning to ensure scalability for high-dimensional problems.\nBuilding upon this framework, an algorithm is developed that approximates the\ncharacteristic curves piecewise linearly for state-dependent Hamiltonians.\nExtensive experimental results demonstrate that the proposed method delivers\nhighly accurate solutions, even for nonconvex Hamiltonians, and exhibits\nremarkable scalability, achieving computational efficiency for problems up to\n40 dimensions.\n", "link": "http://arxiv.org/abs/2501.19351v1", "date": "2025-01-31", "relevancy": 2.1567, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.441}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Implicit%20Solution%20Formula%20for%20Efficiently%20Solving%20Hamilton-Jacobi%0A%20%20Equations&body=Title%3A%20Neural%20Implicit%20Solution%20Formula%20for%20Efficiently%20Solving%20Hamilton-Jacobi%0A%20%20Equations%0AAuthor%3A%20Yesom%20Park%20and%20Stanley%20Osher%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20implicit%20solution%20formula%20for%20the%20Hamilton-Jacobi%0Apartial%20differential%20equation%20%28HJ%20PDE%29.%20The%20formula%20is%20derived%20using%20the%20method%0Aof%20characteristics%20and%20is%20shown%20to%20coincide%20with%20the%20Hopf%20and%20Lax%20formulas%20in%0Athe%20case%20where%20either%20the%20Hamiltonian%20or%20the%20initial%20function%20is%20convex.%20It%0Aprovides%20a%20simple%20and%20efficient%20numerical%20approach%20for%20computing%20the%20viscosity%0Asolution%20of%20HJ%20PDEs%2C%20bypassing%20the%20need%20for%20the%20Legendre%20transform%20of%20the%0AHamiltonian%20or%20the%20initial%20condition%2C%20and%20the%20explicit%20computation%20of%0Aindividual%20characteristic%20trajectories.%20A%20deep%20learning-based%20methodology%20is%0Aproposed%20to%20learn%20this%20implicit%20solution%20formula%2C%20leveraging%20the%20mesh-free%0Anature%20of%20deep%20learning%20to%20ensure%20scalability%20for%20high-dimensional%20problems.%0ABuilding%20upon%20this%20framework%2C%20an%20algorithm%20is%20developed%20that%20approximates%20the%0Acharacteristic%20curves%20piecewise%20linearly%20for%20state-dependent%20Hamiltonians.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%20delivers%0Ahighly%20accurate%20solutions%2C%20even%20for%20nonconvex%20Hamiltonians%2C%20and%20exhibits%0Aremarkable%20scalability%2C%20achieving%20computational%20efficiency%20for%20problems%20up%20to%0A40%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Implicit%2520Solution%2520Formula%2520for%2520Efficiently%2520Solving%2520Hamilton-Jacobi%250A%2520%2520Equations%26entry.906535625%3DYesom%2520Park%2520and%2520Stanley%2520Osher%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520implicit%2520solution%2520formula%2520for%2520the%2520Hamilton-Jacobi%250Apartial%2520differential%2520equation%2520%2528HJ%2520PDE%2529.%2520The%2520formula%2520is%2520derived%2520using%2520the%2520method%250Aof%2520characteristics%2520and%2520is%2520shown%2520to%2520coincide%2520with%2520the%2520Hopf%2520and%2520Lax%2520formulas%2520in%250Athe%2520case%2520where%2520either%2520the%2520Hamiltonian%2520or%2520the%2520initial%2520function%2520is%2520convex.%2520It%250Aprovides%2520a%2520simple%2520and%2520efficient%2520numerical%2520approach%2520for%2520computing%2520the%2520viscosity%250Asolution%2520of%2520HJ%2520PDEs%252C%2520bypassing%2520the%2520need%2520for%2520the%2520Legendre%2520transform%2520of%2520the%250AHamiltonian%2520or%2520the%2520initial%2520condition%252C%2520and%2520the%2520explicit%2520computation%2520of%250Aindividual%2520characteristic%2520trajectories.%2520A%2520deep%2520learning-based%2520methodology%2520is%250Aproposed%2520to%2520learn%2520this%2520implicit%2520solution%2520formula%252C%2520leveraging%2520the%2520mesh-free%250Anature%2520of%2520deep%2520learning%2520to%2520ensure%2520scalability%2520for%2520high-dimensional%2520problems.%250ABuilding%2520upon%2520this%2520framework%252C%2520an%2520algorithm%2520is%2520developed%2520that%2520approximates%2520the%250Acharacteristic%2520curves%2520piecewise%2520linearly%2520for%2520state-dependent%2520Hamiltonians.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520delivers%250Ahighly%2520accurate%2520solutions%252C%2520even%2520for%2520nonconvex%2520Hamiltonians%252C%2520and%2520exhibits%250Aremarkable%2520scalability%252C%2520achieving%2520computational%2520efficiency%2520for%2520problems%2520up%2520to%250A40%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%20Solution%20Formula%20for%20Efficiently%20Solving%20Hamilton-Jacobi%0A%20%20Equations&entry.906535625=Yesom%20Park%20and%20Stanley%20Osher&entry.1292438233=%20%20This%20paper%20presents%20an%20implicit%20solution%20formula%20for%20the%20Hamilton-Jacobi%0Apartial%20differential%20equation%20%28HJ%20PDE%29.%20The%20formula%20is%20derived%20using%20the%20method%0Aof%20characteristics%20and%20is%20shown%20to%20coincide%20with%20the%20Hopf%20and%20Lax%20formulas%20in%0Athe%20case%20where%20either%20the%20Hamiltonian%20or%20the%20initial%20function%20is%20convex.%20It%0Aprovides%20a%20simple%20and%20efficient%20numerical%20approach%20for%20computing%20the%20viscosity%0Asolution%20of%20HJ%20PDEs%2C%20bypassing%20the%20need%20for%20the%20Legendre%20transform%20of%20the%0AHamiltonian%20or%20the%20initial%20condition%2C%20and%20the%20explicit%20computation%20of%0Aindividual%20characteristic%20trajectories.%20A%20deep%20learning-based%20methodology%20is%0Aproposed%20to%20learn%20this%20implicit%20solution%20formula%2C%20leveraging%20the%20mesh-free%0Anature%20of%20deep%20learning%20to%20ensure%20scalability%20for%20high-dimensional%20problems.%0ABuilding%20upon%20this%20framework%2C%20an%20algorithm%20is%20developed%20that%20approximates%20the%0Acharacteristic%20curves%20piecewise%20linearly%20for%20state-dependent%20Hamiltonians.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%20delivers%0Ahighly%20accurate%20solutions%2C%20even%20for%20nonconvex%20Hamiltonians%2C%20and%20exhibits%0Aremarkable%20scalability%2C%20achieving%20computational%20efficiency%20for%20problems%20up%20to%0A40%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19351v1&entry.124074799=Read"},
{"title": "Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer\n  Using Generative Artificial Intelligence", "author": "Aurora Rofena and Claudia Lucia Piccolo and Bruno Beomonte Zobel and Paolo Soda and Valerio Guarrasi", "abstract": "  Full-Field Digital Mammography (FFDM) is the primary imaging modality for\nroutine breast cancer screening; however, its effectiveness is limited in\npatients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers enhanced\naccuracy in tumor detection. Nonetheless, its application is restricted due to\nhigher radiation exposure, the use of contrast agents, and limited\naccessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral\noblique views to classify lesions as malignant or benign. To address the\nchallenge of missing CESM data, we leverage generative artificial intelligence\nto impute CESM images from FFDM scans. Experimental results demonstrate that\nincorporating the CESM modality is crucial to enhance the performance of\nvirtual biopsy. When real CESM data is missing, synthetic CESM images proved\neffective, outperforming the use of FFDM alone, particularly in multimodal\nconfigurations that combine FFDM and CESM modalities. The proposed approach has\nthe potential to improve diagnostic workflows, providing clinicians with\naugmented intelligence tools to improve diagnostic accuracy and patient care.\nAdditionally, as a contribution to the research community, we publicly release\nthe dataset used in our experiments, facilitating further advancements in this\nfield.\n", "link": "http://arxiv.org/abs/2501.19176v1", "date": "2025-01-31", "relevancy": 2.1543, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5417}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Intelligence%20for%20Multimodal%20Virtual%20Biopsy%20in%20Breast%20Cancer%0A%20%20Using%20Generative%20Artificial%20Intelligence&body=Title%3A%20Augmented%20Intelligence%20for%20Multimodal%20Virtual%20Biopsy%20in%20Breast%20Cancer%0A%20%20Using%20Generative%20Artificial%20Intelligence%0AAuthor%3A%20Aurora%20Rofena%20and%20Claudia%20Lucia%20Piccolo%20and%20Bruno%20Beomonte%20Zobel%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi%0AAbstract%3A%20%20%20Full-Field%20Digital%20Mammography%20%28FFDM%29%20is%20the%20primary%20imaging%20modality%20for%0Aroutine%20breast%20cancer%20screening%3B%20however%2C%20its%20effectiveness%20is%20limited%20in%0Apatients%20with%20dense%20breast%20tissue%20or%20fibrocystic%20conditions.%20Contrast-Enhanced%0ASpectral%20Mammography%20%28CESM%29%2C%20a%20second-level%20imaging%20technique%2C%20offers%20enhanced%0Aaccuracy%20in%20tumor%20detection.%20Nonetheless%2C%20its%20application%20is%20restricted%20due%20to%0Ahigher%20radiation%20exposure%2C%20the%20use%20of%20contrast%20agents%2C%20and%20limited%0Aaccessibility.%20As%20a%20result%2C%20CESM%20is%20typically%20reserved%20for%20select%20cases%2C%0Aleaving%20many%20patients%20to%20rely%20solely%20on%20FFDM%20despite%20the%20superior%20diagnostic%0Aperformance%20of%20CESM.%20While%20biopsy%20remains%20the%20gold%20standard%20for%20definitive%0Adiagnosis%2C%20it%20is%20an%20invasive%20procedure%20that%20can%20cause%20discomfort%20for%20patients.%0AWe%20introduce%20a%20multimodal%2C%20multi-view%20deep%20learning%20approach%20for%20virtual%0Abiopsy%2C%20integrating%20FFDM%20and%20CESM%20modalities%20in%20craniocaudal%20and%20mediolateral%0Aoblique%20views%20to%20classify%20lesions%20as%20malignant%20or%20benign.%20To%20address%20the%0Achallenge%20of%20missing%20CESM%20data%2C%20we%20leverage%20generative%20artificial%20intelligence%0Ato%20impute%20CESM%20images%20from%20FFDM%20scans.%20Experimental%20results%20demonstrate%20that%0Aincorporating%20the%20CESM%20modality%20is%20crucial%20to%20enhance%20the%20performance%20of%0Avirtual%20biopsy.%20When%20real%20CESM%20data%20is%20missing%2C%20synthetic%20CESM%20images%20proved%0Aeffective%2C%20outperforming%20the%20use%20of%20FFDM%20alone%2C%20particularly%20in%20multimodal%0Aconfigurations%20that%20combine%20FFDM%20and%20CESM%20modalities.%20The%20proposed%20approach%20has%0Athe%20potential%20to%20improve%20diagnostic%20workflows%2C%20providing%20clinicians%20with%0Aaugmented%20intelligence%20tools%20to%20improve%20diagnostic%20accuracy%20and%20patient%20care.%0AAdditionally%2C%20as%20a%20contribution%20to%20the%20research%20community%2C%20we%20publicly%20release%0Athe%20dataset%20used%20in%20our%20experiments%2C%20facilitating%20further%20advancements%20in%20this%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Intelligence%2520for%2520Multimodal%2520Virtual%2520Biopsy%2520in%2520Breast%2520Cancer%250A%2520%2520Using%2520Generative%2520Artificial%2520Intelligence%26entry.906535625%3DAurora%2520Rofena%2520and%2520Claudia%2520Lucia%2520Piccolo%2520and%2520Bruno%2520Beomonte%2520Zobel%2520and%2520Paolo%2520Soda%2520and%2520Valerio%2520Guarrasi%26entry.1292438233%3D%2520%2520Full-Field%2520Digital%2520Mammography%2520%2528FFDM%2529%2520is%2520the%2520primary%2520imaging%2520modality%2520for%250Aroutine%2520breast%2520cancer%2520screening%253B%2520however%252C%2520its%2520effectiveness%2520is%2520limited%2520in%250Apatients%2520with%2520dense%2520breast%2520tissue%2520or%2520fibrocystic%2520conditions.%2520Contrast-Enhanced%250ASpectral%2520Mammography%2520%2528CESM%2529%252C%2520a%2520second-level%2520imaging%2520technique%252C%2520offers%2520enhanced%250Aaccuracy%2520in%2520tumor%2520detection.%2520Nonetheless%252C%2520its%2520application%2520is%2520restricted%2520due%2520to%250Ahigher%2520radiation%2520exposure%252C%2520the%2520use%2520of%2520contrast%2520agents%252C%2520and%2520limited%250Aaccessibility.%2520As%2520a%2520result%252C%2520CESM%2520is%2520typically%2520reserved%2520for%2520select%2520cases%252C%250Aleaving%2520many%2520patients%2520to%2520rely%2520solely%2520on%2520FFDM%2520despite%2520the%2520superior%2520diagnostic%250Aperformance%2520of%2520CESM.%2520While%2520biopsy%2520remains%2520the%2520gold%2520standard%2520for%2520definitive%250Adiagnosis%252C%2520it%2520is%2520an%2520invasive%2520procedure%2520that%2520can%2520cause%2520discomfort%2520for%2520patients.%250AWe%2520introduce%2520a%2520multimodal%252C%2520multi-view%2520deep%2520learning%2520approach%2520for%2520virtual%250Abiopsy%252C%2520integrating%2520FFDM%2520and%2520CESM%2520modalities%2520in%2520craniocaudal%2520and%2520mediolateral%250Aoblique%2520views%2520to%2520classify%2520lesions%2520as%2520malignant%2520or%2520benign.%2520To%2520address%2520the%250Achallenge%2520of%2520missing%2520CESM%2520data%252C%2520we%2520leverage%2520generative%2520artificial%2520intelligence%250Ato%2520impute%2520CESM%2520images%2520from%2520FFDM%2520scans.%2520Experimental%2520results%2520demonstrate%2520that%250Aincorporating%2520the%2520CESM%2520modality%2520is%2520crucial%2520to%2520enhance%2520the%2520performance%2520of%250Avirtual%2520biopsy.%2520When%2520real%2520CESM%2520data%2520is%2520missing%252C%2520synthetic%2520CESM%2520images%2520proved%250Aeffective%252C%2520outperforming%2520the%2520use%2520of%2520FFDM%2520alone%252C%2520particularly%2520in%2520multimodal%250Aconfigurations%2520that%2520combine%2520FFDM%2520and%2520CESM%2520modalities.%2520The%2520proposed%2520approach%2520has%250Athe%2520potential%2520to%2520improve%2520diagnostic%2520workflows%252C%2520providing%2520clinicians%2520with%250Aaugmented%2520intelligence%2520tools%2520to%2520improve%2520diagnostic%2520accuracy%2520and%2520patient%2520care.%250AAdditionally%252C%2520as%2520a%2520contribution%2520to%2520the%2520research%2520community%252C%2520we%2520publicly%2520release%250Athe%2520dataset%2520used%2520in%2520our%2520experiments%252C%2520facilitating%2520further%2520advancements%2520in%2520this%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Intelligence%20for%20Multimodal%20Virtual%20Biopsy%20in%20Breast%20Cancer%0A%20%20Using%20Generative%20Artificial%20Intelligence&entry.906535625=Aurora%20Rofena%20and%20Claudia%20Lucia%20Piccolo%20and%20Bruno%20Beomonte%20Zobel%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi&entry.1292438233=%20%20Full-Field%20Digital%20Mammography%20%28FFDM%29%20is%20the%20primary%20imaging%20modality%20for%0Aroutine%20breast%20cancer%20screening%3B%20however%2C%20its%20effectiveness%20is%20limited%20in%0Apatients%20with%20dense%20breast%20tissue%20or%20fibrocystic%20conditions.%20Contrast-Enhanced%0ASpectral%20Mammography%20%28CESM%29%2C%20a%20second-level%20imaging%20technique%2C%20offers%20enhanced%0Aaccuracy%20in%20tumor%20detection.%20Nonetheless%2C%20its%20application%20is%20restricted%20due%20to%0Ahigher%20radiation%20exposure%2C%20the%20use%20of%20contrast%20agents%2C%20and%20limited%0Aaccessibility.%20As%20a%20result%2C%20CESM%20is%20typically%20reserved%20for%20select%20cases%2C%0Aleaving%20many%20patients%20to%20rely%20solely%20on%20FFDM%20despite%20the%20superior%20diagnostic%0Aperformance%20of%20CESM.%20While%20biopsy%20remains%20the%20gold%20standard%20for%20definitive%0Adiagnosis%2C%20it%20is%20an%20invasive%20procedure%20that%20can%20cause%20discomfort%20for%20patients.%0AWe%20introduce%20a%20multimodal%2C%20multi-view%20deep%20learning%20approach%20for%20virtual%0Abiopsy%2C%20integrating%20FFDM%20and%20CESM%20modalities%20in%20craniocaudal%20and%20mediolateral%0Aoblique%20views%20to%20classify%20lesions%20as%20malignant%20or%20benign.%20To%20address%20the%0Achallenge%20of%20missing%20CESM%20data%2C%20we%20leverage%20generative%20artificial%20intelligence%0Ato%20impute%20CESM%20images%20from%20FFDM%20scans.%20Experimental%20results%20demonstrate%20that%0Aincorporating%20the%20CESM%20modality%20is%20crucial%20to%20enhance%20the%20performance%20of%0Avirtual%20biopsy.%20When%20real%20CESM%20data%20is%20missing%2C%20synthetic%20CESM%20images%20proved%0Aeffective%2C%20outperforming%20the%20use%20of%20FFDM%20alone%2C%20particularly%20in%20multimodal%0Aconfigurations%20that%20combine%20FFDM%20and%20CESM%20modalities.%20The%20proposed%20approach%20has%0Athe%20potential%20to%20improve%20diagnostic%20workflows%2C%20providing%20clinicians%20with%0Aaugmented%20intelligence%20tools%20to%20improve%20diagnostic%20accuracy%20and%20patient%20care.%0AAdditionally%2C%20as%20a%20contribution%20to%20the%20research%20community%2C%20we%20publicly%20release%0Athe%20dataset%20used%20in%20our%20experiments%2C%20facilitating%20further%20advancements%20in%20this%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19176v1&entry.124074799=Read"},
{"title": "Improving Multi-Label Contrastive Learning by Leveraging Label\n  Distribution", "author": "Ning Chen and Shen-Huan Lyu and Tian-Shuang Wu and Yanyan Wang and Bin Tang", "abstract": "  In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.\n", "link": "http://arxiv.org/abs/2501.19145v1", "date": "2025-01-31", "relevancy": 2.1497, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5327}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multi-Label%20Contrastive%20Learning%20by%20Leveraging%20Label%0A%20%20Distribution&body=Title%3A%20Improving%20Multi-Label%20Contrastive%20Learning%20by%20Leveraging%20Label%0A%20%20Distribution%0AAuthor%3A%20Ning%20Chen%20and%20Shen-Huan%20Lyu%20and%20Tian-Shuang%20Wu%20and%20Yanyan%20Wang%20and%20Bin%20Tang%0AAbstract%3A%20%20%20In%20multi-label%20learning%2C%20leveraging%20contrastive%20learning%20to%20learn%20better%0Arepresentations%20faces%20a%20key%20challenge%3A%20selecting%20positive%20and%20negative%20samples%0Aand%20effectively%20utilizing%20label%20information.%20Previous%20studies%20selected%20positive%0Aand%20negative%20samples%20based%20on%20the%20overlap%20between%20labels%20and%20used%20them%20for%0Alabel-wise%20loss%20balancing.%20However%2C%20these%20methods%20suffer%20from%20a%20complex%0Aselection%20process%20and%20fail%20to%20account%20for%20the%20varying%20importance%20of%20different%0Alabels.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20method%20that%20improves%0Amulti-label%20contrastive%20learning%20through%20label%20distribution.%20Specifically%2C%20when%0Aselecting%20positive%20and%20negative%20samples%2C%20we%20only%20need%20to%20consider%20whether%20there%0Ais%20an%20intersection%20between%20labels.%20To%20model%20the%20relationships%20between%20labels%2C%0Awe%20introduce%20two%20methods%20to%20recover%20label%20distributions%20from%20logical%20labels%2C%0Abased%20on%20Radial%20Basis%20Function%20%28RBF%29%20and%20contrastive%20loss%2C%20respectively.%20We%0Aevaluate%20our%20method%20on%20nine%20widely%20used%20multi-label%20datasets%2C%20including%20image%0Aand%20vector%20datasets.%20The%20results%20demonstrate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods%20in%20six%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multi-Label%2520Contrastive%2520Learning%2520by%2520Leveraging%2520Label%250A%2520%2520Distribution%26entry.906535625%3DNing%2520Chen%2520and%2520Shen-Huan%2520Lyu%2520and%2520Tian-Shuang%2520Wu%2520and%2520Yanyan%2520Wang%2520and%2520Bin%2520Tang%26entry.1292438233%3D%2520%2520In%2520multi-label%2520learning%252C%2520leveraging%2520contrastive%2520learning%2520to%2520learn%2520better%250Arepresentations%2520faces%2520a%2520key%2520challenge%253A%2520selecting%2520positive%2520and%2520negative%2520samples%250Aand%2520effectively%2520utilizing%2520label%2520information.%2520Previous%2520studies%2520selected%2520positive%250Aand%2520negative%2520samples%2520based%2520on%2520the%2520overlap%2520between%2520labels%2520and%2520used%2520them%2520for%250Alabel-wise%2520loss%2520balancing.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520a%2520complex%250Aselection%2520process%2520and%2520fail%2520to%2520account%2520for%2520the%2520varying%2520importance%2520of%2520different%250Alabels.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%2520improves%250Amulti-label%2520contrastive%2520learning%2520through%2520label%2520distribution.%2520Specifically%252C%2520when%250Aselecting%2520positive%2520and%2520negative%2520samples%252C%2520we%2520only%2520need%2520to%2520consider%2520whether%2520there%250Ais%2520an%2520intersection%2520between%2520labels.%2520To%2520model%2520the%2520relationships%2520between%2520labels%252C%250Awe%2520introduce%2520two%2520methods%2520to%2520recover%2520label%2520distributions%2520from%2520logical%2520labels%252C%250Abased%2520on%2520Radial%2520Basis%2520Function%2520%2528RBF%2529%2520and%2520contrastive%2520loss%252C%2520respectively.%2520We%250Aevaluate%2520our%2520method%2520on%2520nine%2520widely%2520used%2520multi-label%2520datasets%252C%2520including%2520image%250Aand%2520vector%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520six%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multi-Label%20Contrastive%20Learning%20by%20Leveraging%20Label%0A%20%20Distribution&entry.906535625=Ning%20Chen%20and%20Shen-Huan%20Lyu%20and%20Tian-Shuang%20Wu%20and%20Yanyan%20Wang%20and%20Bin%20Tang&entry.1292438233=%20%20In%20multi-label%20learning%2C%20leveraging%20contrastive%20learning%20to%20learn%20better%0Arepresentations%20faces%20a%20key%20challenge%3A%20selecting%20positive%20and%20negative%20samples%0Aand%20effectively%20utilizing%20label%20information.%20Previous%20studies%20selected%20positive%0Aand%20negative%20samples%20based%20on%20the%20overlap%20between%20labels%20and%20used%20them%20for%0Alabel-wise%20loss%20balancing.%20However%2C%20these%20methods%20suffer%20from%20a%20complex%0Aselection%20process%20and%20fail%20to%20account%20for%20the%20varying%20importance%20of%20different%0Alabels.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20method%20that%20improves%0Amulti-label%20contrastive%20learning%20through%20label%20distribution.%20Specifically%2C%20when%0Aselecting%20positive%20and%20negative%20samples%2C%20we%20only%20need%20to%20consider%20whether%20there%0Ais%20an%20intersection%20between%20labels.%20To%20model%20the%20relationships%20between%20labels%2C%0Awe%20introduce%20two%20methods%20to%20recover%20label%20distributions%20from%20logical%20labels%2C%0Abased%20on%20Radial%20Basis%20Function%20%28RBF%29%20and%20contrastive%20loss%2C%20respectively.%20We%0Aevaluate%20our%20method%20on%20nine%20widely%20used%20multi-label%20datasets%2C%20including%20image%0Aand%20vector%20datasets.%20The%20results%20demonstrate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods%20in%20six%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19145v1&entry.124074799=Read"},
{"title": "A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration", "author": "Yuchen Hu and Xi Chen and Weidong Liu and Xiaojun Mao", "abstract": "  Distributed stochastic optimization algorithms can handle large-scale data\nsimultaneously and accelerate model training. However, the sparsity of\ndistributed networks and the heterogeneity of data limit these advantages. This\npaper proposes a momentum-accelerated distributed stochastic gradient\nalgorithm, referred to as Exact-Diffusion with Momentum (EDM), which can\ncorrect the bias caused by data heterogeneity and introduces the momentum\nmethod commonly used in deep learning to accelerate the convergence of the\nalgorithm. We theoretically demonstrate that this algorithm converges to the\nneighborhood of the optimum sub-linearly irrelevant to data heterogeneity when\napplied to non-convex objective functions and linearly under the\nPolyak-{\\L}ojasiewicz condition (a weaker assumption than $\\mu$-strongly\nconvexity). Finally, we evaluate the performance of the proposed algorithm by\nsimulation, comparing it with a range of existing decentralized optimization\nalgorithms to demonstrate its effectiveness in addressing data heterogeneity\nand network sparsity.\n", "link": "http://arxiv.org/abs/2501.19082v1", "date": "2025-01-31", "relevancy": 2.1476, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration&body=Title%3A%20A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration%0AAuthor%3A%20Yuchen%20Hu%20and%20Xi%20Chen%20and%20Weidong%20Liu%20and%20Xiaojun%20Mao%0AAbstract%3A%20%20%20Distributed%20stochastic%20optimization%20algorithms%20can%20handle%20large-scale%20data%0Asimultaneously%20and%20accelerate%20model%20training.%20However%2C%20the%20sparsity%20of%0Adistributed%20networks%20and%20the%20heterogeneity%20of%20data%20limit%20these%20advantages.%20This%0Apaper%20proposes%20a%20momentum-accelerated%20distributed%20stochastic%20gradient%0Aalgorithm%2C%20referred%20to%20as%20Exact-Diffusion%20with%20Momentum%20%28EDM%29%2C%20which%20can%0Acorrect%20the%20bias%20caused%20by%20data%20heterogeneity%20and%20introduces%20the%20momentum%0Amethod%20commonly%20used%20in%20deep%20learning%20to%20accelerate%20the%20convergence%20of%20the%0Aalgorithm.%20We%20theoretically%20demonstrate%20that%20this%20algorithm%20converges%20to%20the%0Aneighborhood%20of%20the%20optimum%20sub-linearly%20irrelevant%20to%20data%20heterogeneity%20when%0Aapplied%20to%20non-convex%20objective%20functions%20and%20linearly%20under%20the%0APolyak-%7B%5CL%7Dojasiewicz%20condition%20%28a%20weaker%20assumption%20than%20%24%5Cmu%24-strongly%0Aconvexity%29.%20Finally%2C%20we%20evaluate%20the%20performance%20of%20the%20proposed%20algorithm%20by%0Asimulation%2C%20comparing%20it%20with%20a%20range%20of%20existing%20decentralized%20optimization%0Aalgorithms%20to%20demonstrate%20its%20effectiveness%20in%20addressing%20data%20heterogeneity%0Aand%20network%20sparsity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bias-Correction%2520Decentralized%2520Stochastic%2520Gradient%2520Algorithm%2520with%250A%2520%2520Momentum%2520Acceleration%26entry.906535625%3DYuchen%2520Hu%2520and%2520Xi%2520Chen%2520and%2520Weidong%2520Liu%2520and%2520Xiaojun%2520Mao%26entry.1292438233%3D%2520%2520Distributed%2520stochastic%2520optimization%2520algorithms%2520can%2520handle%2520large-scale%2520data%250Asimultaneously%2520and%2520accelerate%2520model%2520training.%2520However%252C%2520the%2520sparsity%2520of%250Adistributed%2520networks%2520and%2520the%2520heterogeneity%2520of%2520data%2520limit%2520these%2520advantages.%2520This%250Apaper%2520proposes%2520a%2520momentum-accelerated%2520distributed%2520stochastic%2520gradient%250Aalgorithm%252C%2520referred%2520to%2520as%2520Exact-Diffusion%2520with%2520Momentum%2520%2528EDM%2529%252C%2520which%2520can%250Acorrect%2520the%2520bias%2520caused%2520by%2520data%2520heterogeneity%2520and%2520introduces%2520the%2520momentum%250Amethod%2520commonly%2520used%2520in%2520deep%2520learning%2520to%2520accelerate%2520the%2520convergence%2520of%2520the%250Aalgorithm.%2520We%2520theoretically%2520demonstrate%2520that%2520this%2520algorithm%2520converges%2520to%2520the%250Aneighborhood%2520of%2520the%2520optimum%2520sub-linearly%2520irrelevant%2520to%2520data%2520heterogeneity%2520when%250Aapplied%2520to%2520non-convex%2520objective%2520functions%2520and%2520linearly%2520under%2520the%250APolyak-%257B%255CL%257Dojasiewicz%2520condition%2520%2528a%2520weaker%2520assumption%2520than%2520%2524%255Cmu%2524-strongly%250Aconvexity%2529.%2520Finally%252C%2520we%2520evaluate%2520the%2520performance%2520of%2520the%2520proposed%2520algorithm%2520by%250Asimulation%252C%2520comparing%2520it%2520with%2520a%2520range%2520of%2520existing%2520decentralized%2520optimization%250Aalgorithms%2520to%2520demonstrate%2520its%2520effectiveness%2520in%2520addressing%2520data%2520heterogeneity%250Aand%2520network%2520sparsity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration&entry.906535625=Yuchen%20Hu%20and%20Xi%20Chen%20and%20Weidong%20Liu%20and%20Xiaojun%20Mao&entry.1292438233=%20%20Distributed%20stochastic%20optimization%20algorithms%20can%20handle%20large-scale%20data%0Asimultaneously%20and%20accelerate%20model%20training.%20However%2C%20the%20sparsity%20of%0Adistributed%20networks%20and%20the%20heterogeneity%20of%20data%20limit%20these%20advantages.%20This%0Apaper%20proposes%20a%20momentum-accelerated%20distributed%20stochastic%20gradient%0Aalgorithm%2C%20referred%20to%20as%20Exact-Diffusion%20with%20Momentum%20%28EDM%29%2C%20which%20can%0Acorrect%20the%20bias%20caused%20by%20data%20heterogeneity%20and%20introduces%20the%20momentum%0Amethod%20commonly%20used%20in%20deep%20learning%20to%20accelerate%20the%20convergence%20of%20the%0Aalgorithm.%20We%20theoretically%20demonstrate%20that%20this%20algorithm%20converges%20to%20the%0Aneighborhood%20of%20the%20optimum%20sub-linearly%20irrelevant%20to%20data%20heterogeneity%20when%0Aapplied%20to%20non-convex%20objective%20functions%20and%20linearly%20under%20the%0APolyak-%7B%5CL%7Dojasiewicz%20condition%20%28a%20weaker%20assumption%20than%20%24%5Cmu%24-strongly%0Aconvexity%29.%20Finally%2C%20we%20evaluate%20the%20performance%20of%20the%20proposed%20algorithm%20by%0Asimulation%2C%20comparing%20it%20with%20a%20range%20of%20existing%20decentralized%20optimization%0Aalgorithms%20to%20demonstrate%20its%20effectiveness%20in%20addressing%20data%20heterogeneity%0Aand%20network%20sparsity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19082v1&entry.124074799=Read"},
{"title": "EgoMe: Follow Me via Egocentric View in Real World", "author": "Heqian Qiu and Zhaofeng Shi and Lanxiao Wang and Huiyu Xiong and Xiang Li and Hongliang Li", "abstract": "  When interacting with the real world, human often take the egocentric\n(first-person) view as a benchmark, naturally transferring behaviors observed\nfrom a exocentric (third-person) view to their own. This cognitive theory\nprovides a foundation for researching how robots can more effectively imitate\nhuman behavior. However, current research either employs multiple cameras with\ndifferent views focusing on the same individual's behavior simultaneously or\nencounters unpair ego-exo view scenarios, there is no effort to fully exploit\nhuman cognitive behavior in the real world. To fill this gap, in this paper, we\nintroduce a novel large-scale egocentric dataset, called EgoMe, which towards\nfollowing the process of human imitation learning via egocentric view in the\nreal world. Our dataset includes 7902 pairs of videos (15804 videos) for\ndiverse daily behaviors in real-world scenarios. For a pair of videos, one\nvideo captures a exocentric view of the imitator observing the demonstrator's\nactions, while the other captures a egocentric view of the imitator\nsubsequently following those actions. Notably, our dataset also contain exo-ego\neye gaze, angular velocity, acceleration, magnetic strength and other sensor\nmulti-modal data for assisting in establishing correlations between observing\nand following process. In addition, we also propose eight challenging benchmark\ntasks for fully leveraging this data resource and promoting the research of\nrobot imitation learning ability. Extensive statistical analysis demonstrates\nsignificant advantages compared to existing datasets. The proposed EgoMe\ndataset and benchmark will be released soon.\n", "link": "http://arxiv.org/abs/2501.19061v1", "date": "2025-01-31", "relevancy": 2.1459, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5451}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5335}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoMe%3A%20Follow%20Me%20via%20Egocentric%20View%20in%20Real%20World&body=Title%3A%20EgoMe%3A%20Follow%20Me%20via%20Egocentric%20View%20in%20Real%20World%0AAuthor%3A%20Heqian%20Qiu%20and%20Zhaofeng%20Shi%20and%20Lanxiao%20Wang%20and%20Huiyu%20Xiong%20and%20Xiang%20Li%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20When%20interacting%20with%20the%20real%20world%2C%20human%20often%20take%20the%20egocentric%0A%28first-person%29%20view%20as%20a%20benchmark%2C%20naturally%20transferring%20behaviors%20observed%0Afrom%20a%20exocentric%20%28third-person%29%20view%20to%20their%20own.%20This%20cognitive%20theory%0Aprovides%20a%20foundation%20for%20researching%20how%20robots%20can%20more%20effectively%20imitate%0Ahuman%20behavior.%20However%2C%20current%20research%20either%20employs%20multiple%20cameras%20with%0Adifferent%20views%20focusing%20on%20the%20same%20individual%27s%20behavior%20simultaneously%20or%0Aencounters%20unpair%20ego-exo%20view%20scenarios%2C%20there%20is%20no%20effort%20to%20fully%20exploit%0Ahuman%20cognitive%20behavior%20in%20the%20real%20world.%20To%20fill%20this%20gap%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20large-scale%20egocentric%20dataset%2C%20called%20EgoMe%2C%20which%20towards%0Afollowing%20the%20process%20of%20human%20imitation%20learning%20via%20egocentric%20view%20in%20the%0Areal%20world.%20Our%20dataset%20includes%207902%20pairs%20of%20videos%20%2815804%20videos%29%20for%0Adiverse%20daily%20behaviors%20in%20real-world%20scenarios.%20For%20a%20pair%20of%20videos%2C%20one%0Avideo%20captures%20a%20exocentric%20view%20of%20the%20imitator%20observing%20the%20demonstrator%27s%0Aactions%2C%20while%20the%20other%20captures%20a%20egocentric%20view%20of%20the%20imitator%0Asubsequently%20following%20those%20actions.%20Notably%2C%20our%20dataset%20also%20contain%20exo-ego%0Aeye%20gaze%2C%20angular%20velocity%2C%20acceleration%2C%20magnetic%20strength%20and%20other%20sensor%0Amulti-modal%20data%20for%20assisting%20in%20establishing%20correlations%20between%20observing%0Aand%20following%20process.%20In%20addition%2C%20we%20also%20propose%20eight%20challenging%20benchmark%0Atasks%20for%20fully%20leveraging%20this%20data%20resource%20and%20promoting%20the%20research%20of%0Arobot%20imitation%20learning%20ability.%20Extensive%20statistical%20analysis%20demonstrates%0Asignificant%20advantages%20compared%20to%20existing%20datasets.%20The%20proposed%20EgoMe%0Adataset%20and%20benchmark%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoMe%253A%2520Follow%2520Me%2520via%2520Egocentric%2520View%2520in%2520Real%2520World%26entry.906535625%3DHeqian%2520Qiu%2520and%2520Zhaofeng%2520Shi%2520and%2520Lanxiao%2520Wang%2520and%2520Huiyu%2520Xiong%2520and%2520Xiang%2520Li%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520When%2520interacting%2520with%2520the%2520real%2520world%252C%2520human%2520often%2520take%2520the%2520egocentric%250A%2528first-person%2529%2520view%2520as%2520a%2520benchmark%252C%2520naturally%2520transferring%2520behaviors%2520observed%250Afrom%2520a%2520exocentric%2520%2528third-person%2529%2520view%2520to%2520their%2520own.%2520This%2520cognitive%2520theory%250Aprovides%2520a%2520foundation%2520for%2520researching%2520how%2520robots%2520can%2520more%2520effectively%2520imitate%250Ahuman%2520behavior.%2520However%252C%2520current%2520research%2520either%2520employs%2520multiple%2520cameras%2520with%250Adifferent%2520views%2520focusing%2520on%2520the%2520same%2520individual%2527s%2520behavior%2520simultaneously%2520or%250Aencounters%2520unpair%2520ego-exo%2520view%2520scenarios%252C%2520there%2520is%2520no%2520effort%2520to%2520fully%2520exploit%250Ahuman%2520cognitive%2520behavior%2520in%2520the%2520real%2520world.%2520To%2520fill%2520this%2520gap%252C%2520in%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520large-scale%2520egocentric%2520dataset%252C%2520called%2520EgoMe%252C%2520which%2520towards%250Afollowing%2520the%2520process%2520of%2520human%2520imitation%2520learning%2520via%2520egocentric%2520view%2520in%2520the%250Areal%2520world.%2520Our%2520dataset%2520includes%25207902%2520pairs%2520of%2520videos%2520%252815804%2520videos%2529%2520for%250Adiverse%2520daily%2520behaviors%2520in%2520real-world%2520scenarios.%2520For%2520a%2520pair%2520of%2520videos%252C%2520one%250Avideo%2520captures%2520a%2520exocentric%2520view%2520of%2520the%2520imitator%2520observing%2520the%2520demonstrator%2527s%250Aactions%252C%2520while%2520the%2520other%2520captures%2520a%2520egocentric%2520view%2520of%2520the%2520imitator%250Asubsequently%2520following%2520those%2520actions.%2520Notably%252C%2520our%2520dataset%2520also%2520contain%2520exo-ego%250Aeye%2520gaze%252C%2520angular%2520velocity%252C%2520acceleration%252C%2520magnetic%2520strength%2520and%2520other%2520sensor%250Amulti-modal%2520data%2520for%2520assisting%2520in%2520establishing%2520correlations%2520between%2520observing%250Aand%2520following%2520process.%2520In%2520addition%252C%2520we%2520also%2520propose%2520eight%2520challenging%2520benchmark%250Atasks%2520for%2520fully%2520leveraging%2520this%2520data%2520resource%2520and%2520promoting%2520the%2520research%2520of%250Arobot%2520imitation%2520learning%2520ability.%2520Extensive%2520statistical%2520analysis%2520demonstrates%250Asignificant%2520advantages%2520compared%2520to%2520existing%2520datasets.%2520The%2520proposed%2520EgoMe%250Adataset%2520and%2520benchmark%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoMe%3A%20Follow%20Me%20via%20Egocentric%20View%20in%20Real%20World&entry.906535625=Heqian%20Qiu%20and%20Zhaofeng%20Shi%20and%20Lanxiao%20Wang%20and%20Huiyu%20Xiong%20and%20Xiang%20Li%20and%20Hongliang%20Li&entry.1292438233=%20%20When%20interacting%20with%20the%20real%20world%2C%20human%20often%20take%20the%20egocentric%0A%28first-person%29%20view%20as%20a%20benchmark%2C%20naturally%20transferring%20behaviors%20observed%0Afrom%20a%20exocentric%20%28third-person%29%20view%20to%20their%20own.%20This%20cognitive%20theory%0Aprovides%20a%20foundation%20for%20researching%20how%20robots%20can%20more%20effectively%20imitate%0Ahuman%20behavior.%20However%2C%20current%20research%20either%20employs%20multiple%20cameras%20with%0Adifferent%20views%20focusing%20on%20the%20same%20individual%27s%20behavior%20simultaneously%20or%0Aencounters%20unpair%20ego-exo%20view%20scenarios%2C%20there%20is%20no%20effort%20to%20fully%20exploit%0Ahuman%20cognitive%20behavior%20in%20the%20real%20world.%20To%20fill%20this%20gap%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20large-scale%20egocentric%20dataset%2C%20called%20EgoMe%2C%20which%20towards%0Afollowing%20the%20process%20of%20human%20imitation%20learning%20via%20egocentric%20view%20in%20the%0Areal%20world.%20Our%20dataset%20includes%207902%20pairs%20of%20videos%20%2815804%20videos%29%20for%0Adiverse%20daily%20behaviors%20in%20real-world%20scenarios.%20For%20a%20pair%20of%20videos%2C%20one%0Avideo%20captures%20a%20exocentric%20view%20of%20the%20imitator%20observing%20the%20demonstrator%27s%0Aactions%2C%20while%20the%20other%20captures%20a%20egocentric%20view%20of%20the%20imitator%0Asubsequently%20following%20those%20actions.%20Notably%2C%20our%20dataset%20also%20contain%20exo-ego%0Aeye%20gaze%2C%20angular%20velocity%2C%20acceleration%2C%20magnetic%20strength%20and%20other%20sensor%0Amulti-modal%20data%20for%20assisting%20in%20establishing%20correlations%20between%20observing%0Aand%20following%20process.%20In%20addition%2C%20we%20also%20propose%20eight%20challenging%20benchmark%0Atasks%20for%20fully%20leveraging%20this%20data%20resource%20and%20promoting%20the%20research%20of%0Arobot%20imitation%20learning%20ability.%20Extensive%20statistical%20analysis%20demonstrates%0Asignificant%20advantages%20compared%20to%20existing%20datasets.%20The%20proposed%20EgoMe%0Adataset%20and%20benchmark%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19061v1&entry.124074799=Read"},
{"title": "A Comunication Framework for Compositional Generation", "author": "Rafael Elberg and Mircea Petrache and Denis Parra", "abstract": "  Compositionality and compositional generalization--the ability to understand\nnovel combinations of known concepts--are central characteristics of human\nlanguage and are hypothesized to be essential for human cognition. In machine\nlearning, the emergence of this property has been studied in a communication\ngame setting, where independent agents (a sender and a receiver) converge to a\nshared encoding policy from a set of states to a space of discrete messages,\nwhere the receiver can correctly reconstruct the states observed by the sender\nusing only the sender's messages. The use of communication games in generation\ntasks is still largely unexplored, with recent methods for compositional\ngeneration focusing mainly on the use of supervised guidance (either through\nclass labels or text). In this work, we take the first steps to fill this gap,\nand we present a self-supervised generative communication game-based framework\nfor creating compositional encodings in learned representations from\npre-trained encoder-decoder models. In an Iterated Learning (IL) protocol\ninvolving a sender and a receiver, we apply alternating pressures for\ncompression and diversity of encoded discrete messages, so that the protocol\nconverges to an efficient but unambiguous encoding. Approximate message entropy\nregularization is used to favor compositional encodings. Our framework is based\non rigorous justifications and proofs of defining and balancing the concepts of\nEficiency, Unambiguity and Non-Holisticity in encoding. We test our method on\nthe compositional image dataset Shapes3D, demonstrating robust performance in\nboth reconstruction and compositionality metrics, surpassing other tested\ndiscrete message frameworks.\n", "link": "http://arxiv.org/abs/2501.19182v1", "date": "2025-01-31", "relevancy": 2.1306, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5879}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5263}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comunication%20Framework%20for%20Compositional%20Generation&body=Title%3A%20A%20Comunication%20Framework%20for%20Compositional%20Generation%0AAuthor%3A%20Rafael%20Elberg%20and%20Mircea%20Petrache%20and%20Denis%20Parra%0AAbstract%3A%20%20%20Compositionality%20and%20compositional%20generalization--the%20ability%20to%20understand%0Anovel%20combinations%20of%20known%20concepts--are%20central%20characteristics%20of%20human%0Alanguage%20and%20are%20hypothesized%20to%20be%20essential%20for%20human%20cognition.%20In%20machine%0Alearning%2C%20the%20emergence%20of%20this%20property%20has%20been%20studied%20in%20a%20communication%0Agame%20setting%2C%20where%20independent%20agents%20%28a%20sender%20and%20a%20receiver%29%20converge%20to%20a%0Ashared%20encoding%20policy%20from%20a%20set%20of%20states%20to%20a%20space%20of%20discrete%20messages%2C%0Awhere%20the%20receiver%20can%20correctly%20reconstruct%20the%20states%20observed%20by%20the%20sender%0Ausing%20only%20the%20sender%27s%20messages.%20The%20use%20of%20communication%20games%20in%20generation%0Atasks%20is%20still%20largely%20unexplored%2C%20with%20recent%20methods%20for%20compositional%0Ageneration%20focusing%20mainly%20on%20the%20use%20of%20supervised%20guidance%20%28either%20through%0Aclass%20labels%20or%20text%29.%20In%20this%20work%2C%20we%20take%20the%20first%20steps%20to%20fill%20this%20gap%2C%0Aand%20we%20present%20a%20self-supervised%20generative%20communication%20game-based%20framework%0Afor%20creating%20compositional%20encodings%20in%20learned%20representations%20from%0Apre-trained%20encoder-decoder%20models.%20In%20an%20Iterated%20Learning%20%28IL%29%20protocol%0Ainvolving%20a%20sender%20and%20a%20receiver%2C%20we%20apply%20alternating%20pressures%20for%0Acompression%20and%20diversity%20of%20encoded%20discrete%20messages%2C%20so%20that%20the%20protocol%0Aconverges%20to%20an%20efficient%20but%20unambiguous%20encoding.%20Approximate%20message%20entropy%0Aregularization%20is%20used%20to%20favor%20compositional%20encodings.%20Our%20framework%20is%20based%0Aon%20rigorous%20justifications%20and%20proofs%20of%20defining%20and%20balancing%20the%20concepts%20of%0AEficiency%2C%20Unambiguity%20and%20Non-Holisticity%20in%20encoding.%20We%20test%20our%20method%20on%0Athe%20compositional%20image%20dataset%20Shapes3D%2C%20demonstrating%20robust%20performance%20in%0Aboth%20reconstruction%20and%20compositionality%20metrics%2C%20surpassing%20other%20tested%0Adiscrete%20message%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comunication%2520Framework%2520for%2520Compositional%2520Generation%26entry.906535625%3DRafael%2520Elberg%2520and%2520Mircea%2520Petrache%2520and%2520Denis%2520Parra%26entry.1292438233%3D%2520%2520Compositionality%2520and%2520compositional%2520generalization--the%2520ability%2520to%2520understand%250Anovel%2520combinations%2520of%2520known%2520concepts--are%2520central%2520characteristics%2520of%2520human%250Alanguage%2520and%2520are%2520hypothesized%2520to%2520be%2520essential%2520for%2520human%2520cognition.%2520In%2520machine%250Alearning%252C%2520the%2520emergence%2520of%2520this%2520property%2520has%2520been%2520studied%2520in%2520a%2520communication%250Agame%2520setting%252C%2520where%2520independent%2520agents%2520%2528a%2520sender%2520and%2520a%2520receiver%2529%2520converge%2520to%2520a%250Ashared%2520encoding%2520policy%2520from%2520a%2520set%2520of%2520states%2520to%2520a%2520space%2520of%2520discrete%2520messages%252C%250Awhere%2520the%2520receiver%2520can%2520correctly%2520reconstruct%2520the%2520states%2520observed%2520by%2520the%2520sender%250Ausing%2520only%2520the%2520sender%2527s%2520messages.%2520The%2520use%2520of%2520communication%2520games%2520in%2520generation%250Atasks%2520is%2520still%2520largely%2520unexplored%252C%2520with%2520recent%2520methods%2520for%2520compositional%250Ageneration%2520focusing%2520mainly%2520on%2520the%2520use%2520of%2520supervised%2520guidance%2520%2528either%2520through%250Aclass%2520labels%2520or%2520text%2529.%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520first%2520steps%2520to%2520fill%2520this%2520gap%252C%250Aand%2520we%2520present%2520a%2520self-supervised%2520generative%2520communication%2520game-based%2520framework%250Afor%2520creating%2520compositional%2520encodings%2520in%2520learned%2520representations%2520from%250Apre-trained%2520encoder-decoder%2520models.%2520In%2520an%2520Iterated%2520Learning%2520%2528IL%2529%2520protocol%250Ainvolving%2520a%2520sender%2520and%2520a%2520receiver%252C%2520we%2520apply%2520alternating%2520pressures%2520for%250Acompression%2520and%2520diversity%2520of%2520encoded%2520discrete%2520messages%252C%2520so%2520that%2520the%2520protocol%250Aconverges%2520to%2520an%2520efficient%2520but%2520unambiguous%2520encoding.%2520Approximate%2520message%2520entropy%250Aregularization%2520is%2520used%2520to%2520favor%2520compositional%2520encodings.%2520Our%2520framework%2520is%2520based%250Aon%2520rigorous%2520justifications%2520and%2520proofs%2520of%2520defining%2520and%2520balancing%2520the%2520concepts%2520of%250AEficiency%252C%2520Unambiguity%2520and%2520Non-Holisticity%2520in%2520encoding.%2520We%2520test%2520our%2520method%2520on%250Athe%2520compositional%2520image%2520dataset%2520Shapes3D%252C%2520demonstrating%2520robust%2520performance%2520in%250Aboth%2520reconstruction%2520and%2520compositionality%2520metrics%252C%2520surpassing%2520other%2520tested%250Adiscrete%2520message%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comunication%20Framework%20for%20Compositional%20Generation&entry.906535625=Rafael%20Elberg%20and%20Mircea%20Petrache%20and%20Denis%20Parra&entry.1292438233=%20%20Compositionality%20and%20compositional%20generalization--the%20ability%20to%20understand%0Anovel%20combinations%20of%20known%20concepts--are%20central%20characteristics%20of%20human%0Alanguage%20and%20are%20hypothesized%20to%20be%20essential%20for%20human%20cognition.%20In%20machine%0Alearning%2C%20the%20emergence%20of%20this%20property%20has%20been%20studied%20in%20a%20communication%0Agame%20setting%2C%20where%20independent%20agents%20%28a%20sender%20and%20a%20receiver%29%20converge%20to%20a%0Ashared%20encoding%20policy%20from%20a%20set%20of%20states%20to%20a%20space%20of%20discrete%20messages%2C%0Awhere%20the%20receiver%20can%20correctly%20reconstruct%20the%20states%20observed%20by%20the%20sender%0Ausing%20only%20the%20sender%27s%20messages.%20The%20use%20of%20communication%20games%20in%20generation%0Atasks%20is%20still%20largely%20unexplored%2C%20with%20recent%20methods%20for%20compositional%0Ageneration%20focusing%20mainly%20on%20the%20use%20of%20supervised%20guidance%20%28either%20through%0Aclass%20labels%20or%20text%29.%20In%20this%20work%2C%20we%20take%20the%20first%20steps%20to%20fill%20this%20gap%2C%0Aand%20we%20present%20a%20self-supervised%20generative%20communication%20game-based%20framework%0Afor%20creating%20compositional%20encodings%20in%20learned%20representations%20from%0Apre-trained%20encoder-decoder%20models.%20In%20an%20Iterated%20Learning%20%28IL%29%20protocol%0Ainvolving%20a%20sender%20and%20a%20receiver%2C%20we%20apply%20alternating%20pressures%20for%0Acompression%20and%20diversity%20of%20encoded%20discrete%20messages%2C%20so%20that%20the%20protocol%0Aconverges%20to%20an%20efficient%20but%20unambiguous%20encoding.%20Approximate%20message%20entropy%0Aregularization%20is%20used%20to%20favor%20compositional%20encodings.%20Our%20framework%20is%20based%0Aon%20rigorous%20justifications%20and%20proofs%20of%20defining%20and%20balancing%20the%20concepts%20of%0AEficiency%2C%20Unambiguity%20and%20Non-Holisticity%20in%20encoding.%20We%20test%20our%20method%20on%0Athe%20compositional%20image%20dataset%20Shapes3D%2C%20demonstrating%20robust%20performance%20in%0Aboth%20reconstruction%20and%20compositionality%20metrics%2C%20surpassing%20other%20tested%0Adiscrete%20message%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19182v1&entry.124074799=Read"},
{"title": "Quantum SMOTE with Angular Outliers: Redefining Minority Class Handling", "author": "Nishikanta Mohanty and Bikash K. Behera and Christopher Ferrie", "abstract": "  This paper introduces Quantum-SMOTEV2, an advanced variant of the\nQuantum-SMOTE method, leveraging quantum computing to address class imbalance\nin machine learning datasets without K-Means clustering. Quantum-SMOTEV2\nsynthesizes data samples using swap tests and quantum rotation centered around\na single data centroid, concentrating on the angular distribution of minority\ndata points and the concept of angular outliers (AOL). Experimental results\nshow significant enhancements in model performance metrics at moderate SMOTE\nlevels (30-36%), which previously required up to 50% with the original method.\nQuantum-SMOTEV2 maintains essential features of its predecessor\n(arXiv:2402.17398), such as rotation angle, minority percentage, and splitting\nfactor, allowing for tailored adaptation to specific dataset needs. The method\nis scalable, utilizing compact swap tests and low depth quantum circuits to\naccommodate a large number of features. Evaluation on the public Cell-to-Cell\nTelecom dataset with Random Forest (RF), K-Nearest Neighbours (KNN) Classifier,\nand Neural Network (NN) illustrates that integrating Angular Outliers modestly\nboosts classification metrics like accuracy, F1 Score, AUC-ROC, and AUC-PR\nacross different proportions of synthetic data, highlighting the effectiveness\nof Quantum-SMOTEV2 in enhancing model performance for edge cases.\n", "link": "http://arxiv.org/abs/2501.19001v1", "date": "2025-01-31", "relevancy": 1.8539, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5074}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20SMOTE%20with%20Angular%20Outliers%3A%20Redefining%20Minority%20Class%20Handling&body=Title%3A%20Quantum%20SMOTE%20with%20Angular%20Outliers%3A%20Redefining%20Minority%20Class%20Handling%0AAuthor%3A%20Nishikanta%20Mohanty%20and%20Bikash%20K.%20Behera%20and%20Christopher%20Ferrie%0AAbstract%3A%20%20%20This%20paper%20introduces%20Quantum-SMOTEV2%2C%20an%20advanced%20variant%20of%20the%0AQuantum-SMOTE%20method%2C%20leveraging%20quantum%20computing%20to%20address%20class%20imbalance%0Ain%20machine%20learning%20datasets%20without%20K-Means%20clustering.%20Quantum-SMOTEV2%0Asynthesizes%20data%20samples%20using%20swap%20tests%20and%20quantum%20rotation%20centered%20around%0Aa%20single%20data%20centroid%2C%20concentrating%20on%20the%20angular%20distribution%20of%20minority%0Adata%20points%20and%20the%20concept%20of%20angular%20outliers%20%28AOL%29.%20Experimental%20results%0Ashow%20significant%20enhancements%20in%20model%20performance%20metrics%20at%20moderate%20SMOTE%0Alevels%20%2830-36%25%29%2C%20which%20previously%20required%20up%20to%2050%25%20with%20the%20original%20method.%0AQuantum-SMOTEV2%20maintains%20essential%20features%20of%20its%20predecessor%0A%28arXiv%3A2402.17398%29%2C%20such%20as%20rotation%20angle%2C%20minority%20percentage%2C%20and%20splitting%0Afactor%2C%20allowing%20for%20tailored%20adaptation%20to%20specific%20dataset%20needs.%20The%20method%0Ais%20scalable%2C%20utilizing%20compact%20swap%20tests%20and%20low%20depth%20quantum%20circuits%20to%0Aaccommodate%20a%20large%20number%20of%20features.%20Evaluation%20on%20the%20public%20Cell-to-Cell%0ATelecom%20dataset%20with%20Random%20Forest%20%28RF%29%2C%20K-Nearest%20Neighbours%20%28KNN%29%20Classifier%2C%0Aand%20Neural%20Network%20%28NN%29%20illustrates%20that%20integrating%20Angular%20Outliers%20modestly%0Aboosts%20classification%20metrics%20like%20accuracy%2C%20F1%20Score%2C%20AUC-ROC%2C%20and%20AUC-PR%0Aacross%20different%20proportions%20of%20synthetic%20data%2C%20highlighting%20the%20effectiveness%0Aof%20Quantum-SMOTEV2%20in%20enhancing%20model%20performance%20for%20edge%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520SMOTE%2520with%2520Angular%2520Outliers%253A%2520Redefining%2520Minority%2520Class%2520Handling%26entry.906535625%3DNishikanta%2520Mohanty%2520and%2520Bikash%2520K.%2520Behera%2520and%2520Christopher%2520Ferrie%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Quantum-SMOTEV2%252C%2520an%2520advanced%2520variant%2520of%2520the%250AQuantum-SMOTE%2520method%252C%2520leveraging%2520quantum%2520computing%2520to%2520address%2520class%2520imbalance%250Ain%2520machine%2520learning%2520datasets%2520without%2520K-Means%2520clustering.%2520Quantum-SMOTEV2%250Asynthesizes%2520data%2520samples%2520using%2520swap%2520tests%2520and%2520quantum%2520rotation%2520centered%2520around%250Aa%2520single%2520data%2520centroid%252C%2520concentrating%2520on%2520the%2520angular%2520distribution%2520of%2520minority%250Adata%2520points%2520and%2520the%2520concept%2520of%2520angular%2520outliers%2520%2528AOL%2529.%2520Experimental%2520results%250Ashow%2520significant%2520enhancements%2520in%2520model%2520performance%2520metrics%2520at%2520moderate%2520SMOTE%250Alevels%2520%252830-36%2525%2529%252C%2520which%2520previously%2520required%2520up%2520to%252050%2525%2520with%2520the%2520original%2520method.%250AQuantum-SMOTEV2%2520maintains%2520essential%2520features%2520of%2520its%2520predecessor%250A%2528arXiv%253A2402.17398%2529%252C%2520such%2520as%2520rotation%2520angle%252C%2520minority%2520percentage%252C%2520and%2520splitting%250Afactor%252C%2520allowing%2520for%2520tailored%2520adaptation%2520to%2520specific%2520dataset%2520needs.%2520The%2520method%250Ais%2520scalable%252C%2520utilizing%2520compact%2520swap%2520tests%2520and%2520low%2520depth%2520quantum%2520circuits%2520to%250Aaccommodate%2520a%2520large%2520number%2520of%2520features.%2520Evaluation%2520on%2520the%2520public%2520Cell-to-Cell%250ATelecom%2520dataset%2520with%2520Random%2520Forest%2520%2528RF%2529%252C%2520K-Nearest%2520Neighbours%2520%2528KNN%2529%2520Classifier%252C%250Aand%2520Neural%2520Network%2520%2528NN%2529%2520illustrates%2520that%2520integrating%2520Angular%2520Outliers%2520modestly%250Aboosts%2520classification%2520metrics%2520like%2520accuracy%252C%2520F1%2520Score%252C%2520AUC-ROC%252C%2520and%2520AUC-PR%250Aacross%2520different%2520proportions%2520of%2520synthetic%2520data%252C%2520highlighting%2520the%2520effectiveness%250Aof%2520Quantum-SMOTEV2%2520in%2520enhancing%2520model%2520performance%2520for%2520edge%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20SMOTE%20with%20Angular%20Outliers%3A%20Redefining%20Minority%20Class%20Handling&entry.906535625=Nishikanta%20Mohanty%20and%20Bikash%20K.%20Behera%20and%20Christopher%20Ferrie&entry.1292438233=%20%20This%20paper%20introduces%20Quantum-SMOTEV2%2C%20an%20advanced%20variant%20of%20the%0AQuantum-SMOTE%20method%2C%20leveraging%20quantum%20computing%20to%20address%20class%20imbalance%0Ain%20machine%20learning%20datasets%20without%20K-Means%20clustering.%20Quantum-SMOTEV2%0Asynthesizes%20data%20samples%20using%20swap%20tests%20and%20quantum%20rotation%20centered%20around%0Aa%20single%20data%20centroid%2C%20concentrating%20on%20the%20angular%20distribution%20of%20minority%0Adata%20points%20and%20the%20concept%20of%20angular%20outliers%20%28AOL%29.%20Experimental%20results%0Ashow%20significant%20enhancements%20in%20model%20performance%20metrics%20at%20moderate%20SMOTE%0Alevels%20%2830-36%25%29%2C%20which%20previously%20required%20up%20to%2050%25%20with%20the%20original%20method.%0AQuantum-SMOTEV2%20maintains%20essential%20features%20of%20its%20predecessor%0A%28arXiv%3A2402.17398%29%2C%20such%20as%20rotation%20angle%2C%20minority%20percentage%2C%20and%20splitting%0Afactor%2C%20allowing%20for%20tailored%20adaptation%20to%20specific%20dataset%20needs.%20The%20method%0Ais%20scalable%2C%20utilizing%20compact%20swap%20tests%20and%20low%20depth%20quantum%20circuits%20to%0Aaccommodate%20a%20large%20number%20of%20features.%20Evaluation%20on%20the%20public%20Cell-to-Cell%0ATelecom%20dataset%20with%20Random%20Forest%20%28RF%29%2C%20K-Nearest%20Neighbours%20%28KNN%29%20Classifier%2C%0Aand%20Neural%20Network%20%28NN%29%20illustrates%20that%20integrating%20Angular%20Outliers%20modestly%0Aboosts%20classification%20metrics%20like%20accuracy%2C%20F1%20Score%2C%20AUC-ROC%2C%20and%20AUC-PR%0Aacross%20different%20proportions%20of%20synthetic%20data%2C%20highlighting%20the%20effectiveness%0Aof%20Quantum-SMOTEV2%20in%20enhancing%20model%20performance%20for%20edge%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19001v1&entry.124074799=Read"},
{"title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking", "author": "Yuchun Miao and Sen Zhang and Liang Ding and Yuqi Zhang and Lefei Zhang and Dacheng Tao", "abstract": "  This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.\n", "link": "http://arxiv.org/abs/2501.19358v1", "date": "2025-01-31", "relevancy": 1.3605, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Energy%20Loss%20Phenomenon%20in%20RLHF%3A%20A%20New%20Perspective%20on%20Mitigating%0A%20%20Reward%20Hacking&body=Title%3A%20The%20Energy%20Loss%20Phenomenon%20in%20RLHF%3A%20A%20New%20Perspective%20on%20Mitigating%0A%20%20Reward%20Hacking%0AAuthor%3A%20Yuchun%20Miao%20and%20Sen%20Zhang%20and%20Liang%20Ding%20and%20Yuqi%20Zhang%20and%20Lefei%20Zhang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20This%20work%20identifies%20the%20Energy%20Loss%20Phenomenon%20in%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20its%20connection%20to%20reward%20hacking.%20Specifically%2C%0Aenergy%20loss%20in%20the%20final%20layer%20of%20a%20Large%20Language%20Model%20%28LLM%29%20gradually%0Aincreases%20during%20the%20RL%20process%2C%20with%20an%20excessive%20increase%20in%20energy%20loss%0Acharacterizing%20reward%20hacking.%20Beyond%20empirical%20analysis%2C%20we%20further%20provide%20a%0Atheoretical%20foundation%20by%20proving%20that%2C%20under%20mild%20conditions%2C%20the%20increased%0Aenergy%20loss%20reduces%20the%20upper%20bound%20of%20contextual%20relevance%20in%20LLMs%2C%20which%20is%20a%0Acritical%20aspect%20of%20reward%20hacking%20as%20the%20reduced%20contextual%20relevance%20typically%0Aindicates%20overfitting%20to%20reward%20model-favored%20patterns%20in%20RL.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20Energy%20loss-aware%20PPO%20algorithm%20%28EPPO%29%20which%20penalizes%20the%0Aincrease%20in%20energy%20loss%20in%20the%20LLM%27s%20final%20layer%20during%20reward%20calculation%20to%0Aprevent%20excessive%20energy%20loss%2C%20thereby%20mitigating%20reward%20hacking.%20We%0Atheoretically%20show%20that%20EPPO%20can%20be%20conceptually%20interpreted%20as%20an%0Aentropy-regularized%20RL%20algorithm%2C%20which%20provides%20deeper%20insights%20into%20its%0Aeffectiveness.%20Extensive%20experiments%20across%20various%20LLMs%20and%20tasks%20demonstrate%0Athe%20commonality%20of%20the%20energy%20loss%20phenomenon%2C%20as%20well%20as%20the%20effectiveness%20of%0A%5Ctexttt%7BEPPO%7D%20in%20mitigating%20reward%20hacking%20and%20improving%20RLHF%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Energy%2520Loss%2520Phenomenon%2520in%2520RLHF%253A%2520A%2520New%2520Perspective%2520on%2520Mitigating%250A%2520%2520Reward%2520Hacking%26entry.906535625%3DYuchun%2520Miao%2520and%2520Sen%2520Zhang%2520and%2520Liang%2520Ding%2520and%2520Yuqi%2520Zhang%2520and%2520Lefei%2520Zhang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520This%2520work%2520identifies%2520the%2520Energy%2520Loss%2520Phenomenon%2520in%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520its%2520connection%2520to%2520reward%2520hacking.%2520Specifically%252C%250Aenergy%2520loss%2520in%2520the%2520final%2520layer%2520of%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520gradually%250Aincreases%2520during%2520the%2520RL%2520process%252C%2520with%2520an%2520excessive%2520increase%2520in%2520energy%2520loss%250Acharacterizing%2520reward%2520hacking.%2520Beyond%2520empirical%2520analysis%252C%2520we%2520further%2520provide%2520a%250Atheoretical%2520foundation%2520by%2520proving%2520that%252C%2520under%2520mild%2520conditions%252C%2520the%2520increased%250Aenergy%2520loss%2520reduces%2520the%2520upper%2520bound%2520of%2520contextual%2520relevance%2520in%2520LLMs%252C%2520which%2520is%2520a%250Acritical%2520aspect%2520of%2520reward%2520hacking%2520as%2520the%2520reduced%2520contextual%2520relevance%2520typically%250Aindicates%2520overfitting%2520to%2520reward%2520model-favored%2520patterns%2520in%2520RL.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520an%2520Energy%2520loss-aware%2520PPO%2520algorithm%2520%2528EPPO%2529%2520which%2520penalizes%2520the%250Aincrease%2520in%2520energy%2520loss%2520in%2520the%2520LLM%2527s%2520final%2520layer%2520during%2520reward%2520calculation%2520to%250Aprevent%2520excessive%2520energy%2520loss%252C%2520thereby%2520mitigating%2520reward%2520hacking.%2520We%250Atheoretically%2520show%2520that%2520EPPO%2520can%2520be%2520conceptually%2520interpreted%2520as%2520an%250Aentropy-regularized%2520RL%2520algorithm%252C%2520which%2520provides%2520deeper%2520insights%2520into%2520its%250Aeffectiveness.%2520Extensive%2520experiments%2520across%2520various%2520LLMs%2520and%2520tasks%2520demonstrate%250Athe%2520commonality%2520of%2520the%2520energy%2520loss%2520phenomenon%252C%2520as%2520well%2520as%2520the%2520effectiveness%2520of%250A%255Ctexttt%257BEPPO%257D%2520in%2520mitigating%2520reward%2520hacking%2520and%2520improving%2520RLHF%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Energy%20Loss%20Phenomenon%20in%20RLHF%3A%20A%20New%20Perspective%20on%20Mitigating%0A%20%20Reward%20Hacking&entry.906535625=Yuchun%20Miao%20and%20Sen%20Zhang%20and%20Liang%20Ding%20and%20Yuqi%20Zhang%20and%20Lefei%20Zhang%20and%20Dacheng%20Tao&entry.1292438233=%20%20This%20work%20identifies%20the%20Energy%20Loss%20Phenomenon%20in%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20its%20connection%20to%20reward%20hacking.%20Specifically%2C%0Aenergy%20loss%20in%20the%20final%20layer%20of%20a%20Large%20Language%20Model%20%28LLM%29%20gradually%0Aincreases%20during%20the%20RL%20process%2C%20with%20an%20excessive%20increase%20in%20energy%20loss%0Acharacterizing%20reward%20hacking.%20Beyond%20empirical%20analysis%2C%20we%20further%20provide%20a%0Atheoretical%20foundation%20by%20proving%20that%2C%20under%20mild%20conditions%2C%20the%20increased%0Aenergy%20loss%20reduces%20the%20upper%20bound%20of%20contextual%20relevance%20in%20LLMs%2C%20which%20is%20a%0Acritical%20aspect%20of%20reward%20hacking%20as%20the%20reduced%20contextual%20relevance%20typically%0Aindicates%20overfitting%20to%20reward%20model-favored%20patterns%20in%20RL.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20Energy%20loss-aware%20PPO%20algorithm%20%28EPPO%29%20which%20penalizes%20the%0Aincrease%20in%20energy%20loss%20in%20the%20LLM%27s%20final%20layer%20during%20reward%20calculation%20to%0Aprevent%20excessive%20energy%20loss%2C%20thereby%20mitigating%20reward%20hacking.%20We%0Atheoretically%20show%20that%20EPPO%20can%20be%20conceptually%20interpreted%20as%20an%0Aentropy-regularized%20RL%20algorithm%2C%20which%20provides%20deeper%20insights%20into%20its%0Aeffectiveness.%20Extensive%20experiments%20across%20various%20LLMs%20and%20tasks%20demonstrate%0Athe%20commonality%20of%20the%20energy%20loss%20phenomenon%2C%20as%20well%20as%20the%20effectiveness%20of%0A%5Ctexttt%7BEPPO%7D%20in%20mitigating%20reward%20hacking%20and%20improving%20RLHF%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19358v1&entry.124074799=Read"},
{"title": "Average Certified Radius is a Poor Metric for Randomized Smoothing", "author": "Chenhao Sun and Yuhao Mao and Mark Niklas M\u00fcller and Martin Vechev", "abstract": "  Randomized smoothing is a popular approach for providing certified robustness\nguarantees against adversarial attacks, and has become an active area of\nresearch. Over the past years, the average certified radius (ACR) has emerged\nas the most important metric for comparing methods and tracking progress in the\nfield. However, in this work, for the first time we show that ACR is a poor\nmetric for evaluating robustness guarantees provided by randomized smoothing.\nWe theoretically prove not only that a trivial classifier can have arbitrarily\nlarge ACR, but also that ACR is much more sensitive to improvements on easy\nsamples than on hard ones. Empirically, we confirm that existing training\nstrategies, though improving ACR with different approaches, reduce the model's\nrobustness on hard samples consistently. To strengthen our conclusion, we\npropose strategies, including explicitly discarding hard samples, reweighting\nthe dataset with approximate certified radius, and extreme optimization for\neasy samples, to achieve state-of-the-art ACR, without training for robustness\non the full data distribution. Overall, our results suggest that ACR has\nintroduced a strong undesired bias to the field, and its application should be\ndiscontinued when evaluating randomized smoothing.\n", "link": "http://arxiv.org/abs/2410.06895v2", "date": "2025-01-31", "relevancy": 2.1109, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Average%20Certified%20Radius%20is%20a%20Poor%20Metric%20for%20Randomized%20Smoothing&body=Title%3A%20Average%20Certified%20Radius%20is%20a%20Poor%20Metric%20for%20Randomized%20Smoothing%0AAuthor%3A%20Chenhao%20Sun%20and%20Yuhao%20Mao%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Randomized%20smoothing%20is%20a%20popular%20approach%20for%20providing%20certified%20robustness%0Aguarantees%20against%20adversarial%20attacks%2C%20and%20has%20become%20an%20active%20area%20of%0Aresearch.%20Over%20the%20past%20years%2C%20the%20average%20certified%20radius%20%28ACR%29%20has%20emerged%0Aas%20the%20most%20important%20metric%20for%20comparing%20methods%20and%20tracking%20progress%20in%20the%0Afield.%20However%2C%20in%20this%20work%2C%20for%20the%20first%20time%20we%20show%20that%20ACR%20is%20a%20poor%0Ametric%20for%20evaluating%20robustness%20guarantees%20provided%20by%20randomized%20smoothing.%0AWe%20theoretically%20prove%20not%20only%20that%20a%20trivial%20classifier%20can%20have%20arbitrarily%0Alarge%20ACR%2C%20but%20also%20that%20ACR%20is%20much%20more%20sensitive%20to%20improvements%20on%20easy%0Asamples%20than%20on%20hard%20ones.%20Empirically%2C%20we%20confirm%20that%20existing%20training%0Astrategies%2C%20though%20improving%20ACR%20with%20different%20approaches%2C%20reduce%20the%20model%27s%0Arobustness%20on%20hard%20samples%20consistently.%20To%20strengthen%20our%20conclusion%2C%20we%0Apropose%20strategies%2C%20including%20explicitly%20discarding%20hard%20samples%2C%20reweighting%0Athe%20dataset%20with%20approximate%20certified%20radius%2C%20and%20extreme%20optimization%20for%0Aeasy%20samples%2C%20to%20achieve%20state-of-the-art%20ACR%2C%20without%20training%20for%20robustness%0Aon%20the%20full%20data%20distribution.%20Overall%2C%20our%20results%20suggest%20that%20ACR%20has%0Aintroduced%20a%20strong%20undesired%20bias%20to%20the%20field%2C%20and%20its%20application%20should%20be%0Adiscontinued%20when%20evaluating%20randomized%20smoothing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06895v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAverage%2520Certified%2520Radius%2520is%2520a%2520Poor%2520Metric%2520for%2520Randomized%2520Smoothing%26entry.906535625%3DChenhao%2520Sun%2520and%2520Yuhao%2520Mao%2520and%2520Mark%2520Niklas%2520M%25C3%25BCller%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Randomized%2520smoothing%2520is%2520a%2520popular%2520approach%2520for%2520providing%2520certified%2520robustness%250Aguarantees%2520against%2520adversarial%2520attacks%252C%2520and%2520has%2520become%2520an%2520active%2520area%2520of%250Aresearch.%2520Over%2520the%2520past%2520years%252C%2520the%2520average%2520certified%2520radius%2520%2528ACR%2529%2520has%2520emerged%250Aas%2520the%2520most%2520important%2520metric%2520for%2520comparing%2520methods%2520and%2520tracking%2520progress%2520in%2520the%250Afield.%2520However%252C%2520in%2520this%2520work%252C%2520for%2520the%2520first%2520time%2520we%2520show%2520that%2520ACR%2520is%2520a%2520poor%250Ametric%2520for%2520evaluating%2520robustness%2520guarantees%2520provided%2520by%2520randomized%2520smoothing.%250AWe%2520theoretically%2520prove%2520not%2520only%2520that%2520a%2520trivial%2520classifier%2520can%2520have%2520arbitrarily%250Alarge%2520ACR%252C%2520but%2520also%2520that%2520ACR%2520is%2520much%2520more%2520sensitive%2520to%2520improvements%2520on%2520easy%250Asamples%2520than%2520on%2520hard%2520ones.%2520Empirically%252C%2520we%2520confirm%2520that%2520existing%2520training%250Astrategies%252C%2520though%2520improving%2520ACR%2520with%2520different%2520approaches%252C%2520reduce%2520the%2520model%2527s%250Arobustness%2520on%2520hard%2520samples%2520consistently.%2520To%2520strengthen%2520our%2520conclusion%252C%2520we%250Apropose%2520strategies%252C%2520including%2520explicitly%2520discarding%2520hard%2520samples%252C%2520reweighting%250Athe%2520dataset%2520with%2520approximate%2520certified%2520radius%252C%2520and%2520extreme%2520optimization%2520for%250Aeasy%2520samples%252C%2520to%2520achieve%2520state-of-the-art%2520ACR%252C%2520without%2520training%2520for%2520robustness%250Aon%2520the%2520full%2520data%2520distribution.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520ACR%2520has%250Aintroduced%2520a%2520strong%2520undesired%2520bias%2520to%2520the%2520field%252C%2520and%2520its%2520application%2520should%2520be%250Adiscontinued%2520when%2520evaluating%2520randomized%2520smoothing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06895v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Average%20Certified%20Radius%20is%20a%20Poor%20Metric%20for%20Randomized%20Smoothing&entry.906535625=Chenhao%20Sun%20and%20Yuhao%20Mao%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev&entry.1292438233=%20%20Randomized%20smoothing%20is%20a%20popular%20approach%20for%20providing%20certified%20robustness%0Aguarantees%20against%20adversarial%20attacks%2C%20and%20has%20become%20an%20active%20area%20of%0Aresearch.%20Over%20the%20past%20years%2C%20the%20average%20certified%20radius%20%28ACR%29%20has%20emerged%0Aas%20the%20most%20important%20metric%20for%20comparing%20methods%20and%20tracking%20progress%20in%20the%0Afield.%20However%2C%20in%20this%20work%2C%20for%20the%20first%20time%20we%20show%20that%20ACR%20is%20a%20poor%0Ametric%20for%20evaluating%20robustness%20guarantees%20provided%20by%20randomized%20smoothing.%0AWe%20theoretically%20prove%20not%20only%20that%20a%20trivial%20classifier%20can%20have%20arbitrarily%0Alarge%20ACR%2C%20but%20also%20that%20ACR%20is%20much%20more%20sensitive%20to%20improvements%20on%20easy%0Asamples%20than%20on%20hard%20ones.%20Empirically%2C%20we%20confirm%20that%20existing%20training%0Astrategies%2C%20though%20improving%20ACR%20with%20different%20approaches%2C%20reduce%20the%20model%27s%0Arobustness%20on%20hard%20samples%20consistently.%20To%20strengthen%20our%20conclusion%2C%20we%0Apropose%20strategies%2C%20including%20explicitly%20discarding%20hard%20samples%2C%20reweighting%0Athe%20dataset%20with%20approximate%20certified%20radius%2C%20and%20extreme%20optimization%20for%0Aeasy%20samples%2C%20to%20achieve%20state-of-the-art%20ACR%2C%20without%20training%20for%20robustness%0Aon%20the%20full%20data%20distribution.%20Overall%2C%20our%20results%20suggest%20that%20ACR%20has%0Aintroduced%20a%20strong%20undesired%20bias%20to%20the%20field%2C%20and%20its%20application%20should%20be%0Adiscontinued%20when%20evaluating%20randomized%20smoothing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06895v2&entry.124074799=Read"},
{"title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models", "author": "Messi H. J. Lee and Soyeon Jeon", "abstract": "  Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.\n", "link": "http://arxiv.org/abs/2501.19337v1", "date": "2025-01-31", "relevancy": 1.9178, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5062}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Homogeneity%20Bias%20as%20Differential%20Sampling%20Uncertainty%20in%20Language%20Models&body=Title%3A%20Homogeneity%20Bias%20as%20Differential%20Sampling%20Uncertainty%20in%20Language%20Models%0AAuthor%3A%20Messi%20H.%20J.%20Lee%20and%20Soyeon%20Jeon%0AAbstract%3A%20%20%20Prior%20research%20show%20that%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%0AModels%20%28VLMs%29%20represent%20marginalized%20groups%20more%20homogeneously%20than%20dominant%0Agroups.%20However%2C%20the%20mechanisms%20underlying%20this%20homogeneity%20bias%20remain%0Arelatively%20unexplored.%20We%20propose%20that%20this%20bias%20emerges%20from%20systematic%0Adifferences%20in%20the%20probability%20distributions%20from%20which%20tokens%20are%20sampled%20at%0Ainference-time.%20Analyzing%20three%20measures%20of%20uncertainty%20in%20token%20sampling%0Adistributions-entropy%2C%20perplexity%2C%20and%20probability%20of%20differentiation-we%20find%0Athat%20in%20some%20models%2C%20specifically%20GPT-4%20Turbo%20and%20Llama-3.2%2C%20tokens%20are%20sampled%0Amore%20deterministically%20when%20generating%20texts%20about%20marginalized%20groups%20%28i.e.%2C%0ABlack%20Americans%20and%20women%29%20compared%20to%20their%20dominant%20group%20counterparts%20%28i.e.%2C%0AWhite%20Americans%20and%20men%29.%20While%20these%20findings%20may%20help%20explain%20homogeneity%0Abias%20in%20certain%20models%2C%20the%20patterns%20did%20not%20replicate%20across%20all%20VLMs%20tested%2C%0Asuggesting%20multiple%20mechanisms%20may%20contribute%20to%20homogeneity%20bias%20in%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHomogeneity%2520Bias%2520as%2520Differential%2520Sampling%2520Uncertainty%2520in%2520Language%2520Models%26entry.906535625%3DMessi%2520H.%2520J.%2520Lee%2520and%2520Soyeon%2520Jeon%26entry.1292438233%3D%2520%2520Prior%2520research%2520show%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520represent%2520marginalized%2520groups%2520more%2520homogeneously%2520than%2520dominant%250Agroups.%2520However%252C%2520the%2520mechanisms%2520underlying%2520this%2520homogeneity%2520bias%2520remain%250Arelatively%2520unexplored.%2520We%2520propose%2520that%2520this%2520bias%2520emerges%2520from%2520systematic%250Adifferences%2520in%2520the%2520probability%2520distributions%2520from%2520which%2520tokens%2520are%2520sampled%2520at%250Ainference-time.%2520Analyzing%2520three%2520measures%2520of%2520uncertainty%2520in%2520token%2520sampling%250Adistributions-entropy%252C%2520perplexity%252C%2520and%2520probability%2520of%2520differentiation-we%2520find%250Athat%2520in%2520some%2520models%252C%2520specifically%2520GPT-4%2520Turbo%2520and%2520Llama-3.2%252C%2520tokens%2520are%2520sampled%250Amore%2520deterministically%2520when%2520generating%2520texts%2520about%2520marginalized%2520groups%2520%2528i.e.%252C%250ABlack%2520Americans%2520and%2520women%2529%2520compared%2520to%2520their%2520dominant%2520group%2520counterparts%2520%2528i.e.%252C%250AWhite%2520Americans%2520and%2520men%2529.%2520While%2520these%2520findings%2520may%2520help%2520explain%2520homogeneity%250Abias%2520in%2520certain%2520models%252C%2520the%2520patterns%2520did%2520not%2520replicate%2520across%2520all%2520VLMs%2520tested%252C%250Asuggesting%2520multiple%2520mechanisms%2520may%2520contribute%2520to%2520homogeneity%2520bias%2520in%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homogeneity%20Bias%20as%20Differential%20Sampling%20Uncertainty%20in%20Language%20Models&entry.906535625=Messi%20H.%20J.%20Lee%20and%20Soyeon%20Jeon&entry.1292438233=%20%20Prior%20research%20show%20that%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%0AModels%20%28VLMs%29%20represent%20marginalized%20groups%20more%20homogeneously%20than%20dominant%0Agroups.%20However%2C%20the%20mechanisms%20underlying%20this%20homogeneity%20bias%20remain%0Arelatively%20unexplored.%20We%20propose%20that%20this%20bias%20emerges%20from%20systematic%0Adifferences%20in%20the%20probability%20distributions%20from%20which%20tokens%20are%20sampled%20at%0Ainference-time.%20Analyzing%20three%20measures%20of%20uncertainty%20in%20token%20sampling%0Adistributions-entropy%2C%20perplexity%2C%20and%20probability%20of%20differentiation-we%20find%0Athat%20in%20some%20models%2C%20specifically%20GPT-4%20Turbo%20and%20Llama-3.2%2C%20tokens%20are%20sampled%0Amore%20deterministically%20when%20generating%20texts%20about%20marginalized%20groups%20%28i.e.%2C%0ABlack%20Americans%20and%20women%29%20compared%20to%20their%20dominant%20group%20counterparts%20%28i.e.%2C%0AWhite%20Americans%20and%20men%29.%20While%20these%20findings%20may%20help%20explain%20homogeneity%0Abias%20in%20certain%20models%2C%20the%20patterns%20did%20not%20replicate%20across%20all%20VLMs%20tested%2C%0Asuggesting%20multiple%20mechanisms%20may%20contribute%20to%20homogeneity%20bias%20in%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19337v1&entry.124074799=Read"},
{"title": "FaceQSORT: a Multi-Face Tracking Method based on Biometric and\n  Appearance Features", "author": "Robert J\u00f6chl and Andreas Uhl", "abstract": "  Tracking multiple faces is a difficult problem, as there may be partially\noccluded or lateral faces. In multiple face tracking, association is typically\nbased on (biometric) face features. However, the models used to extract these\nface features usually require frontal face images, which can limit the tracking\nperformance. In this work, a multi-face tracking method inspired by StrongSort,\nFaceQSORT, is proposed. To mitigate the problem of partially occluded or\nlateral faces, biometric face features are combined with visual appearance\nfeatures (i.e., generated by a generic object classifier), with both features\nare extracted from the same face patch. A comprehensive experimental evaluation\nis performed, including a comparison of different face descriptors, an\nevaluation of different parameter settings, and the application of a different\nsimilarity metric. All experiments are conducted with a new multi-face tracking\ndataset and a subset of the ChokePoint dataset. The `Paris Lodron University\nSalzburg Faces in a Queue' dataset consists of a total of seven fully annotated\nsequences (12730 frames) and is made publicly available as part of this work.\nTogether with this dataset, annotations of 6 sequences from the ChokePoint\ndataset are also provided.\n", "link": "http://arxiv.org/abs/2501.11741v2", "date": "2025-01-31", "relevancy": 1.9363, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceQSORT%3A%20a%20Multi-Face%20Tracking%20Method%20based%20on%20Biometric%20and%0A%20%20Appearance%20Features&body=Title%3A%20FaceQSORT%3A%20a%20Multi-Face%20Tracking%20Method%20based%20on%20Biometric%20and%0A%20%20Appearance%20Features%0AAuthor%3A%20Robert%20J%C3%B6chl%20and%20Andreas%20Uhl%0AAbstract%3A%20%20%20Tracking%20multiple%20faces%20is%20a%20difficult%20problem%2C%20as%20there%20may%20be%20partially%0Aoccluded%20or%20lateral%20faces.%20In%20multiple%20face%20tracking%2C%20association%20is%20typically%0Abased%20on%20%28biometric%29%20face%20features.%20However%2C%20the%20models%20used%20to%20extract%20these%0Aface%20features%20usually%20require%20frontal%20face%20images%2C%20which%20can%20limit%20the%20tracking%0Aperformance.%20In%20this%20work%2C%20a%20multi-face%20tracking%20method%20inspired%20by%20StrongSort%2C%0AFaceQSORT%2C%20is%20proposed.%20To%20mitigate%20the%20problem%20of%20partially%20occluded%20or%0Alateral%20faces%2C%20biometric%20face%20features%20are%20combined%20with%20visual%20appearance%0Afeatures%20%28i.e.%2C%20generated%20by%20a%20generic%20object%20classifier%29%2C%20with%20both%20features%0Aare%20extracted%20from%20the%20same%20face%20patch.%20A%20comprehensive%20experimental%20evaluation%0Ais%20performed%2C%20including%20a%20comparison%20of%20different%20face%20descriptors%2C%20an%0Aevaluation%20of%20different%20parameter%20settings%2C%20and%20the%20application%20of%20a%20different%0Asimilarity%20metric.%20All%20experiments%20are%20conducted%20with%20a%20new%20multi-face%20tracking%0Adataset%20and%20a%20subset%20of%20the%20ChokePoint%20dataset.%20The%20%60Paris%20Lodron%20University%0ASalzburg%20Faces%20in%20a%20Queue%27%20dataset%20consists%20of%20a%20total%20of%20seven%20fully%20annotated%0Asequences%20%2812730%20frames%29%20and%20is%20made%20publicly%20available%20as%20part%20of%20this%20work.%0ATogether%20with%20this%20dataset%2C%20annotations%20of%206%20sequences%20from%20the%20ChokePoint%0Adataset%20are%20also%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceQSORT%253A%2520a%2520Multi-Face%2520Tracking%2520Method%2520based%2520on%2520Biometric%2520and%250A%2520%2520Appearance%2520Features%26entry.906535625%3DRobert%2520J%25C3%25B6chl%2520and%2520Andreas%2520Uhl%26entry.1292438233%3D%2520%2520Tracking%2520multiple%2520faces%2520is%2520a%2520difficult%2520problem%252C%2520as%2520there%2520may%2520be%2520partially%250Aoccluded%2520or%2520lateral%2520faces.%2520In%2520multiple%2520face%2520tracking%252C%2520association%2520is%2520typically%250Abased%2520on%2520%2528biometric%2529%2520face%2520features.%2520However%252C%2520the%2520models%2520used%2520to%2520extract%2520these%250Aface%2520features%2520usually%2520require%2520frontal%2520face%2520images%252C%2520which%2520can%2520limit%2520the%2520tracking%250Aperformance.%2520In%2520this%2520work%252C%2520a%2520multi-face%2520tracking%2520method%2520inspired%2520by%2520StrongSort%252C%250AFaceQSORT%252C%2520is%2520proposed.%2520To%2520mitigate%2520the%2520problem%2520of%2520partially%2520occluded%2520or%250Alateral%2520faces%252C%2520biometric%2520face%2520features%2520are%2520combined%2520with%2520visual%2520appearance%250Afeatures%2520%2528i.e.%252C%2520generated%2520by%2520a%2520generic%2520object%2520classifier%2529%252C%2520with%2520both%2520features%250Aare%2520extracted%2520from%2520the%2520same%2520face%2520patch.%2520A%2520comprehensive%2520experimental%2520evaluation%250Ais%2520performed%252C%2520including%2520a%2520comparison%2520of%2520different%2520face%2520descriptors%252C%2520an%250Aevaluation%2520of%2520different%2520parameter%2520settings%252C%2520and%2520the%2520application%2520of%2520a%2520different%250Asimilarity%2520metric.%2520All%2520experiments%2520are%2520conducted%2520with%2520a%2520new%2520multi-face%2520tracking%250Adataset%2520and%2520a%2520subset%2520of%2520the%2520ChokePoint%2520dataset.%2520The%2520%2560Paris%2520Lodron%2520University%250ASalzburg%2520Faces%2520in%2520a%2520Queue%2527%2520dataset%2520consists%2520of%2520a%2520total%2520of%2520seven%2520fully%2520annotated%250Asequences%2520%252812730%2520frames%2529%2520and%2520is%2520made%2520publicly%2520available%2520as%2520part%2520of%2520this%2520work.%250ATogether%2520with%2520this%2520dataset%252C%2520annotations%2520of%25206%2520sequences%2520from%2520the%2520ChokePoint%250Adataset%2520are%2520also%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceQSORT%3A%20a%20Multi-Face%20Tracking%20Method%20based%20on%20Biometric%20and%0A%20%20Appearance%20Features&entry.906535625=Robert%20J%C3%B6chl%20and%20Andreas%20Uhl&entry.1292438233=%20%20Tracking%20multiple%20faces%20is%20a%20difficult%20problem%2C%20as%20there%20may%20be%20partially%0Aoccluded%20or%20lateral%20faces.%20In%20multiple%20face%20tracking%2C%20association%20is%20typically%0Abased%20on%20%28biometric%29%20face%20features.%20However%2C%20the%20models%20used%20to%20extract%20these%0Aface%20features%20usually%20require%20frontal%20face%20images%2C%20which%20can%20limit%20the%20tracking%0Aperformance.%20In%20this%20work%2C%20a%20multi-face%20tracking%20method%20inspired%20by%20StrongSort%2C%0AFaceQSORT%2C%20is%20proposed.%20To%20mitigate%20the%20problem%20of%20partially%20occluded%20or%0Alateral%20faces%2C%20biometric%20face%20features%20are%20combined%20with%20visual%20appearance%0Afeatures%20%28i.e.%2C%20generated%20by%20a%20generic%20object%20classifier%29%2C%20with%20both%20features%0Aare%20extracted%20from%20the%20same%20face%20patch.%20A%20comprehensive%20experimental%20evaluation%0Ais%20performed%2C%20including%20a%20comparison%20of%20different%20face%20descriptors%2C%20an%0Aevaluation%20of%20different%20parameter%20settings%2C%20and%20the%20application%20of%20a%20different%0Asimilarity%20metric.%20All%20experiments%20are%20conducted%20with%20a%20new%20multi-face%20tracking%0Adataset%20and%20a%20subset%20of%20the%20ChokePoint%20dataset.%20The%20%60Paris%20Lodron%20University%0ASalzburg%20Faces%20in%20a%20Queue%27%20dataset%20consists%20of%20a%20total%20of%20seven%20fully%20annotated%0Asequences%20%2812730%20frames%29%20and%20is%20made%20publicly%20available%20as%20part%20of%20this%20work.%0ATogether%20with%20this%20dataset%2C%20annotations%20of%206%20sequences%20from%20the%20ChokePoint%0Adataset%20are%20also%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11741v2&entry.124074799=Read"},
{"title": "Secured Communication Schemes for UAVs in 5G: CRYSTALS-Kyber and IDS", "author": "Taneya Sharma and Seyed Ahmad Soleymani and Mohammad Shojafar and Rahim Tafazolli", "abstract": "  This paper introduces a secure communication architecture for Unmanned Aerial\nVehicles (UAVs) and ground stations in 5G networks, addressing critical\nchallenges in network security. The proposed solution integrates the Advanced\nEncryption Standard (AES) with Elliptic Curve Cryptography (ECC) and\nCRYSTALS-Kyber for key encapsulation, offering a hybrid cryptographic approach.\nBy incorporating CRYSTALS-Kyber, the framework mitigates vulnerabilities in ECC\nagainst quantum attacks, positioning it as a quantum-resistant alternative. The\narchitecture is based on a server-client model, with UAVs functioning as\nclients and the ground station acting as the server. The system was rigorously\nevaluated in both VPN and 5G environments. Experimental results confirm that\nCRYSTALS-Kyber delivers strong protection against quantum threats with minimal\nperformance overhead, making it highly suitable for UAVs with resource\nconstraints. Moreover, the proposed architecture integrates an Artificial\nIntelligence (AI)-based Intrusion Detection System (IDS) to further enhance\nsecurity. In performance evaluations, the IDS demonstrated strong results\nacross multiple models with XGBoost, particularly in more demanding scenarios,\noutperforming other models with an accuracy of 97.33% and an AUC of 0.94. These\nfindings underscore the potential of combining quantum-resistant encryption\nmechanisms with AI-driven IDS to create a robust, scalable, and secure\ncommunication framework for UAV networks, particularly within the\nhigh-performance requirements of 5G environments.\n", "link": "http://arxiv.org/abs/2501.19191v1", "date": "2025-01-31", "relevancy": 1.4554, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3741}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.363}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secured%20Communication%20Schemes%20for%20UAVs%20in%205G%3A%20CRYSTALS-Kyber%20and%20IDS&body=Title%3A%20Secured%20Communication%20Schemes%20for%20UAVs%20in%205G%3A%20CRYSTALS-Kyber%20and%20IDS%0AAuthor%3A%20Taneya%20Sharma%20and%20Seyed%20Ahmad%20Soleymani%20and%20Mohammad%20Shojafar%20and%20Rahim%20Tafazolli%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20secure%20communication%20architecture%20for%20Unmanned%20Aerial%0AVehicles%20%28UAVs%29%20and%20ground%20stations%20in%205G%20networks%2C%20addressing%20critical%0Achallenges%20in%20network%20security.%20The%20proposed%20solution%20integrates%20the%20Advanced%0AEncryption%20Standard%20%28AES%29%20with%20Elliptic%20Curve%20Cryptography%20%28ECC%29%20and%0ACRYSTALS-Kyber%20for%20key%20encapsulation%2C%20offering%20a%20hybrid%20cryptographic%20approach.%0ABy%20incorporating%20CRYSTALS-Kyber%2C%20the%20framework%20mitigates%20vulnerabilities%20in%20ECC%0Aagainst%20quantum%20attacks%2C%20positioning%20it%20as%20a%20quantum-resistant%20alternative.%20The%0Aarchitecture%20is%20based%20on%20a%20server-client%20model%2C%20with%20UAVs%20functioning%20as%0Aclients%20and%20the%20ground%20station%20acting%20as%20the%20server.%20The%20system%20was%20rigorously%0Aevaluated%20in%20both%20VPN%20and%205G%20environments.%20Experimental%20results%20confirm%20that%0ACRYSTALS-Kyber%20delivers%20strong%20protection%20against%20quantum%20threats%20with%20minimal%0Aperformance%20overhead%2C%20making%20it%20highly%20suitable%20for%20UAVs%20with%20resource%0Aconstraints.%20Moreover%2C%20the%20proposed%20architecture%20integrates%20an%20Artificial%0AIntelligence%20%28AI%29-based%20Intrusion%20Detection%20System%20%28IDS%29%20to%20further%20enhance%0Asecurity.%20In%20performance%20evaluations%2C%20the%20IDS%20demonstrated%20strong%20results%0Aacross%20multiple%20models%20with%20XGBoost%2C%20particularly%20in%20more%20demanding%20scenarios%2C%0Aoutperforming%20other%20models%20with%20an%20accuracy%20of%2097.33%25%20and%20an%20AUC%20of%200.94.%20These%0Afindings%20underscore%20the%20potential%20of%20combining%20quantum-resistant%20encryption%0Amechanisms%20with%20AI-driven%20IDS%20to%20create%20a%20robust%2C%20scalable%2C%20and%20secure%0Acommunication%20framework%20for%20UAV%20networks%2C%20particularly%20within%20the%0Ahigh-performance%20requirements%20of%205G%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecured%2520Communication%2520Schemes%2520for%2520UAVs%2520in%25205G%253A%2520CRYSTALS-Kyber%2520and%2520IDS%26entry.906535625%3DTaneya%2520Sharma%2520and%2520Seyed%2520Ahmad%2520Soleymani%2520and%2520Mohammad%2520Shojafar%2520and%2520Rahim%2520Tafazolli%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520secure%2520communication%2520architecture%2520for%2520Unmanned%2520Aerial%250AVehicles%2520%2528UAVs%2529%2520and%2520ground%2520stations%2520in%25205G%2520networks%252C%2520addressing%2520critical%250Achallenges%2520in%2520network%2520security.%2520The%2520proposed%2520solution%2520integrates%2520the%2520Advanced%250AEncryption%2520Standard%2520%2528AES%2529%2520with%2520Elliptic%2520Curve%2520Cryptography%2520%2528ECC%2529%2520and%250ACRYSTALS-Kyber%2520for%2520key%2520encapsulation%252C%2520offering%2520a%2520hybrid%2520cryptographic%2520approach.%250ABy%2520incorporating%2520CRYSTALS-Kyber%252C%2520the%2520framework%2520mitigates%2520vulnerabilities%2520in%2520ECC%250Aagainst%2520quantum%2520attacks%252C%2520positioning%2520it%2520as%2520a%2520quantum-resistant%2520alternative.%2520The%250Aarchitecture%2520is%2520based%2520on%2520a%2520server-client%2520model%252C%2520with%2520UAVs%2520functioning%2520as%250Aclients%2520and%2520the%2520ground%2520station%2520acting%2520as%2520the%2520server.%2520The%2520system%2520was%2520rigorously%250Aevaluated%2520in%2520both%2520VPN%2520and%25205G%2520environments.%2520Experimental%2520results%2520confirm%2520that%250ACRYSTALS-Kyber%2520delivers%2520strong%2520protection%2520against%2520quantum%2520threats%2520with%2520minimal%250Aperformance%2520overhead%252C%2520making%2520it%2520highly%2520suitable%2520for%2520UAVs%2520with%2520resource%250Aconstraints.%2520Moreover%252C%2520the%2520proposed%2520architecture%2520integrates%2520an%2520Artificial%250AIntelligence%2520%2528AI%2529-based%2520Intrusion%2520Detection%2520System%2520%2528IDS%2529%2520to%2520further%2520enhance%250Asecurity.%2520In%2520performance%2520evaluations%252C%2520the%2520IDS%2520demonstrated%2520strong%2520results%250Aacross%2520multiple%2520models%2520with%2520XGBoost%252C%2520particularly%2520in%2520more%2520demanding%2520scenarios%252C%250Aoutperforming%2520other%2520models%2520with%2520an%2520accuracy%2520of%252097.33%2525%2520and%2520an%2520AUC%2520of%25200.94.%2520These%250Afindings%2520underscore%2520the%2520potential%2520of%2520combining%2520quantum-resistant%2520encryption%250Amechanisms%2520with%2520AI-driven%2520IDS%2520to%2520create%2520a%2520robust%252C%2520scalable%252C%2520and%2520secure%250Acommunication%2520framework%2520for%2520UAV%2520networks%252C%2520particularly%2520within%2520the%250Ahigh-performance%2520requirements%2520of%25205G%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secured%20Communication%20Schemes%20for%20UAVs%20in%205G%3A%20CRYSTALS-Kyber%20and%20IDS&entry.906535625=Taneya%20Sharma%20and%20Seyed%20Ahmad%20Soleymani%20and%20Mohammad%20Shojafar%20and%20Rahim%20Tafazolli&entry.1292438233=%20%20This%20paper%20introduces%20a%20secure%20communication%20architecture%20for%20Unmanned%20Aerial%0AVehicles%20%28UAVs%29%20and%20ground%20stations%20in%205G%20networks%2C%20addressing%20critical%0Achallenges%20in%20network%20security.%20The%20proposed%20solution%20integrates%20the%20Advanced%0AEncryption%20Standard%20%28AES%29%20with%20Elliptic%20Curve%20Cryptography%20%28ECC%29%20and%0ACRYSTALS-Kyber%20for%20key%20encapsulation%2C%20offering%20a%20hybrid%20cryptographic%20approach.%0ABy%20incorporating%20CRYSTALS-Kyber%2C%20the%20framework%20mitigates%20vulnerabilities%20in%20ECC%0Aagainst%20quantum%20attacks%2C%20positioning%20it%20as%20a%20quantum-resistant%20alternative.%20The%0Aarchitecture%20is%20based%20on%20a%20server-client%20model%2C%20with%20UAVs%20functioning%20as%0Aclients%20and%20the%20ground%20station%20acting%20as%20the%20server.%20The%20system%20was%20rigorously%0Aevaluated%20in%20both%20VPN%20and%205G%20environments.%20Experimental%20results%20confirm%20that%0ACRYSTALS-Kyber%20delivers%20strong%20protection%20against%20quantum%20threats%20with%20minimal%0Aperformance%20overhead%2C%20making%20it%20highly%20suitable%20for%20UAVs%20with%20resource%0Aconstraints.%20Moreover%2C%20the%20proposed%20architecture%20integrates%20an%20Artificial%0AIntelligence%20%28AI%29-based%20Intrusion%20Detection%20System%20%28IDS%29%20to%20further%20enhance%0Asecurity.%20In%20performance%20evaluations%2C%20the%20IDS%20demonstrated%20strong%20results%0Aacross%20multiple%20models%20with%20XGBoost%2C%20particularly%20in%20more%20demanding%20scenarios%2C%0Aoutperforming%20other%20models%20with%20an%20accuracy%20of%2097.33%25%20and%20an%20AUC%20of%200.94.%20These%0Afindings%20underscore%20the%20potential%20of%20combining%20quantum-resistant%20encryption%0Amechanisms%20with%20AI-driven%20IDS%20to%20create%20a%20robust%2C%20scalable%2C%20and%20secure%0Acommunication%20framework%20for%20UAV%20networks%2C%20particularly%20within%20the%0Ahigh-performance%20requirements%20of%205G%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19191v1&entry.124074799=Read"},
{"title": "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a\n  Modified Spherical Harmonic Loss Function", "author": "Christopher Subich and Syed Zahid Husain and Leo Separovic and Jing Yang", "abstract": "  Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes.\n", "link": "http://arxiv.org/abs/2501.19374v1", "date": "2025-01-31", "relevancy": 1.3264, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4509}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4462}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fixing%20the%20Double%20Penalty%20in%20Data-Driven%20Weather%20Forecasting%20Through%20a%0A%20%20Modified%20Spherical%20Harmonic%20Loss%20Function&body=Title%3A%20Fixing%20the%20Double%20Penalty%20in%20Data-Driven%20Weather%20Forecasting%20Through%20a%0A%20%20Modified%20Spherical%20Harmonic%20Loss%20Function%0AAuthor%3A%20Christopher%20Subich%20and%20Syed%20Zahid%20Husain%20and%20Leo%20Separovic%20and%20Jing%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20data-driven%20weather%20forecasting%20models%20have%20delivered%0Adeterministic%20models%20that%20outperform%20the%20leading%20operational%20forecast%20systems%0Abased%20on%20traditional%2C%20physics-based%20models.%20However%2C%20these%20data-driven%20models%0Aare%20typically%20trained%20with%20a%20mean%20squared%20error%20loss%20function%2C%20which%20causes%0Asmoothing%20of%20fine%20scales%20through%20a%20%22double%20penalty%22%20effect.%20We%20develop%20a%0Asimple%2C%20parameter-free%20modification%20to%20this%20loss%20function%20that%20avoids%20this%0Aproblem%20by%20separating%20the%20loss%20attributable%20to%20decorrelation%20from%20the%20loss%0Aattributable%20to%20spectral%20amplitude%20errors.%20Fine-tuning%20the%20GraphCast%20model%20with%0Athis%20new%20loss%20function%20results%20in%20sharp%20deterministic%20weather%20forecasts%2C%20an%0Aincrease%20of%20the%20model%27s%20effective%20resolution%20from%201%2C250km%20to%20160km%2C%0Aimprovements%20to%20ensemble%20spread%2C%20and%20improvements%20to%20predictions%20of%20tropical%0Acyclone%20strength%20and%20surface%20wind%20extremes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFixing%2520the%2520Double%2520Penalty%2520in%2520Data-Driven%2520Weather%2520Forecasting%2520Through%2520a%250A%2520%2520Modified%2520Spherical%2520Harmonic%2520Loss%2520Function%26entry.906535625%3DChristopher%2520Subich%2520and%2520Syed%2520Zahid%2520Husain%2520and%2520Leo%2520Separovic%2520and%2520Jing%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520data-driven%2520weather%2520forecasting%2520models%2520have%2520delivered%250Adeterministic%2520models%2520that%2520outperform%2520the%2520leading%2520operational%2520forecast%2520systems%250Abased%2520on%2520traditional%252C%2520physics-based%2520models.%2520However%252C%2520these%2520data-driven%2520models%250Aare%2520typically%2520trained%2520with%2520a%2520mean%2520squared%2520error%2520loss%2520function%252C%2520which%2520causes%250Asmoothing%2520of%2520fine%2520scales%2520through%2520a%2520%2522double%2520penalty%2522%2520effect.%2520We%2520develop%2520a%250Asimple%252C%2520parameter-free%2520modification%2520to%2520this%2520loss%2520function%2520that%2520avoids%2520this%250Aproblem%2520by%2520separating%2520the%2520loss%2520attributable%2520to%2520decorrelation%2520from%2520the%2520loss%250Aattributable%2520to%2520spectral%2520amplitude%2520errors.%2520Fine-tuning%2520the%2520GraphCast%2520model%2520with%250Athis%2520new%2520loss%2520function%2520results%2520in%2520sharp%2520deterministic%2520weather%2520forecasts%252C%2520an%250Aincrease%2520of%2520the%2520model%2527s%2520effective%2520resolution%2520from%25201%252C250km%2520to%2520160km%252C%250Aimprovements%2520to%2520ensemble%2520spread%252C%2520and%2520improvements%2520to%2520predictions%2520of%2520tropical%250Acyclone%2520strength%2520and%2520surface%2520wind%2520extremes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fixing%20the%20Double%20Penalty%20in%20Data-Driven%20Weather%20Forecasting%20Through%20a%0A%20%20Modified%20Spherical%20Harmonic%20Loss%20Function&entry.906535625=Christopher%20Subich%20and%20Syed%20Zahid%20Husain%20and%20Leo%20Separovic%20and%20Jing%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20data-driven%20weather%20forecasting%20models%20have%20delivered%0Adeterministic%20models%20that%20outperform%20the%20leading%20operational%20forecast%20systems%0Abased%20on%20traditional%2C%20physics-based%20models.%20However%2C%20these%20data-driven%20models%0Aare%20typically%20trained%20with%20a%20mean%20squared%20error%20loss%20function%2C%20which%20causes%0Asmoothing%20of%20fine%20scales%20through%20a%20%22double%20penalty%22%20effect.%20We%20develop%20a%0Asimple%2C%20parameter-free%20modification%20to%20this%20loss%20function%20that%20avoids%20this%0Aproblem%20by%20separating%20the%20loss%20attributable%20to%20decorrelation%20from%20the%20loss%0Aattributable%20to%20spectral%20amplitude%20errors.%20Fine-tuning%20the%20GraphCast%20model%20with%0Athis%20new%20loss%20function%20results%20in%20sharp%20deterministic%20weather%20forecasts%2C%20an%0Aincrease%20of%20the%20model%27s%20effective%20resolution%20from%201%2C250km%20to%20160km%2C%0Aimprovements%20to%20ensemble%20spread%2C%20and%20improvements%20to%20predictions%20of%20tropical%0Acyclone%20strength%20and%20surface%20wind%20extremes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19374v1&entry.124074799=Read"},
{"title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets\n  and Models", "author": "Lukas Helff and Felix Friedrich and Manuel Brack and Kristian Kersting and Patrick Schramowski", "abstract": "  This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that\naddress the critical need for reliable guardrails in the era of large-scale\ndata and models. To this end, we establish a novel open framework, describing a\ncustomizable safety taxonomy, data preprocessing, augmentation, and training\nsetup. For teaching a VLM safeguard on safety, we further create a multimodal\nsafety dataset with high-quality human expert annotations, where each image is\nlabeled with a safety rating, category and rationale. We also employ advanced\naugmentations to support context-specific assessments. The resulting LlavaGuard\nmodels, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the\nsafety compliance of visual content against flexible policies. In comprehensive\nexperiments, LlavaGuard outperforms both state-of-the-art safeguards and VLMs\nin accuracy and in flexibly handling different policies. Additionally, we\ndemonstrate LlavaGuard's performance in two real-world applications:\nlarge-scale dataset annotation and moderation of text-to-image models. We make\nour entire framework publicly available, including the dataset and model\nweights.\n", "link": "http://arxiv.org/abs/2406.05113v2", "date": "2025-01-31", "relevancy": 2.0747, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5473}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models&body=Title%3A%20LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models%0AAuthor%3A%20Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski%0AAbstract%3A%20%20%20This%20paper%20introduces%20LlavaGuard%2C%20a%20suite%20of%20VLM-based%20vision%20safeguards%20that%0Aaddress%20the%20critical%20need%20for%20reliable%20guardrails%20in%20the%20era%20of%20large-scale%0Adata%20and%20models.%20To%20this%20end%2C%20we%20establish%20a%20novel%20open%20framework%2C%20describing%20a%0Acustomizable%20safety%20taxonomy%2C%20data%20preprocessing%2C%20augmentation%2C%20and%20training%0Asetup.%20For%20teaching%20a%20VLM%20safeguard%20on%20safety%2C%20we%20further%20create%20a%20multimodal%0Asafety%20dataset%20with%20high-quality%20human%20expert%20annotations%2C%20where%20each%20image%20is%0Alabeled%20with%20a%20safety%20rating%2C%20category%20and%20rationale.%20We%20also%20employ%20advanced%0Aaugmentations%20to%20support%20context-specific%20assessments.%20The%20resulting%20LlavaGuard%0Amodels%2C%20ranging%20from%200.5B%20to%207B%2C%20serve%20as%20a%20versatile%20tool%20for%20evaluating%20the%0Asafety%20compliance%20of%20visual%20content%20against%20flexible%20policies.%20In%20comprehensive%0Aexperiments%2C%20LlavaGuard%20outperforms%20both%20state-of-the-art%20safeguards%20and%20VLMs%0Ain%20accuracy%20and%20in%20flexibly%20handling%20different%20policies.%20Additionally%2C%20we%0Ademonstrate%20LlavaGuard%27s%20performance%20in%20two%20real-world%20applications%3A%0Alarge-scale%20dataset%20annotation%20and%20moderation%20of%20text-to-image%20models.%20We%20make%0Aour%20entire%20framework%20publicly%20available%2C%20including%20the%20dataset%20and%20model%0Aweights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlavaGuard%253A%2520An%2520Open%2520VLM-based%2520Framework%2520for%2520Safeguarding%2520Vision%2520Datasets%250A%2520%2520and%2520Models%26entry.906535625%3DLukas%2520Helff%2520and%2520Felix%2520Friedrich%2520and%2520Manuel%2520Brack%2520and%2520Kristian%2520Kersting%2520and%2520Patrick%2520Schramowski%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LlavaGuard%252C%2520a%2520suite%2520of%2520VLM-based%2520vision%2520safeguards%2520that%250Aaddress%2520the%2520critical%2520need%2520for%2520reliable%2520guardrails%2520in%2520the%2520era%2520of%2520large-scale%250Adata%2520and%2520models.%2520To%2520this%2520end%252C%2520we%2520establish%2520a%2520novel%2520open%2520framework%252C%2520describing%2520a%250Acustomizable%2520safety%2520taxonomy%252C%2520data%2520preprocessing%252C%2520augmentation%252C%2520and%2520training%250Asetup.%2520For%2520teaching%2520a%2520VLM%2520safeguard%2520on%2520safety%252C%2520we%2520further%2520create%2520a%2520multimodal%250Asafety%2520dataset%2520with%2520high-quality%2520human%2520expert%2520annotations%252C%2520where%2520each%2520image%2520is%250Alabeled%2520with%2520a%2520safety%2520rating%252C%2520category%2520and%2520rationale.%2520We%2520also%2520employ%2520advanced%250Aaugmentations%2520to%2520support%2520context-specific%2520assessments.%2520The%2520resulting%2520LlavaGuard%250Amodels%252C%2520ranging%2520from%25200.5B%2520to%25207B%252C%2520serve%2520as%2520a%2520versatile%2520tool%2520for%2520evaluating%2520the%250Asafety%2520compliance%2520of%2520visual%2520content%2520against%2520flexible%2520policies.%2520In%2520comprehensive%250Aexperiments%252C%2520LlavaGuard%2520outperforms%2520both%2520state-of-the-art%2520safeguards%2520and%2520VLMs%250Ain%2520accuracy%2520and%2520in%2520flexibly%2520handling%2520different%2520policies.%2520Additionally%252C%2520we%250Ademonstrate%2520LlavaGuard%2527s%2520performance%2520in%2520two%2520real-world%2520applications%253A%250Alarge-scale%2520dataset%2520annotation%2520and%2520moderation%2520of%2520text-to-image%2520models.%2520We%2520make%250Aour%2520entire%2520framework%2520publicly%2520available%252C%2520including%2520the%2520dataset%2520and%2520model%250Aweights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models&entry.906535625=Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski&entry.1292438233=%20%20This%20paper%20introduces%20LlavaGuard%2C%20a%20suite%20of%20VLM-based%20vision%20safeguards%20that%0Aaddress%20the%20critical%20need%20for%20reliable%20guardrails%20in%20the%20era%20of%20large-scale%0Adata%20and%20models.%20To%20this%20end%2C%20we%20establish%20a%20novel%20open%20framework%2C%20describing%20a%0Acustomizable%20safety%20taxonomy%2C%20data%20preprocessing%2C%20augmentation%2C%20and%20training%0Asetup.%20For%20teaching%20a%20VLM%20safeguard%20on%20safety%2C%20we%20further%20create%20a%20multimodal%0Asafety%20dataset%20with%20high-quality%20human%20expert%20annotations%2C%20where%20each%20image%20is%0Alabeled%20with%20a%20safety%20rating%2C%20category%20and%20rationale.%20We%20also%20employ%20advanced%0Aaugmentations%20to%20support%20context-specific%20assessments.%20The%20resulting%20LlavaGuard%0Amodels%2C%20ranging%20from%200.5B%20to%207B%2C%20serve%20as%20a%20versatile%20tool%20for%20evaluating%20the%0Asafety%20compliance%20of%20visual%20content%20against%20flexible%20policies.%20In%20comprehensive%0Aexperiments%2C%20LlavaGuard%20outperforms%20both%20state-of-the-art%20safeguards%20and%20VLMs%0Ain%20accuracy%20and%20in%20flexibly%20handling%20different%20policies.%20Additionally%2C%20we%0Ademonstrate%20LlavaGuard%27s%20performance%20in%20two%20real-world%20applications%3A%0Alarge-scale%20dataset%20annotation%20and%20moderation%20of%20text-to-image%20models.%20We%20make%0Aour%20entire%20framework%20publicly%20available%2C%20including%20the%20dataset%20and%20model%0Aweights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05113v2&entry.124074799=Read"},
{"title": "Residual Connections Harm Generative Representation Learning", "author": "Xiao Zhang and Ruoxi Jiang and William Gao and Rebecca Willett and Michael Maire", "abstract": "  We show that introducing a weighting factor to reduce the influence of\nidentity shortcuts in residual networks significantly enhances semantic feature\nlearning in generative representation learning frameworks, such as masked\nautoencoders (MAEs) and diffusion models. Our modification notably improves\nfeature quality, raising ImageNet-1K K-Nearest Neighbor accuracy from 27.4% to\n63.9% and linear probing accuracy from 67.8% to 72.7% for MAEs with a ViT-B/16\nbackbone, while also enhancing generation quality in diffusion models. This\nsignificant gap suggests that, while residual connection structure serves an\nessential role in facilitating gradient propagation, it may have a harmful side\neffect of reducing capacity for abstract learning by virtue of injecting an\necho of shallower representations into deeper layers. We ameliorate this\ndownside via a fixed formula for monotonically decreasing the contribution of\nidentity connections as layer depth increases. Our design promotes the gradual\ndevelopment of feature abstractions, without impacting network trainability.\nAnalyzing the representations learned by our modified residual networks, we\nfind correlation between low effective feature rank and downstream task\nperformance.\n", "link": "http://arxiv.org/abs/2404.10947v4", "date": "2025-01-31", "relevancy": 1.6137, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5439}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Connections%20Harm%20Generative%20Representation%20Learning&body=Title%3A%20Residual%20Connections%20Harm%20Generative%20Representation%20Learning%0AAuthor%3A%20Xiao%20Zhang%20and%20Ruoxi%20Jiang%20and%20William%20Gao%20and%20Rebecca%20Willett%20and%20Michael%20Maire%0AAbstract%3A%20%20%20We%20show%20that%20introducing%20a%20weighting%20factor%20to%20reduce%20the%20influence%20of%0Aidentity%20shortcuts%20in%20residual%20networks%20significantly%20enhances%20semantic%20feature%0Alearning%20in%20generative%20representation%20learning%20frameworks%2C%20such%20as%20masked%0Aautoencoders%20%28MAEs%29%20and%20diffusion%20models.%20Our%20modification%20notably%20improves%0Afeature%20quality%2C%20raising%20ImageNet-1K%20K-Nearest%20Neighbor%20accuracy%20from%2027.4%25%20to%0A63.9%25%20and%20linear%20probing%20accuracy%20from%2067.8%25%20to%2072.7%25%20for%20MAEs%20with%20a%20ViT-B/16%0Abackbone%2C%20while%20also%20enhancing%20generation%20quality%20in%20diffusion%20models.%20This%0Asignificant%20gap%20suggests%20that%2C%20while%20residual%20connection%20structure%20serves%20an%0Aessential%20role%20in%20facilitating%20gradient%20propagation%2C%20it%20may%20have%20a%20harmful%20side%0Aeffect%20of%20reducing%20capacity%20for%20abstract%20learning%20by%20virtue%20of%20injecting%20an%0Aecho%20of%20shallower%20representations%20into%20deeper%20layers.%20We%20ameliorate%20this%0Adownside%20via%20a%20fixed%20formula%20for%20monotonically%20decreasing%20the%20contribution%20of%0Aidentity%20connections%20as%20layer%20depth%20increases.%20Our%20design%20promotes%20the%20gradual%0Adevelopment%20of%20feature%20abstractions%2C%20without%20impacting%20network%20trainability.%0AAnalyzing%20the%20representations%20learned%20by%20our%20modified%20residual%20networks%2C%20we%0Afind%20correlation%20between%20low%20effective%20feature%20rank%20and%20downstream%20task%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10947v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Connections%2520Harm%2520Generative%2520Representation%2520Learning%26entry.906535625%3DXiao%2520Zhang%2520and%2520Ruoxi%2520Jiang%2520and%2520William%2520Gao%2520and%2520Rebecca%2520Willett%2520and%2520Michael%2520Maire%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520introducing%2520a%2520weighting%2520factor%2520to%2520reduce%2520the%2520influence%2520of%250Aidentity%2520shortcuts%2520in%2520residual%2520networks%2520significantly%2520enhances%2520semantic%2520feature%250Alearning%2520in%2520generative%2520representation%2520learning%2520frameworks%252C%2520such%2520as%2520masked%250Aautoencoders%2520%2528MAEs%2529%2520and%2520diffusion%2520models.%2520Our%2520modification%2520notably%2520improves%250Afeature%2520quality%252C%2520raising%2520ImageNet-1K%2520K-Nearest%2520Neighbor%2520accuracy%2520from%252027.4%2525%2520to%250A63.9%2525%2520and%2520linear%2520probing%2520accuracy%2520from%252067.8%2525%2520to%252072.7%2525%2520for%2520MAEs%2520with%2520a%2520ViT-B/16%250Abackbone%252C%2520while%2520also%2520enhancing%2520generation%2520quality%2520in%2520diffusion%2520models.%2520This%250Asignificant%2520gap%2520suggests%2520that%252C%2520while%2520residual%2520connection%2520structure%2520serves%2520an%250Aessential%2520role%2520in%2520facilitating%2520gradient%2520propagation%252C%2520it%2520may%2520have%2520a%2520harmful%2520side%250Aeffect%2520of%2520reducing%2520capacity%2520for%2520abstract%2520learning%2520by%2520virtue%2520of%2520injecting%2520an%250Aecho%2520of%2520shallower%2520representations%2520into%2520deeper%2520layers.%2520We%2520ameliorate%2520this%250Adownside%2520via%2520a%2520fixed%2520formula%2520for%2520monotonically%2520decreasing%2520the%2520contribution%2520of%250Aidentity%2520connections%2520as%2520layer%2520depth%2520increases.%2520Our%2520design%2520promotes%2520the%2520gradual%250Adevelopment%2520of%2520feature%2520abstractions%252C%2520without%2520impacting%2520network%2520trainability.%250AAnalyzing%2520the%2520representations%2520learned%2520by%2520our%2520modified%2520residual%2520networks%252C%2520we%250Afind%2520correlation%2520between%2520low%2520effective%2520feature%2520rank%2520and%2520downstream%2520task%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10947v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Connections%20Harm%20Generative%20Representation%20Learning&entry.906535625=Xiao%20Zhang%20and%20Ruoxi%20Jiang%20and%20William%20Gao%20and%20Rebecca%20Willett%20and%20Michael%20Maire&entry.1292438233=%20%20We%20show%20that%20introducing%20a%20weighting%20factor%20to%20reduce%20the%20influence%20of%0Aidentity%20shortcuts%20in%20residual%20networks%20significantly%20enhances%20semantic%20feature%0Alearning%20in%20generative%20representation%20learning%20frameworks%2C%20such%20as%20masked%0Aautoencoders%20%28MAEs%29%20and%20diffusion%20models.%20Our%20modification%20notably%20improves%0Afeature%20quality%2C%20raising%20ImageNet-1K%20K-Nearest%20Neighbor%20accuracy%20from%2027.4%25%20to%0A63.9%25%20and%20linear%20probing%20accuracy%20from%2067.8%25%20to%2072.7%25%20for%20MAEs%20with%20a%20ViT-B/16%0Abackbone%2C%20while%20also%20enhancing%20generation%20quality%20in%20diffusion%20models.%20This%0Asignificant%20gap%20suggests%20that%2C%20while%20residual%20connection%20structure%20serves%20an%0Aessential%20role%20in%20facilitating%20gradient%20propagation%2C%20it%20may%20have%20a%20harmful%20side%0Aeffect%20of%20reducing%20capacity%20for%20abstract%20learning%20by%20virtue%20of%20injecting%20an%0Aecho%20of%20shallower%20representations%20into%20deeper%20layers.%20We%20ameliorate%20this%0Adownside%20via%20a%20fixed%20formula%20for%20monotonically%20decreasing%20the%20contribution%20of%0Aidentity%20connections%20as%20layer%20depth%20increases.%20Our%20design%20promotes%20the%20gradual%0Adevelopment%20of%20feature%20abstractions%2C%20without%20impacting%20network%20trainability.%0AAnalyzing%20the%20representations%20learned%20by%20our%20modified%20residual%20networks%2C%20we%0Afind%20correlation%20between%20low%20effective%20feature%20rank%20and%20downstream%20task%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10947v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


