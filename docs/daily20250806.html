<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250805.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation\n  Learning", "author": "Shashanka Venkataramanan and Valentinos Pariza and Mohammadreza Salehi and Lukas Knobel and Spyros Gidaris and Elias Ramzi and Andrei Bursuc and Yuki M. Asano", "abstract": "  We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.\n", "link": "http://arxiv.org/abs/2507.14137v2", "date": "2025-08-05", "relevancy": 2.7633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning&body=Title%3A%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning%0AAuthor%3A%20Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%0A%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%0Asurpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%0ACLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%0Apipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%0Aa%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%0ASSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%0Alarge%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%0Aaccount%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%0Aintroduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%0Anested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%0Ainto%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%0Aenabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%0Anovel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%0Abiases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%0Acontent.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%0Ademonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%0Aestablish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%0Aopen%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%0Athe%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/valeoai/Franca.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFranca%253A%2520Nested%2520Matryoshka%2520Clustering%2520for%2520Scalable%2520Visual%2520Representation%250A%2520%2520Learning%26entry.906535625%3DShashanka%2520Venkataramanan%2520and%2520Valentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Lukas%2520Knobel%2520and%2520Spyros%2520Gidaris%2520and%2520Elias%2520Ramzi%2520and%2520Andrei%2520Bursuc%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520present%2520Franca%2520%2528pronounced%2520Fran-ka%2529%253A%2520free%2520one%253B%2520the%2520first%2520fully%2520open-source%250A%2528data%252C%2520code%252C%2520weights%2529%2520vision%2520foundation%2520model%2520that%2520matches%2520and%2520in%2520many%2520cases%250Asurpasses%2520the%2520performance%2520of%2520state-of-the-art%2520proprietary%2520models%252C%2520e.g.%252C%2520DINOv2%252C%250ACLIP%252C%2520SigLIPv2%252C%2520etc.%2520Our%2520approach%2520is%2520grounded%2520in%2520a%2520transparent%2520training%250Apipeline%2520inspired%2520by%2520Web-SSL%2520and%2520uses%2520publicly%2520available%2520data%253A%2520ImageNet-21K%2520and%250Aa%2520subset%2520of%2520ReLAION-2B.%2520Beyond%2520model%2520release%252C%2520we%2520tackle%2520critical%2520limitations%2520in%250ASSL%2520clustering%2520methods.%2520While%2520modern%2520models%2520rely%2520on%2520assigning%2520image%2520features%2520to%250Alarge%2520codebooks%2520via%2520clustering%2520algorithms%2520like%2520Sinkhorn-Knopp%252C%2520they%2520fail%2520to%250Aaccount%2520for%2520the%2520inherent%2520ambiguity%2520in%2520clustering%2520semantics.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520parameter-efficient%252C%2520multi-head%2520clustering%2520projector%2520based%2520on%250Anested%2520Matryoshka%2520representations.%2520This%2520design%2520progressively%2520refines%2520features%250Ainto%2520increasingly%2520fine-grained%2520clusters%2520without%2520increasing%2520the%2520model%2520size%252C%250Aenabling%2520both%2520performance%2520and%2520memory%2520efficiency.%2520Additionally%252C%2520we%2520propose%2520a%250Anovel%2520positional%2520disentanglement%2520strategy%2520that%2520explicitly%2520removes%2520positional%250Abiases%2520from%2520dense%2520representations%252C%2520thereby%2520improving%2520the%2520encoding%2520of%2520semantic%250Acontent.%2520This%2520leads%2520to%2520consistent%2520gains%2520on%2520several%2520downstream%2520benchmarks%252C%250Ademonstrating%2520the%2520utility%2520of%2520cleaner%2520feature%2520spaces.%2520Our%2520contributions%250Aestablish%2520a%2520new%2520standard%2520for%2520transparent%252C%2520high-performance%2520vision%2520models%2520and%250Aopen%2520a%2520path%2520toward%2520more%2520reproducible%2520and%2520generalizable%2520foundation%2520models%2520for%250Athe%2520broader%2520AI%2520community.%2520The%2520code%2520and%2520model%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/valeoai/Franca.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning&entry.906535625=Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%0A%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%0Asurpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%0ACLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%0Apipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%0Aa%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%0ASSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%0Alarge%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%0Aaccount%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%0Aintroduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%0Anested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%0Ainto%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%0Aenabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%0Anovel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%0Abiases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%0Acontent.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%0Ademonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%0Aestablish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%0Aopen%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%0Athe%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/valeoai/Franca.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14137v2&entry.124074799=Read"},
{"title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via\n  Generalizable Gaussian Splatting from Unposed Multi-View Images", "author": "Xiangyu Sun and Haoyi jiang and Liu Liu and Seungtae Nam and Gyeongjin Kang and Xinjie wang and Wei Sui and Zhizhong Su and Wenyu Liu and Xinggang Wang and Eunbyung Park", "abstract": "  Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.\n", "link": "http://arxiv.org/abs/2508.03643v1", "date": "2025-08-05", "relevancy": 2.702, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6856}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6742}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images&body=Title%3A%20Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images%0AAuthor%3A%20Xiangyu%20Sun%20and%20Haoyi%20jiang%20and%20Liu%20Liu%20and%20Seungtae%20Nam%20and%20Gyeongjin%20Kang%20and%20Xinjie%20wang%20and%20Wei%20Sui%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20Reconstructing%20and%20semantically%20interpreting%203D%20scenes%20from%20sparse%202D%20views%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision.%20Conventional%20methods%20often%0Adecouple%20semantic%20understanding%20from%20reconstruction%20or%20necessitate%20costly%0Aper-scene%20optimization%2C%20thereby%20restricting%20their%20scalability%20and%0Ageneralizability.%20In%20this%20paper%2C%20we%20introduce%20Uni3R%2C%20a%20novel%20feed-forward%0Aframework%20that%20jointly%20reconstructs%20a%20unified%203D%20scene%20representation%20enriched%0Awith%20open-vocabulary%20semantics%2C%20directly%20from%20unposed%20multi-view%20images.%20Our%0Aapproach%20leverages%20a%20Cross-View%20Transformer%20to%20robustly%20integrate%20information%0Aacross%20arbitrary%20multi-view%20inputs%2C%20which%20then%20regresses%20a%20set%20of%203D%20Gaussian%0Aprimitives%20endowed%20with%20semantic%20feature%20fields.%20This%20unified%20representation%0Afacilitates%20high-fidelity%20novel%20view%20synthesis%2C%20open-vocabulary%203D%20semantic%0Asegmentation%2C%20and%20depth%20prediction%2C%20all%20within%20a%20single%2C%20feed-forward%20pass.%0AExtensive%20experiments%20demonstrate%20that%20Uni3R%20establishes%20a%20new%20state-of-the-art%0Aacross%20multiple%20benchmarks%2C%20including%2025.07%20PSNR%20on%20RE10K%20and%2055.84%20mIoU%20on%0AScanNet.%20Our%20work%20signifies%20a%20novel%20paradigm%20towards%20generalizable%2C%20unified%203D%0Ascene%20reconstruction%20and%20understanding.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/Uni3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni3R%253A%2520Unified%25203D%2520Reconstruction%2520and%2520Semantic%2520Understanding%2520via%250A%2520%2520Generalizable%2520Gaussian%2520Splatting%2520from%2520Unposed%2520Multi-View%2520Images%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Haoyi%2520jiang%2520and%2520Liu%2520Liu%2520and%2520Seungtae%2520Nam%2520and%2520Gyeongjin%2520Kang%2520and%2520Xinjie%2520wang%2520and%2520Wei%2520Sui%2520and%2520Zhizhong%2520Su%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520semantically%2520interpreting%25203D%2520scenes%2520from%2520sparse%25202D%2520views%250Aremains%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision.%2520Conventional%2520methods%2520often%250Adecouple%2520semantic%2520understanding%2520from%2520reconstruction%2520or%2520necessitate%2520costly%250Aper-scene%2520optimization%252C%2520thereby%2520restricting%2520their%2520scalability%2520and%250Ageneralizability.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Uni3R%252C%2520a%2520novel%2520feed-forward%250Aframework%2520that%2520jointly%2520reconstructs%2520a%2520unified%25203D%2520scene%2520representation%2520enriched%250Awith%2520open-vocabulary%2520semantics%252C%2520directly%2520from%2520unposed%2520multi-view%2520images.%2520Our%250Aapproach%2520leverages%2520a%2520Cross-View%2520Transformer%2520to%2520robustly%2520integrate%2520information%250Aacross%2520arbitrary%2520multi-view%2520inputs%252C%2520which%2520then%2520regresses%2520a%2520set%2520of%25203D%2520Gaussian%250Aprimitives%2520endowed%2520with%2520semantic%2520feature%2520fields.%2520This%2520unified%2520representation%250Afacilitates%2520high-fidelity%2520novel%2520view%2520synthesis%252C%2520open-vocabulary%25203D%2520semantic%250Asegmentation%252C%2520and%2520depth%2520prediction%252C%2520all%2520within%2520a%2520single%252C%2520feed-forward%2520pass.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Uni3R%2520establishes%2520a%2520new%2520state-of-the-art%250Aacross%2520multiple%2520benchmarks%252C%2520including%252025.07%2520PSNR%2520on%2520RE10K%2520and%252055.84%2520mIoU%2520on%250AScanNet.%2520Our%2520work%2520signifies%2520a%2520novel%2520paradigm%2520towards%2520generalizable%252C%2520unified%25203D%250Ascene%2520reconstruction%2520and%2520understanding.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HorizonRobotics/Uni3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images&entry.906535625=Xiangyu%20Sun%20and%20Haoyi%20jiang%20and%20Liu%20Liu%20and%20Seungtae%20Nam%20and%20Gyeongjin%20Kang%20and%20Xinjie%20wang%20and%20Wei%20Sui%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Eunbyung%20Park&entry.1292438233=%20%20Reconstructing%20and%20semantically%20interpreting%203D%20scenes%20from%20sparse%202D%20views%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision.%20Conventional%20methods%20often%0Adecouple%20semantic%20understanding%20from%20reconstruction%20or%20necessitate%20costly%0Aper-scene%20optimization%2C%20thereby%20restricting%20their%20scalability%20and%0Ageneralizability.%20In%20this%20paper%2C%20we%20introduce%20Uni3R%2C%20a%20novel%20feed-forward%0Aframework%20that%20jointly%20reconstructs%20a%20unified%203D%20scene%20representation%20enriched%0Awith%20open-vocabulary%20semantics%2C%20directly%20from%20unposed%20multi-view%20images.%20Our%0Aapproach%20leverages%20a%20Cross-View%20Transformer%20to%20robustly%20integrate%20information%0Aacross%20arbitrary%20multi-view%20inputs%2C%20which%20then%20regresses%20a%20set%20of%203D%20Gaussian%0Aprimitives%20endowed%20with%20semantic%20feature%20fields.%20This%20unified%20representation%0Afacilitates%20high-fidelity%20novel%20view%20synthesis%2C%20open-vocabulary%203D%20semantic%0Asegmentation%2C%20and%20depth%20prediction%2C%20all%20within%20a%20single%2C%20feed-forward%20pass.%0AExtensive%20experiments%20demonstrate%20that%20Uni3R%20establishes%20a%20new%20state-of-the-art%0Aacross%20multiple%20benchmarks%2C%20including%2025.07%20PSNR%20on%20RE10K%20and%2055.84%20mIoU%20on%0AScanNet.%20Our%20work%20signifies%20a%20novel%20paradigm%20towards%20generalizable%2C%20unified%203D%0Ascene%20reconstruction%20and%20understanding.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/Uni3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03643v1&entry.124074799=Read"},
{"title": "Streaming Generated Gaussian Process Experts for Online Learning and\n  Control", "author": "Zewen Yang and Dongfa Zhang and Xiaobing Dai and Fengyi Yu and Chi Zhang and Bingkun Huang and Hamid Sadeghian and Sami Haddadin", "abstract": "  Gaussian Processes (GPs), as a nonparametric learning method, offer flexible\nmodeling capabilities and calibrated uncertainty quantification for function\napproximations. Additionally, GPs support online learning by efficiently\nincorporating new data with polynomial-time computation, making them\nwell-suited for safety-critical dynamical systems that require rapid\nadaptation. However, the inference and online updates of exact GPs, when\nprocessing streaming data, incur cubic computation time and quadratic storage\nmemory complexity, limiting their scalability to large datasets in real-time\nsettings. In this paper, we propose a \\underline{s}treaming\n\\underline{k}ernel-induced progressivel\\underline{y} generated expert framework\nof \\underline{G}aussian \\underline{p}rocesses (SkyGP) that addresses both\ncomputational and memory constraints by maintaining a bounded set of experts,\nwhile inheriting the learning performance guarantees from exact Gaussian\nprocesses. Furthermore, two SkyGP variants are introduced, each tailored to a\nspecific objective, either maximizing prediction accuracy (SkyGP-Dense) or\nimproving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is\nvalidated through extensive benchmarks and real-time control experiments\ndemonstrating its superior performance compared to state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2508.03679v1", "date": "2025-08-05", "relevancy": 2.553, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5213}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5172}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control&body=Title%3A%20Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control%0AAuthor%3A%20Zewen%20Yang%20and%20Dongfa%20Zhang%20and%20Xiaobing%20Dai%20and%20Fengyi%20Yu%20and%20Chi%20Zhang%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%2C%20as%20a%20nonparametric%20learning%20method%2C%20offer%20flexible%0Amodeling%20capabilities%20and%20calibrated%20uncertainty%20quantification%20for%20function%0Aapproximations.%20Additionally%2C%20GPs%20support%20online%20learning%20by%20efficiently%0Aincorporating%20new%20data%20with%20polynomial-time%20computation%2C%20making%20them%0Awell-suited%20for%20safety-critical%20dynamical%20systems%20that%20require%20rapid%0Aadaptation.%20However%2C%20the%20inference%20and%20online%20updates%20of%20exact%20GPs%2C%20when%0Aprocessing%20streaming%20data%2C%20incur%20cubic%20computation%20time%20and%20quadratic%20storage%0Amemory%20complexity%2C%20limiting%20their%20scalability%20to%20large%20datasets%20in%20real-time%0Asettings.%20In%20this%20paper%2C%20we%20propose%20a%20%5Cunderline%7Bs%7Dtreaming%0A%5Cunderline%7Bk%7Dernel-induced%20progressivel%5Cunderline%7By%7D%20generated%20expert%20framework%0Aof%20%5Cunderline%7BG%7Daussian%20%5Cunderline%7Bp%7Drocesses%20%28SkyGP%29%20that%20addresses%20both%0Acomputational%20and%20memory%20constraints%20by%20maintaining%20a%20bounded%20set%20of%20experts%2C%0Awhile%20inheriting%20the%20learning%20performance%20guarantees%20from%20exact%20Gaussian%0Aprocesses.%20Furthermore%2C%20two%20SkyGP%20variants%20are%20introduced%2C%20each%20tailored%20to%20a%0Aspecific%20objective%2C%20either%20maximizing%20prediction%20accuracy%20%28SkyGP-Dense%29%20or%0Aimproving%20computational%20efficiency%20%28SkyGP-Fast%29.%20The%20effectiveness%20of%20SkyGP%20is%0Avalidated%20through%20extensive%20benchmarks%20and%20real-time%20control%20experiments%0Ademonstrating%20its%20superior%20performance%20compared%20to%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%2520Generated%2520Gaussian%2520Process%2520Experts%2520for%2520Online%2520Learning%2520and%250A%2520%2520Control%26entry.906535625%3DZewen%2520Yang%2520and%2520Dongfa%2520Zhang%2520and%2520Xiaobing%2520Dai%2520and%2520Fengyi%2520Yu%2520and%2520Chi%2520Zhang%2520and%2520Bingkun%2520Huang%2520and%2520Hamid%2520Sadeghian%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%252C%2520as%2520a%2520nonparametric%2520learning%2520method%252C%2520offer%2520flexible%250Amodeling%2520capabilities%2520and%2520calibrated%2520uncertainty%2520quantification%2520for%2520function%250Aapproximations.%2520Additionally%252C%2520GPs%2520support%2520online%2520learning%2520by%2520efficiently%250Aincorporating%2520new%2520data%2520with%2520polynomial-time%2520computation%252C%2520making%2520them%250Awell-suited%2520for%2520safety-critical%2520dynamical%2520systems%2520that%2520require%2520rapid%250Aadaptation.%2520However%252C%2520the%2520inference%2520and%2520online%2520updates%2520of%2520exact%2520GPs%252C%2520when%250Aprocessing%2520streaming%2520data%252C%2520incur%2520cubic%2520computation%2520time%2520and%2520quadratic%2520storage%250Amemory%2520complexity%252C%2520limiting%2520their%2520scalability%2520to%2520large%2520datasets%2520in%2520real-time%250Asettings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520%255Cunderline%257Bs%257Dtreaming%250A%255Cunderline%257Bk%257Dernel-induced%2520progressivel%255Cunderline%257By%257D%2520generated%2520expert%2520framework%250Aof%2520%255Cunderline%257BG%257Daussian%2520%255Cunderline%257Bp%257Drocesses%2520%2528SkyGP%2529%2520that%2520addresses%2520both%250Acomputational%2520and%2520memory%2520constraints%2520by%2520maintaining%2520a%2520bounded%2520set%2520of%2520experts%252C%250Awhile%2520inheriting%2520the%2520learning%2520performance%2520guarantees%2520from%2520exact%2520Gaussian%250Aprocesses.%2520Furthermore%252C%2520two%2520SkyGP%2520variants%2520are%2520introduced%252C%2520each%2520tailored%2520to%2520a%250Aspecific%2520objective%252C%2520either%2520maximizing%2520prediction%2520accuracy%2520%2528SkyGP-Dense%2529%2520or%250Aimproving%2520computational%2520efficiency%2520%2528SkyGP-Fast%2529.%2520The%2520effectiveness%2520of%2520SkyGP%2520is%250Avalidated%2520through%2520extensive%2520benchmarks%2520and%2520real-time%2520control%2520experiments%250Ademonstrating%2520its%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control&entry.906535625=Zewen%20Yang%20and%20Dongfa%20Zhang%20and%20Xiaobing%20Dai%20and%20Fengyi%20Yu%20and%20Chi%20Zhang%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%2C%20as%20a%20nonparametric%20learning%20method%2C%20offer%20flexible%0Amodeling%20capabilities%20and%20calibrated%20uncertainty%20quantification%20for%20function%0Aapproximations.%20Additionally%2C%20GPs%20support%20online%20learning%20by%20efficiently%0Aincorporating%20new%20data%20with%20polynomial-time%20computation%2C%20making%20them%0Awell-suited%20for%20safety-critical%20dynamical%20systems%20that%20require%20rapid%0Aadaptation.%20However%2C%20the%20inference%20and%20online%20updates%20of%20exact%20GPs%2C%20when%0Aprocessing%20streaming%20data%2C%20incur%20cubic%20computation%20time%20and%20quadratic%20storage%0Amemory%20complexity%2C%20limiting%20their%20scalability%20to%20large%20datasets%20in%20real-time%0Asettings.%20In%20this%20paper%2C%20we%20propose%20a%20%5Cunderline%7Bs%7Dtreaming%0A%5Cunderline%7Bk%7Dernel-induced%20progressivel%5Cunderline%7By%7D%20generated%20expert%20framework%0Aof%20%5Cunderline%7BG%7Daussian%20%5Cunderline%7Bp%7Drocesses%20%28SkyGP%29%20that%20addresses%20both%0Acomputational%20and%20memory%20constraints%20by%20maintaining%20a%20bounded%20set%20of%20experts%2C%0Awhile%20inheriting%20the%20learning%20performance%20guarantees%20from%20exact%20Gaussian%0Aprocesses.%20Furthermore%2C%20two%20SkyGP%20variants%20are%20introduced%2C%20each%20tailored%20to%20a%0Aspecific%20objective%2C%20either%20maximizing%20prediction%20accuracy%20%28SkyGP-Dense%29%20or%0Aimproving%20computational%20efficiency%20%28SkyGP-Fast%29.%20The%20effectiveness%20of%20SkyGP%20is%0Avalidated%20through%20extensive%20benchmarks%20and%20real-time%20control%20experiments%0Ademonstrating%20its%20superior%20performance%20compared%20to%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03679v1&entry.124074799=Read"},
{"title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation", "author": "Jianxiong Gao and Zhaoxi Chen and Xian Liu and Jianfeng Feng and Chenyang Si and Yanwei Fu and Yu Qiao and Ziwei Liu", "abstract": "  Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.\n", "link": "http://arxiv.org/abs/2508.03694v1", "date": "2025-08-05", "relevancy": 2.5093, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6464}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.631}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVie%3A%20Multimodal-Guided%20Controllable%20Ultra-Long%20Video%20Generation&body=Title%3A%20LongVie%3A%20Multimodal-Guided%20Controllable%20Ultra-Long%20Video%20Generation%0AAuthor%3A%20Jianxiong%20Gao%20and%20Zhaoxi%20Chen%20and%20Xian%20Liu%20and%20Jianfeng%20Feng%20and%20Chenyang%20Si%20and%20Yanwei%20Fu%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Controllable%20ultra-long%20video%20generation%20is%20a%20fundamental%20yet%20challenging%0Atask.%20Although%20existing%20methods%20are%20effective%20for%20short%20clips%2C%20they%20struggle%20to%0Ascale%20due%20to%20issues%20such%20as%20temporal%20inconsistency%20and%20visual%20degradation.%20In%0Athis%20paper%2C%20we%20initially%20investigate%20and%20identify%20three%20key%20factors%3A%20separate%0Anoise%20initialization%2C%20independent%20control%20signal%20normalization%2C%20and%20the%0Alimitations%20of%20single-modality%20guidance.%20To%20address%20these%20issues%2C%20we%20propose%0ALongVie%2C%20an%20end-to-end%20autoregressive%20framework%20for%20controllable%20long%20video%0Ageneration.%20LongVie%20introduces%20two%20core%20designs%20to%20ensure%20temporal%20consistency%3A%0A1%29%20a%20unified%20noise%20initialization%20strategy%20that%20maintains%20consistent%20generation%0Aacross%20clips%2C%20and%202%29%20global%20control%20signal%20normalization%20that%20enforces%0Aalignment%20in%20the%20control%20space%20throughout%20the%20entire%20video.%20To%20mitigate%20visual%0Adegradation%2C%20LongVie%20employs%203%29%20a%20multi-modal%20control%20framework%20that%20integrates%0Aboth%20dense%20%28e.g.%2C%20depth%20maps%29%20and%20sparse%20%28e.g.%2C%20keypoints%29%20control%20signals%2C%0Acomplemented%20by%204%29%20a%20degradation-aware%20training%20strategy%20that%20adaptively%0Abalances%20modality%20contributions%20over%20time%20to%20preserve%20visual%20quality.%20We%20also%0Aintroduce%20LongVGenBench%2C%20a%20comprehensive%20benchmark%20consisting%20of%20100%0Ahigh-resolution%20videos%20spanning%20diverse%20real-world%20and%20synthetic%20environments%2C%0Aeach%20lasting%20over%20one%20minute.%20Extensive%20experiments%20show%20that%20LongVie%20achieves%0Astate-of-the-art%20performance%20in%20long-range%20controllability%2C%20consistency%2C%20and%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVie%253A%2520Multimodal-Guided%2520Controllable%2520Ultra-Long%2520Video%2520Generation%26entry.906535625%3DJianxiong%2520Gao%2520and%2520Zhaoxi%2520Chen%2520and%2520Xian%2520Liu%2520and%2520Jianfeng%2520Feng%2520and%2520Chenyang%2520Si%2520and%2520Yanwei%2520Fu%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Controllable%2520ultra-long%2520video%2520generation%2520is%2520a%2520fundamental%2520yet%2520challenging%250Atask.%2520Although%2520existing%2520methods%2520are%2520effective%2520for%2520short%2520clips%252C%2520they%2520struggle%2520to%250Ascale%2520due%2520to%2520issues%2520such%2520as%2520temporal%2520inconsistency%2520and%2520visual%2520degradation.%2520In%250Athis%2520paper%252C%2520we%2520initially%2520investigate%2520and%2520identify%2520three%2520key%2520factors%253A%2520separate%250Anoise%2520initialization%252C%2520independent%2520control%2520signal%2520normalization%252C%2520and%2520the%250Alimitations%2520of%2520single-modality%2520guidance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ALongVie%252C%2520an%2520end-to-end%2520autoregressive%2520framework%2520for%2520controllable%2520long%2520video%250Ageneration.%2520LongVie%2520introduces%2520two%2520core%2520designs%2520to%2520ensure%2520temporal%2520consistency%253A%250A1%2529%2520a%2520unified%2520noise%2520initialization%2520strategy%2520that%2520maintains%2520consistent%2520generation%250Aacross%2520clips%252C%2520and%25202%2529%2520global%2520control%2520signal%2520normalization%2520that%2520enforces%250Aalignment%2520in%2520the%2520control%2520space%2520throughout%2520the%2520entire%2520video.%2520To%2520mitigate%2520visual%250Adegradation%252C%2520LongVie%2520employs%25203%2529%2520a%2520multi-modal%2520control%2520framework%2520that%2520integrates%250Aboth%2520dense%2520%2528e.g.%252C%2520depth%2520maps%2529%2520and%2520sparse%2520%2528e.g.%252C%2520keypoints%2529%2520control%2520signals%252C%250Acomplemented%2520by%25204%2529%2520a%2520degradation-aware%2520training%2520strategy%2520that%2520adaptively%250Abalances%2520modality%2520contributions%2520over%2520time%2520to%2520preserve%2520visual%2520quality.%2520We%2520also%250Aintroduce%2520LongVGenBench%252C%2520a%2520comprehensive%2520benchmark%2520consisting%2520of%2520100%250Ahigh-resolution%2520videos%2520spanning%2520diverse%2520real-world%2520and%2520synthetic%2520environments%252C%250Aeach%2520lasting%2520over%2520one%2520minute.%2520Extensive%2520experiments%2520show%2520that%2520LongVie%2520achieves%250Astate-of-the-art%2520performance%2520in%2520long-range%2520controllability%252C%2520consistency%252C%2520and%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVie%3A%20Multimodal-Guided%20Controllable%20Ultra-Long%20Video%20Generation&entry.906535625=Jianxiong%20Gao%20and%20Zhaoxi%20Chen%20and%20Xian%20Liu%20and%20Jianfeng%20Feng%20and%20Chenyang%20Si%20and%20Yanwei%20Fu%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Controllable%20ultra-long%20video%20generation%20is%20a%20fundamental%20yet%20challenging%0Atask.%20Although%20existing%20methods%20are%20effective%20for%20short%20clips%2C%20they%20struggle%20to%0Ascale%20due%20to%20issues%20such%20as%20temporal%20inconsistency%20and%20visual%20degradation.%20In%0Athis%20paper%2C%20we%20initially%20investigate%20and%20identify%20three%20key%20factors%3A%20separate%0Anoise%20initialization%2C%20independent%20control%20signal%20normalization%2C%20and%20the%0Alimitations%20of%20single-modality%20guidance.%20To%20address%20these%20issues%2C%20we%20propose%0ALongVie%2C%20an%20end-to-end%20autoregressive%20framework%20for%20controllable%20long%20video%0Ageneration.%20LongVie%20introduces%20two%20core%20designs%20to%20ensure%20temporal%20consistency%3A%0A1%29%20a%20unified%20noise%20initialization%20strategy%20that%20maintains%20consistent%20generation%0Aacross%20clips%2C%20and%202%29%20global%20control%20signal%20normalization%20that%20enforces%0Aalignment%20in%20the%20control%20space%20throughout%20the%20entire%20video.%20To%20mitigate%20visual%0Adegradation%2C%20LongVie%20employs%203%29%20a%20multi-modal%20control%20framework%20that%20integrates%0Aboth%20dense%20%28e.g.%2C%20depth%20maps%29%20and%20sparse%20%28e.g.%2C%20keypoints%29%20control%20signals%2C%0Acomplemented%20by%204%29%20a%20degradation-aware%20training%20strategy%20that%20adaptively%0Abalances%20modality%20contributions%20over%20time%20to%20preserve%20visual%20quality.%20We%20also%0Aintroduce%20LongVGenBench%2C%20a%20comprehensive%20benchmark%20consisting%20of%20100%0Ahigh-resolution%20videos%20spanning%20diverse%20real-world%20and%20synthetic%20environments%2C%0Aeach%20lasting%20over%20one%20minute.%20Extensive%20experiments%20show%20that%20LongVie%20achieves%0Astate-of-the-art%20performance%20in%20long-range%20controllability%2C%20consistency%2C%20and%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03694v1&entry.124074799=Read"},
{"title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image", "author": "Youquan Liu and Lingdong Kong and Weidong Yang and Ao Liang and Jianxiong Gao and Yang Wu and Xiang Xu and Xin Li and Linfeng Li and Runnan Chen and Ben Fei", "abstract": "  Realistic and controllable panoramic LiDAR data generation is critical for\nscalable 3D perception in autonomous driving and robotics. Existing methods\neither perform unconditional generation with poor controllability or adopt\ntext-guided synthesis, which lacks fine-grained spatial control. Leveraging a\nmonocular RGB image as a spatial control signal offers a scalable and low-cost\nalternative, which remains an open problem. However, it faces three core\nchallenges: (i) semantic and depth cues from RGB are vary spatially,\ncomplicating reliable conditioning generation; (ii) modality gaps between RGB\nappearance and LiDAR geometry amplify alignment errors under noisy diffusion;\nand (iii) maintaining structural coherence between monocular RGB and panoramic\nLiDAR is challenging, particularly in non-overlap regions between images and\nLiDAR. To address these challenges, we propose Veila, a novel conditional\ndiffusion framework that integrates: a Confidence-Aware Conditioning Mechanism\n(CACM) that strengthens RGB conditioning by adaptively balancing semantic and\ndepth cues according to their local reliability; a Geometric Cross-Modal\nAlignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a\nPanoramic Feature Coherence (PFC) for enforcing global structural consistency\nacross monocular RGB and panoramic LiDAR. Additionally, we introduce two\nmetrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to\nevaluate alignment quality across modalities. Experiments on nuScenes,\nSemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila\nachieves state-of-the-art generation fidelity and cross-modal consistency,\nwhile enabling generative data augmentation that improves downstream LiDAR\nsemantic segmentation.\n", "link": "http://arxiv.org/abs/2508.03690v1", "date": "2025-08-05", "relevancy": 2.4686, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6383}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6285}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Veila%3A%20Panoramic%20LiDAR%20Generation%20from%20a%20Monocular%20RGB%20Image&body=Title%3A%20Veila%3A%20Panoramic%20LiDAR%20Generation%20from%20a%20Monocular%20RGB%20Image%0AAuthor%3A%20Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Weidong%20Yang%20and%20Ao%20Liang%20and%20Jianxiong%20Gao%20and%20Yang%20Wu%20and%20Xiang%20Xu%20and%20Xin%20Li%20and%20Linfeng%20Li%20and%20Runnan%20Chen%20and%20Ben%20Fei%0AAbstract%3A%20%20%20Realistic%20and%20controllable%20panoramic%20LiDAR%20data%20generation%20is%20critical%20for%0Ascalable%203D%20perception%20in%20autonomous%20driving%20and%20robotics.%20Existing%20methods%0Aeither%20perform%20unconditional%20generation%20with%20poor%20controllability%20or%20adopt%0Atext-guided%20synthesis%2C%20which%20lacks%20fine-grained%20spatial%20control.%20Leveraging%20a%0Amonocular%20RGB%20image%20as%20a%20spatial%20control%20signal%20offers%20a%20scalable%20and%20low-cost%0Aalternative%2C%20which%20remains%20an%20open%20problem.%20However%2C%20it%20faces%20three%20core%0Achallenges%3A%20%28i%29%20semantic%20and%20depth%20cues%20from%20RGB%20are%20vary%20spatially%2C%0Acomplicating%20reliable%20conditioning%20generation%3B%20%28ii%29%20modality%20gaps%20between%20RGB%0Aappearance%20and%20LiDAR%20geometry%20amplify%20alignment%20errors%20under%20noisy%20diffusion%3B%0Aand%20%28iii%29%20maintaining%20structural%20coherence%20between%20monocular%20RGB%20and%20panoramic%0ALiDAR%20is%20challenging%2C%20particularly%20in%20non-overlap%20regions%20between%20images%20and%0ALiDAR.%20To%20address%20these%20challenges%2C%20we%20propose%20Veila%2C%20a%20novel%20conditional%0Adiffusion%20framework%20that%20integrates%3A%20a%20Confidence-Aware%20Conditioning%20Mechanism%0A%28CACM%29%20that%20strengthens%20RGB%20conditioning%20by%20adaptively%20balancing%20semantic%20and%0Adepth%20cues%20according%20to%20their%20local%20reliability%3B%20a%20Geometric%20Cross-Modal%0AAlignment%20%28GCMA%29%20for%20robust%20RGB-LiDAR%20alignment%20under%20noisy%20diffusion%3B%20and%20a%0APanoramic%20Feature%20Coherence%20%28PFC%29%20for%20enforcing%20global%20structural%20consistency%0Aacross%20monocular%20RGB%20and%20panoramic%20LiDAR.%20Additionally%2C%20we%20introduce%20two%0Ametrics%2C%20Cross-Modal%20Semantic%20Consistency%20and%20Cross-Modal%20Depth%20Consistency%2C%20to%0Aevaluate%20alignment%20quality%20across%20modalities.%20Experiments%20on%20nuScenes%2C%0ASemanticKITTI%2C%20and%20our%20proposed%20KITTI-Weather%20benchmark%20demonstrate%20that%20Veila%0Aachieves%20state-of-the-art%20generation%20fidelity%20and%20cross-modal%20consistency%2C%0Awhile%20enabling%20generative%20data%20augmentation%20that%20improves%20downstream%20LiDAR%0Asemantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeila%253A%2520Panoramic%2520LiDAR%2520Generation%2520from%2520a%2520Monocular%2520RGB%2520Image%26entry.906535625%3DYouquan%2520Liu%2520and%2520Lingdong%2520Kong%2520and%2520Weidong%2520Yang%2520and%2520Ao%2520Liang%2520and%2520Jianxiong%2520Gao%2520and%2520Yang%2520Wu%2520and%2520Xiang%2520Xu%2520and%2520Xin%2520Li%2520and%2520Linfeng%2520Li%2520and%2520Runnan%2520Chen%2520and%2520Ben%2520Fei%26entry.1292438233%3D%2520%2520Realistic%2520and%2520controllable%2520panoramic%2520LiDAR%2520data%2520generation%2520is%2520critical%2520for%250Ascalable%25203D%2520perception%2520in%2520autonomous%2520driving%2520and%2520robotics.%2520Existing%2520methods%250Aeither%2520perform%2520unconditional%2520generation%2520with%2520poor%2520controllability%2520or%2520adopt%250Atext-guided%2520synthesis%252C%2520which%2520lacks%2520fine-grained%2520spatial%2520control.%2520Leveraging%2520a%250Amonocular%2520RGB%2520image%2520as%2520a%2520spatial%2520control%2520signal%2520offers%2520a%2520scalable%2520and%2520low-cost%250Aalternative%252C%2520which%2520remains%2520an%2520open%2520problem.%2520However%252C%2520it%2520faces%2520three%2520core%250Achallenges%253A%2520%2528i%2529%2520semantic%2520and%2520depth%2520cues%2520from%2520RGB%2520are%2520vary%2520spatially%252C%250Acomplicating%2520reliable%2520conditioning%2520generation%253B%2520%2528ii%2529%2520modality%2520gaps%2520between%2520RGB%250Aappearance%2520and%2520LiDAR%2520geometry%2520amplify%2520alignment%2520errors%2520under%2520noisy%2520diffusion%253B%250Aand%2520%2528iii%2529%2520maintaining%2520structural%2520coherence%2520between%2520monocular%2520RGB%2520and%2520panoramic%250ALiDAR%2520is%2520challenging%252C%2520particularly%2520in%2520non-overlap%2520regions%2520between%2520images%2520and%250ALiDAR.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Veila%252C%2520a%2520novel%2520conditional%250Adiffusion%2520framework%2520that%2520integrates%253A%2520a%2520Confidence-Aware%2520Conditioning%2520Mechanism%250A%2528CACM%2529%2520that%2520strengthens%2520RGB%2520conditioning%2520by%2520adaptively%2520balancing%2520semantic%2520and%250Adepth%2520cues%2520according%2520to%2520their%2520local%2520reliability%253B%2520a%2520Geometric%2520Cross-Modal%250AAlignment%2520%2528GCMA%2529%2520for%2520robust%2520RGB-LiDAR%2520alignment%2520under%2520noisy%2520diffusion%253B%2520and%2520a%250APanoramic%2520Feature%2520Coherence%2520%2528PFC%2529%2520for%2520enforcing%2520global%2520structural%2520consistency%250Aacross%2520monocular%2520RGB%2520and%2520panoramic%2520LiDAR.%2520Additionally%252C%2520we%2520introduce%2520two%250Ametrics%252C%2520Cross-Modal%2520Semantic%2520Consistency%2520and%2520Cross-Modal%2520Depth%2520Consistency%252C%2520to%250Aevaluate%2520alignment%2520quality%2520across%2520modalities.%2520Experiments%2520on%2520nuScenes%252C%250ASemanticKITTI%252C%2520and%2520our%2520proposed%2520KITTI-Weather%2520benchmark%2520demonstrate%2520that%2520Veila%250Aachieves%2520state-of-the-art%2520generation%2520fidelity%2520and%2520cross-modal%2520consistency%252C%250Awhile%2520enabling%2520generative%2520data%2520augmentation%2520that%2520improves%2520downstream%2520LiDAR%250Asemantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Veila%3A%20Panoramic%20LiDAR%20Generation%20from%20a%20Monocular%20RGB%20Image&entry.906535625=Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Weidong%20Yang%20and%20Ao%20Liang%20and%20Jianxiong%20Gao%20and%20Yang%20Wu%20and%20Xiang%20Xu%20and%20Xin%20Li%20and%20Linfeng%20Li%20and%20Runnan%20Chen%20and%20Ben%20Fei&entry.1292438233=%20%20Realistic%20and%20controllable%20panoramic%20LiDAR%20data%20generation%20is%20critical%20for%0Ascalable%203D%20perception%20in%20autonomous%20driving%20and%20robotics.%20Existing%20methods%0Aeither%20perform%20unconditional%20generation%20with%20poor%20controllability%20or%20adopt%0Atext-guided%20synthesis%2C%20which%20lacks%20fine-grained%20spatial%20control.%20Leveraging%20a%0Amonocular%20RGB%20image%20as%20a%20spatial%20control%20signal%20offers%20a%20scalable%20and%20low-cost%0Aalternative%2C%20which%20remains%20an%20open%20problem.%20However%2C%20it%20faces%20three%20core%0Achallenges%3A%20%28i%29%20semantic%20and%20depth%20cues%20from%20RGB%20are%20vary%20spatially%2C%0Acomplicating%20reliable%20conditioning%20generation%3B%20%28ii%29%20modality%20gaps%20between%20RGB%0Aappearance%20and%20LiDAR%20geometry%20amplify%20alignment%20errors%20under%20noisy%20diffusion%3B%0Aand%20%28iii%29%20maintaining%20structural%20coherence%20between%20monocular%20RGB%20and%20panoramic%0ALiDAR%20is%20challenging%2C%20particularly%20in%20non-overlap%20regions%20between%20images%20and%0ALiDAR.%20To%20address%20these%20challenges%2C%20we%20propose%20Veila%2C%20a%20novel%20conditional%0Adiffusion%20framework%20that%20integrates%3A%20a%20Confidence-Aware%20Conditioning%20Mechanism%0A%28CACM%29%20that%20strengthens%20RGB%20conditioning%20by%20adaptively%20balancing%20semantic%20and%0Adepth%20cues%20according%20to%20their%20local%20reliability%3B%20a%20Geometric%20Cross-Modal%0AAlignment%20%28GCMA%29%20for%20robust%20RGB-LiDAR%20alignment%20under%20noisy%20diffusion%3B%20and%20a%0APanoramic%20Feature%20Coherence%20%28PFC%29%20for%20enforcing%20global%20structural%20consistency%0Aacross%20monocular%20RGB%20and%20panoramic%20LiDAR.%20Additionally%2C%20we%20introduce%20two%0Ametrics%2C%20Cross-Modal%20Semantic%20Consistency%20and%20Cross-Modal%20Depth%20Consistency%2C%20to%0Aevaluate%20alignment%20quality%20across%20modalities.%20Experiments%20on%20nuScenes%2C%0ASemanticKITTI%2C%20and%20our%20proposed%20KITTI-Weather%20benchmark%20demonstrate%20that%20Veila%0Aachieves%20state-of-the-art%20generation%20fidelity%20and%20cross-modal%20consistency%2C%0Awhile%20enabling%20generative%20data%20augmentation%20that%20improves%20downstream%20LiDAR%0Asemantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03690v1&entry.124074799=Read"},
{"title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences", "author": "Ao Liang and Youquan Liu and Yu Yang and Dongyue Lu and Linfeng Li and Lingdong Kong and Huaici Zhao and Wei Tsang Ooi", "abstract": "  Generative world models have become essential data engines for autonomous\ndriving, yet most existing efforts focus on videos or occupancy grids,\noverlooking the unique LiDAR properties. Extending LiDAR generation to dynamic\n4D world modeling presents challenges in controllability, temporal coherence,\nand evaluation standardization. To this end, we present LiDARCrafter, a unified\nframework for 4D LiDAR generation and editing. Given free-form natural language\ninputs, we parse instructions into ego-centric scene graphs, which condition a\ntri-branch diffusion network to generate object structures, motion\ntrajectories, and geometry. These structured conditions enable diverse and\nfine-grained scene editing. Additionally, an autoregressive module generates\ntemporally coherent 4D LiDAR sequences with smooth transitions. To support\nstandardized evaluation, we establish a comprehensive benchmark with diverse\nmetrics spanning scene-, object-, and sequence-level aspects. Experiments on\nthe nuScenes dataset using this benchmark demonstrate that LiDARCrafter\nachieves state-of-the-art performance in fidelity, controllability, and\ntemporal consistency across all levels, paving the way for data augmentation\nand simulation. The code and benchmark are released to the community.\n", "link": "http://arxiv.org/abs/2508.03692v1", "date": "2025-08-05", "relevancy": 2.442, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6136}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6136}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDARCrafter%3A%20Dynamic%204D%20World%20Modeling%20from%20LiDAR%20Sequences&body=Title%3A%20LiDARCrafter%3A%20Dynamic%204D%20World%20Modeling%20from%20LiDAR%20Sequences%0AAuthor%3A%20Ao%20Liang%20and%20Youquan%20Liu%20and%20Yu%20Yang%20and%20Dongyue%20Lu%20and%20Linfeng%20Li%20and%20Lingdong%20Kong%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20Generative%20world%20models%20have%20become%20essential%20data%20engines%20for%20autonomous%0Adriving%2C%20yet%20most%20existing%20efforts%20focus%20on%20videos%20or%20occupancy%20grids%2C%0Aoverlooking%20the%20unique%20LiDAR%20properties.%20Extending%20LiDAR%20generation%20to%20dynamic%0A4D%20world%20modeling%20presents%20challenges%20in%20controllability%2C%20temporal%20coherence%2C%0Aand%20evaluation%20standardization.%20To%20this%20end%2C%20we%20present%20LiDARCrafter%2C%20a%20unified%0Aframework%20for%204D%20LiDAR%20generation%20and%20editing.%20Given%20free-form%20natural%20language%0Ainputs%2C%20we%20parse%20instructions%20into%20ego-centric%20scene%20graphs%2C%20which%20condition%20a%0Atri-branch%20diffusion%20network%20to%20generate%20object%20structures%2C%20motion%0Atrajectories%2C%20and%20geometry.%20These%20structured%20conditions%20enable%20diverse%20and%0Afine-grained%20scene%20editing.%20Additionally%2C%20an%20autoregressive%20module%20generates%0Atemporally%20coherent%204D%20LiDAR%20sequences%20with%20smooth%20transitions.%20To%20support%0Astandardized%20evaluation%2C%20we%20establish%20a%20comprehensive%20benchmark%20with%20diverse%0Ametrics%20spanning%20scene-%2C%20object-%2C%20and%20sequence-level%20aspects.%20Experiments%20on%0Athe%20nuScenes%20dataset%20using%20this%20benchmark%20demonstrate%20that%20LiDARCrafter%0Aachieves%20state-of-the-art%20performance%20in%20fidelity%2C%20controllability%2C%20and%0Atemporal%20consistency%20across%20all%20levels%2C%20paving%20the%20way%20for%20data%20augmentation%0Aand%20simulation.%20The%20code%20and%20benchmark%20are%20released%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDARCrafter%253A%2520Dynamic%25204D%2520World%2520Modeling%2520from%2520LiDAR%2520Sequences%26entry.906535625%3DAo%2520Liang%2520and%2520Youquan%2520Liu%2520and%2520Yu%2520Yang%2520and%2520Dongyue%2520Lu%2520and%2520Linfeng%2520Li%2520and%2520Lingdong%2520Kong%2520and%2520Huaici%2520Zhao%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520Generative%2520world%2520models%2520have%2520become%2520essential%2520data%2520engines%2520for%2520autonomous%250Adriving%252C%2520yet%2520most%2520existing%2520efforts%2520focus%2520on%2520videos%2520or%2520occupancy%2520grids%252C%250Aoverlooking%2520the%2520unique%2520LiDAR%2520properties.%2520Extending%2520LiDAR%2520generation%2520to%2520dynamic%250A4D%2520world%2520modeling%2520presents%2520challenges%2520in%2520controllability%252C%2520temporal%2520coherence%252C%250Aand%2520evaluation%2520standardization.%2520To%2520this%2520end%252C%2520we%2520present%2520LiDARCrafter%252C%2520a%2520unified%250Aframework%2520for%25204D%2520LiDAR%2520generation%2520and%2520editing.%2520Given%2520free-form%2520natural%2520language%250Ainputs%252C%2520we%2520parse%2520instructions%2520into%2520ego-centric%2520scene%2520graphs%252C%2520which%2520condition%2520a%250Atri-branch%2520diffusion%2520network%2520to%2520generate%2520object%2520structures%252C%2520motion%250Atrajectories%252C%2520and%2520geometry.%2520These%2520structured%2520conditions%2520enable%2520diverse%2520and%250Afine-grained%2520scene%2520editing.%2520Additionally%252C%2520an%2520autoregressive%2520module%2520generates%250Atemporally%2520coherent%25204D%2520LiDAR%2520sequences%2520with%2520smooth%2520transitions.%2520To%2520support%250Astandardized%2520evaluation%252C%2520we%2520establish%2520a%2520comprehensive%2520benchmark%2520with%2520diverse%250Ametrics%2520spanning%2520scene-%252C%2520object-%252C%2520and%2520sequence-level%2520aspects.%2520Experiments%2520on%250Athe%2520nuScenes%2520dataset%2520using%2520this%2520benchmark%2520demonstrate%2520that%2520LiDARCrafter%250Aachieves%2520state-of-the-art%2520performance%2520in%2520fidelity%252C%2520controllability%252C%2520and%250Atemporal%2520consistency%2520across%2520all%2520levels%252C%2520paving%2520the%2520way%2520for%2520data%2520augmentation%250Aand%2520simulation.%2520The%2520code%2520and%2520benchmark%2520are%2520released%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDARCrafter%3A%20Dynamic%204D%20World%20Modeling%20from%20LiDAR%20Sequences&entry.906535625=Ao%20Liang%20and%20Youquan%20Liu%20and%20Yu%20Yang%20and%20Dongyue%20Lu%20and%20Linfeng%20Li%20and%20Lingdong%20Kong%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20Generative%20world%20models%20have%20become%20essential%20data%20engines%20for%20autonomous%0Adriving%2C%20yet%20most%20existing%20efforts%20focus%20on%20videos%20or%20occupancy%20grids%2C%0Aoverlooking%20the%20unique%20LiDAR%20properties.%20Extending%20LiDAR%20generation%20to%20dynamic%0A4D%20world%20modeling%20presents%20challenges%20in%20controllability%2C%20temporal%20coherence%2C%0Aand%20evaluation%20standardization.%20To%20this%20end%2C%20we%20present%20LiDARCrafter%2C%20a%20unified%0Aframework%20for%204D%20LiDAR%20generation%20and%20editing.%20Given%20free-form%20natural%20language%0Ainputs%2C%20we%20parse%20instructions%20into%20ego-centric%20scene%20graphs%2C%20which%20condition%20a%0Atri-branch%20diffusion%20network%20to%20generate%20object%20structures%2C%20motion%0Atrajectories%2C%20and%20geometry.%20These%20structured%20conditions%20enable%20diverse%20and%0Afine-grained%20scene%20editing.%20Additionally%2C%20an%20autoregressive%20module%20generates%0Atemporally%20coherent%204D%20LiDAR%20sequences%20with%20smooth%20transitions.%20To%20support%0Astandardized%20evaluation%2C%20we%20establish%20a%20comprehensive%20benchmark%20with%20diverse%0Ametrics%20spanning%20scene-%2C%20object-%2C%20and%20sequence-level%20aspects.%20Experiments%20on%0Athe%20nuScenes%20dataset%20using%20this%20benchmark%20demonstrate%20that%20LiDARCrafter%0Aachieves%20state-of-the-art%20performance%20in%20fidelity%2C%20controllability%2C%20and%0Atemporal%20consistency%20across%20all%20levels%2C%20paving%20the%20way%20for%20data%20augmentation%0Aand%20simulation.%20The%20code%20and%20benchmark%20are%20released%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03692v1&entry.124074799=Read"},
{"title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data", "author": "Youquan Liu and Lingdong Kong and Weidong Yang and Xin Li and Ao Liang and Runnan Chen and Ben Fei and Tongliang Liu", "abstract": "  Controllable generation of realistic LiDAR scenes is crucial for applications\nsuch as autonomous driving and robotics. While recent diffusion-based models\nachieve high-fidelity LiDAR generation, they lack explicit control over\nforeground objects and spatial relationships, limiting their usefulness for\nscenario simulation and safety validation. To address these limitations, we\npropose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a\nnovel layout-guided generative framework that introduces semantic-enhanced\nscene graph diffusion with relation-aware contextual conditioning for\nstructured LiDAR layout generation, followed by foreground-aware control\ninjection for complete scene generation. This enables customizable control over\nobject placement while ensuring spatial and semantic consistency. To support\nour structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two\nlarge-scale LiDAR scene graph datasets, along with new evaluation metrics for\nlayout synthesis. Extensive experiments demonstrate that La La LiDAR achieves\nstate-of-the-art performance in both LiDAR generation and downstream perception\ntasks, establishing a new benchmark for controllable 3D scene generation.\n", "link": "http://arxiv.org/abs/2508.03691v1", "date": "2025-08-05", "relevancy": 2.3964, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.639}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.571}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20La%20La%20LiDAR%3A%20Large-Scale%20Layout%20Generation%20from%20LiDAR%20Data&body=Title%3A%20La%20La%20LiDAR%3A%20Large-Scale%20Layout%20Generation%20from%20LiDAR%20Data%0AAuthor%3A%20Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Weidong%20Yang%20and%20Xin%20Li%20and%20Ao%20Liang%20and%20Runnan%20Chen%20and%20Ben%20Fei%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20Controllable%20generation%20of%20realistic%20LiDAR%20scenes%20is%20crucial%20for%20applications%0Asuch%20as%20autonomous%20driving%20and%20robotics.%20While%20recent%20diffusion-based%20models%0Aachieve%20high-fidelity%20LiDAR%20generation%2C%20they%20lack%20explicit%20control%20over%0Aforeground%20objects%20and%20spatial%20relationships%2C%20limiting%20their%20usefulness%20for%0Ascenario%20simulation%20and%20safety%20validation.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Large-scale%20Layout-guided%20LiDAR%20generation%20model%20%28%22La%20La%20LiDAR%22%29%2C%20a%0Anovel%20layout-guided%20generative%20framework%20that%20introduces%20semantic-enhanced%0Ascene%20graph%20diffusion%20with%20relation-aware%20contextual%20conditioning%20for%0Astructured%20LiDAR%20layout%20generation%2C%20followed%20by%20foreground-aware%20control%0Ainjection%20for%20complete%20scene%20generation.%20This%20enables%20customizable%20control%20over%0Aobject%20placement%20while%20ensuring%20spatial%20and%20semantic%20consistency.%20To%20support%0Aour%20structured%20LiDAR%20generation%2C%20we%20introduce%20Waymo-SG%20and%20nuScenes-SG%2C%20two%0Alarge-scale%20LiDAR%20scene%20graph%20datasets%2C%20along%20with%20new%20evaluation%20metrics%20for%0Alayout%20synthesis.%20Extensive%20experiments%20demonstrate%20that%20La%20La%20LiDAR%20achieves%0Astate-of-the-art%20performance%20in%20both%20LiDAR%20generation%20and%20downstream%20perception%0Atasks%2C%20establishing%20a%20new%20benchmark%20for%20controllable%203D%20scene%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLa%2520La%2520LiDAR%253A%2520Large-Scale%2520Layout%2520Generation%2520from%2520LiDAR%2520Data%26entry.906535625%3DYouquan%2520Liu%2520and%2520Lingdong%2520Kong%2520and%2520Weidong%2520Yang%2520and%2520Xin%2520Li%2520and%2520Ao%2520Liang%2520and%2520Runnan%2520Chen%2520and%2520Ben%2520Fei%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520Controllable%2520generation%2520of%2520realistic%2520LiDAR%2520scenes%2520is%2520crucial%2520for%2520applications%250Asuch%2520as%2520autonomous%2520driving%2520and%2520robotics.%2520While%2520recent%2520diffusion-based%2520models%250Aachieve%2520high-fidelity%2520LiDAR%2520generation%252C%2520they%2520lack%2520explicit%2520control%2520over%250Aforeground%2520objects%2520and%2520spatial%2520relationships%252C%2520limiting%2520their%2520usefulness%2520for%250Ascenario%2520simulation%2520and%2520safety%2520validation.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Large-scale%2520Layout-guided%2520LiDAR%2520generation%2520model%2520%2528%2522La%2520La%2520LiDAR%2522%2529%252C%2520a%250Anovel%2520layout-guided%2520generative%2520framework%2520that%2520introduces%2520semantic-enhanced%250Ascene%2520graph%2520diffusion%2520with%2520relation-aware%2520contextual%2520conditioning%2520for%250Astructured%2520LiDAR%2520layout%2520generation%252C%2520followed%2520by%2520foreground-aware%2520control%250Ainjection%2520for%2520complete%2520scene%2520generation.%2520This%2520enables%2520customizable%2520control%2520over%250Aobject%2520placement%2520while%2520ensuring%2520spatial%2520and%2520semantic%2520consistency.%2520To%2520support%250Aour%2520structured%2520LiDAR%2520generation%252C%2520we%2520introduce%2520Waymo-SG%2520and%2520nuScenes-SG%252C%2520two%250Alarge-scale%2520LiDAR%2520scene%2520graph%2520datasets%252C%2520along%2520with%2520new%2520evaluation%2520metrics%2520for%250Alayout%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520that%2520La%2520La%2520LiDAR%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520LiDAR%2520generation%2520and%2520downstream%2520perception%250Atasks%252C%2520establishing%2520a%2520new%2520benchmark%2520for%2520controllable%25203D%2520scene%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=La%20La%20LiDAR%3A%20Large-Scale%20Layout%20Generation%20from%20LiDAR%20Data&entry.906535625=Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Weidong%20Yang%20and%20Xin%20Li%20and%20Ao%20Liang%20and%20Runnan%20Chen%20and%20Ben%20Fei%20and%20Tongliang%20Liu&entry.1292438233=%20%20Controllable%20generation%20of%20realistic%20LiDAR%20scenes%20is%20crucial%20for%20applications%0Asuch%20as%20autonomous%20driving%20and%20robotics.%20While%20recent%20diffusion-based%20models%0Aachieve%20high-fidelity%20LiDAR%20generation%2C%20they%20lack%20explicit%20control%20over%0Aforeground%20objects%20and%20spatial%20relationships%2C%20limiting%20their%20usefulness%20for%0Ascenario%20simulation%20and%20safety%20validation.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Large-scale%20Layout-guided%20LiDAR%20generation%20model%20%28%22La%20La%20LiDAR%22%29%2C%20a%0Anovel%20layout-guided%20generative%20framework%20that%20introduces%20semantic-enhanced%0Ascene%20graph%20diffusion%20with%20relation-aware%20contextual%20conditioning%20for%0Astructured%20LiDAR%20layout%20generation%2C%20followed%20by%20foreground-aware%20control%0Ainjection%20for%20complete%20scene%20generation.%20This%20enables%20customizable%20control%20over%0Aobject%20placement%20while%20ensuring%20spatial%20and%20semantic%20consistency.%20To%20support%0Aour%20structured%20LiDAR%20generation%2C%20we%20introduce%20Waymo-SG%20and%20nuScenes-SG%2C%20two%0Alarge-scale%20LiDAR%20scene%20graph%20datasets%2C%20along%20with%20new%20evaluation%20metrics%20for%0Alayout%20synthesis.%20Extensive%20experiments%20demonstrate%20that%20La%20La%20LiDAR%20achieves%0Astate-of-the-art%20performance%20in%20both%20LiDAR%20generation%20and%20downstream%20perception%0Atasks%2C%20establishing%20a%20new%20benchmark%20for%20controllable%203D%20scene%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03691v1&entry.124074799=Read"},
{"title": "Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial\n  Simultaneous Localization and Mapping", "author": "Dong Wang and Hannes Haag and Daniel Casado Herraez and Stefan May and Cyrill Stachniss and Andreas N\u00fcchter", "abstract": "  Simultaneous localization and mapping (SLAM) is a critical capability for\nautonomous systems. Traditional SLAM approaches, which often rely on visual or\nLiDAR sensors, face significant challenges in adverse conditions such as low\nlight or featureless environments. To overcome these limitations, we propose a\nnovel Doppler-aided radar-inertial and LiDAR-inertial SLAM framework that\nleverages the complementary strengths of 4D radar, FMCW LiDAR, and inertial\nmeasurement units. Our system integrates Doppler velocity measurements and\nspatial data into a tightly-coupled front-end and graph optimization back-end\nto provide enhanced ego velocity estimation, accurate odometry, and robust\nmapping. We also introduce a Doppler-based scan-matching technique to improve\nfront-end odometry in dynamic environments. In addition, our framework\nincorporates an innovative online extrinsic calibration mechanism, utilizing\nDoppler velocity and loop closure to dynamically maintain sensor alignment.\nExtensive evaluations on both public and proprietary datasets show that our\nsystem significantly outperforms state-of-the-art radar-SLAM and LiDAR-SLAM\nframeworks in terms of accuracy and robustness. To encourage further research,\nthe code of our Doppler-SLAM and our dataset are available at:\nhttps://github.com/Wayne-DWA/Doppler-SLAM.\n", "link": "http://arxiv.org/abs/2504.11634v3", "date": "2025-08-05", "relevancy": 2.3943, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.616}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping&body=Title%3A%20Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping%0AAuthor%3A%20Dong%20Wang%20and%20Hannes%20Haag%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Cyrill%20Stachniss%20and%20Andreas%20N%C3%BCchter%0AAbstract%3A%20%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20for%0Aautonomous%20systems.%20Traditional%20SLAM%20approaches%2C%20which%20often%20rely%20on%20visual%20or%0ALiDAR%20sensors%2C%20face%20significant%20challenges%20in%20adverse%20conditions%20such%20as%20low%0Alight%20or%20featureless%20environments.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Doppler-aided%20radar-inertial%20and%20LiDAR-inertial%20SLAM%20framework%20that%0Aleverages%20the%20complementary%20strengths%20of%204D%20radar%2C%20FMCW%20LiDAR%2C%20and%20inertial%0Ameasurement%20units.%20Our%20system%20integrates%20Doppler%20velocity%20measurements%20and%0Aspatial%20data%20into%20a%20tightly-coupled%20front-end%20and%20graph%20optimization%20back-end%0Ato%20provide%20enhanced%20ego%20velocity%20estimation%2C%20accurate%20odometry%2C%20and%20robust%0Amapping.%20We%20also%20introduce%20a%20Doppler-based%20scan-matching%20technique%20to%20improve%0Afront-end%20odometry%20in%20dynamic%20environments.%20In%20addition%2C%20our%20framework%0Aincorporates%20an%20innovative%20online%20extrinsic%20calibration%20mechanism%2C%20utilizing%0ADoppler%20velocity%20and%20loop%20closure%20to%20dynamically%20maintain%20sensor%20alignment.%0AExtensive%20evaluations%20on%20both%20public%20and%20proprietary%20datasets%20show%20that%20our%0Asystem%20significantly%20outperforms%20state-of-the-art%20radar-SLAM%20and%20LiDAR-SLAM%0Aframeworks%20in%20terms%20of%20accuracy%20and%20robustness.%20To%20encourage%20further%20research%2C%0Athe%20code%20of%20our%20Doppler-SLAM%20and%20our%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Wayne-DWA/Doppler-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoppler-SLAM%253A%2520Doppler-Aided%2520Radar-Inertial%2520and%2520LiDAR-Inertial%250A%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%26entry.906535625%3DDong%2520Wang%2520and%2520Hannes%2520Haag%2520and%2520Daniel%2520Casado%2520Herraez%2520and%2520Stefan%2520May%2520and%2520Cyrill%2520Stachniss%2520and%2520Andreas%2520N%25C3%25BCchter%26entry.1292438233%3D%2520%2520Simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520is%2520a%2520critical%2520capability%2520for%250Aautonomous%2520systems.%2520Traditional%2520SLAM%2520approaches%252C%2520which%2520often%2520rely%2520on%2520visual%2520or%250ALiDAR%2520sensors%252C%2520face%2520significant%2520challenges%2520in%2520adverse%2520conditions%2520such%2520as%2520low%250Alight%2520or%2520featureless%2520environments.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520Doppler-aided%2520radar-inertial%2520and%2520LiDAR-inertial%2520SLAM%2520framework%2520that%250Aleverages%2520the%2520complementary%2520strengths%2520of%25204D%2520radar%252C%2520FMCW%2520LiDAR%252C%2520and%2520inertial%250Ameasurement%2520units.%2520Our%2520system%2520integrates%2520Doppler%2520velocity%2520measurements%2520and%250Aspatial%2520data%2520into%2520a%2520tightly-coupled%2520front-end%2520and%2520graph%2520optimization%2520back-end%250Ato%2520provide%2520enhanced%2520ego%2520velocity%2520estimation%252C%2520accurate%2520odometry%252C%2520and%2520robust%250Amapping.%2520We%2520also%2520introduce%2520a%2520Doppler-based%2520scan-matching%2520technique%2520to%2520improve%250Afront-end%2520odometry%2520in%2520dynamic%2520environments.%2520In%2520addition%252C%2520our%2520framework%250Aincorporates%2520an%2520innovative%2520online%2520extrinsic%2520calibration%2520mechanism%252C%2520utilizing%250ADoppler%2520velocity%2520and%2520loop%2520closure%2520to%2520dynamically%2520maintain%2520sensor%2520alignment.%250AExtensive%2520evaluations%2520on%2520both%2520public%2520and%2520proprietary%2520datasets%2520show%2520that%2520our%250Asystem%2520significantly%2520outperforms%2520state-of-the-art%2520radar-SLAM%2520and%2520LiDAR-SLAM%250Aframeworks%2520in%2520terms%2520of%2520accuracy%2520and%2520robustness.%2520To%2520encourage%2520further%2520research%252C%250Athe%2520code%2520of%2520our%2520Doppler-SLAM%2520and%2520our%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Wayne-DWA/Doppler-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping&entry.906535625=Dong%20Wang%20and%20Hannes%20Haag%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Cyrill%20Stachniss%20and%20Andreas%20N%C3%BCchter&entry.1292438233=%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20for%0Aautonomous%20systems.%20Traditional%20SLAM%20approaches%2C%20which%20often%20rely%20on%20visual%20or%0ALiDAR%20sensors%2C%20face%20significant%20challenges%20in%20adverse%20conditions%20such%20as%20low%0Alight%20or%20featureless%20environments.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Doppler-aided%20radar-inertial%20and%20LiDAR-inertial%20SLAM%20framework%20that%0Aleverages%20the%20complementary%20strengths%20of%204D%20radar%2C%20FMCW%20LiDAR%2C%20and%20inertial%0Ameasurement%20units.%20Our%20system%20integrates%20Doppler%20velocity%20measurements%20and%0Aspatial%20data%20into%20a%20tightly-coupled%20front-end%20and%20graph%20optimization%20back-end%0Ato%20provide%20enhanced%20ego%20velocity%20estimation%2C%20accurate%20odometry%2C%20and%20robust%0Amapping.%20We%20also%20introduce%20a%20Doppler-based%20scan-matching%20technique%20to%20improve%0Afront-end%20odometry%20in%20dynamic%20environments.%20In%20addition%2C%20our%20framework%0Aincorporates%20an%20innovative%20online%20extrinsic%20calibration%20mechanism%2C%20utilizing%0ADoppler%20velocity%20and%20loop%20closure%20to%20dynamically%20maintain%20sensor%20alignment.%0AExtensive%20evaluations%20on%20both%20public%20and%20proprietary%20datasets%20show%20that%20our%0Asystem%20significantly%20outperforms%20state-of-the-art%20radar-SLAM%20and%20LiDAR-SLAM%0Aframeworks%20in%20terms%20of%20accuracy%20and%20robustness.%20To%20encourage%20further%20research%2C%0Athe%20code%20of%20our%20Doppler-SLAM%20and%20our%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Wayne-DWA/Doppler-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11634v3&entry.124074799=Read"},
{"title": "Self-Questioning Language Models", "author": "Lili Chen and Mihir Prabhudesai and Katerina Fragkiadaki and Hao Liu and Deepak Pathak", "abstract": "  Can large language models improve without external data -- by generating\ntheir own questions and answers? We hypothesize that a pre-trained language\nmodel can improve its reasoning skills given only a single prompt specifying\nthe topic (e.g., algebra word problems) and asking the model to generate its\nown questions. To do this, we propose Self-Questioning Language Models (SQLM):\nan asymmetric self-play framework where a proposer is given the topic and\ngenerates a question for a solver, who tries to answer it. Both the proposer\nand solver are trained via reinforcement learning. The proposer receives a\nreward if the problem is not too easy or too difficult, and the solver receives\na reward based on majority voting, a proxy for correctness in the absence of\nground-truth answers. For coding, the proposer can instead generate unit tests\nwhich are used for verification. We study this asymmetric self-play framework\non three benchmarks: three-digit multiplication, algebra problems from the\nOMEGA benchmark, and programming problems from Codeforces. By continually\ngenerating more interesting problems and attempting to solve them, language\nmodels can improve on downstream benchmarks without access to any curated\ntraining datasets.\n", "link": "http://arxiv.org/abs/2508.03682v1", "date": "2025-08-05", "relevancy": 2.3763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Questioning%20Language%20Models&body=Title%3A%20Self-Questioning%20Language%20Models%0AAuthor%3A%20Lili%20Chen%20and%20Mihir%20Prabhudesai%20and%20Katerina%20Fragkiadaki%20and%20Hao%20Liu%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Can%20large%20language%20models%20improve%20without%20external%20data%20--%20by%20generating%0Atheir%20own%20questions%20and%20answers%3F%20We%20hypothesize%20that%20a%20pre-trained%20language%0Amodel%20can%20improve%20its%20reasoning%20skills%20given%20only%20a%20single%20prompt%20specifying%0Athe%20topic%20%28e.g.%2C%20algebra%20word%20problems%29%20and%20asking%20the%20model%20to%20generate%20its%0Aown%20questions.%20To%20do%20this%2C%20we%20propose%20Self-Questioning%20Language%20Models%20%28SQLM%29%3A%0Aan%20asymmetric%20self-play%20framework%20where%20a%20proposer%20is%20given%20the%20topic%20and%0Agenerates%20a%20question%20for%20a%20solver%2C%20who%20tries%20to%20answer%20it.%20Both%20the%20proposer%0Aand%20solver%20are%20trained%20via%20reinforcement%20learning.%20The%20proposer%20receives%20a%0Areward%20if%20the%20problem%20is%20not%20too%20easy%20or%20too%20difficult%2C%20and%20the%20solver%20receives%0Aa%20reward%20based%20on%20majority%20voting%2C%20a%20proxy%20for%20correctness%20in%20the%20absence%20of%0Aground-truth%20answers.%20For%20coding%2C%20the%20proposer%20can%20instead%20generate%20unit%20tests%0Awhich%20are%20used%20for%20verification.%20We%20study%20this%20asymmetric%20self-play%20framework%0Aon%20three%20benchmarks%3A%20three-digit%20multiplication%2C%20algebra%20problems%20from%20the%0AOMEGA%20benchmark%2C%20and%20programming%20problems%20from%20Codeforces.%20By%20continually%0Agenerating%20more%20interesting%20problems%20and%20attempting%20to%20solve%20them%2C%20language%0Amodels%20can%20improve%20on%20downstream%20benchmarks%20without%20access%20to%20any%20curated%0Atraining%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Questioning%2520Language%2520Models%26entry.906535625%3DLili%2520Chen%2520and%2520Mihir%2520Prabhudesai%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Hao%2520Liu%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Can%2520large%2520language%2520models%2520improve%2520without%2520external%2520data%2520--%2520by%2520generating%250Atheir%2520own%2520questions%2520and%2520answers%253F%2520We%2520hypothesize%2520that%2520a%2520pre-trained%2520language%250Amodel%2520can%2520improve%2520its%2520reasoning%2520skills%2520given%2520only%2520a%2520single%2520prompt%2520specifying%250Athe%2520topic%2520%2528e.g.%252C%2520algebra%2520word%2520problems%2529%2520and%2520asking%2520the%2520model%2520to%2520generate%2520its%250Aown%2520questions.%2520To%2520do%2520this%252C%2520we%2520propose%2520Self-Questioning%2520Language%2520Models%2520%2528SQLM%2529%253A%250Aan%2520asymmetric%2520self-play%2520framework%2520where%2520a%2520proposer%2520is%2520given%2520the%2520topic%2520and%250Agenerates%2520a%2520question%2520for%2520a%2520solver%252C%2520who%2520tries%2520to%2520answer%2520it.%2520Both%2520the%2520proposer%250Aand%2520solver%2520are%2520trained%2520via%2520reinforcement%2520learning.%2520The%2520proposer%2520receives%2520a%250Areward%2520if%2520the%2520problem%2520is%2520not%2520too%2520easy%2520or%2520too%2520difficult%252C%2520and%2520the%2520solver%2520receives%250Aa%2520reward%2520based%2520on%2520majority%2520voting%252C%2520a%2520proxy%2520for%2520correctness%2520in%2520the%2520absence%2520of%250Aground-truth%2520answers.%2520For%2520coding%252C%2520the%2520proposer%2520can%2520instead%2520generate%2520unit%2520tests%250Awhich%2520are%2520used%2520for%2520verification.%2520We%2520study%2520this%2520asymmetric%2520self-play%2520framework%250Aon%2520three%2520benchmarks%253A%2520three-digit%2520multiplication%252C%2520algebra%2520problems%2520from%2520the%250AOMEGA%2520benchmark%252C%2520and%2520programming%2520problems%2520from%2520Codeforces.%2520By%2520continually%250Agenerating%2520more%2520interesting%2520problems%2520and%2520attempting%2520to%2520solve%2520them%252C%2520language%250Amodels%2520can%2520improve%2520on%2520downstream%2520benchmarks%2520without%2520access%2520to%2520any%2520curated%250Atraining%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Questioning%20Language%20Models&entry.906535625=Lili%20Chen%20and%20Mihir%20Prabhudesai%20and%20Katerina%20Fragkiadaki%20and%20Hao%20Liu%20and%20Deepak%20Pathak&entry.1292438233=%20%20Can%20large%20language%20models%20improve%20without%20external%20data%20--%20by%20generating%0Atheir%20own%20questions%20and%20answers%3F%20We%20hypothesize%20that%20a%20pre-trained%20language%0Amodel%20can%20improve%20its%20reasoning%20skills%20given%20only%20a%20single%20prompt%20specifying%0Athe%20topic%20%28e.g.%2C%20algebra%20word%20problems%29%20and%20asking%20the%20model%20to%20generate%20its%0Aown%20questions.%20To%20do%20this%2C%20we%20propose%20Self-Questioning%20Language%20Models%20%28SQLM%29%3A%0Aan%20asymmetric%20self-play%20framework%20where%20a%20proposer%20is%20given%20the%20topic%20and%0Agenerates%20a%20question%20for%20a%20solver%2C%20who%20tries%20to%20answer%20it.%20Both%20the%20proposer%0Aand%20solver%20are%20trained%20via%20reinforcement%20learning.%20The%20proposer%20receives%20a%0Areward%20if%20the%20problem%20is%20not%20too%20easy%20or%20too%20difficult%2C%20and%20the%20solver%20receives%0Aa%20reward%20based%20on%20majority%20voting%2C%20a%20proxy%20for%20correctness%20in%20the%20absence%20of%0Aground-truth%20answers.%20For%20coding%2C%20the%20proposer%20can%20instead%20generate%20unit%20tests%0Awhich%20are%20used%20for%20verification.%20We%20study%20this%20asymmetric%20self-play%20framework%0Aon%20three%20benchmarks%3A%20three-digit%20multiplication%2C%20algebra%20problems%20from%20the%0AOMEGA%20benchmark%2C%20and%20programming%20problems%20from%20Codeforces.%20By%20continually%0Agenerating%20more%20interesting%20problems%20and%20attempting%20to%20solve%20them%2C%20language%0Amodels%20can%20improve%20on%20downstream%20benchmarks%20without%20access%20to%20any%20curated%0Atraining%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03682v1&entry.124074799=Read"},
{"title": "OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the\n  Real World", "author": "Katherine Liu and Sergey Zakharov and Dian Chen and Takuya Ikeda and Greg Shakhnarovich and Adrien Gaidon and Rares Ambrus", "abstract": "  We would like to estimate the pose and full shape of an object from a single\nobservation, without assuming known 3D model or category. In this work, we\npropose OmniShape, the first method of its kind to enable probabilistic pose\nand shape estimation. OmniShape is based on the key insight that shape\ncompletion can be decoupled into two multi-modal distributions: one capturing\nhow measurements project into a normalized object reference frame defined by\nthe dataset and the other modelling a prior over object geometries represented\nas triplanar neural fields. By training separate conditional diffusion models\nfor these two distributions, we enable sampling multiple hypotheses from the\njoint pose and shape distribution. OmniShape demonstrates compelling\nperformance on challenging real world datasets. Project website:\nhttps://tri-ml.github.io/omnishape\n", "link": "http://arxiv.org/abs/2508.03669v1", "date": "2025-08-05", "relevancy": 2.3468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5936}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniShape%3A%20Zero-Shot%20Multi-Hypothesis%20Shape%20and%20Pose%20Estimation%20in%20the%0A%20%20Real%20World&body=Title%3A%20OmniShape%3A%20Zero-Shot%20Multi-Hypothesis%20Shape%20and%20Pose%20Estimation%20in%20the%0A%20%20Real%20World%0AAuthor%3A%20Katherine%20Liu%20and%20Sergey%20Zakharov%20and%20Dian%20Chen%20and%20Takuya%20Ikeda%20and%20Greg%20Shakhnarovich%20and%20Adrien%20Gaidon%20and%20Rares%20Ambrus%0AAbstract%3A%20%20%20We%20would%20like%20to%20estimate%20the%20pose%20and%20full%20shape%20of%20an%20object%20from%20a%20single%0Aobservation%2C%20without%20assuming%20known%203D%20model%20or%20category.%20In%20this%20work%2C%20we%0Apropose%20OmniShape%2C%20the%20first%20method%20of%20its%20kind%20to%20enable%20probabilistic%20pose%0Aand%20shape%20estimation.%20OmniShape%20is%20based%20on%20the%20key%20insight%20that%20shape%0Acompletion%20can%20be%20decoupled%20into%20two%20multi-modal%20distributions%3A%20one%20capturing%0Ahow%20measurements%20project%20into%20a%20normalized%20object%20reference%20frame%20defined%20by%0Athe%20dataset%20and%20the%20other%20modelling%20a%20prior%20over%20object%20geometries%20represented%0Aas%20triplanar%20neural%20fields.%20By%20training%20separate%20conditional%20diffusion%20models%0Afor%20these%20two%20distributions%2C%20we%20enable%20sampling%20multiple%20hypotheses%20from%20the%0Ajoint%20pose%20and%20shape%20distribution.%20OmniShape%20demonstrates%20compelling%0Aperformance%20on%20challenging%20real%20world%20datasets.%20Project%20website%3A%0Ahttps%3A//tri-ml.github.io/omnishape%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniShape%253A%2520Zero-Shot%2520Multi-Hypothesis%2520Shape%2520and%2520Pose%2520Estimation%2520in%2520the%250A%2520%2520Real%2520World%26entry.906535625%3DKatherine%2520Liu%2520and%2520Sergey%2520Zakharov%2520and%2520Dian%2520Chen%2520and%2520Takuya%2520Ikeda%2520and%2520Greg%2520Shakhnarovich%2520and%2520Adrien%2520Gaidon%2520and%2520Rares%2520Ambrus%26entry.1292438233%3D%2520%2520We%2520would%2520like%2520to%2520estimate%2520the%2520pose%2520and%2520full%2520shape%2520of%2520an%2520object%2520from%2520a%2520single%250Aobservation%252C%2520without%2520assuming%2520known%25203D%2520model%2520or%2520category.%2520In%2520this%2520work%252C%2520we%250Apropose%2520OmniShape%252C%2520the%2520first%2520method%2520of%2520its%2520kind%2520to%2520enable%2520probabilistic%2520pose%250Aand%2520shape%2520estimation.%2520OmniShape%2520is%2520based%2520on%2520the%2520key%2520insight%2520that%2520shape%250Acompletion%2520can%2520be%2520decoupled%2520into%2520two%2520multi-modal%2520distributions%253A%2520one%2520capturing%250Ahow%2520measurements%2520project%2520into%2520a%2520normalized%2520object%2520reference%2520frame%2520defined%2520by%250Athe%2520dataset%2520and%2520the%2520other%2520modelling%2520a%2520prior%2520over%2520object%2520geometries%2520represented%250Aas%2520triplanar%2520neural%2520fields.%2520By%2520training%2520separate%2520conditional%2520diffusion%2520models%250Afor%2520these%2520two%2520distributions%252C%2520we%2520enable%2520sampling%2520multiple%2520hypotheses%2520from%2520the%250Ajoint%2520pose%2520and%2520shape%2520distribution.%2520OmniShape%2520demonstrates%2520compelling%250Aperformance%2520on%2520challenging%2520real%2520world%2520datasets.%2520Project%2520website%253A%250Ahttps%253A//tri-ml.github.io/omnishape%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniShape%3A%20Zero-Shot%20Multi-Hypothesis%20Shape%20and%20Pose%20Estimation%20in%20the%0A%20%20Real%20World&entry.906535625=Katherine%20Liu%20and%20Sergey%20Zakharov%20and%20Dian%20Chen%20and%20Takuya%20Ikeda%20and%20Greg%20Shakhnarovich%20and%20Adrien%20Gaidon%20and%20Rares%20Ambrus&entry.1292438233=%20%20We%20would%20like%20to%20estimate%20the%20pose%20and%20full%20shape%20of%20an%20object%20from%20a%20single%0Aobservation%2C%20without%20assuming%20known%203D%20model%20or%20category.%20In%20this%20work%2C%20we%0Apropose%20OmniShape%2C%20the%20first%20method%20of%20its%20kind%20to%20enable%20probabilistic%20pose%0Aand%20shape%20estimation.%20OmniShape%20is%20based%20on%20the%20key%20insight%20that%20shape%0Acompletion%20can%20be%20decoupled%20into%20two%20multi-modal%20distributions%3A%20one%20capturing%0Ahow%20measurements%20project%20into%20a%20normalized%20object%20reference%20frame%20defined%20by%0Athe%20dataset%20and%20the%20other%20modelling%20a%20prior%20over%20object%20geometries%20represented%0Aas%20triplanar%20neural%20fields.%20By%20training%20separate%20conditional%20diffusion%20models%0Afor%20these%20two%20distributions%2C%20we%20enable%20sampling%20multiple%20hypotheses%20from%20the%0Ajoint%20pose%20and%20shape%20distribution.%20OmniShape%20demonstrates%20compelling%0Aperformance%20on%20challenging%20real%20world%20datasets.%20Project%20website%3A%0Ahttps%3A//tri-ml.github.io/omnishape%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03669v1&entry.124074799=Read"},
{"title": "Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance\n  for People who are Blind or Visually Impaired", "author": "Ruei-Che Chang and Rosiana Natalie and Wenqian Xu and Jovan Zheng Feng Yap and Anhong Guo", "abstract": "  Recent advancements in large multimodal models have provided blind or\nvisually impaired (BVI) individuals with new capabilities to interpret and\nengage with the real world through interactive systems that utilize live video\nfeeds. However, the potential benefits and challenges of such capabilities to\nsupport diverse real-world assistive tasks remain unclear. In this paper, we\npresent findings from an exploratory study with eight BVI participants.\nParticipants used ChatGPT's Advanced Voice with Video, a state-of-the-art live\nvideo AI released in late 2024, in various real-world scenarios, from locating\nobjects to recognizing visual landmarks, across unfamiliar indoor and outdoor\nenvironments. Our findings indicate that current live video AI effectively\nprovides guidance and answers for static visual scenes but falls short in\ndelivering essential live descriptions required in dynamic situations. Despite\ninaccuracies in spatial and distance information, participants leveraged the\nprovided visual information to supplement their mobility strategies. Although\nthe system was perceived as human-like due to high-quality voice interactions,\nassumptions about users' visual abilities, hallucinations, generic responses,\nand a tendency towards sycophancy led to confusion, distrust, and potential\nrisks for BVI users. Based on the results, we discuss implications for\nassistive video AI agents, including incorporating additional sensing\ncapabilities for real-world use, determining appropriate intervention timing\nbeyond turn-taking interactions, and addressing ecological and safety concerns.\n", "link": "http://arxiv.org/abs/2508.03651v1", "date": "2025-08-05", "relevancy": 2.2433, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5767}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Gaps%20in%20ChatGPT%20Live%20Video%20Chat%20for%20Real-World%20Assistance%0A%20%20for%20People%20who%20are%20Blind%20or%20Visually%20Impaired&body=Title%3A%20Probing%20the%20Gaps%20in%20ChatGPT%20Live%20Video%20Chat%20for%20Real-World%20Assistance%0A%20%20for%20People%20who%20are%20Blind%20or%20Visually%20Impaired%0AAuthor%3A%20Ruei-Che%20Chang%20and%20Rosiana%20Natalie%20and%20Wenqian%20Xu%20and%20Jovan%20Zheng%20Feng%20Yap%20and%20Anhong%20Guo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20multimodal%20models%20have%20provided%20blind%20or%0Avisually%20impaired%20%28BVI%29%20individuals%20with%20new%20capabilities%20to%20interpret%20and%0Aengage%20with%20the%20real%20world%20through%20interactive%20systems%20that%20utilize%20live%20video%0Afeeds.%20However%2C%20the%20potential%20benefits%20and%20challenges%20of%20such%20capabilities%20to%0Asupport%20diverse%20real-world%20assistive%20tasks%20remain%20unclear.%20In%20this%20paper%2C%20we%0Apresent%20findings%20from%20an%20exploratory%20study%20with%20eight%20BVI%20participants.%0AParticipants%20used%20ChatGPT%27s%20Advanced%20Voice%20with%20Video%2C%20a%20state-of-the-art%20live%0Avideo%20AI%20released%20in%20late%202024%2C%20in%20various%20real-world%20scenarios%2C%20from%20locating%0Aobjects%20to%20recognizing%20visual%20landmarks%2C%20across%20unfamiliar%20indoor%20and%20outdoor%0Aenvironments.%20Our%20findings%20indicate%20that%20current%20live%20video%20AI%20effectively%0Aprovides%20guidance%20and%20answers%20for%20static%20visual%20scenes%20but%20falls%20short%20in%0Adelivering%20essential%20live%20descriptions%20required%20in%20dynamic%20situations.%20Despite%0Ainaccuracies%20in%20spatial%20and%20distance%20information%2C%20participants%20leveraged%20the%0Aprovided%20visual%20information%20to%20supplement%20their%20mobility%20strategies.%20Although%0Athe%20system%20was%20perceived%20as%20human-like%20due%20to%20high-quality%20voice%20interactions%2C%0Aassumptions%20about%20users%27%20visual%20abilities%2C%20hallucinations%2C%20generic%20responses%2C%0Aand%20a%20tendency%20towards%20sycophancy%20led%20to%20confusion%2C%20distrust%2C%20and%20potential%0Arisks%20for%20BVI%20users.%20Based%20on%20the%20results%2C%20we%20discuss%20implications%20for%0Aassistive%20video%20AI%20agents%2C%20including%20incorporating%20additional%20sensing%0Acapabilities%20for%20real-world%20use%2C%20determining%20appropriate%20intervention%20timing%0Abeyond%20turn-taking%20interactions%2C%20and%20addressing%20ecological%20and%20safety%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Gaps%2520in%2520ChatGPT%2520Live%2520Video%2520Chat%2520for%2520Real-World%2520Assistance%250A%2520%2520for%2520People%2520who%2520are%2520Blind%2520or%2520Visually%2520Impaired%26entry.906535625%3DRuei-Che%2520Chang%2520and%2520Rosiana%2520Natalie%2520and%2520Wenqian%2520Xu%2520and%2520Jovan%2520Zheng%2520Feng%2520Yap%2520and%2520Anhong%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520multimodal%2520models%2520have%2520provided%2520blind%2520or%250Avisually%2520impaired%2520%2528BVI%2529%2520individuals%2520with%2520new%2520capabilities%2520to%2520interpret%2520and%250Aengage%2520with%2520the%2520real%2520world%2520through%2520interactive%2520systems%2520that%2520utilize%2520live%2520video%250Afeeds.%2520However%252C%2520the%2520potential%2520benefits%2520and%2520challenges%2520of%2520such%2520capabilities%2520to%250Asupport%2520diverse%2520real-world%2520assistive%2520tasks%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520findings%2520from%2520an%2520exploratory%2520study%2520with%2520eight%2520BVI%2520participants.%250AParticipants%2520used%2520ChatGPT%2527s%2520Advanced%2520Voice%2520with%2520Video%252C%2520a%2520state-of-the-art%2520live%250Avideo%2520AI%2520released%2520in%2520late%25202024%252C%2520in%2520various%2520real-world%2520scenarios%252C%2520from%2520locating%250Aobjects%2520to%2520recognizing%2520visual%2520landmarks%252C%2520across%2520unfamiliar%2520indoor%2520and%2520outdoor%250Aenvironments.%2520Our%2520findings%2520indicate%2520that%2520current%2520live%2520video%2520AI%2520effectively%250Aprovides%2520guidance%2520and%2520answers%2520for%2520static%2520visual%2520scenes%2520but%2520falls%2520short%2520in%250Adelivering%2520essential%2520live%2520descriptions%2520required%2520in%2520dynamic%2520situations.%2520Despite%250Ainaccuracies%2520in%2520spatial%2520and%2520distance%2520information%252C%2520participants%2520leveraged%2520the%250Aprovided%2520visual%2520information%2520to%2520supplement%2520their%2520mobility%2520strategies.%2520Although%250Athe%2520system%2520was%2520perceived%2520as%2520human-like%2520due%2520to%2520high-quality%2520voice%2520interactions%252C%250Aassumptions%2520about%2520users%2527%2520visual%2520abilities%252C%2520hallucinations%252C%2520generic%2520responses%252C%250Aand%2520a%2520tendency%2520towards%2520sycophancy%2520led%2520to%2520confusion%252C%2520distrust%252C%2520and%2520potential%250Arisks%2520for%2520BVI%2520users.%2520Based%2520on%2520the%2520results%252C%2520we%2520discuss%2520implications%2520for%250Aassistive%2520video%2520AI%2520agents%252C%2520including%2520incorporating%2520additional%2520sensing%250Acapabilities%2520for%2520real-world%2520use%252C%2520determining%2520appropriate%2520intervention%2520timing%250Abeyond%2520turn-taking%2520interactions%252C%2520and%2520addressing%2520ecological%2520and%2520safety%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Gaps%20in%20ChatGPT%20Live%20Video%20Chat%20for%20Real-World%20Assistance%0A%20%20for%20People%20who%20are%20Blind%20or%20Visually%20Impaired&entry.906535625=Ruei-Che%20Chang%20and%20Rosiana%20Natalie%20and%20Wenqian%20Xu%20and%20Jovan%20Zheng%20Feng%20Yap%20and%20Anhong%20Guo&entry.1292438233=%20%20Recent%20advancements%20in%20large%20multimodal%20models%20have%20provided%20blind%20or%0Avisually%20impaired%20%28BVI%29%20individuals%20with%20new%20capabilities%20to%20interpret%20and%0Aengage%20with%20the%20real%20world%20through%20interactive%20systems%20that%20utilize%20live%20video%0Afeeds.%20However%2C%20the%20potential%20benefits%20and%20challenges%20of%20such%20capabilities%20to%0Asupport%20diverse%20real-world%20assistive%20tasks%20remain%20unclear.%20In%20this%20paper%2C%20we%0Apresent%20findings%20from%20an%20exploratory%20study%20with%20eight%20BVI%20participants.%0AParticipants%20used%20ChatGPT%27s%20Advanced%20Voice%20with%20Video%2C%20a%20state-of-the-art%20live%0Avideo%20AI%20released%20in%20late%202024%2C%20in%20various%20real-world%20scenarios%2C%20from%20locating%0Aobjects%20to%20recognizing%20visual%20landmarks%2C%20across%20unfamiliar%20indoor%20and%20outdoor%0Aenvironments.%20Our%20findings%20indicate%20that%20current%20live%20video%20AI%20effectively%0Aprovides%20guidance%20and%20answers%20for%20static%20visual%20scenes%20but%20falls%20short%20in%0Adelivering%20essential%20live%20descriptions%20required%20in%20dynamic%20situations.%20Despite%0Ainaccuracies%20in%20spatial%20and%20distance%20information%2C%20participants%20leveraged%20the%0Aprovided%20visual%20information%20to%20supplement%20their%20mobility%20strategies.%20Although%0Athe%20system%20was%20perceived%20as%20human-like%20due%20to%20high-quality%20voice%20interactions%2C%0Aassumptions%20about%20users%27%20visual%20abilities%2C%20hallucinations%2C%20generic%20responses%2C%0Aand%20a%20tendency%20towards%20sycophancy%20led%20to%20confusion%2C%20distrust%2C%20and%20potential%0Arisks%20for%20BVI%20users.%20Based%20on%20the%20results%2C%20we%20discuss%20implications%20for%0Aassistive%20video%20AI%20agents%2C%20including%20incorporating%20additional%20sensing%0Acapabilities%20for%20real-world%20use%2C%20determining%20appropriate%20intervention%20timing%0Abeyond%20turn-taking%20interactions%2C%20and%20addressing%20ecological%20and%20safety%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03651v1&entry.124074799=Read"},
{"title": "AttZoom: Attention Zoom for Better Visual Features", "author": "Daniel DeAlcala and Aythami Morales and Julian Fierrez and Ruben Tolosana", "abstract": "  We present Attention Zoom, a modular and model-agnostic spatial attention\nmechanism designed to improve feature extraction in convolutional neural\nnetworks (CNNs). Unlike traditional attention approaches that require\narchitecture-specific integration, our method introduces a standalone layer\nthat spatially emphasizes high-importance regions in the input. We evaluated\nAttention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet,\nshowing consistent improvements in Top-1 and Top-5 classification accuracy.\nVisual analyses using Grad-CAM and spatial warping reveal that our method\nencourages fine-grained and diverse attention patterns. Our results confirm the\neffectiveness and generality of the proposed layer for improving CCNs with\nminimal architectural overhead.\n", "link": "http://arxiv.org/abs/2508.03625v1", "date": "2025-08-05", "relevancy": 2.2107, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttZoom%3A%20Attention%20Zoom%20for%20Better%20Visual%20Features&body=Title%3A%20AttZoom%3A%20Attention%20Zoom%20for%20Better%20Visual%20Features%0AAuthor%3A%20Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Ruben%20Tolosana%0AAbstract%3A%20%20%20We%20present%20Attention%20Zoom%2C%20a%20modular%20and%20model-agnostic%20spatial%20attention%0Amechanism%20designed%20to%20improve%20feature%20extraction%20in%20convolutional%20neural%0Anetworks%20%28CNNs%29.%20Unlike%20traditional%20attention%20approaches%20that%20require%0Aarchitecture-specific%20integration%2C%20our%20method%20introduces%20a%20standalone%20layer%0Athat%20spatially%20emphasizes%20high-importance%20regions%20in%20the%20input.%20We%20evaluated%0AAttention%20Zoom%20on%20multiple%20CNN%20backbones%20using%20CIFAR-100%20and%20TinyImageNet%2C%0Ashowing%20consistent%20improvements%20in%20Top-1%20and%20Top-5%20classification%20accuracy.%0AVisual%20analyses%20using%20Grad-CAM%20and%20spatial%20warping%20reveal%20that%20our%20method%0Aencourages%20fine-grained%20and%20diverse%20attention%20patterns.%20Our%20results%20confirm%20the%0Aeffectiveness%20and%20generality%20of%20the%20proposed%20layer%20for%20improving%20CCNs%20with%0Aminimal%20architectural%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttZoom%253A%2520Attention%2520Zoom%2520for%2520Better%2520Visual%2520Features%26entry.906535625%3DDaniel%2520DeAlcala%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Ruben%2520Tolosana%26entry.1292438233%3D%2520%2520We%2520present%2520Attention%2520Zoom%252C%2520a%2520modular%2520and%2520model-agnostic%2520spatial%2520attention%250Amechanism%2520designed%2520to%2520improve%2520feature%2520extraction%2520in%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529.%2520Unlike%2520traditional%2520attention%2520approaches%2520that%2520require%250Aarchitecture-specific%2520integration%252C%2520our%2520method%2520introduces%2520a%2520standalone%2520layer%250Athat%2520spatially%2520emphasizes%2520high-importance%2520regions%2520in%2520the%2520input.%2520We%2520evaluated%250AAttention%2520Zoom%2520on%2520multiple%2520CNN%2520backbones%2520using%2520CIFAR-100%2520and%2520TinyImageNet%252C%250Ashowing%2520consistent%2520improvements%2520in%2520Top-1%2520and%2520Top-5%2520classification%2520accuracy.%250AVisual%2520analyses%2520using%2520Grad-CAM%2520and%2520spatial%2520warping%2520reveal%2520that%2520our%2520method%250Aencourages%2520fine-grained%2520and%2520diverse%2520attention%2520patterns.%2520Our%2520results%2520confirm%2520the%250Aeffectiveness%2520and%2520generality%2520of%2520the%2520proposed%2520layer%2520for%2520improving%2520CCNs%2520with%250Aminimal%2520architectural%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttZoom%3A%20Attention%20Zoom%20for%20Better%20Visual%20Features&entry.906535625=Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Ruben%20Tolosana&entry.1292438233=%20%20We%20present%20Attention%20Zoom%2C%20a%20modular%20and%20model-agnostic%20spatial%20attention%0Amechanism%20designed%20to%20improve%20feature%20extraction%20in%20convolutional%20neural%0Anetworks%20%28CNNs%29.%20Unlike%20traditional%20attention%20approaches%20that%20require%0Aarchitecture-specific%20integration%2C%20our%20method%20introduces%20a%20standalone%20layer%0Athat%20spatially%20emphasizes%20high-importance%20regions%20in%20the%20input.%20We%20evaluated%0AAttention%20Zoom%20on%20multiple%20CNN%20backbones%20using%20CIFAR-100%20and%20TinyImageNet%2C%0Ashowing%20consistent%20improvements%20in%20Top-1%20and%20Top-5%20classification%20accuracy.%0AVisual%20analyses%20using%20Grad-CAM%20and%20spatial%20warping%20reveal%20that%20our%20method%0Aencourages%20fine-grained%20and%20diverse%20attention%20patterns.%20Our%20results%20confirm%20the%0Aeffectiveness%20and%20generality%20of%20the%20proposed%20layer%20for%20improving%20CCNs%20with%0Aminimal%20architectural%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03625v1&entry.124074799=Read"},
{"title": "Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland\n  Waterways", "author": "Zhongbi Luo and Yunjia Wang and Jan Swevers and Peter Slaets and Herman Bruyninckx", "abstract": "  Accurate geospatial information is crucial for safe, autonomous Inland\nWaterway Transport (IWT), as existing charts (IENC) lack real-time detail and\nconventional LiDAR SLAM fails in waterway environments. These challenges lead\nto vertical drift and non-semantic maps, hindering autonomous navigation.\n  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It\nuses an improved feature extraction and a water surface planar constraint to\nmitigate vertical drift. A novel pipeline transforms 3D point clouds into\nstructured 2D semantic maps using voxel-based geometric analysis, enabling\nreal-time computation of navigational parameters like bridge clearances. An\nautomated module extracts shorelines and exports them into a lightweight,\nIENC-compatible format.\n  Evaluations on a real-world dataset show Inland-LOAM achieves superior\nlocalization accuracy over state-of-the-art methods. The generated semantic\nmaps and shorelines align with real-world conditions, providing reliable data\nfor enhanced situational awareness. The code and dataset will be publicly\navailable\n", "link": "http://arxiv.org/abs/2508.03672v1", "date": "2025-08-05", "relevancy": 2.21, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6092}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inland-LOAM%3A%20Voxel-Based%20Structural%20Semantic%20Mapping%20for%20Inland%0A%20%20Waterways&body=Title%3A%20Inland-LOAM%3A%20Voxel-Based%20Structural%20Semantic%20Mapping%20for%20Inland%0A%20%20Waterways%0AAuthor%3A%20Zhongbi%20Luo%20and%20Yunjia%20Wang%20and%20Jan%20Swevers%20and%20Peter%20Slaets%20and%20Herman%20Bruyninckx%0AAbstract%3A%20%20%20Accurate%20geospatial%20information%20is%20crucial%20for%20safe%2C%20autonomous%20Inland%0AWaterway%20Transport%20%28IWT%29%2C%20as%20existing%20charts%20%28IENC%29%20lack%20real-time%20detail%20and%0Aconventional%20LiDAR%20SLAM%20fails%20in%20waterway%20environments.%20These%20challenges%20lead%0Ato%20vertical%20drift%20and%20non-semantic%20maps%2C%20hindering%20autonomous%20navigation.%0A%20%20This%20paper%20introduces%20Inland-LOAM%2C%20a%20LiDAR%20SLAM%20framework%20for%20waterways.%20It%0Auses%20an%20improved%20feature%20extraction%20and%20a%20water%20surface%20planar%20constraint%20to%0Amitigate%20vertical%20drift.%20A%20novel%20pipeline%20transforms%203D%20point%20clouds%20into%0Astructured%202D%20semantic%20maps%20using%20voxel-based%20geometric%20analysis%2C%20enabling%0Areal-time%20computation%20of%20navigational%20parameters%20like%20bridge%20clearances.%20An%0Aautomated%20module%20extracts%20shorelines%20and%20exports%20them%20into%20a%20lightweight%2C%0AIENC-compatible%20format.%0A%20%20Evaluations%20on%20a%20real-world%20dataset%20show%20Inland-LOAM%20achieves%20superior%0Alocalization%20accuracy%20over%20state-of-the-art%20methods.%20The%20generated%20semantic%0Amaps%20and%20shorelines%20align%20with%20real-world%20conditions%2C%20providing%20reliable%20data%0Afor%20enhanced%20situational%20awareness.%20The%20code%20and%20dataset%20will%20be%20publicly%0Aavailable%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInland-LOAM%253A%2520Voxel-Based%2520Structural%2520Semantic%2520Mapping%2520for%2520Inland%250A%2520%2520Waterways%26entry.906535625%3DZhongbi%2520Luo%2520and%2520Yunjia%2520Wang%2520and%2520Jan%2520Swevers%2520and%2520Peter%2520Slaets%2520and%2520Herman%2520Bruyninckx%26entry.1292438233%3D%2520%2520Accurate%2520geospatial%2520information%2520is%2520crucial%2520for%2520safe%252C%2520autonomous%2520Inland%250AWaterway%2520Transport%2520%2528IWT%2529%252C%2520as%2520existing%2520charts%2520%2528IENC%2529%2520lack%2520real-time%2520detail%2520and%250Aconventional%2520LiDAR%2520SLAM%2520fails%2520in%2520waterway%2520environments.%2520These%2520challenges%2520lead%250Ato%2520vertical%2520drift%2520and%2520non-semantic%2520maps%252C%2520hindering%2520autonomous%2520navigation.%250A%2520%2520This%2520paper%2520introduces%2520Inland-LOAM%252C%2520a%2520LiDAR%2520SLAM%2520framework%2520for%2520waterways.%2520It%250Auses%2520an%2520improved%2520feature%2520extraction%2520and%2520a%2520water%2520surface%2520planar%2520constraint%2520to%250Amitigate%2520vertical%2520drift.%2520A%2520novel%2520pipeline%2520transforms%25203D%2520point%2520clouds%2520into%250Astructured%25202D%2520semantic%2520maps%2520using%2520voxel-based%2520geometric%2520analysis%252C%2520enabling%250Areal-time%2520computation%2520of%2520navigational%2520parameters%2520like%2520bridge%2520clearances.%2520An%250Aautomated%2520module%2520extracts%2520shorelines%2520and%2520exports%2520them%2520into%2520a%2520lightweight%252C%250AIENC-compatible%2520format.%250A%2520%2520Evaluations%2520on%2520a%2520real-world%2520dataset%2520show%2520Inland-LOAM%2520achieves%2520superior%250Alocalization%2520accuracy%2520over%2520state-of-the-art%2520methods.%2520The%2520generated%2520semantic%250Amaps%2520and%2520shorelines%2520align%2520with%2520real-world%2520conditions%252C%2520providing%2520reliable%2520data%250Afor%2520enhanced%2520situational%2520awareness.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520publicly%250Aavailable%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inland-LOAM%3A%20Voxel-Based%20Structural%20Semantic%20Mapping%20for%20Inland%0A%20%20Waterways&entry.906535625=Zhongbi%20Luo%20and%20Yunjia%20Wang%20and%20Jan%20Swevers%20and%20Peter%20Slaets%20and%20Herman%20Bruyninckx&entry.1292438233=%20%20Accurate%20geospatial%20information%20is%20crucial%20for%20safe%2C%20autonomous%20Inland%0AWaterway%20Transport%20%28IWT%29%2C%20as%20existing%20charts%20%28IENC%29%20lack%20real-time%20detail%20and%0Aconventional%20LiDAR%20SLAM%20fails%20in%20waterway%20environments.%20These%20challenges%20lead%0Ato%20vertical%20drift%20and%20non-semantic%20maps%2C%20hindering%20autonomous%20navigation.%0A%20%20This%20paper%20introduces%20Inland-LOAM%2C%20a%20LiDAR%20SLAM%20framework%20for%20waterways.%20It%0Auses%20an%20improved%20feature%20extraction%20and%20a%20water%20surface%20planar%20constraint%20to%0Amitigate%20vertical%20drift.%20A%20novel%20pipeline%20transforms%203D%20point%20clouds%20into%0Astructured%202D%20semantic%20maps%20using%20voxel-based%20geometric%20analysis%2C%20enabling%0Areal-time%20computation%20of%20navigational%20parameters%20like%20bridge%20clearances.%20An%0Aautomated%20module%20extracts%20shorelines%20and%20exports%20them%20into%20a%20lightweight%2C%0AIENC-compatible%20format.%0A%20%20Evaluations%20on%20a%20real-world%20dataset%20show%20Inland-LOAM%20achieves%20superior%0Alocalization%20accuracy%20over%20state-of-the-art%20methods.%20The%20generated%20semantic%0Amaps%20and%20shorelines%20align%20with%20real-world%20conditions%2C%20providing%20reliable%20data%0Afor%20enhanced%20situational%20awareness.%20The%20code%20and%20dataset%20will%20be%20publicly%0Aavailable%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03672v1&entry.124074799=Read"},
{"title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action\n  Recognition", "author": "Pulkit Kumar and Shuaiyi Huang and Matthew Walmer and Sai Saketh Rambhatla and Abhinav Shrivastava", "abstract": "  Video understanding requires effective modeling of both motion and appearance\ninformation, particularly for few-shot action recognition. While recent\nadvances in point tracking have been shown to improve few-shot action\nrecognition, two fundamental challenges persist: selecting informative points\nto track and effectively modeling their motion patterns. We present Trokens, a\nnovel approach that transforms trajectory points into semantic-aware relational\ntokens for action recognition. First, we introduce a semantic-aware sampling\nstrategy to adaptively distribute tracking points based on object scale and\nsemantic relevance. Second, we develop a motion modeling framework that\ncaptures both intra-trajectory dynamics through the Histogram of Oriented\nDisplacements (HoD) and inter-trajectory relationships to model complex action\npatterns. Our approach effectively combines these trajectory tokens with\nsemantic features to enhance appearance features with motion information,\nachieving state-of-the-art performance across six diverse few-shot action\nrecognition benchmarks: Something-Something-V2 (both full and small splits),\nKinetics, UCF101, HMDB51, and FineGym. For project page see\nhttps://trokens-iccv25.github.io\n", "link": "http://arxiv.org/abs/2508.03695v1", "date": "2025-08-05", "relevancy": 2.1789, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trokens%3A%20Semantic-Aware%20Relational%20Trajectory%20Tokens%20for%20Few-Shot%20Action%0A%20%20Recognition&body=Title%3A%20Trokens%3A%20Semantic-Aware%20Relational%20Trajectory%20Tokens%20for%20Few-Shot%20Action%0A%20%20Recognition%0AAuthor%3A%20Pulkit%20Kumar%20and%20Shuaiyi%20Huang%20and%20Matthew%20Walmer%20and%20Sai%20Saketh%20Rambhatla%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Video%20understanding%20requires%20effective%20modeling%20of%20both%20motion%20and%20appearance%0Ainformation%2C%20particularly%20for%20few-shot%20action%20recognition.%20While%20recent%0Aadvances%20in%20point%20tracking%20have%20been%20shown%20to%20improve%20few-shot%20action%0Arecognition%2C%20two%20fundamental%20challenges%20persist%3A%20selecting%20informative%20points%0Ato%20track%20and%20effectively%20modeling%20their%20motion%20patterns.%20We%20present%20Trokens%2C%20a%0Anovel%20approach%20that%20transforms%20trajectory%20points%20into%20semantic-aware%20relational%0Atokens%20for%20action%20recognition.%20First%2C%20we%20introduce%20a%20semantic-aware%20sampling%0Astrategy%20to%20adaptively%20distribute%20tracking%20points%20based%20on%20object%20scale%20and%0Asemantic%20relevance.%20Second%2C%20we%20develop%20a%20motion%20modeling%20framework%20that%0Acaptures%20both%20intra-trajectory%20dynamics%20through%20the%20Histogram%20of%20Oriented%0ADisplacements%20%28HoD%29%20and%20inter-trajectory%20relationships%20to%20model%20complex%20action%0Apatterns.%20Our%20approach%20effectively%20combines%20these%20trajectory%20tokens%20with%0Asemantic%20features%20to%20enhance%20appearance%20features%20with%20motion%20information%2C%0Aachieving%20state-of-the-art%20performance%20across%20six%20diverse%20few-shot%20action%0Arecognition%20benchmarks%3A%20Something-Something-V2%20%28both%20full%20and%20small%20splits%29%2C%0AKinetics%2C%20UCF101%2C%20HMDB51%2C%20and%20FineGym.%20For%20project%20page%20see%0Ahttps%3A//trokens-iccv25.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrokens%253A%2520Semantic-Aware%2520Relational%2520Trajectory%2520Tokens%2520for%2520Few-Shot%2520Action%250A%2520%2520Recognition%26entry.906535625%3DPulkit%2520Kumar%2520and%2520Shuaiyi%2520Huang%2520and%2520Matthew%2520Walmer%2520and%2520Sai%2520Saketh%2520Rambhatla%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Video%2520understanding%2520requires%2520effective%2520modeling%2520of%2520both%2520motion%2520and%2520appearance%250Ainformation%252C%2520particularly%2520for%2520few-shot%2520action%2520recognition.%2520While%2520recent%250Aadvances%2520in%2520point%2520tracking%2520have%2520been%2520shown%2520to%2520improve%2520few-shot%2520action%250Arecognition%252C%2520two%2520fundamental%2520challenges%2520persist%253A%2520selecting%2520informative%2520points%250Ato%2520track%2520and%2520effectively%2520modeling%2520their%2520motion%2520patterns.%2520We%2520present%2520Trokens%252C%2520a%250Anovel%2520approach%2520that%2520transforms%2520trajectory%2520points%2520into%2520semantic-aware%2520relational%250Atokens%2520for%2520action%2520recognition.%2520First%252C%2520we%2520introduce%2520a%2520semantic-aware%2520sampling%250Astrategy%2520to%2520adaptively%2520distribute%2520tracking%2520points%2520based%2520on%2520object%2520scale%2520and%250Asemantic%2520relevance.%2520Second%252C%2520we%2520develop%2520a%2520motion%2520modeling%2520framework%2520that%250Acaptures%2520both%2520intra-trajectory%2520dynamics%2520through%2520the%2520Histogram%2520of%2520Oriented%250ADisplacements%2520%2528HoD%2529%2520and%2520inter-trajectory%2520relationships%2520to%2520model%2520complex%2520action%250Apatterns.%2520Our%2520approach%2520effectively%2520combines%2520these%2520trajectory%2520tokens%2520with%250Asemantic%2520features%2520to%2520enhance%2520appearance%2520features%2520with%2520motion%2520information%252C%250Aachieving%2520state-of-the-art%2520performance%2520across%2520six%2520diverse%2520few-shot%2520action%250Arecognition%2520benchmarks%253A%2520Something-Something-V2%2520%2528both%2520full%2520and%2520small%2520splits%2529%252C%250AKinetics%252C%2520UCF101%252C%2520HMDB51%252C%2520and%2520FineGym.%2520For%2520project%2520page%2520see%250Ahttps%253A//trokens-iccv25.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trokens%3A%20Semantic-Aware%20Relational%20Trajectory%20Tokens%20for%20Few-Shot%20Action%0A%20%20Recognition&entry.906535625=Pulkit%20Kumar%20and%20Shuaiyi%20Huang%20and%20Matthew%20Walmer%20and%20Sai%20Saketh%20Rambhatla%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Video%20understanding%20requires%20effective%20modeling%20of%20both%20motion%20and%20appearance%0Ainformation%2C%20particularly%20for%20few-shot%20action%20recognition.%20While%20recent%0Aadvances%20in%20point%20tracking%20have%20been%20shown%20to%20improve%20few-shot%20action%0Arecognition%2C%20two%20fundamental%20challenges%20persist%3A%20selecting%20informative%20points%0Ato%20track%20and%20effectively%20modeling%20their%20motion%20patterns.%20We%20present%20Trokens%2C%20a%0Anovel%20approach%20that%20transforms%20trajectory%20points%20into%20semantic-aware%20relational%0Atokens%20for%20action%20recognition.%20First%2C%20we%20introduce%20a%20semantic-aware%20sampling%0Astrategy%20to%20adaptively%20distribute%20tracking%20points%20based%20on%20object%20scale%20and%0Asemantic%20relevance.%20Second%2C%20we%20develop%20a%20motion%20modeling%20framework%20that%0Acaptures%20both%20intra-trajectory%20dynamics%20through%20the%20Histogram%20of%20Oriented%0ADisplacements%20%28HoD%29%20and%20inter-trajectory%20relationships%20to%20model%20complex%20action%0Apatterns.%20Our%20approach%20effectively%20combines%20these%20trajectory%20tokens%20with%0Asemantic%20features%20to%20enhance%20appearance%20features%20with%20motion%20information%2C%0Aachieving%20state-of-the-art%20performance%20across%20six%20diverse%20few-shot%20action%0Arecognition%20benchmarks%3A%20Something-Something-V2%20%28both%20full%20and%20small%20splits%29%2C%0AKinetics%2C%20UCF101%2C%20HMDB51%2C%20and%20FineGym.%20For%20project%20page%20see%0Ahttps%3A//trokens-iccv25.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03695v1&entry.124074799=Read"},
{"title": "Personalized Recommendation of Dish and Restaurant Collections on iFood", "author": "Fernando F. Granado and Davi A. Bezerra and Iuri Queiroz and Nathan Oliveira and Pedro Fernandes and Bruno Schock", "abstract": "  Food delivery platforms face the challenge of helping users navigate vast\ncatalogs of restaurants and dishes to find meals they truly enjoy. This paper\npresents RED, an automated recommendation system designed for iFood, Latin\nAmerica's largest on-demand food delivery platform, to personalize the\nselection of curated food collections displayed to millions of users. Our\napproach employs a LightGBM classifier that scores collections based on three\nfeature groups: collection characteristics, user-collection similarity, and\ncontextual information. To address the cold-start problem of recommending newly\ncreated collections, we develop content-based representations using item\nembeddings and implement monotonicity constraints to improve generalization. We\ntackle data scarcity by bootstrapping from category carousel interactions and\naddress visibility bias through unbiased sampling of impressions and purchases\nin production. The system demonstrates significant real-world impact through\nextensive A/B testing with 5-10% of iFood's user base. Online results of our\nA/B tests add up to 97% improvement in Card Conversion Rate and 1.4% increase\nin overall App Conversion Rate compared to popularity-based baselines. Notably,\nour offline accuracy metrics strongly correlate with online performance,\nenabling reliable impact prediction before deployment. To our knowledge, this\nis the first work to detail large-scale recommendation of curated food\ncollections in a dynamic commercial environment.\n", "link": "http://arxiv.org/abs/2508.03670v1", "date": "2025-08-05", "relevancy": 2.1753, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4358}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Recommendation%20of%20Dish%20and%20Restaurant%20Collections%20on%20iFood&body=Title%3A%20Personalized%20Recommendation%20of%20Dish%20and%20Restaurant%20Collections%20on%20iFood%0AAuthor%3A%20Fernando%20F.%20Granado%20and%20Davi%20A.%20Bezerra%20and%20Iuri%20Queiroz%20and%20Nathan%20Oliveira%20and%20Pedro%20Fernandes%20and%20Bruno%20Schock%0AAbstract%3A%20%20%20Food%20delivery%20platforms%20face%20the%20challenge%20of%20helping%20users%20navigate%20vast%0Acatalogs%20of%20restaurants%20and%20dishes%20to%20find%20meals%20they%20truly%20enjoy.%20This%20paper%0Apresents%20RED%2C%20an%20automated%20recommendation%20system%20designed%20for%20iFood%2C%20Latin%0AAmerica%27s%20largest%20on-demand%20food%20delivery%20platform%2C%20to%20personalize%20the%0Aselection%20of%20curated%20food%20collections%20displayed%20to%20millions%20of%20users.%20Our%0Aapproach%20employs%20a%20LightGBM%20classifier%20that%20scores%20collections%20based%20on%20three%0Afeature%20groups%3A%20collection%20characteristics%2C%20user-collection%20similarity%2C%20and%0Acontextual%20information.%20To%20address%20the%20cold-start%20problem%20of%20recommending%20newly%0Acreated%20collections%2C%20we%20develop%20content-based%20representations%20using%20item%0Aembeddings%20and%20implement%20monotonicity%20constraints%20to%20improve%20generalization.%20We%0Atackle%20data%20scarcity%20by%20bootstrapping%20from%20category%20carousel%20interactions%20and%0Aaddress%20visibility%20bias%20through%20unbiased%20sampling%20of%20impressions%20and%20purchases%0Ain%20production.%20The%20system%20demonstrates%20significant%20real-world%20impact%20through%0Aextensive%20A/B%20testing%20with%205-10%25%20of%20iFood%27s%20user%20base.%20Online%20results%20of%20our%0AA/B%20tests%20add%20up%20to%2097%25%20improvement%20in%20Card%20Conversion%20Rate%20and%201.4%25%20increase%0Ain%20overall%20App%20Conversion%20Rate%20compared%20to%20popularity-based%20baselines.%20Notably%2C%0Aour%20offline%20accuracy%20metrics%20strongly%20correlate%20with%20online%20performance%2C%0Aenabling%20reliable%20impact%20prediction%20before%20deployment.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20to%20detail%20large-scale%20recommendation%20of%20curated%20food%0Acollections%20in%20a%20dynamic%20commercial%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Recommendation%2520of%2520Dish%2520and%2520Restaurant%2520Collections%2520on%2520iFood%26entry.906535625%3DFernando%2520F.%2520Granado%2520and%2520Davi%2520A.%2520Bezerra%2520and%2520Iuri%2520Queiroz%2520and%2520Nathan%2520Oliveira%2520and%2520Pedro%2520Fernandes%2520and%2520Bruno%2520Schock%26entry.1292438233%3D%2520%2520Food%2520delivery%2520platforms%2520face%2520the%2520challenge%2520of%2520helping%2520users%2520navigate%2520vast%250Acatalogs%2520of%2520restaurants%2520and%2520dishes%2520to%2520find%2520meals%2520they%2520truly%2520enjoy.%2520This%2520paper%250Apresents%2520RED%252C%2520an%2520automated%2520recommendation%2520system%2520designed%2520for%2520iFood%252C%2520Latin%250AAmerica%2527s%2520largest%2520on-demand%2520food%2520delivery%2520platform%252C%2520to%2520personalize%2520the%250Aselection%2520of%2520curated%2520food%2520collections%2520displayed%2520to%2520millions%2520of%2520users.%2520Our%250Aapproach%2520employs%2520a%2520LightGBM%2520classifier%2520that%2520scores%2520collections%2520based%2520on%2520three%250Afeature%2520groups%253A%2520collection%2520characteristics%252C%2520user-collection%2520similarity%252C%2520and%250Acontextual%2520information.%2520To%2520address%2520the%2520cold-start%2520problem%2520of%2520recommending%2520newly%250Acreated%2520collections%252C%2520we%2520develop%2520content-based%2520representations%2520using%2520item%250Aembeddings%2520and%2520implement%2520monotonicity%2520constraints%2520to%2520improve%2520generalization.%2520We%250Atackle%2520data%2520scarcity%2520by%2520bootstrapping%2520from%2520category%2520carousel%2520interactions%2520and%250Aaddress%2520visibility%2520bias%2520through%2520unbiased%2520sampling%2520of%2520impressions%2520and%2520purchases%250Ain%2520production.%2520The%2520system%2520demonstrates%2520significant%2520real-world%2520impact%2520through%250Aextensive%2520A/B%2520testing%2520with%25205-10%2525%2520of%2520iFood%2527s%2520user%2520base.%2520Online%2520results%2520of%2520our%250AA/B%2520tests%2520add%2520up%2520to%252097%2525%2520improvement%2520in%2520Card%2520Conversion%2520Rate%2520and%25201.4%2525%2520increase%250Ain%2520overall%2520App%2520Conversion%2520Rate%2520compared%2520to%2520popularity-based%2520baselines.%2520Notably%252C%250Aour%2520offline%2520accuracy%2520metrics%2520strongly%2520correlate%2520with%2520online%2520performance%252C%250Aenabling%2520reliable%2520impact%2520prediction%2520before%2520deployment.%2520To%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520work%2520to%2520detail%2520large-scale%2520recommendation%2520of%2520curated%2520food%250Acollections%2520in%2520a%2520dynamic%2520commercial%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Recommendation%20of%20Dish%20and%20Restaurant%20Collections%20on%20iFood&entry.906535625=Fernando%20F.%20Granado%20and%20Davi%20A.%20Bezerra%20and%20Iuri%20Queiroz%20and%20Nathan%20Oliveira%20and%20Pedro%20Fernandes%20and%20Bruno%20Schock&entry.1292438233=%20%20Food%20delivery%20platforms%20face%20the%20challenge%20of%20helping%20users%20navigate%20vast%0Acatalogs%20of%20restaurants%20and%20dishes%20to%20find%20meals%20they%20truly%20enjoy.%20This%20paper%0Apresents%20RED%2C%20an%20automated%20recommendation%20system%20designed%20for%20iFood%2C%20Latin%0AAmerica%27s%20largest%20on-demand%20food%20delivery%20platform%2C%20to%20personalize%20the%0Aselection%20of%20curated%20food%20collections%20displayed%20to%20millions%20of%20users.%20Our%0Aapproach%20employs%20a%20LightGBM%20classifier%20that%20scores%20collections%20based%20on%20three%0Afeature%20groups%3A%20collection%20characteristics%2C%20user-collection%20similarity%2C%20and%0Acontextual%20information.%20To%20address%20the%20cold-start%20problem%20of%20recommending%20newly%0Acreated%20collections%2C%20we%20develop%20content-based%20representations%20using%20item%0Aembeddings%20and%20implement%20monotonicity%20constraints%20to%20improve%20generalization.%20We%0Atackle%20data%20scarcity%20by%20bootstrapping%20from%20category%20carousel%20interactions%20and%0Aaddress%20visibility%20bias%20through%20unbiased%20sampling%20of%20impressions%20and%20purchases%0Ain%20production.%20The%20system%20demonstrates%20significant%20real-world%20impact%20through%0Aextensive%20A/B%20testing%20with%205-10%25%20of%20iFood%27s%20user%20base.%20Online%20results%20of%20our%0AA/B%20tests%20add%20up%20to%2097%25%20improvement%20in%20Card%20Conversion%20Rate%20and%201.4%25%20increase%0Ain%20overall%20App%20Conversion%20Rate%20compared%20to%20popularity-based%20baselines.%20Notably%2C%0Aour%20offline%20accuracy%20metrics%20strongly%20correlate%20with%20online%20performance%2C%0Aenabling%20reliable%20impact%20prediction%20before%20deployment.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20to%20detail%20large-scale%20recommendation%20of%20curated%20food%0Acollections%20in%20a%20dynamic%20commercial%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03670v1&entry.124074799=Read"},
{"title": "Cross-Model Semantics in Representation Learning", "author": "Saleh Nikooroo and Thomas Engel", "abstract": "  The internal representations learned by deep networks are often sensitive to\narchitecture-specific choices, raising questions about the stability,\nalignment, and transferability of learned structure across models. In this\npaper, we investigate how structural constraints--such as linear shaping\noperators and corrective paths--affect the compatibility of internal\nrepresentations across different architectures. Building on the insights from\nprior studies on structured transformations and convergence, we develop a\nframework for measuring and analyzing representational alignment across\nnetworks with distinct but related architectural priors. Through a combination\nof theoretical insights, empirical probes, and controlled transfer experiments,\nwe demonstrate that structural regularities induce representational geometry\nthat is more stable under architectural variation. This suggests that certain\nforms of inductive bias not only support generalization within a model, but\nalso improve the interoperability of learned features across models. We\nconclude with a discussion on the implications of representational\ntransferability for model distillation, modular learning, and the principled\ndesign of robust learning systems.\n", "link": "http://arxiv.org/abs/2508.03649v1", "date": "2025-08-05", "relevancy": 2.1607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Model%20Semantics%20in%20Representation%20Learning&body=Title%3A%20Cross-Model%20Semantics%20in%20Representation%20Learning%0AAuthor%3A%20Saleh%20Nikooroo%20and%20Thomas%20Engel%0AAbstract%3A%20%20%20The%20internal%20representations%20learned%20by%20deep%20networks%20are%20often%20sensitive%20to%0Aarchitecture-specific%20choices%2C%20raising%20questions%20about%20the%20stability%2C%0Aalignment%2C%20and%20transferability%20of%20learned%20structure%20across%20models.%20In%20this%0Apaper%2C%20we%20investigate%20how%20structural%20constraints--such%20as%20linear%20shaping%0Aoperators%20and%20corrective%20paths--affect%20the%20compatibility%20of%20internal%0Arepresentations%20across%20different%20architectures.%20Building%20on%20the%20insights%20from%0Aprior%20studies%20on%20structured%20transformations%20and%20convergence%2C%20we%20develop%20a%0Aframework%20for%20measuring%20and%20analyzing%20representational%20alignment%20across%0Anetworks%20with%20distinct%20but%20related%20architectural%20priors.%20Through%20a%20combination%0Aof%20theoretical%20insights%2C%20empirical%20probes%2C%20and%20controlled%20transfer%20experiments%2C%0Awe%20demonstrate%20that%20structural%20regularities%20induce%20representational%20geometry%0Athat%20is%20more%20stable%20under%20architectural%20variation.%20This%20suggests%20that%20certain%0Aforms%20of%20inductive%20bias%20not%20only%20support%20generalization%20within%20a%20model%2C%20but%0Aalso%20improve%20the%20interoperability%20of%20learned%20features%20across%20models.%20We%0Aconclude%20with%20a%20discussion%20on%20the%20implications%20of%20representational%0Atransferability%20for%20model%20distillation%2C%20modular%20learning%2C%20and%20the%20principled%0Adesign%20of%20robust%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Model%2520Semantics%2520in%2520Representation%2520Learning%26entry.906535625%3DSaleh%2520Nikooroo%2520and%2520Thomas%2520Engel%26entry.1292438233%3D%2520%2520The%2520internal%2520representations%2520learned%2520by%2520deep%2520networks%2520are%2520often%2520sensitive%2520to%250Aarchitecture-specific%2520choices%252C%2520raising%2520questions%2520about%2520the%2520stability%252C%250Aalignment%252C%2520and%2520transferability%2520of%2520learned%2520structure%2520across%2520models.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520how%2520structural%2520constraints--such%2520as%2520linear%2520shaping%250Aoperators%2520and%2520corrective%2520paths--affect%2520the%2520compatibility%2520of%2520internal%250Arepresentations%2520across%2520different%2520architectures.%2520Building%2520on%2520the%2520insights%2520from%250Aprior%2520studies%2520on%2520structured%2520transformations%2520and%2520convergence%252C%2520we%2520develop%2520a%250Aframework%2520for%2520measuring%2520and%2520analyzing%2520representational%2520alignment%2520across%250Anetworks%2520with%2520distinct%2520but%2520related%2520architectural%2520priors.%2520Through%2520a%2520combination%250Aof%2520theoretical%2520insights%252C%2520empirical%2520probes%252C%2520and%2520controlled%2520transfer%2520experiments%252C%250Awe%2520demonstrate%2520that%2520structural%2520regularities%2520induce%2520representational%2520geometry%250Athat%2520is%2520more%2520stable%2520under%2520architectural%2520variation.%2520This%2520suggests%2520that%2520certain%250Aforms%2520of%2520inductive%2520bias%2520not%2520only%2520support%2520generalization%2520within%2520a%2520model%252C%2520but%250Aalso%2520improve%2520the%2520interoperability%2520of%2520learned%2520features%2520across%2520models.%2520We%250Aconclude%2520with%2520a%2520discussion%2520on%2520the%2520implications%2520of%2520representational%250Atransferability%2520for%2520model%2520distillation%252C%2520modular%2520learning%252C%2520and%2520the%2520principled%250Adesign%2520of%2520robust%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Model%20Semantics%20in%20Representation%20Learning&entry.906535625=Saleh%20Nikooroo%20and%20Thomas%20Engel&entry.1292438233=%20%20The%20internal%20representations%20learned%20by%20deep%20networks%20are%20often%20sensitive%20to%0Aarchitecture-specific%20choices%2C%20raising%20questions%20about%20the%20stability%2C%0Aalignment%2C%20and%20transferability%20of%20learned%20structure%20across%20models.%20In%20this%0Apaper%2C%20we%20investigate%20how%20structural%20constraints--such%20as%20linear%20shaping%0Aoperators%20and%20corrective%20paths--affect%20the%20compatibility%20of%20internal%0Arepresentations%20across%20different%20architectures.%20Building%20on%20the%20insights%20from%0Aprior%20studies%20on%20structured%20transformations%20and%20convergence%2C%20we%20develop%20a%0Aframework%20for%20measuring%20and%20analyzing%20representational%20alignment%20across%0Anetworks%20with%20distinct%20but%20related%20architectural%20priors.%20Through%20a%20combination%0Aof%20theoretical%20insights%2C%20empirical%20probes%2C%20and%20controlled%20transfer%20experiments%2C%0Awe%20demonstrate%20that%20structural%20regularities%20induce%20representational%20geometry%0Athat%20is%20more%20stable%20under%20architectural%20variation.%20This%20suggests%20that%20certain%0Aforms%20of%20inductive%20bias%20not%20only%20support%20generalization%20within%20a%20model%2C%20but%0Aalso%20improve%20the%20interoperability%20of%20learned%20features%20across%20models.%20We%0Aconclude%20with%20a%20discussion%20on%20the%20implications%20of%20representational%0Atransferability%20for%20model%20distillation%2C%20modular%20learning%2C%20and%20the%20principled%0Adesign%20of%20robust%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03649v1&entry.124074799=Read"},
{"title": "Physical Degradation Model-Guided Interferometric Hyperspectral\n  Reconstruction with Unfolding Transformer", "author": "Yuansheng Li and Yunhao Zou and Linwei Chen and Ying Fu", "abstract": "  Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method.The code and\nare available at https://github.com/bit1120203554/IHRUT.\n", "link": "http://arxiv.org/abs/2506.21880v2", "date": "2025-08-05", "relevancy": 2.1389, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5417}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5388}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physical%20Degradation%20Model-Guided%20Interferometric%20Hyperspectral%0A%20%20Reconstruction%20with%20Unfolding%20Transformer&body=Title%3A%20Physical%20Degradation%20Model-Guided%20Interferometric%20Hyperspectral%0A%20%20Reconstruction%20with%20Unfolding%20Transformer%0AAuthor%3A%20Yuansheng%20Li%20and%20Yunhao%20Zou%20and%20Linwei%20Chen%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Interferometric%20Hyperspectral%20Imaging%20%28IHI%29%20is%20a%20critical%20technique%20for%0Alarge-scale%20remote%20sensing%20tasks%20due%20to%20its%20advantages%20in%20flux%20and%20spectral%0Aresolution.%20However%2C%20IHI%20is%20susceptible%20to%20complex%20errors%20arising%20from%20imaging%0Asteps%2C%20and%20its%20quality%20is%20limited%20by%20existing%20signal%20processing-based%0Areconstruction%20algorithms.%20Two%20key%20challenges%20hinder%20performance%20enhancement%3A%0A1%29%20the%20lack%20of%20training%20datasets.%202%29%20the%20difficulty%20in%20eliminating%20IHI-specific%0Adegradation%20components%20through%20learning-based%20methods.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20IHI%20reconstruction%20pipeline.%20First%2C%20based%20on%0Aimaging%20physics%20and%20radiometric%20calibration%20data%2C%20we%20establish%20a%20simplified%20yet%0Aaccurate%20IHI%20degradation%20model%20and%20a%20parameter%20estimation%20method.%20This%20model%0Aenables%20the%20synthesis%20of%20realistic%20IHI%20training%20datasets%20from%20hyperspectral%0Aimages%20%28HSIs%29%2C%20bridging%20the%20gap%20between%20IHI%20reconstruction%20and%20deep%20learning.%0ASecond%2C%20we%20design%20the%20Interferometric%20Hyperspectral%20Reconstruction%20Unfolding%0ATransformer%20%28IHRUT%29%2C%20which%20achieves%20effective%20spectral%20correction%20and%20detail%0Arestoration%20through%20a%20stripe-pattern%20enhancement%20mechanism%20and%20a%0Aspatial-spectral%20transformer%20architecture.%20Experimental%20results%20demonstrate%20the%0Asuperior%20performance%20and%20generalization%20capability%20of%20our%20method.The%20code%20and%0Aare%20available%20at%20https%3A//github.com/bit1120203554/IHRUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysical%2520Degradation%2520Model-Guided%2520Interferometric%2520Hyperspectral%250A%2520%2520Reconstruction%2520with%2520Unfolding%2520Transformer%26entry.906535625%3DYuansheng%2520Li%2520and%2520Yunhao%2520Zou%2520and%2520Linwei%2520Chen%2520and%2520Ying%2520Fu%26entry.1292438233%3D%2520%2520Interferometric%2520Hyperspectral%2520Imaging%2520%2528IHI%2529%2520is%2520a%2520critical%2520technique%2520for%250Alarge-scale%2520remote%2520sensing%2520tasks%2520due%2520to%2520its%2520advantages%2520in%2520flux%2520and%2520spectral%250Aresolution.%2520However%252C%2520IHI%2520is%2520susceptible%2520to%2520complex%2520errors%2520arising%2520from%2520imaging%250Asteps%252C%2520and%2520its%2520quality%2520is%2520limited%2520by%2520existing%2520signal%2520processing-based%250Areconstruction%2520algorithms.%2520Two%2520key%2520challenges%2520hinder%2520performance%2520enhancement%253A%250A1%2529%2520the%2520lack%2520of%2520training%2520datasets.%25202%2529%2520the%2520difficulty%2520in%2520eliminating%2520IHI-specific%250Adegradation%2520components%2520through%2520learning-based%2520methods.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520IHI%2520reconstruction%2520pipeline.%2520First%252C%2520based%2520on%250Aimaging%2520physics%2520and%2520radiometric%2520calibration%2520data%252C%2520we%2520establish%2520a%2520simplified%2520yet%250Aaccurate%2520IHI%2520degradation%2520model%2520and%2520a%2520parameter%2520estimation%2520method.%2520This%2520model%250Aenables%2520the%2520synthesis%2520of%2520realistic%2520IHI%2520training%2520datasets%2520from%2520hyperspectral%250Aimages%2520%2528HSIs%2529%252C%2520bridging%2520the%2520gap%2520between%2520IHI%2520reconstruction%2520and%2520deep%2520learning.%250ASecond%252C%2520we%2520design%2520the%2520Interferometric%2520Hyperspectral%2520Reconstruction%2520Unfolding%250ATransformer%2520%2528IHRUT%2529%252C%2520which%2520achieves%2520effective%2520spectral%2520correction%2520and%2520detail%250Arestoration%2520through%2520a%2520stripe-pattern%2520enhancement%2520mechanism%2520and%2520a%250Aspatial-spectral%2520transformer%2520architecture.%2520Experimental%2520results%2520demonstrate%2520the%250Asuperior%2520performance%2520and%2520generalization%2520capability%2520of%2520our%2520method.The%2520code%2520and%250Aare%2520available%2520at%2520https%253A//github.com/bit1120203554/IHRUT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physical%20Degradation%20Model-Guided%20Interferometric%20Hyperspectral%0A%20%20Reconstruction%20with%20Unfolding%20Transformer&entry.906535625=Yuansheng%20Li%20and%20Yunhao%20Zou%20and%20Linwei%20Chen%20and%20Ying%20Fu&entry.1292438233=%20%20Interferometric%20Hyperspectral%20Imaging%20%28IHI%29%20is%20a%20critical%20technique%20for%0Alarge-scale%20remote%20sensing%20tasks%20due%20to%20its%20advantages%20in%20flux%20and%20spectral%0Aresolution.%20However%2C%20IHI%20is%20susceptible%20to%20complex%20errors%20arising%20from%20imaging%0Asteps%2C%20and%20its%20quality%20is%20limited%20by%20existing%20signal%20processing-based%0Areconstruction%20algorithms.%20Two%20key%20challenges%20hinder%20performance%20enhancement%3A%0A1%29%20the%20lack%20of%20training%20datasets.%202%29%20the%20difficulty%20in%20eliminating%20IHI-specific%0Adegradation%20components%20through%20learning-based%20methods.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20IHI%20reconstruction%20pipeline.%20First%2C%20based%20on%0Aimaging%20physics%20and%20radiometric%20calibration%20data%2C%20we%20establish%20a%20simplified%20yet%0Aaccurate%20IHI%20degradation%20model%20and%20a%20parameter%20estimation%20method.%20This%20model%0Aenables%20the%20synthesis%20of%20realistic%20IHI%20training%20datasets%20from%20hyperspectral%0Aimages%20%28HSIs%29%2C%20bridging%20the%20gap%20between%20IHI%20reconstruction%20and%20deep%20learning.%0ASecond%2C%20we%20design%20the%20Interferometric%20Hyperspectral%20Reconstruction%20Unfolding%0ATransformer%20%28IHRUT%29%2C%20which%20achieves%20effective%20spectral%20correction%20and%20detail%0Arestoration%20through%20a%20stripe-pattern%20enhancement%20mechanism%20and%20a%0Aspatial-spectral%20transformer%20architecture.%20Experimental%20results%20demonstrate%20the%0Asuperior%20performance%20and%20generalization%20capability%20of%20our%20method.The%20code%20and%0Aare%20available%20at%20https%3A//github.com/bit1120203554/IHRUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21880v2&entry.124074799=Read"},
{"title": "What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning", "author": "Chi-Hsi Kung and Frangil Ramirez and Juhyung Ha and Yi-Ting Chen and David Crandall and Yi-Hsuan Tsai", "abstract": "  Understanding a procedural activity requires modeling both how action steps\ntransform the scene, and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by modeling the temporal\norder of actions, but has not explicitly learned the state changes (scene\ntransformations). In this work, we study procedure-aware video representation\nlearning by incorporating state-change descriptions generated by Large Language\nModels (LLMs) as supervision signals for video encoders. Moreover, we generate\nstate-change counterfactuals that simulate hypothesized failure outcomes,\nallowing models to learn by imagining unseen \"What if\" scenarios. This\ncounterfactual reasoning facilitates the model's ability to understand the\ncause and effect of each step in an activity. We conduct extensive experiments\non procedure-aware tasks, including temporal action segmentation, error\ndetection, action phase classification, frame retrieval, multi-instance\nretrieval, and action recognition. Our results demonstrate the effectiveness of\nthe proposed state-change descriptions and their counterfactuals, and achieve\nsignificant improvements on multiple tasks. Code is available at\nhttps://github.com/HCIS- Lab/counterfactual-video-pretrain.\n", "link": "http://arxiv.org/abs/2503.21055v5", "date": "2025-08-05", "relevancy": 2.1275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Changed%20and%20What%20Could%20Have%20Changed%3F%20State-Change%20Counterfactuals%0A%20%20for%20Procedure-Aware%20Video%20Representation%20Learning&body=Title%3A%20What%20Changed%20and%20What%20Could%20Have%20Changed%3F%20State-Change%20Counterfactuals%0A%20%20for%20Procedure-Aware%20Video%20Representation%20Learning%0AAuthor%3A%20Chi-Hsi%20Kung%20and%20Frangil%20Ramirez%20and%20Juhyung%20Ha%20and%20Yi-Ting%20Chen%20and%20David%20Crandall%20and%20Yi-Hsuan%20Tsai%0AAbstract%3A%20%20%20Understanding%20a%20procedural%20activity%20requires%20modeling%20both%20how%20action%20steps%0Atransform%20the%20scene%2C%20and%20how%20evolving%20scene%20transformations%20can%20influence%20the%0Asequence%20of%20action%20steps%2C%20even%20those%20that%20are%20accidental%20or%20erroneous.%20Existing%0Awork%20has%20studied%20procedure-aware%20video%20representations%20by%20modeling%20the%20temporal%0Aorder%20of%20actions%2C%20but%20has%20not%20explicitly%20learned%20the%20state%20changes%20%28scene%0Atransformations%29.%20In%20this%20work%2C%20we%20study%20procedure-aware%20video%20representation%0Alearning%20by%20incorporating%20state-change%20descriptions%20generated%20by%20Large%20Language%0AModels%20%28LLMs%29%20as%20supervision%20signals%20for%20video%20encoders.%20Moreover%2C%20we%20generate%0Astate-change%20counterfactuals%20that%20simulate%20hypothesized%20failure%20outcomes%2C%0Aallowing%20models%20to%20learn%20by%20imagining%20unseen%20%22What%20if%22%20scenarios.%20This%0Acounterfactual%20reasoning%20facilitates%20the%20model%27s%20ability%20to%20understand%20the%0Acause%20and%20effect%20of%20each%20step%20in%20an%20activity.%20We%20conduct%20extensive%20experiments%0Aon%20procedure-aware%20tasks%2C%20including%20temporal%20action%20segmentation%2C%20error%0Adetection%2C%20action%20phase%20classification%2C%20frame%20retrieval%2C%20multi-instance%0Aretrieval%2C%20and%20action%20recognition.%20Our%20results%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20state-change%20descriptions%20and%20their%20counterfactuals%2C%20and%20achieve%0Asignificant%20improvements%20on%20multiple%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/HCIS-%20Lab/counterfactual-video-pretrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21055v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Changed%2520and%2520What%2520Could%2520Have%2520Changed%253F%2520State-Change%2520Counterfactuals%250A%2520%2520for%2520Procedure-Aware%2520Video%2520Representation%2520Learning%26entry.906535625%3DChi-Hsi%2520Kung%2520and%2520Frangil%2520Ramirez%2520and%2520Juhyung%2520Ha%2520and%2520Yi-Ting%2520Chen%2520and%2520David%2520Crandall%2520and%2520Yi-Hsuan%2520Tsai%26entry.1292438233%3D%2520%2520Understanding%2520a%2520procedural%2520activity%2520requires%2520modeling%2520both%2520how%2520action%2520steps%250Atransform%2520the%2520scene%252C%2520and%2520how%2520evolving%2520scene%2520transformations%2520can%2520influence%2520the%250Asequence%2520of%2520action%2520steps%252C%2520even%2520those%2520that%2520are%2520accidental%2520or%2520erroneous.%2520Existing%250Awork%2520has%2520studied%2520procedure-aware%2520video%2520representations%2520by%2520modeling%2520the%2520temporal%250Aorder%2520of%2520actions%252C%2520but%2520has%2520not%2520explicitly%2520learned%2520the%2520state%2520changes%2520%2528scene%250Atransformations%2529.%2520In%2520this%2520work%252C%2520we%2520study%2520procedure-aware%2520video%2520representation%250Alearning%2520by%2520incorporating%2520state-change%2520descriptions%2520generated%2520by%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520as%2520supervision%2520signals%2520for%2520video%2520encoders.%2520Moreover%252C%2520we%2520generate%250Astate-change%2520counterfactuals%2520that%2520simulate%2520hypothesized%2520failure%2520outcomes%252C%250Aallowing%2520models%2520to%2520learn%2520by%2520imagining%2520unseen%2520%2522What%2520if%2522%2520scenarios.%2520This%250Acounterfactual%2520reasoning%2520facilitates%2520the%2520model%2527s%2520ability%2520to%2520understand%2520the%250Acause%2520and%2520effect%2520of%2520each%2520step%2520in%2520an%2520activity.%2520We%2520conduct%2520extensive%2520experiments%250Aon%2520procedure-aware%2520tasks%252C%2520including%2520temporal%2520action%2520segmentation%252C%2520error%250Adetection%252C%2520action%2520phase%2520classification%252C%2520frame%2520retrieval%252C%2520multi-instance%250Aretrieval%252C%2520and%2520action%2520recognition.%2520Our%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520state-change%2520descriptions%2520and%2520their%2520counterfactuals%252C%2520and%2520achieve%250Asignificant%2520improvements%2520on%2520multiple%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/HCIS-%2520Lab/counterfactual-video-pretrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21055v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Changed%20and%20What%20Could%20Have%20Changed%3F%20State-Change%20Counterfactuals%0A%20%20for%20Procedure-Aware%20Video%20Representation%20Learning&entry.906535625=Chi-Hsi%20Kung%20and%20Frangil%20Ramirez%20and%20Juhyung%20Ha%20and%20Yi-Ting%20Chen%20and%20David%20Crandall%20and%20Yi-Hsuan%20Tsai&entry.1292438233=%20%20Understanding%20a%20procedural%20activity%20requires%20modeling%20both%20how%20action%20steps%0Atransform%20the%20scene%2C%20and%20how%20evolving%20scene%20transformations%20can%20influence%20the%0Asequence%20of%20action%20steps%2C%20even%20those%20that%20are%20accidental%20or%20erroneous.%20Existing%0Awork%20has%20studied%20procedure-aware%20video%20representations%20by%20modeling%20the%20temporal%0Aorder%20of%20actions%2C%20but%20has%20not%20explicitly%20learned%20the%20state%20changes%20%28scene%0Atransformations%29.%20In%20this%20work%2C%20we%20study%20procedure-aware%20video%20representation%0Alearning%20by%20incorporating%20state-change%20descriptions%20generated%20by%20Large%20Language%0AModels%20%28LLMs%29%20as%20supervision%20signals%20for%20video%20encoders.%20Moreover%2C%20we%20generate%0Astate-change%20counterfactuals%20that%20simulate%20hypothesized%20failure%20outcomes%2C%0Aallowing%20models%20to%20learn%20by%20imagining%20unseen%20%22What%20if%22%20scenarios.%20This%0Acounterfactual%20reasoning%20facilitates%20the%20model%27s%20ability%20to%20understand%20the%0Acause%20and%20effect%20of%20each%20step%20in%20an%20activity.%20We%20conduct%20extensive%20experiments%0Aon%20procedure-aware%20tasks%2C%20including%20temporal%20action%20segmentation%2C%20error%0Adetection%2C%20action%20phase%20classification%2C%20frame%20retrieval%2C%20multi-instance%0Aretrieval%2C%20and%20action%20recognition.%20Our%20results%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20state-change%20descriptions%20and%20their%20counterfactuals%2C%20and%20achieve%0Asignificant%20improvements%20on%20multiple%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/HCIS-%20Lab/counterfactual-video-pretrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21055v5&entry.124074799=Read"},
{"title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward", "author": "Shudong Liu and Hongwei Liu and Junnan Liu and Linchen Xiao and Songyang Gao and Chengqi Lyu and Yuzhe Gu and Wenwei Zhang and Derek F. Wong and Songyang Zhang and Kai Chen", "abstract": "  Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.\n", "link": "http://arxiv.org/abs/2508.03686v1", "date": "2025-08-05", "relevancy": 2.1116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompassVerifier%3A%20A%20Unified%20and%20Robust%20Verifier%20for%20LLMs%20Evaluation%20and%0A%20%20Outcome%20Reward&body=Title%3A%20CompassVerifier%3A%20A%20Unified%20and%20Robust%20Verifier%20for%20LLMs%20Evaluation%20and%0A%20%20Outcome%20Reward%0AAuthor%3A%20Shudong%20Liu%20and%20Hongwei%20Liu%20and%20Junnan%20Liu%20and%20Linchen%20Xiao%20and%20Songyang%20Gao%20and%20Chengqi%20Lyu%20and%20Yuzhe%20Gu%20and%20Wenwei%20Zhang%20and%20Derek%20F.%20Wong%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Answer%20verification%20is%20crucial%20not%20only%20for%20evaluating%20large%20language%20models%0A%28LLMs%29%20by%20matching%20their%20unstructured%20outputs%20against%20standard%20answers%2C%20but%0Aalso%20serves%20as%20the%20reward%20model%20to%20guide%20LLM%20optimization.%20Most%20evaluation%0Aframeworks%20rely%20on%20regularized%20matching%20or%20employ%20general%20LLMs%20for%20answer%0Averification%2C%20which%20demands%20extensive%2C%20repetitive%20customization%20for%20regex%20rules%0Aor%20evaluation%20prompts.%20Two%20fundamental%20limitations%20persist%20in%20current%0Amethodologies%3A%201%29%20the%20absence%20of%20comprehensive%20benchmarks%20that%20systematically%0Aevaluate%20verification%20capabilities%20across%20different%20LLMs%3B%20and%202%29%20the%20nascent%0Astage%20of%20verifier%20development%2C%20where%20existing%20approaches%20lack%20both%20the%0Arobustness%20to%20handle%20complex%20edge%20cases%20and%20the%20generalizability%20across%0Adifferent%20domains.%20In%20this%20work%2C%20we%20develop%20CompassVerifier%2C%20an%20accurate%20and%0Arobust%20lightweight%20verifier%20model%20for%20evaluation%20and%20outcome%20reward.%20It%0Ademonstrates%20multi-domain%20competency%20spanning%20math%2C%20knowledge%2C%20and%20diverse%0Areasoning%20tasks%2C%20with%20the%20capability%20to%20process%20various%20answer%20types%2C%20including%0Amulti-subproblems%2C%20formulas%2C%20and%20sequence%20answers%2C%20while%20effectively%0Aidentifying%20abnormal/invalid%20responses.%20We%20introduce%20VerifierBench%20benchmark%0Acomprising%20model%20outputs%20collected%20from%20multiple%20data%20sources%2C%20augmented%0Athrough%20manual%20analysis%20of%20metaerror%20patterns%20to%20enhance%20CompassVerifier.%20We%0Aanticipate%20that%20CompassVerifier%20and%20VerifierBench%20will%20facilitate%20answer%0Averification%2C%20evaluation%20protocols%2C%20and%20reinforcement%20learning%20research.%20Code%0Aand%20dataset%20are%20available%20at%20https%3A//github.com/open-compass/CompassVerifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompassVerifier%253A%2520A%2520Unified%2520and%2520Robust%2520Verifier%2520for%2520LLMs%2520Evaluation%2520and%250A%2520%2520Outcome%2520Reward%26entry.906535625%3DShudong%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Junnan%2520Liu%2520and%2520Linchen%2520Xiao%2520and%2520Songyang%2520Gao%2520and%2520Chengqi%2520Lyu%2520and%2520Yuzhe%2520Gu%2520and%2520Wenwei%2520Zhang%2520and%2520Derek%2520F.%2520Wong%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Answer%2520verification%2520is%2520crucial%2520not%2520only%2520for%2520evaluating%2520large%2520language%2520models%250A%2528LLMs%2529%2520by%2520matching%2520their%2520unstructured%2520outputs%2520against%2520standard%2520answers%252C%2520but%250Aalso%2520serves%2520as%2520the%2520reward%2520model%2520to%2520guide%2520LLM%2520optimization.%2520Most%2520evaluation%250Aframeworks%2520rely%2520on%2520regularized%2520matching%2520or%2520employ%2520general%2520LLMs%2520for%2520answer%250Averification%252C%2520which%2520demands%2520extensive%252C%2520repetitive%2520customization%2520for%2520regex%2520rules%250Aor%2520evaluation%2520prompts.%2520Two%2520fundamental%2520limitations%2520persist%2520in%2520current%250Amethodologies%253A%25201%2529%2520the%2520absence%2520of%2520comprehensive%2520benchmarks%2520that%2520systematically%250Aevaluate%2520verification%2520capabilities%2520across%2520different%2520LLMs%253B%2520and%25202%2529%2520the%2520nascent%250Astage%2520of%2520verifier%2520development%252C%2520where%2520existing%2520approaches%2520lack%2520both%2520the%250Arobustness%2520to%2520handle%2520complex%2520edge%2520cases%2520and%2520the%2520generalizability%2520across%250Adifferent%2520domains.%2520In%2520this%2520work%252C%2520we%2520develop%2520CompassVerifier%252C%2520an%2520accurate%2520and%250Arobust%2520lightweight%2520verifier%2520model%2520for%2520evaluation%2520and%2520outcome%2520reward.%2520It%250Ademonstrates%2520multi-domain%2520competency%2520spanning%2520math%252C%2520knowledge%252C%2520and%2520diverse%250Areasoning%2520tasks%252C%2520with%2520the%2520capability%2520to%2520process%2520various%2520answer%2520types%252C%2520including%250Amulti-subproblems%252C%2520formulas%252C%2520and%2520sequence%2520answers%252C%2520while%2520effectively%250Aidentifying%2520abnormal/invalid%2520responses.%2520We%2520introduce%2520VerifierBench%2520benchmark%250Acomprising%2520model%2520outputs%2520collected%2520from%2520multiple%2520data%2520sources%252C%2520augmented%250Athrough%2520manual%2520analysis%2520of%2520metaerror%2520patterns%2520to%2520enhance%2520CompassVerifier.%2520We%250Aanticipate%2520that%2520CompassVerifier%2520and%2520VerifierBench%2520will%2520facilitate%2520answer%250Averification%252C%2520evaluation%2520protocols%252C%2520and%2520reinforcement%2520learning%2520research.%2520Code%250Aand%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/open-compass/CompassVerifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompassVerifier%3A%20A%20Unified%20and%20Robust%20Verifier%20for%20LLMs%20Evaluation%20and%0A%20%20Outcome%20Reward&entry.906535625=Shudong%20Liu%20and%20Hongwei%20Liu%20and%20Junnan%20Liu%20and%20Linchen%20Xiao%20and%20Songyang%20Gao%20and%20Chengqi%20Lyu%20and%20Yuzhe%20Gu%20and%20Wenwei%20Zhang%20and%20Derek%20F.%20Wong%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20Answer%20verification%20is%20crucial%20not%20only%20for%20evaluating%20large%20language%20models%0A%28LLMs%29%20by%20matching%20their%20unstructured%20outputs%20against%20standard%20answers%2C%20but%0Aalso%20serves%20as%20the%20reward%20model%20to%20guide%20LLM%20optimization.%20Most%20evaluation%0Aframeworks%20rely%20on%20regularized%20matching%20or%20employ%20general%20LLMs%20for%20answer%0Averification%2C%20which%20demands%20extensive%2C%20repetitive%20customization%20for%20regex%20rules%0Aor%20evaluation%20prompts.%20Two%20fundamental%20limitations%20persist%20in%20current%0Amethodologies%3A%201%29%20the%20absence%20of%20comprehensive%20benchmarks%20that%20systematically%0Aevaluate%20verification%20capabilities%20across%20different%20LLMs%3B%20and%202%29%20the%20nascent%0Astage%20of%20verifier%20development%2C%20where%20existing%20approaches%20lack%20both%20the%0Arobustness%20to%20handle%20complex%20edge%20cases%20and%20the%20generalizability%20across%0Adifferent%20domains.%20In%20this%20work%2C%20we%20develop%20CompassVerifier%2C%20an%20accurate%20and%0Arobust%20lightweight%20verifier%20model%20for%20evaluation%20and%20outcome%20reward.%20It%0Ademonstrates%20multi-domain%20competency%20spanning%20math%2C%20knowledge%2C%20and%20diverse%0Areasoning%20tasks%2C%20with%20the%20capability%20to%20process%20various%20answer%20types%2C%20including%0Amulti-subproblems%2C%20formulas%2C%20and%20sequence%20answers%2C%20while%20effectively%0Aidentifying%20abnormal/invalid%20responses.%20We%20introduce%20VerifierBench%20benchmark%0Acomprising%20model%20outputs%20collected%20from%20multiple%20data%20sources%2C%20augmented%0Athrough%20manual%20analysis%20of%20metaerror%20patterns%20to%20enhance%20CompassVerifier.%20We%0Aanticipate%20that%20CompassVerifier%20and%20VerifierBench%20will%20facilitate%20answer%0Averification%2C%20evaluation%20protocols%2C%20and%20reinforcement%20learning%20research.%20Code%0Aand%20dataset%20are%20available%20at%20https%3A//github.com/open-compass/CompassVerifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03686v1&entry.124074799=Read"},
{"title": "PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement\n  Learning", "author": "Ondrej Bajgar and Dewi S. W. Gould and Jonathon Liu and Alessandro Abate and Konstantinos Gatsis and Michael A. Osborne", "abstract": "  As AI systems become increasingly autonomous, reliably aligning their\ndecision-making to human preferences is essential. Inverse reinforcement\nlearning (IRL) offers a promising approach to infer preferences from\ndemonstrations. These preferences can then be used to produce an apprentice\npolicy that performs well on the demonstrated task. However, in domains like\nautonomous driving or robotics, where errors can have serious consequences, we\nneed not just good average performance but reliable policies with formal\nguarantees -- yet obtaining sufficient human demonstrations for reliability\nguarantees can be costly. Active IRL addresses this challenge by strategically\nselecting the most informative scenarios for human demonstration. We introduce\nPAC-EIG, an information-theoretic acquisition function that directly targets\nprobably-approximately-correct (PAC) guarantees for the learned policy --\nproviding the first such theoretical guarantee for active IRL with noisy expert\ndemonstrations. Our method maximises information gain about the regret of the\napprentice policy, efficiently identifying states requiring further\ndemonstration. We also present Reward-EIG as an alternative when learning the\nreward itself is the primary objective. Focusing on finite state-action spaces,\nwe prove convergence bounds, illustrate failure modes of prior heuristic\nmethods, and demonstrate our method's advantages experimentally.\n", "link": "http://arxiv.org/abs/2508.03693v1", "date": "2025-08-05", "relevancy": 2.0933, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5404}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5266}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning&body=Title%3A%20PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ondrej%20Bajgar%20and%20Dewi%20S.%20W.%20Gould%20and%20Jonathon%20Liu%20and%20Alessandro%20Abate%20and%20Konstantinos%20Gatsis%20and%20Michael%20A.%20Osborne%0AAbstract%3A%20%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20reliably%20aligning%20their%0Adecision-making%20to%20human%20preferences%20is%20essential.%20Inverse%20reinforcement%0Alearning%20%28IRL%29%20offers%20a%20promising%20approach%20to%20infer%20preferences%20from%0Ademonstrations.%20These%20preferences%20can%20then%20be%20used%20to%20produce%20an%20apprentice%0Apolicy%20that%20performs%20well%20on%20the%20demonstrated%20task.%20However%2C%20in%20domains%20like%0Aautonomous%20driving%20or%20robotics%2C%20where%20errors%20can%20have%20serious%20consequences%2C%20we%0Aneed%20not%20just%20good%20average%20performance%20but%20reliable%20policies%20with%20formal%0Aguarantees%20--%20yet%20obtaining%20sufficient%20human%20demonstrations%20for%20reliability%0Aguarantees%20can%20be%20costly.%20Active%20IRL%20addresses%20this%20challenge%20by%20strategically%0Aselecting%20the%20most%20informative%20scenarios%20for%20human%20demonstration.%20We%20introduce%0APAC-EIG%2C%20an%20information-theoretic%20acquisition%20function%20that%20directly%20targets%0Aprobably-approximately-correct%20%28PAC%29%20guarantees%20for%20the%20learned%20policy%20--%0Aproviding%20the%20first%20such%20theoretical%20guarantee%20for%20active%20IRL%20with%20noisy%20expert%0Ademonstrations.%20Our%20method%20maximises%20information%20gain%20about%20the%20regret%20of%20the%0Aapprentice%20policy%2C%20efficiently%20identifying%20states%20requiring%20further%0Ademonstration.%20We%20also%20present%20Reward-EIG%20as%20an%20alternative%20when%20learning%20the%0Areward%20itself%20is%20the%20primary%20objective.%20Focusing%20on%20finite%20state-action%20spaces%2C%0Awe%20prove%20convergence%20bounds%2C%20illustrate%20failure%20modes%20of%20prior%20heuristic%0Amethods%2C%20and%20demonstrate%20our%20method%27s%20advantages%20experimentally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC%2520Apprenticeship%2520Learning%2520with%2520Bayesian%2520Active%2520Inverse%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DOndrej%2520Bajgar%2520and%2520Dewi%2520S.%2520W.%2520Gould%2520and%2520Jonathon%2520Liu%2520and%2520Alessandro%2520Abate%2520and%2520Konstantinos%2520Gatsis%2520and%2520Michael%2520A.%2520Osborne%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520become%2520increasingly%2520autonomous%252C%2520reliably%2520aligning%2520their%250Adecision-making%2520to%2520human%2520preferences%2520is%2520essential.%2520Inverse%2520reinforcement%250Alearning%2520%2528IRL%2529%2520offers%2520a%2520promising%2520approach%2520to%2520infer%2520preferences%2520from%250Ademonstrations.%2520These%2520preferences%2520can%2520then%2520be%2520used%2520to%2520produce%2520an%2520apprentice%250Apolicy%2520that%2520performs%2520well%2520on%2520the%2520demonstrated%2520task.%2520However%252C%2520in%2520domains%2520like%250Aautonomous%2520driving%2520or%2520robotics%252C%2520where%2520errors%2520can%2520have%2520serious%2520consequences%252C%2520we%250Aneed%2520not%2520just%2520good%2520average%2520performance%2520but%2520reliable%2520policies%2520with%2520formal%250Aguarantees%2520--%2520yet%2520obtaining%2520sufficient%2520human%2520demonstrations%2520for%2520reliability%250Aguarantees%2520can%2520be%2520costly.%2520Active%2520IRL%2520addresses%2520this%2520challenge%2520by%2520strategically%250Aselecting%2520the%2520most%2520informative%2520scenarios%2520for%2520human%2520demonstration.%2520We%2520introduce%250APAC-EIG%252C%2520an%2520information-theoretic%2520acquisition%2520function%2520that%2520directly%2520targets%250Aprobably-approximately-correct%2520%2528PAC%2529%2520guarantees%2520for%2520the%2520learned%2520policy%2520--%250Aproviding%2520the%2520first%2520such%2520theoretical%2520guarantee%2520for%2520active%2520IRL%2520with%2520noisy%2520expert%250Ademonstrations.%2520Our%2520method%2520maximises%2520information%2520gain%2520about%2520the%2520regret%2520of%2520the%250Aapprentice%2520policy%252C%2520efficiently%2520identifying%2520states%2520requiring%2520further%250Ademonstration.%2520We%2520also%2520present%2520Reward-EIG%2520as%2520an%2520alternative%2520when%2520learning%2520the%250Areward%2520itself%2520is%2520the%2520primary%2520objective.%2520Focusing%2520on%2520finite%2520state-action%2520spaces%252C%250Awe%2520prove%2520convergence%2520bounds%252C%2520illustrate%2520failure%2520modes%2520of%2520prior%2520heuristic%250Amethods%252C%2520and%2520demonstrate%2520our%2520method%2527s%2520advantages%2520experimentally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning&entry.906535625=Ondrej%20Bajgar%20and%20Dewi%20S.%20W.%20Gould%20and%20Jonathon%20Liu%20and%20Alessandro%20Abate%20and%20Konstantinos%20Gatsis%20and%20Michael%20A.%20Osborne&entry.1292438233=%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20reliably%20aligning%20their%0Adecision-making%20to%20human%20preferences%20is%20essential.%20Inverse%20reinforcement%0Alearning%20%28IRL%29%20offers%20a%20promising%20approach%20to%20infer%20preferences%20from%0Ademonstrations.%20These%20preferences%20can%20then%20be%20used%20to%20produce%20an%20apprentice%0Apolicy%20that%20performs%20well%20on%20the%20demonstrated%20task.%20However%2C%20in%20domains%20like%0Aautonomous%20driving%20or%20robotics%2C%20where%20errors%20can%20have%20serious%20consequences%2C%20we%0Aneed%20not%20just%20good%20average%20performance%20but%20reliable%20policies%20with%20formal%0Aguarantees%20--%20yet%20obtaining%20sufficient%20human%20demonstrations%20for%20reliability%0Aguarantees%20can%20be%20costly.%20Active%20IRL%20addresses%20this%20challenge%20by%20strategically%0Aselecting%20the%20most%20informative%20scenarios%20for%20human%20demonstration.%20We%20introduce%0APAC-EIG%2C%20an%20information-theoretic%20acquisition%20function%20that%20directly%20targets%0Aprobably-approximately-correct%20%28PAC%29%20guarantees%20for%20the%20learned%20policy%20--%0Aproviding%20the%20first%20such%20theoretical%20guarantee%20for%20active%20IRL%20with%20noisy%20expert%0Ademonstrations.%20Our%20method%20maximises%20information%20gain%20about%20the%20regret%20of%20the%0Aapprentice%20policy%2C%20efficiently%20identifying%20states%20requiring%20further%0Ademonstration.%20We%20also%20present%20Reward-EIG%20as%20an%20alternative%20when%20learning%20the%0Areward%20itself%20is%20the%20primary%20objective.%20Focusing%20on%20finite%20state-action%20spaces%2C%0Awe%20prove%20convergence%20bounds%2C%20illustrate%20failure%20modes%20of%20prior%20heuristic%0Amethods%2C%20and%20demonstrate%20our%20method%27s%20advantages%20experimentally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03693v1&entry.124074799=Read"},
{"title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code\n  Generation", "author": "Yangtian Zi and Harshitha Menon and Arjun Guha", "abstract": "  State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.\n", "link": "http://arxiv.org/abs/2508.03678v1", "date": "2025-08-05", "relevancy": 2.0891, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Than%20a%20Score%3A%20Probing%20the%20Impact%20of%20Prompt%20Specificity%20on%20LLM%20Code%0A%20%20Generation&body=Title%3A%20More%20Than%20a%20Score%3A%20Probing%20the%20Impact%20of%20Prompt%20Specificity%20on%20LLM%20Code%0A%20%20Generation%0AAuthor%3A%20Yangtian%20Zi%20and%20Harshitha%20Menon%20and%20Arjun%20Guha%0AAbstract%3A%20%20%20State-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20achieve%20high%20pass%401%20on%20general%0Abenchmarks%20like%20HumanEval%20but%20underperform%20on%20specialized%20suites%20such%20as%0AParEval.%20Is%20this%20due%20to%20LLMs%20missing%20domain%20knowledge%20or%20insufficient%20prompt%0Adetail%20is%20given%3F%20To%20answer%20this%2C%20we%20introduce%20PartialOrderEval%2C%20which%20augments%0Aany%20code%20generation%20benchmark%20with%20a%20partial%20order%20of%20prompts%20from%20minimal%20to%0Amaximally%20detailed.%20Applying%20it%20to%20HumanEval%20and%20both%20serial%20and%20OpenMP%20subsets%0Aof%20ParEval%2C%20we%20measure%20how%20pass%401%20scales%20with%20prompt%20specificity.%20Our%0Aexperiments%20with%20Llama-3.x%20and%20Qwen2.5-Coder%20demonstrate%20varying%20degrees%20of%0Aprompt%20sensitivity%20across%20different%20tasks%2C%20and%20a%20qualitative%20analysis%0Ahighlights%20explicit%20I/O%20specifications%2C%20edge-case%20handling%2C%20and%20stepwise%0Abreakdowns%20as%20the%20key%20drivers%20of%20prompt%20detail%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Than%2520a%2520Score%253A%2520Probing%2520the%2520Impact%2520of%2520Prompt%2520Specificity%2520on%2520LLM%2520Code%250A%2520%2520Generation%26entry.906535625%3DYangtian%2520Zi%2520and%2520Harshitha%2520Menon%2520and%2520Arjun%2520Guha%26entry.1292438233%3D%2520%2520State-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520high%2520pass%25401%2520on%2520general%250Abenchmarks%2520like%2520HumanEval%2520but%2520underperform%2520on%2520specialized%2520suites%2520such%2520as%250AParEval.%2520Is%2520this%2520due%2520to%2520LLMs%2520missing%2520domain%2520knowledge%2520or%2520insufficient%2520prompt%250Adetail%2520is%2520given%253F%2520To%2520answer%2520this%252C%2520we%2520introduce%2520PartialOrderEval%252C%2520which%2520augments%250Aany%2520code%2520generation%2520benchmark%2520with%2520a%2520partial%2520order%2520of%2520prompts%2520from%2520minimal%2520to%250Amaximally%2520detailed.%2520Applying%2520it%2520to%2520HumanEval%2520and%2520both%2520serial%2520and%2520OpenMP%2520subsets%250Aof%2520ParEval%252C%2520we%2520measure%2520how%2520pass%25401%2520scales%2520with%2520prompt%2520specificity.%2520Our%250Aexperiments%2520with%2520Llama-3.x%2520and%2520Qwen2.5-Coder%2520demonstrate%2520varying%2520degrees%2520of%250Aprompt%2520sensitivity%2520across%2520different%2520tasks%252C%2520and%2520a%2520qualitative%2520analysis%250Ahighlights%2520explicit%2520I/O%2520specifications%252C%2520edge-case%2520handling%252C%2520and%2520stepwise%250Abreakdowns%2520as%2520the%2520key%2520drivers%2520of%2520prompt%2520detail%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Than%20a%20Score%3A%20Probing%20the%20Impact%20of%20Prompt%20Specificity%20on%20LLM%20Code%0A%20%20Generation&entry.906535625=Yangtian%20Zi%20and%20Harshitha%20Menon%20and%20Arjun%20Guha&entry.1292438233=%20%20State-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20achieve%20high%20pass%401%20on%20general%0Abenchmarks%20like%20HumanEval%20but%20underperform%20on%20specialized%20suites%20such%20as%0AParEval.%20Is%20this%20due%20to%20LLMs%20missing%20domain%20knowledge%20or%20insufficient%20prompt%0Adetail%20is%20given%3F%20To%20answer%20this%2C%20we%20introduce%20PartialOrderEval%2C%20which%20augments%0Aany%20code%20generation%20benchmark%20with%20a%20partial%20order%20of%20prompts%20from%20minimal%20to%0Amaximally%20detailed.%20Applying%20it%20to%20HumanEval%20and%20both%20serial%20and%20OpenMP%20subsets%0Aof%20ParEval%2C%20we%20measure%20how%20pass%401%20scales%20with%20prompt%20specificity.%20Our%0Aexperiments%20with%20Llama-3.x%20and%20Qwen2.5-Coder%20demonstrate%20varying%20degrees%20of%0Aprompt%20sensitivity%20across%20different%20tasks%2C%20and%20a%20qualitative%20analysis%0Ahighlights%20explicit%20I/O%20specifications%2C%20edge-case%20handling%2C%20and%20stepwise%0Abreakdowns%20as%20the%20key%20drivers%20of%20prompt%20detail%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03678v1&entry.124074799=Read"},
{"title": "Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?", "author": "Wenxuan Shen and Mingjia Wang and Yaochen Wang and Dongping Chen and Junjie Yang and Yao Wan and Weiwei Lin", "abstract": "  Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.\n", "link": "http://arxiv.org/abs/2508.03644v1", "date": "2025-08-05", "relevancy": 2.0877, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20We%20on%20the%20Right%20Way%20for%20Assessing%20Document%20Retrieval-Augmented%0A%20%20Generation%3F&body=Title%3A%20Are%20We%20on%20the%20Right%20Way%20for%20Assessing%20Document%20Retrieval-Augmented%0A%20%20Generation%3F%0AAuthor%3A%20Wenxuan%20Shen%20and%20Mingjia%20Wang%20and%20Yaochen%20Wang%20and%20Dongping%20Chen%20and%20Junjie%20Yang%20and%20Yao%20Wan%20and%20Weiwei%20Lin%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20using%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20show%20great%20promise%20for%20complex%20document%20understanding%2C%20yet%20their%0Adevelopment%20is%20critically%20hampered%20by%20inadequate%20evaluation.%20Current%20benchmarks%0Aoften%20focus%20on%20specific%20part%20of%20document%20RAG%20system%20and%20use%20synthetic%20data%20with%0Aincomplete%20ground%20truth%20and%20evidence%20labels%2C%20therefore%20failing%20to%20reflect%0Areal-world%20bottlenecks%20and%20challenges.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20Double-Bench%3A%20a%20new%20large-scale%2C%20multilingual%2C%20and%20multimodal%0Aevaluation%20system%20that%20is%20able%20to%20produce%20fine-grained%20assessment%20to%20each%0Acomponent%20within%20document%20RAG%20systems.%20It%20comprises%203%2C276%20documents%20%2872%2C880%0Apages%29%20and%205%2C168%20single-%20and%20multi-hop%20queries%20across%206%20languages%20and%204%0Adocument%20types%20with%20streamlined%20dynamic%20update%20support%20for%20potential%20data%0Acontamination%20issues.%20Queries%20are%20grounded%20in%20exhaustively%20scanned%20evidence%0Apages%20and%20verified%20by%20human%20experts%20to%20ensure%20maximum%20quality%20and%20completeness.%0AOur%20comprehensive%20experiments%20across%209%20state-of-the-art%20embedding%20models%2C%204%0AMLLMs%20and%204%20end-to-end%20document%20RAG%20frameworks%20demonstrate%20the%20gap%20between%20text%0Aand%20visual%20embedding%20models%20is%20narrowing%2C%20highlighting%20the%20need%20in%20building%0Astronger%20document%20retrieval%20models.%20Our%20findings%20also%20reveal%20the%0Aover-confidence%20dilemma%20within%20current%20document%20RAG%20frameworks%20that%20tend%20to%0Aprovide%20answer%20even%20without%20evidence%20support.%20We%20hope%20our%20fully%20open-source%0ADouble-Bench%20provide%20a%20rigorous%20foundation%20for%20future%20research%20in%20advanced%0Adocument%20RAG%20systems.%20We%20plan%20to%20retrieve%20timely%20corpus%20and%20release%20new%0Abenchmarks%20on%20an%20annual%20basis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520We%2520on%2520the%2520Right%2520Way%2520for%2520Assessing%2520Document%2520Retrieval-Augmented%250A%2520%2520Generation%253F%26entry.906535625%3DWenxuan%2520Shen%2520and%2520Mingjia%2520Wang%2520and%2520Yaochen%2520Wang%2520and%2520Dongping%2520Chen%2520and%2520Junjie%2520Yang%2520and%2520Yao%2520Wan%2520and%2520Weiwei%2520Lin%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520using%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520show%2520great%2520promise%2520for%2520complex%2520document%2520understanding%252C%2520yet%2520their%250Adevelopment%2520is%2520critically%2520hampered%2520by%2520inadequate%2520evaluation.%2520Current%2520benchmarks%250Aoften%2520focus%2520on%2520specific%2520part%2520of%2520document%2520RAG%2520system%2520and%2520use%2520synthetic%2520data%2520with%250Aincomplete%2520ground%2520truth%2520and%2520evidence%2520labels%252C%2520therefore%2520failing%2520to%2520reflect%250Areal-world%2520bottlenecks%2520and%2520challenges.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Aintroduce%2520Double-Bench%253A%2520a%2520new%2520large-scale%252C%2520multilingual%252C%2520and%2520multimodal%250Aevaluation%2520system%2520that%2520is%2520able%2520to%2520produce%2520fine-grained%2520assessment%2520to%2520each%250Acomponent%2520within%2520document%2520RAG%2520systems.%2520It%2520comprises%25203%252C276%2520documents%2520%252872%252C880%250Apages%2529%2520and%25205%252C168%2520single-%2520and%2520multi-hop%2520queries%2520across%25206%2520languages%2520and%25204%250Adocument%2520types%2520with%2520streamlined%2520dynamic%2520update%2520support%2520for%2520potential%2520data%250Acontamination%2520issues.%2520Queries%2520are%2520grounded%2520in%2520exhaustively%2520scanned%2520evidence%250Apages%2520and%2520verified%2520by%2520human%2520experts%2520to%2520ensure%2520maximum%2520quality%2520and%2520completeness.%250AOur%2520comprehensive%2520experiments%2520across%25209%2520state-of-the-art%2520embedding%2520models%252C%25204%250AMLLMs%2520and%25204%2520end-to-end%2520document%2520RAG%2520frameworks%2520demonstrate%2520the%2520gap%2520between%2520text%250Aand%2520visual%2520embedding%2520models%2520is%2520narrowing%252C%2520highlighting%2520the%2520need%2520in%2520building%250Astronger%2520document%2520retrieval%2520models.%2520Our%2520findings%2520also%2520reveal%2520the%250Aover-confidence%2520dilemma%2520within%2520current%2520document%2520RAG%2520frameworks%2520that%2520tend%2520to%250Aprovide%2520answer%2520even%2520without%2520evidence%2520support.%2520We%2520hope%2520our%2520fully%2520open-source%250ADouble-Bench%2520provide%2520a%2520rigorous%2520foundation%2520for%2520future%2520research%2520in%2520advanced%250Adocument%2520RAG%2520systems.%2520We%2520plan%2520to%2520retrieve%2520timely%2520corpus%2520and%2520release%2520new%250Abenchmarks%2520on%2520an%2520annual%2520basis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20on%20the%20Right%20Way%20for%20Assessing%20Document%20Retrieval-Augmented%0A%20%20Generation%3F&entry.906535625=Wenxuan%20Shen%20and%20Mingjia%20Wang%20and%20Yaochen%20Wang%20and%20Dongping%20Chen%20and%20Junjie%20Yang%20and%20Yao%20Wan%20and%20Weiwei%20Lin&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20using%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20show%20great%20promise%20for%20complex%20document%20understanding%2C%20yet%20their%0Adevelopment%20is%20critically%20hampered%20by%20inadequate%20evaluation.%20Current%20benchmarks%0Aoften%20focus%20on%20specific%20part%20of%20document%20RAG%20system%20and%20use%20synthetic%20data%20with%0Aincomplete%20ground%20truth%20and%20evidence%20labels%2C%20therefore%20failing%20to%20reflect%0Areal-world%20bottlenecks%20and%20challenges.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20Double-Bench%3A%20a%20new%20large-scale%2C%20multilingual%2C%20and%20multimodal%0Aevaluation%20system%20that%20is%20able%20to%20produce%20fine-grained%20assessment%20to%20each%0Acomponent%20within%20document%20RAG%20systems.%20It%20comprises%203%2C276%20documents%20%2872%2C880%0Apages%29%20and%205%2C168%20single-%20and%20multi-hop%20queries%20across%206%20languages%20and%204%0Adocument%20types%20with%20streamlined%20dynamic%20update%20support%20for%20potential%20data%0Acontamination%20issues.%20Queries%20are%20grounded%20in%20exhaustively%20scanned%20evidence%0Apages%20and%20verified%20by%20human%20experts%20to%20ensure%20maximum%20quality%20and%20completeness.%0AOur%20comprehensive%20experiments%20across%209%20state-of-the-art%20embedding%20models%2C%204%0AMLLMs%20and%204%20end-to-end%20document%20RAG%20frameworks%20demonstrate%20the%20gap%20between%20text%0Aand%20visual%20embedding%20models%20is%20narrowing%2C%20highlighting%20the%20need%20in%20building%0Astronger%20document%20retrieval%20models.%20Our%20findings%20also%20reveal%20the%0Aover-confidence%20dilemma%20within%20current%20document%20RAG%20frameworks%20that%20tend%20to%0Aprovide%20answer%20even%20without%20evidence%20support.%20We%20hope%20our%20fully%20open-source%0ADouble-Bench%20provide%20a%20rigorous%20foundation%20for%20future%20research%20in%20advanced%0Adocument%20RAG%20systems.%20We%20plan%20to%20retrieve%20timely%20corpus%20and%20release%20new%0Abenchmarks%20on%20an%20annual%20basis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03644v1&entry.124074799=Read"},
{"title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning", "author": "Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang", "abstract": "  We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.\n", "link": "http://arxiv.org/abs/2508.03680v1", "date": "2025-08-05", "relevancy": 2.0872, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5695}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5028}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Lightning%3A%20Train%20ANY%20AI%20Agents%20with%20Reinforcement%20Learning&body=Title%3A%20Agent%20Lightning%3A%20Train%20ANY%20AI%20Agents%20with%20Reinforcement%20Learning%0AAuthor%3A%20Xufang%20Luo%20and%20Yuge%20Zhang%20and%20Zhiyuan%20He%20and%20Zilong%20Wang%20and%20Siyun%20Zhao%20and%20Dongsheng%20Li%20and%20Luna%20K.%20Qiu%20and%20Yuqing%20Yang%0AAbstract%3A%20%20%20We%20present%20Agent%20Lightning%2C%20a%20flexible%20and%20extensible%20framework%20that%20enables%0AReinforcement%20Learning%20%28RL%29-based%20training%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aany%20AI%20agent.%20Unlike%20existing%20methods%20that%20tightly%20couple%20RL%20training%20with%0Aagent%20or%20rely%20on%20sequence%20concatenation%20with%20masking%2C%20Agent%20Lightning%20achieves%0Acomplete%20decoupling%20between%20agent%20execution%20and%20training%2C%20allowing%20seamless%0Aintegration%20with%20existing%20agents%20developed%20via%20diverse%20ways%20%28e.g.%2C%20using%0Aframeworks%20like%20LangChain%2C%20OpenAI%20Agents%20SDK%2C%20AutoGen%2C%20and%20building%20from%0Ascratch%29%20with%20almost%20ZERO%20code%20modifications.%20By%20formulating%20agent%20execution%20as%0AMarkov%20decision%20process%2C%20we%20define%20an%20unified%20data%20interface%20and%20propose%20a%0Ahierarchical%20RL%20algorithm%2C%20LightningRL%2C%20which%20contains%20a%20credit%20assignment%0Amodule%2C%20allowing%20us%20to%20decompose%20trajectories%20generated%20by%20ANY%20agents%20into%0Atraining%20transition.%20This%20enables%20RL%20to%20handle%20complex%20interaction%20logic%2C%20such%0Aas%20multi-agent%20scenarios%20and%20dynamic%20workflows.%20For%20the%20system%20design%2C%20we%0Aintroduce%20a%20Training-Agent%20Disaggregation%20architecture%2C%20and%20brings%20agent%0Aobservability%20frameworks%20into%20agent%20runtime%2C%20providing%20a%20standardized%20agent%0Afinetuning%20interface.%20Experiments%20across%20text-to-SQL%2C%20retrieval-augmented%0Ageneration%2C%20and%20math%20tool-use%20tasks%20demonstrate%20stable%2C%20continuous%0Aimprovements%2C%20showcasing%20the%20framework%27s%20potential%20for%20real-world%20agent%0Atraining%20and%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Lightning%253A%2520Train%2520ANY%2520AI%2520Agents%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DXufang%2520Luo%2520and%2520Yuge%2520Zhang%2520and%2520Zhiyuan%2520He%2520and%2520Zilong%2520Wang%2520and%2520Siyun%2520Zhao%2520and%2520Dongsheng%2520Li%2520and%2520Luna%2520K.%2520Qiu%2520and%2520Yuqing%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520Agent%2520Lightning%252C%2520a%2520flexible%2520and%2520extensible%2520framework%2520that%2520enables%250AReinforcement%2520Learning%2520%2528RL%2529-based%2520training%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Aany%2520AI%2520agent.%2520Unlike%2520existing%2520methods%2520that%2520tightly%2520couple%2520RL%2520training%2520with%250Aagent%2520or%2520rely%2520on%2520sequence%2520concatenation%2520with%2520masking%252C%2520Agent%2520Lightning%2520achieves%250Acomplete%2520decoupling%2520between%2520agent%2520execution%2520and%2520training%252C%2520allowing%2520seamless%250Aintegration%2520with%2520existing%2520agents%2520developed%2520via%2520diverse%2520ways%2520%2528e.g.%252C%2520using%250Aframeworks%2520like%2520LangChain%252C%2520OpenAI%2520Agents%2520SDK%252C%2520AutoGen%252C%2520and%2520building%2520from%250Ascratch%2529%2520with%2520almost%2520ZERO%2520code%2520modifications.%2520By%2520formulating%2520agent%2520execution%2520as%250AMarkov%2520decision%2520process%252C%2520we%2520define%2520an%2520unified%2520data%2520interface%2520and%2520propose%2520a%250Ahierarchical%2520RL%2520algorithm%252C%2520LightningRL%252C%2520which%2520contains%2520a%2520credit%2520assignment%250Amodule%252C%2520allowing%2520us%2520to%2520decompose%2520trajectories%2520generated%2520by%2520ANY%2520agents%2520into%250Atraining%2520transition.%2520This%2520enables%2520RL%2520to%2520handle%2520complex%2520interaction%2520logic%252C%2520such%250Aas%2520multi-agent%2520scenarios%2520and%2520dynamic%2520workflows.%2520For%2520the%2520system%2520design%252C%2520we%250Aintroduce%2520a%2520Training-Agent%2520Disaggregation%2520architecture%252C%2520and%2520brings%2520agent%250Aobservability%2520frameworks%2520into%2520agent%2520runtime%252C%2520providing%2520a%2520standardized%2520agent%250Afinetuning%2520interface.%2520Experiments%2520across%2520text-to-SQL%252C%2520retrieval-augmented%250Ageneration%252C%2520and%2520math%2520tool-use%2520tasks%2520demonstrate%2520stable%252C%2520continuous%250Aimprovements%252C%2520showcasing%2520the%2520framework%2527s%2520potential%2520for%2520real-world%2520agent%250Atraining%2520and%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Lightning%3A%20Train%20ANY%20AI%20Agents%20with%20Reinforcement%20Learning&entry.906535625=Xufang%20Luo%20and%20Yuge%20Zhang%20and%20Zhiyuan%20He%20and%20Zilong%20Wang%20and%20Siyun%20Zhao%20and%20Dongsheng%20Li%20and%20Luna%20K.%20Qiu%20and%20Yuqing%20Yang&entry.1292438233=%20%20We%20present%20Agent%20Lightning%2C%20a%20flexible%20and%20extensible%20framework%20that%20enables%0AReinforcement%20Learning%20%28RL%29-based%20training%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aany%20AI%20agent.%20Unlike%20existing%20methods%20that%20tightly%20couple%20RL%20training%20with%0Aagent%20or%20rely%20on%20sequence%20concatenation%20with%20masking%2C%20Agent%20Lightning%20achieves%0Acomplete%20decoupling%20between%20agent%20execution%20and%20training%2C%20allowing%20seamless%0Aintegration%20with%20existing%20agents%20developed%20via%20diverse%20ways%20%28e.g.%2C%20using%0Aframeworks%20like%20LangChain%2C%20OpenAI%20Agents%20SDK%2C%20AutoGen%2C%20and%20building%20from%0Ascratch%29%20with%20almost%20ZERO%20code%20modifications.%20By%20formulating%20agent%20execution%20as%0AMarkov%20decision%20process%2C%20we%20define%20an%20unified%20data%20interface%20and%20propose%20a%0Ahierarchical%20RL%20algorithm%2C%20LightningRL%2C%20which%20contains%20a%20credit%20assignment%0Amodule%2C%20allowing%20us%20to%20decompose%20trajectories%20generated%20by%20ANY%20agents%20into%0Atraining%20transition.%20This%20enables%20RL%20to%20handle%20complex%20interaction%20logic%2C%20such%0Aas%20multi-agent%20scenarios%20and%20dynamic%20workflows.%20For%20the%20system%20design%2C%20we%0Aintroduce%20a%20Training-Agent%20Disaggregation%20architecture%2C%20and%20brings%20agent%0Aobservability%20frameworks%20into%20agent%20runtime%2C%20providing%20a%20standardized%20agent%0Afinetuning%20interface.%20Experiments%20across%20text-to-SQL%2C%20retrieval-augmented%0Ageneration%2C%20and%20math%20tool-use%20tasks%20demonstrate%20stable%2C%20continuous%0Aimprovements%2C%20showcasing%20the%20framework%27s%20potential%20for%20real-world%20agent%0Atraining%20and%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03680v1&entry.124074799=Read"},
{"title": "ProRefine: Inference-Time Prompt Refinement with Textual Feedback", "author": "Deepak Pandita and Tharindu Cyril Weerasooriya and Ankit Parag Shah and Isabelle Diana May-Xin Ng and Christopher M. Homan and Wei Wei", "abstract": "  Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, play a substantial role in many cutting-edge\ncommercial applications, and continue to fascinate researchers across nearly\nall fields for their potential to accomplish expensive, complex tasks that,\nuntil recently, only humans have been trusted to do. These workflows depend\ncritically on the prompts used to provide the roles models play in such\nworkflows. Poorly designed prompts that fail even slightly to guide individual\nagents can lead to sub-optimal performance that may snowball within a system of\nagents, limiting their reliability and scalability. To address this important\nproblem of inference-time prompt optimization, we introduce ProRefine, an\ninnovative inference-time optimization method that uses an agentic loop of LLMs\nto generate and apply textual feedback. ProRefine dynamically refines prompts\nfor multi-step reasoning tasks without additional training or ground truth\nlabels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine\nsignificantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37\npercentage points. This approach not only boosts accuracy but also allows\nsmaller models to approach the performance of their larger counterparts. This\nhighlights its potential for building more cost-effective and powerful hybrid\nAI systems, thereby democratizing access to high-performing AI.\n", "link": "http://arxiv.org/abs/2506.05305v2", "date": "2025-08-05", "relevancy": 2.0648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProRefine%3A%20Inference-Time%20Prompt%20Refinement%20with%20Textual%20Feedback&body=Title%3A%20ProRefine%3A%20Inference-Time%20Prompt%20Refinement%20with%20Textual%20Feedback%0AAuthor%3A%20Deepak%20Pandita%20and%20Tharindu%20Cyril%20Weerasooriya%20and%20Ankit%20Parag%20Shah%20and%20Isabelle%20Diana%20May-Xin%20Ng%20and%20Christopher%20M.%20Homan%20and%20Wei%20Wei%0AAbstract%3A%20%20%20Agentic%20workflows%2C%20where%20multiple%20AI%20agents%20collaborate%20to%20accomplish%20complex%0Atasks%20like%20reasoning%20or%20planning%2C%20play%20a%20substantial%20role%20in%20many%20cutting-edge%0Acommercial%20applications%2C%20and%20continue%20to%20fascinate%20researchers%20across%20nearly%0Aall%20fields%20for%20their%20potential%20to%20accomplish%20expensive%2C%20complex%20tasks%20that%2C%0Auntil%20recently%2C%20only%20humans%20have%20been%20trusted%20to%20do.%20These%20workflows%20depend%0Acritically%20on%20the%20prompts%20used%20to%20provide%20the%20roles%20models%20play%20in%20such%0Aworkflows.%20Poorly%20designed%20prompts%20that%20fail%20even%20slightly%20to%20guide%20individual%0Aagents%20can%20lead%20to%20sub-optimal%20performance%20that%20may%20snowball%20within%20a%20system%20of%0Aagents%2C%20limiting%20their%20reliability%20and%20scalability.%20To%20address%20this%20important%0Aproblem%20of%20inference-time%20prompt%20optimization%2C%20we%20introduce%20ProRefine%2C%20an%0Ainnovative%20inference-time%20optimization%20method%20that%20uses%20an%20agentic%20loop%20of%20LLMs%0Ato%20generate%20and%20apply%20textual%20feedback.%20ProRefine%20dynamically%20refines%20prompts%0Afor%20multi-step%20reasoning%20tasks%20without%20additional%20training%20or%20ground%20truth%0Alabels.%20Evaluated%20on%20five%20benchmark%20mathematical%20reasoning%20datasets%2C%20ProRefine%0Asignificantly%20surpasses%20zero-shot%20Chain-of-Thought%20baselines%20by%203%20to%2037%0Apercentage%20points.%20This%20approach%20not%20only%20boosts%20accuracy%20but%20also%20allows%0Asmaller%20models%20to%20approach%20the%20performance%20of%20their%20larger%20counterparts.%20This%0Ahighlights%20its%20potential%20for%20building%20more%20cost-effective%20and%20powerful%20hybrid%0AAI%20systems%2C%20thereby%20democratizing%20access%20to%20high-performing%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProRefine%253A%2520Inference-Time%2520Prompt%2520Refinement%2520with%2520Textual%2520Feedback%26entry.906535625%3DDeepak%2520Pandita%2520and%2520Tharindu%2520Cyril%2520Weerasooriya%2520and%2520Ankit%2520Parag%2520Shah%2520and%2520Isabelle%2520Diana%2520May-Xin%2520Ng%2520and%2520Christopher%2520M.%2520Homan%2520and%2520Wei%2520Wei%26entry.1292438233%3D%2520%2520Agentic%2520workflows%252C%2520where%2520multiple%2520AI%2520agents%2520collaborate%2520to%2520accomplish%2520complex%250Atasks%2520like%2520reasoning%2520or%2520planning%252C%2520play%2520a%2520substantial%2520role%2520in%2520many%2520cutting-edge%250Acommercial%2520applications%252C%2520and%2520continue%2520to%2520fascinate%2520researchers%2520across%2520nearly%250Aall%2520fields%2520for%2520their%2520potential%2520to%2520accomplish%2520expensive%252C%2520complex%2520tasks%2520that%252C%250Auntil%2520recently%252C%2520only%2520humans%2520have%2520been%2520trusted%2520to%2520do.%2520These%2520workflows%2520depend%250Acritically%2520on%2520the%2520prompts%2520used%2520to%2520provide%2520the%2520roles%2520models%2520play%2520in%2520such%250Aworkflows.%2520Poorly%2520designed%2520prompts%2520that%2520fail%2520even%2520slightly%2520to%2520guide%2520individual%250Aagents%2520can%2520lead%2520to%2520sub-optimal%2520performance%2520that%2520may%2520snowball%2520within%2520a%2520system%2520of%250Aagents%252C%2520limiting%2520their%2520reliability%2520and%2520scalability.%2520To%2520address%2520this%2520important%250Aproblem%2520of%2520inference-time%2520prompt%2520optimization%252C%2520we%2520introduce%2520ProRefine%252C%2520an%250Ainnovative%2520inference-time%2520optimization%2520method%2520that%2520uses%2520an%2520agentic%2520loop%2520of%2520LLMs%250Ato%2520generate%2520and%2520apply%2520textual%2520feedback.%2520ProRefine%2520dynamically%2520refines%2520prompts%250Afor%2520multi-step%2520reasoning%2520tasks%2520without%2520additional%2520training%2520or%2520ground%2520truth%250Alabels.%2520Evaluated%2520on%2520five%2520benchmark%2520mathematical%2520reasoning%2520datasets%252C%2520ProRefine%250Asignificantly%2520surpasses%2520zero-shot%2520Chain-of-Thought%2520baselines%2520by%25203%2520to%252037%250Apercentage%2520points.%2520This%2520approach%2520not%2520only%2520boosts%2520accuracy%2520but%2520also%2520allows%250Asmaller%2520models%2520to%2520approach%2520the%2520performance%2520of%2520their%2520larger%2520counterparts.%2520This%250Ahighlights%2520its%2520potential%2520for%2520building%2520more%2520cost-effective%2520and%2520powerful%2520hybrid%250AAI%2520systems%252C%2520thereby%2520democratizing%2520access%2520to%2520high-performing%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProRefine%3A%20Inference-Time%20Prompt%20Refinement%20with%20Textual%20Feedback&entry.906535625=Deepak%20Pandita%20and%20Tharindu%20Cyril%20Weerasooriya%20and%20Ankit%20Parag%20Shah%20and%20Isabelle%20Diana%20May-Xin%20Ng%20and%20Christopher%20M.%20Homan%20and%20Wei%20Wei&entry.1292438233=%20%20Agentic%20workflows%2C%20where%20multiple%20AI%20agents%20collaborate%20to%20accomplish%20complex%0Atasks%20like%20reasoning%20or%20planning%2C%20play%20a%20substantial%20role%20in%20many%20cutting-edge%0Acommercial%20applications%2C%20and%20continue%20to%20fascinate%20researchers%20across%20nearly%0Aall%20fields%20for%20their%20potential%20to%20accomplish%20expensive%2C%20complex%20tasks%20that%2C%0Auntil%20recently%2C%20only%20humans%20have%20been%20trusted%20to%20do.%20These%20workflows%20depend%0Acritically%20on%20the%20prompts%20used%20to%20provide%20the%20roles%20models%20play%20in%20such%0Aworkflows.%20Poorly%20designed%20prompts%20that%20fail%20even%20slightly%20to%20guide%20individual%0Aagents%20can%20lead%20to%20sub-optimal%20performance%20that%20may%20snowball%20within%20a%20system%20of%0Aagents%2C%20limiting%20their%20reliability%20and%20scalability.%20To%20address%20this%20important%0Aproblem%20of%20inference-time%20prompt%20optimization%2C%20we%20introduce%20ProRefine%2C%20an%0Ainnovative%20inference-time%20optimization%20method%20that%20uses%20an%20agentic%20loop%20of%20LLMs%0Ato%20generate%20and%20apply%20textual%20feedback.%20ProRefine%20dynamically%20refines%20prompts%0Afor%20multi-step%20reasoning%20tasks%20without%20additional%20training%20or%20ground%20truth%0Alabels.%20Evaluated%20on%20five%20benchmark%20mathematical%20reasoning%20datasets%2C%20ProRefine%0Asignificantly%20surpasses%20zero-shot%20Chain-of-Thought%20baselines%20by%203%20to%2037%0Apercentage%20points.%20This%20approach%20not%20only%20boosts%20accuracy%20but%20also%20allows%0Asmaller%20models%20to%20approach%20the%20performance%20of%20their%20larger%20counterparts.%20This%0Ahighlights%20its%20potential%20for%20building%20more%20cost-effective%20and%20powerful%20hybrid%0AAI%20systems%2C%20thereby%20democratizing%20access%20to%20high-performing%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05305v2&entry.124074799=Read"},
{"title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?", "author": "Xinyu Wang and Yue Zhang and Liqiang Jing", "abstract": "  Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.\n", "link": "http://arxiv.org/abs/2508.03654v1", "date": "2025-08-05", "relevancy": 2.0587, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Vision-Language%20Models%20Understand%20Multimodal%20Sarcasm%3F&body=Title%3A%20Can%20Large%20Vision-Language%20Models%20Understand%20Multimodal%20Sarcasm%3F%0AAuthor%3A%20Xinyu%20Wang%20and%20Yue%20Zhang%20and%20Liqiang%20Jing%0AAbstract%3A%20%20%20Sarcasm%20is%20a%20complex%20linguistic%20phenomenon%20that%20involves%20a%20disparity%20between%0Aliteral%20and%20intended%20meanings%2C%20making%20it%20challenging%20for%20sentiment%20analysis%20and%0Aother%20emotion-sensitive%20tasks.%20While%20traditional%20sarcasm%20detection%20methods%0Aprimarily%20focus%20on%20text%2C%20recent%20approaches%20have%20incorporated%20multimodal%0Ainformation.%20However%2C%20the%20application%20of%20Large%20Visual%20Language%20Models%20%28LVLMs%29%0Ain%20Multimodal%20Sarcasm%20Analysis%20%28MSA%29%20remains%20underexplored.%20In%20this%20paper%2C%20we%0Aevaluate%20LVLMs%20in%20MSA%20tasks%2C%20specifically%20focusing%20on%20Multimodal%20Sarcasm%0ADetection%20and%20Multimodal%20Sarcasm%20Explanation.%20Through%20comprehensive%0Aexperiments%2C%20we%20identify%20key%20limitations%2C%20such%20as%20insufficient%20visual%0Aunderstanding%20and%20a%20lack%20of%20conceptual%20knowledge.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20training-free%20framework%20that%20integrates%20in-depth%20object%20extraction%0Aand%20external%20conceptual%20knowledge%20to%20improve%20the%20model%27s%20ability%20to%20interpret%0Aand%20explain%20sarcasm%20in%20multimodal%20contexts.%20The%20experimental%20results%20on%0Amultiple%20models%20show%20the%20effectiveness%20of%20our%20proposed%20framework.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/cp-cp/LVLM-MSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Vision-Language%2520Models%2520Understand%2520Multimodal%2520Sarcasm%253F%26entry.906535625%3DXinyu%2520Wang%2520and%2520Yue%2520Zhang%2520and%2520Liqiang%2520Jing%26entry.1292438233%3D%2520%2520Sarcasm%2520is%2520a%2520complex%2520linguistic%2520phenomenon%2520that%2520involves%2520a%2520disparity%2520between%250Aliteral%2520and%2520intended%2520meanings%252C%2520making%2520it%2520challenging%2520for%2520sentiment%2520analysis%2520and%250Aother%2520emotion-sensitive%2520tasks.%2520While%2520traditional%2520sarcasm%2520detection%2520methods%250Aprimarily%2520focus%2520on%2520text%252C%2520recent%2520approaches%2520have%2520incorporated%2520multimodal%250Ainformation.%2520However%252C%2520the%2520application%2520of%2520Large%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%250Ain%2520Multimodal%2520Sarcasm%2520Analysis%2520%2528MSA%2529%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%250Aevaluate%2520LVLMs%2520in%2520MSA%2520tasks%252C%2520specifically%2520focusing%2520on%2520Multimodal%2520Sarcasm%250ADetection%2520and%2520Multimodal%2520Sarcasm%2520Explanation.%2520Through%2520comprehensive%250Aexperiments%252C%2520we%2520identify%2520key%2520limitations%252C%2520such%2520as%2520insufficient%2520visual%250Aunderstanding%2520and%2520a%2520lack%2520of%2520conceptual%2520knowledge.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520training-free%2520framework%2520that%2520integrates%2520in-depth%2520object%2520extraction%250Aand%2520external%2520conceptual%2520knowledge%2520to%2520improve%2520the%2520model%2527s%2520ability%2520to%2520interpret%250Aand%2520explain%2520sarcasm%2520in%2520multimodal%2520contexts.%2520The%2520experimental%2520results%2520on%250Amultiple%2520models%2520show%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/cp-cp/LVLM-MSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Vision-Language%20Models%20Understand%20Multimodal%20Sarcasm%3F&entry.906535625=Xinyu%20Wang%20and%20Yue%20Zhang%20and%20Liqiang%20Jing&entry.1292438233=%20%20Sarcasm%20is%20a%20complex%20linguistic%20phenomenon%20that%20involves%20a%20disparity%20between%0Aliteral%20and%20intended%20meanings%2C%20making%20it%20challenging%20for%20sentiment%20analysis%20and%0Aother%20emotion-sensitive%20tasks.%20While%20traditional%20sarcasm%20detection%20methods%0Aprimarily%20focus%20on%20text%2C%20recent%20approaches%20have%20incorporated%20multimodal%0Ainformation.%20However%2C%20the%20application%20of%20Large%20Visual%20Language%20Models%20%28LVLMs%29%0Ain%20Multimodal%20Sarcasm%20Analysis%20%28MSA%29%20remains%20underexplored.%20In%20this%20paper%2C%20we%0Aevaluate%20LVLMs%20in%20MSA%20tasks%2C%20specifically%20focusing%20on%20Multimodal%20Sarcasm%0ADetection%20and%20Multimodal%20Sarcasm%20Explanation.%20Through%20comprehensive%0Aexperiments%2C%20we%20identify%20key%20limitations%2C%20such%20as%20insufficient%20visual%0Aunderstanding%20and%20a%20lack%20of%20conceptual%20knowledge.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20training-free%20framework%20that%20integrates%20in-depth%20object%20extraction%0Aand%20external%20conceptual%20knowledge%20to%20improve%20the%20model%27s%20ability%20to%20interpret%0Aand%20explain%20sarcasm%20in%20multimodal%20contexts.%20The%20experimental%20results%20on%0Amultiple%20models%20show%20the%20effectiveness%20of%20our%20proposed%20framework.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/cp-cp/LVLM-MSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03654v1&entry.124074799=Read"},
{"title": "The Starlink Robot: A Platform and Dataset for Mobile Satellite\n  Communication", "author": "Boyi Liu and Qianyi Zhang and Qiang Yang and Jianhao Jiao and Jagmohan Chauhan and Dimitrios Kanoulas", "abstract": "  The integration of satellite communication into mobile devices represents a\nparadigm shift in connectivity, yet the performance characteristics under\nmotion and environmental occlusion remain poorly understood. We present the\nStarlink Robot, the first mobile robotic platform equipped with Starlink\nsatellite internet, comprehensive sensor suite including upward-facing camera,\nLiDAR, and IMU, designed to systematically study satellite communication\nperformance during movement. Our multi-modal dataset captures synchronized\ncommunication metrics, motion dynamics, sky visibility, and 3D environmental\ncontext across diverse scenarios including steady-state motion, variable\nspeeds, and different occlusion conditions. This platform and dataset enable\nresearchers to develop motion-aware communication protocols, predict\nconnectivity disruptions, and optimize satellite communication for emerging\nmobile applications from smartphones to autonomous vehicles. In this work, we\nuse LEOViz for real-time satellite tracking and data collection. The starlink\nrobot project is available at https://github.com/StarlinkRobot.\n", "link": "http://arxiv.org/abs/2506.19781v3", "date": "2025-08-05", "relevancy": 2.0372, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5221}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Starlink%20Robot%3A%20A%20Platform%20and%20Dataset%20for%20Mobile%20Satellite%0A%20%20Communication&body=Title%3A%20The%20Starlink%20Robot%3A%20A%20Platform%20and%20Dataset%20for%20Mobile%20Satellite%0A%20%20Communication%0AAuthor%3A%20Boyi%20Liu%20and%20Qianyi%20Zhang%20and%20Qiang%20Yang%20and%20Jianhao%20Jiao%20and%20Jagmohan%20Chauhan%20and%20Dimitrios%20Kanoulas%0AAbstract%3A%20%20%20The%20integration%20of%20satellite%20communication%20into%20mobile%20devices%20represents%20a%0Aparadigm%20shift%20in%20connectivity%2C%20yet%20the%20performance%20characteristics%20under%0Amotion%20and%20environmental%20occlusion%20remain%20poorly%20understood.%20We%20present%20the%0AStarlink%20Robot%2C%20the%20first%20mobile%20robotic%20platform%20equipped%20with%20Starlink%0Asatellite%20internet%2C%20comprehensive%20sensor%20suite%20including%20upward-facing%20camera%2C%0ALiDAR%2C%20and%20IMU%2C%20designed%20to%20systematically%20study%20satellite%20communication%0Aperformance%20during%20movement.%20Our%20multi-modal%20dataset%20captures%20synchronized%0Acommunication%20metrics%2C%20motion%20dynamics%2C%20sky%20visibility%2C%20and%203D%20environmental%0Acontext%20across%20diverse%20scenarios%20including%20steady-state%20motion%2C%20variable%0Aspeeds%2C%20and%20different%20occlusion%20conditions.%20This%20platform%20and%20dataset%20enable%0Aresearchers%20to%20develop%20motion-aware%20communication%20protocols%2C%20predict%0Aconnectivity%20disruptions%2C%20and%20optimize%20satellite%20communication%20for%20emerging%0Amobile%20applications%20from%20smartphones%20to%20autonomous%20vehicles.%20In%20this%20work%2C%20we%0Ause%20LEOViz%20for%20real-time%20satellite%20tracking%20and%20data%20collection.%20The%20starlink%0Arobot%20project%20is%20available%20at%20https%3A//github.com/StarlinkRobot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Starlink%2520Robot%253A%2520A%2520Platform%2520and%2520Dataset%2520for%2520Mobile%2520Satellite%250A%2520%2520Communication%26entry.906535625%3DBoyi%2520Liu%2520and%2520Qianyi%2520Zhang%2520and%2520Qiang%2520Yang%2520and%2520Jianhao%2520Jiao%2520and%2520Jagmohan%2520Chauhan%2520and%2520Dimitrios%2520Kanoulas%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520satellite%2520communication%2520into%2520mobile%2520devices%2520represents%2520a%250Aparadigm%2520shift%2520in%2520connectivity%252C%2520yet%2520the%2520performance%2520characteristics%2520under%250Amotion%2520and%2520environmental%2520occlusion%2520remain%2520poorly%2520understood.%2520We%2520present%2520the%250AStarlink%2520Robot%252C%2520the%2520first%2520mobile%2520robotic%2520platform%2520equipped%2520with%2520Starlink%250Asatellite%2520internet%252C%2520comprehensive%2520sensor%2520suite%2520including%2520upward-facing%2520camera%252C%250ALiDAR%252C%2520and%2520IMU%252C%2520designed%2520to%2520systematically%2520study%2520satellite%2520communication%250Aperformance%2520during%2520movement.%2520Our%2520multi-modal%2520dataset%2520captures%2520synchronized%250Acommunication%2520metrics%252C%2520motion%2520dynamics%252C%2520sky%2520visibility%252C%2520and%25203D%2520environmental%250Acontext%2520across%2520diverse%2520scenarios%2520including%2520steady-state%2520motion%252C%2520variable%250Aspeeds%252C%2520and%2520different%2520occlusion%2520conditions.%2520This%2520platform%2520and%2520dataset%2520enable%250Aresearchers%2520to%2520develop%2520motion-aware%2520communication%2520protocols%252C%2520predict%250Aconnectivity%2520disruptions%252C%2520and%2520optimize%2520satellite%2520communication%2520for%2520emerging%250Amobile%2520applications%2520from%2520smartphones%2520to%2520autonomous%2520vehicles.%2520In%2520this%2520work%252C%2520we%250Ause%2520LEOViz%2520for%2520real-time%2520satellite%2520tracking%2520and%2520data%2520collection.%2520The%2520starlink%250Arobot%2520project%2520is%2520available%2520at%2520https%253A//github.com/StarlinkRobot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Starlink%20Robot%3A%20A%20Platform%20and%20Dataset%20for%20Mobile%20Satellite%0A%20%20Communication&entry.906535625=Boyi%20Liu%20and%20Qianyi%20Zhang%20and%20Qiang%20Yang%20and%20Jianhao%20Jiao%20and%20Jagmohan%20Chauhan%20and%20Dimitrios%20Kanoulas&entry.1292438233=%20%20The%20integration%20of%20satellite%20communication%20into%20mobile%20devices%20represents%20a%0Aparadigm%20shift%20in%20connectivity%2C%20yet%20the%20performance%20characteristics%20under%0Amotion%20and%20environmental%20occlusion%20remain%20poorly%20understood.%20We%20present%20the%0AStarlink%20Robot%2C%20the%20first%20mobile%20robotic%20platform%20equipped%20with%20Starlink%0Asatellite%20internet%2C%20comprehensive%20sensor%20suite%20including%20upward-facing%20camera%2C%0ALiDAR%2C%20and%20IMU%2C%20designed%20to%20systematically%20study%20satellite%20communication%0Aperformance%20during%20movement.%20Our%20multi-modal%20dataset%20captures%20synchronized%0Acommunication%20metrics%2C%20motion%20dynamics%2C%20sky%20visibility%2C%20and%203D%20environmental%0Acontext%20across%20diverse%20scenarios%20including%20steady-state%20motion%2C%20variable%0Aspeeds%2C%20and%20different%20occlusion%20conditions.%20This%20platform%20and%20dataset%20enable%0Aresearchers%20to%20develop%20motion-aware%20communication%20protocols%2C%20predict%0Aconnectivity%20disruptions%2C%20and%20optimize%20satellite%20communication%20for%20emerging%0Amobile%20applications%20from%20smartphones%20to%20autonomous%20vehicles.%20In%20this%20work%2C%20we%0Ause%20LEOViz%20for%20real-time%20satellite%20tracking%20and%20data%20collection.%20The%20starlink%0Arobot%20project%20is%20available%20at%20https%3A//github.com/StarlinkRobot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19781v3&entry.124074799=Read"},
{"title": "Efficient Morphology-Aware Policy Transfer to New Embodiments", "author": "Michael Przystupa and Hongyao Tang and Martin Jagersand and Santiago Miret and Mariano Phielipp and Matthew E. Taylor and Glen Berseth", "abstract": "  Morphology-aware policy learning is a means of enhancing policy sample\nefficiency by aggregating data from multiple agents. These types of policies\nhave previously been shown to help generalize over dynamic, kinematic, and limb\nconfiguration variations between agent morphologies. Unfortunately, these\npolicies still have sub-optimal zero-shot performance compared to end-to-end\nfinetuning on morphologies at deployment. This limitation has ramifications in\npractical applications such as robotics because further data collection to\nperform end-to-end finetuning can be computationally expensive. In this work,\nwe investigate combining morphology-aware pretraining with parameter efficient\nfinetuning (PEFT) techniques to help reduce the learnable parameters necessary\nto specialize a morphology-aware policy to a target embodiment. We compare\ndirectly tuning sub-sets of model weights, input learnable adapters, and prefix\ntuning techniques for online finetuning. Our analysis reveals that PEFT\ntechniques in conjunction with policy pre-training generally help reduce the\nnumber of samples to necessary to improve a policy compared to training models\nend-to-end from scratch. We further find that tuning as few as less than 1% of\ntotal parameters will improve policy performance compared the zero-shot\nperformance of the base pretrained a policy.\n", "link": "http://arxiv.org/abs/2508.03660v1", "date": "2025-08-05", "relevancy": 2.037, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Morphology-Aware%20Policy%20Transfer%20to%20New%20Embodiments&body=Title%3A%20Efficient%20Morphology-Aware%20Policy%20Transfer%20to%20New%20Embodiments%0AAuthor%3A%20Michael%20Przystupa%20and%20Hongyao%20Tang%20and%20Martin%20Jagersand%20and%20Santiago%20Miret%20and%20Mariano%20Phielipp%20and%20Matthew%20E.%20Taylor%20and%20Glen%20Berseth%0AAbstract%3A%20%20%20Morphology-aware%20policy%20learning%20is%20a%20means%20of%20enhancing%20policy%20sample%0Aefficiency%20by%20aggregating%20data%20from%20multiple%20agents.%20These%20types%20of%20policies%0Ahave%20previously%20been%20shown%20to%20help%20generalize%20over%20dynamic%2C%20kinematic%2C%20and%20limb%0Aconfiguration%20variations%20between%20agent%20morphologies.%20Unfortunately%2C%20these%0Apolicies%20still%20have%20sub-optimal%20zero-shot%20performance%20compared%20to%20end-to-end%0Afinetuning%20on%20morphologies%20at%20deployment.%20This%20limitation%20has%20ramifications%20in%0Apractical%20applications%20such%20as%20robotics%20because%20further%20data%20collection%20to%0Aperform%20end-to-end%20finetuning%20can%20be%20computationally%20expensive.%20In%20this%20work%2C%0Awe%20investigate%20combining%20morphology-aware%20pretraining%20with%20parameter%20efficient%0Afinetuning%20%28PEFT%29%20techniques%20to%20help%20reduce%20the%20learnable%20parameters%20necessary%0Ato%20specialize%20a%20morphology-aware%20policy%20to%20a%20target%20embodiment.%20We%20compare%0Adirectly%20tuning%20sub-sets%20of%20model%20weights%2C%20input%20learnable%20adapters%2C%20and%20prefix%0Atuning%20techniques%20for%20online%20finetuning.%20Our%20analysis%20reveals%20that%20PEFT%0Atechniques%20in%20conjunction%20with%20policy%20pre-training%20generally%20help%20reduce%20the%0Anumber%20of%20samples%20to%20necessary%20to%20improve%20a%20policy%20compared%20to%20training%20models%0Aend-to-end%20from%20scratch.%20We%20further%20find%20that%20tuning%20as%20few%20as%20less%20than%201%25%20of%0Atotal%20parameters%20will%20improve%20policy%20performance%20compared%20the%20zero-shot%0Aperformance%20of%20the%20base%20pretrained%20a%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Morphology-Aware%2520Policy%2520Transfer%2520to%2520New%2520Embodiments%26entry.906535625%3DMichael%2520Przystupa%2520and%2520Hongyao%2520Tang%2520and%2520Martin%2520Jagersand%2520and%2520Santiago%2520Miret%2520and%2520Mariano%2520Phielipp%2520and%2520Matthew%2520E.%2520Taylor%2520and%2520Glen%2520Berseth%26entry.1292438233%3D%2520%2520Morphology-aware%2520policy%2520learning%2520is%2520a%2520means%2520of%2520enhancing%2520policy%2520sample%250Aefficiency%2520by%2520aggregating%2520data%2520from%2520multiple%2520agents.%2520These%2520types%2520of%2520policies%250Ahave%2520previously%2520been%2520shown%2520to%2520help%2520generalize%2520over%2520dynamic%252C%2520kinematic%252C%2520and%2520limb%250Aconfiguration%2520variations%2520between%2520agent%2520morphologies.%2520Unfortunately%252C%2520these%250Apolicies%2520still%2520have%2520sub-optimal%2520zero-shot%2520performance%2520compared%2520to%2520end-to-end%250Afinetuning%2520on%2520morphologies%2520at%2520deployment.%2520This%2520limitation%2520has%2520ramifications%2520in%250Apractical%2520applications%2520such%2520as%2520robotics%2520because%2520further%2520data%2520collection%2520to%250Aperform%2520end-to-end%2520finetuning%2520can%2520be%2520computationally%2520expensive.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520combining%2520morphology-aware%2520pretraining%2520with%2520parameter%2520efficient%250Afinetuning%2520%2528PEFT%2529%2520techniques%2520to%2520help%2520reduce%2520the%2520learnable%2520parameters%2520necessary%250Ato%2520specialize%2520a%2520morphology-aware%2520policy%2520to%2520a%2520target%2520embodiment.%2520We%2520compare%250Adirectly%2520tuning%2520sub-sets%2520of%2520model%2520weights%252C%2520input%2520learnable%2520adapters%252C%2520and%2520prefix%250Atuning%2520techniques%2520for%2520online%2520finetuning.%2520Our%2520analysis%2520reveals%2520that%2520PEFT%250Atechniques%2520in%2520conjunction%2520with%2520policy%2520pre-training%2520generally%2520help%2520reduce%2520the%250Anumber%2520of%2520samples%2520to%2520necessary%2520to%2520improve%2520a%2520policy%2520compared%2520to%2520training%2520models%250Aend-to-end%2520from%2520scratch.%2520We%2520further%2520find%2520that%2520tuning%2520as%2520few%2520as%2520less%2520than%25201%2525%2520of%250Atotal%2520parameters%2520will%2520improve%2520policy%2520performance%2520compared%2520the%2520zero-shot%250Aperformance%2520of%2520the%2520base%2520pretrained%2520a%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Morphology-Aware%20Policy%20Transfer%20to%20New%20Embodiments&entry.906535625=Michael%20Przystupa%20and%20Hongyao%20Tang%20and%20Martin%20Jagersand%20and%20Santiago%20Miret%20and%20Mariano%20Phielipp%20and%20Matthew%20E.%20Taylor%20and%20Glen%20Berseth&entry.1292438233=%20%20Morphology-aware%20policy%20learning%20is%20a%20means%20of%20enhancing%20policy%20sample%0Aefficiency%20by%20aggregating%20data%20from%20multiple%20agents.%20These%20types%20of%20policies%0Ahave%20previously%20been%20shown%20to%20help%20generalize%20over%20dynamic%2C%20kinematic%2C%20and%20limb%0Aconfiguration%20variations%20between%20agent%20morphologies.%20Unfortunately%2C%20these%0Apolicies%20still%20have%20sub-optimal%20zero-shot%20performance%20compared%20to%20end-to-end%0Afinetuning%20on%20morphologies%20at%20deployment.%20This%20limitation%20has%20ramifications%20in%0Apractical%20applications%20such%20as%20robotics%20because%20further%20data%20collection%20to%0Aperform%20end-to-end%20finetuning%20can%20be%20computationally%20expensive.%20In%20this%20work%2C%0Awe%20investigate%20combining%20morphology-aware%20pretraining%20with%20parameter%20efficient%0Afinetuning%20%28PEFT%29%20techniques%20to%20help%20reduce%20the%20learnable%20parameters%20necessary%0Ato%20specialize%20a%20morphology-aware%20policy%20to%20a%20target%20embodiment.%20We%20compare%0Adirectly%20tuning%20sub-sets%20of%20model%20weights%2C%20input%20learnable%20adapters%2C%20and%20prefix%0Atuning%20techniques%20for%20online%20finetuning.%20Our%20analysis%20reveals%20that%20PEFT%0Atechniques%20in%20conjunction%20with%20policy%20pre-training%20generally%20help%20reduce%20the%0Anumber%20of%20samples%20to%20necessary%20to%20improve%20a%20policy%20compared%20to%20training%20models%0Aend-to-end%20from%20scratch.%20We%20further%20find%20that%20tuning%20as%20few%20as%20less%20than%201%25%20of%0Atotal%20parameters%20will%20improve%20policy%20performance%20compared%20the%20zero-shot%0Aperformance%20of%20the%20base%20pretrained%20a%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03660v1&entry.124074799=Read"},
{"title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization", "author": "Yihong Dong and Xue Jiang and Yongding Tao and Huanyu Liu and Kechi Zhang and Lili Mou and Rongyu Cao and Yingwei Ma and Jue Chen and Binhua Li and Zhi Jin and Fei Huang and Yongbin Li and Ge Li", "abstract": "  Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address for distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.\n", "link": "http://arxiv.org/abs/2508.00222v2", "date": "2025-08-05", "relevancy": 1.9955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-PLUS%3A%20Countering%20Capability%20Boundary%20Collapse%20of%20LLMs%20in%0A%20%20Reinforcement%20Learning%20with%20Hybrid-policy%20Optimization&body=Title%3A%20RL-PLUS%3A%20Countering%20Capability%20Boundary%20Collapse%20of%20LLMs%20in%0A%20%20Reinforcement%20Learning%20with%20Hybrid-policy%20Optimization%0AAuthor%3A%20Yihong%20Dong%20and%20Xue%20Jiang%20and%20Yongding%20Tao%20and%20Huanyu%20Liu%20and%20Kechi%20Zhang%20and%20Lili%20Mou%20and%20Rongyu%20Cao%20and%20Yingwei%20Ma%20and%20Jue%20Chen%20and%20Binhua%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Ge%20Li%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20significantly%0Aadvanced%20the%20complex%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20it%20struggles%20to%20break%20through%20the%20inherent%20capability%20boundaries%20of%0Athe%20base%20LLM%2C%20due%20to%20its%20essentially%20on-policy%20strategy%20coupled%20with%20LLM%27s%0Aimmense%20action%20space%20and%20sparse%20reward.%20Critically%2C%20RLVR%20can%20lead%20to%20the%0Acapability%20boundary%20collapse%2C%20narrowing%20the%20LLM%27s%20problem-solving%20scope.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20RL-PLUS%2C%20a%20novel%20hybrid-policy%20optimization%0Aapproach%20for%20LLMs%20that%20synergizes%20internal%20exploitation%20with%20external%20data%20to%0Aachieve%20stronger%20reasoning%20capabilities%20and%20surpass%20the%20boundaries%20of%20base%0Amodels.%20RL-PLUS%20integrates%20two%20core%20components%2C%20i.e.%2C%20Multiple%20Importance%0ASampling%20to%20address%20for%20distributional%20mismatch%20from%20external%20data%2C%20and%0AExploration-Based%20Advantage%20Function%20to%20guide%20the%20model%20towards%20high-value%2C%0Aunexplored%20reasoning%20paths.%20We%20provide%20both%20theoretical%20analysis%20and%20extensive%0Aexperiments%20to%20demonstrate%20the%20superiority%20and%20generalizability%20of%20our%0Aapproach.%20Compared%20with%20existing%20RLVR%20methods%2C%20RL-PLUS%20achieves%201%29%0Astate-of-the-art%20performance%20on%20six%20math%20reasoning%20benchmarks%3B%202%29%20superior%0Aperformance%20on%20six%20out-of-distribution%20reasoning%20tasks%3B%203%29%20consistent%20and%0Asignificant%20gains%20across%20diverse%20model%20families%2C%20with%20average%20relative%0Aimprovements%20up%20to%2069.2%5C%25.%20Moreover%2C%20the%20analysis%20of%20Pass%40k%20curves%20indicates%0Athat%20RL-PLUS%20effectively%20resolves%20the%20capability%20boundary%20collapse%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-PLUS%253A%2520Countering%2520Capability%2520Boundary%2520Collapse%2520of%2520LLMs%2520in%250A%2520%2520Reinforcement%2520Learning%2520with%2520Hybrid-policy%2520Optimization%26entry.906535625%3DYihong%2520Dong%2520and%2520Xue%2520Jiang%2520and%2520Yongding%2520Tao%2520and%2520Huanyu%2520Liu%2520and%2520Kechi%2520Zhang%2520and%2520Lili%2520Mou%2520and%2520Rongyu%2520Cao%2520and%2520Yingwei%2520Ma%2520and%2520Jue%2520Chen%2520and%2520Binhua%2520Li%2520and%2520Zhi%2520Jin%2520and%2520Fei%2520Huang%2520and%2520Yongbin%2520Li%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Reward%2520%2528RLVR%2529%2520has%2520significantly%250Aadvanced%2520the%2520complex%2520reasoning%2520abilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AHowever%252C%2520it%2520struggles%2520to%2520break%2520through%2520the%2520inherent%2520capability%2520boundaries%2520of%250Athe%2520base%2520LLM%252C%2520due%2520to%2520its%2520essentially%2520on-policy%2520strategy%2520coupled%2520with%2520LLM%2527s%250Aimmense%2520action%2520space%2520and%2520sparse%2520reward.%2520Critically%252C%2520RLVR%2520can%2520lead%2520to%2520the%250Acapability%2520boundary%2520collapse%252C%2520narrowing%2520the%2520LLM%2527s%2520problem-solving%2520scope.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520RL-PLUS%252C%2520a%2520novel%2520hybrid-policy%2520optimization%250Aapproach%2520for%2520LLMs%2520that%2520synergizes%2520internal%2520exploitation%2520with%2520external%2520data%2520to%250Aachieve%2520stronger%2520reasoning%2520capabilities%2520and%2520surpass%2520the%2520boundaries%2520of%2520base%250Amodels.%2520RL-PLUS%2520integrates%2520two%2520core%2520components%252C%2520i.e.%252C%2520Multiple%2520Importance%250ASampling%2520to%2520address%2520for%2520distributional%2520mismatch%2520from%2520external%2520data%252C%2520and%250AExploration-Based%2520Advantage%2520Function%2520to%2520guide%2520the%2520model%2520towards%2520high-value%252C%250Aunexplored%2520reasoning%2520paths.%2520We%2520provide%2520both%2520theoretical%2520analysis%2520and%2520extensive%250Aexperiments%2520to%2520demonstrate%2520the%2520superiority%2520and%2520generalizability%2520of%2520our%250Aapproach.%2520Compared%2520with%2520existing%2520RLVR%2520methods%252C%2520RL-PLUS%2520achieves%25201%2529%250Astate-of-the-art%2520performance%2520on%2520six%2520math%2520reasoning%2520benchmarks%253B%25202%2529%2520superior%250Aperformance%2520on%2520six%2520out-of-distribution%2520reasoning%2520tasks%253B%25203%2529%2520consistent%2520and%250Asignificant%2520gains%2520across%2520diverse%2520model%2520families%252C%2520with%2520average%2520relative%250Aimprovements%2520up%2520to%252069.2%255C%2525.%2520Moreover%252C%2520the%2520analysis%2520of%2520Pass%2540k%2520curves%2520indicates%250Athat%2520RL-PLUS%2520effectively%2520resolves%2520the%2520capability%2520boundary%2520collapse%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-PLUS%3A%20Countering%20Capability%20Boundary%20Collapse%20of%20LLMs%20in%0A%20%20Reinforcement%20Learning%20with%20Hybrid-policy%20Optimization&entry.906535625=Yihong%20Dong%20and%20Xue%20Jiang%20and%20Yongding%20Tao%20and%20Huanyu%20Liu%20and%20Kechi%20Zhang%20and%20Lili%20Mou%20and%20Rongyu%20Cao%20and%20Yingwei%20Ma%20and%20Jue%20Chen%20and%20Binhua%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Ge%20Li&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20significantly%0Aadvanced%20the%20complex%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20it%20struggles%20to%20break%20through%20the%20inherent%20capability%20boundaries%20of%0Athe%20base%20LLM%2C%20due%20to%20its%20essentially%20on-policy%20strategy%20coupled%20with%20LLM%27s%0Aimmense%20action%20space%20and%20sparse%20reward.%20Critically%2C%20RLVR%20can%20lead%20to%20the%0Acapability%20boundary%20collapse%2C%20narrowing%20the%20LLM%27s%20problem-solving%20scope.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20RL-PLUS%2C%20a%20novel%20hybrid-policy%20optimization%0Aapproach%20for%20LLMs%20that%20synergizes%20internal%20exploitation%20with%20external%20data%20to%0Aachieve%20stronger%20reasoning%20capabilities%20and%20surpass%20the%20boundaries%20of%20base%0Amodels.%20RL-PLUS%20integrates%20two%20core%20components%2C%20i.e.%2C%20Multiple%20Importance%0ASampling%20to%20address%20for%20distributional%20mismatch%20from%20external%20data%2C%20and%0AExploration-Based%20Advantage%20Function%20to%20guide%20the%20model%20towards%20high-value%2C%0Aunexplored%20reasoning%20paths.%20We%20provide%20both%20theoretical%20analysis%20and%20extensive%0Aexperiments%20to%20demonstrate%20the%20superiority%20and%20generalizability%20of%20our%0Aapproach.%20Compared%20with%20existing%20RLVR%20methods%2C%20RL-PLUS%20achieves%201%29%0Astate-of-the-art%20performance%20on%20six%20math%20reasoning%20benchmarks%3B%202%29%20superior%0Aperformance%20on%20six%20out-of-distribution%20reasoning%20tasks%3B%203%29%20consistent%20and%0Asignificant%20gains%20across%20diverse%20model%20families%2C%20with%20average%20relative%0Aimprovements%20up%20to%2069.2%5C%25.%20Moreover%2C%20the%20analysis%20of%20Pass%40k%20curves%20indicates%0Athat%20RL-PLUS%20effectively%20resolves%20the%20capability%20boundary%20collapse%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00222v2&entry.124074799=Read"},
{"title": "$\\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection", "author": "Daniil Orel and Indraneil Paul and Iryna Gurevych and Preslav Nakov", "abstract": "  In this work, we compile $\\textbf{$\\texttt{DroidCollection}$}$, the most\nextensive open data suite for training and evaluating machine-generated code\ndetectors, comprising over a million code samples, seven programming languages,\noutputs from 43 coding models, and over three real-world coding domains.\nAlongside fully AI-generated samples, our collection includes human-AI\nco-authored code, as well as adversarial samples explicitly crafted to evade\ndetection. Subsequently, we develop $\\textbf{$\\texttt{DroidDetect}$}$, a suite\nof encoder-only detectors trained using a multi-task objective over\n$\\texttt{DroidCollection}$. Our experiments show that existing detectors'\nperformance fails to generalise to diverse coding domains and programming\nlanguages outside of their narrow training data. Additionally, we demonstrate\nthat while most detectors are easily compromised by humanising the output\ndistributions using superficial prompting and alignment approaches, this\nproblem can be easily amended by training on a small amount of adversarial\ndata. Finally, we demonstrate the effectiveness of metric learning and\nuncertainty-based resampling as means to enhance detector training on possibly\nnoisy distributions.\n", "link": "http://arxiv.org/abs/2507.10583v2", "date": "2025-08-05", "relevancy": 1.9535, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4842}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctexttt%7BDroid%7D%24%3A%20A%20Resource%20Suite%20for%20AI-Generated%20Code%20Detection&body=Title%3A%20%24%5Ctexttt%7BDroid%7D%24%3A%20A%20Resource%20Suite%20for%20AI-Generated%20Code%20Detection%0AAuthor%3A%20Daniil%20Orel%20and%20Indraneil%20Paul%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20compile%20%24%5Ctextbf%7B%24%5Ctexttt%7BDroidCollection%7D%24%7D%24%2C%20the%20most%0Aextensive%20open%20data%20suite%20for%20training%20and%20evaluating%20machine-generated%20code%0Adetectors%2C%20comprising%20over%20a%20million%20code%20samples%2C%20seven%20programming%20languages%2C%0Aoutputs%20from%2043%20coding%20models%2C%20and%20over%20three%20real-world%20coding%20domains.%0AAlongside%20fully%20AI-generated%20samples%2C%20our%20collection%20includes%20human-AI%0Aco-authored%20code%2C%20as%20well%20as%20adversarial%20samples%20explicitly%20crafted%20to%20evade%0Adetection.%20Subsequently%2C%20we%20develop%20%24%5Ctextbf%7B%24%5Ctexttt%7BDroidDetect%7D%24%7D%24%2C%20a%20suite%0Aof%20encoder-only%20detectors%20trained%20using%20a%20multi-task%20objective%20over%0A%24%5Ctexttt%7BDroidCollection%7D%24.%20Our%20experiments%20show%20that%20existing%20detectors%27%0Aperformance%20fails%20to%20generalise%20to%20diverse%20coding%20domains%20and%20programming%0Alanguages%20outside%20of%20their%20narrow%20training%20data.%20Additionally%2C%20we%20demonstrate%0Athat%20while%20most%20detectors%20are%20easily%20compromised%20by%20humanising%20the%20output%0Adistributions%20using%20superficial%20prompting%20and%20alignment%20approaches%2C%20this%0Aproblem%20can%20be%20easily%20amended%20by%20training%20on%20a%20small%20amount%20of%20adversarial%0Adata.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20metric%20learning%20and%0Auncertainty-based%20resampling%20as%20means%20to%20enhance%20detector%20training%20on%20possibly%0Anoisy%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10583v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctexttt%257BDroid%257D%2524%253A%2520A%2520Resource%2520Suite%2520for%2520AI-Generated%2520Code%2520Detection%26entry.906535625%3DDaniil%2520Orel%2520and%2520Indraneil%2520Paul%2520and%2520Iryna%2520Gurevych%2520and%2520Preslav%2520Nakov%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520compile%2520%2524%255Ctextbf%257B%2524%255Ctexttt%257BDroidCollection%257D%2524%257D%2524%252C%2520the%2520most%250Aextensive%2520open%2520data%2520suite%2520for%2520training%2520and%2520evaluating%2520machine-generated%2520code%250Adetectors%252C%2520comprising%2520over%2520a%2520million%2520code%2520samples%252C%2520seven%2520programming%2520languages%252C%250Aoutputs%2520from%252043%2520coding%2520models%252C%2520and%2520over%2520three%2520real-world%2520coding%2520domains.%250AAlongside%2520fully%2520AI-generated%2520samples%252C%2520our%2520collection%2520includes%2520human-AI%250Aco-authored%2520code%252C%2520as%2520well%2520as%2520adversarial%2520samples%2520explicitly%2520crafted%2520to%2520evade%250Adetection.%2520Subsequently%252C%2520we%2520develop%2520%2524%255Ctextbf%257B%2524%255Ctexttt%257BDroidDetect%257D%2524%257D%2524%252C%2520a%2520suite%250Aof%2520encoder-only%2520detectors%2520trained%2520using%2520a%2520multi-task%2520objective%2520over%250A%2524%255Ctexttt%257BDroidCollection%257D%2524.%2520Our%2520experiments%2520show%2520that%2520existing%2520detectors%2527%250Aperformance%2520fails%2520to%2520generalise%2520to%2520diverse%2520coding%2520domains%2520and%2520programming%250Alanguages%2520outside%2520of%2520their%2520narrow%2520training%2520data.%2520Additionally%252C%2520we%2520demonstrate%250Athat%2520while%2520most%2520detectors%2520are%2520easily%2520compromised%2520by%2520humanising%2520the%2520output%250Adistributions%2520using%2520superficial%2520prompting%2520and%2520alignment%2520approaches%252C%2520this%250Aproblem%2520can%2520be%2520easily%2520amended%2520by%2520training%2520on%2520a%2520small%2520amount%2520of%2520adversarial%250Adata.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520metric%2520learning%2520and%250Auncertainty-based%2520resampling%2520as%2520means%2520to%2520enhance%2520detector%2520training%2520on%2520possibly%250Anoisy%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10583v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctexttt%7BDroid%7D%24%3A%20A%20Resource%20Suite%20for%20AI-Generated%20Code%20Detection&entry.906535625=Daniil%20Orel%20and%20Indraneil%20Paul%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov&entry.1292438233=%20%20In%20this%20work%2C%20we%20compile%20%24%5Ctextbf%7B%24%5Ctexttt%7BDroidCollection%7D%24%7D%24%2C%20the%20most%0Aextensive%20open%20data%20suite%20for%20training%20and%20evaluating%20machine-generated%20code%0Adetectors%2C%20comprising%20over%20a%20million%20code%20samples%2C%20seven%20programming%20languages%2C%0Aoutputs%20from%2043%20coding%20models%2C%20and%20over%20three%20real-world%20coding%20domains.%0AAlongside%20fully%20AI-generated%20samples%2C%20our%20collection%20includes%20human-AI%0Aco-authored%20code%2C%20as%20well%20as%20adversarial%20samples%20explicitly%20crafted%20to%20evade%0Adetection.%20Subsequently%2C%20we%20develop%20%24%5Ctextbf%7B%24%5Ctexttt%7BDroidDetect%7D%24%7D%24%2C%20a%20suite%0Aof%20encoder-only%20detectors%20trained%20using%20a%20multi-task%20objective%20over%0A%24%5Ctexttt%7BDroidCollection%7D%24.%20Our%20experiments%20show%20that%20existing%20detectors%27%0Aperformance%20fails%20to%20generalise%20to%20diverse%20coding%20domains%20and%20programming%0Alanguages%20outside%20of%20their%20narrow%20training%20data.%20Additionally%2C%20we%20demonstrate%0Athat%20while%20most%20detectors%20are%20easily%20compromised%20by%20humanising%20the%20output%0Adistributions%20using%20superficial%20prompting%20and%20alignment%20approaches%2C%20this%0Aproblem%20can%20be%20easily%20amended%20by%20training%20on%20a%20small%20amount%20of%20adversarial%0Adata.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20metric%20learning%20and%0Auncertainty-based%20resampling%20as%20means%20to%20enhance%20detector%20training%20on%20possibly%0Anoisy%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10583v2&entry.124074799=Read"},
{"title": "Learning quadratic neural networks in high dimensions: SGD dynamics and\n  scaling laws", "author": "G\u00e9rard Ben Arous and Murat A. Erdogdu and N. Mert Vural and Denny Wu", "abstract": "  We study the optimization and sample complexity of gradient-based training of\na two-layer neural network with quadratic activation function in the\nhigh-dimensional regime, where the data is generated as $y \\propto\n\\sum_{j=1}^{r}\\lambda_j \\sigma\\left(\\langle \\boldsymbol{\\theta_j},\n\\boldsymbol{x}\\rangle\\right), \\boldsymbol{x} \\sim N(0,\\boldsymbol{I}_d)$,\n$\\sigma$ is the 2nd Hermite polynomial, and $\\lbrace\\boldsymbol{\\theta}_j\n\\rbrace_{j=1}^{r} \\subset \\mathbb{R}^d$ are orthonormal signal directions. We\nconsider the extensive-width regime $r \\asymp d^\\beta$ for $\\beta \\in [0, 1)$,\nand assume a power-law decay on the (non-negative) second-layer coefficients\n$\\lambda_j\\asymp j^{-\\alpha}$ for $\\alpha \\geq 0$. We present a sharp analysis\nof the SGD dynamics in the feature learning regime, for both the population\nlimit and the finite-sample (online) discretization, and derive scaling laws\nfor the prediction risk that highlight the power-law dependencies on the\noptimization time, sample size, and model width. Our analysis combines a\nprecise characterization of the associated matrix Riccati differential equation\nwith novel matrix monotonicity arguments to establish convergence guarantees\nfor the infinite-dimensional effective dynamics.\n", "link": "http://arxiv.org/abs/2508.03688v1", "date": "2025-08-05", "relevancy": 1.9457, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4815}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20quadratic%20neural%20networks%20in%20high%20dimensions%3A%20SGD%20dynamics%20and%0A%20%20scaling%20laws&body=Title%3A%20Learning%20quadratic%20neural%20networks%20in%20high%20dimensions%3A%20SGD%20dynamics%20and%0A%20%20scaling%20laws%0AAuthor%3A%20G%C3%A9rard%20Ben%20Arous%20and%20Murat%20A.%20Erdogdu%20and%20N.%20Mert%20Vural%20and%20Denny%20Wu%0AAbstract%3A%20%20%20We%20study%20the%20optimization%20and%20sample%20complexity%20of%20gradient-based%20training%20of%0Aa%20two-layer%20neural%20network%20with%20quadratic%20activation%20function%20in%20the%0Ahigh-dimensional%20regime%2C%20where%20the%20data%20is%20generated%20as%20%24y%20%5Cpropto%0A%5Csum_%7Bj%3D1%7D%5E%7Br%7D%5Clambda_j%20%5Csigma%5Cleft%28%5Clangle%20%5Cboldsymbol%7B%5Ctheta_j%7D%2C%0A%5Cboldsymbol%7Bx%7D%5Crangle%5Cright%29%2C%20%5Cboldsymbol%7Bx%7D%20%5Csim%20N%280%2C%5Cboldsymbol%7BI%7D_d%29%24%2C%0A%24%5Csigma%24%20is%20the%202nd%20Hermite%20polynomial%2C%20and%20%24%5Clbrace%5Cboldsymbol%7B%5Ctheta%7D_j%0A%5Crbrace_%7Bj%3D1%7D%5E%7Br%7D%20%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20are%20orthonormal%20signal%20directions.%20We%0Aconsider%20the%20extensive-width%20regime%20%24r%20%5Casymp%20d%5E%5Cbeta%24%20for%20%24%5Cbeta%20%5Cin%20%5B0%2C%201%29%24%2C%0Aand%20assume%20a%20power-law%20decay%20on%20the%20%28non-negative%29%20second-layer%20coefficients%0A%24%5Clambda_j%5Casymp%20j%5E%7B-%5Calpha%7D%24%20for%20%24%5Calpha%20%5Cgeq%200%24.%20We%20present%20a%20sharp%20analysis%0Aof%20the%20SGD%20dynamics%20in%20the%20feature%20learning%20regime%2C%20for%20both%20the%20population%0Alimit%20and%20the%20finite-sample%20%28online%29%20discretization%2C%20and%20derive%20scaling%20laws%0Afor%20the%20prediction%20risk%20that%20highlight%20the%20power-law%20dependencies%20on%20the%0Aoptimization%20time%2C%20sample%20size%2C%20and%20model%20width.%20Our%20analysis%20combines%20a%0Aprecise%20characterization%20of%20the%20associated%20matrix%20Riccati%20differential%20equation%0Awith%20novel%20matrix%20monotonicity%20arguments%20to%20establish%20convergence%20guarantees%0Afor%20the%20infinite-dimensional%20effective%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520quadratic%2520neural%2520networks%2520in%2520high%2520dimensions%253A%2520SGD%2520dynamics%2520and%250A%2520%2520scaling%2520laws%26entry.906535625%3DG%25C3%25A9rard%2520Ben%2520Arous%2520and%2520Murat%2520A.%2520Erdogdu%2520and%2520N.%2520Mert%2520Vural%2520and%2520Denny%2520Wu%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520optimization%2520and%2520sample%2520complexity%2520of%2520gradient-based%2520training%2520of%250Aa%2520two-layer%2520neural%2520network%2520with%2520quadratic%2520activation%2520function%2520in%2520the%250Ahigh-dimensional%2520regime%252C%2520where%2520the%2520data%2520is%2520generated%2520as%2520%2524y%2520%255Cpropto%250A%255Csum_%257Bj%253D1%257D%255E%257Br%257D%255Clambda_j%2520%255Csigma%255Cleft%2528%255Clangle%2520%255Cboldsymbol%257B%255Ctheta_j%257D%252C%250A%255Cboldsymbol%257Bx%257D%255Crangle%255Cright%2529%252C%2520%255Cboldsymbol%257Bx%257D%2520%255Csim%2520N%25280%252C%255Cboldsymbol%257BI%257D_d%2529%2524%252C%250A%2524%255Csigma%2524%2520is%2520the%25202nd%2520Hermite%2520polynomial%252C%2520and%2520%2524%255Clbrace%255Cboldsymbol%257B%255Ctheta%257D_j%250A%255Crbrace_%257Bj%253D1%257D%255E%257Br%257D%2520%255Csubset%2520%255Cmathbb%257BR%257D%255Ed%2524%2520are%2520orthonormal%2520signal%2520directions.%2520We%250Aconsider%2520the%2520extensive-width%2520regime%2520%2524r%2520%255Casymp%2520d%255E%255Cbeta%2524%2520for%2520%2524%255Cbeta%2520%255Cin%2520%255B0%252C%25201%2529%2524%252C%250Aand%2520assume%2520a%2520power-law%2520decay%2520on%2520the%2520%2528non-negative%2529%2520second-layer%2520coefficients%250A%2524%255Clambda_j%255Casymp%2520j%255E%257B-%255Calpha%257D%2524%2520for%2520%2524%255Calpha%2520%255Cgeq%25200%2524.%2520We%2520present%2520a%2520sharp%2520analysis%250Aof%2520the%2520SGD%2520dynamics%2520in%2520the%2520feature%2520learning%2520regime%252C%2520for%2520both%2520the%2520population%250Alimit%2520and%2520the%2520finite-sample%2520%2528online%2529%2520discretization%252C%2520and%2520derive%2520scaling%2520laws%250Afor%2520the%2520prediction%2520risk%2520that%2520highlight%2520the%2520power-law%2520dependencies%2520on%2520the%250Aoptimization%2520time%252C%2520sample%2520size%252C%2520and%2520model%2520width.%2520Our%2520analysis%2520combines%2520a%250Aprecise%2520characterization%2520of%2520the%2520associated%2520matrix%2520Riccati%2520differential%2520equation%250Awith%2520novel%2520matrix%2520monotonicity%2520arguments%2520to%2520establish%2520convergence%2520guarantees%250Afor%2520the%2520infinite-dimensional%2520effective%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20quadratic%20neural%20networks%20in%20high%20dimensions%3A%20SGD%20dynamics%20and%0A%20%20scaling%20laws&entry.906535625=G%C3%A9rard%20Ben%20Arous%20and%20Murat%20A.%20Erdogdu%20and%20N.%20Mert%20Vural%20and%20Denny%20Wu&entry.1292438233=%20%20We%20study%20the%20optimization%20and%20sample%20complexity%20of%20gradient-based%20training%20of%0Aa%20two-layer%20neural%20network%20with%20quadratic%20activation%20function%20in%20the%0Ahigh-dimensional%20regime%2C%20where%20the%20data%20is%20generated%20as%20%24y%20%5Cpropto%0A%5Csum_%7Bj%3D1%7D%5E%7Br%7D%5Clambda_j%20%5Csigma%5Cleft%28%5Clangle%20%5Cboldsymbol%7B%5Ctheta_j%7D%2C%0A%5Cboldsymbol%7Bx%7D%5Crangle%5Cright%29%2C%20%5Cboldsymbol%7Bx%7D%20%5Csim%20N%280%2C%5Cboldsymbol%7BI%7D_d%29%24%2C%0A%24%5Csigma%24%20is%20the%202nd%20Hermite%20polynomial%2C%20and%20%24%5Clbrace%5Cboldsymbol%7B%5Ctheta%7D_j%0A%5Crbrace_%7Bj%3D1%7D%5E%7Br%7D%20%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20are%20orthonormal%20signal%20directions.%20We%0Aconsider%20the%20extensive-width%20regime%20%24r%20%5Casymp%20d%5E%5Cbeta%24%20for%20%24%5Cbeta%20%5Cin%20%5B0%2C%201%29%24%2C%0Aand%20assume%20a%20power-law%20decay%20on%20the%20%28non-negative%29%20second-layer%20coefficients%0A%24%5Clambda_j%5Casymp%20j%5E%7B-%5Calpha%7D%24%20for%20%24%5Calpha%20%5Cgeq%200%24.%20We%20present%20a%20sharp%20analysis%0Aof%20the%20SGD%20dynamics%20in%20the%20feature%20learning%20regime%2C%20for%20both%20the%20population%0Alimit%20and%20the%20finite-sample%20%28online%29%20discretization%2C%20and%20derive%20scaling%20laws%0Afor%20the%20prediction%20risk%20that%20highlight%20the%20power-law%20dependencies%20on%20the%0Aoptimization%20time%2C%20sample%20size%2C%20and%20model%20width.%20Our%20analysis%20combines%20a%0Aprecise%20characterization%20of%20the%20associated%20matrix%20Riccati%20differential%20equation%0Awith%20novel%20matrix%20monotonicity%20arguments%20to%20establish%20convergence%20guarantees%0Afor%20the%20infinite-dimensional%20effective%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03688v1&entry.124074799=Read"},
{"title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural\n  Network Robustness", "author": "Nathan Drenkow and Mathias Unberath", "abstract": "  Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) that have been widely shown to exhibit sensitivity to changes\nin imaging conditions. Conventional image quality assessment (IQA) seeks to\nmeasure and align quality relative to human perceptual judgments, but we often\nneed a metric that is not only sensitive to imaging conditions but also\nwell-aligned with DNN sensitivities. We first ask whether conventional IQA\nmetrics are also informative of DNN performance. We show theoretically and\nempirically that conventional IQA metrics are weak predictors of DNN\nperformance for image classification. Using our causal framework, we then\ndevelop metrics that exhibit strong correlation with DNN performance, thus\nenabling us to effectively estimate the quality distribution of large image\ndatasets relative to targeted vision tasks.\n", "link": "http://arxiv.org/abs/2503.02797v2", "date": "2025-08-05", "relevancy": 1.9279, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4805}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Causal%20Framework%20for%20Aligning%20Image%20Quality%20Metrics%20and%20Deep%20Neural%0A%20%20Network%20Robustness&body=Title%3A%20A%20Causal%20Framework%20for%20Aligning%20Image%20Quality%20Metrics%20and%20Deep%20Neural%0A%20%20Network%20Robustness%0AAuthor%3A%20Nathan%20Drenkow%20and%20Mathias%20Unberath%0AAbstract%3A%20%20%20Image%20quality%20plays%20an%20important%20role%20in%20the%20performance%20of%20deep%20neural%0Anetworks%20%28DNNs%29%20that%20have%20been%20widely%20shown%20to%20exhibit%20sensitivity%20to%20changes%0Ain%20imaging%20conditions.%20Conventional%20image%20quality%20assessment%20%28IQA%29%20seeks%20to%0Ameasure%20and%20align%20quality%20relative%20to%20human%20perceptual%20judgments%2C%20but%20we%20often%0Aneed%20a%20metric%20that%20is%20not%20only%20sensitive%20to%20imaging%20conditions%20but%20also%0Awell-aligned%20with%20DNN%20sensitivities.%20We%20first%20ask%20whether%20conventional%20IQA%0Ametrics%20are%20also%20informative%20of%20DNN%20performance.%20We%20show%20theoretically%20and%0Aempirically%20that%20conventional%20IQA%20metrics%20are%20weak%20predictors%20of%20DNN%0Aperformance%20for%20image%20classification.%20Using%20our%20causal%20framework%2C%20we%20then%0Adevelop%20metrics%20that%20exhibit%20strong%20correlation%20with%20DNN%20performance%2C%20thus%0Aenabling%20us%20to%20effectively%20estimate%20the%20quality%20distribution%20of%20large%20image%0Adatasets%20relative%20to%20targeted%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Causal%2520Framework%2520for%2520Aligning%2520Image%2520Quality%2520Metrics%2520and%2520Deep%2520Neural%250A%2520%2520Network%2520Robustness%26entry.906535625%3DNathan%2520Drenkow%2520and%2520Mathias%2520Unberath%26entry.1292438233%3D%2520%2520Image%2520quality%2520plays%2520an%2520important%2520role%2520in%2520the%2520performance%2520of%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%2520that%2520have%2520been%2520widely%2520shown%2520to%2520exhibit%2520sensitivity%2520to%2520changes%250Ain%2520imaging%2520conditions.%2520Conventional%2520image%2520quality%2520assessment%2520%2528IQA%2529%2520seeks%2520to%250Ameasure%2520and%2520align%2520quality%2520relative%2520to%2520human%2520perceptual%2520judgments%252C%2520but%2520we%2520often%250Aneed%2520a%2520metric%2520that%2520is%2520not%2520only%2520sensitive%2520to%2520imaging%2520conditions%2520but%2520also%250Awell-aligned%2520with%2520DNN%2520sensitivities.%2520We%2520first%2520ask%2520whether%2520conventional%2520IQA%250Ametrics%2520are%2520also%2520informative%2520of%2520DNN%2520performance.%2520We%2520show%2520theoretically%2520and%250Aempirically%2520that%2520conventional%2520IQA%2520metrics%2520are%2520weak%2520predictors%2520of%2520DNN%250Aperformance%2520for%2520image%2520classification.%2520Using%2520our%2520causal%2520framework%252C%2520we%2520then%250Adevelop%2520metrics%2520that%2520exhibit%2520strong%2520correlation%2520with%2520DNN%2520performance%252C%2520thus%250Aenabling%2520us%2520to%2520effectively%2520estimate%2520the%2520quality%2520distribution%2520of%2520large%2520image%250Adatasets%2520relative%2520to%2520targeted%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Causal%20Framework%20for%20Aligning%20Image%20Quality%20Metrics%20and%20Deep%20Neural%0A%20%20Network%20Robustness&entry.906535625=Nathan%20Drenkow%20and%20Mathias%20Unberath&entry.1292438233=%20%20Image%20quality%20plays%20an%20important%20role%20in%20the%20performance%20of%20deep%20neural%0Anetworks%20%28DNNs%29%20that%20have%20been%20widely%20shown%20to%20exhibit%20sensitivity%20to%20changes%0Ain%20imaging%20conditions.%20Conventional%20image%20quality%20assessment%20%28IQA%29%20seeks%20to%0Ameasure%20and%20align%20quality%20relative%20to%20human%20perceptual%20judgments%2C%20but%20we%20often%0Aneed%20a%20metric%20that%20is%20not%20only%20sensitive%20to%20imaging%20conditions%20but%20also%0Awell-aligned%20with%20DNN%20sensitivities.%20We%20first%20ask%20whether%20conventional%20IQA%0Ametrics%20are%20also%20informative%20of%20DNN%20performance.%20We%20show%20theoretically%20and%0Aempirically%20that%20conventional%20IQA%20metrics%20are%20weak%20predictors%20of%20DNN%0Aperformance%20for%20image%20classification.%20Using%20our%20causal%20framework%2C%20we%20then%0Adevelop%20metrics%20that%20exhibit%20strong%20correlation%20with%20DNN%20performance%2C%20thus%0Aenabling%20us%20to%20effectively%20estimate%20the%20quality%20distribution%20of%20large%20image%0Adatasets%20relative%20to%20targeted%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02797v2&entry.124074799=Read"},
{"title": "MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized\n  Domain Question-Answering", "author": "Kunal Sawarkar and Shivam R. Solanki and Abhilasha Mangal", "abstract": "  Retrieval-Augmented Generation (RAG) struggles with domain-specific\nenterprise datasets, often isolated behind firewalls and rich in complex,\nspecialized terminology unseen by LLMs during pre-training. Semantic\nvariability across domains like medicine, networking, or law hampers RAG's\ncontext precision, while fine-tuning solutions are costly, slow, and lack\ngeneralization as new data emerges. Achieving zero-shot precision with\nretrievers without fine-tuning still remains a key challenge. We introduce\n'MetaGen Blended RAG', a novel enterprise search approach that enhances\nsemantic retrievers through a metadata generation pipeline and hybrid query\nindexes using dense and sparse vectors. By leveraging key concepts, topics, and\nacronyms, our method creates metadata-enriched semantic indexes and boosted\nhybrid queries, delivering robust, scalable performance without fine-tuning. On\nthe biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks\nand even rivaling fine-tuned models on that dataset, while also excelling on\ndatasets like SQuAD and NQ. This approach redefines enterprise search using a\nnew approach to building semantic retrievers with unmatched generalization\nacross specialized domains.\n", "link": "http://arxiv.org/abs/2505.18247v3", "date": "2025-08-05", "relevancy": 1.9116, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4865}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaGen%20Blended%20RAG%3A%20Unlocking%20Zero-Shot%20Precision%20for%20Specialized%0A%20%20Domain%20Question-Answering&body=Title%3A%20MetaGen%20Blended%20RAG%3A%20Unlocking%20Zero-Shot%20Precision%20for%20Specialized%0A%20%20Domain%20Question-Answering%0AAuthor%3A%20Kunal%20Sawarkar%20and%20Shivam%20R.%20Solanki%20and%20Abhilasha%20Mangal%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20struggles%20with%20domain-specific%0Aenterprise%20datasets%2C%20often%20isolated%20behind%20firewalls%20and%20rich%20in%20complex%2C%0Aspecialized%20terminology%20unseen%20by%20LLMs%20during%20pre-training.%20Semantic%0Avariability%20across%20domains%20like%20medicine%2C%20networking%2C%20or%20law%20hampers%20RAG%27s%0Acontext%20precision%2C%20while%20fine-tuning%20solutions%20are%20costly%2C%20slow%2C%20and%20lack%0Ageneralization%20as%20new%20data%20emerges.%20Achieving%20zero-shot%20precision%20with%0Aretrievers%20without%20fine-tuning%20still%20remains%20a%20key%20challenge.%20We%20introduce%0A%27MetaGen%20Blended%20RAG%27%2C%20a%20novel%20enterprise%20search%20approach%20that%20enhances%0Asemantic%20retrievers%20through%20a%20metadata%20generation%20pipeline%20and%20hybrid%20query%0Aindexes%20using%20dense%20and%20sparse%20vectors.%20By%20leveraging%20key%20concepts%2C%20topics%2C%20and%0Aacronyms%2C%20our%20method%20creates%20metadata-enriched%20semantic%20indexes%20and%20boosted%0Ahybrid%20queries%2C%20delivering%20robust%2C%20scalable%20performance%20without%20fine-tuning.%20On%0Athe%20biomedical%20PubMedQA%20dataset%2C%20MetaGen%20Blended%20RAG%20achieves%2082%25%20retrieval%0Aaccuracy%20and%2077%25%20RAG%20accuracy%2C%20surpassing%20all%20prior%20zero-shot%20RAG%20benchmarks%0Aand%20even%20rivaling%20fine-tuned%20models%20on%20that%20dataset%2C%20while%20also%20excelling%20on%0Adatasets%20like%20SQuAD%20and%20NQ.%20This%20approach%20redefines%20enterprise%20search%20using%20a%0Anew%20approach%20to%20building%20semantic%20retrievers%20with%20unmatched%20generalization%0Aacross%20specialized%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18247v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaGen%2520Blended%2520RAG%253A%2520Unlocking%2520Zero-Shot%2520Precision%2520for%2520Specialized%250A%2520%2520Domain%2520Question-Answering%26entry.906535625%3DKunal%2520Sawarkar%2520and%2520Shivam%2520R.%2520Solanki%2520and%2520Abhilasha%2520Mangal%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520struggles%2520with%2520domain-specific%250Aenterprise%2520datasets%252C%2520often%2520isolated%2520behind%2520firewalls%2520and%2520rich%2520in%2520complex%252C%250Aspecialized%2520terminology%2520unseen%2520by%2520LLMs%2520during%2520pre-training.%2520Semantic%250Avariability%2520across%2520domains%2520like%2520medicine%252C%2520networking%252C%2520or%2520law%2520hampers%2520RAG%2527s%250Acontext%2520precision%252C%2520while%2520fine-tuning%2520solutions%2520are%2520costly%252C%2520slow%252C%2520and%2520lack%250Ageneralization%2520as%2520new%2520data%2520emerges.%2520Achieving%2520zero-shot%2520precision%2520with%250Aretrievers%2520without%2520fine-tuning%2520still%2520remains%2520a%2520key%2520challenge.%2520We%2520introduce%250A%2527MetaGen%2520Blended%2520RAG%2527%252C%2520a%2520novel%2520enterprise%2520search%2520approach%2520that%2520enhances%250Asemantic%2520retrievers%2520through%2520a%2520metadata%2520generation%2520pipeline%2520and%2520hybrid%2520query%250Aindexes%2520using%2520dense%2520and%2520sparse%2520vectors.%2520By%2520leveraging%2520key%2520concepts%252C%2520topics%252C%2520and%250Aacronyms%252C%2520our%2520method%2520creates%2520metadata-enriched%2520semantic%2520indexes%2520and%2520boosted%250Ahybrid%2520queries%252C%2520delivering%2520robust%252C%2520scalable%2520performance%2520without%2520fine-tuning.%2520On%250Athe%2520biomedical%2520PubMedQA%2520dataset%252C%2520MetaGen%2520Blended%2520RAG%2520achieves%252082%2525%2520retrieval%250Aaccuracy%2520and%252077%2525%2520RAG%2520accuracy%252C%2520surpassing%2520all%2520prior%2520zero-shot%2520RAG%2520benchmarks%250Aand%2520even%2520rivaling%2520fine-tuned%2520models%2520on%2520that%2520dataset%252C%2520while%2520also%2520excelling%2520on%250Adatasets%2520like%2520SQuAD%2520and%2520NQ.%2520This%2520approach%2520redefines%2520enterprise%2520search%2520using%2520a%250Anew%2520approach%2520to%2520building%2520semantic%2520retrievers%2520with%2520unmatched%2520generalization%250Aacross%2520specialized%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18247v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaGen%20Blended%20RAG%3A%20Unlocking%20Zero-Shot%20Precision%20for%20Specialized%0A%20%20Domain%20Question-Answering&entry.906535625=Kunal%20Sawarkar%20and%20Shivam%20R.%20Solanki%20and%20Abhilasha%20Mangal&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20struggles%20with%20domain-specific%0Aenterprise%20datasets%2C%20often%20isolated%20behind%20firewalls%20and%20rich%20in%20complex%2C%0Aspecialized%20terminology%20unseen%20by%20LLMs%20during%20pre-training.%20Semantic%0Avariability%20across%20domains%20like%20medicine%2C%20networking%2C%20or%20law%20hampers%20RAG%27s%0Acontext%20precision%2C%20while%20fine-tuning%20solutions%20are%20costly%2C%20slow%2C%20and%20lack%0Ageneralization%20as%20new%20data%20emerges.%20Achieving%20zero-shot%20precision%20with%0Aretrievers%20without%20fine-tuning%20still%20remains%20a%20key%20challenge.%20We%20introduce%0A%27MetaGen%20Blended%20RAG%27%2C%20a%20novel%20enterprise%20search%20approach%20that%20enhances%0Asemantic%20retrievers%20through%20a%20metadata%20generation%20pipeline%20and%20hybrid%20query%0Aindexes%20using%20dense%20and%20sparse%20vectors.%20By%20leveraging%20key%20concepts%2C%20topics%2C%20and%0Aacronyms%2C%20our%20method%20creates%20metadata-enriched%20semantic%20indexes%20and%20boosted%0Ahybrid%20queries%2C%20delivering%20robust%2C%20scalable%20performance%20without%20fine-tuning.%20On%0Athe%20biomedical%20PubMedQA%20dataset%2C%20MetaGen%20Blended%20RAG%20achieves%2082%25%20retrieval%0Aaccuracy%20and%2077%25%20RAG%20accuracy%2C%20surpassing%20all%20prior%20zero-shot%20RAG%20benchmarks%0Aand%20even%20rivaling%20fine-tuned%20models%20on%20that%20dataset%2C%20while%20also%20excelling%20on%0Adatasets%20like%20SQuAD%20and%20NQ.%20This%20approach%20redefines%20enterprise%20search%20using%20a%0Anew%20approach%20to%20building%20semantic%20retrievers%20with%20unmatched%20generalization%0Aacross%20specialized%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18247v3&entry.124074799=Read"},
{"title": "Refining Critical Thinking in LLM Code Generation: A Faulty\n  Premise-based Evaluation Framework", "author": "Jialin Li and Jinzhe Li and Gengxu Li and Yi Chang and Yuan Wu", "abstract": "  With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.\n", "link": "http://arxiv.org/abs/2508.03622v1", "date": "2025-08-05", "relevancy": 1.856, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refining%20Critical%20Thinking%20in%20LLM%20Code%20Generation%3A%20A%20Faulty%0A%20%20Premise-based%20Evaluation%20Framework&body=Title%3A%20Refining%20Critical%20Thinking%20in%20LLM%20Code%20Generation%3A%20A%20Faulty%0A%20%20Premise-based%20Evaluation%20Framework%0AAuthor%3A%20Jialin%20Li%20and%20Jinzhe%20Li%20and%20Gengxu%20Li%20and%20Yi%20Chang%20and%20Yuan%20Wu%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20code%20generation%20capabilities%20in%20large%20language%20models%0A%28LLMs%29%2C%20their%20reliance%20on%20input%20premises%20has%20intensified.%20When%20users%20provide%0Ainputs%20containing%20faulty%20premises%2C%20the%20probability%20of%20code%20generation%0Ahallucinations%20rises%20significantly%2C%20exposing%20deficiencies%20in%20their%0Aself-scrutiny%20capabilities.%20This%20paper%20proposes%20Faulty%20Premises%20Bench%0A%28FPBench%29%2C%20the%20first%20code%20generation%20evaluation%20framework%20targeting%20faulty%0Apremises.%20By%20systematically%20constructing%20three%20categories%20of%20faulty%20premises%0Aand%20integrating%20multi-dimensional%20evaluation%20metrics%2C%20it%20conducts%20in-depth%0Aassessments%20of%2015%20representative%20LLMs.%20The%20key%20findings%20are%20as%20follows%3A%20%281%29%0AMost%20models%20exhibit%20poor%20reasoning%20abilities%20and%20suboptimal%20code%20generation%0Aperformance%20under%20faulty%20premises%2C%20heavily%20relying%20on%20explicit%20prompts%20for%0Aerror%20detection%2C%20with%20limited%20self-scrutiny%20capabilities%3B%20%282%29%20Faulty%20premises%0Atrigger%20a%20point%20of%20diminishing%20returns%20in%20resource%20investment%2C%20leading%20to%0Ablindly%20increasing%20length%20fails%20to%20enhance%20quality%3B%20%283%29%20The%20three%20types%20of%0Afaulty%20premises%20respectively%20activate%20distinct%20defect%20patterns%20in%20models%2C%0Arevealing%20a%20triple%20dissociation%20in%20the%20cognitive%20mechanisms%20of%20code%20generation%0Amodels.%20This%20study%20not%20only%20highlights%20the%20urgent%20need%20for%20LLMs%20to%20proactively%0Averify%20premises%20in%20code%20generation%20but%20also%2C%20through%20the%20proposed%20FPBench%0Aframework%20and%20multi-dimensional%20evaluation%20system%2C%20provides%20a%20theoretical%0Afoundation%20and%20practical%20pathway%20for%20developing%20reliable%2C%20human-centric%20code%0Ageneration%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefining%2520Critical%2520Thinking%2520in%2520LLM%2520Code%2520Generation%253A%2520A%2520Faulty%250A%2520%2520Premise-based%2520Evaluation%2520Framework%26entry.906535625%3DJialin%2520Li%2520and%2520Jinzhe%2520Li%2520and%2520Gengxu%2520Li%2520and%2520Yi%2520Chang%2520and%2520Yuan%2520Wu%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520code%2520generation%2520capabilities%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520their%2520reliance%2520on%2520input%2520premises%2520has%2520intensified.%2520When%2520users%2520provide%250Ainputs%2520containing%2520faulty%2520premises%252C%2520the%2520probability%2520of%2520code%2520generation%250Ahallucinations%2520rises%2520significantly%252C%2520exposing%2520deficiencies%2520in%2520their%250Aself-scrutiny%2520capabilities.%2520This%2520paper%2520proposes%2520Faulty%2520Premises%2520Bench%250A%2528FPBench%2529%252C%2520the%2520first%2520code%2520generation%2520evaluation%2520framework%2520targeting%2520faulty%250Apremises.%2520By%2520systematically%2520constructing%2520three%2520categories%2520of%2520faulty%2520premises%250Aand%2520integrating%2520multi-dimensional%2520evaluation%2520metrics%252C%2520it%2520conducts%2520in-depth%250Aassessments%2520of%252015%2520representative%2520LLMs.%2520The%2520key%2520findings%2520are%2520as%2520follows%253A%2520%25281%2529%250AMost%2520models%2520exhibit%2520poor%2520reasoning%2520abilities%2520and%2520suboptimal%2520code%2520generation%250Aperformance%2520under%2520faulty%2520premises%252C%2520heavily%2520relying%2520on%2520explicit%2520prompts%2520for%250Aerror%2520detection%252C%2520with%2520limited%2520self-scrutiny%2520capabilities%253B%2520%25282%2529%2520Faulty%2520premises%250Atrigger%2520a%2520point%2520of%2520diminishing%2520returns%2520in%2520resource%2520investment%252C%2520leading%2520to%250Ablindly%2520increasing%2520length%2520fails%2520to%2520enhance%2520quality%253B%2520%25283%2529%2520The%2520three%2520types%2520of%250Afaulty%2520premises%2520respectively%2520activate%2520distinct%2520defect%2520patterns%2520in%2520models%252C%250Arevealing%2520a%2520triple%2520dissociation%2520in%2520the%2520cognitive%2520mechanisms%2520of%2520code%2520generation%250Amodels.%2520This%2520study%2520not%2520only%2520highlights%2520the%2520urgent%2520need%2520for%2520LLMs%2520to%2520proactively%250Averify%2520premises%2520in%2520code%2520generation%2520but%2520also%252C%2520through%2520the%2520proposed%2520FPBench%250Aframework%2520and%2520multi-dimensional%2520evaluation%2520system%252C%2520provides%2520a%2520theoretical%250Afoundation%2520and%2520practical%2520pathway%2520for%2520developing%2520reliable%252C%2520human-centric%2520code%250Ageneration%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20Critical%20Thinking%20in%20LLM%20Code%20Generation%3A%20A%20Faulty%0A%20%20Premise-based%20Evaluation%20Framework&entry.906535625=Jialin%20Li%20and%20Jinzhe%20Li%20and%20Gengxu%20Li%20and%20Yi%20Chang%20and%20Yuan%20Wu&entry.1292438233=%20%20With%20the%20advancement%20of%20code%20generation%20capabilities%20in%20large%20language%20models%0A%28LLMs%29%2C%20their%20reliance%20on%20input%20premises%20has%20intensified.%20When%20users%20provide%0Ainputs%20containing%20faulty%20premises%2C%20the%20probability%20of%20code%20generation%0Ahallucinations%20rises%20significantly%2C%20exposing%20deficiencies%20in%20their%0Aself-scrutiny%20capabilities.%20This%20paper%20proposes%20Faulty%20Premises%20Bench%0A%28FPBench%29%2C%20the%20first%20code%20generation%20evaluation%20framework%20targeting%20faulty%0Apremises.%20By%20systematically%20constructing%20three%20categories%20of%20faulty%20premises%0Aand%20integrating%20multi-dimensional%20evaluation%20metrics%2C%20it%20conducts%20in-depth%0Aassessments%20of%2015%20representative%20LLMs.%20The%20key%20findings%20are%20as%20follows%3A%20%281%29%0AMost%20models%20exhibit%20poor%20reasoning%20abilities%20and%20suboptimal%20code%20generation%0Aperformance%20under%20faulty%20premises%2C%20heavily%20relying%20on%20explicit%20prompts%20for%0Aerror%20detection%2C%20with%20limited%20self-scrutiny%20capabilities%3B%20%282%29%20Faulty%20premises%0Atrigger%20a%20point%20of%20diminishing%20returns%20in%20resource%20investment%2C%20leading%20to%0Ablindly%20increasing%20length%20fails%20to%20enhance%20quality%3B%20%283%29%20The%20three%20types%20of%0Afaulty%20premises%20respectively%20activate%20distinct%20defect%20patterns%20in%20models%2C%0Arevealing%20a%20triple%20dissociation%20in%20the%20cognitive%20mechanisms%20of%20code%20generation%0Amodels.%20This%20study%20not%20only%20highlights%20the%20urgent%20need%20for%20LLMs%20to%20proactively%0Averify%20premises%20in%20code%20generation%20but%20also%2C%20through%20the%20proposed%20FPBench%0Aframework%20and%20multi-dimensional%20evaluation%20system%2C%20provides%20a%20theoretical%0Afoundation%20and%20practical%20pathway%20for%20developing%20reliable%2C%20human-centric%20code%0Ageneration%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03622v1&entry.124074799=Read"},
{"title": "Prior2Former -- Evidential Modeling of Mask Transformers for\n  Assumption-Free Open-World Panoptic Segmentation", "author": "Sebastian Schmidt and Julius K\u00f6rner and Dominik Fuchsgruber and Stefano Gasperini and Federico Tombari and Stephan G\u00fcnnemann", "abstract": "  In panoptic segmentation, individual instances must be separated within\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\nclasses, they struggle with novel categories and out-of-distribution (OOD)\ndata. This is particularly problematic in safety-critical applications, such as\nautonomous driving, where reliability in unseen scenarios is essential. We\naddress the gap between outstanding benchmark performance and reliability by\nproposing Prior2Former (P2F), the first approach for segmentation vision\ntransformers rooted in evidential learning. P2F extends the mask vision\ntransformer architecture by incorporating a Beta prior for computing model\nuncertainty in pixel-wise binary mask assignments. This design enables\nhigh-quality uncertainty estimation that effectively detects novel and OOD\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\npanoptic segmentation. Unlike most segmentation models addressing unknown\nclasses, P2F operates without access to OOD data samples or contrastive\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\nreal-world scenarios where such prior information is unavailable. Additionally,\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\nand OoDIS datasets, P2F demonstrates state-of-the-art performance across the\nboard.\n", "link": "http://arxiv.org/abs/2504.04841v2", "date": "2025-08-05", "relevancy": 1.812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior2Former%20--%20Evidential%20Modeling%20of%20Mask%20Transformers%20for%0A%20%20Assumption-Free%20Open-World%20Panoptic%20Segmentation&body=Title%3A%20Prior2Former%20--%20Evidential%20Modeling%20of%20Mask%20Transformers%20for%0A%20%20Assumption-Free%20Open-World%20Panoptic%20Segmentation%0AAuthor%3A%20Sebastian%20Schmidt%20and%20Julius%20K%C3%B6rner%20and%20Dominik%20Fuchsgruber%20and%20Stefano%20Gasperini%20and%20Federico%20Tombari%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20In%20panoptic%20segmentation%2C%20individual%20instances%20must%20be%20separated%20within%0Asemantic%20classes.%20As%20state-of-the-art%20methods%20rely%20on%20a%20pre-defined%20set%20of%0Aclasses%2C%20they%20struggle%20with%20novel%20categories%20and%20out-of-distribution%20%28OOD%29%0Adata.%20This%20is%20particularly%20problematic%20in%20safety-critical%20applications%2C%20such%20as%0Aautonomous%20driving%2C%20where%20reliability%20in%20unseen%20scenarios%20is%20essential.%20We%0Aaddress%20the%20gap%20between%20outstanding%20benchmark%20performance%20and%20reliability%20by%0Aproposing%20Prior2Former%20%28P2F%29%2C%20the%20first%20approach%20for%20segmentation%20vision%0Atransformers%20rooted%20in%20evidential%20learning.%20P2F%20extends%20the%20mask%20vision%0Atransformer%20architecture%20by%20incorporating%20a%20Beta%20prior%20for%20computing%20model%0Auncertainty%20in%20pixel-wise%20binary%20mask%20assignments.%20This%20design%20enables%0Ahigh-quality%20uncertainty%20estimation%20that%20effectively%20detects%20novel%20and%20OOD%0Aobjects%20enabling%20state-of-the-art%20anomaly%20instance%20segmentation%20and%20open-world%0Apanoptic%20segmentation.%20Unlike%20most%20segmentation%20models%20addressing%20unknown%0Aclasses%2C%20P2F%20operates%20without%20access%20to%20OOD%20data%20samples%20or%20contrastive%0Atraining%20on%20void%20%28i.e.%2C%20unlabeled%29%20classes%2C%20making%20it%20highly%20applicable%20in%0Areal-world%20scenarios%20where%20such%20prior%20information%20is%20unavailable.%20Additionally%2C%0AP2F%20can%20be%20flexibly%20applied%20to%20anomaly%20instance%20and%20panoptic%20segmentation.%0AThrough%20comprehensive%20experiments%20on%20the%20Cityscapes%2C%20COCO%2C%20SegmentMeIfYouCan%2C%0Aand%20OoDIS%20datasets%2C%20P2F%20demonstrates%20state-of-the-art%20performance%20across%20the%0Aboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior2Former%2520--%2520Evidential%2520Modeling%2520of%2520Mask%2520Transformers%2520for%250A%2520%2520Assumption-Free%2520Open-World%2520Panoptic%2520Segmentation%26entry.906535625%3DSebastian%2520Schmidt%2520and%2520Julius%2520K%25C3%25B6rner%2520and%2520Dominik%2520Fuchsgruber%2520and%2520Stefano%2520Gasperini%2520and%2520Federico%2520Tombari%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520In%2520panoptic%2520segmentation%252C%2520individual%2520instances%2520must%2520be%2520separated%2520within%250Asemantic%2520classes.%2520As%2520state-of-the-art%2520methods%2520rely%2520on%2520a%2520pre-defined%2520set%2520of%250Aclasses%252C%2520they%2520struggle%2520with%2520novel%2520categories%2520and%2520out-of-distribution%2520%2528OOD%2529%250Adata.%2520This%2520is%2520particularly%2520problematic%2520in%2520safety-critical%2520applications%252C%2520such%2520as%250Aautonomous%2520driving%252C%2520where%2520reliability%2520in%2520unseen%2520scenarios%2520is%2520essential.%2520We%250Aaddress%2520the%2520gap%2520between%2520outstanding%2520benchmark%2520performance%2520and%2520reliability%2520by%250Aproposing%2520Prior2Former%2520%2528P2F%2529%252C%2520the%2520first%2520approach%2520for%2520segmentation%2520vision%250Atransformers%2520rooted%2520in%2520evidential%2520learning.%2520P2F%2520extends%2520the%2520mask%2520vision%250Atransformer%2520architecture%2520by%2520incorporating%2520a%2520Beta%2520prior%2520for%2520computing%2520model%250Auncertainty%2520in%2520pixel-wise%2520binary%2520mask%2520assignments.%2520This%2520design%2520enables%250Ahigh-quality%2520uncertainty%2520estimation%2520that%2520effectively%2520detects%2520novel%2520and%2520OOD%250Aobjects%2520enabling%2520state-of-the-art%2520anomaly%2520instance%2520segmentation%2520and%2520open-world%250Apanoptic%2520segmentation.%2520Unlike%2520most%2520segmentation%2520models%2520addressing%2520unknown%250Aclasses%252C%2520P2F%2520operates%2520without%2520access%2520to%2520OOD%2520data%2520samples%2520or%2520contrastive%250Atraining%2520on%2520void%2520%2528i.e.%252C%2520unlabeled%2529%2520classes%252C%2520making%2520it%2520highly%2520applicable%2520in%250Areal-world%2520scenarios%2520where%2520such%2520prior%2520information%2520is%2520unavailable.%2520Additionally%252C%250AP2F%2520can%2520be%2520flexibly%2520applied%2520to%2520anomaly%2520instance%2520and%2520panoptic%2520segmentation.%250AThrough%2520comprehensive%2520experiments%2520on%2520the%2520Cityscapes%252C%2520COCO%252C%2520SegmentMeIfYouCan%252C%250Aand%2520OoDIS%2520datasets%252C%2520P2F%2520demonstrates%2520state-of-the-art%2520performance%2520across%2520the%250Aboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior2Former%20--%20Evidential%20Modeling%20of%20Mask%20Transformers%20for%0A%20%20Assumption-Free%20Open-World%20Panoptic%20Segmentation&entry.906535625=Sebastian%20Schmidt%20and%20Julius%20K%C3%B6rner%20and%20Dominik%20Fuchsgruber%20and%20Stefano%20Gasperini%20and%20Federico%20Tombari%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20In%20panoptic%20segmentation%2C%20individual%20instances%20must%20be%20separated%20within%0Asemantic%20classes.%20As%20state-of-the-art%20methods%20rely%20on%20a%20pre-defined%20set%20of%0Aclasses%2C%20they%20struggle%20with%20novel%20categories%20and%20out-of-distribution%20%28OOD%29%0Adata.%20This%20is%20particularly%20problematic%20in%20safety-critical%20applications%2C%20such%20as%0Aautonomous%20driving%2C%20where%20reliability%20in%20unseen%20scenarios%20is%20essential.%20We%0Aaddress%20the%20gap%20between%20outstanding%20benchmark%20performance%20and%20reliability%20by%0Aproposing%20Prior2Former%20%28P2F%29%2C%20the%20first%20approach%20for%20segmentation%20vision%0Atransformers%20rooted%20in%20evidential%20learning.%20P2F%20extends%20the%20mask%20vision%0Atransformer%20architecture%20by%20incorporating%20a%20Beta%20prior%20for%20computing%20model%0Auncertainty%20in%20pixel-wise%20binary%20mask%20assignments.%20This%20design%20enables%0Ahigh-quality%20uncertainty%20estimation%20that%20effectively%20detects%20novel%20and%20OOD%0Aobjects%20enabling%20state-of-the-art%20anomaly%20instance%20segmentation%20and%20open-world%0Apanoptic%20segmentation.%20Unlike%20most%20segmentation%20models%20addressing%20unknown%0Aclasses%2C%20P2F%20operates%20without%20access%20to%20OOD%20data%20samples%20or%20contrastive%0Atraining%20on%20void%20%28i.e.%2C%20unlabeled%29%20classes%2C%20making%20it%20highly%20applicable%20in%0Areal-world%20scenarios%20where%20such%20prior%20information%20is%20unavailable.%20Additionally%2C%0AP2F%20can%20be%20flexibly%20applied%20to%20anomaly%20instance%20and%20panoptic%20segmentation.%0AThrough%20comprehensive%20experiments%20on%20the%20Cityscapes%2C%20COCO%2C%20SegmentMeIfYouCan%2C%0Aand%20OoDIS%20datasets%2C%20P2F%20demonstrates%20state-of-the-art%20performance%20across%20the%0Aboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04841v2&entry.124074799=Read"},
{"title": "LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for\n  Advertiser Keyphrase Recommendations at eBay", "author": "Soumik Dey and Benjamin Braun and Naveen Ravipati and Hansi Wu and Binbin Li", "abstract": "  Sellers at eBay are recommended keyphrases to bid on to enhance the\nperformance of their advertising campaigns. The relevance of these keyphrases\nis crucial in avoiding the overcrowding of search systems with irrelevant items\nand maintaining a positive seller perception. It is essential that keyphrase\nrecommendations align with both seller and Search judgments regarding auctions.\nDue to the difficulty in procuring negative human judgment at scale, employing\nLLM-as-a-judge to mimic seller judgment has been established as the norm in\nseveral studies. This study introduces a novel two-step LLM distillation\nprocess from a LLM-judge used to debias our Embedding Based Retrieval (EBR)\nmodel from the various biases that exist in click-data. We distill from an LLM\nteacher via a cross-encoder assistant into a bi-encoder student using a\nmulti-task training approach, ultimately employing the student bi-encoder to\nretrieve relevant advertiser keyphrases. We show that integrating a knowledge\ndistillation process from LLMs in a multi-task training setup enhances\nbi-encoder performance in retrieving relevant advertiser keyphrases at eBay.\n", "link": "http://arxiv.org/abs/2508.03628v1", "date": "2025-08-05", "relevancy": 1.8022, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4773}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMDistill4Ads%3A%20Using%20Cross-Encoders%20to%20Distill%20from%20LLM%20Signals%20for%0A%20%20Advertiser%20Keyphrase%20Recommendations%20at%20eBay&body=Title%3A%20LLMDistill4Ads%3A%20Using%20Cross-Encoders%20to%20Distill%20from%20LLM%20Signals%20for%0A%20%20Advertiser%20Keyphrase%20Recommendations%20at%20eBay%0AAuthor%3A%20Soumik%20Dey%20and%20Benjamin%20Braun%20and%20Naveen%20Ravipati%20and%20Hansi%20Wu%20and%20Binbin%20Li%0AAbstract%3A%20%20%20Sellers%20at%20eBay%20are%20recommended%20keyphrases%20to%20bid%20on%20to%20enhance%20the%0Aperformance%20of%20their%20advertising%20campaigns.%20The%20relevance%20of%20these%20keyphrases%0Ais%20crucial%20in%20avoiding%20the%20overcrowding%20of%20search%20systems%20with%20irrelevant%20items%0Aand%20maintaining%20a%20positive%20seller%20perception.%20It%20is%20essential%20that%20keyphrase%0Arecommendations%20align%20with%20both%20seller%20and%20Search%20judgments%20regarding%20auctions.%0ADue%20to%20the%20difficulty%20in%20procuring%20negative%20human%20judgment%20at%20scale%2C%20employing%0ALLM-as-a-judge%20to%20mimic%20seller%20judgment%20has%20been%20established%20as%20the%20norm%20in%0Aseveral%20studies.%20This%20study%20introduces%20a%20novel%20two-step%20LLM%20distillation%0Aprocess%20from%20a%20LLM-judge%20used%20to%20debias%20our%20Embedding%20Based%20Retrieval%20%28EBR%29%0Amodel%20from%20the%20various%20biases%20that%20exist%20in%20click-data.%20We%20distill%20from%20an%20LLM%0Ateacher%20via%20a%20cross-encoder%20assistant%20into%20a%20bi-encoder%20student%20using%20a%0Amulti-task%20training%20approach%2C%20ultimately%20employing%20the%20student%20bi-encoder%20to%0Aretrieve%20relevant%20advertiser%20keyphrases.%20We%20show%20that%20integrating%20a%20knowledge%0Adistillation%20process%20from%20LLMs%20in%20a%20multi-task%20training%20setup%20enhances%0Abi-encoder%20performance%20in%20retrieving%20relevant%20advertiser%20keyphrases%20at%20eBay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMDistill4Ads%253A%2520Using%2520Cross-Encoders%2520to%2520Distill%2520from%2520LLM%2520Signals%2520for%250A%2520%2520Advertiser%2520Keyphrase%2520Recommendations%2520at%2520eBay%26entry.906535625%3DSoumik%2520Dey%2520and%2520Benjamin%2520Braun%2520and%2520Naveen%2520Ravipati%2520and%2520Hansi%2520Wu%2520and%2520Binbin%2520Li%26entry.1292438233%3D%2520%2520Sellers%2520at%2520eBay%2520are%2520recommended%2520keyphrases%2520to%2520bid%2520on%2520to%2520enhance%2520the%250Aperformance%2520of%2520their%2520advertising%2520campaigns.%2520The%2520relevance%2520of%2520these%2520keyphrases%250Ais%2520crucial%2520in%2520avoiding%2520the%2520overcrowding%2520of%2520search%2520systems%2520with%2520irrelevant%2520items%250Aand%2520maintaining%2520a%2520positive%2520seller%2520perception.%2520It%2520is%2520essential%2520that%2520keyphrase%250Arecommendations%2520align%2520with%2520both%2520seller%2520and%2520Search%2520judgments%2520regarding%2520auctions.%250ADue%2520to%2520the%2520difficulty%2520in%2520procuring%2520negative%2520human%2520judgment%2520at%2520scale%252C%2520employing%250ALLM-as-a-judge%2520to%2520mimic%2520seller%2520judgment%2520has%2520been%2520established%2520as%2520the%2520norm%2520in%250Aseveral%2520studies.%2520This%2520study%2520introduces%2520a%2520novel%2520two-step%2520LLM%2520distillation%250Aprocess%2520from%2520a%2520LLM-judge%2520used%2520to%2520debias%2520our%2520Embedding%2520Based%2520Retrieval%2520%2528EBR%2529%250Amodel%2520from%2520the%2520various%2520biases%2520that%2520exist%2520in%2520click-data.%2520We%2520distill%2520from%2520an%2520LLM%250Ateacher%2520via%2520a%2520cross-encoder%2520assistant%2520into%2520a%2520bi-encoder%2520student%2520using%2520a%250Amulti-task%2520training%2520approach%252C%2520ultimately%2520employing%2520the%2520student%2520bi-encoder%2520to%250Aretrieve%2520relevant%2520advertiser%2520keyphrases.%2520We%2520show%2520that%2520integrating%2520a%2520knowledge%250Adistillation%2520process%2520from%2520LLMs%2520in%2520a%2520multi-task%2520training%2520setup%2520enhances%250Abi-encoder%2520performance%2520in%2520retrieving%2520relevant%2520advertiser%2520keyphrases%2520at%2520eBay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMDistill4Ads%3A%20Using%20Cross-Encoders%20to%20Distill%20from%20LLM%20Signals%20for%0A%20%20Advertiser%20Keyphrase%20Recommendations%20at%20eBay&entry.906535625=Soumik%20Dey%20and%20Benjamin%20Braun%20and%20Naveen%20Ravipati%20and%20Hansi%20Wu%20and%20Binbin%20Li&entry.1292438233=%20%20Sellers%20at%20eBay%20are%20recommended%20keyphrases%20to%20bid%20on%20to%20enhance%20the%0Aperformance%20of%20their%20advertising%20campaigns.%20The%20relevance%20of%20these%20keyphrases%0Ais%20crucial%20in%20avoiding%20the%20overcrowding%20of%20search%20systems%20with%20irrelevant%20items%0Aand%20maintaining%20a%20positive%20seller%20perception.%20It%20is%20essential%20that%20keyphrase%0Arecommendations%20align%20with%20both%20seller%20and%20Search%20judgments%20regarding%20auctions.%0ADue%20to%20the%20difficulty%20in%20procuring%20negative%20human%20judgment%20at%20scale%2C%20employing%0ALLM-as-a-judge%20to%20mimic%20seller%20judgment%20has%20been%20established%20as%20the%20norm%20in%0Aseveral%20studies.%20This%20study%20introduces%20a%20novel%20two-step%20LLM%20distillation%0Aprocess%20from%20a%20LLM-judge%20used%20to%20debias%20our%20Embedding%20Based%20Retrieval%20%28EBR%29%0Amodel%20from%20the%20various%20biases%20that%20exist%20in%20click-data.%20We%20distill%20from%20an%20LLM%0Ateacher%20via%20a%20cross-encoder%20assistant%20into%20a%20bi-encoder%20student%20using%20a%0Amulti-task%20training%20approach%2C%20ultimately%20employing%20the%20student%20bi-encoder%20to%0Aretrieve%20relevant%20advertiser%20keyphrases.%20We%20show%20that%20integrating%20a%20knowledge%0Adistillation%20process%20from%20LLMs%20in%20a%20multi-task%20training%20setup%20enhances%0Abi-encoder%20performance%20in%20retrieving%20relevant%20advertiser%20keyphrases%20at%20eBay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03628v1&entry.124074799=Read"},
{"title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak\n  Single-Photon Lidar Imaging", "author": "Kyungmin Choi and JaKeoung Koo and Stephen McLaughlin and Abderrahim Halimi", "abstract": "  Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.\n", "link": "http://arxiv.org/abs/2504.02480v2", "date": "2025-08-05", "relevancy": 1.7297, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5766}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Attention-Driven%20Bayesian%20Deep%20Unrolling%20for%20Dual-Peak%0A%20%20Single-Photon%20Lidar%20Imaging&body=Title%3A%20Graph%20Attention-Driven%20Bayesian%20Deep%20Unrolling%20for%20Dual-Peak%0A%20%20Single-Photon%20Lidar%20Imaging%0AAuthor%3A%20Kyungmin%20Choi%20and%20JaKeoung%20Koo%20and%20Stephen%20McLaughlin%20and%20Abderrahim%20Halimi%0AAbstract%3A%20%20%20Single-photon%20Lidar%20imaging%20offers%20a%20significant%20advantage%20in%203D%20imaging%20due%0Ato%20its%20high%20resolution%20and%20long-range%20capabilities%2C%20however%20it%20is%20challenging%0Ato%20apply%20in%20noisy%20environments%20with%20multiple%20targets%20per%20pixel.%20To%20tackle%20these%0Achallenges%2C%20several%20methods%20have%20been%20proposed.%20Statistical%20methods%20demonstrate%0Ainterpretability%20on%20the%20inferred%20parameters%2C%20but%20they%20are%20often%20limited%20in%0Atheir%20ability%20to%20handle%20complex%20scenes.%20Deep%20learning-based%20methods%20have%20shown%0Asuperior%20performance%20in%20terms%20of%20accuracy%20and%20robustness%2C%20but%20they%20lack%0Ainterpretability%20or%20they%20are%20limited%20to%20a%20single-peak%20per%20pixel.%20In%20this%20paper%2C%0Awe%20propose%20a%20deep%20unrolling%20algorithm%20for%20dual-peak%20single-photon%20Lidar%0Aimaging.%20We%20introduce%20a%20hierarchical%20Bayesian%20model%20for%20multiple%20targets%20and%0Apropose%20a%20neural%20network%20that%20unrolls%20the%20underlying%20statistical%20method.%20To%0Asupport%20multiple%20targets%2C%20we%20adopt%20a%20dual%20depth%20maps%20representation%20and%20exploit%0Ageometric%20deep%20learning%20to%20extract%20features%20from%20the%20point%20cloud.%20The%20proposed%0Amethod%20takes%20advantages%20of%20statistical%20methods%20and%20learning-based%20methods%20in%0Aterms%20of%20accuracy%20and%20quantifying%20uncertainty.%20The%20experimental%20results%20on%0Asynthetic%20and%20real%20data%20demonstrate%20the%20competitive%20performance%20when%20compared%0Ato%20existing%20methods%2C%20while%20also%20providing%20uncertainty%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Attention-Driven%2520Bayesian%2520Deep%2520Unrolling%2520for%2520Dual-Peak%250A%2520%2520Single-Photon%2520Lidar%2520Imaging%26entry.906535625%3DKyungmin%2520Choi%2520and%2520JaKeoung%2520Koo%2520and%2520Stephen%2520McLaughlin%2520and%2520Abderrahim%2520Halimi%26entry.1292438233%3D%2520%2520Single-photon%2520Lidar%2520imaging%2520offers%2520a%2520significant%2520advantage%2520in%25203D%2520imaging%2520due%250Ato%2520its%2520high%2520resolution%2520and%2520long-range%2520capabilities%252C%2520however%2520it%2520is%2520challenging%250Ato%2520apply%2520in%2520noisy%2520environments%2520with%2520multiple%2520targets%2520per%2520pixel.%2520To%2520tackle%2520these%250Achallenges%252C%2520several%2520methods%2520have%2520been%2520proposed.%2520Statistical%2520methods%2520demonstrate%250Ainterpretability%2520on%2520the%2520inferred%2520parameters%252C%2520but%2520they%2520are%2520often%2520limited%2520in%250Atheir%2520ability%2520to%2520handle%2520complex%2520scenes.%2520Deep%2520learning-based%2520methods%2520have%2520shown%250Asuperior%2520performance%2520in%2520terms%2520of%2520accuracy%2520and%2520robustness%252C%2520but%2520they%2520lack%250Ainterpretability%2520or%2520they%2520are%2520limited%2520to%2520a%2520single-peak%2520per%2520pixel.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520deep%2520unrolling%2520algorithm%2520for%2520dual-peak%2520single-photon%2520Lidar%250Aimaging.%2520We%2520introduce%2520a%2520hierarchical%2520Bayesian%2520model%2520for%2520multiple%2520targets%2520and%250Apropose%2520a%2520neural%2520network%2520that%2520unrolls%2520the%2520underlying%2520statistical%2520method.%2520To%250Asupport%2520multiple%2520targets%252C%2520we%2520adopt%2520a%2520dual%2520depth%2520maps%2520representation%2520and%2520exploit%250Ageometric%2520deep%2520learning%2520to%2520extract%2520features%2520from%2520the%2520point%2520cloud.%2520The%2520proposed%250Amethod%2520takes%2520advantages%2520of%2520statistical%2520methods%2520and%2520learning-based%2520methods%2520in%250Aterms%2520of%2520accuracy%2520and%2520quantifying%2520uncertainty.%2520The%2520experimental%2520results%2520on%250Asynthetic%2520and%2520real%2520data%2520demonstrate%2520the%2520competitive%2520performance%2520when%2520compared%250Ato%2520existing%2520methods%252C%2520while%2520also%2520providing%2520uncertainty%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Attention-Driven%20Bayesian%20Deep%20Unrolling%20for%20Dual-Peak%0A%20%20Single-Photon%20Lidar%20Imaging&entry.906535625=Kyungmin%20Choi%20and%20JaKeoung%20Koo%20and%20Stephen%20McLaughlin%20and%20Abderrahim%20Halimi&entry.1292438233=%20%20Single-photon%20Lidar%20imaging%20offers%20a%20significant%20advantage%20in%203D%20imaging%20due%0Ato%20its%20high%20resolution%20and%20long-range%20capabilities%2C%20however%20it%20is%20challenging%0Ato%20apply%20in%20noisy%20environments%20with%20multiple%20targets%20per%20pixel.%20To%20tackle%20these%0Achallenges%2C%20several%20methods%20have%20been%20proposed.%20Statistical%20methods%20demonstrate%0Ainterpretability%20on%20the%20inferred%20parameters%2C%20but%20they%20are%20often%20limited%20in%0Atheir%20ability%20to%20handle%20complex%20scenes.%20Deep%20learning-based%20methods%20have%20shown%0Asuperior%20performance%20in%20terms%20of%20accuracy%20and%20robustness%2C%20but%20they%20lack%0Ainterpretability%20or%20they%20are%20limited%20to%20a%20single-peak%20per%20pixel.%20In%20this%20paper%2C%0Awe%20propose%20a%20deep%20unrolling%20algorithm%20for%20dual-peak%20single-photon%20Lidar%0Aimaging.%20We%20introduce%20a%20hierarchical%20Bayesian%20model%20for%20multiple%20targets%20and%0Apropose%20a%20neural%20network%20that%20unrolls%20the%20underlying%20statistical%20method.%20To%0Asupport%20multiple%20targets%2C%20we%20adopt%20a%20dual%20depth%20maps%20representation%20and%20exploit%0Ageometric%20deep%20learning%20to%20extract%20features%20from%20the%20point%20cloud.%20The%20proposed%0Amethod%20takes%20advantages%20of%20statistical%20methods%20and%20learning-based%20methods%20in%0Aterms%20of%20accuracy%20and%20quantifying%20uncertainty.%20The%20experimental%20results%20on%0Asynthetic%20and%20real%20data%20demonstrate%20the%20competitive%20performance%20when%20compared%0Ato%20existing%20methods%2C%20while%20also%20providing%20uncertainty%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02480v2&entry.124074799=Read"},
{"title": "Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation", "author": "Deepak Pandita and Flip Korn and Chris Welty and Christopher M. Homan", "abstract": "  Reproducibility is a cornerstone of scientific validation and of the\nauthority it confers on its results. Reproducibility in machine learning\nevaluations leads to greater trust, confidence, and value. However, the ground\ntruth responses used in machine learning often necessarily come from humans,\namong whom disagreement is prevalent, and surprisingly little research has\nstudied the impact of effectively ignoring disagreement in these responses, as\nis typically the case. One reason for the lack of research is that budgets for\ncollecting human-annotated evaluation data are limited, and obtaining more\nsamples from multiple annotators for each example greatly increases the\nper-item annotation costs. We investigate the trade-off between the number of\nitems ($N$) and the number of responses per item ($K$) needed for reliable\nmachine learning evaluation. We analyze a diverse collection of categorical\ndatasets for which multiple annotations per item exist, and simulated\ndistributions fit to these datasets, to determine the optimal $(N, K)$\nconfiguration, given a fixed budget ($N \\times K$), for collecting evaluation\ndata and reliably comparing the performance of machine learning models. Our\nfindings show, first, that accounting for human disagreement may come with $N\n\\times K$ at no more than 1000 (and often much lower) for every dataset tested\non at least one metric. Moreover, this minimal $N \\times K$ almost always\noccurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and\n$N$ -- or if one even existed -- depends on the evaluation metric, with metrics\nthat are more sensitive to the full distribution of responses performing better\nat higher levels of $K$. Our methods can be used to help ML practitioners get\nmore effective test data by finding the optimal metrics and number of items and\nannotations per item to collect to get the most reliability for their budget.\n", "link": "http://arxiv.org/abs/2508.03663v1", "date": "2025-08-05", "relevancy": 1.7256, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4657}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4315}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forest%20vs%20Tree%3A%20The%20%24%28N%2C%20K%29%24%20Trade-off%20in%20Reproducible%20ML%20Evaluation&body=Title%3A%20Forest%20vs%20Tree%3A%20The%20%24%28N%2C%20K%29%24%20Trade-off%20in%20Reproducible%20ML%20Evaluation%0AAuthor%3A%20Deepak%20Pandita%20and%20Flip%20Korn%20and%20Chris%20Welty%20and%20Christopher%20M.%20Homan%0AAbstract%3A%20%20%20Reproducibility%20is%20a%20cornerstone%20of%20scientific%20validation%20and%20of%20the%0Aauthority%20it%20confers%20on%20its%20results.%20Reproducibility%20in%20machine%20learning%0Aevaluations%20leads%20to%20greater%20trust%2C%20confidence%2C%20and%20value.%20However%2C%20the%20ground%0Atruth%20responses%20used%20in%20machine%20learning%20often%20necessarily%20come%20from%20humans%2C%0Aamong%20whom%20disagreement%20is%20prevalent%2C%20and%20surprisingly%20little%20research%20has%0Astudied%20the%20impact%20of%20effectively%20ignoring%20disagreement%20in%20these%20responses%2C%20as%0Ais%20typically%20the%20case.%20One%20reason%20for%20the%20lack%20of%20research%20is%20that%20budgets%20for%0Acollecting%20human-annotated%20evaluation%20data%20are%20limited%2C%20and%20obtaining%20more%0Asamples%20from%20multiple%20annotators%20for%20each%20example%20greatly%20increases%20the%0Aper-item%20annotation%20costs.%20We%20investigate%20the%20trade-off%20between%20the%20number%20of%0Aitems%20%28%24N%24%29%20and%20the%20number%20of%20responses%20per%20item%20%28%24K%24%29%20needed%20for%20reliable%0Amachine%20learning%20evaluation.%20We%20analyze%20a%20diverse%20collection%20of%20categorical%0Adatasets%20for%20which%20multiple%20annotations%20per%20item%20exist%2C%20and%20simulated%0Adistributions%20fit%20to%20these%20datasets%2C%20to%20determine%20the%20optimal%20%24%28N%2C%20K%29%24%0Aconfiguration%2C%20given%20a%20fixed%20budget%20%28%24N%20%5Ctimes%20K%24%29%2C%20for%20collecting%20evaluation%0Adata%20and%20reliably%20comparing%20the%20performance%20of%20machine%20learning%20models.%20Our%0Afindings%20show%2C%20first%2C%20that%20accounting%20for%20human%20disagreement%20may%20come%20with%20%24N%0A%5Ctimes%20K%24%20at%20no%20more%20than%201000%20%28and%20often%20much%20lower%29%20for%20every%20dataset%20tested%0Aon%20at%20least%20one%20metric.%20Moreover%2C%20this%20minimal%20%24N%20%5Ctimes%20K%24%20almost%20always%0Aoccurred%20for%20%24K%20%3E%2010%24.%20Furthermore%2C%20the%20nature%20of%20the%20tradeoff%20between%20%24K%24%20and%0A%24N%24%20--%20or%20if%20one%20even%20existed%20--%20depends%20on%20the%20evaluation%20metric%2C%20with%20metrics%0Athat%20are%20more%20sensitive%20to%20the%20full%20distribution%20of%20responses%20performing%20better%0Aat%20higher%20levels%20of%20%24K%24.%20Our%20methods%20can%20be%20used%20to%20help%20ML%20practitioners%20get%0Amore%20effective%20test%20data%20by%20finding%20the%20optimal%20metrics%20and%20number%20of%20items%20and%0Aannotations%20per%20item%20to%20collect%20to%20get%20the%20most%20reliability%20for%20their%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForest%2520vs%2520Tree%253A%2520The%2520%2524%2528N%252C%2520K%2529%2524%2520Trade-off%2520in%2520Reproducible%2520ML%2520Evaluation%26entry.906535625%3DDeepak%2520Pandita%2520and%2520Flip%2520Korn%2520and%2520Chris%2520Welty%2520and%2520Christopher%2520M.%2520Homan%26entry.1292438233%3D%2520%2520Reproducibility%2520is%2520a%2520cornerstone%2520of%2520scientific%2520validation%2520and%2520of%2520the%250Aauthority%2520it%2520confers%2520on%2520its%2520results.%2520Reproducibility%2520in%2520machine%2520learning%250Aevaluations%2520leads%2520to%2520greater%2520trust%252C%2520confidence%252C%2520and%2520value.%2520However%252C%2520the%2520ground%250Atruth%2520responses%2520used%2520in%2520machine%2520learning%2520often%2520necessarily%2520come%2520from%2520humans%252C%250Aamong%2520whom%2520disagreement%2520is%2520prevalent%252C%2520and%2520surprisingly%2520little%2520research%2520has%250Astudied%2520the%2520impact%2520of%2520effectively%2520ignoring%2520disagreement%2520in%2520these%2520responses%252C%2520as%250Ais%2520typically%2520the%2520case.%2520One%2520reason%2520for%2520the%2520lack%2520of%2520research%2520is%2520that%2520budgets%2520for%250Acollecting%2520human-annotated%2520evaluation%2520data%2520are%2520limited%252C%2520and%2520obtaining%2520more%250Asamples%2520from%2520multiple%2520annotators%2520for%2520each%2520example%2520greatly%2520increases%2520the%250Aper-item%2520annotation%2520costs.%2520We%2520investigate%2520the%2520trade-off%2520between%2520the%2520number%2520of%250Aitems%2520%2528%2524N%2524%2529%2520and%2520the%2520number%2520of%2520responses%2520per%2520item%2520%2528%2524K%2524%2529%2520needed%2520for%2520reliable%250Amachine%2520learning%2520evaluation.%2520We%2520analyze%2520a%2520diverse%2520collection%2520of%2520categorical%250Adatasets%2520for%2520which%2520multiple%2520annotations%2520per%2520item%2520exist%252C%2520and%2520simulated%250Adistributions%2520fit%2520to%2520these%2520datasets%252C%2520to%2520determine%2520the%2520optimal%2520%2524%2528N%252C%2520K%2529%2524%250Aconfiguration%252C%2520given%2520a%2520fixed%2520budget%2520%2528%2524N%2520%255Ctimes%2520K%2524%2529%252C%2520for%2520collecting%2520evaluation%250Adata%2520and%2520reliably%2520comparing%2520the%2520performance%2520of%2520machine%2520learning%2520models.%2520Our%250Afindings%2520show%252C%2520first%252C%2520that%2520accounting%2520for%2520human%2520disagreement%2520may%2520come%2520with%2520%2524N%250A%255Ctimes%2520K%2524%2520at%2520no%2520more%2520than%25201000%2520%2528and%2520often%2520much%2520lower%2529%2520for%2520every%2520dataset%2520tested%250Aon%2520at%2520least%2520one%2520metric.%2520Moreover%252C%2520this%2520minimal%2520%2524N%2520%255Ctimes%2520K%2524%2520almost%2520always%250Aoccurred%2520for%2520%2524K%2520%253E%252010%2524.%2520Furthermore%252C%2520the%2520nature%2520of%2520the%2520tradeoff%2520between%2520%2524K%2524%2520and%250A%2524N%2524%2520--%2520or%2520if%2520one%2520even%2520existed%2520--%2520depends%2520on%2520the%2520evaluation%2520metric%252C%2520with%2520metrics%250Athat%2520are%2520more%2520sensitive%2520to%2520the%2520full%2520distribution%2520of%2520responses%2520performing%2520better%250Aat%2520higher%2520levels%2520of%2520%2524K%2524.%2520Our%2520methods%2520can%2520be%2520used%2520to%2520help%2520ML%2520practitioners%2520get%250Amore%2520effective%2520test%2520data%2520by%2520finding%2520the%2520optimal%2520metrics%2520and%2520number%2520of%2520items%2520and%250Aannotations%2520per%2520item%2520to%2520collect%2520to%2520get%2520the%2520most%2520reliability%2520for%2520their%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forest%20vs%20Tree%3A%20The%20%24%28N%2C%20K%29%24%20Trade-off%20in%20Reproducible%20ML%20Evaluation&entry.906535625=Deepak%20Pandita%20and%20Flip%20Korn%20and%20Chris%20Welty%20and%20Christopher%20M.%20Homan&entry.1292438233=%20%20Reproducibility%20is%20a%20cornerstone%20of%20scientific%20validation%20and%20of%20the%0Aauthority%20it%20confers%20on%20its%20results.%20Reproducibility%20in%20machine%20learning%0Aevaluations%20leads%20to%20greater%20trust%2C%20confidence%2C%20and%20value.%20However%2C%20the%20ground%0Atruth%20responses%20used%20in%20machine%20learning%20often%20necessarily%20come%20from%20humans%2C%0Aamong%20whom%20disagreement%20is%20prevalent%2C%20and%20surprisingly%20little%20research%20has%0Astudied%20the%20impact%20of%20effectively%20ignoring%20disagreement%20in%20these%20responses%2C%20as%0Ais%20typically%20the%20case.%20One%20reason%20for%20the%20lack%20of%20research%20is%20that%20budgets%20for%0Acollecting%20human-annotated%20evaluation%20data%20are%20limited%2C%20and%20obtaining%20more%0Asamples%20from%20multiple%20annotators%20for%20each%20example%20greatly%20increases%20the%0Aper-item%20annotation%20costs.%20We%20investigate%20the%20trade-off%20between%20the%20number%20of%0Aitems%20%28%24N%24%29%20and%20the%20number%20of%20responses%20per%20item%20%28%24K%24%29%20needed%20for%20reliable%0Amachine%20learning%20evaluation.%20We%20analyze%20a%20diverse%20collection%20of%20categorical%0Adatasets%20for%20which%20multiple%20annotations%20per%20item%20exist%2C%20and%20simulated%0Adistributions%20fit%20to%20these%20datasets%2C%20to%20determine%20the%20optimal%20%24%28N%2C%20K%29%24%0Aconfiguration%2C%20given%20a%20fixed%20budget%20%28%24N%20%5Ctimes%20K%24%29%2C%20for%20collecting%20evaluation%0Adata%20and%20reliably%20comparing%20the%20performance%20of%20machine%20learning%20models.%20Our%0Afindings%20show%2C%20first%2C%20that%20accounting%20for%20human%20disagreement%20may%20come%20with%20%24N%0A%5Ctimes%20K%24%20at%20no%20more%20than%201000%20%28and%20often%20much%20lower%29%20for%20every%20dataset%20tested%0Aon%20at%20least%20one%20metric.%20Moreover%2C%20this%20minimal%20%24N%20%5Ctimes%20K%24%20almost%20always%0Aoccurred%20for%20%24K%20%3E%2010%24.%20Furthermore%2C%20the%20nature%20of%20the%20tradeoff%20between%20%24K%24%20and%0A%24N%24%20--%20or%20if%20one%20even%20existed%20--%20depends%20on%20the%20evaluation%20metric%2C%20with%20metrics%0Athat%20are%20more%20sensitive%20to%20the%20full%20distribution%20of%20responses%20performing%20better%0Aat%20higher%20levels%20of%20%24K%24.%20Our%20methods%20can%20be%20used%20to%20help%20ML%20practitioners%20get%0Amore%20effective%20test%20data%20by%20finding%20the%20optimal%20metrics%20and%20number%20of%20items%20and%0Aannotations%20per%20item%20to%20collect%20to%20get%20the%20most%20reliability%20for%20their%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03663v1&entry.124074799=Read"},
{"title": "What If, But Privately: Private Counterfactual Retrieval", "author": "Shreya Meel and Mohamed Nomeir and Pasan Dissanayake and Sanghamitra Dutta and Sennur Ulukus", "abstract": "  Transparency and explainability are two important aspects to be considered\nwhen employing black-box machine learning models in high-stake applications.\nProviding counterfactual explanations is one way of catering this requirement.\nHowever, this also poses a threat to the privacy of the institution that is\nproviding the explanation, as well as the user who is requesting it. In this\nwork, we are primarily concerned with the user's privacy who wants to retrieve\na counterfactual instance, without revealing their feature vector to the\ninstitution. Our framework retrieves the exact nearest neighbor counterfactual\nexplanation from a database of accepted points while achieving perfect,\ninformation-theoretic, privacy for the user. First, we introduce the problem of\nprivate counterfactual retrieval (PCR) and propose a baseline PCR scheme that\nkeeps the user's feature vector information-theoretically private from the\ninstitution. Building on this, we propose two other schemes that reduce the\namount of information leaked about the institution database to the user,\ncompared to the baseline scheme. Second, we relax the assumption of mutability\nof all features, and consider the setting of immutable PCR (I-PCR). Here, the\nuser retrieves the nearest counterfactual without altering a private subset of\ntheir features, which constitutes the immutable set, while keeping their\nfeature vector and immutable set private from the institution. For this, we\npropose two schemes that preserve the user's privacy information-theoretically,\nbut ensure varying degrees of database privacy. Third, we extend our PCR and\nI-PCR schemes to incorporate user's preference on transforming their\nattributes, so that a more actionable explanation can be received. Finally, we\npresent numerical results to support our theoretical findings, and compare the\ndatabase leakage of the proposed schemes.\n", "link": "http://arxiv.org/abs/2508.03681v1", "date": "2025-08-05", "relevancy": 1.6995, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4355}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4212}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20If%2C%20But%20Privately%3A%20Private%20Counterfactual%20Retrieval&body=Title%3A%20What%20If%2C%20But%20Privately%3A%20Private%20Counterfactual%20Retrieval%0AAuthor%3A%20Shreya%20Meel%20and%20Mohamed%20Nomeir%20and%20Pasan%20Dissanayake%20and%20Sanghamitra%20Dutta%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20Transparency%20and%20explainability%20are%20two%20important%20aspects%20to%20be%20considered%0Awhen%20employing%20black-box%20machine%20learning%20models%20in%20high-stake%20applications.%0AProviding%20counterfactual%20explanations%20is%20one%20way%20of%20catering%20this%20requirement.%0AHowever%2C%20this%20also%20poses%20a%20threat%20to%20the%20privacy%20of%20the%20institution%20that%20is%0Aproviding%20the%20explanation%2C%20as%20well%20as%20the%20user%20who%20is%20requesting%20it.%20In%20this%0Awork%2C%20we%20are%20primarily%20concerned%20with%20the%20user%27s%20privacy%20who%20wants%20to%20retrieve%0Aa%20counterfactual%20instance%2C%20without%20revealing%20their%20feature%20vector%20to%20the%0Ainstitution.%20Our%20framework%20retrieves%20the%20exact%20nearest%20neighbor%20counterfactual%0Aexplanation%20from%20a%20database%20of%20accepted%20points%20while%20achieving%20perfect%2C%0Ainformation-theoretic%2C%20privacy%20for%20the%20user.%20First%2C%20we%20introduce%20the%20problem%20of%0Aprivate%20counterfactual%20retrieval%20%28PCR%29%20and%20propose%20a%20baseline%20PCR%20scheme%20that%0Akeeps%20the%20user%27s%20feature%20vector%20information-theoretically%20private%20from%20the%0Ainstitution.%20Building%20on%20this%2C%20we%20propose%20two%20other%20schemes%20that%20reduce%20the%0Aamount%20of%20information%20leaked%20about%20the%20institution%20database%20to%20the%20user%2C%0Acompared%20to%20the%20baseline%20scheme.%20Second%2C%20we%20relax%20the%20assumption%20of%20mutability%0Aof%20all%20features%2C%20and%20consider%20the%20setting%20of%20immutable%20PCR%20%28I-PCR%29.%20Here%2C%20the%0Auser%20retrieves%20the%20nearest%20counterfactual%20without%20altering%20a%20private%20subset%20of%0Atheir%20features%2C%20which%20constitutes%20the%20immutable%20set%2C%20while%20keeping%20their%0Afeature%20vector%20and%20immutable%20set%20private%20from%20the%20institution.%20For%20this%2C%20we%0Apropose%20two%20schemes%20that%20preserve%20the%20user%27s%20privacy%20information-theoretically%2C%0Abut%20ensure%20varying%20degrees%20of%20database%20privacy.%20Third%2C%20we%20extend%20our%20PCR%20and%0AI-PCR%20schemes%20to%20incorporate%20user%27s%20preference%20on%20transforming%20their%0Aattributes%2C%20so%20that%20a%20more%20actionable%20explanation%20can%20be%20received.%20Finally%2C%20we%0Apresent%20numerical%20results%20to%20support%20our%20theoretical%20findings%2C%20and%20compare%20the%0Adatabase%20leakage%20of%20the%20proposed%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520If%252C%2520But%2520Privately%253A%2520Private%2520Counterfactual%2520Retrieval%26entry.906535625%3DShreya%2520Meel%2520and%2520Mohamed%2520Nomeir%2520and%2520Pasan%2520Dissanayake%2520and%2520Sanghamitra%2520Dutta%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520Transparency%2520and%2520explainability%2520are%2520two%2520important%2520aspects%2520to%2520be%2520considered%250Awhen%2520employing%2520black-box%2520machine%2520learning%2520models%2520in%2520high-stake%2520applications.%250AProviding%2520counterfactual%2520explanations%2520is%2520one%2520way%2520of%2520catering%2520this%2520requirement.%250AHowever%252C%2520this%2520also%2520poses%2520a%2520threat%2520to%2520the%2520privacy%2520of%2520the%2520institution%2520that%2520is%250Aproviding%2520the%2520explanation%252C%2520as%2520well%2520as%2520the%2520user%2520who%2520is%2520requesting%2520it.%2520In%2520this%250Awork%252C%2520we%2520are%2520primarily%2520concerned%2520with%2520the%2520user%2527s%2520privacy%2520who%2520wants%2520to%2520retrieve%250Aa%2520counterfactual%2520instance%252C%2520without%2520revealing%2520their%2520feature%2520vector%2520to%2520the%250Ainstitution.%2520Our%2520framework%2520retrieves%2520the%2520exact%2520nearest%2520neighbor%2520counterfactual%250Aexplanation%2520from%2520a%2520database%2520of%2520accepted%2520points%2520while%2520achieving%2520perfect%252C%250Ainformation-theoretic%252C%2520privacy%2520for%2520the%2520user.%2520First%252C%2520we%2520introduce%2520the%2520problem%2520of%250Aprivate%2520counterfactual%2520retrieval%2520%2528PCR%2529%2520and%2520propose%2520a%2520baseline%2520PCR%2520scheme%2520that%250Akeeps%2520the%2520user%2527s%2520feature%2520vector%2520information-theoretically%2520private%2520from%2520the%250Ainstitution.%2520Building%2520on%2520this%252C%2520we%2520propose%2520two%2520other%2520schemes%2520that%2520reduce%2520the%250Aamount%2520of%2520information%2520leaked%2520about%2520the%2520institution%2520database%2520to%2520the%2520user%252C%250Acompared%2520to%2520the%2520baseline%2520scheme.%2520Second%252C%2520we%2520relax%2520the%2520assumption%2520of%2520mutability%250Aof%2520all%2520features%252C%2520and%2520consider%2520the%2520setting%2520of%2520immutable%2520PCR%2520%2528I-PCR%2529.%2520Here%252C%2520the%250Auser%2520retrieves%2520the%2520nearest%2520counterfactual%2520without%2520altering%2520a%2520private%2520subset%2520of%250Atheir%2520features%252C%2520which%2520constitutes%2520the%2520immutable%2520set%252C%2520while%2520keeping%2520their%250Afeature%2520vector%2520and%2520immutable%2520set%2520private%2520from%2520the%2520institution.%2520For%2520this%252C%2520we%250Apropose%2520two%2520schemes%2520that%2520preserve%2520the%2520user%2527s%2520privacy%2520information-theoretically%252C%250Abut%2520ensure%2520varying%2520degrees%2520of%2520database%2520privacy.%2520Third%252C%2520we%2520extend%2520our%2520PCR%2520and%250AI-PCR%2520schemes%2520to%2520incorporate%2520user%2527s%2520preference%2520on%2520transforming%2520their%250Aattributes%252C%2520so%2520that%2520a%2520more%2520actionable%2520explanation%2520can%2520be%2520received.%2520Finally%252C%2520we%250Apresent%2520numerical%2520results%2520to%2520support%2520our%2520theoretical%2520findings%252C%2520and%2520compare%2520the%250Adatabase%2520leakage%2520of%2520the%2520proposed%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20If%2C%20But%20Privately%3A%20Private%20Counterfactual%20Retrieval&entry.906535625=Shreya%20Meel%20and%20Mohamed%20Nomeir%20and%20Pasan%20Dissanayake%20and%20Sanghamitra%20Dutta%20and%20Sennur%20Ulukus&entry.1292438233=%20%20Transparency%20and%20explainability%20are%20two%20important%20aspects%20to%20be%20considered%0Awhen%20employing%20black-box%20machine%20learning%20models%20in%20high-stake%20applications.%0AProviding%20counterfactual%20explanations%20is%20one%20way%20of%20catering%20this%20requirement.%0AHowever%2C%20this%20also%20poses%20a%20threat%20to%20the%20privacy%20of%20the%20institution%20that%20is%0Aproviding%20the%20explanation%2C%20as%20well%20as%20the%20user%20who%20is%20requesting%20it.%20In%20this%0Awork%2C%20we%20are%20primarily%20concerned%20with%20the%20user%27s%20privacy%20who%20wants%20to%20retrieve%0Aa%20counterfactual%20instance%2C%20without%20revealing%20their%20feature%20vector%20to%20the%0Ainstitution.%20Our%20framework%20retrieves%20the%20exact%20nearest%20neighbor%20counterfactual%0Aexplanation%20from%20a%20database%20of%20accepted%20points%20while%20achieving%20perfect%2C%0Ainformation-theoretic%2C%20privacy%20for%20the%20user.%20First%2C%20we%20introduce%20the%20problem%20of%0Aprivate%20counterfactual%20retrieval%20%28PCR%29%20and%20propose%20a%20baseline%20PCR%20scheme%20that%0Akeeps%20the%20user%27s%20feature%20vector%20information-theoretically%20private%20from%20the%0Ainstitution.%20Building%20on%20this%2C%20we%20propose%20two%20other%20schemes%20that%20reduce%20the%0Aamount%20of%20information%20leaked%20about%20the%20institution%20database%20to%20the%20user%2C%0Acompared%20to%20the%20baseline%20scheme.%20Second%2C%20we%20relax%20the%20assumption%20of%20mutability%0Aof%20all%20features%2C%20and%20consider%20the%20setting%20of%20immutable%20PCR%20%28I-PCR%29.%20Here%2C%20the%0Auser%20retrieves%20the%20nearest%20counterfactual%20without%20altering%20a%20private%20subset%20of%0Atheir%20features%2C%20which%20constitutes%20the%20immutable%20set%2C%20while%20keeping%20their%0Afeature%20vector%20and%20immutable%20set%20private%20from%20the%20institution.%20For%20this%2C%20we%0Apropose%20two%20schemes%20that%20preserve%20the%20user%27s%20privacy%20information-theoretically%2C%0Abut%20ensure%20varying%20degrees%20of%20database%20privacy.%20Third%2C%20we%20extend%20our%20PCR%20and%0AI-PCR%20schemes%20to%20incorporate%20user%27s%20preference%20on%20transforming%20their%0Aattributes%2C%20so%20that%20a%20more%20actionable%20explanation%20can%20be%20received.%20Finally%2C%20we%0Apresent%20numerical%20results%20to%20support%20our%20theoretical%20findings%2C%20and%20compare%20the%0Adatabase%20leakage%20of%20the%20proposed%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03681v1&entry.124074799=Read"},
{"title": "TextMaster: A Unified Framework for Realistic Text Editing via\n  Glyph-Style Dual-Control", "author": "Zhenyu Yan and Jian Wang and Aoqiang Wang and Yuhan Li and Wenxiang Shang and Ran Lin", "abstract": "  In image editing tasks, high-quality text editing capabilities can\nsignificantly reduce both human and material resource costs. Existing methods,\nhowever, face significant limitations in terms of stroke accuracy for complex\ntext and controllability of generated text styles. To address these challenges,\nwe propose TextMaster, a solution capable of accurately editing text across\nvarious scenarios and image regions, while ensuring proper layout and\ncontrollable text style. Our method enhances the accuracy and fidelity of text\nrendering by incorporating high-resolution standard glyph information and\napplying perceptual loss within the text editing region. Additionally, we\nleverage an attention mechanism to compute intermediate layer bounding box\nregression loss for each character, enabling the model to learn text layout\nacross varying contexts. Furthermore, we propose a novel style injection\ntechnique that enables controllable style transfer for the injected text.\nThrough comprehensive experiments, we demonstrate the state-of-the-art\nperformance of our method.\n", "link": "http://arxiv.org/abs/2410.09879v2", "date": "2025-08-05", "relevancy": 1.687, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5715}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5617}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextMaster%3A%20A%20Unified%20Framework%20for%20Realistic%20Text%20Editing%20via%0A%20%20Glyph-Style%20Dual-Control&body=Title%3A%20TextMaster%3A%20A%20Unified%20Framework%20for%20Realistic%20Text%20Editing%20via%0A%20%20Glyph-Style%20Dual-Control%0AAuthor%3A%20Zhenyu%20Yan%20and%20Jian%20Wang%20and%20Aoqiang%20Wang%20and%20Yuhan%20Li%20and%20Wenxiang%20Shang%20and%20Ran%20Lin%0AAbstract%3A%20%20%20In%20image%20editing%20tasks%2C%20high-quality%20text%20editing%20capabilities%20can%0Asignificantly%20reduce%20both%20human%20and%20material%20resource%20costs.%20Existing%20methods%2C%0Ahowever%2C%20face%20significant%20limitations%20in%20terms%20of%20stroke%20accuracy%20for%20complex%0Atext%20and%20controllability%20of%20generated%20text%20styles.%20To%20address%20these%20challenges%2C%0Awe%20propose%20TextMaster%2C%20a%20solution%20capable%20of%20accurately%20editing%20text%20across%0Avarious%20scenarios%20and%20image%20regions%2C%20while%20ensuring%20proper%20layout%20and%0Acontrollable%20text%20style.%20Our%20method%20enhances%20the%20accuracy%20and%20fidelity%20of%20text%0Arendering%20by%20incorporating%20high-resolution%20standard%20glyph%20information%20and%0Aapplying%20perceptual%20loss%20within%20the%20text%20editing%20region.%20Additionally%2C%20we%0Aleverage%20an%20attention%20mechanism%20to%20compute%20intermediate%20layer%20bounding%20box%0Aregression%20loss%20for%20each%20character%2C%20enabling%20the%20model%20to%20learn%20text%20layout%0Aacross%20varying%20contexts.%20Furthermore%2C%20we%20propose%20a%20novel%20style%20injection%0Atechnique%20that%20enables%20controllable%20style%20transfer%20for%20the%20injected%20text.%0AThrough%20comprehensive%20experiments%2C%20we%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextMaster%253A%2520A%2520Unified%2520Framework%2520for%2520Realistic%2520Text%2520Editing%2520via%250A%2520%2520Glyph-Style%2520Dual-Control%26entry.906535625%3DZhenyu%2520Yan%2520and%2520Jian%2520Wang%2520and%2520Aoqiang%2520Wang%2520and%2520Yuhan%2520Li%2520and%2520Wenxiang%2520Shang%2520and%2520Ran%2520Lin%26entry.1292438233%3D%2520%2520In%2520image%2520editing%2520tasks%252C%2520high-quality%2520text%2520editing%2520capabilities%2520can%250Asignificantly%2520reduce%2520both%2520human%2520and%2520material%2520resource%2520costs.%2520Existing%2520methods%252C%250Ahowever%252C%2520face%2520significant%2520limitations%2520in%2520terms%2520of%2520stroke%2520accuracy%2520for%2520complex%250Atext%2520and%2520controllability%2520of%2520generated%2520text%2520styles.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520TextMaster%252C%2520a%2520solution%2520capable%2520of%2520accurately%2520editing%2520text%2520across%250Avarious%2520scenarios%2520and%2520image%2520regions%252C%2520while%2520ensuring%2520proper%2520layout%2520and%250Acontrollable%2520text%2520style.%2520Our%2520method%2520enhances%2520the%2520accuracy%2520and%2520fidelity%2520of%2520text%250Arendering%2520by%2520incorporating%2520high-resolution%2520standard%2520glyph%2520information%2520and%250Aapplying%2520perceptual%2520loss%2520within%2520the%2520text%2520editing%2520region.%2520Additionally%252C%2520we%250Aleverage%2520an%2520attention%2520mechanism%2520to%2520compute%2520intermediate%2520layer%2520bounding%2520box%250Aregression%2520loss%2520for%2520each%2520character%252C%2520enabling%2520the%2520model%2520to%2520learn%2520text%2520layout%250Aacross%2520varying%2520contexts.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520style%2520injection%250Atechnique%2520that%2520enables%2520controllable%2520style%2520transfer%2520for%2520the%2520injected%2520text.%250AThrough%2520comprehensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520state-of-the-art%250Aperformance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextMaster%3A%20A%20Unified%20Framework%20for%20Realistic%20Text%20Editing%20via%0A%20%20Glyph-Style%20Dual-Control&entry.906535625=Zhenyu%20Yan%20and%20Jian%20Wang%20and%20Aoqiang%20Wang%20and%20Yuhan%20Li%20and%20Wenxiang%20Shang%20and%20Ran%20Lin&entry.1292438233=%20%20In%20image%20editing%20tasks%2C%20high-quality%20text%20editing%20capabilities%20can%0Asignificantly%20reduce%20both%20human%20and%20material%20resource%20costs.%20Existing%20methods%2C%0Ahowever%2C%20face%20significant%20limitations%20in%20terms%20of%20stroke%20accuracy%20for%20complex%0Atext%20and%20controllability%20of%20generated%20text%20styles.%20To%20address%20these%20challenges%2C%0Awe%20propose%20TextMaster%2C%20a%20solution%20capable%20of%20accurately%20editing%20text%20across%0Avarious%20scenarios%20and%20image%20regions%2C%20while%20ensuring%20proper%20layout%20and%0Acontrollable%20text%20style.%20Our%20method%20enhances%20the%20accuracy%20and%20fidelity%20of%20text%0Arendering%20by%20incorporating%20high-resolution%20standard%20glyph%20information%20and%0Aapplying%20perceptual%20loss%20within%20the%20text%20editing%20region.%20Additionally%2C%20we%0Aleverage%20an%20attention%20mechanism%20to%20compute%20intermediate%20layer%20bounding%20box%0Aregression%20loss%20for%20each%20character%2C%20enabling%20the%20model%20to%20learn%20text%20layout%0Aacross%20varying%20contexts.%20Furthermore%2C%20we%20propose%20a%20novel%20style%20injection%0Atechnique%20that%20enables%20controllable%20style%20transfer%20for%20the%20injected%20text.%0AThrough%20comprehensive%20experiments%2C%20we%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09879v2&entry.124074799=Read"},
{"title": "Pair Correlation Factor and the Sample Complexity of Gaussian Mixtures", "author": "Farzad Aryan", "abstract": "  We study the problem of learning Gaussian Mixture Models (GMMs) and ask:\nwhich structural properties govern their sample complexity? Prior work has\nlargely tied this complexity to the minimum pairwise separation between\ncomponents, but we demonstrate this view is incomplete.\n  We introduce the \\emph{Pair Correlation Factor} (PCF), a geometric quantity\ncapturing the clustering of component means. Unlike the minimum gap, the PCF\nmore accurately dictates the difficulty of parameter recovery.\n  In the uniform spherical case, we give an algorithm with improved sample\ncomplexity bounds, showing when more than the usual $\\epsilon^{-2}$ samples are\nnecessary.\n", "link": "http://arxiv.org/abs/2508.03633v1", "date": "2025-08-05", "relevancy": 1.6842, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4356}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4135}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pair%20Correlation%20Factor%20and%20the%20Sample%20Complexity%20of%20Gaussian%20Mixtures&body=Title%3A%20Pair%20Correlation%20Factor%20and%20the%20Sample%20Complexity%20of%20Gaussian%20Mixtures%0AAuthor%3A%20Farzad%20Aryan%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20Gaussian%20Mixture%20Models%20%28GMMs%29%20and%20ask%3A%0Awhich%20structural%20properties%20govern%20their%20sample%20complexity%3F%20Prior%20work%20has%0Alargely%20tied%20this%20complexity%20to%20the%20minimum%20pairwise%20separation%20between%0Acomponents%2C%20but%20we%20demonstrate%20this%20view%20is%20incomplete.%0A%20%20We%20introduce%20the%20%5Cemph%7BPair%20Correlation%20Factor%7D%20%28PCF%29%2C%20a%20geometric%20quantity%0Acapturing%20the%20clustering%20of%20component%20means.%20Unlike%20the%20minimum%20gap%2C%20the%20PCF%0Amore%20accurately%20dictates%20the%20difficulty%20of%20parameter%20recovery.%0A%20%20In%20the%20uniform%20spherical%20case%2C%20we%20give%20an%20algorithm%20with%20improved%20sample%0Acomplexity%20bounds%2C%20showing%20when%20more%20than%20the%20usual%20%24%5Cepsilon%5E%7B-2%7D%24%20samples%20are%0Anecessary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPair%2520Correlation%2520Factor%2520and%2520the%2520Sample%2520Complexity%2520of%2520Gaussian%2520Mixtures%26entry.906535625%3DFarzad%2520Aryan%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529%2520and%2520ask%253A%250Awhich%2520structural%2520properties%2520govern%2520their%2520sample%2520complexity%253F%2520Prior%2520work%2520has%250Alargely%2520tied%2520this%2520complexity%2520to%2520the%2520minimum%2520pairwise%2520separation%2520between%250Acomponents%252C%2520but%2520we%2520demonstrate%2520this%2520view%2520is%2520incomplete.%250A%2520%2520We%2520introduce%2520the%2520%255Cemph%257BPair%2520Correlation%2520Factor%257D%2520%2528PCF%2529%252C%2520a%2520geometric%2520quantity%250Acapturing%2520the%2520clustering%2520of%2520component%2520means.%2520Unlike%2520the%2520minimum%2520gap%252C%2520the%2520PCF%250Amore%2520accurately%2520dictates%2520the%2520difficulty%2520of%2520parameter%2520recovery.%250A%2520%2520In%2520the%2520uniform%2520spherical%2520case%252C%2520we%2520give%2520an%2520algorithm%2520with%2520improved%2520sample%250Acomplexity%2520bounds%252C%2520showing%2520when%2520more%2520than%2520the%2520usual%2520%2524%255Cepsilon%255E%257B-2%257D%2524%2520samples%2520are%250Anecessary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pair%20Correlation%20Factor%20and%20the%20Sample%20Complexity%20of%20Gaussian%20Mixtures&entry.906535625=Farzad%20Aryan&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20Gaussian%20Mixture%20Models%20%28GMMs%29%20and%20ask%3A%0Awhich%20structural%20properties%20govern%20their%20sample%20complexity%3F%20Prior%20work%20has%0Alargely%20tied%20this%20complexity%20to%20the%20minimum%20pairwise%20separation%20between%0Acomponents%2C%20but%20we%20demonstrate%20this%20view%20is%20incomplete.%0A%20%20We%20introduce%20the%20%5Cemph%7BPair%20Correlation%20Factor%7D%20%28PCF%29%2C%20a%20geometric%20quantity%0Acapturing%20the%20clustering%20of%20component%20means.%20Unlike%20the%20minimum%20gap%2C%20the%20PCF%0Amore%20accurately%20dictates%20the%20difficulty%20of%20parameter%20recovery.%0A%20%20In%20the%20uniform%20spherical%20case%2C%20we%20give%20an%20algorithm%20with%20improved%20sample%0Acomplexity%20bounds%2C%20showing%20when%20more%20than%20the%20usual%20%24%5Cepsilon%5E%7B-2%7D%24%20samples%20are%0Anecessary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03633v1&entry.124074799=Read"},
{"title": "Cross-patient Seizure Onset Zone Classification by Patient-Dependent\n  Weight", "author": "Xuyang Zhao and Hidenori Sugano and Toshihisa Tanaka", "abstract": "  Identifying the seizure onset zone (SOZ) in patients with focal epilepsy is\nessential for surgical treatment and remains challenging due to its dependence\non visual judgment by clinical experts. The development of machine learning can\nassist in diagnosis and has made promising progress. However, unlike data in\nother fields, medical data is usually collected from individual patients, and\neach patient has different illnesses, physical conditions, and medical\nhistories, which leads to differences in the distribution of each patient's\ndata. This makes it difficult for a machine learning model to achieve\nconsistently reliable performance in every new patient dataset, which we refer\nto as the \"cross-patient problem.\" In this paper, we propose a method to\nfine-tune a pretrained model using patient-specific weights for every new test\npatient to improve diagnostic performance. First, the supervised learning\nmethod is used to train a machine learning model. Next, using the intermediate\nfeatures of the trained model obtained through the test patient data, the\nsimilarity between the test patient data and each training patient's data is\ndefined to determine the weight of each training patient to be used in the\nfollowing fine-tuning. Finally, we fine-tune all parameters in the pretrained\nmodel with training data and patient weights. In the experiment, the\nleave-one-patient-out method is used to evaluate the proposed method, and the\nresults show improved classification accuracy for every test patient, with an\naverage improvement of more than 10%.\n", "link": "http://arxiv.org/abs/2508.03635v1", "date": "2025-08-05", "relevancy": 1.6778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4198}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-patient%20Seizure%20Onset%20Zone%20Classification%20by%20Patient-Dependent%0A%20%20Weight&body=Title%3A%20Cross-patient%20Seizure%20Onset%20Zone%20Classification%20by%20Patient-Dependent%0A%20%20Weight%0AAuthor%3A%20Xuyang%20Zhao%20and%20Hidenori%20Sugano%20and%20Toshihisa%20Tanaka%0AAbstract%3A%20%20%20Identifying%20the%20seizure%20onset%20zone%20%28SOZ%29%20in%20patients%20with%20focal%20epilepsy%20is%0Aessential%20for%20surgical%20treatment%20and%20remains%20challenging%20due%20to%20its%20dependence%0Aon%20visual%20judgment%20by%20clinical%20experts.%20The%20development%20of%20machine%20learning%20can%0Aassist%20in%20diagnosis%20and%20has%20made%20promising%20progress.%20However%2C%20unlike%20data%20in%0Aother%20fields%2C%20medical%20data%20is%20usually%20collected%20from%20individual%20patients%2C%20and%0Aeach%20patient%20has%20different%20illnesses%2C%20physical%20conditions%2C%20and%20medical%0Ahistories%2C%20which%20leads%20to%20differences%20in%20the%20distribution%20of%20each%20patient%27s%0Adata.%20This%20makes%20it%20difficult%20for%20a%20machine%20learning%20model%20to%20achieve%0Aconsistently%20reliable%20performance%20in%20every%20new%20patient%20dataset%2C%20which%20we%20refer%0Ato%20as%20the%20%22cross-patient%20problem.%22%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%0Afine-tune%20a%20pretrained%20model%20using%20patient-specific%20weights%20for%20every%20new%20test%0Apatient%20to%20improve%20diagnostic%20performance.%20First%2C%20the%20supervised%20learning%0Amethod%20is%20used%20to%20train%20a%20machine%20learning%20model.%20Next%2C%20using%20the%20intermediate%0Afeatures%20of%20the%20trained%20model%20obtained%20through%20the%20test%20patient%20data%2C%20the%0Asimilarity%20between%20the%20test%20patient%20data%20and%20each%20training%20patient%27s%20data%20is%0Adefined%20to%20determine%20the%20weight%20of%20each%20training%20patient%20to%20be%20used%20in%20the%0Afollowing%20fine-tuning.%20Finally%2C%20we%20fine-tune%20all%20parameters%20in%20the%20pretrained%0Amodel%20with%20training%20data%20and%20patient%20weights.%20In%20the%20experiment%2C%20the%0Aleave-one-patient-out%20method%20is%20used%20to%20evaluate%20the%20proposed%20method%2C%20and%20the%0Aresults%20show%20improved%20classification%20accuracy%20for%20every%20test%20patient%2C%20with%20an%0Aaverage%20improvement%20of%20more%20than%2010%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-patient%2520Seizure%2520Onset%2520Zone%2520Classification%2520by%2520Patient-Dependent%250A%2520%2520Weight%26entry.906535625%3DXuyang%2520Zhao%2520and%2520Hidenori%2520Sugano%2520and%2520Toshihisa%2520Tanaka%26entry.1292438233%3D%2520%2520Identifying%2520the%2520seizure%2520onset%2520zone%2520%2528SOZ%2529%2520in%2520patients%2520with%2520focal%2520epilepsy%2520is%250Aessential%2520for%2520surgical%2520treatment%2520and%2520remains%2520challenging%2520due%2520to%2520its%2520dependence%250Aon%2520visual%2520judgment%2520by%2520clinical%2520experts.%2520The%2520development%2520of%2520machine%2520learning%2520can%250Aassist%2520in%2520diagnosis%2520and%2520has%2520made%2520promising%2520progress.%2520However%252C%2520unlike%2520data%2520in%250Aother%2520fields%252C%2520medical%2520data%2520is%2520usually%2520collected%2520from%2520individual%2520patients%252C%2520and%250Aeach%2520patient%2520has%2520different%2520illnesses%252C%2520physical%2520conditions%252C%2520and%2520medical%250Ahistories%252C%2520which%2520leads%2520to%2520differences%2520in%2520the%2520distribution%2520of%2520each%2520patient%2527s%250Adata.%2520This%2520makes%2520it%2520difficult%2520for%2520a%2520machine%2520learning%2520model%2520to%2520achieve%250Aconsistently%2520reliable%2520performance%2520in%2520every%2520new%2520patient%2520dataset%252C%2520which%2520we%2520refer%250Ato%2520as%2520the%2520%2522cross-patient%2520problem.%2522%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520to%250Afine-tune%2520a%2520pretrained%2520model%2520using%2520patient-specific%2520weights%2520for%2520every%2520new%2520test%250Apatient%2520to%2520improve%2520diagnostic%2520performance.%2520First%252C%2520the%2520supervised%2520learning%250Amethod%2520is%2520used%2520to%2520train%2520a%2520machine%2520learning%2520model.%2520Next%252C%2520using%2520the%2520intermediate%250Afeatures%2520of%2520the%2520trained%2520model%2520obtained%2520through%2520the%2520test%2520patient%2520data%252C%2520the%250Asimilarity%2520between%2520the%2520test%2520patient%2520data%2520and%2520each%2520training%2520patient%2527s%2520data%2520is%250Adefined%2520to%2520determine%2520the%2520weight%2520of%2520each%2520training%2520patient%2520to%2520be%2520used%2520in%2520the%250Afollowing%2520fine-tuning.%2520Finally%252C%2520we%2520fine-tune%2520all%2520parameters%2520in%2520the%2520pretrained%250Amodel%2520with%2520training%2520data%2520and%2520patient%2520weights.%2520In%2520the%2520experiment%252C%2520the%250Aleave-one-patient-out%2520method%2520is%2520used%2520to%2520evaluate%2520the%2520proposed%2520method%252C%2520and%2520the%250Aresults%2520show%2520improved%2520classification%2520accuracy%2520for%2520every%2520test%2520patient%252C%2520with%2520an%250Aaverage%2520improvement%2520of%2520more%2520than%252010%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-patient%20Seizure%20Onset%20Zone%20Classification%20by%20Patient-Dependent%0A%20%20Weight&entry.906535625=Xuyang%20Zhao%20and%20Hidenori%20Sugano%20and%20Toshihisa%20Tanaka&entry.1292438233=%20%20Identifying%20the%20seizure%20onset%20zone%20%28SOZ%29%20in%20patients%20with%20focal%20epilepsy%20is%0Aessential%20for%20surgical%20treatment%20and%20remains%20challenging%20due%20to%20its%20dependence%0Aon%20visual%20judgment%20by%20clinical%20experts.%20The%20development%20of%20machine%20learning%20can%0Aassist%20in%20diagnosis%20and%20has%20made%20promising%20progress.%20However%2C%20unlike%20data%20in%0Aother%20fields%2C%20medical%20data%20is%20usually%20collected%20from%20individual%20patients%2C%20and%0Aeach%20patient%20has%20different%20illnesses%2C%20physical%20conditions%2C%20and%20medical%0Ahistories%2C%20which%20leads%20to%20differences%20in%20the%20distribution%20of%20each%20patient%27s%0Adata.%20This%20makes%20it%20difficult%20for%20a%20machine%20learning%20model%20to%20achieve%0Aconsistently%20reliable%20performance%20in%20every%20new%20patient%20dataset%2C%20which%20we%20refer%0Ato%20as%20the%20%22cross-patient%20problem.%22%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%0Afine-tune%20a%20pretrained%20model%20using%20patient-specific%20weights%20for%20every%20new%20test%0Apatient%20to%20improve%20diagnostic%20performance.%20First%2C%20the%20supervised%20learning%0Amethod%20is%20used%20to%20train%20a%20machine%20learning%20model.%20Next%2C%20using%20the%20intermediate%0Afeatures%20of%20the%20trained%20model%20obtained%20through%20the%20test%20patient%20data%2C%20the%0Asimilarity%20between%20the%20test%20patient%20data%20and%20each%20training%20patient%27s%20data%20is%0Adefined%20to%20determine%20the%20weight%20of%20each%20training%20patient%20to%20be%20used%20in%20the%0Afollowing%20fine-tuning.%20Finally%2C%20we%20fine-tune%20all%20parameters%20in%20the%20pretrained%0Amodel%20with%20training%20data%20and%20patient%20weights.%20In%20the%20experiment%2C%20the%0Aleave-one-patient-out%20method%20is%20used%20to%20evaluate%20the%20proposed%20method%2C%20and%20the%0Aresults%20show%20improved%20classification%20accuracy%20for%20every%20test%20patient%2C%20with%20an%0Aaverage%20improvement%20of%20more%20than%2010%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03635v1&entry.124074799=Read"},
{"title": "Likelihood Matching for Diffusion Models", "author": "Lei Qian and Wu Su and Yanqi Huang and Song Xi Chen", "abstract": "  We propose a Likelihood Matching approach for training diffusion models by\nfirst establishing an equivalence between the likelihood of the target data\ndistribution and a likelihood along the sample path of the reverse diffusion.\nTo efficiently compute the reverse sample likelihood, a quasi-likelihood is\nconsidered to approximate each reverse transition density by a Gaussian\ndistribution with matched conditional mean and covariance, respectively. The\nscore and Hessian functions for the diffusion generation are estimated by\nmaximizing the quasi-likelihood, ensuring a consistent matching of both the\nfirst two transitional moments between every two time points. A stochastic\nsampler is introduced to facilitate computation that leverages on both the\nestimated score and Hessian information. We establish consistency of the\nquasi-maximum likelihood estimation, and provide non-asymptotic convergence\nguarantees for the proposed sampler, quantifying the rates of the approximation\nerrors due to the score and Hessian estimation, dimensionality, and the number\nof diffusion steps. Empirical and simulation evaluations demonstrate the\neffectiveness of the proposed Likelihood Matching and validate the theoretical\nresults.\n", "link": "http://arxiv.org/abs/2508.03636v1", "date": "2025-08-05", "relevancy": 1.4887, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5371}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4859}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood%20Matching%20for%20Diffusion%20Models&body=Title%3A%20Likelihood%20Matching%20for%20Diffusion%20Models%0AAuthor%3A%20Lei%20Qian%20and%20Wu%20Su%20and%20Yanqi%20Huang%20and%20Song%20Xi%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20Likelihood%20Matching%20approach%20for%20training%20diffusion%20models%20by%0Afirst%20establishing%20an%20equivalence%20between%20the%20likelihood%20of%20the%20target%20data%0Adistribution%20and%20a%20likelihood%20along%20the%20sample%20path%20of%20the%20reverse%20diffusion.%0ATo%20efficiently%20compute%20the%20reverse%20sample%20likelihood%2C%20a%20quasi-likelihood%20is%0Aconsidered%20to%20approximate%20each%20reverse%20transition%20density%20by%20a%20Gaussian%0Adistribution%20with%20matched%20conditional%20mean%20and%20covariance%2C%20respectively.%20The%0Ascore%20and%20Hessian%20functions%20for%20the%20diffusion%20generation%20are%20estimated%20by%0Amaximizing%20the%20quasi-likelihood%2C%20ensuring%20a%20consistent%20matching%20of%20both%20the%0Afirst%20two%20transitional%20moments%20between%20every%20two%20time%20points.%20A%20stochastic%0Asampler%20is%20introduced%20to%20facilitate%20computation%20that%20leverages%20on%20both%20the%0Aestimated%20score%20and%20Hessian%20information.%20We%20establish%20consistency%20of%20the%0Aquasi-maximum%20likelihood%20estimation%2C%20and%20provide%20non-asymptotic%20convergence%0Aguarantees%20for%20the%20proposed%20sampler%2C%20quantifying%20the%20rates%20of%20the%20approximation%0Aerrors%20due%20to%20the%20score%20and%20Hessian%20estimation%2C%20dimensionality%2C%20and%20the%20number%0Aof%20diffusion%20steps.%20Empirical%20and%20simulation%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20Likelihood%20Matching%20and%20validate%20the%20theoretical%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood%2520Matching%2520for%2520Diffusion%2520Models%26entry.906535625%3DLei%2520Qian%2520and%2520Wu%2520Su%2520and%2520Yanqi%2520Huang%2520and%2520Song%2520Xi%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520Likelihood%2520Matching%2520approach%2520for%2520training%2520diffusion%2520models%2520by%250Afirst%2520establishing%2520an%2520equivalence%2520between%2520the%2520likelihood%2520of%2520the%2520target%2520data%250Adistribution%2520and%2520a%2520likelihood%2520along%2520the%2520sample%2520path%2520of%2520the%2520reverse%2520diffusion.%250ATo%2520efficiently%2520compute%2520the%2520reverse%2520sample%2520likelihood%252C%2520a%2520quasi-likelihood%2520is%250Aconsidered%2520to%2520approximate%2520each%2520reverse%2520transition%2520density%2520by%2520a%2520Gaussian%250Adistribution%2520with%2520matched%2520conditional%2520mean%2520and%2520covariance%252C%2520respectively.%2520The%250Ascore%2520and%2520Hessian%2520functions%2520for%2520the%2520diffusion%2520generation%2520are%2520estimated%2520by%250Amaximizing%2520the%2520quasi-likelihood%252C%2520ensuring%2520a%2520consistent%2520matching%2520of%2520both%2520the%250Afirst%2520two%2520transitional%2520moments%2520between%2520every%2520two%2520time%2520points.%2520A%2520stochastic%250Asampler%2520is%2520introduced%2520to%2520facilitate%2520computation%2520that%2520leverages%2520on%2520both%2520the%250Aestimated%2520score%2520and%2520Hessian%2520information.%2520We%2520establish%2520consistency%2520of%2520the%250Aquasi-maximum%2520likelihood%2520estimation%252C%2520and%2520provide%2520non-asymptotic%2520convergence%250Aguarantees%2520for%2520the%2520proposed%2520sampler%252C%2520quantifying%2520the%2520rates%2520of%2520the%2520approximation%250Aerrors%2520due%2520to%2520the%2520score%2520and%2520Hessian%2520estimation%252C%2520dimensionality%252C%2520and%2520the%2520number%250Aof%2520diffusion%2520steps.%2520Empirical%2520and%2520simulation%2520evaluations%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520Likelihood%2520Matching%2520and%2520validate%2520the%2520theoretical%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood%20Matching%20for%20Diffusion%20Models&entry.906535625=Lei%20Qian%20and%20Wu%20Su%20and%20Yanqi%20Huang%20and%20Song%20Xi%20Chen&entry.1292438233=%20%20We%20propose%20a%20Likelihood%20Matching%20approach%20for%20training%20diffusion%20models%20by%0Afirst%20establishing%20an%20equivalence%20between%20the%20likelihood%20of%20the%20target%20data%0Adistribution%20and%20a%20likelihood%20along%20the%20sample%20path%20of%20the%20reverse%20diffusion.%0ATo%20efficiently%20compute%20the%20reverse%20sample%20likelihood%2C%20a%20quasi-likelihood%20is%0Aconsidered%20to%20approximate%20each%20reverse%20transition%20density%20by%20a%20Gaussian%0Adistribution%20with%20matched%20conditional%20mean%20and%20covariance%2C%20respectively.%20The%0Ascore%20and%20Hessian%20functions%20for%20the%20diffusion%20generation%20are%20estimated%20by%0Amaximizing%20the%20quasi-likelihood%2C%20ensuring%20a%20consistent%20matching%20of%20both%20the%0Afirst%20two%20transitional%20moments%20between%20every%20two%20time%20points.%20A%20stochastic%0Asampler%20is%20introduced%20to%20facilitate%20computation%20that%20leverages%20on%20both%20the%0Aestimated%20score%20and%20Hessian%20information.%20We%20establish%20consistency%20of%20the%0Aquasi-maximum%20likelihood%20estimation%2C%20and%20provide%20non-asymptotic%20convergence%0Aguarantees%20for%20the%20proposed%20sampler%2C%20quantifying%20the%20rates%20of%20the%20approximation%0Aerrors%20due%20to%20the%20score%20and%20Hessian%20estimation%2C%20dimensionality%2C%20and%20the%20number%0Aof%20diffusion%20steps.%20Empirical%20and%20simulation%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20Likelihood%20Matching%20and%20validate%20the%20theoretical%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03636v1&entry.124074799=Read"},
{"title": "REALM-Bench: A Benchmark for Evaluating Multi-Agent Systems on\n  Real-world, Dynamic Planning and Scheduling Tasks", "author": "Longling Geng and Edward Y. Chang", "abstract": "  This benchmark suite provides a comprehensive evaluation framework for\nassessing both individual LLMs and multi-agent systems in Real-world planning\nand scheduling scenarios. The suite encompasses 14 designed planning and\nscheduling problems that progress from basic to highly complex, incorporating\nkey aspects such as multi-agent coordination, inter-agent dependencies, and\ndynamic environmental disruptions. Each problem can be scaled along three\ndimensions: the number of parallel planning threads, the complexity of\ninter-dependencies, and the frequency of unexpected disruptions requiring\nReal-time adaptation. The benchmark includes 14 detailed problem\nspecifications, 15 comparison methods including Random, LPT, SPT, STPT, MPSR,\nDRL-Liu, GP, GEP, LSO, SPT/TWKR, DRL-Chen, DRL-Zhang, 2+ evaluation metrics,\nand baseline implementations using 3+ LLMs including GPT-4o, Claude-3.7,\nDeepSeek-R1, and 4 contemporary frameworks including LangGraph, AutoGen,\nCrewAI, and Swarm, enabling rigorous testing of both single-agent and\nmulti-agent planning capabilities. Through standardized evaluation criteria and\nscalable complexity, this benchmark aims to be opened to public, and drive\nprogress in developing more adaptable, robust, and scalable AI planning systems\nfor Real-world applications.\n", "link": "http://arxiv.org/abs/2502.18836v2", "date": "2025-08-05", "relevancy": 1.4536, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5326}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REALM-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Multi-Agent%20Systems%20on%0A%20%20Real-world%2C%20Dynamic%20Planning%20and%20Scheduling%20Tasks&body=Title%3A%20REALM-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Multi-Agent%20Systems%20on%0A%20%20Real-world%2C%20Dynamic%20Planning%20and%20Scheduling%20Tasks%0AAuthor%3A%20Longling%20Geng%20and%20Edward%20Y.%20Chang%0AAbstract%3A%20%20%20This%20benchmark%20suite%20provides%20a%20comprehensive%20evaluation%20framework%20for%0Aassessing%20both%20individual%20LLMs%20and%20multi-agent%20systems%20in%20Real-world%20planning%0Aand%20scheduling%20scenarios.%20The%20suite%20encompasses%2014%20designed%20planning%20and%0Ascheduling%20problems%20that%20progress%20from%20basic%20to%20highly%20complex%2C%20incorporating%0Akey%20aspects%20such%20as%20multi-agent%20coordination%2C%20inter-agent%20dependencies%2C%20and%0Adynamic%20environmental%20disruptions.%20Each%20problem%20can%20be%20scaled%20along%20three%0Adimensions%3A%20the%20number%20of%20parallel%20planning%20threads%2C%20the%20complexity%20of%0Ainter-dependencies%2C%20and%20the%20frequency%20of%20unexpected%20disruptions%20requiring%0AReal-time%20adaptation.%20The%20benchmark%20includes%2014%20detailed%20problem%0Aspecifications%2C%2015%20comparison%20methods%20including%20Random%2C%20LPT%2C%20SPT%2C%20STPT%2C%20MPSR%2C%0ADRL-Liu%2C%20GP%2C%20GEP%2C%20LSO%2C%20SPT/TWKR%2C%20DRL-Chen%2C%20DRL-Zhang%2C%202%2B%20evaluation%20metrics%2C%0Aand%20baseline%20implementations%20using%203%2B%20LLMs%20including%20GPT-4o%2C%20Claude-3.7%2C%0ADeepSeek-R1%2C%20and%204%20contemporary%20frameworks%20including%20LangGraph%2C%20AutoGen%2C%0ACrewAI%2C%20and%20Swarm%2C%20enabling%20rigorous%20testing%20of%20both%20single-agent%20and%0Amulti-agent%20planning%20capabilities.%20Through%20standardized%20evaluation%20criteria%20and%0Ascalable%20complexity%2C%20this%20benchmark%20aims%20to%20be%20opened%20to%20public%2C%20and%20drive%0Aprogress%20in%20developing%20more%20adaptable%2C%20robust%2C%20and%20scalable%20AI%20planning%20systems%0Afor%20Real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREALM-Bench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Multi-Agent%2520Systems%2520on%250A%2520%2520Real-world%252C%2520Dynamic%2520Planning%2520and%2520Scheduling%2520Tasks%26entry.906535625%3DLongling%2520Geng%2520and%2520Edward%2520Y.%2520Chang%26entry.1292438233%3D%2520%2520This%2520benchmark%2520suite%2520provides%2520a%2520comprehensive%2520evaluation%2520framework%2520for%250Aassessing%2520both%2520individual%2520LLMs%2520and%2520multi-agent%2520systems%2520in%2520Real-world%2520planning%250Aand%2520scheduling%2520scenarios.%2520The%2520suite%2520encompasses%252014%2520designed%2520planning%2520and%250Ascheduling%2520problems%2520that%2520progress%2520from%2520basic%2520to%2520highly%2520complex%252C%2520incorporating%250Akey%2520aspects%2520such%2520as%2520multi-agent%2520coordination%252C%2520inter-agent%2520dependencies%252C%2520and%250Adynamic%2520environmental%2520disruptions.%2520Each%2520problem%2520can%2520be%2520scaled%2520along%2520three%250Adimensions%253A%2520the%2520number%2520of%2520parallel%2520planning%2520threads%252C%2520the%2520complexity%2520of%250Ainter-dependencies%252C%2520and%2520the%2520frequency%2520of%2520unexpected%2520disruptions%2520requiring%250AReal-time%2520adaptation.%2520The%2520benchmark%2520includes%252014%2520detailed%2520problem%250Aspecifications%252C%252015%2520comparison%2520methods%2520including%2520Random%252C%2520LPT%252C%2520SPT%252C%2520STPT%252C%2520MPSR%252C%250ADRL-Liu%252C%2520GP%252C%2520GEP%252C%2520LSO%252C%2520SPT/TWKR%252C%2520DRL-Chen%252C%2520DRL-Zhang%252C%25202%252B%2520evaluation%2520metrics%252C%250Aand%2520baseline%2520implementations%2520using%25203%252B%2520LLMs%2520including%2520GPT-4o%252C%2520Claude-3.7%252C%250ADeepSeek-R1%252C%2520and%25204%2520contemporary%2520frameworks%2520including%2520LangGraph%252C%2520AutoGen%252C%250ACrewAI%252C%2520and%2520Swarm%252C%2520enabling%2520rigorous%2520testing%2520of%2520both%2520single-agent%2520and%250Amulti-agent%2520planning%2520capabilities.%2520Through%2520standardized%2520evaluation%2520criteria%2520and%250Ascalable%2520complexity%252C%2520this%2520benchmark%2520aims%2520to%2520be%2520opened%2520to%2520public%252C%2520and%2520drive%250Aprogress%2520in%2520developing%2520more%2520adaptable%252C%2520robust%252C%2520and%2520scalable%2520AI%2520planning%2520systems%250Afor%2520Real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REALM-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Multi-Agent%20Systems%20on%0A%20%20Real-world%2C%20Dynamic%20Planning%20and%20Scheduling%20Tasks&entry.906535625=Longling%20Geng%20and%20Edward%20Y.%20Chang&entry.1292438233=%20%20This%20benchmark%20suite%20provides%20a%20comprehensive%20evaluation%20framework%20for%0Aassessing%20both%20individual%20LLMs%20and%20multi-agent%20systems%20in%20Real-world%20planning%0Aand%20scheduling%20scenarios.%20The%20suite%20encompasses%2014%20designed%20planning%20and%0Ascheduling%20problems%20that%20progress%20from%20basic%20to%20highly%20complex%2C%20incorporating%0Akey%20aspects%20such%20as%20multi-agent%20coordination%2C%20inter-agent%20dependencies%2C%20and%0Adynamic%20environmental%20disruptions.%20Each%20problem%20can%20be%20scaled%20along%20three%0Adimensions%3A%20the%20number%20of%20parallel%20planning%20threads%2C%20the%20complexity%20of%0Ainter-dependencies%2C%20and%20the%20frequency%20of%20unexpected%20disruptions%20requiring%0AReal-time%20adaptation.%20The%20benchmark%20includes%2014%20detailed%20problem%0Aspecifications%2C%2015%20comparison%20methods%20including%20Random%2C%20LPT%2C%20SPT%2C%20STPT%2C%20MPSR%2C%0ADRL-Liu%2C%20GP%2C%20GEP%2C%20LSO%2C%20SPT/TWKR%2C%20DRL-Chen%2C%20DRL-Zhang%2C%202%2B%20evaluation%20metrics%2C%0Aand%20baseline%20implementations%20using%203%2B%20LLMs%20including%20GPT-4o%2C%20Claude-3.7%2C%0ADeepSeek-R1%2C%20and%204%20contemporary%20frameworks%20including%20LangGraph%2C%20AutoGen%2C%0ACrewAI%2C%20and%20Swarm%2C%20enabling%20rigorous%20testing%20of%20both%20single-agent%20and%0Amulti-agent%20planning%20capabilities.%20Through%20standardized%20evaluation%20criteria%20and%0Ascalable%20complexity%2C%20this%20benchmark%20aims%20to%20be%20opened%20to%20public%2C%20and%20drive%0Aprogress%20in%20developing%20more%20adaptable%2C%20robust%2C%20and%20scalable%20AI%20planning%20systems%0Afor%20Real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18836v2&entry.124074799=Read"},
{"title": "No LLM Solved Yu Tsumura's 554th Problem", "author": "Simon Frieder and William Hart", "abstract": "  We show, contrary to the optimism about LLM's problem-solving abilities,\nfueled by the recent gold medals that were attained, that a problem exists --\nYu Tsumura's 554th problem -- that a) is within the scope of an IMO problem in\nterms of proof sophistication, b) is not a combinatorics problem which has\ncaused issues for LLMs, c) requires fewer proof techniques than typical hard\nIMO problems, d) has a publicly available solution (likely in the training data\nof LLMs), and e) that cannot be readily solved by any existing off-the-shelf\nLLM (commercial or open-source).\n", "link": "http://arxiv.org/abs/2508.03685v1", "date": "2025-08-05", "relevancy": 1.4075, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20LLM%20Solved%20Yu%20Tsumura%27s%20554th%20Problem&body=Title%3A%20No%20LLM%20Solved%20Yu%20Tsumura%27s%20554th%20Problem%0AAuthor%3A%20Simon%20Frieder%20and%20William%20Hart%0AAbstract%3A%20%20%20We%20show%2C%20contrary%20to%20the%20optimism%20about%20LLM%27s%20problem-solving%20abilities%2C%0Afueled%20by%20the%20recent%20gold%20medals%20that%20were%20attained%2C%20that%20a%20problem%20exists%20--%0AYu%20Tsumura%27s%20554th%20problem%20--%20that%20a%29%20is%20within%20the%20scope%20of%20an%20IMO%20problem%20in%0Aterms%20of%20proof%20sophistication%2C%20b%29%20is%20not%20a%20combinatorics%20problem%20which%20has%0Acaused%20issues%20for%20LLMs%2C%20c%29%20requires%20fewer%20proof%20techniques%20than%20typical%20hard%0AIMO%20problems%2C%20d%29%20has%20a%20publicly%20available%20solution%20%28likely%20in%20the%20training%20data%0Aof%20LLMs%29%2C%20and%20e%29%20that%20cannot%20be%20readily%20solved%20by%20any%20existing%20off-the-shelf%0ALLM%20%28commercial%20or%20open-source%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520LLM%2520Solved%2520Yu%2520Tsumura%2527s%2520554th%2520Problem%26entry.906535625%3DSimon%2520Frieder%2520and%2520William%2520Hart%26entry.1292438233%3D%2520%2520We%2520show%252C%2520contrary%2520to%2520the%2520optimism%2520about%2520LLM%2527s%2520problem-solving%2520abilities%252C%250Afueled%2520by%2520the%2520recent%2520gold%2520medals%2520that%2520were%2520attained%252C%2520that%2520a%2520problem%2520exists%2520--%250AYu%2520Tsumura%2527s%2520554th%2520problem%2520--%2520that%2520a%2529%2520is%2520within%2520the%2520scope%2520of%2520an%2520IMO%2520problem%2520in%250Aterms%2520of%2520proof%2520sophistication%252C%2520b%2529%2520is%2520not%2520a%2520combinatorics%2520problem%2520which%2520has%250Acaused%2520issues%2520for%2520LLMs%252C%2520c%2529%2520requires%2520fewer%2520proof%2520techniques%2520than%2520typical%2520hard%250AIMO%2520problems%252C%2520d%2529%2520has%2520a%2520publicly%2520available%2520solution%2520%2528likely%2520in%2520the%2520training%2520data%250Aof%2520LLMs%2529%252C%2520and%2520e%2529%2520that%2520cannot%2520be%2520readily%2520solved%2520by%2520any%2520existing%2520off-the-shelf%250ALLM%2520%2528commercial%2520or%2520open-source%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20LLM%20Solved%20Yu%20Tsumura%27s%20554th%20Problem&entry.906535625=Simon%20Frieder%20and%20William%20Hart&entry.1292438233=%20%20We%20show%2C%20contrary%20to%20the%20optimism%20about%20LLM%27s%20problem-solving%20abilities%2C%0Afueled%20by%20the%20recent%20gold%20medals%20that%20were%20attained%2C%20that%20a%20problem%20exists%20--%0AYu%20Tsumura%27s%20554th%20problem%20--%20that%20a%29%20is%20within%20the%20scope%20of%20an%20IMO%20problem%20in%0Aterms%20of%20proof%20sophistication%2C%20b%29%20is%20not%20a%20combinatorics%20problem%20which%20has%0Acaused%20issues%20for%20LLMs%2C%20c%29%20requires%20fewer%20proof%20techniques%20than%20typical%20hard%0AIMO%20problems%2C%20d%29%20has%20a%20publicly%20available%20solution%20%28likely%20in%20the%20training%20data%0Aof%20LLMs%29%2C%20and%20e%29%20that%20cannot%20be%20readily%20solved%20by%20any%20existing%20off-the-shelf%0ALLM%20%28commercial%20or%20open-source%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03685v1&entry.124074799=Read"},
{"title": "MaLV-OS: Rethinking the Operating System Architecture for Machine\n  Learning in Virtualized Clouds", "author": "Stella Bitchebe and Oana Balmau", "abstract": "  A large body of research has employed Machine Learning (ML) models to develop\nlearned operating systems (OSes) and kernels. The latter dynamically adapts to\nthe job load and dynamically adjusts resources (CPU, IO, memory, network\nbandwidth) allocation to respond to the actual user demand. What this work has\nin common is that it utilizes ML to improve kernel decisions. To this day, and\nto the best of our knowledge, no work has taken the opposite direction, i.e.,\nusing OS to improve ML. While some work proposes applying system-level\noptimizations to ML algorithms, they do not tailor the OS to adapt to the ML\ncontext. To address this limitation, we take an orthogonal approach in this\npaper by leveraging the OS to enhance the performance of ML models and\nalgorithms. We explore the path towards an ML-specialized OS, MaLV-OS. MaLV-OS\nrethinks the OS architecture to make it specifically tailored to ML workloads,\nespecially in virtualized clouds, which are now widely used to run ML\napplications. MaLV-OS envisioned architecture includes (1) a micro-kernel,\nMicro-LAKE, which allows kernel space applications to use the GPU, and (2) an\nMLaaS (ML as a Service) subsystem that gathers ML models to help Micro-LAKE\nwith memory management and CPU scheduling. MaLV-OS architecture also offloads\nsystem-sensitive parts of the models to the OS, to lighten the model complexity\nand programming, and speed up its execution. Finally, MaLV-OS integrates an\nopen-source GPU virtualization software, merged directly into the hypervisor.\nFor more flexibility, MaLV-OS vision is to enable the virtual machine to\ndynamically select MLaaS policies that can improve the performance of the model\nthe user is running. Because MLaaS is designed as loadable kernel modules, the\nMaLV-OS architecture enables the dynamic addition of new capabilities to the\nMLaaS subsystem.\n", "link": "http://arxiv.org/abs/2508.03676v1", "date": "2025-08-05", "relevancy": 1.3708, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4697}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaLV-OS%3A%20Rethinking%20the%20Operating%20System%20Architecture%20for%20Machine%0A%20%20Learning%20in%20Virtualized%20Clouds&body=Title%3A%20MaLV-OS%3A%20Rethinking%20the%20Operating%20System%20Architecture%20for%20Machine%0A%20%20Learning%20in%20Virtualized%20Clouds%0AAuthor%3A%20Stella%20Bitchebe%20and%20Oana%20Balmau%0AAbstract%3A%20%20%20A%20large%20body%20of%20research%20has%20employed%20Machine%20Learning%20%28ML%29%20models%20to%20develop%0Alearned%20operating%20systems%20%28OSes%29%20and%20kernels.%20The%20latter%20dynamically%20adapts%20to%0Athe%20job%20load%20and%20dynamically%20adjusts%20resources%20%28CPU%2C%20IO%2C%20memory%2C%20network%0Abandwidth%29%20allocation%20to%20respond%20to%20the%20actual%20user%20demand.%20What%20this%20work%20has%0Ain%20common%20is%20that%20it%20utilizes%20ML%20to%20improve%20kernel%20decisions.%20To%20this%20day%2C%20and%0Ato%20the%20best%20of%20our%20knowledge%2C%20no%20work%20has%20taken%20the%20opposite%20direction%2C%20i.e.%2C%0Ausing%20OS%20to%20improve%20ML.%20While%20some%20work%20proposes%20applying%20system-level%0Aoptimizations%20to%20ML%20algorithms%2C%20they%20do%20not%20tailor%20the%20OS%20to%20adapt%20to%20the%20ML%0Acontext.%20To%20address%20this%20limitation%2C%20we%20take%20an%20orthogonal%20approach%20in%20this%0Apaper%20by%20leveraging%20the%20OS%20to%20enhance%20the%20performance%20of%20ML%20models%20and%0Aalgorithms.%20We%20explore%20the%20path%20towards%20an%20ML-specialized%20OS%2C%20MaLV-OS.%20MaLV-OS%0Arethinks%20the%20OS%20architecture%20to%20make%20it%20specifically%20tailored%20to%20ML%20workloads%2C%0Aespecially%20in%20virtualized%20clouds%2C%20which%20are%20now%20widely%20used%20to%20run%20ML%0Aapplications.%20MaLV-OS%20envisioned%20architecture%20includes%20%281%29%20a%20micro-kernel%2C%0AMicro-LAKE%2C%20which%20allows%20kernel%20space%20applications%20to%20use%20the%20GPU%2C%20and%20%282%29%20an%0AMLaaS%20%28ML%20as%20a%20Service%29%20subsystem%20that%20gathers%20ML%20models%20to%20help%20Micro-LAKE%0Awith%20memory%20management%20and%20CPU%20scheduling.%20MaLV-OS%20architecture%20also%20offloads%0Asystem-sensitive%20parts%20of%20the%20models%20to%20the%20OS%2C%20to%20lighten%20the%20model%20complexity%0Aand%20programming%2C%20and%20speed%20up%20its%20execution.%20Finally%2C%20MaLV-OS%20integrates%20an%0Aopen-source%20GPU%20virtualization%20software%2C%20merged%20directly%20into%20the%20hypervisor.%0AFor%20more%20flexibility%2C%20MaLV-OS%20vision%20is%20to%20enable%20the%20virtual%20machine%20to%0Adynamically%20select%20MLaaS%20policies%20that%20can%20improve%20the%20performance%20of%20the%20model%0Athe%20user%20is%20running.%20Because%20MLaaS%20is%20designed%20as%20loadable%20kernel%20modules%2C%20the%0AMaLV-OS%20architecture%20enables%20the%20dynamic%20addition%20of%20new%20capabilities%20to%20the%0AMLaaS%20subsystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaLV-OS%253A%2520Rethinking%2520the%2520Operating%2520System%2520Architecture%2520for%2520Machine%250A%2520%2520Learning%2520in%2520Virtualized%2520Clouds%26entry.906535625%3DStella%2520Bitchebe%2520and%2520Oana%2520Balmau%26entry.1292438233%3D%2520%2520A%2520large%2520body%2520of%2520research%2520has%2520employed%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520to%2520develop%250Alearned%2520operating%2520systems%2520%2528OSes%2529%2520and%2520kernels.%2520The%2520latter%2520dynamically%2520adapts%2520to%250Athe%2520job%2520load%2520and%2520dynamically%2520adjusts%2520resources%2520%2528CPU%252C%2520IO%252C%2520memory%252C%2520network%250Abandwidth%2529%2520allocation%2520to%2520respond%2520to%2520the%2520actual%2520user%2520demand.%2520What%2520this%2520work%2520has%250Ain%2520common%2520is%2520that%2520it%2520utilizes%2520ML%2520to%2520improve%2520kernel%2520decisions.%2520To%2520this%2520day%252C%2520and%250Ato%2520the%2520best%2520of%2520our%2520knowledge%252C%2520no%2520work%2520has%2520taken%2520the%2520opposite%2520direction%252C%2520i.e.%252C%250Ausing%2520OS%2520to%2520improve%2520ML.%2520While%2520some%2520work%2520proposes%2520applying%2520system-level%250Aoptimizations%2520to%2520ML%2520algorithms%252C%2520they%2520do%2520not%2520tailor%2520the%2520OS%2520to%2520adapt%2520to%2520the%2520ML%250Acontext.%2520To%2520address%2520this%2520limitation%252C%2520we%2520take%2520an%2520orthogonal%2520approach%2520in%2520this%250Apaper%2520by%2520leveraging%2520the%2520OS%2520to%2520enhance%2520the%2520performance%2520of%2520ML%2520models%2520and%250Aalgorithms.%2520We%2520explore%2520the%2520path%2520towards%2520an%2520ML-specialized%2520OS%252C%2520MaLV-OS.%2520MaLV-OS%250Arethinks%2520the%2520OS%2520architecture%2520to%2520make%2520it%2520specifically%2520tailored%2520to%2520ML%2520workloads%252C%250Aespecially%2520in%2520virtualized%2520clouds%252C%2520which%2520are%2520now%2520widely%2520used%2520to%2520run%2520ML%250Aapplications.%2520MaLV-OS%2520envisioned%2520architecture%2520includes%2520%25281%2529%2520a%2520micro-kernel%252C%250AMicro-LAKE%252C%2520which%2520allows%2520kernel%2520space%2520applications%2520to%2520use%2520the%2520GPU%252C%2520and%2520%25282%2529%2520an%250AMLaaS%2520%2528ML%2520as%2520a%2520Service%2529%2520subsystem%2520that%2520gathers%2520ML%2520models%2520to%2520help%2520Micro-LAKE%250Awith%2520memory%2520management%2520and%2520CPU%2520scheduling.%2520MaLV-OS%2520architecture%2520also%2520offloads%250Asystem-sensitive%2520parts%2520of%2520the%2520models%2520to%2520the%2520OS%252C%2520to%2520lighten%2520the%2520model%2520complexity%250Aand%2520programming%252C%2520and%2520speed%2520up%2520its%2520execution.%2520Finally%252C%2520MaLV-OS%2520integrates%2520an%250Aopen-source%2520GPU%2520virtualization%2520software%252C%2520merged%2520directly%2520into%2520the%2520hypervisor.%250AFor%2520more%2520flexibility%252C%2520MaLV-OS%2520vision%2520is%2520to%2520enable%2520the%2520virtual%2520machine%2520to%250Adynamically%2520select%2520MLaaS%2520policies%2520that%2520can%2520improve%2520the%2520performance%2520of%2520the%2520model%250Athe%2520user%2520is%2520running.%2520Because%2520MLaaS%2520is%2520designed%2520as%2520loadable%2520kernel%2520modules%252C%2520the%250AMaLV-OS%2520architecture%2520enables%2520the%2520dynamic%2520addition%2520of%2520new%2520capabilities%2520to%2520the%250AMLaaS%2520subsystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaLV-OS%3A%20Rethinking%20the%20Operating%20System%20Architecture%20for%20Machine%0A%20%20Learning%20in%20Virtualized%20Clouds&entry.906535625=Stella%20Bitchebe%20and%20Oana%20Balmau&entry.1292438233=%20%20A%20large%20body%20of%20research%20has%20employed%20Machine%20Learning%20%28ML%29%20models%20to%20develop%0Alearned%20operating%20systems%20%28OSes%29%20and%20kernels.%20The%20latter%20dynamically%20adapts%20to%0Athe%20job%20load%20and%20dynamically%20adjusts%20resources%20%28CPU%2C%20IO%2C%20memory%2C%20network%0Abandwidth%29%20allocation%20to%20respond%20to%20the%20actual%20user%20demand.%20What%20this%20work%20has%0Ain%20common%20is%20that%20it%20utilizes%20ML%20to%20improve%20kernel%20decisions.%20To%20this%20day%2C%20and%0Ato%20the%20best%20of%20our%20knowledge%2C%20no%20work%20has%20taken%20the%20opposite%20direction%2C%20i.e.%2C%0Ausing%20OS%20to%20improve%20ML.%20While%20some%20work%20proposes%20applying%20system-level%0Aoptimizations%20to%20ML%20algorithms%2C%20they%20do%20not%20tailor%20the%20OS%20to%20adapt%20to%20the%20ML%0Acontext.%20To%20address%20this%20limitation%2C%20we%20take%20an%20orthogonal%20approach%20in%20this%0Apaper%20by%20leveraging%20the%20OS%20to%20enhance%20the%20performance%20of%20ML%20models%20and%0Aalgorithms.%20We%20explore%20the%20path%20towards%20an%20ML-specialized%20OS%2C%20MaLV-OS.%20MaLV-OS%0Arethinks%20the%20OS%20architecture%20to%20make%20it%20specifically%20tailored%20to%20ML%20workloads%2C%0Aespecially%20in%20virtualized%20clouds%2C%20which%20are%20now%20widely%20used%20to%20run%20ML%0Aapplications.%20MaLV-OS%20envisioned%20architecture%20includes%20%281%29%20a%20micro-kernel%2C%0AMicro-LAKE%2C%20which%20allows%20kernel%20space%20applications%20to%20use%20the%20GPU%2C%20and%20%282%29%20an%0AMLaaS%20%28ML%20as%20a%20Service%29%20subsystem%20that%20gathers%20ML%20models%20to%20help%20Micro-LAKE%0Awith%20memory%20management%20and%20CPU%20scheduling.%20MaLV-OS%20architecture%20also%20offloads%0Asystem-sensitive%20parts%20of%20the%20models%20to%20the%20OS%2C%20to%20lighten%20the%20model%20complexity%0Aand%20programming%2C%20and%20speed%20up%20its%20execution.%20Finally%2C%20MaLV-OS%20integrates%20an%0Aopen-source%20GPU%20virtualization%20software%2C%20merged%20directly%20into%20the%20hypervisor.%0AFor%20more%20flexibility%2C%20MaLV-OS%20vision%20is%20to%20enable%20the%20virtual%20machine%20to%0Adynamically%20select%20MLaaS%20policies%20that%20can%20improve%20the%20performance%20of%20the%20model%0Athe%20user%20is%20running.%20Because%20MLaaS%20is%20designed%20as%20loadable%20kernel%20modules%2C%20the%0AMaLV-OS%20architecture%20enables%20the%20dynamic%20addition%20of%20new%20capabilities%20to%20the%0AMLaaS%20subsystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03676v1&entry.124074799=Read"},
{"title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design", "author": "Claudiu Leoveanu-Condrei", "abstract": "  Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.\n", "link": "http://arxiv.org/abs/2508.03665v1", "date": "2025-08-05", "relevancy": 1.3644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4763}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4498}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DbC%20Inspired%20Neurosymbolic%20Layer%20for%20Trustworthy%20Agent%20Design&body=Title%3A%20A%20DbC%20Inspired%20Neurosymbolic%20Layer%20for%20Trustworthy%20Agent%20Design%0AAuthor%3A%20Claudiu%20Leoveanu-Condrei%0AAbstract%3A%20%20%20Generative%20models%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%20produce%20fluent%0Aoutputs%20yet%20lack%20verifiable%20guarantees.%20We%20adapt%20Design%20by%20Contract%20%28DbC%29%20and%0Atype-theoretic%20principles%20to%20introduce%20a%20contract%20layer%20that%20mediates%20every%20LLM%0Acall.%20Contracts%20stipulate%20semantic%20and%20type%20requirements%20on%20inputs%20and%20outputs%2C%0Acoupled%20with%20probabilistic%20remediation%20to%20steer%20generation%20toward%20compliance.%0AThe%20layer%20exposes%20the%20dual%20view%20of%20LLMs%20as%20semantic%20parsers%20and%20probabilistic%0Ablack-box%20components.%20Contract%20satisfaction%20is%20probabilistic%20and%20semantic%0Avalidation%20is%20operationally%20defined%20through%20programmer-specified%20conditions%20on%0Awell-typed%20data%20structures.%20More%20broadly%2C%20this%20work%20postulates%20that%20any%20two%0Aagents%20satisfying%20the%20same%20contracts%20are%20%5Cemph%7Bfunctionally%20equivalent%7D%20with%0Arespect%20to%20those%20contracts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DbC%2520Inspired%2520Neurosymbolic%2520Layer%2520for%2520Trustworthy%2520Agent%2520Design%26entry.906535625%3DClaudiu%2520Leoveanu-Condrei%26entry.1292438233%3D%2520%2520Generative%2520models%252C%2520particularly%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520produce%2520fluent%250Aoutputs%2520yet%2520lack%2520verifiable%2520guarantees.%2520We%2520adapt%2520Design%2520by%2520Contract%2520%2528DbC%2529%2520and%250Atype-theoretic%2520principles%2520to%2520introduce%2520a%2520contract%2520layer%2520that%2520mediates%2520every%2520LLM%250Acall.%2520Contracts%2520stipulate%2520semantic%2520and%2520type%2520requirements%2520on%2520inputs%2520and%2520outputs%252C%250Acoupled%2520with%2520probabilistic%2520remediation%2520to%2520steer%2520generation%2520toward%2520compliance.%250AThe%2520layer%2520exposes%2520the%2520dual%2520view%2520of%2520LLMs%2520as%2520semantic%2520parsers%2520and%2520probabilistic%250Ablack-box%2520components.%2520Contract%2520satisfaction%2520is%2520probabilistic%2520and%2520semantic%250Avalidation%2520is%2520operationally%2520defined%2520through%2520programmer-specified%2520conditions%2520on%250Awell-typed%2520data%2520structures.%2520More%2520broadly%252C%2520this%2520work%2520postulates%2520that%2520any%2520two%250Aagents%2520satisfying%2520the%2520same%2520contracts%2520are%2520%255Cemph%257Bfunctionally%2520equivalent%257D%2520with%250Arespect%2520to%2520those%2520contracts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DbC%20Inspired%20Neurosymbolic%20Layer%20for%20Trustworthy%20Agent%20Design&entry.906535625=Claudiu%20Leoveanu-Condrei&entry.1292438233=%20%20Generative%20models%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%20produce%20fluent%0Aoutputs%20yet%20lack%20verifiable%20guarantees.%20We%20adapt%20Design%20by%20Contract%20%28DbC%29%20and%0Atype-theoretic%20principles%20to%20introduce%20a%20contract%20layer%20that%20mediates%20every%20LLM%0Acall.%20Contracts%20stipulate%20semantic%20and%20type%20requirements%20on%20inputs%20and%20outputs%2C%0Acoupled%20with%20probabilistic%20remediation%20to%20steer%20generation%20toward%20compliance.%0AThe%20layer%20exposes%20the%20dual%20view%20of%20LLMs%20as%20semantic%20parsers%20and%20probabilistic%0Ablack-box%20components.%20Contract%20satisfaction%20is%20probabilistic%20and%20semantic%0Avalidation%20is%20operationally%20defined%20through%20programmer-specified%20conditions%20on%0Awell-typed%20data%20structures.%20More%20broadly%2C%20this%20work%20postulates%20that%20any%20two%0Aagents%20satisfying%20the%20same%20contracts%20are%20%5Cemph%7Bfunctionally%20equivalent%7D%20with%0Arespect%20to%20those%20contracts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03665v1&entry.124074799=Read"},
{"title": "DiWA: Diffusion Policy Adaptation with World Models", "author": "Akshay L Chandra and Iman Nematollahi and Chenguang Huang and Tim Welschehold and Wolfram Burgard and Abhinav Valada", "abstract": "  Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2508.03645v1", "date": "2025-08-05", "relevancy": 1.0572, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiWA%3A%20Diffusion%20Policy%20Adaptation%20with%20World%20Models&body=Title%3A%20DiWA%3A%20Diffusion%20Policy%20Adaptation%20with%20World%20Models%0AAuthor%3A%20Akshay%20L%20Chandra%20and%20Iman%20Nematollahi%20and%20Chenguang%20Huang%20and%20Tim%20Welschehold%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Fine-tuning%20diffusion%20policies%20with%20reinforcement%20learning%20%28RL%29%20presents%0Asignificant%20challenges.%20The%20long%20denoising%20sequence%20for%20each%20action%20prediction%0Aimpedes%20effective%20reward%20propagation.%20Moreover%2C%20standard%20RL%20methods%20require%0Amillions%20of%20real-world%20interactions%2C%20posing%20a%20major%20bottleneck%20for%20practical%0Afine-tuning.%20Although%20prior%20work%20frames%20the%20denoising%20process%20in%20diffusion%0Apolicies%20as%20a%20Markov%20Decision%20Process%20to%20enable%20RL-based%20updates%2C%20its%20strong%0Adependence%20on%20environment%20interaction%20remains%20highly%20inefficient.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20DiWA%2C%20a%20novel%20framework%20that%20leverages%20a%20world%20model%20for%0Afine-tuning%20diffusion-based%20robotic%20skills%20entirely%20offline%20with%20reinforcement%0Alearning.%20Unlike%20model-free%20approaches%20that%20require%20millions%20of%20environment%0Ainteractions%20to%20fine-tune%20a%20repertoire%20of%20robot%20skills%2C%20DiWA%20achieves%20effective%0Aadaptation%20using%20a%20world%20model%20trained%20once%20on%20a%20few%20hundred%20thousand%20offline%0Aplay%20interactions.%20This%20results%20in%20dramatically%20improved%20sample%20efficiency%2C%0Amaking%20the%20approach%20significantly%20more%20practical%20and%20safer%20for%20real-world%20robot%0Alearning.%20On%20the%20challenging%20CALVIN%20benchmark%2C%20DiWA%20improves%20performance%20across%0Aeight%20tasks%20using%20only%20offline%20adaptation%2C%20while%20requiring%20orders%20of%20magnitude%0Afewer%20physical%20interactions%20than%20model-free%20baselines.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20demonstration%20of%20fine-tuning%20diffusion%20policies%20for%20real-world%0Arobotic%20skills%20using%20an%20offline%20world%20model.%20We%20make%20the%20code%20publicly%0Aavailable%20at%20https%3A//diwa.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiWA%253A%2520Diffusion%2520Policy%2520Adaptation%2520with%2520World%2520Models%26entry.906535625%3DAkshay%2520L%2520Chandra%2520and%2520Iman%2520Nematollahi%2520and%2520Chenguang%2520Huang%2520and%2520Tim%2520Welschehold%2520and%2520Wolfram%2520Burgard%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Fine-tuning%2520diffusion%2520policies%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520presents%250Asignificant%2520challenges.%2520The%2520long%2520denoising%2520sequence%2520for%2520each%2520action%2520prediction%250Aimpedes%2520effective%2520reward%2520propagation.%2520Moreover%252C%2520standard%2520RL%2520methods%2520require%250Amillions%2520of%2520real-world%2520interactions%252C%2520posing%2520a%2520major%2520bottleneck%2520for%2520practical%250Afine-tuning.%2520Although%2520prior%2520work%2520frames%2520the%2520denoising%2520process%2520in%2520diffusion%250Apolicies%2520as%2520a%2520Markov%2520Decision%2520Process%2520to%2520enable%2520RL-based%2520updates%252C%2520its%2520strong%250Adependence%2520on%2520environment%2520interaction%2520remains%2520highly%2520inefficient.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520DiWA%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520a%2520world%2520model%2520for%250Afine-tuning%2520diffusion-based%2520robotic%2520skills%2520entirely%2520offline%2520with%2520reinforcement%250Alearning.%2520Unlike%2520model-free%2520approaches%2520that%2520require%2520millions%2520of%2520environment%250Ainteractions%2520to%2520fine-tune%2520a%2520repertoire%2520of%2520robot%2520skills%252C%2520DiWA%2520achieves%2520effective%250Aadaptation%2520using%2520a%2520world%2520model%2520trained%2520once%2520on%2520a%2520few%2520hundred%2520thousand%2520offline%250Aplay%2520interactions.%2520This%2520results%2520in%2520dramatically%2520improved%2520sample%2520efficiency%252C%250Amaking%2520the%2520approach%2520significantly%2520more%2520practical%2520and%2520safer%2520for%2520real-world%2520robot%250Alearning.%2520On%2520the%2520challenging%2520CALVIN%2520benchmark%252C%2520DiWA%2520improves%2520performance%2520across%250Aeight%2520tasks%2520using%2520only%2520offline%2520adaptation%252C%2520while%2520requiring%2520orders%2520of%2520magnitude%250Afewer%2520physical%2520interactions%2520than%2520model-free%2520baselines.%2520To%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520demonstration%2520of%2520fine-tuning%2520diffusion%2520policies%2520for%2520real-world%250Arobotic%2520skills%2520using%2520an%2520offline%2520world%2520model.%2520We%2520make%2520the%2520code%2520publicly%250Aavailable%2520at%2520https%253A//diwa.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiWA%3A%20Diffusion%20Policy%20Adaptation%20with%20World%20Models&entry.906535625=Akshay%20L%20Chandra%20and%20Iman%20Nematollahi%20and%20Chenguang%20Huang%20and%20Tim%20Welschehold%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada&entry.1292438233=%20%20Fine-tuning%20diffusion%20policies%20with%20reinforcement%20learning%20%28RL%29%20presents%0Asignificant%20challenges.%20The%20long%20denoising%20sequence%20for%20each%20action%20prediction%0Aimpedes%20effective%20reward%20propagation.%20Moreover%2C%20standard%20RL%20methods%20require%0Amillions%20of%20real-world%20interactions%2C%20posing%20a%20major%20bottleneck%20for%20practical%0Afine-tuning.%20Although%20prior%20work%20frames%20the%20denoising%20process%20in%20diffusion%0Apolicies%20as%20a%20Markov%20Decision%20Process%20to%20enable%20RL-based%20updates%2C%20its%20strong%0Adependence%20on%20environment%20interaction%20remains%20highly%20inefficient.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20DiWA%2C%20a%20novel%20framework%20that%20leverages%20a%20world%20model%20for%0Afine-tuning%20diffusion-based%20robotic%20skills%20entirely%20offline%20with%20reinforcement%0Alearning.%20Unlike%20model-free%20approaches%20that%20require%20millions%20of%20environment%0Ainteractions%20to%20fine-tune%20a%20repertoire%20of%20robot%20skills%2C%20DiWA%20achieves%20effective%0Aadaptation%20using%20a%20world%20model%20trained%20once%20on%20a%20few%20hundred%20thousand%20offline%0Aplay%20interactions.%20This%20results%20in%20dramatically%20improved%20sample%20efficiency%2C%0Amaking%20the%20approach%20significantly%20more%20practical%20and%20safer%20for%20real-world%20robot%0Alearning.%20On%20the%20challenging%20CALVIN%20benchmark%2C%20DiWA%20improves%20performance%20across%0Aeight%20tasks%20using%20only%20offline%20adaptation%2C%20while%20requiring%20orders%20of%20magnitude%0Afewer%20physical%20interactions%20than%20model-free%20baselines.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20demonstration%20of%20fine-tuning%20diffusion%20policies%20for%20real-world%0Arobotic%20skills%20using%20an%20offline%20world%20model.%20We%20make%20the%20code%20publicly%0Aavailable%20at%20https%3A//diwa.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03645v1&entry.124074799=Read"},
{"title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided\n  by LLM-Informed Evolutionary Monte Carlo Tree Search", "author": "He Wang and Liang Zeng", "abstract": "  Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.\n", "link": "http://arxiv.org/abs/2508.03661v1", "date": "2025-08-05", "relevancy": 0.9425, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Algorithmic%20Discovery%20for%20Gravitational-Wave%20Detection%20Guided%0A%20%20by%20LLM-Informed%20Evolutionary%20Monte%20Carlo%20Tree%20Search&body=Title%3A%20Automated%20Algorithmic%20Discovery%20for%20Gravitational-Wave%20Detection%20Guided%0A%20%20by%20LLM-Informed%20Evolutionary%20Monte%20Carlo%20Tree%20Search%0AAuthor%3A%20He%20Wang%20and%20Liang%20Zeng%0AAbstract%3A%20%20%20Computational%20scientific%20discovery%20increasingly%20relies%20on%20algorithms%20to%0Aprocess%20complex%20data%20and%20identify%20meaningful%20patterns%20-%20yet%20faces%20persistent%0Achallenges%20in%20gravitational-wave%20signal%20identification.%20While%20existing%0Aalgorithmic%20approaches%20like%20matched%20filtering%20%28MF%29%20and%20deep%20neural%20networks%0A%28DNNs%29%20have%20achieved%20partial%20success%2C%20their%20limitations%20directly%20stem%20from%0Afundamental%20limitations%3A%20MF%27s%20excessive%20computational%20demands%20arise%20from%20its%0Areliance%20on%20predefined%20theoretical%20waveform%20templates%2C%20while%20DNNs%27%20black-box%0Aarchitectures%20obscure%20decision%20logic%20and%20introduce%20hidden%20biases.%20We%20propose%0AEvolutionary%20Monte%20Carlo%20Tree%20Search%20%28Evo-MCTS%29%2C%20a%20framework%20that%20addresses%0Athese%20limitations%20through%20systematic%20algorithm%20space%20exploration%20guided%20by%0Adomain-aware%20physical%20constraints.%20Our%20approach%20combines%20tree-structured%20search%0Awith%20evolutionary%20optimization%20and%20large%20language%20model%20heuristics%20to%20create%0Ainterpretable%20algorithmic%20solutions.%20Our%20Evo-MCTS%20framework%20demonstrates%0Asubstantial%20improvements%2C%20achieving%20a%2020.2%5C%25%20improvement%20over%20state-of-the-art%0Agravitational%20wave%20detection%20algorithms%20on%20the%20MLGWSC-1%20benchmark%20dataset.%0AHigh-performing%20algorithm%20variants%20consistently%20exceed%20thresholds.%20The%0Aframework%20generates%20human-interpretable%20algorithmic%20pathways%20that%20reveal%0Adistinct%20performance%20patterns.%20Beyond%20performance%20improvements%2C%20our%20framework%0Adiscovers%20novel%20algorithmic%20combinations%2C%20thereby%20establishing%20a%20transferable%0Amethodology%20for%20automated%20algorithmic%20discovery%20across%20computational%20science%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Algorithmic%2520Discovery%2520for%2520Gravitational-Wave%2520Detection%2520Guided%250A%2520%2520by%2520LLM-Informed%2520Evolutionary%2520Monte%2520Carlo%2520Tree%2520Search%26entry.906535625%3DHe%2520Wang%2520and%2520Liang%2520Zeng%26entry.1292438233%3D%2520%2520Computational%2520scientific%2520discovery%2520increasingly%2520relies%2520on%2520algorithms%2520to%250Aprocess%2520complex%2520data%2520and%2520identify%2520meaningful%2520patterns%2520-%2520yet%2520faces%2520persistent%250Achallenges%2520in%2520gravitational-wave%2520signal%2520identification.%2520While%2520existing%250Aalgorithmic%2520approaches%2520like%2520matched%2520filtering%2520%2528MF%2529%2520and%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520have%2520achieved%2520partial%2520success%252C%2520their%2520limitations%2520directly%2520stem%2520from%250Afundamental%2520limitations%253A%2520MF%2527s%2520excessive%2520computational%2520demands%2520arise%2520from%2520its%250Areliance%2520on%2520predefined%2520theoretical%2520waveform%2520templates%252C%2520while%2520DNNs%2527%2520black-box%250Aarchitectures%2520obscure%2520decision%2520logic%2520and%2520introduce%2520hidden%2520biases.%2520We%2520propose%250AEvolutionary%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528Evo-MCTS%2529%252C%2520a%2520framework%2520that%2520addresses%250Athese%2520limitations%2520through%2520systematic%2520algorithm%2520space%2520exploration%2520guided%2520by%250Adomain-aware%2520physical%2520constraints.%2520Our%2520approach%2520combines%2520tree-structured%2520search%250Awith%2520evolutionary%2520optimization%2520and%2520large%2520language%2520model%2520heuristics%2520to%2520create%250Ainterpretable%2520algorithmic%2520solutions.%2520Our%2520Evo-MCTS%2520framework%2520demonstrates%250Asubstantial%2520improvements%252C%2520achieving%2520a%252020.2%255C%2525%2520improvement%2520over%2520state-of-the-art%250Agravitational%2520wave%2520detection%2520algorithms%2520on%2520the%2520MLGWSC-1%2520benchmark%2520dataset.%250AHigh-performing%2520algorithm%2520variants%2520consistently%2520exceed%2520thresholds.%2520The%250Aframework%2520generates%2520human-interpretable%2520algorithmic%2520pathways%2520that%2520reveal%250Adistinct%2520performance%2520patterns.%2520Beyond%2520performance%2520improvements%252C%2520our%2520framework%250Adiscovers%2520novel%2520algorithmic%2520combinations%252C%2520thereby%2520establishing%2520a%2520transferable%250Amethodology%2520for%2520automated%2520algorithmic%2520discovery%2520across%2520computational%2520science%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Algorithmic%20Discovery%20for%20Gravitational-Wave%20Detection%20Guided%0A%20%20by%20LLM-Informed%20Evolutionary%20Monte%20Carlo%20Tree%20Search&entry.906535625=He%20Wang%20and%20Liang%20Zeng&entry.1292438233=%20%20Computational%20scientific%20discovery%20increasingly%20relies%20on%20algorithms%20to%0Aprocess%20complex%20data%20and%20identify%20meaningful%20patterns%20-%20yet%20faces%20persistent%0Achallenges%20in%20gravitational-wave%20signal%20identification.%20While%20existing%0Aalgorithmic%20approaches%20like%20matched%20filtering%20%28MF%29%20and%20deep%20neural%20networks%0A%28DNNs%29%20have%20achieved%20partial%20success%2C%20their%20limitations%20directly%20stem%20from%0Afundamental%20limitations%3A%20MF%27s%20excessive%20computational%20demands%20arise%20from%20its%0Areliance%20on%20predefined%20theoretical%20waveform%20templates%2C%20while%20DNNs%27%20black-box%0Aarchitectures%20obscure%20decision%20logic%20and%20introduce%20hidden%20biases.%20We%20propose%0AEvolutionary%20Monte%20Carlo%20Tree%20Search%20%28Evo-MCTS%29%2C%20a%20framework%20that%20addresses%0Athese%20limitations%20through%20systematic%20algorithm%20space%20exploration%20guided%20by%0Adomain-aware%20physical%20constraints.%20Our%20approach%20combines%20tree-structured%20search%0Awith%20evolutionary%20optimization%20and%20large%20language%20model%20heuristics%20to%20create%0Ainterpretable%20algorithmic%20solutions.%20Our%20Evo-MCTS%20framework%20demonstrates%0Asubstantial%20improvements%2C%20achieving%20a%2020.2%5C%25%20improvement%20over%20state-of-the-art%0Agravitational%20wave%20detection%20algorithms%20on%20the%20MLGWSC-1%20benchmark%20dataset.%0AHigh-performing%20algorithm%20variants%20consistently%20exceed%20thresholds.%20The%0Aframework%20generates%20human-interpretable%20algorithmic%20pathways%20that%20reveal%0Adistinct%20performance%20patterns.%20Beyond%20performance%20improvements%2C%20our%20framework%0Adiscovers%20novel%20algorithmic%20combinations%2C%20thereby%20establishing%20a%20transferable%0Amethodology%20for%20automated%20algorithmic%20discovery%20across%20computational%20science%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03661v1&entry.124074799=Read"},
{"title": "Beyond risk: A proto-framework for assessing the societal impact of AI\n  systems", "author": "Willem Fourie", "abstract": "  In the discourse on AI regulation, 'responsible AI' is the dominant paradigm,\nwith the focus on mitigating the risks related to AI systems. While this focus\nis important and necessary, it has limited use for a systematic consideration\nof AI's societal impact. This paper proposes a proto-framework for assessing\nthe societal impact of AI systems by operationalising the concept of freedom.\nThis proto-framework is intended as a step towards a fully operationalised\nframework to be used in policymaking contexts. By drawing on Kantian philosophy\nand related contemporary interpretations, freedom is developed as the\ncounterpart to the concept of responsibility. Two dimensions of freedom are\ndeveloped in further detail: freedom as capability and freedom as opportunity.\nThese two dimensions of freedom are then applied in a proto-framework that\nsystematically considers AI's impact on society using the Sustainable\nDevelopment Goals. This proto-framework aims to complement current risk-based\napproaches and thereby offers a first step towards operationalising the concept\nof freedom in AI regulation.\n", "link": "http://arxiv.org/abs/2508.03666v1", "date": "2025-08-05", "relevancy": 0.8216, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4201}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4076}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20risk%3A%20A%20proto-framework%20for%20assessing%20the%20societal%20impact%20of%20AI%0A%20%20systems&body=Title%3A%20Beyond%20risk%3A%20A%20proto-framework%20for%20assessing%20the%20societal%20impact%20of%20AI%0A%20%20systems%0AAuthor%3A%20Willem%20Fourie%0AAbstract%3A%20%20%20In%20the%20discourse%20on%20AI%20regulation%2C%20%27responsible%20AI%27%20is%20the%20dominant%20paradigm%2C%0Awith%20the%20focus%20on%20mitigating%20the%20risks%20related%20to%20AI%20systems.%20While%20this%20focus%0Ais%20important%20and%20necessary%2C%20it%20has%20limited%20use%20for%20a%20systematic%20consideration%0Aof%20AI%27s%20societal%20impact.%20This%20paper%20proposes%20a%20proto-framework%20for%20assessing%0Athe%20societal%20impact%20of%20AI%20systems%20by%20operationalising%20the%20concept%20of%20freedom.%0AThis%20proto-framework%20is%20intended%20as%20a%20step%20towards%20a%20fully%20operationalised%0Aframework%20to%20be%20used%20in%20policymaking%20contexts.%20By%20drawing%20on%20Kantian%20philosophy%0Aand%20related%20contemporary%20interpretations%2C%20freedom%20is%20developed%20as%20the%0Acounterpart%20to%20the%20concept%20of%20responsibility.%20Two%20dimensions%20of%20freedom%20are%0Adeveloped%20in%20further%20detail%3A%20freedom%20as%20capability%20and%20freedom%20as%20opportunity.%0AThese%20two%20dimensions%20of%20freedom%20are%20then%20applied%20in%20a%20proto-framework%20that%0Asystematically%20considers%20AI%27s%20impact%20on%20society%20using%20the%20Sustainable%0ADevelopment%20Goals.%20This%20proto-framework%20aims%20to%20complement%20current%20risk-based%0Aapproaches%20and%20thereby%20offers%20a%20first%20step%20towards%20operationalising%20the%20concept%0Aof%20freedom%20in%20AI%20regulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520risk%253A%2520A%2520proto-framework%2520for%2520assessing%2520the%2520societal%2520impact%2520of%2520AI%250A%2520%2520systems%26entry.906535625%3DWillem%2520Fourie%26entry.1292438233%3D%2520%2520In%2520the%2520discourse%2520on%2520AI%2520regulation%252C%2520%2527responsible%2520AI%2527%2520is%2520the%2520dominant%2520paradigm%252C%250Awith%2520the%2520focus%2520on%2520mitigating%2520the%2520risks%2520related%2520to%2520AI%2520systems.%2520While%2520this%2520focus%250Ais%2520important%2520and%2520necessary%252C%2520it%2520has%2520limited%2520use%2520for%2520a%2520systematic%2520consideration%250Aof%2520AI%2527s%2520societal%2520impact.%2520This%2520paper%2520proposes%2520a%2520proto-framework%2520for%2520assessing%250Athe%2520societal%2520impact%2520of%2520AI%2520systems%2520by%2520operationalising%2520the%2520concept%2520of%2520freedom.%250AThis%2520proto-framework%2520is%2520intended%2520as%2520a%2520step%2520towards%2520a%2520fully%2520operationalised%250Aframework%2520to%2520be%2520used%2520in%2520policymaking%2520contexts.%2520By%2520drawing%2520on%2520Kantian%2520philosophy%250Aand%2520related%2520contemporary%2520interpretations%252C%2520freedom%2520is%2520developed%2520as%2520the%250Acounterpart%2520to%2520the%2520concept%2520of%2520responsibility.%2520Two%2520dimensions%2520of%2520freedom%2520are%250Adeveloped%2520in%2520further%2520detail%253A%2520freedom%2520as%2520capability%2520and%2520freedom%2520as%2520opportunity.%250AThese%2520two%2520dimensions%2520of%2520freedom%2520are%2520then%2520applied%2520in%2520a%2520proto-framework%2520that%250Asystematically%2520considers%2520AI%2527s%2520impact%2520on%2520society%2520using%2520the%2520Sustainable%250ADevelopment%2520Goals.%2520This%2520proto-framework%2520aims%2520to%2520complement%2520current%2520risk-based%250Aapproaches%2520and%2520thereby%2520offers%2520a%2520first%2520step%2520towards%2520operationalising%2520the%2520concept%250Aof%2520freedom%2520in%2520AI%2520regulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20risk%3A%20A%20proto-framework%20for%20assessing%20the%20societal%20impact%20of%20AI%0A%20%20systems&entry.906535625=Willem%20Fourie&entry.1292438233=%20%20In%20the%20discourse%20on%20AI%20regulation%2C%20%27responsible%20AI%27%20is%20the%20dominant%20paradigm%2C%0Awith%20the%20focus%20on%20mitigating%20the%20risks%20related%20to%20AI%20systems.%20While%20this%20focus%0Ais%20important%20and%20necessary%2C%20it%20has%20limited%20use%20for%20a%20systematic%20consideration%0Aof%20AI%27s%20societal%20impact.%20This%20paper%20proposes%20a%20proto-framework%20for%20assessing%0Athe%20societal%20impact%20of%20AI%20systems%20by%20operationalising%20the%20concept%20of%20freedom.%0AThis%20proto-framework%20is%20intended%20as%20a%20step%20towards%20a%20fully%20operationalised%0Aframework%20to%20be%20used%20in%20policymaking%20contexts.%20By%20drawing%20on%20Kantian%20philosophy%0Aand%20related%20contemporary%20interpretations%2C%20freedom%20is%20developed%20as%20the%0Acounterpart%20to%20the%20concept%20of%20responsibility.%20Two%20dimensions%20of%20freedom%20are%0Adeveloped%20in%20further%20detail%3A%20freedom%20as%20capability%20and%20freedom%20as%20opportunity.%0AThese%20two%20dimensions%20of%20freedom%20are%20then%20applied%20in%20a%20proto-framework%20that%0Asystematically%20considers%20AI%27s%20impact%20on%20society%20using%20the%20Sustainable%0ADevelopment%20Goals.%20This%20proto-framework%20aims%20to%20complement%20current%20risk-based%0Aapproaches%20and%20thereby%20offers%20a%20first%20step%20towards%20operationalising%20the%20concept%0Aof%20freedom%20in%20AI%20regulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03666v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


