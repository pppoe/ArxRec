<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241119.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable\n  Single-stage Image-to-3D Generation", "author": "Yuanhao Cai and He Zhang and Kai Zhang and Yixun Liang and Mengwei Ren and Fujun Luan and Qing Liu and Soo Ye Kim and Jianming Zhang and Zhifei Zhang and Yuqian Zhou and Zhe Lin and Alan Yuille", "abstract": "  Existing feed-forward image-to-3D methods mainly rely on 2D multi-view\ndiffusion models that cannot guarantee 3D consistency. These methods easily\ncollapse when changing the prompt view direction and mainly handle\nobject-centric prompt images. In this paper, we propose a novel single-stage 3D\ndiffusion model, DiffusionGS, for object and scene generation from a single\nview. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to\nenforce view consistency and allow the model to generate robustly given prompt\nviews of any directions, beyond object-centric inputs. Plus, to improve the\ncapability and generalization ability of DiffusionGS, we scale up 3D training\ndata by developing a scene-object mixed training strategy. Experiments show\nthat our method enjoys better generation quality (2.20 dB higher in PSNR and\n23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA\nmethods. The user study and text-to-3D applications also reveals the practical\nvalues of our method. Our Project page at\nhttps://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and\ninteractive generation results.\n", "link": "http://arxiv.org/abs/2411.14384v1", "date": "2024-11-21", "relevancy": 3.4472, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6901}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6901}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Baking%20Gaussian%20Splatting%20into%20Diffusion%20Denoiser%20for%20Fast%20and%20Scalable%0A%20%20Single-stage%20Image-to-3D%20Generation&body=Title%3A%20Baking%20Gaussian%20Splatting%20into%20Diffusion%20Denoiser%20for%20Fast%20and%20Scalable%0A%20%20Single-stage%20Image-to-3D%20Generation%0AAuthor%3A%20Yuanhao%20Cai%20and%20He%20Zhang%20and%20Kai%20Zhang%20and%20Yixun%20Liang%20and%20Mengwei%20Ren%20and%20Fujun%20Luan%20and%20Qing%20Liu%20and%20Soo%20Ye%20Kim%20and%20Jianming%20Zhang%20and%20Zhifei%20Zhang%20and%20Yuqian%20Zhou%20and%20Zhe%20Lin%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20Existing%20feed-forward%20image-to-3D%20methods%20mainly%20rely%20on%202D%20multi-view%0Adiffusion%20models%20that%20cannot%20guarantee%203D%20consistency.%20These%20methods%20easily%0Acollapse%20when%20changing%20the%20prompt%20view%20direction%20and%20mainly%20handle%0Aobject-centric%20prompt%20images.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20single-stage%203D%0Adiffusion%20model%2C%20DiffusionGS%2C%20for%20object%20and%20scene%20generation%20from%20a%20single%0Aview.%20DiffusionGS%20directly%20outputs%203D%20Gaussian%20point%20clouds%20at%20each%20timestep%20to%0Aenforce%20view%20consistency%20and%20allow%20the%20model%20to%20generate%20robustly%20given%20prompt%0Aviews%20of%20any%20directions%2C%20beyond%20object-centric%20inputs.%20Plus%2C%20to%20improve%20the%0Acapability%20and%20generalization%20ability%20of%20DiffusionGS%2C%20we%20scale%20up%203D%20training%0Adata%20by%20developing%20a%20scene-object%20mixed%20training%20strategy.%20Experiments%20show%0Athat%20our%20method%20enjoys%20better%20generation%20quality%20%282.20%20dB%20higher%20in%20PSNR%20and%0A23.25%20lower%20in%20FID%29%20and%20over%205x%20faster%20speed%20%28~6s%20on%20an%20A100%20GPU%29%20than%20SOTA%0Amethods.%20The%20user%20study%20and%20text-to-3D%20applications%20also%20reveals%20the%20practical%0Avalues%20of%20our%20method.%20Our%20Project%20page%20at%0Ahttps%3A//caiyuanhao1998.github.io/project/DiffusionGS/%20shows%20the%20video%20and%0Ainteractive%20generation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaking%2520Gaussian%2520Splatting%2520into%2520Diffusion%2520Denoiser%2520for%2520Fast%2520and%2520Scalable%250A%2520%2520Single-stage%2520Image-to-3D%2520Generation%26entry.906535625%3DYuanhao%2520Cai%2520and%2520He%2520Zhang%2520and%2520Kai%2520Zhang%2520and%2520Yixun%2520Liang%2520and%2520Mengwei%2520Ren%2520and%2520Fujun%2520Luan%2520and%2520Qing%2520Liu%2520and%2520Soo%2520Ye%2520Kim%2520and%2520Jianming%2520Zhang%2520and%2520Zhifei%2520Zhang%2520and%2520Yuqian%2520Zhou%2520and%2520Zhe%2520Lin%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520Existing%2520feed-forward%2520image-to-3D%2520methods%2520mainly%2520rely%2520on%25202D%2520multi-view%250Adiffusion%2520models%2520that%2520cannot%2520guarantee%25203D%2520consistency.%2520These%2520methods%2520easily%250Acollapse%2520when%2520changing%2520the%2520prompt%2520view%2520direction%2520and%2520mainly%2520handle%250Aobject-centric%2520prompt%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520single-stage%25203D%250Adiffusion%2520model%252C%2520DiffusionGS%252C%2520for%2520object%2520and%2520scene%2520generation%2520from%2520a%2520single%250Aview.%2520DiffusionGS%2520directly%2520outputs%25203D%2520Gaussian%2520point%2520clouds%2520at%2520each%2520timestep%2520to%250Aenforce%2520view%2520consistency%2520and%2520allow%2520the%2520model%2520to%2520generate%2520robustly%2520given%2520prompt%250Aviews%2520of%2520any%2520directions%252C%2520beyond%2520object-centric%2520inputs.%2520Plus%252C%2520to%2520improve%2520the%250Acapability%2520and%2520generalization%2520ability%2520of%2520DiffusionGS%252C%2520we%2520scale%2520up%25203D%2520training%250Adata%2520by%2520developing%2520a%2520scene-object%2520mixed%2520training%2520strategy.%2520Experiments%2520show%250Athat%2520our%2520method%2520enjoys%2520better%2520generation%2520quality%2520%25282.20%2520dB%2520higher%2520in%2520PSNR%2520and%250A23.25%2520lower%2520in%2520FID%2529%2520and%2520over%25205x%2520faster%2520speed%2520%2528~6s%2520on%2520an%2520A100%2520GPU%2529%2520than%2520SOTA%250Amethods.%2520The%2520user%2520study%2520and%2520text-to-3D%2520applications%2520also%2520reveals%2520the%2520practical%250Avalues%2520of%2520our%2520method.%2520Our%2520Project%2520page%2520at%250Ahttps%253A//caiyuanhao1998.github.io/project/DiffusionGS/%2520shows%2520the%2520video%2520and%250Ainteractive%2520generation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Baking%20Gaussian%20Splatting%20into%20Diffusion%20Denoiser%20for%20Fast%20and%20Scalable%0A%20%20Single-stage%20Image-to-3D%20Generation&entry.906535625=Yuanhao%20Cai%20and%20He%20Zhang%20and%20Kai%20Zhang%20and%20Yixun%20Liang%20and%20Mengwei%20Ren%20and%20Fujun%20Luan%20and%20Qing%20Liu%20and%20Soo%20Ye%20Kim%20and%20Jianming%20Zhang%20and%20Zhifei%20Zhang%20and%20Yuqian%20Zhou%20and%20Zhe%20Lin%20and%20Alan%20Yuille&entry.1292438233=%20%20Existing%20feed-forward%20image-to-3D%20methods%20mainly%20rely%20on%202D%20multi-view%0Adiffusion%20models%20that%20cannot%20guarantee%203D%20consistency.%20These%20methods%20easily%0Acollapse%20when%20changing%20the%20prompt%20view%20direction%20and%20mainly%20handle%0Aobject-centric%20prompt%20images.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20single-stage%203D%0Adiffusion%20model%2C%20DiffusionGS%2C%20for%20object%20and%20scene%20generation%20from%20a%20single%0Aview.%20DiffusionGS%20directly%20outputs%203D%20Gaussian%20point%20clouds%20at%20each%20timestep%20to%0Aenforce%20view%20consistency%20and%20allow%20the%20model%20to%20generate%20robustly%20given%20prompt%0Aviews%20of%20any%20directions%2C%20beyond%20object-centric%20inputs.%20Plus%2C%20to%20improve%20the%0Acapability%20and%20generalization%20ability%20of%20DiffusionGS%2C%20we%20scale%20up%203D%20training%0Adata%20by%20developing%20a%20scene-object%20mixed%20training%20strategy.%20Experiments%20show%0Athat%20our%20method%20enjoys%20better%20generation%20quality%20%282.20%20dB%20higher%20in%20PSNR%20and%0A23.25%20lower%20in%20FID%29%20and%20over%205x%20faster%20speed%20%28~6s%20on%20an%20A100%20GPU%29%20than%20SOTA%0Amethods.%20The%20user%20study%20and%20text-to-3D%20applications%20also%20reveals%20the%20practical%0Avalues%20of%20our%20method.%20Our%20Project%20page%20at%0Ahttps%3A//caiyuanhao1998.github.io/project/DiffusionGS/%20shows%20the%20video%20and%0Ainteractive%20generation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14384v1&entry.124074799=Read"},
{"title": "RoGs: Large Scale Road Surface Reconstruction with Meshgrid Gaussian", "author": "Zhiheng Feng and Wenhua Wu and Tianchen Deng and Hesheng Wang", "abstract": "  Road surface reconstruction plays a crucial role in autonomous driving, which\ncan be used for road lane perception and autolabeling. Recently, mesh-based\nroad surface reconstruction algorithms have shown promising reconstruction\nresults. However, these mesh-based methods suffer from slow speed and poor\nreconstruction quality. To address these limitations, we propose a novel\nlarge-scale road surface reconstruction approach with meshgrid Gaussian, named\nRoGs. Specifically, we model the road surface by placing Gaussian surfels in\nthe vertices of a uniformly distributed square mesh, where each surfel stores\ncolor, semantic, and geometric information. This square mesh-based layout\ncovers the entire road with fewer Gaussian surfels and reduces the overlap\nbetween Gaussian surfels during training. In addition, because the road surface\nhas no thickness, 2D Gaussian surfel is more consistent with the physical\nreality of the road surface than 3D Gaussian sphere. Then, unlike previous\ninitialization methods that rely on point clouds, we introduce a vehicle\npose-based initialization method to initialize the height and rotation of the\nGaussian surfel. Thanks to this meshgrid Gaussian modeling and pose-based\ninitialization, our method achieves significant speedups while improving\nreconstruction quality. We obtain excellent results in reconstruction of road\nsurfaces in a variety of challenging real-world scenes.\n", "link": "http://arxiv.org/abs/2405.14342v3", "date": "2024-11-21", "relevancy": 3.2755, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6976}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6877}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoGs%3A%20Large%20Scale%20Road%20Surface%20Reconstruction%20with%20Meshgrid%20Gaussian&body=Title%3A%20RoGs%3A%20Large%20Scale%20Road%20Surface%20Reconstruction%20with%20Meshgrid%20Gaussian%0AAuthor%3A%20Zhiheng%20Feng%20and%20Wenhua%20Wu%20and%20Tianchen%20Deng%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Road%20surface%20reconstruction%20plays%20a%20crucial%20role%20in%20autonomous%20driving%2C%20which%0Acan%20be%20used%20for%20road%20lane%20perception%20and%20autolabeling.%20Recently%2C%20mesh-based%0Aroad%20surface%20reconstruction%20algorithms%20have%20shown%20promising%20reconstruction%0Aresults.%20However%2C%20these%20mesh-based%20methods%20suffer%20from%20slow%20speed%20and%20poor%0Areconstruction%20quality.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Alarge-scale%20road%20surface%20reconstruction%20approach%20with%20meshgrid%20Gaussian%2C%20named%0ARoGs.%20Specifically%2C%20we%20model%20the%20road%20surface%20by%20placing%20Gaussian%20surfels%20in%0Athe%20vertices%20of%20a%20uniformly%20distributed%20square%20mesh%2C%20where%20each%20surfel%20stores%0Acolor%2C%20semantic%2C%20and%20geometric%20information.%20This%20square%20mesh-based%20layout%0Acovers%20the%20entire%20road%20with%20fewer%20Gaussian%20surfels%20and%20reduces%20the%20overlap%0Abetween%20Gaussian%20surfels%20during%20training.%20In%20addition%2C%20because%20the%20road%20surface%0Ahas%20no%20thickness%2C%202D%20Gaussian%20surfel%20is%20more%20consistent%20with%20the%20physical%0Areality%20of%20the%20road%20surface%20than%203D%20Gaussian%20sphere.%20Then%2C%20unlike%20previous%0Ainitialization%20methods%20that%20rely%20on%20point%20clouds%2C%20we%20introduce%20a%20vehicle%0Apose-based%20initialization%20method%20to%20initialize%20the%20height%20and%20rotation%20of%20the%0AGaussian%20surfel.%20Thanks%20to%20this%20meshgrid%20Gaussian%20modeling%20and%20pose-based%0Ainitialization%2C%20our%20method%20achieves%20significant%20speedups%20while%20improving%0Areconstruction%20quality.%20We%20obtain%20excellent%20results%20in%20reconstruction%20of%20road%0Asurfaces%20in%20a%20variety%20of%20challenging%20real-world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14342v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoGs%253A%2520Large%2520Scale%2520Road%2520Surface%2520Reconstruction%2520with%2520Meshgrid%2520Gaussian%26entry.906535625%3DZhiheng%2520Feng%2520and%2520Wenhua%2520Wu%2520and%2520Tianchen%2520Deng%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Road%2520surface%2520reconstruction%2520plays%2520a%2520crucial%2520role%2520in%2520autonomous%2520driving%252C%2520which%250Acan%2520be%2520used%2520for%2520road%2520lane%2520perception%2520and%2520autolabeling.%2520Recently%252C%2520mesh-based%250Aroad%2520surface%2520reconstruction%2520algorithms%2520have%2520shown%2520promising%2520reconstruction%250Aresults.%2520However%252C%2520these%2520mesh-based%2520methods%2520suffer%2520from%2520slow%2520speed%2520and%2520poor%250Areconstruction%2520quality.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Alarge-scale%2520road%2520surface%2520reconstruction%2520approach%2520with%2520meshgrid%2520Gaussian%252C%2520named%250ARoGs.%2520Specifically%252C%2520we%2520model%2520the%2520road%2520surface%2520by%2520placing%2520Gaussian%2520surfels%2520in%250Athe%2520vertices%2520of%2520a%2520uniformly%2520distributed%2520square%2520mesh%252C%2520where%2520each%2520surfel%2520stores%250Acolor%252C%2520semantic%252C%2520and%2520geometric%2520information.%2520This%2520square%2520mesh-based%2520layout%250Acovers%2520the%2520entire%2520road%2520with%2520fewer%2520Gaussian%2520surfels%2520and%2520reduces%2520the%2520overlap%250Abetween%2520Gaussian%2520surfels%2520during%2520training.%2520In%2520addition%252C%2520because%2520the%2520road%2520surface%250Ahas%2520no%2520thickness%252C%25202D%2520Gaussian%2520surfel%2520is%2520more%2520consistent%2520with%2520the%2520physical%250Areality%2520of%2520the%2520road%2520surface%2520than%25203D%2520Gaussian%2520sphere.%2520Then%252C%2520unlike%2520previous%250Ainitialization%2520methods%2520that%2520rely%2520on%2520point%2520clouds%252C%2520we%2520introduce%2520a%2520vehicle%250Apose-based%2520initialization%2520method%2520to%2520initialize%2520the%2520height%2520and%2520rotation%2520of%2520the%250AGaussian%2520surfel.%2520Thanks%2520to%2520this%2520meshgrid%2520Gaussian%2520modeling%2520and%2520pose-based%250Ainitialization%252C%2520our%2520method%2520achieves%2520significant%2520speedups%2520while%2520improving%250Areconstruction%2520quality.%2520We%2520obtain%2520excellent%2520results%2520in%2520reconstruction%2520of%2520road%250Asurfaces%2520in%2520a%2520variety%2520of%2520challenging%2520real-world%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14342v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoGs%3A%20Large%20Scale%20Road%20Surface%20Reconstruction%20with%20Meshgrid%20Gaussian&entry.906535625=Zhiheng%20Feng%20and%20Wenhua%20Wu%20and%20Tianchen%20Deng%20and%20Hesheng%20Wang&entry.1292438233=%20%20Road%20surface%20reconstruction%20plays%20a%20crucial%20role%20in%20autonomous%20driving%2C%20which%0Acan%20be%20used%20for%20road%20lane%20perception%20and%20autolabeling.%20Recently%2C%20mesh-based%0Aroad%20surface%20reconstruction%20algorithms%20have%20shown%20promising%20reconstruction%0Aresults.%20However%2C%20these%20mesh-based%20methods%20suffer%20from%20slow%20speed%20and%20poor%0Areconstruction%20quality.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Alarge-scale%20road%20surface%20reconstruction%20approach%20with%20meshgrid%20Gaussian%2C%20named%0ARoGs.%20Specifically%2C%20we%20model%20the%20road%20surface%20by%20placing%20Gaussian%20surfels%20in%0Athe%20vertices%20of%20a%20uniformly%20distributed%20square%20mesh%2C%20where%20each%20surfel%20stores%0Acolor%2C%20semantic%2C%20and%20geometric%20information.%20This%20square%20mesh-based%20layout%0Acovers%20the%20entire%20road%20with%20fewer%20Gaussian%20surfels%20and%20reduces%20the%20overlap%0Abetween%20Gaussian%20surfels%20during%20training.%20In%20addition%2C%20because%20the%20road%20surface%0Ahas%20no%20thickness%2C%202D%20Gaussian%20surfel%20is%20more%20consistent%20with%20the%20physical%0Areality%20of%20the%20road%20surface%20than%203D%20Gaussian%20sphere.%20Then%2C%20unlike%20previous%0Ainitialization%20methods%20that%20rely%20on%20point%20clouds%2C%20we%20introduce%20a%20vehicle%0Apose-based%20initialization%20method%20to%20initialize%20the%20height%20and%20rotation%20of%20the%0AGaussian%20surfel.%20Thanks%20to%20this%20meshgrid%20Gaussian%20modeling%20and%20pose-based%0Ainitialization%2C%20our%20method%20achieves%20significant%20speedups%20while%20improving%0Areconstruction%20quality.%20We%20obtain%20excellent%20results%20in%20reconstruction%20of%20road%0Asurfaces%20in%20a%20variety%20of%20challenging%20real-world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14342v3&entry.124074799=Read"},
{"title": "SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching", "author": "Arjun P S and Andrew Melnik and Gora Chand Nandi", "abstract": "  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n", "link": "http://arxiv.org/abs/2411.14322v1", "date": "2024-11-21", "relevancy": 3.2054, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6665}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.65}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatR%20%3A%20Experience%20Goal%20Visual%20Rearrangement%20with%203D%20Gaussian%20Splatting%0A%20%20and%20Dense%20Feature%20Matching&body=Title%3A%20SplatR%20%3A%20Experience%20Goal%20Visual%20Rearrangement%20with%203D%20Gaussian%20Splatting%0A%20%20and%20Dense%20Feature%20Matching%0AAuthor%3A%20Arjun%20P%20S%20and%20Andrew%20Melnik%20and%20Gora%20Chand%20Nandi%0AAbstract%3A%20%20%20Experience%20Goal%20Visual%20Rearrangement%20task%20stands%20as%20a%20foundational%20challenge%0Awithin%20Embodied%20AI%2C%20requiring%20an%20agent%20to%20construct%20a%20robust%20world%20model%20that%0Aaccurately%20captures%20the%20goal%20state.%20The%20agent%20uses%20this%20world%20model%20to%20restore%0Aa%20shuffled%20scene%20to%20its%20original%20configuration%2C%20making%20an%20accurate%0Arepresentation%20of%20the%20world%20essential%20for%20successfully%20completing%20the%20task.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20framework%20that%20leverages%20on%203D%20Gaussian%20Splatting%0Aas%20a%203D%20scene%20representation%20for%20experience%20goal%20visual%20rearrangement%20task.%0ARecent%20advances%20in%20volumetric%20scene%20representation%20like%203D%20Gaussian%20Splatting%2C%0Aoffer%20fast%20rendering%20of%20high%20quality%20and%20photo-realistic%20novel%20views.%20Our%0Aapproach%20enables%20the%20agent%20to%20have%20consistent%20views%20of%20the%20current%20and%20the%20goal%0Asetting%20of%20the%20rearrangement%20task%2C%20which%20enables%20the%20agent%20to%20directly%20compare%0Athe%20goal%20state%20and%20the%20shuffled%20state%20of%20the%20world%20in%20image%20space.%20To%20compare%0Athese%20views%2C%20we%20propose%20to%20use%20a%20dense%20feature%20matching%20method%20with%20visual%0Afeatures%20extracted%20from%20a%20foundation%20model%2C%20leveraging%20its%20advantages%20of%20a%20more%0Auniversal%20feature%20representation%2C%20which%20facilitates%20robustness%2C%20and%0Ageneralization.%20We%20validate%20our%20approach%20on%20the%20AI2-THOR%20rearrangement%0Achallenge%20benchmark%20and%20demonstrate%20improvements%20over%20the%20current%20state%20of%20the%0Aart%20methods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatR%2520%253A%2520Experience%2520Goal%2520Visual%2520Rearrangement%2520with%25203D%2520Gaussian%2520Splatting%250A%2520%2520and%2520Dense%2520Feature%2520Matching%26entry.906535625%3DArjun%2520P%2520S%2520and%2520Andrew%2520Melnik%2520and%2520Gora%2520Chand%2520Nandi%26entry.1292438233%3D%2520%2520Experience%2520Goal%2520Visual%2520Rearrangement%2520task%2520stands%2520as%2520a%2520foundational%2520challenge%250Awithin%2520Embodied%2520AI%252C%2520requiring%2520an%2520agent%2520to%2520construct%2520a%2520robust%2520world%2520model%2520that%250Aaccurately%2520captures%2520the%2520goal%2520state.%2520The%2520agent%2520uses%2520this%2520world%2520model%2520to%2520restore%250Aa%2520shuffled%2520scene%2520to%2520its%2520original%2520configuration%252C%2520making%2520an%2520accurate%250Arepresentation%2520of%2520the%2520world%2520essential%2520for%2520successfully%2520completing%2520the%2520task.%2520In%250Athis%2520work%252C%2520we%2520present%2520a%2520novel%2520framework%2520that%2520leverages%2520on%25203D%2520Gaussian%2520Splatting%250Aas%2520a%25203D%2520scene%2520representation%2520for%2520experience%2520goal%2520visual%2520rearrangement%2520task.%250ARecent%2520advances%2520in%2520volumetric%2520scene%2520representation%2520like%25203D%2520Gaussian%2520Splatting%252C%250Aoffer%2520fast%2520rendering%2520of%2520high%2520quality%2520and%2520photo-realistic%2520novel%2520views.%2520Our%250Aapproach%2520enables%2520the%2520agent%2520to%2520have%2520consistent%2520views%2520of%2520the%2520current%2520and%2520the%2520goal%250Asetting%2520of%2520the%2520rearrangement%2520task%252C%2520which%2520enables%2520the%2520agent%2520to%2520directly%2520compare%250Athe%2520goal%2520state%2520and%2520the%2520shuffled%2520state%2520of%2520the%2520world%2520in%2520image%2520space.%2520To%2520compare%250Athese%2520views%252C%2520we%2520propose%2520to%2520use%2520a%2520dense%2520feature%2520matching%2520method%2520with%2520visual%250Afeatures%2520extracted%2520from%2520a%2520foundation%2520model%252C%2520leveraging%2520its%2520advantages%2520of%2520a%2520more%250Auniversal%2520feature%2520representation%252C%2520which%2520facilitates%2520robustness%252C%2520and%250Ageneralization.%2520We%2520validate%2520our%2520approach%2520on%2520the%2520AI2-THOR%2520rearrangement%250Achallenge%2520benchmark%2520and%2520demonstrate%2520improvements%2520over%2520the%2520current%2520state%2520of%2520the%250Aart%2520methods%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatR%20%3A%20Experience%20Goal%20Visual%20Rearrangement%20with%203D%20Gaussian%20Splatting%0A%20%20and%20Dense%20Feature%20Matching&entry.906535625=Arjun%20P%20S%20and%20Andrew%20Melnik%20and%20Gora%20Chand%20Nandi&entry.1292438233=%20%20Experience%20Goal%20Visual%20Rearrangement%20task%20stands%20as%20a%20foundational%20challenge%0Awithin%20Embodied%20AI%2C%20requiring%20an%20agent%20to%20construct%20a%20robust%20world%20model%20that%0Aaccurately%20captures%20the%20goal%20state.%20The%20agent%20uses%20this%20world%20model%20to%20restore%0Aa%20shuffled%20scene%20to%20its%20original%20configuration%2C%20making%20an%20accurate%0Arepresentation%20of%20the%20world%20essential%20for%20successfully%20completing%20the%20task.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20framework%20that%20leverages%20on%203D%20Gaussian%20Splatting%0Aas%20a%203D%20scene%20representation%20for%20experience%20goal%20visual%20rearrangement%20task.%0ARecent%20advances%20in%20volumetric%20scene%20representation%20like%203D%20Gaussian%20Splatting%2C%0Aoffer%20fast%20rendering%20of%20high%20quality%20and%20photo-realistic%20novel%20views.%20Our%0Aapproach%20enables%20the%20agent%20to%20have%20consistent%20views%20of%20the%20current%20and%20the%20goal%0Asetting%20of%20the%20rearrangement%20task%2C%20which%20enables%20the%20agent%20to%20directly%20compare%0Athe%20goal%20state%20and%20the%20shuffled%20state%20of%20the%20world%20in%20image%20space.%20To%20compare%0Athese%20views%2C%20we%20propose%20to%20use%20a%20dense%20feature%20matching%20method%20with%20visual%0Afeatures%20extracted%20from%20a%20foundation%20model%2C%20leveraging%20its%20advantages%20of%20a%20more%0Auniversal%20feature%20representation%2C%20which%20facilitates%20robustness%2C%20and%0Ageneralization.%20We%20validate%20our%20approach%20on%20the%20AI2-THOR%20rearrangement%0Achallenge%20benchmark%20and%20demonstrate%20improvements%20over%20the%20current%20state%20of%20the%0Aart%20methods%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14322v1&entry.124074799=Read"},
{"title": "DINO-X: A Unified Vision Model for Open-World Object Detection and\n  Understanding", "author": "Tianhe Ren and Yihao Chen and Qing Jiang and Zhaoyang Zeng and Yuda Xiong and Wenlong Liu and Zhengyu Ma and Junyi Shen and Yuan Gao and Xiaoke Jiang and Xingyu Chen and Zhuheng Song and Yuhong Zhang and Hongjie Huang and Han Gao and Shilong Liu and Hao Zhang and Feng Li and Kent Yu and Lei Zhang", "abstract": "  In this paper, we introduce DINO-X, which is a unified object-centric vision\nmodel developed by IDEA Research with the best open-world object detection\nperformance to date. DINO-X employs the same Transformer-based encoder-decoder\narchitecture as Grounding DINO 1.5 to pursue an object-level representation for\nopen-world object understanding. To make long-tailed object detection easy,\nDINO-X extends its input options to support text prompt, visual prompt, and\ncustomized prompt. With such flexible prompt options, we develop a universal\nobject prompt to support prompt-free open-world detection, making it possible\nto detect anything in an image without requiring users to provide any prompt.\nTo enhance the model's core grounding capability, we have constructed a\nlarge-scale dataset with over 100 million high-quality grounding samples,\nreferred to as Grounding-100M, for advancing the model's open-vocabulary\ndetection performance. Pre-training on such a large-scale grounding dataset\nleads to a foundational object-level representation, which enables DINO-X to\nintegrate multiple perception heads to simultaneously support multiple object\nperception and understanding tasks, including detection, segmentation, pose\nestimation, object captioning, object-based QA, etc. Experimental results\ndemonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro\nmodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and\nLVIS-val zero-shot object detection benchmarks, respectively. Notably, it\nscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val\nbenchmarks, both improving the previous SOTA performance by 5.8 AP. Such a\nresult underscores its significantly improved capacity for recognizing\nlong-tailed objects.\n", "link": "http://arxiv.org/abs/2411.14347v1", "date": "2024-11-21", "relevancy": 3.1931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6663}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding&body=Title%3A%20DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding%0AAuthor%3A%20Tianhe%20Ren%20and%20Yihao%20Chen%20and%20Qing%20Jiang%20and%20Zhaoyang%20Zeng%20and%20Yuda%20Xiong%20and%20Wenlong%20Liu%20and%20Zhengyu%20Ma%20and%20Junyi%20Shen%20and%20Yuan%20Gao%20and%20Xiaoke%20Jiang%20and%20Xingyu%20Chen%20and%20Zhuheng%20Song%20and%20Yuhong%20Zhang%20and%20Hongjie%20Huang%20and%20Han%20Gao%20and%20Shilong%20Liu%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Kent%20Yu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20DINO-X%2C%20which%20is%20a%20unified%20object-centric%20vision%0Amodel%20developed%20by%20IDEA%20Research%20with%20the%20best%20open-world%20object%20detection%0Aperformance%20to%20date.%20DINO-X%20employs%20the%20same%20Transformer-based%20encoder-decoder%0Aarchitecture%20as%20Grounding%20DINO%201.5%20to%20pursue%20an%20object-level%20representation%20for%0Aopen-world%20object%20understanding.%20To%20make%20long-tailed%20object%20detection%20easy%2C%0ADINO-X%20extends%20its%20input%20options%20to%20support%20text%20prompt%2C%20visual%20prompt%2C%20and%0Acustomized%20prompt.%20With%20such%20flexible%20prompt%20options%2C%20we%20develop%20a%20universal%0Aobject%20prompt%20to%20support%20prompt-free%20open-world%20detection%2C%20making%20it%20possible%0Ato%20detect%20anything%20in%20an%20image%20without%20requiring%20users%20to%20provide%20any%20prompt.%0ATo%20enhance%20the%20model%27s%20core%20grounding%20capability%2C%20we%20have%20constructed%20a%0Alarge-scale%20dataset%20with%20over%20100%20million%20high-quality%20grounding%20samples%2C%0Areferred%20to%20as%20Grounding-100M%2C%20for%20advancing%20the%20model%27s%20open-vocabulary%0Adetection%20performance.%20Pre-training%20on%20such%20a%20large-scale%20grounding%20dataset%0Aleads%20to%20a%20foundational%20object-level%20representation%2C%20which%20enables%20DINO-X%20to%0Aintegrate%20multiple%20perception%20heads%20to%20simultaneously%20support%20multiple%20object%0Aperception%20and%20understanding%20tasks%2C%20including%20detection%2C%20segmentation%2C%20pose%0Aestimation%2C%20object%20captioning%2C%20object-based%20QA%2C%20etc.%20Experimental%20results%0Ademonstrate%20the%20superior%20performance%20of%20DINO-X.%20Specifically%2C%20the%20DINO-X%20Pro%0Amodel%20achieves%2056.0%20AP%2C%2059.8%20AP%2C%20and%2052.4%20AP%20on%20the%20COCO%2C%20LVIS-minival%2C%20and%0ALVIS-val%20zero-shot%20object%20detection%20benchmarks%2C%20respectively.%20Notably%2C%20it%0Ascores%2063.3%20AP%20and%2056.5%20AP%20on%20the%20rare%20classes%20of%20LVIS-minival%20and%20LVIS-val%0Abenchmarks%2C%20both%20improving%20the%20previous%20SOTA%20performance%20by%205.8%20AP.%20Such%20a%0Aresult%20underscores%20its%20significantly%20improved%20capacity%20for%20recognizing%0Along-tailed%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-X%253A%2520A%2520Unified%2520Vision%2520Model%2520for%2520Open-World%2520Object%2520Detection%2520and%250A%2520%2520Understanding%26entry.906535625%3DTianhe%2520Ren%2520and%2520Yihao%2520Chen%2520and%2520Qing%2520Jiang%2520and%2520Zhaoyang%2520Zeng%2520and%2520Yuda%2520Xiong%2520and%2520Wenlong%2520Liu%2520and%2520Zhengyu%2520Ma%2520and%2520Junyi%2520Shen%2520and%2520Yuan%2520Gao%2520and%2520Xiaoke%2520Jiang%2520and%2520Xingyu%2520Chen%2520and%2520Zhuheng%2520Song%2520and%2520Yuhong%2520Zhang%2520and%2520Hongjie%2520Huang%2520and%2520Han%2520Gao%2520and%2520Shilong%2520Liu%2520and%2520Hao%2520Zhang%2520and%2520Feng%2520Li%2520and%2520Kent%2520Yu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DINO-X%252C%2520which%2520is%2520a%2520unified%2520object-centric%2520vision%250Amodel%2520developed%2520by%2520IDEA%2520Research%2520with%2520the%2520best%2520open-world%2520object%2520detection%250Aperformance%2520to%2520date.%2520DINO-X%2520employs%2520the%2520same%2520Transformer-based%2520encoder-decoder%250Aarchitecture%2520as%2520Grounding%2520DINO%25201.5%2520to%2520pursue%2520an%2520object-level%2520representation%2520for%250Aopen-world%2520object%2520understanding.%2520To%2520make%2520long-tailed%2520object%2520detection%2520easy%252C%250ADINO-X%2520extends%2520its%2520input%2520options%2520to%2520support%2520text%2520prompt%252C%2520visual%2520prompt%252C%2520and%250Acustomized%2520prompt.%2520With%2520such%2520flexible%2520prompt%2520options%252C%2520we%2520develop%2520a%2520universal%250Aobject%2520prompt%2520to%2520support%2520prompt-free%2520open-world%2520detection%252C%2520making%2520it%2520possible%250Ato%2520detect%2520anything%2520in%2520an%2520image%2520without%2520requiring%2520users%2520to%2520provide%2520any%2520prompt.%250ATo%2520enhance%2520the%2520model%2527s%2520core%2520grounding%2520capability%252C%2520we%2520have%2520constructed%2520a%250Alarge-scale%2520dataset%2520with%2520over%2520100%2520million%2520high-quality%2520grounding%2520samples%252C%250Areferred%2520to%2520as%2520Grounding-100M%252C%2520for%2520advancing%2520the%2520model%2527s%2520open-vocabulary%250Adetection%2520performance.%2520Pre-training%2520on%2520such%2520a%2520large-scale%2520grounding%2520dataset%250Aleads%2520to%2520a%2520foundational%2520object-level%2520representation%252C%2520which%2520enables%2520DINO-X%2520to%250Aintegrate%2520multiple%2520perception%2520heads%2520to%2520simultaneously%2520support%2520multiple%2520object%250Aperception%2520and%2520understanding%2520tasks%252C%2520including%2520detection%252C%2520segmentation%252C%2520pose%250Aestimation%252C%2520object%2520captioning%252C%2520object-based%2520QA%252C%2520etc.%2520Experimental%2520results%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520DINO-X.%2520Specifically%252C%2520the%2520DINO-X%2520Pro%250Amodel%2520achieves%252056.0%2520AP%252C%252059.8%2520AP%252C%2520and%252052.4%2520AP%2520on%2520the%2520COCO%252C%2520LVIS-minival%252C%2520and%250ALVIS-val%2520zero-shot%2520object%2520detection%2520benchmarks%252C%2520respectively.%2520Notably%252C%2520it%250Ascores%252063.3%2520AP%2520and%252056.5%2520AP%2520on%2520the%2520rare%2520classes%2520of%2520LVIS-minival%2520and%2520LVIS-val%250Abenchmarks%252C%2520both%2520improving%2520the%2520previous%2520SOTA%2520performance%2520by%25205.8%2520AP.%2520Such%2520a%250Aresult%2520underscores%2520its%2520significantly%2520improved%2520capacity%2520for%2520recognizing%250Along-tailed%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding&entry.906535625=Tianhe%20Ren%20and%20Yihao%20Chen%20and%20Qing%20Jiang%20and%20Zhaoyang%20Zeng%20and%20Yuda%20Xiong%20and%20Wenlong%20Liu%20and%20Zhengyu%20Ma%20and%20Junyi%20Shen%20and%20Yuan%20Gao%20and%20Xiaoke%20Jiang%20and%20Xingyu%20Chen%20and%20Zhuheng%20Song%20and%20Yuhong%20Zhang%20and%20Hongjie%20Huang%20and%20Han%20Gao%20and%20Shilong%20Liu%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Kent%20Yu%20and%20Lei%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20DINO-X%2C%20which%20is%20a%20unified%20object-centric%20vision%0Amodel%20developed%20by%20IDEA%20Research%20with%20the%20best%20open-world%20object%20detection%0Aperformance%20to%20date.%20DINO-X%20employs%20the%20same%20Transformer-based%20encoder-decoder%0Aarchitecture%20as%20Grounding%20DINO%201.5%20to%20pursue%20an%20object-level%20representation%20for%0Aopen-world%20object%20understanding.%20To%20make%20long-tailed%20object%20detection%20easy%2C%0ADINO-X%20extends%20its%20input%20options%20to%20support%20text%20prompt%2C%20visual%20prompt%2C%20and%0Acustomized%20prompt.%20With%20such%20flexible%20prompt%20options%2C%20we%20develop%20a%20universal%0Aobject%20prompt%20to%20support%20prompt-free%20open-world%20detection%2C%20making%20it%20possible%0Ato%20detect%20anything%20in%20an%20image%20without%20requiring%20users%20to%20provide%20any%20prompt.%0ATo%20enhance%20the%20model%27s%20core%20grounding%20capability%2C%20we%20have%20constructed%20a%0Alarge-scale%20dataset%20with%20over%20100%20million%20high-quality%20grounding%20samples%2C%0Areferred%20to%20as%20Grounding-100M%2C%20for%20advancing%20the%20model%27s%20open-vocabulary%0Adetection%20performance.%20Pre-training%20on%20such%20a%20large-scale%20grounding%20dataset%0Aleads%20to%20a%20foundational%20object-level%20representation%2C%20which%20enables%20DINO-X%20to%0Aintegrate%20multiple%20perception%20heads%20to%20simultaneously%20support%20multiple%20object%0Aperception%20and%20understanding%20tasks%2C%20including%20detection%2C%20segmentation%2C%20pose%0Aestimation%2C%20object%20captioning%2C%20object-based%20QA%2C%20etc.%20Experimental%20results%0Ademonstrate%20the%20superior%20performance%20of%20DINO-X.%20Specifically%2C%20the%20DINO-X%20Pro%0Amodel%20achieves%2056.0%20AP%2C%2059.8%20AP%2C%20and%2052.4%20AP%20on%20the%20COCO%2C%20LVIS-minival%2C%20and%0ALVIS-val%20zero-shot%20object%20detection%20benchmarks%2C%20respectively.%20Notably%2C%20it%0Ascores%2063.3%20AP%20and%2056.5%20AP%20on%20the%20rare%20classes%20of%20LVIS-minival%20and%20LVIS-val%0Abenchmarks%2C%20both%20improving%20the%20previous%20SOTA%20performance%20by%205.8%20AP.%20Such%20a%0Aresult%20underscores%20its%20significantly%20improved%20capacity%20for%20recognizing%0Along-tailed%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14347v1&entry.124074799=Read"},
{"title": "Unleashing the Potential of Multi-modal Foundation Models and Video\n  Diffusion for 4D Dynamic Physical Scene Simulation", "author": "Zhuoman Liu and Weicai Ye and Yan Luximon and Pengfei Wan and Di Zhang", "abstract": "  Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce a novel approach\nthat leverages multi-modal foundation models and video diffusion to achieve\nenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to\nidentify material types and initialize material parameters through image\nqueries, while simultaneously inferring 3D Gaussian splats for detailed scene\nrepresentation. We further refine these material parameters using video\ndiffusion with a differentiable Material Point Method (MPM) and optical flow\nguidance rather than render loss or Score Distillation Sampling (SDS) loss.\nThis integrated framework enables accurate prediction and realistic simulation\nof dynamic interactions in real-world scenarios, advancing both accuracy and\nflexibility in physics-based simulations.\n", "link": "http://arxiv.org/abs/2411.14423v1", "date": "2024-11-21", "relevancy": 3.1885, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6391}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%20Video%0A%20%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation&body=Title%3A%20Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%20Video%0A%20%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation%0AAuthor%3A%20Zhuoman%20Liu%20and%20Weicai%20Ye%20and%20Yan%20Luximon%20and%20Pengfei%20Wan%20and%20Di%20Zhang%0AAbstract%3A%20%20%20Realistic%20simulation%20of%20dynamic%20scenes%20requires%20accurately%20capturing%20diverse%0Amaterial%20properties%20and%20modeling%20complex%20object%20interactions%20grounded%20in%0Aphysical%20principles.%20However%2C%20existing%20methods%20are%20constrained%20to%20basic%0Amaterial%20types%20with%20limited%20predictable%20parameters%2C%20making%20them%20insufficient%20to%0Arepresent%20the%20complexity%20of%20real-world%20materials.%20We%20introduce%20a%20novel%20approach%0Athat%20leverages%20multi-modal%20foundation%20models%20and%20video%20diffusion%20to%20achieve%0Aenhanced%204D%20dynamic%20scene%20simulation.%20Our%20method%20utilizes%20multi-modal%20models%20to%0Aidentify%20material%20types%20and%20initialize%20material%20parameters%20through%20image%0Aqueries%2C%20while%20simultaneously%20inferring%203D%20Gaussian%20splats%20for%20detailed%20scene%0Arepresentation.%20We%20further%20refine%20these%20material%20parameters%20using%20video%0Adiffusion%20with%20a%20differentiable%20Material%20Point%20Method%20%28MPM%29%20and%20optical%20flow%0Aguidance%20rather%20than%20render%20loss%20or%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%0AThis%20integrated%20framework%20enables%20accurate%20prediction%20and%20realistic%20simulation%0Aof%20dynamic%20interactions%20in%20real-world%20scenarios%2C%20advancing%20both%20accuracy%20and%0Aflexibility%20in%20physics-based%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Potential%2520of%2520Multi-modal%2520Foundation%2520Models%2520and%2520Video%250A%2520%2520Diffusion%2520for%25204D%2520Dynamic%2520Physical%2520Scene%2520Simulation%26entry.906535625%3DZhuoman%2520Liu%2520and%2520Weicai%2520Ye%2520and%2520Yan%2520Luximon%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%26entry.1292438233%3D%2520%2520Realistic%2520simulation%2520of%2520dynamic%2520scenes%2520requires%2520accurately%2520capturing%2520diverse%250Amaterial%2520properties%2520and%2520modeling%2520complex%2520object%2520interactions%2520grounded%2520in%250Aphysical%2520principles.%2520However%252C%2520existing%2520methods%2520are%2520constrained%2520to%2520basic%250Amaterial%2520types%2520with%2520limited%2520predictable%2520parameters%252C%2520making%2520them%2520insufficient%2520to%250Arepresent%2520the%2520complexity%2520of%2520real-world%2520materials.%2520We%2520introduce%2520a%2520novel%2520approach%250Athat%2520leverages%2520multi-modal%2520foundation%2520models%2520and%2520video%2520diffusion%2520to%2520achieve%250Aenhanced%25204D%2520dynamic%2520scene%2520simulation.%2520Our%2520method%2520utilizes%2520multi-modal%2520models%2520to%250Aidentify%2520material%2520types%2520and%2520initialize%2520material%2520parameters%2520through%2520image%250Aqueries%252C%2520while%2520simultaneously%2520inferring%25203D%2520Gaussian%2520splats%2520for%2520detailed%2520scene%250Arepresentation.%2520We%2520further%2520refine%2520these%2520material%2520parameters%2520using%2520video%250Adiffusion%2520with%2520a%2520differentiable%2520Material%2520Point%2520Method%2520%2528MPM%2529%2520and%2520optical%2520flow%250Aguidance%2520rather%2520than%2520render%2520loss%2520or%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520loss.%250AThis%2520integrated%2520framework%2520enables%2520accurate%2520prediction%2520and%2520realistic%2520simulation%250Aof%2520dynamic%2520interactions%2520in%2520real-world%2520scenarios%252C%2520advancing%2520both%2520accuracy%2520and%250Aflexibility%2520in%2520physics-based%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%20Video%0A%20%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation&entry.906535625=Zhuoman%20Liu%20and%20Weicai%20Ye%20and%20Yan%20Luximon%20and%20Pengfei%20Wan%20and%20Di%20Zhang&entry.1292438233=%20%20Realistic%20simulation%20of%20dynamic%20scenes%20requires%20accurately%20capturing%20diverse%0Amaterial%20properties%20and%20modeling%20complex%20object%20interactions%20grounded%20in%0Aphysical%20principles.%20However%2C%20existing%20methods%20are%20constrained%20to%20basic%0Amaterial%20types%20with%20limited%20predictable%20parameters%2C%20making%20them%20insufficient%20to%0Arepresent%20the%20complexity%20of%20real-world%20materials.%20We%20introduce%20a%20novel%20approach%0Athat%20leverages%20multi-modal%20foundation%20models%20and%20video%20diffusion%20to%20achieve%0Aenhanced%204D%20dynamic%20scene%20simulation.%20Our%20method%20utilizes%20multi-modal%20models%20to%0Aidentify%20material%20types%20and%20initialize%20material%20parameters%20through%20image%0Aqueries%2C%20while%20simultaneously%20inferring%203D%20Gaussian%20splats%20for%20detailed%20scene%0Arepresentation.%20We%20further%20refine%20these%20material%20parameters%20using%20video%0Adiffusion%20with%20a%20differentiable%20Material%20Point%20Method%20%28MPM%29%20and%20optical%20flow%0Aguidance%20rather%20than%20render%20loss%20or%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%0AThis%20integrated%20framework%20enables%20accurate%20prediction%20and%20realistic%20simulation%0Aof%20dynamic%20interactions%20in%20real-world%20scenarios%2C%20advancing%20both%20accuracy%20and%0Aflexibility%20in%20physics-based%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14423v1&entry.124074799=Read"},
{"title": "Efficient 3D Instance Mapping and Localization with Neural Fields", "author": "George Tang and Krishna Murthy Jatavallabhula and Antonio Torralba", "abstract": "  We tackle the problem of learning an implicit scene representation for 3D\ninstance segmentation from a sequence of posed RGB images. Towards this, we\nintroduce 3DIML, a novel framework that efficiently learns a neural label field\nwhich can render 3D instance segmentation masks from novel viewpoints. Opposed\nto prior art that optimizes a neural field in a self-supervised manner,\nrequiring complicated training procedures and loss function design, 3DIML\nleverages a two-phase process. The first phase, InstanceMap, takes as input 2D\nsegmentation masks of the image sequence generated by a frontend instance\nsegmentation model, and associates corresponding masks across images to 3D\nlabels. These almost 3D-consistent pseudolabel masks are then used in the\nsecond phase, InstanceLift, to supervise the training of a neural label field,\nwhich interpolates regions missed by InstanceMap and resolves ambiguities.\nAdditionally, we introduce InstanceLoc, which enables near realtime\nlocalization of instance masks given a trained neural label field. We evaluate\n3DIML on sequences from the Replica and ScanNet datasets and demonstrate its\neffectiveness under mild assumptions for the image sequences. We achieve a\nlarge practical speedup over existing implicit scene representation methods\nwith comparable quality, showcasing its potential to facilitate faster and more\neffective 3D scene understanding.\n", "link": "http://arxiv.org/abs/2403.19797v4", "date": "2024-11-21", "relevancy": 3.1143, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields&body=Title%3A%20Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields%0AAuthor%3A%20George%20Tang%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Antonio%20Torralba%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20learning%20an%20implicit%20scene%20representation%20for%203D%0Ainstance%20segmentation%20from%20a%20sequence%20of%20posed%20RGB%20images.%20Towards%20this%2C%20we%0Aintroduce%203DIML%2C%20a%20novel%20framework%20that%20efficiently%20learns%20a%20neural%20label%20field%0Awhich%20can%20render%203D%20instance%20segmentation%20masks%20from%20novel%20viewpoints.%20Opposed%0Ato%20prior%20art%20that%20optimizes%20a%20neural%20field%20in%20a%20self-supervised%20manner%2C%0Arequiring%20complicated%20training%20procedures%20and%20loss%20function%20design%2C%203DIML%0Aleverages%20a%20two-phase%20process.%20The%20first%20phase%2C%20InstanceMap%2C%20takes%20as%20input%202D%0Asegmentation%20masks%20of%20the%20image%20sequence%20generated%20by%20a%20frontend%20instance%0Asegmentation%20model%2C%20and%20associates%20corresponding%20masks%20across%20images%20to%203D%0Alabels.%20These%20almost%203D-consistent%20pseudolabel%20masks%20are%20then%20used%20in%20the%0Asecond%20phase%2C%20InstanceLift%2C%20to%20supervise%20the%20training%20of%20a%20neural%20label%20field%2C%0Awhich%20interpolates%20regions%20missed%20by%20InstanceMap%20and%20resolves%20ambiguities.%0AAdditionally%2C%20we%20introduce%20InstanceLoc%2C%20which%20enables%20near%20realtime%0Alocalization%20of%20instance%20masks%20given%20a%20trained%20neural%20label%20field.%20We%20evaluate%0A3DIML%20on%20sequences%20from%20the%20Replica%20and%20ScanNet%20datasets%20and%20demonstrate%20its%0Aeffectiveness%20under%20mild%20assumptions%20for%20the%20image%20sequences.%20We%20achieve%20a%0Alarge%20practical%20speedup%20over%20existing%20implicit%20scene%20representation%20methods%0Awith%20comparable%20quality%2C%20showcasing%20its%20potential%20to%20facilitate%20faster%20and%20more%0Aeffective%203D%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19797v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Instance%2520Mapping%2520and%2520Localization%2520with%2520Neural%2520Fields%26entry.906535625%3DGeorge%2520Tang%2520and%2520Krishna%2520Murthy%2520Jatavallabhula%2520and%2520Antonio%2520Torralba%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520learning%2520an%2520implicit%2520scene%2520representation%2520for%25203D%250Ainstance%2520segmentation%2520from%2520a%2520sequence%2520of%2520posed%2520RGB%2520images.%2520Towards%2520this%252C%2520we%250Aintroduce%25203DIML%252C%2520a%2520novel%2520framework%2520that%2520efficiently%2520learns%2520a%2520neural%2520label%2520field%250Awhich%2520can%2520render%25203D%2520instance%2520segmentation%2520masks%2520from%2520novel%2520viewpoints.%2520Opposed%250Ato%2520prior%2520art%2520that%2520optimizes%2520a%2520neural%2520field%2520in%2520a%2520self-supervised%2520manner%252C%250Arequiring%2520complicated%2520training%2520procedures%2520and%2520loss%2520function%2520design%252C%25203DIML%250Aleverages%2520a%2520two-phase%2520process.%2520The%2520first%2520phase%252C%2520InstanceMap%252C%2520takes%2520as%2520input%25202D%250Asegmentation%2520masks%2520of%2520the%2520image%2520sequence%2520generated%2520by%2520a%2520frontend%2520instance%250Asegmentation%2520model%252C%2520and%2520associates%2520corresponding%2520masks%2520across%2520images%2520to%25203D%250Alabels.%2520These%2520almost%25203D-consistent%2520pseudolabel%2520masks%2520are%2520then%2520used%2520in%2520the%250Asecond%2520phase%252C%2520InstanceLift%252C%2520to%2520supervise%2520the%2520training%2520of%2520a%2520neural%2520label%2520field%252C%250Awhich%2520interpolates%2520regions%2520missed%2520by%2520InstanceMap%2520and%2520resolves%2520ambiguities.%250AAdditionally%252C%2520we%2520introduce%2520InstanceLoc%252C%2520which%2520enables%2520near%2520realtime%250Alocalization%2520of%2520instance%2520masks%2520given%2520a%2520trained%2520neural%2520label%2520field.%2520We%2520evaluate%250A3DIML%2520on%2520sequences%2520from%2520the%2520Replica%2520and%2520ScanNet%2520datasets%2520and%2520demonstrate%2520its%250Aeffectiveness%2520under%2520mild%2520assumptions%2520for%2520the%2520image%2520sequences.%2520We%2520achieve%2520a%250Alarge%2520practical%2520speedup%2520over%2520existing%2520implicit%2520scene%2520representation%2520methods%250Awith%2520comparable%2520quality%252C%2520showcasing%2520its%2520potential%2520to%2520facilitate%2520faster%2520and%2520more%250Aeffective%25203D%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19797v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields&entry.906535625=George%20Tang%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Antonio%20Torralba&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20learning%20an%20implicit%20scene%20representation%20for%203D%0Ainstance%20segmentation%20from%20a%20sequence%20of%20posed%20RGB%20images.%20Towards%20this%2C%20we%0Aintroduce%203DIML%2C%20a%20novel%20framework%20that%20efficiently%20learns%20a%20neural%20label%20field%0Awhich%20can%20render%203D%20instance%20segmentation%20masks%20from%20novel%20viewpoints.%20Opposed%0Ato%20prior%20art%20that%20optimizes%20a%20neural%20field%20in%20a%20self-supervised%20manner%2C%0Arequiring%20complicated%20training%20procedures%20and%20loss%20function%20design%2C%203DIML%0Aleverages%20a%20two-phase%20process.%20The%20first%20phase%2C%20InstanceMap%2C%20takes%20as%20input%202D%0Asegmentation%20masks%20of%20the%20image%20sequence%20generated%20by%20a%20frontend%20instance%0Asegmentation%20model%2C%20and%20associates%20corresponding%20masks%20across%20images%20to%203D%0Alabels.%20These%20almost%203D-consistent%20pseudolabel%20masks%20are%20then%20used%20in%20the%0Asecond%20phase%2C%20InstanceLift%2C%20to%20supervise%20the%20training%20of%20a%20neural%20label%20field%2C%0Awhich%20interpolates%20regions%20missed%20by%20InstanceMap%20and%20resolves%20ambiguities.%0AAdditionally%2C%20we%20introduce%20InstanceLoc%2C%20which%20enables%20near%20realtime%0Alocalization%20of%20instance%20masks%20given%20a%20trained%20neural%20label%20field.%20We%20evaluate%0A3DIML%20on%20sequences%20from%20the%20Replica%20and%20ScanNet%20datasets%20and%20demonstrate%20its%0Aeffectiveness%20under%20mild%20assumptions%20for%20the%20image%20sequences.%20We%20achieve%20a%0Alarge%20practical%20speedup%20over%20existing%20implicit%20scene%20representation%20methods%0Awith%20comparable%20quality%2C%20showcasing%20its%20potential%20to%20facilitate%20faster%20and%20more%0Aeffective%203D%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19797v4&entry.124074799=Read"},
{"title": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model", "author": "Benlin Liu and Yuhao Dong and Yiqin Wang and Zixian Ma and Yansong Tang and Luming Tang and Yongming Rao and Wei-Chiu Ma and Ranjay Krishna", "abstract": "  Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning.\n", "link": "http://arxiv.org/abs/2408.00754v2", "date": "2024-11-21", "relevancy": 3.0626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse%20Correspondences%20Boost%20Spatial-Temporal%20Reasoning%20in%20Multimodal%0A%20%20Language%20Model&body=Title%3A%20Coarse%20Correspondences%20Boost%20Spatial-Temporal%20Reasoning%20in%20Multimodal%0A%20%20Language%20Model%0AAuthor%3A%20Benlin%20Liu%20and%20Yuhao%20Dong%20and%20Yiqin%20Wang%20and%20Zixian%20Ma%20and%20Yansong%20Tang%20and%20Luming%20Tang%20and%20Yongming%20Rao%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20being%20applied%20in%0Areal-world%20environments%2C%20necessitating%20their%20ability%20to%20interpret%203D%20spaces%20and%0Acomprehend%20temporal%20dynamics.%20Current%20methods%20often%20rely%20on%20specialized%0Aarchitectural%20designs%20or%20task-specific%20fine-tuning%20to%20achieve%20this.%20We%0Aintroduce%20Coarse%20Correspondences%2C%20a%20simple%20lightweight%20method%20that%20enhances%0AMLLMs%27%20spatial-temporal%20reasoning%20with%202D%20images%20as%20input%2C%20without%20modifying%0Athe%20architecture%20or%20requiring%20task-specific%20fine-tuning.%20Our%20method%20uses%20a%0Alightweight%20tracking%20model%20to%20identify%20primary%20object%20correspondences%20between%0Aframes%20in%20a%20video%20or%20across%20different%20image%20viewpoints%2C%20and%20then%20conveys%20this%0Ainformation%20to%20MLLMs%20through%20visual%20prompting.%20We%20demonstrate%20that%20this%20simple%0Atraining-free%20approach%20brings%20substantial%20gains%20to%20GPT4-V/O%20consistently%20on%0Afour%20benchmarks%20that%20require%20spatial-temporal%20reasoning%2C%20including%20%2B20.5%5C%25%0Aimprovement%20on%20ScanQA%2C%20%2B9.7%5C%25%20on%20OpenEQA%27s%20episodic%20memory%20subset%2C%20%2B6.0%5C%25%20on%0Athe%20long-form%20video%20benchmark%20EgoSchema%2C%20and%20%2B11%5C%25%20on%20the%20R2R%20navigation%0Abenchmark.%20Additionally%2C%20we%20show%20that%20Coarse%20Correspondences%20can%20also%20enhance%0Aopen-source%20MLLMs%27%20spatial%20reasoning%20%28by%20%2B6.9%5C%25%20on%20ScanQA%29%20when%20applied%20in%20both%0Atraining%20and%20inference%20and%20that%20the%20improvement%20can%20generalize%20to%20unseen%0Adatasets%20such%20as%20SQA3D%20%28%2B3.1%5C%25%29.%20Taken%20together%2C%20we%20show%20that%20Coarse%0ACorrespondences%20effectively%20and%20efficiently%20boosts%20models%27%20performance%20on%0Adownstream%20tasks%20requiring%20spatial-temporal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse%2520Correspondences%2520Boost%2520Spatial-Temporal%2520Reasoning%2520in%2520Multimodal%250A%2520%2520Language%2520Model%26entry.906535625%3DBenlin%2520Liu%2520and%2520Yuhao%2520Dong%2520and%2520Yiqin%2520Wang%2520and%2520Zixian%2520Ma%2520and%2520Yansong%2520Tang%2520and%2520Luming%2520Tang%2520and%2520Yongming%2520Rao%2520and%2520Wei-Chiu%2520Ma%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520models%2520%2528MLLMs%2529%2520are%2520increasingly%2520being%2520applied%2520in%250Areal-world%2520environments%252C%2520necessitating%2520their%2520ability%2520to%2520interpret%25203D%2520spaces%2520and%250Acomprehend%2520temporal%2520dynamics.%2520Current%2520methods%2520often%2520rely%2520on%2520specialized%250Aarchitectural%2520designs%2520or%2520task-specific%2520fine-tuning%2520to%2520achieve%2520this.%2520We%250Aintroduce%2520Coarse%2520Correspondences%252C%2520a%2520simple%2520lightweight%2520method%2520that%2520enhances%250AMLLMs%2527%2520spatial-temporal%2520reasoning%2520with%25202D%2520images%2520as%2520input%252C%2520without%2520modifying%250Athe%2520architecture%2520or%2520requiring%2520task-specific%2520fine-tuning.%2520Our%2520method%2520uses%2520a%250Alightweight%2520tracking%2520model%2520to%2520identify%2520primary%2520object%2520correspondences%2520between%250Aframes%2520in%2520a%2520video%2520or%2520across%2520different%2520image%2520viewpoints%252C%2520and%2520then%2520conveys%2520this%250Ainformation%2520to%2520MLLMs%2520through%2520visual%2520prompting.%2520We%2520demonstrate%2520that%2520this%2520simple%250Atraining-free%2520approach%2520brings%2520substantial%2520gains%2520to%2520GPT4-V/O%2520consistently%2520on%250Afour%2520benchmarks%2520that%2520require%2520spatial-temporal%2520reasoning%252C%2520including%2520%252B20.5%255C%2525%250Aimprovement%2520on%2520ScanQA%252C%2520%252B9.7%255C%2525%2520on%2520OpenEQA%2527s%2520episodic%2520memory%2520subset%252C%2520%252B6.0%255C%2525%2520on%250Athe%2520long-form%2520video%2520benchmark%2520EgoSchema%252C%2520and%2520%252B11%255C%2525%2520on%2520the%2520R2R%2520navigation%250Abenchmark.%2520Additionally%252C%2520we%2520show%2520that%2520Coarse%2520Correspondences%2520can%2520also%2520enhance%250Aopen-source%2520MLLMs%2527%2520spatial%2520reasoning%2520%2528by%2520%252B6.9%255C%2525%2520on%2520ScanQA%2529%2520when%2520applied%2520in%2520both%250Atraining%2520and%2520inference%2520and%2520that%2520the%2520improvement%2520can%2520generalize%2520to%2520unseen%250Adatasets%2520such%2520as%2520SQA3D%2520%2528%252B3.1%255C%2525%2529.%2520Taken%2520together%252C%2520we%2520show%2520that%2520Coarse%250ACorrespondences%2520effectively%2520and%2520efficiently%2520boosts%2520models%2527%2520performance%2520on%250Adownstream%2520tasks%2520requiring%2520spatial-temporal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse%20Correspondences%20Boost%20Spatial-Temporal%20Reasoning%20in%20Multimodal%0A%20%20Language%20Model&entry.906535625=Benlin%20Liu%20and%20Yuhao%20Dong%20and%20Yiqin%20Wang%20and%20Zixian%20Ma%20and%20Yansong%20Tang%20and%20Luming%20Tang%20and%20Yongming%20Rao%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20being%20applied%20in%0Areal-world%20environments%2C%20necessitating%20their%20ability%20to%20interpret%203D%20spaces%20and%0Acomprehend%20temporal%20dynamics.%20Current%20methods%20often%20rely%20on%20specialized%0Aarchitectural%20designs%20or%20task-specific%20fine-tuning%20to%20achieve%20this.%20We%0Aintroduce%20Coarse%20Correspondences%2C%20a%20simple%20lightweight%20method%20that%20enhances%0AMLLMs%27%20spatial-temporal%20reasoning%20with%202D%20images%20as%20input%2C%20without%20modifying%0Athe%20architecture%20or%20requiring%20task-specific%20fine-tuning.%20Our%20method%20uses%20a%0Alightweight%20tracking%20model%20to%20identify%20primary%20object%20correspondences%20between%0Aframes%20in%20a%20video%20or%20across%20different%20image%20viewpoints%2C%20and%20then%20conveys%20this%0Ainformation%20to%20MLLMs%20through%20visual%20prompting.%20We%20demonstrate%20that%20this%20simple%0Atraining-free%20approach%20brings%20substantial%20gains%20to%20GPT4-V/O%20consistently%20on%0Afour%20benchmarks%20that%20require%20spatial-temporal%20reasoning%2C%20including%20%2B20.5%5C%25%0Aimprovement%20on%20ScanQA%2C%20%2B9.7%5C%25%20on%20OpenEQA%27s%20episodic%20memory%20subset%2C%20%2B6.0%5C%25%20on%0Athe%20long-form%20video%20benchmark%20EgoSchema%2C%20and%20%2B11%5C%25%20on%20the%20R2R%20navigation%0Abenchmark.%20Additionally%2C%20we%20show%20that%20Coarse%20Correspondences%20can%20also%20enhance%0Aopen-source%20MLLMs%27%20spatial%20reasoning%20%28by%20%2B6.9%5C%25%20on%20ScanQA%29%20when%20applied%20in%20both%0Atraining%20and%20inference%20and%20that%20the%20improvement%20can%20generalize%20to%20unseen%0Adatasets%20such%20as%20SQA3D%20%28%2B3.1%5C%25%29.%20Taken%20together%2C%20we%20show%20that%20Coarse%0ACorrespondences%20effectively%20and%20efficiently%20boosts%20models%27%20performance%20on%0Adownstream%20tasks%20requiring%20spatial-temporal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00754v2&entry.124074799=Read"},
{"title": "Neuro-3D: Towards 3D Visual Decoding from EEG Signals", "author": "Zhanqiang Guo and Jiamin Wu and Yonghao Song and Jiahui Bu and Weijian Mai and Qihao Zheng and Wanli Ouyang and Chunfeng Song", "abstract": "  Human's perception of the visual world is shaped by the stereo processing of\n3D information. Understanding how the brain perceives and processes 3D visual\nstimuli in the real world has been a longstanding endeavor in neuroscience.\nTowards this goal, we introduce a new neuroscience task: decoding 3D visual\nperception from EEG signals, a neuroimaging technique that enables real-time\nmonitoring of neural dynamics enriched with complex visual cues. To provide the\nessential benchmark, we first present EEG-3D, a pioneering dataset featuring\nmultimodal analysis data and extensive EEG recordings from 12 subjects viewing\n72 categories of 3D objects rendered in both videos and images. Furthermore, we\npropose Neuro-3D, a 3D visual decoding framework based on EEG signals. This\nframework adaptively integrates EEG features derived from static and dynamic\nstimuli to learn complementary and robust neural representations, which are\nsubsequently utilized to recover both the shape and color of 3D objects through\nthe proposed diffusion-based colored point cloud decoder. To the best of our\nknowledge, we are the first to explore EEG-based 3D visual decoding.\nExperiments indicate that Neuro-3D not only reconstructs colored 3D objects\nwith high fidelity, but also learns effective neural representations that\nenable insightful brain region analysis. The dataset and associated code will\nbe made publicly available.\n", "link": "http://arxiv.org/abs/2411.12248v2", "date": "2024-11-21", "relevancy": 3.0573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6312}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-3D%3A%20Towards%203D%20Visual%20Decoding%20from%20EEG%20Signals&body=Title%3A%20Neuro-3D%3A%20Towards%203D%20Visual%20Decoding%20from%20EEG%20Signals%0AAuthor%3A%20Zhanqiang%20Guo%20and%20Jiamin%20Wu%20and%20Yonghao%20Song%20and%20Jiahui%20Bu%20and%20Weijian%20Mai%20and%20Qihao%20Zheng%20and%20Wanli%20Ouyang%20and%20Chunfeng%20Song%0AAbstract%3A%20%20%20Human%27s%20perception%20of%20the%20visual%20world%20is%20shaped%20by%20the%20stereo%20processing%20of%0A3D%20information.%20Understanding%20how%20the%20brain%20perceives%20and%20processes%203D%20visual%0Astimuli%20in%20the%20real%20world%20has%20been%20a%20longstanding%20endeavor%20in%20neuroscience.%0ATowards%20this%20goal%2C%20we%20introduce%20a%20new%20neuroscience%20task%3A%20decoding%203D%20visual%0Aperception%20from%20EEG%20signals%2C%20a%20neuroimaging%20technique%20that%20enables%20real-time%0Amonitoring%20of%20neural%20dynamics%20enriched%20with%20complex%20visual%20cues.%20To%20provide%20the%0Aessential%20benchmark%2C%20we%20first%20present%20EEG-3D%2C%20a%20pioneering%20dataset%20featuring%0Amultimodal%20analysis%20data%20and%20extensive%20EEG%20recordings%20from%2012%20subjects%20viewing%0A72%20categories%20of%203D%20objects%20rendered%20in%20both%20videos%20and%20images.%20Furthermore%2C%20we%0Apropose%20Neuro-3D%2C%20a%203D%20visual%20decoding%20framework%20based%20on%20EEG%20signals.%20This%0Aframework%20adaptively%20integrates%20EEG%20features%20derived%20from%20static%20and%20dynamic%0Astimuli%20to%20learn%20complementary%20and%20robust%20neural%20representations%2C%20which%20are%0Asubsequently%20utilized%20to%20recover%20both%20the%20shape%20and%20color%20of%203D%20objects%20through%0Athe%20proposed%20diffusion-based%20colored%20point%20cloud%20decoder.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20explore%20EEG-based%203D%20visual%20decoding.%0AExperiments%20indicate%20that%20Neuro-3D%20not%20only%20reconstructs%20colored%203D%20objects%0Awith%20high%20fidelity%2C%20but%20also%20learns%20effective%20neural%20representations%20that%0Aenable%20insightful%20brain%20region%20analysis.%20The%20dataset%20and%20associated%20code%20will%0Abe%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12248v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-3D%253A%2520Towards%25203D%2520Visual%2520Decoding%2520from%2520EEG%2520Signals%26entry.906535625%3DZhanqiang%2520Guo%2520and%2520Jiamin%2520Wu%2520and%2520Yonghao%2520Song%2520and%2520Jiahui%2520Bu%2520and%2520Weijian%2520Mai%2520and%2520Qihao%2520Zheng%2520and%2520Wanli%2520Ouyang%2520and%2520Chunfeng%2520Song%26entry.1292438233%3D%2520%2520Human%2527s%2520perception%2520of%2520the%2520visual%2520world%2520is%2520shaped%2520by%2520the%2520stereo%2520processing%2520of%250A3D%2520information.%2520Understanding%2520how%2520the%2520brain%2520perceives%2520and%2520processes%25203D%2520visual%250Astimuli%2520in%2520the%2520real%2520world%2520has%2520been%2520a%2520longstanding%2520endeavor%2520in%2520neuroscience.%250ATowards%2520this%2520goal%252C%2520we%2520introduce%2520a%2520new%2520neuroscience%2520task%253A%2520decoding%25203D%2520visual%250Aperception%2520from%2520EEG%2520signals%252C%2520a%2520neuroimaging%2520technique%2520that%2520enables%2520real-time%250Amonitoring%2520of%2520neural%2520dynamics%2520enriched%2520with%2520complex%2520visual%2520cues.%2520To%2520provide%2520the%250Aessential%2520benchmark%252C%2520we%2520first%2520present%2520EEG-3D%252C%2520a%2520pioneering%2520dataset%2520featuring%250Amultimodal%2520analysis%2520data%2520and%2520extensive%2520EEG%2520recordings%2520from%252012%2520subjects%2520viewing%250A72%2520categories%2520of%25203D%2520objects%2520rendered%2520in%2520both%2520videos%2520and%2520images.%2520Furthermore%252C%2520we%250Apropose%2520Neuro-3D%252C%2520a%25203D%2520visual%2520decoding%2520framework%2520based%2520on%2520EEG%2520signals.%2520This%250Aframework%2520adaptively%2520integrates%2520EEG%2520features%2520derived%2520from%2520static%2520and%2520dynamic%250Astimuli%2520to%2520learn%2520complementary%2520and%2520robust%2520neural%2520representations%252C%2520which%2520are%250Asubsequently%2520utilized%2520to%2520recover%2520both%2520the%2520shape%2520and%2520color%2520of%25203D%2520objects%2520through%250Athe%2520proposed%2520diffusion-based%2520colored%2520point%2520cloud%2520decoder.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520we%2520are%2520the%2520first%2520to%2520explore%2520EEG-based%25203D%2520visual%2520decoding.%250AExperiments%2520indicate%2520that%2520Neuro-3D%2520not%2520only%2520reconstructs%2520colored%25203D%2520objects%250Awith%2520high%2520fidelity%252C%2520but%2520also%2520learns%2520effective%2520neural%2520representations%2520that%250Aenable%2520insightful%2520brain%2520region%2520analysis.%2520The%2520dataset%2520and%2520associated%2520code%2520will%250Abe%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12248v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-3D%3A%20Towards%203D%20Visual%20Decoding%20from%20EEG%20Signals&entry.906535625=Zhanqiang%20Guo%20and%20Jiamin%20Wu%20and%20Yonghao%20Song%20and%20Jiahui%20Bu%20and%20Weijian%20Mai%20and%20Qihao%20Zheng%20and%20Wanli%20Ouyang%20and%20Chunfeng%20Song&entry.1292438233=%20%20Human%27s%20perception%20of%20the%20visual%20world%20is%20shaped%20by%20the%20stereo%20processing%20of%0A3D%20information.%20Understanding%20how%20the%20brain%20perceives%20and%20processes%203D%20visual%0Astimuli%20in%20the%20real%20world%20has%20been%20a%20longstanding%20endeavor%20in%20neuroscience.%0ATowards%20this%20goal%2C%20we%20introduce%20a%20new%20neuroscience%20task%3A%20decoding%203D%20visual%0Aperception%20from%20EEG%20signals%2C%20a%20neuroimaging%20technique%20that%20enables%20real-time%0Amonitoring%20of%20neural%20dynamics%20enriched%20with%20complex%20visual%20cues.%20To%20provide%20the%0Aessential%20benchmark%2C%20we%20first%20present%20EEG-3D%2C%20a%20pioneering%20dataset%20featuring%0Amultimodal%20analysis%20data%20and%20extensive%20EEG%20recordings%20from%2012%20subjects%20viewing%0A72%20categories%20of%203D%20objects%20rendered%20in%20both%20videos%20and%20images.%20Furthermore%2C%20we%0Apropose%20Neuro-3D%2C%20a%203D%20visual%20decoding%20framework%20based%20on%20EEG%20signals.%20This%0Aframework%20adaptively%20integrates%20EEG%20features%20derived%20from%20static%20and%20dynamic%0Astimuli%20to%20learn%20complementary%20and%20robust%20neural%20representations%2C%20which%20are%0Asubsequently%20utilized%20to%20recover%20both%20the%20shape%20and%20color%20of%203D%20objects%20through%0Athe%20proposed%20diffusion-based%20colored%20point%20cloud%20decoder.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20explore%20EEG-based%203D%20visual%20decoding.%0AExperiments%20indicate%20that%20Neuro-3D%20not%20only%20reconstructs%20colored%203D%20objects%0Awith%20high%20fidelity%2C%20but%20also%20learns%20effective%20neural%20representations%20that%0Aenable%20insightful%20brain%20region%20analysis.%20The%20dataset%20and%20associated%20code%20will%0Abe%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12248v2&entry.124074799=Read"},
{"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models", "author": "Yuhao Dong and Zuyan Liu and Hai-Long Sun and Jingkang Yang and Winston Hu and Yongming Rao and Ziwei Liu", "abstract": "  Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.\n", "link": "http://arxiv.org/abs/2411.14432v1", "date": "2024-11-21", "relevancy": 3.0438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuhao%20Dong%20and%20Zuyan%20Liu%20and%20Hai-Long%20Sun%20and%20Jingkang%20Yang%20and%20Winston%20Hu%20and%20Yongming%20Rao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20enhanced%20capabilities%20and%0Areliability%20by%20reasoning%20more%2C%20evolving%20from%20Chain-of-Thought%20prompting%20to%0Aproduct-level%20solutions%20like%20OpenAI%20o1.%20Despite%20various%20efforts%20to%20improve%20LLM%0Areasoning%2C%20high-quality%20long-chain%20reasoning%20data%20and%20optimized%20training%0Apipelines%20still%20remain%20inadequately%20explored%20in%20vision-language%20tasks.%20In%20this%0Apaper%2C%20we%20present%20Insight-V%2C%20an%20early%20effort%20to%201%29%20scalably%20produce%20long%20and%0Arobust%20reasoning%20data%20for%20complex%20multi-modal%20tasks%2C%20and%202%29%20an%20effective%0Atraining%20pipeline%20to%20enhance%20the%20reasoning%20capabilities%20of%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29.%20Specifically%2C%20to%20create%20long%20and%20structured%20reasoning%0Adata%20without%20human%20labor%2C%20we%20design%20a%20two-step%20pipeline%20with%20a%20progressive%0Astrategy%20to%20generate%20sufficiently%20long%20and%20diverse%20reasoning%20paths%20and%20a%0Amulti-granularity%20assessment%20method%20to%20ensure%20data%20quality.%20We%20observe%20that%0Adirectly%20supervising%20MLLMs%20with%20such%20long%20and%20complex%20reasoning%20data%20will%20not%0Ayield%20ideal%20reasoning%20ability.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20multi-agent%0Asystem%20consisting%20of%20a%20reasoning%20agent%20dedicated%20to%20performing%20long-chain%0Areasoning%20and%20a%20summary%20agent%20trained%20to%20judge%20and%20summarize%20reasoning%20results.%0AWe%20further%20incorporate%20an%20iterative%20DPO%20algorithm%20to%20enhance%20the%20reasoning%0Aagent%27s%20generation%20stability%20and%20quality.%20Based%20on%20the%20popular%20LLaVA-NeXT%20model%0Aand%20our%20stronger%20base%20MLLM%2C%20we%20demonstrate%20significant%20performance%20gains%20across%0Achallenging%20multi-modal%20benchmarks%20requiring%20visual%20reasoning.%20Benefiting%20from%0Aour%20multi-agent%20system%2C%20Insight-V%20can%20also%20easily%20maintain%20or%20improve%0Aperformance%20on%20perception-focused%20multi-modal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsight-V%253A%2520Exploring%2520Long-Chain%2520Visual%2520Reasoning%2520with%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuhao%2520Dong%2520and%2520Zuyan%2520Liu%2520and%2520Hai-Long%2520Sun%2520and%2520Jingkang%2520Yang%2520and%2520Winston%2520Hu%2520and%2520Yongming%2520Rao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520enhanced%2520capabilities%2520and%250Areliability%2520by%2520reasoning%2520more%252C%2520evolving%2520from%2520Chain-of-Thought%2520prompting%2520to%250Aproduct-level%2520solutions%2520like%2520OpenAI%2520o1.%2520Despite%2520various%2520efforts%2520to%2520improve%2520LLM%250Areasoning%252C%2520high-quality%2520long-chain%2520reasoning%2520data%2520and%2520optimized%2520training%250Apipelines%2520still%2520remain%2520inadequately%2520explored%2520in%2520vision-language%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Insight-V%252C%2520an%2520early%2520effort%2520to%25201%2529%2520scalably%2520produce%2520long%2520and%250Arobust%2520reasoning%2520data%2520for%2520complex%2520multi-modal%2520tasks%252C%2520and%25202%2529%2520an%2520effective%250Atraining%2520pipeline%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520multi-modal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520Specifically%252C%2520to%2520create%2520long%2520and%2520structured%2520reasoning%250Adata%2520without%2520human%2520labor%252C%2520we%2520design%2520a%2520two-step%2520pipeline%2520with%2520a%2520progressive%250Astrategy%2520to%2520generate%2520sufficiently%2520long%2520and%2520diverse%2520reasoning%2520paths%2520and%2520a%250Amulti-granularity%2520assessment%2520method%2520to%2520ensure%2520data%2520quality.%2520We%2520observe%2520that%250Adirectly%2520supervising%2520MLLMs%2520with%2520such%2520long%2520and%2520complex%2520reasoning%2520data%2520will%2520not%250Ayield%2520ideal%2520reasoning%2520ability.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520design%2520a%2520multi-agent%250Asystem%2520consisting%2520of%2520a%2520reasoning%2520agent%2520dedicated%2520to%2520performing%2520long-chain%250Areasoning%2520and%2520a%2520summary%2520agent%2520trained%2520to%2520judge%2520and%2520summarize%2520reasoning%2520results.%250AWe%2520further%2520incorporate%2520an%2520iterative%2520DPO%2520algorithm%2520to%2520enhance%2520the%2520reasoning%250Aagent%2527s%2520generation%2520stability%2520and%2520quality.%2520Based%2520on%2520the%2520popular%2520LLaVA-NeXT%2520model%250Aand%2520our%2520stronger%2520base%2520MLLM%252C%2520we%2520demonstrate%2520significant%2520performance%2520gains%2520across%250Achallenging%2520multi-modal%2520benchmarks%2520requiring%2520visual%2520reasoning.%2520Benefiting%2520from%250Aour%2520multi-agent%2520system%252C%2520Insight-V%2520can%2520also%2520easily%2520maintain%2520or%2520improve%250Aperformance%2520on%2520perception-focused%2520multi-modal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Yuhao%20Dong%20and%20Zuyan%20Liu%20and%20Hai-Long%20Sun%20and%20Jingkang%20Yang%20and%20Winston%20Hu%20and%20Yongming%20Rao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20enhanced%20capabilities%20and%0Areliability%20by%20reasoning%20more%2C%20evolving%20from%20Chain-of-Thought%20prompting%20to%0Aproduct-level%20solutions%20like%20OpenAI%20o1.%20Despite%20various%20efforts%20to%20improve%20LLM%0Areasoning%2C%20high-quality%20long-chain%20reasoning%20data%20and%20optimized%20training%0Apipelines%20still%20remain%20inadequately%20explored%20in%20vision-language%20tasks.%20In%20this%0Apaper%2C%20we%20present%20Insight-V%2C%20an%20early%20effort%20to%201%29%20scalably%20produce%20long%20and%0Arobust%20reasoning%20data%20for%20complex%20multi-modal%20tasks%2C%20and%202%29%20an%20effective%0Atraining%20pipeline%20to%20enhance%20the%20reasoning%20capabilities%20of%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29.%20Specifically%2C%20to%20create%20long%20and%20structured%20reasoning%0Adata%20without%20human%20labor%2C%20we%20design%20a%20two-step%20pipeline%20with%20a%20progressive%0Astrategy%20to%20generate%20sufficiently%20long%20and%20diverse%20reasoning%20paths%20and%20a%0Amulti-granularity%20assessment%20method%20to%20ensure%20data%20quality.%20We%20observe%20that%0Adirectly%20supervising%20MLLMs%20with%20such%20long%20and%20complex%20reasoning%20data%20will%20not%0Ayield%20ideal%20reasoning%20ability.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20multi-agent%0Asystem%20consisting%20of%20a%20reasoning%20agent%20dedicated%20to%20performing%20long-chain%0Areasoning%20and%20a%20summary%20agent%20trained%20to%20judge%20and%20summarize%20reasoning%20results.%0AWe%20further%20incorporate%20an%20iterative%20DPO%20algorithm%20to%20enhance%20the%20reasoning%0Aagent%27s%20generation%20stability%20and%20quality.%20Based%20on%20the%20popular%20LLaVA-NeXT%20model%0Aand%20our%20stronger%20base%20MLLM%2C%20we%20demonstrate%20significant%20performance%20gains%20across%0Achallenging%20multi-modal%20benchmarks%20requiring%20visual%20reasoning.%20Benefiting%20from%0Aour%20multi-agent%20system%2C%20Insight-V%20can%20also%20easily%20maintain%20or%20improve%0Aperformance%20on%20perception-focused%20multi-modal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14432v1&entry.124074799=Read"},
{"title": "Multimodal Autoregressive Pre-training of Large Vision Encoders", "author": "Enrico Fini and Mustafa Shukor and Xiujun Li and Philipp Dufter and Michal Klein and David Haldimann and Sai Aitharaju and Victor Guilherme Turrisi da Costa and Louis B\u00e9thune and Zhe Gan and Alexander T Toshev and Marcin Eichner and Moin Nabi and Yinfei Yang and Joshua M. Susskind and Alaaeldin El-Nouby", "abstract": "  We introduce a novel method for pre-training of large-scale vision encoders.\nBuilding on recent advancements in autoregressive pre-training of vision\nmodels, we extend this framework to a multimodal setting, i.e., images and\ntext. In this paper, we present AIMV2, a family of generalist vision encoders\ncharacterized by a straightforward pre-training process, scalability, and\nremarkable performance across a range of downstream tasks. This is achieved by\npairing the vision encoder with a multimodal decoder that autoregressively\ngenerates raw image patches and text tokens. Our encoders excel not only in\nmultimodal evaluations but also in vision benchmarks such as localization,\ngrounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%\naccuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently\noutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in\nmultimodal image understanding across diverse settings.\n", "link": "http://arxiv.org/abs/2411.14402v1", "date": "2024-11-21", "relevancy": 3.0378, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Autoregressive%20Pre-training%20of%20Large%20Vision%20Encoders&body=Title%3A%20Multimodal%20Autoregressive%20Pre-training%20of%20Large%20Vision%20Encoders%0AAuthor%3A%20Enrico%20Fini%20and%20Mustafa%20Shukor%20and%20Xiujun%20Li%20and%20Philipp%20Dufter%20and%20Michal%20Klein%20and%20David%20Haldimann%20and%20Sai%20Aitharaju%20and%20Victor%20Guilherme%20Turrisi%20da%20Costa%20and%20Louis%20B%C3%A9thune%20and%20Zhe%20Gan%20and%20Alexander%20T%20Toshev%20and%20Marcin%20Eichner%20and%20Moin%20Nabi%20and%20Yinfei%20Yang%20and%20Joshua%20M.%20Susskind%20and%20Alaaeldin%20El-Nouby%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20pre-training%20of%20large-scale%20vision%20encoders.%0ABuilding%20on%20recent%20advancements%20in%20autoregressive%20pre-training%20of%20vision%0Amodels%2C%20we%20extend%20this%20framework%20to%20a%20multimodal%20setting%2C%20i.e.%2C%20images%20and%0Atext.%20In%20this%20paper%2C%20we%20present%20AIMV2%2C%20a%20family%20of%20generalist%20vision%20encoders%0Acharacterized%20by%20a%20straightforward%20pre-training%20process%2C%20scalability%2C%20and%0Aremarkable%20performance%20across%20a%20range%20of%20downstream%20tasks.%20This%20is%20achieved%20by%0Apairing%20the%20vision%20encoder%20with%20a%20multimodal%20decoder%20that%20autoregressively%0Agenerates%20raw%20image%20patches%20and%20text%20tokens.%20Our%20encoders%20excel%20not%20only%20in%0Amultimodal%20evaluations%20but%20also%20in%20vision%20benchmarks%20such%20as%20localization%2C%0Agrounding%2C%20and%20classification.%20Notably%2C%20our%20AIMV2-3B%20encoder%20achieves%2089.5%25%0Aaccuracy%20on%20ImageNet-1k%20with%20a%20frozen%20trunk.%20Furthermore%2C%20AIMV2%20consistently%0Aoutperforms%20state-of-the-art%20contrastive%20models%20%28e.g.%2C%20CLIP%2C%20SigLIP%29%20in%0Amultimodal%20image%20understanding%20across%20diverse%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Autoregressive%2520Pre-training%2520of%2520Large%2520Vision%2520Encoders%26entry.906535625%3DEnrico%2520Fini%2520and%2520Mustafa%2520Shukor%2520and%2520Xiujun%2520Li%2520and%2520Philipp%2520Dufter%2520and%2520Michal%2520Klein%2520and%2520David%2520Haldimann%2520and%2520Sai%2520Aitharaju%2520and%2520Victor%2520Guilherme%2520Turrisi%2520da%2520Costa%2520and%2520Louis%2520B%25C3%25A9thune%2520and%2520Zhe%2520Gan%2520and%2520Alexander%2520T%2520Toshev%2520and%2520Marcin%2520Eichner%2520and%2520Moin%2520Nabi%2520and%2520Yinfei%2520Yang%2520and%2520Joshua%2520M.%2520Susskind%2520and%2520Alaaeldin%2520El-Nouby%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520pre-training%2520of%2520large-scale%2520vision%2520encoders.%250ABuilding%2520on%2520recent%2520advancements%2520in%2520autoregressive%2520pre-training%2520of%2520vision%250Amodels%252C%2520we%2520extend%2520this%2520framework%2520to%2520a%2520multimodal%2520setting%252C%2520i.e.%252C%2520images%2520and%250Atext.%2520In%2520this%2520paper%252C%2520we%2520present%2520AIMV2%252C%2520a%2520family%2520of%2520generalist%2520vision%2520encoders%250Acharacterized%2520by%2520a%2520straightforward%2520pre-training%2520process%252C%2520scalability%252C%2520and%250Aremarkable%2520performance%2520across%2520a%2520range%2520of%2520downstream%2520tasks.%2520This%2520is%2520achieved%2520by%250Apairing%2520the%2520vision%2520encoder%2520with%2520a%2520multimodal%2520decoder%2520that%2520autoregressively%250Agenerates%2520raw%2520image%2520patches%2520and%2520text%2520tokens.%2520Our%2520encoders%2520excel%2520not%2520only%2520in%250Amultimodal%2520evaluations%2520but%2520also%2520in%2520vision%2520benchmarks%2520such%2520as%2520localization%252C%250Agrounding%252C%2520and%2520classification.%2520Notably%252C%2520our%2520AIMV2-3B%2520encoder%2520achieves%252089.5%2525%250Aaccuracy%2520on%2520ImageNet-1k%2520with%2520a%2520frozen%2520trunk.%2520Furthermore%252C%2520AIMV2%2520consistently%250Aoutperforms%2520state-of-the-art%2520contrastive%2520models%2520%2528e.g.%252C%2520CLIP%252C%2520SigLIP%2529%2520in%250Amultimodal%2520image%2520understanding%2520across%2520diverse%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Autoregressive%20Pre-training%20of%20Large%20Vision%20Encoders&entry.906535625=Enrico%20Fini%20and%20Mustafa%20Shukor%20and%20Xiujun%20Li%20and%20Philipp%20Dufter%20and%20Michal%20Klein%20and%20David%20Haldimann%20and%20Sai%20Aitharaju%20and%20Victor%20Guilherme%20Turrisi%20da%20Costa%20and%20Louis%20B%C3%A9thune%20and%20Zhe%20Gan%20and%20Alexander%20T%20Toshev%20and%20Marcin%20Eichner%20and%20Moin%20Nabi%20and%20Yinfei%20Yang%20and%20Joshua%20M.%20Susskind%20and%20Alaaeldin%20El-Nouby&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20pre-training%20of%20large-scale%20vision%20encoders.%0ABuilding%20on%20recent%20advancements%20in%20autoregressive%20pre-training%20of%20vision%0Amodels%2C%20we%20extend%20this%20framework%20to%20a%20multimodal%20setting%2C%20i.e.%2C%20images%20and%0Atext.%20In%20this%20paper%2C%20we%20present%20AIMV2%2C%20a%20family%20of%20generalist%20vision%20encoders%0Acharacterized%20by%20a%20straightforward%20pre-training%20process%2C%20scalability%2C%20and%0Aremarkable%20performance%20across%20a%20range%20of%20downstream%20tasks.%20This%20is%20achieved%20by%0Apairing%20the%20vision%20encoder%20with%20a%20multimodal%20decoder%20that%20autoregressively%0Agenerates%20raw%20image%20patches%20and%20text%20tokens.%20Our%20encoders%20excel%20not%20only%20in%0Amultimodal%20evaluations%20but%20also%20in%20vision%20benchmarks%20such%20as%20localization%2C%0Agrounding%2C%20and%20classification.%20Notably%2C%20our%20AIMV2-3B%20encoder%20achieves%2089.5%25%0Aaccuracy%20on%20ImageNet-1k%20with%20a%20frozen%20trunk.%20Furthermore%2C%20AIMV2%20consistently%0Aoutperforms%20state-of-the-art%20contrastive%20models%20%28e.g.%2C%20CLIP%2C%20SigLIP%29%20in%0Amultimodal%20image%20understanding%20across%20diverse%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14402v1&entry.124074799=Read"},
{"title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting\n  and Calibrating Abnormal Human-body", "author": "Zeqing Wang and Qingyang Ma and Wentao Wan and Haojie Li and Keze Wang and Yonghong Tian", "abstract": "  Recent improvements in visual synthesis have significantly enhanced the\ndepiction of generated human photos, which are pivotal due to their wide\napplicability and demand. Nonetheless, the existing text-to-image or\ntext-to-video models often generate low-quality human photos that might differ\nconsiderably from real-world body structures, referred to as \"abnormal human\nbodies\". Such abnormalities, typically deemed unacceptable, pose considerable\nchallenges in the detection and repair of them within human photos. These\nchallenges require precise abnormality recognition capabilities, which entail\npinpointing both the location and the abnormality type. Intuitively, Visual\nLanguage Models (VLMs) that have obtained remarkable performance on various\nvisual tasks are quite suitable for this task. However, their performance on\nabnormality detection in human photos is quite poor. Hence, it is quite\nimportant to highlight this task for the research community. In this paper, we\nfirst introduce a simple yet challenging task, i.e., \\textbf{F}ine-grained\n\\textbf{H}uman-body \\textbf{A}bnormality \\textbf{D}etection \\textbf{(FHAD)},\nand construct two high-quality datasets for evaluation. Then, we propose a\nmeticulous framework, named HumanCalibrator, which identifies and repairs\nabnormalities in human body structures while preserving the other content.\nExperiments indicate that our HumanCalibrator achieves high accuracy in\nabnormality detection and accomplishes an increase in visual comparisons while\npreserving the other visual content.\n", "link": "http://arxiv.org/abs/2411.14205v1", "date": "2024-11-21", "relevancy": 2.9865, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6193}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5999}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20this%20Generated%20Person%20Existed%20in%20Real-world%3F%20Fine-grained%20Detecting%0A%20%20and%20Calibrating%20Abnormal%20Human-body&body=Title%3A%20Is%20this%20Generated%20Person%20Existed%20in%20Real-world%3F%20Fine-grained%20Detecting%0A%20%20and%20Calibrating%20Abnormal%20Human-body%0AAuthor%3A%20Zeqing%20Wang%20and%20Qingyang%20Ma%20and%20Wentao%20Wan%20and%20Haojie%20Li%20and%20Keze%20Wang%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Recent%20improvements%20in%20visual%20synthesis%20have%20significantly%20enhanced%20the%0Adepiction%20of%20generated%20human%20photos%2C%20which%20are%20pivotal%20due%20to%20their%20wide%0Aapplicability%20and%20demand.%20Nonetheless%2C%20the%20existing%20text-to-image%20or%0Atext-to-video%20models%20often%20generate%20low-quality%20human%20photos%20that%20might%20differ%0Aconsiderably%20from%20real-world%20body%20structures%2C%20referred%20to%20as%20%22abnormal%20human%0Abodies%22.%20Such%20abnormalities%2C%20typically%20deemed%20unacceptable%2C%20pose%20considerable%0Achallenges%20in%20the%20detection%20and%20repair%20of%20them%20within%20human%20photos.%20These%0Achallenges%20require%20precise%20abnormality%20recognition%20capabilities%2C%20which%20entail%0Apinpointing%20both%20the%20location%20and%20the%20abnormality%20type.%20Intuitively%2C%20Visual%0ALanguage%20Models%20%28VLMs%29%20that%20have%20obtained%20remarkable%20performance%20on%20various%0Avisual%20tasks%20are%20quite%20suitable%20for%20this%20task.%20However%2C%20their%20performance%20on%0Aabnormality%20detection%20in%20human%20photos%20is%20quite%20poor.%20Hence%2C%20it%20is%20quite%0Aimportant%20to%20highlight%20this%20task%20for%20the%20research%20community.%20In%20this%20paper%2C%20we%0Afirst%20introduce%20a%20simple%20yet%20challenging%20task%2C%20i.e.%2C%20%5Ctextbf%7BF%7Dine-grained%0A%5Ctextbf%7BH%7Duman-body%20%5Ctextbf%7BA%7Dbnormality%20%5Ctextbf%7BD%7Detection%20%5Ctextbf%7B%28FHAD%29%7D%2C%0Aand%20construct%20two%20high-quality%20datasets%20for%20evaluation.%20Then%2C%20we%20propose%20a%0Ameticulous%20framework%2C%20named%20HumanCalibrator%2C%20which%20identifies%20and%20repairs%0Aabnormalities%20in%20human%20body%20structures%20while%20preserving%20the%20other%20content.%0AExperiments%20indicate%20that%20our%20HumanCalibrator%20achieves%20high%20accuracy%20in%0Aabnormality%20detection%20and%20accomplishes%20an%20increase%20in%20visual%20comparisons%20while%0Apreserving%20the%20other%20visual%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520this%2520Generated%2520Person%2520Existed%2520in%2520Real-world%253F%2520Fine-grained%2520Detecting%250A%2520%2520and%2520Calibrating%2520Abnormal%2520Human-body%26entry.906535625%3DZeqing%2520Wang%2520and%2520Qingyang%2520Ma%2520and%2520Wentao%2520Wan%2520and%2520Haojie%2520Li%2520and%2520Keze%2520Wang%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520Recent%2520improvements%2520in%2520visual%2520synthesis%2520have%2520significantly%2520enhanced%2520the%250Adepiction%2520of%2520generated%2520human%2520photos%252C%2520which%2520are%2520pivotal%2520due%2520to%2520their%2520wide%250Aapplicability%2520and%2520demand.%2520Nonetheless%252C%2520the%2520existing%2520text-to-image%2520or%250Atext-to-video%2520models%2520often%2520generate%2520low-quality%2520human%2520photos%2520that%2520might%2520differ%250Aconsiderably%2520from%2520real-world%2520body%2520structures%252C%2520referred%2520to%2520as%2520%2522abnormal%2520human%250Abodies%2522.%2520Such%2520abnormalities%252C%2520typically%2520deemed%2520unacceptable%252C%2520pose%2520considerable%250Achallenges%2520in%2520the%2520detection%2520and%2520repair%2520of%2520them%2520within%2520human%2520photos.%2520These%250Achallenges%2520require%2520precise%2520abnormality%2520recognition%2520capabilities%252C%2520which%2520entail%250Apinpointing%2520both%2520the%2520location%2520and%2520the%2520abnormality%2520type.%2520Intuitively%252C%2520Visual%250ALanguage%2520Models%2520%2528VLMs%2529%2520that%2520have%2520obtained%2520remarkable%2520performance%2520on%2520various%250Avisual%2520tasks%2520are%2520quite%2520suitable%2520for%2520this%2520task.%2520However%252C%2520their%2520performance%2520on%250Aabnormality%2520detection%2520in%2520human%2520photos%2520is%2520quite%2520poor.%2520Hence%252C%2520it%2520is%2520quite%250Aimportant%2520to%2520highlight%2520this%2520task%2520for%2520the%2520research%2520community.%2520In%2520this%2520paper%252C%2520we%250Afirst%2520introduce%2520a%2520simple%2520yet%2520challenging%2520task%252C%2520i.e.%252C%2520%255Ctextbf%257BF%257Dine-grained%250A%255Ctextbf%257BH%257Duman-body%2520%255Ctextbf%257BA%257Dbnormality%2520%255Ctextbf%257BD%257Detection%2520%255Ctextbf%257B%2528FHAD%2529%257D%252C%250Aand%2520construct%2520two%2520high-quality%2520datasets%2520for%2520evaluation.%2520Then%252C%2520we%2520propose%2520a%250Ameticulous%2520framework%252C%2520named%2520HumanCalibrator%252C%2520which%2520identifies%2520and%2520repairs%250Aabnormalities%2520in%2520human%2520body%2520structures%2520while%2520preserving%2520the%2520other%2520content.%250AExperiments%2520indicate%2520that%2520our%2520HumanCalibrator%2520achieves%2520high%2520accuracy%2520in%250Aabnormality%2520detection%2520and%2520accomplishes%2520an%2520increase%2520in%2520visual%2520comparisons%2520while%250Apreserving%2520the%2520other%2520visual%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20this%20Generated%20Person%20Existed%20in%20Real-world%3F%20Fine-grained%20Detecting%0A%20%20and%20Calibrating%20Abnormal%20Human-body&entry.906535625=Zeqing%20Wang%20and%20Qingyang%20Ma%20and%20Wentao%20Wan%20and%20Haojie%20Li%20and%20Keze%20Wang%20and%20Yonghong%20Tian&entry.1292438233=%20%20Recent%20improvements%20in%20visual%20synthesis%20have%20significantly%20enhanced%20the%0Adepiction%20of%20generated%20human%20photos%2C%20which%20are%20pivotal%20due%20to%20their%20wide%0Aapplicability%20and%20demand.%20Nonetheless%2C%20the%20existing%20text-to-image%20or%0Atext-to-video%20models%20often%20generate%20low-quality%20human%20photos%20that%20might%20differ%0Aconsiderably%20from%20real-world%20body%20structures%2C%20referred%20to%20as%20%22abnormal%20human%0Abodies%22.%20Such%20abnormalities%2C%20typically%20deemed%20unacceptable%2C%20pose%20considerable%0Achallenges%20in%20the%20detection%20and%20repair%20of%20them%20within%20human%20photos.%20These%0Achallenges%20require%20precise%20abnormality%20recognition%20capabilities%2C%20which%20entail%0Apinpointing%20both%20the%20location%20and%20the%20abnormality%20type.%20Intuitively%2C%20Visual%0ALanguage%20Models%20%28VLMs%29%20that%20have%20obtained%20remarkable%20performance%20on%20various%0Avisual%20tasks%20are%20quite%20suitable%20for%20this%20task.%20However%2C%20their%20performance%20on%0Aabnormality%20detection%20in%20human%20photos%20is%20quite%20poor.%20Hence%2C%20it%20is%20quite%0Aimportant%20to%20highlight%20this%20task%20for%20the%20research%20community.%20In%20this%20paper%2C%20we%0Afirst%20introduce%20a%20simple%20yet%20challenging%20task%2C%20i.e.%2C%20%5Ctextbf%7BF%7Dine-grained%0A%5Ctextbf%7BH%7Duman-body%20%5Ctextbf%7BA%7Dbnormality%20%5Ctextbf%7BD%7Detection%20%5Ctextbf%7B%28FHAD%29%7D%2C%0Aand%20construct%20two%20high-quality%20datasets%20for%20evaluation.%20Then%2C%20we%20propose%20a%0Ameticulous%20framework%2C%20named%20HumanCalibrator%2C%20which%20identifies%20and%20repairs%0Aabnormalities%20in%20human%20body%20structures%20while%20preserving%20the%20other%20content.%0AExperiments%20indicate%20that%20our%20HumanCalibrator%20achieves%20high%20accuracy%20in%0Aabnormality%20detection%20and%20accomplishes%20an%20increase%20in%20visual%20comparisons%20while%0Apreserving%20the%20other%20visual%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14205v1&entry.124074799=Read"},
{"title": "Novel View Extrapolation with Video Diffusion Priors", "author": "Kunhao Liu and Ling Shao and Shijian Lu", "abstract": "  The field of novel view synthesis has made significant strides thanks to the\ndevelopment of radiance field methods. However, most radiance field techniques\nare far better at novel view interpolation than novel view extrapolation where\nthe synthesis novel views are far beyond the observed training views. We design\nViewExtrapolator, a novel view synthesis approach that leverages the generative\npriors of Stable Video Diffusion (SVD) for realistic novel view extrapolation.\nBy redesigning the SVD denoising process, ViewExtrapolator refines the\nartifact-prone views rendered by radiance fields, greatly enhancing the clarity\nand realism of the synthesized novel views. ViewExtrapolator is a generic novel\nview extrapolator that can work with different types of 3D rendering such as\nviews rendered from point clouds when only a single view or monocular video is\navailable. Additionally, ViewExtrapolator requires no fine-tuning of SVD,\nmaking it both data-efficient and computation-efficient. Extensive experiments\ndemonstrate the superiority of ViewExtrapolator in novel view extrapolation.\nProject page: \\url{https://kunhao-liu.github.io/ViewExtrapolator/}.\n", "link": "http://arxiv.org/abs/2411.14208v1", "date": "2024-11-21", "relevancy": 2.9692, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6019}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20View%20Extrapolation%20with%20Video%20Diffusion%20Priors&body=Title%3A%20Novel%20View%20Extrapolation%20with%20Video%20Diffusion%20Priors%0AAuthor%3A%20Kunhao%20Liu%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20The%20field%20of%20novel%20view%20synthesis%20has%20made%20significant%20strides%20thanks%20to%20the%0Adevelopment%20of%20radiance%20field%20methods.%20However%2C%20most%20radiance%20field%20techniques%0Aare%20far%20better%20at%20novel%20view%20interpolation%20than%20novel%20view%20extrapolation%20where%0Athe%20synthesis%20novel%20views%20are%20far%20beyond%20the%20observed%20training%20views.%20We%20design%0AViewExtrapolator%2C%20a%20novel%20view%20synthesis%20approach%20that%20leverages%20the%20generative%0Apriors%20of%20Stable%20Video%20Diffusion%20%28SVD%29%20for%20realistic%20novel%20view%20extrapolation.%0ABy%20redesigning%20the%20SVD%20denoising%20process%2C%20ViewExtrapolator%20refines%20the%0Aartifact-prone%20views%20rendered%20by%20radiance%20fields%2C%20greatly%20enhancing%20the%20clarity%0Aand%20realism%20of%20the%20synthesized%20novel%20views.%20ViewExtrapolator%20is%20a%20generic%20novel%0Aview%20extrapolator%20that%20can%20work%20with%20different%20types%20of%203D%20rendering%20such%20as%0Aviews%20rendered%20from%20point%20clouds%20when%20only%20a%20single%20view%20or%20monocular%20video%20is%0Aavailable.%20Additionally%2C%20ViewExtrapolator%20requires%20no%20fine-tuning%20of%20SVD%2C%0Amaking%20it%20both%20data-efficient%20and%20computation-efficient.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20ViewExtrapolator%20in%20novel%20view%20extrapolation.%0AProject%20page%3A%20%5Curl%7Bhttps%3A//kunhao-liu.github.io/ViewExtrapolator/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520View%2520Extrapolation%2520with%2520Video%2520Diffusion%2520Priors%26entry.906535625%3DKunhao%2520Liu%2520and%2520Ling%2520Shao%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520novel%2520view%2520synthesis%2520has%2520made%2520significant%2520strides%2520thanks%2520to%2520the%250Adevelopment%2520of%2520radiance%2520field%2520methods.%2520However%252C%2520most%2520radiance%2520field%2520techniques%250Aare%2520far%2520better%2520at%2520novel%2520view%2520interpolation%2520than%2520novel%2520view%2520extrapolation%2520where%250Athe%2520synthesis%2520novel%2520views%2520are%2520far%2520beyond%2520the%2520observed%2520training%2520views.%2520We%2520design%250AViewExtrapolator%252C%2520a%2520novel%2520view%2520synthesis%2520approach%2520that%2520leverages%2520the%2520generative%250Apriors%2520of%2520Stable%2520Video%2520Diffusion%2520%2528SVD%2529%2520for%2520realistic%2520novel%2520view%2520extrapolation.%250ABy%2520redesigning%2520the%2520SVD%2520denoising%2520process%252C%2520ViewExtrapolator%2520refines%2520the%250Aartifact-prone%2520views%2520rendered%2520by%2520radiance%2520fields%252C%2520greatly%2520enhancing%2520the%2520clarity%250Aand%2520realism%2520of%2520the%2520synthesized%2520novel%2520views.%2520ViewExtrapolator%2520is%2520a%2520generic%2520novel%250Aview%2520extrapolator%2520that%2520can%2520work%2520with%2520different%2520types%2520of%25203D%2520rendering%2520such%2520as%250Aviews%2520rendered%2520from%2520point%2520clouds%2520when%2520only%2520a%2520single%2520view%2520or%2520monocular%2520video%2520is%250Aavailable.%2520Additionally%252C%2520ViewExtrapolator%2520requires%2520no%2520fine-tuning%2520of%2520SVD%252C%250Amaking%2520it%2520both%2520data-efficient%2520and%2520computation-efficient.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520ViewExtrapolator%2520in%2520novel%2520view%2520extrapolation.%250AProject%2520page%253A%2520%255Curl%257Bhttps%253A//kunhao-liu.github.io/ViewExtrapolator/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20View%20Extrapolation%20with%20Video%20Diffusion%20Priors&entry.906535625=Kunhao%20Liu%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20The%20field%20of%20novel%20view%20synthesis%20has%20made%20significant%20strides%20thanks%20to%20the%0Adevelopment%20of%20radiance%20field%20methods.%20However%2C%20most%20radiance%20field%20techniques%0Aare%20far%20better%20at%20novel%20view%20interpolation%20than%20novel%20view%20extrapolation%20where%0Athe%20synthesis%20novel%20views%20are%20far%20beyond%20the%20observed%20training%20views.%20We%20design%0AViewExtrapolator%2C%20a%20novel%20view%20synthesis%20approach%20that%20leverages%20the%20generative%0Apriors%20of%20Stable%20Video%20Diffusion%20%28SVD%29%20for%20realistic%20novel%20view%20extrapolation.%0ABy%20redesigning%20the%20SVD%20denoising%20process%2C%20ViewExtrapolator%20refines%20the%0Aartifact-prone%20views%20rendered%20by%20radiance%20fields%2C%20greatly%20enhancing%20the%20clarity%0Aand%20realism%20of%20the%20synthesized%20novel%20views.%20ViewExtrapolator%20is%20a%20generic%20novel%0Aview%20extrapolator%20that%20can%20work%20with%20different%20types%20of%203D%20rendering%20such%20as%0Aviews%20rendered%20from%20point%20clouds%20when%20only%20a%20single%20view%20or%20monocular%20video%20is%0Aavailable.%20Additionally%2C%20ViewExtrapolator%20requires%20no%20fine-tuning%20of%20SVD%2C%0Amaking%20it%20both%20data-efficient%20and%20computation-efficient.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20ViewExtrapolator%20in%20novel%20view%20extrapolation.%0AProject%20page%3A%20%5Curl%7Bhttps%3A//kunhao-liu.github.io/ViewExtrapolator/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14208v1&entry.124074799=Read"},
{"title": "A Fusion of Variational Distribution Priors and Saliency Map Replay for\n  Continual 3D Reconstruction", "author": "Sanchar Palit and Sandika Biswas", "abstract": "  Single-image 3D reconstruction is a research challenge focused on predicting\n3D object shapes from single-view images. This task requires significant data\nacquisition to predict both visible and occluded portions of the shape.\nFurthermore, learning-based methods face the difficulty of creating a\ncomprehensive training dataset for all possible classes. To this end, we\npropose a continual learning-based 3D reconstruction method where our goal is\nto design a model using Variational Priors that can still reconstruct the\npreviously seen classes reasonably even after training on new classes.\nVariational Priors represent abstract shapes and combat forgetting, whereas\nsaliency maps preserve object attributes with less memory usage. This is vital\ndue to resource constraints in storing extensive training data. Additionally,\nwe introduce saliency map-based experience replay to capture global and\ndistinct object features. Thorough experiments show competitive results\ncompared to established methods, both quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2308.08812v2", "date": "2024-11-21", "relevancy": 2.9622, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6125}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5824}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fusion%20of%20Variational%20Distribution%20Priors%20and%20Saliency%20Map%20Replay%20for%0A%20%20Continual%203D%20Reconstruction&body=Title%3A%20A%20Fusion%20of%20Variational%20Distribution%20Priors%20and%20Saliency%20Map%20Replay%20for%0A%20%20Continual%203D%20Reconstruction%0AAuthor%3A%20Sanchar%20Palit%20and%20Sandika%20Biswas%0AAbstract%3A%20%20%20Single-image%203D%20reconstruction%20is%20a%20research%20challenge%20focused%20on%20predicting%0A3D%20object%20shapes%20from%20single-view%20images.%20This%20task%20requires%20significant%20data%0Aacquisition%20to%20predict%20both%20visible%20and%20occluded%20portions%20of%20the%20shape.%0AFurthermore%2C%20learning-based%20methods%20face%20the%20difficulty%20of%20creating%20a%0Acomprehensive%20training%20dataset%20for%20all%20possible%20classes.%20To%20this%20end%2C%20we%0Apropose%20a%20continual%20learning-based%203D%20reconstruction%20method%20where%20our%20goal%20is%0Ato%20design%20a%20model%20using%20Variational%20Priors%20that%20can%20still%20reconstruct%20the%0Apreviously%20seen%20classes%20reasonably%20even%20after%20training%20on%20new%20classes.%0AVariational%20Priors%20represent%20abstract%20shapes%20and%20combat%20forgetting%2C%20whereas%0Asaliency%20maps%20preserve%20object%20attributes%20with%20less%20memory%20usage.%20This%20is%20vital%0Adue%20to%20resource%20constraints%20in%20storing%20extensive%20training%20data.%20Additionally%2C%0Awe%20introduce%20saliency%20map-based%20experience%20replay%20to%20capture%20global%20and%0Adistinct%20object%20features.%20Thorough%20experiments%20show%20competitive%20results%0Acompared%20to%20established%20methods%2C%20both%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fusion%2520of%2520Variational%2520Distribution%2520Priors%2520and%2520Saliency%2520Map%2520Replay%2520for%250A%2520%2520Continual%25203D%2520Reconstruction%26entry.906535625%3DSanchar%2520Palit%2520and%2520Sandika%2520Biswas%26entry.1292438233%3D%2520%2520Single-image%25203D%2520reconstruction%2520is%2520a%2520research%2520challenge%2520focused%2520on%2520predicting%250A3D%2520object%2520shapes%2520from%2520single-view%2520images.%2520This%2520task%2520requires%2520significant%2520data%250Aacquisition%2520to%2520predict%2520both%2520visible%2520and%2520occluded%2520portions%2520of%2520the%2520shape.%250AFurthermore%252C%2520learning-based%2520methods%2520face%2520the%2520difficulty%2520of%2520creating%2520a%250Acomprehensive%2520training%2520dataset%2520for%2520all%2520possible%2520classes.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520continual%2520learning-based%25203D%2520reconstruction%2520method%2520where%2520our%2520goal%2520is%250Ato%2520design%2520a%2520model%2520using%2520Variational%2520Priors%2520that%2520can%2520still%2520reconstruct%2520the%250Apreviously%2520seen%2520classes%2520reasonably%2520even%2520after%2520training%2520on%2520new%2520classes.%250AVariational%2520Priors%2520represent%2520abstract%2520shapes%2520and%2520combat%2520forgetting%252C%2520whereas%250Asaliency%2520maps%2520preserve%2520object%2520attributes%2520with%2520less%2520memory%2520usage.%2520This%2520is%2520vital%250Adue%2520to%2520resource%2520constraints%2520in%2520storing%2520extensive%2520training%2520data.%2520Additionally%252C%250Awe%2520introduce%2520saliency%2520map-based%2520experience%2520replay%2520to%2520capture%2520global%2520and%250Adistinct%2520object%2520features.%2520Thorough%2520experiments%2520show%2520competitive%2520results%250Acompared%2520to%2520established%2520methods%252C%2520both%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fusion%20of%20Variational%20Distribution%20Priors%20and%20Saliency%20Map%20Replay%20for%0A%20%20Continual%203D%20Reconstruction&entry.906535625=Sanchar%20Palit%20and%20Sandika%20Biswas&entry.1292438233=%20%20Single-image%203D%20reconstruction%20is%20a%20research%20challenge%20focused%20on%20predicting%0A3D%20object%20shapes%20from%20single-view%20images.%20This%20task%20requires%20significant%20data%0Aacquisition%20to%20predict%20both%20visible%20and%20occluded%20portions%20of%20the%20shape.%0AFurthermore%2C%20learning-based%20methods%20face%20the%20difficulty%20of%20creating%20a%0Acomprehensive%20training%20dataset%20for%20all%20possible%20classes.%20To%20this%20end%2C%20we%0Apropose%20a%20continual%20learning-based%203D%20reconstruction%20method%20where%20our%20goal%20is%0Ato%20design%20a%20model%20using%20Variational%20Priors%20that%20can%20still%20reconstruct%20the%0Apreviously%20seen%20classes%20reasonably%20even%20after%20training%20on%20new%20classes.%0AVariational%20Priors%20represent%20abstract%20shapes%20and%20combat%20forgetting%2C%20whereas%0Asaliency%20maps%20preserve%20object%20attributes%20with%20less%20memory%20usage.%20This%20is%20vital%0Adue%20to%20resource%20constraints%20in%20storing%20extensive%20training%20data.%20Additionally%2C%0Awe%20introduce%20saliency%20map-based%20experience%20replay%20to%20capture%20global%20and%0Adistinct%20object%20features.%20Thorough%20experiments%20show%20competitive%20results%0Acompared%20to%20established%20methods%2C%20both%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08812v2&entry.124074799=Read"},
{"title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression", "author": "Yuke Zhu and Chi Xie and Shuang Liang and Bo Zheng and Sheng Guo", "abstract": "  Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets.\n", "link": "http://arxiv.org/abs/2411.14228v1", "date": "2024-11-21", "relevancy": 2.9598, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusLLaVA%3A%20A%20Coarse-to-Fine%20Approach%20for%20Efficient%20and%20Effective%20Visual%0A%20%20Token%20Compression&body=Title%3A%20FocusLLaVA%3A%20A%20Coarse-to-Fine%20Approach%20for%20Efficient%20and%20Effective%20Visual%0A%20%20Token%20Compression%0AAuthor%3A%20Yuke%20Zhu%20and%20Chi%20Xie%20and%20Shuang%20Liang%20and%20Bo%20Zheng%20and%20Sheng%20Guo%0AAbstract%3A%20%20%20Recent%20advances%20on%20Multi-modal%20Large%20Language%20Models%20have%20demonstrated%20that%0Ahigh-resolution%20image%20input%20is%20crucial%20for%20model%20capabilities%2C%20especially%20for%0Afine-grained%20tasks.%20However%2C%20high-resolution%20images%20lead%20to%20a%20quadratic%0Aincrease%20in%20the%20number%20of%20visual%20tokens%20input%20into%20LLMs%2C%20resulting%20in%0Asignificant%20computational%20costs.%20Current%20work%20develop%20visual%20token%20compression%0Amethods%20to%20achieve%20efficiency%20improvements%2C%20often%20at%20the%20expense%20of%0Aperformance.%20We%20argue%20that%20removing%20visual%20redundancy%20can%20simultaneously%0Aimprove%20both%20efficiency%20and%20performance.%20We%20build%20a%20coarse-to-fine%20visual%20token%0Acompression%20method%2C%20with%20a%20vision-guided%20sampler%20for%20compressing%20redundant%0Aregions%20with%20low%20information%20density%2C%20and%20a%20text-guided%20sampler%20for%20selecting%0Avisual%20tokens%20that%20are%20strongly%20correlated%20with%20the%20user%20instructions.With%0Athese%20two%20modules%2C%20the%20proposed%20FocusLLaVA%20achieves%20improvements%20in%20both%0Aefficiency%20and%20performance.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20on%20a%0Awide%20range%20of%20evaluation%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusLLaVA%253A%2520A%2520Coarse-to-Fine%2520Approach%2520for%2520Efficient%2520and%2520Effective%2520Visual%250A%2520%2520Token%2520Compression%26entry.906535625%3DYuke%2520Zhu%2520and%2520Chi%2520Xie%2520and%2520Shuang%2520Liang%2520and%2520Bo%2520Zheng%2520and%2520Sheng%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520on%2520Multi-modal%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520that%250Ahigh-resolution%2520image%2520input%2520is%2520crucial%2520for%2520model%2520capabilities%252C%2520especially%2520for%250Afine-grained%2520tasks.%2520However%252C%2520high-resolution%2520images%2520lead%2520to%2520a%2520quadratic%250Aincrease%2520in%2520the%2520number%2520of%2520visual%2520tokens%2520input%2520into%2520LLMs%252C%2520resulting%2520in%250Asignificant%2520computational%2520costs.%2520Current%2520work%2520develop%2520visual%2520token%2520compression%250Amethods%2520to%2520achieve%2520efficiency%2520improvements%252C%2520often%2520at%2520the%2520expense%2520of%250Aperformance.%2520We%2520argue%2520that%2520removing%2520visual%2520redundancy%2520can%2520simultaneously%250Aimprove%2520both%2520efficiency%2520and%2520performance.%2520We%2520build%2520a%2520coarse-to-fine%2520visual%2520token%250Acompression%2520method%252C%2520with%2520a%2520vision-guided%2520sampler%2520for%2520compressing%2520redundant%250Aregions%2520with%2520low%2520information%2520density%252C%2520and%2520a%2520text-guided%2520sampler%2520for%2520selecting%250Avisual%2520tokens%2520that%2520are%2520strongly%2520correlated%2520with%2520the%2520user%2520instructions.With%250Athese%2520two%2520modules%252C%2520the%2520proposed%2520FocusLLaVA%2520achieves%2520improvements%2520in%2520both%250Aefficiency%2520and%2520performance.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520a%250Awide%2520range%2520of%2520evaluation%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusLLaVA%3A%20A%20Coarse-to-Fine%20Approach%20for%20Efficient%20and%20Effective%20Visual%0A%20%20Token%20Compression&entry.906535625=Yuke%20Zhu%20and%20Chi%20Xie%20and%20Shuang%20Liang%20and%20Bo%20Zheng%20and%20Sheng%20Guo&entry.1292438233=%20%20Recent%20advances%20on%20Multi-modal%20Large%20Language%20Models%20have%20demonstrated%20that%0Ahigh-resolution%20image%20input%20is%20crucial%20for%20model%20capabilities%2C%20especially%20for%0Afine-grained%20tasks.%20However%2C%20high-resolution%20images%20lead%20to%20a%20quadratic%0Aincrease%20in%20the%20number%20of%20visual%20tokens%20input%20into%20LLMs%2C%20resulting%20in%0Asignificant%20computational%20costs.%20Current%20work%20develop%20visual%20token%20compression%0Amethods%20to%20achieve%20efficiency%20improvements%2C%20often%20at%20the%20expense%20of%0Aperformance.%20We%20argue%20that%20removing%20visual%20redundancy%20can%20simultaneously%0Aimprove%20both%20efficiency%20and%20performance.%20We%20build%20a%20coarse-to-fine%20visual%20token%0Acompression%20method%2C%20with%20a%20vision-guided%20sampler%20for%20compressing%20redundant%0Aregions%20with%20low%20information%20density%2C%20and%20a%20text-guided%20sampler%20for%20selecting%0Avisual%20tokens%20that%20are%20strongly%20correlated%20with%20the%20user%20instructions.With%0Athese%20two%20modules%2C%20the%20proposed%20FocusLLaVA%20achieves%20improvements%20in%20both%0Aefficiency%20and%20performance.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20on%20a%0Awide%20range%20of%20evaluation%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14228v1&entry.124074799=Read"},
{"title": "StereoCrafter-Zero: Zero-Shot Stereo Video Generation with Noisy Restart", "author": "Jian Shi and Qian Wang and Zhenyu Li and Peter Wonka", "abstract": "  Generating high-quality stereo videos that mimic human binocular vision\nrequires maintaining consistent depth perception and temporal coherence across\nframes. While diffusion models have advanced image and video synthesis,\ngenerating high-quality stereo videos remains challenging due to the difficulty\nof maintaining consistent temporal and spatial coherence between left and right\nviews. We introduce \\textit{StereoCrafter-Zero}, a novel framework for\nzero-shot stereo video generation that leverages video diffusion priors without\nthe need for paired training data. Key innovations include a noisy restart\nstrategy to initialize stereo-aware latents and an iterative refinement process\nthat progressively harmonizes the latent space, addressing issues like temporal\nflickering and view inconsistencies. Comprehensive evaluations, including\nquantitative metrics and user studies, demonstrate that\n\\textit{StereoCrafter-Zero} produces high-quality stereo videos with improved\ndepth consistency and temporal smoothness, even when depth estimations are\nimperfect. Our framework is robust and adaptable across various diffusion\nmodels, setting a new benchmark for zero-shot stereo video generation and\nenabling more immersive visual experiences. Our code can be found\nin~\\url{https://github.com/shijianjian/StereoCrafter-Zero}.\n", "link": "http://arxiv.org/abs/2411.14295v1", "date": "2024-11-21", "relevancy": 2.9591, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6216}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5857}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StereoCrafter-Zero%3A%20Zero-Shot%20Stereo%20Video%20Generation%20with%20Noisy%20Restart&body=Title%3A%20StereoCrafter-Zero%3A%20Zero-Shot%20Stereo%20Video%20Generation%20with%20Noisy%20Restart%0AAuthor%3A%20Jian%20Shi%20and%20Qian%20Wang%20and%20Zhenyu%20Li%20and%20Peter%20Wonka%0AAbstract%3A%20%20%20Generating%20high-quality%20stereo%20videos%20that%20mimic%20human%20binocular%20vision%0Arequires%20maintaining%20consistent%20depth%20perception%20and%20temporal%20coherence%20across%0Aframes.%20While%20diffusion%20models%20have%20advanced%20image%20and%20video%20synthesis%2C%0Agenerating%20high-quality%20stereo%20videos%20remains%20challenging%20due%20to%20the%20difficulty%0Aof%20maintaining%20consistent%20temporal%20and%20spatial%20coherence%20between%20left%20and%20right%0Aviews.%20We%20introduce%20%5Ctextit%7BStereoCrafter-Zero%7D%2C%20a%20novel%20framework%20for%0Azero-shot%20stereo%20video%20generation%20that%20leverages%20video%20diffusion%20priors%20without%0Athe%20need%20for%20paired%20training%20data.%20Key%20innovations%20include%20a%20noisy%20restart%0Astrategy%20to%20initialize%20stereo-aware%20latents%20and%20an%20iterative%20refinement%20process%0Athat%20progressively%20harmonizes%20the%20latent%20space%2C%20addressing%20issues%20like%20temporal%0Aflickering%20and%20view%20inconsistencies.%20Comprehensive%20evaluations%2C%20including%0Aquantitative%20metrics%20and%20user%20studies%2C%20demonstrate%20that%0A%5Ctextit%7BStereoCrafter-Zero%7D%20produces%20high-quality%20stereo%20videos%20with%20improved%0Adepth%20consistency%20and%20temporal%20smoothness%2C%20even%20when%20depth%20estimations%20are%0Aimperfect.%20Our%20framework%20is%20robust%20and%20adaptable%20across%20various%20diffusion%0Amodels%2C%20setting%20a%20new%20benchmark%20for%20zero-shot%20stereo%20video%20generation%20and%0Aenabling%20more%20immersive%20visual%20experiences.%20Our%20code%20can%20be%20found%0Ain~%5Curl%7Bhttps%3A//github.com/shijianjian/StereoCrafter-Zero%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereoCrafter-Zero%253A%2520Zero-Shot%2520Stereo%2520Video%2520Generation%2520with%2520Noisy%2520Restart%26entry.906535625%3DJian%2520Shi%2520and%2520Qian%2520Wang%2520and%2520Zhenyu%2520Li%2520and%2520Peter%2520Wonka%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520stereo%2520videos%2520that%2520mimic%2520human%2520binocular%2520vision%250Arequires%2520maintaining%2520consistent%2520depth%2520perception%2520and%2520temporal%2520coherence%2520across%250Aframes.%2520While%2520diffusion%2520models%2520have%2520advanced%2520image%2520and%2520video%2520synthesis%252C%250Agenerating%2520high-quality%2520stereo%2520videos%2520remains%2520challenging%2520due%2520to%2520the%2520difficulty%250Aof%2520maintaining%2520consistent%2520temporal%2520and%2520spatial%2520coherence%2520between%2520left%2520and%2520right%250Aviews.%2520We%2520introduce%2520%255Ctextit%257BStereoCrafter-Zero%257D%252C%2520a%2520novel%2520framework%2520for%250Azero-shot%2520stereo%2520video%2520generation%2520that%2520leverages%2520video%2520diffusion%2520priors%2520without%250Athe%2520need%2520for%2520paired%2520training%2520data.%2520Key%2520innovations%2520include%2520a%2520noisy%2520restart%250Astrategy%2520to%2520initialize%2520stereo-aware%2520latents%2520and%2520an%2520iterative%2520refinement%2520process%250Athat%2520progressively%2520harmonizes%2520the%2520latent%2520space%252C%2520addressing%2520issues%2520like%2520temporal%250Aflickering%2520and%2520view%2520inconsistencies.%2520Comprehensive%2520evaluations%252C%2520including%250Aquantitative%2520metrics%2520and%2520user%2520studies%252C%2520demonstrate%2520that%250A%255Ctextit%257BStereoCrafter-Zero%257D%2520produces%2520high-quality%2520stereo%2520videos%2520with%2520improved%250Adepth%2520consistency%2520and%2520temporal%2520smoothness%252C%2520even%2520when%2520depth%2520estimations%2520are%250Aimperfect.%2520Our%2520framework%2520is%2520robust%2520and%2520adaptable%2520across%2520various%2520diffusion%250Amodels%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520zero-shot%2520stereo%2520video%2520generation%2520and%250Aenabling%2520more%2520immersive%2520visual%2520experiences.%2520Our%2520code%2520can%2520be%2520found%250Ain~%255Curl%257Bhttps%253A//github.com/shijianjian/StereoCrafter-Zero%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StereoCrafter-Zero%3A%20Zero-Shot%20Stereo%20Video%20Generation%20with%20Noisy%20Restart&entry.906535625=Jian%20Shi%20and%20Qian%20Wang%20and%20Zhenyu%20Li%20and%20Peter%20Wonka&entry.1292438233=%20%20Generating%20high-quality%20stereo%20videos%20that%20mimic%20human%20binocular%20vision%0Arequires%20maintaining%20consistent%20depth%20perception%20and%20temporal%20coherence%20across%0Aframes.%20While%20diffusion%20models%20have%20advanced%20image%20and%20video%20synthesis%2C%0Agenerating%20high-quality%20stereo%20videos%20remains%20challenging%20due%20to%20the%20difficulty%0Aof%20maintaining%20consistent%20temporal%20and%20spatial%20coherence%20between%20left%20and%20right%0Aviews.%20We%20introduce%20%5Ctextit%7BStereoCrafter-Zero%7D%2C%20a%20novel%20framework%20for%0Azero-shot%20stereo%20video%20generation%20that%20leverages%20video%20diffusion%20priors%20without%0Athe%20need%20for%20paired%20training%20data.%20Key%20innovations%20include%20a%20noisy%20restart%0Astrategy%20to%20initialize%20stereo-aware%20latents%20and%20an%20iterative%20refinement%20process%0Athat%20progressively%20harmonizes%20the%20latent%20space%2C%20addressing%20issues%20like%20temporal%0Aflickering%20and%20view%20inconsistencies.%20Comprehensive%20evaluations%2C%20including%0Aquantitative%20metrics%20and%20user%20studies%2C%20demonstrate%20that%0A%5Ctextit%7BStereoCrafter-Zero%7D%20produces%20high-quality%20stereo%20videos%20with%20improved%0Adepth%20consistency%20and%20temporal%20smoothness%2C%20even%20when%20depth%20estimations%20are%0Aimperfect.%20Our%20framework%20is%20robust%20and%20adaptable%20across%20various%20diffusion%0Amodels%2C%20setting%20a%20new%20benchmark%20for%20zero-shot%20stereo%20video%20generation%20and%0Aenabling%20more%20immersive%20visual%20experiences.%20Our%20code%20can%20be%20found%0Ain~%5Curl%7Bhttps%3A//github.com/shijianjian/StereoCrafter-Zero%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14295v1&entry.124074799=Read"},
{"title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild", "author": "Yumeng Liu and Xiaoxiao Long and Zemin Yang and Yuan Liu and Marc Habermann and Christian Theobalt and Yuexin Ma and Wenping Wang", "abstract": "  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n", "link": "http://arxiv.org/abs/2411.14280v1", "date": "2024-11-21", "relevancy": 2.9312, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6041}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5881}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild&body=Title%3A%20EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild%0AAuthor%3A%20Yumeng%20Liu%20and%20Xiaoxiao%20Long%20and%20Zemin%20Yang%20and%20Yuan%20Liu%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Yuexin%20Ma%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20Our%20work%20aims%20to%20reconstruct%20hand-object%20interactions%20from%20a%20single-view%0Aimage%2C%20which%20is%20a%20fundamental%20but%20ill-posed%20task.%20Unlike%20methods%20that%0Areconstruct%20from%20videos%2C%20multi-view%20images%2C%20or%20predefined%203D%20templates%2C%0Asingle-view%20reconstruction%20faces%20significant%20challenges%20due%20to%20inherent%0Aambiguities%20and%20occlusions.%20These%20challenges%20are%20further%20amplified%20by%20the%0Adiverse%20nature%20of%20hand%20poses%20and%20the%20vast%20variety%20of%20object%20shapes%20and%20sizes.%0AOur%20key%20insight%20is%20that%20current%20foundational%20models%20for%20segmentation%2C%0Ainpainting%2C%20and%203D%20reconstruction%20robustly%20generalize%20to%20in-the-wild%20images%2C%0Awhich%20could%20provide%20strong%20visual%20and%20geometric%20priors%20for%20reconstructing%0Ahand-object%20interactions.%20Specifically%2C%20given%20a%20single%20image%2C%20we%20first%20design%20a%0Anovel%20pipeline%20to%20estimate%20the%20underlying%20hand%20pose%20and%20object%20shape%20using%0Aoff-the-shelf%20large%20models.%20Furthermore%2C%20with%20the%20initial%20reconstruction%2C%20we%0Aemploy%20a%20prior-guided%20optimization%20scheme%2C%20which%20optimizes%20hand%20pose%20to%20comply%0Awith%203D%20physical%20constraints%20and%20the%202D%20input%20image%20content.%20We%20perform%0Aexperiments%20across%20several%20datasets%20and%20show%20that%20our%20method%20consistently%0Aoutperforms%20baselines%20and%20faithfully%20reconstructs%20a%20diverse%20set%20of%20hand-object%0Ainteractions.%20Here%20is%20the%20link%20of%20our%20project%20page%3A%0Ahttps%3A//lym29.github.io/EasyHOI-page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyHOI%253A%2520Unleashing%2520the%2520Power%2520of%2520Large%2520Models%2520for%2520Reconstructing%250A%2520%2520Hand-Object%2520Interactions%2520in%2520the%2520Wild%26entry.906535625%3DYumeng%2520Liu%2520and%2520Xiaoxiao%2520Long%2520and%2520Zemin%2520Yang%2520and%2520Yuan%2520Liu%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%2520and%2520Yuexin%2520Ma%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520Our%2520work%2520aims%2520to%2520reconstruct%2520hand-object%2520interactions%2520from%2520a%2520single-view%250Aimage%252C%2520which%2520is%2520a%2520fundamental%2520but%2520ill-posed%2520task.%2520Unlike%2520methods%2520that%250Areconstruct%2520from%2520videos%252C%2520multi-view%2520images%252C%2520or%2520predefined%25203D%2520templates%252C%250Asingle-view%2520reconstruction%2520faces%2520significant%2520challenges%2520due%2520to%2520inherent%250Aambiguities%2520and%2520occlusions.%2520These%2520challenges%2520are%2520further%2520amplified%2520by%2520the%250Adiverse%2520nature%2520of%2520hand%2520poses%2520and%2520the%2520vast%2520variety%2520of%2520object%2520shapes%2520and%2520sizes.%250AOur%2520key%2520insight%2520is%2520that%2520current%2520foundational%2520models%2520for%2520segmentation%252C%250Ainpainting%252C%2520and%25203D%2520reconstruction%2520robustly%2520generalize%2520to%2520in-the-wild%2520images%252C%250Awhich%2520could%2520provide%2520strong%2520visual%2520and%2520geometric%2520priors%2520for%2520reconstructing%250Ahand-object%2520interactions.%2520Specifically%252C%2520given%2520a%2520single%2520image%252C%2520we%2520first%2520design%2520a%250Anovel%2520pipeline%2520to%2520estimate%2520the%2520underlying%2520hand%2520pose%2520and%2520object%2520shape%2520using%250Aoff-the-shelf%2520large%2520models.%2520Furthermore%252C%2520with%2520the%2520initial%2520reconstruction%252C%2520we%250Aemploy%2520a%2520prior-guided%2520optimization%2520scheme%252C%2520which%2520optimizes%2520hand%2520pose%2520to%2520comply%250Awith%25203D%2520physical%2520constraints%2520and%2520the%25202D%2520input%2520image%2520content.%2520We%2520perform%250Aexperiments%2520across%2520several%2520datasets%2520and%2520show%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520baselines%2520and%2520faithfully%2520reconstructs%2520a%2520diverse%2520set%2520of%2520hand-object%250Ainteractions.%2520Here%2520is%2520the%2520link%2520of%2520our%2520project%2520page%253A%250Ahttps%253A//lym29.github.io/EasyHOI-page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild&entry.906535625=Yumeng%20Liu%20and%20Xiaoxiao%20Long%20and%20Zemin%20Yang%20and%20Yuan%20Liu%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Yuexin%20Ma%20and%20Wenping%20Wang&entry.1292438233=%20%20Our%20work%20aims%20to%20reconstruct%20hand-object%20interactions%20from%20a%20single-view%0Aimage%2C%20which%20is%20a%20fundamental%20but%20ill-posed%20task.%20Unlike%20methods%20that%0Areconstruct%20from%20videos%2C%20multi-view%20images%2C%20or%20predefined%203D%20templates%2C%0Asingle-view%20reconstruction%20faces%20significant%20challenges%20due%20to%20inherent%0Aambiguities%20and%20occlusions.%20These%20challenges%20are%20further%20amplified%20by%20the%0Adiverse%20nature%20of%20hand%20poses%20and%20the%20vast%20variety%20of%20object%20shapes%20and%20sizes.%0AOur%20key%20insight%20is%20that%20current%20foundational%20models%20for%20segmentation%2C%0Ainpainting%2C%20and%203D%20reconstruction%20robustly%20generalize%20to%20in-the-wild%20images%2C%0Awhich%20could%20provide%20strong%20visual%20and%20geometric%20priors%20for%20reconstructing%0Ahand-object%20interactions.%20Specifically%2C%20given%20a%20single%20image%2C%20we%20first%20design%20a%0Anovel%20pipeline%20to%20estimate%20the%20underlying%20hand%20pose%20and%20object%20shape%20using%0Aoff-the-shelf%20large%20models.%20Furthermore%2C%20with%20the%20initial%20reconstruction%2C%20we%0Aemploy%20a%20prior-guided%20optimization%20scheme%2C%20which%20optimizes%20hand%20pose%20to%20comply%0Awith%203D%20physical%20constraints%20and%20the%202D%20input%20image%20content.%20We%20perform%0Aexperiments%20across%20several%20datasets%20and%20show%20that%20our%20method%20consistently%0Aoutperforms%20baselines%20and%20faithfully%20reconstructs%20a%20diverse%20set%20of%20hand-object%0Ainteractions.%20Here%20is%20the%20link%20of%20our%20project%20page%3A%0Ahttps%3A//lym29.github.io/EasyHOI-page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14280v1&entry.124074799=Read"},
{"title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance", "author": "Haozhe Zhao and Shuzheng Si and Liang Chen and Yichi Zhang and Maosong Sun and Mingjia Zhang and Baobao Chang", "abstract": "  Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).\n", "link": "http://arxiv.org/abs/2411.14279v1", "date": "2024-11-21", "relevancy": 2.9085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5818}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20Beyond%20Text%3A%20Reducing%20Language%20bias%20in%20Large%20Vision-Language%0A%20%20Models%20via%20Multimodal%20Dual-Attention%20and%20Soft-Image%20Guidance&body=Title%3A%20Looking%20Beyond%20Text%3A%20Reducing%20Language%20bias%20in%20Large%20Vision-Language%0A%20%20Models%20via%20Multimodal%20Dual-Attention%20and%20Soft-Image%20Guidance%0AAuthor%3A%20Haozhe%20Zhao%20and%20Shuzheng%20Si%20and%20Liang%20Chen%20and%20Yichi%20Zhang%20and%20Maosong%20Sun%20and%20Mingjia%20Zhang%20and%20Baobao%20Chang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20achieved%20impressive%20results%20in%0Avarious%20vision-language%20tasks.%20However%2C%20despite%20showing%20promising%20performance%2C%0ALVLMs%20suffer%20from%20hallucinations%20caused%20by%20language%20bias%2C%20leading%20to%20diminished%0Afocus%20on%20images%20and%20ineffective%20visual%20comprehension.%20We%20identify%20two%20primary%0Areasons%20for%20this%20bias%3A%201.%20Different%20scales%20of%20training%20data%20between%20the%0Apretraining%20stage%20of%20LLM%20and%20multimodal%20alignment%20stage.%202.%20The%20learned%0Ainference%20bias%20due%20to%20short-term%20dependency%20of%20text%20data.%20Therefore%2C%20we%20propose%0ALACING%2C%20a%20systemic%20framework%20designed%20to%20address%20the%20language%20bias%20of%20LVLMs%0Awith%20muLtimodal%20duAl-attention%20meChanIsm%20%28MDA%29%20aNd%20soft-image%20Guidance%20%28IFG%29.%0ASpecifically%2C%20MDA%20introduces%20a%20parallel%20dual-attention%20mechanism%20that%20enhances%0Athe%20integration%20of%20visual%20inputs%20across%20the%20model.%20IFG%20introduces%20a%20learnable%0Asoft%20visual%20prompt%20during%20training%20and%20inference%20to%20replace%20visual%20inputs%2C%0Adesigned%20to%20compel%20LVLMs%20to%20prioritize%20text%20inputs.%20Then%2C%20IFG%20further%20proposes%0Aa%20novel%20decoding%20strategy%20using%20the%20soft%20visual%20prompt%20to%20mitigate%20the%20model%27s%0Aover-reliance%20on%20adjacent%20text%20inputs.%20Comprehensive%20experiments%20demonstrate%0Athat%20our%20method%20effectively%20debiases%20LVLMs%20from%20their%20language%20bias%2C%20enhancing%0Avisual%20comprehension%20and%20reducing%20hallucinations%20without%20requiring%20additional%0Atraining%20resources%20or%20data.%20The%20code%20and%20model%20are%20available%20at%0A%5Blacing-lvlm.github.io%5D%28https%3A//lacing-lvlm.github.io%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520Beyond%2520Text%253A%2520Reducing%2520Language%2520bias%2520in%2520Large%2520Vision-Language%250A%2520%2520Models%2520via%2520Multimodal%2520Dual-Attention%2520and%2520Soft-Image%2520Guidance%26entry.906535625%3DHaozhe%2520Zhao%2520and%2520Shuzheng%2520Si%2520and%2520Liang%2520Chen%2520and%2520Yichi%2520Zhang%2520and%2520Maosong%2520Sun%2520and%2520Mingjia%2520Zhang%2520and%2520Baobao%2520Chang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520achieved%2520impressive%2520results%2520in%250Avarious%2520vision-language%2520tasks.%2520However%252C%2520despite%2520showing%2520promising%2520performance%252C%250ALVLMs%2520suffer%2520from%2520hallucinations%2520caused%2520by%2520language%2520bias%252C%2520leading%2520to%2520diminished%250Afocus%2520on%2520images%2520and%2520ineffective%2520visual%2520comprehension.%2520We%2520identify%2520two%2520primary%250Areasons%2520for%2520this%2520bias%253A%25201.%2520Different%2520scales%2520of%2520training%2520data%2520between%2520the%250Apretraining%2520stage%2520of%2520LLM%2520and%2520multimodal%2520alignment%2520stage.%25202.%2520The%2520learned%250Ainference%2520bias%2520due%2520to%2520short-term%2520dependency%2520of%2520text%2520data.%2520Therefore%252C%2520we%2520propose%250ALACING%252C%2520a%2520systemic%2520framework%2520designed%2520to%2520address%2520the%2520language%2520bias%2520of%2520LVLMs%250Awith%2520muLtimodal%2520duAl-attention%2520meChanIsm%2520%2528MDA%2529%2520aNd%2520soft-image%2520Guidance%2520%2528IFG%2529.%250ASpecifically%252C%2520MDA%2520introduces%2520a%2520parallel%2520dual-attention%2520mechanism%2520that%2520enhances%250Athe%2520integration%2520of%2520visual%2520inputs%2520across%2520the%2520model.%2520IFG%2520introduces%2520a%2520learnable%250Asoft%2520visual%2520prompt%2520during%2520training%2520and%2520inference%2520to%2520replace%2520visual%2520inputs%252C%250Adesigned%2520to%2520compel%2520LVLMs%2520to%2520prioritize%2520text%2520inputs.%2520Then%252C%2520IFG%2520further%2520proposes%250Aa%2520novel%2520decoding%2520strategy%2520using%2520the%2520soft%2520visual%2520prompt%2520to%2520mitigate%2520the%2520model%2527s%250Aover-reliance%2520on%2520adjacent%2520text%2520inputs.%2520Comprehensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520effectively%2520debiases%2520LVLMs%2520from%2520their%2520language%2520bias%252C%2520enhancing%250Avisual%2520comprehension%2520and%2520reducing%2520hallucinations%2520without%2520requiring%2520additional%250Atraining%2520resources%2520or%2520data.%2520The%2520code%2520and%2520model%2520are%2520available%2520at%250A%255Blacing-lvlm.github.io%255D%2528https%253A//lacing-lvlm.github.io%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20Beyond%20Text%3A%20Reducing%20Language%20bias%20in%20Large%20Vision-Language%0A%20%20Models%20via%20Multimodal%20Dual-Attention%20and%20Soft-Image%20Guidance&entry.906535625=Haozhe%20Zhao%20and%20Shuzheng%20Si%20and%20Liang%20Chen%20and%20Yichi%20Zhang%20and%20Maosong%20Sun%20and%20Mingjia%20Zhang%20and%20Baobao%20Chang&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20achieved%20impressive%20results%20in%0Avarious%20vision-language%20tasks.%20However%2C%20despite%20showing%20promising%20performance%2C%0ALVLMs%20suffer%20from%20hallucinations%20caused%20by%20language%20bias%2C%20leading%20to%20diminished%0Afocus%20on%20images%20and%20ineffective%20visual%20comprehension.%20We%20identify%20two%20primary%0Areasons%20for%20this%20bias%3A%201.%20Different%20scales%20of%20training%20data%20between%20the%0Apretraining%20stage%20of%20LLM%20and%20multimodal%20alignment%20stage.%202.%20The%20learned%0Ainference%20bias%20due%20to%20short-term%20dependency%20of%20text%20data.%20Therefore%2C%20we%20propose%0ALACING%2C%20a%20systemic%20framework%20designed%20to%20address%20the%20language%20bias%20of%20LVLMs%0Awith%20muLtimodal%20duAl-attention%20meChanIsm%20%28MDA%29%20aNd%20soft-image%20Guidance%20%28IFG%29.%0ASpecifically%2C%20MDA%20introduces%20a%20parallel%20dual-attention%20mechanism%20that%20enhances%0Athe%20integration%20of%20visual%20inputs%20across%20the%20model.%20IFG%20introduces%20a%20learnable%0Asoft%20visual%20prompt%20during%20training%20and%20inference%20to%20replace%20visual%20inputs%2C%0Adesigned%20to%20compel%20LVLMs%20to%20prioritize%20text%20inputs.%20Then%2C%20IFG%20further%20proposes%0Aa%20novel%20decoding%20strategy%20using%20the%20soft%20visual%20prompt%20to%20mitigate%20the%20model%27s%0Aover-reliance%20on%20adjacent%20text%20inputs.%20Comprehensive%20experiments%20demonstrate%0Athat%20our%20method%20effectively%20debiases%20LVLMs%20from%20their%20language%20bias%2C%20enhancing%0Avisual%20comprehension%20and%20reducing%20hallucinations%20without%20requiring%20additional%0Atraining%20resources%20or%20data.%20The%20code%20and%20model%20are%20available%20at%0A%5Blacing-lvlm.github.io%5D%28https%3A//lacing-lvlm.github.io%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14279v1&entry.124074799=Read"},
{"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "author": "Lei Jiang and Weizhe Huang and Tongxuan Liu and Yuting Zeng and Jing Li and Lechao Cheng and Xiaohua Xu", "abstract": "  Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency.\n", "link": "http://arxiv.org/abs/2411.14164v1", "date": "2024-11-21", "relevancy": 2.8933, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoPru%3A%20Focal%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Models&body=Title%3A%20FoPru%3A%20Focal%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Models%0AAuthor%3A%20Lei%20Jiang%20and%20Weizhe%20Huang%20and%20Tongxuan%20Liu%20and%20Yuting%20Zeng%20and%20Jing%20Li%20and%20Lechao%20Cheng%20and%20Xiaohua%20Xu%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20represent%20a%20significant%20advancement%0Atoward%20achieving%20superior%20multimodal%20capabilities%20by%20enabling%20powerful%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20understand%20visual%20input.%20Typically%2C%20LVLMs%20utilize%0Avisual%20encoders%2C%20such%20as%20CLIP%2C%20to%20transform%20images%20into%20visual%20tokens%2C%20which%0Aare%20then%20aligned%20with%20textual%20tokens%20through%20projection%20layers%20before%20being%0Ainput%20into%20the%20LLM%20for%20inference.%20Although%20existing%20LVLMs%20have%20achieved%0Asignificant%20success%2C%20their%20inference%20efficiency%20is%20still%20limited%20by%20the%0Asubstantial%20number%20of%20visual%20tokens%20and%20the%20potential%20redundancy%20among%20them.%20To%0Amitigate%20this%20issue%2C%20we%20propose%20Focal%20Pruning%20%28FoPru%29%2C%20a%20training-free%20method%0Athat%20prunes%20visual%20tokens%20based%20on%20the%20attention-based%20token%20significance%0Aderived%20from%20the%20vision%20encoder.%20Specifically%2C%20we%20introduce%20two%20alternative%0Apruning%20strategies%3A%201%29%20the%20rank%20strategy%2C%20which%20leverages%20all%20token%0Asignificance%20scores%20to%20retain%20more%20critical%20tokens%20in%20a%20global%20view%3B%202%29%20the%20row%0Astrategy%2C%20which%20focuses%20on%20preserving%20continuous%20key%20information%20in%20images%20from%0Aa%20local%20perspective.%20Finally%2C%20the%20selected%20tokens%20are%20reordered%20to%20maintain%0Atheir%20original%20positional%20relationships.%20Extensive%20experiments%20across%20various%0ALVLMs%20and%20multimodal%20datasets%20demonstrate%20that%20our%20method%20can%20prune%20a%20large%0Anumber%20of%20redundant%20tokens%20while%20maintaining%20high%20accuracy%2C%20leading%20to%0Asignificant%20improvements%20in%20inference%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoPru%253A%2520Focal%2520Pruning%2520for%2520Efficient%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DLei%2520Jiang%2520and%2520Weizhe%2520Huang%2520and%2520Tongxuan%2520Liu%2520and%2520Yuting%2520Zeng%2520and%2520Jing%2520Li%2520and%2520Lechao%2520Cheng%2520and%2520Xiaohua%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520represent%2520a%2520significant%2520advancement%250Atoward%2520achieving%2520superior%2520multimodal%2520capabilities%2520by%2520enabling%2520powerful%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520understand%2520visual%2520input.%2520Typically%252C%2520LVLMs%2520utilize%250Avisual%2520encoders%252C%2520such%2520as%2520CLIP%252C%2520to%2520transform%2520images%2520into%2520visual%2520tokens%252C%2520which%250Aare%2520then%2520aligned%2520with%2520textual%2520tokens%2520through%2520projection%2520layers%2520before%2520being%250Ainput%2520into%2520the%2520LLM%2520for%2520inference.%2520Although%2520existing%2520LVLMs%2520have%2520achieved%250Asignificant%2520success%252C%2520their%2520inference%2520efficiency%2520is%2520still%2520limited%2520by%2520the%250Asubstantial%2520number%2520of%2520visual%2520tokens%2520and%2520the%2520potential%2520redundancy%2520among%2520them.%2520To%250Amitigate%2520this%2520issue%252C%2520we%2520propose%2520Focal%2520Pruning%2520%2528FoPru%2529%252C%2520a%2520training-free%2520method%250Athat%2520prunes%2520visual%2520tokens%2520based%2520on%2520the%2520attention-based%2520token%2520significance%250Aderived%2520from%2520the%2520vision%2520encoder.%2520Specifically%252C%2520we%2520introduce%2520two%2520alternative%250Apruning%2520strategies%253A%25201%2529%2520the%2520rank%2520strategy%252C%2520which%2520leverages%2520all%2520token%250Asignificance%2520scores%2520to%2520retain%2520more%2520critical%2520tokens%2520in%2520a%2520global%2520view%253B%25202%2529%2520the%2520row%250Astrategy%252C%2520which%2520focuses%2520on%2520preserving%2520continuous%2520key%2520information%2520in%2520images%2520from%250Aa%2520local%2520perspective.%2520Finally%252C%2520the%2520selected%2520tokens%2520are%2520reordered%2520to%2520maintain%250Atheir%2520original%2520positional%2520relationships.%2520Extensive%2520experiments%2520across%2520various%250ALVLMs%2520and%2520multimodal%2520datasets%2520demonstrate%2520that%2520our%2520method%2520can%2520prune%2520a%2520large%250Anumber%2520of%2520redundant%2520tokens%2520while%2520maintaining%2520high%2520accuracy%252C%2520leading%2520to%250Asignificant%2520improvements%2520in%2520inference%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoPru%3A%20Focal%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Models&entry.906535625=Lei%20Jiang%20and%20Weizhe%20Huang%20and%20Tongxuan%20Liu%20and%20Yuting%20Zeng%20and%20Jing%20Li%20and%20Lechao%20Cheng%20and%20Xiaohua%20Xu&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20represent%20a%20significant%20advancement%0Atoward%20achieving%20superior%20multimodal%20capabilities%20by%20enabling%20powerful%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20understand%20visual%20input.%20Typically%2C%20LVLMs%20utilize%0Avisual%20encoders%2C%20such%20as%20CLIP%2C%20to%20transform%20images%20into%20visual%20tokens%2C%20which%0Aare%20then%20aligned%20with%20textual%20tokens%20through%20projection%20layers%20before%20being%0Ainput%20into%20the%20LLM%20for%20inference.%20Although%20existing%20LVLMs%20have%20achieved%0Asignificant%20success%2C%20their%20inference%20efficiency%20is%20still%20limited%20by%20the%0Asubstantial%20number%20of%20visual%20tokens%20and%20the%20potential%20redundancy%20among%20them.%20To%0Amitigate%20this%20issue%2C%20we%20propose%20Focal%20Pruning%20%28FoPru%29%2C%20a%20training-free%20method%0Athat%20prunes%20visual%20tokens%20based%20on%20the%20attention-based%20token%20significance%0Aderived%20from%20the%20vision%20encoder.%20Specifically%2C%20we%20introduce%20two%20alternative%0Apruning%20strategies%3A%201%29%20the%20rank%20strategy%2C%20which%20leverages%20all%20token%0Asignificance%20scores%20to%20retain%20more%20critical%20tokens%20in%20a%20global%20view%3B%202%29%20the%20row%0Astrategy%2C%20which%20focuses%20on%20preserving%20continuous%20key%20information%20in%20images%20from%0Aa%20local%20perspective.%20Finally%2C%20the%20selected%20tokens%20are%20reordered%20to%20maintain%0Atheir%20original%20positional%20relationships.%20Extensive%20experiments%20across%20various%0ALVLMs%20and%20multimodal%20datasets%20demonstrate%20that%20our%20method%20can%20prune%20a%20large%0Anumber%20of%20redundant%20tokens%20while%20maintaining%20high%20accuracy%2C%20leading%20to%0Asignificant%20improvements%20in%20inference%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14164v1&entry.124074799=Read"},
{"title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning", "author": "Yihong Tang and Ao Qu and Zhaokai Wang and Dingyi Zhuang and Zhaofeng Wu and Wei Ma and Shenhao Wang and Yunhan Zheng and Zhan Zhao and Jinhua Zhao", "abstract": "  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, most of\nthese tasks rely on the core spatial reasoning capabilities in two-dimensional\n(2D) environments, and our evaluation reveals that state-of-the-art VLMs\nfrequently generate implausible and incorrect responses to composite spatial\nreasoning problems, including simple pathfinding tasks that humans can solve\neffortlessly at a glance. To address this, we explore an effective approach to\nenhance 2D spatial reasoning within VLMs by training the model solely on basic\nspatial capabilities. We begin by disentangling the key components of 2D\nspatial reasoning: direction comprehension, distance estimation, and\nlocalization. Our central hypothesis is that mastering these basic spatial\ncapabilities can significantly enhance a model's performance on composite\nspatial tasks requiring advanced spatial understanding and combinatorial\nproblem-solving, with generalized improvements in visual-spatial tasks. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks.\nThese findings underscore the effectiveness of mastering basic spatial\ncapabilities in enhancing composite spatial problem-solving, offering insights\ninto systematic strategies for improving VLMs' spatial reasoning capabilities.\n", "link": "http://arxiv.org/abs/2410.16162v2", "date": "2024-11-21", "relevancy": 2.8236, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparkle%3A%20Mastering%20Basic%20Spatial%20Capabilities%20in%20Vision%20Language%20Models%0A%20%20Elicits%20Generalization%20to%20Composite%20Spatial%20Reasoning&body=Title%3A%20Sparkle%3A%20Mastering%20Basic%20Spatial%20Capabilities%20in%20Vision%20Language%20Models%0A%20%20Elicits%20Generalization%20to%20Composite%20Spatial%20Reasoning%0AAuthor%3A%20Yihong%20Tang%20and%20Ao%20Qu%20and%20Zhaokai%20Wang%20and%20Dingyi%20Zhuang%20and%20Zhaofeng%20Wu%20and%20Wei%20Ma%20and%20Shenhao%20Wang%20and%20Yunhan%20Zheng%20and%20Zhan%20Zhao%20and%20Jinhua%20Zhao%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20performance%20across%0Aa%20wide%20range%20of%20downstream%20tasks.%20However%2C%20their%20proficiency%20in%20spatial%0Areasoning%20remains%20limited%2C%20despite%20its%20crucial%20role%20in%20tasks%20involving%0Anavigation%20and%20interaction%20with%20physical%20environments.%20Specifically%2C%20most%20of%0Athese%20tasks%20rely%20on%20the%20core%20spatial%20reasoning%20capabilities%20in%20two-dimensional%0A%282D%29%20environments%2C%20and%20our%20evaluation%20reveals%20that%20state-of-the-art%20VLMs%0Afrequently%20generate%20implausible%20and%20incorrect%20responses%20to%20composite%20spatial%0Areasoning%20problems%2C%20including%20simple%20pathfinding%20tasks%20that%20humans%20can%20solve%0Aeffortlessly%20at%20a%20glance.%20To%20address%20this%2C%20we%20explore%20an%20effective%20approach%20to%0Aenhance%202D%20spatial%20reasoning%20within%20VLMs%20by%20training%20the%20model%20solely%20on%20basic%0Aspatial%20capabilities.%20We%20begin%20by%20disentangling%20the%20key%20components%20of%202D%0Aspatial%20reasoning%3A%20direction%20comprehension%2C%20distance%20estimation%2C%20and%0Alocalization.%20Our%20central%20hypothesis%20is%20that%20mastering%20these%20basic%20spatial%0Acapabilities%20can%20significantly%20enhance%20a%20model%27s%20performance%20on%20composite%0Aspatial%20tasks%20requiring%20advanced%20spatial%20understanding%20and%20combinatorial%0Aproblem-solving%2C%20with%20generalized%20improvements%20in%20visual-spatial%20tasks.%20To%0Ainvestigate%20this%20hypothesis%2C%20we%20introduce%20Sparkle%2C%20a%20framework%20that%20fine-tunes%0AVLMs%20on%20these%20three%20basic%20spatial%20capabilities%20by%20synthetic%20data%20generation%20and%0Atargeted%20supervision%20to%20form%20an%20instruction%20dataset%20for%20each%20capability.%20Our%0Aexperiments%20demonstrate%20that%20VLMs%20fine-tuned%20with%20Sparkle%20achieve%20significant%0Aperformance%20gains%2C%20not%20only%20in%20the%20basic%20tasks%20themselves%20but%20also%20in%0Ageneralizing%20to%20composite%20and%20out-of-distribution%20spatial%20reasoning%20tasks.%0AThese%20findings%20underscore%20the%20effectiveness%20of%20mastering%20basic%20spatial%0Acapabilities%20in%20enhancing%20composite%20spatial%20problem-solving%2C%20offering%20insights%0Ainto%20systematic%20strategies%20for%20improving%20VLMs%27%20spatial%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparkle%253A%2520Mastering%2520Basic%2520Spatial%2520Capabilities%2520in%2520Vision%2520Language%2520Models%250A%2520%2520Elicits%2520Generalization%2520to%2520Composite%2520Spatial%2520Reasoning%26entry.906535625%3DYihong%2520Tang%2520and%2520Ao%2520Qu%2520and%2520Zhaokai%2520Wang%2520and%2520Dingyi%2520Zhuang%2520and%2520Zhaofeng%2520Wu%2520and%2520Wei%2520Ma%2520and%2520Shenhao%2520Wang%2520and%2520Yunhan%2520Zheng%2520and%2520Zhan%2520Zhao%2520and%2520Jinhua%2520Zhao%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520across%250Aa%2520wide%2520range%2520of%2520downstream%2520tasks.%2520However%252C%2520their%2520proficiency%2520in%2520spatial%250Areasoning%2520remains%2520limited%252C%2520despite%2520its%2520crucial%2520role%2520in%2520tasks%2520involving%250Anavigation%2520and%2520interaction%2520with%2520physical%2520environments.%2520Specifically%252C%2520most%2520of%250Athese%2520tasks%2520rely%2520on%2520the%2520core%2520spatial%2520reasoning%2520capabilities%2520in%2520two-dimensional%250A%25282D%2529%2520environments%252C%2520and%2520our%2520evaluation%2520reveals%2520that%2520state-of-the-art%2520VLMs%250Afrequently%2520generate%2520implausible%2520and%2520incorrect%2520responses%2520to%2520composite%2520spatial%250Areasoning%2520problems%252C%2520including%2520simple%2520pathfinding%2520tasks%2520that%2520humans%2520can%2520solve%250Aeffortlessly%2520at%2520a%2520glance.%2520To%2520address%2520this%252C%2520we%2520explore%2520an%2520effective%2520approach%2520to%250Aenhance%25202D%2520spatial%2520reasoning%2520within%2520VLMs%2520by%2520training%2520the%2520model%2520solely%2520on%2520basic%250Aspatial%2520capabilities.%2520We%2520begin%2520by%2520disentangling%2520the%2520key%2520components%2520of%25202D%250Aspatial%2520reasoning%253A%2520direction%2520comprehension%252C%2520distance%2520estimation%252C%2520and%250Alocalization.%2520Our%2520central%2520hypothesis%2520is%2520that%2520mastering%2520these%2520basic%2520spatial%250Acapabilities%2520can%2520significantly%2520enhance%2520a%2520model%2527s%2520performance%2520on%2520composite%250Aspatial%2520tasks%2520requiring%2520advanced%2520spatial%2520understanding%2520and%2520combinatorial%250Aproblem-solving%252C%2520with%2520generalized%2520improvements%2520in%2520visual-spatial%2520tasks.%2520To%250Ainvestigate%2520this%2520hypothesis%252C%2520we%2520introduce%2520Sparkle%252C%2520a%2520framework%2520that%2520fine-tunes%250AVLMs%2520on%2520these%2520three%2520basic%2520spatial%2520capabilities%2520by%2520synthetic%2520data%2520generation%2520and%250Atargeted%2520supervision%2520to%2520form%2520an%2520instruction%2520dataset%2520for%2520each%2520capability.%2520Our%250Aexperiments%2520demonstrate%2520that%2520VLMs%2520fine-tuned%2520with%2520Sparkle%2520achieve%2520significant%250Aperformance%2520gains%252C%2520not%2520only%2520in%2520the%2520basic%2520tasks%2520themselves%2520but%2520also%2520in%250Ageneralizing%2520to%2520composite%2520and%2520out-of-distribution%2520spatial%2520reasoning%2520tasks.%250AThese%2520findings%2520underscore%2520the%2520effectiveness%2520of%2520mastering%2520basic%2520spatial%250Acapabilities%2520in%2520enhancing%2520composite%2520spatial%2520problem-solving%252C%2520offering%2520insights%250Ainto%2520systematic%2520strategies%2520for%2520improving%2520VLMs%2527%2520spatial%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparkle%3A%20Mastering%20Basic%20Spatial%20Capabilities%20in%20Vision%20Language%20Models%0A%20%20Elicits%20Generalization%20to%20Composite%20Spatial%20Reasoning&entry.906535625=Yihong%20Tang%20and%20Ao%20Qu%20and%20Zhaokai%20Wang%20and%20Dingyi%20Zhuang%20and%20Zhaofeng%20Wu%20and%20Wei%20Ma%20and%20Shenhao%20Wang%20and%20Yunhan%20Zheng%20and%20Zhan%20Zhao%20and%20Jinhua%20Zhao&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20performance%20across%0Aa%20wide%20range%20of%20downstream%20tasks.%20However%2C%20their%20proficiency%20in%20spatial%0Areasoning%20remains%20limited%2C%20despite%20its%20crucial%20role%20in%20tasks%20involving%0Anavigation%20and%20interaction%20with%20physical%20environments.%20Specifically%2C%20most%20of%0Athese%20tasks%20rely%20on%20the%20core%20spatial%20reasoning%20capabilities%20in%20two-dimensional%0A%282D%29%20environments%2C%20and%20our%20evaluation%20reveals%20that%20state-of-the-art%20VLMs%0Afrequently%20generate%20implausible%20and%20incorrect%20responses%20to%20composite%20spatial%0Areasoning%20problems%2C%20including%20simple%20pathfinding%20tasks%20that%20humans%20can%20solve%0Aeffortlessly%20at%20a%20glance.%20To%20address%20this%2C%20we%20explore%20an%20effective%20approach%20to%0Aenhance%202D%20spatial%20reasoning%20within%20VLMs%20by%20training%20the%20model%20solely%20on%20basic%0Aspatial%20capabilities.%20We%20begin%20by%20disentangling%20the%20key%20components%20of%202D%0Aspatial%20reasoning%3A%20direction%20comprehension%2C%20distance%20estimation%2C%20and%0Alocalization.%20Our%20central%20hypothesis%20is%20that%20mastering%20these%20basic%20spatial%0Acapabilities%20can%20significantly%20enhance%20a%20model%27s%20performance%20on%20composite%0Aspatial%20tasks%20requiring%20advanced%20spatial%20understanding%20and%20combinatorial%0Aproblem-solving%2C%20with%20generalized%20improvements%20in%20visual-spatial%20tasks.%20To%0Ainvestigate%20this%20hypothesis%2C%20we%20introduce%20Sparkle%2C%20a%20framework%20that%20fine-tunes%0AVLMs%20on%20these%20three%20basic%20spatial%20capabilities%20by%20synthetic%20data%20generation%20and%0Atargeted%20supervision%20to%20form%20an%20instruction%20dataset%20for%20each%20capability.%20Our%0Aexperiments%20demonstrate%20that%20VLMs%20fine-tuned%20with%20Sparkle%20achieve%20significant%0Aperformance%20gains%2C%20not%20only%20in%20the%20basic%20tasks%20themselves%20but%20also%20in%0Ageneralizing%20to%20composite%20and%20out-of-distribution%20spatial%20reasoning%20tasks.%0AThese%20findings%20underscore%20the%20effectiveness%20of%20mastering%20basic%20spatial%0Acapabilities%20in%20enhancing%20composite%20spatial%20problem-solving%2C%20offering%20insights%0Ainto%20systematic%20strategies%20for%20improving%20VLMs%27%20spatial%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16162v2&entry.124074799=Read"},
{"title": "RestorerID: Towards Tuning-Free Face Restoration with ID Preservation", "author": "Jiacheng Ying and Mushui Liu and Zhe Wu and Runming Zhang and Zhu Yu and Siming Fu and Si-Yuan Cao and Chao Wu and Yunlong Yu and Hui-Liang Shen", "abstract": "  Blind face restoration has made great progress in producing high-quality and\nlifelike images. Yet it remains challenging to preserve the ID information\nespecially when the degradation is heavy. Current reference-guided face\nrestoration approaches either require face alignment or personalized\ntest-tuning, which are unfaithful or time-consuming. In this paper, we propose\na tuning-free method named RestorerID that incorporates ID preservation during\nface restoration. RestorerID is a diffusion model-based method that restores\nlow-quality images with varying levels of degradation by using a single\nreference image. To achieve this, we propose a unified framework to combine the\nID injection with the base blind face restoration model. In addition, we design\na novel Face ID Rebalancing Adapter (FIR-Adapter) to tackle the problems of\ncontent unconsistency and contours misalignment that are caused by information\nconflicts between the low-quality input and reference image. Furthermore, by\nemploying an Adaptive ID-Scale Adjusting strategy, RestorerID can produce\nsuperior restored images across various levels of degradation. Experimental\nresults on the Celeb-Ref dataset and real-world scenarios demonstrate that\nRestorerID effectively delivers high-quality face restoration with ID\npreservation, achieving a superior performance compared to the test-tuning\napproaches and other reference-guided ones. The code of RestorerID is available\nat \\url{https://github.com/YingJiacheng/RestorerID}.\n", "link": "http://arxiv.org/abs/2411.14125v1", "date": "2024-11-21", "relevancy": 2.8116, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RestorerID%3A%20Towards%20Tuning-Free%20Face%20Restoration%20with%20ID%20Preservation&body=Title%3A%20RestorerID%3A%20Towards%20Tuning-Free%20Face%20Restoration%20with%20ID%20Preservation%0AAuthor%3A%20Jiacheng%20Ying%20and%20Mushui%20Liu%20and%20Zhe%20Wu%20and%20Runming%20Zhang%20and%20Zhu%20Yu%20and%20Siming%20Fu%20and%20Si-Yuan%20Cao%20and%20Chao%20Wu%20and%20Yunlong%20Yu%20and%20Hui-Liang%20Shen%0AAbstract%3A%20%20%20Blind%20face%20restoration%20has%20made%20great%20progress%20in%20producing%20high-quality%20and%0Alifelike%20images.%20Yet%20it%20remains%20challenging%20to%20preserve%20the%20ID%20information%0Aespecially%20when%20the%20degradation%20is%20heavy.%20Current%20reference-guided%20face%0Arestoration%20approaches%20either%20require%20face%20alignment%20or%20personalized%0Atest-tuning%2C%20which%20are%20unfaithful%20or%20time-consuming.%20In%20this%20paper%2C%20we%20propose%0Aa%20tuning-free%20method%20named%20RestorerID%20that%20incorporates%20ID%20preservation%20during%0Aface%20restoration.%20RestorerID%20is%20a%20diffusion%20model-based%20method%20that%20restores%0Alow-quality%20images%20with%20varying%20levels%20of%20degradation%20by%20using%20a%20single%0Areference%20image.%20To%20achieve%20this%2C%20we%20propose%20a%20unified%20framework%20to%20combine%20the%0AID%20injection%20with%20the%20base%20blind%20face%20restoration%20model.%20In%20addition%2C%20we%20design%0Aa%20novel%20Face%20ID%20Rebalancing%20Adapter%20%28FIR-Adapter%29%20to%20tackle%20the%20problems%20of%0Acontent%20unconsistency%20and%20contours%20misalignment%20that%20are%20caused%20by%20information%0Aconflicts%20between%20the%20low-quality%20input%20and%20reference%20image.%20Furthermore%2C%20by%0Aemploying%20an%20Adaptive%20ID-Scale%20Adjusting%20strategy%2C%20RestorerID%20can%20produce%0Asuperior%20restored%20images%20across%20various%20levels%20of%20degradation.%20Experimental%0Aresults%20on%20the%20Celeb-Ref%20dataset%20and%20real-world%20scenarios%20demonstrate%20that%0ARestorerID%20effectively%20delivers%20high-quality%20face%20restoration%20with%20ID%0Apreservation%2C%20achieving%20a%20superior%20performance%20compared%20to%20the%20test-tuning%0Aapproaches%20and%20other%20reference-guided%20ones.%20The%20code%20of%20RestorerID%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/YingJiacheng/RestorerID%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestorerID%253A%2520Towards%2520Tuning-Free%2520Face%2520Restoration%2520with%2520ID%2520Preservation%26entry.906535625%3DJiacheng%2520Ying%2520and%2520Mushui%2520Liu%2520and%2520Zhe%2520Wu%2520and%2520Runming%2520Zhang%2520and%2520Zhu%2520Yu%2520and%2520Siming%2520Fu%2520and%2520Si-Yuan%2520Cao%2520and%2520Chao%2520Wu%2520and%2520Yunlong%2520Yu%2520and%2520Hui-Liang%2520Shen%26entry.1292438233%3D%2520%2520Blind%2520face%2520restoration%2520has%2520made%2520great%2520progress%2520in%2520producing%2520high-quality%2520and%250Alifelike%2520images.%2520Yet%2520it%2520remains%2520challenging%2520to%2520preserve%2520the%2520ID%2520information%250Aespecially%2520when%2520the%2520degradation%2520is%2520heavy.%2520Current%2520reference-guided%2520face%250Arestoration%2520approaches%2520either%2520require%2520face%2520alignment%2520or%2520personalized%250Atest-tuning%252C%2520which%2520are%2520unfaithful%2520or%2520time-consuming.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520tuning-free%2520method%2520named%2520RestorerID%2520that%2520incorporates%2520ID%2520preservation%2520during%250Aface%2520restoration.%2520RestorerID%2520is%2520a%2520diffusion%2520model-based%2520method%2520that%2520restores%250Alow-quality%2520images%2520with%2520varying%2520levels%2520of%2520degradation%2520by%2520using%2520a%2520single%250Areference%2520image.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520unified%2520framework%2520to%2520combine%2520the%250AID%2520injection%2520with%2520the%2520base%2520blind%2520face%2520restoration%2520model.%2520In%2520addition%252C%2520we%2520design%250Aa%2520novel%2520Face%2520ID%2520Rebalancing%2520Adapter%2520%2528FIR-Adapter%2529%2520to%2520tackle%2520the%2520problems%2520of%250Acontent%2520unconsistency%2520and%2520contours%2520misalignment%2520that%2520are%2520caused%2520by%2520information%250Aconflicts%2520between%2520the%2520low-quality%2520input%2520and%2520reference%2520image.%2520Furthermore%252C%2520by%250Aemploying%2520an%2520Adaptive%2520ID-Scale%2520Adjusting%2520strategy%252C%2520RestorerID%2520can%2520produce%250Asuperior%2520restored%2520images%2520across%2520various%2520levels%2520of%2520degradation.%2520Experimental%250Aresults%2520on%2520the%2520Celeb-Ref%2520dataset%2520and%2520real-world%2520scenarios%2520demonstrate%2520that%250ARestorerID%2520effectively%2520delivers%2520high-quality%2520face%2520restoration%2520with%2520ID%250Apreservation%252C%2520achieving%2520a%2520superior%2520performance%2520compared%2520to%2520the%2520test-tuning%250Aapproaches%2520and%2520other%2520reference-guided%2520ones.%2520The%2520code%2520of%2520RestorerID%2520is%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/YingJiacheng/RestorerID%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RestorerID%3A%20Towards%20Tuning-Free%20Face%20Restoration%20with%20ID%20Preservation&entry.906535625=Jiacheng%20Ying%20and%20Mushui%20Liu%20and%20Zhe%20Wu%20and%20Runming%20Zhang%20and%20Zhu%20Yu%20and%20Siming%20Fu%20and%20Si-Yuan%20Cao%20and%20Chao%20Wu%20and%20Yunlong%20Yu%20and%20Hui-Liang%20Shen&entry.1292438233=%20%20Blind%20face%20restoration%20has%20made%20great%20progress%20in%20producing%20high-quality%20and%0Alifelike%20images.%20Yet%20it%20remains%20challenging%20to%20preserve%20the%20ID%20information%0Aespecially%20when%20the%20degradation%20is%20heavy.%20Current%20reference-guided%20face%0Arestoration%20approaches%20either%20require%20face%20alignment%20or%20personalized%0Atest-tuning%2C%20which%20are%20unfaithful%20or%20time-consuming.%20In%20this%20paper%2C%20we%20propose%0Aa%20tuning-free%20method%20named%20RestorerID%20that%20incorporates%20ID%20preservation%20during%0Aface%20restoration.%20RestorerID%20is%20a%20diffusion%20model-based%20method%20that%20restores%0Alow-quality%20images%20with%20varying%20levels%20of%20degradation%20by%20using%20a%20single%0Areference%20image.%20To%20achieve%20this%2C%20we%20propose%20a%20unified%20framework%20to%20combine%20the%0AID%20injection%20with%20the%20base%20blind%20face%20restoration%20model.%20In%20addition%2C%20we%20design%0Aa%20novel%20Face%20ID%20Rebalancing%20Adapter%20%28FIR-Adapter%29%20to%20tackle%20the%20problems%20of%0Acontent%20unconsistency%20and%20contours%20misalignment%20that%20are%20caused%20by%20information%0Aconflicts%20between%20the%20low-quality%20input%20and%20reference%20image.%20Furthermore%2C%20by%0Aemploying%20an%20Adaptive%20ID-Scale%20Adjusting%20strategy%2C%20RestorerID%20can%20produce%0Asuperior%20restored%20images%20across%20various%20levels%20of%20degradation.%20Experimental%0Aresults%20on%20the%20Celeb-Ref%20dataset%20and%20real-world%20scenarios%20demonstrate%20that%0ARestorerID%20effectively%20delivers%20high-quality%20face%20restoration%20with%20ID%0Apreservation%2C%20achieving%20a%20superior%20performance%20compared%20to%20the%20test-tuning%0Aapproaches%20and%20other%20reference-guided%20ones.%20The%20code%20of%20RestorerID%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/YingJiacheng/RestorerID%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14125v1&entry.124074799=Read"},
{"title": "Point Cloud Denoising With Fine-Granularity Dynamic Graph Convolutional\n  Networks", "author": "Wenqiang Xu and Wenrui Dai and Duoduo Xue and Ziyang Zheng and Chenglin Li and Junni Zou and Hongkai Xiong", "abstract": "  Due to limitations in acquisition equipment, noise perturbations often\ncorrupt 3-D point clouds, hindering down-stream tasks such as surface\nreconstruction, rendering, and further processing. Existing 3-D point cloud\ndenoising methods typically fail to reliably fit the underlying continuous\nsurface, resulting in a degradation of reconstruction performance. This paper\nintroduces fine-granularity dynamic graph convolutional networks called GD-GCN,\na novel approach to denoising in 3-D point clouds. The GD-GCN employs\nmicro-step temporal graph convolution (MST-GConv) to perform feature learning\nin a gradual manner. Compared with the conventional GCN, which commonly uses\ndiscrete integer-step graph convolution, this modification introduces a more\nadaptable and nuanced approach to feature learning within graph convolution\nnetworks. It more accurately depicts the process of fitting the point cloud\nwith noise to the underlying surface by and the learning process for MST-GConv\nacts like a changing system and is managed through a type of neural network\nknown as neural Partial Differential Equations (PDEs). This means it can adapt\nand improve over time. GD-GCN approximates the Riemannian metric, calculating\ndistances between points along a low-dimensional manifold. This capability\nallows it to understand the local geometric structure and effectively capture\ndiverse relationships between points from different geometric regions through\ngeometric graph construction based on Riemannian distances. Additionally,\nGD-GCN incorporates robust graph spectral filters based on the Bernstein\npolynomial approximation, which modulate eigenvalues for complex and arbitrary\nspectral responses, providing theoretical guarantees for BIBO stability.\nSymmetric channel mixing matrices further enhance filter flexibility by\nenabling channel-level scaling and shifting in the spectral domain.\n", "link": "http://arxiv.org/abs/2411.14158v1", "date": "2024-11-21", "relevancy": 2.8022, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6051}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.54}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Denoising%20With%20Fine-Granularity%20Dynamic%20Graph%20Convolutional%0A%20%20Networks&body=Title%3A%20Point%20Cloud%20Denoising%20With%20Fine-Granularity%20Dynamic%20Graph%20Convolutional%0A%20%20Networks%0AAuthor%3A%20Wenqiang%20Xu%20and%20Wenrui%20Dai%20and%20Duoduo%20Xue%20and%20Ziyang%20Zheng%20and%20Chenglin%20Li%20and%20Junni%20Zou%20and%20Hongkai%20Xiong%0AAbstract%3A%20%20%20Due%20to%20limitations%20in%20acquisition%20equipment%2C%20noise%20perturbations%20often%0Acorrupt%203-D%20point%20clouds%2C%20hindering%20down-stream%20tasks%20such%20as%20surface%0Areconstruction%2C%20rendering%2C%20and%20further%20processing.%20Existing%203-D%20point%20cloud%0Adenoising%20methods%20typically%20fail%20to%20reliably%20fit%20the%20underlying%20continuous%0Asurface%2C%20resulting%20in%20a%20degradation%20of%20reconstruction%20performance.%20This%20paper%0Aintroduces%20fine-granularity%20dynamic%20graph%20convolutional%20networks%20called%20GD-GCN%2C%0Aa%20novel%20approach%20to%20denoising%20in%203-D%20point%20clouds.%20The%20GD-GCN%20employs%0Amicro-step%20temporal%20graph%20convolution%20%28MST-GConv%29%20to%20perform%20feature%20learning%0Ain%20a%20gradual%20manner.%20Compared%20with%20the%20conventional%20GCN%2C%20which%20commonly%20uses%0Adiscrete%20integer-step%20graph%20convolution%2C%20this%20modification%20introduces%20a%20more%0Aadaptable%20and%20nuanced%20approach%20to%20feature%20learning%20within%20graph%20convolution%0Anetworks.%20It%20more%20accurately%20depicts%20the%20process%20of%20fitting%20the%20point%20cloud%0Awith%20noise%20to%20the%20underlying%20surface%20by%20and%20the%20learning%20process%20for%20MST-GConv%0Aacts%20like%20a%20changing%20system%20and%20is%20managed%20through%20a%20type%20of%20neural%20network%0Aknown%20as%20neural%20Partial%20Differential%20Equations%20%28PDEs%29.%20This%20means%20it%20can%20adapt%0Aand%20improve%20over%20time.%20GD-GCN%20approximates%20the%20Riemannian%20metric%2C%20calculating%0Adistances%20between%20points%20along%20a%20low-dimensional%20manifold.%20This%20capability%0Aallows%20it%20to%20understand%20the%20local%20geometric%20structure%20and%20effectively%20capture%0Adiverse%20relationships%20between%20points%20from%20different%20geometric%20regions%20through%0Ageometric%20graph%20construction%20based%20on%20Riemannian%20distances.%20Additionally%2C%0AGD-GCN%20incorporates%20robust%20graph%20spectral%20filters%20based%20on%20the%20Bernstein%0Apolynomial%20approximation%2C%20which%20modulate%20eigenvalues%20for%20complex%20and%20arbitrary%0Aspectral%20responses%2C%20providing%20theoretical%20guarantees%20for%20BIBO%20stability.%0ASymmetric%20channel%20mixing%20matrices%20further%20enhance%20filter%20flexibility%20by%0Aenabling%20channel-level%20scaling%20and%20shifting%20in%20the%20spectral%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Cloud%2520Denoising%2520With%2520Fine-Granularity%2520Dynamic%2520Graph%2520Convolutional%250A%2520%2520Networks%26entry.906535625%3DWenqiang%2520Xu%2520and%2520Wenrui%2520Dai%2520and%2520Duoduo%2520Xue%2520and%2520Ziyang%2520Zheng%2520and%2520Chenglin%2520Li%2520and%2520Junni%2520Zou%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3D%2520%2520Due%2520to%2520limitations%2520in%2520acquisition%2520equipment%252C%2520noise%2520perturbations%2520often%250Acorrupt%25203-D%2520point%2520clouds%252C%2520hindering%2520down-stream%2520tasks%2520such%2520as%2520surface%250Areconstruction%252C%2520rendering%252C%2520and%2520further%2520processing.%2520Existing%25203-D%2520point%2520cloud%250Adenoising%2520methods%2520typically%2520fail%2520to%2520reliably%2520fit%2520the%2520underlying%2520continuous%250Asurface%252C%2520resulting%2520in%2520a%2520degradation%2520of%2520reconstruction%2520performance.%2520This%2520paper%250Aintroduces%2520fine-granularity%2520dynamic%2520graph%2520convolutional%2520networks%2520called%2520GD-GCN%252C%250Aa%2520novel%2520approach%2520to%2520denoising%2520in%25203-D%2520point%2520clouds.%2520The%2520GD-GCN%2520employs%250Amicro-step%2520temporal%2520graph%2520convolution%2520%2528MST-GConv%2529%2520to%2520perform%2520feature%2520learning%250Ain%2520a%2520gradual%2520manner.%2520Compared%2520with%2520the%2520conventional%2520GCN%252C%2520which%2520commonly%2520uses%250Adiscrete%2520integer-step%2520graph%2520convolution%252C%2520this%2520modification%2520introduces%2520a%2520more%250Aadaptable%2520and%2520nuanced%2520approach%2520to%2520feature%2520learning%2520within%2520graph%2520convolution%250Anetworks.%2520It%2520more%2520accurately%2520depicts%2520the%2520process%2520of%2520fitting%2520the%2520point%2520cloud%250Awith%2520noise%2520to%2520the%2520underlying%2520surface%2520by%2520and%2520the%2520learning%2520process%2520for%2520MST-GConv%250Aacts%2520like%2520a%2520changing%2520system%2520and%2520is%2520managed%2520through%2520a%2520type%2520of%2520neural%2520network%250Aknown%2520as%2520neural%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529.%2520This%2520means%2520it%2520can%2520adapt%250Aand%2520improve%2520over%2520time.%2520GD-GCN%2520approximates%2520the%2520Riemannian%2520metric%252C%2520calculating%250Adistances%2520between%2520points%2520along%2520a%2520low-dimensional%2520manifold.%2520This%2520capability%250Aallows%2520it%2520to%2520understand%2520the%2520local%2520geometric%2520structure%2520and%2520effectively%2520capture%250Adiverse%2520relationships%2520between%2520points%2520from%2520different%2520geometric%2520regions%2520through%250Ageometric%2520graph%2520construction%2520based%2520on%2520Riemannian%2520distances.%2520Additionally%252C%250AGD-GCN%2520incorporates%2520robust%2520graph%2520spectral%2520filters%2520based%2520on%2520the%2520Bernstein%250Apolynomial%2520approximation%252C%2520which%2520modulate%2520eigenvalues%2520for%2520complex%2520and%2520arbitrary%250Aspectral%2520responses%252C%2520providing%2520theoretical%2520guarantees%2520for%2520BIBO%2520stability.%250ASymmetric%2520channel%2520mixing%2520matrices%2520further%2520enhance%2520filter%2520flexibility%2520by%250Aenabling%2520channel-level%2520scaling%2520and%2520shifting%2520in%2520the%2520spectral%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Denoising%20With%20Fine-Granularity%20Dynamic%20Graph%20Convolutional%0A%20%20Networks&entry.906535625=Wenqiang%20Xu%20and%20Wenrui%20Dai%20and%20Duoduo%20Xue%20and%20Ziyang%20Zheng%20and%20Chenglin%20Li%20and%20Junni%20Zou%20and%20Hongkai%20Xiong&entry.1292438233=%20%20Due%20to%20limitations%20in%20acquisition%20equipment%2C%20noise%20perturbations%20often%0Acorrupt%203-D%20point%20clouds%2C%20hindering%20down-stream%20tasks%20such%20as%20surface%0Areconstruction%2C%20rendering%2C%20and%20further%20processing.%20Existing%203-D%20point%20cloud%0Adenoising%20methods%20typically%20fail%20to%20reliably%20fit%20the%20underlying%20continuous%0Asurface%2C%20resulting%20in%20a%20degradation%20of%20reconstruction%20performance.%20This%20paper%0Aintroduces%20fine-granularity%20dynamic%20graph%20convolutional%20networks%20called%20GD-GCN%2C%0Aa%20novel%20approach%20to%20denoising%20in%203-D%20point%20clouds.%20The%20GD-GCN%20employs%0Amicro-step%20temporal%20graph%20convolution%20%28MST-GConv%29%20to%20perform%20feature%20learning%0Ain%20a%20gradual%20manner.%20Compared%20with%20the%20conventional%20GCN%2C%20which%20commonly%20uses%0Adiscrete%20integer-step%20graph%20convolution%2C%20this%20modification%20introduces%20a%20more%0Aadaptable%20and%20nuanced%20approach%20to%20feature%20learning%20within%20graph%20convolution%0Anetworks.%20It%20more%20accurately%20depicts%20the%20process%20of%20fitting%20the%20point%20cloud%0Awith%20noise%20to%20the%20underlying%20surface%20by%20and%20the%20learning%20process%20for%20MST-GConv%0Aacts%20like%20a%20changing%20system%20and%20is%20managed%20through%20a%20type%20of%20neural%20network%0Aknown%20as%20neural%20Partial%20Differential%20Equations%20%28PDEs%29.%20This%20means%20it%20can%20adapt%0Aand%20improve%20over%20time.%20GD-GCN%20approximates%20the%20Riemannian%20metric%2C%20calculating%0Adistances%20between%20points%20along%20a%20low-dimensional%20manifold.%20This%20capability%0Aallows%20it%20to%20understand%20the%20local%20geometric%20structure%20and%20effectively%20capture%0Adiverse%20relationships%20between%20points%20from%20different%20geometric%20regions%20through%0Ageometric%20graph%20construction%20based%20on%20Riemannian%20distances.%20Additionally%2C%0AGD-GCN%20incorporates%20robust%20graph%20spectral%20filters%20based%20on%20the%20Bernstein%0Apolynomial%20approximation%2C%20which%20modulate%20eigenvalues%20for%20complex%20and%20arbitrary%0Aspectral%20responses%2C%20providing%20theoretical%20guarantees%20for%20BIBO%20stability.%0ASymmetric%20channel%20mixing%20matrices%20further%20enhance%20filter%20flexibility%20by%0Aenabling%20channel-level%20scaling%20and%20shifting%20in%20the%20spectral%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14158v1&entry.124074799=Read"},
{"title": "CompetitorFormer: Competitor Transformer for 3D Instance Segmentation", "author": "Duanchu Wang and Jing Liu and Haoran Gong and Yinghui Quan and Di Wang", "abstract": "  Transformer-based methods have become the dominant approach for 3D instance\nsegmentation. These methods predict instance masks via instance queries,\nranking them by classification confidence and IoU scores to select the top\nprediction as the final outcome. However, it has been observed that the current\nmodels employ a fixed and higher number of queries than the instances present\nwithin a scene. In such instances, multiple queries predict the same instance,\nyet only a single query is ultimately optimized. The close scores of queries in\nthe lower-level decoders make it challenging for the dominant query to\ndistinguish itself rapidly, which ultimately impairs the model's accuracy and\nconvergence efficiency. This phenomenon is referred to as inter-query\ncompetition. To address this challenge, we put forth a series of plug-and-play\ncompetition-oriented designs, collectively designated as the CompetitorFormer,\nwith the aim of reducing competition and facilitating a dominant query.\nExperiments showed that integrating our designs with state-of-the-art\nframeworks consistently resulted in significant performance improvements in 3D\ninstance segmentation across a range of datasets.\n", "link": "http://arxiv.org/abs/2411.14179v1", "date": "2024-11-21", "relevancy": 2.7683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompetitorFormer%3A%20Competitor%20Transformer%20for%203D%20Instance%20Segmentation&body=Title%3A%20CompetitorFormer%3A%20Competitor%20Transformer%20for%203D%20Instance%20Segmentation%0AAuthor%3A%20Duanchu%20Wang%20and%20Jing%20Liu%20and%20Haoran%20Gong%20and%20Yinghui%20Quan%20and%20Di%20Wang%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20become%20the%20dominant%20approach%20for%203D%20instance%0Asegmentation.%20These%20methods%20predict%20instance%20masks%20via%20instance%20queries%2C%0Aranking%20them%20by%20classification%20confidence%20and%20IoU%20scores%20to%20select%20the%20top%0Aprediction%20as%20the%20final%20outcome.%20However%2C%20it%20has%20been%20observed%20that%20the%20current%0Amodels%20employ%20a%20fixed%20and%20higher%20number%20of%20queries%20than%20the%20instances%20present%0Awithin%20a%20scene.%20In%20such%20instances%2C%20multiple%20queries%20predict%20the%20same%20instance%2C%0Ayet%20only%20a%20single%20query%20is%20ultimately%20optimized.%20The%20close%20scores%20of%20queries%20in%0Athe%20lower-level%20decoders%20make%20it%20challenging%20for%20the%20dominant%20query%20to%0Adistinguish%20itself%20rapidly%2C%20which%20ultimately%20impairs%20the%20model%27s%20accuracy%20and%0Aconvergence%20efficiency.%20This%20phenomenon%20is%20referred%20to%20as%20inter-query%0Acompetition.%20To%20address%20this%20challenge%2C%20we%20put%20forth%20a%20series%20of%20plug-and-play%0Acompetition-oriented%20designs%2C%20collectively%20designated%20as%20the%20CompetitorFormer%2C%0Awith%20the%20aim%20of%20reducing%20competition%20and%20facilitating%20a%20dominant%20query.%0AExperiments%20showed%20that%20integrating%20our%20designs%20with%20state-of-the-art%0Aframeworks%20consistently%20resulted%20in%20significant%20performance%20improvements%20in%203D%0Ainstance%20segmentation%20across%20a%20range%20of%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompetitorFormer%253A%2520Competitor%2520Transformer%2520for%25203D%2520Instance%2520Segmentation%26entry.906535625%3DDuanchu%2520Wang%2520and%2520Jing%2520Liu%2520and%2520Haoran%2520Gong%2520and%2520Yinghui%2520Quan%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520become%2520the%2520dominant%2520approach%2520for%25203D%2520instance%250Asegmentation.%2520These%2520methods%2520predict%2520instance%2520masks%2520via%2520instance%2520queries%252C%250Aranking%2520them%2520by%2520classification%2520confidence%2520and%2520IoU%2520scores%2520to%2520select%2520the%2520top%250Aprediction%2520as%2520the%2520final%2520outcome.%2520However%252C%2520it%2520has%2520been%2520observed%2520that%2520the%2520current%250Amodels%2520employ%2520a%2520fixed%2520and%2520higher%2520number%2520of%2520queries%2520than%2520the%2520instances%2520present%250Awithin%2520a%2520scene.%2520In%2520such%2520instances%252C%2520multiple%2520queries%2520predict%2520the%2520same%2520instance%252C%250Ayet%2520only%2520a%2520single%2520query%2520is%2520ultimately%2520optimized.%2520The%2520close%2520scores%2520of%2520queries%2520in%250Athe%2520lower-level%2520decoders%2520make%2520it%2520challenging%2520for%2520the%2520dominant%2520query%2520to%250Adistinguish%2520itself%2520rapidly%252C%2520which%2520ultimately%2520impairs%2520the%2520model%2527s%2520accuracy%2520and%250Aconvergence%2520efficiency.%2520This%2520phenomenon%2520is%2520referred%2520to%2520as%2520inter-query%250Acompetition.%2520To%2520address%2520this%2520challenge%252C%2520we%2520put%2520forth%2520a%2520series%2520of%2520plug-and-play%250Acompetition-oriented%2520designs%252C%2520collectively%2520designated%2520as%2520the%2520CompetitorFormer%252C%250Awith%2520the%2520aim%2520of%2520reducing%2520competition%2520and%2520facilitating%2520a%2520dominant%2520query.%250AExperiments%2520showed%2520that%2520integrating%2520our%2520designs%2520with%2520state-of-the-art%250Aframeworks%2520consistently%2520resulted%2520in%2520significant%2520performance%2520improvements%2520in%25203D%250Ainstance%2520segmentation%2520across%2520a%2520range%2520of%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompetitorFormer%3A%20Competitor%20Transformer%20for%203D%20Instance%20Segmentation&entry.906535625=Duanchu%20Wang%20and%20Jing%20Liu%20and%20Haoran%20Gong%20and%20Yinghui%20Quan%20and%20Di%20Wang&entry.1292438233=%20%20Transformer-based%20methods%20have%20become%20the%20dominant%20approach%20for%203D%20instance%0Asegmentation.%20These%20methods%20predict%20instance%20masks%20via%20instance%20queries%2C%0Aranking%20them%20by%20classification%20confidence%20and%20IoU%20scores%20to%20select%20the%20top%0Aprediction%20as%20the%20final%20outcome.%20However%2C%20it%20has%20been%20observed%20that%20the%20current%0Amodels%20employ%20a%20fixed%20and%20higher%20number%20of%20queries%20than%20the%20instances%20present%0Awithin%20a%20scene.%20In%20such%20instances%2C%20multiple%20queries%20predict%20the%20same%20instance%2C%0Ayet%20only%20a%20single%20query%20is%20ultimately%20optimized.%20The%20close%20scores%20of%20queries%20in%0Athe%20lower-level%20decoders%20make%20it%20challenging%20for%20the%20dominant%20query%20to%0Adistinguish%20itself%20rapidly%2C%20which%20ultimately%20impairs%20the%20model%27s%20accuracy%20and%0Aconvergence%20efficiency.%20This%20phenomenon%20is%20referred%20to%20as%20inter-query%0Acompetition.%20To%20address%20this%20challenge%2C%20we%20put%20forth%20a%20series%20of%20plug-and-play%0Acompetition-oriented%20designs%2C%20collectively%20designated%20as%20the%20CompetitorFormer%2C%0Awith%20the%20aim%20of%20reducing%20competition%20and%20facilitating%20a%20dominant%20query.%0AExperiments%20showed%20that%20integrating%20our%20designs%20with%20state-of-the-art%0Aframeworks%20consistently%20resulted%20in%20significant%20performance%20improvements%20in%203D%0Ainstance%20segmentation%20across%20a%20range%20of%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14179v1&entry.124074799=Read"},
{"title": "ViSTa Dataset: Do vision-language models understand sequential tasks?", "author": "Ev\u017een Wybitul and Evan Ryan Gunter and Mikhail Seleznyov and David Lindner", "abstract": "  Using vision-language models (VLMs) as reward models in reinforcement\nlearning holds promise for reducing costs and improving safety. So far, VLM\nreward models have only been used for goal-oriented tasks, where the agent must\nreach a particular final outcome. We explore VLMs' potential to supervise tasks\nthat cannot be scored by the final state alone. To this end, we introduce\nViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.\nViSTa comprises over 4,000 videos with step-by-step descriptions in virtual\nhome, Minecraft, and real-world environments. Its novel hierarchical structure\n-- basic single-step tasks composed into more and more complex sequential tasks\n-- allows a fine-grained understanding of how well VLMs can judge tasks with\nvarying complexity. To illustrate this, we use ViSTa to evaluate\nstate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while\nthey are all good at object recognition, they fail to understand sequential\ntasks, with only GPT-4o achieving non-trivial performance.\n", "link": "http://arxiv.org/abs/2411.13211v2", "date": "2024-11-21", "relevancy": 2.7655, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSTa%20Dataset%3A%20Do%20vision-language%20models%20understand%20sequential%20tasks%3F&body=Title%3A%20ViSTa%20Dataset%3A%20Do%20vision-language%20models%20understand%20sequential%20tasks%3F%0AAuthor%3A%20Ev%C5%BEen%20Wybitul%20and%20Evan%20Ryan%20Gunter%20and%20Mikhail%20Seleznyov%20and%20David%20Lindner%0AAbstract%3A%20%20%20Using%20vision-language%20models%20%28VLMs%29%20as%20reward%20models%20in%20reinforcement%0Alearning%20holds%20promise%20for%20reducing%20costs%20and%20improving%20safety.%20So%20far%2C%20VLM%0Areward%20models%20have%20only%20been%20used%20for%20goal-oriented%20tasks%2C%20where%20the%20agent%20must%0Areach%20a%20particular%20final%20outcome.%20We%20explore%20VLMs%27%20potential%20to%20supervise%20tasks%0Athat%20cannot%20be%20scored%20by%20the%20final%20state%20alone.%20To%20this%20end%2C%20we%20introduce%0AViSTa%2C%20a%20dataset%20for%20evaluating%20Vision-based%20understanding%20of%20Sequential%20Tasks.%0AViSTa%20comprises%20over%204%2C000%20videos%20with%20step-by-step%20descriptions%20in%20virtual%0Ahome%2C%20Minecraft%2C%20and%20real-world%20environments.%20Its%20novel%20hierarchical%20structure%0A--%20basic%20single-step%20tasks%20composed%20into%20more%20and%20more%20complex%20sequential%20tasks%0A--%20allows%20a%20fine-grained%20understanding%20of%20how%20well%20VLMs%20can%20judge%20tasks%20with%0Avarying%20complexity.%20To%20illustrate%20this%2C%20we%20use%20ViSTa%20to%20evaluate%0Astate-of-the-art%20VLMs%2C%20including%20CLIP%2C%20ViCLIP%2C%20and%20GPT-4o.%20We%20find%20that%2C%20while%0Athey%20are%20all%20good%20at%20object%20recognition%2C%20they%20fail%20to%20understand%20sequential%0Atasks%2C%20with%20only%20GPT-4o%20achieving%20non-trivial%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSTa%2520Dataset%253A%2520Do%2520vision-language%2520models%2520understand%2520sequential%2520tasks%253F%26entry.906535625%3DEv%25C5%25BEen%2520Wybitul%2520and%2520Evan%2520Ryan%2520Gunter%2520and%2520Mikhail%2520Seleznyov%2520and%2520David%2520Lindner%26entry.1292438233%3D%2520%2520Using%2520vision-language%2520models%2520%2528VLMs%2529%2520as%2520reward%2520models%2520in%2520reinforcement%250Alearning%2520holds%2520promise%2520for%2520reducing%2520costs%2520and%2520improving%2520safety.%2520So%2520far%252C%2520VLM%250Areward%2520models%2520have%2520only%2520been%2520used%2520for%2520goal-oriented%2520tasks%252C%2520where%2520the%2520agent%2520must%250Areach%2520a%2520particular%2520final%2520outcome.%2520We%2520explore%2520VLMs%2527%2520potential%2520to%2520supervise%2520tasks%250Athat%2520cannot%2520be%2520scored%2520by%2520the%2520final%2520state%2520alone.%2520To%2520this%2520end%252C%2520we%2520introduce%250AViSTa%252C%2520a%2520dataset%2520for%2520evaluating%2520Vision-based%2520understanding%2520of%2520Sequential%2520Tasks.%250AViSTa%2520comprises%2520over%25204%252C000%2520videos%2520with%2520step-by-step%2520descriptions%2520in%2520virtual%250Ahome%252C%2520Minecraft%252C%2520and%2520real-world%2520environments.%2520Its%2520novel%2520hierarchical%2520structure%250A--%2520basic%2520single-step%2520tasks%2520composed%2520into%2520more%2520and%2520more%2520complex%2520sequential%2520tasks%250A--%2520allows%2520a%2520fine-grained%2520understanding%2520of%2520how%2520well%2520VLMs%2520can%2520judge%2520tasks%2520with%250Avarying%2520complexity.%2520To%2520illustrate%2520this%252C%2520we%2520use%2520ViSTa%2520to%2520evaluate%250Astate-of-the-art%2520VLMs%252C%2520including%2520CLIP%252C%2520ViCLIP%252C%2520and%2520GPT-4o.%2520We%2520find%2520that%252C%2520while%250Athey%2520are%2520all%2520good%2520at%2520object%2520recognition%252C%2520they%2520fail%2520to%2520understand%2520sequential%250Atasks%252C%2520with%2520only%2520GPT-4o%2520achieving%2520non-trivial%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSTa%20Dataset%3A%20Do%20vision-language%20models%20understand%20sequential%20tasks%3F&entry.906535625=Ev%C5%BEen%20Wybitul%20and%20Evan%20Ryan%20Gunter%20and%20Mikhail%20Seleznyov%20and%20David%20Lindner&entry.1292438233=%20%20Using%20vision-language%20models%20%28VLMs%29%20as%20reward%20models%20in%20reinforcement%0Alearning%20holds%20promise%20for%20reducing%20costs%20and%20improving%20safety.%20So%20far%2C%20VLM%0Areward%20models%20have%20only%20been%20used%20for%20goal-oriented%20tasks%2C%20where%20the%20agent%20must%0Areach%20a%20particular%20final%20outcome.%20We%20explore%20VLMs%27%20potential%20to%20supervise%20tasks%0Athat%20cannot%20be%20scored%20by%20the%20final%20state%20alone.%20To%20this%20end%2C%20we%20introduce%0AViSTa%2C%20a%20dataset%20for%20evaluating%20Vision-based%20understanding%20of%20Sequential%20Tasks.%0AViSTa%20comprises%20over%204%2C000%20videos%20with%20step-by-step%20descriptions%20in%20virtual%0Ahome%2C%20Minecraft%2C%20and%20real-world%20environments.%20Its%20novel%20hierarchical%20structure%0A--%20basic%20single-step%20tasks%20composed%20into%20more%20and%20more%20complex%20sequential%20tasks%0A--%20allows%20a%20fine-grained%20understanding%20of%20how%20well%20VLMs%20can%20judge%20tasks%20with%0Avarying%20complexity.%20To%20illustrate%20this%2C%20we%20use%20ViSTa%20to%20evaluate%0Astate-of-the-art%20VLMs%2C%20including%20CLIP%2C%20ViCLIP%2C%20and%20GPT-4o.%20We%20find%20that%2C%20while%0Athey%20are%20all%20good%20at%20object%20recognition%2C%20they%20fail%20to%20understand%20sequential%0Atasks%2C%20with%20only%20GPT-4o%20achieving%20non-trivial%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13211v2&entry.124074799=Read"},
{"title": "Generalizing End-To-End Autonomous Driving In Real-World Environments\n  Using Zero-Shot LLMs", "author": "Zeyu Dong and Yimin Zhu and Yansong Li and Kevin Mahon and Yu Sun", "abstract": "  Traditional autonomous driving methods adopt a modular design, decomposing\ntasks into sub-tasks. In contrast, end-to-end autonomous driving directly\noutputs actions from raw sensor data, avoiding error accumulation. However,\ntraining an end-to-end model requires a comprehensive dataset; otherwise, the\nmodel exhibits poor generalization capabilities. Recently, large language\nmodels (LLMs) have been applied to enhance the generalization capabilities of\nend-to-end driving models. Most studies explore LLMs in an open-loop manner,\nwhere the output actions are compared to those of experts without direct\nfeedback from the real world, while others examine closed-loop results only in\nsimulations. This paper proposes an efficient architecture that integrates\nmultimodal LLMs into end-to-end driving models operating in closed-loop\nsettings in real-world environments. In our architecture, the LLM periodically\nprocesses raw sensor data to generate high-level driving instructions,\neffectively guiding the end-to-end model, even at a slower rate than the raw\nsensor data. This architecture relaxes the trade-off between the latency and\ninference quality of the LLM. It also allows us to choose from a wide variety\nof LLMs to improve high-level driving instructions and minimize fine-tuning\ncosts. Consequently, our architecture reduces data collection requirements\nbecause the LLMs do not directly output actions; we only need to train a simple\nimitation learning model to output actions. In our experiments, the training\ndata for the end-to-end model in a real-world environment consists of only\nsimple obstacle configurations with one traffic cone, while the test\nenvironment is more complex and contains multiple obstacles placed in various\npositions. Experiments show that the proposed architecture enhances the\ngeneralization capabilities of the end-to-end model even without fine-tuning\nthe LLM.\n", "link": "http://arxiv.org/abs/2411.14256v1", "date": "2024-11-21", "relevancy": 2.7565, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.557}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20End-To-End%20Autonomous%20Driving%20In%20Real-World%20Environments%0A%20%20Using%20Zero-Shot%20LLMs&body=Title%3A%20Generalizing%20End-To-End%20Autonomous%20Driving%20In%20Real-World%20Environments%0A%20%20Using%20Zero-Shot%20LLMs%0AAuthor%3A%20Zeyu%20Dong%20and%20Yimin%20Zhu%20and%20Yansong%20Li%20and%20Kevin%20Mahon%20and%20Yu%20Sun%0AAbstract%3A%20%20%20Traditional%20autonomous%20driving%20methods%20adopt%20a%20modular%20design%2C%20decomposing%0Atasks%20into%20sub-tasks.%20In%20contrast%2C%20end-to-end%20autonomous%20driving%20directly%0Aoutputs%20actions%20from%20raw%20sensor%20data%2C%20avoiding%20error%20accumulation.%20However%2C%0Atraining%20an%20end-to-end%20model%20requires%20a%20comprehensive%20dataset%3B%20otherwise%2C%20the%0Amodel%20exhibits%20poor%20generalization%20capabilities.%20Recently%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20been%20applied%20to%20enhance%20the%20generalization%20capabilities%20of%0Aend-to-end%20driving%20models.%20Most%20studies%20explore%20LLMs%20in%20an%20open-loop%20manner%2C%0Awhere%20the%20output%20actions%20are%20compared%20to%20those%20of%20experts%20without%20direct%0Afeedback%20from%20the%20real%20world%2C%20while%20others%20examine%20closed-loop%20results%20only%20in%0Asimulations.%20This%20paper%20proposes%20an%20efficient%20architecture%20that%20integrates%0Amultimodal%20LLMs%20into%20end-to-end%20driving%20models%20operating%20in%20closed-loop%0Asettings%20in%20real-world%20environments.%20In%20our%20architecture%2C%20the%20LLM%20periodically%0Aprocesses%20raw%20sensor%20data%20to%20generate%20high-level%20driving%20instructions%2C%0Aeffectively%20guiding%20the%20end-to-end%20model%2C%20even%20at%20a%20slower%20rate%20than%20the%20raw%0Asensor%20data.%20This%20architecture%20relaxes%20the%20trade-off%20between%20the%20latency%20and%0Ainference%20quality%20of%20the%20LLM.%20It%20also%20allows%20us%20to%20choose%20from%20a%20wide%20variety%0Aof%20LLMs%20to%20improve%20high-level%20driving%20instructions%20and%20minimize%20fine-tuning%0Acosts.%20Consequently%2C%20our%20architecture%20reduces%20data%20collection%20requirements%0Abecause%20the%20LLMs%20do%20not%20directly%20output%20actions%3B%20we%20only%20need%20to%20train%20a%20simple%0Aimitation%20learning%20model%20to%20output%20actions.%20In%20our%20experiments%2C%20the%20training%0Adata%20for%20the%20end-to-end%20model%20in%20a%20real-world%20environment%20consists%20of%20only%0Asimple%20obstacle%20configurations%20with%20one%20traffic%20cone%2C%20while%20the%20test%0Aenvironment%20is%20more%20complex%20and%20contains%20multiple%20obstacles%20placed%20in%20various%0Apositions.%20Experiments%20show%20that%20the%20proposed%20architecture%20enhances%20the%0Ageneralization%20capabilities%20of%20the%20end-to-end%20model%20even%20without%20fine-tuning%0Athe%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520End-To-End%2520Autonomous%2520Driving%2520In%2520Real-World%2520Environments%250A%2520%2520Using%2520Zero-Shot%2520LLMs%26entry.906535625%3DZeyu%2520Dong%2520and%2520Yimin%2520Zhu%2520and%2520Yansong%2520Li%2520and%2520Kevin%2520Mahon%2520and%2520Yu%2520Sun%26entry.1292438233%3D%2520%2520Traditional%2520autonomous%2520driving%2520methods%2520adopt%2520a%2520modular%2520design%252C%2520decomposing%250Atasks%2520into%2520sub-tasks.%2520In%2520contrast%252C%2520end-to-end%2520autonomous%2520driving%2520directly%250Aoutputs%2520actions%2520from%2520raw%2520sensor%2520data%252C%2520avoiding%2520error%2520accumulation.%2520However%252C%250Atraining%2520an%2520end-to-end%2520model%2520requires%2520a%2520comprehensive%2520dataset%253B%2520otherwise%252C%2520the%250Amodel%2520exhibits%2520poor%2520generalization%2520capabilities.%2520Recently%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520been%2520applied%2520to%2520enhance%2520the%2520generalization%2520capabilities%2520of%250Aend-to-end%2520driving%2520models.%2520Most%2520studies%2520explore%2520LLMs%2520in%2520an%2520open-loop%2520manner%252C%250Awhere%2520the%2520output%2520actions%2520are%2520compared%2520to%2520those%2520of%2520experts%2520without%2520direct%250Afeedback%2520from%2520the%2520real%2520world%252C%2520while%2520others%2520examine%2520closed-loop%2520results%2520only%2520in%250Asimulations.%2520This%2520paper%2520proposes%2520an%2520efficient%2520architecture%2520that%2520integrates%250Amultimodal%2520LLMs%2520into%2520end-to-end%2520driving%2520models%2520operating%2520in%2520closed-loop%250Asettings%2520in%2520real-world%2520environments.%2520In%2520our%2520architecture%252C%2520the%2520LLM%2520periodically%250Aprocesses%2520raw%2520sensor%2520data%2520to%2520generate%2520high-level%2520driving%2520instructions%252C%250Aeffectively%2520guiding%2520the%2520end-to-end%2520model%252C%2520even%2520at%2520a%2520slower%2520rate%2520than%2520the%2520raw%250Asensor%2520data.%2520This%2520architecture%2520relaxes%2520the%2520trade-off%2520between%2520the%2520latency%2520and%250Ainference%2520quality%2520of%2520the%2520LLM.%2520It%2520also%2520allows%2520us%2520to%2520choose%2520from%2520a%2520wide%2520variety%250Aof%2520LLMs%2520to%2520improve%2520high-level%2520driving%2520instructions%2520and%2520minimize%2520fine-tuning%250Acosts.%2520Consequently%252C%2520our%2520architecture%2520reduces%2520data%2520collection%2520requirements%250Abecause%2520the%2520LLMs%2520do%2520not%2520directly%2520output%2520actions%253B%2520we%2520only%2520need%2520to%2520train%2520a%2520simple%250Aimitation%2520learning%2520model%2520to%2520output%2520actions.%2520In%2520our%2520experiments%252C%2520the%2520training%250Adata%2520for%2520the%2520end-to-end%2520model%2520in%2520a%2520real-world%2520environment%2520consists%2520of%2520only%250Asimple%2520obstacle%2520configurations%2520with%2520one%2520traffic%2520cone%252C%2520while%2520the%2520test%250Aenvironment%2520is%2520more%2520complex%2520and%2520contains%2520multiple%2520obstacles%2520placed%2520in%2520various%250Apositions.%2520Experiments%2520show%2520that%2520the%2520proposed%2520architecture%2520enhances%2520the%250Ageneralization%2520capabilities%2520of%2520the%2520end-to-end%2520model%2520even%2520without%2520fine-tuning%250Athe%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20End-To-End%20Autonomous%20Driving%20In%20Real-World%20Environments%0A%20%20Using%20Zero-Shot%20LLMs&entry.906535625=Zeyu%20Dong%20and%20Yimin%20Zhu%20and%20Yansong%20Li%20and%20Kevin%20Mahon%20and%20Yu%20Sun&entry.1292438233=%20%20Traditional%20autonomous%20driving%20methods%20adopt%20a%20modular%20design%2C%20decomposing%0Atasks%20into%20sub-tasks.%20In%20contrast%2C%20end-to-end%20autonomous%20driving%20directly%0Aoutputs%20actions%20from%20raw%20sensor%20data%2C%20avoiding%20error%20accumulation.%20However%2C%0Atraining%20an%20end-to-end%20model%20requires%20a%20comprehensive%20dataset%3B%20otherwise%2C%20the%0Amodel%20exhibits%20poor%20generalization%20capabilities.%20Recently%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20been%20applied%20to%20enhance%20the%20generalization%20capabilities%20of%0Aend-to-end%20driving%20models.%20Most%20studies%20explore%20LLMs%20in%20an%20open-loop%20manner%2C%0Awhere%20the%20output%20actions%20are%20compared%20to%20those%20of%20experts%20without%20direct%0Afeedback%20from%20the%20real%20world%2C%20while%20others%20examine%20closed-loop%20results%20only%20in%0Asimulations.%20This%20paper%20proposes%20an%20efficient%20architecture%20that%20integrates%0Amultimodal%20LLMs%20into%20end-to-end%20driving%20models%20operating%20in%20closed-loop%0Asettings%20in%20real-world%20environments.%20In%20our%20architecture%2C%20the%20LLM%20periodically%0Aprocesses%20raw%20sensor%20data%20to%20generate%20high-level%20driving%20instructions%2C%0Aeffectively%20guiding%20the%20end-to-end%20model%2C%20even%20at%20a%20slower%20rate%20than%20the%20raw%0Asensor%20data.%20This%20architecture%20relaxes%20the%20trade-off%20between%20the%20latency%20and%0Ainference%20quality%20of%20the%20LLM.%20It%20also%20allows%20us%20to%20choose%20from%20a%20wide%20variety%0Aof%20LLMs%20to%20improve%20high-level%20driving%20instructions%20and%20minimize%20fine-tuning%0Acosts.%20Consequently%2C%20our%20architecture%20reduces%20data%20collection%20requirements%0Abecause%20the%20LLMs%20do%20not%20directly%20output%20actions%3B%20we%20only%20need%20to%20train%20a%20simple%0Aimitation%20learning%20model%20to%20output%20actions.%20In%20our%20experiments%2C%20the%20training%0Adata%20for%20the%20end-to-end%20model%20in%20a%20real-world%20environment%20consists%20of%20only%0Asimple%20obstacle%20configurations%20with%20one%20traffic%20cone%2C%20while%20the%20test%0Aenvironment%20is%20more%20complex%20and%20contains%20multiple%20obstacles%20placed%20in%20various%0Apositions.%20Experiments%20show%20that%20the%20proposed%20architecture%20enhances%20the%0Ageneralization%20capabilities%20of%20the%20end-to-end%20model%20even%20without%20fine-tuning%0Athe%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14256v1&entry.124074799=Read"},
{"title": "Revisiting the Integration of Convolution and Attention for Vision\n  Backbone", "author": "Lei Zhu and Xinjiang Wang and Wayne Zhang and Rynson W. H. Lau", "abstract": "  Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\nconsidered alternatives to each other for building vision backbones. Although\nsome works try to integrate both, they apply the two operators simultaneously\nat the finest pixel granularity. With Convs responsible for per-pixel feature\nextraction already, the question is whether we still need to include the heavy\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\nscalability issue w.r.t. the input resolution for vision transformers. To\naddress this important problem, we propose in this work to use MSHAs and Convs\nin parallel \\textbf{at different granularity levels} instead. Specifically, in\neach layer, we use two different ways to represent an image: a fine-grained\nregular grid and a coarse-grained set of semantic slots. We apply different\noperations to these two representations: Convs to the grid for local features,\nand MHSAs to the slots for global features. A pair of fully differentiable soft\nclustering and dispatching modules is introduced to bridge the grid and set\nrepresentations, thus enabling local-global fusion. Through extensive\nexperiments on various vision tasks, we empirically verify the potential of the\nproposed integration scheme, named \\textit{GLMix}: by offloading the burden of\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\nfew (e.g., 64) semantic slots to match the performance of recent\nstate-of-the-art backbones, while being more efficient. Our visualization\nresults also demonstrate that the soft clustering module produces a meaningful\nsemantic grouping effect with only IN1k classification supervision, which may\ninduce better interpretability and inspire new weakly-supervised semantic\nsegmentation approaches. Code will be available at\n\\url{https://github.com/rayleizhu/GLMix}.\n", "link": "http://arxiv.org/abs/2411.14429v1", "date": "2024-11-21", "relevancy": 2.7297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Integration%20of%20Convolution%20and%20Attention%20for%20Vision%0A%20%20Backbone&body=Title%3A%20Revisiting%20the%20Integration%20of%20Convolution%20and%20Attention%20for%20Vision%0A%20%20Backbone%0AAuthor%3A%20Lei%20Zhu%20and%20Xinjiang%20Wang%20and%20Wayne%20Zhang%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Convolutions%20%28Convs%29%20and%20multi-head%20self-attentions%20%28MHSAs%29%20are%20typically%0Aconsidered%20alternatives%20to%20each%20other%20for%20building%20vision%20backbones.%20Although%0Asome%20works%20try%20to%20integrate%20both%2C%20they%20apply%20the%20two%20operators%20simultaneously%0Aat%20the%20finest%20pixel%20granularity.%20With%20Convs%20responsible%20for%20per-pixel%20feature%0Aextraction%20already%2C%20the%20question%20is%20whether%20we%20still%20need%20to%20include%20the%20heavy%0AMHSAs%20at%20such%20a%20fine-grained%20level.%20In%20fact%2C%20this%20is%20the%20root%20cause%20of%20the%0Ascalability%20issue%20w.r.t.%20the%20input%20resolution%20for%20vision%20transformers.%20To%0Aaddress%20this%20important%20problem%2C%20we%20propose%20in%20this%20work%20to%20use%20MSHAs%20and%20Convs%0Ain%20parallel%20%5Ctextbf%7Bat%20different%20granularity%20levels%7D%20instead.%20Specifically%2C%20in%0Aeach%20layer%2C%20we%20use%20two%20different%20ways%20to%20represent%20an%20image%3A%20a%20fine-grained%0Aregular%20grid%20and%20a%20coarse-grained%20set%20of%20semantic%20slots.%20We%20apply%20different%0Aoperations%20to%20these%20two%20representations%3A%20Convs%20to%20the%20grid%20for%20local%20features%2C%0Aand%20MHSAs%20to%20the%20slots%20for%20global%20features.%20A%20pair%20of%20fully%20differentiable%20soft%0Aclustering%20and%20dispatching%20modules%20is%20introduced%20to%20bridge%20the%20grid%20and%20set%0Arepresentations%2C%20thus%20enabling%20local-global%20fusion.%20Through%20extensive%0Aexperiments%20on%20various%20vision%20tasks%2C%20we%20empirically%20verify%20the%20potential%20of%20the%0Aproposed%20integration%20scheme%2C%20named%20%5Ctextit%7BGLMix%7D%3A%20by%20offloading%20the%20burden%20of%0Afine-grained%20features%20to%20light-weight%20Convs%2C%20it%20is%20sufficient%20to%20use%20MHSAs%20in%20a%0Afew%20%28e.g.%2C%2064%29%20semantic%20slots%20to%20match%20the%20performance%20of%20recent%0Astate-of-the-art%20backbones%2C%20while%20being%20more%20efficient.%20Our%20visualization%0Aresults%20also%20demonstrate%20that%20the%20soft%20clustering%20module%20produces%20a%20meaningful%0Asemantic%20grouping%20effect%20with%20only%20IN1k%20classification%20supervision%2C%20which%20may%0Ainduce%20better%20interpretability%20and%20inspire%20new%20weakly-supervised%20semantic%0Asegmentation%20approaches.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/rayleizhu/GLMix%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Integration%2520of%2520Convolution%2520and%2520Attention%2520for%2520Vision%250A%2520%2520Backbone%26entry.906535625%3DLei%2520Zhu%2520and%2520Xinjiang%2520Wang%2520and%2520Wayne%2520Zhang%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Convolutions%2520%2528Convs%2529%2520and%2520multi-head%2520self-attentions%2520%2528MHSAs%2529%2520are%2520typically%250Aconsidered%2520alternatives%2520to%2520each%2520other%2520for%2520building%2520vision%2520backbones.%2520Although%250Asome%2520works%2520try%2520to%2520integrate%2520both%252C%2520they%2520apply%2520the%2520two%2520operators%2520simultaneously%250Aat%2520the%2520finest%2520pixel%2520granularity.%2520With%2520Convs%2520responsible%2520for%2520per-pixel%2520feature%250Aextraction%2520already%252C%2520the%2520question%2520is%2520whether%2520we%2520still%2520need%2520to%2520include%2520the%2520heavy%250AMHSAs%2520at%2520such%2520a%2520fine-grained%2520level.%2520In%2520fact%252C%2520this%2520is%2520the%2520root%2520cause%2520of%2520the%250Ascalability%2520issue%2520w.r.t.%2520the%2520input%2520resolution%2520for%2520vision%2520transformers.%2520To%250Aaddress%2520this%2520important%2520problem%252C%2520we%2520propose%2520in%2520this%2520work%2520to%2520use%2520MSHAs%2520and%2520Convs%250Ain%2520parallel%2520%255Ctextbf%257Bat%2520different%2520granularity%2520levels%257D%2520instead.%2520Specifically%252C%2520in%250Aeach%2520layer%252C%2520we%2520use%2520two%2520different%2520ways%2520to%2520represent%2520an%2520image%253A%2520a%2520fine-grained%250Aregular%2520grid%2520and%2520a%2520coarse-grained%2520set%2520of%2520semantic%2520slots.%2520We%2520apply%2520different%250Aoperations%2520to%2520these%2520two%2520representations%253A%2520Convs%2520to%2520the%2520grid%2520for%2520local%2520features%252C%250Aand%2520MHSAs%2520to%2520the%2520slots%2520for%2520global%2520features.%2520A%2520pair%2520of%2520fully%2520differentiable%2520soft%250Aclustering%2520and%2520dispatching%2520modules%2520is%2520introduced%2520to%2520bridge%2520the%2520grid%2520and%2520set%250Arepresentations%252C%2520thus%2520enabling%2520local-global%2520fusion.%2520Through%2520extensive%250Aexperiments%2520on%2520various%2520vision%2520tasks%252C%2520we%2520empirically%2520verify%2520the%2520potential%2520of%2520the%250Aproposed%2520integration%2520scheme%252C%2520named%2520%255Ctextit%257BGLMix%257D%253A%2520by%2520offloading%2520the%2520burden%2520of%250Afine-grained%2520features%2520to%2520light-weight%2520Convs%252C%2520it%2520is%2520sufficient%2520to%2520use%2520MHSAs%2520in%2520a%250Afew%2520%2528e.g.%252C%252064%2529%2520semantic%2520slots%2520to%2520match%2520the%2520performance%2520of%2520recent%250Astate-of-the-art%2520backbones%252C%2520while%2520being%2520more%2520efficient.%2520Our%2520visualization%250Aresults%2520also%2520demonstrate%2520that%2520the%2520soft%2520clustering%2520module%2520produces%2520a%2520meaningful%250Asemantic%2520grouping%2520effect%2520with%2520only%2520IN1k%2520classification%2520supervision%252C%2520which%2520may%250Ainduce%2520better%2520interpretability%2520and%2520inspire%2520new%2520weakly-supervised%2520semantic%250Asegmentation%2520approaches.%2520Code%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/rayleizhu/GLMix%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Integration%20of%20Convolution%20and%20Attention%20for%20Vision%0A%20%20Backbone&entry.906535625=Lei%20Zhu%20and%20Xinjiang%20Wang%20and%20Wayne%20Zhang%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Convolutions%20%28Convs%29%20and%20multi-head%20self-attentions%20%28MHSAs%29%20are%20typically%0Aconsidered%20alternatives%20to%20each%20other%20for%20building%20vision%20backbones.%20Although%0Asome%20works%20try%20to%20integrate%20both%2C%20they%20apply%20the%20two%20operators%20simultaneously%0Aat%20the%20finest%20pixel%20granularity.%20With%20Convs%20responsible%20for%20per-pixel%20feature%0Aextraction%20already%2C%20the%20question%20is%20whether%20we%20still%20need%20to%20include%20the%20heavy%0AMHSAs%20at%20such%20a%20fine-grained%20level.%20In%20fact%2C%20this%20is%20the%20root%20cause%20of%20the%0Ascalability%20issue%20w.r.t.%20the%20input%20resolution%20for%20vision%20transformers.%20To%0Aaddress%20this%20important%20problem%2C%20we%20propose%20in%20this%20work%20to%20use%20MSHAs%20and%20Convs%0Ain%20parallel%20%5Ctextbf%7Bat%20different%20granularity%20levels%7D%20instead.%20Specifically%2C%20in%0Aeach%20layer%2C%20we%20use%20two%20different%20ways%20to%20represent%20an%20image%3A%20a%20fine-grained%0Aregular%20grid%20and%20a%20coarse-grained%20set%20of%20semantic%20slots.%20We%20apply%20different%0Aoperations%20to%20these%20two%20representations%3A%20Convs%20to%20the%20grid%20for%20local%20features%2C%0Aand%20MHSAs%20to%20the%20slots%20for%20global%20features.%20A%20pair%20of%20fully%20differentiable%20soft%0Aclustering%20and%20dispatching%20modules%20is%20introduced%20to%20bridge%20the%20grid%20and%20set%0Arepresentations%2C%20thus%20enabling%20local-global%20fusion.%20Through%20extensive%0Aexperiments%20on%20various%20vision%20tasks%2C%20we%20empirically%20verify%20the%20potential%20of%20the%0Aproposed%20integration%20scheme%2C%20named%20%5Ctextit%7BGLMix%7D%3A%20by%20offloading%20the%20burden%20of%0Afine-grained%20features%20to%20light-weight%20Convs%2C%20it%20is%20sufficient%20to%20use%20MHSAs%20in%20a%0Afew%20%28e.g.%2C%2064%29%20semantic%20slots%20to%20match%20the%20performance%20of%20recent%0Astate-of-the-art%20backbones%2C%20while%20being%20more%20efficient.%20Our%20visualization%0Aresults%20also%20demonstrate%20that%20the%20soft%20clustering%20module%20produces%20a%20meaningful%0Asemantic%20grouping%20effect%20with%20only%20IN1k%20classification%20supervision%2C%20which%20may%0Ainduce%20better%20interpretability%20and%20inspire%20new%20weakly-supervised%20semantic%0Asegmentation%20approaches.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/rayleizhu/GLMix%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14429v1&entry.124074799=Read"},
{"title": "Geometric Algebra Planes: Convex Implicit Neural Volumes", "author": "Irmak Sivgin and Sara Fridovich-Keil and Gordon Wetzstein and Mert Pilanci", "abstract": "  Volume parameterizations abound in recent literature, from the classic voxel\ngrid to the implicit neural representation and everything in between. While\nimplicit representations have shown impressive capacity and better memory\nefficiency compared to voxel grids, to date they require training via nonconvex\noptimization. This nonconvex training process can be slow to converge and\nsensitive to initialization and hyperparameter choices that affect the final\nconverged result. We introduce a family of models, GA-Planes, that is the first\nclass of implicit neural volume representations that can be trained by convex\noptimization. GA-Planes models include any combination of features stored in\ntensor basis elements, followed by a neural feature decoder. They generalize\nmany existing representations and can be adapted for convex, semiconvex, or\nnonconvex training as needed for different inverse problems. In the 2D setting,\nwe prove that GA-Planes is equivalent to a low-rank plus low-resolution matrix\nfactorization; we show that this approximation outperforms the classic low-rank\nplus sparse decomposition for fitting a natural image. In 3D, we demonstrate\nGA-Planes' competitive performance in terms of expressiveness, model size, and\noptimizability across three volume fitting tasks: radiance field\nreconstruction, 3D segmentation, and video segmentation.\n", "link": "http://arxiv.org/abs/2411.13525v2", "date": "2024-11-21", "relevancy": 2.7254, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5516}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5462}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Algebra%20Planes%3A%20Convex%20Implicit%20Neural%20Volumes&body=Title%3A%20Geometric%20Algebra%20Planes%3A%20Convex%20Implicit%20Neural%20Volumes%0AAuthor%3A%20Irmak%20Sivgin%20and%20Sara%20Fridovich-Keil%20and%20Gordon%20Wetzstein%20and%20Mert%20Pilanci%0AAbstract%3A%20%20%20Volume%20parameterizations%20abound%20in%20recent%20literature%2C%20from%20the%20classic%20voxel%0Agrid%20to%20the%20implicit%20neural%20representation%20and%20everything%20in%20between.%20While%0Aimplicit%20representations%20have%20shown%20impressive%20capacity%20and%20better%20memory%0Aefficiency%20compared%20to%20voxel%20grids%2C%20to%20date%20they%20require%20training%20via%20nonconvex%0Aoptimization.%20This%20nonconvex%20training%20process%20can%20be%20slow%20to%20converge%20and%0Asensitive%20to%20initialization%20and%20hyperparameter%20choices%20that%20affect%20the%20final%0Aconverged%20result.%20We%20introduce%20a%20family%20of%20models%2C%20GA-Planes%2C%20that%20is%20the%20first%0Aclass%20of%20implicit%20neural%20volume%20representations%20that%20can%20be%20trained%20by%20convex%0Aoptimization.%20GA-Planes%20models%20include%20any%20combination%20of%20features%20stored%20in%0Atensor%20basis%20elements%2C%20followed%20by%20a%20neural%20feature%20decoder.%20They%20generalize%0Amany%20existing%20representations%20and%20can%20be%20adapted%20for%20convex%2C%20semiconvex%2C%20or%0Anonconvex%20training%20as%20needed%20for%20different%20inverse%20problems.%20In%20the%202D%20setting%2C%0Awe%20prove%20that%20GA-Planes%20is%20equivalent%20to%20a%20low-rank%20plus%20low-resolution%20matrix%0Afactorization%3B%20we%20show%20that%20this%20approximation%20outperforms%20the%20classic%20low-rank%0Aplus%20sparse%20decomposition%20for%20fitting%20a%20natural%20image.%20In%203D%2C%20we%20demonstrate%0AGA-Planes%27%20competitive%20performance%20in%20terms%20of%20expressiveness%2C%20model%20size%2C%20and%0Aoptimizability%20across%20three%20volume%20fitting%20tasks%3A%20radiance%20field%0Areconstruction%2C%203D%20segmentation%2C%20and%20video%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13525v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Algebra%2520Planes%253A%2520Convex%2520Implicit%2520Neural%2520Volumes%26entry.906535625%3DIrmak%2520Sivgin%2520and%2520Sara%2520Fridovich-Keil%2520and%2520Gordon%2520Wetzstein%2520and%2520Mert%2520Pilanci%26entry.1292438233%3D%2520%2520Volume%2520parameterizations%2520abound%2520in%2520recent%2520literature%252C%2520from%2520the%2520classic%2520voxel%250Agrid%2520to%2520the%2520implicit%2520neural%2520representation%2520and%2520everything%2520in%2520between.%2520While%250Aimplicit%2520representations%2520have%2520shown%2520impressive%2520capacity%2520and%2520better%2520memory%250Aefficiency%2520compared%2520to%2520voxel%2520grids%252C%2520to%2520date%2520they%2520require%2520training%2520via%2520nonconvex%250Aoptimization.%2520This%2520nonconvex%2520training%2520process%2520can%2520be%2520slow%2520to%2520converge%2520and%250Asensitive%2520to%2520initialization%2520and%2520hyperparameter%2520choices%2520that%2520affect%2520the%2520final%250Aconverged%2520result.%2520We%2520introduce%2520a%2520family%2520of%2520models%252C%2520GA-Planes%252C%2520that%2520is%2520the%2520first%250Aclass%2520of%2520implicit%2520neural%2520volume%2520representations%2520that%2520can%2520be%2520trained%2520by%2520convex%250Aoptimization.%2520GA-Planes%2520models%2520include%2520any%2520combination%2520of%2520features%2520stored%2520in%250Atensor%2520basis%2520elements%252C%2520followed%2520by%2520a%2520neural%2520feature%2520decoder.%2520They%2520generalize%250Amany%2520existing%2520representations%2520and%2520can%2520be%2520adapted%2520for%2520convex%252C%2520semiconvex%252C%2520or%250Anonconvex%2520training%2520as%2520needed%2520for%2520different%2520inverse%2520problems.%2520In%2520the%25202D%2520setting%252C%250Awe%2520prove%2520that%2520GA-Planes%2520is%2520equivalent%2520to%2520a%2520low-rank%2520plus%2520low-resolution%2520matrix%250Afactorization%253B%2520we%2520show%2520that%2520this%2520approximation%2520outperforms%2520the%2520classic%2520low-rank%250Aplus%2520sparse%2520decomposition%2520for%2520fitting%2520a%2520natural%2520image.%2520In%25203D%252C%2520we%2520demonstrate%250AGA-Planes%2527%2520competitive%2520performance%2520in%2520terms%2520of%2520expressiveness%252C%2520model%2520size%252C%2520and%250Aoptimizability%2520across%2520three%2520volume%2520fitting%2520tasks%253A%2520radiance%2520field%250Areconstruction%252C%25203D%2520segmentation%252C%2520and%2520video%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13525v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Algebra%20Planes%3A%20Convex%20Implicit%20Neural%20Volumes&entry.906535625=Irmak%20Sivgin%20and%20Sara%20Fridovich-Keil%20and%20Gordon%20Wetzstein%20and%20Mert%20Pilanci&entry.1292438233=%20%20Volume%20parameterizations%20abound%20in%20recent%20literature%2C%20from%20the%20classic%20voxel%0Agrid%20to%20the%20implicit%20neural%20representation%20and%20everything%20in%20between.%20While%0Aimplicit%20representations%20have%20shown%20impressive%20capacity%20and%20better%20memory%0Aefficiency%20compared%20to%20voxel%20grids%2C%20to%20date%20they%20require%20training%20via%20nonconvex%0Aoptimization.%20This%20nonconvex%20training%20process%20can%20be%20slow%20to%20converge%20and%0Asensitive%20to%20initialization%20and%20hyperparameter%20choices%20that%20affect%20the%20final%0Aconverged%20result.%20We%20introduce%20a%20family%20of%20models%2C%20GA-Planes%2C%20that%20is%20the%20first%0Aclass%20of%20implicit%20neural%20volume%20representations%20that%20can%20be%20trained%20by%20convex%0Aoptimization.%20GA-Planes%20models%20include%20any%20combination%20of%20features%20stored%20in%0Atensor%20basis%20elements%2C%20followed%20by%20a%20neural%20feature%20decoder.%20They%20generalize%0Amany%20existing%20representations%20and%20can%20be%20adapted%20for%20convex%2C%20semiconvex%2C%20or%0Anonconvex%20training%20as%20needed%20for%20different%20inverse%20problems.%20In%20the%202D%20setting%2C%0Awe%20prove%20that%20GA-Planes%20is%20equivalent%20to%20a%20low-rank%20plus%20low-resolution%20matrix%0Afactorization%3B%20we%20show%20that%20this%20approximation%20outperforms%20the%20classic%20low-rank%0Aplus%20sparse%20decomposition%20for%20fitting%20a%20natural%20image.%20In%203D%2C%20we%20demonstrate%0AGA-Planes%27%20competitive%20performance%20in%20terms%20of%20expressiveness%2C%20model%20size%2C%20and%0Aoptimizability%20across%20three%20volume%20fitting%20tasks%3A%20radiance%20field%0Areconstruction%2C%203D%20segmentation%2C%20and%20video%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13525v2&entry.124074799=Read"},
{"title": "VG-SSL: Benchmarking Self-supervised Representation Learning Approaches\n  for Visual Geo-localization", "author": "Jiuhong Xiao and Gao Zhu and Giuseppe Loianno", "abstract": "  Visual Geo-localization (VG) is a critical research area for identifying\ngeo-locations from visual inputs, particularly in autonomous navigation for\nrobotics and vehicles. Current VG methods often learn feature extractors from\ngeo-labeled images to create dense, geographically relevant representations.\nRecent advances in Self-Supervised Learning (SSL) have demonstrated its\ncapability to achieve performance on par with supervised techniques with\nunlabeled images. This study presents a novel VG-SSL framework, designed for\nversatile integration and benchmarking of diverse SSL methods for\nrepresentation learning in VG, featuring a unique geo-related pair strategy,\nGeoPair. Through extensive performance analysis, we adapt SSL techniques to\nimprove VG on datasets from hand-held and car-mounted cameras used in robotics\nand autonomous vehicles. Our results show that contrastive learning and\ninformation maximization methods yield superior geo-specific representation\nquality, matching or surpassing the performance of state-of-the-art VG\ntechniques. To our knowledge, This is the first benchmarking study of SSL in\nVG, highlighting its potential in enhancing geo-specific visual representations\nfor robotics and autonomous vehicles. The code is publicly available at\nhttps://github.com/arplaboratory/VG-SSL.\n", "link": "http://arxiv.org/abs/2308.00090v3", "date": "2024-11-21", "relevancy": 2.7017, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5702}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5281}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VG-SSL%3A%20Benchmarking%20Self-supervised%20Representation%20Learning%20Approaches%0A%20%20for%20Visual%20Geo-localization&body=Title%3A%20VG-SSL%3A%20Benchmarking%20Self-supervised%20Representation%20Learning%20Approaches%0A%20%20for%20Visual%20Geo-localization%0AAuthor%3A%20Jiuhong%20Xiao%20and%20Gao%20Zhu%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Visual%20Geo-localization%20%28VG%29%20is%20a%20critical%20research%20area%20for%20identifying%0Ageo-locations%20from%20visual%20inputs%2C%20particularly%20in%20autonomous%20navigation%20for%0Arobotics%20and%20vehicles.%20Current%20VG%20methods%20often%20learn%20feature%20extractors%20from%0Ageo-labeled%20images%20to%20create%20dense%2C%20geographically%20relevant%20representations.%0ARecent%20advances%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%20its%0Acapability%20to%20achieve%20performance%20on%20par%20with%20supervised%20techniques%20with%0Aunlabeled%20images.%20This%20study%20presents%20a%20novel%20VG-SSL%20framework%2C%20designed%20for%0Aversatile%20integration%20and%20benchmarking%20of%20diverse%20SSL%20methods%20for%0Arepresentation%20learning%20in%20VG%2C%20featuring%20a%20unique%20geo-related%20pair%20strategy%2C%0AGeoPair.%20Through%20extensive%20performance%20analysis%2C%20we%20adapt%20SSL%20techniques%20to%0Aimprove%20VG%20on%20datasets%20from%20hand-held%20and%20car-mounted%20cameras%20used%20in%20robotics%0Aand%20autonomous%20vehicles.%20Our%20results%20show%20that%20contrastive%20learning%20and%0Ainformation%20maximization%20methods%20yield%20superior%20geo-specific%20representation%0Aquality%2C%20matching%20or%20surpassing%20the%20performance%20of%20state-of-the-art%20VG%0Atechniques.%20To%20our%20knowledge%2C%20This%20is%20the%20first%20benchmarking%20study%20of%20SSL%20in%0AVG%2C%20highlighting%20its%20potential%20in%20enhancing%20geo-specific%20visual%20representations%0Afor%20robotics%20and%20autonomous%20vehicles.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/arplaboratory/VG-SSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00090v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVG-SSL%253A%2520Benchmarking%2520Self-supervised%2520Representation%2520Learning%2520Approaches%250A%2520%2520for%2520Visual%2520Geo-localization%26entry.906535625%3DJiuhong%2520Xiao%2520and%2520Gao%2520Zhu%2520and%2520Giuseppe%2520Loianno%26entry.1292438233%3D%2520%2520Visual%2520Geo-localization%2520%2528VG%2529%2520is%2520a%2520critical%2520research%2520area%2520for%2520identifying%250Ageo-locations%2520from%2520visual%2520inputs%252C%2520particularly%2520in%2520autonomous%2520navigation%2520for%250Arobotics%2520and%2520vehicles.%2520Current%2520VG%2520methods%2520often%2520learn%2520feature%2520extractors%2520from%250Ageo-labeled%2520images%2520to%2520create%2520dense%252C%2520geographically%2520relevant%2520representations.%250ARecent%2520advances%2520in%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520have%2520demonstrated%2520its%250Acapability%2520to%2520achieve%2520performance%2520on%2520par%2520with%2520supervised%2520techniques%2520with%250Aunlabeled%2520images.%2520This%2520study%2520presents%2520a%2520novel%2520VG-SSL%2520framework%252C%2520designed%2520for%250Aversatile%2520integration%2520and%2520benchmarking%2520of%2520diverse%2520SSL%2520methods%2520for%250Arepresentation%2520learning%2520in%2520VG%252C%2520featuring%2520a%2520unique%2520geo-related%2520pair%2520strategy%252C%250AGeoPair.%2520Through%2520extensive%2520performance%2520analysis%252C%2520we%2520adapt%2520SSL%2520techniques%2520to%250Aimprove%2520VG%2520on%2520datasets%2520from%2520hand-held%2520and%2520car-mounted%2520cameras%2520used%2520in%2520robotics%250Aand%2520autonomous%2520vehicles.%2520Our%2520results%2520show%2520that%2520contrastive%2520learning%2520and%250Ainformation%2520maximization%2520methods%2520yield%2520superior%2520geo-specific%2520representation%250Aquality%252C%2520matching%2520or%2520surpassing%2520the%2520performance%2520of%2520state-of-the-art%2520VG%250Atechniques.%2520To%2520our%2520knowledge%252C%2520This%2520is%2520the%2520first%2520benchmarking%2520study%2520of%2520SSL%2520in%250AVG%252C%2520highlighting%2520its%2520potential%2520in%2520enhancing%2520geo-specific%2520visual%2520representations%250Afor%2520robotics%2520and%2520autonomous%2520vehicles.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/arplaboratory/VG-SSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.00090v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VG-SSL%3A%20Benchmarking%20Self-supervised%20Representation%20Learning%20Approaches%0A%20%20for%20Visual%20Geo-localization&entry.906535625=Jiuhong%20Xiao%20and%20Gao%20Zhu%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Visual%20Geo-localization%20%28VG%29%20is%20a%20critical%20research%20area%20for%20identifying%0Ageo-locations%20from%20visual%20inputs%2C%20particularly%20in%20autonomous%20navigation%20for%0Arobotics%20and%20vehicles.%20Current%20VG%20methods%20often%20learn%20feature%20extractors%20from%0Ageo-labeled%20images%20to%20create%20dense%2C%20geographically%20relevant%20representations.%0ARecent%20advances%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%20its%0Acapability%20to%20achieve%20performance%20on%20par%20with%20supervised%20techniques%20with%0Aunlabeled%20images.%20This%20study%20presents%20a%20novel%20VG-SSL%20framework%2C%20designed%20for%0Aversatile%20integration%20and%20benchmarking%20of%20diverse%20SSL%20methods%20for%0Arepresentation%20learning%20in%20VG%2C%20featuring%20a%20unique%20geo-related%20pair%20strategy%2C%0AGeoPair.%20Through%20extensive%20performance%20analysis%2C%20we%20adapt%20SSL%20techniques%20to%0Aimprove%20VG%20on%20datasets%20from%20hand-held%20and%20car-mounted%20cameras%20used%20in%20robotics%0Aand%20autonomous%20vehicles.%20Our%20results%20show%20that%20contrastive%20learning%20and%0Ainformation%20maximization%20methods%20yield%20superior%20geo-specific%20representation%0Aquality%2C%20matching%20or%20surpassing%20the%20performance%20of%20state-of-the-art%20VG%0Atechniques.%20To%20our%20knowledge%2C%20This%20is%20the%20first%20benchmarking%20study%20of%20SSL%20in%0AVG%2C%20highlighting%20its%20potential%20in%20enhancing%20geo-specific%20visual%20representations%0Afor%20robotics%20and%20autonomous%20vehicles.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/arplaboratory/VG-SSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00090v3&entry.124074799=Read"},
{"title": "Localizing Events in Videos with Multimodal Queries", "author": "Gengyuan Zhang and Mang Ling Ada Fok and Jialu Ma and Yan Xia and Daniel Cremers and Philip Torr and Volker Tresp and Jindong Gu", "abstract": "  Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.\n", "link": "http://arxiv.org/abs/2406.10079v3", "date": "2024-11-21", "relevancy": 2.6829, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries&body=Title%3A%20Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries%0AAuthor%3A%20Gengyuan%20Zhang%20and%20Mang%20Ling%20Ada%20Fok%20and%20Jialu%20Ma%20and%20Yan%20Xia%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Localizing%20events%20in%20videos%20based%20on%20semantic%20queries%20is%20a%20pivotal%20task%20in%0Avideo%20understanding%2C%20with%20the%20growing%20significance%20of%20user-oriented%0Aapplications%20like%20video%20search.%20Yet%2C%20current%20research%20predominantly%20relies%20on%0Anatural%20language%20queries%20%28NLQs%29%2C%20overlooking%20the%20potential%20of%20using%20multimodal%0Aqueries%20%28MQs%29%20that%20integrate%20images%20to%20more%20flexibly%20represent%20semantic%20queries%0A--%20especially%20when%20it%20is%20difficult%20to%20express%20non-verbal%20or%20unfamiliar%20concepts%0Ain%20words.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ICQ%2C%20a%20new%20benchmark%20designed%20for%0Alocalizing%20events%20in%20videos%20with%20MQs%2C%20alongside%20an%20evaluation%20dataset%0AICQ-Highlight.%20To%20accommodate%20and%20evaluate%20existing%20video%20localization%20models%0Afor%20this%20new%20task%2C%20we%20propose%203%20Multimodal%20Query%20Adaptation%20methods%20and%20a%20novel%0ASurrogate%20Fine-tuning%20on%20pseudo-MQs%20strategy.%20ICQ%20systematically%20benchmarks%2012%0Astate-of-the-art%20backbone%20models%2C%20spanning%20from%20specialized%20video%20localization%0Amodels%20to%20Video%20LLMs%2C%20across%20diverse%20application%20domains.%20Our%20experiments%0Ahighlight%20the%20high%20potential%20of%20MQs%20in%20real-world%20applications.%20We%20believe%20this%0Abenchmark%20is%20a%20first%20step%20toward%20advancing%20MQs%20in%20video%20event%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10079v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520Events%2520in%2520Videos%2520with%2520Multimodal%2520Queries%26entry.906535625%3DGengyuan%2520Zhang%2520and%2520Mang%2520Ling%2520Ada%2520Fok%2520and%2520Jialu%2520Ma%2520and%2520Yan%2520Xia%2520and%2520Daniel%2520Cremers%2520and%2520Philip%2520Torr%2520and%2520Volker%2520Tresp%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Localizing%2520events%2520in%2520videos%2520based%2520on%2520semantic%2520queries%2520is%2520a%2520pivotal%2520task%2520in%250Avideo%2520understanding%252C%2520with%2520the%2520growing%2520significance%2520of%2520user-oriented%250Aapplications%2520like%2520video%2520search.%2520Yet%252C%2520current%2520research%2520predominantly%2520relies%2520on%250Anatural%2520language%2520queries%2520%2528NLQs%2529%252C%2520overlooking%2520the%2520potential%2520of%2520using%2520multimodal%250Aqueries%2520%2528MQs%2529%2520that%2520integrate%2520images%2520to%2520more%2520flexibly%2520represent%2520semantic%2520queries%250A--%2520especially%2520when%2520it%2520is%2520difficult%2520to%2520express%2520non-verbal%2520or%2520unfamiliar%2520concepts%250Ain%2520words.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ICQ%252C%2520a%2520new%2520benchmark%2520designed%2520for%250Alocalizing%2520events%2520in%2520videos%2520with%2520MQs%252C%2520alongside%2520an%2520evaluation%2520dataset%250AICQ-Highlight.%2520To%2520accommodate%2520and%2520evaluate%2520existing%2520video%2520localization%2520models%250Afor%2520this%2520new%2520task%252C%2520we%2520propose%25203%2520Multimodal%2520Query%2520Adaptation%2520methods%2520and%2520a%2520novel%250ASurrogate%2520Fine-tuning%2520on%2520pseudo-MQs%2520strategy.%2520ICQ%2520systematically%2520benchmarks%252012%250Astate-of-the-art%2520backbone%2520models%252C%2520spanning%2520from%2520specialized%2520video%2520localization%250Amodels%2520to%2520Video%2520LLMs%252C%2520across%2520diverse%2520application%2520domains.%2520Our%2520experiments%250Ahighlight%2520the%2520high%2520potential%2520of%2520MQs%2520in%2520real-world%2520applications.%2520We%2520believe%2520this%250Abenchmark%2520is%2520a%2520first%2520step%2520toward%2520advancing%2520MQs%2520in%2520video%2520event%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10079v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries&entry.906535625=Gengyuan%20Zhang%20and%20Mang%20Ling%20Ada%20Fok%20and%20Jialu%20Ma%20and%20Yan%20Xia%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu&entry.1292438233=%20%20Localizing%20events%20in%20videos%20based%20on%20semantic%20queries%20is%20a%20pivotal%20task%20in%0Avideo%20understanding%2C%20with%20the%20growing%20significance%20of%20user-oriented%0Aapplications%20like%20video%20search.%20Yet%2C%20current%20research%20predominantly%20relies%20on%0Anatural%20language%20queries%20%28NLQs%29%2C%20overlooking%20the%20potential%20of%20using%20multimodal%0Aqueries%20%28MQs%29%20that%20integrate%20images%20to%20more%20flexibly%20represent%20semantic%20queries%0A--%20especially%20when%20it%20is%20difficult%20to%20express%20non-verbal%20or%20unfamiliar%20concepts%0Ain%20words.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ICQ%2C%20a%20new%20benchmark%20designed%20for%0Alocalizing%20events%20in%20videos%20with%20MQs%2C%20alongside%20an%20evaluation%20dataset%0AICQ-Highlight.%20To%20accommodate%20and%20evaluate%20existing%20video%20localization%20models%0Afor%20this%20new%20task%2C%20we%20propose%203%20Multimodal%20Query%20Adaptation%20methods%20and%20a%20novel%0ASurrogate%20Fine-tuning%20on%20pseudo-MQs%20strategy.%20ICQ%20systematically%20benchmarks%2012%0Astate-of-the-art%20backbone%20models%2C%20spanning%20from%20specialized%20video%20localization%0Amodels%20to%20Video%20LLMs%2C%20across%20diverse%20application%20domains.%20Our%20experiments%0Ahighlight%20the%20high%20potential%20of%20MQs%20in%20real-world%20applications.%20We%20believe%20this%0Abenchmark%20is%20a%20first%20step%20toward%20advancing%20MQs%20in%20video%20event%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10079v3&entry.124074799=Read"},
{"title": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model", "author": "Ege Kesim and Selahattin Serdar Helli", "abstract": "  Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets.\n", "link": "http://arxiv.org/abs/2411.14064v1", "date": "2024-11-21", "relevancy": 2.6552, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi%20LoRA%20Meets%20Vision%3A%20Merging%20multiple%20adapters%20to%20create%20a%20multi%0A%20%20task%20model&body=Title%3A%20Multi%20LoRA%20Meets%20Vision%3A%20Merging%20multiple%20adapters%20to%20create%20a%20multi%0A%20%20task%20model%0AAuthor%3A%20Ege%20Kesim%20and%20Selahattin%20Serdar%20Helli%0AAbstract%3A%20%20%20Parameter%20efficient%20finetuning%20%28PEFT%29%20methods%20are%20widely%20used%20in%20LLMs%20and%0Agenerative%20models%20in%20computer%20vision.%20Especially%20one%20can%20use%20multiple%20of%20these%0Aduring%20inference%20to%20change%20the%20behavior%20of%20the%20base%20model.%20In%20this%20paper%20we%0Ainvestigated%20whether%20multiple%20LoRA%20adapters%20trained%20on%20computer%20vision%20tasks%0Acan%20be%20merged%20together%20and%20used%20during%20inference%20without%20loss%20in%20performance.%0ABy%20achieving%20this%2C%20multitask%20models%20can%20be%20created%20just%20by%20merging%20different%0ALoRAs.%20Merging%20these%20will%20reduce%20inference%20time%20and%20it%20will%20not%20require%20any%0Aadditional%20retraining.%20We%20have%20trained%20adapters%20on%20six%20different%20tasks%20and%0Aevaluated%20their%20performance%20when%20they%20are%20merged%20together.%20For%20comparison%20we%0Aused%20a%20model%20with%20a%20frozen%20backbone%20and%20finetuned%20its%20head.%20Our%20results%20show%0Athat%20even%20with%20simple%20merging%20techniques%20creating%20a%20multitask%20model%20by%20merging%0Aadapters%20is%20achievable%20by%20slightly%20loosing%20performance%20in%20some%20cases.%20In%20our%0Aexperiments%20we%20merged%20up%20to%20three%20adapters%20together.%20Depending%20on%20the%20task%20and%0Athe%20similarity%20of%20the%20data%20adapters%20were%20trained%20on%2C%20merges%20can%20outperform%20head%0Afinetuning.%20We%20have%20observed%20that%20LoRAs%20trained%20with%20dissimilar%20datasets%20tend%0Ato%20perform%20better%20compared%20to%20model%20trained%20on%20similar%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti%2520LoRA%2520Meets%2520Vision%253A%2520Merging%2520multiple%2520adapters%2520to%2520create%2520a%2520multi%250A%2520%2520task%2520model%26entry.906535625%3DEge%2520Kesim%2520and%2520Selahattin%2520Serdar%2520Helli%26entry.1292438233%3D%2520%2520Parameter%2520efficient%2520finetuning%2520%2528PEFT%2529%2520methods%2520are%2520widely%2520used%2520in%2520LLMs%2520and%250Agenerative%2520models%2520in%2520computer%2520vision.%2520Especially%2520one%2520can%2520use%2520multiple%2520of%2520these%250Aduring%2520inference%2520to%2520change%2520the%2520behavior%2520of%2520the%2520base%2520model.%2520In%2520this%2520paper%2520we%250Ainvestigated%2520whether%2520multiple%2520LoRA%2520adapters%2520trained%2520on%2520computer%2520vision%2520tasks%250Acan%2520be%2520merged%2520together%2520and%2520used%2520during%2520inference%2520without%2520loss%2520in%2520performance.%250ABy%2520achieving%2520this%252C%2520multitask%2520models%2520can%2520be%2520created%2520just%2520by%2520merging%2520different%250ALoRAs.%2520Merging%2520these%2520will%2520reduce%2520inference%2520time%2520and%2520it%2520will%2520not%2520require%2520any%250Aadditional%2520retraining.%2520We%2520have%2520trained%2520adapters%2520on%2520six%2520different%2520tasks%2520and%250Aevaluated%2520their%2520performance%2520when%2520they%2520are%2520merged%2520together.%2520For%2520comparison%2520we%250Aused%2520a%2520model%2520with%2520a%2520frozen%2520backbone%2520and%2520finetuned%2520its%2520head.%2520Our%2520results%2520show%250Athat%2520even%2520with%2520simple%2520merging%2520techniques%2520creating%2520a%2520multitask%2520model%2520by%2520merging%250Aadapters%2520is%2520achievable%2520by%2520slightly%2520loosing%2520performance%2520in%2520some%2520cases.%2520In%2520our%250Aexperiments%2520we%2520merged%2520up%2520to%2520three%2520adapters%2520together.%2520Depending%2520on%2520the%2520task%2520and%250Athe%2520similarity%2520of%2520the%2520data%2520adapters%2520were%2520trained%2520on%252C%2520merges%2520can%2520outperform%2520head%250Afinetuning.%2520We%2520have%2520observed%2520that%2520LoRAs%2520trained%2520with%2520dissimilar%2520datasets%2520tend%250Ato%2520perform%2520better%2520compared%2520to%2520model%2520trained%2520on%2520similar%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi%20LoRA%20Meets%20Vision%3A%20Merging%20multiple%20adapters%20to%20create%20a%20multi%0A%20%20task%20model&entry.906535625=Ege%20Kesim%20and%20Selahattin%20Serdar%20Helli&entry.1292438233=%20%20Parameter%20efficient%20finetuning%20%28PEFT%29%20methods%20are%20widely%20used%20in%20LLMs%20and%0Agenerative%20models%20in%20computer%20vision.%20Especially%20one%20can%20use%20multiple%20of%20these%0Aduring%20inference%20to%20change%20the%20behavior%20of%20the%20base%20model.%20In%20this%20paper%20we%0Ainvestigated%20whether%20multiple%20LoRA%20adapters%20trained%20on%20computer%20vision%20tasks%0Acan%20be%20merged%20together%20and%20used%20during%20inference%20without%20loss%20in%20performance.%0ABy%20achieving%20this%2C%20multitask%20models%20can%20be%20created%20just%20by%20merging%20different%0ALoRAs.%20Merging%20these%20will%20reduce%20inference%20time%20and%20it%20will%20not%20require%20any%0Aadditional%20retraining.%20We%20have%20trained%20adapters%20on%20six%20different%20tasks%20and%0Aevaluated%20their%20performance%20when%20they%20are%20merged%20together.%20For%20comparison%20we%0Aused%20a%20model%20with%20a%20frozen%20backbone%20and%20finetuned%20its%20head.%20Our%20results%20show%0Athat%20even%20with%20simple%20merging%20techniques%20creating%20a%20multitask%20model%20by%20merging%0Aadapters%20is%20achievable%20by%20slightly%20loosing%20performance%20in%20some%20cases.%20In%20our%0Aexperiments%20we%20merged%20up%20to%20three%20adapters%20together.%20Depending%20on%20the%20task%20and%0Athe%20similarity%20of%20the%20data%20adapters%20were%20trained%20on%2C%20merges%20can%20outperform%20head%0Afinetuning.%20We%20have%20observed%20that%20LoRAs%20trained%20with%20dissimilar%20datasets%20tend%0Ato%20perform%20better%20compared%20to%20model%20trained%20on%20similar%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14064v1&entry.124074799=Read"},
{"title": "Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data", "author": "Xianda Guo and Chenming Zhang and Youmin Zhang and Dujun Nie and Ruilin Wang and Wenzhao Zheng and Matteo Poggi and Long Chen", "abstract": "  Stereo matching has been a pivotal component in 3D vision, aiming to find\ncorresponding points between pairs of stereo images to recover depth\ninformation. In this work, we introduce StereoAnything, a highly practical\nsolution for robust stereo matching. Rather than focusing on a specialized\nmodel, our goal is to develop a versatile foundational model capable of\nhandling stereo images across diverse environments. To this end, we scale up\nthe dataset by collecting labeled stereo images and generating synthetic stereo\npairs from unlabeled monocular images. To further enrich the model's ability to\ngeneralize across different conditions, we introduce a novel synthetic dataset\nthat complements existing data by adding variability in baselines, camera\nangles, and scene types. We extensively evaluate the zero-shot capabilities of\nour model on five public datasets, showcasing its impressive ability to\ngeneralize to new, unseen data. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n", "link": "http://arxiv.org/abs/2411.14053v1", "date": "2024-11-21", "relevancy": 2.6517, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo%20Anything%3A%20Unifying%20Stereo%20Matching%20with%20Large-Scale%20Mixed%20Data&body=Title%3A%20Stereo%20Anything%3A%20Unifying%20Stereo%20Matching%20with%20Large-Scale%20Mixed%20Data%0AAuthor%3A%20Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Youmin%20Zhang%20and%20Dujun%20Nie%20and%20Ruilin%20Wang%20and%20Wenzhao%20Zheng%20and%20Matteo%20Poggi%20and%20Long%20Chen%0AAbstract%3A%20%20%20Stereo%20matching%20has%20been%20a%20pivotal%20component%20in%203D%20vision%2C%20aiming%20to%20find%0Acorresponding%20points%20between%20pairs%20of%20stereo%20images%20to%20recover%20depth%0Ainformation.%20In%20this%20work%2C%20we%20introduce%20StereoAnything%2C%20a%20highly%20practical%0Asolution%20for%20robust%20stereo%20matching.%20Rather%20than%20focusing%20on%20a%20specialized%0Amodel%2C%20our%20goal%20is%20to%20develop%20a%20versatile%20foundational%20model%20capable%20of%0Ahandling%20stereo%20images%20across%20diverse%20environments.%20To%20this%20end%2C%20we%20scale%20up%0Athe%20dataset%20by%20collecting%20labeled%20stereo%20images%20and%20generating%20synthetic%20stereo%0Apairs%20from%20unlabeled%20monocular%20images.%20To%20further%20enrich%20the%20model%27s%20ability%20to%0Ageneralize%20across%20different%20conditions%2C%20we%20introduce%20a%20novel%20synthetic%20dataset%0Athat%20complements%20existing%20data%20by%20adding%20variability%20in%20baselines%2C%20camera%0Aangles%2C%20and%20scene%20types.%20We%20extensively%20evaluate%20the%20zero-shot%20capabilities%20of%0Aour%20model%20on%20five%20public%20datasets%2C%20showcasing%20its%20impressive%20ability%20to%0Ageneralize%20to%20new%2C%20unseen%20data.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo%2520Anything%253A%2520Unifying%2520Stereo%2520Matching%2520with%2520Large-Scale%2520Mixed%2520Data%26entry.906535625%3DXianda%2520Guo%2520and%2520Chenming%2520Zhang%2520and%2520Youmin%2520Zhang%2520and%2520Dujun%2520Nie%2520and%2520Ruilin%2520Wang%2520and%2520Wenzhao%2520Zheng%2520and%2520Matteo%2520Poggi%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520has%2520been%2520a%2520pivotal%2520component%2520in%25203D%2520vision%252C%2520aiming%2520to%2520find%250Acorresponding%2520points%2520between%2520pairs%2520of%2520stereo%2520images%2520to%2520recover%2520depth%250Ainformation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520StereoAnything%252C%2520a%2520highly%2520practical%250Asolution%2520for%2520robust%2520stereo%2520matching.%2520Rather%2520than%2520focusing%2520on%2520a%2520specialized%250Amodel%252C%2520our%2520goal%2520is%2520to%2520develop%2520a%2520versatile%2520foundational%2520model%2520capable%2520of%250Ahandling%2520stereo%2520images%2520across%2520diverse%2520environments.%2520To%2520this%2520end%252C%2520we%2520scale%2520up%250Athe%2520dataset%2520by%2520collecting%2520labeled%2520stereo%2520images%2520and%2520generating%2520synthetic%2520stereo%250Apairs%2520from%2520unlabeled%2520monocular%2520images.%2520To%2520further%2520enrich%2520the%2520model%2527s%2520ability%2520to%250Ageneralize%2520across%2520different%2520conditions%252C%2520we%2520introduce%2520a%2520novel%2520synthetic%2520dataset%250Athat%2520complements%2520existing%2520data%2520by%2520adding%2520variability%2520in%2520baselines%252C%2520camera%250Aangles%252C%2520and%2520scene%2520types.%2520We%2520extensively%2520evaluate%2520the%2520zero-shot%2520capabilities%2520of%250Aour%2520model%2520on%2520five%2520public%2520datasets%252C%2520showcasing%2520its%2520impressive%2520ability%2520to%250Ageneralize%2520to%2520new%252C%2520unseen%2520data.%2520Code%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/XiandaGuo/OpenStereo%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo%20Anything%3A%20Unifying%20Stereo%20Matching%20with%20Large-Scale%20Mixed%20Data&entry.906535625=Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Youmin%20Zhang%20and%20Dujun%20Nie%20and%20Ruilin%20Wang%20and%20Wenzhao%20Zheng%20and%20Matteo%20Poggi%20and%20Long%20Chen&entry.1292438233=%20%20Stereo%20matching%20has%20been%20a%20pivotal%20component%20in%203D%20vision%2C%20aiming%20to%20find%0Acorresponding%20points%20between%20pairs%20of%20stereo%20images%20to%20recover%20depth%0Ainformation.%20In%20this%20work%2C%20we%20introduce%20StereoAnything%2C%20a%20highly%20practical%0Asolution%20for%20robust%20stereo%20matching.%20Rather%20than%20focusing%20on%20a%20specialized%0Amodel%2C%20our%20goal%20is%20to%20develop%20a%20versatile%20foundational%20model%20capable%20of%0Ahandling%20stereo%20images%20across%20diverse%20environments.%20To%20this%20end%2C%20we%20scale%20up%0Athe%20dataset%20by%20collecting%20labeled%20stereo%20images%20and%20generating%20synthetic%20stereo%0Apairs%20from%20unlabeled%20monocular%20images.%20To%20further%20enrich%20the%20model%27s%20ability%20to%0Ageneralize%20across%20different%20conditions%2C%20we%20introduce%20a%20novel%20synthetic%20dataset%0Athat%20complements%20existing%20data%20by%20adding%20variability%20in%20baselines%2C%20camera%0Aangles%2C%20and%20scene%20types.%20We%20extensively%20evaluate%20the%20zero-shot%20capabilities%20of%0Aour%20model%20on%20five%20public%20datasets%2C%20showcasing%20its%20impressive%20ability%20to%0Ageneralize%20to%20new%2C%20unseen%20data.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14053v1&entry.124074799=Read"},
{"title": "Multimodal 3D Brain Tumor Segmentation with Adversarial Training and\n  Conditional Random Field", "author": "Lan Jiang and Yuchao Zheng and Miao Yu and Haiqing Zhang and Fatemah Aladwani and Alessandro Perelli", "abstract": "  Accurate brain tumor segmentation remains a challenging task due to\nstructural complexity and great individual differences of gliomas. Leveraging\nthe pre-eminent detail resilience of CRF and spatial feature extraction\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\nV-net improvement, adds conditional random field after generator and use\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\nGan, FCN and 3D V-net, reaching specificity over 99.8%.\n", "link": "http://arxiv.org/abs/2411.14418v1", "date": "2024-11-21", "relevancy": 2.6213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5129}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%203D%20Brain%20Tumor%20Segmentation%20with%20Adversarial%20Training%20and%0A%20%20Conditional%20Random%20Field&body=Title%3A%20Multimodal%203D%20Brain%20Tumor%20Segmentation%20with%20Adversarial%20Training%20and%0A%20%20Conditional%20Random%20Field%0AAuthor%3A%20Lan%20Jiang%20and%20Yuchao%20Zheng%20and%20Miao%20Yu%20and%20Haiqing%20Zhang%20and%20Fatemah%20Aladwani%20and%20Alessandro%20Perelli%0AAbstract%3A%20%20%20Accurate%20brain%20tumor%20segmentation%20remains%20a%20challenging%20task%20due%20to%0Astructural%20complexity%20and%20great%20individual%20differences%20of%20gliomas.%20Leveraging%0Athe%20pre-eminent%20detail%20resilience%20of%20CRF%20and%20spatial%20feature%20extraction%0Acapacity%20of%20V-net%2C%20we%20propose%20a%20multimodal%203D%20Volume%20Generative%20Adversarial%0ANetwork%20%283D-vGAN%29%20for%20precise%20segmentation.%20The%20model%20utilizes%20Pseudo-3D%20for%0AV-net%20improvement%2C%20adds%20conditional%20random%20field%20after%20generator%20and%20use%0Aoriginal%20image%20as%20supplemental%20guidance.%20Results%2C%20using%20the%20BraTS-2018%20dataset%2C%0Ashow%20that%203D-vGAN%20outperforms%20classical%20segmentation%20models%2C%20including%20U-net%2C%0AGan%2C%20FCN%20and%203D%20V-net%2C%20reaching%20specificity%20over%2099.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%25203D%2520Brain%2520Tumor%2520Segmentation%2520with%2520Adversarial%2520Training%2520and%250A%2520%2520Conditional%2520Random%2520Field%26entry.906535625%3DLan%2520Jiang%2520and%2520Yuchao%2520Zheng%2520and%2520Miao%2520Yu%2520and%2520Haiqing%2520Zhang%2520and%2520Fatemah%2520Aladwani%2520and%2520Alessandro%2520Perelli%26entry.1292438233%3D%2520%2520Accurate%2520brain%2520tumor%2520segmentation%2520remains%2520a%2520challenging%2520task%2520due%2520to%250Astructural%2520complexity%2520and%2520great%2520individual%2520differences%2520of%2520gliomas.%2520Leveraging%250Athe%2520pre-eminent%2520detail%2520resilience%2520of%2520CRF%2520and%2520spatial%2520feature%2520extraction%250Acapacity%2520of%2520V-net%252C%2520we%2520propose%2520a%2520multimodal%25203D%2520Volume%2520Generative%2520Adversarial%250ANetwork%2520%25283D-vGAN%2529%2520for%2520precise%2520segmentation.%2520The%2520model%2520utilizes%2520Pseudo-3D%2520for%250AV-net%2520improvement%252C%2520adds%2520conditional%2520random%2520field%2520after%2520generator%2520and%2520use%250Aoriginal%2520image%2520as%2520supplemental%2520guidance.%2520Results%252C%2520using%2520the%2520BraTS-2018%2520dataset%252C%250Ashow%2520that%25203D-vGAN%2520outperforms%2520classical%2520segmentation%2520models%252C%2520including%2520U-net%252C%250AGan%252C%2520FCN%2520and%25203D%2520V-net%252C%2520reaching%2520specificity%2520over%252099.8%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%203D%20Brain%20Tumor%20Segmentation%20with%20Adversarial%20Training%20and%0A%20%20Conditional%20Random%20Field&entry.906535625=Lan%20Jiang%20and%20Yuchao%20Zheng%20and%20Miao%20Yu%20and%20Haiqing%20Zhang%20and%20Fatemah%20Aladwani%20and%20Alessandro%20Perelli&entry.1292438233=%20%20Accurate%20brain%20tumor%20segmentation%20remains%20a%20challenging%20task%20due%20to%0Astructural%20complexity%20and%20great%20individual%20differences%20of%20gliomas.%20Leveraging%0Athe%20pre-eminent%20detail%20resilience%20of%20CRF%20and%20spatial%20feature%20extraction%0Acapacity%20of%20V-net%2C%20we%20propose%20a%20multimodal%203D%20Volume%20Generative%20Adversarial%0ANetwork%20%283D-vGAN%29%20for%20precise%20segmentation.%20The%20model%20utilizes%20Pseudo-3D%20for%0AV-net%20improvement%2C%20adds%20conditional%20random%20field%20after%20generator%20and%20use%0Aoriginal%20image%20as%20supplemental%20guidance.%20Results%2C%20using%20the%20BraTS-2018%20dataset%2C%0Ashow%20that%203D-vGAN%20outperforms%20classical%20segmentation%20models%2C%20including%20U-net%2C%0AGan%2C%20FCN%20and%203D%20V-net%2C%20reaching%20specificity%20over%2099.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14418v1&entry.124074799=Read"},
{"title": "Towards Context-Rich Automated Biodiversity Assessments: Deriving\n  AI-Powered Insights from Camera Trap Data", "author": "Paul Fergus and Carl Chalmers and Naomi Matthews and Stuart Nixon and Andre Burger and Oliver Hartley and Chris Sutherland and Xavier Lambin and Steven Longmore and Serge Wich", "abstract": "  Camera traps offer enormous new opportunities in ecological studies, but\ncurrent automated image analysis methods often lack the contextual richness\nneeded to support impactful conservation outcomes. Here we present an\nintegrated approach that combines deep learning-based vision and language\nmodels to improve ecological reporting using data from camera traps. We\nintroduce a two-stage system: YOLOv10-X to localise and classify species\n(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read\nYOLOv10-X binding box labels to identify species, overcoming its limitation\nwith hard to classify objects in images. Additionally, Phi-3.5 detects broader\nvariables, such as vegetation type, and time of day, providing rich ecological\nand environmental context to YOLO's species detection output. When combined,\nthis output is processed by the model's natural language system to answer\ncomplex queries, and retrieval-augmented generation (RAG) is employed to enrich\nresponses with external information, like species weight and IUCN status\n(information that cannot be obtained through direct visual analysis). This\ninformation is used to automatically generate structured reports, providing\nbiodiversity stakeholders with deeper insights into, for example, species\nabundance, distribution, animal behaviour, and habitat selection. Our approach\ndelivers contextually rich narratives that aid in wildlife management\ndecisions. By providing contextually rich insights, our approach not only\nreduces manual effort but also supports timely decision-making in conservation,\npotentially shifting efforts from reactive to proactive management.\n", "link": "http://arxiv.org/abs/2411.14219v1", "date": "2024-11-21", "relevancy": 2.6087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Context-Rich%20Automated%20Biodiversity%20Assessments%3A%20Deriving%0A%20%20AI-Powered%20Insights%20from%20Camera%20Trap%20Data&body=Title%3A%20Towards%20Context-Rich%20Automated%20Biodiversity%20Assessments%3A%20Deriving%0A%20%20AI-Powered%20Insights%20from%20Camera%20Trap%20Data%0AAuthor%3A%20Paul%20Fergus%20and%20Carl%20Chalmers%20and%20Naomi%20Matthews%20and%20Stuart%20Nixon%20and%20Andre%20Burger%20and%20Oliver%20Hartley%20and%20Chris%20Sutherland%20and%20Xavier%20Lambin%20and%20Steven%20Longmore%20and%20Serge%20Wich%0AAbstract%3A%20%20%20Camera%20traps%20offer%20enormous%20new%20opportunities%20in%20ecological%20studies%2C%20but%0Acurrent%20automated%20image%20analysis%20methods%20often%20lack%20the%20contextual%20richness%0Aneeded%20to%20support%20impactful%20conservation%20outcomes.%20Here%20we%20present%20an%0Aintegrated%20approach%20that%20combines%20deep%20learning-based%20vision%20and%20language%0Amodels%20to%20improve%20ecological%20reporting%20using%20data%20from%20camera%20traps.%20We%0Aintroduce%20a%20two-stage%20system%3A%20YOLOv10-X%20to%20localise%20and%20classify%20species%0A%28mammals%20and%20birds%29%20within%20images%2C%20and%20a%20Phi-3.5-vision-instruct%20model%20to%20read%0AYOLOv10-X%20binding%20box%20labels%20to%20identify%20species%2C%20overcoming%20its%20limitation%0Awith%20hard%20to%20classify%20objects%20in%20images.%20Additionally%2C%20Phi-3.5%20detects%20broader%0Avariables%2C%20such%20as%20vegetation%20type%2C%20and%20time%20of%20day%2C%20providing%20rich%20ecological%0Aand%20environmental%20context%20to%20YOLO%27s%20species%20detection%20output.%20When%20combined%2C%0Athis%20output%20is%20processed%20by%20the%20model%27s%20natural%20language%20system%20to%20answer%0Acomplex%20queries%2C%20and%20retrieval-augmented%20generation%20%28RAG%29%20is%20employed%20to%20enrich%0Aresponses%20with%20external%20information%2C%20like%20species%20weight%20and%20IUCN%20status%0A%28information%20that%20cannot%20be%20obtained%20through%20direct%20visual%20analysis%29.%20This%0Ainformation%20is%20used%20to%20automatically%20generate%20structured%20reports%2C%20providing%0Abiodiversity%20stakeholders%20with%20deeper%20insights%20into%2C%20for%20example%2C%20species%0Aabundance%2C%20distribution%2C%20animal%20behaviour%2C%20and%20habitat%20selection.%20Our%20approach%0Adelivers%20contextually%20rich%20narratives%20that%20aid%20in%20wildlife%20management%0Adecisions.%20By%20providing%20contextually%20rich%20insights%2C%20our%20approach%20not%20only%0Areduces%20manual%20effort%20but%20also%20supports%20timely%20decision-making%20in%20conservation%2C%0Apotentially%20shifting%20efforts%20from%20reactive%20to%20proactive%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Context-Rich%2520Automated%2520Biodiversity%2520Assessments%253A%2520Deriving%250A%2520%2520AI-Powered%2520Insights%2520from%2520Camera%2520Trap%2520Data%26entry.906535625%3DPaul%2520Fergus%2520and%2520Carl%2520Chalmers%2520and%2520Naomi%2520Matthews%2520and%2520Stuart%2520Nixon%2520and%2520Andre%2520Burger%2520and%2520Oliver%2520Hartley%2520and%2520Chris%2520Sutherland%2520and%2520Xavier%2520Lambin%2520and%2520Steven%2520Longmore%2520and%2520Serge%2520Wich%26entry.1292438233%3D%2520%2520Camera%2520traps%2520offer%2520enormous%2520new%2520opportunities%2520in%2520ecological%2520studies%252C%2520but%250Acurrent%2520automated%2520image%2520analysis%2520methods%2520often%2520lack%2520the%2520contextual%2520richness%250Aneeded%2520to%2520support%2520impactful%2520conservation%2520outcomes.%2520Here%2520we%2520present%2520an%250Aintegrated%2520approach%2520that%2520combines%2520deep%2520learning-based%2520vision%2520and%2520language%250Amodels%2520to%2520improve%2520ecological%2520reporting%2520using%2520data%2520from%2520camera%2520traps.%2520We%250Aintroduce%2520a%2520two-stage%2520system%253A%2520YOLOv10-X%2520to%2520localise%2520and%2520classify%2520species%250A%2528mammals%2520and%2520birds%2529%2520within%2520images%252C%2520and%2520a%2520Phi-3.5-vision-instruct%2520model%2520to%2520read%250AYOLOv10-X%2520binding%2520box%2520labels%2520to%2520identify%2520species%252C%2520overcoming%2520its%2520limitation%250Awith%2520hard%2520to%2520classify%2520objects%2520in%2520images.%2520Additionally%252C%2520Phi-3.5%2520detects%2520broader%250Avariables%252C%2520such%2520as%2520vegetation%2520type%252C%2520and%2520time%2520of%2520day%252C%2520providing%2520rich%2520ecological%250Aand%2520environmental%2520context%2520to%2520YOLO%2527s%2520species%2520detection%2520output.%2520When%2520combined%252C%250Athis%2520output%2520is%2520processed%2520by%2520the%2520model%2527s%2520natural%2520language%2520system%2520to%2520answer%250Acomplex%2520queries%252C%2520and%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520is%2520employed%2520to%2520enrich%250Aresponses%2520with%2520external%2520information%252C%2520like%2520species%2520weight%2520and%2520IUCN%2520status%250A%2528information%2520that%2520cannot%2520be%2520obtained%2520through%2520direct%2520visual%2520analysis%2529.%2520This%250Ainformation%2520is%2520used%2520to%2520automatically%2520generate%2520structured%2520reports%252C%2520providing%250Abiodiversity%2520stakeholders%2520with%2520deeper%2520insights%2520into%252C%2520for%2520example%252C%2520species%250Aabundance%252C%2520distribution%252C%2520animal%2520behaviour%252C%2520and%2520habitat%2520selection.%2520Our%2520approach%250Adelivers%2520contextually%2520rich%2520narratives%2520that%2520aid%2520in%2520wildlife%2520management%250Adecisions.%2520By%2520providing%2520contextually%2520rich%2520insights%252C%2520our%2520approach%2520not%2520only%250Areduces%2520manual%2520effort%2520but%2520also%2520supports%2520timely%2520decision-making%2520in%2520conservation%252C%250Apotentially%2520shifting%2520efforts%2520from%2520reactive%2520to%2520proactive%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Context-Rich%20Automated%20Biodiversity%20Assessments%3A%20Deriving%0A%20%20AI-Powered%20Insights%20from%20Camera%20Trap%20Data&entry.906535625=Paul%20Fergus%20and%20Carl%20Chalmers%20and%20Naomi%20Matthews%20and%20Stuart%20Nixon%20and%20Andre%20Burger%20and%20Oliver%20Hartley%20and%20Chris%20Sutherland%20and%20Xavier%20Lambin%20and%20Steven%20Longmore%20and%20Serge%20Wich&entry.1292438233=%20%20Camera%20traps%20offer%20enormous%20new%20opportunities%20in%20ecological%20studies%2C%20but%0Acurrent%20automated%20image%20analysis%20methods%20often%20lack%20the%20contextual%20richness%0Aneeded%20to%20support%20impactful%20conservation%20outcomes.%20Here%20we%20present%20an%0Aintegrated%20approach%20that%20combines%20deep%20learning-based%20vision%20and%20language%0Amodels%20to%20improve%20ecological%20reporting%20using%20data%20from%20camera%20traps.%20We%0Aintroduce%20a%20two-stage%20system%3A%20YOLOv10-X%20to%20localise%20and%20classify%20species%0A%28mammals%20and%20birds%29%20within%20images%2C%20and%20a%20Phi-3.5-vision-instruct%20model%20to%20read%0AYOLOv10-X%20binding%20box%20labels%20to%20identify%20species%2C%20overcoming%20its%20limitation%0Awith%20hard%20to%20classify%20objects%20in%20images.%20Additionally%2C%20Phi-3.5%20detects%20broader%0Avariables%2C%20such%20as%20vegetation%20type%2C%20and%20time%20of%20day%2C%20providing%20rich%20ecological%0Aand%20environmental%20context%20to%20YOLO%27s%20species%20detection%20output.%20When%20combined%2C%0Athis%20output%20is%20processed%20by%20the%20model%27s%20natural%20language%20system%20to%20answer%0Acomplex%20queries%2C%20and%20retrieval-augmented%20generation%20%28RAG%29%20is%20employed%20to%20enrich%0Aresponses%20with%20external%20information%2C%20like%20species%20weight%20and%20IUCN%20status%0A%28information%20that%20cannot%20be%20obtained%20through%20direct%20visual%20analysis%29.%20This%0Ainformation%20is%20used%20to%20automatically%20generate%20structured%20reports%2C%20providing%0Abiodiversity%20stakeholders%20with%20deeper%20insights%20into%2C%20for%20example%2C%20species%0Aabundance%2C%20distribution%2C%20animal%20behaviour%2C%20and%20habitat%20selection.%20Our%20approach%0Adelivers%20contextually%20rich%20narratives%20that%20aid%20in%20wildlife%20management%0Adecisions.%20By%20providing%20contextually%20rich%20insights%2C%20our%20approach%20not%20only%0Areduces%20manual%20effort%20but%20also%20supports%20timely%20decision-making%20in%20conservation%2C%0Apotentially%20shifting%20efforts%20from%20reactive%20to%20proactive%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14219v1&entry.124074799=Read"},
{"title": "Data-centric Graph Learning: A Survey", "author": "Yuxin Guo and Deyu Bo and Cheng Yang and Zhiyuan Lu and Zhongjian Zhang and Jixi Liu and Yufei Peng and Chuan Shi", "abstract": "  The history of artificial intelligence (AI) has witnessed the significant\nimpact of high-quality data on various deep learning models, such as ImageNet\nfor AlexNet and ResNet. Recently, instead of designing more complex neural\narchitectures as model-centric approaches, the attention of AI community has\nshifted to data-centric ones, which focuses on better processing data to\nstrengthen the ability of neural models. Graph learning, which operates on\nubiquitous topological data, also plays an important role in the era of deep\nlearning. In this survey, we comprehensively review graph learning approaches\nfrom the data-centric perspective, and aim to answer three crucial questions:\n(1) when to modify graph data, (2) what part of the graph data needs\nmodification to unlock the potential of various graph models, and (3) how to\nsafeguard graph models from problematic data influence. Accordingly, we propose\na novel taxonomy based on the stages in the graph learning pipeline, and\nhighlight the processing methods for different data structures in the graph\ndata, i.e., topology, feature and label. Furthermore, we analyze some potential\nproblems embedded in graph data and discuss how to solve them in a data-centric\nmanner. Finally, we provide some promising future directions for data-centric\ngraph learning.\n", "link": "http://arxiv.org/abs/2310.04987v3", "date": "2024-11-21", "relevancy": 2.5734, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-centric%20Graph%20Learning%3A%20A%20Survey&body=Title%3A%20Data-centric%20Graph%20Learning%3A%20A%20Survey%0AAuthor%3A%20Yuxin%20Guo%20and%20Deyu%20Bo%20and%20Cheng%20Yang%20and%20Zhiyuan%20Lu%20and%20Zhongjian%20Zhang%20and%20Jixi%20Liu%20and%20Yufei%20Peng%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20The%20history%20of%20artificial%20intelligence%20%28AI%29%20has%20witnessed%20the%20significant%0Aimpact%20of%20high-quality%20data%20on%20various%20deep%20learning%20models%2C%20such%20as%20ImageNet%0Afor%20AlexNet%20and%20ResNet.%20Recently%2C%20instead%20of%20designing%20more%20complex%20neural%0Aarchitectures%20as%20model-centric%20approaches%2C%20the%20attention%20of%20AI%20community%20has%0Ashifted%20to%20data-centric%20ones%2C%20which%20focuses%20on%20better%20processing%20data%20to%0Astrengthen%20the%20ability%20of%20neural%20models.%20Graph%20learning%2C%20which%20operates%20on%0Aubiquitous%20topological%20data%2C%20also%20plays%20an%20important%20role%20in%20the%20era%20of%20deep%0Alearning.%20In%20this%20survey%2C%20we%20comprehensively%20review%20graph%20learning%20approaches%0Afrom%20the%20data-centric%20perspective%2C%20and%20aim%20to%20answer%20three%20crucial%20questions%3A%0A%281%29%20when%20to%20modify%20graph%20data%2C%20%282%29%20what%20part%20of%20the%20graph%20data%20needs%0Amodification%20to%20unlock%20the%20potential%20of%20various%20graph%20models%2C%20and%20%283%29%20how%20to%0Asafeguard%20graph%20models%20from%20problematic%20data%20influence.%20Accordingly%2C%20we%20propose%0Aa%20novel%20taxonomy%20based%20on%20the%20stages%20in%20the%20graph%20learning%20pipeline%2C%20and%0Ahighlight%20the%20processing%20methods%20for%20different%20data%20structures%20in%20the%20graph%0Adata%2C%20i.e.%2C%20topology%2C%20feature%20and%20label.%20Furthermore%2C%20we%20analyze%20some%20potential%0Aproblems%20embedded%20in%20graph%20data%20and%20discuss%20how%20to%20solve%20them%20in%20a%20data-centric%0Amanner.%20Finally%2C%20we%20provide%20some%20promising%20future%20directions%20for%20data-centric%0Agraph%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04987v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-centric%2520Graph%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DYuxin%2520Guo%2520and%2520Deyu%2520Bo%2520and%2520Cheng%2520Yang%2520and%2520Zhiyuan%2520Lu%2520and%2520Zhongjian%2520Zhang%2520and%2520Jixi%2520Liu%2520and%2520Yufei%2520Peng%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520The%2520history%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520witnessed%2520the%2520significant%250Aimpact%2520of%2520high-quality%2520data%2520on%2520various%2520deep%2520learning%2520models%252C%2520such%2520as%2520ImageNet%250Afor%2520AlexNet%2520and%2520ResNet.%2520Recently%252C%2520instead%2520of%2520designing%2520more%2520complex%2520neural%250Aarchitectures%2520as%2520model-centric%2520approaches%252C%2520the%2520attention%2520of%2520AI%2520community%2520has%250Ashifted%2520to%2520data-centric%2520ones%252C%2520which%2520focuses%2520on%2520better%2520processing%2520data%2520to%250Astrengthen%2520the%2520ability%2520of%2520neural%2520models.%2520Graph%2520learning%252C%2520which%2520operates%2520on%250Aubiquitous%2520topological%2520data%252C%2520also%2520plays%2520an%2520important%2520role%2520in%2520the%2520era%2520of%2520deep%250Alearning.%2520In%2520this%2520survey%252C%2520we%2520comprehensively%2520review%2520graph%2520learning%2520approaches%250Afrom%2520the%2520data-centric%2520perspective%252C%2520and%2520aim%2520to%2520answer%2520three%2520crucial%2520questions%253A%250A%25281%2529%2520when%2520to%2520modify%2520graph%2520data%252C%2520%25282%2529%2520what%2520part%2520of%2520the%2520graph%2520data%2520needs%250Amodification%2520to%2520unlock%2520the%2520potential%2520of%2520various%2520graph%2520models%252C%2520and%2520%25283%2529%2520how%2520to%250Asafeguard%2520graph%2520models%2520from%2520problematic%2520data%2520influence.%2520Accordingly%252C%2520we%2520propose%250Aa%2520novel%2520taxonomy%2520based%2520on%2520the%2520stages%2520in%2520the%2520graph%2520learning%2520pipeline%252C%2520and%250Ahighlight%2520the%2520processing%2520methods%2520for%2520different%2520data%2520structures%2520in%2520the%2520graph%250Adata%252C%2520i.e.%252C%2520topology%252C%2520feature%2520and%2520label.%2520Furthermore%252C%2520we%2520analyze%2520some%2520potential%250Aproblems%2520embedded%2520in%2520graph%2520data%2520and%2520discuss%2520how%2520to%2520solve%2520them%2520in%2520a%2520data-centric%250Amanner.%2520Finally%252C%2520we%2520provide%2520some%2520promising%2520future%2520directions%2520for%2520data-centric%250Agraph%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04987v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-centric%20Graph%20Learning%3A%20A%20Survey&entry.906535625=Yuxin%20Guo%20and%20Deyu%20Bo%20and%20Cheng%20Yang%20and%20Zhiyuan%20Lu%20and%20Zhongjian%20Zhang%20and%20Jixi%20Liu%20and%20Yufei%20Peng%20and%20Chuan%20Shi&entry.1292438233=%20%20The%20history%20of%20artificial%20intelligence%20%28AI%29%20has%20witnessed%20the%20significant%0Aimpact%20of%20high-quality%20data%20on%20various%20deep%20learning%20models%2C%20such%20as%20ImageNet%0Afor%20AlexNet%20and%20ResNet.%20Recently%2C%20instead%20of%20designing%20more%20complex%20neural%0Aarchitectures%20as%20model-centric%20approaches%2C%20the%20attention%20of%20AI%20community%20has%0Ashifted%20to%20data-centric%20ones%2C%20which%20focuses%20on%20better%20processing%20data%20to%0Astrengthen%20the%20ability%20of%20neural%20models.%20Graph%20learning%2C%20which%20operates%20on%0Aubiquitous%20topological%20data%2C%20also%20plays%20an%20important%20role%20in%20the%20era%20of%20deep%0Alearning.%20In%20this%20survey%2C%20we%20comprehensively%20review%20graph%20learning%20approaches%0Afrom%20the%20data-centric%20perspective%2C%20and%20aim%20to%20answer%20three%20crucial%20questions%3A%0A%281%29%20when%20to%20modify%20graph%20data%2C%20%282%29%20what%20part%20of%20the%20graph%20data%20needs%0Amodification%20to%20unlock%20the%20potential%20of%20various%20graph%20models%2C%20and%20%283%29%20how%20to%0Asafeguard%20graph%20models%20from%20problematic%20data%20influence.%20Accordingly%2C%20we%20propose%0Aa%20novel%20taxonomy%20based%20on%20the%20stages%20in%20the%20graph%20learning%20pipeline%2C%20and%0Ahighlight%20the%20processing%20methods%20for%20different%20data%20structures%20in%20the%20graph%0Adata%2C%20i.e.%2C%20topology%2C%20feature%20and%20label.%20Furthermore%2C%20we%20analyze%20some%20potential%0Aproblems%20embedded%20in%20graph%20data%20and%20discuss%20how%20to%20solve%20them%20in%20a%20data-centric%0Amanner.%20Finally%2C%20we%20provide%20some%20promising%20future%20directions%20for%20data-centric%0Agraph%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04987v3&entry.124074799=Read"},
{"title": "CTVR-EHO TDA-IPH Topological Optimized Convolutional Visual Recurrent\n  Network for Brain Tumor Segmentation and Classification", "author": "Dhananjay Joshi and Bhupesh Kumar Singh and Kapil Kumar Nagwanshi and Nitin S. Choubey", "abstract": "  In today's world of health care, brain tumor detection has become common.\nHowever, the manual brain tumor classification approach is time-consuming. So\nDeep Convolutional Neural Network (DCNN) is used by many researchers in the\nmedical field for making accurate diagnoses and aiding in the patient's\ntreatment. The traditional techniques have problems such as overfitting and the\ninability to extract necessary features. To overcome these problems, we\ndeveloped the Topological Data Analysis based Improved Persistent Homology\n(TDA-IPH) and Convolutional Transfer learning and Visual Recurrent learning\nwith Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO) models for\nbrain tumor segmentation and classification. Initially, the Topological Data\nAnalysis based Improved Persistent Homology is designed to segment the brain\ntumor image. Then, from the segmented image, features are extracted using TL\nvia the AlexNet model and Bidirectional Visual Long Short-Term Memory\n(Bi-VLSTM). Next, elephant Herding Optimization (EHO) is used to tune the\nhyperparameters of both networks to get an optimal result. Finally, extracted\nfeatures are concatenated and classified using the softmax activation layer.\nThe simulation result of this proposed CTVR-EHO and TDA-IPH method is analyzed\nbased on precision, accuracy, recall, loss, and F score metrics. When compared\nto other existing brain tumor segmentation and classification models, the\nproposed CTVR-EHO and TDA-IPH approaches show high accuracy (99.8%), high\nrecall (99.23%), high precision (99.67%), and high F score (99.59%).\n", "link": "http://arxiv.org/abs/2207.13021v3", "date": "2024-11-21", "relevancy": 2.5704, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTVR-EHO%20TDA-IPH%20Topological%20Optimized%20Convolutional%20Visual%20Recurrent%0A%20%20Network%20for%20Brain%20Tumor%20Segmentation%20and%20Classification&body=Title%3A%20CTVR-EHO%20TDA-IPH%20Topological%20Optimized%20Convolutional%20Visual%20Recurrent%0A%20%20Network%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0AAuthor%3A%20Dhananjay%20Joshi%20and%20Bhupesh%20Kumar%20Singh%20and%20Kapil%20Kumar%20Nagwanshi%20and%20Nitin%20S.%20Choubey%0AAbstract%3A%20%20%20In%20today%27s%20world%20of%20health%20care%2C%20brain%20tumor%20detection%20has%20become%20common.%0AHowever%2C%20the%20manual%20brain%20tumor%20classification%20approach%20is%20time-consuming.%20So%0ADeep%20Convolutional%20Neural%20Network%20%28DCNN%29%20is%20used%20by%20many%20researchers%20in%20the%0Amedical%20field%20for%20making%20accurate%20diagnoses%20and%20aiding%20in%20the%20patient%27s%0Atreatment.%20The%20traditional%20techniques%20have%20problems%20such%20as%20overfitting%20and%20the%0Ainability%20to%20extract%20necessary%20features.%20To%20overcome%20these%20problems%2C%20we%0Adeveloped%20the%20Topological%20Data%20Analysis%20based%20Improved%20Persistent%20Homology%0A%28TDA-IPH%29%20and%20Convolutional%20Transfer%20learning%20and%20Visual%20Recurrent%20learning%0Awith%20Elephant%20Herding%20Optimization%20hyper-parameter%20tuning%20%28CTVR-EHO%29%20models%20for%0Abrain%20tumor%20segmentation%20and%20classification.%20Initially%2C%20the%20Topological%20Data%0AAnalysis%20based%20Improved%20Persistent%20Homology%20is%20designed%20to%20segment%20the%20brain%0Atumor%20image.%20Then%2C%20from%20the%20segmented%20image%2C%20features%20are%20extracted%20using%20TL%0Avia%20the%20AlexNet%20model%20and%20Bidirectional%20Visual%20Long%20Short-Term%20Memory%0A%28Bi-VLSTM%29.%20Next%2C%20elephant%20Herding%20Optimization%20%28EHO%29%20is%20used%20to%20tune%20the%0Ahyperparameters%20of%20both%20networks%20to%20get%20an%20optimal%20result.%20Finally%2C%20extracted%0Afeatures%20are%20concatenated%20and%20classified%20using%20the%20softmax%20activation%20layer.%0AThe%20simulation%20result%20of%20this%20proposed%20CTVR-EHO%20and%20TDA-IPH%20method%20is%20analyzed%0Abased%20on%20precision%2C%20accuracy%2C%20recall%2C%20loss%2C%20and%20F%20score%20metrics.%20When%20compared%0Ato%20other%20existing%20brain%20tumor%20segmentation%20and%20classification%20models%2C%20the%0Aproposed%20CTVR-EHO%20and%20TDA-IPH%20approaches%20show%20high%20accuracy%20%2899.8%25%29%2C%20high%0Arecall%20%2899.23%25%29%2C%20high%20precision%20%2899.67%25%29%2C%20and%20high%20F%20score%20%2899.59%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.13021v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTVR-EHO%2520TDA-IPH%2520Topological%2520Optimized%2520Convolutional%2520Visual%2520Recurrent%250A%2520%2520Network%2520for%2520Brain%2520Tumor%2520Segmentation%2520and%2520Classification%26entry.906535625%3DDhananjay%2520Joshi%2520and%2520Bhupesh%2520Kumar%2520Singh%2520and%2520Kapil%2520Kumar%2520Nagwanshi%2520and%2520Nitin%2520S.%2520Choubey%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520world%2520of%2520health%2520care%252C%2520brain%2520tumor%2520detection%2520has%2520become%2520common.%250AHowever%252C%2520the%2520manual%2520brain%2520tumor%2520classification%2520approach%2520is%2520time-consuming.%2520So%250ADeep%2520Convolutional%2520Neural%2520Network%2520%2528DCNN%2529%2520is%2520used%2520by%2520many%2520researchers%2520in%2520the%250Amedical%2520field%2520for%2520making%2520accurate%2520diagnoses%2520and%2520aiding%2520in%2520the%2520patient%2527s%250Atreatment.%2520The%2520traditional%2520techniques%2520have%2520problems%2520such%2520as%2520overfitting%2520and%2520the%250Ainability%2520to%2520extract%2520necessary%2520features.%2520To%2520overcome%2520these%2520problems%252C%2520we%250Adeveloped%2520the%2520Topological%2520Data%2520Analysis%2520based%2520Improved%2520Persistent%2520Homology%250A%2528TDA-IPH%2529%2520and%2520Convolutional%2520Transfer%2520learning%2520and%2520Visual%2520Recurrent%2520learning%250Awith%2520Elephant%2520Herding%2520Optimization%2520hyper-parameter%2520tuning%2520%2528CTVR-EHO%2529%2520models%2520for%250Abrain%2520tumor%2520segmentation%2520and%2520classification.%2520Initially%252C%2520the%2520Topological%2520Data%250AAnalysis%2520based%2520Improved%2520Persistent%2520Homology%2520is%2520designed%2520to%2520segment%2520the%2520brain%250Atumor%2520image.%2520Then%252C%2520from%2520the%2520segmented%2520image%252C%2520features%2520are%2520extracted%2520using%2520TL%250Avia%2520the%2520AlexNet%2520model%2520and%2520Bidirectional%2520Visual%2520Long%2520Short-Term%2520Memory%250A%2528Bi-VLSTM%2529.%2520Next%252C%2520elephant%2520Herding%2520Optimization%2520%2528EHO%2529%2520is%2520used%2520to%2520tune%2520the%250Ahyperparameters%2520of%2520both%2520networks%2520to%2520get%2520an%2520optimal%2520result.%2520Finally%252C%2520extracted%250Afeatures%2520are%2520concatenated%2520and%2520classified%2520using%2520the%2520softmax%2520activation%2520layer.%250AThe%2520simulation%2520result%2520of%2520this%2520proposed%2520CTVR-EHO%2520and%2520TDA-IPH%2520method%2520is%2520analyzed%250Abased%2520on%2520precision%252C%2520accuracy%252C%2520recall%252C%2520loss%252C%2520and%2520F%2520score%2520metrics.%2520When%2520compared%250Ato%2520other%2520existing%2520brain%2520tumor%2520segmentation%2520and%2520classification%2520models%252C%2520the%250Aproposed%2520CTVR-EHO%2520and%2520TDA-IPH%2520approaches%2520show%2520high%2520accuracy%2520%252899.8%2525%2529%252C%2520high%250Arecall%2520%252899.23%2525%2529%252C%2520high%2520precision%2520%252899.67%2525%2529%252C%2520and%2520high%2520F%2520score%2520%252899.59%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.13021v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTVR-EHO%20TDA-IPH%20Topological%20Optimized%20Convolutional%20Visual%20Recurrent%0A%20%20Network%20for%20Brain%20Tumor%20Segmentation%20and%20Classification&entry.906535625=Dhananjay%20Joshi%20and%20Bhupesh%20Kumar%20Singh%20and%20Kapil%20Kumar%20Nagwanshi%20and%20Nitin%20S.%20Choubey&entry.1292438233=%20%20In%20today%27s%20world%20of%20health%20care%2C%20brain%20tumor%20detection%20has%20become%20common.%0AHowever%2C%20the%20manual%20brain%20tumor%20classification%20approach%20is%20time-consuming.%20So%0ADeep%20Convolutional%20Neural%20Network%20%28DCNN%29%20is%20used%20by%20many%20researchers%20in%20the%0Amedical%20field%20for%20making%20accurate%20diagnoses%20and%20aiding%20in%20the%20patient%27s%0Atreatment.%20The%20traditional%20techniques%20have%20problems%20such%20as%20overfitting%20and%20the%0Ainability%20to%20extract%20necessary%20features.%20To%20overcome%20these%20problems%2C%20we%0Adeveloped%20the%20Topological%20Data%20Analysis%20based%20Improved%20Persistent%20Homology%0A%28TDA-IPH%29%20and%20Convolutional%20Transfer%20learning%20and%20Visual%20Recurrent%20learning%0Awith%20Elephant%20Herding%20Optimization%20hyper-parameter%20tuning%20%28CTVR-EHO%29%20models%20for%0Abrain%20tumor%20segmentation%20and%20classification.%20Initially%2C%20the%20Topological%20Data%0AAnalysis%20based%20Improved%20Persistent%20Homology%20is%20designed%20to%20segment%20the%20brain%0Atumor%20image.%20Then%2C%20from%20the%20segmented%20image%2C%20features%20are%20extracted%20using%20TL%0Avia%20the%20AlexNet%20model%20and%20Bidirectional%20Visual%20Long%20Short-Term%20Memory%0A%28Bi-VLSTM%29.%20Next%2C%20elephant%20Herding%20Optimization%20%28EHO%29%20is%20used%20to%20tune%20the%0Ahyperparameters%20of%20both%20networks%20to%20get%20an%20optimal%20result.%20Finally%2C%20extracted%0Afeatures%20are%20concatenated%20and%20classified%20using%20the%20softmax%20activation%20layer.%0AThe%20simulation%20result%20of%20this%20proposed%20CTVR-EHO%20and%20TDA-IPH%20method%20is%20analyzed%0Abased%20on%20precision%2C%20accuracy%2C%20recall%2C%20loss%2C%20and%20F%20score%20metrics.%20When%20compared%0Ato%20other%20existing%20brain%20tumor%20segmentation%20and%20classification%20models%2C%20the%0Aproposed%20CTVR-EHO%20and%20TDA-IPH%20approaches%20show%20high%20accuracy%20%2899.8%25%29%2C%20high%0Arecall%20%2899.23%25%29%2C%20high%20precision%20%2899.67%25%29%2C%20and%20high%20F%20score%20%2899.59%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.13021v3&entry.124074799=Read"},
{"title": "Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset", "author": "Heejeong Nam and Jinwoo Ahn", "abstract": "  The ability to perform complex reasoning across multimodal inputs is\nessential for models to effectively interact with humans in real-world\nscenarios. Advancements in vision-language models have significantly improved\nperformance on tasks that require processing explicit and direct textual\ninputs, such as Visual Question Answering (VQA) and Visual Grounding (VG).\nHowever, less attention has been given to improving the model capabilities to\ncomprehend nuanced and ambiguous forms of communication. This presents a\ncritical challenge, as human language in real-world interactions often convey\nhidden intentions that rely on context for accurate interpretation. To address\nthis gap, we propose VAGUE, a multimodal benchmark comprising 3.9K indirect\nhuman utterances paired with corresponding scenes. Additionally, we contribute\na model-based pipeline for generating prompt-solution pairs from input images.\nOur work aims to delve deeper into the ability of models to understand indirect\ncommunication and seek to contribute to the development of models capable of\nmore refined and human-like interactions. Extensive evaluation on multiple VLMs\nreveals that mainstream models still struggle with indirect communication when\nrequired to perform complex linguistic and visual reasoning. We release our\ncode and data at https://github.com/Hazel-Heejeong-Nam/VAGUE.git.\n", "link": "http://arxiv.org/abs/2411.14137v1", "date": "2024-11-21", "relevancy": 2.5608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.655}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Contexts%20Clarify%20Ambiguous%20Expressions%3A%20A%20Benchmark%20Dataset&body=Title%3A%20Visual%20Contexts%20Clarify%20Ambiguous%20Expressions%3A%20A%20Benchmark%20Dataset%0AAuthor%3A%20Heejeong%20Nam%20and%20Jinwoo%20Ahn%0AAbstract%3A%20%20%20The%20ability%20to%20perform%20complex%20reasoning%20across%20multimodal%20inputs%20is%0Aessential%20for%20models%20to%20effectively%20interact%20with%20humans%20in%20real-world%0Ascenarios.%20Advancements%20in%20vision-language%20models%20have%20significantly%20improved%0Aperformance%20on%20tasks%20that%20require%20processing%20explicit%20and%20direct%20textual%0Ainputs%2C%20such%20as%20Visual%20Question%20Answering%20%28VQA%29%20and%20Visual%20Grounding%20%28VG%29.%0AHowever%2C%20less%20attention%20has%20been%20given%20to%20improving%20the%20model%20capabilities%20to%0Acomprehend%20nuanced%20and%20ambiguous%20forms%20of%20communication.%20This%20presents%20a%0Acritical%20challenge%2C%20as%20human%20language%20in%20real-world%20interactions%20often%20convey%0Ahidden%20intentions%20that%20rely%20on%20context%20for%20accurate%20interpretation.%20To%20address%0Athis%20gap%2C%20we%20propose%20VAGUE%2C%20a%20multimodal%20benchmark%20comprising%203.9K%20indirect%0Ahuman%20utterances%20paired%20with%20corresponding%20scenes.%20Additionally%2C%20we%20contribute%0Aa%20model-based%20pipeline%20for%20generating%20prompt-solution%20pairs%20from%20input%20images.%0AOur%20work%20aims%20to%20delve%20deeper%20into%20the%20ability%20of%20models%20to%20understand%20indirect%0Acommunication%20and%20seek%20to%20contribute%20to%20the%20development%20of%20models%20capable%20of%0Amore%20refined%20and%20human-like%20interactions.%20Extensive%20evaluation%20on%20multiple%20VLMs%0Areveals%20that%20mainstream%20models%20still%20struggle%20with%20indirect%20communication%20when%0Arequired%20to%20perform%20complex%20linguistic%20and%20visual%20reasoning.%20We%20release%20our%0Acode%20and%20data%20at%20https%3A//github.com/Hazel-Heejeong-Nam/VAGUE.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Contexts%2520Clarify%2520Ambiguous%2520Expressions%253A%2520A%2520Benchmark%2520Dataset%26entry.906535625%3DHeejeong%2520Nam%2520and%2520Jinwoo%2520Ahn%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520perform%2520complex%2520reasoning%2520across%2520multimodal%2520inputs%2520is%250Aessential%2520for%2520models%2520to%2520effectively%2520interact%2520with%2520humans%2520in%2520real-world%250Ascenarios.%2520Advancements%2520in%2520vision-language%2520models%2520have%2520significantly%2520improved%250Aperformance%2520on%2520tasks%2520that%2520require%2520processing%2520explicit%2520and%2520direct%2520textual%250Ainputs%252C%2520such%2520as%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520and%2520Visual%2520Grounding%2520%2528VG%2529.%250AHowever%252C%2520less%2520attention%2520has%2520been%2520given%2520to%2520improving%2520the%2520model%2520capabilities%2520to%250Acomprehend%2520nuanced%2520and%2520ambiguous%2520forms%2520of%2520communication.%2520This%2520presents%2520a%250Acritical%2520challenge%252C%2520as%2520human%2520language%2520in%2520real-world%2520interactions%2520often%2520convey%250Ahidden%2520intentions%2520that%2520rely%2520on%2520context%2520for%2520accurate%2520interpretation.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520VAGUE%252C%2520a%2520multimodal%2520benchmark%2520comprising%25203.9K%2520indirect%250Ahuman%2520utterances%2520paired%2520with%2520corresponding%2520scenes.%2520Additionally%252C%2520we%2520contribute%250Aa%2520model-based%2520pipeline%2520for%2520generating%2520prompt-solution%2520pairs%2520from%2520input%2520images.%250AOur%2520work%2520aims%2520to%2520delve%2520deeper%2520into%2520the%2520ability%2520of%2520models%2520to%2520understand%2520indirect%250Acommunication%2520and%2520seek%2520to%2520contribute%2520to%2520the%2520development%2520of%2520models%2520capable%2520of%250Amore%2520refined%2520and%2520human-like%2520interactions.%2520Extensive%2520evaluation%2520on%2520multiple%2520VLMs%250Areveals%2520that%2520mainstream%2520models%2520still%2520struggle%2520with%2520indirect%2520communication%2520when%250Arequired%2520to%2520perform%2520complex%2520linguistic%2520and%2520visual%2520reasoning.%2520We%2520release%2520our%250Acode%2520and%2520data%2520at%2520https%253A//github.com/Hazel-Heejeong-Nam/VAGUE.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Contexts%20Clarify%20Ambiguous%20Expressions%3A%20A%20Benchmark%20Dataset&entry.906535625=Heejeong%20Nam%20and%20Jinwoo%20Ahn&entry.1292438233=%20%20The%20ability%20to%20perform%20complex%20reasoning%20across%20multimodal%20inputs%20is%0Aessential%20for%20models%20to%20effectively%20interact%20with%20humans%20in%20real-world%0Ascenarios.%20Advancements%20in%20vision-language%20models%20have%20significantly%20improved%0Aperformance%20on%20tasks%20that%20require%20processing%20explicit%20and%20direct%20textual%0Ainputs%2C%20such%20as%20Visual%20Question%20Answering%20%28VQA%29%20and%20Visual%20Grounding%20%28VG%29.%0AHowever%2C%20less%20attention%20has%20been%20given%20to%20improving%20the%20model%20capabilities%20to%0Acomprehend%20nuanced%20and%20ambiguous%20forms%20of%20communication.%20This%20presents%20a%0Acritical%20challenge%2C%20as%20human%20language%20in%20real-world%20interactions%20often%20convey%0Ahidden%20intentions%20that%20rely%20on%20context%20for%20accurate%20interpretation.%20To%20address%0Athis%20gap%2C%20we%20propose%20VAGUE%2C%20a%20multimodal%20benchmark%20comprising%203.9K%20indirect%0Ahuman%20utterances%20paired%20with%20corresponding%20scenes.%20Additionally%2C%20we%20contribute%0Aa%20model-based%20pipeline%20for%20generating%20prompt-solution%20pairs%20from%20input%20images.%0AOur%20work%20aims%20to%20delve%20deeper%20into%20the%20ability%20of%20models%20to%20understand%20indirect%0Acommunication%20and%20seek%20to%20contribute%20to%20the%20development%20of%20models%20capable%20of%0Amore%20refined%20and%20human-like%20interactions.%20Extensive%20evaluation%20on%20multiple%20VLMs%0Areveals%20that%20mainstream%20models%20still%20struggle%20with%20indirect%20communication%20when%0Arequired%20to%20perform%20complex%20linguistic%20and%20visual%20reasoning.%20We%20release%20our%0Acode%20and%20data%20at%20https%3A//github.com/Hazel-Heejeong-Nam/VAGUE.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14137v1&entry.124074799=Read"},
{"title": "Graph Domain Adaptation with Dual-branch Encoder and Two-level Alignment\n  for Whole Slide Image-based Survival Prediction", "author": "Yuntao Shou and Peiqiang Yan and Xingjian Yuan and Xiangyong Cao and Qian Zhao and Deyu Meng", "abstract": "  In recent years, histopathological whole slide image (WSI)- based survival\nanalysis has attracted much attention in medical image analysis. In practice,\nWSIs usually come from different hospitals or laboratories, which can be seen\nas different domains, and thus may have significant differences in imaging\nequipment, processing procedures, and sample sources. These differences\ngenerally result in large gaps in distribution between different WSI domains,\nand thus the survival analysis models trained on one domain may fail to\ntransfer to another. To address this issue, we propose a Dual-branch Encoder\nand Two-level Alignment (DETA) framework to explore both feature and\ncategory-level alignment between different WSI domains. Specifically, we first\nformulate the concerned problem as graph domain adaptation (GDA) by virtue the\ngraph representation of WSIs. Then we construct a dual-branch graph encoder,\nincluding the message passing branch and the shortest path branch, to\nexplicitly and implicitly extract semantic information from the\ngraph-represented WSIs. To realize GDA, we propose a two-level alignment\napproach: at the category level, we develop a coupling technique by virtue of\nthe dual-branch structure, leading to reduced divergence between the category\ndistributions of the two domains; at the feature level, we introduce an\nadversarial perturbation strategy to better augment source domain feature,\nresulting in improved alignment in feature distribution. To the best of our\nknowledge, our work is the first attempt to alleviate the domain shift issue\nfor WSI data analysis. Extensive experiments on four TCGA datasets have\nvalidated the effectiveness of our proposed DETA framework and demonstrated its\nsuperior performance in WSI-based survival analysis.\n", "link": "http://arxiv.org/abs/2411.14001v1", "date": "2024-11-21", "relevancy": 2.5496, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5045}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Domain%20Adaptation%20with%20Dual-branch%20Encoder%20and%20Two-level%20Alignment%0A%20%20for%20Whole%20Slide%20Image-based%20Survival%20Prediction&body=Title%3A%20Graph%20Domain%20Adaptation%20with%20Dual-branch%20Encoder%20and%20Two-level%20Alignment%0A%20%20for%20Whole%20Slide%20Image-based%20Survival%20Prediction%0AAuthor%3A%20Yuntao%20Shou%20and%20Peiqiang%20Yan%20and%20Xingjian%20Yuan%20and%20Xiangyong%20Cao%20and%20Qian%20Zhao%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20In%20recent%20years%2C%20histopathological%20whole%20slide%20image%20%28WSI%29-%20based%20survival%0Aanalysis%20has%20attracted%20much%20attention%20in%20medical%20image%20analysis.%20In%20practice%2C%0AWSIs%20usually%20come%20from%20different%20hospitals%20or%20laboratories%2C%20which%20can%20be%20seen%0Aas%20different%20domains%2C%20and%20thus%20may%20have%20significant%20differences%20in%20imaging%0Aequipment%2C%20processing%20procedures%2C%20and%20sample%20sources.%20These%20differences%0Agenerally%20result%20in%20large%20gaps%20in%20distribution%20between%20different%20WSI%20domains%2C%0Aand%20thus%20the%20survival%20analysis%20models%20trained%20on%20one%20domain%20may%20fail%20to%0Atransfer%20to%20another.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-branch%20Encoder%0Aand%20Two-level%20Alignment%20%28DETA%29%20framework%20to%20explore%20both%20feature%20and%0Acategory-level%20alignment%20between%20different%20WSI%20domains.%20Specifically%2C%20we%20first%0Aformulate%20the%20concerned%20problem%20as%20graph%20domain%20adaptation%20%28GDA%29%20by%20virtue%20the%0Agraph%20representation%20of%20WSIs.%20Then%20we%20construct%20a%20dual-branch%20graph%20encoder%2C%0Aincluding%20the%20message%20passing%20branch%20and%20the%20shortest%20path%20branch%2C%20to%0Aexplicitly%20and%20implicitly%20extract%20semantic%20information%20from%20the%0Agraph-represented%20WSIs.%20To%20realize%20GDA%2C%20we%20propose%20a%20two-level%20alignment%0Aapproach%3A%20at%20the%20category%20level%2C%20we%20develop%20a%20coupling%20technique%20by%20virtue%20of%0Athe%20dual-branch%20structure%2C%20leading%20to%20reduced%20divergence%20between%20the%20category%0Adistributions%20of%20the%20two%20domains%3B%20at%20the%20feature%20level%2C%20we%20introduce%20an%0Aadversarial%20perturbation%20strategy%20to%20better%20augment%20source%20domain%20feature%2C%0Aresulting%20in%20improved%20alignment%20in%20feature%20distribution.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20attempt%20to%20alleviate%20the%20domain%20shift%20issue%0Afor%20WSI%20data%20analysis.%20Extensive%20experiments%20on%20four%20TCGA%20datasets%20have%0Avalidated%20the%20effectiveness%20of%20our%20proposed%20DETA%20framework%20and%20demonstrated%20its%0Asuperior%20performance%20in%20WSI-based%20survival%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Domain%2520Adaptation%2520with%2520Dual-branch%2520Encoder%2520and%2520Two-level%2520Alignment%250A%2520%2520for%2520Whole%2520Slide%2520Image-based%2520Survival%2520Prediction%26entry.906535625%3DYuntao%2520Shou%2520and%2520Peiqiang%2520Yan%2520and%2520Xingjian%2520Yuan%2520and%2520Xiangyong%2520Cao%2520and%2520Qian%2520Zhao%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520histopathological%2520whole%2520slide%2520image%2520%2528WSI%2529-%2520based%2520survival%250Aanalysis%2520has%2520attracted%2520much%2520attention%2520in%2520medical%2520image%2520analysis.%2520In%2520practice%252C%250AWSIs%2520usually%2520come%2520from%2520different%2520hospitals%2520or%2520laboratories%252C%2520which%2520can%2520be%2520seen%250Aas%2520different%2520domains%252C%2520and%2520thus%2520may%2520have%2520significant%2520differences%2520in%2520imaging%250Aequipment%252C%2520processing%2520procedures%252C%2520and%2520sample%2520sources.%2520These%2520differences%250Agenerally%2520result%2520in%2520large%2520gaps%2520in%2520distribution%2520between%2520different%2520WSI%2520domains%252C%250Aand%2520thus%2520the%2520survival%2520analysis%2520models%2520trained%2520on%2520one%2520domain%2520may%2520fail%2520to%250Atransfer%2520to%2520another.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Dual-branch%2520Encoder%250Aand%2520Two-level%2520Alignment%2520%2528DETA%2529%2520framework%2520to%2520explore%2520both%2520feature%2520and%250Acategory-level%2520alignment%2520between%2520different%2520WSI%2520domains.%2520Specifically%252C%2520we%2520first%250Aformulate%2520the%2520concerned%2520problem%2520as%2520graph%2520domain%2520adaptation%2520%2528GDA%2529%2520by%2520virtue%2520the%250Agraph%2520representation%2520of%2520WSIs.%2520Then%2520we%2520construct%2520a%2520dual-branch%2520graph%2520encoder%252C%250Aincluding%2520the%2520message%2520passing%2520branch%2520and%2520the%2520shortest%2520path%2520branch%252C%2520to%250Aexplicitly%2520and%2520implicitly%2520extract%2520semantic%2520information%2520from%2520the%250Agraph-represented%2520WSIs.%2520To%2520realize%2520GDA%252C%2520we%2520propose%2520a%2520two-level%2520alignment%250Aapproach%253A%2520at%2520the%2520category%2520level%252C%2520we%2520develop%2520a%2520coupling%2520technique%2520by%2520virtue%2520of%250Athe%2520dual-branch%2520structure%252C%2520leading%2520to%2520reduced%2520divergence%2520between%2520the%2520category%250Adistributions%2520of%2520the%2520two%2520domains%253B%2520at%2520the%2520feature%2520level%252C%2520we%2520introduce%2520an%250Aadversarial%2520perturbation%2520strategy%2520to%2520better%2520augment%2520source%2520domain%2520feature%252C%250Aresulting%2520in%2520improved%2520alignment%2520in%2520feature%2520distribution.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520our%2520work%2520is%2520the%2520first%2520attempt%2520to%2520alleviate%2520the%2520domain%2520shift%2520issue%250Afor%2520WSI%2520data%2520analysis.%2520Extensive%2520experiments%2520on%2520four%2520TCGA%2520datasets%2520have%250Avalidated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520DETA%2520framework%2520and%2520demonstrated%2520its%250Asuperior%2520performance%2520in%2520WSI-based%2520survival%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Domain%20Adaptation%20with%20Dual-branch%20Encoder%20and%20Two-level%20Alignment%0A%20%20for%20Whole%20Slide%20Image-based%20Survival%20Prediction&entry.906535625=Yuntao%20Shou%20and%20Peiqiang%20Yan%20and%20Xingjian%20Yuan%20and%20Xiangyong%20Cao%20and%20Qian%20Zhao%20and%20Deyu%20Meng&entry.1292438233=%20%20In%20recent%20years%2C%20histopathological%20whole%20slide%20image%20%28WSI%29-%20based%20survival%0Aanalysis%20has%20attracted%20much%20attention%20in%20medical%20image%20analysis.%20In%20practice%2C%0AWSIs%20usually%20come%20from%20different%20hospitals%20or%20laboratories%2C%20which%20can%20be%20seen%0Aas%20different%20domains%2C%20and%20thus%20may%20have%20significant%20differences%20in%20imaging%0Aequipment%2C%20processing%20procedures%2C%20and%20sample%20sources.%20These%20differences%0Agenerally%20result%20in%20large%20gaps%20in%20distribution%20between%20different%20WSI%20domains%2C%0Aand%20thus%20the%20survival%20analysis%20models%20trained%20on%20one%20domain%20may%20fail%20to%0Atransfer%20to%20another.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-branch%20Encoder%0Aand%20Two-level%20Alignment%20%28DETA%29%20framework%20to%20explore%20both%20feature%20and%0Acategory-level%20alignment%20between%20different%20WSI%20domains.%20Specifically%2C%20we%20first%0Aformulate%20the%20concerned%20problem%20as%20graph%20domain%20adaptation%20%28GDA%29%20by%20virtue%20the%0Agraph%20representation%20of%20WSIs.%20Then%20we%20construct%20a%20dual-branch%20graph%20encoder%2C%0Aincluding%20the%20message%20passing%20branch%20and%20the%20shortest%20path%20branch%2C%20to%0Aexplicitly%20and%20implicitly%20extract%20semantic%20information%20from%20the%0Agraph-represented%20WSIs.%20To%20realize%20GDA%2C%20we%20propose%20a%20two-level%20alignment%0Aapproach%3A%20at%20the%20category%20level%2C%20we%20develop%20a%20coupling%20technique%20by%20virtue%20of%0Athe%20dual-branch%20structure%2C%20leading%20to%20reduced%20divergence%20between%20the%20category%0Adistributions%20of%20the%20two%20domains%3B%20at%20the%20feature%20level%2C%20we%20introduce%20an%0Aadversarial%20perturbation%20strategy%20to%20better%20augment%20source%20domain%20feature%2C%0Aresulting%20in%20improved%20alignment%20in%20feature%20distribution.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20attempt%20to%20alleviate%20the%20domain%20shift%20issue%0Afor%20WSI%20data%20analysis.%20Extensive%20experiments%20on%20four%20TCGA%20datasets%20have%0Avalidated%20the%20effectiveness%20of%20our%20proposed%20DETA%20framework%20and%20demonstrated%20its%0Asuperior%20performance%20in%20WSI-based%20survival%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14001v1&entry.124074799=Read"},
{"title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings", "author": "Duo Wang and Yuan Zuo and Fengzhi Li and Junjie Wu", "abstract": "  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n", "link": "http://arxiv.org/abs/2408.14512v2", "date": "2024-11-21", "relevancy": 2.5479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings&body=Title%3A%20LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings%0AAuthor%3A%20Duo%20Wang%20and%20Yuan%20Zuo%20and%20Fengzhi%20Li%20and%20Junjie%20Wu%0AAbstract%3A%20%20%20Zero-shot%20graph%20machine%20learning%2C%20especially%20with%20graph%20neural%20networks%0A%28GNNs%29%2C%20has%20garnered%20significant%20interest%20due%20to%20the%20challenge%20of%20scarce%0Alabeled%20data.%20While%20methods%20like%20self-supervised%20learning%20and%20graph%20prompt%0Alearning%20have%20been%20extensively%20explored%2C%20they%20often%20rely%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20in%20zero-shot%20scenarios.%0AInspired%20by%20the%20zero-shot%20capabilities%20of%20instruction-fine-tuned%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20introduce%20a%20novel%20framework%20named%20Token%20Embedding-Aligned%0AGraph%20Language%20Model%20%28TEA-GLM%29%20that%20leverages%20LLMs%20as%20cross-dataset%20and%0Across-task%20zero-shot%20learners%20for%20graph%20machine%20learning.%20Concretely%2C%20we%0Apretrain%20a%20GNN%2C%20aligning%20its%20representations%20with%20token%20embeddings%20of%20an%20LLM.%0AWe%20then%20train%20a%20linear%20projector%20that%20transforms%20the%20GNN%27s%20representations%20into%0Aa%20fixed%20number%20of%20graph%20token%20embeddings%20without%20tuning%20the%20LLM.%20A%20unified%0Ainstruction%20is%20designed%20for%20various%20graph%20tasks%20at%20different%20levels%2C%20such%20as%0Anode%20classification%20%28node-level%29%20and%20link%20prediction%20%28edge-level%29.%20These%20design%0Achoices%20collectively%20enhance%20our%20method%27s%20effectiveness%20in%20zero-shot%20learning%2C%0Asetting%20it%20apart%20from%20existing%20methods.%20Experiments%20show%20that%20our%20graph%20token%0Aembeddings%20help%20the%20LLM%20predictor%20achieve%20state-of-the-art%20performance%20on%0Aunseen%20datasets%20and%20tasks%20compared%20to%20other%20methods%20using%20LLMs%20as%20predictors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Zero-shot%2520Graph%2520Learners%253A%2520Alignment%2520of%2520GNN%2520Representations%2520with%250A%2520%2520LLM%2520Token%2520Embeddings%26entry.906535625%3DDuo%2520Wang%2520and%2520Yuan%2520Zuo%2520and%2520Fengzhi%2520Li%2520and%2520Junjie%2520Wu%26entry.1292438233%3D%2520%2520Zero-shot%2520graph%2520machine%2520learning%252C%2520especially%2520with%2520graph%2520neural%2520networks%250A%2528GNNs%2529%252C%2520has%2520garnered%2520significant%2520interest%2520due%2520to%2520the%2520challenge%2520of%2520scarce%250Alabeled%2520data.%2520While%2520methods%2520like%2520self-supervised%2520learning%2520and%2520graph%2520prompt%250Alearning%2520have%2520been%2520extensively%2520explored%252C%2520they%2520often%2520rely%2520on%2520fine-tuning%2520with%250Atask-specific%2520labels%252C%2520limiting%2520their%2520effectiveness%2520in%2520zero-shot%2520scenarios.%250AInspired%2520by%2520the%2520zero-shot%2520capabilities%2520of%2520instruction-fine-tuned%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520named%2520Token%2520Embedding-Aligned%250AGraph%2520Language%2520Model%2520%2528TEA-GLM%2529%2520that%2520leverages%2520LLMs%2520as%2520cross-dataset%2520and%250Across-task%2520zero-shot%2520learners%2520for%2520graph%2520machine%2520learning.%2520Concretely%252C%2520we%250Apretrain%2520a%2520GNN%252C%2520aligning%2520its%2520representations%2520with%2520token%2520embeddings%2520of%2520an%2520LLM.%250AWe%2520then%2520train%2520a%2520linear%2520projector%2520that%2520transforms%2520the%2520GNN%2527s%2520representations%2520into%250Aa%2520fixed%2520number%2520of%2520graph%2520token%2520embeddings%2520without%2520tuning%2520the%2520LLM.%2520A%2520unified%250Ainstruction%2520is%2520designed%2520for%2520various%2520graph%2520tasks%2520at%2520different%2520levels%252C%2520such%2520as%250Anode%2520classification%2520%2528node-level%2529%2520and%2520link%2520prediction%2520%2528edge-level%2529.%2520These%2520design%250Achoices%2520collectively%2520enhance%2520our%2520method%2527s%2520effectiveness%2520in%2520zero-shot%2520learning%252C%250Asetting%2520it%2520apart%2520from%2520existing%2520methods.%2520Experiments%2520show%2520that%2520our%2520graph%2520token%250Aembeddings%2520help%2520the%2520LLM%2520predictor%2520achieve%2520state-of-the-art%2520performance%2520on%250Aunseen%2520datasets%2520and%2520tasks%2520compared%2520to%2520other%2520methods%2520using%2520LLMs%2520as%2520predictors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings&entry.906535625=Duo%20Wang%20and%20Yuan%20Zuo%20and%20Fengzhi%20Li%20and%20Junjie%20Wu&entry.1292438233=%20%20Zero-shot%20graph%20machine%20learning%2C%20especially%20with%20graph%20neural%20networks%0A%28GNNs%29%2C%20has%20garnered%20significant%20interest%20due%20to%20the%20challenge%20of%20scarce%0Alabeled%20data.%20While%20methods%20like%20self-supervised%20learning%20and%20graph%20prompt%0Alearning%20have%20been%20extensively%20explored%2C%20they%20often%20rely%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20in%20zero-shot%20scenarios.%0AInspired%20by%20the%20zero-shot%20capabilities%20of%20instruction-fine-tuned%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20introduce%20a%20novel%20framework%20named%20Token%20Embedding-Aligned%0AGraph%20Language%20Model%20%28TEA-GLM%29%20that%20leverages%20LLMs%20as%20cross-dataset%20and%0Across-task%20zero-shot%20learners%20for%20graph%20machine%20learning.%20Concretely%2C%20we%0Apretrain%20a%20GNN%2C%20aligning%20its%20representations%20with%20token%20embeddings%20of%20an%20LLM.%0AWe%20then%20train%20a%20linear%20projector%20that%20transforms%20the%20GNN%27s%20representations%20into%0Aa%20fixed%20number%20of%20graph%20token%20embeddings%20without%20tuning%20the%20LLM.%20A%20unified%0Ainstruction%20is%20designed%20for%20various%20graph%20tasks%20at%20different%20levels%2C%20such%20as%0Anode%20classification%20%28node-level%29%20and%20link%20prediction%20%28edge-level%29.%20These%20design%0Achoices%20collectively%20enhance%20our%20method%27s%20effectiveness%20in%20zero-shot%20learning%2C%0Asetting%20it%20apart%20from%20existing%20methods.%20Experiments%20show%20that%20our%20graph%20token%0Aembeddings%20help%20the%20LLM%20predictor%20achieve%20state-of-the-art%20performance%20on%0Aunseen%20datasets%20and%20tasks%20compared%20to%20other%20methods%20using%20LLMs%20as%20predictors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14512v2&entry.124074799=Read"},
{"title": "Trajectory Representation Learning on Road Networks and Grids with\n  Spatio-Temporal Dynamics", "author": "Stefan Schestakov and Simon Gottschalk", "abstract": "  Trajectory representation learning is a fundamental task for applications in\nfields including smart city, and urban planning, as it facilitates the\nutilization of trajectory data (e.g., vehicle movements) for various downstream\napplications, such as trajectory similarity computation or travel time\nestimation. This is achieved by learning low-dimensional representations from\nhigh-dimensional and raw trajectory data. However, existing methods for\ntrajectory representation learning either rely on grid-based or road-based\nrepresentations, which are inherently different and thus, could lose\ninformation contained in the other modality. Moreover, these methods overlook\nthe dynamic nature of urban traffic, relying on static road network features\nrather than time varying traffic patterns. In this paper, we propose TIGR, a\nnovel model designed to integrate grid and road network modalities while\nincorporating spatio-temporal dynamics to learn rich, general-purpose\nrepresentations of trajectories. We evaluate TIGR on two realworld datasets and\ndemonstrate the effectiveness of combining both modalities by substantially\noutperforming state-of-the-art methods, i.e., up to 43.22% for trajectory\nsimilarity, up to 16.65% for travel time estimation, and up to 10.16% for\ndestination prediction.\n", "link": "http://arxiv.org/abs/2411.14014v1", "date": "2024-11-21", "relevancy": 2.5468, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Representation%20Learning%20on%20Road%20Networks%20and%20Grids%20with%0A%20%20Spatio-Temporal%20Dynamics&body=Title%3A%20Trajectory%20Representation%20Learning%20on%20Road%20Networks%20and%20Grids%20with%0A%20%20Spatio-Temporal%20Dynamics%0AAuthor%3A%20Stefan%20Schestakov%20and%20Simon%20Gottschalk%0AAbstract%3A%20%20%20Trajectory%20representation%20learning%20is%20a%20fundamental%20task%20for%20applications%20in%0Afields%20including%20smart%20city%2C%20and%20urban%20planning%2C%20as%20it%20facilitates%20the%0Autilization%20of%20trajectory%20data%20%28e.g.%2C%20vehicle%20movements%29%20for%20various%20downstream%0Aapplications%2C%20such%20as%20trajectory%20similarity%20computation%20or%20travel%20time%0Aestimation.%20This%20is%20achieved%20by%20learning%20low-dimensional%20representations%20from%0Ahigh-dimensional%20and%20raw%20trajectory%20data.%20However%2C%20existing%20methods%20for%0Atrajectory%20representation%20learning%20either%20rely%20on%20grid-based%20or%20road-based%0Arepresentations%2C%20which%20are%20inherently%20different%20and%20thus%2C%20could%20lose%0Ainformation%20contained%20in%20the%20other%20modality.%20Moreover%2C%20these%20methods%20overlook%0Athe%20dynamic%20nature%20of%20urban%20traffic%2C%20relying%20on%20static%20road%20network%20features%0Arather%20than%20time%20varying%20traffic%20patterns.%20In%20this%20paper%2C%20we%20propose%20TIGR%2C%20a%0Anovel%20model%20designed%20to%20integrate%20grid%20and%20road%20network%20modalities%20while%0Aincorporating%20spatio-temporal%20dynamics%20to%20learn%20rich%2C%20general-purpose%0Arepresentations%20of%20trajectories.%20We%20evaluate%20TIGR%20on%20two%20realworld%20datasets%20and%0Ademonstrate%20the%20effectiveness%20of%20combining%20both%20modalities%20by%20substantially%0Aoutperforming%20state-of-the-art%20methods%2C%20i.e.%2C%20up%20to%2043.22%25%20for%20trajectory%0Asimilarity%2C%20up%20to%2016.65%25%20for%20travel%20time%20estimation%2C%20and%20up%20to%2010.16%25%20for%0Adestination%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Representation%2520Learning%2520on%2520Road%2520Networks%2520and%2520Grids%2520with%250A%2520%2520Spatio-Temporal%2520Dynamics%26entry.906535625%3DStefan%2520Schestakov%2520and%2520Simon%2520Gottschalk%26entry.1292438233%3D%2520%2520Trajectory%2520representation%2520learning%2520is%2520a%2520fundamental%2520task%2520for%2520applications%2520in%250Afields%2520including%2520smart%2520city%252C%2520and%2520urban%2520planning%252C%2520as%2520it%2520facilitates%2520the%250Autilization%2520of%2520trajectory%2520data%2520%2528e.g.%252C%2520vehicle%2520movements%2529%2520for%2520various%2520downstream%250Aapplications%252C%2520such%2520as%2520trajectory%2520similarity%2520computation%2520or%2520travel%2520time%250Aestimation.%2520This%2520is%2520achieved%2520by%2520learning%2520low-dimensional%2520representations%2520from%250Ahigh-dimensional%2520and%2520raw%2520trajectory%2520data.%2520However%252C%2520existing%2520methods%2520for%250Atrajectory%2520representation%2520learning%2520either%2520rely%2520on%2520grid-based%2520or%2520road-based%250Arepresentations%252C%2520which%2520are%2520inherently%2520different%2520and%2520thus%252C%2520could%2520lose%250Ainformation%2520contained%2520in%2520the%2520other%2520modality.%2520Moreover%252C%2520these%2520methods%2520overlook%250Athe%2520dynamic%2520nature%2520of%2520urban%2520traffic%252C%2520relying%2520on%2520static%2520road%2520network%2520features%250Arather%2520than%2520time%2520varying%2520traffic%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TIGR%252C%2520a%250Anovel%2520model%2520designed%2520to%2520integrate%2520grid%2520and%2520road%2520network%2520modalities%2520while%250Aincorporating%2520spatio-temporal%2520dynamics%2520to%2520learn%2520rich%252C%2520general-purpose%250Arepresentations%2520of%2520trajectories.%2520We%2520evaluate%2520TIGR%2520on%2520two%2520realworld%2520datasets%2520and%250Ademonstrate%2520the%2520effectiveness%2520of%2520combining%2520both%2520modalities%2520by%2520substantially%250Aoutperforming%2520state-of-the-art%2520methods%252C%2520i.e.%252C%2520up%2520to%252043.22%2525%2520for%2520trajectory%250Asimilarity%252C%2520up%2520to%252016.65%2525%2520for%2520travel%2520time%2520estimation%252C%2520and%2520up%2520to%252010.16%2525%2520for%250Adestination%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Representation%20Learning%20on%20Road%20Networks%20and%20Grids%20with%0A%20%20Spatio-Temporal%20Dynamics&entry.906535625=Stefan%20Schestakov%20and%20Simon%20Gottschalk&entry.1292438233=%20%20Trajectory%20representation%20learning%20is%20a%20fundamental%20task%20for%20applications%20in%0Afields%20including%20smart%20city%2C%20and%20urban%20planning%2C%20as%20it%20facilitates%20the%0Autilization%20of%20trajectory%20data%20%28e.g.%2C%20vehicle%20movements%29%20for%20various%20downstream%0Aapplications%2C%20such%20as%20trajectory%20similarity%20computation%20or%20travel%20time%0Aestimation.%20This%20is%20achieved%20by%20learning%20low-dimensional%20representations%20from%0Ahigh-dimensional%20and%20raw%20trajectory%20data.%20However%2C%20existing%20methods%20for%0Atrajectory%20representation%20learning%20either%20rely%20on%20grid-based%20or%20road-based%0Arepresentations%2C%20which%20are%20inherently%20different%20and%20thus%2C%20could%20lose%0Ainformation%20contained%20in%20the%20other%20modality.%20Moreover%2C%20these%20methods%20overlook%0Athe%20dynamic%20nature%20of%20urban%20traffic%2C%20relying%20on%20static%20road%20network%20features%0Arather%20than%20time%20varying%20traffic%20patterns.%20In%20this%20paper%2C%20we%20propose%20TIGR%2C%20a%0Anovel%20model%20designed%20to%20integrate%20grid%20and%20road%20network%20modalities%20while%0Aincorporating%20spatio-temporal%20dynamics%20to%20learn%20rich%2C%20general-purpose%0Arepresentations%20of%20trajectories.%20We%20evaluate%20TIGR%20on%20two%20realworld%20datasets%20and%0Ademonstrate%20the%20effectiveness%20of%20combining%20both%20modalities%20by%20substantially%0Aoutperforming%20state-of-the-art%20methods%2C%20i.e.%2C%20up%20to%2043.22%25%20for%20trajectory%0Asimilarity%2C%20up%20to%2016.65%25%20for%20travel%20time%20estimation%2C%20and%20up%20to%2010.16%25%20for%0Adestination%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14014v1&entry.124074799=Read"},
{"title": "WARLearn: Weather-Adaptive Representation Learning", "author": "Shubham Agarwal and Raz Birman and Ofer Hadar", "abstract": "  This paper introduces WARLearn, a novel framework designed for adaptive\nrepresentation learning in challenging and adversarial weather conditions.\nLeveraging the in-variance principal used in Barlow Twins, we demonstrate the\ncapability to port the existing models initially trained on clear weather data\nto effectively handle adverse weather conditions. With minimal additional\ntraining, our method exhibits remarkable performance gains in scenarios\ncharacterized by fog and low-light conditions. This adaptive framework extends\nits applicability beyond adverse weather settings, offering a versatile\nsolution for domains exhibiting variations in data distributions. Furthermore,\nWARLearn is invaluable in scenarios where data distributions undergo\nsignificant shifts over time, enabling models to remain updated and accurate.\nOur experimental findings reveal a remarkable performance, with a mean average\nprecision (mAP) of 52.6% on unseen real-world foggy dataset (RTTS). Similarly,\nin low light conditions, our framework achieves a mAP of 55.7% on unseen\nreal-world low light dataset (ExDark). Notably, WARLearn surpasses the\nperformance of state-of-the-art frameworks including FeatEnHancer, Image\nAdaptive YOLO, DENet, C2PNet, PairLIE and ZeroDCE, by a substantial margin in\nadverse weather, improving the baseline performance in both foggy and low light\nconditions. The WARLearn code is available at\nhttps://github.com/ShubhamAgarwal12/WARLearn\n", "link": "http://arxiv.org/abs/2411.14095v1", "date": "2024-11-21", "relevancy": 2.5356, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WARLearn%3A%20Weather-Adaptive%20Representation%20Learning&body=Title%3A%20WARLearn%3A%20Weather-Adaptive%20Representation%20Learning%0AAuthor%3A%20Shubham%20Agarwal%20and%20Raz%20Birman%20and%20Ofer%20Hadar%0AAbstract%3A%20%20%20This%20paper%20introduces%20WARLearn%2C%20a%20novel%20framework%20designed%20for%20adaptive%0Arepresentation%20learning%20in%20challenging%20and%20adversarial%20weather%20conditions.%0ALeveraging%20the%20in-variance%20principal%20used%20in%20Barlow%20Twins%2C%20we%20demonstrate%20the%0Acapability%20to%20port%20the%20existing%20models%20initially%20trained%20on%20clear%20weather%20data%0Ato%20effectively%20handle%20adverse%20weather%20conditions.%20With%20minimal%20additional%0Atraining%2C%20our%20method%20exhibits%20remarkable%20performance%20gains%20in%20scenarios%0Acharacterized%20by%20fog%20and%20low-light%20conditions.%20This%20adaptive%20framework%20extends%0Aits%20applicability%20beyond%20adverse%20weather%20settings%2C%20offering%20a%20versatile%0Asolution%20for%20domains%20exhibiting%20variations%20in%20data%20distributions.%20Furthermore%2C%0AWARLearn%20is%20invaluable%20in%20scenarios%20where%20data%20distributions%20undergo%0Asignificant%20shifts%20over%20time%2C%20enabling%20models%20to%20remain%20updated%20and%20accurate.%0AOur%20experimental%20findings%20reveal%20a%20remarkable%20performance%2C%20with%20a%20mean%20average%0Aprecision%20%28mAP%29%20of%2052.6%25%20on%20unseen%20real-world%20foggy%20dataset%20%28RTTS%29.%20Similarly%2C%0Ain%20low%20light%20conditions%2C%20our%20framework%20achieves%20a%20mAP%20of%2055.7%25%20on%20unseen%0Areal-world%20low%20light%20dataset%20%28ExDark%29.%20Notably%2C%20WARLearn%20surpasses%20the%0Aperformance%20of%20state-of-the-art%20frameworks%20including%20FeatEnHancer%2C%20Image%0AAdaptive%20YOLO%2C%20DENet%2C%20C2PNet%2C%20PairLIE%20and%20ZeroDCE%2C%20by%20a%20substantial%20margin%20in%0Aadverse%20weather%2C%20improving%20the%20baseline%20performance%20in%20both%20foggy%20and%20low%20light%0Aconditions.%20The%20WARLearn%20code%20is%20available%20at%0Ahttps%3A//github.com/ShubhamAgarwal12/WARLearn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWARLearn%253A%2520Weather-Adaptive%2520Representation%2520Learning%26entry.906535625%3DShubham%2520Agarwal%2520and%2520Raz%2520Birman%2520and%2520Ofer%2520Hadar%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520WARLearn%252C%2520a%2520novel%2520framework%2520designed%2520for%2520adaptive%250Arepresentation%2520learning%2520in%2520challenging%2520and%2520adversarial%2520weather%2520conditions.%250ALeveraging%2520the%2520in-variance%2520principal%2520used%2520in%2520Barlow%2520Twins%252C%2520we%2520demonstrate%2520the%250Acapability%2520to%2520port%2520the%2520existing%2520models%2520initially%2520trained%2520on%2520clear%2520weather%2520data%250Ato%2520effectively%2520handle%2520adverse%2520weather%2520conditions.%2520With%2520minimal%2520additional%250Atraining%252C%2520our%2520method%2520exhibits%2520remarkable%2520performance%2520gains%2520in%2520scenarios%250Acharacterized%2520by%2520fog%2520and%2520low-light%2520conditions.%2520This%2520adaptive%2520framework%2520extends%250Aits%2520applicability%2520beyond%2520adverse%2520weather%2520settings%252C%2520offering%2520a%2520versatile%250Asolution%2520for%2520domains%2520exhibiting%2520variations%2520in%2520data%2520distributions.%2520Furthermore%252C%250AWARLearn%2520is%2520invaluable%2520in%2520scenarios%2520where%2520data%2520distributions%2520undergo%250Asignificant%2520shifts%2520over%2520time%252C%2520enabling%2520models%2520to%2520remain%2520updated%2520and%2520accurate.%250AOur%2520experimental%2520findings%2520reveal%2520a%2520remarkable%2520performance%252C%2520with%2520a%2520mean%2520average%250Aprecision%2520%2528mAP%2529%2520of%252052.6%2525%2520on%2520unseen%2520real-world%2520foggy%2520dataset%2520%2528RTTS%2529.%2520Similarly%252C%250Ain%2520low%2520light%2520conditions%252C%2520our%2520framework%2520achieves%2520a%2520mAP%2520of%252055.7%2525%2520on%2520unseen%250Areal-world%2520low%2520light%2520dataset%2520%2528ExDark%2529.%2520Notably%252C%2520WARLearn%2520surpasses%2520the%250Aperformance%2520of%2520state-of-the-art%2520frameworks%2520including%2520FeatEnHancer%252C%2520Image%250AAdaptive%2520YOLO%252C%2520DENet%252C%2520C2PNet%252C%2520PairLIE%2520and%2520ZeroDCE%252C%2520by%2520a%2520substantial%2520margin%2520in%250Aadverse%2520weather%252C%2520improving%2520the%2520baseline%2520performance%2520in%2520both%2520foggy%2520and%2520low%2520light%250Aconditions.%2520The%2520WARLearn%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShubhamAgarwal12/WARLearn%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WARLearn%3A%20Weather-Adaptive%20Representation%20Learning&entry.906535625=Shubham%20Agarwal%20and%20Raz%20Birman%20and%20Ofer%20Hadar&entry.1292438233=%20%20This%20paper%20introduces%20WARLearn%2C%20a%20novel%20framework%20designed%20for%20adaptive%0Arepresentation%20learning%20in%20challenging%20and%20adversarial%20weather%20conditions.%0ALeveraging%20the%20in-variance%20principal%20used%20in%20Barlow%20Twins%2C%20we%20demonstrate%20the%0Acapability%20to%20port%20the%20existing%20models%20initially%20trained%20on%20clear%20weather%20data%0Ato%20effectively%20handle%20adverse%20weather%20conditions.%20With%20minimal%20additional%0Atraining%2C%20our%20method%20exhibits%20remarkable%20performance%20gains%20in%20scenarios%0Acharacterized%20by%20fog%20and%20low-light%20conditions.%20This%20adaptive%20framework%20extends%0Aits%20applicability%20beyond%20adverse%20weather%20settings%2C%20offering%20a%20versatile%0Asolution%20for%20domains%20exhibiting%20variations%20in%20data%20distributions.%20Furthermore%2C%0AWARLearn%20is%20invaluable%20in%20scenarios%20where%20data%20distributions%20undergo%0Asignificant%20shifts%20over%20time%2C%20enabling%20models%20to%20remain%20updated%20and%20accurate.%0AOur%20experimental%20findings%20reveal%20a%20remarkable%20performance%2C%20with%20a%20mean%20average%0Aprecision%20%28mAP%29%20of%2052.6%25%20on%20unseen%20real-world%20foggy%20dataset%20%28RTTS%29.%20Similarly%2C%0Ain%20low%20light%20conditions%2C%20our%20framework%20achieves%20a%20mAP%20of%2055.7%25%20on%20unseen%0Areal-world%20low%20light%20dataset%20%28ExDark%29.%20Notably%2C%20WARLearn%20surpasses%20the%0Aperformance%20of%20state-of-the-art%20frameworks%20including%20FeatEnHancer%2C%20Image%0AAdaptive%20YOLO%2C%20DENet%2C%20C2PNet%2C%20PairLIE%20and%20ZeroDCE%2C%20by%20a%20substantial%20margin%20in%0Aadverse%20weather%2C%20improving%20the%20baseline%20performance%20in%20both%20foggy%20and%20low%20light%0Aconditions.%20The%20WARLearn%20code%20is%20available%20at%0Ahttps%3A//github.com/ShubhamAgarwal12/WARLearn%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14095v1&entry.124074799=Read"},
{"title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models", "author": "Cheng Li and Damien Teney and Linyi Yang and Qingsong Wen and Xing Xie and Jindong Wang", "abstract": "  Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.\n", "link": "http://arxiv.org/abs/2405.15145v3", "date": "2024-11-21", "relevancy": 2.5311, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CulturePark%3A%20Boosting%20Cross-cultural%20Understanding%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20CulturePark%3A%20Boosting%20Cross-cultural%20Understanding%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Cheng%20Li%20and%20Damien%20Teney%20and%20Linyi%20Yang%20and%20Qingsong%20Wen%20and%20Xing%20Xie%20and%20Jindong%20Wang%0AAbstract%3A%20%20%20Cultural%20bias%20is%20pervasive%20in%20many%20large%20language%20models%20%28LLMs%29%2C%20largely%20due%0Ato%20the%20deficiency%20of%20data%20representative%20of%20different%20cultures.%20Typically%2C%0Acultural%20datasets%20and%20benchmarks%20are%20constructed%20either%20by%20extracting%20subsets%0Aof%20existing%20datasets%20or%20by%20aggregating%20from%20platforms%20such%20as%20Wikipedia%20and%0Asocial%20media.%20However%2C%20these%20approaches%20are%20highly%20dependent%20on%20real-world%20data%0Aand%20human%20annotations%2C%20making%20them%20costly%20and%20difficult%20to%20scale.%20Inspired%20by%0Acognitive%20theories%20on%20social%20communication%2C%20this%20paper%20introduces%20CulturePark%2C%0Aan%20LLM-powered%20multi-agent%20communication%20framework%20for%20cultural%20data%0Acollection.%20CulturePark%20simulates%20cross-cultural%20human%20communication%20with%0ALLM-based%20agents%20playing%20roles%20in%20different%20cultures.%20It%20generates%20high-quality%0Across-cultural%20dialogues%20encapsulating%20human%20beliefs%2C%20norms%2C%20and%20customs.%20Using%0ACulturePark%2C%20we%20generated%2041%2C000%20cultural%20samples%20to%20fine-tune%20eight%0Aculture-specific%20LLMs.%20We%20evaluated%20these%20models%20across%20three%20downstream%20tasks%3A%0Acontent%20moderation%2C%20cultural%20alignment%2C%20and%20cultural%20education.%20Results%20show%0Athat%20for%20content%20moderation%2C%20our%20GPT-3.5-based%20models%20either%20match%20or%0Aoutperform%20GPT-4%20on%20datasets.%20Regarding%20cultural%20alignment%2C%20our%20models%20surpass%0AGPT-4%20on%20Hofstede%27s%20VSM%2013%20framework.%20Furthermore%2C%20for%20cultural%20education%20of%0Ahuman%20participants%2C%20our%20models%20demonstrate%20superior%20outcomes%20in%20both%20learning%0Aefficacy%20and%20user%20experience%20compared%20to%20GPT-4.%20CulturePark%20proves%20an%20important%0Astep%20in%20addressing%20cultural%20bias%20and%20advancing%20the%20democratization%20of%20AI%2C%0Ahighlighting%20the%20critical%20role%20of%20culturally%20inclusive%20data%20in%20model%20training.%0ACode%20is%20released%20at%20https%3A//github.com/Scarelette/CulturePark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15145v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCulturePark%253A%2520Boosting%2520Cross-cultural%2520Understanding%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DCheng%2520Li%2520and%2520Damien%2520Teney%2520and%2520Linyi%2520Yang%2520and%2520Qingsong%2520Wen%2520and%2520Xing%2520Xie%2520and%2520Jindong%2520Wang%26entry.1292438233%3D%2520%2520Cultural%2520bias%2520is%2520pervasive%2520in%2520many%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520largely%2520due%250Ato%2520the%2520deficiency%2520of%2520data%2520representative%2520of%2520different%2520cultures.%2520Typically%252C%250Acultural%2520datasets%2520and%2520benchmarks%2520are%2520constructed%2520either%2520by%2520extracting%2520subsets%250Aof%2520existing%2520datasets%2520or%2520by%2520aggregating%2520from%2520platforms%2520such%2520as%2520Wikipedia%2520and%250Asocial%2520media.%2520However%252C%2520these%2520approaches%2520are%2520highly%2520dependent%2520on%2520real-world%2520data%250Aand%2520human%2520annotations%252C%2520making%2520them%2520costly%2520and%2520difficult%2520to%2520scale.%2520Inspired%2520by%250Acognitive%2520theories%2520on%2520social%2520communication%252C%2520this%2520paper%2520introduces%2520CulturePark%252C%250Aan%2520LLM-powered%2520multi-agent%2520communication%2520framework%2520for%2520cultural%2520data%250Acollection.%2520CulturePark%2520simulates%2520cross-cultural%2520human%2520communication%2520with%250ALLM-based%2520agents%2520playing%2520roles%2520in%2520different%2520cultures.%2520It%2520generates%2520high-quality%250Across-cultural%2520dialogues%2520encapsulating%2520human%2520beliefs%252C%2520norms%252C%2520and%2520customs.%2520Using%250ACulturePark%252C%2520we%2520generated%252041%252C000%2520cultural%2520samples%2520to%2520fine-tune%2520eight%250Aculture-specific%2520LLMs.%2520We%2520evaluated%2520these%2520models%2520across%2520three%2520downstream%2520tasks%253A%250Acontent%2520moderation%252C%2520cultural%2520alignment%252C%2520and%2520cultural%2520education.%2520Results%2520show%250Athat%2520for%2520content%2520moderation%252C%2520our%2520GPT-3.5-based%2520models%2520either%2520match%2520or%250Aoutperform%2520GPT-4%2520on%2520datasets.%2520Regarding%2520cultural%2520alignment%252C%2520our%2520models%2520surpass%250AGPT-4%2520on%2520Hofstede%2527s%2520VSM%252013%2520framework.%2520Furthermore%252C%2520for%2520cultural%2520education%2520of%250Ahuman%2520participants%252C%2520our%2520models%2520demonstrate%2520superior%2520outcomes%2520in%2520both%2520learning%250Aefficacy%2520and%2520user%2520experience%2520compared%2520to%2520GPT-4.%2520CulturePark%2520proves%2520an%2520important%250Astep%2520in%2520addressing%2520cultural%2520bias%2520and%2520advancing%2520the%2520democratization%2520of%2520AI%252C%250Ahighlighting%2520the%2520critical%2520role%2520of%2520culturally%2520inclusive%2520data%2520in%2520model%2520training.%250ACode%2520is%2520released%2520at%2520https%253A//github.com/Scarelette/CulturePark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15145v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CulturePark%3A%20Boosting%20Cross-cultural%20Understanding%20in%20Large%20Language%0A%20%20Models&entry.906535625=Cheng%20Li%20and%20Damien%20Teney%20and%20Linyi%20Yang%20and%20Qingsong%20Wen%20and%20Xing%20Xie%20and%20Jindong%20Wang&entry.1292438233=%20%20Cultural%20bias%20is%20pervasive%20in%20many%20large%20language%20models%20%28LLMs%29%2C%20largely%20due%0Ato%20the%20deficiency%20of%20data%20representative%20of%20different%20cultures.%20Typically%2C%0Acultural%20datasets%20and%20benchmarks%20are%20constructed%20either%20by%20extracting%20subsets%0Aof%20existing%20datasets%20or%20by%20aggregating%20from%20platforms%20such%20as%20Wikipedia%20and%0Asocial%20media.%20However%2C%20these%20approaches%20are%20highly%20dependent%20on%20real-world%20data%0Aand%20human%20annotations%2C%20making%20them%20costly%20and%20difficult%20to%20scale.%20Inspired%20by%0Acognitive%20theories%20on%20social%20communication%2C%20this%20paper%20introduces%20CulturePark%2C%0Aan%20LLM-powered%20multi-agent%20communication%20framework%20for%20cultural%20data%0Acollection.%20CulturePark%20simulates%20cross-cultural%20human%20communication%20with%0ALLM-based%20agents%20playing%20roles%20in%20different%20cultures.%20It%20generates%20high-quality%0Across-cultural%20dialogues%20encapsulating%20human%20beliefs%2C%20norms%2C%20and%20customs.%20Using%0ACulturePark%2C%20we%20generated%2041%2C000%20cultural%20samples%20to%20fine-tune%20eight%0Aculture-specific%20LLMs.%20We%20evaluated%20these%20models%20across%20three%20downstream%20tasks%3A%0Acontent%20moderation%2C%20cultural%20alignment%2C%20and%20cultural%20education.%20Results%20show%0Athat%20for%20content%20moderation%2C%20our%20GPT-3.5-based%20models%20either%20match%20or%0Aoutperform%20GPT-4%20on%20datasets.%20Regarding%20cultural%20alignment%2C%20our%20models%20surpass%0AGPT-4%20on%20Hofstede%27s%20VSM%2013%20framework.%20Furthermore%2C%20for%20cultural%20education%20of%0Ahuman%20participants%2C%20our%20models%20demonstrate%20superior%20outcomes%20in%20both%20learning%0Aefficacy%20and%20user%20experience%20compared%20to%20GPT-4.%20CulturePark%20proves%20an%20important%0Astep%20in%20addressing%20cultural%20bias%20and%20advancing%20the%20democratization%20of%20AI%2C%0Ahighlighting%20the%20critical%20role%20of%20culturally%20inclusive%20data%20in%20model%20training.%0ACode%20is%20released%20at%20https%3A//github.com/Scarelette/CulturePark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15145v3&entry.124074799=Read"},
{"title": "Leveraging Bi-Focal Perspectives and Granular Feature Integration for\n  Accurate Reliable Early Alzheimer's Detection", "author": "Pandiyaraju V and Shravan Venkatraman and Abeshek A and Pavan Kumar S and Aravintakshan S A and Kannan A", "abstract": "  Alzheimer's disease (AD) is the most common neurodegeneration, annually\ndiagnosed in millions of patients. The present medicine scenario still finds\nchallenges in the exact diagnosis and classification of AD through neuroimaging\ndata. Traditional CNNs can extract a good amount of low-level information in an\nimage but fail to extract high-level minuscule particles, which is a\nsignificant challenge in detecting AD from MRI scans. To overcome this, we\npropose a novel Granular Feature Integration method to combine information\nextraction at different scales combined with an efficient information flow,\nenabling the model to capture both broad and fine-grained features\nsimultaneously. We also propose a Bi-Focal Perspective mechanism to highlight\nthe subtle neurofibrillary tangles and amyloid plaques in the MRI scans,\nensuring that critical pathological markers are accurately identified. Our\nmodel achieved an F1-Score of 99.31%, precision of 99.24%, and recall of\n99.51%. These scores prove that our model is significantly better than the\nstate-of-the-art (SOTA) CNNs in existence.\n", "link": "http://arxiv.org/abs/2407.10921v4", "date": "2024-11-21", "relevancy": 2.5252, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4978}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&body=Title%3A%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Pavan%20Kumar%20S%20and%20Aravintakshan%20S%20A%20and%20Kannan%20A%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20common%20neurodegeneration%2C%20annually%0Adiagnosed%20in%20millions%20of%20patients.%20The%20present%20medicine%20scenario%20still%20finds%0Achallenges%20in%20the%20exact%20diagnosis%20and%20classification%20of%20AD%20through%20neuroimaging%0Adata.%20Traditional%20CNNs%20can%20extract%20a%20good%20amount%20of%20low-level%20information%20in%20an%0Aimage%20but%20fail%20to%20extract%20high-level%20minuscule%20particles%2C%20which%20is%20a%0Asignificant%20challenge%20in%20detecting%20AD%20from%20MRI%20scans.%20To%20overcome%20this%2C%20we%0Apropose%20a%20novel%20Granular%20Feature%20Integration%20method%20to%20combine%20information%0Aextraction%20at%20different%20scales%20combined%20with%20an%20efficient%20information%20flow%2C%0Aenabling%20the%20model%20to%20capture%20both%20broad%20and%20fine-grained%20features%0Asimultaneously.%20We%20also%20propose%20a%20Bi-Focal%20Perspective%20mechanism%20to%20highlight%0Athe%20subtle%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20in%20the%20MRI%20scans%2C%0Aensuring%20that%20critical%20pathological%20markers%20are%20accurately%20identified.%20Our%0Amodel%20achieved%20an%20F1-Score%20of%2099.31%25%2C%20precision%20of%2099.24%25%2C%20and%20recall%20of%0A99.51%25.%20These%20scores%20prove%20that%20our%20model%20is%20significantly%20better%20than%20the%0Astate-of-the-art%20%28SOTA%29%20CNNs%20in%20existence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10921v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Bi-Focal%2520Perspectives%2520and%2520Granular%2520Feature%2520Integration%2520for%250A%2520%2520Accurate%2520Reliable%2520Early%2520Alzheimer%2527s%2520Detection%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Abeshek%2520A%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Aravintakshan%2520S%2520A%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520is%2520the%2520most%2520common%2520neurodegeneration%252C%2520annually%250Adiagnosed%2520in%2520millions%2520of%2520patients.%2520The%2520present%2520medicine%2520scenario%2520still%2520finds%250Achallenges%2520in%2520the%2520exact%2520diagnosis%2520and%2520classification%2520of%2520AD%2520through%2520neuroimaging%250Adata.%2520Traditional%2520CNNs%2520can%2520extract%2520a%2520good%2520amount%2520of%2520low-level%2520information%2520in%2520an%250Aimage%2520but%2520fail%2520to%2520extract%2520high-level%2520minuscule%2520particles%252C%2520which%2520is%2520a%250Asignificant%2520challenge%2520in%2520detecting%2520AD%2520from%2520MRI%2520scans.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520a%2520novel%2520Granular%2520Feature%2520Integration%2520method%2520to%2520combine%2520information%250Aextraction%2520at%2520different%2520scales%2520combined%2520with%2520an%2520efficient%2520information%2520flow%252C%250Aenabling%2520the%2520model%2520to%2520capture%2520both%2520broad%2520and%2520fine-grained%2520features%250Asimultaneously.%2520We%2520also%2520propose%2520a%2520Bi-Focal%2520Perspective%2520mechanism%2520to%2520highlight%250Athe%2520subtle%2520neurofibrillary%2520tangles%2520and%2520amyloid%2520plaques%2520in%2520the%2520MRI%2520scans%252C%250Aensuring%2520that%2520critical%2520pathological%2520markers%2520are%2520accurately%2520identified.%2520Our%250Amodel%2520achieved%2520an%2520F1-Score%2520of%252099.31%2525%252C%2520precision%2520of%252099.24%2525%252C%2520and%2520recall%2520of%250A99.51%2525.%2520These%2520scores%2520prove%2520that%2520our%2520model%2520is%2520significantly%2520better%2520than%2520the%250Astate-of-the-art%2520%2528SOTA%2529%2520CNNs%2520in%2520existence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10921v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Pavan%20Kumar%20S%20and%20Aravintakshan%20S%20A%20and%20Kannan%20A&entry.1292438233=%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20common%20neurodegeneration%2C%20annually%0Adiagnosed%20in%20millions%20of%20patients.%20The%20present%20medicine%20scenario%20still%20finds%0Achallenges%20in%20the%20exact%20diagnosis%20and%20classification%20of%20AD%20through%20neuroimaging%0Adata.%20Traditional%20CNNs%20can%20extract%20a%20good%20amount%20of%20low-level%20information%20in%20an%0Aimage%20but%20fail%20to%20extract%20high-level%20minuscule%20particles%2C%20which%20is%20a%0Asignificant%20challenge%20in%20detecting%20AD%20from%20MRI%20scans.%20To%20overcome%20this%2C%20we%0Apropose%20a%20novel%20Granular%20Feature%20Integration%20method%20to%20combine%20information%0Aextraction%20at%20different%20scales%20combined%20with%20an%20efficient%20information%20flow%2C%0Aenabling%20the%20model%20to%20capture%20both%20broad%20and%20fine-grained%20features%0Asimultaneously.%20We%20also%20propose%20a%20Bi-Focal%20Perspective%20mechanism%20to%20highlight%0Athe%20subtle%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20in%20the%20MRI%20scans%2C%0Aensuring%20that%20critical%20pathological%20markers%20are%20accurately%20identified.%20Our%0Amodel%20achieved%20an%20F1-Score%20of%2099.31%25%2C%20precision%20of%2099.24%25%2C%20and%20recall%20of%0A99.51%25.%20These%20scores%20prove%20that%20our%20model%20is%20significantly%20better%20than%20the%0Astate-of-the-art%20%28SOTA%29%20CNNs%20in%20existence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10921v4&entry.124074799=Read"},
{"title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology\n  Datasets with Foundational Embedding Models", "author": "Aakash Tripathi and Asim Waqas and Matthew B. Schabath and Yasin Yilmaz and Ghulam Rasool", "abstract": "  Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository.\n", "link": "http://arxiv.org/abs/2405.07460v4", "date": "2024-11-21", "relevancy": 2.4822, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models&body=Title%3A%20HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models%0AAuthor%3A%20Aakash%20Tripathi%20and%20Asim%20Waqas%20and%20Matthew%20B.%20Schabath%20and%20Yasin%20Yilmaz%20and%20Ghulam%20Rasool%0AAbstract%3A%20%20%20Developing%20accurate%20machine%20learning%20models%20for%20oncology%20requires%0Alarge-scale%2C%20high-quality%20multimodal%20datasets.%20However%2C%20creating%20such%20datasets%0Aremains%20challenging%20due%20to%20the%20complexity%20and%20heterogeneity%20of%20medical%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20HoneyBee%2C%20a%20scalable%20modular%20framework%20for%0Abuilding%20multimodal%20oncology%20datasets%20that%20leverages%20foundation%20models%20to%0Agenerate%20representative%20embeddings.%20HoneyBee%20integrates%20various%20data%0Amodalities%2C%20including%20clinical%20diagnostic%20and%20pathology%20imaging%20data%2C%20medical%0Anotes%2C%20reports%2C%20records%2C%20and%20molecular%20data.%20It%20employs%20data%20preprocessing%0Atechniques%20and%20foundation%20models%20to%20generate%20embeddings%20that%20capture%20the%0Aessential%20features%20and%20relationships%20within%20the%20raw%20medical%20data.%20The%20generated%0Aembeddings%20are%20stored%20in%20a%20structured%20format%20using%20Hugging%20Face%20datasets%20and%0APyTorch%20dataloaders%20for%20accessibility.%20Vector%20databases%20enable%20efficient%0Aquerying%20and%20retrieval%20for%20machine%20learning%20applications.%20We%20demonstrate%20the%0Aeffectiveness%20of%20HoneyBee%20through%20experiments%20assessing%20the%20quality%20and%0Arepresentativeness%20of%20these%20embeddings.%20The%20framework%20is%20designed%20to%20be%0Aextensible%20to%20other%20medical%20domains%20and%20aims%20to%20accelerate%20oncology%20research%20by%0Aproviding%20high-quality%2C%20machine%20learning-ready%20datasets.%20HoneyBee%20is%20an%20ongoing%0Aopen-source%20effort%2C%20and%20the%20code%2C%20datasets%2C%20and%20models%20are%20available%20at%20the%0Aproject%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07460v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoneyBee%253A%2520A%2520Scalable%2520Modular%2520Framework%2520for%2520Creating%2520Multimodal%2520Oncology%250A%2520%2520Datasets%2520with%2520Foundational%2520Embedding%2520Models%26entry.906535625%3DAakash%2520Tripathi%2520and%2520Asim%2520Waqas%2520and%2520Matthew%2520B.%2520Schabath%2520and%2520Yasin%2520Yilmaz%2520and%2520Ghulam%2520Rasool%26entry.1292438233%3D%2520%2520Developing%2520accurate%2520machine%2520learning%2520models%2520for%2520oncology%2520requires%250Alarge-scale%252C%2520high-quality%2520multimodal%2520datasets.%2520However%252C%2520creating%2520such%2520datasets%250Aremains%2520challenging%2520due%2520to%2520the%2520complexity%2520and%2520heterogeneity%2520of%2520medical%2520data.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520HoneyBee%252C%2520a%2520scalable%2520modular%2520framework%2520for%250Abuilding%2520multimodal%2520oncology%2520datasets%2520that%2520leverages%2520foundation%2520models%2520to%250Agenerate%2520representative%2520embeddings.%2520HoneyBee%2520integrates%2520various%2520data%250Amodalities%252C%2520including%2520clinical%2520diagnostic%2520and%2520pathology%2520imaging%2520data%252C%2520medical%250Anotes%252C%2520reports%252C%2520records%252C%2520and%2520molecular%2520data.%2520It%2520employs%2520data%2520preprocessing%250Atechniques%2520and%2520foundation%2520models%2520to%2520generate%2520embeddings%2520that%2520capture%2520the%250Aessential%2520features%2520and%2520relationships%2520within%2520the%2520raw%2520medical%2520data.%2520The%2520generated%250Aembeddings%2520are%2520stored%2520in%2520a%2520structured%2520format%2520using%2520Hugging%2520Face%2520datasets%2520and%250APyTorch%2520dataloaders%2520for%2520accessibility.%2520Vector%2520databases%2520enable%2520efficient%250Aquerying%2520and%2520retrieval%2520for%2520machine%2520learning%2520applications.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520HoneyBee%2520through%2520experiments%2520assessing%2520the%2520quality%2520and%250Arepresentativeness%2520of%2520these%2520embeddings.%2520The%2520framework%2520is%2520designed%2520to%2520be%250Aextensible%2520to%2520other%2520medical%2520domains%2520and%2520aims%2520to%2520accelerate%2520oncology%2520research%2520by%250Aproviding%2520high-quality%252C%2520machine%2520learning-ready%2520datasets.%2520HoneyBee%2520is%2520an%2520ongoing%250Aopen-source%2520effort%252C%2520and%2520the%2520code%252C%2520datasets%252C%2520and%2520models%2520are%2520available%2520at%2520the%250Aproject%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07460v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models&entry.906535625=Aakash%20Tripathi%20and%20Asim%20Waqas%20and%20Matthew%20B.%20Schabath%20and%20Yasin%20Yilmaz%20and%20Ghulam%20Rasool&entry.1292438233=%20%20Developing%20accurate%20machine%20learning%20models%20for%20oncology%20requires%0Alarge-scale%2C%20high-quality%20multimodal%20datasets.%20However%2C%20creating%20such%20datasets%0Aremains%20challenging%20due%20to%20the%20complexity%20and%20heterogeneity%20of%20medical%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20HoneyBee%2C%20a%20scalable%20modular%20framework%20for%0Abuilding%20multimodal%20oncology%20datasets%20that%20leverages%20foundation%20models%20to%0Agenerate%20representative%20embeddings.%20HoneyBee%20integrates%20various%20data%0Amodalities%2C%20including%20clinical%20diagnostic%20and%20pathology%20imaging%20data%2C%20medical%0Anotes%2C%20reports%2C%20records%2C%20and%20molecular%20data.%20It%20employs%20data%20preprocessing%0Atechniques%20and%20foundation%20models%20to%20generate%20embeddings%20that%20capture%20the%0Aessential%20features%20and%20relationships%20within%20the%20raw%20medical%20data.%20The%20generated%0Aembeddings%20are%20stored%20in%20a%20structured%20format%20using%20Hugging%20Face%20datasets%20and%0APyTorch%20dataloaders%20for%20accessibility.%20Vector%20databases%20enable%20efficient%0Aquerying%20and%20retrieval%20for%20machine%20learning%20applications.%20We%20demonstrate%20the%0Aeffectiveness%20of%20HoneyBee%20through%20experiments%20assessing%20the%20quality%20and%0Arepresentativeness%20of%20these%20embeddings.%20The%20framework%20is%20designed%20to%20be%0Aextensible%20to%20other%20medical%20domains%20and%20aims%20to%20accelerate%20oncology%20research%20by%0Aproviding%20high-quality%2C%20machine%20learning-ready%20datasets.%20HoneyBee%20is%20an%20ongoing%0Aopen-source%20effort%2C%20and%20the%20code%2C%20datasets%2C%20and%20models%20are%20available%20at%20the%0Aproject%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07460v4&entry.124074799=Read"},
{"title": "PatchScaler: An Efficient Patch-Independent Diffusion Model for Image\n  Super-Resolution", "author": "Yong Liu and Hang Dong and Jinshan Pan and Qingji Dong and Kai Chen and Rongxiang Zhang and Lean Fu and Fei Wang", "abstract": "  While diffusion models significantly improve the perceptual quality of\nsuper-resolved images, they usually require a large number of sampling steps,\nresulting in high computational costs and long inference times. Recent efforts\nhave explored reasonable acceleration schemes by reducing the number of\nsampling steps. However, these approaches treat all regions of the image\nequally, overlooking the fact that regions with varying levels of\nreconstruction difficulty require different sampling steps. To address this\nlimitation, we propose PatchScaler, an efficient patch-independent diffusion\npipeline for single image super-resolution. Specifically, PatchScaler\nintroduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature\npatches by quantifying their reconstruction difficulty and establishes shortcut\npaths with different sampling configurations for each group. To further\noptimize the patch-level reconstruction process of PGS, we propose a texture\nprompt that provides rich texture conditional information to the diffusion\nmodel. The texture prompt adaptively retrieves texture priors for the target\npatch from a common reference texture memory. Extensive experiments show that\nour PatchScaler achieves superior performance in both quantitative and\nqualitative evaluations, while significantly speeding up inference. Our code\nwill be available at \\url{https://github.com/yongliuy/PatchScaler}.\n", "link": "http://arxiv.org/abs/2405.17158v4", "date": "2024-11-21", "relevancy": 2.4806, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6741}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.61}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchScaler%3A%20An%20Efficient%20Patch-Independent%20Diffusion%20Model%20for%20Image%0A%20%20Super-Resolution&body=Title%3A%20PatchScaler%3A%20An%20Efficient%20Patch-Independent%20Diffusion%20Model%20for%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Yong%20Liu%20and%20Hang%20Dong%20and%20Jinshan%20Pan%20and%20Qingji%20Dong%20and%20Kai%20Chen%20and%20Rongxiang%20Zhang%20and%20Lean%20Fu%20and%20Fei%20Wang%0AAbstract%3A%20%20%20While%20diffusion%20models%20significantly%20improve%20the%20perceptual%20quality%20of%0Asuper-resolved%20images%2C%20they%20usually%20require%20a%20large%20number%20of%20sampling%20steps%2C%0Aresulting%20in%20high%20computational%20costs%20and%20long%20inference%20times.%20Recent%20efforts%0Ahave%20explored%20reasonable%20acceleration%20schemes%20by%20reducing%20the%20number%20of%0Asampling%20steps.%20However%2C%20these%20approaches%20treat%20all%20regions%20of%20the%20image%0Aequally%2C%20overlooking%20the%20fact%20that%20regions%20with%20varying%20levels%20of%0Areconstruction%20difficulty%20require%20different%20sampling%20steps.%20To%20address%20this%0Alimitation%2C%20we%20propose%20PatchScaler%2C%20an%20efficient%20patch-independent%20diffusion%0Apipeline%20for%20single%20image%20super-resolution.%20Specifically%2C%20PatchScaler%0Aintroduces%20a%20Patch-adaptive%20Group%20Sampling%20%28PGS%29%20strategy%20that%20groups%20feature%0Apatches%20by%20quantifying%20their%20reconstruction%20difficulty%20and%20establishes%20shortcut%0Apaths%20with%20different%20sampling%20configurations%20for%20each%20group.%20To%20further%0Aoptimize%20the%20patch-level%20reconstruction%20process%20of%20PGS%2C%20we%20propose%20a%20texture%0Aprompt%20that%20provides%20rich%20texture%20conditional%20information%20to%20the%20diffusion%0Amodel.%20The%20texture%20prompt%20adaptively%20retrieves%20texture%20priors%20for%20the%20target%0Apatch%20from%20a%20common%20reference%20texture%20memory.%20Extensive%20experiments%20show%20that%0Aour%20PatchScaler%20achieves%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20while%20significantly%20speeding%20up%20inference.%20Our%20code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/yongliuy/PatchScaler%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17158v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchScaler%253A%2520An%2520Efficient%2520Patch-Independent%2520Diffusion%2520Model%2520for%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DYong%2520Liu%2520and%2520Hang%2520Dong%2520and%2520Jinshan%2520Pan%2520and%2520Qingji%2520Dong%2520and%2520Kai%2520Chen%2520and%2520Rongxiang%2520Zhang%2520and%2520Lean%2520Fu%2520and%2520Fei%2520Wang%26entry.1292438233%3D%2520%2520While%2520diffusion%2520models%2520significantly%2520improve%2520the%2520perceptual%2520quality%2520of%250Asuper-resolved%2520images%252C%2520they%2520usually%2520require%2520a%2520large%2520number%2520of%2520sampling%2520steps%252C%250Aresulting%2520in%2520high%2520computational%2520costs%2520and%2520long%2520inference%2520times.%2520Recent%2520efforts%250Ahave%2520explored%2520reasonable%2520acceleration%2520schemes%2520by%2520reducing%2520the%2520number%2520of%250Asampling%2520steps.%2520However%252C%2520these%2520approaches%2520treat%2520all%2520regions%2520of%2520the%2520image%250Aequally%252C%2520overlooking%2520the%2520fact%2520that%2520regions%2520with%2520varying%2520levels%2520of%250Areconstruction%2520difficulty%2520require%2520different%2520sampling%2520steps.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520PatchScaler%252C%2520an%2520efficient%2520patch-independent%2520diffusion%250Apipeline%2520for%2520single%2520image%2520super-resolution.%2520Specifically%252C%2520PatchScaler%250Aintroduces%2520a%2520Patch-adaptive%2520Group%2520Sampling%2520%2528PGS%2529%2520strategy%2520that%2520groups%2520feature%250Apatches%2520by%2520quantifying%2520their%2520reconstruction%2520difficulty%2520and%2520establishes%2520shortcut%250Apaths%2520with%2520different%2520sampling%2520configurations%2520for%2520each%2520group.%2520To%2520further%250Aoptimize%2520the%2520patch-level%2520reconstruction%2520process%2520of%2520PGS%252C%2520we%2520propose%2520a%2520texture%250Aprompt%2520that%2520provides%2520rich%2520texture%2520conditional%2520information%2520to%2520the%2520diffusion%250Amodel.%2520The%2520texture%2520prompt%2520adaptively%2520retrieves%2520texture%2520priors%2520for%2520the%2520target%250Apatch%2520from%2520a%2520common%2520reference%2520texture%2520memory.%2520Extensive%2520experiments%2520show%2520that%250Aour%2520PatchScaler%2520achieves%2520superior%2520performance%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520evaluations%252C%2520while%2520significantly%2520speeding%2520up%2520inference.%2520Our%2520code%250Awill%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/yongliuy/PatchScaler%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17158v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchScaler%3A%20An%20Efficient%20Patch-Independent%20Diffusion%20Model%20for%20Image%0A%20%20Super-Resolution&entry.906535625=Yong%20Liu%20and%20Hang%20Dong%20and%20Jinshan%20Pan%20and%20Qingji%20Dong%20and%20Kai%20Chen%20and%20Rongxiang%20Zhang%20and%20Lean%20Fu%20and%20Fei%20Wang&entry.1292438233=%20%20While%20diffusion%20models%20significantly%20improve%20the%20perceptual%20quality%20of%0Asuper-resolved%20images%2C%20they%20usually%20require%20a%20large%20number%20of%20sampling%20steps%2C%0Aresulting%20in%20high%20computational%20costs%20and%20long%20inference%20times.%20Recent%20efforts%0Ahave%20explored%20reasonable%20acceleration%20schemes%20by%20reducing%20the%20number%20of%0Asampling%20steps.%20However%2C%20these%20approaches%20treat%20all%20regions%20of%20the%20image%0Aequally%2C%20overlooking%20the%20fact%20that%20regions%20with%20varying%20levels%20of%0Areconstruction%20difficulty%20require%20different%20sampling%20steps.%20To%20address%20this%0Alimitation%2C%20we%20propose%20PatchScaler%2C%20an%20efficient%20patch-independent%20diffusion%0Apipeline%20for%20single%20image%20super-resolution.%20Specifically%2C%20PatchScaler%0Aintroduces%20a%20Patch-adaptive%20Group%20Sampling%20%28PGS%29%20strategy%20that%20groups%20feature%0Apatches%20by%20quantifying%20their%20reconstruction%20difficulty%20and%20establishes%20shortcut%0Apaths%20with%20different%20sampling%20configurations%20for%20each%20group.%20To%20further%0Aoptimize%20the%20patch-level%20reconstruction%20process%20of%20PGS%2C%20we%20propose%20a%20texture%0Aprompt%20that%20provides%20rich%20texture%20conditional%20information%20to%20the%20diffusion%0Amodel.%20The%20texture%20prompt%20adaptively%20retrieves%20texture%20priors%20for%20the%20target%0Apatch%20from%20a%20common%20reference%20texture%20memory.%20Extensive%20experiments%20show%20that%0Aour%20PatchScaler%20achieves%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20while%20significantly%20speeding%20up%20inference.%20Our%20code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/yongliuy/PatchScaler%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17158v4&entry.124074799=Read"},
{"title": "Contrasting local and global modeling with machine learning and\n  satellite data: A case study estimating tree canopy height in African\n  savannas", "author": "Esther Rolf and Lucia Gordon and Milind Tambe and Andrew Davies", "abstract": "  While advances in machine learning with satellite imagery (SatML) are\nfacilitating environmental monitoring at a global scale, developing SatML\nmodels that are accurate and useful for local regions remains critical to\nunderstanding and acting on an ever-changing planet. As increasing attention\nand resources are being devoted to training SatML models with global data, it\nis important to understand when improvements in global models will make it\neasier to train or fine-tune models that are accurate in specific regions. To\nexplore this question, we contrast local and global training paradigms for\nSatML through a case study of tree canopy height (TCH) mapping in the Karingani\nGame Reserve, Mozambique. We find that recent advances in global TCH mapping do\nnot necessarily translate to better local modeling abilities in our study\nregion. Specifically, small models trained only with locally-collected data\noutperform published global TCH maps, and even outperform globally pretrained\nmodels that we fine-tune using local data. Analyzing these results further, we\nidentify specific points of conflict and synergy between local and global\nmodeling paradigms that can inform future research toward aligning local and\nglobal performance objectives in geospatial machine learning.\n", "link": "http://arxiv.org/abs/2411.14354v1", "date": "2024-11-21", "relevancy": 2.4646, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrasting%20local%20and%20global%20modeling%20with%20machine%20learning%20and%0A%20%20satellite%20data%3A%20A%20case%20study%20estimating%20tree%20canopy%20height%20in%20African%0A%20%20savannas&body=Title%3A%20Contrasting%20local%20and%20global%20modeling%20with%20machine%20learning%20and%0A%20%20satellite%20data%3A%20A%20case%20study%20estimating%20tree%20canopy%20height%20in%20African%0A%20%20savannas%0AAuthor%3A%20Esther%20Rolf%20and%20Lucia%20Gordon%20and%20Milind%20Tambe%20and%20Andrew%20Davies%0AAbstract%3A%20%20%20While%20advances%20in%20machine%20learning%20with%20satellite%20imagery%20%28SatML%29%20are%0Afacilitating%20environmental%20monitoring%20at%20a%20global%20scale%2C%20developing%20SatML%0Amodels%20that%20are%20accurate%20and%20useful%20for%20local%20regions%20remains%20critical%20to%0Aunderstanding%20and%20acting%20on%20an%20ever-changing%20planet.%20As%20increasing%20attention%0Aand%20resources%20are%20being%20devoted%20to%20training%20SatML%20models%20with%20global%20data%2C%20it%0Ais%20important%20to%20understand%20when%20improvements%20in%20global%20models%20will%20make%20it%0Aeasier%20to%20train%20or%20fine-tune%20models%20that%20are%20accurate%20in%20specific%20regions.%20To%0Aexplore%20this%20question%2C%20we%20contrast%20local%20and%20global%20training%20paradigms%20for%0ASatML%20through%20a%20case%20study%20of%20tree%20canopy%20height%20%28TCH%29%20mapping%20in%20the%20Karingani%0AGame%20Reserve%2C%20Mozambique.%20We%20find%20that%20recent%20advances%20in%20global%20TCH%20mapping%20do%0Anot%20necessarily%20translate%20to%20better%20local%20modeling%20abilities%20in%20our%20study%0Aregion.%20Specifically%2C%20small%20models%20trained%20only%20with%20locally-collected%20data%0Aoutperform%20published%20global%20TCH%20maps%2C%20and%20even%20outperform%20globally%20pretrained%0Amodels%20that%20we%20fine-tune%20using%20local%20data.%20Analyzing%20these%20results%20further%2C%20we%0Aidentify%20specific%20points%20of%20conflict%20and%20synergy%20between%20local%20and%20global%0Amodeling%20paradigms%20that%20can%20inform%20future%20research%20toward%20aligning%20local%20and%0Aglobal%20performance%20objectives%20in%20geospatial%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrasting%2520local%2520and%2520global%2520modeling%2520with%2520machine%2520learning%2520and%250A%2520%2520satellite%2520data%253A%2520A%2520case%2520study%2520estimating%2520tree%2520canopy%2520height%2520in%2520African%250A%2520%2520savannas%26entry.906535625%3DEsther%2520Rolf%2520and%2520Lucia%2520Gordon%2520and%2520Milind%2520Tambe%2520and%2520Andrew%2520Davies%26entry.1292438233%3D%2520%2520While%2520advances%2520in%2520machine%2520learning%2520with%2520satellite%2520imagery%2520%2528SatML%2529%2520are%250Afacilitating%2520environmental%2520monitoring%2520at%2520a%2520global%2520scale%252C%2520developing%2520SatML%250Amodels%2520that%2520are%2520accurate%2520and%2520useful%2520for%2520local%2520regions%2520remains%2520critical%2520to%250Aunderstanding%2520and%2520acting%2520on%2520an%2520ever-changing%2520planet.%2520As%2520increasing%2520attention%250Aand%2520resources%2520are%2520being%2520devoted%2520to%2520training%2520SatML%2520models%2520with%2520global%2520data%252C%2520it%250Ais%2520important%2520to%2520understand%2520when%2520improvements%2520in%2520global%2520models%2520will%2520make%2520it%250Aeasier%2520to%2520train%2520or%2520fine-tune%2520models%2520that%2520are%2520accurate%2520in%2520specific%2520regions.%2520To%250Aexplore%2520this%2520question%252C%2520we%2520contrast%2520local%2520and%2520global%2520training%2520paradigms%2520for%250ASatML%2520through%2520a%2520case%2520study%2520of%2520tree%2520canopy%2520height%2520%2528TCH%2529%2520mapping%2520in%2520the%2520Karingani%250AGame%2520Reserve%252C%2520Mozambique.%2520We%2520find%2520that%2520recent%2520advances%2520in%2520global%2520TCH%2520mapping%2520do%250Anot%2520necessarily%2520translate%2520to%2520better%2520local%2520modeling%2520abilities%2520in%2520our%2520study%250Aregion.%2520Specifically%252C%2520small%2520models%2520trained%2520only%2520with%2520locally-collected%2520data%250Aoutperform%2520published%2520global%2520TCH%2520maps%252C%2520and%2520even%2520outperform%2520globally%2520pretrained%250Amodels%2520that%2520we%2520fine-tune%2520using%2520local%2520data.%2520Analyzing%2520these%2520results%2520further%252C%2520we%250Aidentify%2520specific%2520points%2520of%2520conflict%2520and%2520synergy%2520between%2520local%2520and%2520global%250Amodeling%2520paradigms%2520that%2520can%2520inform%2520future%2520research%2520toward%2520aligning%2520local%2520and%250Aglobal%2520performance%2520objectives%2520in%2520geospatial%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrasting%20local%20and%20global%20modeling%20with%20machine%20learning%20and%0A%20%20satellite%20data%3A%20A%20case%20study%20estimating%20tree%20canopy%20height%20in%20African%0A%20%20savannas&entry.906535625=Esther%20Rolf%20and%20Lucia%20Gordon%20and%20Milind%20Tambe%20and%20Andrew%20Davies&entry.1292438233=%20%20While%20advances%20in%20machine%20learning%20with%20satellite%20imagery%20%28SatML%29%20are%0Afacilitating%20environmental%20monitoring%20at%20a%20global%20scale%2C%20developing%20SatML%0Amodels%20that%20are%20accurate%20and%20useful%20for%20local%20regions%20remains%20critical%20to%0Aunderstanding%20and%20acting%20on%20an%20ever-changing%20planet.%20As%20increasing%20attention%0Aand%20resources%20are%20being%20devoted%20to%20training%20SatML%20models%20with%20global%20data%2C%20it%0Ais%20important%20to%20understand%20when%20improvements%20in%20global%20models%20will%20make%20it%0Aeasier%20to%20train%20or%20fine-tune%20models%20that%20are%20accurate%20in%20specific%20regions.%20To%0Aexplore%20this%20question%2C%20we%20contrast%20local%20and%20global%20training%20paradigms%20for%0ASatML%20through%20a%20case%20study%20of%20tree%20canopy%20height%20%28TCH%29%20mapping%20in%20the%20Karingani%0AGame%20Reserve%2C%20Mozambique.%20We%20find%20that%20recent%20advances%20in%20global%20TCH%20mapping%20do%0Anot%20necessarily%20translate%20to%20better%20local%20modeling%20abilities%20in%20our%20study%0Aregion.%20Specifically%2C%20small%20models%20trained%20only%20with%20locally-collected%20data%0Aoutperform%20published%20global%20TCH%20maps%2C%20and%20even%20outperform%20globally%20pretrained%0Amodels%20that%20we%20fine-tune%20using%20local%20data.%20Analyzing%20these%20results%20further%2C%20we%0Aidentify%20specific%20points%20of%20conflict%20and%20synergy%20between%20local%20and%20global%0Amodeling%20paradigms%20that%20can%20inform%20future%20research%20toward%20aligning%20local%20and%0Aglobal%20performance%20objectives%20in%20geospatial%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14354v1&entry.124074799=Read"},
{"title": "OmniGen: Unified Image Generation", "author": "Shitao Xiao and Yueze Wang and Junjie Zhou and Huaying Yuan and Xingrun Xing and Ruiran Yan and Chaofan Li and Shuting Wang and Tiejun Huang and Zheng Liu", "abstract": "  The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements.\n", "link": "http://arxiv.org/abs/2409.11340v2", "date": "2024-11-21", "relevancy": 2.4476, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6188}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.613}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniGen%3A%20Unified%20Image%20Generation&body=Title%3A%20OmniGen%3A%20Unified%20Image%20Generation%0AAuthor%3A%20Shitao%20Xiao%20and%20Yueze%20Wang%20and%20Junjie%20Zhou%20and%20Huaying%20Yuan%20and%20Xingrun%20Xing%20and%20Ruiran%20Yan%20and%20Chaofan%20Li%20and%20Shuting%20Wang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20unified%20language%20generation%0Atasks%20and%20revolutionized%20human-machine%20interaction.%20However%2C%20in%20the%20realm%20of%0Aimage%20generation%2C%20a%20unified%20model%20capable%20of%20handling%20various%20tasks%20within%20a%0Asingle%20framework%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%0AOmniGen%2C%20a%20new%20diffusion%20model%20for%20unified%20image%20generation.%20OmniGen%20is%0Acharacterized%20by%20the%20following%20features%3A%201%29%20Unification%3A%20OmniGen%20not%20only%0Ademonstrates%20text-to-image%20generation%20capabilities%20but%20also%20inherently%20supports%0Avarious%20downstream%20tasks%2C%20such%20as%20image%20editing%2C%20subject-driven%20generation%2C%20and%0Avisual-conditional%20generation.%202%29%20Simplicity%3A%20The%20architecture%20of%20OmniGen%20is%0Ahighly%20simplified%2C%20eliminating%20the%20need%20for%20additional%20plugins.%20Moreover%2C%0Acompared%20to%20existing%20diffusion%20models%2C%20it%20is%20more%20user-friendly%20and%20can%0Acomplete%20complex%20tasks%20end-to-end%20through%20instructions%20without%20the%20need%20for%0Aextra%20intermediate%20steps%2C%20greatly%20simplifying%20the%20image%20generation%20workflow.%203%29%0AKnowledge%20Transfer%3A%20Benefit%20from%20learning%20in%20a%20unified%20format%2C%20OmniGen%0Aeffectively%20transfers%20knowledge%20across%20different%20tasks%2C%20manages%20unseen%20tasks%0Aand%20domains%2C%20and%20exhibits%20novel%20capabilities.%20We%20also%20explore%20the%20model%27s%0Areasoning%20capabilities%20and%20potential%20applications%20of%20the%20chain-of-thought%0Amechanism.%20This%20work%20represents%20the%20first%20attempt%20at%20a%20general-purpose%20image%0Ageneration%20model%2C%20and%20we%20will%20release%20our%20resources%20at%0Ahttps%3A//github.com/VectorSpaceLab/OmniGen%20to%20foster%20future%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniGen%253A%2520Unified%2520Image%2520Generation%26entry.906535625%3DShitao%2520Xiao%2520and%2520Yueze%2520Wang%2520and%2520Junjie%2520Zhou%2520and%2520Huaying%2520Yuan%2520and%2520Xingrun%2520Xing%2520and%2520Ruiran%2520Yan%2520and%2520Chaofan%2520Li%2520and%2520Shuting%2520Wang%2520and%2520Tiejun%2520Huang%2520and%2520Zheng%2520Liu%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520unified%2520language%2520generation%250Atasks%2520and%2520revolutionized%2520human-machine%2520interaction.%2520However%252C%2520in%2520the%2520realm%2520of%250Aimage%2520generation%252C%2520a%2520unified%2520model%2520capable%2520of%2520handling%2520various%2520tasks%2520within%2520a%250Asingle%2520framework%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%250AOmniGen%252C%2520a%2520new%2520diffusion%2520model%2520for%2520unified%2520image%2520generation.%2520OmniGen%2520is%250Acharacterized%2520by%2520the%2520following%2520features%253A%25201%2529%2520Unification%253A%2520OmniGen%2520not%2520only%250Ademonstrates%2520text-to-image%2520generation%2520capabilities%2520but%2520also%2520inherently%2520supports%250Avarious%2520downstream%2520tasks%252C%2520such%2520as%2520image%2520editing%252C%2520subject-driven%2520generation%252C%2520and%250Avisual-conditional%2520generation.%25202%2529%2520Simplicity%253A%2520The%2520architecture%2520of%2520OmniGen%2520is%250Ahighly%2520simplified%252C%2520eliminating%2520the%2520need%2520for%2520additional%2520plugins.%2520Moreover%252C%250Acompared%2520to%2520existing%2520diffusion%2520models%252C%2520it%2520is%2520more%2520user-friendly%2520and%2520can%250Acomplete%2520complex%2520tasks%2520end-to-end%2520through%2520instructions%2520without%2520the%2520need%2520for%250Aextra%2520intermediate%2520steps%252C%2520greatly%2520simplifying%2520the%2520image%2520generation%2520workflow.%25203%2529%250AKnowledge%2520Transfer%253A%2520Benefit%2520from%2520learning%2520in%2520a%2520unified%2520format%252C%2520OmniGen%250Aeffectively%2520transfers%2520knowledge%2520across%2520different%2520tasks%252C%2520manages%2520unseen%2520tasks%250Aand%2520domains%252C%2520and%2520exhibits%2520novel%2520capabilities.%2520We%2520also%2520explore%2520the%2520model%2527s%250Areasoning%2520capabilities%2520and%2520potential%2520applications%2520of%2520the%2520chain-of-thought%250Amechanism.%2520This%2520work%2520represents%2520the%2520first%2520attempt%2520at%2520a%2520general-purpose%2520image%250Ageneration%2520model%252C%2520and%2520we%2520will%2520release%2520our%2520resources%2520at%250Ahttps%253A//github.com/VectorSpaceLab/OmniGen%2520to%2520foster%2520future%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniGen%3A%20Unified%20Image%20Generation&entry.906535625=Shitao%20Xiao%20and%20Yueze%20Wang%20and%20Junjie%20Zhou%20and%20Huaying%20Yuan%20and%20Xingrun%20Xing%20and%20Ruiran%20Yan%20and%20Chaofan%20Li%20and%20Shuting%20Wang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu&entry.1292438233=%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20unified%20language%20generation%0Atasks%20and%20revolutionized%20human-machine%20interaction.%20However%2C%20in%20the%20realm%20of%0Aimage%20generation%2C%20a%20unified%20model%20capable%20of%20handling%20various%20tasks%20within%20a%0Asingle%20framework%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%0AOmniGen%2C%20a%20new%20diffusion%20model%20for%20unified%20image%20generation.%20OmniGen%20is%0Acharacterized%20by%20the%20following%20features%3A%201%29%20Unification%3A%20OmniGen%20not%20only%0Ademonstrates%20text-to-image%20generation%20capabilities%20but%20also%20inherently%20supports%0Avarious%20downstream%20tasks%2C%20such%20as%20image%20editing%2C%20subject-driven%20generation%2C%20and%0Avisual-conditional%20generation.%202%29%20Simplicity%3A%20The%20architecture%20of%20OmniGen%20is%0Ahighly%20simplified%2C%20eliminating%20the%20need%20for%20additional%20plugins.%20Moreover%2C%0Acompared%20to%20existing%20diffusion%20models%2C%20it%20is%20more%20user-friendly%20and%20can%0Acomplete%20complex%20tasks%20end-to-end%20through%20instructions%20without%20the%20need%20for%0Aextra%20intermediate%20steps%2C%20greatly%20simplifying%20the%20image%20generation%20workflow.%203%29%0AKnowledge%20Transfer%3A%20Benefit%20from%20learning%20in%20a%20unified%20format%2C%20OmniGen%0Aeffectively%20transfers%20knowledge%20across%20different%20tasks%2C%20manages%20unseen%20tasks%0Aand%20domains%2C%20and%20exhibits%20novel%20capabilities.%20We%20also%20explore%20the%20model%27s%0Areasoning%20capabilities%20and%20potential%20applications%20of%20the%20chain-of-thought%0Amechanism.%20This%20work%20represents%20the%20first%20attempt%20at%20a%20general-purpose%20image%0Ageneration%20model%2C%20and%20we%20will%20release%20our%20resources%20at%0Ahttps%3A//github.com/VectorSpaceLab/OmniGen%20to%20foster%20future%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11340v2&entry.124074799=Read"},
{"title": "Improving Steering Vectors by Targeting Sparse Autoencoder Features", "author": "Sviatoslav Chalnev and Matthew Siu and Arthur Conmy", "abstract": "  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nmethods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents\n[Templeton et al., 2024]. In our work, we address this issue by using SAEs to\nmeasure the effects of steering vectors, giving us a method that can be used to\nunderstand the causal effect of any steering vector intervention. We use this\nmethod for measuring causal effects to develop an improved steering method,\nSAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific\nSAE features while minimizing unintended side effects. We show that overall,\nSAE-TS balances steering effects with coherence better than CAA and SAE feature\nsteering, when evaluated on a range of tasks.\n", "link": "http://arxiv.org/abs/2411.02193v2", "date": "2024-11-21", "relevancy": 2.4469, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features&body=Title%3A%20Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features%0AAuthor%3A%20Sviatoslav%20Chalnev%20and%20Matthew%20Siu%20and%20Arthur%20Conmy%0AAbstract%3A%20%20%20To%20control%20the%20behavior%20of%20language%20models%2C%20steering%20methods%20attempt%20to%0Aensure%20that%20outputs%20of%20the%20model%20satisfy%20specific%20pre-defined%20properties.%0AAdding%20steering%20vectors%20to%20the%20model%20is%20a%20promising%20method%20of%20model%20control%0Athat%20is%20easier%20than%20finetuning%2C%20and%20may%20be%20more%20robust%20than%20prompting.%20However%2C%0Ait%20can%20be%20difficult%20to%20anticipate%20the%20effects%20of%20steering%20vectors%20produced%20by%0Amethods%20such%20as%20CAA%20%5BPanickssery%20et%20al.%2C%202024%5D%20or%20the%20direct%20use%20of%20SAE%20latents%0A%5BTempleton%20et%20al.%2C%202024%5D.%20In%20our%20work%2C%20we%20address%20this%20issue%20by%20using%20SAEs%20to%0Ameasure%20the%20effects%20of%20steering%20vectors%2C%20giving%20us%20a%20method%20that%20can%20be%20used%20to%0Aunderstand%20the%20causal%20effect%20of%20any%20steering%20vector%20intervention.%20We%20use%20this%0Amethod%20for%20measuring%20causal%20effects%20to%20develop%20an%20improved%20steering%20method%2C%0ASAE-Targeted%20Steering%20%28SAE-TS%29%2C%20which%20finds%20steering%20vectors%20to%20target%20specific%0ASAE%20features%20while%20minimizing%20unintended%20side%20effects.%20We%20show%20that%20overall%2C%0ASAE-TS%20balances%20steering%20effects%20with%20coherence%20better%20than%20CAA%20and%20SAE%20feature%0Asteering%2C%20when%20evaluated%20on%20a%20range%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Steering%2520Vectors%2520by%2520Targeting%2520Sparse%2520Autoencoder%2520Features%26entry.906535625%3DSviatoslav%2520Chalnev%2520and%2520Matthew%2520Siu%2520and%2520Arthur%2520Conmy%26entry.1292438233%3D%2520%2520To%2520control%2520the%2520behavior%2520of%2520language%2520models%252C%2520steering%2520methods%2520attempt%2520to%250Aensure%2520that%2520outputs%2520of%2520the%2520model%2520satisfy%2520specific%2520pre-defined%2520properties.%250AAdding%2520steering%2520vectors%2520to%2520the%2520model%2520is%2520a%2520promising%2520method%2520of%2520model%2520control%250Athat%2520is%2520easier%2520than%2520finetuning%252C%2520and%2520may%2520be%2520more%2520robust%2520than%2520prompting.%2520However%252C%250Ait%2520can%2520be%2520difficult%2520to%2520anticipate%2520the%2520effects%2520of%2520steering%2520vectors%2520produced%2520by%250Amethods%2520such%2520as%2520CAA%2520%255BPanickssery%2520et%2520al.%252C%25202024%255D%2520or%2520the%2520direct%2520use%2520of%2520SAE%2520latents%250A%255BTempleton%2520et%2520al.%252C%25202024%255D.%2520In%2520our%2520work%252C%2520we%2520address%2520this%2520issue%2520by%2520using%2520SAEs%2520to%250Ameasure%2520the%2520effects%2520of%2520steering%2520vectors%252C%2520giving%2520us%2520a%2520method%2520that%2520can%2520be%2520used%2520to%250Aunderstand%2520the%2520causal%2520effect%2520of%2520any%2520steering%2520vector%2520intervention.%2520We%2520use%2520this%250Amethod%2520for%2520measuring%2520causal%2520effects%2520to%2520develop%2520an%2520improved%2520steering%2520method%252C%250ASAE-Targeted%2520Steering%2520%2528SAE-TS%2529%252C%2520which%2520finds%2520steering%2520vectors%2520to%2520target%2520specific%250ASAE%2520features%2520while%2520minimizing%2520unintended%2520side%2520effects.%2520We%2520show%2520that%2520overall%252C%250ASAE-TS%2520balances%2520steering%2520effects%2520with%2520coherence%2520better%2520than%2520CAA%2520and%2520SAE%2520feature%250Asteering%252C%2520when%2520evaluated%2520on%2520a%2520range%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features&entry.906535625=Sviatoslav%20Chalnev%20and%20Matthew%20Siu%20and%20Arthur%20Conmy&entry.1292438233=%20%20To%20control%20the%20behavior%20of%20language%20models%2C%20steering%20methods%20attempt%20to%0Aensure%20that%20outputs%20of%20the%20model%20satisfy%20specific%20pre-defined%20properties.%0AAdding%20steering%20vectors%20to%20the%20model%20is%20a%20promising%20method%20of%20model%20control%0Athat%20is%20easier%20than%20finetuning%2C%20and%20may%20be%20more%20robust%20than%20prompting.%20However%2C%0Ait%20can%20be%20difficult%20to%20anticipate%20the%20effects%20of%20steering%20vectors%20produced%20by%0Amethods%20such%20as%20CAA%20%5BPanickssery%20et%20al.%2C%202024%5D%20or%20the%20direct%20use%20of%20SAE%20latents%0A%5BTempleton%20et%20al.%2C%202024%5D.%20In%20our%20work%2C%20we%20address%20this%20issue%20by%20using%20SAEs%20to%0Ameasure%20the%20effects%20of%20steering%20vectors%2C%20giving%20us%20a%20method%20that%20can%20be%20used%20to%0Aunderstand%20the%20causal%20effect%20of%20any%20steering%20vector%20intervention.%20We%20use%20this%0Amethod%20for%20measuring%20causal%20effects%20to%20develop%20an%20improved%20steering%20method%2C%0ASAE-Targeted%20Steering%20%28SAE-TS%29%2C%20which%20finds%20steering%20vectors%20to%20target%20specific%0ASAE%20features%20while%20minimizing%20unintended%20side%20effects.%20We%20show%20that%20overall%2C%0ASAE-TS%20balances%20steering%20effects%20with%20coherence%20better%20than%20CAA%20and%20SAE%20feature%0Asteering%2C%20when%20evaluated%20on%20a%20range%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02193v2&entry.124074799=Read"},
{"title": "Variational Nearest Neighbor Gaussian Process", "author": "Luhuan Wu and Geoff Pleiss and John Cunningham", "abstract": "  Variational approximations to Gaussian processes (GPs) typically use a small\nset of inducing points to form a low-rank approximation to the covariance\nmatrix. In this work, we instead exploit a sparse approximation of the\nprecision matrix. We propose variational nearest neighbor Gaussian process\n(VNNGP), which introduces a prior that only retains correlations within $K$\nnearest-neighboring observations, thereby inducing sparse precision structure.\nUsing the variational framework, VNNGP's objective can be factorized over both\nobservations and inducing points, enabling stochastic optimization with a time\ncomplexity of $O(K^3)$. Hence, we can arbitrarily scale the inducing point\nsize, even to the point of putting inducing points at every observed location.\nWe compare VNNGP to other scalable GPs through various experiments, and\ndemonstrate that VNNGP (1) can dramatically outperform low-rank methods, and\n(2) is less prone to overfitting than other nearest neighbor methods.\n", "link": "http://arxiv.org/abs/2202.01694v4", "date": "2024-11-21", "relevancy": 2.4387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4919}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4902}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Nearest%20Neighbor%20Gaussian%20Process&body=Title%3A%20Variational%20Nearest%20Neighbor%20Gaussian%20Process%0AAuthor%3A%20Luhuan%20Wu%20and%20Geoff%20Pleiss%20and%20John%20Cunningham%0AAbstract%3A%20%20%20Variational%20approximations%20to%20Gaussian%20processes%20%28GPs%29%20typically%20use%20a%20small%0Aset%20of%20inducing%20points%20to%20form%20a%20low-rank%20approximation%20to%20the%20covariance%0Amatrix.%20In%20this%20work%2C%20we%20instead%20exploit%20a%20sparse%20approximation%20of%20the%0Aprecision%20matrix.%20We%20propose%20variational%20nearest%20neighbor%20Gaussian%20process%0A%28VNNGP%29%2C%20which%20introduces%20a%20prior%20that%20only%20retains%20correlations%20within%20%24K%24%0Anearest-neighboring%20observations%2C%20thereby%20inducing%20sparse%20precision%20structure.%0AUsing%20the%20variational%20framework%2C%20VNNGP%27s%20objective%20can%20be%20factorized%20over%20both%0Aobservations%20and%20inducing%20points%2C%20enabling%20stochastic%20optimization%20with%20a%20time%0Acomplexity%20of%20%24O%28K%5E3%29%24.%20Hence%2C%20we%20can%20arbitrarily%20scale%20the%20inducing%20point%0Asize%2C%20even%20to%20the%20point%20of%20putting%20inducing%20points%20at%20every%20observed%20location.%0AWe%20compare%20VNNGP%20to%20other%20scalable%20GPs%20through%20various%20experiments%2C%20and%0Ademonstrate%20that%20VNNGP%20%281%29%20can%20dramatically%20outperform%20low-rank%20methods%2C%20and%0A%282%29%20is%20less%20prone%20to%20overfitting%20than%20other%20nearest%20neighbor%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.01694v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Nearest%2520Neighbor%2520Gaussian%2520Process%26entry.906535625%3DLuhuan%2520Wu%2520and%2520Geoff%2520Pleiss%2520and%2520John%2520Cunningham%26entry.1292438233%3D%2520%2520Variational%2520approximations%2520to%2520Gaussian%2520processes%2520%2528GPs%2529%2520typically%2520use%2520a%2520small%250Aset%2520of%2520inducing%2520points%2520to%2520form%2520a%2520low-rank%2520approximation%2520to%2520the%2520covariance%250Amatrix.%2520In%2520this%2520work%252C%2520we%2520instead%2520exploit%2520a%2520sparse%2520approximation%2520of%2520the%250Aprecision%2520matrix.%2520We%2520propose%2520variational%2520nearest%2520neighbor%2520Gaussian%2520process%250A%2528VNNGP%2529%252C%2520which%2520introduces%2520a%2520prior%2520that%2520only%2520retains%2520correlations%2520within%2520%2524K%2524%250Anearest-neighboring%2520observations%252C%2520thereby%2520inducing%2520sparse%2520precision%2520structure.%250AUsing%2520the%2520variational%2520framework%252C%2520VNNGP%2527s%2520objective%2520can%2520be%2520factorized%2520over%2520both%250Aobservations%2520and%2520inducing%2520points%252C%2520enabling%2520stochastic%2520optimization%2520with%2520a%2520time%250Acomplexity%2520of%2520%2524O%2528K%255E3%2529%2524.%2520Hence%252C%2520we%2520can%2520arbitrarily%2520scale%2520the%2520inducing%2520point%250Asize%252C%2520even%2520to%2520the%2520point%2520of%2520putting%2520inducing%2520points%2520at%2520every%2520observed%2520location.%250AWe%2520compare%2520VNNGP%2520to%2520other%2520scalable%2520GPs%2520through%2520various%2520experiments%252C%2520and%250Ademonstrate%2520that%2520VNNGP%2520%25281%2529%2520can%2520dramatically%2520outperform%2520low-rank%2520methods%252C%2520and%250A%25282%2529%2520is%2520less%2520prone%2520to%2520overfitting%2520than%2520other%2520nearest%2520neighbor%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.01694v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Nearest%20Neighbor%20Gaussian%20Process&entry.906535625=Luhuan%20Wu%20and%20Geoff%20Pleiss%20and%20John%20Cunningham&entry.1292438233=%20%20Variational%20approximations%20to%20Gaussian%20processes%20%28GPs%29%20typically%20use%20a%20small%0Aset%20of%20inducing%20points%20to%20form%20a%20low-rank%20approximation%20to%20the%20covariance%0Amatrix.%20In%20this%20work%2C%20we%20instead%20exploit%20a%20sparse%20approximation%20of%20the%0Aprecision%20matrix.%20We%20propose%20variational%20nearest%20neighbor%20Gaussian%20process%0A%28VNNGP%29%2C%20which%20introduces%20a%20prior%20that%20only%20retains%20correlations%20within%20%24K%24%0Anearest-neighboring%20observations%2C%20thereby%20inducing%20sparse%20precision%20structure.%0AUsing%20the%20variational%20framework%2C%20VNNGP%27s%20objective%20can%20be%20factorized%20over%20both%0Aobservations%20and%20inducing%20points%2C%20enabling%20stochastic%20optimization%20with%20a%20time%0Acomplexity%20of%20%24O%28K%5E3%29%24.%20Hence%2C%20we%20can%20arbitrarily%20scale%20the%20inducing%20point%0Asize%2C%20even%20to%20the%20point%20of%20putting%20inducing%20points%20at%20every%20observed%20location.%0AWe%20compare%20VNNGP%20to%20other%20scalable%20GPs%20through%20various%20experiments%2C%20and%0Ademonstrate%20that%20VNNGP%20%281%29%20can%20dramatically%20outperform%20low-rank%20methods%2C%20and%0A%282%29%20is%20less%20prone%20to%20overfitting%20than%20other%20nearest%20neighbor%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.01694v4&entry.124074799=Read"},
{"title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image\n  Diffusion Models", "author": "Jaerin Lee and Daniel Sungho Jung and Kanggeon Lee and Kyoung Mu Lee", "abstract": "  We introduce SemanticDraw, a new paradigm of interactive content creation\nwhere high-quality images are generated in near real-time from given multiple\nhand-drawn regions, each encoding prescribed semantic meaning. In order to\nmaximize the productivity of content creators and to fully realize their\nartistic imagination, it requires both quick interactive interfaces and\nfine-grained regional controls in their tools. Despite astonishing generation\nquality from recent diffusion models, we find that existing approaches for\nregional controllability are very slow (52 seconds for $512 \\times 512$ image)\nwhile not compatible with acceleration methods such as LCM, blocking their huge\npotential in interactive content creation. From this observation, we build our\nsolution for interactive content creation in two steps: (1) we establish\ncompatibility between region-based controls and acceleration techniques for\ndiffusion models, maintaining high fidelity of multi-prompt image generation\nwith $\\times 10$ reduced number of inference steps, (2) we increase the\ngeneration throughput with our new multi-prompt stream batch pipeline, enabling\nlow-latency generation from multiple, region-based text prompts on a single RTX\n2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion\nmodels and acceleration schedulers, allowing sub-second (0.64 seconds) image\ncontent creation application upon well-established image diffusion models. Our\nproject page is: https://jaerinlee.com/research/semantic-draw.\n", "link": "http://arxiv.org/abs/2403.09055v3", "date": "2024-11-21", "relevancy": 2.4298, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemanticDraw%3A%20Towards%20Real-Time%20Interactive%20Content%20Creation%20from%20Image%0A%20%20Diffusion%20Models&body=Title%3A%20SemanticDraw%3A%20Towards%20Real-Time%20Interactive%20Content%20Creation%20from%20Image%0A%20%20Diffusion%20Models%0AAuthor%3A%20Jaerin%20Lee%20and%20Daniel%20Sungho%20Jung%20and%20Kanggeon%20Lee%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20We%20introduce%20SemanticDraw%2C%20a%20new%20paradigm%20of%20interactive%20content%20creation%0Awhere%20high-quality%20images%20are%20generated%20in%20near%20real-time%20from%20given%20multiple%0Ahand-drawn%20regions%2C%20each%20encoding%20prescribed%20semantic%20meaning.%20In%20order%20to%0Amaximize%20the%20productivity%20of%20content%20creators%20and%20to%20fully%20realize%20their%0Aartistic%20imagination%2C%20it%20requires%20both%20quick%20interactive%20interfaces%20and%0Afine-grained%20regional%20controls%20in%20their%20tools.%20Despite%20astonishing%20generation%0Aquality%20from%20recent%20diffusion%20models%2C%20we%20find%20that%20existing%20approaches%20for%0Aregional%20controllability%20are%20very%20slow%20%2852%20seconds%20for%20%24512%20%5Ctimes%20512%24%20image%29%0Awhile%20not%20compatible%20with%20acceleration%20methods%20such%20as%20LCM%2C%20blocking%20their%20huge%0Apotential%20in%20interactive%20content%20creation.%20From%20this%20observation%2C%20we%20build%20our%0Asolution%20for%20interactive%20content%20creation%20in%20two%20steps%3A%20%281%29%20we%20establish%0Acompatibility%20between%20region-based%20controls%20and%20acceleration%20techniques%20for%0Adiffusion%20models%2C%20maintaining%20high%20fidelity%20of%20multi-prompt%20image%20generation%0Awith%20%24%5Ctimes%2010%24%20reduced%20number%20of%20inference%20steps%2C%20%282%29%20we%20increase%20the%0Ageneration%20throughput%20with%20our%20new%20multi-prompt%20stream%20batch%20pipeline%2C%20enabling%0Alow-latency%20generation%20from%20multiple%2C%20region-based%20text%20prompts%20on%20a%20single%20RTX%0A2080%20Ti%20GPU.%20Our%20proposed%20framework%20is%20generalizable%20to%20any%20existing%20diffusion%0Amodels%20and%20acceleration%20schedulers%2C%20allowing%20sub-second%20%280.64%20seconds%29%20image%0Acontent%20creation%20application%20upon%20well-established%20image%20diffusion%20models.%20Our%0Aproject%20page%20is%3A%20https%3A//jaerinlee.com/research/semantic-draw.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09055v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemanticDraw%253A%2520Towards%2520Real-Time%2520Interactive%2520Content%2520Creation%2520from%2520Image%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DJaerin%2520Lee%2520and%2520Daniel%2520Sungho%2520Jung%2520and%2520Kanggeon%2520Lee%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3D%2520%2520We%2520introduce%2520SemanticDraw%252C%2520a%2520new%2520paradigm%2520of%2520interactive%2520content%2520creation%250Awhere%2520high-quality%2520images%2520are%2520generated%2520in%2520near%2520real-time%2520from%2520given%2520multiple%250Ahand-drawn%2520regions%252C%2520each%2520encoding%2520prescribed%2520semantic%2520meaning.%2520In%2520order%2520to%250Amaximize%2520the%2520productivity%2520of%2520content%2520creators%2520and%2520to%2520fully%2520realize%2520their%250Aartistic%2520imagination%252C%2520it%2520requires%2520both%2520quick%2520interactive%2520interfaces%2520and%250Afine-grained%2520regional%2520controls%2520in%2520their%2520tools.%2520Despite%2520astonishing%2520generation%250Aquality%2520from%2520recent%2520diffusion%2520models%252C%2520we%2520find%2520that%2520existing%2520approaches%2520for%250Aregional%2520controllability%2520are%2520very%2520slow%2520%252852%2520seconds%2520for%2520%2524512%2520%255Ctimes%2520512%2524%2520image%2529%250Awhile%2520not%2520compatible%2520with%2520acceleration%2520methods%2520such%2520as%2520LCM%252C%2520blocking%2520their%2520huge%250Apotential%2520in%2520interactive%2520content%2520creation.%2520From%2520this%2520observation%252C%2520we%2520build%2520our%250Asolution%2520for%2520interactive%2520content%2520creation%2520in%2520two%2520steps%253A%2520%25281%2529%2520we%2520establish%250Acompatibility%2520between%2520region-based%2520controls%2520and%2520acceleration%2520techniques%2520for%250Adiffusion%2520models%252C%2520maintaining%2520high%2520fidelity%2520of%2520multi-prompt%2520image%2520generation%250Awith%2520%2524%255Ctimes%252010%2524%2520reduced%2520number%2520of%2520inference%2520steps%252C%2520%25282%2529%2520we%2520increase%2520the%250Ageneration%2520throughput%2520with%2520our%2520new%2520multi-prompt%2520stream%2520batch%2520pipeline%252C%2520enabling%250Alow-latency%2520generation%2520from%2520multiple%252C%2520region-based%2520text%2520prompts%2520on%2520a%2520single%2520RTX%250A2080%2520Ti%2520GPU.%2520Our%2520proposed%2520framework%2520is%2520generalizable%2520to%2520any%2520existing%2520diffusion%250Amodels%2520and%2520acceleration%2520schedulers%252C%2520allowing%2520sub-second%2520%25280.64%2520seconds%2529%2520image%250Acontent%2520creation%2520application%2520upon%2520well-established%2520image%2520diffusion%2520models.%2520Our%250Aproject%2520page%2520is%253A%2520https%253A//jaerinlee.com/research/semantic-draw.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09055v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemanticDraw%3A%20Towards%20Real-Time%20Interactive%20Content%20Creation%20from%20Image%0A%20%20Diffusion%20Models&entry.906535625=Jaerin%20Lee%20and%20Daniel%20Sungho%20Jung%20and%20Kanggeon%20Lee%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20We%20introduce%20SemanticDraw%2C%20a%20new%20paradigm%20of%20interactive%20content%20creation%0Awhere%20high-quality%20images%20are%20generated%20in%20near%20real-time%20from%20given%20multiple%0Ahand-drawn%20regions%2C%20each%20encoding%20prescribed%20semantic%20meaning.%20In%20order%20to%0Amaximize%20the%20productivity%20of%20content%20creators%20and%20to%20fully%20realize%20their%0Aartistic%20imagination%2C%20it%20requires%20both%20quick%20interactive%20interfaces%20and%0Afine-grained%20regional%20controls%20in%20their%20tools.%20Despite%20astonishing%20generation%0Aquality%20from%20recent%20diffusion%20models%2C%20we%20find%20that%20existing%20approaches%20for%0Aregional%20controllability%20are%20very%20slow%20%2852%20seconds%20for%20%24512%20%5Ctimes%20512%24%20image%29%0Awhile%20not%20compatible%20with%20acceleration%20methods%20such%20as%20LCM%2C%20blocking%20their%20huge%0Apotential%20in%20interactive%20content%20creation.%20From%20this%20observation%2C%20we%20build%20our%0Asolution%20for%20interactive%20content%20creation%20in%20two%20steps%3A%20%281%29%20we%20establish%0Acompatibility%20between%20region-based%20controls%20and%20acceleration%20techniques%20for%0Adiffusion%20models%2C%20maintaining%20high%20fidelity%20of%20multi-prompt%20image%20generation%0Awith%20%24%5Ctimes%2010%24%20reduced%20number%20of%20inference%20steps%2C%20%282%29%20we%20increase%20the%0Ageneration%20throughput%20with%20our%20new%20multi-prompt%20stream%20batch%20pipeline%2C%20enabling%0Alow-latency%20generation%20from%20multiple%2C%20region-based%20text%20prompts%20on%20a%20single%20RTX%0A2080%20Ti%20GPU.%20Our%20proposed%20framework%20is%20generalizable%20to%20any%20existing%20diffusion%0Amodels%20and%20acceleration%20schedulers%2C%20allowing%20sub-second%20%280.64%20seconds%29%20image%0Acontent%20creation%20application%20upon%20well-established%20image%20diffusion%20models.%20Our%0Aproject%20page%20is%3A%20https%3A//jaerinlee.com/research/semantic-draw.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09055v3&entry.124074799=Read"},
{"title": "Self-supervised learning for radio-astronomy source classification: a\n  benchmark", "author": "Thomas Cecconello and Simone Riggi and Ugo Becciano and Fabio Vitello and Andrew M. Hopkins and Giuseppe Vizzari and Concetto Spampinato and Simone Palazzo", "abstract": "  The upcoming Square Kilometer Array (SKA) telescope marks a significant step\nforward in radio astronomy, presenting new opportunities and challenges for\ndata analysis. Traditional visual models pretrained on optical photography\nimages may not perform optimally on radio interferometry images, which have\ndistinct visual characteristics.\n  Self-Supervised Learning (SSL) offers a promising approach to address this\nissue, leveraging the abundant unlabeled data in radio astronomy to train\nneural networks that learn useful representations from radio images. This study\nexplores the application of SSL to radio astronomy, comparing the performance\nof SSL-trained models with that of traditional models pretrained on natural\nimages, evaluating the importance of data curation for SSL, and assessing the\npotential benefits of self-supervision to different domain-specific radio\nastronomy datasets.\n  Our results indicate that, SSL-trained models achieve significant\nimprovements over the baseline in several downstream tasks, especially in the\nlinear evaluation setting; when the entire backbone is fine-tuned, the benefits\nof SSL are less evident but still outperform pretraining. These findings\nsuggest that SSL can play a valuable role in efficiently enhancing the analysis\nof radio astronomical data. The trained models and code is available at:\n\\url{https://github.com/dr4thmos/solo-learn-radio}\n", "link": "http://arxiv.org/abs/2411.14078v1", "date": "2024-11-21", "relevancy": 2.4143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5168}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20for%20radio-astronomy%20source%20classification%3A%20a%0A%20%20benchmark&body=Title%3A%20Self-supervised%20learning%20for%20radio-astronomy%20source%20classification%3A%20a%0A%20%20benchmark%0AAuthor%3A%20Thomas%20Cecconello%20and%20Simone%20Riggi%20and%20Ugo%20Becciano%20and%20Fabio%20Vitello%20and%20Andrew%20M.%20Hopkins%20and%20Giuseppe%20Vizzari%20and%20Concetto%20Spampinato%20and%20Simone%20Palazzo%0AAbstract%3A%20%20%20The%20upcoming%20Square%20Kilometer%20Array%20%28SKA%29%20telescope%20marks%20a%20significant%20step%0Aforward%20in%20radio%20astronomy%2C%20presenting%20new%20opportunities%20and%20challenges%20for%0Adata%20analysis.%20Traditional%20visual%20models%20pretrained%20on%20optical%20photography%0Aimages%20may%20not%20perform%20optimally%20on%20radio%20interferometry%20images%2C%20which%20have%0Adistinct%20visual%20characteristics.%0A%20%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20promising%20approach%20to%20address%20this%0Aissue%2C%20leveraging%20the%20abundant%20unlabeled%20data%20in%20radio%20astronomy%20to%20train%0Aneural%20networks%20that%20learn%20useful%20representations%20from%20radio%20images.%20This%20study%0Aexplores%20the%20application%20of%20SSL%20to%20radio%20astronomy%2C%20comparing%20the%20performance%0Aof%20SSL-trained%20models%20with%20that%20of%20traditional%20models%20pretrained%20on%20natural%0Aimages%2C%20evaluating%20the%20importance%20of%20data%20curation%20for%20SSL%2C%20and%20assessing%20the%0Apotential%20benefits%20of%20self-supervision%20to%20different%20domain-specific%20radio%0Aastronomy%20datasets.%0A%20%20Our%20results%20indicate%20that%2C%20SSL-trained%20models%20achieve%20significant%0Aimprovements%20over%20the%20baseline%20in%20several%20downstream%20tasks%2C%20especially%20in%20the%0Alinear%20evaluation%20setting%3B%20when%20the%20entire%20backbone%20is%20fine-tuned%2C%20the%20benefits%0Aof%20SSL%20are%20less%20evident%20but%20still%20outperform%20pretraining.%20These%20findings%0Asuggest%20that%20SSL%20can%20play%20a%20valuable%20role%20in%20efficiently%20enhancing%20the%20analysis%0Aof%20radio%20astronomical%20data.%20The%20trained%20models%20and%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/dr4thmos/solo-learn-radio%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520learning%2520for%2520radio-astronomy%2520source%2520classification%253A%2520a%250A%2520%2520benchmark%26entry.906535625%3DThomas%2520Cecconello%2520and%2520Simone%2520Riggi%2520and%2520Ugo%2520Becciano%2520and%2520Fabio%2520Vitello%2520and%2520Andrew%2520M.%2520Hopkins%2520and%2520Giuseppe%2520Vizzari%2520and%2520Concetto%2520Spampinato%2520and%2520Simone%2520Palazzo%26entry.1292438233%3D%2520%2520The%2520upcoming%2520Square%2520Kilometer%2520Array%2520%2528SKA%2529%2520telescope%2520marks%2520a%2520significant%2520step%250Aforward%2520in%2520radio%2520astronomy%252C%2520presenting%2520new%2520opportunities%2520and%2520challenges%2520for%250Adata%2520analysis.%2520Traditional%2520visual%2520models%2520pretrained%2520on%2520optical%2520photography%250Aimages%2520may%2520not%2520perform%2520optimally%2520on%2520radio%2520interferometry%2520images%252C%2520which%2520have%250Adistinct%2520visual%2520characteristics.%250A%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520offers%2520a%2520promising%2520approach%2520to%2520address%2520this%250Aissue%252C%2520leveraging%2520the%2520abundant%2520unlabeled%2520data%2520in%2520radio%2520astronomy%2520to%2520train%250Aneural%2520networks%2520that%2520learn%2520useful%2520representations%2520from%2520radio%2520images.%2520This%2520study%250Aexplores%2520the%2520application%2520of%2520SSL%2520to%2520radio%2520astronomy%252C%2520comparing%2520the%2520performance%250Aof%2520SSL-trained%2520models%2520with%2520that%2520of%2520traditional%2520models%2520pretrained%2520on%2520natural%250Aimages%252C%2520evaluating%2520the%2520importance%2520of%2520data%2520curation%2520for%2520SSL%252C%2520and%2520assessing%2520the%250Apotential%2520benefits%2520of%2520self-supervision%2520to%2520different%2520domain-specific%2520radio%250Aastronomy%2520datasets.%250A%2520%2520Our%2520results%2520indicate%2520that%252C%2520SSL-trained%2520models%2520achieve%2520significant%250Aimprovements%2520over%2520the%2520baseline%2520in%2520several%2520downstream%2520tasks%252C%2520especially%2520in%2520the%250Alinear%2520evaluation%2520setting%253B%2520when%2520the%2520entire%2520backbone%2520is%2520fine-tuned%252C%2520the%2520benefits%250Aof%2520SSL%2520are%2520less%2520evident%2520but%2520still%2520outperform%2520pretraining.%2520These%2520findings%250Asuggest%2520that%2520SSL%2520can%2520play%2520a%2520valuable%2520role%2520in%2520efficiently%2520enhancing%2520the%2520analysis%250Aof%2520radio%2520astronomical%2520data.%2520The%2520trained%2520models%2520and%2520code%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/dr4thmos/solo-learn-radio%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20for%20radio-astronomy%20source%20classification%3A%20a%0A%20%20benchmark&entry.906535625=Thomas%20Cecconello%20and%20Simone%20Riggi%20and%20Ugo%20Becciano%20and%20Fabio%20Vitello%20and%20Andrew%20M.%20Hopkins%20and%20Giuseppe%20Vizzari%20and%20Concetto%20Spampinato%20and%20Simone%20Palazzo&entry.1292438233=%20%20The%20upcoming%20Square%20Kilometer%20Array%20%28SKA%29%20telescope%20marks%20a%20significant%20step%0Aforward%20in%20radio%20astronomy%2C%20presenting%20new%20opportunities%20and%20challenges%20for%0Adata%20analysis.%20Traditional%20visual%20models%20pretrained%20on%20optical%20photography%0Aimages%20may%20not%20perform%20optimally%20on%20radio%20interferometry%20images%2C%20which%20have%0Adistinct%20visual%20characteristics.%0A%20%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20promising%20approach%20to%20address%20this%0Aissue%2C%20leveraging%20the%20abundant%20unlabeled%20data%20in%20radio%20astronomy%20to%20train%0Aneural%20networks%20that%20learn%20useful%20representations%20from%20radio%20images.%20This%20study%0Aexplores%20the%20application%20of%20SSL%20to%20radio%20astronomy%2C%20comparing%20the%20performance%0Aof%20SSL-trained%20models%20with%20that%20of%20traditional%20models%20pretrained%20on%20natural%0Aimages%2C%20evaluating%20the%20importance%20of%20data%20curation%20for%20SSL%2C%20and%20assessing%20the%0Apotential%20benefits%20of%20self-supervision%20to%20different%20domain-specific%20radio%0Aastronomy%20datasets.%0A%20%20Our%20results%20indicate%20that%2C%20SSL-trained%20models%20achieve%20significant%0Aimprovements%20over%20the%20baseline%20in%20several%20downstream%20tasks%2C%20especially%20in%20the%0Alinear%20evaluation%20setting%3B%20when%20the%20entire%20backbone%20is%20fine-tuned%2C%20the%20benefits%0Aof%20SSL%20are%20less%20evident%20but%20still%20outperform%20pretraining.%20These%20findings%0Asuggest%20that%20SSL%20can%20play%20a%20valuable%20role%20in%20efficiently%20enhancing%20the%20analysis%0Aof%20radio%20astronomical%20data.%20The%20trained%20models%20and%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/dr4thmos/solo-learn-radio%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14078v1&entry.124074799=Read"},
{"title": "MVMS-RCN: A Dual-Domain Unfolding CT Reconstruction with\n  Multi-sparse-view and Multi-scale Refinement-correction", "author": "Xiaohong Fan and Ke Chen and Huaming Yi and Yin Yang and Jianping Zhang", "abstract": "  X-ray Computed Tomography (CT) is one of the most important diagnostic\nimaging techniques in clinical applications. Sparse-view CT imaging reduces the\nnumber of projection views to a lower radiation dose and alleviates the\npotential risk of radiation exposure. Most existing deep learning (DL) and deep\nunfolding sparse-view CT reconstruction methods: 1) do not fully use the\nprojection data; 2) do not always link their architecture designs to a\nmathematical theory; 3) do not flexibly deal with multi-sparse-view\nreconstruction assignments. This paper aims to use mathematical ideas and\ndesign optimal DL imaging algorithms for sparse-view tomography\nreconstructions. We propose a novel dual-domain deep unfolding unified\nframework that offers a great deal of flexibility for multi-sparse-view CT\nreconstruction with different sampling views through a single model. This\nframework combines the theoretical advantages of model-based methods with the\nsuperior reconstruction performance of DL-based methods, resulting in the\nexpected generalizability of DL. We propose a refinement module that utilizes\nunfolding projection domain to refine full-sparse-view projection errors, as\nwell as an image domain correction module that distills multi-scale geometric\nerror corrections to reconstruct sparse-view CT. This provides us with a new\nway to explore the potential of projection information and a new perspective on\ndesigning network architectures. All parameters of our proposed framework are\nlearnable end to end, and our method possesses the potential to be applied to\nplug-and-play reconstruction. Extensive experiments demonstrate that our\nframework is superior to other existing state-of-the-art methods. Our source\ncodes are available at https://github.com/fanxiaohong/MVMS-RCN.\n", "link": "http://arxiv.org/abs/2405.17141v2", "date": "2024-11-21", "relevancy": 2.4131, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6135}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6135}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction&body=Title%3A%20MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction%0AAuthor%3A%20Xiaohong%20Fan%20and%20Ke%20Chen%20and%20Huaming%20Yi%20and%20Yin%20Yang%20and%20Jianping%20Zhang%0AAbstract%3A%20%20%20X-ray%20Computed%20Tomography%20%28CT%29%20is%20one%20of%20the%20most%20important%20diagnostic%0Aimaging%20techniques%20in%20clinical%20applications.%20Sparse-view%20CT%20imaging%20reduces%20the%0Anumber%20of%20projection%20views%20to%20a%20lower%20radiation%20dose%20and%20alleviates%20the%0Apotential%20risk%20of%20radiation%20exposure.%20Most%20existing%20deep%20learning%20%28DL%29%20and%20deep%0Aunfolding%20sparse-view%20CT%20reconstruction%20methods%3A%201%29%20do%20not%20fully%20use%20the%0Aprojection%20data%3B%202%29%20do%20not%20always%20link%20their%20architecture%20designs%20to%20a%0Amathematical%20theory%3B%203%29%20do%20not%20flexibly%20deal%20with%20multi-sparse-view%0Areconstruction%20assignments.%20This%20paper%20aims%20to%20use%20mathematical%20ideas%20and%0Adesign%20optimal%20DL%20imaging%20algorithms%20for%20sparse-view%20tomography%0Areconstructions.%20We%20propose%20a%20novel%20dual-domain%20deep%20unfolding%20unified%0Aframework%20that%20offers%20a%20great%20deal%20of%20flexibility%20for%20multi-sparse-view%20CT%0Areconstruction%20with%20different%20sampling%20views%20through%20a%20single%20model.%20This%0Aframework%20combines%20the%20theoretical%20advantages%20of%20model-based%20methods%20with%20the%0Asuperior%20reconstruction%20performance%20of%20DL-based%20methods%2C%20resulting%20in%20the%0Aexpected%20generalizability%20of%20DL.%20We%20propose%20a%20refinement%20module%20that%20utilizes%0Aunfolding%20projection%20domain%20to%20refine%20full-sparse-view%20projection%20errors%2C%20as%0Awell%20as%20an%20image%20domain%20correction%20module%20that%20distills%20multi-scale%20geometric%0Aerror%20corrections%20to%20reconstruct%20sparse-view%20CT.%20This%20provides%20us%20with%20a%20new%0Away%20to%20explore%20the%20potential%20of%20projection%20information%20and%20a%20new%20perspective%20on%0Adesigning%20network%20architectures.%20All%20parameters%20of%20our%20proposed%20framework%20are%0Alearnable%20end%20to%20end%2C%20and%20our%20method%20possesses%20the%20potential%20to%20be%20applied%20to%0Aplug-and-play%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20is%20superior%20to%20other%20existing%20state-of-the-art%20methods.%20Our%20source%0Acodes%20are%20available%20at%20https%3A//github.com/fanxiaohong/MVMS-RCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVMS-RCN%253A%2520A%2520Dual-Domain%2520Unfolding%2520CT%2520Reconstruction%2520with%250A%2520%2520Multi-sparse-view%2520and%2520Multi-scale%2520Refinement-correction%26entry.906535625%3DXiaohong%2520Fan%2520and%2520Ke%2520Chen%2520and%2520Huaming%2520Yi%2520and%2520Yin%2520Yang%2520and%2520Jianping%2520Zhang%26entry.1292438233%3D%2520%2520X-ray%2520Computed%2520Tomography%2520%2528CT%2529%2520is%2520one%2520of%2520the%2520most%2520important%2520diagnostic%250Aimaging%2520techniques%2520in%2520clinical%2520applications.%2520Sparse-view%2520CT%2520imaging%2520reduces%2520the%250Anumber%2520of%2520projection%2520views%2520to%2520a%2520lower%2520radiation%2520dose%2520and%2520alleviates%2520the%250Apotential%2520risk%2520of%2520radiation%2520exposure.%2520Most%2520existing%2520deep%2520learning%2520%2528DL%2529%2520and%2520deep%250Aunfolding%2520sparse-view%2520CT%2520reconstruction%2520methods%253A%25201%2529%2520do%2520not%2520fully%2520use%2520the%250Aprojection%2520data%253B%25202%2529%2520do%2520not%2520always%2520link%2520their%2520architecture%2520designs%2520to%2520a%250Amathematical%2520theory%253B%25203%2529%2520do%2520not%2520flexibly%2520deal%2520with%2520multi-sparse-view%250Areconstruction%2520assignments.%2520This%2520paper%2520aims%2520to%2520use%2520mathematical%2520ideas%2520and%250Adesign%2520optimal%2520DL%2520imaging%2520algorithms%2520for%2520sparse-view%2520tomography%250Areconstructions.%2520We%2520propose%2520a%2520novel%2520dual-domain%2520deep%2520unfolding%2520unified%250Aframework%2520that%2520offers%2520a%2520great%2520deal%2520of%2520flexibility%2520for%2520multi-sparse-view%2520CT%250Areconstruction%2520with%2520different%2520sampling%2520views%2520through%2520a%2520single%2520model.%2520This%250Aframework%2520combines%2520the%2520theoretical%2520advantages%2520of%2520model-based%2520methods%2520with%2520the%250Asuperior%2520reconstruction%2520performance%2520of%2520DL-based%2520methods%252C%2520resulting%2520in%2520the%250Aexpected%2520generalizability%2520of%2520DL.%2520We%2520propose%2520a%2520refinement%2520module%2520that%2520utilizes%250Aunfolding%2520projection%2520domain%2520to%2520refine%2520full-sparse-view%2520projection%2520errors%252C%2520as%250Awell%2520as%2520an%2520image%2520domain%2520correction%2520module%2520that%2520distills%2520multi-scale%2520geometric%250Aerror%2520corrections%2520to%2520reconstruct%2520sparse-view%2520CT.%2520This%2520provides%2520us%2520with%2520a%2520new%250Away%2520to%2520explore%2520the%2520potential%2520of%2520projection%2520information%2520and%2520a%2520new%2520perspective%2520on%250Adesigning%2520network%2520architectures.%2520All%2520parameters%2520of%2520our%2520proposed%2520framework%2520are%250Alearnable%2520end%2520to%2520end%252C%2520and%2520our%2520method%2520possesses%2520the%2520potential%2520to%2520be%2520applied%2520to%250Aplug-and-play%2520reconstruction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aframework%2520is%2520superior%2520to%2520other%2520existing%2520state-of-the-art%2520methods.%2520Our%2520source%250Acodes%2520are%2520available%2520at%2520https%253A//github.com/fanxiaohong/MVMS-RCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction&entry.906535625=Xiaohong%20Fan%20and%20Ke%20Chen%20and%20Huaming%20Yi%20and%20Yin%20Yang%20and%20Jianping%20Zhang&entry.1292438233=%20%20X-ray%20Computed%20Tomography%20%28CT%29%20is%20one%20of%20the%20most%20important%20diagnostic%0Aimaging%20techniques%20in%20clinical%20applications.%20Sparse-view%20CT%20imaging%20reduces%20the%0Anumber%20of%20projection%20views%20to%20a%20lower%20radiation%20dose%20and%20alleviates%20the%0Apotential%20risk%20of%20radiation%20exposure.%20Most%20existing%20deep%20learning%20%28DL%29%20and%20deep%0Aunfolding%20sparse-view%20CT%20reconstruction%20methods%3A%201%29%20do%20not%20fully%20use%20the%0Aprojection%20data%3B%202%29%20do%20not%20always%20link%20their%20architecture%20designs%20to%20a%0Amathematical%20theory%3B%203%29%20do%20not%20flexibly%20deal%20with%20multi-sparse-view%0Areconstruction%20assignments.%20This%20paper%20aims%20to%20use%20mathematical%20ideas%20and%0Adesign%20optimal%20DL%20imaging%20algorithms%20for%20sparse-view%20tomography%0Areconstructions.%20We%20propose%20a%20novel%20dual-domain%20deep%20unfolding%20unified%0Aframework%20that%20offers%20a%20great%20deal%20of%20flexibility%20for%20multi-sparse-view%20CT%0Areconstruction%20with%20different%20sampling%20views%20through%20a%20single%20model.%20This%0Aframework%20combines%20the%20theoretical%20advantages%20of%20model-based%20methods%20with%20the%0Asuperior%20reconstruction%20performance%20of%20DL-based%20methods%2C%20resulting%20in%20the%0Aexpected%20generalizability%20of%20DL.%20We%20propose%20a%20refinement%20module%20that%20utilizes%0Aunfolding%20projection%20domain%20to%20refine%20full-sparse-view%20projection%20errors%2C%20as%0Awell%20as%20an%20image%20domain%20correction%20module%20that%20distills%20multi-scale%20geometric%0Aerror%20corrections%20to%20reconstruct%20sparse-view%20CT.%20This%20provides%20us%20with%20a%20new%0Away%20to%20explore%20the%20potential%20of%20projection%20information%20and%20a%20new%20perspective%20on%0Adesigning%20network%20architectures.%20All%20parameters%20of%20our%20proposed%20framework%20are%0Alearnable%20end%20to%20end%2C%20and%20our%20method%20possesses%20the%20potential%20to%20be%20applied%20to%0Aplug-and-play%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20is%20superior%20to%20other%20existing%20state-of-the-art%20methods.%20Our%20source%0Acodes%20are%20available%20at%20https%3A//github.com/fanxiaohong/MVMS-RCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17141v2&entry.124074799=Read"},
{"title": "Accelerated zero-order SGD under high-order smoothness and\n  overparameterized regime", "author": "Georgii Bychkov and Darina Dvinskikh and Anastasia Antsiferova and Alexander Gasnikov and Aleksandr Lobanov", "abstract": "  We present a novel gradient-free algorithm to solve a convex stochastic\noptimization problem, such as those encountered in medicine, physics, and\nmachine learning (e.g., adversarial multi-armed bandit problem), where the\nobjective function can only be computed through numerical simulation, either as\nthe result of a real experiment or as feedback given by the function\nevaluations from an adversary. Thus we suppose that only a black-box access to\nthe function values of the objective is available, possibly corrupted by\nadversarial noise: deterministic or stochastic. The noisy setup can arise\nnaturally from modeling randomness within a simulation or by computer\ndiscretization, or when exact values of function are forbidden due to privacy\nissues, or when solving non-convex problems as convex ones with an inexact\nfunction oracle. By exploiting higher-order smoothness, fulfilled, e.g., in\nlogistic regression, we improve the performance of zero-order methods developed\nunder the assumption of classical smoothness (or having a Lipschitz gradient).\nThe proposed algorithm enjoys optimal oracle complexity and is designed under\nan overparameterization setup, i.e., when the number of model parameters is\nmuch larger than the size of the training dataset. Overparametrized models fit\nto the training data perfectly while also having good generalization and\noutperforming underparameterized models on unseen data. We provide convergence\nguarantees for the proposed algorithm under both types of noise. Moreover, we\nestimate the maximum permissible adversarial noise level that maintains the\ndesired accuracy in the Euclidean setup, and then we extend our results to a\nnon-Euclidean setup. Our theoretical results are verified on the logistic\nregression problem.\n", "link": "http://arxiv.org/abs/2411.13999v1", "date": "2024-11-21", "relevancy": 2.3985, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4826}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20zero-order%20SGD%20under%20high-order%20smoothness%20and%0A%20%20overparameterized%20regime&body=Title%3A%20Accelerated%20zero-order%20SGD%20under%20high-order%20smoothness%20and%0A%20%20overparameterized%20regime%0AAuthor%3A%20Georgii%20Bychkov%20and%20Darina%20Dvinskikh%20and%20Anastasia%20Antsiferova%20and%20Alexander%20Gasnikov%20and%20Aleksandr%20Lobanov%0AAbstract%3A%20%20%20We%20present%20a%20novel%20gradient-free%20algorithm%20to%20solve%20a%20convex%20stochastic%0Aoptimization%20problem%2C%20such%20as%20those%20encountered%20in%20medicine%2C%20physics%2C%20and%0Amachine%20learning%20%28e.g.%2C%20adversarial%20multi-armed%20bandit%20problem%29%2C%20where%20the%0Aobjective%20function%20can%20only%20be%20computed%20through%20numerical%20simulation%2C%20either%20as%0Athe%20result%20of%20a%20real%20experiment%20or%20as%20feedback%20given%20by%20the%20function%0Aevaluations%20from%20an%20adversary.%20Thus%20we%20suppose%20that%20only%20a%20black-box%20access%20to%0Athe%20function%20values%20of%20the%20objective%20is%20available%2C%20possibly%20corrupted%20by%0Aadversarial%20noise%3A%20deterministic%20or%20stochastic.%20The%20noisy%20setup%20can%20arise%0Anaturally%20from%20modeling%20randomness%20within%20a%20simulation%20or%20by%20computer%0Adiscretization%2C%20or%20when%20exact%20values%20of%20function%20are%20forbidden%20due%20to%20privacy%0Aissues%2C%20or%20when%20solving%20non-convex%20problems%20as%20convex%20ones%20with%20an%20inexact%0Afunction%20oracle.%20By%20exploiting%20higher-order%20smoothness%2C%20fulfilled%2C%20e.g.%2C%20in%0Alogistic%20regression%2C%20we%20improve%20the%20performance%20of%20zero-order%20methods%20developed%0Aunder%20the%20assumption%20of%20classical%20smoothness%20%28or%20having%20a%20Lipschitz%20gradient%29.%0AThe%20proposed%20algorithm%20enjoys%20optimal%20oracle%20complexity%20and%20is%20designed%20under%0Aan%20overparameterization%20setup%2C%20i.e.%2C%20when%20the%20number%20of%20model%20parameters%20is%0Amuch%20larger%20than%20the%20size%20of%20the%20training%20dataset.%20Overparametrized%20models%20fit%0Ato%20the%20training%20data%20perfectly%20while%20also%20having%20good%20generalization%20and%0Aoutperforming%20underparameterized%20models%20on%20unseen%20data.%20We%20provide%20convergence%0Aguarantees%20for%20the%20proposed%20algorithm%20under%20both%20types%20of%20noise.%20Moreover%2C%20we%0Aestimate%20the%20maximum%20permissible%20adversarial%20noise%20level%20that%20maintains%20the%0Adesired%20accuracy%20in%20the%20Euclidean%20setup%2C%20and%20then%20we%20extend%20our%20results%20to%20a%0Anon-Euclidean%20setup.%20Our%20theoretical%20results%20are%20verified%20on%20the%20logistic%0Aregression%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520zero-order%2520SGD%2520under%2520high-order%2520smoothness%2520and%250A%2520%2520overparameterized%2520regime%26entry.906535625%3DGeorgii%2520Bychkov%2520and%2520Darina%2520Dvinskikh%2520and%2520Anastasia%2520Antsiferova%2520and%2520Alexander%2520Gasnikov%2520and%2520Aleksandr%2520Lobanov%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520gradient-free%2520algorithm%2520to%2520solve%2520a%2520convex%2520stochastic%250Aoptimization%2520problem%252C%2520such%2520as%2520those%2520encountered%2520in%2520medicine%252C%2520physics%252C%2520and%250Amachine%2520learning%2520%2528e.g.%252C%2520adversarial%2520multi-armed%2520bandit%2520problem%2529%252C%2520where%2520the%250Aobjective%2520function%2520can%2520only%2520be%2520computed%2520through%2520numerical%2520simulation%252C%2520either%2520as%250Athe%2520result%2520of%2520a%2520real%2520experiment%2520or%2520as%2520feedback%2520given%2520by%2520the%2520function%250Aevaluations%2520from%2520an%2520adversary.%2520Thus%2520we%2520suppose%2520that%2520only%2520a%2520black-box%2520access%2520to%250Athe%2520function%2520values%2520of%2520the%2520objective%2520is%2520available%252C%2520possibly%2520corrupted%2520by%250Aadversarial%2520noise%253A%2520deterministic%2520or%2520stochastic.%2520The%2520noisy%2520setup%2520can%2520arise%250Anaturally%2520from%2520modeling%2520randomness%2520within%2520a%2520simulation%2520or%2520by%2520computer%250Adiscretization%252C%2520or%2520when%2520exact%2520values%2520of%2520function%2520are%2520forbidden%2520due%2520to%2520privacy%250Aissues%252C%2520or%2520when%2520solving%2520non-convex%2520problems%2520as%2520convex%2520ones%2520with%2520an%2520inexact%250Afunction%2520oracle.%2520By%2520exploiting%2520higher-order%2520smoothness%252C%2520fulfilled%252C%2520e.g.%252C%2520in%250Alogistic%2520regression%252C%2520we%2520improve%2520the%2520performance%2520of%2520zero-order%2520methods%2520developed%250Aunder%2520the%2520assumption%2520of%2520classical%2520smoothness%2520%2528or%2520having%2520a%2520Lipschitz%2520gradient%2529.%250AThe%2520proposed%2520algorithm%2520enjoys%2520optimal%2520oracle%2520complexity%2520and%2520is%2520designed%2520under%250Aan%2520overparameterization%2520setup%252C%2520i.e.%252C%2520when%2520the%2520number%2520of%2520model%2520parameters%2520is%250Amuch%2520larger%2520than%2520the%2520size%2520of%2520the%2520training%2520dataset.%2520Overparametrized%2520models%2520fit%250Ato%2520the%2520training%2520data%2520perfectly%2520while%2520also%2520having%2520good%2520generalization%2520and%250Aoutperforming%2520underparameterized%2520models%2520on%2520unseen%2520data.%2520We%2520provide%2520convergence%250Aguarantees%2520for%2520the%2520proposed%2520algorithm%2520under%2520both%2520types%2520of%2520noise.%2520Moreover%252C%2520we%250Aestimate%2520the%2520maximum%2520permissible%2520adversarial%2520noise%2520level%2520that%2520maintains%2520the%250Adesired%2520accuracy%2520in%2520the%2520Euclidean%2520setup%252C%2520and%2520then%2520we%2520extend%2520our%2520results%2520to%2520a%250Anon-Euclidean%2520setup.%2520Our%2520theoretical%2520results%2520are%2520verified%2520on%2520the%2520logistic%250Aregression%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20zero-order%20SGD%20under%20high-order%20smoothness%20and%0A%20%20overparameterized%20regime&entry.906535625=Georgii%20Bychkov%20and%20Darina%20Dvinskikh%20and%20Anastasia%20Antsiferova%20and%20Alexander%20Gasnikov%20and%20Aleksandr%20Lobanov&entry.1292438233=%20%20We%20present%20a%20novel%20gradient-free%20algorithm%20to%20solve%20a%20convex%20stochastic%0Aoptimization%20problem%2C%20such%20as%20those%20encountered%20in%20medicine%2C%20physics%2C%20and%0Amachine%20learning%20%28e.g.%2C%20adversarial%20multi-armed%20bandit%20problem%29%2C%20where%20the%0Aobjective%20function%20can%20only%20be%20computed%20through%20numerical%20simulation%2C%20either%20as%0Athe%20result%20of%20a%20real%20experiment%20or%20as%20feedback%20given%20by%20the%20function%0Aevaluations%20from%20an%20adversary.%20Thus%20we%20suppose%20that%20only%20a%20black-box%20access%20to%0Athe%20function%20values%20of%20the%20objective%20is%20available%2C%20possibly%20corrupted%20by%0Aadversarial%20noise%3A%20deterministic%20or%20stochastic.%20The%20noisy%20setup%20can%20arise%0Anaturally%20from%20modeling%20randomness%20within%20a%20simulation%20or%20by%20computer%0Adiscretization%2C%20or%20when%20exact%20values%20of%20function%20are%20forbidden%20due%20to%20privacy%0Aissues%2C%20or%20when%20solving%20non-convex%20problems%20as%20convex%20ones%20with%20an%20inexact%0Afunction%20oracle.%20By%20exploiting%20higher-order%20smoothness%2C%20fulfilled%2C%20e.g.%2C%20in%0Alogistic%20regression%2C%20we%20improve%20the%20performance%20of%20zero-order%20methods%20developed%0Aunder%20the%20assumption%20of%20classical%20smoothness%20%28or%20having%20a%20Lipschitz%20gradient%29.%0AThe%20proposed%20algorithm%20enjoys%20optimal%20oracle%20complexity%20and%20is%20designed%20under%0Aan%20overparameterization%20setup%2C%20i.e.%2C%20when%20the%20number%20of%20model%20parameters%20is%0Amuch%20larger%20than%20the%20size%20of%20the%20training%20dataset.%20Overparametrized%20models%20fit%0Ato%20the%20training%20data%20perfectly%20while%20also%20having%20good%20generalization%20and%0Aoutperforming%20underparameterized%20models%20on%20unseen%20data.%20We%20provide%20convergence%0Aguarantees%20for%20the%20proposed%20algorithm%20under%20both%20types%20of%20noise.%20Moreover%2C%20we%0Aestimate%20the%20maximum%20permissible%20adversarial%20noise%20level%20that%20maintains%20the%0Adesired%20accuracy%20in%20the%20Euclidean%20setup%2C%20and%20then%20we%20extend%20our%20results%20to%20a%0Anon-Euclidean%20setup.%20Our%20theoretical%20results%20are%20verified%20on%20the%20logistic%0Aregression%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13999v1&entry.124074799=Read"},
{"title": "Indiscriminate Disruption of Conditional Inference on Multivariate\n  Gaussians", "author": "William N. Caballero and Matthew LaRosa and Alexander Fisher and Vahid Tarokh", "abstract": "  The multivariate Gaussian distribution underpins myriad operations-research,\ndecision-analytic, and machine-learning models (e.g., Bayesian optimization,\nGaussian influence diagrams, and variational autoencoders). However, despite\nrecent advances in adversarial machine learning (AML), inference for Gaussian\nmodels in the presence of an adversary is notably understudied. Therefore, we\nconsider a self-interested attacker who wishes to disrupt a decisionmaker's\nconditional inference and subsequent actions by corrupting a set of evidentiary\nvariables. To avoid detection, the attacker also desires the attack to appear\nplausible wherein plausibility is determined by the density of the corrupted\nevidence. We consider white- and grey-box settings such that the attacker has\ncomplete and incomplete knowledge about the decisionmaker's underlying\nmultivariate Gaussian distribution, respectively. Select instances are shown to\nreduce to quadratic and stochastic quadratic programs, and structural\nproperties are derived to inform solution methods. We assess the impact and\nefficacy of these attacks in three examples, including, real estate evaluation,\ninterest rate estimation and signals processing. Each example leverages an\nalternative underlying model, thereby highlighting the attacks' broad\napplicability. Through these applications, we also juxtapose the behavior of\nthe white- and grey-box attacks to understand how uncertainty and structure\naffect attacker behavior.\n", "link": "http://arxiv.org/abs/2411.14351v1", "date": "2024-11-21", "relevancy": 2.392, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Indiscriminate%20Disruption%20of%20Conditional%20Inference%20on%20Multivariate%0A%20%20Gaussians&body=Title%3A%20Indiscriminate%20Disruption%20of%20Conditional%20Inference%20on%20Multivariate%0A%20%20Gaussians%0AAuthor%3A%20William%20N.%20Caballero%20and%20Matthew%20LaRosa%20and%20Alexander%20Fisher%20and%20Vahid%20Tarokh%0AAbstract%3A%20%20%20The%20multivariate%20Gaussian%20distribution%20underpins%20myriad%20operations-research%2C%0Adecision-analytic%2C%20and%20machine-learning%20models%20%28e.g.%2C%20Bayesian%20optimization%2C%0AGaussian%20influence%20diagrams%2C%20and%20variational%20autoencoders%29.%20However%2C%20despite%0Arecent%20advances%20in%20adversarial%20machine%20learning%20%28AML%29%2C%20inference%20for%20Gaussian%0Amodels%20in%20the%20presence%20of%20an%20adversary%20is%20notably%20understudied.%20Therefore%2C%20we%0Aconsider%20a%20self-interested%20attacker%20who%20wishes%20to%20disrupt%20a%20decisionmaker%27s%0Aconditional%20inference%20and%20subsequent%20actions%20by%20corrupting%20a%20set%20of%20evidentiary%0Avariables.%20To%20avoid%20detection%2C%20the%20attacker%20also%20desires%20the%20attack%20to%20appear%0Aplausible%20wherein%20plausibility%20is%20determined%20by%20the%20density%20of%20the%20corrupted%0Aevidence.%20We%20consider%20white-%20and%20grey-box%20settings%20such%20that%20the%20attacker%20has%0Acomplete%20and%20incomplete%20knowledge%20about%20the%20decisionmaker%27s%20underlying%0Amultivariate%20Gaussian%20distribution%2C%20respectively.%20Select%20instances%20are%20shown%20to%0Areduce%20to%20quadratic%20and%20stochastic%20quadratic%20programs%2C%20and%20structural%0Aproperties%20are%20derived%20to%20inform%20solution%20methods.%20We%20assess%20the%20impact%20and%0Aefficacy%20of%20these%20attacks%20in%20three%20examples%2C%20including%2C%20real%20estate%20evaluation%2C%0Ainterest%20rate%20estimation%20and%20signals%20processing.%20Each%20example%20leverages%20an%0Aalternative%20underlying%20model%2C%20thereby%20highlighting%20the%20attacks%27%20broad%0Aapplicability.%20Through%20these%20applications%2C%20we%20also%20juxtapose%20the%20behavior%20of%0Athe%20white-%20and%20grey-box%20attacks%20to%20understand%20how%20uncertainty%20and%20structure%0Aaffect%20attacker%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndiscriminate%2520Disruption%2520of%2520Conditional%2520Inference%2520on%2520Multivariate%250A%2520%2520Gaussians%26entry.906535625%3DWilliam%2520N.%2520Caballero%2520and%2520Matthew%2520LaRosa%2520and%2520Alexander%2520Fisher%2520and%2520Vahid%2520Tarokh%26entry.1292438233%3D%2520%2520The%2520multivariate%2520Gaussian%2520distribution%2520underpins%2520myriad%2520operations-research%252C%250Adecision-analytic%252C%2520and%2520machine-learning%2520models%2520%2528e.g.%252C%2520Bayesian%2520optimization%252C%250AGaussian%2520influence%2520diagrams%252C%2520and%2520variational%2520autoencoders%2529.%2520However%252C%2520despite%250Arecent%2520advances%2520in%2520adversarial%2520machine%2520learning%2520%2528AML%2529%252C%2520inference%2520for%2520Gaussian%250Amodels%2520in%2520the%2520presence%2520of%2520an%2520adversary%2520is%2520notably%2520understudied.%2520Therefore%252C%2520we%250Aconsider%2520a%2520self-interested%2520attacker%2520who%2520wishes%2520to%2520disrupt%2520a%2520decisionmaker%2527s%250Aconditional%2520inference%2520and%2520subsequent%2520actions%2520by%2520corrupting%2520a%2520set%2520of%2520evidentiary%250Avariables.%2520To%2520avoid%2520detection%252C%2520the%2520attacker%2520also%2520desires%2520the%2520attack%2520to%2520appear%250Aplausible%2520wherein%2520plausibility%2520is%2520determined%2520by%2520the%2520density%2520of%2520the%2520corrupted%250Aevidence.%2520We%2520consider%2520white-%2520and%2520grey-box%2520settings%2520such%2520that%2520the%2520attacker%2520has%250Acomplete%2520and%2520incomplete%2520knowledge%2520about%2520the%2520decisionmaker%2527s%2520underlying%250Amultivariate%2520Gaussian%2520distribution%252C%2520respectively.%2520Select%2520instances%2520are%2520shown%2520to%250Areduce%2520to%2520quadratic%2520and%2520stochastic%2520quadratic%2520programs%252C%2520and%2520structural%250Aproperties%2520are%2520derived%2520to%2520inform%2520solution%2520methods.%2520We%2520assess%2520the%2520impact%2520and%250Aefficacy%2520of%2520these%2520attacks%2520in%2520three%2520examples%252C%2520including%252C%2520real%2520estate%2520evaluation%252C%250Ainterest%2520rate%2520estimation%2520and%2520signals%2520processing.%2520Each%2520example%2520leverages%2520an%250Aalternative%2520underlying%2520model%252C%2520thereby%2520highlighting%2520the%2520attacks%2527%2520broad%250Aapplicability.%2520Through%2520these%2520applications%252C%2520we%2520also%2520juxtapose%2520the%2520behavior%2520of%250Athe%2520white-%2520and%2520grey-box%2520attacks%2520to%2520understand%2520how%2520uncertainty%2520and%2520structure%250Aaffect%2520attacker%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Indiscriminate%20Disruption%20of%20Conditional%20Inference%20on%20Multivariate%0A%20%20Gaussians&entry.906535625=William%20N.%20Caballero%20and%20Matthew%20LaRosa%20and%20Alexander%20Fisher%20and%20Vahid%20Tarokh&entry.1292438233=%20%20The%20multivariate%20Gaussian%20distribution%20underpins%20myriad%20operations-research%2C%0Adecision-analytic%2C%20and%20machine-learning%20models%20%28e.g.%2C%20Bayesian%20optimization%2C%0AGaussian%20influence%20diagrams%2C%20and%20variational%20autoencoders%29.%20However%2C%20despite%0Arecent%20advances%20in%20adversarial%20machine%20learning%20%28AML%29%2C%20inference%20for%20Gaussian%0Amodels%20in%20the%20presence%20of%20an%20adversary%20is%20notably%20understudied.%20Therefore%2C%20we%0Aconsider%20a%20self-interested%20attacker%20who%20wishes%20to%20disrupt%20a%20decisionmaker%27s%0Aconditional%20inference%20and%20subsequent%20actions%20by%20corrupting%20a%20set%20of%20evidentiary%0Avariables.%20To%20avoid%20detection%2C%20the%20attacker%20also%20desires%20the%20attack%20to%20appear%0Aplausible%20wherein%20plausibility%20is%20determined%20by%20the%20density%20of%20the%20corrupted%0Aevidence.%20We%20consider%20white-%20and%20grey-box%20settings%20such%20that%20the%20attacker%20has%0Acomplete%20and%20incomplete%20knowledge%20about%20the%20decisionmaker%27s%20underlying%0Amultivariate%20Gaussian%20distribution%2C%20respectively.%20Select%20instances%20are%20shown%20to%0Areduce%20to%20quadratic%20and%20stochastic%20quadratic%20programs%2C%20and%20structural%0Aproperties%20are%20derived%20to%20inform%20solution%20methods.%20We%20assess%20the%20impact%20and%0Aefficacy%20of%20these%20attacks%20in%20three%20examples%2C%20including%2C%20real%20estate%20evaluation%2C%0Ainterest%20rate%20estimation%20and%20signals%20processing.%20Each%20example%20leverages%20an%0Aalternative%20underlying%20model%2C%20thereby%20highlighting%20the%20attacks%27%20broad%0Aapplicability.%20Through%20these%20applications%2C%20we%20also%20juxtapose%20the%20behavior%20of%0Athe%20white-%20and%20grey-box%20attacks%20to%20understand%20how%20uncertainty%20and%20structure%0Aaffect%20attacker%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14351v1&entry.124074799=Read"},
{"title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts", "author": "Zhuohan Gu and Jiayi Yao and Kuntai Du and Junchen Jiang", "abstract": "  As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.\n", "link": "http://arxiv.org/abs/2411.13009v2", "date": "2024-11-21", "relevancy": 2.3863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMSteer%3A%20Improving%20Long-Context%20LLM%20Inference%20by%20Steering%20Attention%20on%0A%20%20Reused%20Contexts&body=Title%3A%20LLMSteer%3A%20Improving%20Long-Context%20LLM%20Inference%20by%20Steering%20Attention%20on%0A%20%20Reused%20Contexts%0AAuthor%3A%20Zhuohan%20Gu%20and%20Jiayi%20Yao%20and%20Kuntai%20Du%20and%20Junchen%20Jiang%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20show%20impressive%20performance%20on%20complex%20tasks%2C%0Athey%20still%20struggle%20with%20longer%20contextual%20understanding%20and%20high%20computational%0Acosts.%20To%20balance%20efficiency%20and%20quality%2C%20we%20introduce%20LLMSteer%2C%20a%0Afine-tuning-free%20framework%20that%20enhances%20LLMs%20through%20query-independent%0Aattention%20steering.%20Tested%20on%20popular%20LLMs%20and%20datasets%2C%20LLMSteer%20narrows%20the%0Aperformance%20gap%20with%20baselines%20by%2065.9%25%20and%20reduces%20the%20runtime%20delay%20by%20up%20to%0A4.8x%20compared%20to%20recent%20attention%20steering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMSteer%253A%2520Improving%2520Long-Context%2520LLM%2520Inference%2520by%2520Steering%2520Attention%2520on%250A%2520%2520Reused%2520Contexts%26entry.906535625%3DZhuohan%2520Gu%2520and%2520Jiayi%2520Yao%2520and%2520Kuntai%2520Du%2520and%2520Junchen%2520Jiang%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520impressive%2520performance%2520on%2520complex%2520tasks%252C%250Athey%2520still%2520struggle%2520with%2520longer%2520contextual%2520understanding%2520and%2520high%2520computational%250Acosts.%2520To%2520balance%2520efficiency%2520and%2520quality%252C%2520we%2520introduce%2520LLMSteer%252C%2520a%250Afine-tuning-free%2520framework%2520that%2520enhances%2520LLMs%2520through%2520query-independent%250Aattention%2520steering.%2520Tested%2520on%2520popular%2520LLMs%2520and%2520datasets%252C%2520LLMSteer%2520narrows%2520the%250Aperformance%2520gap%2520with%2520baselines%2520by%252065.9%2525%2520and%2520reduces%2520the%2520runtime%2520delay%2520by%2520up%2520to%250A4.8x%2520compared%2520to%2520recent%2520attention%2520steering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMSteer%3A%20Improving%20Long-Context%20LLM%20Inference%20by%20Steering%20Attention%20on%0A%20%20Reused%20Contexts&entry.906535625=Zhuohan%20Gu%20and%20Jiayi%20Yao%20and%20Kuntai%20Du%20and%20Junchen%20Jiang&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20show%20impressive%20performance%20on%20complex%20tasks%2C%0Athey%20still%20struggle%20with%20longer%20contextual%20understanding%20and%20high%20computational%0Acosts.%20To%20balance%20efficiency%20and%20quality%2C%20we%20introduce%20LLMSteer%2C%20a%0Afine-tuning-free%20framework%20that%20enhances%20LLMs%20through%20query-independent%0Aattention%20steering.%20Tested%20on%20popular%20LLMs%20and%20datasets%2C%20LLMSteer%20narrows%20the%0Aperformance%20gap%20with%20baselines%20by%2065.9%25%20and%20reduces%20the%20runtime%20delay%20by%20up%20to%0A4.8x%20compared%20to%20recent%20attention%20steering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13009v2&entry.124074799=Read"},
{"title": "Learning Humanoid Locomotion with Perceptive Internal Model", "author": "Junfeng Long and Junli Ren and Moji Shi and Zirui Wang and Tao Huang and Ping Luo and Jiangmiao Pang", "abstract": "  In contrast to quadruped robots that can navigate diverse terrains using a\n\"blind\" policy, humanoid robots require accurate perception for stable\nlocomotion due to their high degrees of freedom and inherently unstable\nmorphology. However, incorporating perceptual signals often introduces\nadditional disturbances to the system, potentially reducing its robustness,\ngeneralizability, and efficiency. This paper presents the Perceptive Internal\nModel (PIM), which relies on onboard, continuously updated elevation maps\ncentered around the robot to perceive its surroundings. We train the policy\nusing ground-truth obstacle heights surrounding the robot in simulation,\noptimizing it based on the Hybrid Internal Model (HIM), and perform inference\nwith heights sampled from the constructed elevation map. Unlike previous\nmethods that directly encode depth maps or raw point clouds, our approach\nallows the robot to perceive the terrain beneath its feet clearly and is less\naffected by camera movement or noise. Furthermore, since depth map rendering is\nnot required in simulation, our method introduces minimal additional\ncomputational costs and can train the policy in 3 hours on an RTX 4090 GPU. We\nverify the effectiveness of our method across various humanoid robots, various\nindoor and outdoor terrains, stairs, and various sensor configurations. Our\nmethod can enable a humanoid robot to continuously climb stairs and has the\npotential to serve as a foundational algorithm for the development of future\nhumanoid control methods.\n", "link": "http://arxiv.org/abs/2411.14386v1", "date": "2024-11-21", "relevancy": 2.3838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5872}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Humanoid%20Locomotion%20with%20Perceptive%20Internal%20Model&body=Title%3A%20Learning%20Humanoid%20Locomotion%20with%20Perceptive%20Internal%20Model%0AAuthor%3A%20Junfeng%20Long%20and%20Junli%20Ren%20and%20Moji%20Shi%20and%20Zirui%20Wang%20and%20Tao%20Huang%20and%20Ping%20Luo%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20In%20contrast%20to%20quadruped%20robots%20that%20can%20navigate%20diverse%20terrains%20using%20a%0A%22blind%22%20policy%2C%20humanoid%20robots%20require%20accurate%20perception%20for%20stable%0Alocomotion%20due%20to%20their%20high%20degrees%20of%20freedom%20and%20inherently%20unstable%0Amorphology.%20However%2C%20incorporating%20perceptual%20signals%20often%20introduces%0Aadditional%20disturbances%20to%20the%20system%2C%20potentially%20reducing%20its%20robustness%2C%0Ageneralizability%2C%20and%20efficiency.%20This%20paper%20presents%20the%20Perceptive%20Internal%0AModel%20%28PIM%29%2C%20which%20relies%20on%20onboard%2C%20continuously%20updated%20elevation%20maps%0Acentered%20around%20the%20robot%20to%20perceive%20its%20surroundings.%20We%20train%20the%20policy%0Ausing%20ground-truth%20obstacle%20heights%20surrounding%20the%20robot%20in%20simulation%2C%0Aoptimizing%20it%20based%20on%20the%20Hybrid%20Internal%20Model%20%28HIM%29%2C%20and%20perform%20inference%0Awith%20heights%20sampled%20from%20the%20constructed%20elevation%20map.%20Unlike%20previous%0Amethods%20that%20directly%20encode%20depth%20maps%20or%20raw%20point%20clouds%2C%20our%20approach%0Aallows%20the%20robot%20to%20perceive%20the%20terrain%20beneath%20its%20feet%20clearly%20and%20is%20less%0Aaffected%20by%20camera%20movement%20or%20noise.%20Furthermore%2C%20since%20depth%20map%20rendering%20is%0Anot%20required%20in%20simulation%2C%20our%20method%20introduces%20minimal%20additional%0Acomputational%20costs%20and%20can%20train%20the%20policy%20in%203%20hours%20on%20an%20RTX%204090%20GPU.%20We%0Averify%20the%20effectiveness%20of%20our%20method%20across%20various%20humanoid%20robots%2C%20various%0Aindoor%20and%20outdoor%20terrains%2C%20stairs%2C%20and%20various%20sensor%20configurations.%20Our%0Amethod%20can%20enable%20a%20humanoid%20robot%20to%20continuously%20climb%20stairs%20and%20has%20the%0Apotential%20to%20serve%20as%20a%20foundational%20algorithm%20for%20the%20development%20of%20future%0Ahumanoid%20control%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Humanoid%2520Locomotion%2520with%2520Perceptive%2520Internal%2520Model%26entry.906535625%3DJunfeng%2520Long%2520and%2520Junli%2520Ren%2520and%2520Moji%2520Shi%2520and%2520Zirui%2520Wang%2520and%2520Tao%2520Huang%2520and%2520Ping%2520Luo%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520quadruped%2520robots%2520that%2520can%2520navigate%2520diverse%2520terrains%2520using%2520a%250A%2522blind%2522%2520policy%252C%2520humanoid%2520robots%2520require%2520accurate%2520perception%2520for%2520stable%250Alocomotion%2520due%2520to%2520their%2520high%2520degrees%2520of%2520freedom%2520and%2520inherently%2520unstable%250Amorphology.%2520However%252C%2520incorporating%2520perceptual%2520signals%2520often%2520introduces%250Aadditional%2520disturbances%2520to%2520the%2520system%252C%2520potentially%2520reducing%2520its%2520robustness%252C%250Ageneralizability%252C%2520and%2520efficiency.%2520This%2520paper%2520presents%2520the%2520Perceptive%2520Internal%250AModel%2520%2528PIM%2529%252C%2520which%2520relies%2520on%2520onboard%252C%2520continuously%2520updated%2520elevation%2520maps%250Acentered%2520around%2520the%2520robot%2520to%2520perceive%2520its%2520surroundings.%2520We%2520train%2520the%2520policy%250Ausing%2520ground-truth%2520obstacle%2520heights%2520surrounding%2520the%2520robot%2520in%2520simulation%252C%250Aoptimizing%2520it%2520based%2520on%2520the%2520Hybrid%2520Internal%2520Model%2520%2528HIM%2529%252C%2520and%2520perform%2520inference%250Awith%2520heights%2520sampled%2520from%2520the%2520constructed%2520elevation%2520map.%2520Unlike%2520previous%250Amethods%2520that%2520directly%2520encode%2520depth%2520maps%2520or%2520raw%2520point%2520clouds%252C%2520our%2520approach%250Aallows%2520the%2520robot%2520to%2520perceive%2520the%2520terrain%2520beneath%2520its%2520feet%2520clearly%2520and%2520is%2520less%250Aaffected%2520by%2520camera%2520movement%2520or%2520noise.%2520Furthermore%252C%2520since%2520depth%2520map%2520rendering%2520is%250Anot%2520required%2520in%2520simulation%252C%2520our%2520method%2520introduces%2520minimal%2520additional%250Acomputational%2520costs%2520and%2520can%2520train%2520the%2520policy%2520in%25203%2520hours%2520on%2520an%2520RTX%25204090%2520GPU.%2520We%250Averify%2520the%2520effectiveness%2520of%2520our%2520method%2520across%2520various%2520humanoid%2520robots%252C%2520various%250Aindoor%2520and%2520outdoor%2520terrains%252C%2520stairs%252C%2520and%2520various%2520sensor%2520configurations.%2520Our%250Amethod%2520can%2520enable%2520a%2520humanoid%2520robot%2520to%2520continuously%2520climb%2520stairs%2520and%2520has%2520the%250Apotential%2520to%2520serve%2520as%2520a%2520foundational%2520algorithm%2520for%2520the%2520development%2520of%2520future%250Ahumanoid%2520control%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Humanoid%20Locomotion%20with%20Perceptive%20Internal%20Model&entry.906535625=Junfeng%20Long%20and%20Junli%20Ren%20and%20Moji%20Shi%20and%20Zirui%20Wang%20and%20Tao%20Huang%20and%20Ping%20Luo%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20In%20contrast%20to%20quadruped%20robots%20that%20can%20navigate%20diverse%20terrains%20using%20a%0A%22blind%22%20policy%2C%20humanoid%20robots%20require%20accurate%20perception%20for%20stable%0Alocomotion%20due%20to%20their%20high%20degrees%20of%20freedom%20and%20inherently%20unstable%0Amorphology.%20However%2C%20incorporating%20perceptual%20signals%20often%20introduces%0Aadditional%20disturbances%20to%20the%20system%2C%20potentially%20reducing%20its%20robustness%2C%0Ageneralizability%2C%20and%20efficiency.%20This%20paper%20presents%20the%20Perceptive%20Internal%0AModel%20%28PIM%29%2C%20which%20relies%20on%20onboard%2C%20continuously%20updated%20elevation%20maps%0Acentered%20around%20the%20robot%20to%20perceive%20its%20surroundings.%20We%20train%20the%20policy%0Ausing%20ground-truth%20obstacle%20heights%20surrounding%20the%20robot%20in%20simulation%2C%0Aoptimizing%20it%20based%20on%20the%20Hybrid%20Internal%20Model%20%28HIM%29%2C%20and%20perform%20inference%0Awith%20heights%20sampled%20from%20the%20constructed%20elevation%20map.%20Unlike%20previous%0Amethods%20that%20directly%20encode%20depth%20maps%20or%20raw%20point%20clouds%2C%20our%20approach%0Aallows%20the%20robot%20to%20perceive%20the%20terrain%20beneath%20its%20feet%20clearly%20and%20is%20less%0Aaffected%20by%20camera%20movement%20or%20noise.%20Furthermore%2C%20since%20depth%20map%20rendering%20is%0Anot%20required%20in%20simulation%2C%20our%20method%20introduces%20minimal%20additional%0Acomputational%20costs%20and%20can%20train%20the%20policy%20in%203%20hours%20on%20an%20RTX%204090%20GPU.%20We%0Averify%20the%20effectiveness%20of%20our%20method%20across%20various%20humanoid%20robots%2C%20various%0Aindoor%20and%20outdoor%20terrains%2C%20stairs%2C%20and%20various%20sensor%20configurations.%20Our%0Amethod%20can%20enable%20a%20humanoid%20robot%20to%20continuously%20climb%20stairs%20and%20has%20the%0Apotential%20to%20serve%20as%20a%20foundational%20algorithm%20for%20the%20development%20of%20future%0Ahumanoid%20control%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14386v1&entry.124074799=Read"},
{"title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional\n  Network and LSTM-CNN", "author": "Seyed Amir Latifi and Hassan Ghassemian and Maryam Imani", "abstract": "  This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.\n", "link": "http://arxiv.org/abs/2407.10689v5", "date": "2024-11-21", "relevancy": 2.3835, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4786}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&body=Title%3A%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN%0AAuthor%3A%20Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20fast%20and%20cost-effective%20method%20for%20diagnosing%20cardiac%0Aabnormalities%20with%20high%20accuracy%20and%20reliability%20using%20low-cost%20systems%20in%0Aclinics.%20The%20primary%20limitation%20of%20automatic%20diagnosing%20of%20cardiac%20diseases%20is%0Athe%20rarity%20of%20correct%20and%20acceptable%20labeled%20samples%2C%20which%20can%20be%20expensive%20to%0Aprepare.%20To%20address%20this%20issue%2C%20two%20methods%20are%20proposed%20in%20this%20work.%20The%0Afirst%20method%20is%20a%20unique%20Multi-Branch%20Deep%20Convolutional%20Neural%20Network%20%28MBDCN%29%0Aarchitecture%20inspired%20by%20human%20auditory%20processing%2C%20specifically%20designed%20to%0Aoptimize%20feature%20extraction%20by%20employing%20various%20sizes%20of%20convolutional%20filters%0Aand%20audio%20signal%20power%20spectrum%20as%20input.%20In%20the%20second%20method%2C%20called%20as%20Long%0Ashort-term%20memory-Convolutional%20Neural%20%28LSCN%29%20model%2C%20Additionally%2C%20the%20network%0Aarchitecture%20includes%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20blocks%20to%20improve%0Afeature%20extraction%20in%20the%20time%20domain.%20The%20innovative%20approach%20of%20combining%0Amultiple%20parallel%20branches%20consisting%20of%20the%20one-dimensional%20convolutional%0Alayers%20along%20with%20LSTM%20blocks%20helps%20in%20achieving%20superior%20results%20in%20audio%0Asignal%20processing%20tasks.%20The%20experimental%20results%20demonstrate%20superiority%20of%0Athe%20proposed%20methods%20over%20the%20state-of-the-art%20techniques.%20The%20overall%0Aclassification%20accuracy%20of%20heart%20sounds%20with%20the%20LSCN%20network%20is%20more%20than%2096%25.%0AThe%20efficiency%20of%20this%20network%20is%20significant%20compared%20to%20common%20feature%0Aextraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%0Awavelet%20transform.%20Therefore%2C%20the%20proposed%20method%20shows%20promising%20results%20in%0Athe%20automatic%20analysis%20of%20heart%20sounds%20and%20has%20potential%20applications%20in%20the%0Adiagnosis%20and%20early%20detection%20of%20cardiovascular%20diseases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10689v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Heart%2520Sounds%2520Using%2520Multi-Branch%2520Deep%2520Convolutional%250A%2520%2520Network%2520and%2520LSTM-CNN%26entry.906535625%3DSeyed%2520Amir%2520Latifi%2520and%2520Hassan%2520Ghassemian%2520and%2520Maryam%2520Imani%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520fast%2520and%2520cost-effective%2520method%2520for%2520diagnosing%2520cardiac%250Aabnormalities%2520with%2520high%2520accuracy%2520and%2520reliability%2520using%2520low-cost%2520systems%2520in%250Aclinics.%2520The%2520primary%2520limitation%2520of%2520automatic%2520diagnosing%2520of%2520cardiac%2520diseases%2520is%250Athe%2520rarity%2520of%2520correct%2520and%2520acceptable%2520labeled%2520samples%252C%2520which%2520can%2520be%2520expensive%2520to%250Aprepare.%2520To%2520address%2520this%2520issue%252C%2520two%2520methods%2520are%2520proposed%2520in%2520this%2520work.%2520The%250Afirst%2520method%2520is%2520a%2520unique%2520Multi-Branch%2520Deep%2520Convolutional%2520Neural%2520Network%2520%2528MBDCN%2529%250Aarchitecture%2520inspired%2520by%2520human%2520auditory%2520processing%252C%2520specifically%2520designed%2520to%250Aoptimize%2520feature%2520extraction%2520by%2520employing%2520various%2520sizes%2520of%2520convolutional%2520filters%250Aand%2520audio%2520signal%2520power%2520spectrum%2520as%2520input.%2520In%2520the%2520second%2520method%252C%2520called%2520as%2520Long%250Ashort-term%2520memory-Convolutional%2520Neural%2520%2528LSCN%2529%2520model%252C%2520Additionally%252C%2520the%2520network%250Aarchitecture%2520includes%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520network%2520blocks%2520to%2520improve%250Afeature%2520extraction%2520in%2520the%2520time%2520domain.%2520The%2520innovative%2520approach%2520of%2520combining%250Amultiple%2520parallel%2520branches%2520consisting%2520of%2520the%2520one-dimensional%2520convolutional%250Alayers%2520along%2520with%2520LSTM%2520blocks%2520helps%2520in%2520achieving%2520superior%2520results%2520in%2520audio%250Asignal%2520processing%2520tasks.%2520The%2520experimental%2520results%2520demonstrate%2520superiority%2520of%250Athe%2520proposed%2520methods%2520over%2520the%2520state-of-the-art%2520techniques.%2520The%2520overall%250Aclassification%2520accuracy%2520of%2520heart%2520sounds%2520with%2520the%2520LSCN%2520network%2520is%2520more%2520than%252096%2525.%250AThe%2520efficiency%2520of%2520this%2520network%2520is%2520significant%2520compared%2520to%2520common%2520feature%250Aextraction%2520methods%2520such%2520as%2520Mel%2520Frequency%2520Cepstral%2520Coefficients%2520%2528MFCC%2529%2520and%250Awavelet%2520transform.%2520Therefore%252C%2520the%2520proposed%2520method%2520shows%2520promising%2520results%2520in%250Athe%2520automatic%2520analysis%2520of%2520heart%2520sounds%2520and%2520has%2520potential%2520applications%2520in%2520the%250Adiagnosis%2520and%2520early%2520detection%2520of%2520cardiovascular%2520diseases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10689v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&entry.906535625=Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani&entry.1292438233=%20%20This%20paper%20presents%20a%20fast%20and%20cost-effective%20method%20for%20diagnosing%20cardiac%0Aabnormalities%20with%20high%20accuracy%20and%20reliability%20using%20low-cost%20systems%20in%0Aclinics.%20The%20primary%20limitation%20of%20automatic%20diagnosing%20of%20cardiac%20diseases%20is%0Athe%20rarity%20of%20correct%20and%20acceptable%20labeled%20samples%2C%20which%20can%20be%20expensive%20to%0Aprepare.%20To%20address%20this%20issue%2C%20two%20methods%20are%20proposed%20in%20this%20work.%20The%0Afirst%20method%20is%20a%20unique%20Multi-Branch%20Deep%20Convolutional%20Neural%20Network%20%28MBDCN%29%0Aarchitecture%20inspired%20by%20human%20auditory%20processing%2C%20specifically%20designed%20to%0Aoptimize%20feature%20extraction%20by%20employing%20various%20sizes%20of%20convolutional%20filters%0Aand%20audio%20signal%20power%20spectrum%20as%20input.%20In%20the%20second%20method%2C%20called%20as%20Long%0Ashort-term%20memory-Convolutional%20Neural%20%28LSCN%29%20model%2C%20Additionally%2C%20the%20network%0Aarchitecture%20includes%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20blocks%20to%20improve%0Afeature%20extraction%20in%20the%20time%20domain.%20The%20innovative%20approach%20of%20combining%0Amultiple%20parallel%20branches%20consisting%20of%20the%20one-dimensional%20convolutional%0Alayers%20along%20with%20LSTM%20blocks%20helps%20in%20achieving%20superior%20results%20in%20audio%0Asignal%20processing%20tasks.%20The%20experimental%20results%20demonstrate%20superiority%20of%0Athe%20proposed%20methods%20over%20the%20state-of-the-art%20techniques.%20The%20overall%0Aclassification%20accuracy%20of%20heart%20sounds%20with%20the%20LSCN%20network%20is%20more%20than%2096%25.%0AThe%20efficiency%20of%20this%20network%20is%20significant%20compared%20to%20common%20feature%0Aextraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%0Awavelet%20transform.%20Therefore%2C%20the%20proposed%20method%20shows%20promising%20results%20in%0Athe%20automatic%20analysis%20of%20heart%20sounds%20and%20has%20potential%20applications%20in%20the%0Adiagnosis%20and%20early%20detection%20of%20cardiovascular%20diseases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10689v5&entry.124074799=Read"},
{"title": "InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation", "author": "Marziyeh Bamdad and Hans-Peter Hutter and Alireza Darvishy", "abstract": "  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios, demonstrating the need and value of the new\ndataset to advance SLAM research for visually impaired navigation in complex\nindoor environments.\n", "link": "http://arxiv.org/abs/2411.14358v1", "date": "2024-11-21", "relevancy": 2.3808, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6135}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InCrowd-VI%3A%20A%20Realistic%20Visual-Inertial%20Dataset%20for%20Evaluating%20SLAM%20in%0A%20%20Indoor%20Pedestrian-Rich%20Spaces%20for%20Human%20Navigation&body=Title%3A%20InCrowd-VI%3A%20A%20Realistic%20Visual-Inertial%20Dataset%20for%20Evaluating%20SLAM%20in%0A%20%20Indoor%20Pedestrian-Rich%20Spaces%20for%20Human%20Navigation%0AAuthor%3A%20Marziyeh%20Bamdad%20and%20Hans-Peter%20Hutter%20and%20Alireza%20Darvishy%0AAbstract%3A%20%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20techniques%20can%20be%20used%20to%0Anavigate%20the%20visually%20impaired%2C%20but%20the%20development%20of%20robust%20SLAM%20solutions%0Afor%20crowded%20spaces%20is%20limited%20by%20the%20lack%20of%20realistic%20datasets.%20To%20address%0Athis%2C%20we%20introduce%20InCrowd-VI%2C%20a%20novel%20visual-inertial%20dataset%20specifically%0Adesigned%20for%20human%20navigation%20in%20indoor%20pedestrian-rich%20environments.%20Recorded%0Ausing%20Meta%20Aria%20Project%20glasses%2C%20it%20captures%20realistic%20scenarios%20without%0Aenvironmental%20control.%20InCrowd-VI%20features%2058%20sequences%20totaling%20a%205%20km%0Atrajectory%20length%20and%201.5%20hours%20of%20recording%20time%2C%20including%20RGB%2C%20stereo%0Aimages%2C%20and%20IMU%20measurements.%20The%20dataset%20captures%20important%20challenges%20such%20as%0Apedestrian%20occlusions%2C%20varying%20crowd%20densities%2C%20complex%20layouts%2C%20and%20lighting%0Achanges.%20Ground-truth%20trajectories%2C%20accurate%20to%20approximately%202%20cm%2C%20are%0Aprovided%20in%20the%20dataset%2C%20originating%20from%20the%20Meta%20Aria%20project%20machine%0Aperception%20SLAM%20service.%20In%20addition%2C%20a%20semi-dense%203D%20point%20cloud%20of%20scenes%20is%0Aprovided%20for%20each%20sequence.%20The%20evaluation%20of%20state-of-the-art%20visual%20odometry%0A%28VO%29%20and%20SLAM%20algorithms%20on%20InCrowd-VI%20revealed%20severe%20performance%20limitations%0Ain%20these%20realistic%20scenarios%2C%20demonstrating%20the%20need%20and%20value%20of%20the%20new%0Adataset%20to%20advance%20SLAM%20research%20for%20visually%20impaired%20navigation%20in%20complex%0Aindoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInCrowd-VI%253A%2520A%2520Realistic%2520Visual-Inertial%2520Dataset%2520for%2520Evaluating%2520SLAM%2520in%250A%2520%2520Indoor%2520Pedestrian-Rich%2520Spaces%2520for%2520Human%2520Navigation%26entry.906535625%3DMarziyeh%2520Bamdad%2520and%2520Hans-Peter%2520Hutter%2520and%2520Alireza%2520Darvishy%26entry.1292438233%3D%2520%2520Simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520techniques%2520can%2520be%2520used%2520to%250Anavigate%2520the%2520visually%2520impaired%252C%2520but%2520the%2520development%2520of%2520robust%2520SLAM%2520solutions%250Afor%2520crowded%2520spaces%2520is%2520limited%2520by%2520the%2520lack%2520of%2520realistic%2520datasets.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520InCrowd-VI%252C%2520a%2520novel%2520visual-inertial%2520dataset%2520specifically%250Adesigned%2520for%2520human%2520navigation%2520in%2520indoor%2520pedestrian-rich%2520environments.%2520Recorded%250Ausing%2520Meta%2520Aria%2520Project%2520glasses%252C%2520it%2520captures%2520realistic%2520scenarios%2520without%250Aenvironmental%2520control.%2520InCrowd-VI%2520features%252058%2520sequences%2520totaling%2520a%25205%2520km%250Atrajectory%2520length%2520and%25201.5%2520hours%2520of%2520recording%2520time%252C%2520including%2520RGB%252C%2520stereo%250Aimages%252C%2520and%2520IMU%2520measurements.%2520The%2520dataset%2520captures%2520important%2520challenges%2520such%2520as%250Apedestrian%2520occlusions%252C%2520varying%2520crowd%2520densities%252C%2520complex%2520layouts%252C%2520and%2520lighting%250Achanges.%2520Ground-truth%2520trajectories%252C%2520accurate%2520to%2520approximately%25202%2520cm%252C%2520are%250Aprovided%2520in%2520the%2520dataset%252C%2520originating%2520from%2520the%2520Meta%2520Aria%2520project%2520machine%250Aperception%2520SLAM%2520service.%2520In%2520addition%252C%2520a%2520semi-dense%25203D%2520point%2520cloud%2520of%2520scenes%2520is%250Aprovided%2520for%2520each%2520sequence.%2520The%2520evaluation%2520of%2520state-of-the-art%2520visual%2520odometry%250A%2528VO%2529%2520and%2520SLAM%2520algorithms%2520on%2520InCrowd-VI%2520revealed%2520severe%2520performance%2520limitations%250Ain%2520these%2520realistic%2520scenarios%252C%2520demonstrating%2520the%2520need%2520and%2520value%2520of%2520the%2520new%250Adataset%2520to%2520advance%2520SLAM%2520research%2520for%2520visually%2520impaired%2520navigation%2520in%2520complex%250Aindoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InCrowd-VI%3A%20A%20Realistic%20Visual-Inertial%20Dataset%20for%20Evaluating%20SLAM%20in%0A%20%20Indoor%20Pedestrian-Rich%20Spaces%20for%20Human%20Navigation&entry.906535625=Marziyeh%20Bamdad%20and%20Hans-Peter%20Hutter%20and%20Alireza%20Darvishy&entry.1292438233=%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20techniques%20can%20be%20used%20to%0Anavigate%20the%20visually%20impaired%2C%20but%20the%20development%20of%20robust%20SLAM%20solutions%0Afor%20crowded%20spaces%20is%20limited%20by%20the%20lack%20of%20realistic%20datasets.%20To%20address%0Athis%2C%20we%20introduce%20InCrowd-VI%2C%20a%20novel%20visual-inertial%20dataset%20specifically%0Adesigned%20for%20human%20navigation%20in%20indoor%20pedestrian-rich%20environments.%20Recorded%0Ausing%20Meta%20Aria%20Project%20glasses%2C%20it%20captures%20realistic%20scenarios%20without%0Aenvironmental%20control.%20InCrowd-VI%20features%2058%20sequences%20totaling%20a%205%20km%0Atrajectory%20length%20and%201.5%20hours%20of%20recording%20time%2C%20including%20RGB%2C%20stereo%0Aimages%2C%20and%20IMU%20measurements.%20The%20dataset%20captures%20important%20challenges%20such%20as%0Apedestrian%20occlusions%2C%20varying%20crowd%20densities%2C%20complex%20layouts%2C%20and%20lighting%0Achanges.%20Ground-truth%20trajectories%2C%20accurate%20to%20approximately%202%20cm%2C%20are%0Aprovided%20in%20the%20dataset%2C%20originating%20from%20the%20Meta%20Aria%20project%20machine%0Aperception%20SLAM%20service.%20In%20addition%2C%20a%20semi-dense%203D%20point%20cloud%20of%20scenes%20is%0Aprovided%20for%20each%20sequence.%20The%20evaluation%20of%20state-of-the-art%20visual%20odometry%0A%28VO%29%20and%20SLAM%20algorithms%20on%20InCrowd-VI%20revealed%20severe%20performance%20limitations%0Ain%20these%20realistic%20scenarios%2C%20demonstrating%20the%20need%20and%20value%20of%20the%20new%0Adataset%20to%20advance%20SLAM%20research%20for%20visually%20impaired%20navigation%20in%20complex%0Aindoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14358v1&entry.124074799=Read"},
{"title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning", "author": "Daniel Ekpo and Mara Levy and Saksham Suri and Chuong Huynh and Abhinav Shrivastava", "abstract": "  Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.\n", "link": "http://arxiv.org/abs/2411.10446v2", "date": "2024-11-21", "relevancy": 2.3795, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning&body=Title%3A%20VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning%0AAuthor%3A%20Daniel%20Ekpo%20and%20Mara%20Levy%20and%20Saksham%20Suri%20and%20Chuong%20Huynh%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20offer%20potential%20for%0Arobot%20task%20planning%2C%20but%20challenges%20remain%20due%20to%20VLMs%27%20tendency%20to%20generate%0Aincorrect%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20propose%20VeriGraph%2C%0Aa%20novel%20framework%20that%20integrates%20VLMs%20for%20robotic%20planning%20while%20verifying%0Aaction%20feasibility.%20VeriGraph%20employs%20scene%20graphs%20as%20an%20intermediate%0Arepresentation%2C%20capturing%20key%20objects%20and%20spatial%20relationships%20to%20improve%20plan%0Averification%20and%20refinement.%20The%20system%20generates%20a%20scene%20graph%20from%20input%0Aimages%20and%20uses%20it%20to%20iteratively%20check%20and%20correct%20action%20sequences%20generated%0Aby%20an%20LLM-based%20task%20planner%2C%20ensuring%20constraints%20are%20respected%20and%20actions%0Aare%20executable.%20Our%20approach%20significantly%20enhances%20task%20completion%20rates%0Aacross%20diverse%20manipulation%20scenarios%2C%20outperforming%20baseline%20methods%20by%2058%25%0Afor%20language-based%20tasks%20and%2030%25%20for%20image-based%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeriGraph%253A%2520Scene%2520Graphs%2520for%2520Execution%2520Verifiable%2520Robot%2520Planning%26entry.906535625%3DDaniel%2520Ekpo%2520and%2520Mara%2520Levy%2520and%2520Saksham%2520Suri%2520and%2520Chuong%2520Huynh%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520offer%2520potential%2520for%250Arobot%2520task%2520planning%252C%2520but%2520challenges%2520remain%2520due%2520to%2520VLMs%2527%2520tendency%2520to%2520generate%250Aincorrect%2520action%2520sequences.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520VeriGraph%252C%250Aa%2520novel%2520framework%2520that%2520integrates%2520VLMs%2520for%2520robotic%2520planning%2520while%2520verifying%250Aaction%2520feasibility.%2520VeriGraph%2520employs%2520scene%2520graphs%2520as%2520an%2520intermediate%250Arepresentation%252C%2520capturing%2520key%2520objects%2520and%2520spatial%2520relationships%2520to%2520improve%2520plan%250Averification%2520and%2520refinement.%2520The%2520system%2520generates%2520a%2520scene%2520graph%2520from%2520input%250Aimages%2520and%2520uses%2520it%2520to%2520iteratively%2520check%2520and%2520correct%2520action%2520sequences%2520generated%250Aby%2520an%2520LLM-based%2520task%2520planner%252C%2520ensuring%2520constraints%2520are%2520respected%2520and%2520actions%250Aare%2520executable.%2520Our%2520approach%2520significantly%2520enhances%2520task%2520completion%2520rates%250Aacross%2520diverse%2520manipulation%2520scenarios%252C%2520outperforming%2520baseline%2520methods%2520by%252058%2525%250Afor%2520language-based%2520tasks%2520and%252030%2525%2520for%2520image-based%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning&entry.906535625=Daniel%20Ekpo%20and%20Mara%20Levy%20and%20Saksham%20Suri%20and%20Chuong%20Huynh%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20offer%20potential%20for%0Arobot%20task%20planning%2C%20but%20challenges%20remain%20due%20to%20VLMs%27%20tendency%20to%20generate%0Aincorrect%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20propose%20VeriGraph%2C%0Aa%20novel%20framework%20that%20integrates%20VLMs%20for%20robotic%20planning%20while%20verifying%0Aaction%20feasibility.%20VeriGraph%20employs%20scene%20graphs%20as%20an%20intermediate%0Arepresentation%2C%20capturing%20key%20objects%20and%20spatial%20relationships%20to%20improve%20plan%0Averification%20and%20refinement.%20The%20system%20generates%20a%20scene%20graph%20from%20input%0Aimages%20and%20uses%20it%20to%20iteratively%20check%20and%20correct%20action%20sequences%20generated%0Aby%20an%20LLM-based%20task%20planner%2C%20ensuring%20constraints%20are%20respected%20and%20actions%0Aare%20executable.%20Our%20approach%20significantly%20enhances%20task%20completion%20rates%0Aacross%20diverse%20manipulation%20scenarios%2C%20outperforming%20baseline%20methods%20by%2058%25%0Afor%20language-based%20tasks%20and%2030%25%20for%20image-based%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10446v2&entry.124074799=Read"},
{"title": "Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models", "author": "Danush Kumar Venkatesh and Dominik Rivoir and Micha Pfeiffer and Fiona Kolbinger and Stefanie Speidel", "abstract": "  In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis\n", "link": "http://arxiv.org/abs/2410.07753v2", "date": "2024-11-21", "relevancy": 2.359, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Surgical%20Scene%20Segmentation%20with%20Anatomy-Aware%0A%20%20Diffusion%20Models&body=Title%3A%20Data%20Augmentation%20for%20Surgical%20Scene%20Segmentation%20with%20Anatomy-Aware%0A%20%20Diffusion%20Models%0AAuthor%3A%20Danush%20Kumar%20Venkatesh%20and%20Dominik%20Rivoir%20and%20Micha%20Pfeiffer%20and%20Fiona%20Kolbinger%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20In%20computer-assisted%20surgery%2C%20automatically%20recognizing%20anatomical%20organs%20is%0Acrucial%20for%20understanding%20the%20surgical%20scene%20and%20providing%20intraoperative%0Aassistance.%20While%20machine%20learning%20models%20can%20identify%20such%20structures%2C%20their%0Adeployment%20is%20hindered%20by%20the%20need%20for%20labeled%2C%20diverse%20surgical%20datasets%20with%0Aanatomical%20annotations.%20Labeling%20multiple%20classes%20%28i.e.%2C%20organs%29%20in%20a%20surgical%0Ascene%20is%20time-intensive%2C%20requiring%20medical%20experts.%20Although%20synthetically%0Agenerated%20images%20can%20enhance%20segmentation%20performance%2C%20maintaining%20both%20organ%0Astructure%20and%20texture%20during%20generation%20is%20challenging.%20We%20introduce%20a%0Amulti-stage%20approach%20using%20diffusion%20models%20to%20generate%20multi-class%20surgical%0Adatasets%20with%20annotations.%20Our%20framework%20improves%20anatomy%20awareness%20by%20training%0Aorgan%20specific%20models%20with%20an%20inpainting%20objective%20guided%20by%20binary%0Asegmentation%20masks.%20The%20organs%20are%20generated%20with%20an%20inference%20pipeline%20using%0Apre-trained%20ControlNet%20to%20maintain%20the%20organ%20structure.%20The%20synthetic%0Amulti-class%20datasets%20are%20constructed%20through%20an%20image%20composition%20step%2C%0Aensuring%20structural%20and%20textural%20consistency.%20This%20versatile%20approach%20allows%0Athe%20generation%20of%20multi-class%20datasets%20from%20real%20binary%20datasets%20and%20simulated%0Asurgical%20masks.%20We%20thoroughly%20evaluate%20the%20generated%20datasets%20on%20image%20quality%0Aand%20downstream%20segmentation%2C%20achieving%20a%20%2415%5C%25%24%20improvement%20in%20segmentation%0Ascores%20when%20combined%20with%20real%20images.%20The%20code%20is%20available%20at%0Ahttps%3A//gitlab.com/nct_tso_public/muli-class-image-synthesis%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520for%2520Surgical%2520Scene%2520Segmentation%2520with%2520Anatomy-Aware%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DDanush%2520Kumar%2520Venkatesh%2520and%2520Dominik%2520Rivoir%2520and%2520Micha%2520Pfeiffer%2520and%2520Fiona%2520Kolbinger%2520and%2520Stefanie%2520Speidel%26entry.1292438233%3D%2520%2520In%2520computer-assisted%2520surgery%252C%2520automatically%2520recognizing%2520anatomical%2520organs%2520is%250Acrucial%2520for%2520understanding%2520the%2520surgical%2520scene%2520and%2520providing%2520intraoperative%250Aassistance.%2520While%2520machine%2520learning%2520models%2520can%2520identify%2520such%2520structures%252C%2520their%250Adeployment%2520is%2520hindered%2520by%2520the%2520need%2520for%2520labeled%252C%2520diverse%2520surgical%2520datasets%2520with%250Aanatomical%2520annotations.%2520Labeling%2520multiple%2520classes%2520%2528i.e.%252C%2520organs%2529%2520in%2520a%2520surgical%250Ascene%2520is%2520time-intensive%252C%2520requiring%2520medical%2520experts.%2520Although%2520synthetically%250Agenerated%2520images%2520can%2520enhance%2520segmentation%2520performance%252C%2520maintaining%2520both%2520organ%250Astructure%2520and%2520texture%2520during%2520generation%2520is%2520challenging.%2520We%2520introduce%2520a%250Amulti-stage%2520approach%2520using%2520diffusion%2520models%2520to%2520generate%2520multi-class%2520surgical%250Adatasets%2520with%2520annotations.%2520Our%2520framework%2520improves%2520anatomy%2520awareness%2520by%2520training%250Aorgan%2520specific%2520models%2520with%2520an%2520inpainting%2520objective%2520guided%2520by%2520binary%250Asegmentation%2520masks.%2520The%2520organs%2520are%2520generated%2520with%2520an%2520inference%2520pipeline%2520using%250Apre-trained%2520ControlNet%2520to%2520maintain%2520the%2520organ%2520structure.%2520The%2520synthetic%250Amulti-class%2520datasets%2520are%2520constructed%2520through%2520an%2520image%2520composition%2520step%252C%250Aensuring%2520structural%2520and%2520textural%2520consistency.%2520This%2520versatile%2520approach%2520allows%250Athe%2520generation%2520of%2520multi-class%2520datasets%2520from%2520real%2520binary%2520datasets%2520and%2520simulated%250Asurgical%2520masks.%2520We%2520thoroughly%2520evaluate%2520the%2520generated%2520datasets%2520on%2520image%2520quality%250Aand%2520downstream%2520segmentation%252C%2520achieving%2520a%2520%252415%255C%2525%2524%2520improvement%2520in%2520segmentation%250Ascores%2520when%2520combined%2520with%2520real%2520images.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//gitlab.com/nct_tso_public/muli-class-image-synthesis%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Surgical%20Scene%20Segmentation%20with%20Anatomy-Aware%0A%20%20Diffusion%20Models&entry.906535625=Danush%20Kumar%20Venkatesh%20and%20Dominik%20Rivoir%20and%20Micha%20Pfeiffer%20and%20Fiona%20Kolbinger%20and%20Stefanie%20Speidel&entry.1292438233=%20%20In%20computer-assisted%20surgery%2C%20automatically%20recognizing%20anatomical%20organs%20is%0Acrucial%20for%20understanding%20the%20surgical%20scene%20and%20providing%20intraoperative%0Aassistance.%20While%20machine%20learning%20models%20can%20identify%20such%20structures%2C%20their%0Adeployment%20is%20hindered%20by%20the%20need%20for%20labeled%2C%20diverse%20surgical%20datasets%20with%0Aanatomical%20annotations.%20Labeling%20multiple%20classes%20%28i.e.%2C%20organs%29%20in%20a%20surgical%0Ascene%20is%20time-intensive%2C%20requiring%20medical%20experts.%20Although%20synthetically%0Agenerated%20images%20can%20enhance%20segmentation%20performance%2C%20maintaining%20both%20organ%0Astructure%20and%20texture%20during%20generation%20is%20challenging.%20We%20introduce%20a%0Amulti-stage%20approach%20using%20diffusion%20models%20to%20generate%20multi-class%20surgical%0Adatasets%20with%20annotations.%20Our%20framework%20improves%20anatomy%20awareness%20by%20training%0Aorgan%20specific%20models%20with%20an%20inpainting%20objective%20guided%20by%20binary%0Asegmentation%20masks.%20The%20organs%20are%20generated%20with%20an%20inference%20pipeline%20using%0Apre-trained%20ControlNet%20to%20maintain%20the%20organ%20structure.%20The%20synthetic%0Amulti-class%20datasets%20are%20constructed%20through%20an%20image%20composition%20step%2C%0Aensuring%20structural%20and%20textural%20consistency.%20This%20versatile%20approach%20allows%0Athe%20generation%20of%20multi-class%20datasets%20from%20real%20binary%20datasets%20and%20simulated%0Asurgical%20masks.%20We%20thoroughly%20evaluate%20the%20generated%20datasets%20on%20image%20quality%0Aand%20downstream%20segmentation%2C%20achieving%20a%20%2415%5C%25%24%20improvement%20in%20segmentation%0Ascores%20when%20combined%20with%20real%20images.%20The%20code%20is%20available%20at%0Ahttps%3A//gitlab.com/nct_tso_public/muli-class-image-synthesis%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07753v2&entry.124074799=Read"},
{"title": "GNN-MultiFix: Addressing the pitfalls for GNNs for multi-label node\n  classification", "author": "Tianqi Zhao and Megha Khosla", "abstract": "  Graph neural networks (GNNs) have emerged as powerful models for learning\nrepresentations of graph data showing state of the art results in various\ntasks. Nevertheless, the superiority of these methods is usually supported by\neither evaluating their performance on small subset of benchmark datasets or by\nreasoning about their expressive power in terms of certain graph isomorphism\ntests. In this paper we critically analyse both these aspects through a\ntransductive setting for the task of node classification. First, we delve\ndeeper into the case of multi-label node classification which offers a more\nrealistic scenario and has been ignored in most of the related works. Through\nanalysing the training dynamics for GNN methods we highlight the failure of\nGNNs to learn over multi-label graph datasets even for the case of abundant\ntraining data. Second, we show that specifically for transductive node\nclassification, even the most expressive GNN may fail to learn in absence of\nnode attributes and without using explicit label information as input. To\novercome this deficit, we propose a straightforward approach, referred to as\nGNN-MultiFix, that integrates the feature, label, and positional information of\na node. GNN-MultiFix demonstrates significant improvement across all the\nmulti-label datasets. We release our code at\nhttps://anonymous.4open.science/r/Graph-MultiFix-4121.\n", "link": "http://arxiv.org/abs/2411.14094v1", "date": "2024-11-21", "relevancy": 2.349, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4896}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4621}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-MultiFix%3A%20Addressing%20the%20pitfalls%20for%20GNNs%20for%20multi-label%20node%0A%20%20classification&body=Title%3A%20GNN-MultiFix%3A%20Addressing%20the%20pitfalls%20for%20GNNs%20for%20multi-label%20node%0A%20%20classification%0AAuthor%3A%20Tianqi%20Zhao%20and%20Megha%20Khosla%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Arepresentations%20of%20graph%20data%20showing%20state%20of%20the%20art%20results%20in%20various%0Atasks.%20Nevertheless%2C%20the%20superiority%20of%20these%20methods%20is%20usually%20supported%20by%0Aeither%20evaluating%20their%20performance%20on%20small%20subset%20of%20benchmark%20datasets%20or%20by%0Areasoning%20about%20their%20expressive%20power%20in%20terms%20of%20certain%20graph%20isomorphism%0Atests.%20In%20this%20paper%20we%20critically%20analyse%20both%20these%20aspects%20through%20a%0Atransductive%20setting%20for%20the%20task%20of%20node%20classification.%20First%2C%20we%20delve%0Adeeper%20into%20the%20case%20of%20multi-label%20node%20classification%20which%20offers%20a%20more%0Arealistic%20scenario%20and%20has%20been%20ignored%20in%20most%20of%20the%20related%20works.%20Through%0Aanalysing%20the%20training%20dynamics%20for%20GNN%20methods%20we%20highlight%20the%20failure%20of%0AGNNs%20to%20learn%20over%20multi-label%20graph%20datasets%20even%20for%20the%20case%20of%20abundant%0Atraining%20data.%20Second%2C%20we%20show%20that%20specifically%20for%20transductive%20node%0Aclassification%2C%20even%20the%20most%20expressive%20GNN%20may%20fail%20to%20learn%20in%20absence%20of%0Anode%20attributes%20and%20without%20using%20explicit%20label%20information%20as%20input.%20To%0Aovercome%20this%20deficit%2C%20we%20propose%20a%20straightforward%20approach%2C%20referred%20to%20as%0AGNN-MultiFix%2C%20that%20integrates%20the%20feature%2C%20label%2C%20and%20positional%20information%20of%0Aa%20node.%20GNN-MultiFix%20demonstrates%20significant%20improvement%20across%20all%20the%0Amulti-label%20datasets.%20We%20release%20our%20code%20at%0Ahttps%3A//anonymous.4open.science/r/Graph-MultiFix-4121.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-MultiFix%253A%2520Addressing%2520the%2520pitfalls%2520for%2520GNNs%2520for%2520multi-label%2520node%250A%2520%2520classification%26entry.906535625%3DTianqi%2520Zhao%2520and%2520Megha%2520Khosla%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520powerful%2520models%2520for%2520learning%250Arepresentations%2520of%2520graph%2520data%2520showing%2520state%2520of%2520the%2520art%2520results%2520in%2520various%250Atasks.%2520Nevertheless%252C%2520the%2520superiority%2520of%2520these%2520methods%2520is%2520usually%2520supported%2520by%250Aeither%2520evaluating%2520their%2520performance%2520on%2520small%2520subset%2520of%2520benchmark%2520datasets%2520or%2520by%250Areasoning%2520about%2520their%2520expressive%2520power%2520in%2520terms%2520of%2520certain%2520graph%2520isomorphism%250Atests.%2520In%2520this%2520paper%2520we%2520critically%2520analyse%2520both%2520these%2520aspects%2520through%2520a%250Atransductive%2520setting%2520for%2520the%2520task%2520of%2520node%2520classification.%2520First%252C%2520we%2520delve%250Adeeper%2520into%2520the%2520case%2520of%2520multi-label%2520node%2520classification%2520which%2520offers%2520a%2520more%250Arealistic%2520scenario%2520and%2520has%2520been%2520ignored%2520in%2520most%2520of%2520the%2520related%2520works.%2520Through%250Aanalysing%2520the%2520training%2520dynamics%2520for%2520GNN%2520methods%2520we%2520highlight%2520the%2520failure%2520of%250AGNNs%2520to%2520learn%2520over%2520multi-label%2520graph%2520datasets%2520even%2520for%2520the%2520case%2520of%2520abundant%250Atraining%2520data.%2520Second%252C%2520we%2520show%2520that%2520specifically%2520for%2520transductive%2520node%250Aclassification%252C%2520even%2520the%2520most%2520expressive%2520GNN%2520may%2520fail%2520to%2520learn%2520in%2520absence%2520of%250Anode%2520attributes%2520and%2520without%2520using%2520explicit%2520label%2520information%2520as%2520input.%2520To%250Aovercome%2520this%2520deficit%252C%2520we%2520propose%2520a%2520straightforward%2520approach%252C%2520referred%2520to%2520as%250AGNN-MultiFix%252C%2520that%2520integrates%2520the%2520feature%252C%2520label%252C%2520and%2520positional%2520information%2520of%250Aa%2520node.%2520GNN-MultiFix%2520demonstrates%2520significant%2520improvement%2520across%2520all%2520the%250Amulti-label%2520datasets.%2520We%2520release%2520our%2520code%2520at%250Ahttps%253A//anonymous.4open.science/r/Graph-MultiFix-4121.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-MultiFix%3A%20Addressing%20the%20pitfalls%20for%20GNNs%20for%20multi-label%20node%0A%20%20classification&entry.906535625=Tianqi%20Zhao%20and%20Megha%20Khosla&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Arepresentations%20of%20graph%20data%20showing%20state%20of%20the%20art%20results%20in%20various%0Atasks.%20Nevertheless%2C%20the%20superiority%20of%20these%20methods%20is%20usually%20supported%20by%0Aeither%20evaluating%20their%20performance%20on%20small%20subset%20of%20benchmark%20datasets%20or%20by%0Areasoning%20about%20their%20expressive%20power%20in%20terms%20of%20certain%20graph%20isomorphism%0Atests.%20In%20this%20paper%20we%20critically%20analyse%20both%20these%20aspects%20through%20a%0Atransductive%20setting%20for%20the%20task%20of%20node%20classification.%20First%2C%20we%20delve%0Adeeper%20into%20the%20case%20of%20multi-label%20node%20classification%20which%20offers%20a%20more%0Arealistic%20scenario%20and%20has%20been%20ignored%20in%20most%20of%20the%20related%20works.%20Through%0Aanalysing%20the%20training%20dynamics%20for%20GNN%20methods%20we%20highlight%20the%20failure%20of%0AGNNs%20to%20learn%20over%20multi-label%20graph%20datasets%20even%20for%20the%20case%20of%20abundant%0Atraining%20data.%20Second%2C%20we%20show%20that%20specifically%20for%20transductive%20node%0Aclassification%2C%20even%20the%20most%20expressive%20GNN%20may%20fail%20to%20learn%20in%20absence%20of%0Anode%20attributes%20and%20without%20using%20explicit%20label%20information%20as%20input.%20To%0Aovercome%20this%20deficit%2C%20we%20propose%20a%20straightforward%20approach%2C%20referred%20to%20as%0AGNN-MultiFix%2C%20that%20integrates%20the%20feature%2C%20label%2C%20and%20positional%20information%20of%0Aa%20node.%20GNN-MultiFix%20demonstrates%20significant%20improvement%20across%20all%20the%0Amulti-label%20datasets.%20We%20release%20our%20code%20at%0Ahttps%3A//anonymous.4open.science/r/Graph-MultiFix-4121.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14094v1&entry.124074799=Read"},
{"title": "Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding", "author": "Yiming Zhang and Zhuokai Zhao and Zhaorun Chen and Zenghui Ding and Xianjun Yang and Yining Sun", "abstract": "  Recent advancements in multimodal large language models (MLLMs) have opened\nnew avenues for video understanding. However, achieving high fidelity in\nzero-shot video tasks remains challenging. Traditional video processing methods\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\nincurs significant data and computation costs. In contrast, training-free\napproaches, though efficient, often lack robustness in preserving context-rich\nfeatures across complex video content. To this end, we propose DYTO, a novel\ndynamic token merging framework for zero-shot video understanding that\nadaptively optimizes token efficiency while preserving crucial scene details.\nDYTO integrates a hierarchical frame selection and a bipartite token merging\nstrategy to dynamically cluster key frames and selectively compress token\nsequences, striking a balance between computational efficiency with semantic\nrichness. Extensive experiments across multiple benchmarks demonstrate the\neffectiveness of DYTO, achieving superior performance compared to both\nfine-tuned and training-free methods and setting a new state-of-the-art for\nzero-shot video understanding.\n", "link": "http://arxiv.org/abs/2411.14401v1", "date": "2024-11-21", "relevancy": 2.33, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.607}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5666}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Training%3A%20Dynamic%20Token%20Merging%20for%20Zero-Shot%20Video%20Understanding&body=Title%3A%20Beyond%20Training%3A%20Dynamic%20Token%20Merging%20for%20Zero-Shot%20Video%20Understanding%0AAuthor%3A%20Yiming%20Zhang%20and%20Zhuokai%20Zhao%20and%20Zhaorun%20Chen%20and%20Zenghui%20Ding%20and%20Xianjun%20Yang%20and%20Yining%20Sun%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20opened%0Anew%20avenues%20for%20video%20understanding.%20However%2C%20achieving%20high%20fidelity%20in%0Azero-shot%20video%20tasks%20remains%20challenging.%20Traditional%20video%20processing%20methods%0Arely%20heavily%20on%20fine-tuning%20to%20capture%20nuanced%20spatial-temporal%20details%2C%20which%0Aincurs%20significant%20data%20and%20computation%20costs.%20In%20contrast%2C%20training-free%0Aapproaches%2C%20though%20efficient%2C%20often%20lack%20robustness%20in%20preserving%20context-rich%0Afeatures%20across%20complex%20video%20content.%20To%20this%20end%2C%20we%20propose%20DYTO%2C%20a%20novel%0Adynamic%20token%20merging%20framework%20for%20zero-shot%20video%20understanding%20that%0Aadaptively%20optimizes%20token%20efficiency%20while%20preserving%20crucial%20scene%20details.%0ADYTO%20integrates%20a%20hierarchical%20frame%20selection%20and%20a%20bipartite%20token%20merging%0Astrategy%20to%20dynamically%20cluster%20key%20frames%20and%20selectively%20compress%20token%0Asequences%2C%20striking%20a%20balance%20between%20computational%20efficiency%20with%20semantic%0Arichness.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20DYTO%2C%20achieving%20superior%20performance%20compared%20to%20both%0Afine-tuned%20and%20training-free%20methods%20and%20setting%20a%20new%20state-of-the-art%20for%0Azero-shot%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Training%253A%2520Dynamic%2520Token%2520Merging%2520for%2520Zero-Shot%2520Video%2520Understanding%26entry.906535625%3DYiming%2520Zhang%2520and%2520Zhuokai%2520Zhao%2520and%2520Zhaorun%2520Chen%2520and%2520Zenghui%2520Ding%2520and%2520Xianjun%2520Yang%2520and%2520Yining%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520opened%250Anew%2520avenues%2520for%2520video%2520understanding.%2520However%252C%2520achieving%2520high%2520fidelity%2520in%250Azero-shot%2520video%2520tasks%2520remains%2520challenging.%2520Traditional%2520video%2520processing%2520methods%250Arely%2520heavily%2520on%2520fine-tuning%2520to%2520capture%2520nuanced%2520spatial-temporal%2520details%252C%2520which%250Aincurs%2520significant%2520data%2520and%2520computation%2520costs.%2520In%2520contrast%252C%2520training-free%250Aapproaches%252C%2520though%2520efficient%252C%2520often%2520lack%2520robustness%2520in%2520preserving%2520context-rich%250Afeatures%2520across%2520complex%2520video%2520content.%2520To%2520this%2520end%252C%2520we%2520propose%2520DYTO%252C%2520a%2520novel%250Adynamic%2520token%2520merging%2520framework%2520for%2520zero-shot%2520video%2520understanding%2520that%250Aadaptively%2520optimizes%2520token%2520efficiency%2520while%2520preserving%2520crucial%2520scene%2520details.%250ADYTO%2520integrates%2520a%2520hierarchical%2520frame%2520selection%2520and%2520a%2520bipartite%2520token%2520merging%250Astrategy%2520to%2520dynamically%2520cluster%2520key%2520frames%2520and%2520selectively%2520compress%2520token%250Asequences%252C%2520striking%2520a%2520balance%2520between%2520computational%2520efficiency%2520with%2520semantic%250Arichness.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520DYTO%252C%2520achieving%2520superior%2520performance%2520compared%2520to%2520both%250Afine-tuned%2520and%2520training-free%2520methods%2520and%2520setting%2520a%2520new%2520state-of-the-art%2520for%250Azero-shot%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Training%3A%20Dynamic%20Token%20Merging%20for%20Zero-Shot%20Video%20Understanding&entry.906535625=Yiming%20Zhang%20and%20Zhuokai%20Zhao%20and%20Zhaorun%20Chen%20and%20Zenghui%20Ding%20and%20Xianjun%20Yang%20and%20Yining%20Sun&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20opened%0Anew%20avenues%20for%20video%20understanding.%20However%2C%20achieving%20high%20fidelity%20in%0Azero-shot%20video%20tasks%20remains%20challenging.%20Traditional%20video%20processing%20methods%0Arely%20heavily%20on%20fine-tuning%20to%20capture%20nuanced%20spatial-temporal%20details%2C%20which%0Aincurs%20significant%20data%20and%20computation%20costs.%20In%20contrast%2C%20training-free%0Aapproaches%2C%20though%20efficient%2C%20often%20lack%20robustness%20in%20preserving%20context-rich%0Afeatures%20across%20complex%20video%20content.%20To%20this%20end%2C%20we%20propose%20DYTO%2C%20a%20novel%0Adynamic%20token%20merging%20framework%20for%20zero-shot%20video%20understanding%20that%0Aadaptively%20optimizes%20token%20efficiency%20while%20preserving%20crucial%20scene%20details.%0ADYTO%20integrates%20a%20hierarchical%20frame%20selection%20and%20a%20bipartite%20token%20merging%0Astrategy%20to%20dynamically%20cluster%20key%20frames%20and%20selectively%20compress%20token%0Asequences%2C%20striking%20a%20balance%20between%20computational%20efficiency%20with%20semantic%0Arichness.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20DYTO%2C%20achieving%20superior%20performance%20compared%20to%20both%0Afine-tuned%20and%20training-free%20methods%20and%20setting%20a%20new%20state-of-the-art%20for%0Azero-shot%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14401v1&entry.124074799=Read"},
{"title": "FunctionChat-Bench: Comprehensive Evaluation of Language Models'\n  Generative Capabilities in Korean Tool-use Dialogs", "author": "Shinbok Lee and Gaeun Seo and Daniel Lee and Byeongil Ko and Sunghee Jung and Myeongcheol Shin", "abstract": "  This study investigates language models' generative capabilities in tool-use\ndialogs. We categorize the models' outputs in tool-use dialogs into four\ndistinct types: Tool Call, Answer Completion, Slot Question, and Relevance\nDetection, which serve as aspects for evaluation. We introduce\nFunctionChat-Bench, comprising 700 evaluation items and automated assessment\nprograms. Using this benchmark, we evaluate several language models that\nsupport function calling. Our findings indicate that while language models may\nexhibit high accuracy in single-turn Tool Call scenarios, this does not\nnecessarily translate to superior generative performance in multi-turn\nenvironments. We argue that the capabilities required for function calling\nextend beyond generating tool call messages; they must also effectively\ngenerate conversational messages that engage the user.\n", "link": "http://arxiv.org/abs/2411.14054v1", "date": "2024-11-21", "relevancy": 2.3072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FunctionChat-Bench%3A%20Comprehensive%20Evaluation%20of%20Language%20Models%27%0A%20%20Generative%20Capabilities%20in%20Korean%20Tool-use%20Dialogs&body=Title%3A%20FunctionChat-Bench%3A%20Comprehensive%20Evaluation%20of%20Language%20Models%27%0A%20%20Generative%20Capabilities%20in%20Korean%20Tool-use%20Dialogs%0AAuthor%3A%20Shinbok%20Lee%20and%20Gaeun%20Seo%20and%20Daniel%20Lee%20and%20Byeongil%20Ko%20and%20Sunghee%20Jung%20and%20Myeongcheol%20Shin%0AAbstract%3A%20%20%20This%20study%20investigates%20language%20models%27%20generative%20capabilities%20in%20tool-use%0Adialogs.%20We%20categorize%20the%20models%27%20outputs%20in%20tool-use%20dialogs%20into%20four%0Adistinct%20types%3A%20Tool%20Call%2C%20Answer%20Completion%2C%20Slot%20Question%2C%20and%20Relevance%0ADetection%2C%20which%20serve%20as%20aspects%20for%20evaluation.%20We%20introduce%0AFunctionChat-Bench%2C%20comprising%20700%20evaluation%20items%20and%20automated%20assessment%0Aprograms.%20Using%20this%20benchmark%2C%20we%20evaluate%20several%20language%20models%20that%0Asupport%20function%20calling.%20Our%20findings%20indicate%20that%20while%20language%20models%20may%0Aexhibit%20high%20accuracy%20in%20single-turn%20Tool%20Call%20scenarios%2C%20this%20does%20not%0Anecessarily%20translate%20to%20superior%20generative%20performance%20in%20multi-turn%0Aenvironments.%20We%20argue%20that%20the%20capabilities%20required%20for%20function%20calling%0Aextend%20beyond%20generating%20tool%20call%20messages%3B%20they%20must%20also%20effectively%0Agenerate%20conversational%20messages%20that%20engage%20the%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctionChat-Bench%253A%2520Comprehensive%2520Evaluation%2520of%2520Language%2520Models%2527%250A%2520%2520Generative%2520Capabilities%2520in%2520Korean%2520Tool-use%2520Dialogs%26entry.906535625%3DShinbok%2520Lee%2520and%2520Gaeun%2520Seo%2520and%2520Daniel%2520Lee%2520and%2520Byeongil%2520Ko%2520and%2520Sunghee%2520Jung%2520and%2520Myeongcheol%2520Shin%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520language%2520models%2527%2520generative%2520capabilities%2520in%2520tool-use%250Adialogs.%2520We%2520categorize%2520the%2520models%2527%2520outputs%2520in%2520tool-use%2520dialogs%2520into%2520four%250Adistinct%2520types%253A%2520Tool%2520Call%252C%2520Answer%2520Completion%252C%2520Slot%2520Question%252C%2520and%2520Relevance%250ADetection%252C%2520which%2520serve%2520as%2520aspects%2520for%2520evaluation.%2520We%2520introduce%250AFunctionChat-Bench%252C%2520comprising%2520700%2520evaluation%2520items%2520and%2520automated%2520assessment%250Aprograms.%2520Using%2520this%2520benchmark%252C%2520we%2520evaluate%2520several%2520language%2520models%2520that%250Asupport%2520function%2520calling.%2520Our%2520findings%2520indicate%2520that%2520while%2520language%2520models%2520may%250Aexhibit%2520high%2520accuracy%2520in%2520single-turn%2520Tool%2520Call%2520scenarios%252C%2520this%2520does%2520not%250Anecessarily%2520translate%2520to%2520superior%2520generative%2520performance%2520in%2520multi-turn%250Aenvironments.%2520We%2520argue%2520that%2520the%2520capabilities%2520required%2520for%2520function%2520calling%250Aextend%2520beyond%2520generating%2520tool%2520call%2520messages%253B%2520they%2520must%2520also%2520effectively%250Agenerate%2520conversational%2520messages%2520that%2520engage%2520the%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FunctionChat-Bench%3A%20Comprehensive%20Evaluation%20of%20Language%20Models%27%0A%20%20Generative%20Capabilities%20in%20Korean%20Tool-use%20Dialogs&entry.906535625=Shinbok%20Lee%20and%20Gaeun%20Seo%20and%20Daniel%20Lee%20and%20Byeongil%20Ko%20and%20Sunghee%20Jung%20and%20Myeongcheol%20Shin&entry.1292438233=%20%20This%20study%20investigates%20language%20models%27%20generative%20capabilities%20in%20tool-use%0Adialogs.%20We%20categorize%20the%20models%27%20outputs%20in%20tool-use%20dialogs%20into%20four%0Adistinct%20types%3A%20Tool%20Call%2C%20Answer%20Completion%2C%20Slot%20Question%2C%20and%20Relevance%0ADetection%2C%20which%20serve%20as%20aspects%20for%20evaluation.%20We%20introduce%0AFunctionChat-Bench%2C%20comprising%20700%20evaluation%20items%20and%20automated%20assessment%0Aprograms.%20Using%20this%20benchmark%2C%20we%20evaluate%20several%20language%20models%20that%0Asupport%20function%20calling.%20Our%20findings%20indicate%20that%20while%20language%20models%20may%0Aexhibit%20high%20accuracy%20in%20single-turn%20Tool%20Call%20scenarios%2C%20this%20does%20not%0Anecessarily%20translate%20to%20superior%20generative%20performance%20in%20multi-turn%0Aenvironments.%20We%20argue%20that%20the%20capabilities%20required%20for%20function%20calling%0Aextend%20beyond%20generating%20tool%20call%20messages%3B%20they%20must%20also%20effectively%0Agenerate%20conversational%20messages%20that%20engage%20the%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14054v1&entry.124074799=Read"},
{"title": "Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP", "author": "Zixin Wang and Dong Gong and Sen Wang and Zi Huang and Yadan Luo", "abstract": "  Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency.\n", "link": "http://arxiv.org/abs/2410.14729v2", "date": "2024-11-21", "relevancy": 2.306, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5754}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Less%20More%3F%20Exploring%20Token%20Condensation%20as%20Training-free%20Adaptation%0A%20%20for%20CLIP&body=Title%3A%20Is%20Less%20More%3F%20Exploring%20Token%20Condensation%20as%20Training-free%20Adaptation%0A%20%20for%20CLIP%0AAuthor%3A%20Zixin%20Wang%20and%20Dong%20Gong%20and%20Sen%20Wang%20and%20Zi%20Huang%20and%20Yadan%20Luo%0AAbstract%3A%20%20%20Contrastive%20language-image%20pre-training%20%28CLIP%29%20has%20shown%20remarkable%0Ageneralization%20ability%20in%20image%20classification.%20However%2C%20CLIP%20sometimes%0Aencounters%20performance%20drops%20on%20downstream%20datasets%20during%20zero-shot%20inference.%0ATest-time%20adaptation%20methods%20attempt%20to%20mitigate%20this%20by%20adjusting%0Anormalization%20layers%20or%20tuning%20context%20prompts%20with%20large%20batch%20sizes%20and%0Aextensive%20augmentations%3B%20yet%2C%20these%20methods%20are%20computationally%20intensive.%20This%0Araises%20an%20important%20question%3A%20Is%20there%20a%20training-free%20approach%20that%20can%0Aefficiently%20address%20CLIP%27s%20performance%20drop%20in%20such%20cases%3F%20To%20explore%20this%2C%20we%0Abenchmark%20token%20condensation%20techniques%2C%20originally%20designed%20to%20enhance%20the%0Aefficiency%20of%20vision%20transformers%2C%20on%20CLIP%20zero-shot%20inference%20tasks.%20We%0Aobserve%20that%20although%20token%20condensation%20may%20compromise%20in-domain%20accuracy%2C%20it%0Asurprisingly%20enhances%20CLIP%27s%20performance%20on%20certain%20cross-dataset%20benchmarks.%0AThis%20motivates%20two%20key%20inquiries%3A%20%281%29%20Can%20token%20condensation%20serve%20as%20a%0A%22free-lunch%22%20solution%20for%20CLIP%20zero-shot%20inference%3F%20%282%29%20What%20criteria%20should%0Aguide%20condensation%20--%20how%20can%20essential%20tokens%20be%20identified%20and%20redundant%20ones%0Aeliminated%3F%20To%20address%20these%20questions%2C%20we%20propose%20Token%20Condensation%20as%0AAdaptation%20%28TCA%29%2C%20a%20training-free%20adaptation%20method%20for%20CLIP%20by%20pruning%0Aclass-irrelevant%20visual%20tokens%20while%20merging%20class-ambiguous%20tokens.%20As%20the%0Afirst%20approach%20for%20CLIP%27s%20token%20efficiency%2C%20TCA%20demonstrates%20superior%0Aperformance%20across%20cross-dataset%20tasks%2C%20achieving%20up%20to%20a%2021.4%5C%25%20improvement%0Aover%20the%20strongest%20baseline%20while%20reducing%20GFLOPs%20by%2012.2%5C%25%20to%2048.9%5C%25%2C%20with%0Aminimized%20hyperparameter%20dependency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Less%2520More%253F%2520Exploring%2520Token%2520Condensation%2520as%2520Training-free%2520Adaptation%250A%2520%2520for%2520CLIP%26entry.906535625%3DZixin%2520Wang%2520and%2520Dong%2520Gong%2520and%2520Sen%2520Wang%2520and%2520Zi%2520Huang%2520and%2520Yadan%2520Luo%26entry.1292438233%3D%2520%2520Contrastive%2520language-image%2520pre-training%2520%2528CLIP%2529%2520has%2520shown%2520remarkable%250Ageneralization%2520ability%2520in%2520image%2520classification.%2520However%252C%2520CLIP%2520sometimes%250Aencounters%2520performance%2520drops%2520on%2520downstream%2520datasets%2520during%2520zero-shot%2520inference.%250ATest-time%2520adaptation%2520methods%2520attempt%2520to%2520mitigate%2520this%2520by%2520adjusting%250Anormalization%2520layers%2520or%2520tuning%2520context%2520prompts%2520with%2520large%2520batch%2520sizes%2520and%250Aextensive%2520augmentations%253B%2520yet%252C%2520these%2520methods%2520are%2520computationally%2520intensive.%2520This%250Araises%2520an%2520important%2520question%253A%2520Is%2520there%2520a%2520training-free%2520approach%2520that%2520can%250Aefficiently%2520address%2520CLIP%2527s%2520performance%2520drop%2520in%2520such%2520cases%253F%2520To%2520explore%2520this%252C%2520we%250Abenchmark%2520token%2520condensation%2520techniques%252C%2520originally%2520designed%2520to%2520enhance%2520the%250Aefficiency%2520of%2520vision%2520transformers%252C%2520on%2520CLIP%2520zero-shot%2520inference%2520tasks.%2520We%250Aobserve%2520that%2520although%2520token%2520condensation%2520may%2520compromise%2520in-domain%2520accuracy%252C%2520it%250Asurprisingly%2520enhances%2520CLIP%2527s%2520performance%2520on%2520certain%2520cross-dataset%2520benchmarks.%250AThis%2520motivates%2520two%2520key%2520inquiries%253A%2520%25281%2529%2520Can%2520token%2520condensation%2520serve%2520as%2520a%250A%2522free-lunch%2522%2520solution%2520for%2520CLIP%2520zero-shot%2520inference%253F%2520%25282%2529%2520What%2520criteria%2520should%250Aguide%2520condensation%2520--%2520how%2520can%2520essential%2520tokens%2520be%2520identified%2520and%2520redundant%2520ones%250Aeliminated%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520propose%2520Token%2520Condensation%2520as%250AAdaptation%2520%2528TCA%2529%252C%2520a%2520training-free%2520adaptation%2520method%2520for%2520CLIP%2520by%2520pruning%250Aclass-irrelevant%2520visual%2520tokens%2520while%2520merging%2520class-ambiguous%2520tokens.%2520As%2520the%250Afirst%2520approach%2520for%2520CLIP%2527s%2520token%2520efficiency%252C%2520TCA%2520demonstrates%2520superior%250Aperformance%2520across%2520cross-dataset%2520tasks%252C%2520achieving%2520up%2520to%2520a%252021.4%255C%2525%2520improvement%250Aover%2520the%2520strongest%2520baseline%2520while%2520reducing%2520GFLOPs%2520by%252012.2%255C%2525%2520to%252048.9%255C%2525%252C%2520with%250Aminimized%2520hyperparameter%2520dependency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Less%20More%3F%20Exploring%20Token%20Condensation%20as%20Training-free%20Adaptation%0A%20%20for%20CLIP&entry.906535625=Zixin%20Wang%20and%20Dong%20Gong%20and%20Sen%20Wang%20and%20Zi%20Huang%20and%20Yadan%20Luo&entry.1292438233=%20%20Contrastive%20language-image%20pre-training%20%28CLIP%29%20has%20shown%20remarkable%0Ageneralization%20ability%20in%20image%20classification.%20However%2C%20CLIP%20sometimes%0Aencounters%20performance%20drops%20on%20downstream%20datasets%20during%20zero-shot%20inference.%0ATest-time%20adaptation%20methods%20attempt%20to%20mitigate%20this%20by%20adjusting%0Anormalization%20layers%20or%20tuning%20context%20prompts%20with%20large%20batch%20sizes%20and%0Aextensive%20augmentations%3B%20yet%2C%20these%20methods%20are%20computationally%20intensive.%20This%0Araises%20an%20important%20question%3A%20Is%20there%20a%20training-free%20approach%20that%20can%0Aefficiently%20address%20CLIP%27s%20performance%20drop%20in%20such%20cases%3F%20To%20explore%20this%2C%20we%0Abenchmark%20token%20condensation%20techniques%2C%20originally%20designed%20to%20enhance%20the%0Aefficiency%20of%20vision%20transformers%2C%20on%20CLIP%20zero-shot%20inference%20tasks.%20We%0Aobserve%20that%20although%20token%20condensation%20may%20compromise%20in-domain%20accuracy%2C%20it%0Asurprisingly%20enhances%20CLIP%27s%20performance%20on%20certain%20cross-dataset%20benchmarks.%0AThis%20motivates%20two%20key%20inquiries%3A%20%281%29%20Can%20token%20condensation%20serve%20as%20a%0A%22free-lunch%22%20solution%20for%20CLIP%20zero-shot%20inference%3F%20%282%29%20What%20criteria%20should%0Aguide%20condensation%20--%20how%20can%20essential%20tokens%20be%20identified%20and%20redundant%20ones%0Aeliminated%3F%20To%20address%20these%20questions%2C%20we%20propose%20Token%20Condensation%20as%0AAdaptation%20%28TCA%29%2C%20a%20training-free%20adaptation%20method%20for%20CLIP%20by%20pruning%0Aclass-irrelevant%20visual%20tokens%20while%20merging%20class-ambiguous%20tokens.%20As%20the%0Afirst%20approach%20for%20CLIP%27s%20token%20efficiency%2C%20TCA%20demonstrates%20superior%0Aperformance%20across%20cross-dataset%20tasks%2C%20achieving%20up%20to%20a%2021.4%5C%25%20improvement%0Aover%20the%20strongest%20baseline%20while%20reducing%20GFLOPs%20by%2012.2%5C%25%20to%2048.9%5C%25%2C%20with%0Aminimized%20hyperparameter%20dependency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14729v2&entry.124074799=Read"},
{"title": "Learning multivariate Gaussians with imperfect advice", "author": "Arnab Bhattacharyya and Davin Choo and Philips George John and Themis Gouleakis", "abstract": "  We revisit the problem of distribution learning within the framework of\nlearning-augmented algorithms. In this setting, we explore the scenario where a\nprobability distribution is provided as potentially inaccurate advice on the\ntrue, unknown distribution. Our objective is to develop learning algorithms\nwhose sample complexity decreases as the quality of the advice improves,\nthereby surpassing standard learning lower bounds when the advice is\nsufficiently accurate.\n  Specifically, we demonstrate that this outcome is achievable for the problem\nof learning a multivariate Gaussian distribution $N(\\boldsymbol{\\mu},\n\\boldsymbol{\\Sigma})$ in the PAC learning setting. Classically, in the\nadvice-free setting, $\\tilde{\\Theta}(d^2/\\varepsilon^2)$ samples are sufficient\nand worst case necessary to learn $d$-dimensional Gaussians up to TV distance\n$\\varepsilon$ with constant probability. When we are additionally given a\nparameter $\\tilde{\\boldsymbol{\\Sigma}}$ as advice, we show that\n$\\tilde{O}(d^{2-\\beta}/\\varepsilon^2)$ samples suffices whenever $\\|\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} \\boldsymbol{\\Sigma}\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon\nd^{1-\\beta}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any\n$\\beta > 0$, yielding a polynomial improvement over the advice-free setting.\n", "link": "http://arxiv.org/abs/2411.12700v2", "date": "2024-11-21", "relevancy": 2.2879, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4719}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4627}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20multivariate%20Gaussians%20with%20imperfect%20advice&body=Title%3A%20Learning%20multivariate%20Gaussians%20with%20imperfect%20advice%0AAuthor%3A%20Arnab%20Bhattacharyya%20and%20Davin%20Choo%20and%20Philips%20George%20John%20and%20Themis%20Gouleakis%0AAbstract%3A%20%20%20We%20revisit%20the%20problem%20of%20distribution%20learning%20within%20the%20framework%20of%0Alearning-augmented%20algorithms.%20In%20this%20setting%2C%20we%20explore%20the%20scenario%20where%20a%0Aprobability%20distribution%20is%20provided%20as%20potentially%20inaccurate%20advice%20on%20the%0Atrue%2C%20unknown%20distribution.%20Our%20objective%20is%20to%20develop%20learning%20algorithms%0Awhose%20sample%20complexity%20decreases%20as%20the%20quality%20of%20the%20advice%20improves%2C%0Athereby%20surpassing%20standard%20learning%20lower%20bounds%20when%20the%20advice%20is%0Asufficiently%20accurate.%0A%20%20Specifically%2C%20we%20demonstrate%20that%20this%20outcome%20is%20achievable%20for%20the%20problem%0Aof%20learning%20a%20multivariate%20Gaussian%20distribution%20%24N%28%5Cboldsymbol%7B%5Cmu%7D%2C%0A%5Cboldsymbol%7B%5CSigma%7D%29%24%20in%20the%20PAC%20learning%20setting.%20Classically%2C%20in%20the%0Aadvice-free%20setting%2C%20%24%5Ctilde%7B%5CTheta%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20samples%20are%20sufficient%0Aand%20worst%20case%20necessary%20to%20learn%20%24d%24-dimensional%20Gaussians%20up%20to%20TV%20distance%0A%24%5Cvarepsilon%24%20with%20constant%20probability.%20When%20we%20are%20additionally%20given%20a%0Aparameter%20%24%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%24%20as%20advice%2C%20we%20show%20that%0A%24%5Ctilde%7BO%7D%28d%5E%7B2-%5Cbeta%7D/%5Cvarepsilon%5E2%29%24%20samples%20suffices%20whenever%20%24%5C%7C%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20%5Cboldsymbol%7B%5CSigma%7D%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20-%20%5Cboldsymbol%7BI_d%7D%20%5C%7C_1%20%5Cleq%20%5Cvarepsilon%0Ad%5E%7B1-%5Cbeta%7D%24%20%28where%20%24%5C%7C%5Ccdot%5C%7C_1%24%20denotes%20the%20entrywise%20%24%5Cell_1%24%20norm%29%20for%20any%0A%24%5Cbeta%20%3E%200%24%2C%20yielding%20a%20polynomial%20improvement%20over%20the%20advice-free%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520multivariate%2520Gaussians%2520with%2520imperfect%2520advice%26entry.906535625%3DArnab%2520Bhattacharyya%2520and%2520Davin%2520Choo%2520and%2520Philips%2520George%2520John%2520and%2520Themis%2520Gouleakis%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520problem%2520of%2520distribution%2520learning%2520within%2520the%2520framework%2520of%250Alearning-augmented%2520algorithms.%2520In%2520this%2520setting%252C%2520we%2520explore%2520the%2520scenario%2520where%2520a%250Aprobability%2520distribution%2520is%2520provided%2520as%2520potentially%2520inaccurate%2520advice%2520on%2520the%250Atrue%252C%2520unknown%2520distribution.%2520Our%2520objective%2520is%2520to%2520develop%2520learning%2520algorithms%250Awhose%2520sample%2520complexity%2520decreases%2520as%2520the%2520quality%2520of%2520the%2520advice%2520improves%252C%250Athereby%2520surpassing%2520standard%2520learning%2520lower%2520bounds%2520when%2520the%2520advice%2520is%250Asufficiently%2520accurate.%250A%2520%2520Specifically%252C%2520we%2520demonstrate%2520that%2520this%2520outcome%2520is%2520achievable%2520for%2520the%2520problem%250Aof%2520learning%2520a%2520multivariate%2520Gaussian%2520distribution%2520%2524N%2528%255Cboldsymbol%257B%255Cmu%257D%252C%250A%255Cboldsymbol%257B%255CSigma%257D%2529%2524%2520in%2520the%2520PAC%2520learning%2520setting.%2520Classically%252C%2520in%2520the%250Aadvice-free%2520setting%252C%2520%2524%255Ctilde%257B%255CTheta%257D%2528d%255E2/%255Cvarepsilon%255E2%2529%2524%2520samples%2520are%2520sufficient%250Aand%2520worst%2520case%2520necessary%2520to%2520learn%2520%2524d%2524-dimensional%2520Gaussians%2520up%2520to%2520TV%2520distance%250A%2524%255Cvarepsilon%2524%2520with%2520constant%2520probability.%2520When%2520we%2520are%2520additionally%2520given%2520a%250Aparameter%2520%2524%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%2524%2520as%2520advice%252C%2520we%2520show%2520that%250A%2524%255Ctilde%257BO%257D%2528d%255E%257B2-%255Cbeta%257D/%255Cvarepsilon%255E2%2529%2524%2520samples%2520suffices%2520whenever%2520%2524%255C%257C%250A%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%255E%257B-1/2%257D%2520%255Cboldsymbol%257B%255CSigma%257D%250A%255Ctilde%257B%255Cboldsymbol%257B%255CSigma%257D%257D%255E%257B-1/2%257D%2520-%2520%255Cboldsymbol%257BI_d%257D%2520%255C%257C_1%2520%255Cleq%2520%255Cvarepsilon%250Ad%255E%257B1-%255Cbeta%257D%2524%2520%2528where%2520%2524%255C%257C%255Ccdot%255C%257C_1%2524%2520denotes%2520the%2520entrywise%2520%2524%255Cell_1%2524%2520norm%2529%2520for%2520any%250A%2524%255Cbeta%2520%253E%25200%2524%252C%2520yielding%2520a%2520polynomial%2520improvement%2520over%2520the%2520advice-free%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20multivariate%20Gaussians%20with%20imperfect%20advice&entry.906535625=Arnab%20Bhattacharyya%20and%20Davin%20Choo%20and%20Philips%20George%20John%20and%20Themis%20Gouleakis&entry.1292438233=%20%20We%20revisit%20the%20problem%20of%20distribution%20learning%20within%20the%20framework%20of%0Alearning-augmented%20algorithms.%20In%20this%20setting%2C%20we%20explore%20the%20scenario%20where%20a%0Aprobability%20distribution%20is%20provided%20as%20potentially%20inaccurate%20advice%20on%20the%0Atrue%2C%20unknown%20distribution.%20Our%20objective%20is%20to%20develop%20learning%20algorithms%0Awhose%20sample%20complexity%20decreases%20as%20the%20quality%20of%20the%20advice%20improves%2C%0Athereby%20surpassing%20standard%20learning%20lower%20bounds%20when%20the%20advice%20is%0Asufficiently%20accurate.%0A%20%20Specifically%2C%20we%20demonstrate%20that%20this%20outcome%20is%20achievable%20for%20the%20problem%0Aof%20learning%20a%20multivariate%20Gaussian%20distribution%20%24N%28%5Cboldsymbol%7B%5Cmu%7D%2C%0A%5Cboldsymbol%7B%5CSigma%7D%29%24%20in%20the%20PAC%20learning%20setting.%20Classically%2C%20in%20the%0Aadvice-free%20setting%2C%20%24%5Ctilde%7B%5CTheta%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20samples%20are%20sufficient%0Aand%20worst%20case%20necessary%20to%20learn%20%24d%24-dimensional%20Gaussians%20up%20to%20TV%20distance%0A%24%5Cvarepsilon%24%20with%20constant%20probability.%20When%20we%20are%20additionally%20given%20a%0Aparameter%20%24%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%24%20as%20advice%2C%20we%20show%20that%0A%24%5Ctilde%7BO%7D%28d%5E%7B2-%5Cbeta%7D/%5Cvarepsilon%5E2%29%24%20samples%20suffices%20whenever%20%24%5C%7C%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20%5Cboldsymbol%7B%5CSigma%7D%0A%5Ctilde%7B%5Cboldsymbol%7B%5CSigma%7D%7D%5E%7B-1/2%7D%20-%20%5Cboldsymbol%7BI_d%7D%20%5C%7C_1%20%5Cleq%20%5Cvarepsilon%0Ad%5E%7B1-%5Cbeta%7D%24%20%28where%20%24%5C%7C%5Ccdot%5C%7C_1%24%20denotes%20the%20entrywise%20%24%5Cell_1%24%20norm%29%20for%20any%0A%24%5Cbeta%20%3E%200%24%2C%20yielding%20a%20polynomial%20improvement%20over%20the%20advice-free%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12700v2&entry.124074799=Read"},
{"title": "Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in\n  Regression", "author": "Ismail Nejjar and Gaetan Frusque and Florent Forest and Olga Fink", "abstract": "  Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt models\nfrom a labeled source domain to an unlabeled target domain for regression\ntasks. Traditional feature alignment methods, successful in classification,\noften prove ineffective for regression due to the correlated nature of\nregression features. To address this challenge, we propose Uncertainty-Guided\nAlignment (UGA), a novel method that integrates predictive uncertainty into the\nfeature alignment process. UGA employs Evidential Deep Learning to predict both\ntarget values and their associated uncertainties. This uncertainty information\nguides the alignment process and fuses information within the embedding space,\neffectively mitigating issues such as feature collapse in out-of-distribution\nscenarios. We evaluate UGA on two computer vision benchmarks and a real-world\nbattery state-of-charge prediction across different manufacturers and operating\ntemperatures. Across 52 transfer tasks, UGA on average outperforms existing\nstate-of-the-art methods. Our approach not only improves adaptation performance\nbut also provides well-calibrated uncertainty estimates.\n", "link": "http://arxiv.org/abs/2401.13721v3", "date": "2024-11-21", "relevancy": 2.2769, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6184}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Guided%20Alignment%20for%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Regression&body=Title%3A%20Uncertainty-Guided%20Alignment%20for%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Regression%0AAuthor%3A%20Ismail%20Nejjar%20and%20Gaetan%20Frusque%20and%20Florent%20Forest%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptation%20for%20Regression%20%28UDAR%29%20aims%20to%20adapt%20models%0Afrom%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain%20for%20regression%0Atasks.%20Traditional%20feature%20alignment%20methods%2C%20successful%20in%20classification%2C%0Aoften%20prove%20ineffective%20for%20regression%20due%20to%20the%20correlated%20nature%20of%0Aregression%20features.%20To%20address%20this%20challenge%2C%20we%20propose%20Uncertainty-Guided%0AAlignment%20%28UGA%29%2C%20a%20novel%20method%20that%20integrates%20predictive%20uncertainty%20into%20the%0Afeature%20alignment%20process.%20UGA%20employs%20Evidential%20Deep%20Learning%20to%20predict%20both%0Atarget%20values%20and%20their%20associated%20uncertainties.%20This%20uncertainty%20information%0Aguides%20the%20alignment%20process%20and%20fuses%20information%20within%20the%20embedding%20space%2C%0Aeffectively%20mitigating%20issues%20such%20as%20feature%20collapse%20in%20out-of-distribution%0Ascenarios.%20We%20evaluate%20UGA%20on%20two%20computer%20vision%20benchmarks%20and%20a%20real-world%0Abattery%20state-of-charge%20prediction%20across%20different%20manufacturers%20and%20operating%0Atemperatures.%20Across%2052%20transfer%20tasks%2C%20UGA%20on%20average%20outperforms%20existing%0Astate-of-the-art%20methods.%20Our%20approach%20not%20only%20improves%20adaptation%20performance%0Abut%20also%20provides%20well-calibrated%20uncertainty%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13721v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Guided%2520Alignment%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520in%250A%2520%2520Regression%26entry.906535625%3DIsmail%2520Nejjar%2520and%2520Gaetan%2520Frusque%2520and%2520Florent%2520Forest%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptation%2520for%2520Regression%2520%2528UDAR%2529%2520aims%2520to%2520adapt%2520models%250Afrom%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain%2520for%2520regression%250Atasks.%2520Traditional%2520feature%2520alignment%2520methods%252C%2520successful%2520in%2520classification%252C%250Aoften%2520prove%2520ineffective%2520for%2520regression%2520due%2520to%2520the%2520correlated%2520nature%2520of%250Aregression%2520features.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Uncertainty-Guided%250AAlignment%2520%2528UGA%2529%252C%2520a%2520novel%2520method%2520that%2520integrates%2520predictive%2520uncertainty%2520into%2520the%250Afeature%2520alignment%2520process.%2520UGA%2520employs%2520Evidential%2520Deep%2520Learning%2520to%2520predict%2520both%250Atarget%2520values%2520and%2520their%2520associated%2520uncertainties.%2520This%2520uncertainty%2520information%250Aguides%2520the%2520alignment%2520process%2520and%2520fuses%2520information%2520within%2520the%2520embedding%2520space%252C%250Aeffectively%2520mitigating%2520issues%2520such%2520as%2520feature%2520collapse%2520in%2520out-of-distribution%250Ascenarios.%2520We%2520evaluate%2520UGA%2520on%2520two%2520computer%2520vision%2520benchmarks%2520and%2520a%2520real-world%250Abattery%2520state-of-charge%2520prediction%2520across%2520different%2520manufacturers%2520and%2520operating%250Atemperatures.%2520Across%252052%2520transfer%2520tasks%252C%2520UGA%2520on%2520average%2520outperforms%2520existing%250Astate-of-the-art%2520methods.%2520Our%2520approach%2520not%2520only%2520improves%2520adaptation%2520performance%250Abut%2520also%2520provides%2520well-calibrated%2520uncertainty%2520estimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13721v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Guided%20Alignment%20for%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Regression&entry.906535625=Ismail%20Nejjar%20and%20Gaetan%20Frusque%20and%20Florent%20Forest%20and%20Olga%20Fink&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptation%20for%20Regression%20%28UDAR%29%20aims%20to%20adapt%20models%0Afrom%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain%20for%20regression%0Atasks.%20Traditional%20feature%20alignment%20methods%2C%20successful%20in%20classification%2C%0Aoften%20prove%20ineffective%20for%20regression%20due%20to%20the%20correlated%20nature%20of%0Aregression%20features.%20To%20address%20this%20challenge%2C%20we%20propose%20Uncertainty-Guided%0AAlignment%20%28UGA%29%2C%20a%20novel%20method%20that%20integrates%20predictive%20uncertainty%20into%20the%0Afeature%20alignment%20process.%20UGA%20employs%20Evidential%20Deep%20Learning%20to%20predict%20both%0Atarget%20values%20and%20their%20associated%20uncertainties.%20This%20uncertainty%20information%0Aguides%20the%20alignment%20process%20and%20fuses%20information%20within%20the%20embedding%20space%2C%0Aeffectively%20mitigating%20issues%20such%20as%20feature%20collapse%20in%20out-of-distribution%0Ascenarios.%20We%20evaluate%20UGA%20on%20two%20computer%20vision%20benchmarks%20and%20a%20real-world%0Abattery%20state-of-charge%20prediction%20across%20different%20manufacturers%20and%20operating%0Atemperatures.%20Across%2052%20transfer%20tasks%2C%20UGA%20on%20average%20outperforms%20existing%0Astate-of-the-art%20methods.%20Our%20approach%20not%20only%20improves%20adaptation%20performance%0Abut%20also%20provides%20well-calibrated%20uncertainty%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13721v3&entry.124074799=Read"},
{"title": "Collaborative Distributed Machine Learning", "author": "David Jin and Niclas Kannengie\u00dfer and Sascha Rank and Ali Sunyaev", "abstract": "  Various collaborative distributed machine learning (CDML) systems, including\nfederated learning systems and swarm learning systems, with diferent key traits\nwere developed to leverage resources for the development and use of machine\nlearning(ML) models in a conidentiality-preserving way. To meet use case\nrequirements, suitable CDML systems need to be selected. However, comparison\nbetween CDML systems to assess their suitability for use cases is often\ndiicult. To support comparison of CDML systems and introduce scientiic and\npractical audiences to the principal functioning and key traits of CDML\nsystems, this work presents a CDML system conceptualization and CDML\narchetypes.\n", "link": "http://arxiv.org/abs/2309.16584v4", "date": "2024-11-21", "relevancy": 2.2752, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Distributed%20Machine%20Learning&body=Title%3A%20Collaborative%20Distributed%20Machine%20Learning%0AAuthor%3A%20David%20Jin%20and%20Niclas%20Kannengie%C3%9Fer%20and%20Sascha%20Rank%20and%20Ali%20Sunyaev%0AAbstract%3A%20%20%20Various%20collaborative%20distributed%20machine%20learning%20%28CDML%29%20systems%2C%20including%0Afederated%20learning%20systems%20and%20swarm%20learning%20systems%2C%20with%20diferent%20key%20traits%0Awere%20developed%20to%20leverage%20resources%20for%20the%20development%20and%20use%20of%20machine%0Alearning%28ML%29%20models%20in%20a%20conidentiality-preserving%20way.%20To%20meet%20use%20case%0Arequirements%2C%20suitable%20CDML%20systems%20need%20to%20be%20selected.%20However%2C%20comparison%0Abetween%20CDML%20systems%20to%20assess%20their%20suitability%20for%20use%20cases%20is%20often%0Adiicult.%20To%20support%20comparison%20of%20CDML%20systems%20and%20introduce%20scientiic%20and%0Apractical%20audiences%20to%20the%20principal%20functioning%20and%20key%20traits%20of%20CDML%0Asystems%2C%20this%20work%20presents%20a%20CDML%20system%20conceptualization%20and%20CDML%0Aarchetypes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16584v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Distributed%2520Machine%2520Learning%26entry.906535625%3DDavid%2520Jin%2520and%2520Niclas%2520Kannengie%25C3%259Fer%2520and%2520Sascha%2520Rank%2520and%2520Ali%2520Sunyaev%26entry.1292438233%3D%2520%2520Various%2520collaborative%2520distributed%2520machine%2520learning%2520%2528CDML%2529%2520systems%252C%2520including%250Afederated%2520learning%2520systems%2520and%2520swarm%2520learning%2520systems%252C%2520with%2520diferent%2520key%2520traits%250Awere%2520developed%2520to%2520leverage%2520resources%2520for%2520the%2520development%2520and%2520use%2520of%2520machine%250Alearning%2528ML%2529%2520models%2520in%2520a%2520conidentiality-preserving%2520way.%2520To%2520meet%2520use%2520case%250Arequirements%252C%2520suitable%2520CDML%2520systems%2520need%2520to%2520be%2520selected.%2520However%252C%2520comparison%250Abetween%2520CDML%2520systems%2520to%2520assess%2520their%2520suitability%2520for%2520use%2520cases%2520is%2520often%250Adiicult.%2520To%2520support%2520comparison%2520of%2520CDML%2520systems%2520and%2520introduce%2520scientiic%2520and%250Apractical%2520audiences%2520to%2520the%2520principal%2520functioning%2520and%2520key%2520traits%2520of%2520CDML%250Asystems%252C%2520this%2520work%2520presents%2520a%2520CDML%2520system%2520conceptualization%2520and%2520CDML%250Aarchetypes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16584v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Distributed%20Machine%20Learning&entry.906535625=David%20Jin%20and%20Niclas%20Kannengie%C3%9Fer%20and%20Sascha%20Rank%20and%20Ali%20Sunyaev&entry.1292438233=%20%20Various%20collaborative%20distributed%20machine%20learning%20%28CDML%29%20systems%2C%20including%0Afederated%20learning%20systems%20and%20swarm%20learning%20systems%2C%20with%20diferent%20key%20traits%0Awere%20developed%20to%20leverage%20resources%20for%20the%20development%20and%20use%20of%20machine%0Alearning%28ML%29%20models%20in%20a%20conidentiality-preserving%20way.%20To%20meet%20use%20case%0Arequirements%2C%20suitable%20CDML%20systems%20need%20to%20be%20selected.%20However%2C%20comparison%0Abetween%20CDML%20systems%20to%20assess%20their%20suitability%20for%20use%20cases%20is%20often%0Adiicult.%20To%20support%20comparison%20of%20CDML%20systems%20and%20introduce%20scientiic%20and%0Apractical%20audiences%20to%20the%20principal%20functioning%20and%20key%20traits%20of%20CDML%0Asystems%2C%20this%20work%20presents%20a%20CDML%20system%20conceptualization%20and%20CDML%0Aarchetypes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16584v4&entry.124074799=Read"},
{"title": "Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals", "author": "Anxin Guo and Aravindan Vijayaraghavan", "abstract": "  We consider the problem of learning an arbitrarily-biased ReLU activation (or\nneuron) over Gaussian marginals with the squared loss objective. Despite the\nReLU neuron being the basic building block of modern neural networks, we still\ndo not understand the basic algorithmic question of whether one arbitrary ReLU\nneuron is learnable in the non-realizable setting. In particular, all existing\npolynomial time algorithms only provide approximation guarantees for the\nbetter-behaved unbiased setting or restricted bias setting.\n  Our main result is a polynomial time statistical query (SQ) algorithm that\ngives the first constant factor approximation for arbitrary bias. It outputs a\nReLU activation that achieves a loss of $O(\\mathrm{OPT}) + \\varepsilon$ in time\n$\\mathrm{poly}(d,1/\\varepsilon)$, where $\\mathrm{OPT}$ is the loss obtained by\nthe optimal ReLU activation. Our algorithm presents an interesting departure\nfrom existing algorithms, which are all based on gradient descent and thus fall\nwithin the class of correlational statistical query (CSQ) algorithms. We\ncomplement our algorithmic result by showing that no polynomial time CSQ\nalgorithm can achieve a constant factor approximation. Together, these results\nshed light on the intrinsic limitation of gradient descent, while identifying\narguably the simplest setting (a single neuron) where there is a separation\nbetween SQ and CSQ algorithms.\n", "link": "http://arxiv.org/abs/2411.14349v1", "date": "2024-11-21", "relevancy": 2.2387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4563}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4458}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agnostic%20Learning%20of%20Arbitrary%20ReLU%20Activation%20under%20Gaussian%20Marginals&body=Title%3A%20Agnostic%20Learning%20of%20Arbitrary%20ReLU%20Activation%20under%20Gaussian%20Marginals%0AAuthor%3A%20Anxin%20Guo%20and%20Aravindan%20Vijayaraghavan%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20an%20arbitrarily-biased%20ReLU%20activation%20%28or%0Aneuron%29%20over%20Gaussian%20marginals%20with%20the%20squared%20loss%20objective.%20Despite%20the%0AReLU%20neuron%20being%20the%20basic%20building%20block%20of%20modern%20neural%20networks%2C%20we%20still%0Ado%20not%20understand%20the%20basic%20algorithmic%20question%20of%20whether%20one%20arbitrary%20ReLU%0Aneuron%20is%20learnable%20in%20the%20non-realizable%20setting.%20In%20particular%2C%20all%20existing%0Apolynomial%20time%20algorithms%20only%20provide%20approximation%20guarantees%20for%20the%0Abetter-behaved%20unbiased%20setting%20or%20restricted%20bias%20setting.%0A%20%20Our%20main%20result%20is%20a%20polynomial%20time%20statistical%20query%20%28SQ%29%20algorithm%20that%0Agives%20the%20first%20constant%20factor%20approximation%20for%20arbitrary%20bias.%20It%20outputs%20a%0AReLU%20activation%20that%20achieves%20a%20loss%20of%20%24O%28%5Cmathrm%7BOPT%7D%29%20%2B%20%5Cvarepsilon%24%20in%20time%0A%24%5Cmathrm%7Bpoly%7D%28d%2C1/%5Cvarepsilon%29%24%2C%20where%20%24%5Cmathrm%7BOPT%7D%24%20is%20the%20loss%20obtained%20by%0Athe%20optimal%20ReLU%20activation.%20Our%20algorithm%20presents%20an%20interesting%20departure%0Afrom%20existing%20algorithms%2C%20which%20are%20all%20based%20on%20gradient%20descent%20and%20thus%20fall%0Awithin%20the%20class%20of%20correlational%20statistical%20query%20%28CSQ%29%20algorithms.%20We%0Acomplement%20our%20algorithmic%20result%20by%20showing%20that%20no%20polynomial%20time%20CSQ%0Aalgorithm%20can%20achieve%20a%20constant%20factor%20approximation.%20Together%2C%20these%20results%0Ashed%20light%20on%20the%20intrinsic%20limitation%20of%20gradient%20descent%2C%20while%20identifying%0Aarguably%20the%20simplest%20setting%20%28a%20single%20neuron%29%20where%20there%20is%20a%20separation%0Abetween%20SQ%20and%20CSQ%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgnostic%2520Learning%2520of%2520Arbitrary%2520ReLU%2520Activation%2520under%2520Gaussian%2520Marginals%26entry.906535625%3DAnxin%2520Guo%2520and%2520Aravindan%2520Vijayaraghavan%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520an%2520arbitrarily-biased%2520ReLU%2520activation%2520%2528or%250Aneuron%2529%2520over%2520Gaussian%2520marginals%2520with%2520the%2520squared%2520loss%2520objective.%2520Despite%2520the%250AReLU%2520neuron%2520being%2520the%2520basic%2520building%2520block%2520of%2520modern%2520neural%2520networks%252C%2520we%2520still%250Ado%2520not%2520understand%2520the%2520basic%2520algorithmic%2520question%2520of%2520whether%2520one%2520arbitrary%2520ReLU%250Aneuron%2520is%2520learnable%2520in%2520the%2520non-realizable%2520setting.%2520In%2520particular%252C%2520all%2520existing%250Apolynomial%2520time%2520algorithms%2520only%2520provide%2520approximation%2520guarantees%2520for%2520the%250Abetter-behaved%2520unbiased%2520setting%2520or%2520restricted%2520bias%2520setting.%250A%2520%2520Our%2520main%2520result%2520is%2520a%2520polynomial%2520time%2520statistical%2520query%2520%2528SQ%2529%2520algorithm%2520that%250Agives%2520the%2520first%2520constant%2520factor%2520approximation%2520for%2520arbitrary%2520bias.%2520It%2520outputs%2520a%250AReLU%2520activation%2520that%2520achieves%2520a%2520loss%2520of%2520%2524O%2528%255Cmathrm%257BOPT%257D%2529%2520%252B%2520%255Cvarepsilon%2524%2520in%2520time%250A%2524%255Cmathrm%257Bpoly%257D%2528d%252C1/%255Cvarepsilon%2529%2524%252C%2520where%2520%2524%255Cmathrm%257BOPT%257D%2524%2520is%2520the%2520loss%2520obtained%2520by%250Athe%2520optimal%2520ReLU%2520activation.%2520Our%2520algorithm%2520presents%2520an%2520interesting%2520departure%250Afrom%2520existing%2520algorithms%252C%2520which%2520are%2520all%2520based%2520on%2520gradient%2520descent%2520and%2520thus%2520fall%250Awithin%2520the%2520class%2520of%2520correlational%2520statistical%2520query%2520%2528CSQ%2529%2520algorithms.%2520We%250Acomplement%2520our%2520algorithmic%2520result%2520by%2520showing%2520that%2520no%2520polynomial%2520time%2520CSQ%250Aalgorithm%2520can%2520achieve%2520a%2520constant%2520factor%2520approximation.%2520Together%252C%2520these%2520results%250Ashed%2520light%2520on%2520the%2520intrinsic%2520limitation%2520of%2520gradient%2520descent%252C%2520while%2520identifying%250Aarguably%2520the%2520simplest%2520setting%2520%2528a%2520single%2520neuron%2529%2520where%2520there%2520is%2520a%2520separation%250Abetween%2520SQ%2520and%2520CSQ%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agnostic%20Learning%20of%20Arbitrary%20ReLU%20Activation%20under%20Gaussian%20Marginals&entry.906535625=Anxin%20Guo%20and%20Aravindan%20Vijayaraghavan&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20an%20arbitrarily-biased%20ReLU%20activation%20%28or%0Aneuron%29%20over%20Gaussian%20marginals%20with%20the%20squared%20loss%20objective.%20Despite%20the%0AReLU%20neuron%20being%20the%20basic%20building%20block%20of%20modern%20neural%20networks%2C%20we%20still%0Ado%20not%20understand%20the%20basic%20algorithmic%20question%20of%20whether%20one%20arbitrary%20ReLU%0Aneuron%20is%20learnable%20in%20the%20non-realizable%20setting.%20In%20particular%2C%20all%20existing%0Apolynomial%20time%20algorithms%20only%20provide%20approximation%20guarantees%20for%20the%0Abetter-behaved%20unbiased%20setting%20or%20restricted%20bias%20setting.%0A%20%20Our%20main%20result%20is%20a%20polynomial%20time%20statistical%20query%20%28SQ%29%20algorithm%20that%0Agives%20the%20first%20constant%20factor%20approximation%20for%20arbitrary%20bias.%20It%20outputs%20a%0AReLU%20activation%20that%20achieves%20a%20loss%20of%20%24O%28%5Cmathrm%7BOPT%7D%29%20%2B%20%5Cvarepsilon%24%20in%20time%0A%24%5Cmathrm%7Bpoly%7D%28d%2C1/%5Cvarepsilon%29%24%2C%20where%20%24%5Cmathrm%7BOPT%7D%24%20is%20the%20loss%20obtained%20by%0Athe%20optimal%20ReLU%20activation.%20Our%20algorithm%20presents%20an%20interesting%20departure%0Afrom%20existing%20algorithms%2C%20which%20are%20all%20based%20on%20gradient%20descent%20and%20thus%20fall%0Awithin%20the%20class%20of%20correlational%20statistical%20query%20%28CSQ%29%20algorithms.%20We%0Acomplement%20our%20algorithmic%20result%20by%20showing%20that%20no%20polynomial%20time%20CSQ%0Aalgorithm%20can%20achieve%20a%20constant%20factor%20approximation.%20Together%2C%20these%20results%0Ashed%20light%20on%20the%20intrinsic%20limitation%20of%20gradient%20descent%2C%20while%20identifying%0Aarguably%20the%20simplest%20setting%20%28a%20single%20neuron%29%20where%20there%20is%20a%20separation%0Abetween%20SQ%20and%20CSQ%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14349v1&entry.124074799=Read"},
{"title": "CP-UNet: Contour-based Probabilistic Model for Medical Ultrasound Images\n  Segmentation", "author": "Ruiguo Yu and Yiyang Zhang and Yuan Tian and Zhiqiang Liu and Xuewei Li and Jie Gao", "abstract": "  Deep learning-based segmentation methods are widely utilized for detecting\nlesions in ultrasound images. Throughout the imaging procedure, the attenuation\nand scattering of ultrasound waves cause contour blurring and the formation of\nartifacts, limiting the clarity of the acquired ultrasound images. To overcome\nthis challenge, we propose a contour-based probabilistic segmentation model\nCP-UNet, which guides the segmentation network to enhance its focus on contour\nduring decoding. We design a novel down-sampling module to enable the contour\nprobability distribution modeling and encoding stages to acquire global-local\nfeatures. Furthermore, the Gaussian Mixture Model utilizes optimized features\nto model the contour distribution, capturing the uncertainty of lesion\nboundaries. Extensive experiments with several state-of-the-art deep learning\nsegmentation methods on three ultrasound image datasets show that our method\nperforms better on breast and thyroid lesions segmentation.\n", "link": "http://arxiv.org/abs/2411.14250v1", "date": "2024-11-21", "relevancy": 2.2366, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5817}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5789}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-UNet%3A%20Contour-based%20Probabilistic%20Model%20for%20Medical%20Ultrasound%20Images%0A%20%20Segmentation&body=Title%3A%20CP-UNet%3A%20Contour-based%20Probabilistic%20Model%20for%20Medical%20Ultrasound%20Images%0A%20%20Segmentation%0AAuthor%3A%20Ruiguo%20Yu%20and%20Yiyang%20Zhang%20and%20Yuan%20Tian%20and%20Zhiqiang%20Liu%20and%20Xuewei%20Li%20and%20Jie%20Gao%0AAbstract%3A%20%20%20Deep%20learning-based%20segmentation%20methods%20are%20widely%20utilized%20for%20detecting%0Alesions%20in%20ultrasound%20images.%20Throughout%20the%20imaging%20procedure%2C%20the%20attenuation%0Aand%20scattering%20of%20ultrasound%20waves%20cause%20contour%20blurring%20and%20the%20formation%20of%0Aartifacts%2C%20limiting%20the%20clarity%20of%20the%20acquired%20ultrasound%20images.%20To%20overcome%0Athis%20challenge%2C%20we%20propose%20a%20contour-based%20probabilistic%20segmentation%20model%0ACP-UNet%2C%20which%20guides%20the%20segmentation%20network%20to%20enhance%20its%20focus%20on%20contour%0Aduring%20decoding.%20We%20design%20a%20novel%20down-sampling%20module%20to%20enable%20the%20contour%0Aprobability%20distribution%20modeling%20and%20encoding%20stages%20to%20acquire%20global-local%0Afeatures.%20Furthermore%2C%20the%20Gaussian%20Mixture%20Model%20utilizes%20optimized%20features%0Ato%20model%20the%20contour%20distribution%2C%20capturing%20the%20uncertainty%20of%20lesion%0Aboundaries.%20Extensive%20experiments%20with%20several%20state-of-the-art%20deep%20learning%0Asegmentation%20methods%20on%20three%20ultrasound%20image%20datasets%20show%20that%20our%20method%0Aperforms%20better%20on%20breast%20and%20thyroid%20lesions%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-UNet%253A%2520Contour-based%2520Probabilistic%2520Model%2520for%2520Medical%2520Ultrasound%2520Images%250A%2520%2520Segmentation%26entry.906535625%3DRuiguo%2520Yu%2520and%2520Yiyang%2520Zhang%2520and%2520Yuan%2520Tian%2520and%2520Zhiqiang%2520Liu%2520and%2520Xuewei%2520Li%2520and%2520Jie%2520Gao%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520segmentation%2520methods%2520are%2520widely%2520utilized%2520for%2520detecting%250Alesions%2520in%2520ultrasound%2520images.%2520Throughout%2520the%2520imaging%2520procedure%252C%2520the%2520attenuation%250Aand%2520scattering%2520of%2520ultrasound%2520waves%2520cause%2520contour%2520blurring%2520and%2520the%2520formation%2520of%250Aartifacts%252C%2520limiting%2520the%2520clarity%2520of%2520the%2520acquired%2520ultrasound%2520images.%2520To%2520overcome%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520contour-based%2520probabilistic%2520segmentation%2520model%250ACP-UNet%252C%2520which%2520guides%2520the%2520segmentation%2520network%2520to%2520enhance%2520its%2520focus%2520on%2520contour%250Aduring%2520decoding.%2520We%2520design%2520a%2520novel%2520down-sampling%2520module%2520to%2520enable%2520the%2520contour%250Aprobability%2520distribution%2520modeling%2520and%2520encoding%2520stages%2520to%2520acquire%2520global-local%250Afeatures.%2520Furthermore%252C%2520the%2520Gaussian%2520Mixture%2520Model%2520utilizes%2520optimized%2520features%250Ato%2520model%2520the%2520contour%2520distribution%252C%2520capturing%2520the%2520uncertainty%2520of%2520lesion%250Aboundaries.%2520Extensive%2520experiments%2520with%2520several%2520state-of-the-art%2520deep%2520learning%250Asegmentation%2520methods%2520on%2520three%2520ultrasound%2520image%2520datasets%2520show%2520that%2520our%2520method%250Aperforms%2520better%2520on%2520breast%2520and%2520thyroid%2520lesions%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-UNet%3A%20Contour-based%20Probabilistic%20Model%20for%20Medical%20Ultrasound%20Images%0A%20%20Segmentation&entry.906535625=Ruiguo%20Yu%20and%20Yiyang%20Zhang%20and%20Yuan%20Tian%20and%20Zhiqiang%20Liu%20and%20Xuewei%20Li%20and%20Jie%20Gao&entry.1292438233=%20%20Deep%20learning-based%20segmentation%20methods%20are%20widely%20utilized%20for%20detecting%0Alesions%20in%20ultrasound%20images.%20Throughout%20the%20imaging%20procedure%2C%20the%20attenuation%0Aand%20scattering%20of%20ultrasound%20waves%20cause%20contour%20blurring%20and%20the%20formation%20of%0Aartifacts%2C%20limiting%20the%20clarity%20of%20the%20acquired%20ultrasound%20images.%20To%20overcome%0Athis%20challenge%2C%20we%20propose%20a%20contour-based%20probabilistic%20segmentation%20model%0ACP-UNet%2C%20which%20guides%20the%20segmentation%20network%20to%20enhance%20its%20focus%20on%20contour%0Aduring%20decoding.%20We%20design%20a%20novel%20down-sampling%20module%20to%20enable%20the%20contour%0Aprobability%20distribution%20modeling%20and%20encoding%20stages%20to%20acquire%20global-local%0Afeatures.%20Furthermore%2C%20the%20Gaussian%20Mixture%20Model%20utilizes%20optimized%20features%0Ato%20model%20the%20contour%20distribution%2C%20capturing%20the%20uncertainty%20of%20lesion%0Aboundaries.%20Extensive%20experiments%20with%20several%20state-of-the-art%20deep%20learning%0Asegmentation%20methods%20on%20three%20ultrasound%20image%20datasets%20show%20that%20our%20method%0Aperforms%20better%20on%20breast%20and%20thyroid%20lesions%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14250v1&entry.124074799=Read"},
{"title": "Layer Pruning with Consensus: A Triple-Win Solution", "author": "Leandro Giusti Mugnaini and Carolina Tavares Duarte and Anna H. Reali Costa and Artur Jordao", "abstract": "  Layer pruning offers a promising alternative to standard structured pruning,\neffectively reducing computational costs, latency, and memory footprint. While\nnotable layer-pruning approaches aim to detect unimportant layers for removal,\nthey often rely on single criteria that may not fully capture the complex,\nunderlying properties of layers. We propose a novel approach that combines\nmultiple similarity metrics into a single expressive measure of low-importance\nlayers, called the Consensus criterion. Our technique delivers a triple-win\nsolution: low accuracy drop, high-performance improvement, and increased\nrobustness to adversarial attacks. With up to 78.80% FLOPs reduction and\nperformance on par with state-of-the-art methods across different benchmarks,\nour approach reduces energy consumption and carbon emissions by up to 66.99%\nand 68.75%, respectively. Additionally, it avoids shortcut learning and\nimproves robustness by up to 4 percentage points under various adversarial\nattacks. Overall, the Consensus criterion demonstrates its effectiveness in\ncreating robust, efficient, and environmentally friendly pruned models.\n", "link": "http://arxiv.org/abs/2411.14345v1", "date": "2024-11-21", "relevancy": 2.2355, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4617}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layer%20Pruning%20with%20Consensus%3A%20A%20Triple-Win%20Solution&body=Title%3A%20Layer%20Pruning%20with%20Consensus%3A%20A%20Triple-Win%20Solution%0AAuthor%3A%20Leandro%20Giusti%20Mugnaini%20and%20Carolina%20Tavares%20Duarte%20and%20Anna%20H.%20Reali%20Costa%20and%20Artur%20Jordao%0AAbstract%3A%20%20%20Layer%20pruning%20offers%20a%20promising%20alternative%20to%20standard%20structured%20pruning%2C%0Aeffectively%20reducing%20computational%20costs%2C%20latency%2C%20and%20memory%20footprint.%20While%0Anotable%20layer-pruning%20approaches%20aim%20to%20detect%20unimportant%20layers%20for%20removal%2C%0Athey%20often%20rely%20on%20single%20criteria%20that%20may%20not%20fully%20capture%20the%20complex%2C%0Aunderlying%20properties%20of%20layers.%20We%20propose%20a%20novel%20approach%20that%20combines%0Amultiple%20similarity%20metrics%20into%20a%20single%20expressive%20measure%20of%20low-importance%0Alayers%2C%20called%20the%20Consensus%20criterion.%20Our%20technique%20delivers%20a%20triple-win%0Asolution%3A%20low%20accuracy%20drop%2C%20high-performance%20improvement%2C%20and%20increased%0Arobustness%20to%20adversarial%20attacks.%20With%20up%20to%2078.80%25%20FLOPs%20reduction%20and%0Aperformance%20on%20par%20with%20state-of-the-art%20methods%20across%20different%20benchmarks%2C%0Aour%20approach%20reduces%20energy%20consumption%20and%20carbon%20emissions%20by%20up%20to%2066.99%25%0Aand%2068.75%25%2C%20respectively.%20Additionally%2C%20it%20avoids%20shortcut%20learning%20and%0Aimproves%20robustness%20by%20up%20to%204%20percentage%20points%20under%20various%20adversarial%0Aattacks.%20Overall%2C%20the%20Consensus%20criterion%20demonstrates%20its%20effectiveness%20in%0Acreating%20robust%2C%20efficient%2C%20and%20environmentally%20friendly%20pruned%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayer%2520Pruning%2520with%2520Consensus%253A%2520A%2520Triple-Win%2520Solution%26entry.906535625%3DLeandro%2520Giusti%2520Mugnaini%2520and%2520Carolina%2520Tavares%2520Duarte%2520and%2520Anna%2520H.%2520Reali%2520Costa%2520and%2520Artur%2520Jordao%26entry.1292438233%3D%2520%2520Layer%2520pruning%2520offers%2520a%2520promising%2520alternative%2520to%2520standard%2520structured%2520pruning%252C%250Aeffectively%2520reducing%2520computational%2520costs%252C%2520latency%252C%2520and%2520memory%2520footprint.%2520While%250Anotable%2520layer-pruning%2520approaches%2520aim%2520to%2520detect%2520unimportant%2520layers%2520for%2520removal%252C%250Athey%2520often%2520rely%2520on%2520single%2520criteria%2520that%2520may%2520not%2520fully%2520capture%2520the%2520complex%252C%250Aunderlying%2520properties%2520of%2520layers.%2520We%2520propose%2520a%2520novel%2520approach%2520that%2520combines%250Amultiple%2520similarity%2520metrics%2520into%2520a%2520single%2520expressive%2520measure%2520of%2520low-importance%250Alayers%252C%2520called%2520the%2520Consensus%2520criterion.%2520Our%2520technique%2520delivers%2520a%2520triple-win%250Asolution%253A%2520low%2520accuracy%2520drop%252C%2520high-performance%2520improvement%252C%2520and%2520increased%250Arobustness%2520to%2520adversarial%2520attacks.%2520With%2520up%2520to%252078.80%2525%2520FLOPs%2520reduction%2520and%250Aperformance%2520on%2520par%2520with%2520state-of-the-art%2520methods%2520across%2520different%2520benchmarks%252C%250Aour%2520approach%2520reduces%2520energy%2520consumption%2520and%2520carbon%2520emissions%2520by%2520up%2520to%252066.99%2525%250Aand%252068.75%2525%252C%2520respectively.%2520Additionally%252C%2520it%2520avoids%2520shortcut%2520learning%2520and%250Aimproves%2520robustness%2520by%2520up%2520to%25204%2520percentage%2520points%2520under%2520various%2520adversarial%250Aattacks.%2520Overall%252C%2520the%2520Consensus%2520criterion%2520demonstrates%2520its%2520effectiveness%2520in%250Acreating%2520robust%252C%2520efficient%252C%2520and%2520environmentally%2520friendly%2520pruned%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layer%20Pruning%20with%20Consensus%3A%20A%20Triple-Win%20Solution&entry.906535625=Leandro%20Giusti%20Mugnaini%20and%20Carolina%20Tavares%20Duarte%20and%20Anna%20H.%20Reali%20Costa%20and%20Artur%20Jordao&entry.1292438233=%20%20Layer%20pruning%20offers%20a%20promising%20alternative%20to%20standard%20structured%20pruning%2C%0Aeffectively%20reducing%20computational%20costs%2C%20latency%2C%20and%20memory%20footprint.%20While%0Anotable%20layer-pruning%20approaches%20aim%20to%20detect%20unimportant%20layers%20for%20removal%2C%0Athey%20often%20rely%20on%20single%20criteria%20that%20may%20not%20fully%20capture%20the%20complex%2C%0Aunderlying%20properties%20of%20layers.%20We%20propose%20a%20novel%20approach%20that%20combines%0Amultiple%20similarity%20metrics%20into%20a%20single%20expressive%20measure%20of%20low-importance%0Alayers%2C%20called%20the%20Consensus%20criterion.%20Our%20technique%20delivers%20a%20triple-win%0Asolution%3A%20low%20accuracy%20drop%2C%20high-performance%20improvement%2C%20and%20increased%0Arobustness%20to%20adversarial%20attacks.%20With%20up%20to%2078.80%25%20FLOPs%20reduction%20and%0Aperformance%20on%20par%20with%20state-of-the-art%20methods%20across%20different%20benchmarks%2C%0Aour%20approach%20reduces%20energy%20consumption%20and%20carbon%20emissions%20by%20up%20to%2066.99%25%0Aand%2068.75%25%2C%20respectively.%20Additionally%2C%20it%20avoids%20shortcut%20learning%20and%0Aimproves%20robustness%20by%20up%20to%204%20percentage%20points%20under%20various%20adversarial%0Aattacks.%20Overall%2C%20the%20Consensus%20criterion%20demonstrates%20its%20effectiveness%20in%0Acreating%20robust%2C%20efficient%2C%20and%20environmentally%20friendly%20pruned%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14345v1&entry.124074799=Read"},
{"title": "SEMPose: A Single End-to-end Network for Multi-object Pose Estimation", "author": "Xin Liu and Hao Wang and Shibei Xue and Dezong Zhao", "abstract": "  In computer vision, estimating the six-degree-of-freedom pose from an RGB\nimage is a fundamental task. However, this task becomes highly challenging in\nmulti-object scenes. Currently, the best methods typically employ an indirect\nstrategy, which identifies 2D and 3D correspondences, and then solves with the\nPerspective-n-Points method. Yet, this approach cannot be trained end-to-end.\nDirect methods, on the other hand, suffer from lower accuracy due to challenges\nsuch as varying object sizes and occlusions. To address these issues, we\npropose SEMPose, an end-to-end multi-object pose estimation network. SEMPose\nutilizes a well-designed texture-shape guided feature pyramid network,\neffectively tackling the challenge of object size variations. Additionally, it\nemploys an iterative refinement head structure, progressively regressing\nrotation and translation separately to enhance estimation accuracy. During\ntraining, we alleviate the impact of occlusion by selecting positive samples\nfrom visible parts. Experimental results demonstrate that SEMPose can perform\ninference at 32 FPS without requiring inputs other than the RGB image. It can\naccurately estimate the poses of multiple objects in real time, with inference\ntime unaffected by the number of target objects. On the LM-O and YCB-V\ndatasets, our method outperforms other RGB-based single-model methods,\nachieving higher accuracy. Even when compared with multi-model methods and\napproaches that use additional refinement, our results remain competitive.\n", "link": "http://arxiv.org/abs/2411.14002v1", "date": "2024-11-21", "relevancy": 2.2298, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5588}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.558}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEMPose%3A%20A%20Single%20End-to-end%20Network%20for%20Multi-object%20Pose%20Estimation&body=Title%3A%20SEMPose%3A%20A%20Single%20End-to-end%20Network%20for%20Multi-object%20Pose%20Estimation%0AAuthor%3A%20Xin%20Liu%20and%20Hao%20Wang%20and%20Shibei%20Xue%20and%20Dezong%20Zhao%0AAbstract%3A%20%20%20In%20computer%20vision%2C%20estimating%20the%20six-degree-of-freedom%20pose%20from%20an%20RGB%0Aimage%20is%20a%20fundamental%20task.%20However%2C%20this%20task%20becomes%20highly%20challenging%20in%0Amulti-object%20scenes.%20Currently%2C%20the%20best%20methods%20typically%20employ%20an%20indirect%0Astrategy%2C%20which%20identifies%202D%20and%203D%20correspondences%2C%20and%20then%20solves%20with%20the%0APerspective-n-Points%20method.%20Yet%2C%20this%20approach%20cannot%20be%20trained%20end-to-end.%0ADirect%20methods%2C%20on%20the%20other%20hand%2C%20suffer%20from%20lower%20accuracy%20due%20to%20challenges%0Asuch%20as%20varying%20object%20sizes%20and%20occlusions.%20To%20address%20these%20issues%2C%20we%0Apropose%20SEMPose%2C%20an%20end-to-end%20multi-object%20pose%20estimation%20network.%20SEMPose%0Autilizes%20a%20well-designed%20texture-shape%20guided%20feature%20pyramid%20network%2C%0Aeffectively%20tackling%20the%20challenge%20of%20object%20size%20variations.%20Additionally%2C%20it%0Aemploys%20an%20iterative%20refinement%20head%20structure%2C%20progressively%20regressing%0Arotation%20and%20translation%20separately%20to%20enhance%20estimation%20accuracy.%20During%0Atraining%2C%20we%20alleviate%20the%20impact%20of%20occlusion%20by%20selecting%20positive%20samples%0Afrom%20visible%20parts.%20Experimental%20results%20demonstrate%20that%20SEMPose%20can%20perform%0Ainference%20at%2032%20FPS%20without%20requiring%20inputs%20other%20than%20the%20RGB%20image.%20It%20can%0Aaccurately%20estimate%20the%20poses%20of%20multiple%20objects%20in%20real%20time%2C%20with%20inference%0Atime%20unaffected%20by%20the%20number%20of%20target%20objects.%20On%20the%20LM-O%20and%20YCB-V%0Adatasets%2C%20our%20method%20outperforms%20other%20RGB-based%20single-model%20methods%2C%0Aachieving%20higher%20accuracy.%20Even%20when%20compared%20with%20multi-model%20methods%20and%0Aapproaches%20that%20use%20additional%20refinement%2C%20our%20results%20remain%20competitive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEMPose%253A%2520A%2520Single%2520End-to-end%2520Network%2520for%2520Multi-object%2520Pose%2520Estimation%26entry.906535625%3DXin%2520Liu%2520and%2520Hao%2520Wang%2520and%2520Shibei%2520Xue%2520and%2520Dezong%2520Zhao%26entry.1292438233%3D%2520%2520In%2520computer%2520vision%252C%2520estimating%2520the%2520six-degree-of-freedom%2520pose%2520from%2520an%2520RGB%250Aimage%2520is%2520a%2520fundamental%2520task.%2520However%252C%2520this%2520task%2520becomes%2520highly%2520challenging%2520in%250Amulti-object%2520scenes.%2520Currently%252C%2520the%2520best%2520methods%2520typically%2520employ%2520an%2520indirect%250Astrategy%252C%2520which%2520identifies%25202D%2520and%25203D%2520correspondences%252C%2520and%2520then%2520solves%2520with%2520the%250APerspective-n-Points%2520method.%2520Yet%252C%2520this%2520approach%2520cannot%2520be%2520trained%2520end-to-end.%250ADirect%2520methods%252C%2520on%2520the%2520other%2520hand%252C%2520suffer%2520from%2520lower%2520accuracy%2520due%2520to%2520challenges%250Asuch%2520as%2520varying%2520object%2520sizes%2520and%2520occlusions.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520SEMPose%252C%2520an%2520end-to-end%2520multi-object%2520pose%2520estimation%2520network.%2520SEMPose%250Autilizes%2520a%2520well-designed%2520texture-shape%2520guided%2520feature%2520pyramid%2520network%252C%250Aeffectively%2520tackling%2520the%2520challenge%2520of%2520object%2520size%2520variations.%2520Additionally%252C%2520it%250Aemploys%2520an%2520iterative%2520refinement%2520head%2520structure%252C%2520progressively%2520regressing%250Arotation%2520and%2520translation%2520separately%2520to%2520enhance%2520estimation%2520accuracy.%2520During%250Atraining%252C%2520we%2520alleviate%2520the%2520impact%2520of%2520occlusion%2520by%2520selecting%2520positive%2520samples%250Afrom%2520visible%2520parts.%2520Experimental%2520results%2520demonstrate%2520that%2520SEMPose%2520can%2520perform%250Ainference%2520at%252032%2520FPS%2520without%2520requiring%2520inputs%2520other%2520than%2520the%2520RGB%2520image.%2520It%2520can%250Aaccurately%2520estimate%2520the%2520poses%2520of%2520multiple%2520objects%2520in%2520real%2520time%252C%2520with%2520inference%250Atime%2520unaffected%2520by%2520the%2520number%2520of%2520target%2520objects.%2520On%2520the%2520LM-O%2520and%2520YCB-V%250Adatasets%252C%2520our%2520method%2520outperforms%2520other%2520RGB-based%2520single-model%2520methods%252C%250Aachieving%2520higher%2520accuracy.%2520Even%2520when%2520compared%2520with%2520multi-model%2520methods%2520and%250Aapproaches%2520that%2520use%2520additional%2520refinement%252C%2520our%2520results%2520remain%2520competitive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEMPose%3A%20A%20Single%20End-to-end%20Network%20for%20Multi-object%20Pose%20Estimation&entry.906535625=Xin%20Liu%20and%20Hao%20Wang%20and%20Shibei%20Xue%20and%20Dezong%20Zhao&entry.1292438233=%20%20In%20computer%20vision%2C%20estimating%20the%20six-degree-of-freedom%20pose%20from%20an%20RGB%0Aimage%20is%20a%20fundamental%20task.%20However%2C%20this%20task%20becomes%20highly%20challenging%20in%0Amulti-object%20scenes.%20Currently%2C%20the%20best%20methods%20typically%20employ%20an%20indirect%0Astrategy%2C%20which%20identifies%202D%20and%203D%20correspondences%2C%20and%20then%20solves%20with%20the%0APerspective-n-Points%20method.%20Yet%2C%20this%20approach%20cannot%20be%20trained%20end-to-end.%0ADirect%20methods%2C%20on%20the%20other%20hand%2C%20suffer%20from%20lower%20accuracy%20due%20to%20challenges%0Asuch%20as%20varying%20object%20sizes%20and%20occlusions.%20To%20address%20these%20issues%2C%20we%0Apropose%20SEMPose%2C%20an%20end-to-end%20multi-object%20pose%20estimation%20network.%20SEMPose%0Autilizes%20a%20well-designed%20texture-shape%20guided%20feature%20pyramid%20network%2C%0Aeffectively%20tackling%20the%20challenge%20of%20object%20size%20variations.%20Additionally%2C%20it%0Aemploys%20an%20iterative%20refinement%20head%20structure%2C%20progressively%20regressing%0Arotation%20and%20translation%20separately%20to%20enhance%20estimation%20accuracy.%20During%0Atraining%2C%20we%20alleviate%20the%20impact%20of%20occlusion%20by%20selecting%20positive%20samples%0Afrom%20visible%20parts.%20Experimental%20results%20demonstrate%20that%20SEMPose%20can%20perform%0Ainference%20at%2032%20FPS%20without%20requiring%20inputs%20other%20than%20the%20RGB%20image.%20It%20can%0Aaccurately%20estimate%20the%20poses%20of%20multiple%20objects%20in%20real%20time%2C%20with%20inference%0Atime%20unaffected%20by%20the%20number%20of%20target%20objects.%20On%20the%20LM-O%20and%20YCB-V%0Adatasets%2C%20our%20method%20outperforms%20other%20RGB-based%20single-model%20methods%2C%0Aachieving%20higher%20accuracy.%20Even%20when%20compared%20with%20multi-model%20methods%20and%0Aapproaches%20that%20use%20additional%20refinement%2C%20our%20results%20remain%20competitive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14002v1&entry.124074799=Read"},
{"title": "HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset", "author": "Shivam Saini and J\u00fcrgen Peissig", "abstract": "  This contribution introduces a dataset of 7th-order Ambisonic Room Impulse\nResponses (HOA-RIRs), created using the Image Source Method. By employing\nhigher-order Ambisonics, our dataset enables precise spatial audio\nreproduction, a critical requirement for realistic immersive audio\napplications. Leveraging the virtual simulation, we present a unique microphone\nconfiguration, based on the superposition principle, designed to optimize sound\nfield coverage while addressing the limitations of traditional microphone\narrays. The presented 64-microphone configuration allows us to capture RIRs\ndirectly in the Spherical Harmonics domain. The dataset features a wide range\nof room configurations, encompassing variations in room geometry, acoustic\nabsorption materials, and source-receiver distances. A detailed description of\nthe simulation setup is provided alongside for an accurate reproduction. The\ndataset serves as a vital resource for researchers working on spatial audio,\nparticularly in applications involving machine learning to improve room\nacoustics modeling and sound field synthesis. It further provides a very high\nlevel of spatial resolution and realism crucial for tasks such as source\nlocalization, reverberation prediction, and immersive sound reproduction.\n", "link": "http://arxiv.org/abs/2411.14207v1", "date": "2024-11-21", "relevancy": 2.2045, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4555}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4336}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARP%3A%20A%20Large-Scale%20Higher-Order%20Ambisonic%20Room%20Impulse%20Response%20Dataset&body=Title%3A%20HARP%3A%20A%20Large-Scale%20Higher-Order%20Ambisonic%20Room%20Impulse%20Response%20Dataset%0AAuthor%3A%20Shivam%20Saini%20and%20J%C3%BCrgen%20Peissig%0AAbstract%3A%20%20%20This%20contribution%20introduces%20a%20dataset%20of%207th-order%20Ambisonic%20Room%20Impulse%0AResponses%20%28HOA-RIRs%29%2C%20created%20using%20the%20Image%20Source%20Method.%20By%20employing%0Ahigher-order%20Ambisonics%2C%20our%20dataset%20enables%20precise%20spatial%20audio%0Areproduction%2C%20a%20critical%20requirement%20for%20realistic%20immersive%20audio%0Aapplications.%20Leveraging%20the%20virtual%20simulation%2C%20we%20present%20a%20unique%20microphone%0Aconfiguration%2C%20based%20on%20the%20superposition%20principle%2C%20designed%20to%20optimize%20sound%0Afield%20coverage%20while%20addressing%20the%20limitations%20of%20traditional%20microphone%0Aarrays.%20The%20presented%2064-microphone%20configuration%20allows%20us%20to%20capture%20RIRs%0Adirectly%20in%20the%20Spherical%20Harmonics%20domain.%20The%20dataset%20features%20a%20wide%20range%0Aof%20room%20configurations%2C%20encompassing%20variations%20in%20room%20geometry%2C%20acoustic%0Aabsorption%20materials%2C%20and%20source-receiver%20distances.%20A%20detailed%20description%20of%0Athe%20simulation%20setup%20is%20provided%20alongside%20for%20an%20accurate%20reproduction.%20The%0Adataset%20serves%20as%20a%20vital%20resource%20for%20researchers%20working%20on%20spatial%20audio%2C%0Aparticularly%20in%20applications%20involving%20machine%20learning%20to%20improve%20room%0Aacoustics%20modeling%20and%20sound%20field%20synthesis.%20It%20further%20provides%20a%20very%20high%0Alevel%20of%20spatial%20resolution%20and%20realism%20crucial%20for%20tasks%20such%20as%20source%0Alocalization%2C%20reverberation%20prediction%2C%20and%20immersive%20sound%20reproduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARP%253A%2520A%2520Large-Scale%2520Higher-Order%2520Ambisonic%2520Room%2520Impulse%2520Response%2520Dataset%26entry.906535625%3DShivam%2520Saini%2520and%2520J%25C3%25BCrgen%2520Peissig%26entry.1292438233%3D%2520%2520This%2520contribution%2520introduces%2520a%2520dataset%2520of%25207th-order%2520Ambisonic%2520Room%2520Impulse%250AResponses%2520%2528HOA-RIRs%2529%252C%2520created%2520using%2520the%2520Image%2520Source%2520Method.%2520By%2520employing%250Ahigher-order%2520Ambisonics%252C%2520our%2520dataset%2520enables%2520precise%2520spatial%2520audio%250Areproduction%252C%2520a%2520critical%2520requirement%2520for%2520realistic%2520immersive%2520audio%250Aapplications.%2520Leveraging%2520the%2520virtual%2520simulation%252C%2520we%2520present%2520a%2520unique%2520microphone%250Aconfiguration%252C%2520based%2520on%2520the%2520superposition%2520principle%252C%2520designed%2520to%2520optimize%2520sound%250Afield%2520coverage%2520while%2520addressing%2520the%2520limitations%2520of%2520traditional%2520microphone%250Aarrays.%2520The%2520presented%252064-microphone%2520configuration%2520allows%2520us%2520to%2520capture%2520RIRs%250Adirectly%2520in%2520the%2520Spherical%2520Harmonics%2520domain.%2520The%2520dataset%2520features%2520a%2520wide%2520range%250Aof%2520room%2520configurations%252C%2520encompassing%2520variations%2520in%2520room%2520geometry%252C%2520acoustic%250Aabsorption%2520materials%252C%2520and%2520source-receiver%2520distances.%2520A%2520detailed%2520description%2520of%250Athe%2520simulation%2520setup%2520is%2520provided%2520alongside%2520for%2520an%2520accurate%2520reproduction.%2520The%250Adataset%2520serves%2520as%2520a%2520vital%2520resource%2520for%2520researchers%2520working%2520on%2520spatial%2520audio%252C%250Aparticularly%2520in%2520applications%2520involving%2520machine%2520learning%2520to%2520improve%2520room%250Aacoustics%2520modeling%2520and%2520sound%2520field%2520synthesis.%2520It%2520further%2520provides%2520a%2520very%2520high%250Alevel%2520of%2520spatial%2520resolution%2520and%2520realism%2520crucial%2520for%2520tasks%2520such%2520as%2520source%250Alocalization%252C%2520reverberation%2520prediction%252C%2520and%2520immersive%2520sound%2520reproduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARP%3A%20A%20Large-Scale%20Higher-Order%20Ambisonic%20Room%20Impulse%20Response%20Dataset&entry.906535625=Shivam%20Saini%20and%20J%C3%BCrgen%20Peissig&entry.1292438233=%20%20This%20contribution%20introduces%20a%20dataset%20of%207th-order%20Ambisonic%20Room%20Impulse%0AResponses%20%28HOA-RIRs%29%2C%20created%20using%20the%20Image%20Source%20Method.%20By%20employing%0Ahigher-order%20Ambisonics%2C%20our%20dataset%20enables%20precise%20spatial%20audio%0Areproduction%2C%20a%20critical%20requirement%20for%20realistic%20immersive%20audio%0Aapplications.%20Leveraging%20the%20virtual%20simulation%2C%20we%20present%20a%20unique%20microphone%0Aconfiguration%2C%20based%20on%20the%20superposition%20principle%2C%20designed%20to%20optimize%20sound%0Afield%20coverage%20while%20addressing%20the%20limitations%20of%20traditional%20microphone%0Aarrays.%20The%20presented%2064-microphone%20configuration%20allows%20us%20to%20capture%20RIRs%0Adirectly%20in%20the%20Spherical%20Harmonics%20domain.%20The%20dataset%20features%20a%20wide%20range%0Aof%20room%20configurations%2C%20encompassing%20variations%20in%20room%20geometry%2C%20acoustic%0Aabsorption%20materials%2C%20and%20source-receiver%20distances.%20A%20detailed%20description%20of%0Athe%20simulation%20setup%20is%20provided%20alongside%20for%20an%20accurate%20reproduction.%20The%0Adataset%20serves%20as%20a%20vital%20resource%20for%20researchers%20working%20on%20spatial%20audio%2C%0Aparticularly%20in%20applications%20involving%20machine%20learning%20to%20improve%20room%0Aacoustics%20modeling%20and%20sound%20field%20synthesis.%20It%20further%20provides%20a%20very%20high%0Alevel%20of%20spatial%20resolution%20and%20realism%20crucial%20for%20tasks%20such%20as%20source%0Alocalization%2C%20reverberation%20prediction%2C%20and%20immersive%20sound%20reproduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14207v1&entry.124074799=Read"},
{"title": "Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning", "author": "Andy Li and Aiden Durrant and Milan Markovic and Lu Yin and Georgios Leontidis", "abstract": "  Pruning of deep neural networks has been an effective technique for reducing\nmodel size while preserving most of the performance of dense networks, crucial\nfor deploying models on memory and power-constrained devices. While recent\nsparse learning methods have shown promising performance up to moderate\nsparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing\nsparsities to extreme levels. Obtaining sparse networks at such extreme\nsparsity levels presents unique challenges, such as fragile gradient flow and\nheightened risk of layer collapse. In this work, we explore network performance\nbeyond the commonly studied sparsities, and propose a collection of techniques\nthat enable the continuous learning of networks without accuracy collapse even\nat extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNet\narchitectures. Our approach combines 1) Dynamic ReLU phasing, where DyReLU\ninitially allows for richer parameter exploration before being gradually\nreplaced by standard ReLU, 2) weight sharing which reuses parameters within a\nresidual layer while maintaining the same number of learnable parameters, and\n3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve\ndynamically throughout training to better encourage parameter exploration. We\nevaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at\nextreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and\nImageNet, achieving significant performance improvements over state-of-the-art\nmethods we compared with.\n", "link": "http://arxiv.org/abs/2411.13545v2", "date": "2024-11-21", "relevancy": 2.1967, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5762}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5358}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20Limits%20of%20Sparsity%3A%20A%20Bag%20of%20Tricks%20for%20Extreme%20Pruning&body=Title%3A%20Pushing%20the%20Limits%20of%20Sparsity%3A%20A%20Bag%20of%20Tricks%20for%20Extreme%20Pruning%0AAuthor%3A%20Andy%20Li%20and%20Aiden%20Durrant%20and%20Milan%20Markovic%20and%20Lu%20Yin%20and%20Georgios%20Leontidis%0AAbstract%3A%20%20%20Pruning%20of%20deep%20neural%20networks%20has%20been%20an%20effective%20technique%20for%20reducing%0Amodel%20size%20while%20preserving%20most%20of%20the%20performance%20of%20dense%20networks%2C%20crucial%0Afor%20deploying%20models%20on%20memory%20and%20power-constrained%20devices.%20While%20recent%0Asparse%20learning%20methods%20have%20shown%20promising%20performance%20up%20to%20moderate%0Asparsity%20levels%20such%20as%2095%25%20and%2098%25%2C%20accuracy%20quickly%20deteriorates%20when%20pushing%0Asparsities%20to%20extreme%20levels.%20Obtaining%20sparse%20networks%20at%20such%20extreme%0Asparsity%20levels%20presents%20unique%20challenges%2C%20such%20as%20fragile%20gradient%20flow%20and%0Aheightened%20risk%20of%20layer%20collapse.%20In%20this%20work%2C%20we%20explore%20network%20performance%0Abeyond%20the%20commonly%20studied%20sparsities%2C%20and%20propose%20a%20collection%20of%20techniques%0Athat%20enable%20the%20continuous%20learning%20of%20networks%20without%20accuracy%20collapse%20even%0Aat%20extreme%20sparsities%2C%20including%2099.90%25%2C%2099.95%25%20and%2099.99%25%20on%20ResNet%0Aarchitectures.%20Our%20approach%20combines%201%29%20Dynamic%20ReLU%20phasing%2C%20where%20DyReLU%0Ainitially%20allows%20for%20richer%20parameter%20exploration%20before%20being%20gradually%0Areplaced%20by%20standard%20ReLU%2C%202%29%20weight%20sharing%20which%20reuses%20parameters%20within%20a%0Aresidual%20layer%20while%20maintaining%20the%20same%20number%20of%20learnable%20parameters%2C%20and%0A3%29%20cyclic%20sparsity%2C%20where%20both%20sparsity%20levels%20and%20sparsity%20patterns%20evolve%0Adynamically%20throughout%20training%20to%20better%20encourage%20parameter%20exploration.%20We%0Aevaluate%20our%20method%2C%20which%20we%20term%20Extreme%20Adaptive%20Sparse%20Training%20%28EAST%29%20at%0Aextreme%20sparsities%20using%20ResNet-34%20and%20ResNet-50%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet%2C%20achieving%20significant%20performance%20improvements%20over%20state-of-the-art%0Amethods%20we%20compared%20with.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520Limits%2520of%2520Sparsity%253A%2520A%2520Bag%2520of%2520Tricks%2520for%2520Extreme%2520Pruning%26entry.906535625%3DAndy%2520Li%2520and%2520Aiden%2520Durrant%2520and%2520Milan%2520Markovic%2520and%2520Lu%2520Yin%2520and%2520Georgios%2520Leontidis%26entry.1292438233%3D%2520%2520Pruning%2520of%2520deep%2520neural%2520networks%2520has%2520been%2520an%2520effective%2520technique%2520for%2520reducing%250Amodel%2520size%2520while%2520preserving%2520most%2520of%2520the%2520performance%2520of%2520dense%2520networks%252C%2520crucial%250Afor%2520deploying%2520models%2520on%2520memory%2520and%2520power-constrained%2520devices.%2520While%2520recent%250Asparse%2520learning%2520methods%2520have%2520shown%2520promising%2520performance%2520up%2520to%2520moderate%250Asparsity%2520levels%2520such%2520as%252095%2525%2520and%252098%2525%252C%2520accuracy%2520quickly%2520deteriorates%2520when%2520pushing%250Asparsities%2520to%2520extreme%2520levels.%2520Obtaining%2520sparse%2520networks%2520at%2520such%2520extreme%250Asparsity%2520levels%2520presents%2520unique%2520challenges%252C%2520such%2520as%2520fragile%2520gradient%2520flow%2520and%250Aheightened%2520risk%2520of%2520layer%2520collapse.%2520In%2520this%2520work%252C%2520we%2520explore%2520network%2520performance%250Abeyond%2520the%2520commonly%2520studied%2520sparsities%252C%2520and%2520propose%2520a%2520collection%2520of%2520techniques%250Athat%2520enable%2520the%2520continuous%2520learning%2520of%2520networks%2520without%2520accuracy%2520collapse%2520even%250Aat%2520extreme%2520sparsities%252C%2520including%252099.90%2525%252C%252099.95%2525%2520and%252099.99%2525%2520on%2520ResNet%250Aarchitectures.%2520Our%2520approach%2520combines%25201%2529%2520Dynamic%2520ReLU%2520phasing%252C%2520where%2520DyReLU%250Ainitially%2520allows%2520for%2520richer%2520parameter%2520exploration%2520before%2520being%2520gradually%250Areplaced%2520by%2520standard%2520ReLU%252C%25202%2529%2520weight%2520sharing%2520which%2520reuses%2520parameters%2520within%2520a%250Aresidual%2520layer%2520while%2520maintaining%2520the%2520same%2520number%2520of%2520learnable%2520parameters%252C%2520and%250A3%2529%2520cyclic%2520sparsity%252C%2520where%2520both%2520sparsity%2520levels%2520and%2520sparsity%2520patterns%2520evolve%250Adynamically%2520throughout%2520training%2520to%2520better%2520encourage%2520parameter%2520exploration.%2520We%250Aevaluate%2520our%2520method%252C%2520which%2520we%2520term%2520Extreme%2520Adaptive%2520Sparse%2520Training%2520%2528EAST%2529%2520at%250Aextreme%2520sparsities%2520using%2520ResNet-34%2520and%2520ResNet-50%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%250AImageNet%252C%2520achieving%2520significant%2520performance%2520improvements%2520over%2520state-of-the-art%250Amethods%2520we%2520compared%2520with.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20Limits%20of%20Sparsity%3A%20A%20Bag%20of%20Tricks%20for%20Extreme%20Pruning&entry.906535625=Andy%20Li%20and%20Aiden%20Durrant%20and%20Milan%20Markovic%20and%20Lu%20Yin%20and%20Georgios%20Leontidis&entry.1292438233=%20%20Pruning%20of%20deep%20neural%20networks%20has%20been%20an%20effective%20technique%20for%20reducing%0Amodel%20size%20while%20preserving%20most%20of%20the%20performance%20of%20dense%20networks%2C%20crucial%0Afor%20deploying%20models%20on%20memory%20and%20power-constrained%20devices.%20While%20recent%0Asparse%20learning%20methods%20have%20shown%20promising%20performance%20up%20to%20moderate%0Asparsity%20levels%20such%20as%2095%25%20and%2098%25%2C%20accuracy%20quickly%20deteriorates%20when%20pushing%0Asparsities%20to%20extreme%20levels.%20Obtaining%20sparse%20networks%20at%20such%20extreme%0Asparsity%20levels%20presents%20unique%20challenges%2C%20such%20as%20fragile%20gradient%20flow%20and%0Aheightened%20risk%20of%20layer%20collapse.%20In%20this%20work%2C%20we%20explore%20network%20performance%0Abeyond%20the%20commonly%20studied%20sparsities%2C%20and%20propose%20a%20collection%20of%20techniques%0Athat%20enable%20the%20continuous%20learning%20of%20networks%20without%20accuracy%20collapse%20even%0Aat%20extreme%20sparsities%2C%20including%2099.90%25%2C%2099.95%25%20and%2099.99%25%20on%20ResNet%0Aarchitectures.%20Our%20approach%20combines%201%29%20Dynamic%20ReLU%20phasing%2C%20where%20DyReLU%0Ainitially%20allows%20for%20richer%20parameter%20exploration%20before%20being%20gradually%0Areplaced%20by%20standard%20ReLU%2C%202%29%20weight%20sharing%20which%20reuses%20parameters%20within%20a%0Aresidual%20layer%20while%20maintaining%20the%20same%20number%20of%20learnable%20parameters%2C%20and%0A3%29%20cyclic%20sparsity%2C%20where%20both%20sparsity%20levels%20and%20sparsity%20patterns%20evolve%0Adynamically%20throughout%20training%20to%20better%20encourage%20parameter%20exploration.%20We%0Aevaluate%20our%20method%2C%20which%20we%20term%20Extreme%20Adaptive%20Sparse%20Training%20%28EAST%29%20at%0Aextreme%20sparsities%20using%20ResNet-34%20and%20ResNet-50%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet%2C%20achieving%20significant%20performance%20improvements%20over%20state-of-the-art%0Amethods%20we%20compared%20with.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13545v2&entry.124074799=Read"},
{"title": "ComfyGI: Automatic Improvement of Image Generation Workflows", "author": "Dominik Sobania and Martin Briesch and Franz Rothlauf", "abstract": "  Automatic image generation is no longer just of interest to researchers, but\nalso to practitioners. However, current models are sensitive to the settings\nused and automatic optimization methods often require human involvement. To\nbridge this gap, we introduce ComfyGI, a novel approach to automatically\nimprove workflows for image generation without the need for human intervention\ndriven by techniques from genetic improvement. This enables image generation\nwith significantly higher quality in terms of the alignment with the given\ndescription and the perceived aesthetics. On the performance side, we find that\noverall, the images generated with an optimized workflow are about 50% better\ncompared to the initial workflow in terms of the median ImageReward score.\nThese already good results are even surpassed in our human evaluation, as the\nparticipants preferred the images improved by ComfyGI in around 90% of the\ncases.\n", "link": "http://arxiv.org/abs/2411.14193v1", "date": "2024-11-21", "relevancy": 2.195, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5736}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5454}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComfyGI%3A%20Automatic%20Improvement%20of%20Image%20Generation%20Workflows&body=Title%3A%20ComfyGI%3A%20Automatic%20Improvement%20of%20Image%20Generation%20Workflows%0AAuthor%3A%20Dominik%20Sobania%20and%20Martin%20Briesch%20and%20Franz%20Rothlauf%0AAbstract%3A%20%20%20Automatic%20image%20generation%20is%20no%20longer%20just%20of%20interest%20to%20researchers%2C%20but%0Aalso%20to%20practitioners.%20However%2C%20current%20models%20are%20sensitive%20to%20the%20settings%0Aused%20and%20automatic%20optimization%20methods%20often%20require%20human%20involvement.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20ComfyGI%2C%20a%20novel%20approach%20to%20automatically%0Aimprove%20workflows%20for%20image%20generation%20without%20the%20need%20for%20human%20intervention%0Adriven%20by%20techniques%20from%20genetic%20improvement.%20This%20enables%20image%20generation%0Awith%20significantly%20higher%20quality%20in%20terms%20of%20the%20alignment%20with%20the%20given%0Adescription%20and%20the%20perceived%20aesthetics.%20On%20the%20performance%20side%2C%20we%20find%20that%0Aoverall%2C%20the%20images%20generated%20with%20an%20optimized%20workflow%20are%20about%2050%25%20better%0Acompared%20to%20the%20initial%20workflow%20in%20terms%20of%20the%20median%20ImageReward%20score.%0AThese%20already%20good%20results%20are%20even%20surpassed%20in%20our%20human%20evaluation%2C%20as%20the%0Aparticipants%20preferred%20the%20images%20improved%20by%20ComfyGI%20in%20around%2090%25%20of%20the%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComfyGI%253A%2520Automatic%2520Improvement%2520of%2520Image%2520Generation%2520Workflows%26entry.906535625%3DDominik%2520Sobania%2520and%2520Martin%2520Briesch%2520and%2520Franz%2520Rothlauf%26entry.1292438233%3D%2520%2520Automatic%2520image%2520generation%2520is%2520no%2520longer%2520just%2520of%2520interest%2520to%2520researchers%252C%2520but%250Aalso%2520to%2520practitioners.%2520However%252C%2520current%2520models%2520are%2520sensitive%2520to%2520the%2520settings%250Aused%2520and%2520automatic%2520optimization%2520methods%2520often%2520require%2520human%2520involvement.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520ComfyGI%252C%2520a%2520novel%2520approach%2520to%2520automatically%250Aimprove%2520workflows%2520for%2520image%2520generation%2520without%2520the%2520need%2520for%2520human%2520intervention%250Adriven%2520by%2520techniques%2520from%2520genetic%2520improvement.%2520This%2520enables%2520image%2520generation%250Awith%2520significantly%2520higher%2520quality%2520in%2520terms%2520of%2520the%2520alignment%2520with%2520the%2520given%250Adescription%2520and%2520the%2520perceived%2520aesthetics.%2520On%2520the%2520performance%2520side%252C%2520we%2520find%2520that%250Aoverall%252C%2520the%2520images%2520generated%2520with%2520an%2520optimized%2520workflow%2520are%2520about%252050%2525%2520better%250Acompared%2520to%2520the%2520initial%2520workflow%2520in%2520terms%2520of%2520the%2520median%2520ImageReward%2520score.%250AThese%2520already%2520good%2520results%2520are%2520even%2520surpassed%2520in%2520our%2520human%2520evaluation%252C%2520as%2520the%250Aparticipants%2520preferred%2520the%2520images%2520improved%2520by%2520ComfyGI%2520in%2520around%252090%2525%2520of%2520the%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComfyGI%3A%20Automatic%20Improvement%20of%20Image%20Generation%20Workflows&entry.906535625=Dominik%20Sobania%20and%20Martin%20Briesch%20and%20Franz%20Rothlauf&entry.1292438233=%20%20Automatic%20image%20generation%20is%20no%20longer%20just%20of%20interest%20to%20researchers%2C%20but%0Aalso%20to%20practitioners.%20However%2C%20current%20models%20are%20sensitive%20to%20the%20settings%0Aused%20and%20automatic%20optimization%20methods%20often%20require%20human%20involvement.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20ComfyGI%2C%20a%20novel%20approach%20to%20automatically%0Aimprove%20workflows%20for%20image%20generation%20without%20the%20need%20for%20human%20intervention%0Adriven%20by%20techniques%20from%20genetic%20improvement.%20This%20enables%20image%20generation%0Awith%20significantly%20higher%20quality%20in%20terms%20of%20the%20alignment%20with%20the%20given%0Adescription%20and%20the%20perceived%20aesthetics.%20On%20the%20performance%20side%2C%20we%20find%20that%0Aoverall%2C%20the%20images%20generated%20with%20an%20optimized%20workflow%20are%20about%2050%25%20better%0Acompared%20to%20the%20initial%20workflow%20in%20terms%20of%20the%20median%20ImageReward%20score.%0AThese%20already%20good%20results%20are%20even%20surpassed%20in%20our%20human%20evaluation%2C%20as%20the%0Aparticipants%20preferred%20the%20images%20improved%20by%20ComfyGI%20in%20around%2090%25%20of%20the%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14193v1&entry.124074799=Read"},
{"title": "Accelerating Gaussian Variational Inference for Motion Planning Under\n  Uncertainty", "author": "Zinuo Chang and Hongzhe Yu and Patricio Vela and Yongxin Chen", "abstract": "  This work addresses motion planning under uncertainty as a stochastic optimal\ncontrol problem. The path distribution induced by the optimal controller\ncorresponds to a posterior path distribution with a known form. To approximate\nthis posterior, we frame an optimization problem in the space of Gaussian\ndistributions, which aligns with the Gaussian Variational Inference Motion\nPlanning (GVIMP) paradigm introduced in \\cite{yu2023gaussian}. In this\nframework, the computation bottleneck lies in evaluating the expectation of\ncollision costs over a dense discretized trajectory and computing the marginal\ncovariances. This work exploits the sparse motion planning factor graph, which\nallows for parallel computing collision costs and Gaussian Belief Propagation\n(GBP) marginal covariance computation, to introduce a computationally efficient\napproach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian\nVariational Inference Motion Planning (P-GVIMP). We validate the proposed\nframework on various robotic systems, demonstrating significant speed\nacceleration achieved by leveraging Graphics Processing Units (GPUs) for\nparallel computation. An open-sourced implementation is presented at\nhttps://github.com/hzyu17/VIMP.\n", "link": "http://arxiv.org/abs/2411.03416v2", "date": "2024-11-21", "relevancy": 2.1927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5509}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5464}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Gaussian%20Variational%20Inference%20for%20Motion%20Planning%20Under%0A%20%20Uncertainty&body=Title%3A%20Accelerating%20Gaussian%20Variational%20Inference%20for%20Motion%20Planning%20Under%0A%20%20Uncertainty%0AAuthor%3A%20Zinuo%20Chang%20and%20Hongzhe%20Yu%20and%20Patricio%20Vela%20and%20Yongxin%20Chen%0AAbstract%3A%20%20%20This%20work%20addresses%20motion%20planning%20under%20uncertainty%20as%20a%20stochastic%20optimal%0Acontrol%20problem.%20The%20path%20distribution%20induced%20by%20the%20optimal%20controller%0Acorresponds%20to%20a%20posterior%20path%20distribution%20with%20a%20known%20form.%20To%20approximate%0Athis%20posterior%2C%20we%20frame%20an%20optimization%20problem%20in%20the%20space%20of%20Gaussian%0Adistributions%2C%20which%20aligns%20with%20the%20Gaussian%20Variational%20Inference%20Motion%0APlanning%20%28GVIMP%29%20paradigm%20introduced%20in%20%5Ccite%7Byu2023gaussian%7D.%20In%20this%0Aframework%2C%20the%20computation%20bottleneck%20lies%20in%20evaluating%20the%20expectation%20of%0Acollision%20costs%20over%20a%20dense%20discretized%20trajectory%20and%20computing%20the%20marginal%0Acovariances.%20This%20work%20exploits%20the%20sparse%20motion%20planning%20factor%20graph%2C%20which%0Aallows%20for%20parallel%20computing%20collision%20costs%20and%20Gaussian%20Belief%20Propagation%0A%28GBP%29%20marginal%20covariance%20computation%2C%20to%20introduce%20a%20computationally%20efficient%0Aapproach%20to%20solving%20GVIMP.%20We%20term%20the%20novel%20paradigm%20as%20the%20Parallel%20Gaussian%0AVariational%20Inference%20Motion%20Planning%20%28P-GVIMP%29.%20We%20validate%20the%20proposed%0Aframework%20on%20various%20robotic%20systems%2C%20demonstrating%20significant%20speed%0Aacceleration%20achieved%20by%20leveraging%20Graphics%20Processing%20Units%20%28GPUs%29%20for%0Aparallel%20computation.%20An%20open-sourced%20implementation%20is%20presented%20at%0Ahttps%3A//github.com/hzyu17/VIMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Gaussian%2520Variational%2520Inference%2520for%2520Motion%2520Planning%2520Under%250A%2520%2520Uncertainty%26entry.906535625%3DZinuo%2520Chang%2520and%2520Hongzhe%2520Yu%2520and%2520Patricio%2520Vela%2520and%2520Yongxin%2520Chen%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520motion%2520planning%2520under%2520uncertainty%2520as%2520a%2520stochastic%2520optimal%250Acontrol%2520problem.%2520The%2520path%2520distribution%2520induced%2520by%2520the%2520optimal%2520controller%250Acorresponds%2520to%2520a%2520posterior%2520path%2520distribution%2520with%2520a%2520known%2520form.%2520To%2520approximate%250Athis%2520posterior%252C%2520we%2520frame%2520an%2520optimization%2520problem%2520in%2520the%2520space%2520of%2520Gaussian%250Adistributions%252C%2520which%2520aligns%2520with%2520the%2520Gaussian%2520Variational%2520Inference%2520Motion%250APlanning%2520%2528GVIMP%2529%2520paradigm%2520introduced%2520in%2520%255Ccite%257Byu2023gaussian%257D.%2520In%2520this%250Aframework%252C%2520the%2520computation%2520bottleneck%2520lies%2520in%2520evaluating%2520the%2520expectation%2520of%250Acollision%2520costs%2520over%2520a%2520dense%2520discretized%2520trajectory%2520and%2520computing%2520the%2520marginal%250Acovariances.%2520This%2520work%2520exploits%2520the%2520sparse%2520motion%2520planning%2520factor%2520graph%252C%2520which%250Aallows%2520for%2520parallel%2520computing%2520collision%2520costs%2520and%2520Gaussian%2520Belief%2520Propagation%250A%2528GBP%2529%2520marginal%2520covariance%2520computation%252C%2520to%2520introduce%2520a%2520computationally%2520efficient%250Aapproach%2520to%2520solving%2520GVIMP.%2520We%2520term%2520the%2520novel%2520paradigm%2520as%2520the%2520Parallel%2520Gaussian%250AVariational%2520Inference%2520Motion%2520Planning%2520%2528P-GVIMP%2529.%2520We%2520validate%2520the%2520proposed%250Aframework%2520on%2520various%2520robotic%2520systems%252C%2520demonstrating%2520significant%2520speed%250Aacceleration%2520achieved%2520by%2520leveraging%2520Graphics%2520Processing%2520Units%2520%2528GPUs%2529%2520for%250Aparallel%2520computation.%2520An%2520open-sourced%2520implementation%2520is%2520presented%2520at%250Ahttps%253A//github.com/hzyu17/VIMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Gaussian%20Variational%20Inference%20for%20Motion%20Planning%20Under%0A%20%20Uncertainty&entry.906535625=Zinuo%20Chang%20and%20Hongzhe%20Yu%20and%20Patricio%20Vela%20and%20Yongxin%20Chen&entry.1292438233=%20%20This%20work%20addresses%20motion%20planning%20under%20uncertainty%20as%20a%20stochastic%20optimal%0Acontrol%20problem.%20The%20path%20distribution%20induced%20by%20the%20optimal%20controller%0Acorresponds%20to%20a%20posterior%20path%20distribution%20with%20a%20known%20form.%20To%20approximate%0Athis%20posterior%2C%20we%20frame%20an%20optimization%20problem%20in%20the%20space%20of%20Gaussian%0Adistributions%2C%20which%20aligns%20with%20the%20Gaussian%20Variational%20Inference%20Motion%0APlanning%20%28GVIMP%29%20paradigm%20introduced%20in%20%5Ccite%7Byu2023gaussian%7D.%20In%20this%0Aframework%2C%20the%20computation%20bottleneck%20lies%20in%20evaluating%20the%20expectation%20of%0Acollision%20costs%20over%20a%20dense%20discretized%20trajectory%20and%20computing%20the%20marginal%0Acovariances.%20This%20work%20exploits%20the%20sparse%20motion%20planning%20factor%20graph%2C%20which%0Aallows%20for%20parallel%20computing%20collision%20costs%20and%20Gaussian%20Belief%20Propagation%0A%28GBP%29%20marginal%20covariance%20computation%2C%20to%20introduce%20a%20computationally%20efficient%0Aapproach%20to%20solving%20GVIMP.%20We%20term%20the%20novel%20paradigm%20as%20the%20Parallel%20Gaussian%0AVariational%20Inference%20Motion%20Planning%20%28P-GVIMP%29.%20We%20validate%20the%20proposed%0Aframework%20on%20various%20robotic%20systems%2C%20demonstrating%20significant%20speed%0Aacceleration%20achieved%20by%20leveraging%20Graphics%20Processing%20Units%20%28GPUs%29%20for%0Aparallel%20computation.%20An%20open-sourced%20implementation%20is%20presented%20at%0Ahttps%3A//github.com/hzyu17/VIMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03416v2&entry.124074799=Read"},
{"title": "Spatiotemporal Decoupling for Efficient Vision-Based Occupancy\n  Forecasting", "author": "Jingyi Xu and Xieyuanli Chen and Junyi Ma and Jiawei Huang and Jintao Xu and Yue Wang and Ling Pei", "abstract": "  The task of occupancy forecasting (OCF) involves utilizing past and present\nperception data to predict future occupancy states of autonomous vehicle\nsurrounding environments, which is critical for downstream tasks such as\nobstacle avoidance and path planning. Existing 3D OCF approaches struggle to\npredict plausible spatial details for movable objects and suffer from slow\ninference speeds due to neglecting the bias and uneven distribution of changing\noccupancy states in both space and time. In this paper, we propose a novel\nspatiotemporal decoupling vision-based paradigm to explicitly tackle the bias\nand achieve both effective and efficient 3D OCF. To tackle spatial bias in\nempty areas, we introduce a novel spatial representation that decouples the\nconventional dense 3D format into 2D bird's-eye view (BEV) occupancy with\ncorresponding height values, enabling 3D OCF derived only from 2D predictions\nthus enhancing efficiency. To reduce temporal bias on static voxels, we design\ntemporal decoupling to improve end-to-end OCF by temporally associating\ninstances via predicted flows. We develop an efficient multi-head network\nEfficientOCF to achieve 3D OCF with our devised spatiotemporally decoupled\nrepresentation. A new metric, conditional IoU (C-IoU), is also introduced to\nprovide a robust 3D OCF performance assessment, especially in datasets with\nmissing or incomplete annotations. The experimental results demonstrate that\nEfficientOCF surpasses existing baseline methods on accuracy and efficiency,\nachieving state-of-the-art performance with a fast inference time of 82.33ms\nwith a single GPU. Our code will be released as open source.\n", "link": "http://arxiv.org/abs/2411.14169v1", "date": "2024-11-21", "relevancy": 2.1862, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5537}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Decoupling%20for%20Efficient%20Vision-Based%20Occupancy%0A%20%20Forecasting&body=Title%3A%20Spatiotemporal%20Decoupling%20for%20Efficient%20Vision-Based%20Occupancy%0A%20%20Forecasting%0AAuthor%3A%20Jingyi%20Xu%20and%20Xieyuanli%20Chen%20and%20Junyi%20Ma%20and%20Jiawei%20Huang%20and%20Jintao%20Xu%20and%20Yue%20Wang%20and%20Ling%20Pei%0AAbstract%3A%20%20%20The%20task%20of%20occupancy%20forecasting%20%28OCF%29%20involves%20utilizing%20past%20and%20present%0Aperception%20data%20to%20predict%20future%20occupancy%20states%20of%20autonomous%20vehicle%0Asurrounding%20environments%2C%20which%20is%20critical%20for%20downstream%20tasks%20such%20as%0Aobstacle%20avoidance%20and%20path%20planning.%20Existing%203D%20OCF%20approaches%20struggle%20to%0Apredict%20plausible%20spatial%20details%20for%20movable%20objects%20and%20suffer%20from%20slow%0Ainference%20speeds%20due%20to%20neglecting%20the%20bias%20and%20uneven%20distribution%20of%20changing%0Aoccupancy%20states%20in%20both%20space%20and%20time.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aspatiotemporal%20decoupling%20vision-based%20paradigm%20to%20explicitly%20tackle%20the%20bias%0Aand%20achieve%20both%20effective%20and%20efficient%203D%20OCF.%20To%20tackle%20spatial%20bias%20in%0Aempty%20areas%2C%20we%20introduce%20a%20novel%20spatial%20representation%20that%20decouples%20the%0Aconventional%20dense%203D%20format%20into%202D%20bird%27s-eye%20view%20%28BEV%29%20occupancy%20with%0Acorresponding%20height%20values%2C%20enabling%203D%20OCF%20derived%20only%20from%202D%20predictions%0Athus%20enhancing%20efficiency.%20To%20reduce%20temporal%20bias%20on%20static%20voxels%2C%20we%20design%0Atemporal%20decoupling%20to%20improve%20end-to-end%20OCF%20by%20temporally%20associating%0Ainstances%20via%20predicted%20flows.%20We%20develop%20an%20efficient%20multi-head%20network%0AEfficientOCF%20to%20achieve%203D%20OCF%20with%20our%20devised%20spatiotemporally%20decoupled%0Arepresentation.%20A%20new%20metric%2C%20conditional%20IoU%20%28C-IoU%29%2C%20is%20also%20introduced%20to%0Aprovide%20a%20robust%203D%20OCF%20performance%20assessment%2C%20especially%20in%20datasets%20with%0Amissing%20or%20incomplete%20annotations.%20The%20experimental%20results%20demonstrate%20that%0AEfficientOCF%20surpasses%20existing%20baseline%20methods%20on%20accuracy%20and%20efficiency%2C%0Aachieving%20state-of-the-art%20performance%20with%20a%20fast%20inference%20time%20of%2082.33ms%0Awith%20a%20single%20GPU.%20Our%20code%20will%20be%20released%20as%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520Decoupling%2520for%2520Efficient%2520Vision-Based%2520Occupancy%250A%2520%2520Forecasting%26entry.906535625%3DJingyi%2520Xu%2520and%2520Xieyuanli%2520Chen%2520and%2520Junyi%2520Ma%2520and%2520Jiawei%2520Huang%2520and%2520Jintao%2520Xu%2520and%2520Yue%2520Wang%2520and%2520Ling%2520Pei%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520occupancy%2520forecasting%2520%2528OCF%2529%2520involves%2520utilizing%2520past%2520and%2520present%250Aperception%2520data%2520to%2520predict%2520future%2520occupancy%2520states%2520of%2520autonomous%2520vehicle%250Asurrounding%2520environments%252C%2520which%2520is%2520critical%2520for%2520downstream%2520tasks%2520such%2520as%250Aobstacle%2520avoidance%2520and%2520path%2520planning.%2520Existing%25203D%2520OCF%2520approaches%2520struggle%2520to%250Apredict%2520plausible%2520spatial%2520details%2520for%2520movable%2520objects%2520and%2520suffer%2520from%2520slow%250Ainference%2520speeds%2520due%2520to%2520neglecting%2520the%2520bias%2520and%2520uneven%2520distribution%2520of%2520changing%250Aoccupancy%2520states%2520in%2520both%2520space%2520and%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aspatiotemporal%2520decoupling%2520vision-based%2520paradigm%2520to%2520explicitly%2520tackle%2520the%2520bias%250Aand%2520achieve%2520both%2520effective%2520and%2520efficient%25203D%2520OCF.%2520To%2520tackle%2520spatial%2520bias%2520in%250Aempty%2520areas%252C%2520we%2520introduce%2520a%2520novel%2520spatial%2520representation%2520that%2520decouples%2520the%250Aconventional%2520dense%25203D%2520format%2520into%25202D%2520bird%2527s-eye%2520view%2520%2528BEV%2529%2520occupancy%2520with%250Acorresponding%2520height%2520values%252C%2520enabling%25203D%2520OCF%2520derived%2520only%2520from%25202D%2520predictions%250Athus%2520enhancing%2520efficiency.%2520To%2520reduce%2520temporal%2520bias%2520on%2520static%2520voxels%252C%2520we%2520design%250Atemporal%2520decoupling%2520to%2520improve%2520end-to-end%2520OCF%2520by%2520temporally%2520associating%250Ainstances%2520via%2520predicted%2520flows.%2520We%2520develop%2520an%2520efficient%2520multi-head%2520network%250AEfficientOCF%2520to%2520achieve%25203D%2520OCF%2520with%2520our%2520devised%2520spatiotemporally%2520decoupled%250Arepresentation.%2520A%2520new%2520metric%252C%2520conditional%2520IoU%2520%2528C-IoU%2529%252C%2520is%2520also%2520introduced%2520to%250Aprovide%2520a%2520robust%25203D%2520OCF%2520performance%2520assessment%252C%2520especially%2520in%2520datasets%2520with%250Amissing%2520or%2520incomplete%2520annotations.%2520The%2520experimental%2520results%2520demonstrate%2520that%250AEfficientOCF%2520surpasses%2520existing%2520baseline%2520methods%2520on%2520accuracy%2520and%2520efficiency%252C%250Aachieving%2520state-of-the-art%2520performance%2520with%2520a%2520fast%2520inference%2520time%2520of%252082.33ms%250Awith%2520a%2520single%2520GPU.%2520Our%2520code%2520will%2520be%2520released%2520as%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Decoupling%20for%20Efficient%20Vision-Based%20Occupancy%0A%20%20Forecasting&entry.906535625=Jingyi%20Xu%20and%20Xieyuanli%20Chen%20and%20Junyi%20Ma%20and%20Jiawei%20Huang%20and%20Jintao%20Xu%20and%20Yue%20Wang%20and%20Ling%20Pei&entry.1292438233=%20%20The%20task%20of%20occupancy%20forecasting%20%28OCF%29%20involves%20utilizing%20past%20and%20present%0Aperception%20data%20to%20predict%20future%20occupancy%20states%20of%20autonomous%20vehicle%0Asurrounding%20environments%2C%20which%20is%20critical%20for%20downstream%20tasks%20such%20as%0Aobstacle%20avoidance%20and%20path%20planning.%20Existing%203D%20OCF%20approaches%20struggle%20to%0Apredict%20plausible%20spatial%20details%20for%20movable%20objects%20and%20suffer%20from%20slow%0Ainference%20speeds%20due%20to%20neglecting%20the%20bias%20and%20uneven%20distribution%20of%20changing%0Aoccupancy%20states%20in%20both%20space%20and%20time.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aspatiotemporal%20decoupling%20vision-based%20paradigm%20to%20explicitly%20tackle%20the%20bias%0Aand%20achieve%20both%20effective%20and%20efficient%203D%20OCF.%20To%20tackle%20spatial%20bias%20in%0Aempty%20areas%2C%20we%20introduce%20a%20novel%20spatial%20representation%20that%20decouples%20the%0Aconventional%20dense%203D%20format%20into%202D%20bird%27s-eye%20view%20%28BEV%29%20occupancy%20with%0Acorresponding%20height%20values%2C%20enabling%203D%20OCF%20derived%20only%20from%202D%20predictions%0Athus%20enhancing%20efficiency.%20To%20reduce%20temporal%20bias%20on%20static%20voxels%2C%20we%20design%0Atemporal%20decoupling%20to%20improve%20end-to-end%20OCF%20by%20temporally%20associating%0Ainstances%20via%20predicted%20flows.%20We%20develop%20an%20efficient%20multi-head%20network%0AEfficientOCF%20to%20achieve%203D%20OCF%20with%20our%20devised%20spatiotemporally%20decoupled%0Arepresentation.%20A%20new%20metric%2C%20conditional%20IoU%20%28C-IoU%29%2C%20is%20also%20introduced%20to%0Aprovide%20a%20robust%203D%20OCF%20performance%20assessment%2C%20especially%20in%20datasets%20with%0Amissing%20or%20incomplete%20annotations.%20The%20experimental%20results%20demonstrate%20that%0AEfficientOCF%20surpasses%20existing%20baseline%20methods%20on%20accuracy%20and%20efficiency%2C%0Aachieving%20state-of-the-art%20performance%20with%20a%20fast%20inference%20time%20of%2082.33ms%0Awith%20a%20single%20GPU.%20Our%20code%20will%20be%20released%20as%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14169v1&entry.124074799=Read"},
{"title": "Outlier-robust Mean Estimation near the Breakdown Point via\n  Sum-of-Squares", "author": "Hongjie Chen and Deepak Narayanan Sridharan and David Steurer", "abstract": "  We revisit the problem of estimating the mean of a high-dimensional\ndistribution in the presence of an $\\varepsilon$-fraction of adversarial\noutliers.\n  When $\\varepsilon$ is at most some sufficiently small constant, previous\nworks can achieve optimal error rate efficiently\n\\cite{diakonikolas2018robustly, kothari2018robust}. As $\\varepsilon$ approaches\nthe breakdown point $\\frac{1}{2}$, all previous algorithms incur either\nsub-optimal error rates or exponential running time.\n  In this paper we give a new analysis of the canonical sum-of-squares program\nintroduced in \\cite{kothari2018robust} and show that this program efficiently\nachieves optimal error rate for all $\\varepsilon \\in[0,\\frac{1}{2})$. The key\ningredient for our results is a new identifiability proof for robust mean\nestimation that focuses on the overlap between the distributions instead of\ntheir statistical distance as in previous works. We capture this proof within\nthe sum-of-squares proof system, thus obtaining efficient algorithms using the\nsum-of-squares proofs to algorithms paradigm \\cite{raghavendra2018high}.\n", "link": "http://arxiv.org/abs/2411.14305v1", "date": "2024-11-21", "relevancy": 2.1712, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.456}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4236}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outlier-robust%20Mean%20Estimation%20near%20the%20Breakdown%20Point%20via%0A%20%20Sum-of-Squares&body=Title%3A%20Outlier-robust%20Mean%20Estimation%20near%20the%20Breakdown%20Point%20via%0A%20%20Sum-of-Squares%0AAuthor%3A%20Hongjie%20Chen%20and%20Deepak%20Narayanan%20Sridharan%20and%20David%20Steurer%0AAbstract%3A%20%20%20We%20revisit%20the%20problem%20of%20estimating%20the%20mean%20of%20a%20high-dimensional%0Adistribution%20in%20the%20presence%20of%20an%20%24%5Cvarepsilon%24-fraction%20of%20adversarial%0Aoutliers.%0A%20%20When%20%24%5Cvarepsilon%24%20is%20at%20most%20some%20sufficiently%20small%20constant%2C%20previous%0Aworks%20can%20achieve%20optimal%20error%20rate%20efficiently%0A%5Ccite%7Bdiakonikolas2018robustly%2C%20kothari2018robust%7D.%20As%20%24%5Cvarepsilon%24%20approaches%0Athe%20breakdown%20point%20%24%5Cfrac%7B1%7D%7B2%7D%24%2C%20all%20previous%20algorithms%20incur%20either%0Asub-optimal%20error%20rates%20or%20exponential%20running%20time.%0A%20%20In%20this%20paper%20we%20give%20a%20new%20analysis%20of%20the%20canonical%20sum-of-squares%20program%0Aintroduced%20in%20%5Ccite%7Bkothari2018robust%7D%20and%20show%20that%20this%20program%20efficiently%0Aachieves%20optimal%20error%20rate%20for%20all%20%24%5Cvarepsilon%20%5Cin%5B0%2C%5Cfrac%7B1%7D%7B2%7D%29%24.%20The%20key%0Aingredient%20for%20our%20results%20is%20a%20new%20identifiability%20proof%20for%20robust%20mean%0Aestimation%20that%20focuses%20on%20the%20overlap%20between%20the%20distributions%20instead%20of%0Atheir%20statistical%20distance%20as%20in%20previous%20works.%20We%20capture%20this%20proof%20within%0Athe%20sum-of-squares%20proof%20system%2C%20thus%20obtaining%20efficient%20algorithms%20using%20the%0Asum-of-squares%20proofs%20to%20algorithms%20paradigm%20%5Ccite%7Braghavendra2018high%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutlier-robust%2520Mean%2520Estimation%2520near%2520the%2520Breakdown%2520Point%2520via%250A%2520%2520Sum-of-Squares%26entry.906535625%3DHongjie%2520Chen%2520and%2520Deepak%2520Narayanan%2520Sridharan%2520and%2520David%2520Steurer%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520problem%2520of%2520estimating%2520the%2520mean%2520of%2520a%2520high-dimensional%250Adistribution%2520in%2520the%2520presence%2520of%2520an%2520%2524%255Cvarepsilon%2524-fraction%2520of%2520adversarial%250Aoutliers.%250A%2520%2520When%2520%2524%255Cvarepsilon%2524%2520is%2520at%2520most%2520some%2520sufficiently%2520small%2520constant%252C%2520previous%250Aworks%2520can%2520achieve%2520optimal%2520error%2520rate%2520efficiently%250A%255Ccite%257Bdiakonikolas2018robustly%252C%2520kothari2018robust%257D.%2520As%2520%2524%255Cvarepsilon%2524%2520approaches%250Athe%2520breakdown%2520point%2520%2524%255Cfrac%257B1%257D%257B2%257D%2524%252C%2520all%2520previous%2520algorithms%2520incur%2520either%250Asub-optimal%2520error%2520rates%2520or%2520exponential%2520running%2520time.%250A%2520%2520In%2520this%2520paper%2520we%2520give%2520a%2520new%2520analysis%2520of%2520the%2520canonical%2520sum-of-squares%2520program%250Aintroduced%2520in%2520%255Ccite%257Bkothari2018robust%257D%2520and%2520show%2520that%2520this%2520program%2520efficiently%250Aachieves%2520optimal%2520error%2520rate%2520for%2520all%2520%2524%255Cvarepsilon%2520%255Cin%255B0%252C%255Cfrac%257B1%257D%257B2%257D%2529%2524.%2520The%2520key%250Aingredient%2520for%2520our%2520results%2520is%2520a%2520new%2520identifiability%2520proof%2520for%2520robust%2520mean%250Aestimation%2520that%2520focuses%2520on%2520the%2520overlap%2520between%2520the%2520distributions%2520instead%2520of%250Atheir%2520statistical%2520distance%2520as%2520in%2520previous%2520works.%2520We%2520capture%2520this%2520proof%2520within%250Athe%2520sum-of-squares%2520proof%2520system%252C%2520thus%2520obtaining%2520efficient%2520algorithms%2520using%2520the%250Asum-of-squares%2520proofs%2520to%2520algorithms%2520paradigm%2520%255Ccite%257Braghavendra2018high%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outlier-robust%20Mean%20Estimation%20near%20the%20Breakdown%20Point%20via%0A%20%20Sum-of-Squares&entry.906535625=Hongjie%20Chen%20and%20Deepak%20Narayanan%20Sridharan%20and%20David%20Steurer&entry.1292438233=%20%20We%20revisit%20the%20problem%20of%20estimating%20the%20mean%20of%20a%20high-dimensional%0Adistribution%20in%20the%20presence%20of%20an%20%24%5Cvarepsilon%24-fraction%20of%20adversarial%0Aoutliers.%0A%20%20When%20%24%5Cvarepsilon%24%20is%20at%20most%20some%20sufficiently%20small%20constant%2C%20previous%0Aworks%20can%20achieve%20optimal%20error%20rate%20efficiently%0A%5Ccite%7Bdiakonikolas2018robustly%2C%20kothari2018robust%7D.%20As%20%24%5Cvarepsilon%24%20approaches%0Athe%20breakdown%20point%20%24%5Cfrac%7B1%7D%7B2%7D%24%2C%20all%20previous%20algorithms%20incur%20either%0Asub-optimal%20error%20rates%20or%20exponential%20running%20time.%0A%20%20In%20this%20paper%20we%20give%20a%20new%20analysis%20of%20the%20canonical%20sum-of-squares%20program%0Aintroduced%20in%20%5Ccite%7Bkothari2018robust%7D%20and%20show%20that%20this%20program%20efficiently%0Aachieves%20optimal%20error%20rate%20for%20all%20%24%5Cvarepsilon%20%5Cin%5B0%2C%5Cfrac%7B1%7D%7B2%7D%29%24.%20The%20key%0Aingredient%20for%20our%20results%20is%20a%20new%20identifiability%20proof%20for%20robust%20mean%0Aestimation%20that%20focuses%20on%20the%20overlap%20between%20the%20distributions%20instead%20of%0Atheir%20statistical%20distance%20as%20in%20previous%20works.%20We%20capture%20this%20proof%20within%0Athe%20sum-of-squares%20proof%20system%2C%20thus%20obtaining%20efficient%20algorithms%20using%20the%0Asum-of-squares%20proofs%20to%20algorithms%20paradigm%20%5Ccite%7Braghavendra2018high%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14305v1&entry.124074799=Read"},
{"title": "Continual Learning and Lifting of Koopman Dynamics for Linear Control of\n  Legged Robots", "author": "Feihan Li and Abulikemu Abuduweili and Yifan Sun and Rui Chen and Weiye Zhao and Changliu Liu", "abstract": "  The control of legged robots, particularly humanoid and quadruped robots,\npresents significant challenges due to their high-dimensional and nonlinear\ndynamics. While linear systems can be effectively controlled using methods like\nModel Predictive Control (MPC), the control of nonlinear systems remains\ncomplex. One promising solution is the Koopman Operator, which approximates\nnonlinear dynamics with a linear model, enabling the use of proven linear\ncontrol techniques. However, achieving accurate linearization through\ndata-driven methods is difficult due to issues like approximation error, domain\nshifts, and the limitations of fixed linear state-space representations. These\nchallenges restrict the scalability of Koopman-based approaches. This paper\naddresses these challenges by proposing a continual learning algorithm designed\nto iteratively refine Koopman dynamics for high-dimensional legged robots. The\nkey idea is to progressively expand the dataset and latent space dimension,\nenabling the learned Koopman dynamics to converge towards accurate\napproximations of the true system dynamics. Theoretical analysis shows that the\nlinear approximation error of our method converges monotonically. Experimental\nresults demonstrate that our method achieves high control performance on robots\nlike Unitree G1/H1/A1/Go2 and ANYmal D, across various terrains using simple\nlinear MPC controllers. This work is the first to successfully apply linearized\nKoopman dynamics for locomotion control of high-dimensional legged robots,\nenabling a scalable model-based control solution.\n", "link": "http://arxiv.org/abs/2411.14321v1", "date": "2024-11-21", "relevancy": 2.1436, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5916}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.527}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20and%20Lifting%20of%20Koopman%20Dynamics%20for%20Linear%20Control%20of%0A%20%20Legged%20Robots&body=Title%3A%20Continual%20Learning%20and%20Lifting%20of%20Koopman%20Dynamics%20for%20Linear%20Control%20of%0A%20%20Legged%20Robots%0AAuthor%3A%20Feihan%20Li%20and%20Abulikemu%20Abuduweili%20and%20Yifan%20Sun%20and%20Rui%20Chen%20and%20Weiye%20Zhao%20and%20Changliu%20Liu%0AAbstract%3A%20%20%20The%20control%20of%20legged%20robots%2C%20particularly%20humanoid%20and%20quadruped%20robots%2C%0Apresents%20significant%20challenges%20due%20to%20their%20high-dimensional%20and%20nonlinear%0Adynamics.%20While%20linear%20systems%20can%20be%20effectively%20controlled%20using%20methods%20like%0AModel%20Predictive%20Control%20%28MPC%29%2C%20the%20control%20of%20nonlinear%20systems%20remains%0Acomplex.%20One%20promising%20solution%20is%20the%20Koopman%20Operator%2C%20which%20approximates%0Anonlinear%20dynamics%20with%20a%20linear%20model%2C%20enabling%20the%20use%20of%20proven%20linear%0Acontrol%20techniques.%20However%2C%20achieving%20accurate%20linearization%20through%0Adata-driven%20methods%20is%20difficult%20due%20to%20issues%20like%20approximation%20error%2C%20domain%0Ashifts%2C%20and%20the%20limitations%20of%20fixed%20linear%20state-space%20representations.%20These%0Achallenges%20restrict%20the%20scalability%20of%20Koopman-based%20approaches.%20This%20paper%0Aaddresses%20these%20challenges%20by%20proposing%20a%20continual%20learning%20algorithm%20designed%0Ato%20iteratively%20refine%20Koopman%20dynamics%20for%20high-dimensional%20legged%20robots.%20The%0Akey%20idea%20is%20to%20progressively%20expand%20the%20dataset%20and%20latent%20space%20dimension%2C%0Aenabling%20the%20learned%20Koopman%20dynamics%20to%20converge%20towards%20accurate%0Aapproximations%20of%20the%20true%20system%20dynamics.%20Theoretical%20analysis%20shows%20that%20the%0Alinear%20approximation%20error%20of%20our%20method%20converges%20monotonically.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20high%20control%20performance%20on%20robots%0Alike%20Unitree%20G1/H1/A1/Go2%20and%20ANYmal%20D%2C%20across%20various%20terrains%20using%20simple%0Alinear%20MPC%20controllers.%20This%20work%20is%20the%20first%20to%20successfully%20apply%20linearized%0AKoopman%20dynamics%20for%20locomotion%20control%20of%20high-dimensional%20legged%20robots%2C%0Aenabling%20a%20scalable%20model-based%20control%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520and%2520Lifting%2520of%2520Koopman%2520Dynamics%2520for%2520Linear%2520Control%2520of%250A%2520%2520Legged%2520Robots%26entry.906535625%3DFeihan%2520Li%2520and%2520Abulikemu%2520Abuduweili%2520and%2520Yifan%2520Sun%2520and%2520Rui%2520Chen%2520and%2520Weiye%2520Zhao%2520and%2520Changliu%2520Liu%26entry.1292438233%3D%2520%2520The%2520control%2520of%2520legged%2520robots%252C%2520particularly%2520humanoid%2520and%2520quadruped%2520robots%252C%250Apresents%2520significant%2520challenges%2520due%2520to%2520their%2520high-dimensional%2520and%2520nonlinear%250Adynamics.%2520While%2520linear%2520systems%2520can%2520be%2520effectively%2520controlled%2520using%2520methods%2520like%250AModel%2520Predictive%2520Control%2520%2528MPC%2529%252C%2520the%2520control%2520of%2520nonlinear%2520systems%2520remains%250Acomplex.%2520One%2520promising%2520solution%2520is%2520the%2520Koopman%2520Operator%252C%2520which%2520approximates%250Anonlinear%2520dynamics%2520with%2520a%2520linear%2520model%252C%2520enabling%2520the%2520use%2520of%2520proven%2520linear%250Acontrol%2520techniques.%2520However%252C%2520achieving%2520accurate%2520linearization%2520through%250Adata-driven%2520methods%2520is%2520difficult%2520due%2520to%2520issues%2520like%2520approximation%2520error%252C%2520domain%250Ashifts%252C%2520and%2520the%2520limitations%2520of%2520fixed%2520linear%2520state-space%2520representations.%2520These%250Achallenges%2520restrict%2520the%2520scalability%2520of%2520Koopman-based%2520approaches.%2520This%2520paper%250Aaddresses%2520these%2520challenges%2520by%2520proposing%2520a%2520continual%2520learning%2520algorithm%2520designed%250Ato%2520iteratively%2520refine%2520Koopman%2520dynamics%2520for%2520high-dimensional%2520legged%2520robots.%2520The%250Akey%2520idea%2520is%2520to%2520progressively%2520expand%2520the%2520dataset%2520and%2520latent%2520space%2520dimension%252C%250Aenabling%2520the%2520learned%2520Koopman%2520dynamics%2520to%2520converge%2520towards%2520accurate%250Aapproximations%2520of%2520the%2520true%2520system%2520dynamics.%2520Theoretical%2520analysis%2520shows%2520that%2520the%250Alinear%2520approximation%2520error%2520of%2520our%2520method%2520converges%2520monotonically.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520achieves%2520high%2520control%2520performance%2520on%2520robots%250Alike%2520Unitree%2520G1/H1/A1/Go2%2520and%2520ANYmal%2520D%252C%2520across%2520various%2520terrains%2520using%2520simple%250Alinear%2520MPC%2520controllers.%2520This%2520work%2520is%2520the%2520first%2520to%2520successfully%2520apply%2520linearized%250AKoopman%2520dynamics%2520for%2520locomotion%2520control%2520of%2520high-dimensional%2520legged%2520robots%252C%250Aenabling%2520a%2520scalable%2520model-based%2520control%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20and%20Lifting%20of%20Koopman%20Dynamics%20for%20Linear%20Control%20of%0A%20%20Legged%20Robots&entry.906535625=Feihan%20Li%20and%20Abulikemu%20Abuduweili%20and%20Yifan%20Sun%20and%20Rui%20Chen%20and%20Weiye%20Zhao%20and%20Changliu%20Liu&entry.1292438233=%20%20The%20control%20of%20legged%20robots%2C%20particularly%20humanoid%20and%20quadruped%20robots%2C%0Apresents%20significant%20challenges%20due%20to%20their%20high-dimensional%20and%20nonlinear%0Adynamics.%20While%20linear%20systems%20can%20be%20effectively%20controlled%20using%20methods%20like%0AModel%20Predictive%20Control%20%28MPC%29%2C%20the%20control%20of%20nonlinear%20systems%20remains%0Acomplex.%20One%20promising%20solution%20is%20the%20Koopman%20Operator%2C%20which%20approximates%0Anonlinear%20dynamics%20with%20a%20linear%20model%2C%20enabling%20the%20use%20of%20proven%20linear%0Acontrol%20techniques.%20However%2C%20achieving%20accurate%20linearization%20through%0Adata-driven%20methods%20is%20difficult%20due%20to%20issues%20like%20approximation%20error%2C%20domain%0Ashifts%2C%20and%20the%20limitations%20of%20fixed%20linear%20state-space%20representations.%20These%0Achallenges%20restrict%20the%20scalability%20of%20Koopman-based%20approaches.%20This%20paper%0Aaddresses%20these%20challenges%20by%20proposing%20a%20continual%20learning%20algorithm%20designed%0Ato%20iteratively%20refine%20Koopman%20dynamics%20for%20high-dimensional%20legged%20robots.%20The%0Akey%20idea%20is%20to%20progressively%20expand%20the%20dataset%20and%20latent%20space%20dimension%2C%0Aenabling%20the%20learned%20Koopman%20dynamics%20to%20converge%20towards%20accurate%0Aapproximations%20of%20the%20true%20system%20dynamics.%20Theoretical%20analysis%20shows%20that%20the%0Alinear%20approximation%20error%20of%20our%20method%20converges%20monotonically.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20high%20control%20performance%20on%20robots%0Alike%20Unitree%20G1/H1/A1/Go2%20and%20ANYmal%20D%2C%20across%20various%20terrains%20using%20simple%0Alinear%20MPC%20controllers.%20This%20work%20is%20the%20first%20to%20successfully%20apply%20linearized%0AKoopman%20dynamics%20for%20locomotion%20control%20of%20high-dimensional%20legged%20robots%2C%0Aenabling%20a%20scalable%20model-based%20control%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14321v1&entry.124074799=Read"},
{"title": "Differentiable Weightless Neural Networks", "author": "Alan T. L. Bacellar and Zachary Susskind and Mauricio Breternitz Jr. and Eugene John and Lizy K. John and Priscila M. V. Lima and Felipe M. G. Fran\u00e7a", "abstract": "  We introduce the Differentiable Weightless Neural Network (DWN), a model\nbased on interconnected lookup tables. Training of DWNs is enabled by a novel\nExtended Finite Difference technique for approximate differentiation of binary\nvalues. We propose Learnable Mapping, Learnable Reduction, and Spectral\nRegularization to further improve the accuracy and efficiency of these models.\nWe evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware\naccelerator, where they demonstrate superior latency, throughput, energy\nefficiency, and model area compared to state-of-the-art solutions, (2) a\nlow-power microcontroller, where they achieve preferable accuracy to XGBoost\nwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,\nwhere they consistently outperform small models in both accuracy and projected\nhardware area. DWNs also compare favorably against leading approaches for\ntabular datasets, with higher average rank. Overall, our work positions DWNs as\na pioneering solution for edge-compatible high-throughput neural networks.\n", "link": "http://arxiv.org/abs/2410.11112v2", "date": "2024-11-21", "relevancy": 2.1381, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5604}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Weightless%20Neural%20Networks&body=Title%3A%20Differentiable%20Weightless%20Neural%20Networks%0AAuthor%3A%20Alan%20T.%20L.%20Bacellar%20and%20Zachary%20Susskind%20and%20Mauricio%20Breternitz%20Jr.%20and%20Eugene%20John%20and%20Lizy%20K.%20John%20and%20Priscila%20M.%20V.%20Lima%20and%20Felipe%20M.%20G.%20Fran%C3%A7a%0AAbstract%3A%20%20%20We%20introduce%20the%20Differentiable%20Weightless%20Neural%20Network%20%28DWN%29%2C%20a%20model%0Abased%20on%20interconnected%20lookup%20tables.%20Training%20of%20DWNs%20is%20enabled%20by%20a%20novel%0AExtended%20Finite%20Difference%20technique%20for%20approximate%20differentiation%20of%20binary%0Avalues.%20We%20propose%20Learnable%20Mapping%2C%20Learnable%20Reduction%2C%20and%20Spectral%0ARegularization%20to%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20these%20models.%0AWe%20evaluate%20DWNs%20in%20three%20edge%20computing%20contexts%3A%20%281%29%20an%20FPGA-based%20hardware%0Aaccelerator%2C%20where%20they%20demonstrate%20superior%20latency%2C%20throughput%2C%20energy%0Aefficiency%2C%20and%20model%20area%20compared%20to%20state-of-the-art%20solutions%2C%20%282%29%20a%0Alow-power%20microcontroller%2C%20where%20they%20achieve%20preferable%20accuracy%20to%20XGBoost%0Awhile%20subject%20to%20stringent%20memory%20constraints%2C%20and%20%283%29%20ultra-low-cost%20chips%2C%0Awhere%20they%20consistently%20outperform%20small%20models%20in%20both%20accuracy%20and%20projected%0Ahardware%20area.%20DWNs%20also%20compare%20favorably%20against%20leading%20approaches%20for%0Atabular%20datasets%2C%20with%20higher%20average%20rank.%20Overall%2C%20our%20work%20positions%20DWNs%20as%0Aa%20pioneering%20solution%20for%20edge-compatible%20high-throughput%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Weightless%2520Neural%2520Networks%26entry.906535625%3DAlan%2520T.%2520L.%2520Bacellar%2520and%2520Zachary%2520Susskind%2520and%2520Mauricio%2520Breternitz%2520Jr.%2520and%2520Eugene%2520John%2520and%2520Lizy%2520K.%2520John%2520and%2520Priscila%2520M.%2520V.%2520Lima%2520and%2520Felipe%2520M.%2520G.%2520Fran%25C3%25A7a%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Differentiable%2520Weightless%2520Neural%2520Network%2520%2528DWN%2529%252C%2520a%2520model%250Abased%2520on%2520interconnected%2520lookup%2520tables.%2520Training%2520of%2520DWNs%2520is%2520enabled%2520by%2520a%2520novel%250AExtended%2520Finite%2520Difference%2520technique%2520for%2520approximate%2520differentiation%2520of%2520binary%250Avalues.%2520We%2520propose%2520Learnable%2520Mapping%252C%2520Learnable%2520Reduction%252C%2520and%2520Spectral%250ARegularization%2520to%2520further%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520these%2520models.%250AWe%2520evaluate%2520DWNs%2520in%2520three%2520edge%2520computing%2520contexts%253A%2520%25281%2529%2520an%2520FPGA-based%2520hardware%250Aaccelerator%252C%2520where%2520they%2520demonstrate%2520superior%2520latency%252C%2520throughput%252C%2520energy%250Aefficiency%252C%2520and%2520model%2520area%2520compared%2520to%2520state-of-the-art%2520solutions%252C%2520%25282%2529%2520a%250Alow-power%2520microcontroller%252C%2520where%2520they%2520achieve%2520preferable%2520accuracy%2520to%2520XGBoost%250Awhile%2520subject%2520to%2520stringent%2520memory%2520constraints%252C%2520and%2520%25283%2529%2520ultra-low-cost%2520chips%252C%250Awhere%2520they%2520consistently%2520outperform%2520small%2520models%2520in%2520both%2520accuracy%2520and%2520projected%250Ahardware%2520area.%2520DWNs%2520also%2520compare%2520favorably%2520against%2520leading%2520approaches%2520for%250Atabular%2520datasets%252C%2520with%2520higher%2520average%2520rank.%2520Overall%252C%2520our%2520work%2520positions%2520DWNs%2520as%250Aa%2520pioneering%2520solution%2520for%2520edge-compatible%2520high-throughput%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Weightless%20Neural%20Networks&entry.906535625=Alan%20T.%20L.%20Bacellar%20and%20Zachary%20Susskind%20and%20Mauricio%20Breternitz%20Jr.%20and%20Eugene%20John%20and%20Lizy%20K.%20John%20and%20Priscila%20M.%20V.%20Lima%20and%20Felipe%20M.%20G.%20Fran%C3%A7a&entry.1292438233=%20%20We%20introduce%20the%20Differentiable%20Weightless%20Neural%20Network%20%28DWN%29%2C%20a%20model%0Abased%20on%20interconnected%20lookup%20tables.%20Training%20of%20DWNs%20is%20enabled%20by%20a%20novel%0AExtended%20Finite%20Difference%20technique%20for%20approximate%20differentiation%20of%20binary%0Avalues.%20We%20propose%20Learnable%20Mapping%2C%20Learnable%20Reduction%2C%20and%20Spectral%0ARegularization%20to%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20these%20models.%0AWe%20evaluate%20DWNs%20in%20three%20edge%20computing%20contexts%3A%20%281%29%20an%20FPGA-based%20hardware%0Aaccelerator%2C%20where%20they%20demonstrate%20superior%20latency%2C%20throughput%2C%20energy%0Aefficiency%2C%20and%20model%20area%20compared%20to%20state-of-the-art%20solutions%2C%20%282%29%20a%0Alow-power%20microcontroller%2C%20where%20they%20achieve%20preferable%20accuracy%20to%20XGBoost%0Awhile%20subject%20to%20stringent%20memory%20constraints%2C%20and%20%283%29%20ultra-low-cost%20chips%2C%0Awhere%20they%20consistently%20outperform%20small%20models%20in%20both%20accuracy%20and%20projected%0Ahardware%20area.%20DWNs%20also%20compare%20favorably%20against%20leading%20approaches%20for%0Atabular%20datasets%2C%20with%20higher%20average%20rank.%20Overall%2C%20our%20work%20positions%20DWNs%20as%0Aa%20pioneering%20solution%20for%20edge-compatible%20high-throughput%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11112v2&entry.124074799=Read"},
{"title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates", "author": "Aida Kostikova and Benjamin Paassen and Dominik Beese and Ole P\u00fctz and Gregor Wiedemann and Steffen Eger", "abstract": "  Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.\n", "link": "http://arxiv.org/abs/2210.04359v3", "date": "2024-11-21", "relevancy": 2.1315, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4281}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Detection%20of%20Solidarity%20for%20Women%20and%20Migrants%20in%20155%20Years%0A%20%20of%20German%20Parliamentary%20Debates&body=Title%3A%20Fine-Grained%20Detection%20of%20Solidarity%20for%20Women%20and%20Migrants%20in%20155%20Years%0A%20%20of%20German%20Parliamentary%20Debates%0AAuthor%3A%20Aida%20Kostikova%20and%20Benjamin%20Paassen%20and%20Dominik%20Beese%20and%20Ole%20P%C3%BCtz%20and%20Gregor%20Wiedemann%20and%20Steffen%20Eger%0AAbstract%3A%20%20%20Solidarity%20is%20a%20crucial%20concept%20to%20understand%20social%20relations%20in%20societies.%0AIn%20this%20paper%2C%20we%20explore%20fine-grained%20solidarity%20frames%20to%20study%20solidarity%0Atowards%20women%20and%20migrants%20in%20German%20parliamentary%20debates%20between%201867%20and%0A2022.%20Using%202%2C864%20manually%20annotated%20text%20snippets%20%28with%20a%20cost%20exceeding%2018k%0AEuro%29%2C%20we%20evaluate%20large%20language%20models%20%28LLMs%29%20like%20Llama%203%2C%20GPT-3.5%2C%20and%0AGPT-4.%20We%20find%20that%20GPT-4%20outperforms%20other%20LLMs%2C%20approaching%20human%20annotation%0Aquality.%20Using%20GPT-4%2C%20we%20automatically%20annotate%20more%20than%2018k%20further%20instances%0A%28with%20a%20cost%20of%20around%20500%20Euro%29%20across%20155%20years%20and%20find%20that%20solidarity%20with%0Amigrants%20outweighs%20anti-solidarity%20but%20that%20frequencies%20and%20solidarity%20types%0Ashift%20over%20time.%20Most%20importantly%2C%20group-based%20notions%20of%20%28anti-%29solidarity%0Afade%20in%20favor%20of%20compassionate%20solidarity%2C%20focusing%20on%20the%20vulnerability%20of%0Amigrant%20groups%2C%20and%20exchange-based%20anti-solidarity%2C%20focusing%20on%20the%20lack%20of%0A%28economic%29%20contribution.%20Our%20study%20highlights%20the%20interplay%20of%20historical%0Aevents%2C%20socio-economic%20needs%2C%20and%20political%20ideologies%20in%20shaping%20migration%0Adiscourse%20and%20social%20cohesion.%20We%20also%20show%20that%20powerful%20LLMs%2C%20if%20carefully%0Aprompted%2C%20can%20be%20cost-effective%20alternatives%20to%20human%20annotation%20for%20hard%0Asocial%20scientific%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04359v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Detection%2520of%2520Solidarity%2520for%2520Women%2520and%2520Migrants%2520in%2520155%2520Years%250A%2520%2520of%2520German%2520Parliamentary%2520Debates%26entry.906535625%3DAida%2520Kostikova%2520and%2520Benjamin%2520Paassen%2520and%2520Dominik%2520Beese%2520and%2520Ole%2520P%25C3%25BCtz%2520and%2520Gregor%2520Wiedemann%2520and%2520Steffen%2520Eger%26entry.1292438233%3D%2520%2520Solidarity%2520is%2520a%2520crucial%2520concept%2520to%2520understand%2520social%2520relations%2520in%2520societies.%250AIn%2520this%2520paper%252C%2520we%2520explore%2520fine-grained%2520solidarity%2520frames%2520to%2520study%2520solidarity%250Atowards%2520women%2520and%2520migrants%2520in%2520German%2520parliamentary%2520debates%2520between%25201867%2520and%250A2022.%2520Using%25202%252C864%2520manually%2520annotated%2520text%2520snippets%2520%2528with%2520a%2520cost%2520exceeding%252018k%250AEuro%2529%252C%2520we%2520evaluate%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%2520Llama%25203%252C%2520GPT-3.5%252C%2520and%250AGPT-4.%2520We%2520find%2520that%2520GPT-4%2520outperforms%2520other%2520LLMs%252C%2520approaching%2520human%2520annotation%250Aquality.%2520Using%2520GPT-4%252C%2520we%2520automatically%2520annotate%2520more%2520than%252018k%2520further%2520instances%250A%2528with%2520a%2520cost%2520of%2520around%2520500%2520Euro%2529%2520across%2520155%2520years%2520and%2520find%2520that%2520solidarity%2520with%250Amigrants%2520outweighs%2520anti-solidarity%2520but%2520that%2520frequencies%2520and%2520solidarity%2520types%250Ashift%2520over%2520time.%2520Most%2520importantly%252C%2520group-based%2520notions%2520of%2520%2528anti-%2529solidarity%250Afade%2520in%2520favor%2520of%2520compassionate%2520solidarity%252C%2520focusing%2520on%2520the%2520vulnerability%2520of%250Amigrant%2520groups%252C%2520and%2520exchange-based%2520anti-solidarity%252C%2520focusing%2520on%2520the%2520lack%2520of%250A%2528economic%2529%2520contribution.%2520Our%2520study%2520highlights%2520the%2520interplay%2520of%2520historical%250Aevents%252C%2520socio-economic%2520needs%252C%2520and%2520political%2520ideologies%2520in%2520shaping%2520migration%250Adiscourse%2520and%2520social%2520cohesion.%2520We%2520also%2520show%2520that%2520powerful%2520LLMs%252C%2520if%2520carefully%250Aprompted%252C%2520can%2520be%2520cost-effective%2520alternatives%2520to%2520human%2520annotation%2520for%2520hard%250Asocial%2520scientific%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04359v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Detection%20of%20Solidarity%20for%20Women%20and%20Migrants%20in%20155%20Years%0A%20%20of%20German%20Parliamentary%20Debates&entry.906535625=Aida%20Kostikova%20and%20Benjamin%20Paassen%20and%20Dominik%20Beese%20and%20Ole%20P%C3%BCtz%20and%20Gregor%20Wiedemann%20and%20Steffen%20Eger&entry.1292438233=%20%20Solidarity%20is%20a%20crucial%20concept%20to%20understand%20social%20relations%20in%20societies.%0AIn%20this%20paper%2C%20we%20explore%20fine-grained%20solidarity%20frames%20to%20study%20solidarity%0Atowards%20women%20and%20migrants%20in%20German%20parliamentary%20debates%20between%201867%20and%0A2022.%20Using%202%2C864%20manually%20annotated%20text%20snippets%20%28with%20a%20cost%20exceeding%2018k%0AEuro%29%2C%20we%20evaluate%20large%20language%20models%20%28LLMs%29%20like%20Llama%203%2C%20GPT-3.5%2C%20and%0AGPT-4.%20We%20find%20that%20GPT-4%20outperforms%20other%20LLMs%2C%20approaching%20human%20annotation%0Aquality.%20Using%20GPT-4%2C%20we%20automatically%20annotate%20more%20than%2018k%20further%20instances%0A%28with%20a%20cost%20of%20around%20500%20Euro%29%20across%20155%20years%20and%20find%20that%20solidarity%20with%0Amigrants%20outweighs%20anti-solidarity%20but%20that%20frequencies%20and%20solidarity%20types%0Ashift%20over%20time.%20Most%20importantly%2C%20group-based%20notions%20of%20%28anti-%29solidarity%0Afade%20in%20favor%20of%20compassionate%20solidarity%2C%20focusing%20on%20the%20vulnerability%20of%0Amigrant%20groups%2C%20and%20exchange-based%20anti-solidarity%2C%20focusing%20on%20the%20lack%20of%0A%28economic%29%20contribution.%20Our%20study%20highlights%20the%20interplay%20of%20historical%0Aevents%2C%20socio-economic%20needs%2C%20and%20political%20ideologies%20in%20shaping%20migration%0Adiscourse%20and%20social%20cohesion.%20We%20also%20show%20that%20powerful%20LLMs%2C%20if%20carefully%0Aprompted%2C%20can%20be%20cost-effective%20alternatives%20to%20human%20annotation%20for%20hard%0Asocial%20scientific%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04359v3&entry.124074799=Read"},
{"title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion", "author": "Sha Luo and Sang Jung Kim and Zening Duan and Kaiping Chen", "abstract": "  In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them.\n", "link": "http://arxiv.org/abs/2406.08222v2", "date": "2024-11-21", "relevancy": 2.1275, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5594}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5279}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion&body=Title%3A%20A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion%0AAuthor%3A%20Sha%20Luo%20and%20Sang%20Jung%20Kim%20and%20Zening%20Duan%20and%20Kaiping%20Chen%0AAbstract%3A%20%20%20In%20the%20evolving%20landscape%20of%20computer%20vision%20%28CV%29%20technologies%2C%20the%20automatic%0Adetection%20and%20interpretation%20of%20gender%20and%20emotion%20in%20images%20is%20a%20critical%20area%0Aof%20study.%20This%20paper%20investigates%20social%20biases%20in%20CV%20models%2C%20emphasizing%20the%0Alimitations%20of%20traditional%20evaluation%20metrics%20such%20as%20precision%2C%20recall%2C%20and%0Aaccuracy.%20These%20metrics%20often%20fall%20short%20in%20capturing%20the%20complexities%20of%0Agender%20and%20emotion%2C%20which%20are%20fluid%20and%20culturally%20nuanced%20constructs.%20Our%0Astudy%20proposes%20a%20sociotechnical%20framework%20for%20evaluating%20CV%20models%2C%0Aincorporating%20both%20technical%20performance%20measures%20and%20considerations%20of%20social%0Afairness.%20Using%20a%20dataset%20of%205%2C570%20images%20related%20to%20vaccination%20and%20climate%0Achange%2C%20we%20empirically%20compared%20the%20performance%20of%20various%20CV%20models%2C%20including%0Atraditional%20models%20like%20DeepFace%20and%20FER%2C%20and%20generative%20models%20like%20GPT-4%0AVision.%20Our%20analysis%20involved%20manually%20validating%20the%20gender%20and%20emotional%0Aexpressions%20in%20a%20subset%20of%20images%20to%20serve%20as%20benchmarks.%20Our%20findings%20reveal%0Athat%20while%20GPT-4%20Vision%20outperforms%20other%20models%20in%20technical%20accuracy%20for%0Agender%20classification%2C%20it%20exhibits%20discriminatory%20biases%2C%20particularly%20in%0Aresponse%20to%20transgender%20and%20non-binary%20personas.%20Furthermore%2C%20the%20model%27s%0Aemotion%20detection%20skew%20heavily%20towards%20positive%20emotions%2C%20with%20a%20notable%20bias%0Atowards%20associating%20female%20images%20with%20happiness%2C%20especially%20when%20prompted%20by%0Amale%20personas.%20These%20findings%20underscore%20the%20necessity%20of%20developing%20more%0Acomprehensive%20evaluation%20criteria%20that%20address%20both%20validity%20and%20discriminatory%0Abiases%20in%20CV%20models.%20Our%20proposed%20framework%20provides%20guidelines%20for%20researchers%0Ato%20critically%20assess%20CV%20tools%2C%20ensuring%20their%20application%20in%20communication%0Aresearch%20is%20both%20ethical%20and%20effective.%20The%20significant%20contribution%20of%20this%0Astudy%20lies%20in%20its%20emphasis%20on%20a%20sociotechnical%20approach%2C%20advocating%20for%20CV%0Atechnologies%20that%20support%20social%20good%20and%20mitigate%20biases%20rather%20than%0Aperpetuate%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sociotechnical%2520Lens%2520for%2520Evaluating%2520Computer%2520Vision%2520Models%253A%2520A%2520Case%250A%2520%2520Study%2520on%2520Detecting%2520and%2520Reasoning%2520about%2520Gender%2520and%2520Emotion%26entry.906535625%3DSha%2520Luo%2520and%2520Sang%2520Jung%2520Kim%2520and%2520Zening%2520Duan%2520and%2520Kaiping%2520Chen%26entry.1292438233%3D%2520%2520In%2520the%2520evolving%2520landscape%2520of%2520computer%2520vision%2520%2528CV%2529%2520technologies%252C%2520the%2520automatic%250Adetection%2520and%2520interpretation%2520of%2520gender%2520and%2520emotion%2520in%2520images%2520is%2520a%2520critical%2520area%250Aof%2520study.%2520This%2520paper%2520investigates%2520social%2520biases%2520in%2520CV%2520models%252C%2520emphasizing%2520the%250Alimitations%2520of%2520traditional%2520evaluation%2520metrics%2520such%2520as%2520precision%252C%2520recall%252C%2520and%250Aaccuracy.%2520These%2520metrics%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520complexities%2520of%250Agender%2520and%2520emotion%252C%2520which%2520are%2520fluid%2520and%2520culturally%2520nuanced%2520constructs.%2520Our%250Astudy%2520proposes%2520a%2520sociotechnical%2520framework%2520for%2520evaluating%2520CV%2520models%252C%250Aincorporating%2520both%2520technical%2520performance%2520measures%2520and%2520considerations%2520of%2520social%250Afairness.%2520Using%2520a%2520dataset%2520of%25205%252C570%2520images%2520related%2520to%2520vaccination%2520and%2520climate%250Achange%252C%2520we%2520empirically%2520compared%2520the%2520performance%2520of%2520various%2520CV%2520models%252C%2520including%250Atraditional%2520models%2520like%2520DeepFace%2520and%2520FER%252C%2520and%2520generative%2520models%2520like%2520GPT-4%250AVision.%2520Our%2520analysis%2520involved%2520manually%2520validating%2520the%2520gender%2520and%2520emotional%250Aexpressions%2520in%2520a%2520subset%2520of%2520images%2520to%2520serve%2520as%2520benchmarks.%2520Our%2520findings%2520reveal%250Athat%2520while%2520GPT-4%2520Vision%2520outperforms%2520other%2520models%2520in%2520technical%2520accuracy%2520for%250Agender%2520classification%252C%2520it%2520exhibits%2520discriminatory%2520biases%252C%2520particularly%2520in%250Aresponse%2520to%2520transgender%2520and%2520non-binary%2520personas.%2520Furthermore%252C%2520the%2520model%2527s%250Aemotion%2520detection%2520skew%2520heavily%2520towards%2520positive%2520emotions%252C%2520with%2520a%2520notable%2520bias%250Atowards%2520associating%2520female%2520images%2520with%2520happiness%252C%2520especially%2520when%2520prompted%2520by%250Amale%2520personas.%2520These%2520findings%2520underscore%2520the%2520necessity%2520of%2520developing%2520more%250Acomprehensive%2520evaluation%2520criteria%2520that%2520address%2520both%2520validity%2520and%2520discriminatory%250Abiases%2520in%2520CV%2520models.%2520Our%2520proposed%2520framework%2520provides%2520guidelines%2520for%2520researchers%250Ato%2520critically%2520assess%2520CV%2520tools%252C%2520ensuring%2520their%2520application%2520in%2520communication%250Aresearch%2520is%2520both%2520ethical%2520and%2520effective.%2520The%2520significant%2520contribution%2520of%2520this%250Astudy%2520lies%2520in%2520its%2520emphasis%2520on%2520a%2520sociotechnical%2520approach%252C%2520advocating%2520for%2520CV%250Atechnologies%2520that%2520support%2520social%2520good%2520and%2520mitigate%2520biases%2520rather%2520than%250Aperpetuate%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion&entry.906535625=Sha%20Luo%20and%20Sang%20Jung%20Kim%20and%20Zening%20Duan%20and%20Kaiping%20Chen&entry.1292438233=%20%20In%20the%20evolving%20landscape%20of%20computer%20vision%20%28CV%29%20technologies%2C%20the%20automatic%0Adetection%20and%20interpretation%20of%20gender%20and%20emotion%20in%20images%20is%20a%20critical%20area%0Aof%20study.%20This%20paper%20investigates%20social%20biases%20in%20CV%20models%2C%20emphasizing%20the%0Alimitations%20of%20traditional%20evaluation%20metrics%20such%20as%20precision%2C%20recall%2C%20and%0Aaccuracy.%20These%20metrics%20often%20fall%20short%20in%20capturing%20the%20complexities%20of%0Agender%20and%20emotion%2C%20which%20are%20fluid%20and%20culturally%20nuanced%20constructs.%20Our%0Astudy%20proposes%20a%20sociotechnical%20framework%20for%20evaluating%20CV%20models%2C%0Aincorporating%20both%20technical%20performance%20measures%20and%20considerations%20of%20social%0Afairness.%20Using%20a%20dataset%20of%205%2C570%20images%20related%20to%20vaccination%20and%20climate%0Achange%2C%20we%20empirically%20compared%20the%20performance%20of%20various%20CV%20models%2C%20including%0Atraditional%20models%20like%20DeepFace%20and%20FER%2C%20and%20generative%20models%20like%20GPT-4%0AVision.%20Our%20analysis%20involved%20manually%20validating%20the%20gender%20and%20emotional%0Aexpressions%20in%20a%20subset%20of%20images%20to%20serve%20as%20benchmarks.%20Our%20findings%20reveal%0Athat%20while%20GPT-4%20Vision%20outperforms%20other%20models%20in%20technical%20accuracy%20for%0Agender%20classification%2C%20it%20exhibits%20discriminatory%20biases%2C%20particularly%20in%0Aresponse%20to%20transgender%20and%20non-binary%20personas.%20Furthermore%2C%20the%20model%27s%0Aemotion%20detection%20skew%20heavily%20towards%20positive%20emotions%2C%20with%20a%20notable%20bias%0Atowards%20associating%20female%20images%20with%20happiness%2C%20especially%20when%20prompted%20by%0Amale%20personas.%20These%20findings%20underscore%20the%20necessity%20of%20developing%20more%0Acomprehensive%20evaluation%20criteria%20that%20address%20both%20validity%20and%20discriminatory%0Abiases%20in%20CV%20models.%20Our%20proposed%20framework%20provides%20guidelines%20for%20researchers%0Ato%20critically%20assess%20CV%20tools%2C%20ensuring%20their%20application%20in%20communication%0Aresearch%20is%20both%20ethical%20and%20effective.%20The%20significant%20contribution%20of%20this%0Astudy%20lies%20in%20its%20emphasis%20on%20a%20sociotechnical%20approach%2C%20advocating%20for%20CV%0Atechnologies%20that%20support%20social%20good%20and%20mitigate%20biases%20rather%20than%0Aperpetuate%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08222v2&entry.124074799=Read"},
{"title": "Guided MRI Reconstruction via Schr\u00f6dinger Bridge", "author": "Yue Wang and Tian Zhou and Zhuo-xu Cui and Bingsheng Huang and Hairong Zheng and Dong Liang and Yanjie Zhu", "abstract": "  Magnetic Resonance Imaging (MRI) is a multi-contrast imaging technique in\nwhich different contrast images share similar structural information. However,\nconventional diffusion models struggle to effectively leverage this structural\nsimilarity. Recently, the Schr\\\"odinger Bridge (SB), a nonlinear extension of\nthe diffusion model, has been proposed to establish diffusion paths between any\ndistributions, allowing the incorporation of guided priors. This study proposes\nan SB-based, multi-contrast image-guided reconstruction framework that\nestablishes a diffusion bridge between the guiding and target image\ndistributions. By using the guiding image along with data consistency during\nsampling, the target image is reconstructed more accurately. To better address\nstructural differences between images, we introduce an inversion strategy from\nthe field of image editing, termed $\\mathbf{I}^2$SB-inversion. Experiments on a\nparied T1 and T2-FLAIR datasets demonstrate that $\\mathbf{I}^2$SB-inversion\nachieve a high acceleration up to 14.4 and outperforms existing methods in\nterms of both reconstruction accuracy and stability.\n", "link": "http://arxiv.org/abs/2411.14269v1", "date": "2024-11-21", "relevancy": 2.1179, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5316}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5316}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20MRI%20Reconstruction%20via%20Schr%C3%B6dinger%20Bridge&body=Title%3A%20Guided%20MRI%20Reconstruction%20via%20Schr%C3%B6dinger%20Bridge%0AAuthor%3A%20Yue%20Wang%20and%20Tian%20Zhou%20and%20Zhuo-xu%20Cui%20and%20Bingsheng%20Huang%20and%20Hairong%20Zheng%20and%20Dong%20Liang%20and%20Yanjie%20Zhu%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20multi-contrast%20imaging%20technique%20in%0Awhich%20different%20contrast%20images%20share%20similar%20structural%20information.%20However%2C%0Aconventional%20diffusion%20models%20struggle%20to%20effectively%20leverage%20this%20structural%0Asimilarity.%20Recently%2C%20the%20Schr%5C%22odinger%20Bridge%20%28SB%29%2C%20a%20nonlinear%20extension%20of%0Athe%20diffusion%20model%2C%20has%20been%20proposed%20to%20establish%20diffusion%20paths%20between%20any%0Adistributions%2C%20allowing%20the%20incorporation%20of%20guided%20priors.%20This%20study%20proposes%0Aan%20SB-based%2C%20multi-contrast%20image-guided%20reconstruction%20framework%20that%0Aestablishes%20a%20diffusion%20bridge%20between%20the%20guiding%20and%20target%20image%0Adistributions.%20By%20using%20the%20guiding%20image%20along%20with%20data%20consistency%20during%0Asampling%2C%20the%20target%20image%20is%20reconstructed%20more%20accurately.%20To%20better%20address%0Astructural%20differences%20between%20images%2C%20we%20introduce%20an%20inversion%20strategy%20from%0Athe%20field%20of%20image%20editing%2C%20termed%20%24%5Cmathbf%7BI%7D%5E2%24SB-inversion.%20Experiments%20on%20a%0Aparied%20T1%20and%20T2-FLAIR%20datasets%20demonstrate%20that%20%24%5Cmathbf%7BI%7D%5E2%24SB-inversion%0Aachieve%20a%20high%20acceleration%20up%20to%2014.4%20and%20outperforms%20existing%20methods%20in%0Aterms%20of%20both%20reconstruction%20accuracy%20and%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520MRI%2520Reconstruction%2520via%2520Schr%25C3%25B6dinger%2520Bridge%26entry.906535625%3DYue%2520Wang%2520and%2520Tian%2520Zhou%2520and%2520Zhuo-xu%2520Cui%2520and%2520Bingsheng%2520Huang%2520and%2520Hairong%2520Zheng%2520and%2520Dong%2520Liang%2520and%2520Yanjie%2520Zhu%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520a%2520multi-contrast%2520imaging%2520technique%2520in%250Awhich%2520different%2520contrast%2520images%2520share%2520similar%2520structural%2520information.%2520However%252C%250Aconventional%2520diffusion%2520models%2520struggle%2520to%2520effectively%2520leverage%2520this%2520structural%250Asimilarity.%2520Recently%252C%2520the%2520Schr%255C%2522odinger%2520Bridge%2520%2528SB%2529%252C%2520a%2520nonlinear%2520extension%2520of%250Athe%2520diffusion%2520model%252C%2520has%2520been%2520proposed%2520to%2520establish%2520diffusion%2520paths%2520between%2520any%250Adistributions%252C%2520allowing%2520the%2520incorporation%2520of%2520guided%2520priors.%2520This%2520study%2520proposes%250Aan%2520SB-based%252C%2520multi-contrast%2520image-guided%2520reconstruction%2520framework%2520that%250Aestablishes%2520a%2520diffusion%2520bridge%2520between%2520the%2520guiding%2520and%2520target%2520image%250Adistributions.%2520By%2520using%2520the%2520guiding%2520image%2520along%2520with%2520data%2520consistency%2520during%250Asampling%252C%2520the%2520target%2520image%2520is%2520reconstructed%2520more%2520accurately.%2520To%2520better%2520address%250Astructural%2520differences%2520between%2520images%252C%2520we%2520introduce%2520an%2520inversion%2520strategy%2520from%250Athe%2520field%2520of%2520image%2520editing%252C%2520termed%2520%2524%255Cmathbf%257BI%257D%255E2%2524SB-inversion.%2520Experiments%2520on%2520a%250Aparied%2520T1%2520and%2520T2-FLAIR%2520datasets%2520demonstrate%2520that%2520%2524%255Cmathbf%257BI%257D%255E2%2524SB-inversion%250Aachieve%2520a%2520high%2520acceleration%2520up%2520to%252014.4%2520and%2520outperforms%2520existing%2520methods%2520in%250Aterms%2520of%2520both%2520reconstruction%2520accuracy%2520and%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20MRI%20Reconstruction%20via%20Schr%C3%B6dinger%20Bridge&entry.906535625=Yue%20Wang%20and%20Tian%20Zhou%20and%20Zhuo-xu%20Cui%20and%20Bingsheng%20Huang%20and%20Hairong%20Zheng%20and%20Dong%20Liang%20and%20Yanjie%20Zhu&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20multi-contrast%20imaging%20technique%20in%0Awhich%20different%20contrast%20images%20share%20similar%20structural%20information.%20However%2C%0Aconventional%20diffusion%20models%20struggle%20to%20effectively%20leverage%20this%20structural%0Asimilarity.%20Recently%2C%20the%20Schr%5C%22odinger%20Bridge%20%28SB%29%2C%20a%20nonlinear%20extension%20of%0Athe%20diffusion%20model%2C%20has%20been%20proposed%20to%20establish%20diffusion%20paths%20between%20any%0Adistributions%2C%20allowing%20the%20incorporation%20of%20guided%20priors.%20This%20study%20proposes%0Aan%20SB-based%2C%20multi-contrast%20image-guided%20reconstruction%20framework%20that%0Aestablishes%20a%20diffusion%20bridge%20between%20the%20guiding%20and%20target%20image%0Adistributions.%20By%20using%20the%20guiding%20image%20along%20with%20data%20consistency%20during%0Asampling%2C%20the%20target%20image%20is%20reconstructed%20more%20accurately.%20To%20better%20address%0Astructural%20differences%20between%20images%2C%20we%20introduce%20an%20inversion%20strategy%20from%0Athe%20field%20of%20image%20editing%2C%20termed%20%24%5Cmathbf%7BI%7D%5E2%24SB-inversion.%20Experiments%20on%20a%0Aparied%20T1%20and%20T2-FLAIR%20datasets%20demonstrate%20that%20%24%5Cmathbf%7BI%7D%5E2%24SB-inversion%0Aachieve%20a%20high%20acceleration%20up%20to%2014.4%20and%20outperforms%20existing%20methods%20in%0Aterms%20of%20both%20reconstruction%20accuracy%20and%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14269v1&entry.124074799=Read"},
{"title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in\n  Language Models", "author": "Javier Ferrando and Oscar Obeso and Senthooran Rajamanoharan and Neel Nanda", "abstract": "  Hallucinations in large language models are a widespread problem, yet the\nmechanisms behind whether models will hallucinate are poorly understood,\nlimiting our ability to solve this problem. Using sparse autoencoders as an\ninterpretability tool, we discover that a key part of these mechanisms is\nentity recognition, where the model detects if an entity is one it can recall\nfacts about. Sparse autoencoders uncover meaningful directions in the\nrepresentation space, these detect whether the model recognizes an entity, e.g.\ndetecting it doesn't know about an athlete or a movie. This suggests that\nmodels can have self-knowledge: internal representations about their own\ncapabilities. These directions are causally relevant: capable of steering the\nmodel to refuse to answer questions about known entities, or to hallucinate\nattributes of unknown entities when it would otherwise refuse. We demonstrate\nthat despite the sparse autoencoders being trained on the base model, these\ndirections have a causal effect on the chat model's refusal behavior,\nsuggesting that chat finetuning has repurposed this existing mechanism.\nFurthermore, we provide an initial exploration into the mechanistic role of\nthese directions in the model, finding that they disrupt the attention of\ndownstream heads that typically move entity attributes to the final token.\n", "link": "http://arxiv.org/abs/2411.14257v1", "date": "2024-11-21", "relevancy": 2.11, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20I%20Know%20This%20Entity%3F%20Knowledge%20Awareness%20and%20Hallucinations%20in%0A%20%20Language%20Models&body=Title%3A%20Do%20I%20Know%20This%20Entity%3F%20Knowledge%20Awareness%20and%20Hallucinations%20in%0A%20%20Language%20Models%0AAuthor%3A%20Javier%20Ferrando%20and%20Oscar%20Obeso%20and%20Senthooran%20Rajamanoharan%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20language%20models%20are%20a%20widespread%20problem%2C%20yet%20the%0Amechanisms%20behind%20whether%20models%20will%20hallucinate%20are%20poorly%20understood%2C%0Alimiting%20our%20ability%20to%20solve%20this%20problem.%20Using%20sparse%20autoencoders%20as%20an%0Ainterpretability%20tool%2C%20we%20discover%20that%20a%20key%20part%20of%20these%20mechanisms%20is%0Aentity%20recognition%2C%20where%20the%20model%20detects%20if%20an%20entity%20is%20one%20it%20can%20recall%0Afacts%20about.%20Sparse%20autoencoders%20uncover%20meaningful%20directions%20in%20the%0Arepresentation%20space%2C%20these%20detect%20whether%20the%20model%20recognizes%20an%20entity%2C%20e.g.%0Adetecting%20it%20doesn%27t%20know%20about%20an%20athlete%20or%20a%20movie.%20This%20suggests%20that%0Amodels%20can%20have%20self-knowledge%3A%20internal%20representations%20about%20their%20own%0Acapabilities.%20These%20directions%20are%20causally%20relevant%3A%20capable%20of%20steering%20the%0Amodel%20to%20refuse%20to%20answer%20questions%20about%20known%20entities%2C%20or%20to%20hallucinate%0Aattributes%20of%20unknown%20entities%20when%20it%20would%20otherwise%20refuse.%20We%20demonstrate%0Athat%20despite%20the%20sparse%20autoencoders%20being%20trained%20on%20the%20base%20model%2C%20these%0Adirections%20have%20a%20causal%20effect%20on%20the%20chat%20model%27s%20refusal%20behavior%2C%0Asuggesting%20that%20chat%20finetuning%20has%20repurposed%20this%20existing%20mechanism.%0AFurthermore%2C%20we%20provide%20an%20initial%20exploration%20into%20the%20mechanistic%20role%20of%0Athese%20directions%20in%20the%20model%2C%20finding%20that%20they%20disrupt%20the%20attention%20of%0Adownstream%20heads%20that%20typically%20move%20entity%20attributes%20to%20the%20final%20token.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520I%2520Know%2520This%2520Entity%253F%2520Knowledge%2520Awareness%2520and%2520Hallucinations%2520in%250A%2520%2520Language%2520Models%26entry.906535625%3DJavier%2520Ferrando%2520and%2520Oscar%2520Obeso%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520language%2520models%2520are%2520a%2520widespread%2520problem%252C%2520yet%2520the%250Amechanisms%2520behind%2520whether%2520models%2520will%2520hallucinate%2520are%2520poorly%2520understood%252C%250Alimiting%2520our%2520ability%2520to%2520solve%2520this%2520problem.%2520Using%2520sparse%2520autoencoders%2520as%2520an%250Ainterpretability%2520tool%252C%2520we%2520discover%2520that%2520a%2520key%2520part%2520of%2520these%2520mechanisms%2520is%250Aentity%2520recognition%252C%2520where%2520the%2520model%2520detects%2520if%2520an%2520entity%2520is%2520one%2520it%2520can%2520recall%250Afacts%2520about.%2520Sparse%2520autoencoders%2520uncover%2520meaningful%2520directions%2520in%2520the%250Arepresentation%2520space%252C%2520these%2520detect%2520whether%2520the%2520model%2520recognizes%2520an%2520entity%252C%2520e.g.%250Adetecting%2520it%2520doesn%2527t%2520know%2520about%2520an%2520athlete%2520or%2520a%2520movie.%2520This%2520suggests%2520that%250Amodels%2520can%2520have%2520self-knowledge%253A%2520internal%2520representations%2520about%2520their%2520own%250Acapabilities.%2520These%2520directions%2520are%2520causally%2520relevant%253A%2520capable%2520of%2520steering%2520the%250Amodel%2520to%2520refuse%2520to%2520answer%2520questions%2520about%2520known%2520entities%252C%2520or%2520to%2520hallucinate%250Aattributes%2520of%2520unknown%2520entities%2520when%2520it%2520would%2520otherwise%2520refuse.%2520We%2520demonstrate%250Athat%2520despite%2520the%2520sparse%2520autoencoders%2520being%2520trained%2520on%2520the%2520base%2520model%252C%2520these%250Adirections%2520have%2520a%2520causal%2520effect%2520on%2520the%2520chat%2520model%2527s%2520refusal%2520behavior%252C%250Asuggesting%2520that%2520chat%2520finetuning%2520has%2520repurposed%2520this%2520existing%2520mechanism.%250AFurthermore%252C%2520we%2520provide%2520an%2520initial%2520exploration%2520into%2520the%2520mechanistic%2520role%2520of%250Athese%2520directions%2520in%2520the%2520model%252C%2520finding%2520that%2520they%2520disrupt%2520the%2520attention%2520of%250Adownstream%2520heads%2520that%2520typically%2520move%2520entity%2520attributes%2520to%2520the%2520final%2520token.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20I%20Know%20This%20Entity%3F%20Knowledge%20Awareness%20and%20Hallucinations%20in%0A%20%20Language%20Models&entry.906535625=Javier%20Ferrando%20and%20Oscar%20Obeso%20and%20Senthooran%20Rajamanoharan%20and%20Neel%20Nanda&entry.1292438233=%20%20Hallucinations%20in%20large%20language%20models%20are%20a%20widespread%20problem%2C%20yet%20the%0Amechanisms%20behind%20whether%20models%20will%20hallucinate%20are%20poorly%20understood%2C%0Alimiting%20our%20ability%20to%20solve%20this%20problem.%20Using%20sparse%20autoencoders%20as%20an%0Ainterpretability%20tool%2C%20we%20discover%20that%20a%20key%20part%20of%20these%20mechanisms%20is%0Aentity%20recognition%2C%20where%20the%20model%20detects%20if%20an%20entity%20is%20one%20it%20can%20recall%0Afacts%20about.%20Sparse%20autoencoders%20uncover%20meaningful%20directions%20in%20the%0Arepresentation%20space%2C%20these%20detect%20whether%20the%20model%20recognizes%20an%20entity%2C%20e.g.%0Adetecting%20it%20doesn%27t%20know%20about%20an%20athlete%20or%20a%20movie.%20This%20suggests%20that%0Amodels%20can%20have%20self-knowledge%3A%20internal%20representations%20about%20their%20own%0Acapabilities.%20These%20directions%20are%20causally%20relevant%3A%20capable%20of%20steering%20the%0Amodel%20to%20refuse%20to%20answer%20questions%20about%20known%20entities%2C%20or%20to%20hallucinate%0Aattributes%20of%20unknown%20entities%20when%20it%20would%20otherwise%20refuse.%20We%20demonstrate%0Athat%20despite%20the%20sparse%20autoencoders%20being%20trained%20on%20the%20base%20model%2C%20these%0Adirections%20have%20a%20causal%20effect%20on%20the%20chat%20model%27s%20refusal%20behavior%2C%0Asuggesting%20that%20chat%20finetuning%20has%20repurposed%20this%20existing%20mechanism.%0AFurthermore%2C%20we%20provide%20an%20initial%20exploration%20into%20the%20mechanistic%20role%20of%0Athese%20directions%20in%20the%20model%2C%20finding%20that%20they%20disrupt%20the%20attention%20of%0Adownstream%20heads%20that%20typically%20move%20entity%20attributes%20to%20the%20final%20token.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14257v1&entry.124074799=Read"},
{"title": "Landing Trajectory Prediction for UAS Based on Generative Adversarial\n  Network", "author": "Jun Xiang and Drake Essick and Luiz Gonzalez Bautista and Junfei Xie and Jun Chen", "abstract": "  Models for trajectory prediction are an essential component of many advanced\nair mobility studies. These models help aircraft detect conflict and plan\navoidance maneuvers, which is especially important in Unmanned Aircraft systems\n(UAS) landing management due to the congested airspace near vertiports. In this\npaper, we propose a landing trajectory prediction model for UAS based on\nGenerative Adversarial Network (GAN). The GAN is a prestigious neural network\nthat has been developed for many years. In previous research, GAN has achieved\nmany state-of-the-art results in many generation tasks. The GAN consists of one\nneural network generator and a neural network discriminator. Because of the\nlearning capacity of the neural networks, the generator is capable to\nunderstand the features of the sample trajectory. The generator takes the\nprevious trajectory as input and outputs some random status of a flight.\nAccording to the results of the experiences, the proposed model can output more\naccurate predictions than the baseline method(GMR) in various datasets. To\nevaluate the proposed model, we also create a real UAV landing dataset that\nincludes more than 2600 trajectories of drone control manually by real pilots.\n", "link": "http://arxiv.org/abs/2411.14403v1", "date": "2024-11-21", "relevancy": 2.1069, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Landing%20Trajectory%20Prediction%20for%20UAS%20Based%20on%20Generative%20Adversarial%0A%20%20Network&body=Title%3A%20Landing%20Trajectory%20Prediction%20for%20UAS%20Based%20on%20Generative%20Adversarial%0A%20%20Network%0AAuthor%3A%20Jun%20Xiang%20and%20Drake%20Essick%20and%20Luiz%20Gonzalez%20Bautista%20and%20Junfei%20Xie%20and%20Jun%20Chen%0AAbstract%3A%20%20%20Models%20for%20trajectory%20prediction%20are%20an%20essential%20component%20of%20many%20advanced%0Aair%20mobility%20studies.%20These%20models%20help%20aircraft%20detect%20conflict%20and%20plan%0Aavoidance%20maneuvers%2C%20which%20is%20especially%20important%20in%20Unmanned%20Aircraft%20systems%0A%28UAS%29%20landing%20management%20due%20to%20the%20congested%20airspace%20near%20vertiports.%20In%20this%0Apaper%2C%20we%20propose%20a%20landing%20trajectory%20prediction%20model%20for%20UAS%20based%20on%0AGenerative%20Adversarial%20Network%20%28GAN%29.%20The%20GAN%20is%20a%20prestigious%20neural%20network%0Athat%20has%20been%20developed%20for%20many%20years.%20In%20previous%20research%2C%20GAN%20has%20achieved%0Amany%20state-of-the-art%20results%20in%20many%20generation%20tasks.%20The%20GAN%20consists%20of%20one%0Aneural%20network%20generator%20and%20a%20neural%20network%20discriminator.%20Because%20of%20the%0Alearning%20capacity%20of%20the%20neural%20networks%2C%20the%20generator%20is%20capable%20to%0Aunderstand%20the%20features%20of%20the%20sample%20trajectory.%20The%20generator%20takes%20the%0Aprevious%20trajectory%20as%20input%20and%20outputs%20some%20random%20status%20of%20a%20flight.%0AAccording%20to%20the%20results%20of%20the%20experiences%2C%20the%20proposed%20model%20can%20output%20more%0Aaccurate%20predictions%20than%20the%20baseline%20method%28GMR%29%20in%20various%20datasets.%20To%0Aevaluate%20the%20proposed%20model%2C%20we%20also%20create%20a%20real%20UAV%20landing%20dataset%20that%0Aincludes%20more%20than%202600%20trajectories%20of%20drone%20control%20manually%20by%20real%20pilots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanding%2520Trajectory%2520Prediction%2520for%2520UAS%2520Based%2520on%2520Generative%2520Adversarial%250A%2520%2520Network%26entry.906535625%3DJun%2520Xiang%2520and%2520Drake%2520Essick%2520and%2520Luiz%2520Gonzalez%2520Bautista%2520and%2520Junfei%2520Xie%2520and%2520Jun%2520Chen%26entry.1292438233%3D%2520%2520Models%2520for%2520trajectory%2520prediction%2520are%2520an%2520essential%2520component%2520of%2520many%2520advanced%250Aair%2520mobility%2520studies.%2520These%2520models%2520help%2520aircraft%2520detect%2520conflict%2520and%2520plan%250Aavoidance%2520maneuvers%252C%2520which%2520is%2520especially%2520important%2520in%2520Unmanned%2520Aircraft%2520systems%250A%2528UAS%2529%2520landing%2520management%2520due%2520to%2520the%2520congested%2520airspace%2520near%2520vertiports.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520landing%2520trajectory%2520prediction%2520model%2520for%2520UAS%2520based%2520on%250AGenerative%2520Adversarial%2520Network%2520%2528GAN%2529.%2520The%2520GAN%2520is%2520a%2520prestigious%2520neural%2520network%250Athat%2520has%2520been%2520developed%2520for%2520many%2520years.%2520In%2520previous%2520research%252C%2520GAN%2520has%2520achieved%250Amany%2520state-of-the-art%2520results%2520in%2520many%2520generation%2520tasks.%2520The%2520GAN%2520consists%2520of%2520one%250Aneural%2520network%2520generator%2520and%2520a%2520neural%2520network%2520discriminator.%2520Because%2520of%2520the%250Alearning%2520capacity%2520of%2520the%2520neural%2520networks%252C%2520the%2520generator%2520is%2520capable%2520to%250Aunderstand%2520the%2520features%2520of%2520the%2520sample%2520trajectory.%2520The%2520generator%2520takes%2520the%250Aprevious%2520trajectory%2520as%2520input%2520and%2520outputs%2520some%2520random%2520status%2520of%2520a%2520flight.%250AAccording%2520to%2520the%2520results%2520of%2520the%2520experiences%252C%2520the%2520proposed%2520model%2520can%2520output%2520more%250Aaccurate%2520predictions%2520than%2520the%2520baseline%2520method%2528GMR%2529%2520in%2520various%2520datasets.%2520To%250Aevaluate%2520the%2520proposed%2520model%252C%2520we%2520also%2520create%2520a%2520real%2520UAV%2520landing%2520dataset%2520that%250Aincludes%2520more%2520than%25202600%2520trajectories%2520of%2520drone%2520control%2520manually%2520by%2520real%2520pilots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Landing%20Trajectory%20Prediction%20for%20UAS%20Based%20on%20Generative%20Adversarial%0A%20%20Network&entry.906535625=Jun%20Xiang%20and%20Drake%20Essick%20and%20Luiz%20Gonzalez%20Bautista%20and%20Junfei%20Xie%20and%20Jun%20Chen&entry.1292438233=%20%20Models%20for%20trajectory%20prediction%20are%20an%20essential%20component%20of%20many%20advanced%0Aair%20mobility%20studies.%20These%20models%20help%20aircraft%20detect%20conflict%20and%20plan%0Aavoidance%20maneuvers%2C%20which%20is%20especially%20important%20in%20Unmanned%20Aircraft%20systems%0A%28UAS%29%20landing%20management%20due%20to%20the%20congested%20airspace%20near%20vertiports.%20In%20this%0Apaper%2C%20we%20propose%20a%20landing%20trajectory%20prediction%20model%20for%20UAS%20based%20on%0AGenerative%20Adversarial%20Network%20%28GAN%29.%20The%20GAN%20is%20a%20prestigious%20neural%20network%0Athat%20has%20been%20developed%20for%20many%20years.%20In%20previous%20research%2C%20GAN%20has%20achieved%0Amany%20state-of-the-art%20results%20in%20many%20generation%20tasks.%20The%20GAN%20consists%20of%20one%0Aneural%20network%20generator%20and%20a%20neural%20network%20discriminator.%20Because%20of%20the%0Alearning%20capacity%20of%20the%20neural%20networks%2C%20the%20generator%20is%20capable%20to%0Aunderstand%20the%20features%20of%20the%20sample%20trajectory.%20The%20generator%20takes%20the%0Aprevious%20trajectory%20as%20input%20and%20outputs%20some%20random%20status%20of%20a%20flight.%0AAccording%20to%20the%20results%20of%20the%20experiences%2C%20the%20proposed%20model%20can%20output%20more%0Aaccurate%20predictions%20than%20the%20baseline%20method%28GMR%29%20in%20various%20datasets.%20To%0Aevaluate%20the%20proposed%20model%2C%20we%20also%20create%20a%20real%20UAV%20landing%20dataset%20that%0Aincludes%20more%20than%202600%20trajectories%20of%20drone%20control%20manually%20by%20real%20pilots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14403v1&entry.124074799=Read"},
{"title": "Convex Approximation of Probabilistic Reachable Sets from Small Samples\n  Using Self-supervised Neural Networks", "author": "Jun Xiang and Jun Chen", "abstract": "  Probabilistic Reachable Set (PRS) plays a crucial role in many fields of\nautonomous systems, yet efficiently generating PRS remains a significant\nchallenge. This paper presents a learning approach to generating 2-dimensional\nPRS for states in a dynamic system. Traditional methods such as Hamilton-Jacobi\nreachability analysis, Monte Carlo, and Gaussian process classification face\nsignificant computational challenges or require detailed dynamics information,\nlimiting their applicability in realistic situations. Existing data-driven\nmethods may lack accuracy. To overcome these limitations, we propose leveraging\nneural networks, commonly used in imitation learning and computer vision, to\nimitate expert methods to generate PRS approximations. We trained the neural\nnetworks using a multi-label, self-supervised learning approach. We selected\nthe fine-tuned convex approximation method as the expert to create expert PRS.\nAdditionally, we continued sampling from the distribution to obtain a diverse\narray of sample sets. Given a small sample set, the trained neural networks can\nreplicate the PRS approximation generated by the expert method, while the\ngeneration speed is much faster.\n", "link": "http://arxiv.org/abs/2411.14356v1", "date": "2024-11-21", "relevancy": 2.0971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5191}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Approximation%20of%20Probabilistic%20Reachable%20Sets%20from%20Small%20Samples%0A%20%20Using%20Self-supervised%20Neural%20Networks&body=Title%3A%20Convex%20Approximation%20of%20Probabilistic%20Reachable%20Sets%20from%20Small%20Samples%0A%20%20Using%20Self-supervised%20Neural%20Networks%0AAuthor%3A%20Jun%20Xiang%20and%20Jun%20Chen%0AAbstract%3A%20%20%20Probabilistic%20Reachable%20Set%20%28PRS%29%20plays%20a%20crucial%20role%20in%20many%20fields%20of%0Aautonomous%20systems%2C%20yet%20efficiently%20generating%20PRS%20remains%20a%20significant%0Achallenge.%20This%20paper%20presents%20a%20learning%20approach%20to%20generating%202-dimensional%0APRS%20for%20states%20in%20a%20dynamic%20system.%20Traditional%20methods%20such%20as%20Hamilton-Jacobi%0Areachability%20analysis%2C%20Monte%20Carlo%2C%20and%20Gaussian%20process%20classification%20face%0Asignificant%20computational%20challenges%20or%20require%20detailed%20dynamics%20information%2C%0Alimiting%20their%20applicability%20in%20realistic%20situations.%20Existing%20data-driven%0Amethods%20may%20lack%20accuracy.%20To%20overcome%20these%20limitations%2C%20we%20propose%20leveraging%0Aneural%20networks%2C%20commonly%20used%20in%20imitation%20learning%20and%20computer%20vision%2C%20to%0Aimitate%20expert%20methods%20to%20generate%20PRS%20approximations.%20We%20trained%20the%20neural%0Anetworks%20using%20a%20multi-label%2C%20self-supervised%20learning%20approach.%20We%20selected%0Athe%20fine-tuned%20convex%20approximation%20method%20as%20the%20expert%20to%20create%20expert%20PRS.%0AAdditionally%2C%20we%20continued%20sampling%20from%20the%20distribution%20to%20obtain%20a%20diverse%0Aarray%20of%20sample%20sets.%20Given%20a%20small%20sample%20set%2C%20the%20trained%20neural%20networks%20can%0Areplicate%20the%20PRS%20approximation%20generated%20by%20the%20expert%20method%2C%20while%20the%0Ageneration%20speed%20is%20much%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Approximation%2520of%2520Probabilistic%2520Reachable%2520Sets%2520from%2520Small%2520Samples%250A%2520%2520Using%2520Self-supervised%2520Neural%2520Networks%26entry.906535625%3DJun%2520Xiang%2520and%2520Jun%2520Chen%26entry.1292438233%3D%2520%2520Probabilistic%2520Reachable%2520Set%2520%2528PRS%2529%2520plays%2520a%2520crucial%2520role%2520in%2520many%2520fields%2520of%250Aautonomous%2520systems%252C%2520yet%2520efficiently%2520generating%2520PRS%2520remains%2520a%2520significant%250Achallenge.%2520This%2520paper%2520presents%2520a%2520learning%2520approach%2520to%2520generating%25202-dimensional%250APRS%2520for%2520states%2520in%2520a%2520dynamic%2520system.%2520Traditional%2520methods%2520such%2520as%2520Hamilton-Jacobi%250Areachability%2520analysis%252C%2520Monte%2520Carlo%252C%2520and%2520Gaussian%2520process%2520classification%2520face%250Asignificant%2520computational%2520challenges%2520or%2520require%2520detailed%2520dynamics%2520information%252C%250Alimiting%2520their%2520applicability%2520in%2520realistic%2520situations.%2520Existing%2520data-driven%250Amethods%2520may%2520lack%2520accuracy.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520leveraging%250Aneural%2520networks%252C%2520commonly%2520used%2520in%2520imitation%2520learning%2520and%2520computer%2520vision%252C%2520to%250Aimitate%2520expert%2520methods%2520to%2520generate%2520PRS%2520approximations.%2520We%2520trained%2520the%2520neural%250Anetworks%2520using%2520a%2520multi-label%252C%2520self-supervised%2520learning%2520approach.%2520We%2520selected%250Athe%2520fine-tuned%2520convex%2520approximation%2520method%2520as%2520the%2520expert%2520to%2520create%2520expert%2520PRS.%250AAdditionally%252C%2520we%2520continued%2520sampling%2520from%2520the%2520distribution%2520to%2520obtain%2520a%2520diverse%250Aarray%2520of%2520sample%2520sets.%2520Given%2520a%2520small%2520sample%2520set%252C%2520the%2520trained%2520neural%2520networks%2520can%250Areplicate%2520the%2520PRS%2520approximation%2520generated%2520by%2520the%2520expert%2520method%252C%2520while%2520the%250Ageneration%2520speed%2520is%2520much%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Approximation%20of%20Probabilistic%20Reachable%20Sets%20from%20Small%20Samples%0A%20%20Using%20Self-supervised%20Neural%20Networks&entry.906535625=Jun%20Xiang%20and%20Jun%20Chen&entry.1292438233=%20%20Probabilistic%20Reachable%20Set%20%28PRS%29%20plays%20a%20crucial%20role%20in%20many%20fields%20of%0Aautonomous%20systems%2C%20yet%20efficiently%20generating%20PRS%20remains%20a%20significant%0Achallenge.%20This%20paper%20presents%20a%20learning%20approach%20to%20generating%202-dimensional%0APRS%20for%20states%20in%20a%20dynamic%20system.%20Traditional%20methods%20such%20as%20Hamilton-Jacobi%0Areachability%20analysis%2C%20Monte%20Carlo%2C%20and%20Gaussian%20process%20classification%20face%0Asignificant%20computational%20challenges%20or%20require%20detailed%20dynamics%20information%2C%0Alimiting%20their%20applicability%20in%20realistic%20situations.%20Existing%20data-driven%0Amethods%20may%20lack%20accuracy.%20To%20overcome%20these%20limitations%2C%20we%20propose%20leveraging%0Aneural%20networks%2C%20commonly%20used%20in%20imitation%20learning%20and%20computer%20vision%2C%20to%0Aimitate%20expert%20methods%20to%20generate%20PRS%20approximations.%20We%20trained%20the%20neural%0Anetworks%20using%20a%20multi-label%2C%20self-supervised%20learning%20approach.%20We%20selected%0Athe%20fine-tuned%20convex%20approximation%20method%20as%20the%20expert%20to%20create%20expert%20PRS.%0AAdditionally%2C%20we%20continued%20sampling%20from%20the%20distribution%20to%20obtain%20a%20diverse%0Aarray%20of%20sample%20sets.%20Given%20a%20small%20sample%20set%2C%20the%20trained%20neural%20networks%20can%0Areplicate%20the%20PRS%20approximation%20generated%20by%20the%20expert%20method%2C%20while%20the%0Ageneration%20speed%20is%20much%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14356v1&entry.124074799=Read"},
{"title": "AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision\n  Transformer", "author": "Yitao Xu and Tong Zhang and Sabine S\u00fcsstrunk", "abstract": "  Vision Transformers (ViTs) demonstrate remarkable performance in image\nclassification through visual-token interaction learning, particularly when\nequipped with local information via region attention or convolutions. Although\nsuch architectures improve the feature aggregation from different\ngranularities, they often fail to contribute to the robustness of the networks.\nNeural Cellular Automata (NCA) enables the modeling of global visual-token\nrepresentations through local interactions, with its training strategies and\narchitecture design conferring strong generalization ability and robustness\nagainst noisy input. In this paper, we propose Adaptor Neural Cellular Automata\n(AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors\nbetween ViT layers, thus enhancing ViT's performance and robustness against\nadversarial samples as well as out-of-distribution inputs. To overcome the\nlarge computational overhead of standard NCAs, we propose Dynamic Interaction\nfor more efficient interaction learning. Using our analysis of AdaNCA placement\nand robustness improvement, we also develop an algorithm for identifying the\nmost effective insertion points for AdaNCA. With less than a 3% increase in\nparameters, AdaNCA contributes to more than 10% absolute improvement in\naccuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we\ndemonstrate with extensive evaluations across eight robustness benchmarks and\nfour ViT architectures that AdaNCA, as a plug-and-play module, consistently\nimproves the robustness of ViTs.\n", "link": "http://arxiv.org/abs/2406.08298v5", "date": "2024-11-21", "relevancy": 2.0916, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5119}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&body=Title%3A%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer%0AAuthor%3A%20Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20demonstrate%20remarkable%20performance%20in%20image%0Aclassification%20through%20visual-token%20interaction%20learning%2C%20particularly%20when%0Aequipped%20with%20local%20information%20via%20region%20attention%20or%20convolutions.%20Although%0Asuch%20architectures%20improve%20the%20feature%20aggregation%20from%20different%0Agranularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%20robustness%20of%20the%20networks.%0ANeural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%20of%20global%20visual-token%0Arepresentations%20through%20local%20interactions%2C%20with%20its%20training%20strategies%20and%0Aarchitecture%20design%20conferring%20strong%20generalization%20ability%20and%20robustness%0Aagainst%20noisy%20input.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%20Cellular%20Automata%0A%28AdaNCA%29%20for%20Vision%20Transformers%20that%20uses%20NCA%20as%20plug-and-play%20adaptors%0Abetween%20ViT%20layers%2C%20thus%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Using%20our%20analysis%20of%20AdaNCA%20placement%0Aand%20robustness%20improvement%2C%20we%20also%20develop%20an%20algorithm%20for%20identifying%20the%0Amost%20effective%20insertion%20points%20for%20AdaNCA.%20With%20less%20than%20a%203%25%20increase%20in%0Aparameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%20improvement%20in%0Aaccuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%20Moreover%2C%20we%0Ademonstrate%20with%20extensive%20evaluations%20across%20eight%20robustness%20benchmarks%20and%0Afour%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-and-play%20module%2C%20consistently%0Aimproves%20the%20robustness%20of%20ViTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08298v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaNCA%253A%2520Neural%2520Cellular%2520Automata%2520As%2520Adaptors%2520For%2520More%2520Robust%2520Vision%250A%2520%2520Transformer%26entry.906535625%3DYitao%2520Xu%2520and%2520Tong%2520Zhang%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520demonstrate%2520remarkable%2520performance%2520in%2520image%250Aclassification%2520through%2520visual-token%2520interaction%2520learning%252C%2520particularly%2520when%250Aequipped%2520with%2520local%2520information%2520via%2520region%2520attention%2520or%2520convolutions.%2520Although%250Asuch%2520architectures%2520improve%2520the%2520feature%2520aggregation%2520from%2520different%250Agranularities%252C%2520they%2520often%2520fail%2520to%2520contribute%2520to%2520the%2520robustness%2520of%2520the%2520networks.%250ANeural%2520Cellular%2520Automata%2520%2528NCA%2529%2520enables%2520the%2520modeling%2520of%2520global%2520visual-token%250Arepresentations%2520through%2520local%2520interactions%252C%2520with%2520its%2520training%2520strategies%2520and%250Aarchitecture%2520design%2520conferring%2520strong%2520generalization%2520ability%2520and%2520robustness%250Aagainst%2520noisy%2520input.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Adaptor%2520Neural%2520Cellular%2520Automata%250A%2528AdaNCA%2529%2520for%2520Vision%2520Transformers%2520that%2520uses%2520NCA%2520as%2520plug-and-play%2520adaptors%250Abetween%2520ViT%2520layers%252C%2520thus%2520enhancing%2520ViT%2527s%2520performance%2520and%2520robustness%2520against%250Aadversarial%2520samples%2520as%2520well%2520as%2520out-of-distribution%2520inputs.%2520To%2520overcome%2520the%250Alarge%2520computational%2520overhead%2520of%2520standard%2520NCAs%252C%2520we%2520propose%2520Dynamic%2520Interaction%250Afor%2520more%2520efficient%2520interaction%2520learning.%2520Using%2520our%2520analysis%2520of%2520AdaNCA%2520placement%250Aand%2520robustness%2520improvement%252C%2520we%2520also%2520develop%2520an%2520algorithm%2520for%2520identifying%2520the%250Amost%2520effective%2520insertion%2520points%2520for%2520AdaNCA.%2520With%2520less%2520than%2520a%25203%2525%2520increase%2520in%250Aparameters%252C%2520AdaNCA%2520contributes%2520to%2520more%2520than%252010%2525%2520absolute%2520improvement%2520in%250Aaccuracy%2520under%2520adversarial%2520attacks%2520on%2520the%2520ImageNet1K%2520benchmark.%2520Moreover%252C%2520we%250Ademonstrate%2520with%2520extensive%2520evaluations%2520across%2520eight%2520robustness%2520benchmarks%2520and%250Afour%2520ViT%2520architectures%2520that%2520AdaNCA%252C%2520as%2520a%2520plug-and-play%2520module%252C%2520consistently%250Aimproves%2520the%2520robustness%2520of%2520ViTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08298v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&entry.906535625=Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20demonstrate%20remarkable%20performance%20in%20image%0Aclassification%20through%20visual-token%20interaction%20learning%2C%20particularly%20when%0Aequipped%20with%20local%20information%20via%20region%20attention%20or%20convolutions.%20Although%0Asuch%20architectures%20improve%20the%20feature%20aggregation%20from%20different%0Agranularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%20robustness%20of%20the%20networks.%0ANeural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%20of%20global%20visual-token%0Arepresentations%20through%20local%20interactions%2C%20with%20its%20training%20strategies%20and%0Aarchitecture%20design%20conferring%20strong%20generalization%20ability%20and%20robustness%0Aagainst%20noisy%20input.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%20Cellular%20Automata%0A%28AdaNCA%29%20for%20Vision%20Transformers%20that%20uses%20NCA%20as%20plug-and-play%20adaptors%0Abetween%20ViT%20layers%2C%20thus%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Using%20our%20analysis%20of%20AdaNCA%20placement%0Aand%20robustness%20improvement%2C%20we%20also%20develop%20an%20algorithm%20for%20identifying%20the%0Amost%20effective%20insertion%20points%20for%20AdaNCA.%20With%20less%20than%20a%203%25%20increase%20in%0Aparameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%20improvement%20in%0Aaccuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%20Moreover%2C%20we%0Ademonstrate%20with%20extensive%20evaluations%20across%20eight%20robustness%20benchmarks%20and%0Afour%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-and-play%20module%2C%20consistently%0Aimproves%20the%20robustness%20of%20ViTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08298v5&entry.124074799=Read"},
{"title": "Dual Attention Model with Reinforcement Learning for Classification of\n  Histology Whole-Slide Images", "author": "Manahil Raza and Ruqayya Awan and Raja Muhammad Saad Bashir and Talha Qaiser and Nasir M. Rajpoot", "abstract": "  Digital whole slide images (WSIs) are generally captured at microscopic\nresolution and encompass extensive spatial data. Directly feeding these images\nto deep learning models is computationally intractable due to memory\nconstraints, while downsampling the WSIs risks incurring information loss.\nAlternatively, splitting the WSIs into smaller patches may result in a loss of\nimportant contextual information. In this paper, we propose a novel dual\nattention approach, consisting of two main components, both inspired by the\nvisual examination process of a pathologist: The first soft attention model\nprocesses a low magnification view of the WSI to identify relevant regions of\ninterest, followed by a custom sampling method to extract diverse and spatially\ndistinct image tiles from the selected ROIs. The second component, the hard\nattention classification model further extracts a sequence of multi-resolution\nglimpses from each tile for classification. Since hard attention is\nnon-differentiable, we train this component using reinforcement learning to\npredict the location of the glimpses. This approach allows the model to focus\non essential regions instead of processing the entire tile, thereby aligning\nwith a pathologist's way of diagnosis. The two components are trained in an\nend-to-end fashion using a joint loss function to demonstrate the efficacy of\nthe model. The proposed model was evaluated on two WSI-level classification\nproblems: Human epidermal growth factor receptor 2 scoring on breast cancer\nhistology images and prediction of Intact/Loss status of two Mismatch Repair\nbiomarkers from colorectal cancer histology images. We show that the proposed\nmodel achieves performance better than or comparable to the state-of-the-art\nmethods while processing less than 10% of the WSI at the highest magnification\nand reducing the time required to infer the WSI-level label by more than 75%.\n", "link": "http://arxiv.org/abs/2302.09682v2", "date": "2024-11-21", "relevancy": 2.0826, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Attention%20Model%20with%20Reinforcement%20Learning%20for%20Classification%20of%0A%20%20Histology%20Whole-Slide%20Images&body=Title%3A%20Dual%20Attention%20Model%20with%20Reinforcement%20Learning%20for%20Classification%20of%0A%20%20Histology%20Whole-Slide%20Images%0AAuthor%3A%20Manahil%20Raza%20and%20Ruqayya%20Awan%20and%20Raja%20Muhammad%20Saad%20Bashir%20and%20Talha%20Qaiser%20and%20Nasir%20M.%20Rajpoot%0AAbstract%3A%20%20%20Digital%20whole%20slide%20images%20%28WSIs%29%20are%20generally%20captured%20at%20microscopic%0Aresolution%20and%20encompass%20extensive%20spatial%20data.%20Directly%20feeding%20these%20images%0Ato%20deep%20learning%20models%20is%20computationally%20intractable%20due%20to%20memory%0Aconstraints%2C%20while%20downsampling%20the%20WSIs%20risks%20incurring%20information%20loss.%0AAlternatively%2C%20splitting%20the%20WSIs%20into%20smaller%20patches%20may%20result%20in%20a%20loss%20of%0Aimportant%20contextual%20information.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20dual%0Aattention%20approach%2C%20consisting%20of%20two%20main%20components%2C%20both%20inspired%20by%20the%0Avisual%20examination%20process%20of%20a%20pathologist%3A%20The%20first%20soft%20attention%20model%0Aprocesses%20a%20low%20magnification%20view%20of%20the%20WSI%20to%20identify%20relevant%20regions%20of%0Ainterest%2C%20followed%20by%20a%20custom%20sampling%20method%20to%20extract%20diverse%20and%20spatially%0Adistinct%20image%20tiles%20from%20the%20selected%20ROIs.%20The%20second%20component%2C%20the%20hard%0Aattention%20classification%20model%20further%20extracts%20a%20sequence%20of%20multi-resolution%0Aglimpses%20from%20each%20tile%20for%20classification.%20Since%20hard%20attention%20is%0Anon-differentiable%2C%20we%20train%20this%20component%20using%20reinforcement%20learning%20to%0Apredict%20the%20location%20of%20the%20glimpses.%20This%20approach%20allows%20the%20model%20to%20focus%0Aon%20essential%20regions%20instead%20of%20processing%20the%20entire%20tile%2C%20thereby%20aligning%0Awith%20a%20pathologist%27s%20way%20of%20diagnosis.%20The%20two%20components%20are%20trained%20in%20an%0Aend-to-end%20fashion%20using%20a%20joint%20loss%20function%20to%20demonstrate%20the%20efficacy%20of%0Athe%20model.%20The%20proposed%20model%20was%20evaluated%20on%20two%20WSI-level%20classification%0Aproblems%3A%20Human%20epidermal%20growth%20factor%20receptor%202%20scoring%20on%20breast%20cancer%0Ahistology%20images%20and%20prediction%20of%20Intact/Loss%20status%20of%20two%20Mismatch%20Repair%0Abiomarkers%20from%20colorectal%20cancer%20histology%20images.%20We%20show%20that%20the%20proposed%0Amodel%20achieves%20performance%20better%20than%20or%20comparable%20to%20the%20state-of-the-art%0Amethods%20while%20processing%20less%20than%2010%25%20of%20the%20WSI%20at%20the%20highest%20magnification%0Aand%20reducing%20the%20time%20required%20to%20infer%20the%20WSI-level%20label%20by%20more%20than%2075%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.09682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Attention%2520Model%2520with%2520Reinforcement%2520Learning%2520for%2520Classification%2520of%250A%2520%2520Histology%2520Whole-Slide%2520Images%26entry.906535625%3DManahil%2520Raza%2520and%2520Ruqayya%2520Awan%2520and%2520Raja%2520Muhammad%2520Saad%2520Bashir%2520and%2520Talha%2520Qaiser%2520and%2520Nasir%2520M.%2520Rajpoot%26entry.1292438233%3D%2520%2520Digital%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520are%2520generally%2520captured%2520at%2520microscopic%250Aresolution%2520and%2520encompass%2520extensive%2520spatial%2520data.%2520Directly%2520feeding%2520these%2520images%250Ato%2520deep%2520learning%2520models%2520is%2520computationally%2520intractable%2520due%2520to%2520memory%250Aconstraints%252C%2520while%2520downsampling%2520the%2520WSIs%2520risks%2520incurring%2520information%2520loss.%250AAlternatively%252C%2520splitting%2520the%2520WSIs%2520into%2520smaller%2520patches%2520may%2520result%2520in%2520a%2520loss%2520of%250Aimportant%2520contextual%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520dual%250Aattention%2520approach%252C%2520consisting%2520of%2520two%2520main%2520components%252C%2520both%2520inspired%2520by%2520the%250Avisual%2520examination%2520process%2520of%2520a%2520pathologist%253A%2520The%2520first%2520soft%2520attention%2520model%250Aprocesses%2520a%2520low%2520magnification%2520view%2520of%2520the%2520WSI%2520to%2520identify%2520relevant%2520regions%2520of%250Ainterest%252C%2520followed%2520by%2520a%2520custom%2520sampling%2520method%2520to%2520extract%2520diverse%2520and%2520spatially%250Adistinct%2520image%2520tiles%2520from%2520the%2520selected%2520ROIs.%2520The%2520second%2520component%252C%2520the%2520hard%250Aattention%2520classification%2520model%2520further%2520extracts%2520a%2520sequence%2520of%2520multi-resolution%250Aglimpses%2520from%2520each%2520tile%2520for%2520classification.%2520Since%2520hard%2520attention%2520is%250Anon-differentiable%252C%2520we%2520train%2520this%2520component%2520using%2520reinforcement%2520learning%2520to%250Apredict%2520the%2520location%2520of%2520the%2520glimpses.%2520This%2520approach%2520allows%2520the%2520model%2520to%2520focus%250Aon%2520essential%2520regions%2520instead%2520of%2520processing%2520the%2520entire%2520tile%252C%2520thereby%2520aligning%250Awith%2520a%2520pathologist%2527s%2520way%2520of%2520diagnosis.%2520The%2520two%2520components%2520are%2520trained%2520in%2520an%250Aend-to-end%2520fashion%2520using%2520a%2520joint%2520loss%2520function%2520to%2520demonstrate%2520the%2520efficacy%2520of%250Athe%2520model.%2520The%2520proposed%2520model%2520was%2520evaluated%2520on%2520two%2520WSI-level%2520classification%250Aproblems%253A%2520Human%2520epidermal%2520growth%2520factor%2520receptor%25202%2520scoring%2520on%2520breast%2520cancer%250Ahistology%2520images%2520and%2520prediction%2520of%2520Intact/Loss%2520status%2520of%2520two%2520Mismatch%2520Repair%250Abiomarkers%2520from%2520colorectal%2520cancer%2520histology%2520images.%2520We%2520show%2520that%2520the%2520proposed%250Amodel%2520achieves%2520performance%2520better%2520than%2520or%2520comparable%2520to%2520the%2520state-of-the-art%250Amethods%2520while%2520processing%2520less%2520than%252010%2525%2520of%2520the%2520WSI%2520at%2520the%2520highest%2520magnification%250Aand%2520reducing%2520the%2520time%2520required%2520to%2520infer%2520the%2520WSI-level%2520label%2520by%2520more%2520than%252075%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.09682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Attention%20Model%20with%20Reinforcement%20Learning%20for%20Classification%20of%0A%20%20Histology%20Whole-Slide%20Images&entry.906535625=Manahil%20Raza%20and%20Ruqayya%20Awan%20and%20Raja%20Muhammad%20Saad%20Bashir%20and%20Talha%20Qaiser%20and%20Nasir%20M.%20Rajpoot&entry.1292438233=%20%20Digital%20whole%20slide%20images%20%28WSIs%29%20are%20generally%20captured%20at%20microscopic%0Aresolution%20and%20encompass%20extensive%20spatial%20data.%20Directly%20feeding%20these%20images%0Ato%20deep%20learning%20models%20is%20computationally%20intractable%20due%20to%20memory%0Aconstraints%2C%20while%20downsampling%20the%20WSIs%20risks%20incurring%20information%20loss.%0AAlternatively%2C%20splitting%20the%20WSIs%20into%20smaller%20patches%20may%20result%20in%20a%20loss%20of%0Aimportant%20contextual%20information.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20dual%0Aattention%20approach%2C%20consisting%20of%20two%20main%20components%2C%20both%20inspired%20by%20the%0Avisual%20examination%20process%20of%20a%20pathologist%3A%20The%20first%20soft%20attention%20model%0Aprocesses%20a%20low%20magnification%20view%20of%20the%20WSI%20to%20identify%20relevant%20regions%20of%0Ainterest%2C%20followed%20by%20a%20custom%20sampling%20method%20to%20extract%20diverse%20and%20spatially%0Adistinct%20image%20tiles%20from%20the%20selected%20ROIs.%20The%20second%20component%2C%20the%20hard%0Aattention%20classification%20model%20further%20extracts%20a%20sequence%20of%20multi-resolution%0Aglimpses%20from%20each%20tile%20for%20classification.%20Since%20hard%20attention%20is%0Anon-differentiable%2C%20we%20train%20this%20component%20using%20reinforcement%20learning%20to%0Apredict%20the%20location%20of%20the%20glimpses.%20This%20approach%20allows%20the%20model%20to%20focus%0Aon%20essential%20regions%20instead%20of%20processing%20the%20entire%20tile%2C%20thereby%20aligning%0Awith%20a%20pathologist%27s%20way%20of%20diagnosis.%20The%20two%20components%20are%20trained%20in%20an%0Aend-to-end%20fashion%20using%20a%20joint%20loss%20function%20to%20demonstrate%20the%20efficacy%20of%0Athe%20model.%20The%20proposed%20model%20was%20evaluated%20on%20two%20WSI-level%20classification%0Aproblems%3A%20Human%20epidermal%20growth%20factor%20receptor%202%20scoring%20on%20breast%20cancer%0Ahistology%20images%20and%20prediction%20of%20Intact/Loss%20status%20of%20two%20Mismatch%20Repair%0Abiomarkers%20from%20colorectal%20cancer%20histology%20images.%20We%20show%20that%20the%20proposed%0Amodel%20achieves%20performance%20better%20than%20or%20comparable%20to%20the%20state-of-the-art%0Amethods%20while%20processing%20less%20than%2010%25%20of%20the%20WSI%20at%20the%20highest%20magnification%0Aand%20reducing%20the%20time%20required%20to%20infer%20the%20WSI-level%20label%20by%20more%20than%2075%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.09682v2&entry.124074799=Read"},
{"title": "Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging", "author": "Michail Theologitis and Georgios Frangias and Georgios Anestis and Vasilis Samoladas and Antonios Deligiannakis", "abstract": "  The ever-growing volume and decentralized nature of data, coupled with the\nneed to harness it and extract knowledge, have led to the extensive use of\ndistributed deep learning (DDL) techniques for training. These techniques rely\non local training performed at distributed nodes using locally collected data,\nfollowed by a periodic synchronization process that combines these models to\ncreate a unified global model. However, the frequent synchronization of deep\nlearning models, encompassing millions to many billions of parameters, creates\na communication bottleneck, severely hindering scalability. Worse yet, DDL\nalgorithms typically waste valuable bandwidth and render themselves less\npractical in bandwidth-constrained federated settings by relying on overly\nsimplistic, periodic, and rigid synchronization schedules. These inefficiencies\nmake the training process increasingly impractical as they demand excessive\ntime for data communication. To address these shortcomings, we propose\nFederated Dynamic Averaging (FDA), a communication-efficient DDL strategy that\ndynamically triggers synchronization based on the value of the model variance.\nIn essence, the costly synchronization step is triggered only if the local\nmodels -- initialized from a common global model after each synchronization --\nhave significantly diverged. This decision is facilitated by the transmission\nof a small local state from each distributed node. Through extensive\nexperiments across a wide range of learning tasks we demonstrate that FDA\nreduces communication cost by orders of magnitude, compared to both traditional\nand cutting-edge communication-efficient algorithms. Additionally, we show that\nFDA maintains robust performance across diverse data heterogeneity settings.\n", "link": "http://arxiv.org/abs/2405.20988v4", "date": "2024-11-21", "relevancy": 2.0821, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5344}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5322}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&body=Title%3A%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging%0AAuthor%3A%20Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis%0AAbstract%3A%20%20%20The%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%20with%20the%0Aneed%20to%20harness%20it%20and%20extract%20knowledge%2C%20have%20led%20to%20the%20extensive%20use%20of%0Adistributed%20deep%20learning%20%28DDL%29%20techniques%20for%20training.%20These%20techniques%20rely%0Aon%20local%20training%20performed%20at%20distributed%20nodes%20using%20locally%20collected%20data%2C%0Afollowed%20by%20a%20periodic%20synchronization%20process%20that%20combines%20these%20models%20to%0Acreate%20a%20unified%20global%20model.%20However%2C%20the%20frequent%20synchronization%20of%20deep%0Alearning%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%20parameters%2C%20creates%0Aa%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%20Worse%20yet%2C%20DDL%0Aalgorithms%20typically%20waste%20valuable%20bandwidth%20and%20render%20themselves%20less%0Apractical%20in%20bandwidth-constrained%20federated%20settings%20by%20relying%20on%20overly%0Asimplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%20These%20inefficiencies%0Amake%20the%20training%20process%20increasingly%20impractical%20as%20they%20demand%20excessive%0Atime%20for%20data%20communication.%20To%20address%20these%20shortcomings%2C%20we%20propose%0AFederated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%20communication-efficient%20DDL%20strategy%20that%0Adynamically%20triggers%20synchronization%20based%20on%20the%20value%20of%20the%20model%20variance.%0AIn%20essence%2C%20the%20costly%20synchronization%20step%20is%20triggered%20only%20if%20the%20local%0Amodels%20--%20initialized%20from%20a%20common%20global%20model%20after%20each%20synchronization%20--%0Ahave%20significantly%20diverged.%20This%20decision%20is%20facilitated%20by%20the%20transmission%0Aof%20a%20small%20local%20state%20from%20each%20distributed%20node.%20Through%20extensive%0Aexperiments%20across%20a%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%0Areduces%20communication%20cost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%0Aand%20cutting-edge%20communication-efficient%20algorithms.%20Additionally%2C%20we%20show%20that%0AFDA%20maintains%20robust%20performance%20across%20diverse%20data%20heterogeneity%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20988v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Distributed%2520Deep%2520Learning%2520via%2520Federated%2520Dynamic%250A%2520%2520Averaging%26entry.906535625%3DMichail%2520Theologitis%2520and%2520Georgios%2520Frangias%2520and%2520Georgios%2520Anestis%2520and%2520Vasilis%2520Samoladas%2520and%2520Antonios%2520Deligiannakis%26entry.1292438233%3D%2520%2520The%2520ever-growing%2520volume%2520and%2520decentralized%2520nature%2520of%2520data%252C%2520coupled%2520with%2520the%250Aneed%2520to%2520harness%2520it%2520and%2520extract%2520knowledge%252C%2520have%2520led%2520to%2520the%2520extensive%2520use%2520of%250Adistributed%2520deep%2520learning%2520%2528DDL%2529%2520techniques%2520for%2520training.%2520These%2520techniques%2520rely%250Aon%2520local%2520training%2520performed%2520at%2520distributed%2520nodes%2520using%2520locally%2520collected%2520data%252C%250Afollowed%2520by%2520a%2520periodic%2520synchronization%2520process%2520that%2520combines%2520these%2520models%2520to%250Acreate%2520a%2520unified%2520global%2520model.%2520However%252C%2520the%2520frequent%2520synchronization%2520of%2520deep%250Alearning%2520models%252C%2520encompassing%2520millions%2520to%2520many%2520billions%2520of%2520parameters%252C%2520creates%250Aa%2520communication%2520bottleneck%252C%2520severely%2520hindering%2520scalability.%2520Worse%2520yet%252C%2520DDL%250Aalgorithms%2520typically%2520waste%2520valuable%2520bandwidth%2520and%2520render%2520themselves%2520less%250Apractical%2520in%2520bandwidth-constrained%2520federated%2520settings%2520by%2520relying%2520on%2520overly%250Asimplistic%252C%2520periodic%252C%2520and%2520rigid%2520synchronization%2520schedules.%2520These%2520inefficiencies%250Amake%2520the%2520training%2520process%2520increasingly%2520impractical%2520as%2520they%2520demand%2520excessive%250Atime%2520for%2520data%2520communication.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520propose%250AFederated%2520Dynamic%2520Averaging%2520%2528FDA%2529%252C%2520a%2520communication-efficient%2520DDL%2520strategy%2520that%250Adynamically%2520triggers%2520synchronization%2520based%2520on%2520the%2520value%2520of%2520the%2520model%2520variance.%250AIn%2520essence%252C%2520the%2520costly%2520synchronization%2520step%2520is%2520triggered%2520only%2520if%2520the%2520local%250Amodels%2520--%2520initialized%2520from%2520a%2520common%2520global%2520model%2520after%2520each%2520synchronization%2520--%250Ahave%2520significantly%2520diverged.%2520This%2520decision%2520is%2520facilitated%2520by%2520the%2520transmission%250Aof%2520a%2520small%2520local%2520state%2520from%2520each%2520distributed%2520node.%2520Through%2520extensive%250Aexperiments%2520across%2520a%2520wide%2520range%2520of%2520learning%2520tasks%2520we%2520demonstrate%2520that%2520FDA%250Areduces%2520communication%2520cost%2520by%2520orders%2520of%2520magnitude%252C%2520compared%2520to%2520both%2520traditional%250Aand%2520cutting-edge%2520communication-efficient%2520algorithms.%2520Additionally%252C%2520we%2520show%2520that%250AFDA%2520maintains%2520robust%2520performance%2520across%2520diverse%2520data%2520heterogeneity%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20988v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&entry.906535625=Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis&entry.1292438233=%20%20The%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%20with%20the%0Aneed%20to%20harness%20it%20and%20extract%20knowledge%2C%20have%20led%20to%20the%20extensive%20use%20of%0Adistributed%20deep%20learning%20%28DDL%29%20techniques%20for%20training.%20These%20techniques%20rely%0Aon%20local%20training%20performed%20at%20distributed%20nodes%20using%20locally%20collected%20data%2C%0Afollowed%20by%20a%20periodic%20synchronization%20process%20that%20combines%20these%20models%20to%0Acreate%20a%20unified%20global%20model.%20However%2C%20the%20frequent%20synchronization%20of%20deep%0Alearning%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%20parameters%2C%20creates%0Aa%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%20Worse%20yet%2C%20DDL%0Aalgorithms%20typically%20waste%20valuable%20bandwidth%20and%20render%20themselves%20less%0Apractical%20in%20bandwidth-constrained%20federated%20settings%20by%20relying%20on%20overly%0Asimplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%20These%20inefficiencies%0Amake%20the%20training%20process%20increasingly%20impractical%20as%20they%20demand%20excessive%0Atime%20for%20data%20communication.%20To%20address%20these%20shortcomings%2C%20we%20propose%0AFederated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%20communication-efficient%20DDL%20strategy%20that%0Adynamically%20triggers%20synchronization%20based%20on%20the%20value%20of%20the%20model%20variance.%0AIn%20essence%2C%20the%20costly%20synchronization%20step%20is%20triggered%20only%20if%20the%20local%0Amodels%20--%20initialized%20from%20a%20common%20global%20model%20after%20each%20synchronization%20--%0Ahave%20significantly%20diverged.%20This%20decision%20is%20facilitated%20by%20the%20transmission%0Aof%20a%20small%20local%20state%20from%20each%20distributed%20node.%20Through%20extensive%0Aexperiments%20across%20a%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%0Areduces%20communication%20cost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%0Aand%20cutting-edge%20communication-efficient%20algorithms.%20Additionally%2C%20we%20show%20that%0AFDA%20maintains%20robust%20performance%20across%20diverse%20data%20heterogeneity%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20988v4&entry.124074799=Read"},
{"title": "VerA: Versatile Anonymization Applicable to Clinical Facial Photographs", "author": "Majed El Helou and Doruk Cetin and Petar Stamenkovic and Niko Benjamin Huber and Fabio Z\u00fcnd", "abstract": "  The demand for privacy in facial image dissemination is gaining ground\ninternationally, echoed by the proliferation of regulations such as GDPR,\nDPDPA, CCPA, PIPL, and APPI. While recent advances in anonymization surpass\npixelation or blur methods, additional constraints to the task pose challenges.\nLargely unaddressed by current anonymization methods are clinical images and\npairs of before-and-after clinical images illustrating facial medical\ninterventions, e.g., facial surgeries or dental procedures. We present VerA,\nthe first Versatile Anonymization framework that solves two challenges in\nclinical applications: A) it preserves selected semantic areas (e.g., mouth\nregion) to show medical intervention results, that is, anonymization is only\napplied to the areas outside the preserved area; and B) it produces anonymized\nimages with consistent personal identity across multiple photographs, which is\ncrucial for anonymizing photographs of the same person taken before and after a\nclinical intervention. We validate our results on both single and paired\nanonymization of clinical images through extensive quantitative and qualitative\nevaluation. We also demonstrate that VerA reaches the state of the art on\nestablished anonymization tasks, in terms of photorealism and\nde-identification.\n", "link": "http://arxiv.org/abs/2312.02124v2", "date": "2024-11-21", "relevancy": 2.0709, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5221}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5213}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VerA%3A%20Versatile%20Anonymization%20Applicable%20to%20Clinical%20Facial%20Photographs&body=Title%3A%20VerA%3A%20Versatile%20Anonymization%20Applicable%20to%20Clinical%20Facial%20Photographs%0AAuthor%3A%20Majed%20El%20Helou%20and%20Doruk%20Cetin%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd%0AAbstract%3A%20%20%20The%20demand%20for%20privacy%20in%20facial%20image%20dissemination%20is%20gaining%20ground%0Ainternationally%2C%20echoed%20by%20the%20proliferation%20of%20regulations%20such%20as%20GDPR%2C%0ADPDPA%2C%20CCPA%2C%20PIPL%2C%20and%20APPI.%20While%20recent%20advances%20in%20anonymization%20surpass%0Apixelation%20or%20blur%20methods%2C%20additional%20constraints%20to%20the%20task%20pose%20challenges.%0ALargely%20unaddressed%20by%20current%20anonymization%20methods%20are%20clinical%20images%20and%0Apairs%20of%20before-and-after%20clinical%20images%20illustrating%20facial%20medical%0Ainterventions%2C%20e.g.%2C%20facial%20surgeries%20or%20dental%20procedures.%20We%20present%20VerA%2C%0Athe%20first%20Versatile%20Anonymization%20framework%20that%20solves%20two%20challenges%20in%0Aclinical%20applications%3A%20A%29%20it%20preserves%20selected%20semantic%20areas%20%28e.g.%2C%20mouth%0Aregion%29%20to%20show%20medical%20intervention%20results%2C%20that%20is%2C%20anonymization%20is%20only%0Aapplied%20to%20the%20areas%20outside%20the%20preserved%20area%3B%20and%20B%29%20it%20produces%20anonymized%0Aimages%20with%20consistent%20personal%20identity%20across%20multiple%20photographs%2C%20which%20is%0Acrucial%20for%20anonymizing%20photographs%20of%20the%20same%20person%20taken%20before%20and%20after%20a%0Aclinical%20intervention.%20We%20validate%20our%20results%20on%20both%20single%20and%20paired%0Aanonymization%20of%20clinical%20images%20through%20extensive%20quantitative%20and%20qualitative%0Aevaluation.%20We%20also%20demonstrate%20that%20VerA%20reaches%20the%20state%20of%20the%20art%20on%0Aestablished%20anonymization%20tasks%2C%20in%20terms%20of%20photorealism%20and%0Ade-identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerA%253A%2520Versatile%2520Anonymization%2520Applicable%2520to%2520Clinical%2520Facial%2520Photographs%26entry.906535625%3DMajed%2520El%2520Helou%2520and%2520Doruk%2520Cetin%2520and%2520Petar%2520Stamenkovic%2520and%2520Niko%2520Benjamin%2520Huber%2520and%2520Fabio%2520Z%25C3%25BCnd%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520privacy%2520in%2520facial%2520image%2520dissemination%2520is%2520gaining%2520ground%250Ainternationally%252C%2520echoed%2520by%2520the%2520proliferation%2520of%2520regulations%2520such%2520as%2520GDPR%252C%250ADPDPA%252C%2520CCPA%252C%2520PIPL%252C%2520and%2520APPI.%2520While%2520recent%2520advances%2520in%2520anonymization%2520surpass%250Apixelation%2520or%2520blur%2520methods%252C%2520additional%2520constraints%2520to%2520the%2520task%2520pose%2520challenges.%250ALargely%2520unaddressed%2520by%2520current%2520anonymization%2520methods%2520are%2520clinical%2520images%2520and%250Apairs%2520of%2520before-and-after%2520clinical%2520images%2520illustrating%2520facial%2520medical%250Ainterventions%252C%2520e.g.%252C%2520facial%2520surgeries%2520or%2520dental%2520procedures.%2520We%2520present%2520VerA%252C%250Athe%2520first%2520Versatile%2520Anonymization%2520framework%2520that%2520solves%2520two%2520challenges%2520in%250Aclinical%2520applications%253A%2520A%2529%2520it%2520preserves%2520selected%2520semantic%2520areas%2520%2528e.g.%252C%2520mouth%250Aregion%2529%2520to%2520show%2520medical%2520intervention%2520results%252C%2520that%2520is%252C%2520anonymization%2520is%2520only%250Aapplied%2520to%2520the%2520areas%2520outside%2520the%2520preserved%2520area%253B%2520and%2520B%2529%2520it%2520produces%2520anonymized%250Aimages%2520with%2520consistent%2520personal%2520identity%2520across%2520multiple%2520photographs%252C%2520which%2520is%250Acrucial%2520for%2520anonymizing%2520photographs%2520of%2520the%2520same%2520person%2520taken%2520before%2520and%2520after%2520a%250Aclinical%2520intervention.%2520We%2520validate%2520our%2520results%2520on%2520both%2520single%2520and%2520paired%250Aanonymization%2520of%2520clinical%2520images%2520through%2520extensive%2520quantitative%2520and%2520qualitative%250Aevaluation.%2520We%2520also%2520demonstrate%2520that%2520VerA%2520reaches%2520the%2520state%2520of%2520the%2520art%2520on%250Aestablished%2520anonymization%2520tasks%252C%2520in%2520terms%2520of%2520photorealism%2520and%250Ade-identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VerA%3A%20Versatile%20Anonymization%20Applicable%20to%20Clinical%20Facial%20Photographs&entry.906535625=Majed%20El%20Helou%20and%20Doruk%20Cetin%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd&entry.1292438233=%20%20The%20demand%20for%20privacy%20in%20facial%20image%20dissemination%20is%20gaining%20ground%0Ainternationally%2C%20echoed%20by%20the%20proliferation%20of%20regulations%20such%20as%20GDPR%2C%0ADPDPA%2C%20CCPA%2C%20PIPL%2C%20and%20APPI.%20While%20recent%20advances%20in%20anonymization%20surpass%0Apixelation%20or%20blur%20methods%2C%20additional%20constraints%20to%20the%20task%20pose%20challenges.%0ALargely%20unaddressed%20by%20current%20anonymization%20methods%20are%20clinical%20images%20and%0Apairs%20of%20before-and-after%20clinical%20images%20illustrating%20facial%20medical%0Ainterventions%2C%20e.g.%2C%20facial%20surgeries%20or%20dental%20procedures.%20We%20present%20VerA%2C%0Athe%20first%20Versatile%20Anonymization%20framework%20that%20solves%20two%20challenges%20in%0Aclinical%20applications%3A%20A%29%20it%20preserves%20selected%20semantic%20areas%20%28e.g.%2C%20mouth%0Aregion%29%20to%20show%20medical%20intervention%20results%2C%20that%20is%2C%20anonymization%20is%20only%0Aapplied%20to%20the%20areas%20outside%20the%20preserved%20area%3B%20and%20B%29%20it%20produces%20anonymized%0Aimages%20with%20consistent%20personal%20identity%20across%20multiple%20photographs%2C%20which%20is%0Acrucial%20for%20anonymizing%20photographs%20of%20the%20same%20person%20taken%20before%20and%20after%20a%0Aclinical%20intervention.%20We%20validate%20our%20results%20on%20both%20single%20and%20paired%0Aanonymization%20of%20clinical%20images%20through%20extensive%20quantitative%20and%20qualitative%0Aevaluation.%20We%20also%20demonstrate%20that%20VerA%20reaches%20the%20state%20of%20the%20art%20on%0Aestablished%20anonymization%20tasks%2C%20in%20terms%20of%20photorealism%20and%0Ade-identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02124v2&entry.124074799=Read"},
{"title": "Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models", "author": "Martha Lewis and Melanie Mitchell", "abstract": "  LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.\n", "link": "http://arxiv.org/abs/2411.14215v1", "date": "2024-11-21", "relevancy": 2.0635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Robustness%20of%20Analogical%20Reasoning%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Evaluating%20the%20Robustness%20of%20Analogical%20Reasoning%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Martha%20Lewis%20and%20Melanie%20Mitchell%0AAbstract%3A%20%20%20LLMs%20have%20performed%20well%20on%20several%20reasoning%20benchmarks%2C%20including%20ones%20that%0Atest%20analogical%20reasoning%20abilities.%20However%2C%20there%20is%20debate%20on%20the%20extent%20to%0Awhich%20they%20are%20performing%20general%20abstract%20reasoning%20versus%20employing%0Anon-robust%20processes%2C%20e.g.%2C%20that%20overly%20rely%20on%20similarity%20to%20pre-training%0Adata.%20Here%20we%20investigate%20the%20robustness%20of%20analogy-making%20abilities%20previously%0Aclaimed%20for%20LLMs%20on%20three%20of%20four%20domains%20studied%20by%20Webb%2C%20Holyoak%2C%20and%20Lu%0A%282023%29%3A%20letter-string%20analogies%2C%20digit%20matrices%2C%20and%20story%20analogies.%20For%20each%0Adomain%20we%20test%20humans%20and%20GPT%20models%20on%20robustness%20to%20variants%20of%20the%20original%0Aanalogy%20problems%20that%20test%20the%20same%20abstract%20reasoning%20abilities%20but%20are%20likely%0Adissimilar%20from%20tasks%20in%20the%20pre-training%20data.%20The%20performance%20of%20a%20system%0Athat%20uses%20robust%20abstract%20reasoning%20should%20not%20decline%20substantially%20on%20these%0Avariants.%0A%20%20On%20simple%20letter-string%20analogies%2C%20we%20find%20that%20while%20the%20performance%20of%0Ahumans%20remains%20high%20for%20two%20types%20of%20variants%20we%20tested%2C%20the%20GPT%20models%27%0Aperformance%20declines%20sharply.%20This%20pattern%20is%20less%20pronounced%20as%20the%20complexity%0Aof%20these%20problems%20is%20increased%2C%20as%20both%20humans%20and%20GPT%20models%20perform%20poorly%20on%0Aboth%20the%20original%20and%20variant%20problems%20requiring%20more%20complex%20analogies.%20On%0Adigit-matrix%20problems%2C%20we%20find%20a%20similar%20pattern%20but%20only%20on%20one%20out%20of%20the%20two%0Atypes%20of%20variants%20we%20tested.%20On%20story-based%20analogy%20problems%2C%20we%20find%20that%2C%0Aunlike%20humans%2C%20the%20performance%20of%20GPT%20models%20are%20susceptible%20to%20answer-order%0Aeffects%2C%20and%20that%20GPT%20models%20also%20may%20be%20more%20sensitive%20than%20humans%20to%0Aparaphrasing.%0A%20%20This%20work%20provides%20evidence%20that%20LLMs%20often%20lack%20the%20robustness%20of%20zero-shot%0Ahuman%20analogy-making%2C%20exhibiting%20brittleness%20on%20most%20of%20the%20variations%20we%0Atested.%20More%20generally%2C%20this%20work%20points%20to%20the%20importance%20of%20carefully%0Aevaluating%20AI%20systems%20not%20only%20for%20accuracy%20but%20also%20robustness%20when%20testing%0Atheir%20cognitive%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Robustness%2520of%2520Analogical%2520Reasoning%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DMartha%2520Lewis%2520and%2520Melanie%2520Mitchell%26entry.1292438233%3D%2520%2520LLMs%2520have%2520performed%2520well%2520on%2520several%2520reasoning%2520benchmarks%252C%2520including%2520ones%2520that%250Atest%2520analogical%2520reasoning%2520abilities.%2520However%252C%2520there%2520is%2520debate%2520on%2520the%2520extent%2520to%250Awhich%2520they%2520are%2520performing%2520general%2520abstract%2520reasoning%2520versus%2520employing%250Anon-robust%2520processes%252C%2520e.g.%252C%2520that%2520overly%2520rely%2520on%2520similarity%2520to%2520pre-training%250Adata.%2520Here%2520we%2520investigate%2520the%2520robustness%2520of%2520analogy-making%2520abilities%2520previously%250Aclaimed%2520for%2520LLMs%2520on%2520three%2520of%2520four%2520domains%2520studied%2520by%2520Webb%252C%2520Holyoak%252C%2520and%2520Lu%250A%25282023%2529%253A%2520letter-string%2520analogies%252C%2520digit%2520matrices%252C%2520and%2520story%2520analogies.%2520For%2520each%250Adomain%2520we%2520test%2520humans%2520and%2520GPT%2520models%2520on%2520robustness%2520to%2520variants%2520of%2520the%2520original%250Aanalogy%2520problems%2520that%2520test%2520the%2520same%2520abstract%2520reasoning%2520abilities%2520but%2520are%2520likely%250Adissimilar%2520from%2520tasks%2520in%2520the%2520pre-training%2520data.%2520The%2520performance%2520of%2520a%2520system%250Athat%2520uses%2520robust%2520abstract%2520reasoning%2520should%2520not%2520decline%2520substantially%2520on%2520these%250Avariants.%250A%2520%2520On%2520simple%2520letter-string%2520analogies%252C%2520we%2520find%2520that%2520while%2520the%2520performance%2520of%250Ahumans%2520remains%2520high%2520for%2520two%2520types%2520of%2520variants%2520we%2520tested%252C%2520the%2520GPT%2520models%2527%250Aperformance%2520declines%2520sharply.%2520This%2520pattern%2520is%2520less%2520pronounced%2520as%2520the%2520complexity%250Aof%2520these%2520problems%2520is%2520increased%252C%2520as%2520both%2520humans%2520and%2520GPT%2520models%2520perform%2520poorly%2520on%250Aboth%2520the%2520original%2520and%2520variant%2520problems%2520requiring%2520more%2520complex%2520analogies.%2520On%250Adigit-matrix%2520problems%252C%2520we%2520find%2520a%2520similar%2520pattern%2520but%2520only%2520on%2520one%2520out%2520of%2520the%2520two%250Atypes%2520of%2520variants%2520we%2520tested.%2520On%2520story-based%2520analogy%2520problems%252C%2520we%2520find%2520that%252C%250Aunlike%2520humans%252C%2520the%2520performance%2520of%2520GPT%2520models%2520are%2520susceptible%2520to%2520answer-order%250Aeffects%252C%2520and%2520that%2520GPT%2520models%2520also%2520may%2520be%2520more%2520sensitive%2520than%2520humans%2520to%250Aparaphrasing.%250A%2520%2520This%2520work%2520provides%2520evidence%2520that%2520LLMs%2520often%2520lack%2520the%2520robustness%2520of%2520zero-shot%250Ahuman%2520analogy-making%252C%2520exhibiting%2520brittleness%2520on%2520most%2520of%2520the%2520variations%2520we%250Atested.%2520More%2520generally%252C%2520this%2520work%2520points%2520to%2520the%2520importance%2520of%2520carefully%250Aevaluating%2520AI%2520systems%2520not%2520only%2520for%2520accuracy%2520but%2520also%2520robustness%2520when%2520testing%250Atheir%2520cognitive%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Robustness%20of%20Analogical%20Reasoning%20in%20Large%20Language%0A%20%20Models&entry.906535625=Martha%20Lewis%20and%20Melanie%20Mitchell&entry.1292438233=%20%20LLMs%20have%20performed%20well%20on%20several%20reasoning%20benchmarks%2C%20including%20ones%20that%0Atest%20analogical%20reasoning%20abilities.%20However%2C%20there%20is%20debate%20on%20the%20extent%20to%0Awhich%20they%20are%20performing%20general%20abstract%20reasoning%20versus%20employing%0Anon-robust%20processes%2C%20e.g.%2C%20that%20overly%20rely%20on%20similarity%20to%20pre-training%0Adata.%20Here%20we%20investigate%20the%20robustness%20of%20analogy-making%20abilities%20previously%0Aclaimed%20for%20LLMs%20on%20three%20of%20four%20domains%20studied%20by%20Webb%2C%20Holyoak%2C%20and%20Lu%0A%282023%29%3A%20letter-string%20analogies%2C%20digit%20matrices%2C%20and%20story%20analogies.%20For%20each%0Adomain%20we%20test%20humans%20and%20GPT%20models%20on%20robustness%20to%20variants%20of%20the%20original%0Aanalogy%20problems%20that%20test%20the%20same%20abstract%20reasoning%20abilities%20but%20are%20likely%0Adissimilar%20from%20tasks%20in%20the%20pre-training%20data.%20The%20performance%20of%20a%20system%0Athat%20uses%20robust%20abstract%20reasoning%20should%20not%20decline%20substantially%20on%20these%0Avariants.%0A%20%20On%20simple%20letter-string%20analogies%2C%20we%20find%20that%20while%20the%20performance%20of%0Ahumans%20remains%20high%20for%20two%20types%20of%20variants%20we%20tested%2C%20the%20GPT%20models%27%0Aperformance%20declines%20sharply.%20This%20pattern%20is%20less%20pronounced%20as%20the%20complexity%0Aof%20these%20problems%20is%20increased%2C%20as%20both%20humans%20and%20GPT%20models%20perform%20poorly%20on%0Aboth%20the%20original%20and%20variant%20problems%20requiring%20more%20complex%20analogies.%20On%0Adigit-matrix%20problems%2C%20we%20find%20a%20similar%20pattern%20but%20only%20on%20one%20out%20of%20the%20two%0Atypes%20of%20variants%20we%20tested.%20On%20story-based%20analogy%20problems%2C%20we%20find%20that%2C%0Aunlike%20humans%2C%20the%20performance%20of%20GPT%20models%20are%20susceptible%20to%20answer-order%0Aeffects%2C%20and%20that%20GPT%20models%20also%20may%20be%20more%20sensitive%20than%20humans%20to%0Aparaphrasing.%0A%20%20This%20work%20provides%20evidence%20that%20LLMs%20often%20lack%20the%20robustness%20of%20zero-shot%0Ahuman%20analogy-making%2C%20exhibiting%20brittleness%20on%20most%20of%20the%20variations%20we%0Atested.%20More%20generally%2C%20this%20work%20points%20to%20the%20importance%20of%20carefully%0Aevaluating%20AI%20systems%20not%20only%20for%20accuracy%20but%20also%20robustness%20when%20testing%0Atheir%20cognitive%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14215v1&entry.124074799=Read"},
{"title": "Creating a Formally Verified Neural Network for Autonomous Navigation:\n  An Experience Report", "author": "Syed Ali Asadullah Bukhari and Thomas Flinkow and Medet Inkarbekov and Barak A. Pearlmutter and Rosemary Monahan", "abstract": "  The increased reliance of self-driving vehicles on neural networks opens up\nthe challenge of their verification. In this paper we present an experience\nreport, describing a case study which we undertook to explore the design and\ntraining of a neural network on a custom dataset for vision-based autonomous\nnavigation. We are particularly interested in the use of machine learning with\ndifferentiable logics to obtain networks satisfying basic safety properties by\ndesign, guaranteeing the behaviour of the neural network after training. We\nmotivate the choice of a suitable neural network verifier for our purposes and\nreport our observations on the use of neural network verifiers for self-driving\nsystems.\n", "link": "http://arxiv.org/abs/2411.14163v1", "date": "2024-11-21", "relevancy": 2.0616, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20Formally%20Verified%20Neural%20Network%20for%20Autonomous%20Navigation%3A%0A%20%20An%20Experience%20Report&body=Title%3A%20Creating%20a%20Formally%20Verified%20Neural%20Network%20for%20Autonomous%20Navigation%3A%0A%20%20An%20Experience%20Report%0AAuthor%3A%20Syed%20Ali%20Asadullah%20Bukhari%20and%20Thomas%20Flinkow%20and%20Medet%20Inkarbekov%20and%20Barak%20A.%20Pearlmutter%20and%20Rosemary%20Monahan%0AAbstract%3A%20%20%20The%20increased%20reliance%20of%20self-driving%20vehicles%20on%20neural%20networks%20opens%20up%0Athe%20challenge%20of%20their%20verification.%20In%20this%20paper%20we%20present%20an%20experience%0Areport%2C%20describing%20a%20case%20study%20which%20we%20undertook%20to%20explore%20the%20design%20and%0Atraining%20of%20a%20neural%20network%20on%20a%20custom%20dataset%20for%20vision-based%20autonomous%0Anavigation.%20We%20are%20particularly%20interested%20in%20the%20use%20of%20machine%20learning%20with%0Adifferentiable%20logics%20to%20obtain%20networks%20satisfying%20basic%20safety%20properties%20by%0Adesign%2C%20guaranteeing%20the%20behaviour%20of%20the%20neural%20network%20after%20training.%20We%0Amotivate%20the%20choice%20of%20a%20suitable%20neural%20network%20verifier%20for%20our%20purposes%20and%0Areport%20our%20observations%20on%20the%20use%20of%20neural%20network%20verifiers%20for%20self-driving%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520a%2520Formally%2520Verified%2520Neural%2520Network%2520for%2520Autonomous%2520Navigation%253A%250A%2520%2520An%2520Experience%2520Report%26entry.906535625%3DSyed%2520Ali%2520Asadullah%2520Bukhari%2520and%2520Thomas%2520Flinkow%2520and%2520Medet%2520Inkarbekov%2520and%2520Barak%2520A.%2520Pearlmutter%2520and%2520Rosemary%2520Monahan%26entry.1292438233%3D%2520%2520The%2520increased%2520reliance%2520of%2520self-driving%2520vehicles%2520on%2520neural%2520networks%2520opens%2520up%250Athe%2520challenge%2520of%2520their%2520verification.%2520In%2520this%2520paper%2520we%2520present%2520an%2520experience%250Areport%252C%2520describing%2520a%2520case%2520study%2520which%2520we%2520undertook%2520to%2520explore%2520the%2520design%2520and%250Atraining%2520of%2520a%2520neural%2520network%2520on%2520a%2520custom%2520dataset%2520for%2520vision-based%2520autonomous%250Anavigation.%2520We%2520are%2520particularly%2520interested%2520in%2520the%2520use%2520of%2520machine%2520learning%2520with%250Adifferentiable%2520logics%2520to%2520obtain%2520networks%2520satisfying%2520basic%2520safety%2520properties%2520by%250Adesign%252C%2520guaranteeing%2520the%2520behaviour%2520of%2520the%2520neural%2520network%2520after%2520training.%2520We%250Amotivate%2520the%2520choice%2520of%2520a%2520suitable%2520neural%2520network%2520verifier%2520for%2520our%2520purposes%2520and%250Areport%2520our%2520observations%2520on%2520the%2520use%2520of%2520neural%2520network%2520verifiers%2520for%2520self-driving%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20Formally%20Verified%20Neural%20Network%20for%20Autonomous%20Navigation%3A%0A%20%20An%20Experience%20Report&entry.906535625=Syed%20Ali%20Asadullah%20Bukhari%20and%20Thomas%20Flinkow%20and%20Medet%20Inkarbekov%20and%20Barak%20A.%20Pearlmutter%20and%20Rosemary%20Monahan&entry.1292438233=%20%20The%20increased%20reliance%20of%20self-driving%20vehicles%20on%20neural%20networks%20opens%20up%0Athe%20challenge%20of%20their%20verification.%20In%20this%20paper%20we%20present%20an%20experience%0Areport%2C%20describing%20a%20case%20study%20which%20we%20undertook%20to%20explore%20the%20design%20and%0Atraining%20of%20a%20neural%20network%20on%20a%20custom%20dataset%20for%20vision-based%20autonomous%0Anavigation.%20We%20are%20particularly%20interested%20in%20the%20use%20of%20machine%20learning%20with%0Adifferentiable%20logics%20to%20obtain%20networks%20satisfying%20basic%20safety%20properties%20by%0Adesign%2C%20guaranteeing%20the%20behaviour%20of%20the%20neural%20network%20after%20training.%20We%0Amotivate%20the%20choice%20of%20a%20suitable%20neural%20network%20verifier%20for%20our%20purposes%20and%0Areport%20our%20observations%20on%20the%20use%20of%20neural%20network%20verifiers%20for%20self-driving%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14163v1&entry.124074799=Read"},
{"title": "Logic Augmented Generation", "author": "Aldo Gangemi and Andrea Giovanni Nuzzolese", "abstract": "  Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.\n", "link": "http://arxiv.org/abs/2411.14012v1", "date": "2024-11-21", "relevancy": 2.0606, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5338}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logic%20Augmented%20Generation&body=Title%3A%20Logic%20Augmented%20Generation%0AAuthor%3A%20Aldo%20Gangemi%20and%20Andrea%20Giovanni%20Nuzzolese%0AAbstract%3A%20%20%20Semantic%20Knowledge%20Graphs%20%28SKG%29%20face%20challenges%20with%20scalability%2C%0Aflexibility%2C%20contextual%20understanding%2C%20and%20handling%20unstructured%20or%20ambiguous%0Ainformation.%20However%2C%20they%20offer%20formal%20and%20structured%20knowledge%20enabling%0Ahighly%20interpretable%20and%20reliable%20results%20by%20means%20of%20reasoning%20and%20querying.%0ALarge%20Language%20Models%20%28LLMs%29%20overcome%20those%20limitations%20making%20them%20suitable%20in%0Aopen-ended%20tasks%20and%20unstructured%20environments.%20Nevertheless%2C%20LLMs%20are%20neither%0Ainterpretable%20nor%20reliable.%20To%20solve%20the%20dichotomy%20between%20LLMs%20and%20SKGs%20we%0Aenvision%20Logic%20Augmented%20Generation%20%28LAG%29%20that%20combines%20the%20benefits%20of%20the%20two%0Aworlds.%20LAG%20uses%20LLMs%20as%20Reactive%20Continuous%20Knowledge%20Graphs%20that%20can%20generate%0Apotentially%20infinite%20relations%20and%20tacit%20knowledge%20on-demand.%20SKGs%20are%20key%20for%0Ainjecting%20a%20discrete%20heuristic%20dimension%20with%20clear%20logical%20and%20factual%0Aboundaries.%20We%20exemplify%20LAG%20in%20two%20tasks%20of%20collective%20intelligence%2C%20i.e.%2C%0Amedical%20diagnostics%20and%20climate%20projections.%20Understanding%20the%20properties%20and%0Alimitations%20of%20LAG%2C%20which%20are%20still%20mostly%20unknown%2C%20is%20of%20utmost%20importance%20for%0Aenabling%20a%20variety%20of%20tasks%20involving%20tacit%20knowledge%20in%20order%20to%20provide%0Ainterpretable%20and%20effective%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogic%2520Augmented%2520Generation%26entry.906535625%3DAldo%2520Gangemi%2520and%2520Andrea%2520Giovanni%2520Nuzzolese%26entry.1292438233%3D%2520%2520Semantic%2520Knowledge%2520Graphs%2520%2528SKG%2529%2520face%2520challenges%2520with%2520scalability%252C%250Aflexibility%252C%2520contextual%2520understanding%252C%2520and%2520handling%2520unstructured%2520or%2520ambiguous%250Ainformation.%2520However%252C%2520they%2520offer%2520formal%2520and%2520structured%2520knowledge%2520enabling%250Ahighly%2520interpretable%2520and%2520reliable%2520results%2520by%2520means%2520of%2520reasoning%2520and%2520querying.%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520overcome%2520those%2520limitations%2520making%2520them%2520suitable%2520in%250Aopen-ended%2520tasks%2520and%2520unstructured%2520environments.%2520Nevertheless%252C%2520LLMs%2520are%2520neither%250Ainterpretable%2520nor%2520reliable.%2520To%2520solve%2520the%2520dichotomy%2520between%2520LLMs%2520and%2520SKGs%2520we%250Aenvision%2520Logic%2520Augmented%2520Generation%2520%2528LAG%2529%2520that%2520combines%2520the%2520benefits%2520of%2520the%2520two%250Aworlds.%2520LAG%2520uses%2520LLMs%2520as%2520Reactive%2520Continuous%2520Knowledge%2520Graphs%2520that%2520can%2520generate%250Apotentially%2520infinite%2520relations%2520and%2520tacit%2520knowledge%2520on-demand.%2520SKGs%2520are%2520key%2520for%250Ainjecting%2520a%2520discrete%2520heuristic%2520dimension%2520with%2520clear%2520logical%2520and%2520factual%250Aboundaries.%2520We%2520exemplify%2520LAG%2520in%2520two%2520tasks%2520of%2520collective%2520intelligence%252C%2520i.e.%252C%250Amedical%2520diagnostics%2520and%2520climate%2520projections.%2520Understanding%2520the%2520properties%2520and%250Alimitations%2520of%2520LAG%252C%2520which%2520are%2520still%2520mostly%2520unknown%252C%2520is%2520of%2520utmost%2520importance%2520for%250Aenabling%2520a%2520variety%2520of%2520tasks%2520involving%2520tacit%2520knowledge%2520in%2520order%2520to%2520provide%250Ainterpretable%2520and%2520effective%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logic%20Augmented%20Generation&entry.906535625=Aldo%20Gangemi%20and%20Andrea%20Giovanni%20Nuzzolese&entry.1292438233=%20%20Semantic%20Knowledge%20Graphs%20%28SKG%29%20face%20challenges%20with%20scalability%2C%0Aflexibility%2C%20contextual%20understanding%2C%20and%20handling%20unstructured%20or%20ambiguous%0Ainformation.%20However%2C%20they%20offer%20formal%20and%20structured%20knowledge%20enabling%0Ahighly%20interpretable%20and%20reliable%20results%20by%20means%20of%20reasoning%20and%20querying.%0ALarge%20Language%20Models%20%28LLMs%29%20overcome%20those%20limitations%20making%20them%20suitable%20in%0Aopen-ended%20tasks%20and%20unstructured%20environments.%20Nevertheless%2C%20LLMs%20are%20neither%0Ainterpretable%20nor%20reliable.%20To%20solve%20the%20dichotomy%20between%20LLMs%20and%20SKGs%20we%0Aenvision%20Logic%20Augmented%20Generation%20%28LAG%29%20that%20combines%20the%20benefits%20of%20the%20two%0Aworlds.%20LAG%20uses%20LLMs%20as%20Reactive%20Continuous%20Knowledge%20Graphs%20that%20can%20generate%0Apotentially%20infinite%20relations%20and%20tacit%20knowledge%20on-demand.%20SKGs%20are%20key%20for%0Ainjecting%20a%20discrete%20heuristic%20dimension%20with%20clear%20logical%20and%20factual%0Aboundaries.%20We%20exemplify%20LAG%20in%20two%20tasks%20of%20collective%20intelligence%2C%20i.e.%2C%0Amedical%20diagnostics%20and%20climate%20projections.%20Understanding%20the%20properties%20and%0Alimitations%20of%20LAG%2C%20which%20are%20still%20mostly%20unknown%2C%20is%20of%20utmost%20importance%20for%0Aenabling%20a%20variety%20of%20tasks%20involving%20tacit%20knowledge%20in%20order%20to%20provide%0Ainterpretable%20and%20effective%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14012v1&entry.124074799=Read"},
{"title": "Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification", "author": "Junhua Liu and Yong Keat Tan and Bin Fu and Kwan Hui Lim", "abstract": "  Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area.\n", "link": "http://arxiv.org/abs/2411.14252v1", "date": "2024-11-21", "relevancy": 1.9958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5254}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intent-Aware%20Dialogue%20Generation%20and%20Multi-Task%20Contrastive%20Learning%20for%0A%20%20Multi-Turn%20Intent%20Classification&body=Title%3A%20Intent-Aware%20Dialogue%20Generation%20and%20Multi-Task%20Contrastive%20Learning%20for%0A%20%20Multi-Turn%20Intent%20Classification%0AAuthor%3A%20Junhua%20Liu%20and%20Yong%20Keat%20Tan%20and%20Bin%20Fu%20and%20Kwan%20Hui%20Lim%0AAbstract%3A%20%20%20Generating%20large-scale%2C%20domain-specific%2C%20multilingual%20multi-turn%20dialogue%0Adatasets%20remains%20a%20significant%20hurdle%20for%20training%20effective%20Multi-Turn%20Intent%0AClassification%20models%20in%20chatbot%20systems.%20In%20this%20paper%2C%20we%20introduce%0AChain-of-Intent%2C%20a%20novel%20mechanism%20that%20combines%20Hidden%20Markov%20Models%20with%0ALarge%20Language%20Models%20%28LLMs%29%20to%20generate%20contextually%20aware%2C%20intent-driven%0Aconversations%20through%20self-play.%20By%20extracting%20domain-specific%20knowledge%20from%0Ae-commerce%20chat%20logs%2C%20we%20estimate%20conversation%20turns%20and%20intent%20transitions%2C%0Awhich%20guide%20the%20generation%20of%20coherent%20dialogues.%20Leveraging%20LLMs%20to%20enhance%0Aemission%20probabilities%2C%20our%20approach%20produces%20natural%20and%20contextually%0Aconsistent%20questions%20and%20answers.%20We%20also%20propose%20MINT-CL%2C%20a%20framework%20for%0Amulti-turn%20intent%20classification%20using%20multi-task%20contrastive%20learning%2C%0Aimproving%20classification%20accuracy%20without%20the%20need%20for%20extensive%20annotated%0Adata.%20Evaluations%20show%20that%20our%20methods%20outperform%20baselines%20in%20dialogue%0Aquality%20and%20intent%20classification%20accuracy%2C%20especially%20in%20multilingual%0Asettings%2C%20while%20significantly%20reducing%20data%20generation%20efforts.%20Furthermore%2C%20we%0Arelease%20MINT-E%2C%20a%20multilingual%2C%20intent-aware%20multi-turn%20e-commerce%20dialogue%0Acorpus%20to%20support%20future%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntent-Aware%2520Dialogue%2520Generation%2520and%2520Multi-Task%2520Contrastive%2520Learning%2520for%250A%2520%2520Multi-Turn%2520Intent%2520Classification%26entry.906535625%3DJunhua%2520Liu%2520and%2520Yong%2520Keat%2520Tan%2520and%2520Bin%2520Fu%2520and%2520Kwan%2520Hui%2520Lim%26entry.1292438233%3D%2520%2520Generating%2520large-scale%252C%2520domain-specific%252C%2520multilingual%2520multi-turn%2520dialogue%250Adatasets%2520remains%2520a%2520significant%2520hurdle%2520for%2520training%2520effective%2520Multi-Turn%2520Intent%250AClassification%2520models%2520in%2520chatbot%2520systems.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AChain-of-Intent%252C%2520a%2520novel%2520mechanism%2520that%2520combines%2520Hidden%2520Markov%2520Models%2520with%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520contextually%2520aware%252C%2520intent-driven%250Aconversations%2520through%2520self-play.%2520By%2520extracting%2520domain-specific%2520knowledge%2520from%250Ae-commerce%2520chat%2520logs%252C%2520we%2520estimate%2520conversation%2520turns%2520and%2520intent%2520transitions%252C%250Awhich%2520guide%2520the%2520generation%2520of%2520coherent%2520dialogues.%2520Leveraging%2520LLMs%2520to%2520enhance%250Aemission%2520probabilities%252C%2520our%2520approach%2520produces%2520natural%2520and%2520contextually%250Aconsistent%2520questions%2520and%2520answers.%2520We%2520also%2520propose%2520MINT-CL%252C%2520a%2520framework%2520for%250Amulti-turn%2520intent%2520classification%2520using%2520multi-task%2520contrastive%2520learning%252C%250Aimproving%2520classification%2520accuracy%2520without%2520the%2520need%2520for%2520extensive%2520annotated%250Adata.%2520Evaluations%2520show%2520that%2520our%2520methods%2520outperform%2520baselines%2520in%2520dialogue%250Aquality%2520and%2520intent%2520classification%2520accuracy%252C%2520especially%2520in%2520multilingual%250Asettings%252C%2520while%2520significantly%2520reducing%2520data%2520generation%2520efforts.%2520Furthermore%252C%2520we%250Arelease%2520MINT-E%252C%2520a%2520multilingual%252C%2520intent-aware%2520multi-turn%2520e-commerce%2520dialogue%250Acorpus%2520to%2520support%2520future%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intent-Aware%20Dialogue%20Generation%20and%20Multi-Task%20Contrastive%20Learning%20for%0A%20%20Multi-Turn%20Intent%20Classification&entry.906535625=Junhua%20Liu%20and%20Yong%20Keat%20Tan%20and%20Bin%20Fu%20and%20Kwan%20Hui%20Lim&entry.1292438233=%20%20Generating%20large-scale%2C%20domain-specific%2C%20multilingual%20multi-turn%20dialogue%0Adatasets%20remains%20a%20significant%20hurdle%20for%20training%20effective%20Multi-Turn%20Intent%0AClassification%20models%20in%20chatbot%20systems.%20In%20this%20paper%2C%20we%20introduce%0AChain-of-Intent%2C%20a%20novel%20mechanism%20that%20combines%20Hidden%20Markov%20Models%20with%0ALarge%20Language%20Models%20%28LLMs%29%20to%20generate%20contextually%20aware%2C%20intent-driven%0Aconversations%20through%20self-play.%20By%20extracting%20domain-specific%20knowledge%20from%0Ae-commerce%20chat%20logs%2C%20we%20estimate%20conversation%20turns%20and%20intent%20transitions%2C%0Awhich%20guide%20the%20generation%20of%20coherent%20dialogues.%20Leveraging%20LLMs%20to%20enhance%0Aemission%20probabilities%2C%20our%20approach%20produces%20natural%20and%20contextually%0Aconsistent%20questions%20and%20answers.%20We%20also%20propose%20MINT-CL%2C%20a%20framework%20for%0Amulti-turn%20intent%20classification%20using%20multi-task%20contrastive%20learning%2C%0Aimproving%20classification%20accuracy%20without%20the%20need%20for%20extensive%20annotated%0Adata.%20Evaluations%20show%20that%20our%20methods%20outperform%20baselines%20in%20dialogue%0Aquality%20and%20intent%20classification%20accuracy%2C%20especially%20in%20multilingual%0Asettings%2C%20while%20significantly%20reducing%20data%20generation%20efforts.%20Furthermore%2C%20we%0Arelease%20MINT-E%2C%20a%20multilingual%2C%20intent-aware%20multi-turn%20e-commerce%20dialogue%0Acorpus%20to%20support%20future%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14252v1&entry.124074799=Read"},
{"title": "ETA-IK: Execution-Time-Aware Inverse Kinematics for Dual-Arm Systems", "author": "Yucheng Tang and Xi Huang and Yongzhou Zhang and Tao Chen and Ilshat Mamaev and Bj\u00f6rn Hein", "abstract": "  This paper presents ETA-IK, a novel Execution-Time-Aware Inverse Kinematics\nmethod tailored for dual-arm robotic systems. The primary goal is to optimize\nmotion execution time by leveraging the redundancy of both arms, specifically\nin tasks where only the relative pose of the robots is constrained, such as\ndual-arm scanning of unknown objects. Unlike traditional inverse kinematics\nmethods that use surrogate metrics such as joint configuration distance, our\nmethod incorporates direct motion execution time and implicit collisions into\nthe optimization process, thereby finding target joints that allow subsequent\ntrajectory generation to get more efficient and collision-free motion. A neural\nnetwork based execution time approximator is employed to predict time-efficient\njoint configurations while accounting for potential collisions. Through\nexperimental evaluation on a system composed of a UR5 and a KUKA iiwa robot, we\ndemonstrate significant reductions in execution time. The proposed method\noutperforms conventional approaches, showing improved motion efficiency without\nsacrificing positioning accuracy. These results highlight the potential of\nETA-IK to improve the performance of dual-arm systems in applications, where\nefficiency and safety are paramount.\n", "link": "http://arxiv.org/abs/2411.14381v1", "date": "2024-11-21", "relevancy": 1.5497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5162}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ETA-IK%3A%20Execution-Time-Aware%20Inverse%20Kinematics%20for%20Dual-Arm%20Systems&body=Title%3A%20ETA-IK%3A%20Execution-Time-Aware%20Inverse%20Kinematics%20for%20Dual-Arm%20Systems%0AAuthor%3A%20Yucheng%20Tang%20and%20Xi%20Huang%20and%20Yongzhou%20Zhang%20and%20Tao%20Chen%20and%20Ilshat%20Mamaev%20and%20Bj%C3%B6rn%20Hein%0AAbstract%3A%20%20%20This%20paper%20presents%20ETA-IK%2C%20a%20novel%20Execution-Time-Aware%20Inverse%20Kinematics%0Amethod%20tailored%20for%20dual-arm%20robotic%20systems.%20The%20primary%20goal%20is%20to%20optimize%0Amotion%20execution%20time%20by%20leveraging%20the%20redundancy%20of%20both%20arms%2C%20specifically%0Ain%20tasks%20where%20only%20the%20relative%20pose%20of%20the%20robots%20is%20constrained%2C%20such%20as%0Adual-arm%20scanning%20of%20unknown%20objects.%20Unlike%20traditional%20inverse%20kinematics%0Amethods%20that%20use%20surrogate%20metrics%20such%20as%20joint%20configuration%20distance%2C%20our%0Amethod%20incorporates%20direct%20motion%20execution%20time%20and%20implicit%20collisions%20into%0Athe%20optimization%20process%2C%20thereby%20finding%20target%20joints%20that%20allow%20subsequent%0Atrajectory%20generation%20to%20get%20more%20efficient%20and%20collision-free%20motion.%20A%20neural%0Anetwork%20based%20execution%20time%20approximator%20is%20employed%20to%20predict%20time-efficient%0Ajoint%20configurations%20while%20accounting%20for%20potential%20collisions.%20Through%0Aexperimental%20evaluation%20on%20a%20system%20composed%20of%20a%20UR5%20and%20a%20KUKA%20iiwa%20robot%2C%20we%0Ademonstrate%20significant%20reductions%20in%20execution%20time.%20The%20proposed%20method%0Aoutperforms%20conventional%20approaches%2C%20showing%20improved%20motion%20efficiency%20without%0Asacrificing%20positioning%20accuracy.%20These%20results%20highlight%20the%20potential%20of%0AETA-IK%20to%20improve%20the%20performance%20of%20dual-arm%20systems%20in%20applications%2C%20where%0Aefficiency%20and%20safety%20are%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DETA-IK%253A%2520Execution-Time-Aware%2520Inverse%2520Kinematics%2520for%2520Dual-Arm%2520Systems%26entry.906535625%3DYucheng%2520Tang%2520and%2520Xi%2520Huang%2520and%2520Yongzhou%2520Zhang%2520and%2520Tao%2520Chen%2520and%2520Ilshat%2520Mamaev%2520and%2520Bj%25C3%25B6rn%2520Hein%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ETA-IK%252C%2520a%2520novel%2520Execution-Time-Aware%2520Inverse%2520Kinematics%250Amethod%2520tailored%2520for%2520dual-arm%2520robotic%2520systems.%2520The%2520primary%2520goal%2520is%2520to%2520optimize%250Amotion%2520execution%2520time%2520by%2520leveraging%2520the%2520redundancy%2520of%2520both%2520arms%252C%2520specifically%250Ain%2520tasks%2520where%2520only%2520the%2520relative%2520pose%2520of%2520the%2520robots%2520is%2520constrained%252C%2520such%2520as%250Adual-arm%2520scanning%2520of%2520unknown%2520objects.%2520Unlike%2520traditional%2520inverse%2520kinematics%250Amethods%2520that%2520use%2520surrogate%2520metrics%2520such%2520as%2520joint%2520configuration%2520distance%252C%2520our%250Amethod%2520incorporates%2520direct%2520motion%2520execution%2520time%2520and%2520implicit%2520collisions%2520into%250Athe%2520optimization%2520process%252C%2520thereby%2520finding%2520target%2520joints%2520that%2520allow%2520subsequent%250Atrajectory%2520generation%2520to%2520get%2520more%2520efficient%2520and%2520collision-free%2520motion.%2520A%2520neural%250Anetwork%2520based%2520execution%2520time%2520approximator%2520is%2520employed%2520to%2520predict%2520time-efficient%250Ajoint%2520configurations%2520while%2520accounting%2520for%2520potential%2520collisions.%2520Through%250Aexperimental%2520evaluation%2520on%2520a%2520system%2520composed%2520of%2520a%2520UR5%2520and%2520a%2520KUKA%2520iiwa%2520robot%252C%2520we%250Ademonstrate%2520significant%2520reductions%2520in%2520execution%2520time.%2520The%2520proposed%2520method%250Aoutperforms%2520conventional%2520approaches%252C%2520showing%2520improved%2520motion%2520efficiency%2520without%250Asacrificing%2520positioning%2520accuracy.%2520These%2520results%2520highlight%2520the%2520potential%2520of%250AETA-IK%2520to%2520improve%2520the%2520performance%2520of%2520dual-arm%2520systems%2520in%2520applications%252C%2520where%250Aefficiency%2520and%2520safety%2520are%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ETA-IK%3A%20Execution-Time-Aware%20Inverse%20Kinematics%20for%20Dual-Arm%20Systems&entry.906535625=Yucheng%20Tang%20and%20Xi%20Huang%20and%20Yongzhou%20Zhang%20and%20Tao%20Chen%20and%20Ilshat%20Mamaev%20and%20Bj%C3%B6rn%20Hein&entry.1292438233=%20%20This%20paper%20presents%20ETA-IK%2C%20a%20novel%20Execution-Time-Aware%20Inverse%20Kinematics%0Amethod%20tailored%20for%20dual-arm%20robotic%20systems.%20The%20primary%20goal%20is%20to%20optimize%0Amotion%20execution%20time%20by%20leveraging%20the%20redundancy%20of%20both%20arms%2C%20specifically%0Ain%20tasks%20where%20only%20the%20relative%20pose%20of%20the%20robots%20is%20constrained%2C%20such%20as%0Adual-arm%20scanning%20of%20unknown%20objects.%20Unlike%20traditional%20inverse%20kinematics%0Amethods%20that%20use%20surrogate%20metrics%20such%20as%20joint%20configuration%20distance%2C%20our%0Amethod%20incorporates%20direct%20motion%20execution%20time%20and%20implicit%20collisions%20into%0Athe%20optimization%20process%2C%20thereby%20finding%20target%20joints%20that%20allow%20subsequent%0Atrajectory%20generation%20to%20get%20more%20efficient%20and%20collision-free%20motion.%20A%20neural%0Anetwork%20based%20execution%20time%20approximator%20is%20employed%20to%20predict%20time-efficient%0Ajoint%20configurations%20while%20accounting%20for%20potential%20collisions.%20Through%0Aexperimental%20evaluation%20on%20a%20system%20composed%20of%20a%20UR5%20and%20a%20KUKA%20iiwa%20robot%2C%20we%0Ademonstrate%20significant%20reductions%20in%20execution%20time.%20The%20proposed%20method%0Aoutperforms%20conventional%20approaches%2C%20showing%20improved%20motion%20efficiency%20without%0Asacrificing%20positioning%20accuracy.%20These%20results%20highlight%20the%20potential%20of%0AETA-IK%20to%20improve%20the%20performance%20of%20dual-arm%20systems%20in%20applications%2C%20where%0Aefficiency%20and%20safety%20are%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14381v1&entry.124074799=Read"},
{"title": "Enhancing Medical Image Segmentation with Deep Learning and Diffusion\n  Models", "author": "Houze Liu and Tong Zhou and Yanlin Xiang and Aoran Shen and Jiacheng Hu and Junliang Du", "abstract": "  Medical image segmentation is crucial for accurate clinical diagnoses, yet it\nfaces challenges such as low contrast between lesions and normal tissues,\nunclear boundaries, and high variability across patients. Deep learning has\nimproved segmentation accuracy and efficiency, but it still relies heavily on\nexpert annotations and struggles with the complexities of medical images. The\nsmall size of medical image datasets and the high cost of data acquisition\nfurther limit the performance of segmentation networks. Diffusion models, with\ntheir iterative denoising process, offer a promising alternative for better\ndetail capture in segmentation. However, they face difficulties in accurately\nsegmenting small targets and maintaining the precision of boundary details.\nThis article discusses the importance of medical image segmentation, the\nlimitations of current deep learning approaches, and the potential of diffusion\nmodels to address these challenges.\n", "link": "http://arxiv.org/abs/2411.14353v1", "date": "2024-11-21", "relevancy": 1.6896, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6238}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5494}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Medical%20Image%20Segmentation%20with%20Deep%20Learning%20and%20Diffusion%0A%20%20Models&body=Title%3A%20Enhancing%20Medical%20Image%20Segmentation%20with%20Deep%20Learning%20and%20Diffusion%0A%20%20Models%0AAuthor%3A%20Houze%20Liu%20and%20Tong%20Zhou%20and%20Yanlin%20Xiang%20and%20Aoran%20Shen%20and%20Jiacheng%20Hu%20and%20Junliang%20Du%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20crucial%20for%20accurate%20clinical%20diagnoses%2C%20yet%20it%0Afaces%20challenges%20such%20as%20low%20contrast%20between%20lesions%20and%20normal%20tissues%2C%0Aunclear%20boundaries%2C%20and%20high%20variability%20across%20patients.%20Deep%20learning%20has%0Aimproved%20segmentation%20accuracy%20and%20efficiency%2C%20but%20it%20still%20relies%20heavily%20on%0Aexpert%20annotations%20and%20struggles%20with%20the%20complexities%20of%20medical%20images.%20The%0Asmall%20size%20of%20medical%20image%20datasets%20and%20the%20high%20cost%20of%20data%20acquisition%0Afurther%20limit%20the%20performance%20of%20segmentation%20networks.%20Diffusion%20models%2C%20with%0Atheir%20iterative%20denoising%20process%2C%20offer%20a%20promising%20alternative%20for%20better%0Adetail%20capture%20in%20segmentation.%20However%2C%20they%20face%20difficulties%20in%20accurately%0Asegmenting%20small%20targets%20and%20maintaining%20the%20precision%20of%20boundary%20details.%0AThis%20article%20discusses%20the%20importance%20of%20medical%20image%20segmentation%2C%20the%0Alimitations%20of%20current%20deep%20learning%20approaches%2C%20and%20the%20potential%20of%20diffusion%0Amodels%20to%20address%20these%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Medical%2520Image%2520Segmentation%2520with%2520Deep%2520Learning%2520and%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DHouze%2520Liu%2520and%2520Tong%2520Zhou%2520and%2520Yanlin%2520Xiang%2520and%2520Aoran%2520Shen%2520and%2520Jiacheng%2520Hu%2520and%2520Junliang%2520Du%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520accurate%2520clinical%2520diagnoses%252C%2520yet%2520it%250Afaces%2520challenges%2520such%2520as%2520low%2520contrast%2520between%2520lesions%2520and%2520normal%2520tissues%252C%250Aunclear%2520boundaries%252C%2520and%2520high%2520variability%2520across%2520patients.%2520Deep%2520learning%2520has%250Aimproved%2520segmentation%2520accuracy%2520and%2520efficiency%252C%2520but%2520it%2520still%2520relies%2520heavily%2520on%250Aexpert%2520annotations%2520and%2520struggles%2520with%2520the%2520complexities%2520of%2520medical%2520images.%2520The%250Asmall%2520size%2520of%2520medical%2520image%2520datasets%2520and%2520the%2520high%2520cost%2520of%2520data%2520acquisition%250Afurther%2520limit%2520the%2520performance%2520of%2520segmentation%2520networks.%2520Diffusion%2520models%252C%2520with%250Atheir%2520iterative%2520denoising%2520process%252C%2520offer%2520a%2520promising%2520alternative%2520for%2520better%250Adetail%2520capture%2520in%2520segmentation.%2520However%252C%2520they%2520face%2520difficulties%2520in%2520accurately%250Asegmenting%2520small%2520targets%2520and%2520maintaining%2520the%2520precision%2520of%2520boundary%2520details.%250AThis%2520article%2520discusses%2520the%2520importance%2520of%2520medical%2520image%2520segmentation%252C%2520the%250Alimitations%2520of%2520current%2520deep%2520learning%2520approaches%252C%2520and%2520the%2520potential%2520of%2520diffusion%250Amodels%2520to%2520address%2520these%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Medical%20Image%20Segmentation%20with%20Deep%20Learning%20and%20Diffusion%0A%20%20Models&entry.906535625=Houze%20Liu%20and%20Tong%20Zhou%20and%20Yanlin%20Xiang%20and%20Aoran%20Shen%20and%20Jiacheng%20Hu%20and%20Junliang%20Du&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20crucial%20for%20accurate%20clinical%20diagnoses%2C%20yet%20it%0Afaces%20challenges%20such%20as%20low%20contrast%20between%20lesions%20and%20normal%20tissues%2C%0Aunclear%20boundaries%2C%20and%20high%20variability%20across%20patients.%20Deep%20learning%20has%0Aimproved%20segmentation%20accuracy%20and%20efficiency%2C%20but%20it%20still%20relies%20heavily%20on%0Aexpert%20annotations%20and%20struggles%20with%20the%20complexities%20of%20medical%20images.%20The%0Asmall%20size%20of%20medical%20image%20datasets%20and%20the%20high%20cost%20of%20data%20acquisition%0Afurther%20limit%20the%20performance%20of%20segmentation%20networks.%20Diffusion%20models%2C%20with%0Atheir%20iterative%20denoising%20process%2C%20offer%20a%20promising%20alternative%20for%20better%0Adetail%20capture%20in%20segmentation.%20However%2C%20they%20face%20difficulties%20in%20accurately%0Asegmenting%20small%20targets%20and%20maintaining%20the%20precision%20of%20boundary%20details.%0AThis%20article%20discusses%20the%20importance%20of%20medical%20image%20segmentation%2C%20the%0Alimitations%20of%20current%20deep%20learning%20approaches%2C%20and%20the%20potential%20of%20diffusion%0Amodels%20to%20address%20these%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14353v1&entry.124074799=Read"},
{"title": "Multi-Agent Environments for Vehicle Routing Problems", "author": "Ricardo Gama and Daniel Fuertes and Carlos R. del-Blanco and Hugo L. Fernandes", "abstract": "  Research on Reinforcement Learning (RL) approaches for discrete optimization\nproblems has increased considerably, extending RL to an area classically\ndominated by Operations Research (OR). Vehicle routing problems are a good\nexample of discrete optimization problems with high practical relevance where\nRL techniques have had considerable success. Despite these advances,\nopen-source development frameworks remain scarce, hampering both the testing of\nalgorithms and the ability to objectively compare results. This ultimately\nslows down progress in the field and limits the exchange of ideas between the\nRL and OR communities.\n  Here we propose a library composed of multi-agent environments that simulates\nclassic vehicle routing problems. The library, built on PyTorch, provides a\nflexible modular architecture design that allows easy customization and\nincorporation of new routing problems. It follows the Agent Environment Cycle\n(\"AEC\") games model and has an intuitive API, enabling rapid adoption and easy\nintegration into existing reinforcement learning frameworks.\n  The library allows for a straightforward use of classical OR benchmark\ninstances in order to narrow the gap between the test beds for algorithm\nbenchmarking used by the RL and OR communities. Additionally, we provide\nbenchmark instance sets for each environment, as well as baseline RL models and\ntraining code.\n", "link": "http://arxiv.org/abs/2411.14411v1", "date": "2024-11-21", "relevancy": 1.5243, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5125}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Environments%20for%20Vehicle%20Routing%20Problems&body=Title%3A%20Multi-Agent%20Environments%20for%20Vehicle%20Routing%20Problems%0AAuthor%3A%20Ricardo%20Gama%20and%20Daniel%20Fuertes%20and%20Carlos%20R.%20del-Blanco%20and%20Hugo%20L.%20Fernandes%0AAbstract%3A%20%20%20Research%20on%20Reinforcement%20Learning%20%28RL%29%20approaches%20for%20discrete%20optimization%0Aproblems%20has%20increased%20considerably%2C%20extending%20RL%20to%20an%20area%20classically%0Adominated%20by%20Operations%20Research%20%28OR%29.%20Vehicle%20routing%20problems%20are%20a%20good%0Aexample%20of%20discrete%20optimization%20problems%20with%20high%20practical%20relevance%20where%0ARL%20techniques%20have%20had%20considerable%20success.%20Despite%20these%20advances%2C%0Aopen-source%20development%20frameworks%20remain%20scarce%2C%20hampering%20both%20the%20testing%20of%0Aalgorithms%20and%20the%20ability%20to%20objectively%20compare%20results.%20This%20ultimately%0Aslows%20down%20progress%20in%20the%20field%20and%20limits%20the%20exchange%20of%20ideas%20between%20the%0ARL%20and%20OR%20communities.%0A%20%20Here%20we%20propose%20a%20library%20composed%20of%20multi-agent%20environments%20that%20simulates%0Aclassic%20vehicle%20routing%20problems.%20The%20library%2C%20built%20on%20PyTorch%2C%20provides%20a%0Aflexible%20modular%20architecture%20design%20that%20allows%20easy%20customization%20and%0Aincorporation%20of%20new%20routing%20problems.%20It%20follows%20the%20Agent%20Environment%20Cycle%0A%28%22AEC%22%29%20games%20model%20and%20has%20an%20intuitive%20API%2C%20enabling%20rapid%20adoption%20and%20easy%0Aintegration%20into%20existing%20reinforcement%20learning%20frameworks.%0A%20%20The%20library%20allows%20for%20a%20straightforward%20use%20of%20classical%20OR%20benchmark%0Ainstances%20in%20order%20to%20narrow%20the%20gap%20between%20the%20test%20beds%20for%20algorithm%0Abenchmarking%20used%20by%20the%20RL%20and%20OR%20communities.%20Additionally%2C%20we%20provide%0Abenchmark%20instance%20sets%20for%20each%20environment%2C%20as%20well%20as%20baseline%20RL%20models%20and%0Atraining%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Environments%2520for%2520Vehicle%2520Routing%2520Problems%26entry.906535625%3DRicardo%2520Gama%2520and%2520Daniel%2520Fuertes%2520and%2520Carlos%2520R.%2520del-Blanco%2520and%2520Hugo%2520L.%2520Fernandes%26entry.1292438233%3D%2520%2520Research%2520on%2520Reinforcement%2520Learning%2520%2528RL%2529%2520approaches%2520for%2520discrete%2520optimization%250Aproblems%2520has%2520increased%2520considerably%252C%2520extending%2520RL%2520to%2520an%2520area%2520classically%250Adominated%2520by%2520Operations%2520Research%2520%2528OR%2529.%2520Vehicle%2520routing%2520problems%2520are%2520a%2520good%250Aexample%2520of%2520discrete%2520optimization%2520problems%2520with%2520high%2520practical%2520relevance%2520where%250ARL%2520techniques%2520have%2520had%2520considerable%2520success.%2520Despite%2520these%2520advances%252C%250Aopen-source%2520development%2520frameworks%2520remain%2520scarce%252C%2520hampering%2520both%2520the%2520testing%2520of%250Aalgorithms%2520and%2520the%2520ability%2520to%2520objectively%2520compare%2520results.%2520This%2520ultimately%250Aslows%2520down%2520progress%2520in%2520the%2520field%2520and%2520limits%2520the%2520exchange%2520of%2520ideas%2520between%2520the%250ARL%2520and%2520OR%2520communities.%250A%2520%2520Here%2520we%2520propose%2520a%2520library%2520composed%2520of%2520multi-agent%2520environments%2520that%2520simulates%250Aclassic%2520vehicle%2520routing%2520problems.%2520The%2520library%252C%2520built%2520on%2520PyTorch%252C%2520provides%2520a%250Aflexible%2520modular%2520architecture%2520design%2520that%2520allows%2520easy%2520customization%2520and%250Aincorporation%2520of%2520new%2520routing%2520problems.%2520It%2520follows%2520the%2520Agent%2520Environment%2520Cycle%250A%2528%2522AEC%2522%2529%2520games%2520model%2520and%2520has%2520an%2520intuitive%2520API%252C%2520enabling%2520rapid%2520adoption%2520and%2520easy%250Aintegration%2520into%2520existing%2520reinforcement%2520learning%2520frameworks.%250A%2520%2520The%2520library%2520allows%2520for%2520a%2520straightforward%2520use%2520of%2520classical%2520OR%2520benchmark%250Ainstances%2520in%2520order%2520to%2520narrow%2520the%2520gap%2520between%2520the%2520test%2520beds%2520for%2520algorithm%250Abenchmarking%2520used%2520by%2520the%2520RL%2520and%2520OR%2520communities.%2520Additionally%252C%2520we%2520provide%250Abenchmark%2520instance%2520sets%2520for%2520each%2520environment%252C%2520as%2520well%2520as%2520baseline%2520RL%2520models%2520and%250Atraining%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Environments%20for%20Vehicle%20Routing%20Problems&entry.906535625=Ricardo%20Gama%20and%20Daniel%20Fuertes%20and%20Carlos%20R.%20del-Blanco%20and%20Hugo%20L.%20Fernandes&entry.1292438233=%20%20Research%20on%20Reinforcement%20Learning%20%28RL%29%20approaches%20for%20discrete%20optimization%0Aproblems%20has%20increased%20considerably%2C%20extending%20RL%20to%20an%20area%20classically%0Adominated%20by%20Operations%20Research%20%28OR%29.%20Vehicle%20routing%20problems%20are%20a%20good%0Aexample%20of%20discrete%20optimization%20problems%20with%20high%20practical%20relevance%20where%0ARL%20techniques%20have%20had%20considerable%20success.%20Despite%20these%20advances%2C%0Aopen-source%20development%20frameworks%20remain%20scarce%2C%20hampering%20both%20the%20testing%20of%0Aalgorithms%20and%20the%20ability%20to%20objectively%20compare%20results.%20This%20ultimately%0Aslows%20down%20progress%20in%20the%20field%20and%20limits%20the%20exchange%20of%20ideas%20between%20the%0ARL%20and%20OR%20communities.%0A%20%20Here%20we%20propose%20a%20library%20composed%20of%20multi-agent%20environments%20that%20simulates%0Aclassic%20vehicle%20routing%20problems.%20The%20library%2C%20built%20on%20PyTorch%2C%20provides%20a%0Aflexible%20modular%20architecture%20design%20that%20allows%20easy%20customization%20and%0Aincorporation%20of%20new%20routing%20problems.%20It%20follows%20the%20Agent%20Environment%20Cycle%0A%28%22AEC%22%29%20games%20model%20and%20has%20an%20intuitive%20API%2C%20enabling%20rapid%20adoption%20and%20easy%0Aintegration%20into%20existing%20reinforcement%20learning%20frameworks.%0A%20%20The%20library%20allows%20for%20a%20straightforward%20use%20of%20classical%20OR%20benchmark%0Ainstances%20in%20order%20to%20narrow%20the%20gap%20between%20the%20test%20beds%20for%20algorithm%0Abenchmarking%20used%20by%20the%20RL%20and%20OR%20communities.%20Additionally%2C%20we%20provide%0Abenchmark%20instance%20sets%20for%20each%20environment%2C%20as%20well%20as%20baseline%20RL%20models%20and%0Atraining%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14411v1&entry.124074799=Read"},
{"title": "Simulation-Aided Policy Tuning for Black-Box Robot Learning", "author": "Shiming He and Alexander von Rohr and Dominik Baumann and Ji Xiang and Sebastian Trimpe", "abstract": "  How can robots learn and adapt to new tasks and situations with little data?\nSystematic exploration and simulation are crucial tools for efficient robot\nlearning. We present a novel black-box policy search algorithm focused on\ndata-efficient policy improvements. The algorithm learns directly on the robot\nand treats simulation as an additional information source to speed up the\nlearning process. At the core of the algorithm, a probabilistic model learns\nthe dependence of the policy parameters and the robot learning objective not\nonly by performing experiments on the robot, but also by leveraging data from a\nsimulator. This substantially reduces interaction time with the robot. Using\nthis model, we can guarantee improvements with high probability for each policy\nupdate, thereby facilitating fast, goal-oriented learning. We evaluate our\nalgorithm on simulated fine-tuning tasks and demonstrate the data-efficiency of\nthe proposed dual-information source optimization algorithm. In a real robot\nlearning experiment, we show fast and successful task learning on a robot\nmanipulator with the aid of an imperfect simulator.\n", "link": "http://arxiv.org/abs/2411.14246v1", "date": "2024-11-21", "relevancy": 1.6699, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.564}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5562}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulation-Aided%20Policy%20Tuning%20for%20Black-Box%20Robot%20Learning&body=Title%3A%20Simulation-Aided%20Policy%20Tuning%20for%20Black-Box%20Robot%20Learning%0AAuthor%3A%20Shiming%20He%20and%20Alexander%20von%20Rohr%20and%20Dominik%20Baumann%20and%20Ji%20Xiang%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20How%20can%20robots%20learn%20and%20adapt%20to%20new%20tasks%20and%20situations%20with%20little%20data%3F%0ASystematic%20exploration%20and%20simulation%20are%20crucial%20tools%20for%20efficient%20robot%0Alearning.%20We%20present%20a%20novel%20black-box%20policy%20search%20algorithm%20focused%20on%0Adata-efficient%20policy%20improvements.%20The%20algorithm%20learns%20directly%20on%20the%20robot%0Aand%20treats%20simulation%20as%20an%20additional%20information%20source%20to%20speed%20up%20the%0Alearning%20process.%20At%20the%20core%20of%20the%20algorithm%2C%20a%20probabilistic%20model%20learns%0Athe%20dependence%20of%20the%20policy%20parameters%20and%20the%20robot%20learning%20objective%20not%0Aonly%20by%20performing%20experiments%20on%20the%20robot%2C%20but%20also%20by%20leveraging%20data%20from%20a%0Asimulator.%20This%20substantially%20reduces%20interaction%20time%20with%20the%20robot.%20Using%0Athis%20model%2C%20we%20can%20guarantee%20improvements%20with%20high%20probability%20for%20each%20policy%0Aupdate%2C%20thereby%20facilitating%20fast%2C%20goal-oriented%20learning.%20We%20evaluate%20our%0Aalgorithm%20on%20simulated%20fine-tuning%20tasks%20and%20demonstrate%20the%20data-efficiency%20of%0Athe%20proposed%20dual-information%20source%20optimization%20algorithm.%20In%20a%20real%20robot%0Alearning%20experiment%2C%20we%20show%20fast%20and%20successful%20task%20learning%20on%20a%20robot%0Amanipulator%20with%20the%20aid%20of%20an%20imperfect%20simulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulation-Aided%2520Policy%2520Tuning%2520for%2520Black-Box%2520Robot%2520Learning%26entry.906535625%3DShiming%2520He%2520and%2520Alexander%2520von%2520Rohr%2520and%2520Dominik%2520Baumann%2520and%2520Ji%2520Xiang%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520How%2520can%2520robots%2520learn%2520and%2520adapt%2520to%2520new%2520tasks%2520and%2520situations%2520with%2520little%2520data%253F%250ASystematic%2520exploration%2520and%2520simulation%2520are%2520crucial%2520tools%2520for%2520efficient%2520robot%250Alearning.%2520We%2520present%2520a%2520novel%2520black-box%2520policy%2520search%2520algorithm%2520focused%2520on%250Adata-efficient%2520policy%2520improvements.%2520The%2520algorithm%2520learns%2520directly%2520on%2520the%2520robot%250Aand%2520treats%2520simulation%2520as%2520an%2520additional%2520information%2520source%2520to%2520speed%2520up%2520the%250Alearning%2520process.%2520At%2520the%2520core%2520of%2520the%2520algorithm%252C%2520a%2520probabilistic%2520model%2520learns%250Athe%2520dependence%2520of%2520the%2520policy%2520parameters%2520and%2520the%2520robot%2520learning%2520objective%2520not%250Aonly%2520by%2520performing%2520experiments%2520on%2520the%2520robot%252C%2520but%2520also%2520by%2520leveraging%2520data%2520from%2520a%250Asimulator.%2520This%2520substantially%2520reduces%2520interaction%2520time%2520with%2520the%2520robot.%2520Using%250Athis%2520model%252C%2520we%2520can%2520guarantee%2520improvements%2520with%2520high%2520probability%2520for%2520each%2520policy%250Aupdate%252C%2520thereby%2520facilitating%2520fast%252C%2520goal-oriented%2520learning.%2520We%2520evaluate%2520our%250Aalgorithm%2520on%2520simulated%2520fine-tuning%2520tasks%2520and%2520demonstrate%2520the%2520data-efficiency%2520of%250Athe%2520proposed%2520dual-information%2520source%2520optimization%2520algorithm.%2520In%2520a%2520real%2520robot%250Alearning%2520experiment%252C%2520we%2520show%2520fast%2520and%2520successful%2520task%2520learning%2520on%2520a%2520robot%250Amanipulator%2520with%2520the%2520aid%2520of%2520an%2520imperfect%2520simulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulation-Aided%20Policy%20Tuning%20for%20Black-Box%20Robot%20Learning&entry.906535625=Shiming%20He%20and%20Alexander%20von%20Rohr%20and%20Dominik%20Baumann%20and%20Ji%20Xiang%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20How%20can%20robots%20learn%20and%20adapt%20to%20new%20tasks%20and%20situations%20with%20little%20data%3F%0ASystematic%20exploration%20and%20simulation%20are%20crucial%20tools%20for%20efficient%20robot%0Alearning.%20We%20present%20a%20novel%20black-box%20policy%20search%20algorithm%20focused%20on%0Adata-efficient%20policy%20improvements.%20The%20algorithm%20learns%20directly%20on%20the%20robot%0Aand%20treats%20simulation%20as%20an%20additional%20information%20source%20to%20speed%20up%20the%0Alearning%20process.%20At%20the%20core%20of%20the%20algorithm%2C%20a%20probabilistic%20model%20learns%0Athe%20dependence%20of%20the%20policy%20parameters%20and%20the%20robot%20learning%20objective%20not%0Aonly%20by%20performing%20experiments%20on%20the%20robot%2C%20but%20also%20by%20leveraging%20data%20from%20a%0Asimulator.%20This%20substantially%20reduces%20interaction%20time%20with%20the%20robot.%20Using%0Athis%20model%2C%20we%20can%20guarantee%20improvements%20with%20high%20probability%20for%20each%20policy%0Aupdate%2C%20thereby%20facilitating%20fast%2C%20goal-oriented%20learning.%20We%20evaluate%20our%0Aalgorithm%20on%20simulated%20fine-tuning%20tasks%20and%20demonstrate%20the%20data-efficiency%20of%0Athe%20proposed%20dual-information%20source%20optimization%20algorithm.%20In%20a%20real%20robot%0Alearning%20experiment%2C%20we%20show%20fast%20and%20successful%20task%20learning%20on%20a%20robot%0Amanipulator%20with%20the%20aid%20of%20an%20imperfect%20simulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14246v1&entry.124074799=Read"},
{"title": "Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling", "author": "Daehoon Gwak and Junwoo Park and Minho Park and Chaehun Park and Hyunchan Lee and Edward Choi and Jaegul Choo", "abstract": "  Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction.\n", "link": "http://arxiv.org/abs/2411.14042v1", "date": "2024-11-21", "relevancy": 1.8173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Future%20International%20Events%3A%20A%20Reliable%20Dataset%20for%0A%20%20Text-Based%20Event%20Modeling&body=Title%3A%20Forecasting%20Future%20International%20Events%3A%20A%20Reliable%20Dataset%20for%0A%20%20Text-Based%20Event%20Modeling%0AAuthor%3A%20Daehoon%20Gwak%20and%20Junwoo%20Park%20and%20Minho%20Park%20and%20Chaehun%20Park%20and%20Hyunchan%20Lee%20and%20Edward%20Choi%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Predicting%20future%20international%20events%20from%20textual%20information%2C%20such%20as%20news%0Aarticles%2C%20has%20tremendous%20potential%20for%20applications%20in%20global%20policy%2C%20strategic%0Adecision-making%2C%20and%20geopolitics.%20However%2C%20existing%20datasets%20available%20for%20this%0Atask%20are%20often%20limited%20in%20quality%2C%20hindering%20the%20progress%20of%20related%20research.%0AIn%20this%20paper%2C%20we%20introduce%20WORLDREP%20%28WORLD%20Relationship%20and%20Event%20Prediction%29%2C%0Aa%20novel%20dataset%20designed%20to%20address%20these%20limitations%20by%20leveraging%20the%0Aadvanced%20reasoning%20capabilities%20of%20large-language%20models%20%28LLMs%29.%20Our%20dataset%0Afeatures%20high-quality%20scoring%20labels%20generated%20through%20advanced%20prompt%20modeling%0Aand%20rigorously%20validated%20by%20domain%20experts%20in%20political%20science.%20We%20showcase%0Athe%20quality%20and%20utility%20of%20WORLDREP%20for%20real-world%20event%20prediction%20tasks%2C%0Ademonstrating%20its%20effectiveness%20through%20extensive%20experiments%20and%20analysis.%0AFurthermore%2C%20we%20publicly%20release%20our%20dataset%20along%20with%20the%20full%20automation%0Asource%20code%20for%20data%20collection%2C%20labeling%2C%20and%20benchmarking%2C%20aiming%20to%20support%0Aand%20advance%20research%20in%20text-based%20event%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Future%2520International%2520Events%253A%2520A%2520Reliable%2520Dataset%2520for%250A%2520%2520Text-Based%2520Event%2520Modeling%26entry.906535625%3DDaehoon%2520Gwak%2520and%2520Junwoo%2520Park%2520and%2520Minho%2520Park%2520and%2520Chaehun%2520Park%2520and%2520Hyunchan%2520Lee%2520and%2520Edward%2520Choi%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Predicting%2520future%2520international%2520events%2520from%2520textual%2520information%252C%2520such%2520as%2520news%250Aarticles%252C%2520has%2520tremendous%2520potential%2520for%2520applications%2520in%2520global%2520policy%252C%2520strategic%250Adecision-making%252C%2520and%2520geopolitics.%2520However%252C%2520existing%2520datasets%2520available%2520for%2520this%250Atask%2520are%2520often%2520limited%2520in%2520quality%252C%2520hindering%2520the%2520progress%2520of%2520related%2520research.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520WORLDREP%2520%2528WORLD%2520Relationship%2520and%2520Event%2520Prediction%2529%252C%250Aa%2520novel%2520dataset%2520designed%2520to%2520address%2520these%2520limitations%2520by%2520leveraging%2520the%250Aadvanced%2520reasoning%2520capabilities%2520of%2520large-language%2520models%2520%2528LLMs%2529.%2520Our%2520dataset%250Afeatures%2520high-quality%2520scoring%2520labels%2520generated%2520through%2520advanced%2520prompt%2520modeling%250Aand%2520rigorously%2520validated%2520by%2520domain%2520experts%2520in%2520political%2520science.%2520We%2520showcase%250Athe%2520quality%2520and%2520utility%2520of%2520WORLDREP%2520for%2520real-world%2520event%2520prediction%2520tasks%252C%250Ademonstrating%2520its%2520effectiveness%2520through%2520extensive%2520experiments%2520and%2520analysis.%250AFurthermore%252C%2520we%2520publicly%2520release%2520our%2520dataset%2520along%2520with%2520the%2520full%2520automation%250Asource%2520code%2520for%2520data%2520collection%252C%2520labeling%252C%2520and%2520benchmarking%252C%2520aiming%2520to%2520support%250Aand%2520advance%2520research%2520in%2520text-based%2520event%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Future%20International%20Events%3A%20A%20Reliable%20Dataset%20for%0A%20%20Text-Based%20Event%20Modeling&entry.906535625=Daehoon%20Gwak%20and%20Junwoo%20Park%20and%20Minho%20Park%20and%20Chaehun%20Park%20and%20Hyunchan%20Lee%20and%20Edward%20Choi%20and%20Jaegul%20Choo&entry.1292438233=%20%20Predicting%20future%20international%20events%20from%20textual%20information%2C%20such%20as%20news%0Aarticles%2C%20has%20tremendous%20potential%20for%20applications%20in%20global%20policy%2C%20strategic%0Adecision-making%2C%20and%20geopolitics.%20However%2C%20existing%20datasets%20available%20for%20this%0Atask%20are%20often%20limited%20in%20quality%2C%20hindering%20the%20progress%20of%20related%20research.%0AIn%20this%20paper%2C%20we%20introduce%20WORLDREP%20%28WORLD%20Relationship%20and%20Event%20Prediction%29%2C%0Aa%20novel%20dataset%20designed%20to%20address%20these%20limitations%20by%20leveraging%20the%0Aadvanced%20reasoning%20capabilities%20of%20large-language%20models%20%28LLMs%29.%20Our%20dataset%0Afeatures%20high-quality%20scoring%20labels%20generated%20through%20advanced%20prompt%20modeling%0Aand%20rigorously%20validated%20by%20domain%20experts%20in%20political%20science.%20We%20showcase%0Athe%20quality%20and%20utility%20of%20WORLDREP%20for%20real-world%20event%20prediction%20tasks%2C%0Ademonstrating%20its%20effectiveness%20through%20extensive%20experiments%20and%20analysis.%0AFurthermore%2C%20we%20publicly%20release%20our%20dataset%20along%20with%20the%20full%20automation%0Asource%20code%20for%20data%20collection%2C%20labeling%2C%20and%20benchmarking%2C%20aiming%20to%20support%0Aand%20advance%20research%20in%20text-based%20event%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14042v1&entry.124074799=Read"},
{"title": "Model Inversion Attacks Through Target-Specific Conditional Diffusion\n  Models", "author": "Ouxiang Li and Yanbin Hao and Zhicai Wang and Bin Zhu and Shuo Wang and Zaixi Zhang and Fuli Feng", "abstract": "  Model inversion attacks (MIAs) aim to reconstruct private images from a\ntarget classifier's training set, thereby raising privacy concerns in AI\napplications. Previous GAN-based MIAs tend to suffer from inferior generative\nfidelity due to GAN's inherent flaws and biased optimization within latent\nspace. To alleviate these issues, leveraging on diffusion models' remarkable\nsynthesis capabilities, we propose Diffusion-based Model Inversion (Diff-MI)\nattacks. Specifically, we introduce a novel target-specific conditional\ndiffusion model (CDM) to purposely approximate target classifier's private\ndistribution and achieve superior accuracy-fidelity balance. Our method\ninvolves a two-step learning paradigm. Step-1 incorporates the target\nclassifier into the entire CDM learning under a pretrain-then-finetune fashion,\nwith creating pseudo-labels as model conditions in pretraining and adjusting\nspecified layers with image predictions in fine-tuning. Step-2 presents an\niterative image reconstruction method, further enhancing the attack performance\nthrough a combination of diffusion priors and target knowledge. Additionally,\nwe propose an improved max-margin loss that replaces the hard max with top-k\nmaxes, fully leveraging feature information and soft labels from the target\nclassifier. Extensive experiments demonstrate that Diff-MI significantly\nimproves generative fidelity with an average decrease of 20\\% in FID while\nmaintaining competitive attack accuracy compared to state-of-the-art methods\nacross various datasets and models. Our code is available at:\n\\url{https://github.com/Ouxiang-Li/Diff-MI}.\n", "link": "http://arxiv.org/abs/2407.11424v2", "date": "2024-11-21", "relevancy": 1.8006, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6308}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5991}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Inversion%20Attacks%20Through%20Target-Specific%20Conditional%20Diffusion%0A%20%20Models&body=Title%3A%20Model%20Inversion%20Attacks%20Through%20Target-Specific%20Conditional%20Diffusion%0A%20%20Models%0AAuthor%3A%20Ouxiang%20Li%20and%20Yanbin%20Hao%20and%20Zhicai%20Wang%20and%20Bin%20Zhu%20and%20Shuo%20Wang%20and%20Zaixi%20Zhang%20and%20Fuli%20Feng%0AAbstract%3A%20%20%20Model%20inversion%20attacks%20%28MIAs%29%20aim%20to%20reconstruct%20private%20images%20from%20a%0Atarget%20classifier%27s%20training%20set%2C%20thereby%20raising%20privacy%20concerns%20in%20AI%0Aapplications.%20Previous%20GAN-based%20MIAs%20tend%20to%20suffer%20from%20inferior%20generative%0Afidelity%20due%20to%20GAN%27s%20inherent%20flaws%20and%20biased%20optimization%20within%20latent%0Aspace.%20To%20alleviate%20these%20issues%2C%20leveraging%20on%20diffusion%20models%27%20remarkable%0Asynthesis%20capabilities%2C%20we%20propose%20Diffusion-based%20Model%20Inversion%20%28Diff-MI%29%0Aattacks.%20Specifically%2C%20we%20introduce%20a%20novel%20target-specific%20conditional%0Adiffusion%20model%20%28CDM%29%20to%20purposely%20approximate%20target%20classifier%27s%20private%0Adistribution%20and%20achieve%20superior%20accuracy-fidelity%20balance.%20Our%20method%0Ainvolves%20a%20two-step%20learning%20paradigm.%20Step-1%20incorporates%20the%20target%0Aclassifier%20into%20the%20entire%20CDM%20learning%20under%20a%20pretrain-then-finetune%20fashion%2C%0Awith%20creating%20pseudo-labels%20as%20model%20conditions%20in%20pretraining%20and%20adjusting%0Aspecified%20layers%20with%20image%20predictions%20in%20fine-tuning.%20Step-2%20presents%20an%0Aiterative%20image%20reconstruction%20method%2C%20further%20enhancing%20the%20attack%20performance%0Athrough%20a%20combination%20of%20diffusion%20priors%20and%20target%20knowledge.%20Additionally%2C%0Awe%20propose%20an%20improved%20max-margin%20loss%20that%20replaces%20the%20hard%20max%20with%20top-k%0Amaxes%2C%20fully%20leveraging%20feature%20information%20and%20soft%20labels%20from%20the%20target%0Aclassifier.%20Extensive%20experiments%20demonstrate%20that%20Diff-MI%20significantly%0Aimproves%20generative%20fidelity%20with%20an%20average%20decrease%20of%2020%5C%25%20in%20FID%20while%0Amaintaining%20competitive%20attack%20accuracy%20compared%20to%20state-of-the-art%20methods%0Aacross%20various%20datasets%20and%20models.%20Our%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Ouxiang-Li/Diff-MI%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11424v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Inversion%2520Attacks%2520Through%2520Target-Specific%2520Conditional%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DOuxiang%2520Li%2520and%2520Yanbin%2520Hao%2520and%2520Zhicai%2520Wang%2520and%2520Bin%2520Zhu%2520and%2520Shuo%2520Wang%2520and%2520Zaixi%2520Zhang%2520and%2520Fuli%2520Feng%26entry.1292438233%3D%2520%2520Model%2520inversion%2520attacks%2520%2528MIAs%2529%2520aim%2520to%2520reconstruct%2520private%2520images%2520from%2520a%250Atarget%2520classifier%2527s%2520training%2520set%252C%2520thereby%2520raising%2520privacy%2520concerns%2520in%2520AI%250Aapplications.%2520Previous%2520GAN-based%2520MIAs%2520tend%2520to%2520suffer%2520from%2520inferior%2520generative%250Afidelity%2520due%2520to%2520GAN%2527s%2520inherent%2520flaws%2520and%2520biased%2520optimization%2520within%2520latent%250Aspace.%2520To%2520alleviate%2520these%2520issues%252C%2520leveraging%2520on%2520diffusion%2520models%2527%2520remarkable%250Asynthesis%2520capabilities%252C%2520we%2520propose%2520Diffusion-based%2520Model%2520Inversion%2520%2528Diff-MI%2529%250Aattacks.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520target-specific%2520conditional%250Adiffusion%2520model%2520%2528CDM%2529%2520to%2520purposely%2520approximate%2520target%2520classifier%2527s%2520private%250Adistribution%2520and%2520achieve%2520superior%2520accuracy-fidelity%2520balance.%2520Our%2520method%250Ainvolves%2520a%2520two-step%2520learning%2520paradigm.%2520Step-1%2520incorporates%2520the%2520target%250Aclassifier%2520into%2520the%2520entire%2520CDM%2520learning%2520under%2520a%2520pretrain-then-finetune%2520fashion%252C%250Awith%2520creating%2520pseudo-labels%2520as%2520model%2520conditions%2520in%2520pretraining%2520and%2520adjusting%250Aspecified%2520layers%2520with%2520image%2520predictions%2520in%2520fine-tuning.%2520Step-2%2520presents%2520an%250Aiterative%2520image%2520reconstruction%2520method%252C%2520further%2520enhancing%2520the%2520attack%2520performance%250Athrough%2520a%2520combination%2520of%2520diffusion%2520priors%2520and%2520target%2520knowledge.%2520Additionally%252C%250Awe%2520propose%2520an%2520improved%2520max-margin%2520loss%2520that%2520replaces%2520the%2520hard%2520max%2520with%2520top-k%250Amaxes%252C%2520fully%2520leveraging%2520feature%2520information%2520and%2520soft%2520labels%2520from%2520the%2520target%250Aclassifier.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Diff-MI%2520significantly%250Aimproves%2520generative%2520fidelity%2520with%2520an%2520average%2520decrease%2520of%252020%255C%2525%2520in%2520FID%2520while%250Amaintaining%2520competitive%2520attack%2520accuracy%2520compared%2520to%2520state-of-the-art%2520methods%250Aacross%2520various%2520datasets%2520and%2520models.%2520Our%2520code%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/Ouxiang-Li/Diff-MI%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11424v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Inversion%20Attacks%20Through%20Target-Specific%20Conditional%20Diffusion%0A%20%20Models&entry.906535625=Ouxiang%20Li%20and%20Yanbin%20Hao%20and%20Zhicai%20Wang%20and%20Bin%20Zhu%20and%20Shuo%20Wang%20and%20Zaixi%20Zhang%20and%20Fuli%20Feng&entry.1292438233=%20%20Model%20inversion%20attacks%20%28MIAs%29%20aim%20to%20reconstruct%20private%20images%20from%20a%0Atarget%20classifier%27s%20training%20set%2C%20thereby%20raising%20privacy%20concerns%20in%20AI%0Aapplications.%20Previous%20GAN-based%20MIAs%20tend%20to%20suffer%20from%20inferior%20generative%0Afidelity%20due%20to%20GAN%27s%20inherent%20flaws%20and%20biased%20optimization%20within%20latent%0Aspace.%20To%20alleviate%20these%20issues%2C%20leveraging%20on%20diffusion%20models%27%20remarkable%0Asynthesis%20capabilities%2C%20we%20propose%20Diffusion-based%20Model%20Inversion%20%28Diff-MI%29%0Aattacks.%20Specifically%2C%20we%20introduce%20a%20novel%20target-specific%20conditional%0Adiffusion%20model%20%28CDM%29%20to%20purposely%20approximate%20target%20classifier%27s%20private%0Adistribution%20and%20achieve%20superior%20accuracy-fidelity%20balance.%20Our%20method%0Ainvolves%20a%20two-step%20learning%20paradigm.%20Step-1%20incorporates%20the%20target%0Aclassifier%20into%20the%20entire%20CDM%20learning%20under%20a%20pretrain-then-finetune%20fashion%2C%0Awith%20creating%20pseudo-labels%20as%20model%20conditions%20in%20pretraining%20and%20adjusting%0Aspecified%20layers%20with%20image%20predictions%20in%20fine-tuning.%20Step-2%20presents%20an%0Aiterative%20image%20reconstruction%20method%2C%20further%20enhancing%20the%20attack%20performance%0Athrough%20a%20combination%20of%20diffusion%20priors%20and%20target%20knowledge.%20Additionally%2C%0Awe%20propose%20an%20improved%20max-margin%20loss%20that%20replaces%20the%20hard%20max%20with%20top-k%0Amaxes%2C%20fully%20leveraging%20feature%20information%20and%20soft%20labels%20from%20the%20target%0Aclassifier.%20Extensive%20experiments%20demonstrate%20that%20Diff-MI%20significantly%0Aimproves%20generative%20fidelity%20with%20an%20average%20decrease%20of%2020%5C%25%20in%20FID%20while%0Amaintaining%20competitive%20attack%20accuracy%20compared%20to%20state-of-the-art%20methods%0Aacross%20various%20datasets%20and%20models.%20Our%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Ouxiang-Li/Diff-MI%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11424v2&entry.124074799=Read"},
{"title": "M-SET: Multi-Drone Swarm Intelligence Experimentation with Collision\n  Avoidance Realism", "author": "Chuhao Qin and Alexander Robins and Callum Lillywhite-Roake and Adam Pearce and Hritik Mehta and Scott James and Tsz Ho Wong and Evangelos Pournaras", "abstract": "  Distributed sensing by cooperative drone swarms is crucial for several Smart\nCity applications, such as traffic monitoring and disaster response. Using an\nindoor lab with inexpensive drones, a testbed supports complex and ambitious\nstudies on these systems while maintaining low cost, rigor, and external\nvalidity. This paper introduces the Multi-drone Sensing Experimentation Testbed\n(M-SET), a novel platform designed to prototype, develop, test, and evaluate\ndistributed sensing with swarm intelligence. M-SET addresses the limitations of\nexisting testbeds that fail to emulate collisions, thus lacking realism in\noutdoor environments. By integrating a collision avoidance method based on a\npotential field algorithm, M-SET ensures collision-free navigation and sensing,\nfurther optimized via a multi-agent collective learning algorithm. Extensive\nevaluation demonstrates accurate energy consumption estimation and a low risk\nof collisions, providing a robust proof-of-concept. New insights show that\nM-SET has significant potential to support ambitious research with minimal\ncost, simplicity, and high sensing quality.\n", "link": "http://arxiv.org/abs/2406.10916v2", "date": "2024-11-21", "relevancy": 1.5161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5387}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M-SET%3A%20Multi-Drone%20Swarm%20Intelligence%20Experimentation%20with%20Collision%0A%20%20Avoidance%20Realism&body=Title%3A%20M-SET%3A%20Multi-Drone%20Swarm%20Intelligence%20Experimentation%20with%20Collision%0A%20%20Avoidance%20Realism%0AAuthor%3A%20Chuhao%20Qin%20and%20Alexander%20Robins%20and%20Callum%20Lillywhite-Roake%20and%20Adam%20Pearce%20and%20Hritik%20Mehta%20and%20Scott%20James%20and%20Tsz%20Ho%20Wong%20and%20Evangelos%20Pournaras%0AAbstract%3A%20%20%20Distributed%20sensing%20by%20cooperative%20drone%20swarms%20is%20crucial%20for%20several%20Smart%0ACity%20applications%2C%20such%20as%20traffic%20monitoring%20and%20disaster%20response.%20Using%20an%0Aindoor%20lab%20with%20inexpensive%20drones%2C%20a%20testbed%20supports%20complex%20and%20ambitious%0Astudies%20on%20these%20systems%20while%20maintaining%20low%20cost%2C%20rigor%2C%20and%20external%0Avalidity.%20This%20paper%20introduces%20the%20Multi-drone%20Sensing%20Experimentation%20Testbed%0A%28M-SET%29%2C%20a%20novel%20platform%20designed%20to%20prototype%2C%20develop%2C%20test%2C%20and%20evaluate%0Adistributed%20sensing%20with%20swarm%20intelligence.%20M-SET%20addresses%20the%20limitations%20of%0Aexisting%20testbeds%20that%20fail%20to%20emulate%20collisions%2C%20thus%20lacking%20realism%20in%0Aoutdoor%20environments.%20By%20integrating%20a%20collision%20avoidance%20method%20based%20on%20a%0Apotential%20field%20algorithm%2C%20M-SET%20ensures%20collision-free%20navigation%20and%20sensing%2C%0Afurther%20optimized%20via%20a%20multi-agent%20collective%20learning%20algorithm.%20Extensive%0Aevaluation%20demonstrates%20accurate%20energy%20consumption%20estimation%20and%20a%20low%20risk%0Aof%20collisions%2C%20providing%20a%20robust%20proof-of-concept.%20New%20insights%20show%20that%0AM-SET%20has%20significant%20potential%20to%20support%20ambitious%20research%20with%20minimal%0Acost%2C%20simplicity%2C%20and%20high%20sensing%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM-SET%253A%2520Multi-Drone%2520Swarm%2520Intelligence%2520Experimentation%2520with%2520Collision%250A%2520%2520Avoidance%2520Realism%26entry.906535625%3DChuhao%2520Qin%2520and%2520Alexander%2520Robins%2520and%2520Callum%2520Lillywhite-Roake%2520and%2520Adam%2520Pearce%2520and%2520Hritik%2520Mehta%2520and%2520Scott%2520James%2520and%2520Tsz%2520Ho%2520Wong%2520and%2520Evangelos%2520Pournaras%26entry.1292438233%3D%2520%2520Distributed%2520sensing%2520by%2520cooperative%2520drone%2520swarms%2520is%2520crucial%2520for%2520several%2520Smart%250ACity%2520applications%252C%2520such%2520as%2520traffic%2520monitoring%2520and%2520disaster%2520response.%2520Using%2520an%250Aindoor%2520lab%2520with%2520inexpensive%2520drones%252C%2520a%2520testbed%2520supports%2520complex%2520and%2520ambitious%250Astudies%2520on%2520these%2520systems%2520while%2520maintaining%2520low%2520cost%252C%2520rigor%252C%2520and%2520external%250Avalidity.%2520This%2520paper%2520introduces%2520the%2520Multi-drone%2520Sensing%2520Experimentation%2520Testbed%250A%2528M-SET%2529%252C%2520a%2520novel%2520platform%2520designed%2520to%2520prototype%252C%2520develop%252C%2520test%252C%2520and%2520evaluate%250Adistributed%2520sensing%2520with%2520swarm%2520intelligence.%2520M-SET%2520addresses%2520the%2520limitations%2520of%250Aexisting%2520testbeds%2520that%2520fail%2520to%2520emulate%2520collisions%252C%2520thus%2520lacking%2520realism%2520in%250Aoutdoor%2520environments.%2520By%2520integrating%2520a%2520collision%2520avoidance%2520method%2520based%2520on%2520a%250Apotential%2520field%2520algorithm%252C%2520M-SET%2520ensures%2520collision-free%2520navigation%2520and%2520sensing%252C%250Afurther%2520optimized%2520via%2520a%2520multi-agent%2520collective%2520learning%2520algorithm.%2520Extensive%250Aevaluation%2520demonstrates%2520accurate%2520energy%2520consumption%2520estimation%2520and%2520a%2520low%2520risk%250Aof%2520collisions%252C%2520providing%2520a%2520robust%2520proof-of-concept.%2520New%2520insights%2520show%2520that%250AM-SET%2520has%2520significant%2520potential%2520to%2520support%2520ambitious%2520research%2520with%2520minimal%250Acost%252C%2520simplicity%252C%2520and%2520high%2520sensing%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-SET%3A%20Multi-Drone%20Swarm%20Intelligence%20Experimentation%20with%20Collision%0A%20%20Avoidance%20Realism&entry.906535625=Chuhao%20Qin%20and%20Alexander%20Robins%20and%20Callum%20Lillywhite-Roake%20and%20Adam%20Pearce%20and%20Hritik%20Mehta%20and%20Scott%20James%20and%20Tsz%20Ho%20Wong%20and%20Evangelos%20Pournaras&entry.1292438233=%20%20Distributed%20sensing%20by%20cooperative%20drone%20swarms%20is%20crucial%20for%20several%20Smart%0ACity%20applications%2C%20such%20as%20traffic%20monitoring%20and%20disaster%20response.%20Using%20an%0Aindoor%20lab%20with%20inexpensive%20drones%2C%20a%20testbed%20supports%20complex%20and%20ambitious%0Astudies%20on%20these%20systems%20while%20maintaining%20low%20cost%2C%20rigor%2C%20and%20external%0Avalidity.%20This%20paper%20introduces%20the%20Multi-drone%20Sensing%20Experimentation%20Testbed%0A%28M-SET%29%2C%20a%20novel%20platform%20designed%20to%20prototype%2C%20develop%2C%20test%2C%20and%20evaluate%0Adistributed%20sensing%20with%20swarm%20intelligence.%20M-SET%20addresses%20the%20limitations%20of%0Aexisting%20testbeds%20that%20fail%20to%20emulate%20collisions%2C%20thus%20lacking%20realism%20in%0Aoutdoor%20environments.%20By%20integrating%20a%20collision%20avoidance%20method%20based%20on%20a%0Apotential%20field%20algorithm%2C%20M-SET%20ensures%20collision-free%20navigation%20and%20sensing%2C%0Afurther%20optimized%20via%20a%20multi-agent%20collective%20learning%20algorithm.%20Extensive%0Aevaluation%20demonstrates%20accurate%20energy%20consumption%20estimation%20and%20a%20low%20risk%0Aof%20collisions%2C%20providing%20a%20robust%20proof-of-concept.%20New%20insights%20show%20that%0AM-SET%20has%20significant%20potential%20to%20support%20ambitious%20research%20with%20minimal%0Acost%2C%20simplicity%2C%20and%20high%20sensing%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10916v2&entry.124074799=Read"},
{"title": "Model Checking for Reinforcement Learning in Autonomous Driving: One Can\n  Do More Than You Think!", "author": "Rong Gu", "abstract": "  Most reinforcement learning (RL) platforms use high-level programming\nlanguages, such as OpenAI Gymnasium using Python. These frameworks provide\nvarious API and benchmarks for testing RL algorithms in different domains, such\nas autonomous driving (AD) and robotics. These platforms often emphasise the\ndesign of RL algorithms and the training performance but neglect the\ncorrectness of models and reward functions, which can be crucial for the\nsuccessful application of RL. This paper proposes using formal methods to model\nAD systems and demonstrates how model checking (MC) can be used in RL for AD.\nMost studies combining MC and RL focus on safety, such as safety shields.\nHowever, this paper shows different facets where MC can strengthen RL. First,\nan MC-based model pre-analysis can reveal bugs with respect to sensor accuracy\nand learning step size. This step serves as a preparation of RL, which saves\ntime if bugs exist and deepens users' understanding of the target system.\nSecond, reward automata can benefit the design of reward functions and greatly\nimprove learning performance especially when the learning objectives are\nmultiple. All these findings are supported by experiments.\n", "link": "http://arxiv.org/abs/2411.14375v1", "date": "2024-11-21", "relevancy": 1.4098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4831}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Checking%20for%20Reinforcement%20Learning%20in%20Autonomous%20Driving%3A%20One%20Can%0A%20%20Do%20More%20Than%20You%20Think%21&body=Title%3A%20Model%20Checking%20for%20Reinforcement%20Learning%20in%20Autonomous%20Driving%3A%20One%20Can%0A%20%20Do%20More%20Than%20You%20Think%21%0AAuthor%3A%20Rong%20Gu%0AAbstract%3A%20%20%20Most%20reinforcement%20learning%20%28RL%29%20platforms%20use%20high-level%20programming%0Alanguages%2C%20such%20as%20OpenAI%20Gymnasium%20using%20Python.%20These%20frameworks%20provide%0Avarious%20API%20and%20benchmarks%20for%20testing%20RL%20algorithms%20in%20different%20domains%2C%20such%0Aas%20autonomous%20driving%20%28AD%29%20and%20robotics.%20These%20platforms%20often%20emphasise%20the%0Adesign%20of%20RL%20algorithms%20and%20the%20training%20performance%20but%20neglect%20the%0Acorrectness%20of%20models%20and%20reward%20functions%2C%20which%20can%20be%20crucial%20for%20the%0Asuccessful%20application%20of%20RL.%20This%20paper%20proposes%20using%20formal%20methods%20to%20model%0AAD%20systems%20and%20demonstrates%20how%20model%20checking%20%28MC%29%20can%20be%20used%20in%20RL%20for%20AD.%0AMost%20studies%20combining%20MC%20and%20RL%20focus%20on%20safety%2C%20such%20as%20safety%20shields.%0AHowever%2C%20this%20paper%20shows%20different%20facets%20where%20MC%20can%20strengthen%20RL.%20First%2C%0Aan%20MC-based%20model%20pre-analysis%20can%20reveal%20bugs%20with%20respect%20to%20sensor%20accuracy%0Aand%20learning%20step%20size.%20This%20step%20serves%20as%20a%20preparation%20of%20RL%2C%20which%20saves%0Atime%20if%20bugs%20exist%20and%20deepens%20users%27%20understanding%20of%20the%20target%20system.%0ASecond%2C%20reward%20automata%20can%20benefit%20the%20design%20of%20reward%20functions%20and%20greatly%0Aimprove%20learning%20performance%20especially%20when%20the%20learning%20objectives%20are%0Amultiple.%20All%20these%20findings%20are%20supported%20by%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Checking%2520for%2520Reinforcement%2520Learning%2520in%2520Autonomous%2520Driving%253A%2520One%2520Can%250A%2520%2520Do%2520More%2520Than%2520You%2520Think%2521%26entry.906535625%3DRong%2520Gu%26entry.1292438233%3D%2520%2520Most%2520reinforcement%2520learning%2520%2528RL%2529%2520platforms%2520use%2520high-level%2520programming%250Alanguages%252C%2520such%2520as%2520OpenAI%2520Gymnasium%2520using%2520Python.%2520These%2520frameworks%2520provide%250Avarious%2520API%2520and%2520benchmarks%2520for%2520testing%2520RL%2520algorithms%2520in%2520different%2520domains%252C%2520such%250Aas%2520autonomous%2520driving%2520%2528AD%2529%2520and%2520robotics.%2520These%2520platforms%2520often%2520emphasise%2520the%250Adesign%2520of%2520RL%2520algorithms%2520and%2520the%2520training%2520performance%2520but%2520neglect%2520the%250Acorrectness%2520of%2520models%2520and%2520reward%2520functions%252C%2520which%2520can%2520be%2520crucial%2520for%2520the%250Asuccessful%2520application%2520of%2520RL.%2520This%2520paper%2520proposes%2520using%2520formal%2520methods%2520to%2520model%250AAD%2520systems%2520and%2520demonstrates%2520how%2520model%2520checking%2520%2528MC%2529%2520can%2520be%2520used%2520in%2520RL%2520for%2520AD.%250AMost%2520studies%2520combining%2520MC%2520and%2520RL%2520focus%2520on%2520safety%252C%2520such%2520as%2520safety%2520shields.%250AHowever%252C%2520this%2520paper%2520shows%2520different%2520facets%2520where%2520MC%2520can%2520strengthen%2520RL.%2520First%252C%250Aan%2520MC-based%2520model%2520pre-analysis%2520can%2520reveal%2520bugs%2520with%2520respect%2520to%2520sensor%2520accuracy%250Aand%2520learning%2520step%2520size.%2520This%2520step%2520serves%2520as%2520a%2520preparation%2520of%2520RL%252C%2520which%2520saves%250Atime%2520if%2520bugs%2520exist%2520and%2520deepens%2520users%2527%2520understanding%2520of%2520the%2520target%2520system.%250ASecond%252C%2520reward%2520automata%2520can%2520benefit%2520the%2520design%2520of%2520reward%2520functions%2520and%2520greatly%250Aimprove%2520learning%2520performance%2520especially%2520when%2520the%2520learning%2520objectives%2520are%250Amultiple.%2520All%2520these%2520findings%2520are%2520supported%2520by%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Checking%20for%20Reinforcement%20Learning%20in%20Autonomous%20Driving%3A%20One%20Can%0A%20%20Do%20More%20Than%20You%20Think%21&entry.906535625=Rong%20Gu&entry.1292438233=%20%20Most%20reinforcement%20learning%20%28RL%29%20platforms%20use%20high-level%20programming%0Alanguages%2C%20such%20as%20OpenAI%20Gymnasium%20using%20Python.%20These%20frameworks%20provide%0Avarious%20API%20and%20benchmarks%20for%20testing%20RL%20algorithms%20in%20different%20domains%2C%20such%0Aas%20autonomous%20driving%20%28AD%29%20and%20robotics.%20These%20platforms%20often%20emphasise%20the%0Adesign%20of%20RL%20algorithms%20and%20the%20training%20performance%20but%20neglect%20the%0Acorrectness%20of%20models%20and%20reward%20functions%2C%20which%20can%20be%20crucial%20for%20the%0Asuccessful%20application%20of%20RL.%20This%20paper%20proposes%20using%20formal%20methods%20to%20model%0AAD%20systems%20and%20demonstrates%20how%20model%20checking%20%28MC%29%20can%20be%20used%20in%20RL%20for%20AD.%0AMost%20studies%20combining%20MC%20and%20RL%20focus%20on%20safety%2C%20such%20as%20safety%20shields.%0AHowever%2C%20this%20paper%20shows%20different%20facets%20where%20MC%20can%20strengthen%20RL.%20First%2C%0Aan%20MC-based%20model%20pre-analysis%20can%20reveal%20bugs%20with%20respect%20to%20sensor%20accuracy%0Aand%20learning%20step%20size.%20This%20step%20serves%20as%20a%20preparation%20of%20RL%2C%20which%20saves%0Atime%20if%20bugs%20exist%20and%20deepens%20users%27%20understanding%20of%20the%20target%20system.%0ASecond%2C%20reward%20automata%20can%20benefit%20the%20design%20of%20reward%20functions%20and%20greatly%0Aimprove%20learning%20performance%20especially%20when%20the%20learning%20objectives%20are%0Amultiple.%20All%20these%20findings%20are%20supported%20by%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14375v1&entry.124074799=Read"},
{"title": "Uterine Ultrasound Image Captioning Using Deep Learning Techniques", "author": "Abdennour Boulesnane and Boutheina Mokhtari and Oumnia Rana Segueni and Slimane Segueni", "abstract": "  Medical imaging has significantly revolutionized medical diagnostics and\ntreatment planning, progressing from early X-ray usage to sophisticated methods\nlike MRIs, CT scans, and ultrasounds. This paper investigates the use of deep\nlearning for medical image captioning, with a particular focus on uterine\nultrasound images. These images are vital in obstetrics and gynecology for\ndiagnosing and monitoring various conditions across different age groups.\nHowever, their interpretation is often challenging due to their complexity and\nvariability. To address this, a deep learning-based medical image captioning\nsystem was developed, integrating Convolutional Neural Networks with a\nBidirectional Gated Recurrent Unit network. This hybrid model processes both\nimage and text features to generate descriptive captions for uterine ultrasound\nimages. Our experimental results demonstrate the effectiveness of this approach\nover baseline methods, with the proposed model achieving superior performance\nin generating accurate and informative captions, as indicated by higher BLEU\nand ROUGE scores. By enhancing the interpretation of uterine ultrasound images,\nour research aims to assist medical professionals in making timely and accurate\ndiagnoses, ultimately contributing to improved patient care.\n", "link": "http://arxiv.org/abs/2411.14039v1", "date": "2024-11-21", "relevancy": 1.5135, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4939}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uterine%20Ultrasound%20Image%20Captioning%20Using%20Deep%20Learning%20Techniques&body=Title%3A%20Uterine%20Ultrasound%20Image%20Captioning%20Using%20Deep%20Learning%20Techniques%0AAuthor%3A%20Abdennour%20Boulesnane%20and%20Boutheina%20Mokhtari%20and%20Oumnia%20Rana%20Segueni%20and%20Slimane%20Segueni%0AAbstract%3A%20%20%20Medical%20imaging%20has%20significantly%20revolutionized%20medical%20diagnostics%20and%0Atreatment%20planning%2C%20progressing%20from%20early%20X-ray%20usage%20to%20sophisticated%20methods%0Alike%20MRIs%2C%20CT%20scans%2C%20and%20ultrasounds.%20This%20paper%20investigates%20the%20use%20of%20deep%0Alearning%20for%20medical%20image%20captioning%2C%20with%20a%20particular%20focus%20on%20uterine%0Aultrasound%20images.%20These%20images%20are%20vital%20in%20obstetrics%20and%20gynecology%20for%0Adiagnosing%20and%20monitoring%20various%20conditions%20across%20different%20age%20groups.%0AHowever%2C%20their%20interpretation%20is%20often%20challenging%20due%20to%20their%20complexity%20and%0Avariability.%20To%20address%20this%2C%20a%20deep%20learning-based%20medical%20image%20captioning%0Asystem%20was%20developed%2C%20integrating%20Convolutional%20Neural%20Networks%20with%20a%0ABidirectional%20Gated%20Recurrent%20Unit%20network.%20This%20hybrid%20model%20processes%20both%0Aimage%20and%20text%20features%20to%20generate%20descriptive%20captions%20for%20uterine%20ultrasound%0Aimages.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20this%20approach%0Aover%20baseline%20methods%2C%20with%20the%20proposed%20model%20achieving%20superior%20performance%0Ain%20generating%20accurate%20and%20informative%20captions%2C%20as%20indicated%20by%20higher%20BLEU%0Aand%20ROUGE%20scores.%20By%20enhancing%20the%20interpretation%20of%20uterine%20ultrasound%20images%2C%0Aour%20research%20aims%20to%20assist%20medical%20professionals%20in%20making%20timely%20and%20accurate%0Adiagnoses%2C%20ultimately%20contributing%20to%20improved%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUterine%2520Ultrasound%2520Image%2520Captioning%2520Using%2520Deep%2520Learning%2520Techniques%26entry.906535625%3DAbdennour%2520Boulesnane%2520and%2520Boutheina%2520Mokhtari%2520and%2520Oumnia%2520Rana%2520Segueni%2520and%2520Slimane%2520Segueni%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520has%2520significantly%2520revolutionized%2520medical%2520diagnostics%2520and%250Atreatment%2520planning%252C%2520progressing%2520from%2520early%2520X-ray%2520usage%2520to%2520sophisticated%2520methods%250Alike%2520MRIs%252C%2520CT%2520scans%252C%2520and%2520ultrasounds.%2520This%2520paper%2520investigates%2520the%2520use%2520of%2520deep%250Alearning%2520for%2520medical%2520image%2520captioning%252C%2520with%2520a%2520particular%2520focus%2520on%2520uterine%250Aultrasound%2520images.%2520These%2520images%2520are%2520vital%2520in%2520obstetrics%2520and%2520gynecology%2520for%250Adiagnosing%2520and%2520monitoring%2520various%2520conditions%2520across%2520different%2520age%2520groups.%250AHowever%252C%2520their%2520interpretation%2520is%2520often%2520challenging%2520due%2520to%2520their%2520complexity%2520and%250Avariability.%2520To%2520address%2520this%252C%2520a%2520deep%2520learning-based%2520medical%2520image%2520captioning%250Asystem%2520was%2520developed%252C%2520integrating%2520Convolutional%2520Neural%2520Networks%2520with%2520a%250ABidirectional%2520Gated%2520Recurrent%2520Unit%2520network.%2520This%2520hybrid%2520model%2520processes%2520both%250Aimage%2520and%2520text%2520features%2520to%2520generate%2520descriptive%2520captions%2520for%2520uterine%2520ultrasound%250Aimages.%2520Our%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%250Aover%2520baseline%2520methods%252C%2520with%2520the%2520proposed%2520model%2520achieving%2520superior%2520performance%250Ain%2520generating%2520accurate%2520and%2520informative%2520captions%252C%2520as%2520indicated%2520by%2520higher%2520BLEU%250Aand%2520ROUGE%2520scores.%2520By%2520enhancing%2520the%2520interpretation%2520of%2520uterine%2520ultrasound%2520images%252C%250Aour%2520research%2520aims%2520to%2520assist%2520medical%2520professionals%2520in%2520making%2520timely%2520and%2520accurate%250Adiagnoses%252C%2520ultimately%2520contributing%2520to%2520improved%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uterine%20Ultrasound%20Image%20Captioning%20Using%20Deep%20Learning%20Techniques&entry.906535625=Abdennour%20Boulesnane%20and%20Boutheina%20Mokhtari%20and%20Oumnia%20Rana%20Segueni%20and%20Slimane%20Segueni&entry.1292438233=%20%20Medical%20imaging%20has%20significantly%20revolutionized%20medical%20diagnostics%20and%0Atreatment%20planning%2C%20progressing%20from%20early%20X-ray%20usage%20to%20sophisticated%20methods%0Alike%20MRIs%2C%20CT%20scans%2C%20and%20ultrasounds.%20This%20paper%20investigates%20the%20use%20of%20deep%0Alearning%20for%20medical%20image%20captioning%2C%20with%20a%20particular%20focus%20on%20uterine%0Aultrasound%20images.%20These%20images%20are%20vital%20in%20obstetrics%20and%20gynecology%20for%0Adiagnosing%20and%20monitoring%20various%20conditions%20across%20different%20age%20groups.%0AHowever%2C%20their%20interpretation%20is%20often%20challenging%20due%20to%20their%20complexity%20and%0Avariability.%20To%20address%20this%2C%20a%20deep%20learning-based%20medical%20image%20captioning%0Asystem%20was%20developed%2C%20integrating%20Convolutional%20Neural%20Networks%20with%20a%0ABidirectional%20Gated%20Recurrent%20Unit%20network.%20This%20hybrid%20model%20processes%20both%0Aimage%20and%20text%20features%20to%20generate%20descriptive%20captions%20for%20uterine%20ultrasound%0Aimages.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20this%20approach%0Aover%20baseline%20methods%2C%20with%20the%20proposed%20model%20achieving%20superior%20performance%0Ain%20generating%20accurate%20and%20informative%20captions%2C%20as%20indicated%20by%20higher%20BLEU%0Aand%20ROUGE%20scores.%20By%20enhancing%20the%20interpretation%20of%20uterine%20ultrasound%20images%2C%0Aour%20research%20aims%20to%20assist%20medical%20professionals%20in%20making%20timely%20and%20accurate%0Adiagnoses%2C%20ultimately%20contributing%20to%20improved%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14039v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


